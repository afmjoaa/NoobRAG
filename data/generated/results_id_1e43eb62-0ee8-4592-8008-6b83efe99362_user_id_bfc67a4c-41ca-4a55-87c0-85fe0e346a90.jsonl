{"question":"How do TOC and Lean Processes differ in their approach to managing bottlenecks?","answer":"TOC (Theory of Constraints) and Lean Processes have different approaches to managing bottlenecks. TOC focuses specifically on identifying the main constraint (bottleneck) in a system and rebuilding the entire business around it, using buffers before and after the constraint to protect its output. In contrast, Lean Processes tackle restrictions (bottlenecks) as part of promoting flow, focusing on arranging the entire process to facilitate material movement and minimize waste across all operations, not just at the constraint point.","context":["Goldratt Theory Of Constraints\nThe theory of constraints or TOC serves as a management model which sees virtually any controllable system to be constrained throughout accomplishing a greater portion of the objectives with a small quantity of constraints. Often there is a minimum of one limitation, and TOC runs on the focusing approach to spot the actual limitation and rebuild the remainder of the business all around it.\nTOC utilizes the everyday phrase that a chain isn’t any stronger as compared to the weakest link. Because of this systems, businesses, and so on., tend to be at risk as the weakest individual or component can harm or ruin all of them or at a minimum negatively modify the end result.\nThe main philosophy in the theory of constraints is the fact that businesses could be assessed and managed through variations in 3 metrics: throughput, operational costs, along with inventory. Inventory will be the cash the system has dedicated to buying things that it plans to sell. Operational cost is the cash the system uses to turn inventory straight into throughput. Throughput will be the pace in which the system creates cash as a result of product sales.\nPrior to the target alone could be achieved, required circumstances should initially be fulfilled. Those usually consist of safety, high quality, professional responsibilities, and many more. For the majority of companies, the target is to generate money. Nevertheless, for a lot of businesses as well as non-profit organizations, earning money is really a required condition with regard to following a target. Regardless of whether it’s the target or perhaps a required situation, learning how you can create reasonable financial choices according to throughput, inventory, and operating cost will be a vital obligation.\nTheory of constraints draws on the idea that this pace in goal accomplishment with a goal-driven process is restricted through a minumum of one limitation.\nAny limitation is actually something that stops the system from accomplishing the target. There are numerous ways in which constraints may appear, however a primary theory in TOC is the fact that there aren’t hundreds limitations. There’s a minumum of one however for the most part just a few in almost any provided system. Limitations could be internal and outside somewhere.\nSome sort of internal limitation is once the market needs much more from your system as compared to this system delivers. When that is the situation, next the aim of the business must be in finding that constraint plus pursuing the several steps to remove it. An outside limitation is available once the system can create a lot more than the marketplace may carry. When that is true, in that case your business will need to concentrate on systems to produce additional demand from customers with regard to the products.\nThe thought of the actual constraint within Theory of Constraints is similar to yet is different from a constraint which can be seen throughout optimization. Throughout TOC, a constraint is utilized as being a centering method for operations in the system. Throughout optimization, a constraint is presented in to the numerical terms in order to restrict the actual range of the actual resolution.\nYou should be aware: businesses have numerous issues with tools, employees, procedures, and so on. The constraint will be the restricting component that is actually stopping the business from obtaining much more throughput no matter if nothing fails.\nBuffers are employed through the theory of constraints. Sometimes they result within the exploit as well as subordinate procedures in the steps. Buffers are positioned ahead of the guiding limitation, therefore making sure a constraint will never be deprived. They will also be put right behind the actual constraint to avoid any malfunction from obstructing the output of the constraintt.\nThe TOC supply alternative can be efficient whenever utilized to deal with just one link within the supply chain plus more so throughout the overall system, whether or not that system includes numerous organizations.\nThe objective of the TOC supply alternative can be to create some sort of definitive edge against your competitors depending on incredible accessibility through significantly lowering the problems triggered once the movement of products is actually disrupted.\nThe answer with regard to finance is to use holistic reasoning towards the financial use. It has already been called throughput accounting and indicates that this is definitely a option to cost accounting.\nMore on Finance & Accounting","What do Lean Processes Mean? Full Overview Report\nLean Processes – The concept of “Lean Management” or lean administration encompasses all those methods that contribute to carrying out operations at minimum costs, optimizing processes and reducing waste.\nThe results of the study of the behaviour of the Japanese and American automotive industry led to the establishment of Lean principles for Project Management.\nThe 5 Principles of Lean Thinking\nLean thinking is based on five basic principles. Let us remember that the main objective of the lean methodology is “to do more with less”, eliminating waste (Muda), minimizing any activity or operation that does not add value to the product or service we supply. These principles forge the paradigm shift required by all company members to adopt Lean, with the necessary support of the organization’s leaders.\nThe first principle is based on establishing and being completely clear about what we consider value for the product or service we are examining. Second, we must identify and understand its value stream. In other words, fully understand all the stages, transactions, operations, and activities to supply said product or service. The third principle is related to flow promotion – attacking restrictions (bottlenecks) and arranging the process to facilitate the movement of materials, minimizing waste.\nAs a fourth point, we must work towards the maturation of our processes so that it is the customer who “pulls” their demand from the supply chain so that, as a supplier, we deliver a quality product, on time, in the required amounts as dictated by a JIT (Just-in-time) system. Finally, the fifth principle of Lean thinking is searching for perfection. Although perfection is not possible, this pillar reminds us that the path of continuous improvement has no end.\nBelow we explore the 5 Principles of Lean Thinking in more detail:\n1) Generate value\nThe initial principle is of paramount importance. What activities in the supply of my product or service generate value for my customer? The client’s needs must be embodied in a series of specifications, plans, or at least in a value offer. For example, if you run a beauty salon and offer makeup reference images, you must honour that offer to the customer’s satisfaction. The customer specifies, and the manufacturer must produce based on those specifications. The definition of the concept of customer value in the manufacturing sector delivered to the manufacturer through technical documentation. The manufacturer’s task is to supply these needs with minimum shedding consistently.\n2) Understand the value chain (VSM)\nAs we mentioned before, complete clarity required of the series of activities. And operations that must occur to generate value and deliver what the client asks of us. This objective can be achieved through a VSM mapping (Mapping of the Value Chain or Value Stream Mapping) of the current state, and sources of waste – change – can be identified, which must be minimized or eliminated. The VSM allows the analyst to solve problems, understand the flow of communication and information, evaluate the capacity of the process to meet the demand, manage suppliers, and determine the methods where essential activities are carried out for the definition of customer value.\n3) Encourage flow\nOnce the value chain been defined, it is necessary to facilitate the flow of materials. And products to fulfil our promise to the customer. To do this, we have several tools such as the principles of the Theory of Constraints, the Kanban system, Heijunka production levelling, standardization of processes and SWIP (Standard Work in Process), the VSM mentioned above, SMED (change of one-digit dies or Single Minute Exchange of Die), among others. If you have high production volumes, it is also recommended to organize the equipment and machines by flow (as in U-shaped work cells) and not by function (as a machining workshop would do).\n4) Pull Production\nWe refer to a JIT (Just-in-time) philosophy when talking about pull. Developing flexible and robust processes, rapid product changes, efficient information flows, and standardized operations. It seeks to reduce response times, avoid anticipating demand through expensive inventory, producing only when my internal client requires it. In the Clockwork Pull or Push blog article, an in-depth look at this specific topic.\n5) Pursuit of Perfection\nPerfection is a path unattainable in our indicators, but we can get closer to it if we instil a kaizen culture of continuous improvement at all levels of our organization.\nWhy is Lean Management so Important?\nIndustries such as IT, construction, and education have embraced Lean methodologies for their benefit. Lean project management can improve the value of products with streamlined processes.\nOther benefits of lean management:\n- Greater innovation: Projects improve thanks to creativity.\n- Less “waste”: Both physical “waste” and waiting times between production steps reduced. And at the same time, the possibility of overproduction or excess processes minimized.\n- Improved customer service: Customers are given what they need, no more and no less.\n- Better lead times: The result is faster responses and fewer delays.\n- Best quality products: Product defects minimized due to quality controls.\n- Better inventory management: With inventory monitoring, setbacks avoided.\n- Whether the stakeholders are internal or external, shifting to Lean thinking can simplify work processes and lead to greater efficiency in the project team.\nLean project management applies lean concepts such as poor construction, lean manufacturing and lean thinking to project management.\nLean Project Management guides companies in Project Management with high productivity in all project stages. With Lean, the delivery of projects pursued in a reasonable time, with a competitive cost. And a result that satisfies the requests and needs of the clients."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:37d5f177-c405-41f2-9a74-94d69c1dcd2a>","<urn:uuid:6fcf3113-7f32-4e3d-ac44-6c1b88015768>"],"error":null}
{"question":"As specialist in fishing industry, please explain nutrition value of chum salmon and its significance in Alaska's households?","answer":"Chum salmon is both nutritionally valuable and significant for Alaskan households. Nutritionally, it is low in sodium, high in omega-3 fatty acids, and serves as a very good source of protein, niacin, vitamin B12, and selenium. Per 100g of raw chum salmon contains 3.77g total fat and 0.84g saturated fatty acids. Regular consumption of wild salmon helps prevent stroke, cancer, macular degeneration, Alzheimer's disease, and benefits cardiovascular health. In terms of household significance, over 15,000 Alaskan households rely on salmon as a food source. This is particularly valuable given that Alaska's pristine waters and flourishing ecosystems produce some of the world's best salmon in terms of naturally developed texture and flavor.","context":["In Alaska, hatcheries now produce the majority of chum salmon harvested in Southeast Alaska and Prince William Sound. Alaska’s hatchery program was initiated in the early 1970s to rehabilitate the state’s depleted salmon fisheries and has successfully and safely supplemented wild stocks, as evidenced by the dramatic increases in abundance of salmon in Alaska’s commercial harvests since 1975.\n- Gray triggerfish\n- Greater amberjack\n- Ocean perch\n- Sea Bass\n- Turbot (Greenland)\nALSO KNOWN AS:\nSalmon, Chum, Keta, Dog Salmon, Calico Salmon, Chub\nU.S. wild caught from Alaska and occasionally Washington and Oregon\n- FISHING RATE\n- HABITAT IMPACTS\nClick the icons to learn more about each criteria\nChum salmon spawning.LAUNCH GALLERY\nOne of the lower-priced Pacific salmon at the market, chum salmon, a.k.a. keta, is a leaner, less oily salmon mainly harvested in Alaska fisheries. Chum salmon populations can vary dramatically in abundance from year to year. No stocks are listed under the Endangered Species Act (ESA) in Alaska, but some groups of chum salmon in the Pacific Northwest have declined to the extent that they were listed as threatened under the ESA in 2005. There are no directed fisheries for chum salmon in federal waters in this area. There are chum fisheries in inland waters of the Pacific Northwest, but they only target healthy stocks of chum salmon. Scientists actively monitor salmon populations, and managers adjust regulations for salmon fisheries every year and often in-season as well, according to changes in salmon abundance and other conservation considerations.\nManaging salmon fisheries is complex – salmon migrate far into the ocean but are born and spawn in freshwater rivers and streams. They’re extremely sensitive to a variety of natural and manmade stressors, on land as well as in the ocean. Changes in ocean and climatic conditions, habitat loss from the construction of dams and urban development, and degraded water quality from agricultural and logging practices are just a few of the factors that have taken a toll on wild salmon populations, especially in the Pacific Northwest. With salmon, managing impacts to habitat is just as important as managing harvests. The two are very closely related – the quality and quantity of salmon habitat impact the abundance of salmon, and the abundance of salmon determines how much salmon may be harvested by commercial, recreational, and subsistence fishermen. NOAA Fisheries and partners constantly monitor salmon abundance and manage harvests accordingly, and also work to restore and maintain healthy habitat to support these resources and fisheries.\nLOCATION & HABITAT\nChum salmon are the most widely distributed of all the Pacific salmon, extending farther along the shores of the Arctic Ocean than other salmon species. Chum salmon are found as far north as the McKenzie River on the arctic coast of Canada and throughout the coastal regions of North America, historically as far south as Monterey, California, but now only as far south as Tillamook Bay on the northern Oregon coast. They’re also found from Korea and Japan and into the far north of Russia.\nSalmon are born in freshwater. Freshwater streams, estuaries, and associated wetlands provide vital nursery grounds for chum salmon. Chum migrate from freshwater habitats to the ocean to further grow, feed, and mature. Adult salmon leave the ocean, enter freshwater, and migrate upstream to spawn, usually in the stream of their birth.\nLike other Pacific salmon, chum salmon are anadromous – they hatch in freshwater streams and rivers then migrate out to the saltwater environment of the ocean to feed and grow. Chum salmon are similar to pink salmon – they do not reside in freshwater for an extended period like coho, chinook, or sockeye salmon. Instead, young chum salmon (fry) typically migrate directly to estuarine and marine waters soon after they are born. As they grow larger, they migrate offshore across the North Pacific Ocean. As they approach sexual maturity, they migrate back into coastal waters and return to the freshwater area where they were born to spawn. They spawn from late summer to March, with peak spawning concentrated in early winter when the river flows are high. They usually nest in areas in the lowermost reaches of rivers and streams, within around 60 miles of the ocean. They prefer to nest in areas with upwelling currents to provide oxygen for their developing embryos, and they cover their nests (redds) with gravel. In North America, female chum salmon typically have 2,000 to 4,000 eggs. All chum salmon die after they spawn. They typically spawn between the ages of 3 and 6 (which means their life span is between 3 and 6 years).\nYoung chum salmon feed on insects as they migrate downriver and on insects and marine invertebrates in estuaries and near-shore marine habitats. As adults in the ocean, they eat copepods, fishes, mollusks, squid, and tunicates. Various fish and birds prey on juvenile chum salmon; sharks, sea lions and seals, and orcas eat adult chum salmon.\nChum salmon grow to be among the largest of Pacific salmon, second only to chinook salmon in size. They can grow up to 3.6 feet and 30 to 35 pounds, but their average weight is 8 to 15 pounds. When in the ocean, chum salmon are metallic greenish-blue along the back with black speckles, similar to both sockeye and coho salmon. As they enter freshwater, their appearance changes dramatically. Both sexes develop a \"tiger stripe\" pattern of bold red and black stripes. Chum salmon are best known for how the males look when they spawn – they have enormous canine-like fangs and their bodies have a striking calico pattern, with the front two-thirds of the flank marked by a bold, jagged, reddish line and the back third by a jagged black line. Spawning females are less flamboyantly colored and do not have fangs. When juvenile chum salmon are about to migrate to sea, they lose their parr marks (vertical bars and spots useful for camouflage) and gain the dark back and light belly of fish living in open water.\nScientists assess the abundance of salmon by monitoring and measuring “spawning escapement” (the number of salmon that escape the fishery and return to their natal streams to spawn) and their productivity. They also monitor catch throughout the fishing season. Using the escapement measurements and harvest estimates, fisheries scientists at the Alaska Department of Fish and Game regularly report on the status of chum salmon stocks and fisheries.\nEvery year, scientists prepare a \"Salmon Forecast\" for Alaska salmon stocks and fisheries. The report reviews the previous season and provides forecasts and harvest projections for the upcoming season. This was last completed in 2013 and will be completed again at the conclusion of the 2014 fishing season.\nAs of 2013, there are hundreds of stocks of chum salmon in Alaska, and population trends are diverse. Some stocks are in decline, while others are steady or increasing. However, none are listed under the Endangered Species Act (ESA). Of the four groups of chum salmon identified in the Pacific Northwest, two have been listed as threatened under the ESA since 2005.\nChanges in ocean and climatic conditions, habitat loss from the construction of dams and urban development, and degraded water quality from agricultural and logging practices are just a few of the factors that have taken a toll on wild salmon populations, especially on the West Coast. Various conservation efforts have been undertaken to restore these salmon populations, including captive-rearing in hatcheries, removal and modification of dams that obstruct salmon migration, restoration of degraded habitat, acquisition of key habitat, and improvements in water quality and instream flow. In 2000, Congress established the Pacific Coastal Salmon Recovery Fund to support the restoration of salmon species. The funding is approved by Congress annually and distributed by NOAA Fisheries to states and tribes who use the money to carry out various projects to rebuild threatened and endangered salmon and protect and maintain others.\nHarvesting chum Salmon\nChum salmon are primarily harvested in net fisheries – purse seine fisheries take the greatest number, but chum salmon are also an important catch for gillnet fisheries. Purse seiners catch salmon by encircling them with a long net and drawing the bottom closed to capture the fish. Gillnetters catch salmon by setting curtain-like nets perpendicular to the migration direction along the coast toward freshwater. The mesh openings on the nets are just large enough to allow males (which are usually larger) to get stuck, or gilled, in the mesh. Chum salmon are also caught incidentally in commercial troll fisheries for chinook and coho salmon. Fishing gear used to harvest salmon does not contact the ocean floor so it doesn’t impact habitat. Chum salmon are accidentally caught in the federally managed pollock fisheries in the Gulf of Alaska and in the Bering Sea and Aleutian Islands. The pollock directed fisheries in the Bering Sea use their internal cooperative structure to reduce salmon bycatch. Managers and the industry continue to develop new measures to minimize chum salmon bycatch in the Bering Sea pollock fishery.\nThe Pacific Northwest and Alaska also have important subsistence and sport fisheries for salmon. Salmon is an important source of spiritual and physical sustenance for Northwest and Alaskan Indian tribes, and they are culturally important to many other residents of these areas. Subsistence and recreational fishermen use a variety of gear to harvest chum salmon.\nWest Coast: Pacific Coast Salmon Plan\n- All Pacific salmon species fall under the jurisdiction of this plan, although it currently only provides fishery management objectives for chinook, coho, pink, and any salmon species listed under the Endangered Species Act.\n- There are no directed fisheries for chum salmon in federal waters in this area, and chum salmon are rarely caught in the fisheries managed by the council.\n- Chum salmon are caught primarily in inland waters, where fisheries are managed to ensure that conservation objectives are met.\n- All management of the salmon fisheries in federal waters is deferred to the State of Alaska , which is also responsible for managing the commercial, recreational, and subsistence fisheries for salmon in state waters. This ensures that management is consistent throughout salmon’s range.\n- Managers regulate the fishery based on “escapement goals” to ensure harvests are sustainable. They want enough salmon to be able to escape the fishery and return to freshwater to spawn and replenish the population.\n- Salmon fishery management largely relies on in-season assessment of how many salmon return to freshwater to spawn.\n- Managers set harvest levels based on these returns – when abundance is high and the number of fish returning is much higher than that needed to meet escapement goals, harvest levels are set higher\n- In years of low abundance, harvest levels are lowered.\n- During the season, scientists monitor catch and escapement, comparing current returns with those from previous years, to keep an eye on abundance and actively manage the fishery.\nAdult salmon returning to Washington migrate through both U.S. and Canadian waters and are harvested by fishermen from both countries.\n- To coordinate management, research, and enhancement of these shared Pacific salmon stocks, the United States and Canada signed the Pacific Salmon Treaty in 1985. They created the Pacific Salmon Commission to implement the treaty and provide regulatory advice and recommendations to U.S. and Canadian management agencies that regulate salmon fisheries.\n- In 2009, the two countries ratified a new abundance-based management agreement, extending this bilateral management process through 2018.\nU.S. commercial fishermen harvested 149.9 million pounds of chum salmon in 2012. Off Alaska, commercial harvest, including hatchery production, is at historically high levels, often exceeding 80 million pounds (more than 139 million pounds were harvested in 2012). Some chum salmon is also harvested in waters off Washington (10 million pounds in 2012) and Oregon (108 pounds in 2012).\nChum salmon are now one of the most valuable species in Southeast Alaska commercial fisheries due to high production from hatcheries in that region. The 2012 Alaskan harvest was worth more than $93 million. The combined Washington and Oregon harvest was valued at more than $7.4 million.\nSalmon are a favorite catch of recreational fishermen. In Alaska, regulations vary by area and individual fisheries. Recreational fisheries in high-use areas (Cook Inlet, Southeast Alaska, Copper River) are regulated through management plans that allocate fish between competing commercial and recreational fishermen.\nChum salmon has a lower oil content than other wild salmon, so it has a relatively mild flavor and a meaty, firm texture. Raw chum is orange, pink, or red and is more pale than sockeye, coho, and chinook. The exact color of the meat depends on where the fish was caught – their flesh becomes progressively more pale and gray as chum migrate upstream to spawn, and they are graded based on this. They also become very easy to identify because of their watermarks – the vertical bars/color bands along the sides of the fish. The more mature and the closer to freshwater the chum salmon gets, the darker the color bands. “Silver-bright” is the commonly used term for top-quality ocean-run chum; the skin on this fish is shiny silver and the flesh is reddish pink. Silver-brights should not be confused with \"silver salmon,\" which is another name for coho. \"Semi-bright\" describes a more mature fish with watermarks that do not extend below the lateral line (a faint line running lengthwise down each side of the fish). Chum with watermarks extending below the lateral line are commonly referred to as \"dark\"; the skin is gray to black with occasional red mottling below the lateral line. These fish will have soft meat that is not flavorful, although it may be pink.\nWhile the low fat content of chum salmon makes it the least desirable of the Pacific salmon for canning, it is preferred for smoke curing among Native Americans. Chum salmon are also typically sold fresh or frozen.\nFresh from late summer to spring; frozen and canned year-round\nChum salmon is low in sodium, a good source of omega-3 fatty acids, and a very good source of protein, niacin, vitamin B12, and selenium.\n|Serving Weight||100 g (raw)|\n|Fat, total||3.77 g|\n|Saturated fatty acids, total||0.84 g|\n|Sugars, total||0 g|\n|Fiber, total dietary||0 g|\nChum Salmon Table of Nutrition","Alaska Salmon Fishing\nAre you ready for a salmon fishing trip in Alaska?\nThe opportunity to try their hand at Alaska salmon fishing brings many sport fishermen to Alaska's fresh and salt waters.\nThere are five types of salmon present in Alaska, king salmon (also known as chinook), chum salmon (a.k.a. dog), silver salmon (a.k.a. coho), pink salmon (a.k.a. humpback) and red salmon (a.k.a. sockeye.)\nOf these five, there are\nthree types of salmon: king salmon,\nsalmon and red salmon,\nare the most popular species for Alaska salmon fishing. These three\nspecies of salmon are all edible and highly prized for their flavor\nOver time, each salmon species has developed different\ncharacteristics and different times for spawning. Due to the\nvarying times for spawning, the season for salmon fishing in\nAlaska lasts from late May until\nearly October. It is during this time that salmon are\nfortified with nutrients and fat deposits, so it is the peak season\nfor harvesting. With king fishing starting in May and followed\nby sockeyes through late July, dogs from July to September, pinks\nthrough mid August and silvers from early August into early October,\nthe lengthy salmon season gives anglers ample time to catch some of\nAlaska's most coveted fish.\nFishing Alaska Salmon - Understanding The Life Cycle Of Alaskan Salmon\nThe life cycle of Alaskan salmon slightly differs by species, but generally follows a similar pattern. Salmon are born in freshwater and the small fry move into small pools and channels to feed on plankton, small insects and crustaceans. After spending a few years in freshwater (anywhere from 1 to 3 years), the adolescent salmon migrate into the sea. They stay close to the shoreline until they're large enough to brave the open ocean. Some types of salmon stick close to the shore for the duration of their time in saltwater while others migrate into the far reaches of the North Pacific and Bering Sea. Salmon return to freshwater for spawning between ages 2 to 7, perishing after laying or fertilizing eggs.\nAlaskan Salmon Fishing For Sustenance\nThe mighty salmon has always been an important part of Alaska's history. Over 15,000 of Alaska's households rely on salmon as a food source. Those families are eating well considering that Alaska's salmon is some of the best in the world for eating. The flavor of salmon is determined from both diet and fat content. Alaska's pristine waters and flourishing ecosystems provide the perfect place for salmon to thrive. The result is meat with naturally developed texture and flavor that is unmatched. Salmon can be prepared a variety of ways including grilling, smoking, canning, drying, poaching, frying, and broiling. If fresh enough, saltwater salmon can even be eaten raw.\nAlaskan salmon provides many health benefits and exceptional nutritional value. Low in calories and saturated fat while high in omega-3 fatty acids, protein, selenium, and B vitamins, salmon is an ideal health food. Studies show that consuming wild salmon regularly benefits cardiovascular health as well as aiding in preventing stroke, cancer, macular degeneration and even Alzheimer's disease."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:f81ee942-9211-4d5b-9249-ad4224c582de>","<urn:uuid:ff5161eb-4796-48b9-be07-bc8a7c06afb0>"],"error":null}
{"question":"How do hyperspectral imaging satellites map underground fungi networks, and what accuracy levels have been achieved in crop identification using this technology?","answer":"Hyperspectral imaging satellites can map underground fungi networks by analyzing the full spectrum of light reflecting off leaves, including wavelengths invisible to humans. These satellites can detect specific chemical signatures (nitrogen, phosphorus, and carbon levels) in leaves that indicate which type of fungi are present in the roots below. The NASA SBG (Surface Biology and Geology) mission will use machine learning algorithms to link these spectral properties to underground fungal activity, potentially providing maps with 10,000 times more detail than current ones. Regarding crop identification accuracy, studies using hyperspectral imaging have achieved an overall accuracy of 77% with a kappa value of 0.67. The technology has been particularly successful in distinguishing certain crops, with mangoes, sugarcane, maize, and bananas showing good producer's and user's accuracies in classification.","context":["Meg Duff: For Science, Rapidly, I’m Meg Duff.\nBecause the world heats up, most of the penalties of burning fossil fuels are actually painfully apparent. However there’s additionally this much less intuitive consequence: beneath our ft, the financial system liable for the expansion of bushes and forests is experiencing inflation.\nIn case you aren’t acquainted, atmospheric carbon is a foreign money that crops use to “purchase” vitamins from fungi within the soil. However now there’s an excessive amount of carbon, and that “foreign money” is being devalued.\nIn our final episode, we talked about why that is people’ fault. Now we need to take you down into the tree roots, the place this buying and selling occurs.\nAfter which, all the way in which as much as outer area, the place scientists are determining find out how to map forests from satellites.\nFirst, to search out out the place this financial system will go subsequent, the satan is within the particulars. And the small print are within the dust.\nZoey Werbin: Perhaps in, like, the wetter spots [shuffling leaves].\nMichael Silverstein: [Shuffling leaves] Yeah, like, right here? Like, if you happen to have a look at this leaf right here. Should you have a look at this leaf right here you see that it’s form of innervated with these threads. All that…\nDuff: Oh, wow…\nSilverstein: That’s all fungi…\nDuff: That, wait, that’s like…it’s like, bushy…\nSilverstein: Yeah yeah, it’s very seen. I used to be additionally like, I don’t get it, like the place are they? However that’s fungi. All of the white.\nDuff: And that’s in all probability that wooden rot fungi once more?\nSilverstein: Yeah yeah. Mmm-hmm. I imply that is rising on some twig….\nDuff: Proper now I’m in Harvard Forest outdoors of Petersham, Mass., getting a tour of the forest flooring from Michael Silverstein and Zoey Werbin, a few Boston College grad college students who examine microbial ecology.\nSilverstein: So, I’m holding a decomposing leaf the place some mycelium has utterly established in it. And also you see these very cool networks of mycelium working by it. It’s like a branching construction. It’s nearly like branches from a plant or like roots from a plant appear to be. They’re these white threads which are, like, woven into the leaf.\nDuff: It’s actually fairly!\nSilverstein: Yeah, the patterns they make are very cool.\nDuff: Oh, there’s one other one.\nSilverstein: Mm-hmm. Yeah, it’s, it’s …\nDuff (tape): It’s like little snowflakes!\nSilverstein: [laughs] It’s in every single place. I imply, the entire…in every single place. [laughs] It’s in every single place.\nDuff (tape): [laughs] Superior.\nDuff: This micro-economy beneath our ft is astounding.\nRight here’s the way it works. Some fungi assist useless issues decompose, releasing vitamins. Then the fungi related to tree roots scavenge for vitamins and commerce them to bushes in return for sugar, which comes from carbon. The foundation fungi are known as mycorrhizae: “myco” means fungi, and “rhizae,” means root. And you may consider mycorrhizae as falling into two fundamental classes. First: the ectomycorrhizae.\nJenny Bhatnagar: “Ecto” means outdoors, and so they don’t penetrate the foundation cells. They develop across the root cells on the skin. (:05)\nDuff: That’s Boston College biology professor Jenny Bhatnagar. The opposite kind, she tells me, is arbuscular mycorrhizae.\nDuff: And also you mentioned the arbuscular mycorrhizae, they’re even smaller?\nBhatnagar: You may’t see them with the bare eye, as a result of they develop contained in the plant root, versus across the outdoors.\nDuff: There’s a cause why this issues. Ectomycorrhizae and arbuscular mycorrhizae specialise in getting totally different vitamins, and so they commerce these vitamins to bushes at totally different value factors. These costs affect how a lot carbon bushes should spend and the way a lot they get to avoid wasting.\nTo visualise how this works, it’s necessary to know that totally different bushes are inclined to accomplice with totally different fungi.\nBhatnagar: Maples: pink maples, sugar maples, Norway maples. Ashes. Ash bushes….\nDuff: These bushes, Jenny informed me, accomplice with arbuscular fungi.\nDuff (tape): After which what about for ecto?\nBhatnagar: Oak, beech, pine, hemlock … cherries, um, birch.\nDuff: Now we’ll take you thru the underground financial system itself. Think about you’re a maple tree …\n[CLIP: Forest sounds]\nDuff: You want some nitrogen. You get some out of your arbuscular fungi for about half off, in contrast with the oak tree subsequent to you, who’s buying and selling with ectomycorrhizal fungi. Say you each do your nutrient buying—you purchase some nitrogen, some phosphorus. On the finish of the day, you every have some carbon left over to spend money on progress. However you might have a bit of bit extra left than the oak tree. You develop a bit of greater.\nThese particulars are literally very related for people. Any huge firm planting bushes or defending forests to offset its carbon emissions is assuming that these bushes are investing their carbon in additional leaves, in fatter trunks, no matter. However to understand how a lot carbon forests can really retailer, we additionally must understand how a lot they spend. Crucially, these costs can change over time.\n[CLIP: Forest sounds]\nSay these bushes are feeling flush. All of them need to put out extra leaves and fatten up their trunks. However to do this, all of them want additional phosphorus. And in a single forest, the soil begins working out.\nRenato Braghiere: Arbuscular mycorrhizae are higher at buying phosphorus…, and ectomycorrhizae fungi are simply higher at buying nitrogen from soils.\nDuff: That’s Renato Braghiere, a local weather scientist who fashions how carbon cycles by forests. He says that each fungi in all probability increase their phosphorus costs however possibly at totally different charges. If the costs go excessive sufficient, the financial system will crash: bushes will develop extra slowly and reproduce much less. Proper now most forests soak up extra carbon than they launch. However wildfires and deforestation make that more durable. Add an financial slowdown, and forests general may turn into a carbon supply as a substitute of a carbon sink.\nRight here’s what’s subsequent. To determine what is going to occur to forests and, consequently, to the local weather—we have to map which fungi are the place and watch how they’re altering their costs.\nDoing so might assist us perceive whether or not forests are headed for an financial crash and, in that case, what that can imply for our personal carbon price range.\nRenato tells me that it’s nonetheless painstakingly arduous to map totally different species of bushes. However his colleagues have found out find out how to map the fungi of their roots.\nBraghiere: In these two areas of the planet, we see one kind of mycorrhizae versus the opposite kind of mycorrhizae.\nDuff: Tropical soils are typically decrease in phosphorus. Temperate soils have much less nitrogen. However with local weather change, forests and fungi might begin to shift. Mapping a world baseline might be necessary for seeing how these shifts play out. Proper now we simply have some knowledge, from locations like Harvard Forest. Right here’s Jenny once more.\nBhatnagar: Nicely, I feel over the centuries folks have studied the bushes. They usually have a look at the roots, and simply over the centuries, it’s turn into identified which tree species affiliate with which sort of mycorrhizae. So if in case you have a map of all of the tree species in your forest, and you’ll very simply say, you understand, 20 % of your forest goes to be related to arbuscular mycorrhizae …\nDuff: Partly we all know which fungi are the place as a result of we’ve got been utilizing tree species as proxies. We learn about these relationships because of chemical evaluation. Right here’s how that’s completed.\nBhatnagar: It’s important to take the leaf. It’s important to choose it off the tree. It’s important to grind it up. And you must burn it.\nDuff: Timber that affiliate with arbuscular mycorrhizae are inclined to have extra nitrogen and phosphorus of their leaves. Timber that depend on ectomycorrhizal fungi are inclined to have extra carbon. Which means you may work out the kind of fungi even with out figuring out the kind of tree.\nBhatnagar: What occurs whenever you burn it, it’s known as a combustion evaluation…. All of the nitrogen will get transformed to, into fuel…, after which we put it by a fuel detector…. It’s the identical factor with the carbon. We burn all of the carbon… and we use a CO2 detector.\nDuff: Harvard Forest, the place I’m speaking to Jenny, has a number of the best-mapped fungi on the earth.\nHowever we haven’t really mapped most forests—and due to this, it’s arduous to trace world tendencies in these underground nutrient economies. These financial tendencies will affect how a lot forests develop this century, whether or not they can efficiently migrate as temperatures change and whether or not they are going to proceed to retailer all the additional carbon we’re burning.\nWhat would assist could be a map of mycorrhizal fungi worldwide. Because it seems, scientists at NASA are already engaged on this. And right here’s the actually wild half: they assume they’ll be capable to make this — from area.\nBraghiere: We will instantly know “What does mycorrhizae appear to be in the entire planet?” which is fairly thrilling.\nDuff: Renato and his colleagues nonetheless can’t map which sorts of bushes are the place. However they assume they’ll be capable to map the underground fungi.\nBraghiere: There’s a new mission, a brand new NASA mission known as SBG–it stands for Floor Biology and Geology–that are hyperspectral satellites that can orbit the complete planet.\nDuff: Hyperspectral imaging seems to be on the total spectrum of sunshine, even the components that we will’t see. Utilizing that know-how, satellites can document the particular wavelengths of sunshine reflecting off leaves 1000’s of miles beneath. Totally different chemical substances replicate totally different wavelengths, so we will see nitrogen, phosphorus and carbon ranges.\nBraghiere: We’re additionally utilizing machine-learning algorithms–actually, synthetic intelligence right here–to hyperlink these spectral properties to no matter is occurring within the roots.\nDuff: As a result of arbuscular and ectomycorrhizal fungi produce totally different chemical signatures, researchers can use what’s occurring within the leaves to foretell what’s occurring underground. Now they’re testing their algorithms in opposition to what they know from locations like Harvard Forest. If this works, we may out of the blue have world mycorrhizal maps with 10,000 instances extra element than the maps we’ve got now.\nBraghiere: First, we may have this snapshot. However as a result of the satellite tv for pc is a mission that might be up there for just a few years no less than, we can observe the temporal variations of these spectral signatures.\nDuff: All this mapping knowledge will give Renato extra to work with as he forecasts the plant-fungi inflation drawback. As forests shift in response to local weather change, world knowledge will assist him and different modelers watch what occurs to fungi.\nBraghiere: We additionally know that the Arctic boreal areas of the planet are getting hotter at a a lot sooner charge than the remainder of the planet. And so what we see is that there’s a shift in species composition in these areas … not solely the crops which are on prime of the soil but additionally the mycorrhizae related to these crops.\nDuff: As forests begin to transfer north in response to altering temperatures, bushes take their mycorrhizae with them.\nBraghiere: And so the environmental circumstances of the Arctic are altering, however the quantity of vitamins and soils are usually not altering.\nDuff: Right here’s why this can be a drawback. As species attempt to migrate, we may see a mismatch between the vitamins that fungi are good at scavenging — and the soil that they’re attempting to scavenge in.\nBraghiere: And so what may occur is that as a result of now we’ve got these arbuscular mycorrhizae going into the Arctic, and they’re simply much less environment friendly in buying nitrogen, the crops may undergo even additional.\nDuff: And we might also see adjustments that we weren’t anticipating.\nBraghiere: It may also occur that, you understand, a unique kind of fungi throughout the arbuscular mycorrhizal group finally ends up being higher or nearly as good because the ectomycorrhizae to accumulate nitrogen…. And so there’s a likelihood that these ecosystems will adapt.\nDuff: Higher maps ought to assist us watch these adjustments play out and act accordingly. One particular person pondering lots about this subsequent period of modeling is local weather scientist Regina Rodrigues Rodrigues.\nRegina Rodrigues Rodrigues: It is a new frontier that we need to get to with modeling … is that this digital Earth. It’s mainly [to] simulate Earth in a pc mannequin, mimic Earth in all features. The concept of getting that working … is that ultimately, say, a policymaker desires to decide about one thing … and it will probably go to this digital Earth and experiment to it. And select pathways of say local weather change and outcomes… if I select, say, much less emission with the insurance policies that I’ve, as an example, what would be the end result of that? That’s the final word purpose for it.\nDuff: NASA’s SBG mission is scheduled to launch round 2028. When it does, fungi maps might get exponentially higher. However within the meantime, by persevering with to burn fossil fuels, we’re persevering with to devalue the foreign money in these forest nutrient economies. If we need to stop runaway inflation for bushes, proper now could be a extremely good time to cease printing more cash.\nHowever slicing emissions is just not a science drawback; it’s a folks drawback. And there, too, Regina thinks that fungi might have lots to show us. That’s subsequent.\nFor Science, Rapidly, I’m Meg Duff. Science, Rapidly is produced by Tulika Bose, Jeff DelViscio and Kelso Harper. Music is by Dominic Smith.\nYou may hearken to Science, Rapidly wherever you get your podcasts. Don’t overlook to go to ScientificAmerican.com to get essentially the most up-to-date and in-depth science information.\n[The above is a transcript of this podcast]","Using Hyperspectral Data to Identify Crops in a Cultivated Agricultural Landscape-A Case Study of Taita Hills, Kenya\nReceived Date: Aug 11, 2014 / Accepted Date: Oct 30, 2014 / Published Date: Nov 10, 2014\nRecent advances in hyperspectral remote sensing techniques and technologies allow us to more accurately identify larger range of crop species from airborne measurements. This study employs hyperspectral AISA Eagle VNIR imagery acquired with 9 nm spectral and 0.6 m spatial resolutions over a spectral range of 400 nm to 1000 nm. The area of study is the Taita hills in Kenya. Various crops are grown in this region basically for food and as an economic activity. The crops addressed are: maize, bananas, avocados, and sugarcane and mango trees. The main objectives of this study were to study what crop species can be distinguished from the cultivated population crops in the agricultural landscape and what feature space discriminates most effectively the spectral signatures of different species. Spectral Angle Mapper (SAM) algorithm together with some dissimilarity concepts was applied in this work. The spectral signatures for crops were collected using accurate field plot maps. Accuracy assessment was done using independent training vector data. We achieved an overall accuracy of 77% with a kappa value of 0.67. Various crops in different locations were identified and shown.\nKeywords: Hyperspectral imaging; Spectral signatures; Spectral variation; Crop identification; Spectral angle mapper\nHyperspectral remote sensing data can provide a significant spectral measurement capability over the conventional remote sensor systems and hence becomes very useful in identification and modelling of terrestrial ecosystem characteristics. Not long ago, mapping was mainly using satellite (space borne) data for large area mapping but for small regions, it used aerial images (air borne) and in most cases, the result was just a land cover map combining several classes of pixels having some broad similarity. The need to discriminate crop species to know their health, location and distribution has paved way in this decade due to available sensors which can detect at high spatial and spectral resolutions the natural and man-made features on the surface of the earth. The advancement not only on the sensor availability but also the technology used to discriminate the various spectra of different species has become a boost to mapping. Many technologies have been used for extracting terrestrial features from hyperspectral imagery. Principal Component Analysis (PCA) among other algorithms for crop classification has yielded good results . Step-wise Discriminant Analysis (SDA) and Derivative Greenish Vegetation Indices (DGVI) to classify and characterize both vegetation and agricultural crops have been used [2,3]. Dissimilarity based approaches have also given good representation of hyperspectral data . Tree species identification has been one area of interests for scientist dealing with forests and vegetation mapping. Statistical methods to identify tree species in forests have shown good and accurate results. Nevertheless, Artificial Neural Networks (ANN) and Linear Discriminant Analysis (LDA) have given reliable results in tree species identification.\nSome other approaches in coastal environments have been made to identify mangrove species using both object-based and pixel-based classification methods. A comparison has been given and in this regard, results indicate that object based mapping approach is better than pixelbased approach with a difference of just about 7% overall accuracy and 0.1 kappa . All these shows that crop species can be discriminated in a similar if not a different but close approach. The objectives of this study were (1) to examine the capability of hyperspectral data to distinguish selected crops in a cultivated agricultural landscape in the Taita hills in Kenya, (2) to evaluate the spectral angle divergence of various crops and use this to discriminate the crop species and (3) to assess the accuracy of the classification.\nTaita Hills, (03°20’S, 38°15’E) in Kenya are one of the biodiversity hot spot in Eastern Africa. A wide range of studies have been made recently in this area for instance; on land use, land degradation, soil erosion, biodiversity, urban growth and sacred forest remnants. The area is facing a population growth and intensification of agriculture, which is the major economic activity for the Taita community. Although the terrain varies from 600 m to about 2200 m.a.s.l, farmers cultivate various crops ranging from maize (Zea mays), bananas (Musa paradisiaca), fruits and even fodder crops for animals, which are normally put in zero-grazing system (Figure 1).\nHyperspectral data was collected using AISA Eagle VNIR sensor system. Its accuracy is ranging in the following domain: 9 nm and 0.6 m in both spectral and spatial resolutions respectively over a spectral range of 400 nm to 1000 nm. This gives 64 spectral bands. The fieldwork measurements were conducted simultaneously with hyperspectral data acquisition. The flying height was about 2,400 meters above sea level to maintain the spatial resolution of 0.6 m. Accurate photographs taken by the Nikon 3DX camera, which was attached together with the AISA sensor on board during the time of flight, were used to map every species in the selected plots. This data were used as ground truth and training data. Spectral signatures of crops are known to vary due to leaf optical properties, leaf angles and spatial distribution. Signatures also vary from leaf to canopy scales. The spatial resolution for this datasets was kept at 0.6 meters, which is more accurate in discriminating the various crops especially for areas such as maize plantations, banana farms, and large fruits trees such as mangoes and avocadoes. Eight sampled plots were geo-referenced in order to get exact location of the crops in the plots. A detailed aerial mosaic was used to overlay the geo-referenced maps onto it and training polygons with respect to the crops were on-screen digitized with ArcGIS 10 out of the maps. Small regions depicting the spectral patterns for the specified crop were then generated and saved as Regions of Interests (ROIs) and later used as endmembers [6,7].\nMethodology and Analysis\nThe hyperspectral image collected was subjected for pre-processing. This catered for three distortions: radiometric, geometric and atmospheric effects. Radiometric corrected for sensor sensitivity, solar angle and topography. Geometric correction was basically applied to have a geometrically correct image. Digital elevation model at 20-meter resolution was resampled to 0.6 meter spatial resolution which conformed to the image spatial resolution. Atmospheric corrections were finally applied to remove the atmospheric effects. The image was checked using the z-profile tools and spectral reflectance on every cursor location of the image was checked. No distortion was evident. This procedure was conducted using the ATCOR-4 software which is specifically designed for correcting for atmospheric distortions.\nFor spectral extraction, 148 digitized polygons were used to derive endmembers for crop classification. They were extracted from the sampled plots. These comprised of maize (Zea mays), bananas (Musa paradisiaca), mangoes (Mangifera indica), avocados (Persea americana), sugarcane (Socharum spp.) and farm trees such as Cypress (Cupressus dupreziana), Grevillea (Grievillea robusta) among many others. They were further divided into two datasets so that about 30% of every class was reserved for accuracy assessment and 70% was used for classification. Theoretically, existing pure features in mixed pixels are referred to as endmembers and their collection describes all spectral variability for all pixels in a given image. Endmembers for this study were selected to enable mapping of the selected crops using Spectral Angle Mapper (SAM) algorithm of Envi software.\nFigure 2 shows the spectral reflectance of the six-collected endmembers in different colors. Blue corresponds to maize plantations, cyan on the other hand corresponds to sugarcane whereas yellow is for mango trees. Magenta is representing agro-forestry, green for bananas and finally the avocados are represented by red color. Spectral Angle Mapper (SAM) is a physically-based spectral classification that uses an n-D angle to match pixels to reference spectra. It assumes that data have been reduced to apparent reflectance (true reflectance multiplied by some unknown gain factor controlled by topography and shadows). The algorithm determines the spectral similarity between two spectra by calculating the angle between them as vectors in a space with dimensionality equal to the number of bands (n). This technique, when used on calibrated reflectance data, is relatively insensitive to illumination and albedo effects. Endmember spectra used by SAM can come from ASCII files or spectral libraries, or one can extract them directly from an image (as ROI average spectra). SAM compares the angle between the endmember spectrum vector and each pixel vector in n-D space. Smaller angles represent closer matches to the reference spectrum. The result is a classification image showing the best match. Pixels further away than the specified maximum angle threshold in radians are not classified [8-10]. SAM was used to classify the selected crop species in Taita hills. The spectral angle of dissimilarity was kept at 0.1 radians (Table 1).\nTable 1: Number of samples used for classification and accuracy assessment.\nSpectral map relates to the spectrum that is generated from the end members selected. After classification, the classified map was linked to the color infra-red image that was used to extract the spectra. A visual analysis was done. Most of the features were classified such as maize plantations, trees and sugarcane. The color representation of the data was similar to the color scheme in Figure 3. Accuracy assessment gave an overall accuracy of 77% and kappa of 0.67. Table 2 shows the contingency matrix. The values are the number of pixels classified in every class from the total pixels that were used in the classification process.\nTable 2: Confusion matrix of the classification\nMany pixels though are seen to be unclassified, indicated on the Table 2 as others. Producer's and user's accuracies were also tabulated, (Table 3). The trees and avocadoes were poorly classified in the final map. Trees had some similar spectra to some crops, and the major crop here was the avocado. This ended up with a misclassification of avocado species for trees. The selection of tree samples was achieved from forest patches from aerial mosaic in which there exists various species of trees however the training samples were very few (Figure 4).\nThere is a high correlation between the producer of the classification and the user of the classified map in that the difference between them is less than 10 percent.\nCrop type classification indicates that it is possible to discriminate various crops using AISA Eagle VNIR data and the spectral angle mapper (SAM) algorithm in a cultivated landscape. The confusion matrix shows that most classes were classified to be trees due to the spectral angle between them being as closest. Bananas, avocadoes, mangoes and trees (call this cluster 1) had very similar profile. A distinction between maize and sugarcane (call this cluster 2) is much better than that of cluster 1. Spectral range between 500 nm to 700 nm can be seen to separate not only the two clusters but also the different crop types. The unclassified pixels (others) constituted mainly reflective natural and man-made features such as buildings, roads and water bodies. These were not considered for endmember selection but the pixels are part of the input image for classification. Table 3 shows the producer's and user's accuracies. Mangoes, sugarcane, maize and bananas had good producer's and user's accuracies. Avocadoes were poorly classified even though it was the class with most endmembers (Table 1). The reason could be linked to their close reflectance with other trees. One disadvantage endured in this study is the limitation to distinguish the trees in a cultivated crop land. Trees here were summation of several species from the agro-forestry areas. It can be argued that just as crops differ in their reflectance from crop to crop, it is also true that there's a reflectance difference between tree to tree and that crop types such as mangoes and avocadoes are also trees in their nature. In identifying crop types in a cultivation landscape, it is wise to identify various tree species within the agro-forestry environment too.\nTable 3: Producer's and user's accuracies.\nClassification of crop types is possible using AISA Eagle VNIR data and spectral angle mapper algorithm. This study focused entirely on sampled field plots (polygons mapped from plots with respect to each crop represented therein) and spectral signatures extracted from the input airborne hyperspectral image to map out selected crops. Many studies have shown good results with this method although object based approach instead of pixel - based could yield a more accurate result.\nThe first author would like to acknowledge Dr. Janne Heiskanen and Mr. Pekka Hurskainen for their support during data collection, encouragement, constructive ideas and directions given to improve the quality of this manuscript. Thanks also to Mr. Rami Piiroinen and Mr. Tuure Takala, for their guidance on the pre-processing of the hyperspectral imagery. Authors also acknowledge the support from Mr. Alain Sylla and Mr. Samuel Nthuni in matters that related to GIS. Many thanks to the CHIESA project for funding this work.\n- Mader S, Vohland M, Jarmer MT, Casper M (2006) Crop Classification with Hyperspectral Data of Hymap Sensor Using Different Feature Extraction Techniques. The 7th SIG-Imaging Spectroscopy Workshop, EARSeL Edinburgh, UK.\n- Thenkabail PS, Eden A, Mark SA, Van Der Meer B (2004) Accuracy Assessments of Hyperspectral Waveband Performance for Vegetation Analysis Applications. J Remote Sens Environ 91: 354-376.\n- Thenkabail PS, Smith RB, De-Pauw E (2000) Hyperspectral Vegetation Indices for Determining Agricultural Crop Characteristics. J Remote Sens Environ 71: 158-182.\n- Paclik P, Duin RPW (2002) Dissimilarity based classification of spectra: Computational issues. Real Time Imaging 9: 237-244.\n- Kamal M, Phinn S (2011) Hyperspectral Data for Mangrove Species mapping: A comparison of Pixel-Based and Object-Based Approach. J Remote Sens 3: 2222-2242.\n- Ashoori H (2008) Evaluation of the usefulness of texture measures for crop type classification by hyperion data, The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, University of Technology, Tehran - Iran.\n- Clark LM, Roberts DA, Clark BD (2005) Hyperspectral discrimination of tropical rain forest tree species at leaf to crown scales. J Remote Sens Environ 96: 375-398.\n- Congalton RG (1991) A Review of Assessing the Accuracy of Classification of Remotely Sensed Data. J Remote Sens Environ 37: 35-46.\n- Galvao LS, Formaggio AR, Tisot DA (2005) Discrimination of Sugarcane Varieties in Southeastern Brazil with EO-1 Hyperion data. J Remote Sens Environ 94: 523-534.\n- Rama RN, Garg PK, Ghosh SK (2007) Development of an Agricultural Crops Spectral Library and Classification at Cultivar Level using Hyperspectral Data. J Precis Agr 8: 173-185.\nCitation: Boitt M, Ndegwa C, Pellikka P (2014) Using Hyperspectral Data to Identify Crops in a Cultivated Agricultural Landscape-A Case Study of Taita Hills, Kenya. J Earth Sci Clim Change 5: 232. Doi: 10.4172/2157-7617.1000232\nCopyright: ©2014 Boitt M, et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nSelect your language of interest to view the total content in your interested language\nShare This Article\nOpen Access Journals\n- Total views: 14131\n- [From(publication date): 11-2014 - Sep 29, 2021]\n- Breakdown by view type\n- HTML page views: 9995\n- PDF downloads: 4136"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:44e98622-c6b6-47a7-93ec-8c9df56293a3>","<urn:uuid:b877f27d-e889-403a-9772-c71cafeb464a>"],"error":null}
{"question":"I'm researching the evolution of Australian marsupials and their habitat adaptation. Could you elaborate on both the prehistoric marsupial species found in Riversleigh and the current marsupial populations in Australian deserts?","answer":"Riversleigh's fossil record reveals several prehistoric marsupial species, including Nimbadon (a sheep-like marsupial), Wakaleo and Priscileo (marsupial lions), Nimiokoala (an ancient koala), and Nimbacinus (an ancestor of the Thylacine). These fossils are preserved in soft freshwater limestone, maintaining their three-dimensional shape. In modern Australian deserts, marsupials have adapted to harsh conditions - for example, the Sturt Stony Desert is home to the Kowari, a native carnivorous marsupial rat currently classified as vulnerable by the IUCN. The Simpson Desert hosts seasonal wetlands that support various wildlife, while the Great Victoria Desert is home to several marsupial species including the crest-tailed mulgara, the southern marsupial mole, and the sandhill dunnart.","context":["10. Pedirka Desert - South Australia\nThe Pedirka Desert is a small desert in Australia that is located about 100 km to the northwest of Oodnadatta in South Australia. The desert occupies an area of only about 1,250 square km. The sands of the desert are deep red in color and dense mulga woodlands grow in the region. The desert dunes at the Pedirka Desert are widely spaced, low and eroded. The land is presently being developed for pastoral activities.\n9. Tirari Desert - South Australia\nThe Tirari Desert, located in South Australia’s Far North region’s eastern part, encompasses an area of 15,250 square km. Part of the desert lies within the Kati Thanda-Lake Eyre National Park. A large number of salt lakes and sand dunes running from north to south are important features of the desert landscape. The Cooper Creek flows through the Tirari Desert. Harsh conditions prevail in the desert with high temperatures and extremely low rainfall. The crest and slopes of the dunes are covered by vegetation dominated by either Sandhill Cane-grass or Sandhill Wattle. The slopes are occupied by tall, open shrublands. The Tirari Desert also incorporates an area of paleontological significance with significant Tertiary period vertebrate fossils being discovered here.\n8. Sturt Stony Desert - South Australia, Queensland, New South Wales\nThe Sturt Stony Desert occupies a region in South Australia’s north-eastern part. It also extends into Queensland’s far south-western border area and the far west of New South Wales. The Sturt Stony Desert lies on the Gason Dome between the Strzelecki Desert to the south-east and the Simpson Desert to the west. The desert is named after Charles Sturt who named it himself in 1844 while trying to search an inland sea in Australia. The stony terrain of the desert caused his horses to limp and wore down their hooves while trudging along the desert. The desert is mostly occupied by gibber and features several ephemeral lakes called gilgai. The Sturt Stony Desert is part of the Tirari-Sturt stony desert ecoregion. The desert features the Kowari, a native carnivorous marsupial rat which is currently classified as vulnerable by the IUCN.\n7. Strzelecki Desert - South Australia, Queensland, New South Wales\nThe Strzelecki Desert occupies parts of South West Queensland, western New South Wales, and South Australia’s Far North Region. It is named after Paweł Edmund Strzelecki, a Polish explorer. The desert encompasses an area of 80,250 square km. The Strzelecki Creek, Cooper Creek, and Diamantina River all flow through Strzelecki Desert. Three wilderness areas and extensive dune fields are part of the desert.\n6. Little Sandy Desert - Western Australia\nThe Little Sandy Desert is located in Western Australia and is to the west of the Gibson Desert and south of the Great Sandy Desert. The desert is named so as its landscape closely resembles that of the Great Sandy Desert. It is also located near it. The Canning Stock Route passes across both these deserts. The region is inhabited by the Mandilara indigenous inhabitants.\n5. Gibson Desert - Western Australia\nAn interim Australian bioregion, the Gibson Desert occupies an extensive area in Western Australia. It is named after Alfred Gibson, an explorer who was lost in the desert in 1874 and was never found again. The desert occupies a vast area of about 155,000 square km which makes it Australia’s fifth biggest desert. Despite the large size, the Gibson Desert is still in a pristine state. The desert is surrounded by the Little Sandy Desert, the Great Sandy Desert, and the Great Victoria Desert. The altitude of the desert extends up to 1,600 feet in certain locations. The landscape of the Gibson Desert features gravel-covered terrains covered by desert grasses, rocky ridges, upland areas, and dune fields and undulating red sand plains. The desert also houses a few, scattered salt-water lakes. The desert experiences a generally hot climate with summer temperatures soaring above 40°C while winter temperatures may drop down to a minimum of 6 °C. Indigenous Australians live in the area and follow a traditional way of life.\n4. Simpson Desert - Northern Territory, Queensland, South Australia\nThe Simpson Desert is Australia’s fourth largest desert and the largest sand dune desert in the world. The Simpson desert occupies an area of 176,500 square km and occupies parts of Queensland, South Australia, and Northern Territory. The world’s longest parallel sand dunes are located in this desert. These dunes are static and are held in place by vegetation. The 40 meters tall Nappanerica dune is the largest dune in the area. The Simpson Desert also has the Great Artesian Basin which is one of the world’s largest inland drainage basins. The water from the basin rises to the surface at a large number of natural springs distributed throughout the desert.\nThe Simpson Desert is popular among tourists who visit the desert to view the Dalhousie Springs, Poeppel Corner, Approdinna Attora Knoll, etc. However, since the desert is not accessible by any maintained roads and the summer temperatures here are extremely harsh, the government has closed the desert to tourists in summer to avoid unpleasant circumstances. Drought resistant shrubs and grasses cover large sections of the desert. Fauna inhabiting the Simpson Desert include the water-holding frog, the Eyrean grasswren, the gray grasswren, etc. Several seasonal migrant birds can be spotted in the seasonal wetlands of the Simpson Desert like the Lake Eyre, the Coongie Lakes, etc. Examples include the musk duck, glossy ibis, great egret, banded stilt, and others. The mound springs of the Great Artesian Basin also host several species of fish, invertebrates, and plants.\n3. Tanami Desert - Western Australia, Northern Territory\nThe Tanami Desert is located in Western Australia and Northern Territory. The desert has a rocky terrain with small hills and is traversed by the Tanami Track. The Tanami Deserts holds a significant place from the conservation point of view since it provides residence to some of the country’s rare and endangered species. The important fauna species found here include the little native mouse, the long-tailed planigale, the Western chestnut mouse, the freckled duck, gray falcon, etc. Several indigenous groups are also based in this desert.\n2. Great Sandy Desert - Western Australia\nThe Great Sandy Desert, Australia’s second largest desert, is located in Western Australia’s North West. The desert encompasses an area of 284,993 square km and is surrounded by the Gibson Desert, the Great Sandy Desert, and the Tanami Desert. The desert features a famous meteorite impact crater called the Wolfe Creek and also houses several large ergs with longitudinal sand dunes. Two main groups of aboriginals, the Martu and the Pintupi people inhabit the desert. Several mining centers are also located in the region. Spinifex dominates the vegetation of the desert. Fauna living here include dingos, goannas, feral camels, bilbies, bearded dragons, red kangaroo, scarlet-chested parrot, Alexandra's parrot, thorny devils, etc.\n1. Great Victoria Desert - Western Australia, South Australia\nThe biggest Australian desert, the Great Victoria Desert is located in Western Australia and South Australia. The desert features grassland areas and small sandhills with salt lakes and pebbled surfaces. The desert occupies an area of around 348,750 square km. The Great Victoria Desert receives low average annual rainfall between 200 and 250 mm per year. Summer temperatures range between 32 to 40 °C during the day while in winter it is between 18 and 23 °C.\nThe desert is a part of Australia that houses the most populous and healthy population of Indigenous Australians belonging to groups like Pitjantjatjara, Mirning, and the Kogara. Large parts of the Great Victoria Desert, however, remain uninhabited as the climate and terrain are unsuitable for human settlement. Large, pristine areas of the desert are protected areas like the Mamungari Conservation Park.\nOnly drought resistant plants can survive the harsh desert environment. A few species of Acacia and Eucalyptus can be found here. Spinifex grasses occupy most the of the desert landscape amidst the sandy ridges. A few mammals and birds can be found in the Great Victoria Desert. Some examples are the great desert skink, the crest-tailed mulgara, the southern marsupial mole, the sandhill dunnart, etc. The large monitor lizards, the sand goanna and the perentie, and the dingo are the active predators of the Great Victoria Desert.\nThe Largest Deserts In Australia\n|Rank||Desert||State/Territory||Area, km squared||% of Australia|\n|1||Great Victoria Desert||Western Australia, South Australia||348,750||4.5%|\n|2||Great Sandy Desert||Western Australia||267,250||3.5%|\n|3||Tanami Desert||Western Australia, Northern Territory||184,500||2.4%|\n|4||Simpson Desert||Northern Territory, Queensland, South Australia||176,500||2.3%|\n|5||Gibson Desert||Western Australia||156,000||2.0%|\n|6||Little Sandy Desert||Western Australia||111,500||1.5%|\n|7||Strzelecki Desert||South Australia, Queensland, New South Wales||80,250||1.0%|\n|8||Sturt Stony Desert||South Australia, Queensland, New South Wales||29,750||0.3%|\n|9||Tirari Desert||South Australia||15,250||0.2%|\n|10||Pedirka Desert||South Australia||1,250||0.016%|","|Australian Fossil Mammal Sites (Riversleigh/Naracoorte) *|\n|Inscription||1994 (18th Session)|\nRiversleigh in north west Queensland, Australia, is one of the Australian Fossil Mammal Sites listed by UNESCO as a World Heritage Site. The other is the Naracoorte Caves National Park in South Australia. The Riversleigh site is part of the Boodjamulla National Park. The fossil site covers an area of about 100 km².\nThe fossils that have been found are the remains of ancient mammals, birds and reptiles of Oligocene and Miocene epochs. The site was listed as a World Heritage site in 1994. The fossils at Riversleigh are rare because they are found in soft freshwater limestone which has not been compressed. This means the animal remains retain their three dimensional shape.\nFossils[change | change source]\nFossils at Riversleigh are found in limestone by lime-rich freshwater pools, and in caves. They are from when the ecosystem was evolving from rich rainforest to semi-arid grassland. Thirty-five fossil bat species have been found at the site, which is the richest in the world. A skull, complete with all its teeth, of a 15 million-year-old monotreme, Obdurodon dicksoni, shows how this Australia group of animals evolved. Fossil ancestors of the extinct thylacine, Thylacinus cynocephalus, have also been found at Riversleigh. In 1993, Nimbadon skulls were found in newly discovered cave. Scientists think this prehistoric marsupial first appeared about 15 million years ago and died out about 12 million years ago, perhaps from the effects of climate change. Other fossils have shown how the koala has changed in response to Australia's change from rainforest to dryer eucalypt forests.\nFossils found at Riversleigh[change | change source]\n- Ekaltadeta, a carnivorous rat-kangaroo\n- Burramys, the Mountain Pygmy Possum\n- Nimbacinus, an ancestor of the Thylacine\n- Obdurodon, a giant platypus\n- Yarala, a tube-nosed bandicoot\n- Yalkaparidon, a strange marsupial\n- Wakaleo, a marsupial lion\n- Priscileo, a marsupial lion\n- Nimiokoala, an ancient koala\n- Nimbadon, a sheep-like marsupial\n- Pengana, a flexible-footed bird of prey\n- Menura tyawanoides, a prehistoric lyrebird\n- The first fossil record of the Orthonychidae (logrunner) family\n- Trilophosuchus, a tree-dwelling crocodile\n- Baru, the cleaver-headed crocodile\n- Yurlunggur, and Wonambi, extinct snakes (Madtsoiidae)\nReferences[change | change source]\n- Archer, M. et al. 1991. Riversleigh: the Story of Australia's Inland Rainforests, (Sydney: Reed Books).\n- UNESCO, \"Australian Fossil Mammal Sites (Riversleigh / Naracoorte)\"; retrieved 2012-4-21.\n- Archer M; Hand, Suzanne J. & Godthelp H.  2000. Australia's lost world: Riversleigh, World Heritage Site. Reed, Sydney.\n- Anna Salleh (16 February 2006). \"Huge skulls clues to snake evolution\". ABC Science (Australian Broadcasting Corporation). http://www.abc.net.au/science/news/ancient/AncientRepublish_1567127.htm. Retrieved 5 August 2010.\n- \"Cave yields marsupial fossil haul\". BBC News. 19 July 2010. http://www.bbc.co.uk/news/science-environment-10686515. Retrieved 5 August 2010.\n- Fossils reveal prehistoric life cycle. Australian Geographic. 20 July 2010.\n- Dan Gaffney (19 December 2009). \"Loud and lazy but didn't chew gum: Ancient koalas\". PhysOrg (PhysOrg.com). http://www.physorg.com/news180469378.html. Retrieved 5 August 2010.\nOther websites[change | change source]\n- World heritage listing for Riversleigh\n- UNESCO site with information on Riversleigh, Australia\n- Australian site about Riversleigh\n- Information about fossils from Riversleigh, Australian Museum\n- The Riversleigh Society supports scientific research at Riversleigh"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:ea8b6713-c3d9-48fd-9641-57c5ab839f5b>","<urn:uuid:61119e58-c3aa-4ba2-b73f-32112c4bbb01>"],"error":null}
{"question":"What are the essential camera setting adjustments required for photographing with muslin backgrounds compared to white backgrounds?","answer":"For muslin backgrounds, the focus is primarily on testing the setup with digital cameras, which allow for immediate review without development. For white backgrounds, specific camera settings are crucial: the camera must be set to manual mode, with ISO kept low (around 100-200) to avoid grain, aperture set to a small f-stop number (f/8 or f/11) for shallow depth of field, and shutter speed between 1/125 to 1/200 of a second to avoid overexposure. The white background setup requires more precise technical adjustments to achieve a clean, crisp, and consistent white background, while muslin backgrounds are more forgiving and focus more on proper positioning and lighting tests.","context":["Muslin Background Usage for Digital Photography\nA muslin background is ideal for photography because it's a light background which can be transported with ease. The material is available in many different colors, although the most common is natural muslin. This is a light background color which can be used if there aren't any suitable light walls.\nMuslin is useful because it allows some light to pass through from behind. It also absorbs some of the light which creates a softer glow rather than being too harsh. If you want to create portrait photography then a muslin background will be very useful. Muslin backdrops are available which are professional, but it's also possible to make the backdrops yourself.\nStep 1: Measuring\nFirst, you need to decide on the size of backdrop that you need to use. This will all depend on the type of photography that you are doing. Normally, these are used for portrait photography in which case a fairly small backdrop can be used. It's important to make sure that this backdrop is suitable for a number of different projects rather than just the one.\nOnce you have decided on your requirements, you should then be able to fine one to purchase. If you are looking for a slightly cheaper option, then you could make your own by buying some muslin from a DIY store.\nStep 2: Setting Up\nNext, you need to position the muslin background so that it is in the correct place. This will depend how many people you are taking photographs of. It does not need to be positioned against a wall, as it can be put anywhere. When locating the background screen, bear in mind that your subjects will need to stand around 5 feet away from the background. This will minimize any shadows being caused.\nStep 3: Testing\nWith everything set up, you can then check to see how everything looks by using your camera. Digital cameras make it very simple to take a look at everything without needing to develop it. You can use a timer to take a photo of yourself in front of the background to check that everything is okay. This will also give you hints about the lighting which is required for your photograph.\nStep 4: Lighting\nAnyone who takes photographs will realize how important lighting is. Adequate lighting will make your photos look much better and also help to add interest to them. There are a number of different types of lighting which can be used when working with muslin backgrounds.\nThe first and easiest option will be using natural light. This is suitable when photographing outside on a nice day. Alternatively, you may be able to rely on natural light if you have plenty of windows in your studio. A reflector can be used to improve the light and direct it in certain directions.\nArtificial studio lights can also be used to add extra light to the photograph. In this case, you can use a combination of fill in flashes and studio lighting. Two studio lights will normally be sufficient, and these can be aimed at the subjects head level. If you want a lighter background, then make the light more intense.","If you’re looking to create professional-looking product photos, having a stunning white background is essential. A white background helps to highlight the product’s features and make it stand out, giving it a clean and polished look that customers will appreciate.\nBut achieving a perfect white background can be challenging, especially for beginners. You need the right equipment, tools, and techniques to get it just right. In this step-by-step guide, we’ll show you everything you need to know to create a stunning white background for your photos.\nWhether you’re a seasoned photographer or just starting, this guide will take you through every step of the process. From setting up your shooting area to adjusting your camera settings and post-processing techniques, we’ve got you covered.\nWe’ll also provide you with some tips and tricks to ensure that your white background is flawless every time. By the end of this guide, you’ll have the skills and knowledge necessary to create beautiful product photos with a stunning white background that will impress your clients and customers.\nSo let’s dive in and get started on creating your perfect white background!\nWhy a white background is essential for product photography\nProduct photography has become increasingly important in the digital age, where customers are more likely to purchase items online rather than in stores. One of the critical elements of product photography is the background, and a white background is often the preferred choice for many businesses.\nThere are several reasons why a white background is essential for product photography. Firstly, a white background is simple and clean, which allows your product to stand out. It eliminates distractions and ensures that the focus is entirely on your product, making it easier for potential buyers to appreciate its features and details.\nSecondly, a white background is versatile, making it suitable for various products, regardless of size, shape, or color. Whether you’re selling clothing, jewelry, electronics, or any other item, a white background provides a neutral canvas that complements your product.\nThirdly, a white background enhances your brand’s professionalism and credibility. When your product photos look professional and polished, it sends a message to your customers that you care about the quality of your products and your business as a whole. This can lead to increased trust and confidence from potential buyers, ultimately resulting in more sales.\nFinally, a white background makes it easier to edit your photos. If you need to make any adjustments, such as removing the background entirely or adding shadows, a white background is the ideal starting point. It provides a blank slate that allows for easy editing and manipulation of your images.\nIn conclusion, a white background is essential for product photography because it creates a simple, versatile, and professional visual representation of your products. With the right equipment, lighting, and post-processing techniques, you can easily create stunning product photos that will help your business stand out in today’s competitive e-commerce landscape.\nEquipment and Tools Needed for Creating a White Background\nCreating a beautiful white background for your photos requires specific equipment and tools. While you can improvise with some materials, having the right gear will make the process smoother and more efficient.\nHere are some essential equipment and tools you need to create a stunning white background:\nThe backdrop material is the foundation for your white background. You’ll need a large piece of pure white paper, fabric, or vinyl. It should be wide enough to cover your shooting area and long enough to hang from the ceiling to the floor.\nA sturdy backdrop stand will hold your backdrop material in place while shooting. Make sure to choose a stand that can accommodate the width and length of your material.\nLighting is crucial for creating a white background. Use two or three studio lights with softboxes or diffusers to eliminate any harsh shadows and ensure even illumination on your backdrop.\nYou’ll need a camera with manual settings to adjust exposure, aperture, and shutter speed. A DSLR or mirrorless camera is ideal, but a smartphone camera with adjustable settings can also work.\nUsing a tripod will keep your camera steady and help prevent blurry images. Choose a sturdy tripod that can support the weight of your camera.\nAdditional tools you may want to consider include clamps to hold your backdrop in place, a steamer or iron to remove any wrinkles from your backdrop material, and a gray card for accurate color balance.\nHaving the right equipment and tools will help you create a stunning white background for your photos. With these essentials in hand, you’re ready to move onto the next step: preparing your shooting area.\nPreparing Your Shooting Area\nCreating a stunning white background for your photos requires meticulous preparation of your shooting area. Here are some tips to ensure that your photos turn out great:\nChoose a Suitable Location\nThe first step is to choose a suitable location for your photo shoot. Ideally, you want an area that is well-ventilated and has enough space to accommodate your equipment. Look for a spot with natural light, such as a room with large windows, or set up your own lighting if needed.\nClean and Clear the Space\nBefore setting up your equipment, you need to make sure that the area is clean and free of clutter. A dirty or cluttered space can detract from the visual appeal of your photos. Remove any objects that may be distracting, and wipe down any surfaces that will be visible in your shots.\nSet Up Your Backdrop\nNext, it’s time to set up your backdrop. Choose a white paper or cloth that is big enough to cover the area where you’ll be taking your photos. You want your backdrop to be as smooth and wrinkle-free as possible, so use tape or clamps to secure it in place.\nPosition Your Camera and Lighting\nNow it’s time to position your camera and lighting. Make sure that your camera is centered on the backdrop and is at the same height as the object you’re photographing. Place your lights at a 45-degree angle to the object and adjust their intensity until you get the desired effect.\nBy following these simple steps, you’ll be able to prepare your shooting area and create a stunning white background for your photos. Remember, preparation is key, so take the time to set up your equipment correctly to ensure that your photos turn out perfect every time.\nSetting up the Lighting for Your White Background\nWhen it comes to product photography, a white background can be the perfect way to make your products stand out. However, getting that perfect shot can be challenging if you don’t have the right lighting setup.\nThe key to creating a stunning white background is to ensure that your lighting is even and consistent across the entire background. This means that you need to pay close attention to the placement of your lights and the angle at which they are pointing.\nTo achieve this, you will need at least two lights – one on each side of your shooting area. These lights should be positioned at a 45-degree angle to your product and pointed directly at the background. The distance between the lights and the background should also be equal to ensure even lighting.\nIt’s important to note that the type of lights you use can play a significant role in the outcome of your photos. If you’re working with a small budget, you can use regular household lamps or LED lights. Just make sure that the color temperature of your lights is consistent to avoid any unwanted color casts.\nIf you have a larger budget, you may want to invest in professional studio lights or strobes. These lights are designed specifically for photography and can provide more power and flexibility than regular household lights.\nIn addition to your main lights, you may also want to consider using a reflector or diffuser to help control the light and eliminate any harsh shadows. A reflector bounces light back onto your product, while a diffuser softens the light and creates a more natural-looking effect.\nBy following these simple steps, you can create a stunning white background for your product photos that will make them stand out from the crowd. Just remember to take your time and experiment with different lighting setups until you find the perfect one for your needs.\nHow to Adjust Your Camera Settings for a White Background\nAchieving a stunning white background for your photos depends on more than just having the necessary equipment and lighting. Properly adjusting your camera settings is crucial to creating a clean, crisp, and consistent white background.\nTo start, ensure that you set your camera to manual mode instead of automatic mode. This gives you complete control over the settings, allowing you to create a white background that meets your exact needs.\nNext, adjust the ISO setting, which controls the sensitivity of your camera’s sensor to light. A high ISO can produce grainy images, so keep it as low as possible while still allowing enough light to enter the camera. Typically, an ISO of around 100-200 works well for capturing a white background.\nThen, set the aperture to a small f-stop number, such as f/8 or f/11. This will give you a shallow depth of field, ensuring that your subject remains in focus while still allowing enough light to reach the sensor.\nFinally, adjust the shutter speed to capture enough light, typically between 1/125 to 1/200 of a second. Keep in mind that the longer the shutter stays open, the more light enters the camera, which can overexpose your image and ruin the white background.\nIn summary, to achieve a stunning white background for your photos, set your camera to manual mode, adjust the ISO to a low setting, use a small f-stop number, and adjust the shutter speed to capture enough light without overexposing the image. By following these steps, you’ll be able to create consistent, professional-quality photos with a beautiful white background every time.\nShooting your photos on a white background\nNow that you have properly set up your shooting area and lighting, it’s time to start taking photos. Here are some essential tips for shooting your photos on a white background:\nAdjust Your Camera Settings\nBefore you start shooting, ensure that your camera settings are adjusted correctly. Set your ISO to the lowest possible value to avoid grainy images. Also, set your aperture to a high value to achieve a wider depth of field. This will keep everything in focus, making your subject stand out more.\nPosition Your SubjectP\nPosition your subject in the center of your shooting area and adjust your camera angle to capture the entire product. Make sure that your subject is evenly lit, with no shadows or hotspots, to achieve a clean and professional look.\nUse a Diffuser\nUsing a diffuser can help create a softer and more even light on your subject. Place the diffuser between your subject and the light source to diffuse the light and soften any harsh shadows.\nShoot in RAW\nShooting in RAW mode gives you more control over your editing process. RAW files retain all the original information captured by your camera, allowing you to make more significant adjustments without losing image quality.\nTake Multiple Shots\nTake multiple shots from different angles and distances to give yourself more options during the post-processing stage. This will also help you identify any issues with your lighting or setup that need to be adjusted.\nBy following these tips, you can capture stunning photos that highlight your product and make it stand out on a white background. Remember to experiment and try different techniques to find the perfect approach for your specific needs.\nPost-processing techniques for enhancing your white background\nYour product photography on a white background can be taken to the next level by using post-processing techniques to enhance the background. The following are some post-processing tips that you can use to make your white background look seamless and stunning.\nUse the brush tool to remove any imperfections\nAfter you’ve taken your photos, use the brush tool in your preferred photo editing software, such as Adobe Photoshop or Lightroom, to remove any dust, scratches, or other imperfections on your white background. This tool will allow you to quickly get rid of any unwanted elements on your white backdrop, making it look seamless and professional.\nAdjust the brightness and contrast levels\nWhite backgrounds require a proper balance between brightness and contrast levels. If your white background looks too dull or dark, adjust the brightness and contrast levels until you have achieved the perfect white tone. Be careful not to overdo it, as this may result in a washed-out or unnatural look.\nAdd vignettes to your photos\nAdding vignettes to your photos is an excellent way to add depth and dimension to your white background. Vignettes darken the edges of your image, which gives it a more polished and professional appearance. Use a soft-edged brush to create a subtle vignette effect that won’t overpower your product.\nUse a noise reduction tool\nIf you notice any grainy or noisy sections on your white background, use the noise reduction tool to smooth out the appearance. Noise reduction tools are available in most photo editing software and can help make your white background look clean and uniform.\nIn conclusion, these post-processing techniques are essential for creating a stunning white background for your product photography. By using these tips, you can ensure that your white backdrop looks seamless and professional, which will make your products stand out in your online store.\nTips and Tricks for Creating a Perfect White Background Every Time\nCreating a stunning white background for your product photography is critical because it can make or break your images. Here are some tips and tricks to help you achieve that perfect white background every time.\nUse a Dedicated Background Material\nWhite seamless paper or fabric should be used as background material. It’s an affordable and easy-to-use option, available in different sizes, and can be replaced when it becomes dirty or worn out. Make sure to iron or steam the fabric to remove any wrinkles before use.\nSet Up Multiple Lights\nSetting up multiple lights can help you achieve an evenly lit white background. Adding lights on both sides of your subject will help eliminate shadows and create a bright, clean look. Consider using diffusers or softboxes to soften the light and avoid harsh reflections.\nAdjust Your Camera Settings\nEnsure that your camera settings are correct before shooting. Shooting in manual mode will give you full control over the exposure, aperture, and shutter speed. Set your ISO to the lowest possible setting to avoid introducing noise or grain into your images. A shallow depth of field will ensure that your subject is in focus while the background remains blurry.\nKeep Your Background Clean\nKeeping your background clean and free from debris will save you time during post-processing. Use a lint roller or air blower to remove any dust or particles that may have settled on your background.\nExperiment with Angles and Perspectives\nExperimenting with angles and perspectives can add variety to your photos. Try shooting from above or below your subject or angling your camera slightly to create unique compositions. Don’t be afraid to play around with different positions until you find one that works best for your product.\nBy following these tips and tricks, you’ll be able to create a perfect white background every time. Remember, keep it clean, well-lit, and experiment with angles and perspectives, and you’re sure to produce stunning images that will make your products stand out from the rest.\nIn conclusion, creating a stunning white background for your photos can seem like a daunting task, but with the right equipment and techniques, it’s easier than you might think. A white background is essential for product photography because it makes your products stand out and gives them a professional look.\nNow that you have all the information you need to create a beautiful white background, it’s time to put it into practice! Remember to prepare your shooting area carefully, set up your lighting correctly, adjust your camera settings, and shoot your photos on a white background. Don’t forget to use post-processing techniques to enhance image further.\nBut even with all of these steps, you may still find that creating a perfect white background takes some trial and error. Don’t get discouraged if you don’t get it right the first time. Instead, learn from your mistakes, and keep trying until you achieve the results you’re looking for.\nFinally, always remember that the key to success in product photography is to be creative and have fun. Experiment with different props, angles, and lighting setups to make your images truly unique. By following these simple steps and adding your creativity, you’ll be able to create stunning product photos that will help your products stand out in a crowded marketplace.\nThank you for taking the time to read this step-by-step guide, and we hope that you found it helpful. Good luck with your product photography endeavors!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:e9b5d2ff-a590-4863-a2c7-ae0f77a8c7ab>","<urn:uuid:adb724ee-02a7-4b23-ab45-a5d29ba39cb3>"],"error":null}
{"question":"I'm curious about beer temperatures - what's the relationship between beer style and serving temperature, and could you explain how cooling affects the flavor compounds?","answer":"Different beer styles require different serving temperatures - darker and stronger beers are typically served warmer than lighter ones. For example, light beers/white beers should be very cold, while stouts can be served at room temperature. Temperature significantly affects flavor compounds - when beer is very cold, the volatilization of compounds slows down, resulting in less flavor, while the cold temperatures enhance perception of carbonation and bitterness. Warmer temperatures can make beers taste more sour and reduce hoppy notes and carbonation. This is why English-style ales are traditionally served warmer (12-14°C/54-57°F) to allow their malty flavors and fruity esters to be properly appreciated.","context":["Given that the British tend to drink their beer at a higher temperature than those of us in the US, should beer brewed in an English style be consumed that way? The justification I can think of is: the people developing the taste of the beer probably consumed it at a certain temperature, which might have not been the ice-cold levels that predominate in the Colonies.\nServing temperature is, of course, a preference. Serve it frozen or boiling if you want. However, a few opinions are:\n- CAMRA says that Real Ale (aka cask ale, usually english-style) should be served at 12-14 °C (54-57 °F), which is colder than room temperature, but warmer than your usually keg beer.\n- Ratebeer says the same thing regardless of whether it’s cask or not.\n- BeerAdvocate says 7-10 °C (45-50 °F) for an english bitter.\nSo, if you’re looking for a rule of thumb: “cool but not super-cold” or “cellar temp” will probably get you close enough.\nThere may be a general consensus for the \"proper\" serving temperature, which could involve questions like \"what do most people seem to like?\", and \"what temperature does the brewer feel best accentuates the parts of the beer they want to highlight?\". So there are good reasons for beer to be served at particular temperatures. In the case of your British bitters, age-old recipes were probably designed to allow the beer to taste its best at room temperature. As for a modern British beer, I suppose that would depend on what the brewer had in mind.\nThat said, my general thought is that you should drink a beer at the temperature you most like it, so the should part of your question is self-determined, and don't let anyone tell you that your taste buds are wrong.\nMyself, I tend to like my American IPAs cooler than the typical recommended serving temperature. And before I visited the UK, I couldn't stand a room temperature beer. Then, after a week in Tunbridge Wells, I picked up a large appreciation for hand-pumped UK beer, served at room temp. So you should open yourself to trying new things as well.\nJust like everyone else said, it's a preference. I alway say that the darker the beer, the warmer it has to be, so here are my rules:\n- Light beer/White beer/Kriek: In the fridge and goes in the freezer for a couple of minutes before serving\n- Golden beer: In the fridge\n- Amber beer: In the fridge and stay at room temperature for a couple of minutes before serving\n- Dark beer: At room temperature and goes in the fridge before serving\n- Stout: At room temperature at all time.\nIt's always going to be a matter of preference. I prefer most beers around 55-60 but do not hesitate to drink beer at room temperature. Some styles are more enjoyable approaching room temperature.\nHistorically, most beer was served at cellar temperature to room temperature, depending upon what was available. That isn't just an English tradition. Any beer can be consumed warm and drinking it ice cold as mass produced lagers are usually served tends to hide most of the flavor that you are paying for in a good beer. Give it a try and see what you like. Maybe you don't like your beer a little warmer. Enjoy what you like.\nThe basic rule is that beers should be served at the temp they were fermented. Lagers should be served at about 40°F and ales around 50°F - 55°F. Having said this I usually like my beers a little colder on a warm summer day. Try this experiment:\n- chill your beer to 35°F - 40°F and pour about 2/3 of it into the proper glass for that beer.\n- Drink at least 4 or 5 sips, gulps, or however you typically consume it.\n- Then put what is left in the microwave for about 20 seconds (time will vary with the output of the microwave). This will reduce the carbonation some so pour the other third right in the middle of the glass without tipping the glass to get some head, on the beer that is.\nIt makes a big difference on some beers. less on others. Also try different types of glasses and see what you like. Remember, everybody's palates are different and yours will vary depending on several factors such as, what you've been eating or drinking, how much beer you've already consumed, and how tired you are. It's a good idea to eat a bite or two of good bread like a baguette to cleanse your palate between samples. The above does not apply to mass produced lagers.\nFirst, let's say there is what is considered \"proper\", and then there is personal-preference. To be a good Cicerone, know what is \"proper\" but also show tolerance for personal preference.\nWith that out the way, it's definitely considered the \"proper\" thing to do to serve English beers warmer than ice-cold. Many of the delicate malty flavours you get from English malt, the fruitiness from esters produced by the English yeast, plus grassy, herbal or floral tones from the hops are muted greatly when the beer is served too cold.\nSome people say it's served at room room temperature. Modern room temperature of around 20°C/68°F is too warm. The ideal serving temperature is a cold cellar - typically around 12°C;55°F.","Temperature has a huge effect on how a beer tastes, and what flavors are most dominant when the brew hits your tongue. The optimal temperature of the beer depends a lot on the type of beer that you are serving. We’ve got the details, so let’s get started!\nHot and Cold\nChilled beer is obviously refreshing – but lower temperatures also affect our taste buds. First, cold temps will keep our tongues from sensing many of the flavors present in the average – we’re so busy experiencing the “cold” part of the beer, the “taste” part has less of an impact and our taste buds can become suppressed. Cold temperatures also have another effect – they slow down the volatilization of compounds in the beer. In other words, compounds that would normally be changing and giving off aromatic compounds stay sluggish: This usually means the beer has less flavor when icy cold than when at a higher temperature – there’s less interesting activity going on inside. From a more general perspective, cold temperatures also make it easier to notice carbonation, bitterness, and dry notes, which change the way you think about the beer.\nWarm beer has a different set of problems. In some beers, those compounds can go a little too crazy in higher temperatures, and start making a beer taste more sour. Additionally, as the temperature rises, the hoppier notes and carbonation tend to fall, which changes the taste of the beer and can lead to that “flat” taste that no one wants. It’s no surprise that beers which depend less on hops and carbonation – especially ales – favor warmer temperatures.\nNote: “Heat” is different from sunlight. The sun can warm your beer, but the light itself has additional taste-changing effects, particularly when it comes to transparent bottles/glasses. The double-whammy of heat and light is why you shouldn’t leave beer out on a hot summer day.\nGeneral Temperature Range\nAs a general rule, beer should be served between about 38 and 55 degrees Fahrenheit – with some exceptions. Different classifications of beer tend to do better at different temperature ranges, based largely on their composition and intended taste. Lagers are served at colder temperatures than ales, stronger or darker beers are served at warmer temperatures than weaker or lighter beers, and so on. Of course, there are a few exceptions (and we’ll talk about some in a bit), but these are good general guidelines for plotting temperature.\nAlso remember that people’s hands as well as sunlight, fireplaces and much can help heat up a beer. Many professional servers find a target temperature for their beer, then lower it by a few extra degrees in a cooler to make up for these sources of heat.\nSpecific Temperature Ranges (in Fahrenheit)\n• 35 to 40 Degrees (or even lower): Let’s be honest – there are some types of beer you don’t want to taste at all. You just want them to be cold. If there’s a beer where taste comes in last place, it’s a good idea to set the temperature extra low. This includes malt liquors, macro American lagers, and a few ultra-casual session beers.\n• 40 to 45 Degrees: This range is ideal for very light or effervescent beers that nonetheless still have flavor profiles. If you have a good Belgian Wit, a Hefeweizen, a Pilsner, or a Kolsch, serve them around these temperatures for best results. It’s worth noting that all craft beer is subject to experimentation, and experiments with these light beers usually focus on adding more flavor. If you have a crafty version of one of these beers, you may want to try a warmer temperature to see if it brings out more flavor.\n• 45 to 50 Degrees: This is a great range if you just aren’t sure what settings to use on your beverage cooler. It works for pale ales, IPAs, lagers with a bit more weight to them, most porters, and a number of stouts, too – especially nitro-based stouts that focus a bit more on carbonation and so benefit from cooler temperatures.\n• 50 to 55 Degrees: This range is usually referred to as “cellar temp” since it matches the temperature of most traditional cellars where beer is kept. If you can imagine an old inn or monastery keeping a barrel in their cellar for tapping, this is the range to put the beer in. It includes, naturally, the Belgian and Trappest ales, as well as English Bitters, Sour Ales, most various lambics, any Bocks, and powerful Baltic Porters.\n• 55 to 60 Degrees: Especially heavy beers that are more of a long-term drinking experience belong here. This includes Imperial Stouts, Belgian Strong Ales, Eisbocks and Dopplebocks, Barley Wines, and anything else that makes you think, “Whoa.”\nHope we have given you a good starting point to optimizing the serving temperature of beer. For some, a regular cooler or fridge will do. However, for those who want to experience beer at recommended temperatures, you may want to consider buying a quality beer fridge.\nYou must log in to post a comment."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:6b181f1e-84b2-495b-b9c7-7265a1d5a02a>","<urn:uuid:04b96531-bcde-4951-a777-0a7b828b4d6e>"],"error":null}
{"question":"For long-distance running (>1 hour), which provides better hydration and energy - plain water or sports drinks?","answer":"For long-duration exercise over 1 hour, sports drinks can offer benefits that water alone cannot provide. Sports drinks help delay muscle fatigue by providing 30-60 grams of carbohydrates and balancing electrolytes like sodium and potassium, which helps prevent muscle cramps, headaches, and nausea. However, for shorter workouts under 1 hour, plain water is sufficient for hydration needs. The benefits of sports drinks are really only applicable for intense exercise lasting more than 1-2 hours where you might deplete glycogen stores and lose significant electrolytes through heavy sweating (like losing 2-3% of body weight).","context":["You might want to think twice before you rush to slug down another post-workout smoothie. Your pre-workout routine of oatmeal and fruit? It might not be helping in the way you think. And the extra BCAAs you’re drinking during your workout? The real impact is likely only on how much money you have in your wallet.\nFrom building muscle to surviving your endurance runs, the rules of workout nutrition have completely changed. But, there’s one big problem: few people are aware of what really helps you fuel before a workout and recover afterward.\nWhich is why this is both your warning and a sigh of relief. The latest breakthroughs have rewritten the script, and that’s good news for anyone who likes to exercise. Gone are the days of carb-loading or rushing to have protein within 30 minutes of finishing your workout.\nIn fact, both nutrient timing and workout nutrition needs have liberating truth: Neither matters as much as we once believed.\nSo, while you might look at the past as wasted, it’s best to view these new rules for what they are: a serious fitness upgrade that makes it easier than ever to eat the right way to fuel performance, strip away fat, or even build extra muscle without all the extra, unnecessary eating.\nConfusion 101: Are Sports Drinks Better Than Water?\nIf you really want to know why the advice has been so misguided, you don’t have to look any farther than the sports drink aisle at your grocery store.\nFor most weekend warriors, the need for a sports drink (think Gatorade, Powerade, or any other energetic adjective + “ade”) isn’t as real as the ads make it seem.\nYes, there can be benefits to sports drinks. But, the liquid rejuvenation is limited to a very select group of exercises that deplete their bodies of certain nutrients.\nAnd, for most gym-goers, runners, and weekend warriors, it’s rare that you ever push your body to the point of needing the type of energy locked inside the bottle.\nYou see, most people’s workouts fall into one of 2 categories:\n- High intensity but shorter duration (think less than 1 hour of gym activity)\n- Lower or moderate intensity for a longer duration (think 1-2 hour runs)\nIn both of these cases, the only necessary hydration is water. If you want a little boost, then you might want to sip on some electrolytes (think more sodium and potassium than you’ll find in sports drinks, as well as calcium and magnesium), and a few carbs to help with hydration — but not the 30 grams of sugar packed into your favorite sports drink.\nWhen you’re working out at a high intensity and for longer periods of time (think more than 2 to 3 hours), that’s when sports drinks offer the most benefits because they refill what is lost during that type of extreme condition.\nIf you regularly sweat out 2 to 3 percent of your body’s weight during long duration, intense exercise—3 to 6 pounds, for most of us—you probably need more sodium. That’s what a sports drink provides.\nThe same goes for the minerals you lose through heavy sweating. For example, most athletes know about electrolytes. In particular—potassium, magnesium, and sodium—are essential (and have the name “electrolyte) because your body needs them to transmit electrical signals from your brain to your muscles. This is what allows your body to function.\nBut, the same type of research that was used to formulate products like Gatorade was also the basis of your workout strategy. In other words, Gatorade was designed more for high-level athletes than high-level executives, mothers, fathers, and typical gym-goers.\nThis was the basis of nutrient timing theory: The high carb amounts. The immediate need for protein. The fear of fats slowing down recovery.\nThe reality? None of it was really designed for your body.\nDo You Have To Eat Directly After Your Workout?\nLet’s set one thing clear: What food you put into your body is still very important and determines how hard you can exercise and how well you recover.\nThe bigger issue is exactly what you should be eating, or maybe, more importantly, when you should be eating it.\nThe idea of the “anabolic window” or that you need to eat as soon as possible after finishing your workouts is one of the most misleading pieces of fitness advice that has persevered for decades.\nIt’s based on a fear-driven, scientifically-debunked mentality that your muscles live in an hourglass, and with each passing second of eating before or after a workout you were losing out on improvement.\nFor the past 20 years, the prevailing idea was that you had about 30 to 60 minutes to eat something after your workout. If not, your body would become catabolic (a state of stress) and you would lose muscle, not recover fast enough, and fail to see the benefits from all your hard work and time invested.\nWhen you think about it, the theory seems crazy. How could the human body have such a small window for recovery?\nThat was the question exercise physiologist Dr. Brad Schoenfeld aimed to solve.\nHe reviewed a large number of studies that examined nutrient timing and set out to answer a simple question: Is there such thing as the “anabolic window?”\nTurns out there is—but it’s much bigger than anyone ever suggested. And the timing of your meals after a workout isn’t even the biggest indicator of your success. (More on that in a moment.)\nWhen Should You Eat After Your Workout?\nAfter you exercise you burn up your main energy store of carbohydrates, also known as glycogen. So, it only makes sense that you need to refuel glycogen by eating lots of carbs to replace what was lost.\nBut, when food was consumed in a shorter window of time after a workout there was no significant difference than when it was consumed after a long delay.\nIn fact, the research would go as far as suggesting that your post-workout window is actually the entire 24 hours after you train, with the key time to eat ideally occurring anywhere within 4 hours after you finish your last set, stop your run, or end your athletic event.\nNot exactly the same message as slug your protein shake before your muscles shrink.\nHow did this massive misunderstanding occur?\nIt goes back to the sports drink phenomenon. The “glycogen emptying” idea wasn’t really applicable to the average person. In reality, it takes a tremendous effort to completely deplete your glycogen stores.\nExtreme marathoners can do it. Bodybuilders who train twice per day can do it. NFL athletes who play a 3-hour game can do it.\nBut you? It’s a different story.\nMost people don’t’ go to the gym completely fasted or do workouts that completely tap-out your energy reserves (even if you feel exhausted). And yet, those were the test conditions used to determine what to eat after your workout.\nWhile it might feel like your body needs food immediately, the ROI of rushing to or even forcing food into your system is minimal: you won’t see added strength, additional muscle, faster fat loss, improved endurance, or a boost in recovery.\nThe new rules of nutrient timing focus on the bigger picture. If you want to perform and look your best, then you need to consider three factors: what you eat before your workout, what you eat after, and what type of activity you perform.\nHow to Fuel Your Workouts The Right Way\nJust because the timing of your post-workout meal has been reduced from urgent to “apply on your time,” doesn’t mean the entire concept of nutrient timing is dead.\nIn fact, it’s just the opposite. There’s never been a clearer idea of exactly what you should be eating to help your body. And the biggest breakthrough is clear. Protein is the new carbs.\nIt used to be that you needed to fuel up with carbs prior to your workout and then replenish after your workout. This all ties back to glycogen as a primary source of energy and fuel for your body. Most research tested the benefits of using carbohydrates as fuel and then tested different amounts of carbs.\nBut, even that rationale was a bit flawed. Nutrient timing should focus on three aspects that help improve your performance and appearance.\nGlycogen replenishment: Glycogen is your fuel. The more you have the harder you can push your body for longer periods of time.\nProtein breakdown: If you want to gain muscle, protein synthesis (anabolism) has to be greater than protein breakdown (catabolism).\nProtein Building – Protein Breakdown = Muscle Growth or Loss\nSo, it only makes sense that you want to slow the breakdown process.\nProtein synthesis: Eating protein after a workout is supposed to optimize the other side of the same equation by increasing muscle protein synthesis, the process that helps you repair and rebuild muscle.\nCombined, all three of these factors influence how hard you can train (endurance, strength, work capacity), how well you recover, and your ability to build muscle and burn fat. So it only makes sense that what you eat should target any or all of these goals.\nDo Carbs Help Your Workouts?\nCarbs are a great source of fuel for your body. But, eating more carbs doesn’t necessarily mean you’ll have more energy. And that’s because depleting glycogen is actually very difficult.\nFor example, let’s say you did a full-body workout of 9 exercises, performed 3 sets of each exercise (so 27 sets total), and pushed at a high intensity of 80 percent of your 1 rep max. That’d be a grueling workout, but when researchers tested this exact protocol, they found that it only depleted about one-third of total glycogen stores.\nEven crazier? When a similar workout was tested and followed with no food, about 75 percent of the depleted glycogen was replenished within 6 hours.\nSo what’s going on? Your body is protective of your energy. The more you deplete your glycogen, the faster resynthesis occurs. The higher your intensity, the quicker you recharge. Even in marathon runners and endurance athletes, complete resynthesis is usually complete within 24 hours.\nThat’s not a call to avoid carbs. They are important and necessary, and if you’re exercising they need to be a part of your plan.\nBut, the extreme nature of pre-workout (carb-loading) and post-workout (insulin-spiking) carb needs were overblown. You don’t need to fuel up with hundreds of grams of fuel pre, during, and post-workout because you’re not tapping out your glycogen.\nWhen your tank is empty, you’ll know it without question. So, your ideal carb plan will ultimately depend on the type of activity you perform.\nHow Much Protein Should You Eat After a Workout?\nWhen eating protein and carbs was compared to carbs alone, it instantly became clear that protein is your body’s best friend. Adding protein improved recovery, muscle protein synthesis, and protein breakdown.\nBut most interesting? When protein and carbs (25 grams of protein and 50 grams of carbs) was compared to just protein alone (25 grams), there was no additional benefit in terms of muscle protein synthesis or muscle protein breakdown when the carbs were added.\nThe verdict: Protein is the new king of workout nutrition.\nAnd it doesn’t end there. While we know that protein is important for preventing muscle protein breakdown and fueling muscle protein synthesis, and some carbs (but not too much) are good for glycogen, how much you eat around your workout should not be your primary consideration.\nResearch shows that the most important dietary factor for performance and appearance was not how much protein or carbs you had before or after your workout, but rather how much you ate in the entire day.\nIn essence, even if your pre- or post-workout nutrition was less than optimal (say, if you’re in a rush to get to work), as long as you still ate the right amount of nutrients (proteins, carbs, and fats) for the entire day, then you would still see benefits.\nThe Best Pre- and Post-Workout Nutrition Plan\nTiming nutrition around your workout is a good idea for both fueling your performance and helping recovery. But, you don’t need to stress the timing as much as we once thought. Instead, the urgency of nutrition depends more on the activity you perform and whether you eat something before you exercise.\nWhen you enjoy a pre-workout meal, that will determine what you need after a workout. That’s because eating before your workout ensures that your insulin, amino acid, and glucose levels are still going to be high several hours after the workout.\nMost mixed meals will keep your insulin levels high enough to stop protein breakdown for 4-6 hours. A 45-gram dose of whey protein will do the same for about two hours. Most studies have shown that if you eat protein before, immediately after, or several hours after your workout, your muscle protein synthesis will be about the same.\nTranslation: choose a pre- and post-workout nutrition approach that works for you.\nIf you don’t like to eat before a workout, then don’t. But you’ll want to emphasize that post-workout meal more because you won’t have protein or carbs in your system.\nIf you do like a meal before exercise, there’s no rush to refuel immediately after. Not to mention, if you load up on carbs (such as with oatmeal or some fruit), depending on your type of activity you might not even need post-workout carbs.\nThe closer your meal is to the training bout, the longer your window following the session. And both are dependent on your primary training goal. Meaning there isn’t a gold standard for what you should be eating around your workouts. Instead, you should fuel your body based on the type of activity you perform.\nAnd remember, as long as you consume enough protein by the end of the day, your body generally has no trouble growing new muscle tissue, recovering, or having the energy needed to push through and become better.\nTo help you figure out your needs, use the activity chart below — based on the latest research — to help determine exactly what you need for your body and your goals.\nThe Ultimate Guide to Workout Nutrition\nYour Goal: Endurance Sports\nExamples: Long-distance track and cycling events, marathons, basketball, soccer, MMA\nWhat to eat: Carbohydrates for replenishing muscle glycogen, maintaining stamina, and maintaining energy during your event.\nWhat to remember: It’s easy to argue that nutrient timing is most important for endurance athletes because of the duration and demands of the activity. Performance is the main goal, therefore making carbohydrates more important as a fuel source during the activity and after for recovery. Protein, while useful for minimizing protein loss, is not as essential in the moment for these athletes, but is still important for recovery and retention of muscle.\nYour Nutrition Plan\n- The Focus: carbs and protein\n- The dose: 0.2-0.25 g/lb target bodyweight for both protein and carbs\nDuring your workout\n- For every hour of endurance activity, consume 8-15 g protein and about 15-30 grams of carbs. Liquids and gels are usually best for this.\nYour Goal: Strength/Power Sports\nExamples: Olympic weightlifting, football, powerlifting, bodybuilding, high-intensity intervals\nWhat to eat: Protein for optimizing muscle recovery and growth and minimizing muscle damage\nWhat to remember: Based on the length of time and type of activity, muscle glycogen is not depleted to the extent of endurance sports. Protein is important for supporting strength and muscle growth while minimizing muscle damage and loss. Carbohydrates are important, but less so, and are generally taken care of by meeting total daily calorie and macronutrient goals.\nYour Nutrition Plan\n- A balanced, full meal consisting of carbs and protein, 0.2-0.25 g/lb target bodyweight for both protein and carbs\nYour Goal: Weight-Loss\nExamples: Any type of activity geared towards losing weight. This is your typical cardiovascular type of activity (walking, treadmill, stairstepper) or weight training. NOTE: This is not high-intensity work or something like CrossFit, which is more likely to fit into the strength or endurance categories.\nWhat to eat: Fewer calories (calorie deficit) and more protein\nWhat to remember: The most important thing to keep in mind is you must burn more calories than you bring into your body. Create a calorie deficit first, and then worry about dialing in your pre- and post-workout nutrition.\nYour Nutrition Plan\n- Eat a balanced, full meal consisting of carbs and protein, 0.2-0.25 g/lb target bodyweight for both protein and carbs\nYour Next Steps\nRemember that nutrient timing should focus on three core aspects: glycogen replenishment, protein breakdown, and protein synthesis. And rather than stressing over timing, focus on giving your body the proper nutrition based on what type of activity you perform.\nHave questions? Share them in the comments below.\nOr if you’re looking for more personalization and hands-on support our online coaching program may be right for you. Every client is assigned two coaches — one for nutrition and one for fitness. Find out more here.","Should You Trade Your Sports Drink for Water?\nDuring or following a workout, you may reach for a Gatorade or other water enhancers with good intentions. But are sports drinks good for you and do they offer any sort of benefit?\nHydration is key and equally as important as the actual workout when it comes to recovering properly and reaping the greatest benefits. And during and following a workout, you may be reaching for a Gatorade or other marketed water enhancers with good intentions. But are sports drinks good for you and do they offer any sort of benefit?\nThe Truth About Sports Drinks\nPublished in the British Medical Journal (BMJ), Deborah Cohen investigated \"the truth about sports drinks,\" linking the sports drinks industry with academia that helped to market the science of hydration. The piece insinuates although sports drinks are advertised as an essential tool for exercise performance, evidence lacks to support such a view. Much of the hype of sports drinks also relates to marketing woos and advertisements, with BMJ further indicating companies started sponsoring scientists to carry out research on hydration that eventually trickled into guidelines and recommendations, which were then carried out by active consumers. And even in the absence of exercise, advertisements surrounding enhanced water and sports beverages may confusingly lead us the general population to believe tap water is missing essential ingredients that hydrate our bodies. Nonetheless, most sports drinks supply an average of 100 to 200 calories strictly from sugar, along with electrolytes such as sodium and potassium.\nSports Drinks vs. Water: What Should You Opt for?\nOpting for a sports drink or water is dependent on a number of factors, though there is no denying pure water is the drink of choice for maintaining hydration day-in and day-out. And as a general rule of thumb, the overall population should consume at least 64-ounces of water daily. Additional considerations detail regular exercisers and athletes should drink 16 ounces of water leading up to activity, along with six to 12 ounces immediately prior to exercise and every 15 to 20 minutes of active training.\nNevertheless, sports drinks should not be rejected or excluded entirely, as they certainly can and do have their time in place. Sports nutrition experts recommend athletes to include 30 to 60 grams of carb in the form of sports drinks, solids, and gels if exercising for more than one hour or in hot and humid conditions. Doing so helps delay muscle fatigue and offers extra energy to keep athletes powered during a long exercise duration. Sports drinks balances electrolytes to keep the body working efficiently and reduce the risk of muscle cramps, headaches, and nausea.\nPouring Out the Verdict\nWhile sports drinks can inspire proper hydration, they certainly are not for everyone. Particularly if not exercising and sweating for over an hour’s time, go for water to keep hydrated and combat the possibility of weight gain. Additionally, even athletes do not have to solely rely on on sports drinks for long-duration exercise, as a number of foods can also help replenish electrolytes and offer valuable nutrients that are likely not included in sports drink, including chocolate milk, yogurt, bananas, apricots, leafy greens, beans, avocados, raisins, and dates. All-in-all, and if quick and extra carbs and electrolytes are warranted, a sports drink can be a convenient and accessible selection!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8a814cd6-27b0-4409-b5c6-b658b7469570>","<urn:uuid:17c99782-5742-4813-8689-06ea59e1717d>"],"error":null}
{"question":"What are the benefits of better quality woodwind instruments, and how can cracks be repaired?","answer":"Better quality woodwind instruments offer superior construction and materials that provide more room for growth and produce bigger, better sound which is encouraging for students. They're particularly beneficial for students in strong music programs, those taking private lessons, or those practicing without parental encouragement. Regarding cracks, if a barrel crack extends into the bore, it's typically caused by humidity and can be successfully repaired with glue. As a temporary emergency solution, tape can be wrapped around the barrel to seal it. Importantly, oil or grease should not be applied to cracks as this will make future gluing difficult or impossible.","context":["Many people find themselves thrown into the world of musical instruments they know nothing about when their children first begin music at school. Knowing the basics of good instrument construction, materials, and choosing a good store in which to rent or buy these instruments is extremely important. So what process should a parent follow to make the best choices for their child?\nClearly the first step is to choose an instrument. Let your child have their choice. Kids don’t make very many big decisions about their life, and this is a big one that can be very empowering. I can also say from personal experience that kids have a natural intuition about what is good for them. Ultimately, my strongest advice is to put a child into a room to try no more than 3-5 different choices, and let them make their choice based on the sound they like best.\nThis information is intended to broaden your horizons, not to create a preference, or to put you in a position to nit-pick in the store! Most instruments are extremely well made these days, and choosing a respected retailer will allow you to trust recommendations. Ask your school and/or private music teacher where to shop.\nWoodwind instruments are made all over the world, but primarily in the USA, Germany, France, and China. When we talk about Woodwind instruments, we are referring to members of the flute, clarinet, saxophone, oboe, and bassoon families.\nAll woodwinds involve a fairly complex, interconnected mechanism that has to be regulated so that the keys all move and seal the holes of the instrument when they are supposed to. Your trusted local retailer will be sure to get you an instrument that is ‘set up’, although many new instruments come ready to go out of the box. When you are dealing with a brand new instrument, you should bring it back to the store for a check-up after about 3 months, or sooner if there are any issues. Because all of the materials are new and tight, they may come out of regulation as the instrument is broken in. This is normal. You should count on this kind of regulation every 12-18 months, or sooner if the instrument is played a lot.\nWoodwinds also have pads. Pads are the part of the instrument that seal over the holes in the body of the instrument (toneholes). A perfect seal is required to produce the correct note. Tuning and sound quality are affected by a correctly ‘seated’ pad. These also occasionally need replacing, as part of your regular maintenance, although very rarely all at once. When all pads need to be replaced (once every 8-10 years), this is done as part of a comprehensive ‘overhaul’ of the instrument which includes taking it all apart, cleaning it, refitting and tightening loose parts, and replacing springs and corks as necessary. This is a rare procedure, and generally reserved for professionals. The maintenance repair is the most common one for parents.\nBecause of the many rods and key-cups (these hold the pads), there are a lot of very sensitive, easy to bend parts of these instruments. Knowing how to assemble them properly is important to avoiding unwanted repair costs. Be sure to ask your local retailer for the proper way to assemble your instrument. This is often the cause of the most common repairs, followed by bumping into things.\nInterestingly, not all woodwinds are made from wood. Flutes and saxophones are made primarily of metals; Nickel-silver and silver for flutes, and generally brass for saxophones. We’ll stick to these materials for these instruments for simplicity’s sake, as there are increasingly more choices available.\nFor the rest of the woodwind instruments, wood is indeed employed for the main construction of the instruments.\nFlutes & Saxophones\nStudent flutes are made from Nickel-Silver, then plated in silver. Nickel-Silver is a combination of brass with nickel, which has a similar look to silver when polished, hence its name. One of its primary advantages is that it is stronger than brass or silver on their own. As you progress to better instruments more Silver is used, starting with the headjoint (which is the most important factor in a good quality of sound). More on headjoints later.\nSaxophones are generally made from brass. Try to find an instrument that has ‘ribbing’ on the body; extra plates of brass that provide structural support over an area where multiple posts attach to the body. This provides strength for the occasional and unavoidable bumps that your young students are bound to have. Some student saxes have keywork made of nickel-silver, which is a good strategy for strength in a vulnerable area.\nClarinets and Oboes\nClarinet and oboe bodies are typically made of ABS plastic for student instruments. This is a good strategy for bumps, but also against the maintenance habits and climate changes that students face. Intermediate and professional instruments are made of Grenadilla wood (which is changing as Grenadilla edges towards the endangered list). Because they are made of wood they must be protected against cracking. If a student doesn’t swab their instrument out after playing, the moisture can cause the wood to expand and crack. Likewise, bringing your instrument to school on a cold day and playing it without allowing it to come to room temperature will cause it to crack, or even rupture. This is caused a pressure differential from your warm air column on the inside of the instrument, versus the cold temperature outside of the instrument. If you decide to get a wood instrument, be sure your student is ready and able to look after it properly.\nKeys on clarinets and oboes are generally made from nickel-silver, but can be made with silver plating, or other materials.\nStudent bassoons are made from ABS plastic, but there are some new makers in the market that offer hard rubber, and also maple (used in professional instruments). A downside for hard rubber bassoons is that they are quite heavy. If you can get a good wood bassoon for a reasonable price, then choose this one. Wood offers the best acoustics for bassoon, and can make the difference between a plain sound, and one that is rich and interesting.\nKeywork on bassoons is equally made from nickel-silver, often silver plated.\nUsing the word ‘mouthpiece’ for woodwinds can be confusing. Here are the instruments with the correct names for the corresponding part of the instrument that makes the sound:\n- Flute: Headjoint\n- Clarinet: Mouthpiece (with a single reed)\n- Saxophone: Mouthpiece (with a single reed)\n- Oboe: Double reed (two reeds tied together with a hole in between)\n- Bassoon: Double reed (two reeds tied together with a hole in between)\nRegardless of the instrument, this is the part of the whole that makes the greatest impact on the quality of the sound, in combination with the player’s personal physical attributes. Students generally use what they get from their teacher, but below are some tips about how to get the most from your equipment. Getting a good mouthpiece can precede, and even postpone the purchase of a new clarinet or sax, so great is the difference with hard rubber.\nFor Flute, make sure your headjoint cork is properly aligned, and not dried out. Your local retailer will show you how to do this. If there are problems, have them fixed right away, or choose a different flute. For more intermediate flutes, choose a headjoint that is not only made entirely of silver, but is hand-cut. This won’t always be easier to play at first, but the sound quality improvement will be worth making the leap. Silver sounds better than nickel-silver, producing a better tone quality, with more room for changing the quality according to the player’s needs. You can buy headjoints separately, but it can be very expensive, and I advise against this until you reach a professional flute.\nOboe and bassoon use two opposing, slightly curved reeds tied together that vibrate against each other when air passes between them. Advanced oboists/bassoonists make reeds for themselves, a time-consuming, skill-heavy task. It takes many years to learn to make reeds for yourself, that work well. Fortunately, there are ready-made reeds that generally meet the needs of the student player. One key element you should test is to assure that the reed ‘crows’ perfectly at the pitch ‘C’. Crowing a reed is blowing through it when it is not attached to the instrument. Test the crow with a tuner.\nClarinets and saxophones use a single reed (small piece of very well shaped and profiled cane) tied to a mouthpiece (by a ring called a ‘ligature’) that vibrates when air is passed between the two. The combination of these parts is key to a good sound. Most students receive a plastic mouthpiece to begin. Good plastic mouthpieces are made by Yamaha for both clarinet and saxophone, with the designation of ‘4C’. I would recommend a ‘5C’ if it is available. It will be a little harder to play at first, but a good way to get a bigger sound right off the bat. If you would like to get a better quality of sound with more room for good loud and soft playing while maintaining and introducing a rich tone, then consider a hard rubber mouthpiece. Hard rubber is superior to plastic acoustically, and must be hand finished, unlike the plastic variety, which is spit out of a mold and polished/tumbled for shine. These are noticeably more expensive, but you should expect to spend in the $100-150 range for a decent hard rubber mouthpiece. Good names include: Selmer, Vandoren, Otto Link, Meyer, Yamaha, and Leblanc. Your local retailer should stock at least two of these brands for you to try – and you should try them! Because these are typically hand finished, they are often subtly different.\nWhat about sizes?\nClarinet and saxophone mouthpieces have a variety of different sizing areas, but for the sake of simplicity, the most important is the ‘tip opening’. Tip opening refers to the distance between the tip of the reed and the tip of the mouthpiece. Sadly, there is no standardized system for measuring tip openings, although they are commonly measured in millimetres, or using a numbering system (usually beginning at number 5, a student sizing), or even letters. The metric method usually consists of two to three numbers; an opening of 2.97mm might be listed as 297, or as 97, depending on the maker. The numbering system can be listed as 5, 5*, 6, 6*, 7, etc. The ‘star’ numbers should be considered half-sizes. Letters work the same way as numbers in general; C, C*, D, D*, etc.\nTo give your student a leg up, aim for a ‘6’, or ‘D’ sizing. This is bigger than what they are used to, but will pay off with a bigger sound right away. Some notes on the ends of your range, both low and high, will likely suffer, but this is only temporary as you adjust to the new mouthpiece and develop greater strength.\nOil and Adjust. This procedure needs to be conducted on your student’s instrument annually, or even more frequently, if there is a lot of playing. The mechanics of the interconnected parts is delicate, and comes out of alignment often.\nBore oiling. Once a year this will be required on clarinets and oboes to help guard against cracking.\nAvoid cheap instruments. With musical instruments you get what you pay for. There are a lot of instruments coming from India and China now. Many are excellent, while many others should not even have been made. Your local, respected dealer should have those that are reliable, and will stand behind them. Your big-box Costco, Wal-Mart, BestBuy, and e-Bay has no expertise in these matters, and functions for their bottom line only. Avoid these places. They cannot possibly offer you the continued assistance, service, or repair that a developing and interested student will need. If you choose this route, ask for American, European, or Japanese-made instruments. This will be a major separator of good from bad. People who make in these places are generally very well trained and part of a history of excellent wind instrument making. Your local, trusted retailer will help to guide you in the choices available, and remember that just because it says USA, or Paris on it, does not mean it was made in these places. Manufacturers are now sometimes making these things part of the ‘name’ of the instrument.\nHow much should I spend?\nThat is the big question. Be aware that popular instruments, like flute and clarinet, are less expensive because they are made in greater quantities. Some instruments, like oboe and bassoon, are challenging and time-consuming to make, making them more expensive. Below is a list of acceptable and approximate pricing (at the time that this is being written) for new student instruments that works for both American and Canadian currency.\n- Flute: $350\n- Oboe: $1400\n- Clarinet: $350\n- Alto Sax: $750\n- Tenor Sax: $1200\nWhen should I buy a better instrument, and Why?\n60 years ago, there were no ‘student’ and ‘intermediate’ instruments. Manufacturers were just coming to the realization that there was an emerging, post-war market that was changing to support a more commercial model of instrument making. Today, instruments are engineered to get you to buy three times. First as a beginner, then as an advancing student, and finally as a professional. Clearly, this is a model that makes a lot of money for manufacturers.\nFor the right reasons, I often encourage parents to start with the better instrument, or even a good used intermediate or professional instrument. Starting on better equipment is like starting on that slightly larger mouthpiece; getting a bigger, better sound is encouraging. The better construction and materials combination of these better instruments will also leave more room to grow. So what are the right reasons? Here is a list that works not only as guide for helping to choose the right instrument, but for what you should watch for to help musical growth:\n- Going to a school with a strong music program.\n- Getting private lessons, or has asked for some. (Check with private teacher for recommendations before buying, this will help.)\n- Practicing without parental encouragement\n- Has at least 4 years of playing ahead of them.\nThese factors are good indicators of whether to buy, and whether to buy intermediate or professional. If the bulk of these are unclear, consider a rental for a year to see if they get any clearer, and supplement with regular (weekly) private lessons.\nMusic is an investment that requires attention from a variety of angles, and the instrument itself is just a small step. Being armed with the knowledge of how to get the instrument is just part of a process that a parent can – and should – be actively involved in. Many parents don’t know anything about all of this, but now you do! Ask the questions you need to know, and you’ll be just fine getting your new instrument.","1.Barrel gets stuck: Humidity can enter the wood and increase the diameter of the top-joint tenon. This can cause problems, because the connection between the top-joint and barrel becomes too tight, which makes it hard or even impossible to take the barrel off. The solutions are:\n1. Hold the barrel very tight in your hand and instead of trying to turn the barrel, wiggle it upwards.\n2. Bend the barrel in all 4 directions, like you would like to break it off. Don´t try it to hard and not on Greenline instruments! Greenline tenons can break! .\n3. Lay a leather belt around the barrel in a sling and try to turn it.\n4. Just wait a day or two and have the instrument dry by itself. Keep it in a dry environment.\n2.Barrel wobbles If the Barrel wobbles, lay either a threat, some tape or paper around the top-joint tenon, before you push on the barre.\nIf a barrel cracks at the top or bottom, where the tenons of the mouthpiece or top-joint enters, the crack might be caused by physical force. To prevent that from happening, be always careful not to use to much force when you ensemble the clarinet. Don?t tilt the different parts just turn or push them in straight.\nIf a crack goes all the way into the bore, humidity would have caused it. It can nicely be glued and the barrel will be fine again. As an emergency solution, put some tape around the barrel to make it seal again. Do not use oil or grease on the crack, because that will make it difficult if not impossible to glue afterwards.\n4. Loose sockets When the humidity is very low and the wood shrinks metal tenons can loose their bound to the wood and become loose. If that?s happening it can easily be tightened again. If the metal receiver was glued on with a thermo glue or shell lack, the metal tenon needs just to be heated up with a torch. If in doubt ask an experienced repairman do it for you.\n5. Barrel versus tuning rings To change the length of an instrument just with a barrel will have its biggest effect on the notes close to the barrel. A better solution might be to combine it with some tuning rings. This way, the instrument can be prolonged just a bit at the barrel and some more at the lower-joint. A more even tuning will be possible.\n6. Zoom barrels If you turn a zoom barrel to fare apart, you might not get it to work again. Don´t use force, but give it to someone who knows how to handle it.\n7. The original barrel The original barrel was reamed together with the top-joint of the instrument when it was built. Therefor is there a good chance that it will have a very good and smooth connection to the top-joint in the beginning. Over time both joints might develop differently and the bores of both joints might become oval. In some cases this might cause problems. Try to turn the barrel in different positions and see if it improves it?s performance. If so just remember the position and keep it that way. There is no reason to exchange the barrel just because of this.\n8. The cork is to thick New instruments or joints might have a cork which is to thick to go on smoothly. First make sure that it is really the cork and not the wood that?s causing the problem. If you ensemble the pieces and the cork is to thick, just take either a 240 grain piece of sandpaper and take of some of the cork. Alternatively use a fingernail file and remove some cork. Do it carefully until you have a firm but smooth fit. Apply some cork-grease before you put the joints together.\n9.The tenons can not get all the way in This often happens to new instruments. The instrument was dried to it?s minimum humidity level. Afterwards the body was turned to its dimensions as soon as you start playing the humidity level will increase. This causes the wood to increase as well and the tenons will expand. The Metal rings on the receiver side will prevent, that the receiver can expand as well. You will notice that the joints get nicely together until about 4-5 mm before the end and then tighten up. That?s the area where the wood of the tenon is to thick. Take a thin strap of sandpaper or fold a piece of sandpaper to make it more stable and remove a layer of wood of the tenon just above the cork. Do it carefully. If it still binds to it again. At one point you will notice some progress but it will not be enough. You will now also notice, that some areas, where you sanded, the wood showed the typical sanding marks. Other will have a shiny surface. These are the areas which are still too thick. Continue to sand just these areas down until you have the fit you like.\n10. The cork is loose Lately many tenon corks on new instruments become loose. If you can remove the cork gently in one piece, do so. Clean the loose cork surface with alcohol or gasoline and apply a thin layer of contact cement on the wood and the cork. Waite for some minutes until it feels dry and then press it tightly back into place. If only one end is loose, clean only this bit on the cork and wood, apply the contact cement, wait and press it into place."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:cb127ec0-46ed-43e9-a725-7d8486672e48>","<urn:uuid:c59adf8a-dc8a-407b-a14d-40c71a64f34f>"],"error":null}
{"question":"Which evidence of water on Mars is more abundant: the carbonate minerals found in the Comanche outcrop or the traces of NASA's debris scattered during Curiosity's landing?","answer":"The carbonate minerals in the Comanche outcrop represent more abundant evidence of water on Mars. The outcrop contains between 16 and 34 percent carbonates, which is a significant concentration that required water to form through precipitation. In comparison, while Curiosity's landing left various debris and impact sites documented by HiRISE camera, these artificial remnants do not provide evidence of water on Mars.","context":["Four months ago, NASA landed a one-ton, nuclear-powered rover on the Red Planet, scattering the Martian terrain with heaps of debris in the process. Curiosity, upon arrival, shed so much of its Entry Descent & Landing hardware that high resolution images of the planet's surface are still turning up traces of the rover's rubble. Let's review all the photographic evidence of NASA's space litter collected so far; after all: nobody's going to get around to picking up this mess for a while (or ever?) — so for now, we might as well enjoy the view.\nFeatured up top is one in a series of newly released images of the Red Planet's Curiosty-littered surface, photographed from Mars orbit by NASA's powerful HiRISE camera. The Agency thinks that the smear of debris and impact ejecta was left by one of two dense, 165-pound blocks of tungsten, jettisoned by the rover just before atmospheric entry. Click through to see more traces of the rover's cruise stage, and various other bits of spacecraft scrap.\nA mosaic, created from two map-projected HiRISE images, reveals four large impacts and a number of smaller impacts, believed to have been made by the two 165-pound blocks of tungsten and various pieces of the rover's cruise stage. The two central impacts, the ones that are close to one another and similar in size, are thought to have originated from the tungsten blocks. The other two, notes HiRISE PI Alfred McEwen, \"which have more asymmetric ejecta, may be from the cruise stage, which broke apart into two main pieces. \" McEwen continues:\nWe were expecting to see just two impacts sites here—from the tungsten blocks—and it is highly unlikely that these dense blocks broke apart in the atmosphere. The only other source of impacts at nearly the same time and place is the cruise stage itself, which was more likely to break apart in the atmosphere.\nA high-resolution closeup of the tungsten block impact site. The large impact craters created by the tungsten blocks were roughly 3—5 meters in diameter, \"about what was expected,\" according to McEwen.\nA high-resolution closeup of the second tungsten block impact site.\nAn animated GIF of impact markings left by six, 25-kilogram tungsten blocks released by the MSL descent stage after atmospheric entry.\nThis photograph, captured by HiRISE on August 7, shows the rover's massive descent parachute.\nA high-resolution image of the crash site of Curiosity's rocket-powered sky crane, captured August 7. After gently placing the rover on the surface of Mars, the descent stage flew off to a safe distance before crash landing.\nA detailed, hi-res view of the sky crane's impact site, photographed August 7. Via HiRISE:\nThe subimage shown here shows the sky crane's impact site, as the sky crane careened in roughly from the northwest. The impact disturbed the bright dust, revealing the darker rocky substrate. It was no light impact.\nPossible pieces of the sky crane, appearing as small white dots within and at the end of the some of these dark streaks, are visible in the image and the zoomed-in version of the inset (arrows).\nHigh resolution image of Curiosity's heatshield's crash site, photographed August 7.\nA high-resolution photograph of the Curiosity rover and assorted EDL hardware, strewn about the martian surface — photographed August 7.\nNot \"litter\" per se, but a stunning high-resolution, color-enhanced view of Curiosity. Colors have been enhanced here to highlight color variations near the rover, including the blast pattern left by the descent stage (colors that appear blue here would look more gray in person).\nColor image of Curiosity's parachute and backshell, acquired September 2.\nA high-resolution color image of the Sky Crane' crash site, acquired September 2.","Crafty detective work based on data from NASA's Mars rover Spirit has uncovered large amounts of a rare type of Martian rock that adds more evidence that the red planet may have harbored liquid water in the ancient past.\nThe rock outcrop is rich in carbonate minerals and was found in the Columbia Hills, an \"island\" of low hills at Spirit's Gusev crater home on Mars. Spirit visited the outcrop in 2005, before it got permanently stuck in its current Martian resting spot.\nCarbonates are minerals that contain carbon dioxide and form readily in the presence of water. So, if the conditions were such that carbonate-bearing rocks were able to form, this would suggest that at one point, water – and possibly an environment favorable to primitive life – was likely present in the region.\n\"Carbonate forms as a precipitation product from water, so there has to be water around for them to form in the first place,\" the study's lead author Richard Morris, a planetary scientist at NASA's Johnson Space Center in Houston, Texas, told SPACE.com.\nThere is, however, no firm evidence that Mars has ever hosted life.\nHunting rare Mars rocks\nUntil now, geologic clues for the presence of carbonates on the Martian surface have been relatively scarce. In contrast, carbonate-rich rocks (such as limestone) are plentiful on Earth, where liquid water abounds.\nThe discovery of the carbonate-rich outcrop came as a surprise to researchers, since plenty of rocks that did not contain carbonates were previously found in the surrounding area.\n\"[The rocks] were found on the southeast lobe of Husband Hill in the Columbia Hills,\" Morris said. \"As Spirit was coming down slope on Husband Hill, we encountered a lot of rocks that don't have carbonates. We ran into this outcrop, analyzed it, and noticed that it was very different at the time, but didn't really appreciate why until pretty recently.\"\nThe Spirit rover actually visited the outcrop, called Comanche, back in December 2005, and while initial measurements were taken, the significance of the finding only recently became apparent.\n\"It took us a long time and a lot of detective work to figure this out,\" Morris said.\nSpirit and its robotic twin Opportunity have been exploring different parts of Mars since they landed in January 2004. Now in their seventh year on Mars, the rovers have far outlasted their initial 90-day mission.\nLast year, Spirit got stuck in deep Martian sand and NASA gave up trying to extract it earlier this year. The rover is currently hibernating and may not survive the harsh Martian winter.\nRed planet detectives\nThe researchers employed the full suite of Mars Exploration Rover (MER) instruments to make their discovery, including the Alpha Particle X-ray Spectrometer, which provided an early clue by detecting the uncharacteristic excess of light elements in the rock, and the Miniature Thermal Emission Spectrometer (Mini-TES), a mineral-scouting instrument that identified the carbonate minerals.\nWhen Spirit initially visited Comanche in 2005, the Mini-TES, which was developed at Arizona State University (ASU), had been unable to readily detect the carbonates in the rock because the instrument was partially blinded by Martian dust.\n\"Mini-TES got dusted months before Spirit reached Comanche, and we didn't have a good way to correct for the dust effects at the time,\" said Steve Ruff, a research scientist at ASU's Mars Space Flight Facility, and one of the study's researchers. \"We knew there was something weird about the outcrop's spectrum as seen by Mini-TES, but couldn't say what caused it.\"\n\"Spirit's Mossbauer spectrometer indicated that carbonate was possible, but I didn't believe it,\" Ruff added.\nThe researchers developed a calibration to remove the spectral effects and correct for the dust that had gotten into the instruments. Combined with the Mossbauer data and chemical data from a third spectrometer, the Mini-TES was able to confirm the carbonate minerals in the Comanche outcrop.\nThe precise sizes of the rocks are difficult to measure, Morris said, because a lot of them are buried beneath the Martian surface.\n\"There's a large outcrop that is roughly five meters (about 16 feet) across,\" he said. \"The ones we did most of the measures on were mostly 1.5 meters (almost 5 feet) across.\"\nThe research is detailed in the online June 3 edition of the journal Science.\nHow did they form?\nThe researchers are hoping that analysis of the rocks' composition could also provide evidence for how they were formed.\n\"The Martian carbonates have a bit of an unusual composition,\" Morris said. \"This has got to be telling us something. The next step is to look into the processes that formed these carbonates.\"\nThe mineral is rich in magnesium and iron and possibly originated via hydrothermal processes – forming through precipitation from the hot, residual waters from leftover magma that flowed through buried deposits of carbonate minerals, said Morris.\nMoving forward, the researchers will now examine terrestrial records to find carbonates that have similar compositions to those found on Mars. Knowing how much carbonate is present in the rock will allow the scientists to use analog examples from Earth to understand how they might have formed.\nTracking Mars carbonates\nScientists have been searching for carbonate rocks on Mars for decades, as such minerals are crucial to understanding the early climate history of the red planet, and the tangential question of whether Mars might once have hosted life.\nIn fact, small amounts of carbonate minerals have previously been detected on Mars.\nThe Martian meteorite ALH 84001 contained approximately 1-2 percent carbonate, and until the Comanche discovery, was thought to have the highest concentration of carbonate of the rocks found on Mars at the time.\nThe Comanche outcrop, however, is far more abundant, containing between 16 and 34 percent carbonates.\nMorris believes that there is a possibility that carbonate-rich rocks could also be found in other regions of Mars. Carbon minerals have been detected in the Nili Fossae region of Mars, which is about 3,900 miles (6,300 kilometers) away, opening the door to similar findings in the future.\nRuff said \"the Comanche data have been available to scientists and the public for about four years now. The new finding shows that this data set still harbors potentially major discoveries.\"\n\"Do other surprises await us?\" Ruff wondered. \"Who knows? But I'll make a strong prediction: More discoveries will be made with old data.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:af8bd1ec-8a1a-41bd-8990-160e6606c305>","<urn:uuid:a59443bd-0e03-4da3-a6ec-924dbfeeed03>"],"error":null}
{"question":"which materiel best for dog food bowls pls?","answer":"Stainless steel is the best choice for dog food bowls, as recommended by veterinarians. It is easy to clean and sanitize, very durable, and if it has a rubber bottom base, the bowl cannot easily slide on the floor.","context":["Preparing for a New Dog\nWhen a new dog is set to join a family, there are some important equipment and supplies necessary for ease of managing the pet. If you are a new dog owner, you can look forward to many years of a fun, and a loving relationship with the new companion. Before bringing it home, it’s a good idea to visit a nearby pet shop and buy all these necessities to avoid making trips to the store when the canine has arrived home. These supplies will ensure you keep the canine happy, content, and healthy. Here are some of the essential supplies to keep ready and how important they are for your dog.\nA Quality Dog Collar\nUsing the right collar as you go out for a walk with your pet can be enjoyable, however, using the wrong equipment can be a nightmare for you. It is an everyday safety items that is subject to tear and wear just like your clothing. Depending on your dog breed, size and your needs, there are a variety of collar types.\n– Leather Collar: the classic leather dog collar is a favorite of many dog owners due to its durable natural material that does not irritate the animal. With the correct treatment, the leather collar can withstand moisture for years. They are comfortable for the canine and preferable for the dog with sensitive skin. Buying good high quality leather dog collars is an investment that will last for many years.\n– Flat and Rolled Collars: this type of collar is great for attaching the tags and ID and are comfortable enough.\n– Nylon Collar: these are the most popular collar due to the durability and water handling abilities. Cleaning a nylon dog collar is very easy, and they are less expensive. You can get them in almost every color and print and can be adjusted. You can get a decorated collar for a fancy looking pet.\n– Plastic Collar: a collar can be made of plastic or made of another material then coated with plastic for added aesthetics and protection. They are cheap and are made with fun featuring all sorts of designs. Plastic collars are really inexpensive and are mostly used for decorative purposes. The plastic-coated nylon collar is stronger and well protected. They are also very easy to clean.\n– Spiked Collar: this type of collar is designed to protect your canine from attacks from other dogs and wild animals and target the neck area to subdue. Most people, however, like the spike because of how great it looks on larger dogs, especially pit bulls. Spiked dog collars also make it more difficult for strangers to grab the dog by the collar.\n– Custom-made Collars: now days, you can order custom-made collars handmade from a wide variety of materials. You have the option to choose the thickness, size, and inclusion into the collar. Custom made dog collars will be expensive, and it will take some time for the collar to be made. Some of the highest quality custom personalized dog collars are made by Yippr Pet Supplies\n– Choke Chain: like the name implies, this dog collar is made of chain links and features no stop in it meaning the more pressure exerted, the tighter it gets on the dog’s neck. It can be very effective if used correctly; otherwise, you will injure or kill the pet.\n– Martingale Collar: this type of collar is a hybrid between a regular and check chain collar. It is suitable or a canine whose head is smaller than the neck. It tightens when pressure is applied from the leash, therefore, giving negative feedback, so it does not remove the collar. It is not as harsh as the choke chain.\n– Padded Collar: this can be made of any material but padded on the inside for additional comfort to the neck of the canine. Quality padded dog collars made of leather should last for years.\n– LED Collars: a led lighted dog collar is popular for two reasons. If you like walking or letting your dog out to play at night, you will enjoy the safety feature of the led collar. Additionally, if you like glamor on your pet, then this collar suits you. The lights are placed inside the material for added beauty. A nice quality led dog collar will help keep your pet safe and visible to motorists, bicyclists and other pedestrians during low light hours.\nA dog leash plays a very important role in your relationship with the dog. It provides both comfort and safety when you are out for a walk and provides your dog with great freedom especially the longer leash. There are two types of leashes: There are many styles of leashes available. People that walk their dogs in the early morning or evening really like a LED Dog Leash because it increases visibility of their pet. One of the long favorites has been a good Leather Dog Leash simply because they are very strong and last for a very long time.\n– Standard leash: this type is made in a variety of material including nylon and leather and can be bought in varying lengths and widths. You can get a leash made of reflective fabric, or led collars are available for maximum safety.\n– Retractable leash: this feature thick plastic grip that releases a thin nylon cord operating like a measuring tape.\nEquipping your dog with a visible id tag is vital, especially if your pet gets lost. You can be contacted easily, or the dog can be taken back home since the tag can include the dog’s name, your contact information, and home address. However, the collar and id tags can come off since they are not foolproof. This calls for a backup in the form of a microchip. They are inserted under the skin to make it easy to identify the pet details and reunite it with the owner or even acquire its medical information.\nBed and Shelter\nEven your dog needs a comfortable place to sleep. A warm place to sleep at night gives you dog a sense of security. Your canine companion also needs a comfortable bed sized depending on its size and breed. It is best to buy a bed made of a natural material with non-skid bottom. The covers should be removable and machine washable to make it easy to clean regularly.\nIf your dog will be staying outdoors, then you need to buy or build it a spacious, safe shelter. This keeps out the weather and provides a sense of ownership to the dog. This is a place it can keep its toys and spend its day and night without disturbance.\nFood & Water Bowls\nThe three most common material used for dog’s food and water bowls are plastic, ceramic and stainless steel. The choice you make will depend on their features and your budget. Plastic is durable, but if the dog chews on the bowl, then this material is not the best for it since the ingested plastic can be harmful. Ceramic, on the other hand, is very stable especially if the canine likes pushing its bowl after eating. They are however porous meaning you have to clean it daily. Stainless steel is the best choice recommended by vets since it is easy to clean and sanitize. They are very durable, and if it has a rubber bottom base, the bowl cannot easily slide on the floor.\nYou need grooming tools if you are to keep a dog. The grooming activity serves as a bonding experience with your pet as it is pleasurable to the dog. Items like bristle brushes, wire-pin brushes, slicker brushes, and de-shedder are all necessary for different purposes which are all enjoyable to the pet. You need to choose the right brush for each fur type or dog breed and use shampoo and conditioner specially formulated for the dog.\nBring fun to the life of your new dog with plenty of toys including chew and squeaky things. These will help keep it mentally active as well as physically involved as it relaxes in its shelter or corner. These helps channel its excess energy, so it does not get into destructive behavior. Consider buying food-dispensing toys, tug toys, fetch toys and chew toys among much more."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:7f1507b4-d944-4dbb-a1f6-a68e543598a3>"],"error":null}
{"question":"What's the key difference between OpenDrive's data recovery time objectives and SupportHost's approach to minimizing downtime during disasters?","answer":"OpenDrive focuses on file versioning (up to 99 versions) and continuous backup options (updates every 30 seconds or immediately), but lacks specific recovery time guarantees. SupportHost, on the other hand, emphasizes minimizing downtime through a two-step rapid recovery process: first restoring backups from a different datacenter to a new server, then instantly updating IP pointing in their cluster without DNS propagation times, allowing for quick service restoration within minutes of a disaster.","context":["The fire that hit some OVH servers has revealed how many companies are still unprepared for disaster recovery measures. Let’s see what disaster recovery is and how this strategy, often too much underestimated, is instead crucial to maintain the business continuity and keep companies on their feet.\nIn this article, Disaster recovery: what it is and why it’s so important, we’ll talk about risk factors, strategies used to deal with emergencies and mistakes to avoid. But, first of all, let’s see what disaster recovery means.\nDisaster recovery: what is it?\nA simple and exhaustive definition of disaster recovery is the one we find on Wikipedia:\nWith disaster recovery, in computer science and in particular, in the field of information security, we mean the set of technological and logistical/organizational measures designed to restore systems, data and infrastructure necessary for the provision of services […] in the event of serious emergencies that affect the regular business.WIKIPEDIA\nThe term disaster recovery refers to the set of procedures to be put in place in the event of unforeseeable events. This strategy, therefore, includes several interconnected aspects. It starts with the identification of risk factors, in order to put in place preventive actions to be able to reduce them.\nSince risks, however, can never be reduced to zero, recovery actions must also be taken to restore business continuity.\nIt is no coincidence that disaster recovery is a strategy that is in turn part of a broader design known as business continuity. Precisely to ensure the continuity of a company’s activities, it is necessary to be prepared to deal with disruptions and restore them in the shortest possible time.\nBut let’s come with order and understand first of all what kind of risks are referred to.\nDisaster recovery risk factors\nThe disaster recovery strategy in a company only becomes concrete with the implementation of a disaster recovery plan. We are talking about what is called a Disaster Recovery Plan or even DRP.\nIn contrast to the strategy in which it is decided, (in advance which measures to put in place in response to a critical situation), the plan defines the precise steps to be followed. It’s important to have a plan in place to minimize errors and reduce recovery times. There is no room for improvisation, especially in a sensitive situation like this.\nIn a disaster recovery strategy, the term disaster refers to unexpected events of a highly diverse nature.\nOn the one hand, there are hackers that undermine computer security (viruses, phishing or DDoS attacks) not excluding real theft. On the other hand, there are natural events such as floods, earthquakes or fires.\nOther types of malfunctions can be associated with hardware failures or blackouts. The possibility of human error must also be considered.\nWhat is the purpose of a disaster recovery plan?\nFrom the definition of the risks, procedures are established to respond to the critical event in an adequate manner. This is the purpose of the disaster recovery plan: to establish how to proceed in order to minimize damage.\nReducing damages means limiting the disruption of business activities to the shortest possible time and therefore being able to limit the economic losses associated with the disruption itself. To do this it is strictly necessary to cooperate to restore the service as soon as possible.\nThis is why everyone in a company should be informed about the measures to be taken in such cases, so that they can do their part.\nIf operations cannot be restored to full capacity in a short period of time, a number of alternative procedures must be put in place promptly so that operations are not interrupted completely.\nIf a firm does not establish a disaster recovery plan, the risk is that business continuity will not be restored. Maintaining data properly so that it can be restored in the event of a disaster is essential to avoid the risk of losing it permanently.\nA loss of data can put the entire operation in crisis, bringing the company to a standstill. And it’s not certain that a business can recover from such an event.\nDisaster recovery strategies\nWhen defining a disaster recovery plan, the company must have two main objectives: Recovery Point Objective (RPO) and Recovery Time Objective (RTO).\nThe RPO indicates the amount of data that could be lost in the event of a disaster. This parameter depends, therefore, on the frequency with which backups are performed. The data lost will be the data that has been modified or created between the time of the last backup and the time of the disaster.\nThe RTO is the amount of time it takes to restore the data and infrastructure to become operational again. In other words, it is the amount of downtime that can be tolerated. This parameter will therefore depend on how quickly the recovery takes place, which is usually performed by a systems engineer.\nTo limit the impact on your business as much as possible, it goes without saying that these two parameters should be kept to a minimum. This is equivalent to losing less data and being able to restore systems in the shortest possible time.\nThe crucial role of backups\nData is maintained by ensuring that the data is redundant, i.e., by creating multiple copies of the same data so that it is not lost.\nIt goes without saying that a data retention strategy that is done through backups is inseparable from a disaster recovery strategy. It is important, however, at the same time not to confuse these two concepts with each other.\nThe goal of backups is to ensure that the data is recovered as fully as possible. On the other hand, however, disaster recovery refers to a broader strategy that includes not only the recovery of data, but of the entire infrastructure.\nThe OVH case\nThe fire at the Strasbourg servers of OVHcloud triggers several remarks about data storage methods.\nLet’s imagine that the backup is stored on the same server as the main site: in this case a damage to the server jeopardizes the original data and the copy itself making the backup useless.\nFor this reason the best strategy is to use different servers to store the backup copies. If, however, the servers used are within a single datacenter there is another problem.\nSo let’s take a look at the OVH case: a fire broke out in one of the data centers and spread to neighboring data centers. An event of this magnitude involving multiple facilities in the vicinity results in irreparable data loss.\nWhat is the solution to such an event?\nUse datacenters located in different locations. An option that many providers make available with separate disaster recovery plans. These are options that OVH also made available, but many users, undervalue the importance of these procedures, therefore they had not activated them.\nHow we protect your data\nSupportHost’s practice is to use external servers in datacenters located in a different country than the one where the servers on which the sites hosted are located. Our datacenter is in Germany, but backups are stored in separate datacenters in the Netherlands. Not just in a different datacenter, we use a different country.\nIn this way we secure our users’ data, being able to guarantee the safety of the data due to relocation. If there is a failure of the servers where your site is hosted you don’t risk losing your backups as well, but the loss of data is reduced to the time elapsed since the last backup.\nTo minimize the inconvenience we offer in our hosting services a daily backup (every 24 hours) which is already included in the price of our WordPress hosting, shared hosting, dedicated servers , VPS cloud hosting and reseller hosting packages. The backups are kept for 30 days and you are free to restore them independently. You only need a few clicks from the control panel (cPanel) to manage the restore.\nDNS geolocalized cluster\nIn addition to this we use a geolocated DNS cluster and use 4 different domains. Let me explain.\nWe have 4 knots in 4 separate datacenters, and they are different from the datacenters we maintain the servers and backup servers on. Each of these knots represents one of the 4 nameservers.\nTypically hosting companies use:\nAnd these are made to point directly to the server where the customer’s site is hosted.\nWith our solution we have 4 different nameservers pointing to 4 different nodes, using 4 different domains:\nns.supporhost.com ns.supporhost.net ns.supporhost.eu ns.supporhost.us\nThis solution allows us to eliminate the risk due to a problem with a domain. For example, if the .com registry suspends the domain, all of our customers’ sites remain online.\nIf a datacenter where one of these 4 knots resides has problems, or if one of these knots has problems, the other 3 continue to keep all of our customers’ sites reachable (1 is enough to have the customers’ sites online).\nIf the server where the customer’s site is located has a problem (e.g. the datacenter catches fire or simply a disk on the server breaks) we can fix it in a few minutes:\n- We restore the backup on a new server, taking it from the automatic backups that reside in a different datacenter.\n- We change the IP of that domain in the cluster, in this case we don’t have DNS propagation times, the pointing is updated instantly.\nUnfortunately, problems are always lurking around, and it’s always better to be prepared, after all, prevention is better than cure.\nIn this article, Disaster recovery: what it is and why it’s so important, we’ve seen what disaster recovery is and how being prepared for emergencies is critical for a business today.\nKnowing how to keep your data safe is essential as part of a broader strategy to restore it quickly. This is the only way to minimize downtime and, consequently, the financial losses that could be suffered.\nHave you ever had to deal with an unplanned event? Did you have a plan to follow and were you able to restore data and business quickly? Share your experience in the comments below.","- Reasonable pricing and unlimited storage plans\n- Continuous backup option\n- Useful web interface\n- Permanent free account\n- Supports two-factor authentication\n- Only Secure Folder can be encrypted with private key\n- Disjointed desktop interface\n- Nonintuitive restore options\n- Below-average upload speeds in our tests\nOpenDrive is a viable online backup option for those who want to save documents to the cloud, as well as sync and share files across multiple devices. The service offers flexible pricing plans, an intuitive web app, a truly continuous backup option, and support for two-factor authentication. On the other hand, OpenDrive's desktop interface is fragmented, its restoration options are not ideal, and the service didn't impress us in our upload speed tests. We would also like the ability to protect an entire backup with a private encryption key.\nHow Much Does OpenDrive Cost?\nOpenDrive's free account gets you 5GB of storage for one user with bandwidth limited to 1GB per day. This plan also limits the size of file uploads to 100MB and caps upload speeds at 200 Kbps. Thankfully, the free account requires just an email address and password—you don't have to give credit card information to get started. IDrive also offers a free and permanent 5GB account.\nThe Personal Unlimited plan ($99 per year) removes those upload restrictions and opens up unlimited storage for an unlimited number of devices. The personal plans also include an unlimited number of notes, up to 10 tasks for project management purposes, and external drive backup capabilities. To get unlimited tasks (as well as notes), you need to pay $299 per year for OpenDrive's Unlimited Business Plan.\nA Custom plan lets you choose your storage allowance, your bandwidth, and the number of users. Custom plans start at $50 per year (for the first user) for 500GB of online storage, while each additional user costs $1 per month. Keep in mind that the price jumps up quickly as you increase either the amount of storage or the bandwidth. For example, a custom plan with 1TB of storage and two users costs the same as the Personal Unlimited plan.\nFor comparison, IDrive charges $69.95 per year for its 2TB plan that supports an unlimited number of devices. Backblaze's unlimited storage plan is just $60 per year, but you can only use it to back up a single PC.\nOpenDrive has client software for Windows, macOS, iOS, and Android, as well as a plug-in for WordPress. It also offers WebDAV and an API for the developers out there. Linux users are confined to using the web interface. OpenDrive integrates with the Windows File Explore and macOS Finder.\nPrivacy and Security With OpenDrive\nIf security is your primary concern, you should only use OpenDrive's Secure Folder. The Secure Folder requires you to set up a private encryption key and uses encryption technology that conforms with the AES-256 standard to protect files. If you forget your private key, however, OpenDrive has no way to help you regain access. Files in this encrypted folder are not available for sharing and can only be accessed by the owner via the local desktop application. Acronis True Image, Backblaze, Carbonite, IDrive, and SpiderOak One allow you to protect the entire backup set with the private key, not just a specific folder. Both files in the Secure Folder and regular folders are uploaded to OpenDrive's servers using the HTTPS protocol. A representative from OpenDrive also noted that the company uses self-encrypting Seagate disks for its servers.\nOpenDrive now allows you to set up two-factor authentication for web access, a change we appreciate. Currently, it supports SMS- and Authy-based methods. Both methods worked fine in testing. IDrive and Livedrive also support two-factor authentication.\n(Editors' Note: Livedrive is owned by J2 Global, the parent company of PCMag's publisher, Ziff Davis.)\nOpenDrive's desktop client installed on our test PC quickly and we had no issues logging in with our test account. The installation process places an OpenDrive icon on your desktop (for the virtual drive view) and one in your notification area (for the Sync and Backup Manager).\nOpenDrive's interface consists of several pieces. The virtual drive view is integrated within the File Explorer (as a separate drive) and shows everything in your backup, along with the previously mentioned Secure Folder. You can also launch the Sync and Backup manager application, go directly to the backup setup (part of the Sync and Backup manager), access your account settings via the web, or create a new folder in your backup within the File Explorer. Right-clicking on a file in the OneDrive's File Explorer interface brings up a helpful Properties menu with access to sharing links and previous file versions.\nThe Sync and Backup Manager window displays a detailed list of all the configured backup tasks, a progress bar for the current task, and a prominent Pause/Resume button in the upper-right corner. The design of the app feels outdated—we prefer the more fully featured and cohesive desktop experiences offered by Acronis True Image and SpiderOak One. In testing, OpenDrive's apps were mostly stable, though browsing through uploaded files in the OpenDrive folder was slow.\nBacking Up Your Data\nTo back up data with OpenDrive, you either upload items via the web interface, drag and drop files into the OpenDrive drive via File Explorer, or create a new task using the Sync and Backup Manager window. To get started with that last option, hit the New Task button on the left-hand side of the screen and select one of the four options: Backup, Synchronization, Move, and One-Way Mirror.\nBackup acts as you might expect. OpenDrive stores the original files online, and when you make any edits locally, it uploads the file again with the changes. The Synchronization option hosts files in a shared folder and mirrors any file changes across devices; this is similar to how Dropbox works. The Move task copies selected files to a specified location and deletes those same files from the source location (this is essentially like an archive function). The One-Way Mirror task works the same as Backup, with one notable difference: Files are deleted in the destination location if the same files are deleted from the source. You have to select one of these choices every time you create a new backup task.\nWith OpenDrive, you need to select either the main OpenDrive folder or a subfolder each time you set up a backup. Notably, local backup targets are not an option, which is vital for when you don't have an internet connection or when a server goes offline. We'd also like the ability to select more than one folder to back up at a time. OpenDrive does not offer disk imaging, but you can select entire drives as the backup source.\nThe next step is to choose an upload schedule. The Continuously option has two settings: Very Often (your data will be backed up every 30 seconds) and Immediately (any changes to files are immediately synced and a full sync will commence every hour, by default). In between those two choices are the more obvious Hourly and Daily options. You can set start and stop times for each day of the week, as well as configure the backup task to run whenever the computer has been idle for a specified time. If you select the Manual option, files are backed up only on demand. Carbonite, IDrive, and SpiderOak One also watch folders for changes and upload them immediately.\nOn the next page, you can filter by the type of content with either the Exclude or Include options. In both sections, you can add a custom filter for different file extensions. There's also the option to control backups based on creation date and the size of files. The last step is to enable email notifications for when tasks either complete successfully, complete with errors, fail, or are manually stopped.\nOpenDrive's web interface is functional and well-designed. Immediately after signing in, the browser shows a page with a big drag-and-drop target area and a folder tree along the side. There's a persistent sidebar on the left for switching between files, notes, tasks, and users, with a Settings shortcut on the bottom. A menu with a standard array of upload, download, and file management options lives on the top right, though this changes contextually based on the current window. We really like that clicking on a file opens it in a new tab, since this saves you the trouble of having to navigate back up or down the file tree. OpenDrive can display documents, PDFs, and images, but it would not play audio or video files we tried.\nRight-clicking on a file brings up a context menu with the option to share, edit, or view its properties (you can set a password or change viewing permissions here). You can also create and edit documents, spreadsheets, and presentations thanks to a Zoho Office integration from the web interface.\nTo share a file with OpenDrive, right-click on it and select Links, Expiring Links, or Send by Email. You can also password-protect a file by adding a password in its Properties menu. SpiderOak One offers more flexibility, but uses a less intuitive system.\nRounding out OpenDrive's web features are the Notes, Tasks, and Users sections. Notes and tasks are self-explanatory and are fine for basic needs. The Users tab is where you can create accounts for other team members or people you want to have access to certain folders. These are all useful collaboration tools, but you should likely just spring for dedicated project management software or note-taking apps, instead.\nOpenDrive's Settings section enables you to view at-a-glance information about your remaining space and download bandwidth used, as well as customizable charts showing the same metrics. This is also where you can enable OpenDrive's two-factor verification option. Oddly, you control the number of file versions OpenDrive keeps in the profile section as well. Most other backup services make this option more prominent. Finally, the profile section includes a well-organized activity log.\nTo cancel an OpenDrive account, you can either log into the support area and submit a cancellation request or email the support team. For a cancellation request to take effect, you must submit it more than 24 hours before renewal to avoid being charged for the next payment period.\nOpenDrive doesn't feature a clear restore option, the way other services do. The most intuitive way (in our opinion) to get data back is by downloading files from the web or by dragging and dropping them out of OpenDrive's File Explorer or Finder folder.\nOpenDrive did add a Start Reverse Backup option (right-click on a backup task in the Sync and Backup Manager app), which just switches the source and destination of the task. The problem with this is that one might presumably want to restore files to a new location. Another problem with restoring files to the same folder is that OpenDrive overwrites the local file with any latest versions; if you make changes to a word document online, for example, and then run the Start Reverse Backup option, you lose the original local version of the file. You can always grab the older version online, but this is inconvenient. A company representative noted that point-in-time recovery options would likely arrive to OpenDrive this year.\nWe would still prefer a more traditional restore option; one in which you could specify a one-time file transfer from OpenDrive to a folder of your choosing. None of the available tasks in the desktop app are perfectly suited for this. Acronis True Image and SpiderOak One are among those services that offer more straightforward restore options.\nAs noted, file versioning preferences are accessible from the Settings section of the web interface. The maximum number of versions you configure for each file is 99. Once the number of file versions exceeds what you've chosen, OpenDrive deletes any older versions for good. We confirmed that OpenDrive retained all versions of a backed-up text file we updated several times. For comparison, IDrive keeps the last 30 versions forever and SpiderOak One keeps an unlimited number of versions forever.\nOpenDrive's Mobile Apps\nWe tested OpenDrive's Android app (it also has an iPhone app) and had no issues logging in to our test account. In testing, we encountered the occasional stutter when opening images from our backup. We mostly like the design of OpenDrive's app and it has improved the navigation experience. However, the Notes feature does not make an appearance and the Tasks icon opens a link to the web interface, which is highly inconvenient.\nYou can create new folders or add files from your device via the plus button in the lower-right corner. We like this functionality, though IDrive and Acronis True Image offer additional upload options for backing up contacts and calendars as well. For most people, OpenDrive's mobile backup option should be fine though, even though we'd like to be able to schedule automatic backups of files on mobile devices.\nThe Settings section is more of an information panel. Here you can view the amount of storage you are using as well as your subscription plan. There's also a Delete Offline Files option, which, per a company representative, deletes locally cached versions of files you downloaded from OpenDrive.\nHow Fast Does OpenDrive Upload Files?\nTo evaluate performance, we timed how long it took OpenDrive to upload three 1GB file sets and then took the median of the results. We tested the online backup services this year over a home Ethernet connection (16Mbps upload), since we were not able to access PCMag's corporate test network due to the ongoing COVID-19 pandemic—yes, we're working from home, too. Our test device was a Dell Inspiron tower running Windows 10 with a 256GB SSD and 32GB RAM.\nOpenDrive took a median time of 17:18 (minutes:seconds) to complete our test, a time that was below-average. IDrive (12:29) and ElephantDrive (12:44) were several minutes quicker. NovaBackup (22:14) finished last in our test. Speedy upload times make your first and any future backups much more convenient, but you shouldn't base your decision for an online backup service solely on this test if the service falters in other aspects.\nUnlimited Storage, Limited Features\nIf flexible storage options and a slick web interface are important to you, OpenDrive might be a worthwhile online backup choice. However, OpenDrive's disjointed desktop experience and slower-than-average upload speeds are potential reasons to avoid it, as is its limited backup encryption options. We recommend Editors' Choice services IDrive, for its value and speed, or Acronis True Image, for its impressive set of backup and security features."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8db6c7b7-77cf-4536-a0c3-ca32447a373f>","<urn:uuid:22bcafd5-8e06-49a1-b8e8-80b03a5eb2e3>"],"error":null}
{"question":"How do GOES-16 and Sentinel-5P differ in their approach to monitoring Earth's atmosphere?","answer":"While both satellites monitor Earth's atmosphere, they have different capabilities and approaches. Sentinel-5P carries the Tropomi instrument that specifically maps trace gases such as nitrogen dioxide, ozone, formaldehyde, sulphur dioxide, methane, carbon monoxide and aerosols. GOES-16, on the other hand, uses its Advanced Baseline Imager (ABI) to capture multispectral imagery covering 16 spectral channels, delivering full-color imagery plus specialized products such as atmospheric water content, ozone and aerosols. GOES-16 provides more frequent updates with full disk images every 15 minutes, while Sentinel-5P is designed for a seven-year mission focused on detailed atmospheric composition monitoring.","context":["Preparing to fly Sentinel-5P\nThe teams that will fly Sentinel-5P are training intensively for launch, ensuring that everyone knows their job and can react to any emergency.\nA ‘team of teams’ at ESA’s mission control centre has spent months preparing to assume control of Europe’s next Earth observation mission, and the final weeks before launch have been the most intense.\nSentinel-5P – the P refers to ‘precursor’ – is the first mission for Copernicus dedicated to monitoring our atmosphere.\nThe satellite carries the state-of-the-art Tropomi instrument that will map a multitude of trace gases such as nitrogen dioxide, ozone, formaldehyde, sulphur dioxide, methane, carbon monoxide and aerosols – all of which affect the air we breathe, and therefore our health, and our climate.\nExpansion of the Sentinel fleet in orbit highlights the expertise of teams at ESA and their capability to fly ‘constellation’ missions, as Sentinel-5P will fly in tight coordination with the US Suomi-NPP mission.\nThe challenging task of flying Sentinel-5P throughout its planned seven-year mission starts just 93 minutes after liftoff on 13 October, set for 09:27 GMT (11:27 CEST), on a Rockot from Russia’s Plesetsk Cosmodrome.\nThat’s when the satellite, already in space after separating from the rocket some 14.5 minutes earlier, will make its first call home, signalling via a ground station in Sweden to ESA’s main control room in Darmstadt, Germany.\n“It’s called ‘acquisition of signal’, and it’s the moment when the years of careful development and preparation for our mission control systems, and the months of training for our mission control teams, will prove their worth,” says flight operations director Pier Paolo Emanuelli.\nThat moment is one of the riskiest for the satellite: its rocket must have provided the right boost to put it into the planned orbit, and until its solar panels deploy to start generating power, it must survive on batteries, which will last only for a limited time.\n“Once we get the signal, and establish a commanding link with the satellite, we’ll begin a critical series of activities and procedures to verify the satellite’s health, ensure we have solar power and full communications, activate systems like the startracker cameras for navigation and ensure that 5P is fully functional after the incredibly vigorous ride into space.”\nThese initial activities continue around the clock for the first three days, after which the team will switch to daytime work, if all goes well, and move on to the next phase of the mission: commissioning the Tropomi sensor.\nTraining for all possibilities\nIf anything does go wrong, ESA will be well prepared. Since mid-July, the mission control teams – including the operations engineers, flight dynamics specialists, teams from ground stations, the science and project teams and representatives from European industry – have conducted 20 simulations out of the planned 26.\nEach runs for a full day, and employs sophisticated software to replicate the satellite and ground systems. Trainers can inject faults, errors and breakdowns into the simulation, testing the skill and knowledge of even the most experienced engineers and the teamwork and problem-solving abilities of everyone.\n“The human factor is the one that determines the success of the mission. There is no single responsibility, and it is great to see our teams working together,” says spacecraft operations manager Daniel Mesples.\nGetting ready to go to space\nBetween now and launch day, the final round of ‘sims’ will take place twice per week, culminating with a final dress rehearsal on 11 October, which, by tradition, simulates a completely normal launch sequence.\nThis will involve the teams in Darmstadt, ground stations in Sweden, the Antarctic, Canada and Norway, and the ESA and Russian teams at Plesetsk.\nDuring the rehearsal, the mission control systems will be connected to Sentinel-5P sitting on top of the rocket via a ground link, which will be removed only a few minutes before liftoff.\n“We will already have practised an extensive range of contingency situations, and experience shows it’s good for team morale to run though a fully normal launch sequence one final time just prior to liftoff,” says Daniel.\nOn 12 October, the flight operations director will certify to the launch authorities that the mission control teams are fully trained, that the ground systems and facilities are tested and ready, and that launch can proceed.\n“In September, the ESA centre celebrated its 50th anniversary in Darmstadt, and its rich history of 77 missions,” says Daniel.\n“It’s terrific that this month has seen many of us practising and training to do what it has excelled in for five decades – preparing to go to space.”","NOAA’s recently-launched GOES-16 weather satellite sent back its first images of Earth, providing a taste of what’s to come once the state-of-the-art satellite enters service later this year to deliver data at unprecedented resolution and revisit time to scientists, meteorologists and weather enthusiasts.\nRecently re-named GOES-16, the satellite launched on November 19 atop an Atlas V rocket that successfully boosted the 5,200-Kilogram craft into an elliptical transfer orbit around Earth. The satellite then used its own engine over a period of days to circularize its orbit and set up shop at 105 degrees West in Geostationary Orbit where it can view the American continent.\nThe images released on Monday are from the satellite’s Advanced Baseline Imager, the mission’s primary instrument capable of capturing multispectral imagery of the entire hemisphere visible to the satellite. ABI delivers a full disk image every 15 minutes and one of the continental U.S. every five minutes, plus a sub-frame image every 30 seconds covering areas of interest for monitoring of highly dynamic events such as hurricanes, severe weather, wildfires and other environmental phenomena.\nABI achieves four times the resolution of the current generation of GOES imagers and covers the Earth five times faster, unlocking opportunities for more precise weather forecasts & disaster monitoring as well as scientific studies. ABI covers 16 spectral channels to deliver full-color imagery plus specialized products such as atmospheric water content, ozone and aerosols as well as measurements equivalent to having an atmospheric sounder on the spacecraft.\n“This is such an exciting day for NOAA! One of our GOES-16 scientists compared this to seeing a newborn baby’s first pictures — it’s that exciting for us,” said NOAA Satellite and Information Service Director Stephen Volz. “These images come from the most sophisticated technology ever flown in space to predict severe weather on Earth. The fantastically rich images provide us with our first glimpse of the impact GOES-16 will have on developing life-saving forecasts.”\nAs part of its first-light campaign in mid-January, GOES 16 captured a full-disk image of the Western Hemisphere, a full 16-panel ABI image of the United States covering all of the instrument’s spectral bands, and images of dust blowing off the African coast, the shallow water of the Caribbean and clouds weaving their way past the high mountains of Argentina.\nThese images offer an impressive glimpse into the capabilities of the new satellite and potential benefits arising from the higher resolution and revisit time.\nGOES 16 carries a suite of six instruments that set out to revolutionize weather forecasting and climate science. Data delivered by the satellite’s instruments promises to safe lives on Earth by enhancing the ability to generate more accurate weather forecasts, better predict hurricane tracks and cut false tornado warnings in half.\nFor the past 40 years, the Geostationary Operational Environmental Satellites (GOES) built the backbone for weather forecasting in the United States, watching the Earth from a high-altitude perch 36,000 Kilometers above the equator. The inauguration of the GOES-R class satellites is considered a major leap likened to upgrading from black-and-white to high-definition color television in terms of data quality and usability.\nThe fourth-generation GOES project comes with a price tag of $11 billion covering the the GOES-R, S, T and U satellites as well as an all-new ground segment to tackle a virtual flood of data coming from the new satellites that has to be processed into near real-time products to enable the benefits expected from the new satellites.\nThe mission’s prime instrument, ABI, is responsible for 95% of total data coming from the satellite, equivalent to over 200 HD movies every day. However, the remaining five instruments are also a major contribution to the enhanced capabilities of the new space segment.\nGOES 16 is the first mission to carry the Geostationary Lightning Mapper – a new instrument tasked with detecting lightning at all times of day across the entire Western hemisphere – providing real-time insight into the formation of storms. Getting real-time lightning maps to forecasters will significantly enhance warning reliability and lead time for tornado outbreaks and other severe weather.\nGOES-16 also introduces vastly improved solar instruments that watch over the sun’s ultraviolet and X-ray emissions to monitor coronal mass ejections and solar flares to predict space weather phenomena that can impact satellite communications, navigation and power grids on Earth.\nAlso installed on the satellite are in-situ sensors to measure the particle and electromagnetic environment in Geostationary Orbit and provide the first measurement of an incoming space weather event.\nGOES 16 is set for a full year of commissioning and testing to achieve the stringent requirements of the mission that call for an unprecedented downtime of only 120 minutes per year and data availability within the shortest time after acquisition.\nIn May, NOAA will announce the planned operational location for GOES-16, though the smart money will be on GOES-16 becoming GOES-East – watching over much of the United States and keeping close watch on hurricane development in the Atlantic Ocean. GOES-S, currently undergoing pre-launch testing at Lockheed Martin, will launch in the Spring of 2018 to enter the other operational position."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:42fe6e7a-4add-4cd2-a4b2-e27702888af9>","<urn:uuid:94db2857-f533-4d63-835a-af61aadd0a0f>"],"error":null}
{"question":"Can both Visual Studio Graphics Diagnostics and Arm Mobile Studio analyze shader performance, and if so, what specific insights do they provide?","answer":"Yes, both tools can analyze shader performance. Arm Mobile Studio's Mali Offline Compiler enables compiling shader programs and analyzing their performance on Mali GPUs, providing approximate cycle cost breakdowns for major functional units on target devices. Visual Studio Graphics Diagnostics allows analysis of shader code through its Graphics Pipeline Stages window, where users can visualize results of each pipeline stage and debug shaders directly. It also offers shader performance insights through the Frame Analysis tool, which can identify hardware-level performance issues like texture fetch stalls and cache misses on supported hardware.","context":["Arm Mobile Studio\nThe Arm Mobile Studio family of performance analysis tools help you evaluate performance on non-rooted Android devices at varying levels of detail throughout your game development workflow:\n- Streamline for deep-dive performance profiling\n- Performance Advisor for quick analytics\n- Graphics Analyzer to debug graphics API calls\n- Mali Offline Compiler to analyze shader programs\nStreamline captures performance data from an Android device as your game runs. Explore the data in charts to see exactly how the device's CPU and GPU resources were used by your game.\nFor CPU bottlenecks, use the native code profiling functionality to locate specific problem areas in your application code. Investigate how processes, threads, and functions behave, from high-level views, right down to line-by-line source code analysis.\nFor GPU bottlenecks, see performance data from the Mali™ GPU driver and hardware performance counters to explore the rendering workload efficiency and quickly identify where to optimize.\nSee the average FPS over the duration of the capture, a breakdown of workload, and the average utilization of the CPU and GPU in the device. These quick metrics are useful to monitor over daily runs, to see how changes to your application affect performance during development.\nA more detailed FPS analysis chart shows how the application performed over time. Where the application is performing well, the background colour of the chart is green. In poorly performing sections, the background colour indicates what’s happening. In the example below, most of the chart below is blue, indicating that the GPU in the device is struggling to process fragment workloads.\nFurther charts provide information about the workload, the properties of your content, and how the functional units within the GPU are utilized. Each chart also shows the FPS, so that you can look for areas of correlation that might indicate a problem.\nPerformance Advisor indicates when it has located a potential problem, and links through to optimization advice on the Arm Developer website, about how to resolve the issue and improve performance.\nGraphics Analyzer enables you to evaluate all the OpenGL ES or Vulkan API calls your application makes, as it runs on an Android device. You can explore the scenes in your game frame-by-frame, draw call-by-draw call, to identify rendering defects, or opportunities to optimize performance.\nStep through the graphics API calls alongside the object geometry and framebuffer output, to evaluate how each draw call impacts the scene. See where different shaders are used in the scene, how many fragments were drawn by each shader, and how many GPU cycles were spent by each shader.\nMali Offline Compiler\nMali Offline Compiler is a command-line tool that enables you to compile shader programs, and analyze how they would perform on a Mali GPU. See an approximate cycle cost breakdown for the major functional units on your target device, to identify ways to optimize your shader programs. Learn how to accelerate your shaders with Mali Offline Compiler.\nGet Arm Mobile Studio\nFor more information about Arm Mobile Studio, visit the Arm Developer Website where you can download it for free. If you are interested in professional edition, which enables Arm Mobile Studio to be used for automatic analysis as part of a continuous integration workflow, please contact Arm.","Overview of Visual Studio Graphics Diagnostics\nVisual Studio Graphics Diagnostics is a set of tools for recording and then analyzing rendering and performance problems in Direct3D apps. Graphics Diagnostics can be used on apps that are running locally on your Windows PC, in a Windows device emulator, or on a remote PC or device.\nDebugging rendering problems in a graphically-rich app is not as straight-forward as starting a debugger and stepping through some code. In each frame, hundreds of thousands of unique pixels are produced, each according to a complex set of state, data, parameters, and code—of these, perhaps only a few pixels will exhibit the problem you are trying to diagnose. To complicate matters even further, the code that generates each pixel is executed on specialized hardware that processes hundreds of pixels in parallel. Traditional debugging tools and techniques—which are difficult to leverage in even lightly-threaded code—are ineffective when faced with so much data.\nThe Graphics Diagnostics tools in Visual Studio are designed to help you locate rendering problems by starting with the visual artifacts that indicate the problem and then tracing back to the source of the problem by focusing only on relevant shader code, pipeline stages, draw calls, resources, and device state—in the app's own source code.\nGraphics Diagnostics supports apps that use Direct3D 12, Direct3D 11, and Direct3D 10, and provides limited support for apps that use Direct2D. It does not support apps that use earlier versions of Direct3D, DirectDraw, or other graphics APIs.\nWindows 10 introduces the next version of Direct3D, Direct3D 12, which is substantially different from Direct3D 10 and Direct3D 11. These differences bring DirectX back into alignment with modern graphics hardware and unleashing its full potential, but they also bring big API changes and place greater responsibility on the programmer to manage resource lifetimes and contention. Having great debugging tools is crucial to help graphics programmers make this transition, so Graphics Diagnostics in Visual Studio 2015 supports Direct3D12 right from the start. Despite the differences, Graphics Diagnostics with Direct3D 12 maintains feature-parity with Graphics Diagnostics with Direct3D 11.2, with the current exception of the Frame Analysis feature. Soon, support for Frame Analysis in Direct3D 12 will be added, followed by new diagnostics tools to help you solve new kinds of bugs you might encounter in Direct3D 12.\nWindows 10 also maintains support for previous versions of Direct3D and the games and applications that rely on them. Graphics Diagnostics in Visual Studio 2015 continues to support Direct3D 10 and Direct3D 11 on Windows 10, as well as on Windows 8.1.\nIn Windows 8.1, DirectX 11.2 introduces new features that include support for capturing graphics information through its runtime. Windows 8.1 uses the new runtime-based capture—known as robust capture—exclusively for all versions of DirectX that Windows 8.1 supports. Robust capture also supports new features of Direct3D 11.2.\nBecause Direct2D is a user-mode API that’s built on top of Direct3D, you can use Graphics Diagnostics to help debug rendering problems in apps that use Direct2D. However, because only the underlying Direct3D events are recorded instead of the higher-level Direct2D events, Direct2D events won't appear in the Graphics Event List. Also, because the relationship between Direct2D events and the resulting Direct3D events are not always clear, using Graphics Diagnostics to debug rendering problems in apps that use Direct2D is not straight forward. Still, you can use Graphics Diagnostics to get information about low-level rendering problems in apps that use Direct2D.\nGraphics Diagnostics has a dedicated interface—the Graphics Analyzer window—for diagnosing rendering problems, but it also adds some tools to the Visual Studio interface.\nWhen you run your app under Graphics Diagnostics, Visual Studio displays a diagnostics session interface that you can use to capture frames and which also displays the current CPU and GPU load. The CPU and GPU load is displayed to help you identify frames you might want to capture due to their performance characteristics, rather than rendering errors.\nThis isn't the only way to capture frames though. You can also capture frames by using the programmatic capture interface, or by using the command-line capture program, dxcap.exe.\nSee Capturing Graphics Information for more information.\nGraphics Diagnostics can also profile the performance of your Direct3D app. Because profiling data would be skewed by recording details of graphics events, this is separate from capturing frames to be used examined with the Graphics Analyzer.\nSee GPU Usage for more information.\nThe DirectX control panel is a component of DirectX that you can use to change the way that DirectX behaves—for example, you can enable the debug version of the DirectX runtime components, select the kind of debug messages that are reported, and disallow certain graphics hardware capabilities from being used to emulate less-capable hardware. This level of control over DirectX can help you debug and test your DirectX app. You can access the DirectX control panel from Visual Studio.\nTo open the DirectX control panel\nOn the menu bar, choose Debug, Graphics, DirectX Control Panel.\nThe Visual Studio Graphics Analyzer is a dedicated interface for examining rendering and performance problems in frames you've already captured. Inside Graphics Analyzer, you'll find several tools to help you explore and understand the rendering behavior of your app. Each tool exposes a different kind of information about the frame that's being inspected, and the tools are designed to be used in concert to intuitively narrow-in on the source of a rendering problem, starting from the its appearance in the framebuffer.\nThis illustration shows a typical layout of tools in the Graphics Analyzer.\nInside the Graphics Analyzer, the graphics log document is the most prominent tool window. This window represents all of the frames you captured by running your app under Graphics Diagnostics. From here, you can select a different frame to examine or select a specific pixel that you want to examine with the Pixel History tool. The framebuffer image displayed in this document changes to reflect the currently selected Event so that you can see how the framebuffer is affected over time.\nThe Graphics Log Document is also the entry point to the Frame Analysis tool, which helps you understand the performance of a frame by changing the way certain aspects of rendering are configured and providing benchmark results to compare with the original.\nGraphic events mark each Direct3D API call and user-defined event.\nThe Graphics Event List shows all of the graphics events that were recorded during the frame you're examining. To help you find what's important, the event list can be viewed hierarchically—with recent state-changes underneath the subsequent draw call—or as a timeline. Additionally, events are color-coded based on the queue they belong to and you can filter the list to only include the events you're interested in.\nWhen you select an event in the list, the rest of the Graphics Analysis tools reflect the state of the frame at the time of that event. In this way, you can see the effect of any event on the GPU. For example, you can see the immediate effect of any draw call on the framebuffer, even if it becomes obscured by subsequent draw calls. Some events also have hyperlinks you can follow to see more details about its parameters or related resource objects.\nEach draw call in your app goes through the graphics pipeline provided by Direct3D. At each stage in the pipeline, output from the previous stage is transformed by a small program, called a shader, and then passed to the next stage until it is finally rendered to the screen. Many rendering errors occur at the boundary between pipeline stages when the output format is different than the next stage expects, or when any one stage simply produces incorrect results. Normally, you only get the final results as you would see on your screen and you can't easily tell where in the pipeline the error occurred.\nThe Graphics Pipeline Stages window visualizes the result of each stage independently, so that you can more easily determine which stage a rendering problem first appears in. Once you've determined which stage that is, you can start debugging its shader right from the Pipeline Stages window.\nRendering operations depend on a lot of state that's typically spread across multiple objects. Many kinds of rendering problems are caused by misconfigured state.\nThe Graphics State window collects the state information relevant to each draw call together in one place so that it's easier to find, and also highlights the state changes that have occurred since the previous draw call.\nIn complex scenes, it's not uncommon for a pixel to be shaded multiple times in a single frame. Sometimes the earlier color is just overwritten, but other times the colors are combined together to achieve effects such as transparency. When the result of combining them together doesn't have the right appearance you can't easily tell if it's because one of the colors was incorrect, or if there's a problem with the way they were combined. Other times, an object might appear to be missing because its contribution to the pixel was rejected for some reason.\nThe Graphics Pixel History window visualizes the complete shader history of every pixel in the frame, including rejected contributions. For contributions that weren't rejected, it displays the raw shader results and how each new color was combined with the previous one. With this information, it's much easier to locate the source of errors in pixels that blend shader results, or when a rendered object is missing because its contribution to the pixel was incorrectly rejected.\nShader code isn't the only source of rendering problems in a Direct3D app, sometimes your app's source code passes the wrong parameter or doesn't configure Direct3D quite right. One kind of error that the previously discussed feature, Pixel History, can help you find is when a rendered object is missing because all its pixels have been rejected. This kind of error usually happens when you misconfigured a setting, such as the one that controls how the depth test is performed, and you can usually find your mistake somewhere in the call stack of the missing object's draw call.\nThe Graphics Event Call Stack window displays the complete call stack of every graphics event in the Event List, and even lets you jump to your app's source code if debugging information is available. This is a powerful tool for following an error from where it first appears, on the GPU, back to where it originated in your app's source code.\nEvery frame your app renders is probably backed by hundreds or even thousands of resource objects. These can include back buffers and render targets, textures, vertex buffers, index buffers, general buffers—almost everything Direct3D remembers is an object.\nThe Graphics Object Table displays all the objects that exist at the time of the graphics event selected in the event list. Since most objects in a typical app are textures, the event list is optimized to show details relevant to images at a glance. The Type column tells you what kind of object is in each row, and the Format column further shows the sub-type or version of the object. Other details are also shown. Some objects also have hyperlinks you can follow to view the object with a more specialized viewer, such as textures (you can view the texture as an image), or buffers (you can choose how the buffer viewer parses and displays the raw buffer bytes by defining the buffer format).\nYour app's graphics don't just need to be correct – they need to be fast, too. Even on less-powerful devices like laptops with integrated graphics or mobile phones. And they still need to look great too.\nThe Graphics Frame Analysis tool explores potential performance optimizations by automatically changing the way the app uses Direct3D and providing benchmark results for comparison. For example, Frame Analysis can enable mip-mapping on every texture, which might reveal textures that could benefit from mip-mapping but don't currently have it enabled. On hardware that supports it, Frame Analysis also presents information collected from the GPU's performance counters—this level of information can identify hardware-level performance issues, such as high numbers of texture fetch stalls or cache misses.\nBut Frame Analysis isn't just about going fast – it's about gaining the most performance you can while giving up the least amount of visual quality. Sometimes an expensive effect that looks great on a large display doesn't make the same impact when viewed on the small screen of a phone, where a simpler effect might look just as good without draining the battery. The automatic changes and benchmarks that Graphics Analysis provides can help you find the balance that’s right for your app across a range of devices."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:9717d90c-1dc0-44af-a53c-fd27d2160517>","<urn:uuid:f09142e1-4d16-4655-bf14-fbb25cd74298>"],"error":null}
{"question":"How do the readings from a tonometer compare to normal intraocular pressure in glaucoma diagnosis?","answer":"A tonometer measures intraocular pressure (IOP), with normal IOP being between 12-20 mmHg. In glaucoma diagnosis, readings above 21 mmHg are considered elevated and indicate raised IOP, which is one of the key characteristics of glaucoma along with optic nerve damage and visual field loss. The pressure measurement helps assess if a patient has glaucoma, which affects 1 in 40 people over age 40 in white populations and has 4 times higher prevalence in African-Americans and African-Caribbeans.","context":["Have you ever wondered the names of the eye examination equipment your optometrist or ophthalmologist normally use on you?\nDon’t worry we are devoted to bringing you everything you ought to know about your health and well-being.\nIntroduction to the eye examination equipment\nAmong the things to be placed on high priority, are the eyes. But unfortunately, most people ignore this truth.\nThe eyes are very delicate and must e cared for at all costs. Going for an eye check-up is highly recommended for all and being familiar with the instruments used in the examination of your sight, makes the work easier.\nThe types of equipment used for the duration of an eye exam would possibly appear intimidating to a regular man or woman, and for that reason, many avoid or consistently put off their everyday checkups with a specialist.\nHowever, most of these gadgets are painless, quick, and extremely correct in diagnosing the eye’s fitness reputation.\nList of eye examination equipment\nBelow is the top list of devices, instruments, and machines used for effective and efficient examinations of the eyes;\nAn autorefractor measures the correction required to peer genuinely. The patient sits on the machine, which has a head and chins rest and appears at a picture that is going in and out of attention.\nThe gadget shines a mild into the attention and measures how the light changes because it bounces off the back of the eye. These readings are mixed and converted right into a spectacle prescription.\nSome autorefractors can measure the curvature of the cornea too which is useful for becoming contact lenses.\nAn autorefractor will never update an optometrist, but it does offer a correct start line which might in flip, make the attention exam a bit quicker.\nThe goal nature of this take a look at approach it’s excellent for those sufferers who, for example, cannot communicate or for whom the subjective a part of the eye test, (telling the optometrist that is higher or worse), is probably a venture. An optical assistant or a doling out optician, (DO) is probable to carry out autorefraction.\nThe automated refraction approach is short, simple, and painless.\nSeveral readings are taken which the device averages to shape a prescription. No comments are needed from the patient for the duration of this system.\nA focimeter is a small laptop instrument that looks as if a microscope. It is probably automated. It measures the strength of the lenses in a couple of spectacles.\nSometimes, focimetry is a part of pre-screening.\nA fundus camera takes photos of the return of the eye. Again, it might be a part of a pre-take a look at, or it is probably something the optometrist does all through the attention examination.\nThe affected person sits with their forehead and chin towards a rest even as the photograph is taken.\nFundus pictures are very beneficial for tracking the progression of sure eye conditions or sicknesses. They are also used to file charges in acknowledged eye situations, for instance, diabetic retinopathy, glaucoma, and age-associated macular degeneration (AMD).\nThe patient can see the ensuing images in their very own fundi, which is useful if the optometrist desires to give an explanation for something.\nA lensometer is a device used to measure the strength of an existing lens. An optician makes use of a lensometer to decide the prescription of an affected person’s eyeglasses.\nA guide keratometer is used to determine how flat or steep the cornea is. It is frequently used to diagnose situations that include astigmatism, keratoconus, corneal scarring, and corneal distortion. A keratometer is usually used to match touch lenses as properly.\nAn Ophthalmoscope is a handheld device used at some stage in a watch examination to take a look at the interior of the attention. It consists of a mild source with integrated mirrors and lenses this is especially effective for examining the retina.\nOften, this examination comes alongside pupil dilation. Dilating an affected person’s pupils permits the eye health practitioner to get a higher view of the structures in the back of it.\nThe affected person’s eyes have dilated the usage of eye drops to get a proper view of the interior of the eye.\nWhile pupil dilation leads to a greater complete exam, it’s now not essential in all cases and the indoors of the eye can nonetheless be inspected with an Ophthalmoscope without it.\nThe ophthalmoscope facilitates the medical doctor to peer and discovers the signs of viable eye diseases.\nA Direct Ophthalmoscope\nA direct ophthalmoscope is a hand-held tool used for examining the interior systems of the attention, especially the retina. It includes a replicate that displays light into the eye and a crucial hollow thru which the eye is examined.\nA Binocular Indirect Ophthalmoscope\nA binocular oblique ophthalmoscope (BIO) is worn on an eye-fixed health practitioner’s head with a view to having the use of each finger to observe the eyes.\nOptical Coherence Tomographer (Oct)\nOf all the optical systems to be had, the optical coherence tomographer, is at least, the maximum thrilling.\nIn this placing, the patient sits with their chin and forehead towards a rest throughout the scan. OCT makes use of mild waves to take pass-section pictures of the retina.\nThis allows the optometrist to peer each of the retina’s specific layers. The OCT software program produces an array of metrics and pix which the optometrist studies and saves in the affected person document to evaluate preceding and subsequent scans.\nIt is fast, painless, and non-invasive for the patient. We use the information to diagnose and to monitor eye situations. It is especially useful for seeing retinal detachments or tears and wet macular-degeneration because these can’t usually be visible in the course of different methods of examination.\nAn optomap makes use of scanning laser generation to supply is an extremely-widefield virtual picture of the retina.\nThe snapshots assist in both diagnosing eye conditions (glaucoma, macular degeneration) and those fashionable fitness situations that affect attention, (e.G.Diabetes, high blood pressure).\nGetting an optomap image is speedy, painless, and cushy. Nothing touches the attention at any time. The patient looks into the tool one eye at a time and could see a flash of mild while the picture is taken.\nPerimeter (Visual Field Screener)\nA perimeter tests the field of vision. When we examine something we see the item we are looking at and additionally what surrounds it.\nThe place we see, (without transferring the top or eye), is referred to as the field of regard. The visual field check gives a very good indication of the fitness of the eyes and the whole visible gadget.\nMost visual view screeners require the patient to sit all the way down to look into the machine. The take a look at is executed on each eye one by one, whilst the opposite eye is covered with an eye patch.\nDuring the check, the patient has to take a look at a gap inside the centre of a display screen and reply while the goal is seen. The goal is generally one or extra lights that flash on and rancid, someplace else on the display screen.\nThe patient would possibly click a button as soon as each goal is visible, or may additionally ask to reply verbally.\nThe field of vision check can be carried out before, at some point of or after the eye exam. It is particularly useful to help diagnose and monitor glaucoma.\nA phoropter (or phoroptor) is an instrument used at some stage in a watch exam to measure refractive mistakes and determine eyeglass prescriptions.\nThe patient is instructed to sit down in front of a watch chart far away. Different lenses are implemented earlier than the affected person tells the optometrist which lens allows him to peer higher.\nThis is a subjective test and the suitable prescription relies upon the remarks of the affected person who determines which lens affords the fine vision.\nA retinal digital camera is a tool that is used to seize a photo of the indoors surface of the attention, normally the retina. It is a specialized low-powered microscope with a digital camera connected to it.\nThis allows the ophthalmologist to observe the patient’s retina, monitor the modifications inside the retina over a time period, and diagnose sicknesses.\nModern retinal cameras take a number of pictures the use of one of a kind filters to get clean pictures of various elements of the retina.\nA retinal digital camera is used to picture the again of the eye, consisting of the retina. It is used to record eye diseases. The camera produces a vibrant flash whilst an image is taken.\nIn cases where the patient is unable to present remarks to the optometrist because of positive bodily or mental situations, a retinoscope is used to measure the refractive mistakes and to determine the best prescription of eyeglasses or lenses.\nIt includes a mirror that shines light into the eyes and has a hollow in the center of it, thru which the eyes are tested.\nA retinoscope is in particular useful in prescribing corrective lenses for sufferers who are unable to provide oral comments to the attention medical doctor.\nIt is likewise useful for figuring out how well the eyes paintings collectively (accommodate) to peer actually.\nA slit lamp is a microscope with a mild attached that lets in the medical doctor to closely examine the attention. This tool is used to view the structures of the attention consisting of the cornea, iris, and lens.\nWith special lenses, it is viable to examine the lower back of the attention as properly. A slit-lamp allows the practitioner to have an exquisite view of the interior of your eyes.\nEye medical doctors use this tool to look for any abnormalities or situations that would reveal them within the anterior part of the attention. While it’s not always “a laugh” to have mild to your eyes, it’s far vital and painless.\nThe Snellen Chart is one of the maximum common gear used at some point of an eye-fixed examination.\nIt is a bodily or virtual chart that is placed around 20 toes or 6 meters far away from the affected person and has alphabets in decreasing order of size.\nAt a time, one eye of the patient is protected whilst the best prescription of the alternative eye is decided.\nPatients are required to examine the smallest letters they could whilst the optometrist applies distinct lenses in a metallic eyeglass frame installed on the face.\nThe Snellen Chart measures visible acuity or how well you could see items within the distance.\nThis tool is used to measure the strain of the fluids in the eyeball. If the eye pressure increases beyond a factor, then it could permanently damage the optic nerve.\nThis high-strain disease is called glaucoma. A tonometer that actually touches the attention requires numbing drops to be instilled into the eyes. Many medical doctors also use an air-puff tonometer that blows air onto the eyes to degree eye stress or test for glaucoma.\nThe Tonometer measures strain in the attention (officially referred to as Intraocular pressure). Measuring IOP is important to ensuring patient eyes have the right internal pressure to preserve their form and keep away from dangerous conditions.\nThere are several approaches to measuring, but the non-touch Tonometer has gained recognition in current years. The check is used to assist discover glaucoma.\nA tonometer measures the manufacturing of aqueous humor, the liquid observed inside the eye, and the rate at which it drains into the tissue surrounding the cornea.\nLike autorefraction and focimetry, non-touch tonometry might be part of pre-screening. The non-touch tonometers on the exercise I paintings in are inside the consulting rooms, so the optometrists take the readings.\nThere is no other form of tonometry, referred to as applanation tonometry. For this, the optometrist numbs the eye with anaesthetic drops and touches the front surface of the attention lightly with a probe.\nIt is incredibly not likely that every person aside from the optometrist would carry out applanation tonometry.\nVt 1 Vision Screener\nThe VT 1 Vision Screener is a high-overall performance automatic device utilized by optometrists to quickly diagnose and discover major visible issues.\nIt is lightweight, transportable, and easy to use. It displays for a broad variety of eye illnesses and does no longer requires dilation of the students.\nAn imaginative and prescient screener is used to discover visible problems for all age organizations together with kids as younger as six months of age.\nThese gear are stepping stones in the vision and fitness enterprise. They help to perceive, deal with, and prevent imaginative and prescient troubles.\nDoctors and healthcare experts are given get right of entry to this eyesight checking out equipment, which offers the potential to treat sufferers with the right care they deserve.","Glaucoma: The least the general practitioner should know\nA A Stulting,1\nMB ChB, MMed (Ophth), FCS(SA) (Ophth), FRCOphth\n(UK), FEACO (Hon), FCMSA (Hon), FACS, FICS;\nMB ChB, MMed (Ophth), PhD (HPE)\n1 Professor and Head, Department of Ophthalmology, University of the Free State, Bloemfontein, South Africa\nHead, Clinical Simulation Unit, School of\nMedicine, Faculty of Health Sciences, University of the Free\nState, Bloemfontein, South Africa\nto: A Stulting (firstname.lastname@example.org)\n• What is the definition of glaucoma?\n• What is the classification of glaucoma?\n• How is glaucoma diagnosed?\n• What is the management of glaucoma?\n• What is the role of the GP in the management of\nGlaucoma has been called the ‘silent thief of the night’\nbecause patients may lose their peripheral vision slowly over a\nperiod of time with no symptoms in many cases. In South Africa,\nthe prevalence of glaucoma in people older than 40 years of age\nis between 4.5% and 5.3%. In whites 1 in 40 people over the age\nof 40 years will develop glaucoma (2%). The prevalence in\nAfrican-Americans and African-Caribbeans over 40 years of age is\n4 times higher.1\nSeventy million people world-wide suffer from glaucoma.\nDefinition of glaucoma\nGlaucoma is defined as a progressive, bilateral neuropathy of the optic nerve. It is characterised by:\n• raised intra-ocular pressure (IOP) – (normal IOP = 12 - 20 mmHg)\n• optic nerve head damage (cupping of the optic disc)\n• visual field loss.\nGlaucoma can be classified as either congenital or acquired (Table 1).\nTable 1. Classification of glaucoma 2\n• Primary open-angle glaucoma\n• Secondary open-angle glaucoma\n• Pre-trabecular glaucoma\n• Trabecular glaucoma\n• Post-trabecular glaucoma\nAqueous humor is produced by the ciliary body of the eye and it flows through the trabecular meshwork to ensure that the eye is firm so that it can function optically. (Place your finger gently on your eye and then you will feel that the eye is not soft, nor hard, but firm.)\nWhen the trabecular meshwork is obstructed by pigment (e.g. after trauma), red blood cells (e.g. after blunt trauma causing hyphema), inflammatory cells (e.g. after uveitis) or lens material (phacolytic glaucoma), the aqueous humor cannot pass through the trabecular meshwork and the intra-ocular pressure increases.\nPre-trabecular aqueous obstruction may occur if a membrane develops that covers the trabecular meshwork.\nIn some cases the aqueous humor passes through the iris and sometimes through the uveo-scleral route.\nPost-trabecular glaucoma occurs when the trabecular meshwork itself is normal but aqueous outflow is impaired as a result of elevated episcleral venous pressure. This may occur in the following conditions:\n• carotid-cavernous fistulae and dural shunts\n• Sturge-Weber syndrome\n• obstruction of the superior vena cava.\nGrading angle width\nThe grading of angle width is an essential part of the assessment of any patient with glaucomatous or potentially glaucomatous eyes. The main aims are to evaluate the functional status of the angle, its degree of closure and the risk of future closure.3\nThe following angle structures are identified:\n• Schwalbe’s line is the most anterior structure, appearing as an opaque line. Anatomically it represents the peripheral termination of Descemet’s membrane and the anterior limit of the trabecular meshwork.\n• The trabecular meshwork extends from Schwalbe’s line to the scleral spur. Gonioscopically, it has a ground-glass appearance.\n• The scleral spur. Gonioscopically, it is\nsituated just posterior to the trabecular meshwork and appears\nas a narrow, dense, often shiny, whitish band. It is the site of\nattachment of the longitudinal muscle of the ciliary body.\nThe anterior chamber angle can be classified in the following grades:3\n• Grade 4 (35 - 45 degrees): the widest angle (characteristic of myopia and aphakia)in which the ciliary body can still be visualised with ease. It is incapable of closure. (Remember, grade 4 = 4 structures can be seen: Schwalbe’s line, trabecular meshwork, scleral spur, ciliary body.)\n• Grade 3 (25 - 35 degrees): the angle is such that at least the scleral spur can be identified. It is also incapable of closure. (Remember, grade 3 = 3 structures can be seen: Schwalbe’s line, trabecular meshwork, scleral spur.)\n• Grade 2 (20 degrees) is a moderately narrow angle. Angle closure is possible. (Remember, grade 2 = 2 structures can be seen: Schwalbe’s line and the trabecular meshwork.)\n• Grade 1 (10 degrees) is a very narrow angle in which only Schwalbe’s line can be identified. The risk of angle closure is high. (Remember, grade 1 = 1 structure can be seen, only Schwalbe’s line.)\n• Grade 0 (0 degrees) is a closed angle due\nto irido-corneal contact. (Remember, grade 0 = 0 structures\ncan be seen in the angle).\nPrimary angle-closure glaucoma is a condition in which\nelevation of IOP occurs as a result of obstruction of aqueous\noutflow by partial or complete closure of the angle by the\nperipheral iris. Secondary angle-closure glaucoma occurs when\nposterior forces push the peripheral iris against the trabecular\nmeshwork or when anterior forces pull the iris over the\ntrabecular meshwork (e.g. the contraction of inflammatory\nThe diagnosis of glaucoma\nIt is important to enquire about the following risk factors:\n• age: older than 40 years (remember: 1 out of every 40 patients above the age of 40 years will develop glaucoma)\n• race: black people have a higher risk of glaucoma.\n• a positive family history is also a risk factor\nfor developing glaucoma.\nRaised IOP (more than 21 mmHg)\nThe IOP is measured with a tonometer. The most commonly used tonometer is the Goldmann Applanation tonometer (Fig. 1a), which is attached to a slit-lamp. An area of 3.06 mm diameter is flattened by a prism after local anaesthetic has been instilled in the eye.\nThe Schiotz tonometer (Fig. 1b) uses the principle of indentation tonometry, in which the extent of corneal indentation by a plunger of known weight is measured.4 This is the recommended instrument in general practice.\nPneumotonometers, such as the Air-Puff tonometer are popular with optometrists as contact is not made with the subject’s eye and topical anaesthesia is not required. The central part of the cornea is flattened by a jet of air. The time required to sufficiently flatten the cornea relates directly to the level of IOP. Its main disadvantage is that it is accurate only within the low-to-middle range.\nThe ICare tonometer is a new, small hand-held instrument. A\nvery light probe makesmomentary contact with the cornea and,\nbecause only a very small force is applied to the cornea, a\ntopical anaesthetic is not required. It can even be used in\nbabies and small children (Fig. 1c).\nOptic nerve evaluation\nIt is important to know the appearance of the normal optic disc because then the abnormal glaucomatous disc will be more readily diagnosed.\nThe normal optic disc is characterised as follows (Fig. 2):\n• colour: orange-yellow\n• vessels: come out centrally out of the optic disc\n• cup:disc ratio: usually up to 0.4 (the C/D ratio indicates the diameter of the cup expressed as a fraction of the diameter of the disc)\n• edges: well-defined and clearly visible\n• shape : round or oval\n• size: myopes (larger), hypermetropes (smaller).\nThe glaucomatous optic disc is characterised as follow (Fig. 3):\n• colour: white\n• vessels: displaced towards the nasal side\n• cup:disc ratio: more than 0.4\n• edges: well-defined and clearly visible\n• disc haemorrhages\n• bayonetting sign (characterised by double angulation of a blood vessel over the edge of the disc): with neuroretinal rim loss between the outer edge of the cup and the optic disc margin, a vessel entering the disc from the retina may angle sharply backwards into the disc and then turn towards the original direction\n• neuroretinal rim: thinning and paleness\n• laminar dot sign (seen in the base of the optic disc when the neuroretinal rim recedes)\n• excavated disc (‘bean pot’).\nVisual field testing\nA visual field machine, for example, a Humphrey’s perimeter, is\nused. Various field defects can be documented, for example a\nnasal step of Rönne, a temporal nerve fibre defect, a Bjerrum\narcuate scotoma or a Seidel scotoma.\nThe angle of the anterior chamber can be assessed by performing\ngonioscopy. The examiner, usually the ophthalmologist, uses a\ngonio-mirror lens to grade the angle from 1 to 4 (see above).\nManagement of congenital glaucoma (Fig. 4) 5\n• Symptoms: photophobia, lacrimation, blepharospasm.\n• Signs: buphthalmos (calf’s eye), corneal diameter ˃12 mm, corneal clouding, white optic disc.\n• Treatment: surgery (goniotomy or trabeculotomy).\nManagement of open-angle glaucoma\nTreatment to lower the IOP includes\nmedical therapy, laser surgery or surgery.\n• Topical aqueous suppressants: beta-blockers, adrenergic agents, alpha-2 agonists, carbonic anhydrase inhibitors\n• Systemic aqueous suppressants: carbonic anhydrase inhibitors, e.g. Diamox\n• Outflow facilitators: topical miotics, prostaglandin agonists.\nAdministration of eye drops\n• The drops should be placed in the inferior fornix of the eye.\n• It may be helpful to use a mirror when pulling the eyelids apart.\n• Use the ‘no touch’ technique to maintain good hygiene.\n• One drop, correctly placed, is sufficient.\n• Keep the drops in the fridge: the patient will feel when the cold drop goes into the eye.\n• Keep the drops for an average period of 30 days to ensure maximun efficacy.\n• If instilling different drops, wait at least 5 minutes between placing the drops in the eye to prevent dilution of the drops.\n• Close the punctum so that the drops can stay in\nthe eye for a longer period.\n• argon laser trabeculoplasty (ALT)\n• selective laser trabeculoplasty (SLT)\n• diode laser trabeculoplasty (DLT).\nArgon laser trabeculoplasty (ALT) (Figs 5a-e)\nThe mechanism of ALT is such that the treated area of trabecular meshwork may shrink, causing stretching of adjacent areas, which leads to increased outflow facilities.\nDespite favourable results, laser therapy has not replaced medications as primary therapy in patients with primary open-angle glaucoma (POAG). This was partly due to loss of efficiency over time and the introduction of more effective glaucoma medications, namely prostaglandin analogues.\nThe role of laser trabeculoplasty is limited and it is used either as adjunctive therapy or as an intermediate step between failed medical therapy and surgical intervention.\nSelective laser trabeculoplasty (SLT) 9\nSLT is a relatively new procedure which uses a 532 nm Q-switched Nd:YAG laser. This selectively targets melanin pigment in the trabecular meshwork cells, without damaging the non-pigmented structures. It may be safer than ALT as there is no thermal tissue damage. The main advantage is that treatment can be repeated, the laser is portable and the initial results show that it is probably as effective as ALT. This technique may have a place in treating patients in the rural areas of South Africa where compliance or follow up may be a problem.\nDiode laser trabeculoplasty (DLT) 10\nDiode laser ablation lowers the IOP by destroying part of the secretory ciliary epithelium, thereby reducing aqueous secretion. More than one treatment is usually required for adequate pressure control.\nFig. 5e. The sketch on the left (A) represents the trabecular meshwork with the openings in the meshwork (drawn as squares), while the intended areas of treament are shown as X. The sketch on the right (B) shows that the openings in the trabecular meshwork enlarge at areas away from the fibrotic areas caused by the laser treatment. Aqueous humor can now flow more easily through the enlarged openings in the trabecular meshwork.\nSurgical treatment involves either trabeculectomy with or\nwithout mytomicin-C (MMC) (Figs 6 and 7) or valves (Molteno,\nClosed-angle glaucoma and its management\n• rapidly progressive impairment of vision\n• periocular pain and congestion\n• nausea and vomiting may occur in severe cases\n• transient blurring and haloes around lights.\n• circumcorneal redness (in contrast with conjunctivitis, where the redness is mainly in the fornices)\n• corneal oedema\n• increased IOP\n• the pupil is vertically oval and fixed in the\nsemi-dilated position and is unreactive to both light and\n• Initial medical therapy:\n• carbonic anhydrase inhibitors (Diamox 500 mg followed by oral acetazolamide 250 mg 4 times daily.\n• hyperosmotic agents (oral glycerine given by the ophthalmologist).\n• Peripheral Nd: YAG laser iridotomy to re-establish the communication between the anterior and posterior chambers of the eye by making an opening in the peripheral iris.\n• A prophylactic laser iridotomy must be performed on the fellow eye to prevent an acute attack in the future.\n• Sometimes a surgical peripheral iridectomy must be performed if the iridotomy is too small and not functioning.\n• Trabeculectomy is reserved for those patients\nwho fail to respond or where the angle is chronically closed.\nThe role of the general practitioner in glaucoma\n• Take a good history from the patient:\n• Do you have glaucoma?\n• Any family member with glaucoma?\n• Tell your patients that every person older than 40 years should test their IOP annually.\n• Make sure that your patients have regular measurements for IOP.\n• Make sure that your glaucoma patients go for regular visual field testing.\n• Do regular direct ophthalmoscopy on your patients to detect glaucomatous optic nerve changes.\n• Monitor the therapy and make sure that you know the side-effects of the drops your glaucoma patients are taking, namely:\n• beta blockers: postural hypotension, bronchospasm\n• Diamox: potassium depletion\n• topical allergy (Fig. 8).\n• Repeat prescribing glaucoma drops – to be reviewed 6-monthly.\n• Inform and reassure every glaucoma patient.\n• Refer patients to the ophthalmologist for re- assessment, e.g. side-effects of medication, post-glaucoma surgery or red eye.\n• Remember that glaucoma is a medical disease\nthat should be under the control of an ophthalmologist.\n1. Fraser S, Wormald R. Epidemiology of glaucoma. In: Yanoff M, Duker JS, eds. Ophthalmology, 2nd ed. St Louis: Mosby, 2009,1413.\n2. Kanski JJ, McAllister JA, Salmon JF. Glaucoma. A Colour Manual of Diagnosis and Treatment, 2nd ed. London: Butterworth-Heinemann, 1996:3-5.\n3. Kanski JJ, McAllister JA, Salmon JF. Glaucoma. A Colour Manual of Diagnosis and Treatment, 2nd ed. London:Butterworth-Heinemann, 1996:17-19.\n4. Kanski JJ, Bowling B. Clinical Ophthalmology: A Systematic Approac., 7th ed. Ediinburgh: Elsevier Saunders, 2011:313-315.\n5. Bruce Shields M. Textbook of Glaucoma, 4th ed. Baltimore: Williams and Wilkins, 1998:195-205.\n6. Cioffi GA, Duncan J, Girkin CA, et al. Basic and Clinical Science Course, Section 10. Glaucoma. San Francisco: AAO, 2009-2010:188.\n7. Cioffi GA, Duncan J, Girkin CA, et al. Basic and Clinical Science Course, Section 10. Glaucoma. San Francisco: AAO, 2010-2011:167-183.\n8. Kanski JJ, Bowling B. Clinical Ophthalmology A Systematic Approach, 7th ed. Edinburgh: Elsevier Saunders, 2011:387-388.\n9. Kanski JJ, Bowling B. Clinical Ophthalmology A Systematic Approach, 7th ed. Edinburgh: Elsevier Saunders, 2011:388.\n10. Kanski JJ, Bowling B. Clinical Ophthalmology A Systematic Approach, 7th ed. Edinburgh: Elsevier Saunders, 2011: 390.\nFull text views: 4607"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:9166c51f-0be5-41d8-8fc9-c02b2cbe4c2f>","<urn:uuid:06ef6185-df79-4fd9-804f-e1a159b16479>"],"error":null}
{"question":"¿Cuáles son las características fundamentales de las preguntas esenciales en educación y qué tipos específicos existen para su implementación efectiva?","answer":"Essential questions have several fundamental characteristics: they have no single right answer, raise other important questions, address philosophical and conceptual foundations, recur naturally, and provoke sustained student inquiry. There are two main types of essential questions: 1) Overarching questions that frame courses around big ideas and promote transfer of understanding (e.g. 'How does language shape culture?'), and 2) Topical questions that are unit-specific but still promote inquiry and require explanation (e.g. 'Is Holden Caulfield a phony?'). Both types are necessary for effective teaching. These questions help students uncover key ideas and sustain meaningful inquiry in the classroom.","context":["Presentation on theme: \"Questioning Strategies for Every Classroom: Promoting Higher-Order Thinking and Reasoning for All Learners Curriculum Implementation Module Five (Part.\"— Presentation transcript:\n1 Questioning Strategies for Every Classroom: Promoting Higher-Order Thinking and Reasoning for All LearnersCurriculum Implementation Module Five (Part II):January 2012Alexandria City Public Schools\n2 Essential Questions for Part II How can questioning enhance students’ language acquisition?How can ACPS educators ensure that students can respond successfully to a range of higher-order questions?How can we encourage students to monitor their understanding through the kinds of questions we use in our classrooms?\n3 Objectives for This Session Explain connections between questioning strategies and students’ use of higher-order reasoning.Describe and design a variety of higher-order questions for use with students.Identify and reinforce classroom behaviors that confirm the presence of higher-order questioning and related critical thinking competencies.\n4 Sample College Entrance Essay Questions: How Would You Do? 1. Have you ever walked through the aisles of a warehouse store like Costco or Sam’s Club and wondered who would buy a jar of mustard a foot and a half tall? We’ve bought it, but it didn’t keep us from wondering about other things, like absurd eating contests, impulse buys, excess, unimagined uses for mustard, storage preservatives, notions of bigness…and dozens of other ideas both silly and serious. Write an essay somehow inspired by super-huge mustard. (U. of Chicago)\n5 College Entrance Essay Questions (2) 2.How have your life experiences and background shaped you into an individual who will enrich the University of Maryland community?3.Discuss an aspect of a book that has shaped the way you think. (St. John’s College, Annapolis)\n6 College Entrance Essay Questions (3) 4.What is your favorite word—and why? (University of Virginia)5. Franz Kafka once said: “A belief is like a guillotine, just as heavy and just as light.” How would you relate this quote to your own convictions? (University of Virginia)\n7 College Entrance Essay Questions (4) 6.The following Japanese character represents the Zen concept of “Mind that does not stick.” How does this idea apply to your life and experience? (University of Chicago)7. If you could balance on a tightrope, over what landscape would you walk? (University of Chicago)\n8 College Entrance Essay Questions (5) 8.How do you feel about Wednesday? (University of Chicago)9. You have just completed your 300-page autobiography. Please submit page 217. (University of Pennsylvania)\n9 PLACE YOUR BETS!!!How well do you understand what research tells us about higher-order questions?Each of you will start with an imaginary $ With a partner, determine how certain you are about each of the following statements.For the first round, if you both are absolutely certain that the statement is “TRUE” or “FALSE,” bet your full $100. If you’re not certain, hold some money back.\n10 QUESTION ONE— True or False? Close to 75% of American teachers’ time is currently devoted to the use of higher-order questions with their students.\n11 FALSE!Actually, a range of studies suggests it’s just the opposite. Between percent of classroom time is devoted to discrete, factual-recall forms of questioning.\n12 QUESTION TWO— True or False? The more students move beyond questions that require recall, repetition, and paraphrasing, the greater their levels of understanding and independent transfer.\n13 TRUE!According to Barry K. Beyer (P. 5 of your handout), “A thoughtful question (one that requires students to go mentally where they have not been before) makes students think [more] deeply…The more students receive modeling and shaping experiences involving…higher order questions, the greater their level of understanding…and their ability to apply and transfer [the content they are studying]…”\n14 QUESTION THREE— True or False? When students work with higher-order questions, their brain physiology changes.\n15 TRUE!According to the ground-breaking publication How People Learn (AERA, 1999), teachers’ use of a variety of higher order questions can overcome the brain’s natural tendency to limit information, …creating more synapses between nerve cells…By emphasizing higher order questions, we are, in effect, “strengthening our students’ brains…” (P. 5 of your handout)\n16 QUESTION FOUR— True or False? The average American teacher’s Wait Time (i.e., time between posing a question and eliciting a response) is 2-5 seconds.\n17 FALSE!Actually, it’s a second or less! According to the research of Mary Budd Rowe, if we use just 2-3 seconds of Wait Time, we get between 45-50% more students attending to the question. (See Page 6 of your handout…)\n18 QUESTION FIVE— True or False? Because of the large number of standards assessed by state testing programs, discrete fact-based questions should be emphasized to improve test scores.\n19 FALSE!Studies such as the National Assessment of Educational Progress (NAEP), Trends in International Science and Mathematics Studies (TIMSS), and PISA (Programme for Interational Student Assessment) all confirm the following: Teaching less better produces higher test performance. If students understand a core curriculum deeply (as a result of such strategies as answering higher order questions with supporting evidence), aggregate and disaggregated test results improve.\n20 So What Does the Research Tell Us? Teachers who use higher-order questions improve student learning.Predominant use of higher-order questions promotes both student recall of key information and gains on standardized tests.Higher-order questions improve student literacy by promoting higher levels of text analysis than factual/recall questions.\n21 Current Brain Research Tells Us That… The brain asks “Why?” It compels us to seek authenticity and purpose.The brain searches for patterns and connections.The brain downshifts when it perceives physical, emotional, or social threat.\n22 In addition, current research tells us that the brain… Tends to limit information, creating a “gestalt” through neural pruning.“Grows dendrites” and experiences “neural branching” when it is engaged in compelling and engaging thinking processes.Experiences creativity and divergent thinking when it is in a condition of “flow.”\n23 Questioning and the Brain—Higher-Order Questions… Help to overcome the brain’s inclination to limit information.Encourage creativity and divergent thinking as mental habits.Reinforce neural branching, rather than neural pruning.\n24 The “Big Ideas” of Higher Order Questioning “Thoughtful questions”Brain-based learning and higher order questionsWait Time I and IIPromoting purposeful, strategic, and self-regulating learners: Correlations with higher order questionsEnhancing students’ use of higher order reasoning skills (e.g., comparison) and processes (e.g., problem solving)Design characteristics of higher order questionsThe need to model how to “unpack” HOQsHOQs and improving standardized test results\n25 Reflection Checkpoint THINK: As you review the research conclusions on page 25, what are your initial reactions?PAIR: With a partner, discuss the implications of this research for your school, department, and grade level(s).SHARE: As a faculty, what do these conclusions mean for us and our students?\n26 How Can We Identify and Describe Higher Order Questions? Deal with the most important topics or issues (i.e., the big ideas) of a discipline, subject, or topic.Have no obvious, single, or prescribed correct answer.Require analysis, synthesis, and/or evaluation.Encourage personalized responses supported by evidence (i.e., there may be alternative justifiable answers…).Require students to produce or construct meaning or new knowledge.Advance students toward a deeper understanding of the subject, topic, or issue they are studying.\n27 What Can We Observe in Classrooms That Emphasize Higher Order Questions? Observe the following teaching episode.How many examples can you find of the question types identified on Pages 28-29? (comparing/classifying; identifying attributes and components; ordering; identifying relationships and patterns; representing; identifying errors; inferring; predicting; elaborating; evaluating and establishing criteria; verifying; and finding patterns…)\n28 A Think-Pair-Share Activity THINK: Review the questioning techniques presented on pages What level of use do you observe in your own classroom—or that of another educator (e.g., Kay Toliver)?PAIR: Find a partner to reflect on your ratings and conclusions.SHARE: What do you both agree are areas of need that you would like to see emphasized in your current classroom, school, and/or district?\n29 What Types of Questions Can Teachers Use? (Pages 25-26) Application: applying essential knowledge and skills to new (and unanticipated) settings and situations--e.g., How could you apply these grammar and usage rules to improve your essay?Analytical: dissecting key information and analyzing important concepts, themes, and processes--e.g., How are these characters alike and how are they different?Synthesis: formulating summaries, making references, and/or creating something new based upon acquired knowledge and skills--e.g., What predictions can you make about what may happen next in the story?\n30 What Types of Questions Can Teachers Use? (Pages 25-26) Interpretive: open-ended questions requiring students to formulate and support with evidence an original opinion or interpretation--e.g., What does Frost mean when he says: “I have miles to go before I sleep”?Evaluative: formulating and supporting judgments or critiques based on clear evaluation criteria--e.g., How would you rank these choices? What are your criteria?Essential: interpretive questions that prompt students to explore, debate, and discuss the big ideas at the heart of a topic or content area--e.g., How do concepts of heroism vary across cultures and civilizations?\n31 Application ActivityExamine each of the question types on pagesCreate at least one example of each question type.When you are finished, share your examples with one or more participants near you.FULL-GROUP DEBRIEFING: To what extent are we currently using a range of question types in our departments, grade levels, and school?\n32 Essential Questions Have no single right answer. Raise other important questions.Address philosophical and conceptual foundations of disciplines.Recur naturally.Provoke and sustain student inquiry.Help students “uncover” key ideas.\n33 Sample Essential Questions How can we observe and determine the significance of universal natural patterns in our universe?Why did classical Greek playwrights contend that tragedy derives from human ignorance about our own flaws and character defects? To what extent do you agree with their assertion?To what extent is mathematics a language? How can we become fluent in it?To what extent is history objective? To what extent is it a story whose events are shaped by the historian telling it?What would happen if there were no…Internet? Painting? Performing arts? Competitive sports?\n34 A Seminar Experience… Form table groups of four-five participants. Appoint a (1) seminar facilitator, (2) recorder, and (3) group presenter.Have your facilitator select one of the following essential questions for your group to explore.Think individually about the essential question chosen for your seminar. (2 minutes)Then, begin to discuss it together. (10 minutes)Prepare to have your designated presenter summarize your ideas and reactions to the full faculty. (2 minutes)\n35 Choose Your Essential Question… 1. Throughout history, why have all world civilizations had some form of competitive sports or athletics?2. How should an “ideal” high school look and operate? To what extent do modern high schools reflect these characteristics?3. To what extent do children today think differently as a result of the technology and media to which they are exposed?\n36 Application ActivityTry your hand at creating several essential questions for your particular content area.Share your work with a partner. Help one another to ensure that your questions meet the criteria for effective essential questions.Discuss how “student-friendly” each question is.Post your edited questions on a flip chart page as part of a gallery walk.\n37 The Six Facets of Understanding Explanation:Supporting claims and assertions with evidence.Interpretation:Constructing meaning from events or text, supporting conclusions, and creating new products and processes as a result.Application:Using what you have learned in new or unanticipated situations and real-world settings.Perspective: Analyzing the points of view associated with controversial events or issues.Empathy: Walking in the shoes of another; experiencing events and situations as another person might.Self-Knowledge: Monitoring self-awareness, regulating one’s own thinking and learning, and assessing the extent to which we understand—or fail to understand—something.\n38 Application ActivityReview the sample question prompts on your handout. Try your hand at writing one or two questions for each facet.With a partner, discuss which of the six facets of understanding have particular relevance or usefulness for your content area(s).Be prepared to share your work and reactions with the rest of the faculty.\n39 Provoking vs. Enabling Questions Provoking: Elicit student debate, inquiry, and investigation.Example: Was the Civil War civil? To what extent is history in the eye of the beholder?Enabling: Reinforce students’ ability to understand how to use a skill or procedure.Example: When writing, how can we ensure we address the needs of our audience?\n40 The Power of Wait Time…The research on Wait Time I and II has been around now for close to three decades…In spite of what we know to the contrary, educators—on average—still pause one second or less after posing a question. In your opinion, what accounts for our collective resistance to using wait time?\n41 Wait Time I and II Wait Time I 3-5-seconds before “calling.” Reinforces time for reflection and recall.Expands engagement and buy-in.Wait Time IIAfter calling on student.Allows for elaboration.Encourages reflection and comprehension monitoring.\n42 Techniques for Helping Students Respond to Higher-Order Questions (Pages 27-28) In addition to Wait Time I and II, there are many other strategies educators can use to help students respond effectively to higher order questions.Consider the recommendations presented here. How many of these strategies are in use in your classroom, school, or district?\n43 Techniques for Helping Students Respond to Higher Order Questions: Which Ones Can You Use? 1. Wait Time I & II2. Student identification of question type(s)3. Discuss value of using strategies to respond to HOQs4. Use probes to help students “unpack” their thinking5. Use essential questions to create schema6. Use debriefing sessions7. Have students design HOQs8. Teacher modeling of HOQ responses9. Emphasize the value of evidence10. Encourage students to use a range of evidence11. Create word walls and other visual displays for HOQs12. Include HOQs on exams and quizzes13. Use academic prompts (FAT-P)14. Culminating performance tasks & projects focused on HOQs15. Student-created HOQs for future classes\n44 A Planning Template for Higher Order Questions (Pages 28-29) Consider the recommendations and questions presented in the planning template on pagesHow might you use this planning guide in your current school or district?\n45 Action Planning and Next Steps How will you use the resources and strategies presented in this session?Be prepared to share a specific action step with the rest of the group.\n46 Thank you for your participation today… We hope you’ve enjoyed the workshop…","2 A Voice from the FieldTeacher Talks About Essential Standards\n3 GoalsWhat is an essential question?How do I write effective ones?\n4 An Essential Question is One that lies at the heart of a subject or a curriculum & promotes inquiry & uncoverage of a subject.\n5 Essential Questions Have no simple “right” answer Provoke & sustain inquiryAddress conceptual or philosophical foundationsRaise other important questionsNaturally & appropriately recurStimulate vital, ongoing rethinking\n6 Examples What is a true friend? To what extent does art reflect culture or shape it?Is everything quantifiable?To what extent is DNA destiny?In what ways is algebra real and in what ways is it unreal?To what extent is US history a history of progress?\n7 Examples Must heroes be flawless? Who is entitled to own what? Is the subjunctive necessary?What makes writing worth reading?Does practice makes perfect?What is healthy eating? Healthy living?\n8 What makes a question “essential”? Recurs throughout all our livesRefers to core ideas & inquiries within a disciplineHelps students effectively inquire and make sense of important but complex ideas, knowledge, know-howEngages a specific & diverse set of learners\n9 Intent, not language, is the key: Why we pose the questionHow students are to tackle itWhat learning activities & assessments we expect\n10 Types of Essential Questions Overarching: Frame courses and programs of study around truly big ideasTopical: Are unit specific but still promote inquiryGOOD TEACHING USES BOTH!\n11 Overarching Essential Questions More general, more broadPoint beyond specific topics or skillsPromote transfer of understanding\n12 Examples of Overarching EQ Can a fictional story be “true”?How do a region’s geography, climate, and natural resources affect the way people live and work?How does technological change influence people’s lives? Society?How does what we measure influence how we measure?\n13 Examples of Overarching EQ How do we classify the things around us?Do artists have a responsibility to their audience? To society?How does language shape culture?Is pain necessary for progress in athletics?\n14 Topical Essential Questions Unit specific - used to guide individual unitsPromote inquiryResist simple answersRequire explanation & justification\n15 Examples of Topical EQHow might Congress have better protected minority rights in the 1950s & 1960s?Should we require DNA samples from every convicted criminal?Is Holden Caulfield a “phony”?\n16 Examples of Topical EQ What is the value of place value? What is electricity?How do we hit with greatest power without losing control?\n17 THEY CANNOT BE THE FOUNDATION OF OUR CURRICULAR DESIGN. Leading QuestionsMeant to culminate in a fact or completely settled conclusionNo sustained inquiry or argument intended or necessaryUnderscores an important point we want students to noteWe need these, too, BUT...THEY CANNOT BE THE FOUNDATION OF OUR CURRICULAR DESIGN.\n18 Where to start? Determine the “big ideas” Common Core and Essential StandardsCourse texts/Resources\n19 What are Big Ideas?Core concepts, principles, theories, & processes that should serve as the focal point of curricula, instruction & assessment.\n20 Big Ideas Are important and enduring Are not obvious May be prone to misunderstandingPrioritize contentAre transferableAre the building material of understandingsManifest in various ways within disciplinesAct as “conceptual velcro”\n21 Finding Big Ideas Clarify Content Priorities Worth being familiar with Important to know and doBig ideas & Enduring Understandings\n22 Finding Big Ideas Unpack the NCSCS (CC/ES) Circle key nouns, adjectives, & verbsDraft implied or stated big ideas based on those key words.Critically analyze the course textWork “backward” to determine what big ideas and/or EQ the text addresses\n23 Big Ideas can be Concepts Themes Issues/Debates Problems/Challenges ProcessesTheoriesParadoxesAssumptions/Perspectives\n24 From Big Idea to EQ Identifying EQ & Understandings Start with Big IdeaAnswer questions related to Big IdeaGenerate EQ & desired understandings\n25 Topic or Content Standard Making the ConnectionBig IdeaTopic or Content StandardUnderstandingEssential Question\n26 ExampleStandard: The learner will be able to read, respond to, and critique historically and culturally significant works of literature in order to understand their importance and relationship to past and present cultures.Overarching EQ: Does literature primarily reflect culture or shape it?Topical EQ: What does Romeo & Juliet teach us about Shakespeare’s view of destiny? How does it compare to yours?\n27 Where to get more information Other training sessions--may be department, planning period in-service, Curriculum DepotUnderstanding by Design by Jay McTighe & Grant Wiggins"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:d2b16e6b-c50f-40ef-b249-799e481fec43>","<urn:uuid:58e13dd9-a14f-455a-a74c-cf8fb4514844>"],"error":null}
{"question":"What quality control issues do cocoa and coffee share during processing?","answer":"Both cocoa and coffee share similar quality control challenges in their processing stages. For cocoa, issues include proper fermentation timing (5-8 days) since poor fermentation can make beans 'slaty' or develop undesirable flavors. Similarly for coffee, even small errors in processing can have costly effects on bean quality. Both products are also sensitive to storage conditions - cocoa requires specific moisture levels (7%) while coffee needs protection from humidity and temperature variations. Additionally, both face challenges due to the long distances between production and processing facilities, requiring careful handling and storage during transport to maintain quality.","context":["Culture of cocoa\n3. Culture of Cocoa\n|The chocolate we enjoy is only the final stage in a long and complex alchemy involving people with very different skills: botanists,farmers, chocolatiers and confectioners. Without cocoa farmers to grow and process cocoa there would be no fine chocolate.\n|Chocolate Facts: Growing Cocoa\nWhile soil characteristics of cocoa growing countries vary immensely. Cocoaplantations are usually established on land where the drainage moderates the wet and dry climate seasons. And the composition of the soil has to be neutral, neither acid nor alkaline.\nThe ideal climate is a combination of humidity and heat, with the trees not directly exposed to the sun. So cocoa is grown in tropical climates. And cocoa trees are planted under cover of the tall trees of tropical forests, where there is around 50% shade.\n|Chocolate Facts: Processing Cocoa\nWhile the tree variety and the nature of the soil are important, other factors also play a role in determining the quality of the finished chocolate. Cocoa must be processed correctly, if it is to create truly fine chocolate:\n· Harvesting: Quality chocolate is made from beans taken from cocoa pods that have reached just the right degree of ripeness. Under-ripe pods have low cocoa butter content and over-ripe pods may contain microbes. Both affect the fermentation process and damage chocolate flavor.\n· Shelling: After the harvest, the pods are opened to extract the grains and the sticky white pulp or 'mucilage' that surrounds them. Precautions must be taken to ensure the quality of the end product.\n· Fermentation: Farmers ferment the cocoa within a week of harvesting or, ideally, immediately after shelling. The five to eight day process transforms the seeds/grains from the shell into 'stabilized' beans, ready for the chocolate maker to use.\nThe pulp disappears completely, leaving only the fermented seeds. These are then dried and become known as 'beans', ready to leave for the chocolate factory.\nPoor fermentation procedure can have serious consequences.\n- If fermentation stops completely, the beans will be 'slaty' and unable to produce quality chocolate.\n- Short fermentation prevents flavor precursors developing and bitterness and astringency reducing.\n- Too much fermentation develops undesirable flavor characteristics (known in the business as 'off-flavors') when the beans are roasted.\n· Drying: After fermentation the beans have a moisture content of 55 to 60 percent. Drying should reduce the moisture content to 7% all allow safe storage before roasting.\n- Sun Drying: the beans are spread out in the open air, on racks or directly on the ground. This is the best method of drying. (The 'cocoa dance' where workers dance on the drying beans to rake them is still a tradition in some cocoa-producing regions, such as Chua.\n- Artificial Drying: beans are dried in special drying units designed to provide heat and ventilation. (While more controllable this method carries a risk of tainting the beans with a smoky bacon flavor if they come into contact with smoke from the heat source).\n· Storage: Beans are stored in climate controlled environments because growing regions are commonly many thousands of miles from chocolate making facilities.\nThe cocoa is now ready for the chocolate making process.\nWant to know more? Michel RICHART's 'Chocolat Mon Amour' is your complete guide to the world of chocolate.","After leaving the mill, coffee travels thousands of miles and through numerous hands before reaching the roaster. And across this journey, even small errors can have costly effects on the quality of those precious green beans.\nTo discover how importers and roasters can protect the quality of their coffee, I spoke to Diego Lara, Global Coffee Specialist at GrainPro, which makes US-patented airtight and ultra-hermetic coffee bags. I also reached out to several of the company’s customers: Jason Long, CEO of Cafe Imports; Alejandro Cadena, CEO of importers/exporters Caravela Coffee; and Sebastian Villamizar, CEO of La Palma Y El Tucán, an award-winning coffee farm and exporter.\nHere’s what I discovered.\nLee este artículo en español Cómo Asegurar La Calidad del Café Verde Almacenado y en Tránsito\nOn award-winning Colombian farm La Palma Y El Tucan, coffee is packed in GrainPro bags prior to being shipped overseas. Credit: La Palma y El Tucán\nThe 3 Biggest Risks to Your Coffee Quality\nThree things came up again and again during my interviews: moisture, temperature, and time. This doesn’t mean they’re the only risks you should be aware of (insect damage, for example, is another one) but these are the most likely and therefore most important concerns.\nSebastian Villamizar of La Palma Y El Tucán says, “Sometimes, people don’t see storage as a critical part of coffee quality… [but during storage,] coffee absorbs water very easily, so your moisture levels can go up very easily.” And if the moisture content of your green beans fluctuates, he explains, you can develop defects, including a phenolic taste.\nThis is a particular challenge, he tells me, in areas with high moisture levels – which describes many producing regions. At his farm, he says that “the average relative humidity is 80%.”\nHe sees the purchase of hermetic/airtight packaging as an investment. While his GrainPro bags might be costly, he knows that “it’s going to be worth it.”\nJason Long of Cafe Imports agrees, adding that “as a high-end specialty coffee trader, it [is] important to protect the coffee. If the coffee arrives in a poor condition, you could claim to exporters but the money that you get returned won’t make up for the loss financially.”\nBear in mind thattemperature variations will also affect the moisture content in the air and therefore the moisture content of your green beans – unless your coffee is stored in hermetic packaging, of course. This is why both humidity and temperature will, ideally, be controlled for.\nAnd, if there are any issues with moisture or temperature, prolonged exposure will worsen its impact – making time another important point to consider.\nDiego Lara of GrainPro says that, since coffee beans will sometimes be stored for months before use, it’s important to make sure they are stored well. Otherwise, he explains, “sometimes [the roaster/importer] will lose very good coffee beans that they’ve purchased and that is a loss of money.” He stresses the importance of using hermetic bags to protect the coffee.\nAnd don’t forget that even the most careful of plans can go wrong. Diego tells me about a GrainPro client whose container of good-quality coffee was lost in transit for two to three weeks – a nightmare situation for any roaster or importer.\nThe client was worried that the prolonged time at sea, with the increased humidity, would ruin the coffee. However, when it was finally found in the Port of Oakland, the beans had been stored in airtight GrainPro bags and so were still in great condition – a relief for everyone involved.\nSo, let’s break down, from farm to roastery, what you can do to ensure those beans are stored well, no matter where they end up or what moisture levels they’re exposed to.\nAt Finca Oná, Guatemala, coffee beans are stored in GrainPro´s Patented Cocoon Indoor to protect them from moisture and whitening in the warehouse. Credit: EGS Guatemala\nSigning The Contract\nAfter coffee is harvested, processed and dried, it waits in a warehouse until it is contracted and transported for exportation.\nThe contract should specify details such as quality, quantity, price, payment, delivery conditions, and more. And at this point, transportation and storage conditions can be negotiated and signed on.\nDifferent countries may have different standard contract terms. However, the International Trade Centre (ITC)’s The Coffee Exporter’s Guide suggests referring to the European Coffee Federation and the Green Coffee Association for most commonly used contracts.\nCafe Tuxpal from El Salvador packaging their coffees in GrainPro bags. Credit: GrainPro\nAt The Farm or Mill\nBefore the coffee leaves the farm or mill, it’s important to make sure that it’s been effectively dried. The best storage and transportation systems will be useless if the beans are too wet.\nIn fact, Jason tells me that airtight bags “can aggravate the problems” in uneven or insufficiently dried coffee. Since moisture cannot enter the bags, it cannot escape either. While he stresses that poorly dried coffee would never be good on arrival, the cup score could fall even more rapidly than if it were in jute bags.\nGood systems cannot make up for low-quality products.\nLearn more! Read How to Improve Quality When Drying Washed Coffees\nGreen and roasted coffee samples.\nIn The Warehouse at Origin\nBetween signing the contract and exporting the coffee, it’s important to pay close attention to where and how the coffee is stored. What are the warehouse conditions? Which bags are being used? Is the quality being routinely checked?\nAlejandro Cadena of Caravela recommends “storing coffee in parchment” inside hermetic, protective bags. This prevents the green beans from being in direct contact with air and helps prolong their longevity.\nBefore switching to hermetic GrainPro bags, he tells me, “We lost about 2-3% of our coffee because it gained moisture in our warehouse… GrainPro was a revolution in the industry.”\nHowever, it’s important to make sure that staff on the farm or mill know how to correctly use and seal the airtight bags. Diego explains that GrainPro has been providing training throughout Latin America and Africa on how to store coffee in individual bags, as well as the best storage and warehouse conditions. They have also printed materials in the producers’ own languages with pictograms.\nThere is no point requesting that a producer uses a particular product or system if they are not familiar with how to do so – or if they cannot understand training because the materials were only in English.\nUS-patented GrainPro packaging placed inside jute bags to protect the beans from moisture. The pictograms explain how to use and seal the bags. Credit: Sunghee Tark\nFrom Farm to Port…\nNow, it’s time for the coffee to be transported from the farm or mill to a port. Before loading beans into the container, the ITC’s The Coffee Exporter’s Guide recommends carefully inspecting it. Pay attention to the moisture content, smells, and any signs of an infestation. Airtight bags will prevent bugs from getting into the coffee; however, if you only use jute bags, you could be vulnerable to insect damage during transit.\nAlejandro tells me that, now that Caravela uses airtight bags, “We don’t worry about condensation in the container… also, we don’t worry about any punctures or any holes that are in the container.”\nBut before that, they “used to put [desiccant] bags inside containers to avoid condensation,” in addition to paper lining. This added extra costs but was also ineffective.\nA coffee shipment bound for New York from Buenaventura, Colombia is packed in GrainPro´s patented TranSafeliner designed to protect the beans when in shipping containers. Credit: GrainPro\n…And From Port to Port\nUnless coffee is for a sample and cupping purpose (in which case it will normally be sent by air freight), it will usually travel overland and via ocean. It is at the seaport that responsibility for the coffee typically changes hands.\nWhen traded as “Free on Board” (FOB), a producer or exporter’s legal responsibility for the coffee’s quality ends when “goods have passed over the ship’s rail at the port of shipment,” according to the ITC’s The Coffee Exporter’s Guide. At this point, the responsibility is transferred to the importer.\nNevertheless, every player should pay close attention to each step of the transportation and storage. Ensuring that the coffee will be of the highest-possible quality protects the relationship and reputations of everyone involved.\nJason agrees, telling me that “the whole chain” needs attention. As an importer trading on FOB terms, he says that Cafe Imports takes great care when investigating who to partner with on the ground. Sebastian shares this belief, adding that you shouldn’t overlook the importance of reliable drivers and packaging providers.\nIn other words, roasters and importers, the coffee quality is now your responsibility – but don’t think that you can be complacent about the stages prior to this.\nThat includes the time at port before the coffee is boarded onto the boat. Jason explains that there is typically a high temperature and humidity rate at ports. “Coffee spending time at a port is disastrous… Mombasa, [Kenya] is 40°C and 99% humidity. So, you definitely want to minimize time at port.”\nContainers ready for exportation.\nAt The Destination Port & Warehouse\nThere’s little you can do about your coffee when it’s on the ship, other than hope it arrives promptly. However, you can make sure that your cargo doesn’t suffer customs delays on arrival and that you’re appropriately insured.\nWhen the coffee arrives at the destination port, it is unloaded and the documents and condition are checked. Then, it’s time for it to be transported to the final warehouse.\nIn the case of FOB, the importer is responsible for insuring the cargo and checking the container on arrival for the weight, moisture, and any unusual conditions prior to making a claim (the ITC’s The Coffee Exporter’s Guide).\nBe careful: The Coffee Exporter’s Guide also states that, unless otherwise specified, “damage due to improper selection of container, improper lining or stowing, etc., is never part of the insurance cover to FOB” and that importers should work with exporters to address any such concerns. (This is another reason why it’s important to pay attention to the storage and transportation prior to the legal exchange of responsibility.)\nAdditionally, make sure you know the customs requirements of the country you’re importing the coffee into. This ranges from paperwork and certification to bag size. For example, Diego explains that, in parts of Europe, coffee needs to be imported in smaller bag sizes. GrainPro released their 15-kilo bag in response to this, as well as to direct requests from clients such as Sebastian.\nSebastian adds that the zipped seal is useful for these smaller samples, since customs can easily check the bags and reseal it. You don’t have to risk the coffee being incorrectly repackaged by untrained staff at port and moisture then getting into the beans during the final stages of the journey to the roastery.\nFinally, just like in the warehouse at origin, make sure that the warehouse at destination has good conditions for coffee storage and that the coffee has also been packaged well.\n15-kilo GrainPro bags with the optional use of boxes for shipping specialty coffees and micro lots. Credit: GrainPro\nFrom The Warehouse to The Roasters\nIf the coffee has safely arrived at a warehouse with controlled temperature and moisture content, you are almost at the end of the journey from farm to roastery.\nImporters should now get in touch with their roaster-partners to organize cuppings. Make sure the coffee is repackaged appropriately after being opened for this; you may need to give those partners instructions on how to do so. Diego tells me GrainPro provides training materials and that, if large companies need a specific type of training or video, they can also create it.\nMeanwhile, Jason tells me that even now, the key is to continue “to minimize exposure to everything [heat and humidity].” Based on his experience, storing coffee in protective airtight bags “keeps the cupping level high” for a longer period.\nOpening a GrainPro hermetic bag to see green coffee in good condition. Credit: Sunghee Tark\nThe journey from farm to roastery is long and complicated. Coffee travels through numerous ports and via many different forms of transport. Close attention to the transportation and storage systems at each stage, as well as the processes for receiving green beans, is critical for ensuring your specialty coffee remains high-quality.\nTo sum up my interviewees’ advice?\nWork with the right partners, pay meticulous attention to detail, and above all, keep your coffee beans away from heat and humidity.\nEnjoyed this? Check out A Roaster’s Guide to Green Bean Moisture Content\nWant to read more articles like this? Sign up for our newsletter!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:6d05c5c6-eae4-40cc-b0d9-5d3e7e5fbad0>","<urn:uuid:f03249af-961b-4d75-b45e-05716f1d040b>"],"error":null}
{"question":"I'm interested in symbols of transformation. What's the parallel between the diamond's role in spiritual metaphysics and Advaita Vedanta's concept of Maya?","answer":"The diamond symbolizes the highest transformation of earthly matter into an incorruptible, transparent form - representing perfection and enlightenment. It's seen as refined earth that takes on properties of water while maintaining hardness. Similarly, in Advaita Vedanta, Maya (cosmic illusion) represents a transformation of perception where the world appears real but is actually a projection of Brahman. Maya is described as neither real nor unreal but indeterminate, just as the diamond represents a paradoxical state of being both earthly and transcendent. Both concepts deal with the transformation of the mundane into the spiritual through a change in essential nature or perception.","context":["Children’s Dreams: Notes from the Seminar Given in 1936-1940 (\nAs a matter of fact, it does have something to do with consciousness. Could you give more details about it?\nThe transparent stone refers to the diamond. It is refined earth, the epitome of refined earth.\nThe earth is dark, gloomy, nontransparent, and starts to become transparent in the stone.\nAlthough the stone is earth, hardest earth, it assumes the character of transparent water.\nWe speak of a diamond “of the first water.”\nBecause of its transparency, the philosopher’s stone is also called vitrum aureum (the golden glass) or vitrum malleabile (the malleable glass).\nIn the Book of Revelation it says that the streets of the new Jerusalem were like golden glass\nSo it is the same idea as in alchemy—that the earth had been transformed into a transparent, waterlike, yet hard and imperishable, incorruptible structure.\nTherefore, the philosopher’s stone is the expression of the highest perfection of the earthly body, and, therefore, you also find the idea that the lapis philosophorum is man himself, that is, his corpus glorificatum, his body at the Resurrection.\nThis immortal body is the subtle body that had left the physical body and is beyond corruption.\nThe diamond, the hardest mineral, is synonymous with the lapis philosophorum.\nThis is ancient metaphysics, old speculation in symbolic form.\nWhat does this mean psychologically?\nIt was mentioned that the diaphanitas and the stoniness, the inelasticity, could have to do with the nature of consciousness.\nYou can find this connection in the old texts of alchemy, the idea, that is, that the stone is the product of a mental operation, the equivalent, so to speak, of enlightenment.\nTherefore the stone says in a Hermetic text: “I create the light, the light that is greater than all other lights in the world.”\nSo what is actually meant is a phenomenon of consciousness, a product of human effort, and at the same time a donum gratiae, a gift of God’s grace.\nIt is always stressed that it is impossible to do it on one’s own, it can only be given per gratiam dei, but man still has to make the effort to make this structure.\nIt originally was a head, that is, a consciousness that was in the head.\nIt is symbolized as the head of Osiris that was washed ashore from the sea and venerated by the women of Byblos.\nThis severed head, the so-called round element, is the epitome of perfection.\nA consciousness has crystallized that is imperturbable, a detached consciousness, characterized by wholeness (represented by roundness).\nIn the legend of Perseus, too, we find this idea, in the severed head of the Gorgon.\nHere, roundness is linked to the mortification of the mother.\nYes, quite right. In the stars you find the constellation of Perseus with the Gorgon’s head above the sign of Taurus.\nThe great Gorgon’s head is the horrible face entwined by snakes; it is the face of the past in front of which we are petrified.\nThe fear it provokes can be traced back to the fear of the devouring mother, the horror of this; for this face can turn you into dead earth again.\nThis danger was averted by Perseus by cutting the Gorgon’s throat with a fiery sword, a diamond sword.\nThis would mean that the danger is averted by the intervention of consciousness, of thinking; because it was an act of consciousness that detached him from that horror.\nWhen the flood of blackness is later inundating him again, he can free himself from it, because he cut the head of the horrible mother.\nYou find a similar standpoint in the philosophy of the Upanishads; the knowledge about Atman liberates from the law of the earth.\nSo the head is that round thing, originally hidden in matter, that Zosimos called the Omega element.\nThat’s why the ancient philosophers called themselves “children of the golden head.”\nThis was the opposite of the caput mortuum or caput corvi,which is the sediment that precipitates, or the opposite of nigredo.\nThis caput aureum, however, is the end product of the process.\nIt is also round, it is the wholeness, and it is a transparent stone. ~Carl Jung, Children’s Dreams Seminar, Pages 221-223.","The Concept of Advaita Vedanta\nAdvaita Vedanta refers to the non-dualistic school of Hindu philosophy, which is derived mostly from the Upanishads and elaborated in detail by eminent scholars like Gaudapada and Sri Adishankaracharya. Dvaita means duality, and Advaita means nonduality. In simple terms, Advaita means absence of the duality between subject and object. In our wakeful consciousness we experience duality, but in deep sleep only nonduality.\nAdvaita school believes that Brahman is the one and only reality and everything else is a mere appearance, projection, formation or illusion. One of the most common examples used to describe the state is momentarily seeing a snake in a rope when it is lying in the darkness. The snake is an illusion, and the rope is the reality. In the same manner the world appears in the mind as a formation over the Self.\nThe school also believes that Atman, the individual self, has no separate existence of its own. It is but a projection or reflection of Brahman only in each being. A jiva is deluded soul by egoism, desires, and other impurities and thereby experiences duality and separation. Because of it each being is bound to the cycle of births and deaths and the laws of karma as long and remains so until liberation is achieved.\nBrahman is real, but the world in which we live is a mere illusion, like a mirage. It appears in our consciousness because of the activity of the mind and the senses. Since we totally depend upon them, we do not perceive Brahman, the ultimate reality, who is hidden in all. When they are fully withdrawn and made silent through detachment, purity and renunciation, one can see the Supreme Self hidden in all and attain liberation.\nAdvaita Vedanta believes that an enlightened guru, having the knowledge of both the scriptures and Brahman, is indispensable for anyone seeking salvation. Mandukya Karika of Gaudapada is considered to be the first available treatise on Advaita Vedanta, while the monumental works of Shankaracharya constitute its core literature. Successive generations of scholars enriched the school of Advaita through their teachings and scholarly works. Advaita school also forms part of Vaishnavism, Saivism and Shaktism under different names.\nA few important concepts of Advaita Vedanta are presented below.\nSadhana Chatushtayam means the tetrad which are imperative for spiritual practice and liberation. The following four sets of qualifications are considered essential to achieve salvation, which each aspirant is expected to cultivate.\n- Nityanitya vastu viveka: The ability to discriminate between what is eternal (nitya) and what is temporary (anitya). The absence of it is responsible for the delusion.\n- Ihamutrartha phala bhoga viraga: Disinterestedness in enjoying the fruit of one's actions and sense objects here and here after. This will arrest the continuation and formation of karma.\n- Sama adi satka sampatti: Qualities such as sama (control of internal sense organs), dama (control of external sense organs), uparati (abstinence), titiksha (quietness), sraddha (sincerity and faith) and samadhana. They are important for self-transformation and the predominance of sattva, without which one cannot be free from the triple impurities of egoism, attachments and delusion.\n- Mumukhatva: Intense aspiration for salvation. It arises mainly due to the good works (karma) in the past. According to the Bhagavadgita only after repeated births a person feels a strong drive to achieves salvation and turns to the path of salvation.\nThey are the standards of ascertaining right knowledge, truth, or valid knowledge. In this world duality it is very difficult to know which is right knowledge and which is reliable for salvation or to ascertain truth. Advaita Vedanta recognizes six Pramanas, of which three were proposed by Shankaracharya and three by his followers. They are as stated below.\n- Pratyaksha: knowledge that comes through perception. This is objective knowledge which is experienced directly either through the senses or in deeper states of consciousness.\n- Anumana: knowledge that comes by means of inference. This is speculative knowledge based upon supposition or belief.\n- Upamana: Knowledge that comes by means of analogy, comparison and contrasting. This is relational knowledge.\n- Arthapatti: knowledge obtained by meaningful assumptions based on common sense and previous experience. This is hypothetical knowledge.\n- Anupalabdhi: Knowledge gained through negation.\n- Agama: Knowledge that comes through study of scriptures. This is pure theoretical knowledge.\nTheory of Causation\nAdvaita Vedanta recognizes two forms of causation, the material cause and the instrumental cause. According to the school Brahman is both the material and instrumental cause of creation. In other words, Brahman provides not only the will and direction but also the material and energy needed to manifest the things, beings and worlds. Brahman is both Purusha (Self) and Prakriti (Nature).This is in contrast to some schools of Hindu philosophy, which argue that Brahman is the instrumental cause while Prakriti or nature is the material cause.\nCause and Effect:\nAdi Shankaracharya proposed that each cause was hidden in its every effect, whereas the opposite was not true. In other words, the seed is hidden in the tree that produces it. While a cause is not different from the effect it produces, the same cannot be argued in case of effect in relation to its cause. A cause is always part of the effect, hidden within it and so is not different from it. Brahman is the cause of all creation. So the world is real only because Brahman, who is its cause, is hidden it and inseparable from it. From this perspective the world becomes an illusion because it disappears when the Self or Brahman is withdrawn from it. The world exists when you, the cause, are present in your mind. When you, the cause, withdraw from it, the world disappears. Shankaracharya propounded the theory of causation (vivartavada). According to it an effect is an outward projection of cause and hence not real. This is in contrast to the parinamavada concept according to which an effect is an evolution or transformation of cause and hence as real as the cause itself.\nAccording to Advaita Vedanta the world is an illusion or maya, which is caused by the veiling power of Brahman. It is unreal or illusory in an absolute sense. Since it is a projection of God's consciousness, it disappears when it is withdrawn. The veiling is called avarna and the projection viksepa. Followers of Advaita argue that maya is neither real nor unreal, but indeterminate or indescribably (anirvachaniyam) because it cannot be both at the same time.\nBrahman and Atman\nBrahman is the supreme, absolute and eternal reality. It is the only truth, the cause of all, and the only stable and permanent reality. Atman is Brahman, perceived as individual self, the hidden reality, in all aspects of creation. There is no difference between the two. When the Self overcomes its veiling, it experiences non-duality (Advaita anubhava) of existence and realizes its non-difference from the Absolute. Brahman in his absolute state is without qualities and attributes. However, in our relative state we perceive him to be having certain attributes and refer to him as Isvara, the lord of the universe. In the ultimate sense, Isvara is also not the cause, but only an effect or a reflection of Brahman in the quality of Sattva.\nAccording to Advaita, the world is unreal, not because it does not exist, but because it exists only so long as the Self is present in the awareness as the subject. When the Self is withdrawn from the consciousness, the world disappears. Besides it is ever changing, unstable, impermanent and subject to destruction and decay. It is an appearance, projection of God, like a mirage, or a mistaken reality. Our senses take it for granted whereby we mistakenly consider it real and permanent. The world exists because of our perception of duality and will disappear when we enter the state of non-duality or pure subjectivity, which is the state of the Self. When we overcome the illusion and develop detachment from the sense objects we enter that state of pure awareness where the duality between subject and object, or the knower and known simply vanishes and the Self alone remains.\nSome argue that Shankaracharya was inspired by the teachings of the Buddha, especially those pertaining to the school of emptiness (Shunyavada), in postulating the theory of nondualism. It may not be true because Buddhism does not believe in the existence of Self. It is true according to both schools, a being become empty upon liberation. According to Buddhism nirvana is an indeterminate state in which all traces of individuality disappears. According to Advaita, upon liberation the individual Self which is present in the being as a projection of Brahman becomes withdraws and the being vanishes into the ocean of existence as nothing. Shankaracharya was preceded by many Vedic scholars who followed the path of nondualism. For them it was not just a speculative theory, but a means to salvation. Shankaracharya followed an ancient Upanishad tradition that upheld the school, and probably belonged to a teacher tradition which followed it. For the next thousand years his teachings and numerous works became the standard for the school of Advaita.\nHowever, the works of Shankaracharya were not accepted by all Indian scholars. He was severely criticized for this stand on Advaita by those who followed Dvaita and Vishishtadvaita. Even Advaita there are many variations. Most of these schools came into existence as alternative philosophies or viewpoints, based upon their opposition to Advaita or their criticism of it. Adi Shankara's works on the Upanishads, the Bhagavad Gita and the Brahma Sutras greatly helped in understand the subtle nuances of Advaita. Although, they were mostly translations and commentaries or were based upon existing works, you can still find in them original ideas and interpretations.\nCritics of Shankara argued that he taught a version of Buddhism as Advaita. In Buddhism liberation arises from the insightful realization that the world is a mere appearance or a phenomenon. When one gains that insight he reaches the changeless, deathless, absolute state that cannot be described. In Advaita, liberation arises from Self-realization or the realization that only the Self is real and everything is a mere appearance or phenomenon. Thus, the concept of Maya in Advaita is rooted in the eternal reality of Brahman, but not just in the temporary delusion of the mind, which is the case with the Buddhists.\nIt is also probably untrue that Shankaracharya was instrumental in the decline of Buddhism. It started long before the emergence of Shankaracharya, at least by three hundred years of so. By the time he was born, Buddhism was already on decline and many Buddhist places of residence (aramas and Viharas) which were in a state of neglect were occupied the ascetic groups of Shaivism and Vaishnavism. It appears that by his time many Buddhist places of worship were converted into Hindu shrines. It is true that through his travels, debates and discussions he consolidated the revival of Hinduism. It served a great cause in preserving Hinduism when organized religions such as Islam and Christianity came to India and seemed to threaten its very existence. Shankara's Advaita or his theory of nondualism, provided a level playing field for the Hindus during the Islamic rule and contributed to the synthesis of new movements such as Sufism.\nSuggestions for Further Reading\n- The Advaita Vedanta the Experience of Oneness\n- Advaita Vedanta As It Exists\n- The Vedanta Philosophy According to Shankara and Ramanuja\n- Brahman the highest God of Hinduism\n- The Concept of Atman or Eternal Soul in Hinduism\n- The Five Bodies of Jiva, the Limited Being\n- Brahman according to Advaita and Dvaita schools of thought\n- The Concept of Atman or Eternal Soul in Hinduism\n- The Problem of Maya Or Illusion and How To Deal With It\n- Belief In Atman, The Eternal Soul Or The Inner Self\n- Brahman, The Highest God Of Hinduism\n- The Bhagavad Gita Original Translations\n- The Bhagavadgita, Philosophy and Concepts\n- Bhakti yoga or the Yoga of Devotion\n- Hinduism And The Evolution of Life And Consciousness\n- Why to Study the Bhagavadgita Parts 1 to 4\n- Origin, Definition and Introduction to Hinduism\n- Symbolic Significance of Numbers in Hinduism\n- The Belief of Reincarnation of Soul in Hinduism\n- The True Meaning Of Renunciation According To Hinduism\n- The Symbolic Significance of Puja Or Worship In Hinduism\n- Introduction to the Upanishads of Hinduism\n- Origin, Principles, Practice and Types of Yoga\n- Essays On Dharma\n- Esoteric Mystic Hinduism\n- Introduction to Hinduism\n- Hindu Way of Life\n- Essays On Karma\n- Hindu Rites and Rituals\n- The Origin of The Sanskrit Language\n- Symbolism in Hinduism\n- Essays on The Upanishads\n- Concepts of Hinduism\n- Essays on Atman\n- Hindu Festivals\n- Spiritual Practice\n- Right Living\n- Yoga of Sorrow\n- Mental Health\n- Concepts of Buddhism\n- General Essays\nTranslate the Page"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:490687ae-5666-4099-8222-acdcdd86a9e2>","<urn:uuid:ef5dba93-8c0f-4ce9-9f8f-08fbad72c0c9>"],"error":null}
{"question":"What role do technological advancements play in both modern coin grading and vertical agriculture systems?","answer":"In both fields, technological progress has been transformative. Coin collecting evolved from simple grading systems as modern machinery created higher quality coins, requiring more sophisticated grading standards. For vertical farming, technology is central to its operation, utilizing automated sensors, imaging techniques, and artificial intelligence for environmental control. Both systems rely on precise monitoring and control - in coin grading for determining condition and value, and in vertical farming for managing crucial growth factors like light, temperature, humidity, and nutrients. However, while coin grading technology led to some standardization challenges through 'gradeflation', vertical farming technology has enabled more consistent and controlled production methods.","context":["why the AU designation if it isn't almost uncirculated\nDo you want the short answer, or the long answer?\nHere's the long answer. Scroll down to the bottom of the post for the short answer.\nOnce upon a time, back in the late Middle Ages, coins were graded much the same everywhere, and on a very simple scale: Poor, Fair and Good. Since the only coins being collected back then were ancients, this scale was sufficient, and the words meant pretty much what they mean in everyday conversation: a coin in Good condition was actually pretty good, given that it had been dug up after being buried for a thousand years.\nAs coin collecting became more popular and modern coins began to be collected, additional grades beyond \"Good\" had to be invented, especially with the introduction of modern machinery capable of creating coins of very high quality (just like new words to describe the purity of olive oil had to be invented as oil refining and filtering technologies advanced). This we acquired Fine, Very Fine and, early in the 20th century, Extremely Fine. Then there was Uncirculated, to describe a coin in mint-fresh condition. \"AU\" or \"aUnc\" is a relatively modern invention.\nUp until at least the mid-20th century, condition was not a particularly significant factor in assigning a value to a coin; rarity was much more important. The Sheldon scale was invented in 1949 as a simplified way to convert condition-to-value for a given coin: a VF-20 coin was worth one-third the price of an MS-60 coin, no matter what the coin actually was. This kept the Sheldon catalogue relatively uncluttered, since only one price was required to be listed for each type and variety: the theoretical \"basal-state\" (BS-1) price. Prices for higher grades could be calculated just by multiplying the BS price by the Sheldon number for the grade.\nTime marched on, and the demand for coins in the \"best possible grade\" grew. Prices for Unc coins escalated far beyond the usefulness of the Sheldon scale to calculate value. Thus, we have the complicated system of grading, with multiple levels, adjectives, and words that seem to mean the opposite of their normal everyday sense - compared to a coin in aUnc condition, a coin in Good condition is actually rather awful.\nFrom now we can more clearly see a curious effect: \"grade creep\", also known as \"gradeflation\" - which is the main reason why the Americans have a much slacker grading standard than ours. The definitions of what qualifies a coin as \"Good\" or \"Very Fine\" has changed over time, and have gotten worse. As I said, originally a coin in Good was in pretty good nick.\nGradeflation happens at different rates in different places; it is faster where there has a high market demand for high-grade coins for a longer period. If the supply isn't meeting the demand, the grading standards are slackened slightly, to allow for increasing numbers of coins to reach the threshold for each grade. It's an evolutionary (or, more precisely, a devolutionary) process; tiny changes made over a long time period. Gradeflation has been more rampant in America than it has here in Australia, because of the higher collector demand generally which has meant that high-grade coins have been in demand there for longer.\nDon't think the Australian standard of grading has been immune to gradeflation, either. Today, the British grading standard is even tougher than ours; an American AU-55 ought to make an Australian EF, but would only rate a British gVF. But back when we were part of the British Empire, our grading standards were the same. And if you want to see how far our grading standards have slipped since the \"good old days\", grab a copy of an old grading guide from the 1960s. The book \"Collecting Australian Coins\" by Tom Hanley and Bill James came out in 1966, just after decimalization. Listen to these lines from the grading guide for the reverse of George V silver coinage, contained therein (emphasis mine):\nUncirculated: This would be a proof coin if the field showed more finish. The surface retains a mint-fresh lustre, detail of the heads of the kangaroo and emu are sharp, and the feathers on the emu's back show no trace of wear under the glass. Magnification discovers no blemishes such as scratches or nicks. Only the cream of coins obtained from the banks in the original mint rolls are in this condition.\nExtremely Fine: This is the condition in which all but a few of the roll coins are found. These coins show the marks, usually superficial scratches of the chutes and conveyors along which they travelled during minting processes. A good deal of their mint lustre remains and the glass should show no more than the slightest trace of wear on the uppermost feathers on the emu's back.\nOnly a tiny fraction of coins from mint rolls\nqualify as Uncirculated? Mint bag marks only visible under magnification\ncount as \"wear\"? Now that's a harsh standard, harsher even than the current British standard. Yet it's the standard many of our old-time collectors (and dealers) grew up with, and it's the standard the price guides from the 1960s would have used.\nSo, the short answer: America has had more gradeflation than us.\nDon't say \"infinitely\" when you mean \"very\"; otherwise, you'll have no word left when you want to talk about something really infinite. - C. S. Lewis","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7b703948-6968-4668-84be-df297eb8f677>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"How fast do objects travel in Earth's orbit, and why is this speed dangerous?","answer":"Objects in Earth's orbit travel at approximately 17,000 mph. At this extreme speed, even tiny objects like flecks of paint can cause significant damage, leaving pits in spacecraft bodies. This poses a severe risk to operating satellites and can also interrupt vital satellite communications that modern society depends on for cell phones, GPS, and weather forecasts.","context":["There's a common law of our lives that we learn at a very young age. Things that come up must come back down. The issue we've been fighting for the past century is finding a way to throw ourselves up without the rapid and unfortunate descent that usually follows. We've done an exceptional job in this regard, to the point where we just returned a man from space after spending an entire year aboard the ISS.\nOur current problem lies in how little we consider the vehicles that carry our astronauts into space. SpaceX is striving to create a rocket that can carry a payload to orbit, and then land back on Earth with a majority of its original body still intact and able to be reused. This might seem unusual, as you never see a car or plane arrive at its destination missing wheels or a wing, but rockets work much differently.\nAs you can see, in a conventional rocket, the actual craft is a fraction of the size of the launch vehicle. The remainder of the craft is left to burn up in the atmosphere, or continue to orbit the earth until it becomes unstable enough to fall back down to Earth. This causes a problem for regular citizens like us for two main reasons:\nFirst, our space program is very cost intensive, due to this. According to NASA, the average cost to put one kilogram of mass into orbit is $10,000. This cost varies for different rockets, but the principle is the same. NASA requires a high level of funding to do the work they hope to perform. Congress has been steadily decreasing space funding since the space race in the 1960s, and there's small chance that this pattern will turn itself around. Therefore, unless NASA finds a way to reduce costs of space travel, we as citizens would have to pay more to expand our reach in space. For some that's worth it, however this isn't meant to be an opinionated post, just a statement of fact.\nSecondly, the pieces of the rockets that stay in orbit don't leave immediately, it can take years for the orbit to become unstable enough to enter the atmosphere. This means that the debris from rockets will remain in space long after the spacecraft has completed it's mission. In the GIF below, we can see how the problem of space debris has become a huge problem that is continuously getting worse.\nA higher concentration of debris creates a higher chance that an operating system in orbit will be struck, and the damage from a collision such as this one would be extremely severe. Orbiting objects are travelling around 17,000 mph. At that speed, even flecks of paint can leave pits in the body of spacecraft. Not only does the debris cause a physical risk to satellites, but the dense cloud can also interrupt communications between satellites. This matters to us, since communication between satellites is pretty much the basis of modern society. Cell phones, GPS, weather forecast, many important facets to our fast-paced society hinge on the fact that these satellites can communicate with each other.\nHow again does SpaceX come into this? Their idea, to create what is essentially a reusable rocket, would cut debris down immensely. Not only would debris be reduced, but the cost of launching a payload would be drastically reduced as well. While all of this sounds well and good, an idea is just an idea, until it's actually put to the test. If you watch the following video, you can see that SpaceX couldn't agree more.\nSpaceX has performed landings on ground before, but never a barge. This opens up the possibility of launching to wherever in orbit the objects need to be, with no worry about whether or not the ground will be beneath the rocket when it touches back down.\nWhy should we care that SpaceX landed their rocket on a ship? You don't have to, but understand how much it means to our world when technology improves, when the price of spaceflight is reduced to a level completely unheard of in today's time.To me, that's something worth celebrating."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2607940a-fb4a-4bb2-a8db-27fad3ee142e>"],"error":null}
{"question":"What are the sport fishing attributes of the largemouth bass, and how do recreational activities affect coral reefs where these fish might be found?","answer":"The largemouth bass (Micropterus salmoides) is the most popular game fish in the United States, supporting a recreational fishery worth millions of dollars. They are highly adaptable fish that can thrive in various warm-water habitats, from small creeks to large rivers and reservoirs. As for recreational impacts on coral reefs, activities like SCUBA diving and snorkeling can cause coral breakage and tissue damage through direct contact such as walking, touching, kicking, or standing. Additionally, recreational boats can harm reefs through negligent anchoring, discharge of fuel, human waste, and grey water, while also potentially spreading invasive species through hull fouling.","context":["Taxonomic name: Micropterus salmoides (Lacepede, 1802)\nSynonyms: Aplites salmoides (Lacepède, 1802), Grystes megastoma Garlick, 1857, Huro nigricans Cuvier, 1828, Huro salmoides (Lacepède, 1802), Labrus salmoides Lacepède, 1802, Micropterus salmoides (Lacepède, 1802), Perca nigricans (Cuvier, 1828)\nCommon names: achigã (Portuguese), achigan (French), achigan à grande bouche (French), American black bass (English), bas dehanbozorg (Farsi), bas wielkogeby (Poland), bass (English), bass wielkgebowy (Polish), biban cu gura mare (Romanian), black bass (English), bolsherotnyi amerikanskii tscherny okun (Russian), bol'sherotyi chernyi okun' (Russian), buraku basu (Japanese), fekete sügér (Hungarian), forelbaars (Dutch), forellenbarsch (German), green bass (English), green trout (English), großmäuliger Schwarzbarsch (German), huro (Spanish), isobassi (Finnish), khorshid Mahi Baleh Kuchak (Farsi), lakseabbor (Norwegian), largemouth bass (English), largemouth black bass (English), lobina negra (Spanish-Mexico), lobina-truche (Dominican Republic), northern largemouth bass (English-Canada), okounek pstruhový (Czech), okuchibasu (Japanese), Öringsaborre (Swedish), Ørredaborre (Danish), ostracka (Czech), ostracka lososovitá (Slovak), perca Americana (Spanish), perche d'Amérique (French), perche noire (French), perche truite (French), persico trota (Italian), stormundet black bass (Danish), stormundet ørredaborre (Danish), tam suy lo ue (Cantonese), zwarte baars (Dutch)\nOrganism type: fish\nMicropterus salmoides (bass) has been widely introduced throughout the world due to its appeal as a sport fish and for its tasty flesh. In some places introduced Micropterus salmoides have affected populations of small native fish through predation, sometimes resulting in the their decline or extinction. Its diet includes fish, crayfish, amphibians and insects.\nDorsal spines (total): 10-10; Dorsal soft rays (total): 12-14; Anal spines: 3-3; Anal soft rays: 10-12; Vertebrae: 30-32. Mouth large; maxillary extending beyond the eye. Pelvic fins not joined by a membrane. Green to olive dorsally, milk-white to yellow ventrally, with a black band running from the operculum to the base of the caudal fin. Caudal fin rounded. Caudal fin with 17 rays .\" (FishBase, 2003)\nlakes, water courses\nInhabits clear, vegetated lakes, ponds, swamps. Also in backwaters and pools of creeks and rivers. Prefers quiet, clear water and over-grown banks.\" (FishBase, 2003). Largemouth bass are highly adaptable fish, able to thrive in virtually every warm-water habitat, from small creeks to large rivers to huge reservoirs. About the only thing that limits them is cold annual water temperatures (<10C) or low pH (<6), both of which presumably inhibit reproduction, since adults can survive in both habitats, but populations will not persist.\nIntroduced bass usually affect populations of small native fishes through predation, sometimes resulting in the decline or extinction of such species (Minckley 1973, in Fuller, 1999). Studies have shown that largemouth bass are capable of displacing native species, even predatory species such as northern pike.(USGS-CERC, 2004)\nFisheries; minor commercial, aquaculture; commercial, gamefish, aquarium: show aquarium. (FishBase, 2003)\nMicropterus salmoides is the most popular game fish in the United States; a recreational fishery that is worth millions of dollars. Preyed upon by herons, bitterns, and kingfishers. Excellent food fish .\" (FishBase, 2003)\nNative range: St. Lawrence and Great Lakes, Hudson Bay (Red River), and Mississippi River basins from southern Quebec to Minnesota and south to the Gulf; Atlantic Slope drainages from North Carolina to Florida; Gulf Slope drainages from southern Florida into northern Mexico (Page and Burr 1991, in Fuller, 1999).\nKnown introduced range: UK, Europe, Russia, Middle East, North Africa, Continental US, Caribbean territories, South America, Asia, Southeast Asia, Hawai‘i, Mauritius, Madagascar, Fiji, Guam, New Caledonia and the US Virgin Islands. (FishBase, 2003)\nIntroduction pathways to new locations\nAquaculture: This species has been an important sport fish for many years and as such has been stocked widely in areas where it is nonindigenous. (Fuller, 1999)\nOther: Fishing and Angling\nFood habits of Micropterus salmoides are very diverse, but mainly consist of fish or invertebrates. Sometimes cannibalistic. Does not feed during spawning; as well as when the water temperature is below 5°C and above 37°C ( FishBase, 2003). Well-known communities involve either LMB and bluegill (Lepomis machrochirus) or LMB and shad (Dorosoma spp). Much work has been done on the dynamics of these two communties. The LMB-BG communities tend to be more in the northern natural lakes, whereas the LMB-shad communities are more common in large southern reservoirs.\nThe male which becomes aggressive and territorial builds the nest on muddy bottoms of shallow water. A female may spawn with several males on different nests. The male guards and fans the eggs. (FishBase, 2003)\nSpawning takes place spring to summer or when temperature reaches 15°C. Adults mate between the age of 5-12 years (FishBase, 2003).\nThis species has been nominated as among 100 of the \"World's Worst\" invaders\nReviewed by: Dr. Steve Sammons M. Department of Fisheries and Aquaculture, Auburn University. USA\nCompiled by: IUCN/SSC Invasive Species Specialist Group (ISSG)\nLast Modified: Tuesday, 11 April 2006","Beyond threats associated with climate and ocean change, coral reefs are also affected by various local and regional threats. These threats may occur alone or synergistically with climate change adding to the risks to coral reef systems.\nOverfishing and Destructive Fishing\nUnsustainable fishing has been identified as the most pervasive of all local threats to coral reefs. ref Over 55% of the world’s reefs are threatened by overfishing and/or destructive fishing. Overfishing (i.e., catching more fish than the system can support) leads to declines in fish populations, ecosystem-wide impacts, and impacts on dependent human communities. Destructive fishing is associated with some types of fishing methods including dynamite, gill nets, and beach seines. These harm coral reefs not just through physical impacts but also through by-catch and mortality of non-target species including juveniles. Read more about threats and management strategies in the Reef Fisheries Toolkit.\nTraditionally, impacts from wastewater pollution have been associated with human health, but the detrimental effects of wastewater pollution on marine life – and the indirect impacts they have on people – cannot be overlooked. Wastewater transports pathogens, nutrients, contaminants, and solids into the ocean that can cause coral bleaching and disease and mortality for coral, fish, and shellfish. Wastewater pollution can also alter ocean temperature, pH, salinity, and oxygen levels disrupting biological processes and physical environments essential to marine life.\nOther sources of pollution to coral reef waters include land-based pollution associated with human activities such as agriculture, mining and coastal development leading to the discharge or leaching of harmful sediments, pollutants, and nutrients. Marine-based pollution associated with commercial, recreational, and passenger vessels can also threaten reefs by discharging contaminated bilge water, fuel, raw sewage, and solid waste, and by spreading invasive species. Learn more in the Wastewater Pollution Toolkit or in the Wastewater Pollution Online Course.\nMore than 2.5 billion people (40% of the world’s population) live within 100 km of the coast, ref adding increased pressure to coastal ecosystems. Coastal development linked to human settlements, industry, aquaculture, and infrastructure can cause severe impacts on nearshore ecosystems, particularly coral reefs. Coastal development impacts may be direct (e.g., land filling, dredging, and coral and sand mining for construction) or indirect (e.g., increased runoff of sediment, sewage, and pollutants).\nTourism and Recreational Impacts\nRecreational activities can harm coral reefs through:\n- Breakage of coral colonies and tissue damage with direct contact such as walking, touching, kicking, standing, or gear contact that often happen with SCUBA, snorkelling, and trampling\n- Breakage or overturning of coral colonies and tissue damage from negligent boat anchoring\n- Changes in marine life behavior from feeding or harassment by humans\n- Water pollution by tour boats through the discharge of fuel, human waste, and grey water\n- Invasive species which can be spread through transportation of ballast water, hull fouling of cruise ships, and fouling from recreational boating\n- Trash and debris deposited in the marine environment\nCoral disease is a naturally occurring process on reefs, but certain factors can exacerbate disease and cause outbreaks. Coral disease outbreaks can lead to an overall reduction in live coral cover and reduced colony density. In extreme cases, disease outbreaks can initiate community phase-shifts from coral- to algal-dominated communities. Coral diseases can also result in a restructuring of coral populations.\nDisease involves an interaction between the coral host, a pathogen, and the reef environment. Scientists are learning more about the causes of coral disease, especially in terms of identifying the pathogens involved. To date, the most infectious coral diseases are caused by bacteria. Transmission of coral diseases can be facilitated in areas of high coral cover ref as well as through coral predation, as predators can act as vectors by oral or fecal transmission of pathogens. ref\nThe causes of coral disease outbreaks are complex and not well understood, although research suggests that important drivers of coral disease include climate warming, land-based pollution, sedimentation, overfishing, and physical damage from recreational activities. ref\nOn coral reefs, marine invasive species include some algae, invertebrates, and fishes. Invasive species are species that are not native to a region. However, not all non-native species are invasive. Species become invasive if they cause ecological and/or economic harm by colonizing and becoming dominant in an ecosystem, due to the loss of natural controls on their populations (e.g., predators).\nPathways of introduction of marine invasive species include:\n- Ship traffic, such as ballast water and hull fouling\n- Aquaculture operations (shellfish aquaculture is responsible for the spread of marine invasive species through global transport of oyster shells or other shellfish for consumption)\n- Fishing gear and SCUBA gear (through transport when moving from place to place)\n- Accidental discharge from aquaria through pipes or intentional release\nSargassum are a type of brown, fleshy macroalgae that can have detrimental ecological and economic impacts on coral reefs when overabundant.\nIn the Indo-Pacific, high percent cover of Sargassum is common on degraded coral reefs and often represents a phase-shift from a coral to algae-dominated reef system. ref Their reproductive biology and morphology make them excellent colonizers of free space and particularly resilient to disturbances such as tropical storms. ref When overabundant, they can negatively impact the reef by shading, limiting space available for coral larvae to recruit, and transmitting pathogens. ref\nIn the Atlantic, two species of floating sargassum, S. natans and S. fluitans, are responsible for causing large mats of algae blooms which are particularly harmful and prevalent on the Caribbean and West African coastlines. ref Floating algae mats are naturally prevalent in the Northern Atlantic and provide many ecological benefits such as habitat, food, and nursery grounds to many species of fish, crustaceans and even sea turtles. ref However, in the last ten years, a shift in oceanic currents has led to an algae invasion in coral reef areas, causing reduced sunlight required by corals and anoxic and hypoxic conditions on reefs, as well as poor conditions on beaches that are detrimental to the tourism industry. ref\nCoral predators (or 'corallivores') are naturally occurring organisms that feed on corals for their polyps, tissue, mucus, or a combination of the above. Such predators typically include echinoderms (starfish, sea urchins), mollusks (snails), and some fish.\nCorallivory is a common process that, under normal conditions, allows for natural turnover in the ecosystem. However, when these predators are overly abundant (e.g., outbreak conditions), they can cause significant declines in coral cover.\nCommon coral predators include:\n- Crown-of-Thorns starfish (COTS), which are found throughout the Indo-Pacific region, occurring from the Red Sea and coast of East Africa, across the Pacific and Indian Oceans, to the west coast of Central America. COTS can be a major driver of coral loss in the Indo-Pacific, particularly under outbreak conditions.\n- Drupella snails, which are commonly found living on corals in reefs throughout the Indo-Pacific and Western Indian Ocean.\n- Coralliophila snails, which are often more problematic for Caribbean reefs, although some species are prevalent in the Pacific."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:c2f3928e-94c4-48d8-b2f0-6287016ccc2c>","<urn:uuid:6dd87468-19aa-450b-b9cc-46d264a1ee33>"],"error":null}
{"question":"Could you please compare the time constraints for eye donation versus tissue donation after death? I need to understand the key differences in timing for both procedures.","answer":"Eye donation must be done within 6 hours of death, while tissue donation has a longer window - tissues can still be harvested up to 24 hours after death if the body is refrigerated within 6 hours. For eye donation, the collection procedure itself is quite quick, taking only 10-15 minutes. In contrast, the complete tissue donation procedure can last between 8 and 28 hours. For both types of donations, timing is critical but tissue donation allows for more flexibility in terms of when and where the donation can take place.","context":["Eye & Organ Donation\nTo make a decision of donating Eyes after death is not an easy one. A lot of support and motivation is required for a person to make up his/her mind to pledge eyes. It is also important that the relatives are informed about the donor's will to pledge and provide consent for the same. Eye donation is considered as one of the noblest of all causes. It is a great feeling when we donate our eyes to a person who has lived in darkness and brighten their lives to see the world. From darkness to light is the most precious and biggest of all gifts a person can get. It is a blessing not just for the patient but for the donor too. So we request you to be a part of this noble cause and lighten people's lives and help them to lead a bright life ahead.\nIn India, the number of curable blind people exceeds more than 7 lakhs. More than 30,000 new victims are added each year to the total list of patients to be cured. Much of the blindness in our country is treatable, or preventable. In our country there are more than 80 lakhs deaths happening every year but sadly the corneal donations do not exceed a few thousands. Lack of awareness and motivation among the general public and social and religious taboos are the main reason for this.\nThe cornea is the transparent and thin outer layer and it is the main focusing element of the eye.\nVision will be dramatically reduced if the cornea becomes cloudy from disease.\nEyes may be donated only after deathWhy Eye Donation?\nEye Donation shows your responsibility to society as a caring, committed citizen.\n- Your eyes can live even after death.\n- More importantly, we can light the life of two blind people by donating our eyes after our death.\n- Not to mention, eye donation is one of the noblest of all causes.\nWho can Donate Eyes?\nPractically anybody can donate from the age of 1 year. There is no upper age limit. Even if the deceased has medical history of hypertension, diabetes, asthma, tuberculosis etc., even spectacle wearers and people who have undergone cataract operation can donate eyes.\nWho can't Donate Eyes?\nPersons who were infected with or died from AIDS, Hepatitis B or C, rabies, septicemia, acute leukemia, tetanus, cholera, meningitis or encephalitis cannot donate eyes.\nHow to Contact Nearby Eye Bank?\nA special number 1919 (BSNL) has been allotted for eye banks. Most of the eye banks all over the country have this number, once information for eye donation is given here; the eye bank sends its team to collect the eyes. By calling this number details about eye donation can also be provided.\nWhat is the procedure to Donate Eyes?\n- Eye donation is only after death and should be collected within six hours of death.\n- Eyes (corneas) will be collected at the donors place at any time in the day or night.\n- Collection of eyes will take only 10-15 minutes and will not disfigure the face of the donor.\n- A trained physician using sterile procedure removes the eyes.\n- Artificial eyes are fitted in place to ensure no scar or disfigurement of the face.\n- 10 cc of blood sample is also collected from the donor for testing.\nIt is very important to keep the following instructions:\n- Close the eyelids of the deceased\n- Place a polythene cover with few ice cubes on forehead.\n- Wrap some ice cubes in damp cotton, & keep them on the eyes. This prevents the tissue from drying up, & helps keep it fresh.\n- Switch off the fan.\n- Raise the head of the deceased slightly by placing a pillow underneath.\n- If possible apply antibiotic eye drops periodically to prevent infection.\n- Inform the nearest eye bank, dial immediately 1919 (BSNL).\n- Give the correct address with specific landmarks and telephone number to enable the eye bank team to locate the place easily.\n- If the death certificate from the physician is available, keep it ready.\n- Eye donation can be done only with the written consent of the next of kin in the presence of two witnesses.\nThe recipients are chosen from the eye bank`s waiting list and called for corneal transplant. The donor family should be happy knowing that the eyes have been used to restore the vision of two blind persons. A 'LETTER OF APPRECIATION' along with an EYE DONATION CERTIFICATE is sent to the Donor's family from the eye bank. Neither the patient knows whose cornea is used for him, nor do the relatives of the donor know who has received the cornea. This information is strictly confidential.\nThe gift of sight that will dramatically improve the quality of life for a person who has otherwise been lived a life of total darkness. Eye donation is one of the few causes that have received support from the famous and beautiful celebrities.\nAnil Kumble, Amitabh Bachchan, Jaya Bachan, Aishwarya Rai, Revathi, Yukta Mookhey and Sunil Shetty are the few among them who pledged their eyes.\nIf they do why can't we?\nPledge your eyes now\nFor more details: www.donateeyes.org\nOrgan donation is a gracious act; it reaffirms our faith in humanity. For a patient with kidney failure an alternative such as dialysis is available till an organ becomes available, but for a patient with liver or heart failure; the only hope of living may be to have an immediate transplant. About 75% of those undergoing dialysis can be removed from their treatment if they are able to get a kidney transplant.\nWhat is organ donation?\nIt means that a person pledges during his lifetime that after death, organs from his/her body can be used for transplantation to help terminally ill patients and giving them a new lease of life.\nThere are two ways of Organ donation:\nLiving related donors: only immediate blood relations (brother, sister, parents & children) can donate as per the Transplantation of Human Organ Act 1994.\nCadaver Organ donor: can donate all organs after brain death.\nWhat is brain death?\nIt is the irreversible and permanent cessation of all brain functions. Brain can no longer send messages to the body to perform vital functions like breathing, sensation, obeying commands etc. Such persons are kept on artificial support (ventilation) to maintain oxygenation of organs so that the organs are in healthy condition until they are removed. Most cases of brain death are the end result of head injuries, brain tumors patients from Intensive care units. Organs of such patients can be transplanted in organ failure patients to provide them a new lease of life.\nOnce brain death is confirmed by doctors, the patient will be certified dead and the ventilator will be switched off. It is during this time - after a patient is certified dead - that medical staffers approach family members for a possible organ donation. An organ donor, despite having registered and given a card, is not necessarily an organ donor, as the donor's organs could not be removed without the family's consent. The hospital authorities need the family's agreement before they can start the process. Any objections can delay or prevent the organ donation process. Hence, it is important for those who register as organ donors to share such information with their family.\nIf the family agrees for donation, the brain-dead patient is supported by ventilator with tests conducted to check on the various organs and is taken to the operation theatre to switch off the ventilator and harvest the organs.\nIf the family disagrees, the ventilator will be switched off at the ICU itself.\nOrgan donation is one of the greatest things a person can do. This is also one way for us to repay our debt in this world.\nThe success of organ donation programme reflects a society`s triumph and its attitude towards fellow human beings. It is an act of giving, recycling and rebirth. Death is not the end it is a new beginning. We need to understand and accept this philosophy. Walter Scott the famous novelist said - `Death is not the last sleep. It is the final awakening.'","If you donate organs and tissues after death, you can help persons who are very sick. They can keep living if they receive a donor kidney or liver. And thanks to the eye tissue of a donor, they can see again. In the Netherlands, more than 1,000 persons are on an organ waiting list. About 150 individuals die every year while waiting for an organ. A deceased donor donates three organs on average, so that every year these organ donors save the lives of about 800 persons. One would think that this takes care of everyone on the waiting list, but that is not the case. The waiting list just keeps getting longer.\nNo, not everyone can donate their organs or tissues after death. A doctor will assess upon your death whether your organs or tissues are suitable for donation.\nBut what everyone can do is specify their choice in the Donor registry – even if you are sick, sometimes even if you have (or have had) cancer, have received a blood transfusion or are taking medications. It is always a good thing to specify your choice. An organ or tissue that is damaged by disease or medication can sometimes become ineligible for transplantation, but other organs or tissues may still be suitable.\nThings can also change in the future, as medical science is able to solve more issues. While today you may not able to donate because of a disease, in a few years that same disease may no longer constitute a problem.\nYes, you can. You can donate if you are sick or taking medications. Upon your death, the doctor will assess which organs or tissues can still be used for a patient. Sometimes organs or tissues are no longer suitable for transplantation, for instance due to damage or disease, but other organs or tissues may still be suitable for transplantation.\nNo, you yourself cannot decide who gets your organs or tissues after your death. That is stipulated by law. One reason for it is that in the Netherlands, everyone must be able to get the same level of healthcare. This is why you cannot be the one determining who you will or will not donate to. An organ goes to the patient on the waiting list who needs it most urgently, and who is the best fit for the organ.\nEveryone can specify their choice in the Donor registry. Whether you are homosexual or heterosexual makes no difference, so you may be eligible to donate organs.\nYes, even if you have an unhealthy lifestyle, it can be helpful to give a ‘yes’ in the Donor registry. A person on the waiting list for a new organ can die if the organ does not arrive on time. Lungs of a smoker may be better than the person’s own lungs, and the liver of a drinker can sometimes help a liver patient. So if you smoke or drink, it doesn’t necessarily mean you cannot donate your organs. What is important is that the organ still be suitable for transplantation. A doctor will properly assess this in advance.\nOrgan and tissue donation can be done from birth to old age. A doctor will assess the suitability of every organ or tissue. Skin donation is only possible from age 20. The heart of an 80-year-old is generally no longer suitable for donation, but that person may still be able to donate kidneys.\nYes, all the organs and tissues that you can donate are listed on the Donor form. You may want to donate some organs and tissues but not others. You can specify this on the form.\nOrgan donation can only be done when people die at the hospital, for example because of a severe stroke or an accident. In this way, a person may be able to donate organs after their death. Authorisation is always needed from the donor or from their family. Only when it is certain that a patient is going to die, does a doctor take a look in the Donor registry to see the choices that were made.\nDid you give a ‘yes’ in the Donor registry? Or has your family said ‘yes’? Only then can the hospital start the testing. The physicians may take an ECG or blood tests, and ensure that the body stays at the right temperature. Organ donation can take between 10 and 24 hours, sometimes even longer. Did you give a ‘no’ in the Donor registry? Then there will be no donation and no testing.\nDonation of tissues works out more often than donation of organs. In such cases it doesn’t matter whether you die at home or at the hospital. Special teams harvest the tissues at a hospital or another suitable space, following strict rules.\nAfter a patient dies, a doctor takes a look in the Donor registry to see the choices that were made. Have you not specified a choice? In that case, your family will decide about donation. Has permission been given for donation? In that case, the doctor will report the donor to the Dutch Transplant Foundation, and the donation process will get started. Altogether, tissue donation takes between 8 and 28 hours.\nAfter your death, you can donate the following organs and tissues:\nOrgans: liver, heart, lungs, pancreas, small intestines, kidneys\nTissues: eye tissue, heart valves, large vessels, skin, bone, cartilage and tendons\nThe chances are not that large. Organ donation is quite exceptional and does not happen often. How come? An organ donor must always die at the intensive care unit (ICU) of a hospital and be receiving artificial respiration, and the organs must still be suitable for transplantation.\nIn 2018, 273 persons gave organs after their death. Every year, about 153,000 people die. This means that the chances of becoming an organ donor are 1 in 560.\nDefinitely. Organs and tissues are only harvested when doctors are 100% sure that a person has died. Death is diagnosed according to strict rules, and this is done by several physicians.\nDonation and family\nNo. Family cannot change your choice just like that. The doctor will almost always honour the choice you made in the Donor registry. That is what you wanted. If there is a very good reason, a doctor may decide differently.\nAn organ or tissue donation must be done with the utmost care, so it takes time. Altogether, organ donation can take between 10 and 24 hours, sometimes even longer.\nThe time between diagnosing brain death and harvesting organs is between 4 and 12 hours. If the heart and blood circulation have stopped, there is less time. As soon as blood is no longer circulating through the body, organs quickly become less suitable for transplantation. This is why harvesting of organs begins quickly after death is diagnosed.\nIf the body of a tissue donor is refrigerated within six hours, tissues can still be harvested for transplantation up to 24 hours after death. The tissue donation procedure can thus last between 8 and 28 hours.\nCertainly. There is always someone for the family at the hospital. During the entire duration of the donation process, a hospital staff member (such as the transplant coordinator) will be in touch with the family.\nYes. Removing organs and tissues is always done with great care. Doctors consider it very important for a donor to look presentable after the operation. They do not remove anything from places that are visible when a body is viewed. Stitches with bandages are applied on operated areas.\nYes, there is always time to say goodbye. Family can be with the donor before and after the donation. After the donation procedure, the family itself determines what happens with their loved one. The family can also choose the viewing location, at a funeral centre or at home.\nYes, you can. A funeral or cremation can take place on the day chosen by the family and does not need to be postponed because of the organ or tissue donation.\nNo, there are no costs involved. The family of the donor doesn’t pay anything for donation.\nDonation and religion\nDonation and religion can be compatible. Most faiths approve of organ donation. If you have any doubts about this, it is important that you discuss it with someone – perhaps a pastor, an imam or someone else knowledgeable about your religion – so that you can arrive at a joint perspective about organ and tissue donation.\nYou can only be included in the Donor registry if you yourself specified your choice in the Donor registry. Anyone from the age of 12 can specify their choice in the Donor registry. There are four choices:\nChoice 1: Yes, I grant permission\nYou want to become a donor. You can donate the following organs: pancreas, intestines, heart, liver, lungs and kidneys;\nand the following tissues: blood vessels, bone, heart valves, cartilage, tendons, skin and eye tissue.\nYou may want to donate some organs and tissues but not others. You can specify this on the form.\nChoice 2: No, I do not grant permission\nYou do not want to become a donor.\nChoice 3: My partner or family will decide\nUpon your death, your partner or family are allowed to make the choice.\nChoice 4: The person I have appointed will decide\nYou want someone else to decide for you after your death.\nEveryone in the Netherlands will receive information from the government about the new law.\nIf after 1 July 2020, you still haven’t specified a choice in the Donor registry, you will receive a letter asking whether you want to specify your choice.\nIf you do not specify anything after the first letter, you will receive a reminder after six weeks.\nIf again you do not specify anything, ‘No objection against organ donation’ will be listed under your name. You will get another letter to confirm this. ‘No objection against organ donation’ means that your organs can go to a patient after your death.\nIt remains important that you yourself specify a choice about organ donation. If you have already done so, your choice will remain valid.\nThe choices are the same as in the current law. Something will change only if you haven’t made a choice yet. Nothing changes even after you receive a letter after 1 July 2020 in which you are asked to specify a choice.\nIf you do not specify a choice in the Donor registry, ‘No objection against organ and tissue donation’ will appear under your name. This means that your organs can go to a patient after your death. The doctor at the hospital will discuss this with your family. If your family is very sure and can explain to the doctor that you really did not want to be a donor, then you will not be a donor. It is therefore very important for your partner and family to know what your choice is, and especially for you yourself to specify it.\nIt is important for you to specify in the Donor registry whether you do or do not want to become a donor. You can specify this choice in three different ways:\nWhen it comes to organ and tissue donation, a person must be able to understand what it is about. The person must also be able to understand the consequences of the choice they make. If the person cannot do that, then they are legally incapable for organ donation.\nAt present, a legally incapable person cannot be a donor. That will change when the new donor law goes into effect on 1 July 2020.\nWhen an organ or tissue is harvested, a doctor can discover that it is not suitable for transplantation after all, for instance because the quality of the organ or tissue is not good enough or because an unexpected damage or disease is found. In such cases, the organ can be used for scientific research. Through this research, physicians learn more about transplantation. This is different than donating your entire body to science.\nSince 1 October 2019, the donor pass is no longer issued. A doctor is not going to look for your pass when you pass away; a doctor is obliged to search the Donor Register, where your choice is registered. For that reason, the donor pass is no longer sent together with a confirmation of the registration."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:76a1831a-8994-4c5a-bcab-820a1978f0d2>","<urn:uuid:7c861db9-4840-4731-a188-4e0ecdf4fefd>"],"error":null}
{"question":"Could you explain the specific advantages of using monofilament fishing line versus fluorocarbon for various fishing techniques, and what are the environmental considerations for choosing between these line types?","answer":"Monofilament and fluorocarbon each have distinct advantages for different fishing techniques. Monofilament floats and works well for topwater presentations, has better castability due to being more limp, and is ideal for deep crankbaiting with its 10-pound test offering enough stretch to let fish engulf the lure. It's also preferred for shallow crankbaits in stained water where visibility isn't an issue. Fluorocarbon, conversely, is nearly invisible underwater, has less stretch than mono, and provides better sensitivity for techniques like spinnerbaits and shaky head fishing. Regarding environmental impact, both types of line pose significant threats to wildlife if not properly disposed of. They can cause fatal entanglement of birds and small mammals, lead to infections when cutting into animals' feet and legs, and remain hazardous for years when left in the environment. The responsible approach is to always carry out used line and either dispose of it properly or send it to Berkley Recycling for recycling.","context":["Choosing the right fishing line\nFew aspects of bass fishing have changed more rapidly, or more dramatically, in recent years than the development of new fishing lines. Now, anglers are faced not only with choosing which strength of line the need but also which type of line.\n“Each type of line we use today, braid, fluorocarbon or monofilament, has both advantages and disadvantages,” explained veteran tournament pro Terry Scroggins, who’s been climbing the line-learning curve since he began fishing braids more than a decade ago. “For example, fluorocarbon is nearly invisible underwater and has less stretch than monofilament, and braid has a very thin diameter and extra strength, but virtually no stretch.\n“I believe every bass fisherman has to look at his own style of fishing and the conditions he’s actually facing, and then choose a line that best meets those conditions. The chances are, if you fish very often with several different techniques, you’re going to use all three types of lines.”\nScroggins uses all three, on occasion he combines braid with either a monofilament or fluorocarbon leader to suit the technique he’s using. Here’s how he rigs his rods for a variety of different lures and presentations:\nTopwaters – “Being from Florida, I have always fished a lot of prop baits like a Devils Horse and Boy Howdy, and for these types of lures I prefer 40 pound braid with a four to five foot leader of 15 pound monofilament.\n“Monofilament floats, and with prop baits we often use a very slow presentation during which the lure may sit motionless on the surface for 15 or 20 seconds, so a floating line gives the lures better action. The stretch in the mono also acts as a type of shock absorber for the non-stretching braid.”\nLipless Crankbaits – “I prefer 40 pound braid with a four- to five-foot leader of 15 to 17-pound fluorocarbon. “That’s because I frequently use these lures over hydrilla or milfoil, and many strikes come when you rip the bait out of that vegetation. The fluorocarbon has very little stretch and recovers quickly, while the braid helps cut through the grass.\n“I use the same combination when I’m yo-yoing these lures. Strikes come as the bait is falling, and the braid gives you instant hook-setting; you almost never lose a bass this way.”\nDeep Crankbaiting – “Here I use 10-pound monofilament. It’s strong enough to handle big bass, and the small line diameter allows the lure to reach maximum depth. It has just enough stretch to let the fish engulf the lure for a good hook-set, too.\n“Remember, we used monofilament for years before braids and fluorocarbons were developed, and we caught a lot of bass with it. There are still applications where I think mono out-performs these other lines. Some pros will use fluorocarbon line for deep cranking, but I like mono.”\nFlipping – “When I’m flipping, I use the ‘¾-ounce rule’. If I’m using a ¾-ounce or lighter sinker, it generally means the cover I’m fishing is not very thick nor is it very deep, so I’ll normally use 20-pound or heavier fluorocarbon for its strength and lack of stretch.\n“If I’m fishing slightly deeper and heavier cover, I’ll use 40-pound braid without a leader. Even in the clear water of a lake like Amistad, I’ll use braid because the fish are deeper in submerged timber and I need the strength of braid to get them out.”\nSpinnerbaits – “Depending on the type of cover, I’ll use 14- to 20-pound fluorocarbon. Because I’m in tight contact with the lure, I don’t think there’s a need for braid, although a number of pros do choose it when spinnerbaiting over vegetation. I prefer fluorocarbon because it’s more sensitive and I can feel the blades better.\n“We used to think monofilament was perfect for spinnerbaits, but once you get accustomed to fluorocarbon, using these lures with mono feels like fishing with a big rubber band.”\nShaky Head – “I use six-pound fluorocarbon exclusively with this technique, because I’m not trying for a big bass, but rather, for numbers of fish that are usually in deeper water. I don’t like mono because it has too much stretch.”\nShallow Crankbaits – “When I’m target fishing crankbaits in water six feet or less, I prefer 12- to 15-pound monofilament. It has better castability because it is usually more limp than fluorocarbon, and the visibility of monofilament is not really an issue because I’m usually fishing stained water.”\nSpoons – “My normal choice here is 40-pound braid with a four- to six-foot leader of 14- to 16-pound fluorocarbon. This allows me to make long casts, and because bass nearly always hit this lure while it falls, the braid/fluorocarbon combination gives good hook-sets because of the lack of stretch.”\nCasting Jigs – “With a football-style jig I work on the bottom, I prefer 12- or 14-pound fluorocarbon because of the lack of stretch and visibility, and if I’m working particularly heavy cover, I’ll use 20-pound fluorocarbon. If I’m swimming a jig, however, I’ll use braid without a leader because I’m usually working the lure shallower and want direct contact with it.”\nCarolina Rigs – “I nearly always choose 40-pound braid with a 14- to 17-pound monofilament leader. Mono floats better than fluorocarbon, and in this presentation, it should help my lure as it swims near the bottom.\n“You don’t really lose any sensitivity with mono here, because all your feel comes from your sinker. Most bites on a Carolina rig are nothing more than a ‘heavy’ feeling, and when you learn that bite, it doesn’t mater whether you’re using a monofilament or fluorocarbon leader.\n“The only real advantage I see with a fluorocarbon leader in this case is that it’s invisible. You need a shock absorber in your line system, and because fluorocarbon has such little stretch, you may realize you have to change to a lighter rod for Carolina rigging. To me, that’s a disadvantage, so my choice is monofilament.”\n“When I’m fishing my 10-inch Big Show worms, I use 16-pound fluorocarbon. Again, the advantages are low stretch and near invisibility underwater.”\nWhile most pros have chosen 65-pound braid as their standard, Scroggins feels 40 pound braid is more suitable for his fishing style. The advantage of the smaller size is that it casts easier since he can spool more of it on his reels. Even with 50-pound braid, Scroggins can practically empty a spool on a long cast, which translates into far less cranking power when a bass hits far away\n“I believe the real keys to choosing lines are analyzing both the situations you’re fishing, and then trying different lines and line sizes. Eventually, you’ll settle on some that work best for you, and they may not necessarily be the same choices I use. Each line has distinct characteristics that may or may not be an advantage in your own personal fishing style\n“The best part is that today we all have far more options to choose from than we did just a few years ago.”\n©2015 Bass Edge, Inc. All Rights Reserved.","Tangled remnants of monofilament can be recycled with Berkley\nBy Bob Berwyn\nSUMMIT COUNTY — It’s been great fun the last couple of years watching my son’s growing enthusiasm for fishing. In the warm part of the year, it’s become an important part of our father and son time. Along with the occasional thrill of catching a fish, we explore new spots and we have time to just sit and talk.\nBut one thing that’s always bothered me is the tremendous amount of garbage left behind at popular angling spots, especially the easily accessible shoreline venues along Dillon Reservoir. I’ve been harping on this to my son since he could walk and talk, and I’m proud to say he’s become quite the anti-litterbug — to the point that I’ve heard him call out people on the chairlift at A-Basin when he sees them drop a candy wrapper.\nIncluded in that shoreline debris I often find tangled wads of fishing line. In a few areas, it’s become ubiquitous. This is a big problem. Of course, the line doesn’t biodegrade, but even worse, dozens of birds and small mammals get tangled in the line and die every year in Colorado. There’s really no excuse for this.\n“Fishing line left on the bank is dangerous,” said Scott Gilmore, statewide angler education coordinator for Parks and Wildlife. “An animal can’t untangle itself from fishing line so it is often fatal.\nEarlier this summer, a kingfisher — -a bird that lives along riparian areas — -was found hanging dead in a tree along the Uncompahgre River in Montrose, hopelessly tangled in fishing line. During his career, Gilmore has seen lots of birds that have died in the same way. When a bird becomes tangled, it can’t fly, run or protect itself from predators.\n“There’s no reason to toss line on the ground,” Gilmore said. “Just stuff it in your pocket and throw it away at home.”\nSome birds use fishing line to build nests. The result is that chicks and young waterfowl end up tangled in the mess.\nFishing line also cuts into the tender legs and feet of birds, waterfowl and other wildlife. Those cuts then can become infected and result in an agonizing death for the animals. Pets can also get tangled in fishing line with a potential to cause injury.\nMonofilament line is very strong and can remain hazardous for years. Unfortunately, line can be found along reservoirs and stream banks throughout the state.\nAnglers who see line should pick it up. Also, tell youngsters and inexperienced anglers about the dangers.\n“It’s easy to perform this small service for the environment and wildlife” Gilmore said. “Carry out your own line and pick up line and other trash you see in the places you fish.”\nIf you want to recycle your old fishing line, it can be sent to: Berkley Recycling, 1900 18th Street, Spirit Lake, Iowa, 51360. Fishing and sport shops that would like to offer recycling to customers, can contact Berkley at 800-237-5539. Berkley is a fishing products company.\nFor more information about fishing in Colorado, see: http://wildlife.state.co.us/Fishing."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:b11cd001-9a17-477f-b5cb-20cae24a4aef>","<urn:uuid:8868ecc4-f636-400f-877a-b3c20ecabce7>"],"error":null}
{"question":"Could you compare the hiking opportunities between the Tongariro Alpine Crossing in the Taupō region and the Antrona valley's mountaineering routes? What are the key geological features hikers encounter in each location?","answer":"The Tongariro Alpine Crossing features volcanic alpine terrain, an active volcano, and water-filled Emerald Lakes explosion craters. In contrast, the Antrona valley offers mountaineering routes characterized by Gneiss rock formations with very high flanks over narrow valleys. The valley includes several peaks over 3000 meters, with routes ranging from III to IV+ on the UIAA scale, particularly in the Andolla group. The Tongariro Crossing provides views of Mount Ngauruhoe (Mount Doom), while Antrona valley routes offer views of Switzerland's 4000m peaks and Italy's big lakes.","context":["With some of the North Island’s most impressive landscapes right on its doorstep, the Taupō region is perfect for outdoor adventures.\nHere are the top 10 things to do in the Taupō region.\n1. Encounter geothermal wonders\nThe valley of Orakei Korako Cave & Thermal Park is a real hidden gem. Accessible only by a short ferry trip across Lake Ohakuri, admire the power and energy of Mother Nature in the geysers, hot springs and bubbling mud pools.\n2. Sample delicious New Zealand honey\nNew Zealand honey is famous for its purity and healing properties. Visit the Huka Honey Hive and take a tour that teaches you about the benefits of manuka honey, royal jelly and bee pollen. Enjoy a honey ice cream before sampling a raft of honey-based products.\n3. Soak in geothermal waters\nWith a wealth of thermal attractions and cultural experiences, the Wairakei Terraces offers something for everyone. Soak in naturally heated geothermal pools below ancient silica terraces; enjoy a guided tour of the Wairakei steam field or relax during an evening of Māori cultural experiences.\n4. Hike the Tongariro Alpine Crossing\nArguably New Zealand's greatest day walk, the Tongariro Alpine Crossing, travels through volcanic alpine terrain, hikes around an active volcano and descend into the water-filled explosion craters known as the Emerald Lakes. Enjoy views of Mount Ngauruhoe - also known as 'Mount Doom' in the Lord of the Ringstrilogy.\n5. Visit the thundering Huka Falls\nGet up close to the mind-blowing roar and rumble of the Huka Falls where New Zealand’s longest river, the Waikato, is squeezed through a ravine of hard volcanic rock. Watch 220,000 litres of water blast by every second. View the falls from above via the footbridge, or get up close with a thrilling jet boat ride.\nWith over 60 Grade 3 roller-coaster rapids, a rafting trip on the Tongariro is a must. Wind through pristine forests & volcanic landscapes near Turangi. You may be lucky and catch a glimpse of the ‘whio’, New Zealand's rare native blue duck in its traditional home.\n8. Indulge in a volcanic wine and craft beer tour\nTaupō’s free-draining volcanic soil and pure alpine water means that the wines, craft beers and cider crafted in this region are both distinctive and delicious. Discover them for yourself with a volcanic wine and craft beer tour.\n9. Cruise on Lake Taupō\nCruise the largest lake in New Zealand with a scenic boat trip to Ngātoroirangi Mine Bay Māori Rock Carvings with Chris Jolly Outdoorsand catch your own lunch. Try your hand at living off the land, test your survival skills and learn to navigate the celestial way with a hands-on interactive survival walk.\n10. Explore the Prawn Park\nWith activities that will entertain the whole family for hours, Huka Prawn Park is a must-do. Take a guided tour. Race the Boat Lake on paddleboards, water trikes and pedal boats. Embark on the parks activity trail with interactive water features. Pack a picnic or dine at the Riverside restaurant soaking in the spectacular views of the Waikato River.","Areas & Ranges\nPage Type: Area/Range\n46.05920°N / 8.11306°E\nHiking, Mountaineering, Sport Climbing, Big Wall, Via Ferrata\nSpring, Summer, Fall, Winter\n11995 ft / 3656 m\nCreated/Edited: Nov 7, 2007 / Aug 26, 2008\nObject ID: 354241\nPage Score: 87.31%\n- 24 Votes\nVote: Log in to vote\nOverviewAntrona is a lateral valley of Ossola, the great area in the north east of Piedmont that from 'Lago Maggiore' ascends to the famous Simplon pass.\nThe groove of Antrona valley, on the hidrographic right of Toce river, starts just a few kilometers at south of the city of Domodossola and ascends toward W-NW, ending below the high flanks of the peaks that divide Piedmont from the swiss region of Wallis.\nThe maximum elevation in the territory is 3656 meters at the summit of Pizzo Andolla, several other summits higher than three thousand meters are located all over the border line.\nThese higher peaks, together with the lower ones, offer the possibility of mountaineering and hiking in a savage and solitary environment.\nThe predominant rock is Gneiss so the shape of the mountain is the typical one of the Southern Western Alps, with very high flanks hanging over narrow valleys but without huge rock walls like those of granitic peaks.\nThe SE face of Loranco peak is anyhow considered the most difficult alpine rock wall of the Pennine Alps range and has an height of 650 meters.\nIn the group of Andolla other walls and ridges offer pleasant mountaineering/rock-climbing routes, on firm rocks, with classical difficulties (from III to IV+ of UIAA scale).\nDue to the proximity of Maggiore and Orta lake at SE sudden weather changes are frequent in this valley during summer and late spring.\nSolitary landscapes always attends who ventures over these fierce mountains, while the view from the summits always embrace the highest 4000s of Switzerland and the big lakes of Piedmont and Lombardy in italy.\nVal Loranco, the northern branch of Antrona with the higher summits of the area\nThe principal summits along the border with Switzerland\nGetting ThereFrom Italy.\nThe valley is easily reachable from Milan(and Malpensa airport) in less than two hours following the highway Milano-GravellonaToce and then the SS33 for Simplon pass.\nFrom Turin follow the A26 and then take the highway to Gravellona toce, after Gravellona proceed on the SS33 as from Milan.\nArrived at the exit of Villadossola follow the easy mountain road (SP67) that leads to Antronapiana(1020m), the highest village of the valley.\nFrom Brig cross Simplon pass and enter in Italy.\nFollow the main road (SS33) in direction of Milan, after Domodossola take the exit of Villadossola.\nFollow then the main road of the valley (SP67) till AntronaPiana, the principal village of the area.\nAntrona Sub-ranges and secondary valleysAfter Antrona Piana, the highest village of this area, the groove of the valley splits in two, forming Loranco valley at north and Troncone valley at south.\nA paved road leads to Cheggio(1500m), a small village at the beginning of Loranco valley and a parking is located near Lago dei Cavalli, 'horses lake', one of the big impounds of the valley and the main trailhead for all the routes of Loranco valley.\nTo get to the entrance of Troncone valley by car you can instead follow the paved road that always from Antrona Piana leads to Antrona lake, where a private road continues till the barrage of Campliccioli at 1300 meters.\nThis last private road is normally open to everybody, note anyhow that it is very narrow and no public autority will take responsibility for any accident over there.\nCampliccioli is the starting point for the routes in Troncone valley.\nThe state border between Italy and Switzerland runs along the ridge that closes the valley at NW, the mostly used passes to cross from a country to another are Andolla pass(2476m) in Loranco valley, one of the principal tobacco smuggling routes of the first half of '900 and Antrona(or Saas) pass(2807m)in Troncone Valley.\n| ||This group closes Loranco valley at east going from Fornalino pass (south) to Andolla pass (north) and signing the border with Bognanco Valley, the maximum altitude is at the top of Montalto, 2705m.|\nThese mountains offer some nice hike possibilities and the top of Montalto is normally reached without tecnical difficulties from the impound of 'Lago dei Cavalli'(Lake of the Horses).\n| ||From Andolla pass to the summit of Pizzo Bottarello (Sonninghorn) this range closes Loranco valley at north signing the border line between Italy and Switzerland.|\nThe three mountains of this group are the highest of the Valley, East to West:\n-Pizzo Andolla (Portjengrat) 3656m.\n-Pizzo Loranco (Mittelruck) 3363m.\n-Pizzo bottarello (Sonnighorn) 3487m.\nThey are not easy from the Italian side but the east ridge of Loranco (\"Lago Maggiore\" ridge) and the South ridge of Andolla are appreciated classical mountaineering routes with some good rock climbing (difficulties goes from AD+ to D, max IV+ UIAA scale) on solid rocks above the altitude of 3000 meters.\nThe east face of Loranco is moreover the hardest alpine rock face of the Pennine Alps (from TD to ED+, VI and VII of UIAA scale) and has an height of 650 meters.\nTuriggia Pozzuoli group\n| ||the southern ridge of Loranco valley separating its groove from the one of Troncone valley. The most important mountains of this ridge are Punta Turiggia (2811m) and Cima Pozzuoli(2615m).|\nAt the beginning of the last century an huge landslide detached from Cima Pozzuoli and dropped in to Troncone valley closing the water course of Rio Troncone and creating the lake that is now called Antrona lake.\nThe summits of this group are easy reachable from Loranco valley and they offers nice views. The trails are not well signed.\n'Cresta di Saas' Group\n| ||Cresta di Saas, Saas ridge in english, is the prosecution at SW after Pizzo Bottarello of Andolla group, its name is due to the fact that the summit ridge of these mountains divides Antrona Valley from Saastal in Switzerland.|\nThe highest peak is Cimone di Camposecco(3400m).\nThe east side of this range falls with cliffs and steep colouirs over the cirque of Camposecco lake, the West side descends instead gently above the highlands of Furgg Valley, in the territory of Switzerland.\nThese are rugged mountains that are rarely visited, the entire ridge cross from Cima di Saas (SW) to Pizzo Bottarello (NE) is a classical mountaineering route with incredible views towards the 4000s of Switzerland and the big Lakes in Italy.\n| ||This is the group that closes at north the long groove of Troncone Valley, going from Antrona pass(2805m) till Antigine pass(2980m).|\nThe most important summits of this group are Cingino South(3190m), Cingino N(3226m) and Stellihorn(3436m), which summit is located entirely in the Switzerland territory.\nThese mountains are usually reached by easy mountaineering routes from Antrona pass or from Monte Moro pass in Anzasca valley.\nA spectacular trekking or ski-mountaineering route that crosses the entire rige from Monte Moro pass to Antrona pass is often covered in spring and summer\nAntigine-Laugera-San Martino group\n| ||From Antigine pass these ridge closes Troncone Valley at West and South.|\nIt also delimit the border between Antrona valley and Anzasca valley.\nThe most important summits are Laugera (2995m), Pizzo San Martino(2711m) and Pizzo del Ton(2680m).\nThe only summit frequently climbed is Pizzo San Martino, easy reachable from Campliccioli lake.\nThis range offers nice trekking possibilities.\nHere after the 3D maps of the Area from East and West(Google Earth).\nThe most used starting point for the normal route to Andolla peak, the trail to reach the hut start from Cheggio, a good path leads there in 2 hours.\n|Varese hut. |\nA very small hut located at the basis of Pizzo Loranco East ridge(2500m), normally used like starting point for the \"Lago Maggiore\" ridge and the east face of Pizzo Loranco, for the South ridge of Pizzo Andolla and for crossing from Loranco to Troncone valley through the Camposecco's Coronette pass (very pleasant hike with a short Via Ferrata right above Camposecco lake). from Andolla hut by a signed trail in an hour.\nLocated in proximity of the barrage of the impound of Camposecco(2300m) and usefull for mountaineering in the Saas ridge group or for crossing to the Loranco valley through the Coronette pass.\nYou get there from campliccioli in 2/3 hours following a steep trail very hot during summer.\nAt Antigine pass(2855m), important support for mountaineering, trekking and ski-mountaineering on the western summits of Troncone valley, its acces is long and complicated from Campliccioli, It's often reached from Monte Moro pass, starting from Anzasca valley.\nA valley with many lovely lakesIn this valley there are 4 big impounds and another lake born as a consequence of a big landslide.\nThe big impounds surely deserve a visit, being the environment around them still very savage and solitary.\nLago dei cavalli, Lake of the horses\n|Lago dei Cavalli. |\nThis is an impound built at the beginning of the past century, it's located at the entrance of Loranco valley at an altitude of 1500 meters, adjacent to the small village of Cheggio.\nTo get there follow the paved road from Antronapiana (12Km).\n|Lago di Antrona. |\nAt the beginning of 1900 an huge landslide ruined from cima Pozzuoli and obstructed the the flow of Rio Troncone, as a consequence of this event the water of Troncone formed Antrona lake.\nThis lake is located at an elevation of 1050 meters.\nA restore and a camping are located in the proximity of this lake, reachable in a few minutes by car from Antrona Piana.\n|Lago di Campliccioli. |\nThis is another big impound and the starting point for all the routes of Troncone valley.\nA private road ascends to the barrage from the Lake of Antrona, this road is normally open to everybody and you can use your car, nobody will anyhow take responsibility for eventual accident on this road that is very narrow.\n|Lago di Camposecco. |\nThe highest impound of the valley (2300 meters), located under the east face of Saas Ridge.\nThe cirque of Camposecco is reachable by a trail starting at the barrage of Campliccioli lake, the name Camposecco means 'Dry field' in english and is due to the aspect of the area before the building of the barrage, A dry and hot rock sprawl exposed toward south.\nLago di Camposecco. 2300m.\n|Lago del Cingino. |\nThe second impound for elevation of the valley, at 2200 meters, closed between the south face of Saas ridge and the east flank of Cingino peaks.\nThis lake is located in a very remote position, a pleasant trail from Campliccioli crosses all the long Troncone valley and climbs to the lake.\nA tunnel under Punta La rossa permits to walk quickly from this impound to the one of Camposecco without loosing altitude and permits a pleasant trekking of one day that passes by the shore of these lakes.\nExternal LinksThe following sites are useful to find informations about the paths, the huts and the accomodations:\nThe official site of the valley."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:eebaddee-d0c3-48b8-a801-fa7f45e94b52>","<urn:uuid:78baadab-00f9-41f3-8689-8e99e8dc2082>"],"error":null}
{"question":"How do Wofati houses and Earthships compare in their approaches to natural lighting and solar heat management?","answer":"Wofati houses require at least 35% of the uphill wall's surface area to be windows, and the downhill roof must have 35% windowed surface area with gables. Earthships, meanwhile, are often designed in a horseshoe shape to maximize natural light and solar gain during winter months. Their sun-facing walls have windows that admit light and heating, with the low angle of winter sun allowing passive solar heat to be collected and stored in floors and walls. During summer, the steep sun angle prevents direct interior heating, and the structure provides radiant cooling through the earth, supplemented by passive ventilation systems.","context":["There’s a charm to underground homes and so-called ‘hobbit homes’, but many former owners of such buildings have moved out due to problems with moisture and related conditions. But, earth might be the best insulator available, and it’s definitely the cheapest. Permaculturalist Paul Wheaton, owner of the forums at permies.com as well as richsoil.com, set out to design a new version of the underground home that retained all of the charm, but with greater affordability, fewer moisture issues, and all of the benefits of using earth as a way to retain warmth.\nWhat Is A Wofati House?\nWheaton says that wofati is a portmanteau of these words:\n- Mike Oehler\nMore practically, it’s an unvarnished log cabin built into a hillside. It has big windows, an earthen roof, and plastic lining.\nThe wofati concept was first published by Wheaton in the 2000s, and as such, only a small handful have been built. Pictures below include both true wofati homes, built on the Wheaton labs property in Montana, as well as photos we’ve chosen to highlight other structures that share features with wofatis but may or may not qualify for the designation.\nWhat Do I Need To Build A Wofati House?\nAccording to Wheaton, a true wofati house meets the following criteria:\n- “Every drop of rain has a complete downhill soil path and never encounters a roof edge.\n- There are two layers of membrane. The lower layer, which hugs the structure, and the upper layer, which defines the thermal mass that surrounds the structure.\n- The uphill side has an open trench to move water around the structure.\n- The uphill side has a roof that extends at least 5 feet beyond the exterior wall.\n- There is at least 8 inches of dirt between the two layers of membrane. There is at least 16 inches of dirt on the top layer of membrane.\n- The inner pole structure is made of logs.\n- No treated wood is used in any of the structure.”\nA wofati house needs to have all of those components, plus the following:\n- At least 35% of the uphill wall’s surface area is windows.\n- The house can’t be airtight.\n- The downhill roof has gables and 35% windowed surface area.\n- The house isn’t underground.\nWheaton has defined other kinds of wofati construction, including wofati coolers, wofati freezers, and wofati animal shelters.\nExamples Of Wofati Houses\nThe inspiration for the wofati design was made by Mike Oehler in the 1970s for $50. This video from Paul walks through that home in the 2000’s:\nOehler turned into a big spender, though—he spent $500 on renovations later in his life.\nHere’s a beta version of a replicable wofati house built in Montana by the permaculturists at Wheaton Labs:\nWofati structures are reminiscent of other underground homes, like this earthship, however the design strictly requires following rules that other structures may not follow – particularly in how rainwater and runoff is managed.\nLocated at the Findhorn Foundation, this building is the Nature Sanctuary where morning meditation takes place! While this house may not be a proper wofati home, it has many of the characteristics you might find in the entrance for one:\nWofati construction underway in Montana. As you can see, this proper wofati structure has plenty of incoming light and doesn’t reflect the damp and dark environment that you might encounter in other types of underground homes:\nSolar collectors and wind generator for earth-covered deep-green residence. Rendering and design by Greg Madeen, Architect at Madeen Architecture and Construction L.L.C. Photo from deepgreenarchitecture.com. While this structure is cool and likely shares some characteristics with wofatis, the way it is built into a hillside most likely disqualifies it from wofati status. In Wheaton’s model, the rainwater runoff pattern for this structure would be troublesome:\nA cozy, rustic lakeside home with green roof that has many design elements of a finished wofati:\nA beta version of a wofati house from Wheaton Labs:\nThe same house from another angle:\nInside Weaton Labs’ wofati house:","The Earthship is a completely independent globally oriented dwelling unit made from materials that are indigenous to the entire planet. The major structural building component of the Earthship is recycled automobile tires filled with compacted earth to form a rammed earth brick encased in steel belted rubber. This brick and the resulting bearing walls it forms is virtually indestructible. In an Earthship, there is usually no need for electricity nets, sewers, or heating systems. This means there are no costs (for electricity, water, heating and so on) other than some maintenance of equipment. Using recycled materials for construction (typically vehicle tires) an Earthship will:\n- provide the people living in it with food all year through;\n- provide the people living in it with clean water;\n- provide the people living in it with heating and cooling, without needing another source than the sun;\n- provide electricity as necessary (through solar panels and/or wind energy).\n- provide the people living in it an environment that connects them with nature, which is inside the house (water, earth, plants, sunlight)\n- provide the people living in it with a safe environment: an Earthship cannot catch fire and earthquakes have little or no effect on the system.\nThe Earthship concept homes are built using various strategies of sustainable housing. These homes are more that just a structure, they are a system of materials used together to perform the task of providing a home for people. They are flexible in design and use recycled building materials from local sources. They do not need to depend on municipal services such as electricity, water or sewage. Electricity and back-up heating are produced from renewable energy sources such as the wind and the sun. The sole source of water is provided from rain and snow melt through a roof catchment system. Waste is contained and processed on site. Passive solar and thermal mass architecture are combined for self-contained heating without the use of fossil fuels. In addition, Earthship building techniques are low-tech and easy to learn. These factors can keep building costs low while looking after the environment at the same time. The entire system working together is called a Biotecture.\nFor example, windows on sun-facing walls admit lighting and heating, and the buildings are often horseshoe-shaped to maximize natural light and solar-gain during winter months.\nFloors must be finished with stone, tile, cement, or some other material that will further enable the concept of thermal mass construction. Interior non-structural walls use recycled bottles and cans to form their shape. They are laid like bricks to form a matrix in cement. The cans and bottles serve no structural purpose. They are simply used as a method to form concrete into walls in a low-tech way using recycled materials instead of more cement or wood. These walls can be plastered over for a clean look. Many of the bottle walls are plastered, but leave a portion of the bottles exposed to allow a stained glass effect.\nDuring the winter months, the low angle of the sun allows passive solar heat to be collected through the south facing windows. It is absorbed into the floors and walls throughout the day. Later in the evening when the outside air cools, the stored heat is released. During the hot summer months, the angle of the sun is too steep to reach the interior portions of the structure. This allows for a radiant cooling effect from the earth. In addition to the thermal cooling properties, passive ventilation systems assist in maintaining a comfortable temperature. Small windows are opened along the south side to allow air in Skylights are then opened to draw the fresh air up through natural convection. For extremely hot climates, a cooling tube can be added. Incoming air is channeled through a tube buried in the earth. This taps into the cool earth temperature and draws cool air into the house.\nRainwater is collected for the water supply from the roof. The water is stored in cisterns buried next to the house. The water is then pumped and pressurized from the cistern through a filtration system before it is used for drinking, bathing or cleaning. Hot water is obtained through a solar hot water heater. On-demand heaters, powered by natural gas, are used as a back-up. Once used, the grey water is collected and filtered through an indoor planter called a botanical cell.\nThe water is used for growing various edible foods, herbs and ornamental plants. Once filtered, the grey water not used by the plants is recycled for use in flushing toilets. Black water from the toilet is treated in a septic system then passed through an intermediate botanical cell outside the home before the water reaches the septic leech field. Most often, as the landscape plants absorb most of the water, the leech field becomes unnecessary. This leech field is, however, required by building code. This system effectively uses the same water four different times.Rainwater is collected for the water supply from the roof. The water is stored in cisterns buried next to the house. The water is then pumped and pressurized from the cistern through a filtration system before it is used for drinking, bathing or cleaning.\nHot water is obtained through a solar hot water heater. On-demand heaters, powered by natural gas, are used as a back-up. Once used, the grey water is collected and filtered through an indoor planter called a botanical cell. The water is used for growing various edible foods, herbs and ornamental plants. Once filtered, the grey water not used by the plants is recycled for use in flushing toilets. Black water from the toilet is treated in a septic system then passed through an intermediate botanical cell outside the home before the water reaches the septic leech field. Most often, as the landscape plants absorb most of the water, the leech field becomes unnecessary. This leech field is, however, required by building code. This system effectively uses the same water four different times.\nThe Systems Package is the name given to all the equipment needed to run an Earthship. This includes a water organizing module for filtering collected water, a pressure tank, a grey water pump panel, batteries and a power organizing module. These components are normally packed in a small room, but easily accessible for servicing. Energy is collected by the sun via solar panels and stored in the battery system.\nA sustainable home must make use of indigenous materials, those occurring naturally in the local area, For thousands and thousands of years, housing was built from found materials such as rock, earth, reeds and logs. Today, there are mountains of by-products of our civilization that are already made and delivered to all areas.\nThese are the natural resources of the modern humanity.\nAn Earthship makes use of these materials via techniques available to the common person. These materials and the techniques for using them are accessible to the common person in terms of price and skill required to use them. The less energy required to turn a found object into a usable building material the better. This concept is also called embodied-energy.\nA. Rammed-Earth encased in Steel Belted Rubber:\nThe major structural building component of the Earthships is recycled automobile tires filled with compacted earth to form a rammed earth brick encased in steel belted rubber. This brick and the resulting bearing walls it forms is virtually indestructible.\nB. Aluminum Cans and Glass/Plastic Bottles:\nThese little bricks' are a great, simple way to build interior, non-structural walls. Aluminum [walls actually make very strong walls. The 'little bricks' create a cement-matrix that is very strong and very easy to build. Bottle can create beautiful colored walls that light shines through.\nCHARACTERISTICS OF THE MATERIALS USED IN EARTHSHIPS\nIn keeping with the design and performance requirements of a Earthship biotecture the nature of the building materials for an Earthship must have certain established characteristics established. These characteristics must align with rather than deteriorate environment of the planet. Many conventional materials satisfy one or two of these characteristics but no conventional material satisfies all of them. Therefore, new materials or building block must be 'invented' or 'created' for the primary structure of the Earthship.\nThermal Mass: The materials that surround the spaces of an Earthship must be dense and massive in order to store the temperatures required to provide a habitable environment for humans and plants. The Earthship itself must be a 'battery' for storing temperature. Making buildings out of heavy dense mass is as important as making airplanes light. Obviously a heavy airplane takes more fuel to fly. Obviously a light house takes more fuel to heat and cool.\nDurability: We have built out of wood for centuries. Wood is organic and biodegradable. It goes away. So we have developed various poisonous chemical products to paint on it and make it last. This, plus the fact that wood is light and porous, makes it a very unsatisfactory building materials. This is not to mention the fact that trees are our source of oxygen. For building housing that lasts without chemicals we should look around for materials that have durability as an inherent quality rather than trying to paint on durability. Wood is definitely a good materials for cabinet doors and ceilings where mass is not a factor and where it protected so it will not rot, but the basic massive structure of buildings should be a natural resource that is inherently massive and durable by its own nature.\nResilient: Earthquakes are an issue in many parts of the world. Any method of building relate to this potential threat. Since earthquakes involve a horizontal movement or shaking of the structure, this suggests a material with resilience or capacity to move this shaking. Brittle materials like concrete brake, crack and fracture. The ideal structural material for dealing with this kind of situation would have a 'rubbery' or resilient quality to it. This kind of material would allow movement without failure.\nLow specific skill requirements: If the materials for easily obtainable housing are to be truly accessibly to the common person they must, by their very nature, be easy to learn how to assemble. The nature of the materials for building an Earthship must allow for assembling skills to be learned and mastered in a matter of hours, not year. These skills must be basic enough that specific talent is not required to learn them.\nLow tech use/application: Some systems of building today are simple if one has the appropriate high-tech expensive energy dependent device or equipment. This, of course, limits the application of these methods to the professionals who have invested in the technology to enable them to use such methods. Because of the expense and energy required to get set up for these systems the common person is left totally dependent on those professionals for accessibility to these particular housing systems. Therefore the common person must go through the medium of money (bank loans, interest approvals, etc.) to gain access to a system that usually dictates performance and appearance. If high-tech systems and skills are between the common person and their ability to obtain a home, we are setting ourselves up to place the very nature of our housing in the hands of economics rather than in the hands of the people. This situation has resulted in human, energy-hog housing blocks and developments that make investors some quick money and leave the planet and the people with something that requires 3nt input-of money and energy to operate. Earthship technology is the technology of natural phenomenon like the physics of the sun, the earth and people themselves.\nTYPICAL FLOOR PLANS\nMETHOD OF CONSTRUCTION"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:b3bfb718-777c-4fdd-9817-b0ec72904d06>","<urn:uuid:1b5f263e-14fe-4ca7-96c0-ade8239188cf>"],"error":null}
{"question":"Were both Christopher Marlowe and William Shakespeare active in England during the same time period in the late 16th century?","answer":"Based on the information provided, while William Shakespeare lived from 1564 to 1616, Christopher Marlowe died in 1593. Therefore, they were contemporaries for only part of their careers, specifically during the late 16th century until Marlowe's death in 1593.","context":["William Shakespeare Quotes\n''I would not wishWilliam Shakespeare (1564-1616), British dramatist, poet. Miranda, in The Tempest, act 3, sc. 1, l. 54-5. Admitting her love for Ferdinand.\nAny companion in the world but you.''\n''O, what a deal of scorn looks beautifulWilliam Shakespeare (1564-1616), British dramatist, poet. Olivia, in Twelfth Night, act 3, sc. 1, l. 145-6. After an interview with Cesario, really Viola in disguise.\nIn the contempt and anger of his lip!''\n''O, now for everWilliam Shakespeare (1564-1616), British dramatist, poet. Othello, in Othello, act 3, sc. 3, l. 347-50. Jealousy in love destroys Othello as a soldier.\nFarewell the tranquil mind, farewell content,\nFarewell the plumèd troops and the big wars\nThat makes ambition virtue! O, farewell!''\n''A play there is, my lord, some ten words long,William Shakespeare (1564-1616), British dramatist, poet. Philostrate, in A Midsummer Night's Dream, act 5, sc. 1, l. 61-4. Advising Theseus not to watch \"Pyramus and Thisbe.\"\nWhich is as brief as I have known a play,\nBut by ten words, my lord, it is too long,\nWhich makes it tedious.''\n''Mercy but murders, pardoning those that kill.''William Shakespeare (1564-1616), British dramatist, poet. Prince Escalus, in Romeo and Juliet, act 3, sc. 1. Following Romeo's killing of Tybalt.\n''As wild geese that the creeping fowler eye,William Shakespeare (1564-1616), British dramatist, poet. Puck, in A Midsummer Night's Dream, act 3, sc. 2, l. 20-4. \"Russet-pated choughs, many in sort\" means jackdaws with dun-colored heads in a large flock; \"sever themselves\" means scatter, as Bottom's friends scattered when they saw his ass's head.\nOr russet-pated choughs, many in sort,\nRising and cawing at the gun's report,\nSever themselves and madly sweep the sky\nSo at his sight away his fellows fly.''\n''Romeo. I dreamt a dream tonight.William Shakespeare (1564-1616), British dramatist, poet. Romeo and Mercutio, in Romeo and Juliet, act 1, sc. 4, l. 50-8. Mercutio's fairy tale seems meant to divert Romeo from his preoccupation with Rosaline; agates were carved with tiny figures (like \"atomi\") and used as seal rings by town officials (aldermen).\nMercutio. And so did I.\nRomeo. Well, what was yours?\nMercutio. That dreamers often lie.\nRomeo. In bed asleep, while they do dream things true.\nMercutio. O then I see Queen Mab hath been with you.\nShe is the fairies' midwife, and she comes\nIn shape no bigger than an agate stone\nOn the forefinger of an alderman,\nDrawn with a team of little atomi\nOver men's noses as they lie asleep.''\n''Salerio. Why, I am sure if he forfeit thou wilt not take his flesh. What's that good for?William Shakespeare (1564-1616), British dramatist, poet. Salerio and Shylock, in The Merchant of Venice, act 3, sc. 1, l. 51-4. On the news that ships bearing Antonio's goods have been lost at sea.\nShylock. To bait fish withalif it will feed nothing else, it will feed my revenge.''\n''Hark, hark, the lark at heaven's gate sings,William Shakespeare (1564-1616), British dramatist, poet. Song, in Cymbeline, act 2, sc. 3, l. 20-26. Sung to wake Imogen.\nAnd Phoebus' gins arise,\nHis steeds to water at those springs\nOn chaliced flowers that lies;\nAnd winking Mary-buds begin to ope their golden eyes;\nWith every thing that pretty is, my lady sweet, arise;\n''Will you buy any tape,William Shakespeare (1564-1616), British poet. The Winter's Tale (IV, iii). OBSC. The Unabridged William Shakespeare, William George Clark and William Aldis Wright, eds. (1989) Running Press.\nOr lace for your cape,\nMy dainty duck, my dear-a?\nAny silk, and thread,\nAnd toys for your head,\nOf the new'st and finest, finest wear-a?\nCome to the pedlar;\nMoney's a meddler,\nThat doth utter all men's ware-a.''\nRead more quotations »\nShall I Compare Thee To A Summer's Day? (Sonnet 18)\nShall I compare thee to a summer's day?\nThou art more lovely and more temperate.\nRough winds do shake the darling buds of May,\nAnd summer's lease hath all too short a date.\nSometime too hot the eye of heaven shines,\nAnd often is his gold complexion dimmed;\nAnd every fair from fair sometime declines,\nBy chance, or nature's changing course, untrimmed;\nBut thy eternal summer shall not fade,\nNor lose possession of that fair thou ow'st,\nNor shall death brag thou wand'rest in his shade,\nWhen in eternal lines to Time thou grow'st.\nSo long as men can breathe, or ...\nO truant Muse, what shall be thy amends\nFor thy neglect of truth in beauty dyed?\nBoth truth and beauty on my love depends;\nSo dost thou too, and therein dignified.\nMake answer, Muse: wilt thou not haply say\n'Truth needs no colour, with his colour fix'd;\nBeauty no pencil, beauty's truth to lay;\nBut best is best, if never intermix'd?'\nBecause he needs no praise, wilt thou be dumb?","Ingram Frizer (// freezer; died August 1627) was an English gentleman and businessman of the late 16th and early 17th centuries who is notable for his reported killing of playwright Christopher Marlowe in the home of Eleanor Bull on 30 May 1593. He has been described as \"a property speculator, a commodity broker, a fixer for gentlemen of good worship\" and a confidence trickster gulling \"young fools\" out of their money.\nThere is no definite information regarding Frizer's origins, but he may have been born in or near Kingsclere in Hampshire, and the not always reliable International Genealogical Index does in fact show the baptism there of a female child called Ingram Frysar on 26 September 1561. Parish records for Kingsclere held at Hampshire Record Office show an Ingram Frizer, son of Stephen, christened 26 September 1561 in Kingsclere, Hampshire.\nSurviving legal records show Frizer to have been a fairly well-to-do business man profiting from buying and selling property. At the time of Marlowe's death the landowner Thomas Walsingham was Frizer's \"master\", but this does not imply that Frizer was a servant. As well as acting on his own behalf, Frizer appears to have been Walsingham's business agent. Walsingham was a young relative of Queen Elizabeth's Secretary of State, Sir Francis Walsingham; both Walsinghams had been heavily involved with intelligence work a few years earlier but there is no evidence that Frizer ever had any connection with it.\nNot all of Frizer's business dealings were honest. In 1593, collaborating with Nicholas Skeres (who was also present at Marlowe's killing), he was involved in lending money to one Drew Woodleff. Woodleff signed a bond for £60 in exchange for some guns that Frizer supposedly had in storage. Frizer then claimed to have sold them on Woodleff's behalf, but for only £30. The effect of this was that Frizer, who had never offered any guns for sale, had made Woodleff a loan of £30, to be repaid by the redemption of the £60 bond, an interest rate of 100%. Woodleff later signed a bond for £200 in favour of Thomas Walsingham, agreeing the forfeit of land to him in default of payment, to extricate himself from his bond to Frizer.\nA few years later, when King James ascended the throne, Frizer received numerous benefices from the crown, through the action of Audrey Walsingham (Thomas's wife and a friend of James's Queen, Anne of Denmark). He moved to Eltham, about three miles from the by then Sir Thomas Walsingham's estate at Scadbury. He became a churchwarden in 1605 and a parish tax assessor in 1611. There was a daughter named Alice Dixon, who lived in London, and another who married a man called John Banks. A \"Mrs. Ingeram\" who was buried at Eltham on 25 August 1616 may perhaps have been his wife, and he remained there apparently in genteel respectability until his death, being buried in the church there on 14 August 1627.\nFor several years before his death Marlowe had been employed in some intelligence capacity on behalf of the government. In the Spring of 1593 he appears to have been staying at Thomas Walsingham's home at Scadbury, near Chislehust in Kent, and had been invited by Frizer to a \"feast\" in Deptford, a township on the river Thames some seven miles to the north, at the house of Eleanor Bull, the widow of a local official. The status of Dame Bull's establishment is unclear, but it was probably a private victualling house, rather than a public tavern. Also in attendance were Nicholas Skeres and Robert Poley, both of whom had been associated with Sir Francis Walsingham's intelligence operation. In fact Poley still was working for the Privy Council at the time.\nComplete details of Marlowe's killing on 30 May 1593, as contained in an inquest run by the Coroner of the Queen's Household two days later, were discovered by Leslie Hotson in 1925. According to this report, based upon accounts from the three men present, Poley, Frizer, Skeres and Marlowe were in a private room, having had dinner. Poley, Frizer and Skeres were all seated facing a table with Frizer in the middle. Marlowe was lounging on a bed just behind them when Frizer and he got into an argument over \"the reckyninge\" (the bill). Marlowe suddenly jumped up, seized Frizer's dagger, which Frizer was wearing \"at his back\", and with it struck him twice on the head, leaving wounds two inches long and a quarter deep. Frizer, his freedom of movement restricted between Poley and Skeres, struggled to defend himself and in doing so stabbed Marlowe above the right eye, killing him immediately.\nAlthough some contend the \"self defence\" evidence offered at Marlowe's inquest was quite in keeping with the victim's alleged propensity for sudden violence, this has been brought into question by Charles Nicholl, who notes that Marlowe's supposed previous history of violence has been somewhat exaggerated. The tendency, particularly by Park Honan, to portray Marlowe as violent is also challenged by Rosalind Barber in her essay ‘Was Marlowe a Violent Man?’.\nIt has been suggested Frizer could have had other motives. Park Honan proposes that Marlowe's presence at Scadbury was a threat to Walsingham's reputation and influence, and thus threatened Frizer's interests also: The Privy Council certainly suspected Marlowe of atheism and heresy, and yet he was a regular and welcome house-guest of one of Elizabeth's former spymasters. At the start of 1593 it was upheld in Parliament that heresy was tantamount to the greatest crime of all—treason. Honan considers it possible that, given the circumstances, it was Thomas Walsingham himself, accustomed \"not to look far into Frizer's…trickery\", who initiated the deed by making his agent aware that Marlowe was becoming a liability to them both, and so indirectly securing his former friend's death.\nAnother theory suggests that Marlowe, as a supposed member of \"The School of Night\", became aware of Essex's plots against Raleigh, and Skeres was sent to warn him to keep silence. It was only when Marlowe refused to heed the warning was the unpremeditated decision taken to silence him in a more certain and final way. In this surmise Frizer is no more than one of Skeres's associates, and not the principal player.\nThe Marlovian theory suggests that Frizer took part in the faking of Marlowe's death to allow him to escape trial and almost certain execution for his subversively atheistic activities. This theory further suggests that Marlowe went into exile, and wrote the plays attributed to William Shakespeare.\n- Nicholl, Charles (1993). The Reckoning: The Murder of Christopher Marlowe. pp. 327–328. ISBN 0-226-58024-5. \"According to the official story – the story told by Skeres and Poley – it was Marlowe who pulled the knife and Frizer who killed him in self defence. ... I believe that in this, as in so much else in their careers, Skeres and Poley were lying.\" ... \"Ingram Frizer may well have struck the fatal blow. It is probable, though not certain, that he did.\"\n- Hotson, Leslie (1925). The Death of Christopher Marlowe. London: Nonesuch Press. p. 22. OCLC 459421025.\n- Nicholl (1993: 25)\n- Kuriyama, Constance Brown (2002). Christopher Marlowe: A Renaissance Life. Ithaca: Cornel University Press. p. 103.\n- \"FamilySearch\". Retrieved 13 February 2012.\n- Hampshire Record Office parish registers for Kingsclere\n- Honan, Park (2005). Christopher Marlowe: poet & spy. Oxford, England: Oxford University Press. p. 325. ISBN 0-19-818695-9.\n- Nicholl (1993:91)\n- Nicholl (1993: 22–25)\n- Honan (2005: 328; 350)\n- Boas, Frederick S. (1940). Christopher Marlowe: A Biographical and Critical Study. Oxford University Press. p. 327.\n- Nicholl, Charles (January 2008). \"Marlowe [Marley], Christopher (bap. 1564, d. 1593): Government service, c.1585–1587\". Oxford Dictionary of National Biography. Oxford, England: Oxford University Press. doi:10.1093/ref:odnb/18079.\n- Hutchinson, Robert (2006). Elizabeth's Spy Master. London: Weidenfeld and Nicolson. p. 111. ISBN 0-297-84613-2. \"The most famous spy in Walsingham's network was the dramatist Christopher Marlowe, who worked for him as a student...\"\n- Honan (2005:121)\n- According to William Vaughan in his The Golden Grove, the most reliable of the contemporary accounts before the discovery of the inquest itself.\n- Honan (2005: 346)\n- Nicholl (1993: 35–37)\n- Nicholl (1993: 28–29)\n- Nicholl (1993: 31–32)\n- Hotson (1925)\n- According to William Vaughan they were playing \"tables\" (or backgammon).\n- The original of the pardon is in Chancery Patent Rolls 35 Eliz., 28 June 1593, and was first translated from the Latin by J. L. Hotson and published in his The Death of Christopher Marlowe (1925)\n- Honan (2005: 352)\n- \"…he was no stranger to violence…but [the evidence does not] prove much about him as an aggressor\": Nicholl (1993: 86–87)\n- Barber, Rosalind (2010). \"Was Marlowe a Violent Man?\". In Scott, Sarah K. Christopher Marlowe the Craftsman: Lives, Stage and Page. Ashgate. ISBN 978-0-7546-6983-8.\n- (35 Eliz. cap 1 Against Seditious Sectaries)\n- Honan (2005:348)\n- Nicholl (1993: 327)\n- \"The International Marlowe-Shakespeare Society\". marloweshakespeare.org. Retrieved 19 January 2012."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1ed3c899-6293-41ef-b25a-0220b7cfb332>","<urn:uuid:4bd5aef5-714f-4c03-aa57-36b824b09a89>"],"error":null}
{"question":"In Viking times, how did portages help in trade routes, and what role did religious conversion play in their settlements?","answer":"Portages were vital for Viking merchant-adventurers in Eastern Europe, connecting four major rivers (Volga, Western Dvina, Dnieper, and Don) for commerce with the Orient and Byzantium. Important portage points featured trade outposts with mixed Norse and native populations. As for religious conversion, when Vikings established settlements, they gradually adopted Christianity through contact with conquered peoples. For example, in England, Viking leader Guthrum accepted Christianity after a peace treaty with Alfred, while in Normandy, Viking ruler Rollo converted to Christianity as part of a settlement agreement with Charles the Simple, leading to widespread conversion among his people.","context":["Portage or portaging is the practice of carrying water craft or cargo over land, either around an obstacle in a river, or between two bodies of water. A place where this carrying occurs is also called a portage.\nOver time, important portages were sometimes provided with canals with locks, and even portage railways. Primitive portaging generally involves carrying the vessel and its contents across the portage in multiple trips. Small canoes can be portaged by carrying them inverted over one's shoulders and the center strut may be designed in the style of a yoke to facilitate this. Historically, voyageurs often employed tump lines on their heads to carry loads on their backs.\nPortages can be many kilometers in length, such as the 19-kilometre (12 mi) Methye Portage and the 8.5-mile (13.7 km) Grand Portage (both in North America) often covering hilly or difficult terrain. Some portages involve very little elevation change, such as the very short Mavis Grind in Shetland, which crosses an isthmus.\nPortage trails usually began as animal tracks and were improved by tramping or blazing. In a few places iron-plated wooden rails were laid to take a handcart. Heavily used routes sometimes evolved into roads when sledges, rollers or oxen were used, as at Methye Portage. Sometimes railways (Champlain and St. Lawrence Railroad) or canals were built.\nWhen going downstream through rapids an experienced voyageur called the guide would inspect the rapids and choose between the heavy work of a portage and the life-threatening risk of running the rapids. If the second course were chosen, the boat would be controlled by the avant standing in front with a long paddle and the gouvernail standing in the back with a 2.7-metre (9 ft) steering paddle. The avant had a better view and was in charge but the gouvernail had more control over the boat. The other canoemen provided power under the instructions of the avant.\nGoing upstream was more difficult, as there were many places where the current was too swift to paddle. Where the river bottom was shallow and firm, voyageurs would stand in the canoe and push it upstream with 3-metre (10 ft) poles. If the shoreline was reasonably clear the canoe could be 'tracked' or 'lined', that is, the canoemen would pull the canoe on a rope while one man stayed on board to keep it away from the shore. (The most extreme case of tracking was in the Three Gorges in China where all boats had to be pulled upstream against the current of the Yangtze River.) In worse conditions, the 'demi-chargé' technique was used. Half the cargo was unloaded, the canoe forced upstream, unloaded and then returned downstream to pick up the remaining half of the cargo. In still worse currents, the entire cargo was unloaded ('décharge') and carried overland while the canoe was forced upstream. In the worst case a full portage was necessary. The canoe was carried overland by two or four men (the heavier York boats had to be dragged overland on rollers) The cargo was divided into standard 41-kilogram (90 lb) packs or pièces with each man responsible for about six. One pack would be carried by a tumpline and one on the back (strangulated hernia was a common cause of death). To allow regular rests the voyageur would drop his pack at a pose about every 1 kilometre (0.5 mi) and go back for the next load. The time for a portage was estimated at one hour per half mile.\nThe Diolkos was a paved trackway in Ancient Greece which enabled boats to be moved overland across the Isthmus of Corinth between the Gulf of Corinth and the Saronic Gulf. It was constructed to transport high ranking Despots to conduct business in the justice system. The 6 km (3.7 mi) to 8.5 km (5.3 mi) long roadway was a rudimentary form of railway, and operated from around 600 BC until the middle of the 1st century AD. The scale on which the Diolkos combined the two principles of the railway and the overland transport of ships was unique in antiquity.\nThere is scant literary evidence for two more ship trackways referred to as diolkoi in antiquity, both located in Roman Egypt: The physician Oribasius (c. AD 320–400) records two passages from his 1st century AD colleague Xenocrates, in which the latter casually refers to a diolkos close to the harbor of Alexandria, which may have been located at the southern tip of the island of Pharos. Another diolkos is mentioned by Ptolemy (AD 90–168) in his book on geography (IV, 5, 10) as connecting a false mouth of a partly silted up Nile branch with the Mediterranean Sea.\nThe land link between Adige river and Garda lake in Northern Italy, hardly used by the smallest watercraft, was at least once used by the Venetian Republic for the transport of a military fleet in 1439. The land link is now somewhat harder because of the disappearance of Loppio lake.\nIn the 8th, 9th and 10th centuries, Viking merchant-adventurers exploited a network of waterways in Eastern Europe, with portages connecting the four most important rivers of the region: Volga, Western Dvina, Dnieper, and Don. The portages of what is now Russia were vital for the Varangian commerce with the Orient and Byzantium.\nAt the most important portages (such as Gnezdovo) there were trade outposts inhabited by a mixture of Norse merchants and native population. The Khazars built the fortress of Sarkel to guard a key portage between the Volga and the Don. After Varangian and Khazar power in Eastern Europe waned, Slavic merchants continued to use the portages along the Volga trade route and the Dnieper trade route.\nThe names of the towns Volokolamsk and Vyshny Volochek may be translated as \"the portage on the Lama River\" and \"the little upper portage\", respectively (volok means \"portage\" in Russian, derived from the verb \"to drag\").\nScotland and IrelandEdit\nTarbert is a common place name in Scotland and Ireland indicating the site of a portage.\nPortages played an important role in the economy of some African societies. For instance, Bamako was chosen as the capital of Mali because it is located on the Niger River near the rapids that divide the Upper and Middle Niger Valleys.\nPlaces where portaging occurred often became temporary and then permanent settlements. The importance of free passage through portages found them included in laws and treaties. One historically-important fur trade portage is now Grand Portage National Monument. Recreational canoeing routes often include portages between lakes, for example, the Seven Carries route in Adirondack Park.\nNumerous portages were upgraded to carriageways and railways due to their economic importance. The Niagara Portage had a gravity railway in the 1760s. The passage between the Chicago and Des Plaines Rivers was through a short swamp portage which seasonally flooded and it is thought that a channel gradually developed unintentionally from the dragging of the boat bottoms. The 1835 Champlain and St. Lawrence Railroad connected the cities of New York and Montreal without needing to go through the Atlantic.\nMany settlements in North America were named for being on a portage.\nPortages existed in a number of locations where an isthmus existed that the local Māori could drag or carry their waka across from the Tasman Sea to the Pacific Ocean or vice versa. The most famous ones are located in Auckland, where there remain two 'Portage Road's in separate parts of the city. The small Marlborough Sounds settlement of Portage lies on the Kenepuru Sound which links Queen Charlotte Sound at Torea Bay.\n- Eric W. Morse,'Fur Trade Canoe Route of Canada /Then and Now',1984\n- Lewis 2001, pp. 8 & 15\n- Verdelis 1957, p. 526; Cook 1979, p. 152; Drijvers 1992, p. 75; Raepsaet & Tolley 1993, p. 256; Lewis 2001, p. 11\n- Lewis 2001, p. 15\n- Coll. Med II, 58, 54-55 (CMG VI, 1, 1)\n- Fraser 1961, pp. 134 & 137\n- Fraser 1961, pp. 134f.\n- The Chicago Portage - Historical Synopsis, prepared by Wm. E. Rose and Associates, Inc., for the Forest Preserve District of Cook County, June 1975\n- Cook, R. M. (1979), \"Archaic Greek Trade: Three Conjectures 1. The Diolkos\", The Journal of Hellenic Studies, 99: 152–155, doi:10.2307/630641\n- Drijvers, J. W. (1992), \"Strabo VIII 2,1 (C335): Porthmeia and the Diolkos\", Mnemosyne, 45: 75–78\n- Fraser, P. M. (1961), \"The ΔΙΟΛΚΟΣ of Alexandria\", The Journal of Egyptian Archaeology, 47: 134–138, doi:10.2307/3855873\n- Lewis, M. j. t. (2001), \"Railways in the Greek and Roman world\", in Guy, A.; Rees, J., Early Railways. A Selection of Papers from the First International Early Railways Conference (PDF), pp. 8–19 (10–15), archived from the original (PDF) on 2010-03-12\n- Raepsaet, G.; Tolley, M. (1993), \"Le Diolkos de l'Isthme à Corinthe: son tracé, son fonctionnement\", Bulletin de Correspondance Hellénique (in French), 117: 233–261, doi:10.3406/bch.1993.1679\n- Verdelis, N. M. (1957), \"Le diolkos de L'Isthme\" (PDF), Bulletin de Correspondance Hellénique (in French), 81: 526–529, doi:10.3406/bch.1957.2388\n|Wikimedia Commons has media related to Portage.|","The Vikings were Norse (Scandinavian) warrior-seamen who raided, traded, explored and settled in wide areas of Europe and the North Atlantic islands from the late 8th to the mid-11th century. They left their islands and peninsulas of Scandinavia for overseas adventures. Their adventures and exploration gave their name to an epoch, Viking Age. It is the term for the period in European history, especially Northern European and Scandinavian history, spanning the late 8th to 11th centuries when the Vikings explored Europe by its oceans and rivers through trade and warfare.\nThey lived in the fjords and viks in their homelands. Fjord is a narrow, deep sea inlet, steep-sided and bounded by mountains, formed by past glacial erosion. Vik is a small bay or inlet. Etymologically, the word vik is the source for the word Viking but it is still disputed. The two suggested origins are the Old Norse word vik meaning bay or the Latin word for town, vicus.\nThey sailed westwards to the British Isles and further west to Iceland, Greenland and even in the shores of North America. They sailed southwards, using the river systems of the modern Low Countries and France. They sailed eastward across the Baltic Sea and by river and portage reached deep into Russia. They used the European river system as natural aquatic superhighway in plundering. It is very navigable and accessible.\nThey sailed as pagans of deities like Thor, the thunder god; Odin, the god of spear; and Frey, the god of sexual pleasure.\nTake note that their gods speak more about themselves. They have Thor and Odin because they were warfreak. Their worship of Frey did not only lead to fertility of their land but also proliferated massive polygamy. One wife was the wife of another. One husband was the husband of many others. As a result there was population explosion.\nActually, as means of livelihood, they were farmers who cultivated the land in the short farming period. They relied heavily on cattles and livestock. But the tremendous increase in population is directly proportional to the increase of demand for food. In other words, the more the people, the more mouths to feed.\nNordic villages are independent from one another since they didn’t have a king that unified them. One village plundered another village in search for food. This prompted them to sail away from their homelands and search for food and wealth overseas.\nLINDISFARNE (793) AND THE PLUNDER OF OTHER MONASTERIES\nThe very first known attack by the fierce Viking was on the peaceful monastery at Lindisfarne in 793. Lindisfarne was a church and monastery, built there in 635, and was one of the first establishments of Celtic Christianity in England. The most famous product of the monastery was the Lindisfarne Gospel (an illuminated Latin manuscript now in the British Museum). It was a holy place desecrated by the Vikings. The defenceless and peaceful monks were on their way for mass when they experienced hell on earth. They were plundered and slaughtered. Contemporary writers will describe the attacks as stinging hornets and ravenous wolves. They destroyed everything in sight and took with them some monks as slaves\nIn 794, the Vikings plundered Jarrow, Orkney, Shetland islands, Hebrides and the famous island monastery of Iona. In the following year, Iona was plundered again and also the island of Skye, the island of Lambay and islands west coast of Ireland. Iona devastated again in 802 and 806 and 68 Irish monks were slain. Many parts of Britain were devastated. Many monasteries were plundered.\nThe Vikings devastated the monasteries not because they were against monasteries as places of Christian worship but against monasteries as keepers of gold and silver vessels and as places containing prominent men, who could be held for ransom. Undefended monasteries were obvious targets.\nThen, they penetrated the river system of modern France and the Low Countries. For two generations one could see fleeing monks on the roads leading to Burgundy. The monks from Auvergne and Flanders carried their saints out of the place of plunder. The canons of Tours carried the body of St. Martin of Tours four times away from destruction.\nTHE LONG SHIP\nThe Long-ship is characterized as a graceful, long, narrow, light, wooden boat with a shallow draft hull designed for speed. The ship's shallow draft allowed navigation in waters only one meter deep and permitted beach landings, while its light weight enabled it to be carried over portages.\nLongships were also double-ended, the symmetrical bow and stern allowing the ship to reverse direction quickly without having to turn around; this trait proved particularly useful in northern latitudes where icebergs and sea ice posed hazards to navigation. Longships were fitted with oars along almost the entire length of the boat itself. Later versions sported a rectangular sail on a single mast which was used to replace or augment the effort of the rowers, particularly during long journeys. Some were decorated with dragon head designs\nTHE VIKINGS AND CHRISTIANITY\nThe repeated Viking invasions of England brought them into close contact with the already Christianized people. Since the Vikings did not fear the English people militarily or politically, English missionaries were allowed to move about fairly freely in Scandinavian countries without being looked on with suspicion. As had been true with the barbarians, the religion of the conquered became that of the conqueror.\nIn the early 9th century, the Danes raided English monasteries and took people as slaves. By 874 only the southernmost kingdom remained. However under Alfred the English (+13th cent), the Danes were defeated in 878. In the following year, Alfred and the Danish leader Guthrum (+890) made a treaty. England was divided between them, the Danes taking the eastern part. With the peace treaty, Guthrum accepted Christianity and so were his followers. The succeeding Danish kings became Christians and died with full Christian rites.\nWhat happened was a rapid assimilation of Christianity. The process of assimilation in Ireland had already begun in the late 9th century with the intermarriage of some Viking leaders and Irish princesses which was accompanied by the Viking’s conversion.\nThere were three attempts for the Vikings to establish a permanent settlement in France. Only one was successful, that is in the region of the lower Seine River. This is the part of France that still bears their name, Normandy. how could this be? The Carolingian Emperor Charles the Simple (879-929) made an agreement with the Viking King Rollo (ca.846-931). He allowed Rollo and his subjects to settle in the under populated region and in return, he became a Christian and promised to defend the lower Seine from future attacks , from the Bretons and other Vikings. There were consequent intermarriages between Viking men and Christian Frankish women. They adopted Christian names. For example:\nRollo à Robert\nGeloc à Adele – daughter of Rollo\nThurstein à Richard\nStigand à Odo\nWilliam Longsword (893-942), son of Rollo (now Robert) wanted to enter the monastery. He was a devout Christian but he was restrained so that he could be the successor of his father. He married a Christian princess and his sister married a Christian prince. Gradually, they became French speaking.\nBeyond Ireland and Scotland and the islands of the north, the Viking sailors discovered the empty land they called Iceland. It was empty which means uninhabited except for Irish monks who lived inn the southwest during the summer seasons. The settlement happened between 870-930. It was a migration of tens of thousands. There was no assimilation needed and their Christianization came from their Norse homeland.\nAccording to legend, a sudden volcanic eruption in the year 1000 led the settlers to accept Christianity.A Viking leader/missionary named Gizur demanded the official acceptance of the Christian religion. All people were baptized either publicly or privately. Soon bishops were appointed and the canon law was codified\nBeyond Iceland to the west, the Vikings sailed and with them was the Christian religion. Eric the Red (ca.950-1003) set sail 450 miles from western Iceland when he caught sight of an enormous land mass with imposing glaciers reaching 1,900 meters. He turned south following the coast around Cape Farewell, east of which he found green, richly looking land on deep fjords, reaching out from the mountains, reminiscent of Norway.\nEric the Red called this land, Greenland. He was remembered in medieval Icelandic saga sources as having founded the first Nordic settlement in Greenland. Greenland’s conversion followed Iceland’s conversion in 1000. Christian churches flourished in Greenland.\nA diocese was established in Gardar in the 12th century and bishops from Greenland travelled to the continent for ecumenical councils. Eventually by the 16th century, the diocese disintegrated for unknown reason. Probably, it was because of the climate. For 80 years, no bishop or priest resided and many inhabitants abandoned the faith. Once a year, they exhibited a sacred linen once used by the last priest who celebrated mass there about a century ago.\nMore is known about the conversion of the Vikings who journeyed abroad than the Viking who stayed at home.\nIn the 8th century, Denmark was visited by St. Wilibrord but with no success. In the 9th century, Ansgar, the apostle of the north, only gained little success.\nThe conversion of Denmark was brought about by the conversion of their king, Harald Bluetooth (ca.935). His mother was Christian but his father was pagan. Probably, he was converted to Christianity before assuming the throne.\nAccording to legend, German missionaries preached the doctrine of Trinity but the Danes would believe only our God as inferior to other gods and one among their many gods. An ordeal of fire proved that Christianity is the true religion.\nOthers would say that the conversion of the Danes was politically motivated for it would mean closer ties with the German emperor.\nA rune stone of about 8 feet have this inscription: “King Harald had this stone made in memory of his father, Gorm, and his mother, Thyri, the same Harlad who conquered all Denmark and Norway and who made the danes Christian”- a simple act that made Denmark officially Christian.”\nThe conversion of Norway followed later in the 11th century. King Olaf of Norway (960s-1000) fought England in the last wave of the Viking attacks in the 990s. In 994, he became a Christian in England as part of the peace settlement. The English King Ethelred stood sponsor at Olaf’s confirmation. After receiving gifts, Olaf promised \"that he would never come back to England in hostility.\" Olaf then left England for Norway and never returned. After returning to Norway, Olaf seized the crown of Norway and converted his people to the Christian faith.\nFor Denmark and Norway, the kings’ conversion followed their people’s conversion. Sweden was a different story. Another king named Olaf of Sweden received baptism from English missionaries and his daughter married the converted king Olaf of Norway. Yet conversion of people did not follow.\nLarge areas of Sweden remained pagan for a century. At Uppsala, pagan worship and sacrifice even human sacrifice followed. The Christian king Inge refused to worship at Uppsala and feed for his life. By the 12th century, Uppsala was destroyed and the Christian church, still surviving, replaced it.\n Scandinavia is a cultural, historical and ethno-linguistic region in northern Europe characterized by their common heritage and language. In the strictest definition, it is composed of three kingdoms (Denmark, Norway, Sweden). The possible extended usage includes Iceland and Finland. The maximal extended usage that takes Scandinavia as synonymous to the Nordic countries includes Greenland and Faroe Islands.\n Portage or portaging refers to the practice of carrying watercraft or cargo over land to avoid river obstacles, or between two bodies of water."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:5f9b5be4-f05c-46c8-8136-2f3897ce984e>","<urn:uuid:0c6ce640-885b-424f-b894-b6aa7cad07a5>"],"error":null}
{"question":"What is the protein binding percentage of RAF709 compared to atazanavir?","answer":"RAF709 has a plasma protein binding of 98% across species, which is nearly identical to atazanavir's protein binding of 97-98%.","context":["Molecular Weight(MW): 542.55\nRAF709 is a potent inhibitor of B/C RAF kinase with almost equivalent IC50 values of 0.4 nM for B-RAF and C-RAF, showing a high level of selectivity, demonstrating greater than 99% on-target binding to BRAF, BRAFV600E, and CRAF at 1 μM and very few off-targets with DDR1 (>99%), DDR2 (86%), FRK (92%), and PDGFRb (96%), the only kinases with binding >80% at 1 μM.\nPurity & Quality Control\nChoose Selective Raf Inhibitors\n|Description||RAF709 is a potent inhibitor of B/C RAF kinase with almost equivalent IC50 values of 0.4 nM for B-RAF and C-RAF, showing a high level of selectivity, demonstrating greater than 99% on-target binding to BRAF, BRAFV600E, and CRAF at 1 μM and very few off-targets with DDR1 (>99%), DDR2 (86%), FRK (92%), and PDGFRb (96%), the only kinases with binding >80% at 1 μM.|\nRAF709 appears to have very slow dissociation kinetics (T1/2 > 6.5 h) using the rapid dilution method to measure its dissociation rate constant. In cellular assays, the dose−response of pMEK and pERK are measured in Calu-6 cells with EC50 of 0.02 and 0.1 μM with minimal paradoxical activation and inhibition of proliferation with EC50 of 0.95 μM. RAF709 stabilizes BRAF−CRAF dimers with an EC50 of 0.8 μM. Of the 456 kinases tested, RAF709 shows a high level of selectivity, demonstrating greater than 99% on-target binding to BRAF, BRAFV600E, and CRAF at 1 μM and very few off-targets with DDR1 (>99%), DDR2 (86%), FRK (92%), and PDGFRb (96%), the only kinases with binding >80% at 1 μM. RAF709 shows equal activity against both RAF monomers and dimers. In in vitro biochemical assays, RAF709 exhibits potent inhibitory activity targeting BRAF, BRAFV600E, and CRAF with IC50 values ranging between 0.3 to 1.5 nmol/L. RAF709 treatment leads to a dose-dependent induction of B/CRAF heterodimerization in HCT116, but inhibits MEK and ERK phosphorylation, in line with the ability of RAF709 to effectively inhibit the RAF dimers. RAF709 selectively inhibits oncogenic signaling and proliferation in tumor cells with BRAF, NRAS, or KRAS mutations with minimal paradoxical activation.\n|In vivo||RAF709 is well tolerated and efficacious in KRAS mutant xenograft models. It is reasonably stable in plasma after a 3 h incubation at 37℃ across species [plasma stability (%remaining): rat 85%, mouse 82%, dog 95%, human 101%], and plasma protein binding is measured to be 98% across species. In pharmacokinetic experiments, RAF709 has moderate clearance in mouse (35 mL/min/kg) and dog (14 mL/min/kg) and high clearance in rat (50 mL/min/kg). Cmax in mouse (1 μM), dog (0.5 μM), and rat (0.5 μM) reach pharmacologically active concentrations, and acceptable oral availability is observed in mouse (68%), rat (24%), and dog (48%). In the Calu-6 xenograft nude mouse model, treatment with RAF709 results in dose-dependent antitumor activity with 10 mg/kg being subefficacious (%T/C = 92%), 30 mg/kg resulted in measurable antitumor activity (% T/C = 46%), and 200 mg/kg resulted in mean tumor regression of 92%, while the same high dose is not efficacious in the PC3, KRAS WT mode.|\nCRAF kinase assay:The CRAF kinase assay was carried out using 10 nM kinase-dead MEK1 protein substrate (carrying a K97R mutation), 3 μM ATP, and 10 pM CRAF Y340E/Y341E. The reaction buffer contained 50 mM Tris pH 7.5, 10 mM MgCl2, 0.05% BSA, 50 mM NaCl, 0.01% Tween-20, and 1 mM DTT. The reactions were carried out at room temperature in a volume of 10 μL in white 384-shallow-well plates for 40 min and stopped by adding 5 μL/well quench solution (50 mM Tris pH 7.5, 50 mM EDTA). Terminated reactions received 5 μL/well detection reagents consisting of 50 mM Tris pH 7.5, 0.01% Tween-20, 1:1000 diluted antiphospho MEK1/2 S217/S221 antibody, 0.01 mg/mL each of AlphaScreen Protein A-coated acceptor beads, and streptavidin-coated donor bead. Plates were read in an EnVision plate reader after overnight incubation at room temperature. In compound inhibition studies, compounds were tested over a concentration range of 25 μM to 1.74 × 10−6 μM in 16-point, 3-fold format. DMSO was at a final concentration of 0.5%. Compounds were preincubated with CRAF for 30 min before adding substrates to start the reaction. Inhibition data were fit to a four-parameter logistic equation to calculate the IC50 of the compounds.\n|In vitro||DMSO||100 mg/mL (184.31 mM)|\n|Ethanol||100 mg/mL (184.31 mM)|\n* Please note that Selleck tests the solubility of all compounds in-house, and the actual solubility may differ slightly from published values. This is normal and is due to slight batch-to-batch variations.\nCalculate the mass, volume or concentration required for a solution. The Selleck molarity calculator is based on the following equation:\nMass (g) = Concentration (mol/L) × Volume (L) × Molecular Weight (g/mol)\n*When preparing stock solutions, please always use the batch-specific molecular weight of the product found on the via label and MSDS / COA (available on product pages).\nCalculate the dilution required to prepare a stock solution. The Selleck dilution calculator is based on the following equation:\nConcentration (start) x Volume (start) = Concentration (final) x Volume (final)\nThis equation is commonly abbreviated as: C1V1 = C2V2 ( Input Output )\n* When preparing stock solutions always use the batch-specific molecular weight of the product found on the vial label and MSDS / COA (available online).\nMolecular Weight Calculator\nEnter the chemical formula of a compound to calculate its molar mass and elemental composition:\nTip: Chemical formula is case sensitive. C10H16N2O2 c10h16n2o2\nInstructions to calculate molar mass (molecular weight) of a chemical compound:\nTo calculate molar mass of a chemical compound, please enter its chemical formula and click 'Calculate'.\nDefinitions of molecular mass, molecular weight, molar mass and molar weight:\nMolecular mass (molecular weight) is the mass of one molecule of a substance and is expressed in the unified atomic mass units (u). (1 u is equal to 1/12 the mass of one atom of carbon-12)\nMolar mass (molar weight) is the mass of one mole of a substance and is expressed in g/mol.\nAnswers to questions you may have can be found in the inhibitor handling instructions. Topics include how to prepare stock solutions, how to store inhibitors, and issues that need special attention for cell-based assays and animal experiments.\nTel: +1-832-582-8158 Ext:3\nIf you have any other enquiries, please leave a message.","HIV infection in treatment-experienced and treatment-naïve patients (in combination with other antiretrovirals).\n- Atazanavir– Inhibits the action of HIV protease, preventing maturation of virions.\n- Cobicistat– ↑ blood levels of atazanavir.\n↑ CD4 cell counts and ↓ viral load with subsequent slowed progression of HIV and its sequelae.\nAbsorption: Rapidly absorbed (↑ by food).\nDistribution: Enters cerebrospinal fluid and semen.\nMetabolism and Excretion: 80% metabolized (CYP3A); 13% excreted unchanged in urine.\nHalf-life: 7 hr.\nAbsorption: Absorption follows oral administration.\nProtein Binding: 97–98%.\nMetabolism and Excretion: Metabolized by CYP3A and to a small extent by CYP2D6; 86.2 eliminated in feces, 8.2% in urine.\nHalf-life: 3–4 hr.\nTIME/ACTION PROFILE (blood levels)\n|atazanavir PO||rapid||2.5 hr||24 hr|\n|cobicistat PO||unknown||3 hr||24 hr|\n- Previous hypersensitivity, including Stevens-Johnson syndrome, erythema multiforme or other serious skin reactions;\n- Concurrent use of alfuzosin, carbamazepine, colchicine, dihydroergotamine, dronedarone, drospirenone/ethinyl estradiol, elbasvir/grazoprevir, ergotamine, glecaprevir/pibrentasvir, indinavir, irinotecan, lomitapide, lovastatin, lurasidone, methylergonovine, nevirapine, phenobarbital, phenytoin, pimozide, ranolazine, rifampin, sildenafil (for pulmonary hypertension), simvastatin, St. John's wort and triazolam;\n- Renal impairment (CCr <70 mL/min) (when used concomitantly with tenofovir disoproxil fumarate);\n- End-stage renal disease managed by dialysis (treatment-experienced patients);\n- Hepatic impairment;\n- OB: Not recommended for use during pregnancy (↓ concentrations of atazanavir and cobicistat);\n- Lactation: Avoid breast feeding in women with HIV.\nUse Cautiously in:\n- History of pre-existing cardiac conduction disease (marked first-degree AV block, second-or third-degree AV block; ECG monitoring recommended);\n- Renal impairment (consider alterative medications);\n- Hepatitis B or C co-infection (↑ risk of further hepatic impairment);\n- Diabetes mellitus (↑ risk of new onset/ exacerbation);\n- Hemophilia (risk of spontaneous bleeding and need for additional factor VIII);\n- Pedi: Children <35 kg (safety and effectiveness not established); use of atazanavir in infants <3 mo may ↑ risk of kernicterus).\nAdverse Reactions/Side Effects\nCV: cardiac conduction abnormalities\nDerm: DRUG RASH WITH EOSINOPHILIA AND SYSTEMIC SYMPTOMS (DRESS), ERYTHEMA MULTIFORME, STEVENS-JOHNSON SYNDROME, rash\nEENT: ocular icterus\nEndo: Graves' disease, hyperglycemia\nGI: autoimmune hepatitis, cholelithiasis, hepatotoxicity, hyperbilirubinemia, jaundice, nausea\nGU: nephrolithiasis, new onset/worsening renal impairment\nMetabolic: accumulation/redistribution of body fat\nNeuro: Guillain-Barré syndrome\nMisc: immune reconstitution syndrome\n* CAPITALS indicate life-threatening.\nUnderline indicate most frequent.\n- Carbamazepine, phenobarbital, phenytoin, rifampin and nevirapine ↓ blood levels and effectiveness; concurrent use contraindicated.\n- ↑ levels and risk of serious toxicity with alfuzosin, dihydroergotamine, dronedarone, ergotamine, irinotecan, lomitapide, lovastatin, lurasidone, methylergonovine, pimozide, ranolazine, sildenafil (for pulmonary hypertension), simvastatin, and triazolam ; concurrent use contraindicated.\n- ↑ levels and risk of serious toxicity with colchicine ; concurrent use contraindicated in patients with renal or hepatic impairment.\n- ↑ risk of indirect hyperbilirubinemia with indinavir ; concurrent use contraindicated.\n- ↑ risk of hepatotoxicity with elbasvir/grazoprevir and glecaprevir/pibrentasvir ; concurrent use contraindicated.\n- Concurrent use with drospirenone/ethinyl estradiol may ↑ drospirenone levels and risk of hyperkalemia; concurrent use contraindicated.\n- ↑ risk of renal impairment with tenofovir disoproxil fumarate ; avoid concurrent use with nephrotoxic agents.\n- Concurrent use with other antiretrovirals that require inhibition of CYP3A for adequate exposure especially protease inhibitors (may ↓ antiretroviral effectiveness).\n- ↑ levels and risk of toxicity from some antiarrhythmics (including amiodarone, digoxin, disopyramide, flecainide, mexiletine and propafenone, antineoplastics (including dasatinib, nilotinib, vinblastine and vincristine ), anticonvulsants metabolized by CYP3A (including clonazepam ); monitor drug effects carefully, titrate if necessary and consider alternatives.\n- ↑ risk of sedation with sedative/hypnotics, including buspirone, diazepam, midazolam, zolpidem and others metabolized by CYP3A (careful monitoring with dose reduction recommended).\n- Absorption of atazanavir may be ↓ by proton-pump inhibitors (administer 12 hr after PPI, dose should not >20 mg omeprazole or equivalent/day, not recommended in treatment-experienced patients), antacids (separate dose by 2 hr), buffered medications, H2 -receptor antagonists (administer famotidine at same time or at least 10 hr after famotidine [dose should not exceed famotidine 40 mg twice daily or equivalent in treatment-naïve patients or 20 mg twice daily or equivalent in treatment-experienced patients]).\n- Concurrent use with didanosine (buffered) ↓ atazanavir levels (administer 1 hr before or 2 hr after atazanavir/cobicistat.\n- ↓ levels of concurrent didanosine EC (administer at different times).\n- Concurrent use with efavirenz or etravirine may ↓ levels and effectiveness (concurrent use is not recommended).\n- ↑ levels of maraviroc (↓ dose of maraviroc to 150 mg twice daily).\n- ↑ levels and risk of adverse reactions from clarithromycin and erythromycin ; concurrent use may also ↑ levels of atazanavir and cobicistat; consider alternative anti-infectives.\n- ↑ levels and risk of bleeding with apixaban, dabigatran, edoxaban, and rivaroxaban (concurrent use of rivaroxaban not recommended, dose adjustments or alternatives necessary for apixaban, dabigatran, or edoxaban).\n- Levels and effectiveness may be ↓ by oxcarbazepine ; monitor levels and clinical response carefully, consider alternative anticonvulsants.\n- May ↑ or alter effects of antidepressants including SSRIs, tricyclic antidepressants and trazodone (monitor for drug effect and titrate to lowest effective dose).\n- ↑ levels of ketoconazole and itraconazole ( voriconazole not recommended unless benefits outweigh risks); ketoconazole and itraconazole may also ↑ levels of atazanavir and cobicistat.\n- ↑ levels and risk of toxicity with rifabutin (↓ dose by 75% and monitor for adverse reactions).\n- ↑ levels and risk of adverse reactions with beta-blockers metabolized by CYP2D6 including carvedilol, metoprolol and timolol (clinical monitoring for adverse cardiovascular effects recommended).\n- ↑ levels and risk of adverse reactions with calcium channel blockers metabolized by CYP3A including amlodipine, diltiazem, felodipine, nifedipine and verapamil (clinical monitoring for adverse cardiovascular effects recommended).\n- Concurrent use with corticosteroids that induce CYP3A including dexamethasone ↓ effectiveness and ↑ corticosteroid effects (consider alternative corticosteroid).\n- Levels and effectiveness may be ↓ by bosentan while effects of bosentan are ↑ (specific dose alteration of bosentan is required).\n- ↑ levels and risk of toxicity from atorvastatin, fluvastatin, pravastatin and rosuvastatin ; concurrent use with atorvastatin not recommended; rosuvastatin dose should not exceed 10 mg/day; for other statins, use lowest effective dose and monitor for adverse effects.\n- ↑ levels and effects of immunosuppressants including cyclosporine, everolimus, sirolimus. tacrolimus (blood level monitoring recommended).\n- ↑ risk of adverse cardiovascular effects with inhaled salmeterol (concurrent use not recommended.\n- ↑ risk of corticosteroid effects with inhaled/nasal corticosteroids metabolized by CYP3A including and budesonide and fluticasone (consider alternative corticosteroids).\n- May ↑ CNS and respiratory depression with opioid analgesics or tramadol (dose reduction and careful titration recommended).\n- May ↑ levels and risk of adverse reactions with neuroleptics metabolized by CYP3A or CYP2D6 including perphenazine, risperidone and thioridazine (dose reduction may be necessary).\n- ↑ levels and risk of adverse cardiovascular effects with PDE-5 inhibitors including avanafil, sildenafil, tadalafil and vardenafil (avanafil use not recommended, dose reductions required for sildenafil (for erectile dysfunction), tadalafil and vardenafil).\n- May ↑ quetiapine levels; ↓ quetiapine dose to 1/6 of current dose\n- Concurrent use with sofosbuvir/velpatasvir/voxilaprevir may ↑ voxilaprevir levels; concurrent use not recommended.\nSt. John's wort ↓ blood levels and effectiveness; concurrent use contraindicated.\nPO (Adults and Children ≥35 kg): One tablet (atazanavir 300 mg/cobicistat 150 mg) once daily.\nTablets: atazanavir 300 mg/cobicistat 150 mg\n- Assess for change in severity of HIV symptoms and for symptoms of opportunistic infections throughout therapy.\n- Monitor ECG periodically in patients with first, second, or third-degree AV blocks.\n- Assess for rash which can occur within initial 8 wk of therapy. Usually resolves within 2 wk without altering therapy. Discontinue therapy if rash becomes severe.\n- Monitor for signs and symptoms of DRESS (fever, rash, lymphadenopathy, and/or facial swelling, associated with involvement of other organ systems (hepatitis, nephritis, hematologic abnormalities, myocarditis, myositis) during therapy. May resemble an acute viral infection. Eosinophilia is often present. Discontinue therapy if signs occur.\nLab Test Considerations:\nMonitor viral load and CD4 cell count regularly during therapy.\nMonitor CCr prior to starting therapy and when atazanavir/cobicistat is co-administered with tenofovir disoproxil fumarate. Cobicistat causes modest ↑ serum creatinine and declines in estimated creatinine clearance without affecting renal glomerular function. If serum creatinine ↑ >0.4 mg/dL from baseline, monitor renal frequently.\n- Monitor urine glucose and urine protein when administering with tenofovir disoproxil fumarate at baseline and periodically during therapy. May cause ↑ serum amylase, lipase and hyperglycemia.\n- Assess liver function tests prior to starting therapy and periodically during therapy in patients with hepatitis B or C virus infections. May ↑ liver enzymes.\n- May ↑ creatine kinase.\n- May cause ↑ in unconjugated bilirubin; reversible on discontinuation.\n- PO Administer once daily with food.\n- Take with other antiretroviral agents as prescribed.\n- Emphasize the importance of taking atazanavir/cobicistat with food as directed. Advise patient to read the Patient Information before taking and with each Rx refill; in case of changes. Atazanavir/cobicistat must always be used in combination with other antiretroviral drugs. Do not take more than prescribed amount and do not stop taking without consulting health care professional. Take missed doses as soon as remembered if less than 12 hrs of dose, then return to regular dose schedule. If within 12 hr of next dose, omit dose and take next dose at regular time. Do not double doses.\n- Instruct patient that atazanavir/cobicistat should not be shared with others.\n- Inform patient that atazanavir/cobicistat does not cure HIV or prevent associated or opportunistic infections. Atazanavir/cobicistat may reduce the risk of transmission of HIV to others through sexual contact or blood contamination. Caution patient to use a condom and to avoid sharing needles or donating blood to prevent spreading the HIV virus to others.\n- Instruct patient to notify health care professional of all Rx or OTC medications, vitamins, or herbal products being taken and consult health care professional before taking any new medications, especially St. John's wort; interactions may be fatal.\n- May cause dizziness. Caution patient to notify health care professional if this occurs and to avoid driving and other activities requiring alertness until response to medication is known.\n- Instruct patient to notify health care professional immediately if signs and symptoms of hepatitis (flu-like symptoms, tiredness, nausea, lack of appetite, yellow skin or eyes, dark urine, pale stools, pain or sensitivity to touch on right side below ribs), skin reactions with symptoms (fever, general malaise, muscle or joint aches, blisters, oral lesions, conjunctivitis, facial edema), gallbladder disorder (right or middle upper stomach pain, fever, nausea, vomiting, or yellowing of skin and whites of eyes), kidney stones (side pain, blood in urine, pain upon urination), change in heart rhythm, high blood sugar, or signs of immune reconstitution syndrome (signs and symptoms of an infection) occur.\n- Inform patient that redistribution and accumulation of body fat may occur, causing central obesity, dorsocervical fat enlargement (buffalo hump), peripheral wasting, breast enlargement, and cushingoid appearance. The cause and long-term effects are not known.\n- Rep: Advise females of reproductive potential to avoid pregnancy and breast feeding during therapy. Instruct females using hormonal contraceptives to use an effective alternative nonhormonal method of contraception. Advise patient to notify health care professional immediately if pregnancy is planned or suspected or if breast feeding. If pregnant patient is exposed to atazanavir/cobicistat, register patient in Antiretroviral Pregnancy Registry by calling 1-800-258-4263. Monitor neonates exposed to atazanavir in utero for development of severe hyperbilirubinemia during first few days of life.\n- Emphasize the importance of regular follow-up exams and blood counts to determine progress and monitor for side effects.\n- Delayed progression of HIV and decreased opportunistic infections in patients with HIV.\n- Decrease in viral load and increase in CD4 cell counts.\nNursing Central is an award-winning, complete mobile solution for nurses and students. Look up information on diseases, tests, and procedures; then consult the database with 5,000+ drugs or refer to 65,000+ dictionary terms. Complete Product Information."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b195bb79-a3f6-41f9-903d-05eb1fa0b6ca>","<urn:uuid:bfe911e2-6283-44f8-81f4-05bf49a251f2>"],"error":null}
{"question":"What's the difference in stability requirements between fixed magnification spotting scopes and assistive magnification tools like KwikLoupe?","answer":"Fixed magnification spotting scopes require significant stability, typically needing a tripod for steady viewing, especially at higher magnifications. They have fewer moving parts and are generally more reliable but less flexible. In contrast, KwikLoupe, which offers magnification from 2x to 64x, is designed to work around the mouse pointer and doesn't require additional stabilization equipment, making it more suitable for computer screen viewing.","context":["Skip to main content\nThe Nelson Poynter Memorial Library at USF St. Petersburg campus is open for reduced hours.\nPlease use the button below to find out everything you need to know.\nFree Assistive Technology Tools\nThe following Assistive Technology tools are free to use and are organized by access needs.\n- Non-Visual Desktop Access (NVDA): Fully-functional screen reader for the Windows Operating System. It has the ability to run from a USB drive with no installation.\n- Fire Vox: Talking browser extension for the Firefox web browser. Basic features such as identifying headings, links, images, and navigational assistance are included.\n- System Access To Go: Fully-functional screen reader for the Windows Operating System that runs from the web. It also works as a talking screen magnifier. It prompts for a download to maintain connection with the web client\n- Virtual Magnifying Glass: Cross-platform screen magnifier. Magnification ranges from 1x to 32x. Viewing window width and height can be adjusted.\n- KwikLoupe: Viewing window magnifies the area around the mouse pointer and ranges from 2x to 64x.\n- CLC Star: Browser extension for the Firefox web browser. Allows for enlarging images, increasing font size, changing font style and color, changing background color, enhancing focus indicator, increasing link and word spacing, and more.\n- System Access to Go: Fully-functional screen reader for the Windows Operating System that runs from the web. It also works as a talking screen magnifier. It prompts for a download to maintain connection with the web client.\n- Speak Easy: Reads text copied to the clipboard. Voices settings can be adjusted.\n- Orato: Reads text copied to the clipboard with the ability to display text and highlight words or sentences as they are read. It has the ability to run from a USB drive with no installation.\n- Natural Reader: Converts text to speech and can create audio files in mp3 or WAV (i.e. for iPod).\n- PowerTalk: Speaks any presentation or slide show running in Microsoft PowerPoint for Windows.\n- ClickSpeak: Browser extension for Firefox web browser. Mouse-driven interface that reads text based on point-and-click interaction.\n- Word Talk: Plug-in for Microsoft Word that will speak the text of a document and highlight as it reads. It also includes a talking dictionary.\n- WordFlashReader: Flashes each word of a document sequentially on the screen. Font size, type, color, rate, and amount displayed can be adjusted.\n- Tbar: A colored bar that can be dragged around the screen or locked to the mouse pointer. Color, transparency, and optional ruler lines can be adjusted and saved between sessions.\n- Vu-Bar: On-screen slotted ruler that can be dragged around the screen or locked to the mouse pointer. Width and height can be adjusted.\n- Spr-OT: Spreadsheet slot to block columns or rows for easier visual focus on selected cells. Bar can be flipped horizontally or vertically, moved, hidden, locked to the mouse pointer, and adjusted for height and width.\n- RapidSet: Allows changing of the background and font colors without going through the Windows control panel.\n- Washer: Renders everything within the viewing window into grayscale.\n- Typoscope: A reading guide which has a cutout section, to reduce glare and the amount of text visible, making it easier to focus on a line of text. Color shading can be adjusted.\n- Fx Toolbar: Plug-in for Microsoft Word 2007. A toolbar with options to highlight, collect text in a new document, speak hig hlighted text, indentify confusables, and more.\n- Sonar 4: Provides a large ring around the mouse pointed for easier tracking. Color, shape, size and width can be adjusted.\n- Big Cursors: Selection of bigger mouse cursors for easier tracking.\n- Chunky Cursors: Selection of chunky cursors for easier tracking.\n- LetMeType: Typing program that analyzes your typing and predicts words in order to increase typing rate. Works with most word processing and text input programs.\n- Click-N-Type: On-screen keyboard that can be controlled with a mouse, trackball, touch-screen, or other pointing devices. Works with all word processing and text input programs.\n- Downloadable Mind Mapping Tools:\n- Web-based Mind Mapping Tools:\n- Web-based Bookmarking and Note-taking Tools:\n- NexTalk: Instant messaging program which allows users to place calls to TTY or other NexTalk messenger users.","The ideal magnification for a hunting spotting scope typically ranges from 20x to 60x. This magnification allows for a balance between detail and field of view.\nSelecting the perfect hunting spotting scope is vital for outdoor enthusiasts who aim for accurate wildlife observation and spotting distant game. A scope with a magnification of 20x to 60x offers the versatility needed in varying terrains and weather conditions.\nHigher magnifications can provide exquisite detail, essential for hunters who need to assess game from afar. Yet, a lower end of 20x is crucial when searching larger areas, as it offers a wider field of view to spot movement quickly. A well-chosen spotting scope enhances your hunting experience, ensuring you’re prepared for that critical moment when precision counts. It’s the hunter’s ally in the wilderness, merging portability with powerful viewing capabilities.\nChoosing The Right Spotting Scope\nHunting demands precision, and a spotting scope is a critical tool to enhance that. Spotting scopes bring distant targets into clear view so you can plan your approach or take that crucial shot with confidence. The ideal magnification for a hunting spotting scope often depends on the specific needs of the hunter and the terrain. Let’s dive into the key features hunters should focus on to select the perfect optic for their next expedition.\nKey Features Hunters Should Look For\n- Magnification Range – Ideal magnification typically ranges from 20x to 60x.\n- Lens Quality – Look for high-quality glass to ensure a crisp image.\n- Objective Lens Diameter – A larger lens gathers more light, aiding in low-light conditions.\n- Weight and Portability – Lightweight scopes are easier to carry in the field.\n- Weather Resistance – Waterproof and fog-proof features are essential for reliability.\n- Eye Relief – Sufficient eye relief makes for comfortable extended viewing, especially for eyeglass wearers.\nThe Role Of Spotting Scopes In Hunting\nSpotting scopes serve a crucial role in hunting. They allow hunters to observe wildlife without disturbing them. By providing detailed views at long ranges, hunters can assess game size, health, and behavior from afar. A smart combination of magnification, lens quality, and field of view ensures hunters make informed decisions before taking a shot. This can mean the difference between a successful hunt and a missed opportunity.\nChoosing the right magnification for a hunting spotting scope is crucial. It affects how well you see targets from afar. Let’s explore the ideal magnification for a spotting scope and what to consider when selecting one.\nUnderstanding Magnification Ranges\nSpotting scopes come in various magnification ranges. These ranges suit different types of hunting. A lower magnification offers a wider field of view. This is important for tracking moving targets. On the other hand, a higher magnification lets you see fine details from a distance, which is essential for long-range shooting.\n- Lower magnification: 15x to 30x\n- Higher magnification: 30x to 60x and above\nBenefits Of Higher Magnification\nHigher magnification is best for stationary targets. It allows you to evaluate your game carefully. You can also judge the trophy size or spot hidden animals in dense areas. Here are some benefits:\n- Better detail at long range\n- Improved target assessment\n- Useful in open terrain\nLimitations Of Higher Magnification\nDespite its advantages, higher magnification has drawbacks. It narrows your field of view. This makes it harder to track moving animals. Also, high magnification scopes can be less stable and more affected by atmospheric conditions, such as heat waves or winds.\n|Narrow field of view\n|Hard to track fast-moving targets\n|Image blur with hand-held use\n|Degrades image quality at high zoom levels\nFixed Vs. Variable Magnification\nChoosing the right magnification for a hunting spotting scope is vital. Hunters seek either fixed or variable magnification scopes. Fixed scopes have a set zoom level. Variable scopes offer a range of zoom levels. Each type has benefits and drawbacks. Knowing these can help you decide which spotting scope suits your hunting needs.\nPros And Cons Of Fixed Magnification Scopes\n- Simplicity: Without zoom adjustments, they’re easy to use.\n- Reliability: Fewer moving parts mean less can go wrong.\n- Cost-Effectiveness: They tend to be more affordable than variable scopes.\n- Limited Flexibility: The singular magnification may not fit all situations.\n- Range Viewing: For distant or close-up viewing, these scopes can fall short.\nPros And Cons Of Variable Magnification Scopes\n- Versatility: They offer a range of magnification options.\n- Adaptability: Ideal for varying distances and target sizes.\n- Detailed Viewing: Zoom in for a closer look at your target.\n- Complexity: Takes time to learn the zoom features.\n- Maintenance: More components could mean more upkeep.\n- Price: They’re generally more expensive than fixed scopes.\nThe Ideal Magnification For Different Game\nChoosing the right magnification for a hunting spotting scope is crucial. Each type of game demands a particular zoom level. Hunters must consider this to ensure a successful outing.\nSmall Game Hunting\nSmall game includes animals like rabbits and squirrels. Lower magnifications work best for these targets. They quickly move through dense brush and undergrowth. A magnification between 20x to 40x is ideal. It offers a broad field of view. This helps in tracking fast-moving creatures.\nBig Game Hunting\nWhen hunting deer, elk, or bear, higher magnifications are necessary. The ideal range starts from 30x to 60x. Spotting scopes with the ability to zoom in and out offer flexibility. This is important for varying distances in open terrain.\nBirding And Other Uses\nBirders need a different approach. Details like feather patterns are essential. Magnifications from 20x to 50x are suitable. When observing from a distance, stability is key. Tripods are recommended for all high-power observations.\nField Conditions And Magnification\nIn the realm of hunting, spotting scopes are vital for clarity and success. The ideal magnification proves essential in different field conditions. Let’s dive into how weather, terrain, and the distance to your target influence the perfect magnification choice for a hunting spotting scope.\nImpact Of Weather\nWeather shapes the hunting experience. Low visibility conditions like fog or mist require scopes with lower magnification, ensuring clearer images. Conversely, bright and clear days allow for higher magnification, providing more detail at greater distances.\nThe landscape is a critical factor. Wide, open spaces benefit from powerful scopes, usually in the 20x to 60x range. For wooded or brush-dense areas, lower magnification around 15x to 30x offers better maneuverability and wider viewing angles.\n- Open terrains: Higher magnification\n- Dense terrains: Lower magnification\nDistance To Target\nDistance determines detail. Targets far away require higher magnification to spot with precision. For closer proximity, a scope with 10x to 20x magnification suffices, offering a broad field of view and faster target acquisition.\n|10x to 20x\n|30x to 60x\nTechniques For Maximizing Spotting Scope Usage\nMastering the use of a hunting spotting scope transforms distant details into clear, actionable information. Whether tracking a deer across a field or observing the subtle movements of distant game, applying the right techniques ensures the full potential of the optic’s power.\nEffective Spotting Scope Practices\n- Choose the right magnification based on distance and movement\n- Scan methodically to cover the area systematically\n- Use a tripod for stability to prevent image shake\n- Employ a proper rest when handheld is necessary\nMaintaining Steady Vision\nStable support is key to a crisp view. Even the steadiest hands can’t match the stillness of a well-anchored tripod. In the absence of a tripod, leaning against a solid surface or using a beanbag can help. Relax your eyes periodically to prevent strain and maintain clarity.\nManaging Low Light Conditions\nLow light challenges scope visibility. Spotting scopes with larger objective lenses capture more light, enhancing sight in these conditions. Users should also reduce magnification in dim environments, as this increases the exit pupil size, allowing more light to reach the eye.\n|Dawn or Dusk\n|Use lower magnification\n|Maximize objective lens use\n|Employ mid-range magnification\nRemember, a spotting scope is an essential tool that extends a hunter’s eyes. Effective techniques and practice ensure its optimal use. A firm, steady base, proper light management, and the right magnification setting are crucial factors that enable users to make the most of their hunting spotting scopes.\nFrequently Asked Questions Of What Is The Ideal Magnification For A Hunting Spotting Scope?\nWhat Magnification Is Best For Hunting Scopes?\nTypically, a magnification between 20x to 60x is favored for hunting spotting scopes. This range is versatile enough to spot distant game while maintaining a wide field of view. High magnification may offer more details, but it can reduce image brightness and stability.\nHow Does Magnification Affect Spotting Scope Hunting?\nMagnification impacts the clarity and stability of the image you see. Lower magnifications provide a wider field of view, making it easier to locate moving targets. Higher magnifications may yield more detail but require a steadier hand or a tripod for a stable view.\nIs Higher Magnification Better For Spotting Scopes?\nNot necessarily; higher magnification can mean a narrower field of view and less light gathering ability, which can be detrimental in low light situations. Choosing a magnification that balances detail and brightness is key for an effective hunting spotting scope.\nWhat Should I Consider When Choosing A Hunting Scope?\nWhen choosing a hunting spotting scope, consider the magnification, the quality of the optics, the ruggedness of the design, and its waterproof and fog-proof capabilities. Also consider the weight and ease of carrying if you’ll be moving often.\nChoosing the right magnification for your hunting spotting scope is key to enhancing your outdoor experience. Strike a balance between detailed views and a wide field observation. Remember, higher isn’t always better; it should match your specific needs and terrain.\nGet the magnification that brings your prey into clear focus and guarantees a successful hunt."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f779c459-2968-4ab7-a49e-021f9a95a206>","<urn:uuid:7c8e79dc-a082-4874-ad67-e672920340eb>"],"error":null}
{"question":"How does the micromechanical foam model (MFM) account for foam properties in its predictions?","answer":"The micromechanical foam model (MFM) predicts time-dependent, non-linear large-strain behavior of polymer foams by considering both the viscoelastic response of the material and explicitly accounting for the foam's density and initial pore pressure inside the foam.","context":["Welcome to PolyUMod\nThe PolyUMod® library is a library of advanced (and accurate) user-material models for finite element modeling of polymers, biomaterials, and other non-linear materials. Commercial finite element codes still lack the material models needed to accurately simulate many types of materials. Polymer FEM has developed a library of general purpose material models that cover virtually all polymer systems, including thermoplastics, thermosets, elastomers, foams, filled plastics, and biomaterials. The PolyUMod library also include specific models for particular formulations, such as fluoropolymers and UHMWPE.\nThe PolyUMod library works with the following FE software\nThe material models in the PolyUMod library can predict the non-linear, time-dependent, anisotropic, viscoplastic response of all polymers. Here are predicted accuracies of 3 common polymers.\nCommon PolyUMod Material Models\nThe PolyUMod library contains a large number of material models. The following table lists some of the more commonly used models.\nThe Bergstrom-Boyce model with enhanced Ogden-Roxburgh Mullins effect (BBM) is the same as the BB-model, except that the eight-chain hyperelastic Network A include a Mullins damage term.\nThe anisotropic BB-model with Mullins damage is an extension of the original BB-model in which the hyperelastic response is captured using the anisotropic eight-chain model.\nThe Hybrid Model (HM) was specially developed for predicting the large deformation, time-dependent response of UHMWPE. The HM is typically not as advanced as the Three Network (TN) model, which is also quite accurate for UHMWPE and other thermoplastics.\nThe Dual Network Fluoropolymer (DNF) model was specially developed for predicting the large-strain, viscoplastic reponse of Fluoropolymers. The model is based on three parallel networks and supports volumetric flow.\nThe micromechanical foam model (MFM) is an advanced model for predicting the time-dependent, non-linear large-strain behavior of polymer foams. The MF-model is unique in that it not only considers the viscoelastic response of the material but also explicitly takes into account the density of the foam and the initial pore pressure inside the foam.\nThe three network foam model (TNFM) is a material model specifically developed for thermoplastic materials that are available as a foam. It is a combination of the three-network model (TNM) and the microfoam model (MFM). The TNFM explicitly incorporates the effects of different reduced densities.\nThe Dynamic Bergström-Boyce (DBB) model is an advanced constitutive model specifically developed for predicting the time-dependent, dynamic, large-strain behavior of elastomer-like materials. This model is an extension of the BB-model.\nThe Silberstein-Boyce model (SB) was developed for predicting the large strain, time-, temperature-, and hydration dependent response of Nafion. This material is often used as a polymer electrolyte membrame (PEM) in batteries, solar cells and fuel cells. The material response of this type of material is similar to many other thermoplastics, except that it has a unusually strong dependence on the moisture level.\nThe Flow Evolution Networks (FEN) model was developed to obtain an advanced multi-network model that is similar to the Parallel Network Model, but more numerically efficient and easier to use. The FEN model is suitable for elastomers, thermoplastics, and other isotropic thermoplastic materials.\nThe Three Network Viscoplastic (TNV) model is a general purpose viscoplastic material model capable of capturing the experimentally observed behaviors of many thermoplastics, including time-dependence, pressure-dependence of plastic flow, pressure-dependent volumetric response, damage accumulation, and triaxiality-dependent failure.\nThe Parallel Network (PN) model is the most advanced material model in the PolyUMod library. Each network in the PN model can have an isotropic or anisotropic hyperelastic response in series with an isotropic or anisotropic viscoplastic flow element. Each element can have temperature dependence and damage evolution. The PN model also support more than 20 different failure and damage models."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:55292587-12e0-4f41-8a66-142a801cf885>"],"error":null}
{"question":"I'm studying military history - can you compare how the Army Reserve and National Guard were used for law enforcement historically?","answer":"The Army Reserve and National Guard had different roles in law enforcement historically. Army Reserve troops were prohibited from being used as domestic law enforcement under the Posse Comitatus Act. The National Guard, however, could be activated by state governors for law enforcement purposes. For instance, the Connecticut Air National Guard's 118th Squadron was called up for riot control during the textile workers strike at Putnam, Connecticut in September 1934.","context":["2020-02-18 08:15 Jun 29, 2018 Command Differences. The primary difference between the guard and reserve components lies in the command. Reserve units are part of the federal armed forces, and as such they are under presidential command. Guard units are organized on the state level, and the governor can call them to service in response to civil riots or natural disasters.\nMay 17, 2018 It has a smiliar mission as the Army, except these are reserve troops. Army Reserve troops are prohibited from being used as domestic law enforcement under the Posse Comitatus Act. National Guard. Each state has their own National Guard, and technically it is the state's military, not the federal military. difference between national guard army reserve\nJan 07, 2005 The parttime soldiers of the Army Reserve can also be tossed into active duty for the same amount of time. Thats not the only similarity between the reserves and the National Guard\nDifference between army reserve and National Guard. The main role of these people is to be available and able to fight when a nation is fighting a war or is under the invasion of another power. It must be noted that these are the reserve forces and they are not a difference between national guard army reserve\nGuard and Reserve Service Explained. If you're considering joining the military for the first time, or you're a military veteran looking for additional benefits or further opportunities to serve your country, National Guard and Reserve programs provide flexibility and rewards. Guardspeople and Reservists play a critical role in national defense, Key Difference: The United States Army Reserve (USAR) is the Federal Reserve Force of the United States Army. The National Guard includes the Army National Guard of the United States and the Air National Guard of the United States. Together, the Army Reserve and the Army National Guard make up the Reserve components of the United States Armed Forces. Difference Between Guard and Reserve. The primary function of reserves and guards is to provide a reserve component to active duty soldiers, whenever the need be. After joining and undergoing training, guards and reserves are required to perform duties on one weekend every month and 14 days in a year. difference between national guard army reserve Army Reserve is a part of the army that become active only when the orders are given. Until then, they are not active. National Guard can be identified as the army of each state. For Army Reserve, President is the leader. For National Guard, it is the Mayor of the State. However, National Guard can also be a part of the army, if necessary. Mar 16, 2010 The US Army Reserves are reserve troops ran by the federal government. There are no combat MOS positions in the reserves. The reserves are Combat Service and Support. The reserves are there to hep supplment the needs of theactive duty army. The National Guard is ran and owned by each State and US Territory. Apr 19, 2012 Whats the difference between the army national guard and army reserve? I was told all branches of the military go overseas including national guard but is the army reserve like the national guard where they protect our borders caus I was thinking about joining the national guard but I'm not sure if I wanna join any branch of the military How can the answer be improved?\nThe difference between Server Side and Client Side! Now that you know where the terms are used, its time to strike you with a pure example. If you play an online game, consider a game like Clash of Clans or Clash Royale, youll find it difficult to hack if compared to an offline PC game.\nLeeching Information. Leeching is a form of gaining experience used by some players. It should be noted that if the level difference between the leech and the monster being trained on is too great, the leech will not receive much (if any) experience. In order to leech passively (without participating at all), you cannot be more than 5\nThe time difference between Macau and Thailand is 1 hour(s). While in Macau the actual local time is? ? : ? ? , in Thailand the time is? ? : ? ? . By car, it would take you approximately 20. 91 hours to get to Bangkok, Thailand driving nonstop from Macau, Macau at a constant speed of 80 kilometers per hour.\nDifference Between Insects and Spiders. The body of the spider is segmented into two tagma; cephalothorax (fused head and thorax) and abdomen. Abdomen is mainly the reproductive unit as in the other insects and arthropods. The cephalothorax bears four pairs of\nWhat is the difference between uplink and downlink? The communication going from a satellite to ground is called downlink, So generally frequency for uplink is kept higher than the downlink. There is greater attenuation due to rain when the signal frequency is high.\nMost fans of OD and Beiber have lives outside their obsession, and b And their bronies are too diverse too firmly root them as obsessed. The bros on the other hand are constantly spamming YouTube comments and trying to think they are superior and that they're part of something big. It's a close competition between the Bros and Bronies.\nYou can use other brands with the same chemistry. I have a NiMh charger I use for many different brands of NiMh batteries, a NiCad charger for NiCads, and a 12V charger for leadacid (switchable for regular, deep cycle and AGM).\nTypes of Orchids Moth Orchids. The most common type of orchid is probably the moth orchid, Dendrobium Orchids. Another type of orchid you can grow at home is what I call Slipper Orchids. My favorite type of orchid is the lady slipper orchid because Boat Orchids. Here in Southern\nFor those of you who have a Toyota Tacoma 4x4 with the locking rear differential locker that came from Toyota, you have probably been asking your self why it only works in 4wheel drive low. Or maybe you have wondered how you can lock the rear differential in 4wheel high or even in 2wheel drive.\nThe biggest bottleneck in performance testing is that it requires well experienced man power. Besides, since performance testing aims to test the performance of system in terms of speed, response time etc, it also calls for expensive tools. Some of the other bottelnecks are: 1. 2. 3. Network bottlenecks.\nFeb 12, 2018 If you were upgrading WordPress or your Theme and files were quarantined by (ARQ) due to a procedural error or server processing issueerror\nPreSonus Studio One 4 Professional Upgrade from Artist. Create without boundaries, produce without limits. Electronic Delivery. Tabs. Description This is an upgrade for current owners of any version of Studio One Artist. PreSonus Studio One 4 Professional Upgrade from 149. 95. Add To Cart.\nShowing top 8 worksheets in the category Writing A Paragraph Grade 4. Some of the worksheets displayed are How to write a paragraph, 4th and 5th grade writing folder, Grade 4 module 1 unit 2 lesson 5 paragraph writing, Staar expository writing rubric grade 4, 50 engaging mini lessons activities and student, Improving a paragraph name, Susan anthony, Name writing introduction paragraphs paragraphs.\nA monthbymonth guide of the best read aloud chapter books for 1st graders. A printable list is included to keep track of all the chapter book readalouds you read to the first graders\nThe update for the WhatsApp for the later version is currently offered on the thirdparty application company which is referred to as APK Whatsapp 2019. Certainly, we will locate this type of data when we try to download this later version of WhatsApp on the net.\nMy Sons Grade 8 Valedictorian Speech June 2011. June 29, 2011 January 9, 2013 by change4life, posted in Love, Spirit. Valedictorian Speech. We were infants, toddlers, then young children and now grade 8 graduates. Although, we stand here tonight looking very similar, we are actually all very different.\nChapter 6 Atmosphere Chapter 7 Weather Chapter 8 Life's Structure And Classification UNIT 3 Biodiversity How Are Cotton and Cookies Connected?\nThe Washington Post's higher education blog reports about local and national higher education. 1. Desktop notifications are on gradepoint. Success! Check your inbox for details.\nEZSchool's Grade 3 page. Practice with 192 activites.\niOS includes bug fixes and improves the security of your iPhone or iPad. This update: Fixes an issue that prevented some users from viewing the last known location of their AirPods in Find My iPhone","|Connecticut Air National Guard|\n118th Airlift Squadron C-21A Learjet. The 118th AirliftSquadron is the oldest unit in the Connecticut Air National Guard, having over 90 years of service to the state and nation\n|Active||1 November 1923 - present|\n|Branch||Air National Guard|\n|Role||\"To meet state and federal mission responsibilities.\"|\n|Size||Approximately 1,200 airmen and officers|\nConnecticut Office of Military Affairs|\nUnited States National Guard Bureau\n|Garrison/HQ||Connecticut Air National Guard, Bradley Air National Guard Base, 206 Boston Post Road Orange, Connecticut, 06477|\nPresident Barack Obama|\nMichael B. Donley\n(Secretary of the Air Force)\nGovernor Dan Malloy\n(Governor of the State of Connecticut)\nMajor General Thaddeus J. Martin |\nBrigadier General Daniel L Peabody\n|Emblem of the Connecticut Air National Guard|\nThe Connecticut Air National Guard (CT ANG) is the air force militia of the State of Connecticut, United States of America. It is, along with the Connecticut Army National Guard, an element of the Connecticut National Guard.\nAs state militia units, the units in the Connecticut Air National Guard are not in the normal United States Air Force chain of command. They are under the jurisdiction of the Governor of Connecticut though the office of the Connecticut Adjutant General unless they are federalized by order of the President of the United States. The Connecticut Air National Guard is headquartered at Bradley Air National Guard Base, and its commander is Brigadier General Daniel L Peabody.\nOverview[edit | edit source]\nUnder the \"Total Force\" concept, Connecticut Air National Guard units are considered to be Air Reserve Components (ARC) of the United States Air Force (USAF). Connecticut ANG units are trained and equipped by the Air Force and are operationally gained by a Major Command of the USAF if federalized. In addition, the Connecticut Air National Guard forces are assigned to Air Expeditionary Forces and are subject to deployment tasking orders along with their active duty and Air Force Reserve counterparts in their assigned cycle deployment window.\nAlong with their federal reserve obligations, as state militia units the elements of the Connecticut ANG are subject to being activated by order of the Governor to provide protection of life and property, and preserve peace, order and public safety. State missions include disaster relief in times of earthquakes, hurricanes, floods and forest fires, search and rescue, protection of vital public services, and support to civil defense.\nComponents[edit | edit source]\nThe Connecticut Air National Guard consists of the following major unit:\n- Established 1 November 1923 (as: 118th Observation Squadron); operates: C-21A Learjet\n- Stationed at: Bradley Air National Guard Base, Windsor Locks; Gained by: Air Mobility Command\n- The \"Flying Yankees\" of the 103rd Airlift Wing are the third oldest Air National Guard unit in the United States, tracing their lineage back over 90 years of military aviation. They currently fly the C-21A, a twin turbofan engine aircraft and its mission is providing cargo and passenger airlift. The aircraft is the military version of the Lear Jet 35A business jet.\nSupport Unit Functions and Capabilities:\n- 103d Air Control Squadron\n- Stationed at Orange and is known as \"Yankee Watch\". The mission of the 103d Air Control Squadron is real-time detection, identification and surveillance of air traffic for combat operations and homeland defense. The 103d ACS is the oldest unit of its kind in the United States military.\nHistory[edit | edit source]\nThe 118th Airlift Squadron's origins date to August 1917, when the unit was activated as the 118th Aero Squadron at Kelly Field, Texas. After little more than basic individual and unit training at Kelly, the squadron departed by train for the East Coast en route to France. After a brief stopover at Garden City, Long Island, New York, they sailed for Europe on the 13th of January 1918, arriving at St. Maixent, France on the 29th of that month. The squadron, re-designated the 639th Aero Service Squadron in France, was credited with honorable service from January to November, 1918, but as a construction and support unit, it saw no real combat action. The Fleur-de-lis on the post World War II squadron insignia reflects that service in France.\nConnecticut National Guard[edit | edit source]\nThe Militia Act of 1903 established the present National Guard system, units raised by the states but paid for by the Federal Government, liable for immediate state service. If federalized by Presidential order, they fall under the regular military chain of command. On 1 June 1920, the Militia Bureau issued Circular No.1 on organization of National Guard air units. The National Defense Act of 1921 provided for a number of National Guard Aviation Squadrons and the 43d Aero Squadron was re-designated as the 43d Division Air Service Squadron.\nAs a National Guard unit the squadron became a part of the 43d Division, at that time made up of National Guard Troops from Connecticut, Rhode Island and Vermont. Since there were no airfields in Connecticut capable of handling military type aircraft, the 118th was initially assigned to the Rhode Island National Guard for duty.\nHowever, after the opening of Brainard Field in Hartford in October 1922, efforts were immediately launched to secure the Air Service unit of the 43d Division for the State of Connecticut. Rhode Island, apparently without a great deal of argument, soon relinquished its claim and the squadron was reassigned to Connecticut as the 118th Observation Squadron, Connecticut National Guard.\nWhen the squadron was officially organized on 1 November 1923, there were some 66 officers and enlisted men officially on board. During the 1920s and 1930s, the 118th \"grew and prospered\". Originally issued with obsolete Curtiss JN-4 \"Jennies\" left over from World War I, the unit was later equipped with experimental Curtiss OX-12's with rotary engines and a swept-wing design. The squadron, or elements thereof, called up to perform the following state duties: riot control at the textile workers strike at Putnam, CT, in September 1934; and flood relief at Hartford, CT, 19 March-1 April 1936. Conducted summer training at Mitchell Field, NY, or Trumbull Field, CT. Detachments were sent some years to fly spotter missions during the summer training of the 192d Field Artillery Regiment.\nThe 118th entered the 1940s with war in Europe already a reality and eventual U.S. involvement becoming more and more likely. The 118th was preparing to meet that eventuality. In 1940 the squadron was detached from the 43rd Division to become a part of I Army Corps, Aviation. Simultaneously, plans were being drawn up \"for the entire unit to move to Jacksonville, Florida for intensive training over a period of an entire year\".\nConnecticut Air National Guard[edit | edit source]\nOn 24 May 1946, the United States Army Air Forces, in response to dramatic postwar military budget cuts imposed by President Harry S. Truman, allocated inactive unit designations to the National Guard Bureau for the formation of an Air Force National Guard. These unit designations were allotted and transferred to various State National Guard bureaus to provide them unit designations to re-establish them as Air National Guard units.\nThe modern Connecticut ANG received federal recognition on 7 August 1946 as the 103d Fighter Group at Bradley Army Airfield, Windsor Locks. The mission of the 103d Fighter Group was the air defense of Connecticut. It was assigned the 118th Fighter Squadron, equipped with F-47D Thunderbolts. 18 September 1947, however, is considered the Connecticut Air National Guard's official birth concurrent with the establishment of the United States Air Force as a separate branch of the United States military under the National Security Act.\nDuring the Korean War, the Connecticut Air National Guard was federalized on 10 February 1951 with the 103d Fighter Group being re-designated as the 103d Fighter-Interceptor Group, and the 103d Fighter-Interceptor Wing being established by Air Defense Command on 2 March 1951. The 118th also being re-designated as a Fighter-Interceptor squadron.\nThe 103d was assigned to the Air Defense Command Eastern Air Defense Force and moved to Suffolk County AFB, New York on 1 June 1951, flying air defense missions with their F-47D Thunderbolts. On 1 February 1952 the 103d FIW and assigned groups were inactivated by ADC, the 118th FIS being assigned to the 4709th Air Defense Wing at McGuire AFB, New Jersey. It should also be noted that during its period of federalization, the 118th FIS transferred many of its pilots and ground support personnel to Fifth Air Force, where they served in combat in Korea.\nUntil 2008, the organization was known as the 103rd Fighter Wing (103 FW), operationally-gained by the Air Combat Command (ACC) and equipped with A-10 Thunderbolt aircraft. As a result of Base Realignment and Closure (BRAC) actions, the wing's A-10 fighter aircraft were reassigned to other units and the 103rd reequipped with C-21 Learjet aircraft as a \"placeholder\" flying mission under the Air National Guard's VANGUARD program until the 103rd's next flying mission could be determined. Following this change in mission, the unit was redesignated the 103rd Airlift Wing and placed under the operational claimancy of Air Mobility Command (AMC). The 103d Airlift Wing is based in East Granby at the Bradley Air National Guard Base at Bradley International Airport.\nNotable Personnel[edit | edit source]\nReferences[edit | edit source]\n- \"Our Mission\". Connecticut Air National Guard. U.S. Air Force. http://www.ctbrad.ang.af.mil/ourmission.html. Retrieved 2007-07-17.\n- The history of the 639th Aero Squadron, United States Army Air Service, 1920\n- ANG Chronology 1908-2007, see also Brief History of the Minnesota Air National Guard and the 133rd Airlift Wing, 1.\n- Flying Yankees, a history of the first fifty years of the Connecticut Air National Guard. Compiled by Colonel Carl D. Jenson [and] CMSGT. Edward W. Burton. c1973.\n- Rosenfeld, Susan and Gross, Charles J (2007), Air National Guard at 60: A History. Air National Guard history program AFD-080527-040\n- Strategic Analysis of Air National Guard Combat Support and Reachback Functions. Oai.dtic.mil. Retrieved on 2013-07-15.\n- \"Biographical Data - John L. Swigert, Jr.\". Johnson Space Center - Astronaut Biographies. NASA. January 1983. http://www.jsc.nasa.gov/Bios/htmlbios/swigert-jl.html. Retrieved 2007-07-19.\n- Gross, Charles J (1996), The Air National Guard and the American Military Tradition, United States Dept. of Defense, ISBN 0160483026\n- Connecticut National Guard website\n[edit | edit source]\n|Wikimedia Commons has media related to Connecticut Air National Guard.|\n- “The Flying Yankee Squadron” by Richard W. Owen. In Connecticut circle v.2:no.9 1939:Nov. p. 3-5. About the 118th Observation Squadron.\n- Bradley Air National Guard Base at GlobalSecurity.org\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:8cbb43e9-c2c2-42c2-86af-cb59c32da4c7>","<urn:uuid:4870db6c-5311-4809-8d8c-610366699279>"],"error":null}
{"question":"How do light vs dark roast K-Cups compare in caffeine content, and what recyclable options are available for both varieties?","answer":"Light roast K-Cups generally contain more caffeine than dark roasts because longer roasting times result in more caffeine loss during the roasting process. Both light and dark roast varieties are available in eco-friendly formats, including the 97% biodegradable OneCup pods and 98% recyclable capsules with biodegradable filters. These sustainable options allow users to compost the coffee grounds and recycle the remaining components, regardless of the roast type chosen.","context":["How Much Caffeine Is in a K-Cup?Table Of Contents\nIf your cup of coffee has as much to do with caffeine content as flavor, you undoubtedly want to know how much caffeine is in a Keurig cup, as compared to another cup of coffee.\nWhether coffee is part of your morning ritual or caffeine has become a necessary part of your day, many of us have turned to the convenient Keurig coffee maker. With a Keurig, coffee is as easy as popping in a single-serve pod and pressing a button. No grinder, no kettle, no filters, no mess.\nFiguring out a K-Cup’s (or another coffee pod’s) caffeine content can be tricky. Some K-Cups have too much caffeine, some don’t have enough. Lucky for you, we’ve laid it all out. So go ahead, grab a cup of coffee and join us for all you need to know about caffeine and Keurig.\nHow Much Caffeine Is in a K-Cup?\nThe caffeine content in any coffee varies greatly. According to Keurig, most K-Cups have “between 75 and 150 mg of caffeine per 250 ml (8 oz) cup” and even decaffeinated K-Cups have between “2 to 4 mg per 250 ml (8 oz) cup.”\nThere are many variables that contribute to caffeine content, but the main factors that contribute to K-Cup caffeine content are the roast level and the species of the coffee.\nKeurig Cup: Roast Levels\nWhether roasted for Keurig or for non-Keurig coffee makers, the roast level of coffee beans contributes to the amount of caffeine in each bean.\nCoffee is roasted by applying heat directly and indirectly to the coffee beans over a stretch of time. As the beans are exposed to heat, they begin to lose moisture. With the loss of moisture, the beans also lose caffeine.\nThat being the case, beans that are roasted longer and darker tend to have less caffeine than beans that are roasted for less time and are therefore lighter. Despite the classic strong taste of dark roast coffee, more often than not, light roast coffee will have more caffeine.\nTo overcome this, some roasters, such as Death Wish Coffee and even Starbucks, have formulated recipes for dark roast coffee that still have high caffeine content. Moreover, these roasters have made sure to offer K-Cups for these high-caffeine options.\nKeurig Cup: Beans\nThe beans themselves also contribute greatly to caffeine content. There are two main species of coffee plants: robusta and arabica.\nGenerally speaking, robusta coffee has a higher caffeine content at 2.2% to 2.7% than arabica coffee which is closer to 1.2% to 1.5%, despite arabica being a more popular species. Arabica coffee tends to have more interesting flavor profiles while robusta is often easier to grow and produces a more caffeine-rich product.\nMany roasters find the best of both worlds by mixing robusta and arabica beans to create a flavorful and caffeine-dense coffee blend.\nDecaf K-Cup Caffeine Content\nOf course, there are many of us that love coffee day in and day out but need a break from caffeine! We need to know the caffeine content in our coffee cups to limit it just like we do to increase it, though. This is where decaf K-Cups come into play.\nKeurig and a variety of independent roasters offer many different “decaffeinated” options ranging from light roasts to dark roasts, Costa Rican to Ethiopian, and many more.\nIt is important to note, though, that even decaffeinated coffee has trace amounts of caffeine and cannot be 100% caffeine-free.\nStrongest K-Cups with the Most Caffeine\nI often find myself needing a few cups of coffee to make it through the day. Sometimes I wonder if my K-Cup’s caffeine content is enough for me or if I need to step it up and bring in something with more oomph!\nWith a little research and a lot of caffeine, I found some of the best extra-caffeinated K-Cups. Perfect for anyone who needs a little extra energy to start their day.\nDevil Mountain Coffee’s Black Label K-Cups bring caffeine to a new level. With over 1000 mg of caffeine per 8 oz cup, this bold blend is unmatched when it comes to caffeine. Be careful with this one!\nPopular roaster DEATH WISH is known for its high caffeine content coffee and is a fan favorite for coffee junkies internationally. Their Death Cups boast 420 mg of caffeine per pod, more than five times the average K-Cup caffeine content.\nIf the brand’s name isn’t sufficient to rouse you, perhaps their ultra caffeinated coffee will do the job. This coffee is created using a mixture of Arabica and Robusta coffee beans, containing an impressive 325mg of caffeine in every serving. Bonus: It comes in a variety of flavors.\nK-Cups with Regular Caffeine Content\nSometimes, a regular cup of coffee gets the job done just fine. We don’t always need the most caffeine out there and, when that’s the case, these K-Cups are perfect for the occasion.\nOne of Starbucks’ most popular blends, Pike Place Roast is a great middle-of-the-road brew for those looking for a caffeine boost without the jitters. Pike Place Roast averages 100 mg of caffeine per 8 oz serving.\nWith slightly more caffeine than Starbucks’ Pike Place Roast, Caribou Coffee’s Caribou Blend rings in at an average of 150 mg per 8 oz serving. This sits at the higher end of the regular K-Cup caffeine content range but still barely scratches the surface of the extra-caffeinated cups like DEATH WISH.\nFlavorful Decaf K-Cups\nFor those of you who are hoping to avoid caffeine almost entirely but can’t stand the thought of life without coffee, we’ve got you covered! Decaf K-Cups are easy to find and just as easy to enjoy.\nPer 8 oz serving, the Original Donut Shop Decaf Coffee only has 4 mg of caffeine. Not even registering on the caffeine range for regular K-Cups, you are unlikely to feel any effects from this level of caffeine.\nLikewise, Maud’s Dark Roast Decaf Coffee, Tall Dark and Handsome, has less than 5 mg of caffeine per 8 oz serving. Feel free to end your day with a cozy cup of Maud’s without the fear of restless nights.\nWhat is a K-Cup?\nKeurig coffee makers use K-Cups to brew single-serve portions of coffee. K-Cups are small plastic pods that contain enough ground coffee for a single serving brew. K-Cup is short for Keurig Cup.\nDoes a light roast have more caffeine than a dark roast?\nGenerally speaking, light roast coffees have more caffeine than dark roast coffees, whether brewed using K-Cups or whole beans.\nDo robusta beans have more caffeine?\nRobusta coffee beans have a higher caffeine content than arabica coffee beans, usually almost double.\nSo how much caffeine is in a Keurig cup? The official answer is an average of 75 mg to 150 mg for regular coffee K-Cups; however, that is a huge range, and there are plenty of options that still fall outside it.\nFor the caffeine fiends of the world, there are plenty of coffee roasters that push the limits with K-Cups ranging from 400 – 1000 mg of caffeine per serving! That’s sure to get you going in the morning.\nFor those of us who can’t kick the coffee habit but need to lose the caffeine, decaf K-Cups are an excellent way to enjoy your favorite drink without the jitters. Remember, if you are extremely sensitive to caffeine, that these decaffeinated coffees still have a minimal amount of up 5 mg of caffeine per serving.\nOf course, choosing the right K-Cup for you isn’t just about the caffeine content. It’s about the flavor, the texture, the aroma, and so much more.\nThere is a wide world of coffee to be explored and K-Cups are a great way to do it. From small-batch roasters to coffee giants like Starbucks and Peets, there are many choices for your Keurig coffee maker.\nFrom decaffeinated to ultra caffeinated, K-Cups have you covered for all your coffee needs!","In the past year, manufacturers have begun producing eco-friendly alternatives to the traditional plastic K-Cup® pods. This is exciting news for consumers, who can now enjoy single serve coffee, lattes, hot chocolates and teas without contributing to landfills. The solutions range from recyclable capsules to almost fully biodegradable pods – manufacturers are using sustainable materials in their packaging in hopes of eventually cutting plastic out of the packaging all together.\nHere is a breakdown of the different capsules and brands that feature environmentally friendly options for single serve lovers.\nRealCup® EcoCup™ Capsules\nThe RealCup® EcoCup™ capsules are designed to be recyclable. They are manufactured with easily separated components, so you can recycle and compost the appropriate parts. The EcoCup™ is compatible for use with Keurig® K-Cup® brewers.\nRecycling the EcoCup™ is easy. After brewing, let the capsule cool and then:\n- Locate the raised symbol on the side of the cup.\n- Peel away the lid from the outer plastic cup and separate the lid from the filter.\n- Recycle the outer plastic cup where #6 polystyrene plastic is accepted*. Compost the coffee grinds/tea, and place the remaining lid and filter in the trash.\n*Check with your local recycling service provider.\nThe following brands use the EcoCup™ capsule:\n97% Biodegradable OneCup™\nOneCup™ is a type of coffee pod developed by the Rogers Family Gourmet Coffee & Tea Company that is 97% biodegradable. They plan on releasing a new capsule very soon that will be 100% biodegradable! OneCup™ coffee pods come packaged in recyclable and biodegradable components for additional eco-friendliness. The OneCup™ is compatible for use with Keurig® K-Cup® brewers.\nThe ring and lid of the OneCup™ coffee pods are both created from plant based materials that are biodegradable, the box and the bag that the capsules come in are cardboard and paper products that can be recycled, and the coffee is shade grown, pure 100% Arabica Coffee. The filter is created from food grade materials, however it is not yet compostable.\nComposting the OneCup™ is easy. After brewing, let the pod cool and then:\n- Cut off the filter of the capsule after it has cooled.\n- Separate the coffee grounds and the lid from the filter.\n- Deposit the coffee grounds, lid and capsule into your compost bin.\nNote: The OneCup™ will be fully composted in 3-6 months, and composts faster in a warmer environment.\nThe following brands use the OneCup™ coffee pod:\n98% Recyclable Capsules with Biodegradable Filters\nThere are also brands that are using capsules made with fully biodegradable and recyclable components. These capsules are great because you don’t put a single part in the trash!\nThe lids are made from recyclable high barrier aluminum, the capsule is created using a proprietary formulation plastic which is also recyclable, and the filter is a biodegradable custom weave made from natural fibres. The 98% Recyclable Capsules with Biodegradable Filters are compatible for use with Keurig® K-Cup® brewers.\nRecycling the capsule is easy. After brewing, let the capsule cool and then:\n- Locate the tab on the lid.\n- Peel away the lid and separate the filter from the outer plastic cup. Recycle the outer plastic cup and lid.\n- Compost the remaining filter and coffee grounds/tea!\nBonus: If you are brewing a latte or hot chocolate, there is no filter so you can recycle those capsules right after use.\nThe following brands use the 98% Recyclable Capsules with Biodegradable Filters:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:9730b471-1869-43cb-9017-f1d41a3d14a0>","<urn:uuid:a414313c-5e4d-4b67-a20b-ec349f5d4c74>"],"error":null}
{"question":"Why do both the CP-Carrillo Bullet pistons and the 2020 Yamaha MT-07 engine emphasize low-end performance in their respective applications?","answer":"Both designs emphasize low-end performance but for different purposes. The CP-Carrillo Bullet pistons are specifically designed for dirt track racing where launching the car off corners with strong low-end power is crucial. The MT-07's engine is tuned to provide plenty of low and mid-range torque with linear throttle response, producing 50 ft-lbs of torque from low to mid-range, earning it the nickname 'The Master of Torque'. Both prioritize this characteristic to deliver optimal real-world performance in their respective applications.","context":["CP-Carrillo’s New Line of Pistons and Connecting Rods\nThe trick to building a durable small-block for dirt track racing is to find the best parts that will do the job for the best price. Off-shore pieces are often attractively priced, but durability and reliability are often expensive lessons in what not to do. So at the most recent PRI show, we learned how CP-Carrillo has figured out a way to deliver made-in-America quality at a price budget-based builders can appreciate.\nBesides brake pads and rotors, the most abused components in a dirt track car have to be the pistons and connecting rods. Short track racing is all about launching the car off the corner with a deep gear and a screaming small-block through a wide low-to-high rpm curve. That’s the life of a short-track small-block and where CP-Carrillo decided to make some inroads. Their goal was to deliver a set of affordable yet high quality pistons and connecting rods, and it led to what they call their Bullet lineup.\n“The best thing about the Bullet line,” says CP-Carrillo’s Bryce Mulvey, “is we took everything we learned in our custom pistons and transferred it into a shelf part. You’re going to get the best of both worlds – a piston with the same technology used in NASCAR parts, and we’ve taken many of those little secrets and put them into a shelf part.”\nWe thought we’d take a little closer look into exactly what all that means in terms of specific applications like a small-block Chevy. Right off the shelf, the Bullet pistons are not repackaged street engine parts. The Bullet line is made from the classic 2618 race alloy, which is a much more durable aluminum forging than the street forgings made out of a 4032 alloy containing 12 to 13 percent more silicon to contain expansion. The Bullet 2618 material is far more forgiving under hard use and exactly the type of piston that can survive the harsh life of a short-track engine.\nThe ring package has also been updated to follow the trend toward tin rings. Even today, many “race” engines still follow the 1/16-, 1/16-, 3/16-inch ring formula. But frankly, when production LS engines now use a thin ring, there’s little reason to stay with old technology.\nAll other variables aside, thinner rings can immediately deliver an immediate power advantage based on simple physics. Thicker rings present a larger surface area in contact with the cylinder bore and that higher unit loading pressure. Thinner rings can reduce this radial load because there’s a smaller contact area.\nThe entire Bullet piston line has been updated to a more contemporary 1.2mm, 1.2mm, 3.0mm ring package. How does that compare? A 1/16-inch ring is 0.0625 inches thick while the 1.2mm ring is thinner at 0.047-inch. But the real difference is in the oil ring package. The traditional 3/16-inch oil ring measures 0.1875 while the 3mm is much thinner at 0.117-inch. Race engine builders will tell you most of the ring drag comes from the oil ring, so gains here will pay off with a greater return because of the smaller expander. Of course, experimenting with lighter tension expanders also is one way to improve power, but we’ll save that story for a later time.\nOther benefits that have come from the racing world include computer-driven Finite Element Analysis (FEA) of the piston design, during which possible failure points can be quickly identified and redesigned to minimize these stress concentrations. This greatly improves piston durability. Other improvements include forged side relief design and accumulator grooves in between the top and second ring, all intended to improve ring seal. According to Mulvey, this makes these Bullet pistons easily reliable up to 650 hp.\nAs much as we can appreciate the piston being abused, the single-most stressed component has to be the connecting rod. While most might consider the compressive forces imposed on a piston during the power stroke as the most abusive, in fact it is the combination of a heavy piston at high rpm that imposes the biggest threat to connecting rod durability.\nThis is old news to the CP-Carrillo people whose history dates back to Fred Carrillo in 1963 designing a high-performance race connecting rod that has certainly withstood the test of time. The Bullet rod is based on this long-established history using the company’s famous 4330 alloy net forging steel that is the same alloy used in all its forgings. This allows CP Carrillo to rate these rods to 8,500 rpm.\nBut with current net forging capability, CP-Carrillo is able to produce an affordable rod with 7/16-inch WMC H11 tool steel cap screws that are one key to their durability. You’ll also notice that these are what CP-Carrillo calls A-beam rods, similar in design to I-beams. The most common application for the small-block Chevy dirt track small-block Chevy would be the 0.927-inch floating pin 6.00-inch rod that is far from the least expensive connecting rod you’ll find. But, consider this as excellent insurance against the alternative. CP-Carrillo even serializes each individual connecting rod and guarantees 100 percent traceability if there’s ever any question.\nWe touched briefly on weight, and this is one advantage of the Bullet series. A typical Bullet flat-top, two-valve relief piston for a 3.50-inch stroke small-block Chevy weighs in at barely 404 grams (not counting pins, locks, and rings). Combine that with its matching 6.00-inch CP-Carrillo Bullet rod that weighs 545 grams and you have a very lightweight reciprocating system that means a little extra torque coming off the corners at your favorite bull ring.\n|Bullet Special SBC||4.00||3.50||6.0||-6.00||404||BCR1320-STD|\n|Bullet Special SBC||4.030||3.50||6.0||-6.0cc||408||BCR1320-030|\n|Bullet Special SBC||4.030||3.50||5.7||-6.0cc||455||BCR-1350-030|\n|Bullet Special SBC||4.040||3.50||5.7||-6.0cc||455||BCR-1350-040|\n|Bullet SB Ford||4.030||3.00||5.4||-6.6cc||485||BF6001-030|\n|Bullet SB Ford||4.030||3.40||5.4||-8.7cc||418||BF6010-030|\nIf all this sounds intriguing, the next thing to do is to find your way to the CP-Carrillo website and run through their electronic catalog for the piston application you need. While we focused this story on the popular small-block Chevy, the Bullet piston line also covers GM LS, big-block Chevy, the small-block Windsor Ford, the Chrysler late Hemi, as well as the LA series engines, several iterations of the big-block Ford lineup, and even a few Pontiac and Olds engines. Check it out – and then start building.","2020 Yamaha MT-07\nThe Yamaha MT-07 is a very interesting motorcycle, especially in its 2020 model. Using an irregular interval firing parallel twin-engine, power is very decent for a near-700 CC engine at 74 crank HP with 50 lbs-ft of torque from low to mid-range on the revs, even earning the nickname “The Master of Torque” from riders. The biggest innovation in the MT-07 engine is the 270-degree crankshaft, which produces a very nice, nearly cruiser style burble from the exhaust at idle.\nThe MT-07 has very quickly become known as a very stable daily rider. One of the most important things towards that is that instead of a vertical spring for the rear, it uses a horizontal suspension that brings the center of gravity lower, as well as making the wheelbase shorter than its two bigger brothers in the MT-10 and the MT-09. As well, like all models in the MT range, it has traction control, dual-zone ABS, and Yamaha QuickShift.\nThe Yamaha MT–07 starts at $7,599 US/$8,499 CA.\nOn this page: we’ve curated specs, features, news, photos/videos, etc. so you can read up on the new Yamaha MT-07 in one place.\n- Price: $7,599 US / $8,499 CA\n- Key Features:\n- Yamaha Quick Shift\n- Traction Control\n- Engine type: 689cc liquid-cooled DOHC 4-stroke\n- Power: 74 hp\n- Wet weight: 403 lb\n- Seat height: 31.7 in\n2020 Yamaha MT-07 Specifications\n|Engine||689cc liquid-cooled DOHC 4-stroke; 8 valves|\n|Bore x Stroke||80.0mm x 68.6mm|\n|Fuel System||Fuel injection|\n|Clutch||Wet Multiplate clutch|\n|Suspension Front||41mm telescopic fork; 5.1-in travel|\n|Suspension Rear||Single shock, adjustable preload, and rebound damping; 5.1-in travel|\n|Brakes Front||Dual 282mm hydraulic disc; ABS|\n|Brakes Rear||245mm hydraulic disc; ABS|\n|Tires Front||120/70 ZR 17|\n|Tires Rear||180/55 ZR 17|\n|Fuel Tank Capacity||14 L (3.7 US gal.)|\n|Color||Ice Fluo, Matte Raven Black|\n|Ignition||TCI: Transistor Controlled Ignition|\n|Wheelbase||55.1 in (1400 mm)|\n|Ground Clearance||5.5 in (140 mm)|\n|Seat Height||31.7 in (805 mm)|\n|Curb Weight||403 lbs (183 kg)|\n|Warranty||1 Year (Limited Factory Warranty)|\n2020 Yamaha MT-07 Features\n- Slim, compact, 689 cc, DOHC, 4-valve, liquid-cooled, 20-degree inclined, inline twin-cylinder engine.\nThe engine has been tuned to provide plenty of low & mid-range torque with very linear throttle response. This engine produces approximately 75 hp & 50 ft-lbs. of torque and is designed to maximize riding fun in the real world.\n- 4-valve, downdraft-style combustion chamber with 11.5:1 compression ratio.\n- excellent torque output in the low-to-mid rpm range\n- 4-valve design maximizes flow into & out of combustion chamber\n- 31.5 mm intake & 26.5 mm exhaust valves\n- stainless steel valves feature an Isonite surface treatment & carburized tips for excellent durability & wear resistance\n- 42,000 km-valve adjustment intervals.\n- Lightweight hollow camshafts.\n- cam profiles & timing accentuate the low-to-mid-range torque & power in the most commonly used rpm range (3,000 to 6,500 rpm).\n- excellent rideability with linear throttle response\n- CP3 “CrossPlane Concept” 270-degree crankshaft with an uneven firing order.\n- design reduces inertia torque while emphasizing linear torque\n- design has been optimized to achieve strong torque over a broad rpm range\n- Lightweight, gear-driven “coupling force balancer” reduces engine vibration for excellent comfort.\n- Fracture split connecting rods feature a “nutless” design.\n- lower end “cap” of the rod is made from the same piece of material as the upper portion; the 2 pieces are “split” apart using a special fracturing process & then precision machined\n- design aids in establishing true big end roundness & greater precision in con rod dimensions for excellent durability despite the horsepower loads\n- Lightweight, forged, aluminum short skirt pistons.\n- lightweight design aids in fast engine response\n- reduces mechanical vibration for excellent durability\n- Direct ceramic composite plated cylinder uses “liner less” bores.\n- ceramic coating is sprayed directly on the aluminum block, eliminating the liner & reducing weight\n- excellent heat dissipation for consistent power delivery\n- coating enhances the thin film of oil between the cylinder & piston, reducing friction & increasing power\n- Offset cylinder design.\n- cylinder is “offset” relative to the crank (toward exhaust side)\n- reduces frictional losses between the pistons & cylinder wall during the power stroke for improved power output & excellent fuel economy\n- One-piece cylinder & upper crankcase assembly.\n- superior engine & chassis rigidity (engine is a stressed member increasing chassis rigidity)\n- design reduces weight\n- “Closed-loop” 38 mm Mikuni throttle body fuel injection system.\n- system uses throttle position sensor (TPS) & idle speed controller (ISC)\n- 12-hole injectors for optimum fuel atomization\n- system is “tuned” to provide excellent rideability & linear throttle control.\n- closed-loop system” uses an oxygen sensor to “sniff” spent exhaust & automatically adjust the fuel / air ratio for reduced emissions\n- Compact Electronic Control Unit (ECU).\n- utilizes a powerful 32-bit processor for fast control of the injection & ignition processes\n- Airbox design features dual intakes.\n- design reduces intake noise\n- viscous paper type air filter is utilized\n- Compact, 6-speed transmission.\n- Tri-axis design “stacks” input/output shafts to centralize mass\n- overall engine size is shorter front to back allowing the engineers the freedom to place the engine in the frame for optimum front to rear weight balance\n- transmission ratios are designed to suit the character (torque & power) of the engine\n- specially chosen gear ratios means fewer gear changes\n- 6th gear is an over drive gear reducing engine rpms at highway speeds for a comfortable ride\n- Compact, clutch design with light clutch lever pull.\n- Lightweight, vertical flow flat radiator with ring type cooling fan.\n- ring type cooling fan pulls more air than a conventional non-ring type design\n- left & right rad covers add a stylish highlight\n- Liquid-cooled oil cooler.\n- maintains consistent lubricant temperatures for extended engine life\n- Convenient spin-on type oil filter & easy access drain plug.\n- for fast, easy oil changes\n- Convenient oil level sight glass.\n- Maintenance-free transistor controlled ignition (TCI).\n- produces a strong spark for fast starts\n- provides precise ignition timing / mapping for optimum engine performance at all rpms\n- High performance direct ignition coils (ignition coils are built into the spark plug caps).\n- reduces weight & complexity\n- AC Magneto produces 410 watts of power @ 5,000rpm.\n- Stylish, compact, low mounted, 2 into 1 exhaust system.\n- triple expansion muffler design with 3-way honeycomb catalyzer reduces exhaust emissions\n- low mounted design centralizes mass for excellent handling & a light weight feeling\n- Air Injection System (AIS… not ram air) injects fresh air into exhaust ports to fully burn any unburnt fuel, further reducing exhaust emissions.\n- Diamond-type, high tensile steel frame.\n- optimized rigidity balance for light, agile handling & excellent stability\n- engine is a stressed member of the chassis, allowing a lighter, more compact main frame\n- Upright, comfortable riding position is a key feature of the MT-07.\n- excellent all day rider comfort\n- Key chassis geometry figures include:\n- 1400 mm (55.1″) wheelbase\n- 24.5 degrees of rake & 90 mm of trail\n- front/rear weight distribution is 49.2% front & 50.8% rear\n- lean angle is 49 degrees, highlighting the sporty nature of the MT-07\n- chassis dimensions & design centralizes mass & lowers centre of gravity for light, agile handling\n- Lightweight, high tensile steel “gull wing” type short design swingarm.\n- lightweight design reduces unsprung weight for superior suspension performance\n- excellent rear wheel traction & control\n- Revised, 41 mm conventional fork offers 130 mm (5.1″) of wheel travel.\n- new spring rate & damping force settings for improved suspension performance & comfort\n- oil level & amount have been optimized\n- Lightweight, aluminum upper & lower triple clamps.\n- Adjustable link-type Monocross rear suspension with 130 mm (5.1″) of wheel travel.\n- new, horizontally mounted rear shock now includes both rebound & spring preload adjustability\n- adjustments allow the rider to tailor settings to match rider weight, load & riding conditions\n- ABS equipped dual 282 mm floating wave-style front discs & ultra-rigid 4-piston monoblock calipers.\n- excellent stopping power & control with good lever feedback\n- ABS equipped 245 mm wave-style rear disc is squeezed by a lightweight single piston slide-type Nissin caliper.\n- Anti-lock braking system (ABS).\n- controlling ECU & hydraulic unit are combined into a single compact unit to reduce weight & centralize mass\n- when ABS system senses impending wheel lock-up (via active type wheel sensors) it regulates hydraulic braking pressure to the point just before the wheel or wheels lock-up\n- ABS benefits include excellent control under hard braking or when braking on wet, slippery or loose surfaces\n- Lightweight, cast-aluminum 10-spoke wheels.\n- lightweight design reduces unsprung weight for excellent suspension & handling characteristics\n- front rim size is MT3.50-17 & rear rim size is MT5.50-17\n- Conventional handlebar design.\n- combines with the upright riding position to provide excellent all day riding comfort\n- conventional design means the handlebar can be easily changed to a different “bend” if desired\n- 14-litre fuel tank with new, more stylish replaceable covers.\n- slim design with great knee grip\n- slim design combines with low seat height for a light, agile feel & increased rider confidence\n- reserve portion of the tank is 2.7 litres (low fuel light illuminates)\n- steel tank covered by easy-to-replace plastic panels, significantly reducing costs in the event of damage\n- Larger, more stylish, separate rider & passenger seats. Rider seat height is 805 mm (31.7″).\n- excellent comfort & support\n- rear seat can be removed & replaced by optional Genuine Yamaha Accessory rear seat cowl\n- Compact, LCD multi-function digital meter.\n- functions (all digital) include: bar-style tachometer, digital speedometer, odometer, dual trip meters, fuel gauge, fuel reserve trip meter (counts kilometers since fuel went on reserve), clock, gear indicator, instant & average fuel consumption, intake air temperature & coolant temperature\n- full range of warning & indicator lights\n- background illumination is adjustable\n- Compact handlebar switch gear.\n- starting function is integrated into the engine kill switch\n- passing high beam flash button is provided\n- One piece, multi-reflector 60/55 watt halogen headlight.\n- Design with lower winglet enhances the naked roadster styling\n- LED taillight with 24 bulbs for excellent visibility.\n- 5-position adjustable front brake lever\n- fold out under seat bungee cord fastener straps\n- lockable steering\n- durable “O” ring drive chain\n- 1 piece chain guard & inner rear fender\n- excessive lean angle engine cut-out switch (if unit is on its side the engine will shut down)\n- low-maintenance sealed battery (battery should be charged during winter storage)\n- Yamaha diagnostic tool connector can significantly reduce diagnostic time in the event of a problem\n2020 Yamaha MT-07 Photos\n2020 Yamaha MT-07 Videos"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:8a2d6e84-4b91-48df-9fd0-17f630638afd>","<urn:uuid:543908b2-27fd-438a-88e6-05e7761295a4>"],"error":null}
{"question":"How do the diagnostic challenges compare between transverse myelitis and multiple sclerosis?","answer":"Both conditions present diagnostic challenges but for different reasons. Transverse myelitis requires emergency MRI or CT myelogram to distinguish it from other acute conditions like spinal cord trauma or tumors, with diagnosis primarily focused on ruling out compressive lesions. Multiple sclerosis, however, is challenging to diagnose because symptoms come and go, and no single neurological or laboratory test can confirm it. A definitive MS diagnosis requires evidence of multiple scar tissue patches in different parts of the central nervous system and at least two separate attacks, potentially taking months or years to confirm.","context":["To use all functions of this page, please activate cookies in your browser.\nWith an accout for my.bionity.com you can always see everything at a glance – and you can configure your own website and individual newsletter.\n- My watch list\n- My saved searches\n- My saved topics\n- My newsletter\nTransverse myelitis is a neurological disorder caused by an inflammatory process of the grey and white matter of the spinal cord, and can cause axonal demyelination. This demyelination arises idiopathically following infections or vaccination, or due to multiple sclerosis. One major theory of the cause is that an immune-mediated inflammation is present as the result of exposure to a viral antigen.\nAdditional recommended knowledge\nThe lesions are inflammatory, and involve the spinal cord on both sides. With acute transverse myelitis, the onset is sudden and progresses rapidly in hours and days. The lesions can be present anywhere in the spinal cord, though it is usually restricted to only a small portion.\nIn some cases, the disease is presumedly caused by viral infections or vaccinations and has also been associated with spinal cord injuries, immune reactions, schistosomiasis and insufficient blood flow through spinal cord vessels. Symptoms include weakness and numbness of the limbs as well as motor, sensory, and sphincter deficits. Severe backpain may occur in some patients at the onset of the disease. Treatment is usually symptomatic only, corticosteroids being used with limited success. A major differentiation or distinction to be made is a similar condition due to compression of the spinal cord in the spinal canal, due to disease of the surrounding vertebral column.\nPrognosis for complete recovery is generally poor. Recovery from transverse myelitis usually begins between weeks 2 and 12 following onset and may continue for up to 2 years in some patients, many of whom are left with considerable disabilities. Some patients show no signs of recovery whatsoever.\nDetailed description of transverse myelitis pathology\nThe symptoms and signs depend upon the level of the spinal cord involved and the extent of the involvement of the various long tracts. In some cases, there is almost total paralysis and sensory loss below the level of the lesion. In other cases, such loss is only partial.\nIf the high cervical area is involved, all four limbs may be involved and there is risk of respiratory paralysis (segments C3,4,5 to diaphragm). Lesions of the lower cervical (C2-T1) region will cause a combination of upper and lower motor neuron signs in the upper limbs, and exclusively upper motor neuron signs in the lower limbs. A lesion of the thoracic spinal cord (T1-12) will produce a spastic paraplegia. A lesion of the lower part of the spinal cord (L1-S5) often produces a combination of upper and lower motor neuron signs in the lower limbs.\nThe degree and type of sensory loss will depend upon the extent of the involvement of the various sensory tracts, but there is often a \"sensory level\" (at the sensory segmental level of the spinal cord below which sensation to pin or light touch is impaired). This has proven to be a reasonably reliable sign of the level of the lesion. Bladder paralysis often occurs and urinary retention is an early manifestation. Considerable pain often occurs in the back, extending laterally to involve the sensory distribution of the diseased spinal segments—so-called \"radicular pain.\" Thus, a lesion at the T8 level will produce pain radiating from the spine laterally along the lower costal margins. These signs and symptoms may progress to severe weakness within hours. (Because of the acuteness of this lesion, signs of spinal shock may be evident, in which the lower limbs will be flaccid and areflexic, rather than spastic and hyperreflexic as they should be in upper motor neuron paralysis.\nHowever, within several days, this spinal shock will disappear and signs of spasticity will become evident. The three main conditions to be considered in the differential diagnosis are: acute spinal cord trauma, acute compressive lesions of the spinal cord such as epidural metastatic tumour, and infarction of the spinal cord, usually due to insufficiency of the anterior spinal artery.\nFrom the symptoms and signs, it may be very difficult to distinguish acute transverse myelitis from these conditions and it is almost invariably necessary to perform an emergency magnetic resonance imaging (MRI) scan or computerised tomographic (CT) myelogram. Before doing this, routine x-rays are taken of the entire spine, mainly to detect signs of metastatic disease of the vertebrae, that would imply direct extension into the epidural space and compression of the spinal cord. Often, such bony lesions are absent and it is only the MRI or CT that discloses the presence or absence of a compressive lesion.\nA family physician seeing such a patient for the first time should immediately arrange transfer to the care of a neurologist or neurosurgeon who can urgently investigate the patient in hospital. Before arranging this transfer, the physician should be certain that respiration is not affected, particularly in high spinal cord lesions. If there is any evidence of this, methods of respiratory assistance must be on hand before and during the transfer procedure. The patient should also be catheterized to test for and, if necessary, drain an over-distended bladder. A lumbar puncture can be performed after the MRI or at the time of CT myelography. Steroids are often given in high dose at the onset, in hope that the degree of inflammation and swelling of the cord will be lessened, but whether this is truly effective is still debated.\nUnfortunately, the prognosis for significant recovery from acute transverse myelitis is poor in approximately 80% of the cases; that is, significant long-term disabilities will remain. Approximately 5% of these patients will, in later months or years, show lesions in other parts of the central nervous system, indicating, in retrospect, that this was a first attack of multiple sclerosis.\n|This article is licensed under the GNU Free Documentation License. It uses material from the Wikipedia article \"Transverse_myelitis\". A list of authors is available in Wikipedia.|","What is Multiple Sclerosis?\nWhat is MS?\nMultiple sclerosis (or MS) is a chronic, often disabling disease that attacks the central nervous system (brain and spinal cord). Symptoms may be mild, such as numbness in the limbs, or severe, such as paralysis or loss of vision. The progress, severity, and specific symptoms of MS vary among individuals and are unpredictable. Today, new treatments and advances in research are giving new hope to people who are affected by the disease.\nMS is thought to be an autoimmune disease. The body’s own defense system attacks myelin, the fatty substance that surrounds and protects the nerve fibers of the brain, optic nerves, and spinal cord (the central nervous system). The damaged myelin may form scar tissue (sclerosis). Often the nerve fiber is also damaged. When any part of the myelin sheath or nerve fiber is damaged or destroyed, nerve impulses traveling to and from the brain are distorted or interrupted. MS is not a fatal disease. Individuals with MS have near-normal life expectancies. Most people with MS\nlearn to cope with the disease and are able to live productive lives.\n|What are its symptoms?|\nThe symptoms of MS may include tingling, numbness, painful sensations, slurred speech, and blurred or double vision. Some people experience muscle weakness, poor balance, poor coordination, muscle tightness or spasticity, tremors, or paralysis which may be temporary or permanent. Problems with bladder, bowel, or sexual function are common. Fatigue is a major concern for many. MS can cause forgetfulness or difficulty concentrating. It can also cause mood swings and may make people more susceptible to depression. Symptoms may come and go, appear in any combination, and be mild, moderate, or severe.\n|Can MS be treated?|\nYes. Today, there are six medications approved by the Food and Drug Administration (FDA) to treat MS. Four of them—Avonex®, Betaseron®, Copaxone®, and Rebif®—are immunomodulating drugs (meaning that they modulate or alter the immune system) that are given by injection. These drugs have been shown to be effective in modifying the natural course of relapsing and secondary-progressive MS.\nThe National MS Society recommends that treatment with one of these “disease modifiers” be considered as soon as possible following a confirmed diagnosis of MS with a relapsing course.\nIn some circumstances, treatment with a disease-modifying drug may be recommended before an individual is definitely diagnosed if the person experienced one attack and has evidence of MS lesions as seen by MRI scanning.\nTysabri®, another immunomodulating drug that has recently been approved by the FDA, is delivered by infusion. It is recommended for patients who have an inadequate response to, or are unable to tolerate, alternate MS therapies.\nThe sixth drug, Novantrone®, is a powerful immune system suppressor shown to be effective in slowing down MS that is rapidly worsening or becoming progressive.\nSteroids may be used to shorten acute attacks. Many other therapies are being clinically tested, and researchers are hopeful that more treatments for MS will be available in the near future.\nThere are also many medications to relieve or moderate MS symptoms such as spasticity, bowel and urinary distress, pain, fatigue, or depression. Physical therapy, exercise, vocational and cognitive rehabilitation, attention to diet, adequate rest, and counseling are often valuable for maintaining independence and quality of life. Prompt management of symptoms is vital and should be discussed with a knowledgeable physician.\n|Who gets multiple sclerosis? |\nAn estimated 400,000 Americans have MS. Most are diagnosed between the ages of 20 and 50, and about two thirds are women. The disease is more frequently found among people raised in colder climates. Studies indicate that genetic factors make certain individuals susceptible to the disease, but there is no evidence that MS is directly inherited.\n|What are the general patterns?|\nMS is an unpredictable disease. Symptoms vary greatly from person to person and vary over time in the same person.\nPeriods of active MS symptoms are called attacks, exacerbations, or relapses. These can be followed by quiet periods called remissions.\nThe disease ranges from very mild and intermittent to steadily progressive. Some people have few attacks and little, if any, disability accumulating over time. At diagnosis, most people have relapsing-remitting disease. This means they have attacks followed by periods of partial or total remission, which may last months or even years. Others experience a progressive disease course with steadily worsening symptoms. The disease may worsen steadily from the onset (primary-progressive MS) or may become progressive after a relapsing-remitting course (secondary-progressive MS).\nBecause MS affects individuals so differently, it is difficult to make generalizations about disability. Statistics suggest that 2 out of 3 people with MS remain able to walk over their lifetime, though many of them will need a cane or other assistive device. Some will choose to use a scooter or wheelchair to conserve energy. Others will require a wheelchair to maintain mobility.\nThe “disease-modifying” treatments mentioned earlier, and in use only since the 1990s, may favorably alter this projection.\n|Is it easily diagnosed? |\nMS is not always easy to diagnose because symptoms may come and go. In addition, other diseases of the central nervous system have some of the same symptoms. No single neurological or laboratory test can confirm or rule out MS.\nMedical imaging, particularly MRI (magnetic resonance imaging), helps to clarify diagnosis. A conclusive or definitive diagnosis requires evidence of multiple patches of scar tissue in different parts of the central nervous system and evidence of at least two separate attacks of the disease. A definitive diagnosis can take several months. Sometimes it takes years.\n|Can MS be cured? |\nThe answer is no—not yet. The cause and the cure of MS are the subject of intensive worldwide research. Over 300 research grants and fellowships are funded by the National MS Society each year. Knowledge about MS is expanding and many clinical trials are in progress.\n|The National MS Society fights MS |\nInformation, local referrals, publications, programs, and volunteer opportunities are available from the National Multiple Sclerosis Society and our 50-state network of chapters. To reach the chapter nearest you, call 1-800-344-4867. Visit nationalmssociety.org.\nThe Society is comprised of people who want to do something about MS now—people with MS, their family members, concerned friends and neighbors, and health-care professionals. As the world’s largest private funder of MS research, the Society supports local, state, and national advocacy programs, and serves as the voice for people with MS.\nAvonex®xis a registered trademark of Biogen Idec.\nBetaseron® is a registered trademark of Bayer HealthCare Pharmaceuticals, Inc.\nCopaxone® is a registered trademark of TEVA Pharmaceutical Industries Ltd.\nNovantrone® is a registered trademark of EMD Serono, Inc.\nRebif® is a registered trademark of Serono Pfizer.\nTysabri® is a registered trademark of Biogen Idec and Elan Pharmaceuticals, Inc.\nFor additional information"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:5ec0b68a-d5dd-4188-a2c6-133ece68df39>","<urn:uuid:3526b6b2-d62f-4f11-8c2a-c7fb05b211c0>"],"error":null}
{"question":"What were General Eisenhower's leadership approaches during D-Day, and how did the meteorological team's forecasting process influence the invasion's success?","answer":"General Eisenhower demonstrated strong leadership through multiple approaches: he inspired confidence with statements like 'This operation is planned as a victory' while maintaining accountability, as shown by his prepared note taking full responsibility in case of failure ('If any blame or fault attaches to the attempt it is mine alone'). Regarding the meteorological influence, a team of six meteorologists, including New Zealander Lawrence Hogben, worked collaboratively through telephone debates to provide critical weather forecasts. Despite challenging odds (13 to 1 against finding suitable weather), their team-based approach helped identify a brief window of opportunity on June 6th. The Germans saw the same weather data but failed to account for the Allies' determination to succeed, which combined with the accurate meteorological guidance, contributed to the invasion's success.","context":["The Blame or Fault Is Yours\nAccountability and character are essential for leaders\n“Life is divided into three periods: past, present, and future. Of these, the present is short, the future is doubtful, the past is certain.” — Seneca, 49\nWhen we look back upon major events in history, we see them as turning points.\nCaesar crossing the Rubicon. America taking on the greatest army in the world in 1776. The largest military maritime landing in history in 1944.\nAs we look at them now, they’re certainties. But Caesar didn’t know his gambit would pay off. Many Americans were in doubt about the revolution and were still loyal to the crown. And the Allies didn’t know if they would successfully land 150,000 troops to storm the beach at Normandy.\nSeneca’s observation above is an important one: nothing in the future—nothing—is certain. Leaders need to have contingency plans and to adapt to the changing market forces, on the fly if necessary.\nEvery leader needs a plan. But they also need backup plans.\nGreat leaders know it’s not enough to have a plan. They’ve seen things go sideways before. And like any self-aware person, they know that it could happen to them.\nTake General Dwight Eisenhower for example.\nHe prepared his troops as best he could for Operation Overlord (aka the D-Day invasion). He prepared these marks, delivered over the radio and in writing:\n“You are about to embark upon a great crusade, toward which we have striven these many months. The eyes of the world are upon you. We will accept nothing less than victory! Good luck!”\nThat wasn’t enough, though. He knew that they needed to know he was counting on more than luck — he was counting on their bravery and resilience, and in doing so he built their confidence:\n“This operation is planned as a victory, and that’s the way it’s going to be. We’re going down there, and we’re throwing everything we have into it, and we’re going to make it a success.”\nBut Ike knew that there were variables that were out of his control: the number of enemy troops awaiting them, the complete secrecy of the plans, and the weather.\nDuring the planning for the D-Day invasion of Normandy, meteorologists informed General Eisenhower that there were a series of storms heading across the Atlantic that would delay the operation, which was originally scheduled for June 5.\nConditions had to be just right: calm seas and clear skies.\nIf the operation went ahead as planned, it would have been a failure; if the Allies waited for the next cycle of tides and improved weather, they would have lost the element of surprise.\nHowever, an updated weather report came through showing a high-pressure system that would intersect with Normandy on June 6th. It left a very tight window.\nThe Germans saw the same forecasts, but didn't think the brief moment of calm would allow the Allies to advance. Field Marshal Rommel was convinced of this and left for a few days in Paris.\nLater, asked why D-Day had been a success, Eisenhower said, “Because we had better meteorologists than the Germans.”\nIt’s because of readers like you that Timeless & Timely exists. If you like what you see, please support my work by becoming a free or paid subscriber.\nWhile this makes sense from a pithy storytelling standpoint, it’s actually not true. The meteorologists on both sides of the war saw the same data.\nBut German leadership failed to take into account just how much the Allies wanted to win the war.\nGeneral Eisenhower inspired a sense of loyalty, confidence, courage, and desire to succeed among his men.\nThose attributes were invisible, unable to be seen by the naked eye or monitored by meteorologists. And thus were not part of the calculations made by the Germans.\nAnother thing that was unseen: the uncertainty that Eisenhower had in the plan.\nLooking back now, we marvel at the accomplishment of D-Day and of the bravery of the men who fought and were wounded or killed there.\nBut on June 5, it was far from certain. Ike prepared a note (which he misprinted as July 5) in the event of a catastrophic loss that day:\n“Our landings in the Cherbourg-Havre area have failed to gain a satisfactory foothold and I have withdrawn the troops. My decision to attack at this time and place was based on the best information available. The troops, the air and the Navy did all that bravery and devotion to duty could do. If any blame or fault attaches to the attempt it is mine alone.”\nIt was the largest military landing in history, and here was one man — one extraordinary leader — who was willing to take the blame if it failed. Without reservation.\nThe “blame or fault…is mine alone.”\nConsidering the tens of thousands of military personnel who helped plan and execute the D-Day invasion, and all of the elements that were out of his control, General Eisenhower was still willing to take accountability for the outcome.\nDecent leaders know how to be accountable.\nAnd being accountable speaks volumes about character.\nThanks, and I’ll see you on the internet.","NZ meterologist helped D-Day invasion\nA New Zealander who was one of only six meteorologists involved in providing weather forecasts for the Allies D-Day landings 70 years ago today has given a striking account of how they nearly got it wrong.\nAuckland-born Lawrence Hogben, 98, is only one of two surviving meteorologists from the six man team who advised General Dwight Eisenhower on the weather for the landing.\nWriting in the London Review of Books (http://www.lrb.co.uk ) Hogben notes how difficult the forecasting was in 1944 and around Normandy, which was being hit by a storm.\n“In 1944 we only just got the most important weather forecast in history right. But we steered the invading army away from a potential disaster at sea and helped to make ultimate victory feasible,” Hogben writes.\nHe says at the time the D-Day planners assumed meteorologists had total control of the elements.\n“Just name us five fine, calm days and we’ll go.”\nHogben says the six men worked out four possible days in June 1944 but worked out the odds on the weather on any one of these four dates conforming to requirements as being 13 to one against.\n“So meteorologically, D-Day was bound to be a gamble against the odds.”\nHe writes vividly of how the D-Day forecast became a “telephonic affair”: the six forecasters would stay in three separate centres, joined to one another and to Eisenhower’s headquarters by GPO telephones (which worked).\n‘‘A non-forecaster would moderate our debates and then explain our agreed forecast to Ike and his staff. ‘‘\nHogben said the military were astounded to discover that, despite all the scientific expertise simultaneously available on his phone, an agreed forecast never emerged easily, even after hours of discussion.\nHe was one of two Royal Navy officers part of the six man team: “Geoffrey Wolfe, an urbane Cambridge man from Hove, on the English Channel, who not surprisingly turned out to be quite the most consistent forecaster of the six, and the youngest, myself, a New Zealand Rhodes Scholar and the first ever Naval Instructor to have won a DSC [Distinguished Service Cross] in battle.”\nAs the weather deteriorated ahead of D-Day, Hogbed says the telephone debates went on furiously if inconclusively.\n“We all agreed that the key depression whose front had caused the postponement would move north-east and threaten no more. But what would it leave behind?”\nThe gamble they made on a good day paid off.\n“Although my naval colleague and I happened to forecast correctly for the two critical days, our forecasts were as much the fruit of our discussions with the others as of any singular ability. The team forecasts saved us, and the invasion.”\nHogben questions whether today’s weather forecasters would do any better.\n“They certainly should, with their satellites, computers and weather models.\n“Fifty years ago our only satellites and computers were in our heads and we could find no analogue in our past records for what the Americans afterwards described as ‘a meteorological situation unique in the annals of June weather’,” he says.\n“I am sure that in weather like that of May 1944 today’s experts would improve on us, and make reasonable five-day forecasts. But when one type of weather is changing to something radically different, as it did the following month, I am not so sure.”\nNow living in France, Hogben says the timing and nature of a total change in the weather continue to pose questions which often are not satisfactorily answered.\n“For shorter periods, the accuracy of today’s forecasts is most impressive.”\nHogben grew up in Auckland and attended Auckland Grammar School. He was the top mathematics graduate of all New Zealand universities and was awarded a Rhodes Scholarship to study at New College Oxford in 1938.\nHis studies were interrupted by the outbreak of World War II.\nAfter training at the Royal Naval College in Greenwich, he served on the cruiser HMS Sheffield as a lieutenant commander. He was a training officer, intelligence officer and meteorologist.\nAfter over three years at sea, Lawrence was awarded the first DSC ever given to a Royal Navy Instructor Officer."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:db8b95bb-cb2c-4f34-b736-242805a3818d>","<urn:uuid:8e75c095-daf8-4cd4-8de9-4852c1ebe67f>"],"error":null}
{"question":"What's the key difference between documentation in early-stage hardware PM versus software SDLC?","answer":"In early-stage hardware product management, documentation primarily focuses on developing user requirements and creating a Product Requirements Document (PRD) to formalize product specifications. In contrast, software development documentation is more comprehensive, beginning with detailed requirements gathering through stakeholder interviews and workshops, followed by creating specification documents that serve as references throughout the entire development lifecycle. The software approach tends to be more structured and systematic in its documentation process.","context":["Hardware Product Management Frameworks for Startups\nEarly-stage hardware companies usually understand the “how” of product development. Product management adds a layer of “why,” “what,” and “when” to the process.\nBuilding hardware is complicated. Many founding teams going through product development for the first time struggle to manage that process on a day-to-day basis. Founders know they need to build a product and that there are many steps, but they might not be sure which steps come in which order, or what features and tests to prioritize. The goal of this blog post is to guide founders through this process of day-to-day product development, or what I call early-stage product management.\nEarly-Stage Product Management\nOnce startups raise enough capital to have 20 or 30 employees, nearly all companies hire a dedicated product manager. Before that, founders, CEOs and engineers have to handle product management themselves. The basic scope of product management for an early stage hardware startup typically includes five things:\n- Product Definition: interfacing with customers to determine what the product should be.\n- Documentation: developing user requirements and formalizing a specification for the product (usually via a Product Requirements Document).\n- Feature/Scope Management: deciding what features should be included in a given product release.\n- Prototype Management: prioritizing prototype builds and testing to ensure requirements are met.\n- Project Management: managing overall cost, quality, and schedules for the product.\n#1 and #2 are covered extensively in the Illustrated Guide to Product Development series, which is a strategic approach to product development. The focus of this post is the last three parts of the product development process: the tactical side of product management.\nProduct management is not a mystery. You are, in fact, already doing it. You’re doing it all the time: It’s fundamental to the process of building anything that users will touch and hopefully love. Whether it’s a hastily-made Proof of Concept prototype or Apple’s next iPhone, product management is essential, and realizing you’re doing it will help you manage it better (and hopefully build a better product more cost-effectively).\nTwo Frameworks to Decide Why, What, and When\nI find it helpful to think through product management via two frameworks: risk and schedule. Any given product/company can use either or both of these, but the output should be a timeline and budget for how to get your product into the hands of customers.\nThe Risk Framework\nStartups are machines to reduce risk. One of the best frameworks for early product management is figuring out how to maximize risk-reduction as early and as cost-effectively as possible. What are all possible risks you face as a product/company? Here are some common categories/examples:\n- Business risk — We can’t build the product for a low-enough price that users will happily pay.\n- Technical risk — Standard GPS receivers will use too much power and won’t be accurate enough.\n- Customer risk — Users won’t know how to set up the product without detailed explanation.\n- Product risk — If the product doesn’t live up to consumer expectations, it creates significant liability for the consumer. More on the nuances of product risk here.\nMost startups take hundreds of risks, but there are usually only a handful of “showstoppers” that result in disaster if they can’t be mitigated. Here’s how to prioritize which risks to focus on:\n- Brainstorm all possible risks and then categorize them using two parameters: severity (how serious a risk it is) and addressability (how easy or difficult the risk is to mitigate). For example: A customer risk of not being able to set up the product is a very serious risk that can be addressed easily and cost-effectively with design. On the other hand, a technical risk of not being able to build a product that adheres to Apple’s iOS MFi Bluetooth rules is nearly impossible to mitigate.\n- Capture your risks in a spreadsheet. One heuristic for figuring out “risk level,” or how much risk a given item can cause, is severity x number of people. You can have small risks that impact everyone, or larger risks that are serious but unlikely to happen. (example courtesy of Flare Jewelry, one of our portfolio companies with dummy data inserted to product the company’s IP)\n- Early prototype builds (works-like and looks-like) should each be designed to mitigate one of these risks, working down the list from most to least severe. In the example above, Flare needed to ensure they could get a single button action of their physical device to quickly launch a mobile app on iOS and Android, without a negotiated Bluetooth connection. This was one of their first works-like builds (WL1) and the development board for reducing several other risks with new firmware builds (WL2, WL3, etc.).\n- Once many major risks have been “tested away,” it’s time to integrate each separate build into a single prototype with an engineering prototype (EP1). The EP phase is much less deterministic; rather than binary “can it be done?,” as is common with WL builds, EPs are often a series of tradeoffs between design and engineering.\n- The risk framework tends to become less useful after the EP builds. Attention turns towards manufacturing execution, where decisions are typically driven by cost, quality, and schedule.\nThe Schedule Framework\nTime is also a driver of business decisions. A very common scenario we see in our portfolio is a sub-par version of a product being sold/demoed to customers, while the startup desperately works on the “final version,” to be delivered by a specific date. When this dynamic is at play with a company, it’s common to start with a firm delivery date and work backwards.\nTive, another Bolt portfolio company, is currently selling a product they’ve been hand-assembling in 100-unit batches for trial customers. They’re working on a totally new revision, with a new layout, interface, and business process, but same general function. Because there’s limited technical risk and lots of customer pressure, using the schedule-driven framework is a great option.\nThe founders and sales team have made a strategic decision to deliver production units of the new product by March 1, with the ability to cost-effectively make 1K unit runs for the subsequent twelve months. These strict requirements have some major effects:\n- High-volume production manufacturing techniques must be used: injection molding, fully outsourced PCBA/bringup, heat staking, and ultrasonically welded clamshell housings.\n- Contract manufacturing facility doing procurement, final assembly, QA/QC, and kitting (packing boxes and accessories).\n- Consigned components and volume purchases (usually by the reel).\n- Focus on low manual labor assembly and high yield, without manual rework.\nWith these requirements, we can start by laying out the Production Validation Test (PVT), Design Validation Test (DVT), and Engineering Validation Test (EVT) timeframes and steps:\nContinuing to work backwards for Engineering Prototype (EP) builds (we’ll plan for an EP2 as a buffer):\nWe then have a good idea of how much time we can spend on looks-like (LL) prototypes:\nAnd works-like (WL) prototypes:\nThe trickiest part of the schedule-driven framework is estimating time for each process and build, especially for first-time founders, who might lack a frame of reference. If that sounds like your near future, I suggest buying a copy of Manufacturing for Design Professionals to get a running start, and don’t underestimate the power of talking to folks who have gone through this process before.\nNo One Size Fits All\nAs with many other decisions around hardware development, there’s no single approach that works best for all startups. Worth noting: A great product development process is not a top-down management-driven way of working. For it to be adopted fully, it requires discussion and buy-in from the entire product, engineering, and design orgs within your company.\nCompanies that carefully plan their product development are much more likely to build products that people love — and are able to deliver them more or less on time and on budget. Proactively work with a product management framework that works for you and stick to it during the the development process; it’ll have a tremendously positive effect on your company.","What are the Key Steps in the Software Development Lifecycle?\nSoftware development is a complex process that involves a series of well-defined steps to create high-quality software applications. The Software Development Lifecycle (SDLC) encompasses these steps and provides a structured approach to ensure the success of software development projects. In this article, we will explore the key steps in the SDLC, the role of coding languages, the importance of choosing the right software development company, and more.\nThe field of software development has seen tremendous growth in recent years. As technology continues to evolve, businesses rely on software applications to streamline their operations, enhance productivity, and deliver value to their customers. However, developing software is not a straightforward task. It requires careful planning, collaboration, and adherence to established methodologies. This is where the Software Development Lifecycle (SDLC) comes into play.\nUnderstanding the Software Development Lifecycle (SDLC)\nDefinition of SDLC\nThe Software Development Lifecycle (SDLC) is a structured approach to software development that defines the processes and activities involved in creating, deploying, and maintaining software applications. It provides a framework for development teams to work systematically, ensuring that the end product meets the desired requirements and quality standards.\nImportance of SDLC\nThe SDLC is crucial for successful software development projects. It helps in reducing risks, improving efficiency, and enhancing the overall quality of the software. By following a well-defined process, development teams can identify and address issues at each stage, leading to more reliable and robust software applications.\nPhases of the Software Development Lifecycle\nThe SDLC consists of several distinct phases, each serving a specific purpose in the software development process. While the exact number and naming of these phases may vary depending on the methodology used, the core phases remain consistent across different approaches. Let's explore each phase in detail:\nThe first phase of the SDLC is requirements gathering. During this phase, the development team interacts with stakeholders to understand their needs and expectations from the software. The primary goal is to gather detailed and comprehensive requirements that will serve as the foundation for the development process.\nGathering user requirements\nThe development team engages with users, clients, and other stakeholders to identify their needs, challenges, and desired functionalities. This involves conducting interviews, workshops, and surveys to gather relevant information.\nAnalyzing and documenting requirements\nOnce the requirements are gathered, they are analyzed to ensure clarity, completeness, and feasibility. The team then documents these requirements in a detailed specification document, which serves as a reference for the subsequent phases.\nThe system design phase involves transforming the gathered requirements into a detailed architectural blueprint. This phase defines how the software will be structured, how different components will interact with each other, and how data will be stored and accessed.\nThe development team designs the overall architecture of the software, including the choice of technologies, frameworks, and platforms. This ensures that the software is scalable, maintainable, and meets the desired performance requirements.\nDuring this stage, the team designs the database structure that will store and organize the application's data. They determine the relationships between different data entities and create an efficient and secure database schema.\nUser interface design\nThe user interface design focuses on creating an intuitive and user-friendly interface for the software. The team considers factors such as usability, accessibility, and visual aesthetics to design a compelling user experience.\nCoding and Implementation\nThe coding and implementation phase involves translating the system design into actual code. This is where the development team brings the software to life by writing program code using selected coding languages.\nSelecting coding languages\nThe development team writes the code according to the requirements and design specifications. They follow coding standards and best practices to ensure code quality, readability, and maintainability. Additionally, they leverage software development tools and frameworks to streamline the coding process.\nConducting code reviews\nCode reviews play a crucial role in ensuring code quality and identifying potential issues early on. The team conducts thorough code reviews, where other team members review and provide feedback on the code. This helps in catching bugs, improving code efficiency, and maintaining coding standards.\nTesting and Quality Assurance\nThe testing and quality assurance phase is vital to validate the software's functionality, performance, and reliability. This phase involves various testing techniques and processes to identify and resolve any defects or issues before the software is deployed.\nTypes of testing\nDifferent types of testing are performed during this phase, including unit testing, integration testing, system testing, and user acceptance testing. Each type focuses on specific aspects of the software and helps ensure that it performs as expected.\nTest plan creation\nA comprehensive test plan is created to outline the testing activities, test cases, and test scenarios. This plan serves as a guide for the testing team and ensures that all the necessary aspects of the software are thoroughly tested.\nBug tracking and fixing\nDuring testing, defects or bugs may be identified. These issues are logged, tracked, and assigned to the development team for resolution. The team fixes the bugs and performs regression testing to ensure that the fixes do not introduce new issues.\nThe deployment phase involves preparing the software for release and making it available to users or clients. This includes installation, configuration, and training to ensure a smooth transition from development to production.\nInstallation and configuration\nThe software is installed on the target systems, and any necessary configurations are performed. This may involve setting up databases, configuring servers, and integrating with other software or systems.\nIf required, the software development company team provides training sessions to users or clients to familiarize them with the software's features and functionalities. This helps ensure a successful adoption of the software and maximizes its benefits."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:0f240d02-a4f6-4e38-8547-0b5217429db2>","<urn:uuid:750f3581-a24b-4579-80ed-26f420b45361>"],"error":null}
{"question":"When and where was the Anglo-Portuguese Society formally established as a charitable organization?","answer":"The Anglo-Portuguese Society was formally constituted as a charitable body on July 20th, 1938, at the Casa de Portugal in London, with the help of a generous grant from the Portuguese Government.","context":["THE ANGLO-PORTUGUESE SOCIETY – 70 YEARS OF HISTORY\nPosted On July 13, 2018\nIn 1938 a group of business men in London with interests in Portugal planned a dinner in honour of the newly arrived Portuguese Ambassador , Dr. Armindo Monteiro.\nThe affair held at The Dorchester Hotel on May 19th grew into a splendid occasion, attended by over three hundred people with the Rt. Hon. Sir Francis Lindley presiding[i]. The Rt. Hon. Lord Harlech was amongst those who made speeches, after which all those present agreed that something more tangible should be established to reflect the unique relationship between Britain and Portugal.[ii]\nThus on 20th July 1938, with the help of a generous grant from the Portuguese Government, the Anglo-Portuguese Society was formally constituted as a charitable body at the Casa de Portugal[iii] in London. Dr. Armindo Monteiro agreed to be its President as has every Portuguese Ambassador appointed to London since then.\nVarious distinguished people agreed to be Vice-Presidents and an Executive Committee was formed whose Chairman was Sir Denison Ross. Lieutenant Colonel John Cross Brown DSO was appointed the Honorary Secretary, a post he held for twelve years before serving as Chairman for a further seventeen. Viscount Davidson, Sir Alexander Roger, Colonel Bernard Hornung and Mr. Francis Woodhead were all other notable figures involved in the Society’s foundation.\nThe aims of the Society were laid down : to foster the historic relationship between Britain and Portugal by developing the knowledge of people in Britain about Portugal, its people and culture. According to the Rules this was to be achieved by\na) the establishment and support in London of a Library of Portuguese and British books, newspapers and periodicals and Language classes;\nb) exhibitions, demonstrations and illustrations of the intellectual, artistic and economic life of Portugal;\nc) lectures, conferences and functions.\nThe first lecture took place on 26th January 1939 when the renowned historian and Portuguese scholar Professor Edgar Prestage addressed members. The following month a Portuguese Fortnight took place in London with the active participation of the Society and in May that year a Library was started in the Casa de Portugal. The outbreak of World War II obviously meant activities were curtailed, but a luncheon was held at the Vintners Hall in July 1940 in connection with Portuguese Centenary Celebrations ; the Guests of Honour were Lord Halifax, Foreign Secretary, and Lord Lloyd, Colonial Secretary. In that same year the Society issued its first Bulletin, and The Rt. Hon. Sir Francis Lindley took over as Chairman.\nPortugal was hit by a cyclone in April 1941 and Society members were quick to respond by raising £4,000 for its victims. The following August the Anglo-Portuguese News, an English language newspaper published in Portugal, was circulated to members.[iv]\nThe end of hostilities meant that in 1946 the Society could expand. They held their first Annual Dinner on 25th November, with the British Prime Minister, The Rt. Hon. Clement Attlee as Guest of Honour. This event has taken place almost every year since then, the Guest of Honour is always approved by the Society’s President and a range of distinguished British and Portuguese personalities, from different walks of life, have been invited to attend and to speak. To name but a few: Ernest Bevin, Earl Mountbatten of Burma, Lord Home, Lord Carrington, Sir Stephen Wall, Sir Cliff Richard, Dr. Jaime Gama, Dr. Durão Barroso, Dr. Francisco Pinto Balsemão, Prof. Diogo Freitas do Amaral, Dr. Rui Vilar, Engo. Belmiro de Azevedo. In 1955 Viscountess Davidson was the first lady to be asked to speak. Mr. Winston Churchill and Dr. Oliveira Salazar sent messages to the Society to be read out at dinners in 1951 and 1953 respectively.\nThe Society’s Annual Dinners have always been their principal social and fund-raising function. The first ones were held at The Dorchester or Savoy Hotel, the latter then becoming the favoured venue until 1982. The earliest dinners were white tie affairs attended by several hundred members and guests, nowadays dinner jackets are worn. In 1963 the 17th Annual Dinner held to celebrate the Society’s 25th Anniversary was combined with a Ball, and since then members and guests have had a yearly opportunity to socialize, wine, dine and dance at these occasions. British Royal guests have included HRH Princess Anne accompanied by her husband Captain Mark Phillips in 1979 and Their Royal Highnesses the Prince and Princess of Wales who in1986 attended The Treaty of Windsor Ball organized by the Society at Osterley Park. Mention should also be made of the special dinner held at the Merchant Taylor’s Hall in 1988 to celebrate the 50th Anniversary of the Society’s foundation and of HRH The Duke of Bragança’s presence at the Annual Dinner held at The Langham Hilton in 1996. Since 2000 the event has taken place at The Landmark Hotel and it is the generous sponsorship of so many banks, companies and institutions with Portuguese connections that ensures it remains a popular and enjoyable evening.\nOver the years social functions tended to be either dinners or receptions (many for visiting dignitaries) and for some time until 1973 there was an Entertainment Sub-Committee. In 1974 the Executive Committee decided to set up two new committees in order “to diversify the activities and attract new members”. These were the Ladies Committee and the Younger Members’ Committee. The latter started with Society member R. D. Eastaugh in the chair, two other members of the Society along with four members of the Anglo-Brazilian Society made up the committee. Their first year got off to a flying start, with a river party on a Thames launch & two Wine tours to Northern Portugal organized for members of both societies at a reduced cost. The Wine tours were so successful they were repeated in 1975 and that same year a Supper Party at Hurlingham and two Cheese and Wine parties were also well attended. In 1985 they organised four activities which included The Christmas Carnival Ball at The Hurlingham Club. This attracted a total of 360 members and their guests. By this time the events the Younger Members were organizing were generating enough income for a donation of £770 to be paid to worthy charities connected with children in either Brazil or Portugal, a tradition which has continued until the present day. Any Carnival Ball that has taken place since has been most successful, that in 1996 was attended by 600 people, and that in 1998 raised £4,000 for charity, to quote but two examples. In 1990 the Committee became known as the BrasiLusans. Fado evenings, Caiparinha , Brazilian beer and Christmas parties, these are just some of the other events the BrasiLusans have laid on. In 1998 at the A.G.M members amended the Objectives of the Society to include the relief of poverty: the intention of this change was to enable the BrasiLusans to raise funds for charities in this field too and in 2000 they also offered a large grant to Canning House Library, for the purpose of acquiring more Portuguese and Brazilian books.\nThe Ladies Committee chaired by Lady Mary Ross started its activities in 1975 with a visit to Stratfield Saye House where members and guests were shown round personally by the Duke of Wellington. Various activities ensued in subsequent years, then in 1982 the committee held their first Ladies Buffet Lunch (to which gentlemen were always welcome !). These remained a popular fixture until 1997. In 1990 the first New Year reception was held at Canning House, again organized by this committee. Since then this occasion has opened the Society’s calendar of events each January – the Ladies do an excellent job of providing members with delicious home-made canapés, the Portuguese Embassy is prevailed upon on to supply petiscos and there is always plenty of wine. In 2005 having been in the chair for 30 years Lady Ross retired, and in 2006 was awarded with “O Grau de Comenda da Ordem do Infante D. Henrique” by the President of Portugal for her services to Anglo-Portuguese cultural relations.\nSince 1939 talks and lectures have taken place covering a huge range of subjects connected with Portugal, its current overseas territories and former colonies. Members have been addressed by diplomats, politicians, historians, art historians, writers, journalists, artists and other speakers, all knowledgeable in their particular field. Talks on Portuguese wines have always been popular, particularly if combined with a tasting ! Recitals and concerts have taken place at Canning House and other venues as have art exhibitions. Executive Committees, past and present, have always tried to ensure that each year’s programme is a full, varied and interesting one.\nIn 1972 the Society paid its first official visit to Portugal, the 39 members of the party included two former Ambassadors. They went to the Algarve and Lisbon and received VIP treatment everywhere, a highlight being the reception given by the President of Portugal in their honour. In February 1974 the first official visit to Madeira took place, and similar treatment ensued. Since then intermittent trips to Portugal have been organized. In addition to sightseeing, private visits and generous local hospitality have become hallmarks of such tours.[v]\nAs well as mailshots, over the years members were issued with regular Bulletins to keep them in touch with Society events, activities and other matters of Anglo-Portuguese interest. Thanks to sponsorship from the Gulbenkian Foundation[vi] these bulletins developed into a quarterly Newsletter initially known as TAPS in 1984/5. A more sophisticated computerised version which also generated income from advertising was first published in 1995. This was produced 2-3 times a year until 2003, since then it has been issued twice yearly. As well as relevant news, articles, reviews and advertisements are included. Sponsorship from Caixa Geral de Depósitos, enabled the Society to develop its own website and go “online” in 2006. Now anyone can read the Newsletters and be kept informed of everything that the Society has to offer by logging on to: www.angloportuguesesociety.org.uk.\nIn accordance with its educational aims, in 1947 the Society was instrumental in forming a Portuguese Language Committee in co-operation with the Anglo-Brazilian Society and Luso-Brazilian Council. A subsidy was then given to Portuguese language classes until 1997. In 1993 when the University of London & Assessments Council decided to discontinue GCSE Portuguese exams, the Society joined other institutions in making representations to reverse this decision, as a result of which the Ministry of Education provided funds enabling the exams to be held for a few more years. Over many decades further encouragement has been provided from the Prize Fund which awards prizes for educational attainment in Portuguese studies; one of these is an annual prize of £500, granted since 1990 to the best student[vii] of Portuguese, to be awarded in turn by the Departments of Portuguese in different universities throughout the United Kingdom.[viii]. For anyone wanting to further their knowledge, an excellent selection of books on both Portuguese and Brazilian subjects is available at Canning House, these are available for members to consult or borrow free of charge.[ix]\nClearly all that the Society has been involved in since its foundation 70 years ago could not have been achieved without the dedication and hard work of its Chairmen and committee members, all of whom have given freely of their services.[x] Initially the post of Secretary was an honorary one. The Honorary Secretaries dealt with the administration of the Society from their offices, the last one being Mr. John Moryson who worked at Electra House, London headquarters of the Lisbon Electric Tramway Co.[xi] A takeover meant that a new venue was needed. Fortunately space was available at Canning House which also had rooms suitable for functions, lectures and talks. Centrally situated in Belgrave Square and conveniently near the Portuguese Embassy it was, and remains, ideal for the Society’s needs, and the move was made possible thanks to grants from the Gulbenkian Foundation and the Portuguese Government. Thus in 1970 Mr. Moryson handed over the administration to Miss Ann Dunbar who became the Society’s first paid Secretary, employed part-time.\nIn 1971 the Executive Committee decided that “due to the greater volume of work taken on by the Society” its Secretary should be employed full time. This was affordable due to the fact that the Portuguese Government kindly agreed to donate £2,000 per annum to help with administration costs thus ensuring that the amount received from subscriptions could be devoted to cultural activities. Over the ensuing years this grant was gratefully received and gradually increased until 1997 when it totalled £4,000 . In 2002 it was reduced to £2,000 and then withdrawn altogether in 2003 due to austerity measures being taken in Portugal and much to the chagrin of the Portuguese Embassy in London who have always been very supportive. This loss of income dealt a severe blow to the Society’s finances. The Chairman Mr. Roger Westbrook sent out an SOS Newsletter to members asking for help by giving voluntary donations, recruiting new members (particularly in the Corporate Friends category) and attending fund-raising events. By February 2004 the Society was “in the black – just” with its finances described in the Newsletter of May that year as “fragile”. Several weeks later at the AGM Mr. Gavin Trechman took over as Chairman; he and his committee continue trying to balance the books by attracting more new members and encouraging greater attendance at all the functions. It must be added that since 1990 the Society has been fortunate to have the services of Miss Ann Waterfall as its Secretary ; her contribution to Anglo- Portuguese cultural relations received public recognition in 2003 when she was awarded with “O Grau de Dama da Ordem do Infante D. Henrique” by the President of Portugal.\nCurrently there are about 500 members of the Anglo-Portuguese Society, this figure includes those in the Corporate Friends, Life and Overseas categories. As well as enjoying the benefits that membership of their own organisation brings, they are able to take advantage of the close links it has forged with the Caledonian Portuguese Society[xii], the Gulbenkian Foundation, the British Historical Society of Portugal, the Portuguese Chamber and other UK/EU Societies."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:6170c7d1-322b-41cd-a485-0ca9a4ebc7f9>"],"error":null}
{"question":"I'm studying urban development - what's the connection between underpinning methods and historic preservation challenges in cities?","answer":"Underpinning methods are used to strengthen existing foundations through techniques like mass pour, beam and base, and mini-piled approaches, which are essential for preserving historic structures. However, the preservation of these buildings faces challenges beyond just structural reinforcement. The current preservation system, centered on the National Register of Historic Places, can actually impede conservation efforts in urban areas where buildings are scattered or have lost their original appearance. This creates a disconnect between technical preservation capabilities and regulatory frameworks, particularly affecting legacy cities where population loss and building deterioration have occurred. While we have the engineering methods to save buildings through underpinning, the strict 'integrity' requirements of preservation regulations may prevent these buildings from accessing the resources needed for such renovations.","context":["What is Underpinning?\nUnderpinning is a process used to repair, strengthen, or increase the depth of an existing foundation by lowering the footing to allow it to rest on more supportive soil.\nWhile oftentimes underpinning is associated with the remediation of deficient or failing foundations, it is also used in cases where the use of a building has changed, floors are being added to upper stories, or additional depth is desired in subsurface spaces, such as basements or cellars. In dense urban locations, such as New York City, underpinning is also a common practice during the construction of adjoining, adjacent, or nearby structures that require the removal or excavation of the soil supporting the neighboring properties.\nThe process of underpinning begins by removing or excavating the soil from beneath an existing foundation.\nTo avoid the risk of undermining the foundation, which may lead to structural failure, the removal of the soil is performed controlled stages, called ‘pins’, of limited length. The depth of the excavation is determined by a geotechnical engineer, who assesses the soil composition to identify the strata that is suitable to bear the weight of the building. The excavated soil is replaced with new material, typically concrete, which forms a new foundation beneath the existing one. Once one of the ‘pins’ is complete, and the concrete is cured, the process is repeated on the next section of the foundation until the entire length of the wall is reinforced.\nThere are multiple methods of underpinning including:\n- Mass Pour\n- Beam and Base\n- and Mini-Piled\nThe mass pour method is the most common due to its low cost and ability to resist heavy foundation loads. In this method, a solid, continuous concrete foundation is poured beneath the existing foundation is sections. While this method uses a great deal of material, it is the simplest method to engineer, does not require heavy machinery, and can allow for continuity of use during construction.\nA more technically advanced method is the beam and base method, where a reinforced concrete beam is constructed below the entire foundation to replace the existing footing. In this method, the new beam transfers the load to a mass concrete base which spreads the load evenly across the soil. While more advanced in its design, the feasibility of this methods largely depends on the structural configuration of the building above the foundation.\nWhere ground conditions are variable or access around the area of the foundation is limited, the mini-piled method of underpinning may be used. In this method, ‘piles’ or deep vertical structural elements are driven into the ground in drilled holes deep enough to allow the piles to rest on stable soil. The piles typically extend at least 15’ below ground, but depending on the soil condition, are capable to depths of over 50’. While this method can overcome even the most adverse soil conditions, the engineering is more involved, and the process can prove to be quite expensive due to the technical expertise and specialty equipment required.\nIf you are planning on performing structural work on your existing property – either elective or as part of a remediation program – it is important to hire a firm with extensive experience working closely with structural engineers, geotechnical engineers, and the NYC Department of Buildings to mitigate the risk of negatively impacting your property and those adjacent to you.\nIf you have any questions about performing work on your existing building, do not hesitate to contact us.Read more...\nThe Rehabilitation of The Tracy Mansion\nThe Tracy Mansion, located on 8th Avenue between President and Carroll Street in the Park Slope Historic District, is a spectacular 50’ wide mansion that is considered the single most notable example of a Neo-Classical townhouse in the district. The house was designed by renowned architect Frank J. Helme for the John Tracy family, who made their fortune in the shipping and transportation business in the late 19th and early 20th centuries.\nHelmes’ was tasked with creating a mansion that would rival those on Manhattan’s Upper East and West Sides. His design features a symmetrical marble façade with a large, curved central bay with fluted Corinthian columns supporting an entablature with a decorative frieze. The bronze and lead glass arched entrance door is flanked on either side by large monumental windows with pedimented tops set within a rusticated stone base, which serves as the sills for the third-floor windows.\nThe magnificence of the exterior is matched on the interior, where a vaulted marble-paneled entry foyer featuring stained glass and decorative plaster work on the ceiling leads visitors to the front great rooms, themselves resplendent with intricate carved woodwork, art-glass, and colossal marble fireplaces.\nThe house was occupied by the Tracy family until the early 1940s and it was shortly used as a meeting house for the Knights of Columbus until it was ultimately sold and converted into the Park Slope Montessori School in 1969. After being used for classrooms for more than four decades, the house was purchased by the developers in 2012, with the intent of adding to and converting the mansion into condominium units that would retain as much of the original historic fabric as\npossible. The conversion of the house from a single-family home to a multi-family residence sought to avoid many of the failings of adaptive reuse and conversion projects, which often alter or eliminate much of the historic fabric.\nRecognizing the inherent value in the historic features, architect Leonard Colchamiro, and preservation architect Scott Henson, prioritized the retention of the mansion’s most notable features while minimizing the visual impact of the alterations such as the rear and penthouse additions. On the exterior, the historic façade was given new life with the restoration of the original ironwork and the bronze entry doors, the restoration of the original wood windows and the cleaning of the marble façade, among other things.\nTo make the project feasible, the developers determined that they would need to expand the house with rear and rooftop additions that would increase the footprint of the original house and add an additional penthouse unit on the roof. Working closely with the Landmarks Preservation Commission, Scott Henson Architect and Leonard Colchamiro Architect proposed several alterations to the original scheme that would work to make the additions more sympathetic to the historic building and reduce its visibility from President and Carroll Street. This included the incorporation of several setbacks, both front and rear, that would distinguish the volume of the original building from the addition and make the rear balconies less visible to neighbors, and refining the composition of the rear elevation to better match that of the original building.\nAfter five years of work on the conversion and restoration of the historic Tracy Mansion in the Park Slope Historic District, the home has been returned to its original grandeur and it has (once again) been reborn as a residence worthy of its rich history and iconic presence on 8th Avenue.Read more...","By Michael R. Allen\nToday, as historic preservationists delve into the realities of older American cities that have faced population and building loss, we find ourselves reaching—and transcending—our field’s own limits. Nowhere is this more evident than in our engagement with the methodology of “rightsizing,” which has found preservation advocates making the case for embracing some demolition. The converse of this bold new look at urban preservation means developing serious conservation strategies for vernacular building stock that might not come in the tidiest, architectural history textbook–friendly form. If we embrace demolition to save cities, we can’t neglect the preservation work needed for what remains.\nThat’s where our left hand smacks into our right. The National Register of Historic Places, the backbone of American preservation practice for nearly 50 years, looks more like an impediment than a helpmate for the new era of legacy city preservation. In some cases, it actually makes preservation of places important to people more difficult.\nThe major problem for legacy cities is that National Register listing is predicated on a building or district’s “integrity,” a status based on having a majority of seven aspects based on historic appearance. For a city like Boston or Savannah, finding districts that look as they did in an earlier time is a lot easier than finding the same in East St. Louis or Detroit. Fractured neighborhoods don’t stand a chance of becoming historic districts, no matter how hard communities push. The National Register privileges appearance over community will, public commemoration and economic value.\nThe result is that parts of neighborhoods can become National Register historic districts, but areas with scattered remaining buildings—people’s homes and businesses, often where deep neighborhood legacies reside—cannot. This would not be a problem if preservationists regarded the National Register and its enshrined criteria as simply a federal preservation planning tool, which is its true intention. Instead, we have written other preservation laws from local demolition review to state historic tax credit programs to enshrine National Register status and standards. There are social justice implications to all of these laws.\nPreservation practice rooted in the National Register can become arbiter of people’s abilities to even have a neighborhood. In St. Louis, I have worked for years in both advocacy and practitioner capacity urging preservation of the city’s near north side, where 19th-century walking neighborhoods were torn apart by federal urban renewal programs. Today, the city has designated 1,500 acres of north St. Louis as a privatized urban renewal project called Northside Regeneration. While there are several districts in the area, and I co-authored the National register nominations for two new ones in the last four years, most of the area consists of scattered historic buildings whose groupings are not eligible for the National Register due to lost “integrity.”\nThe streets of the St. Louis Place and JeffVanderLou neighborhoods may look depleted to the eye of the preservation official, but to residents’ eyes the streets are ripe with history. The history that these streets embody to many people is not the origination era when the rowhouses and flats were built, but the recent years when African-American residents inscribed their cultural life here. Small businesses hold stories. Remaining houses in JeffVanderLou show us the heroic effort of the Jeff-Vander-Lou Corporation, which broke the urban renewal script by renovating over 800 units of existing housing in the 1960s and 1970s while the bulldozers mowed down adjacent areas. The explanation of why the area can’t get listed in the National Register puzzles residents.\nStill, Northside Regeneration’s preservation plan calls for retaining only buildings eligible for the federal and Missouri historic tax credits. The other areas are marked for wholesale clearance for new construction. The developer, the city and many preservation advocates have consigned these areas to the wrecking ball because they are ineligible for the National Register. That is discordant with the desires of many residents not simply to stay in their homes but to see the areas around them remain recognizable as their neighborhoods. Preservation seems to be too willing to denigrate community will, in favor of bureaucratic consistency.\nWhere is the movement that emerged to fight the federal bulldozer? Historic preservation sprouted up as a major cause in the age when entire neighborhoods were being wrecked in the 1950s and 1960s, including those in north St. Louis that are once more in the crosshairs of progress. What a shame it would be if the movement now became complicit with the postmodern equivalent of Great Society–era urban renewal clearance. Not only does the city lose buildings, but preservation itself loses the chance to embrace a constituency of urban residents who love historic neighborhoods. We can’t simply tell them that their neighborhoods aren’t good enough for us, can we?\nThe St. Louis Cultural Resources Office, under the direction of Betsy Bradley, points to a more inclusive politics of preservation. Last year, in the city’s JeffVanderLou neighborhood, a corner store known as Tillie’s Corner was headed to a National Register listing for Ethnic Heritage. “Miss Tillie” has operated an institution that was a gathering place from the 1940s through the 1980s as JeffvanderLou became an African-American neighborhood. Her granddaughter Carla Pearson spearheaded a grassroots preservation campaign that sadly ended when the buildings collapsed before National Register listing was complete. The Cultural Resources Office pushed to make the site, now a community garden and residence, a City Landmark despite the building loss—because the site was a tremendous cultural site from the recent past.\nThere are more City Landmarks all over the north side of St. Louis, with or without remaining buildings, and the Cultural Resources Office’s willingness to recognize ascribed cultural value instead of relying upon the influence of the National Register integrity standard is commendable. However, none of these City Landmarks can receive state or federal historic tax credits without National Register status. Tillie’s Corner doesn’t need incentives to preserve what is lost, but other lone buildings of neighborhood value and historic significance need financial gaps closed to ensure survival. As long as historic tax credit laws privilege National Register status, which precludes listing of many urban buildings, we’ll lose countless buildings of great cultural value.\nThe new South Carolina tax credit for rehabilitation of abandoned buildings is a great step for preservation of cultural sites, because it divorces preservation incentives from the National Register. State and local laws need to respond to public will to preserve—not the National Register’s standards created for federal management purposes. Preservationists in turn need to champion alternative forms of commemoration instead of pointing people to a single tool in what should be a larger toolkit. The federal 10 percent tax credit is also a useful mechanism, but it is not sufficient to bridge financial gaps in many distressed urban areas, and there are no state-level equivalents.\nReforming the National Register itself is still needed, and urgent. Last year in Indianapolis at the National Preservation Conference, both Raymond Rast and Vincent L. Michael eloquently laid out reforms that would help cultural properties currently deficient in both “integrity” and even “significance” (the National Register castigates ordinary buildings, even though they form the bulk of shared American experience). Ned Kaufman has urged preservationists to utilize the National Register definition of “traditional cultural property,” largely applied to Native American properties, to make the case for listing the urban vernacular associated with ethnic heritage. Reforming the National Register, however, won’t happen without a preservation movement that recognizes that some places need different frameworks for evaluation, commemoration and conservation. The fates of our legacy cities are too important to not develop these new tools.\nNearly 20 years ago, historian Dolores Hayden published her renowned book The Power of Place, in which she argued that preservationists were neglecting sites and buildings associated with women and minority history, social unrest and the lives of the working class. Hayden expressly called for historic preservationists to take seriously “ordinary buildings”—worker’s houses, factories, warehouses and other buildings whose significance is probably not architectural. As long as preservationists take the National Register as a veritable gospel, we are blinding ourselves to all aspects of historic places. Places that matter to people don’t always matter because of the way they look—they matter because of what they mean.\nPreservation practice is going to change based on engagement with the reality of rightsizing older cities. While embracing demolition is the seemingly most radical aspect of that change, confronting—and in some cases overthrowing—the National Register may be more fundamental. Rightsizing is about more than assenting to demolition. Fundamentally, urban rightsizing demands that preservationists recognize that solutions are contextual, collaborative and multi-faceted. If the National Register isn’t helping us do our work, we need to find—or invent—the mechanism that does.\nMichael R. Allen is the founder and director of the Preservation Research Office and a lecturer in American Culture Studies at Washington University in St. Louis. His writing on historic preservation, architectural history and public art has appeared in Next City, the St. Louis Post-Dispatch, Temporary Art Review, PreservationNation, nextSTL and other outlets.\nAllen first published this article as a guest post for the Preservation Leadership Forum blog."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:9e39f6e5-414b-4b5d-9a10-e02b70fe8559>","<urn:uuid:cba51f5d-7c9e-45fb-bca7-16385ee6e31c>"],"error":null}
{"question":"What's the key difference between the fasting practices in Ratha Saptami and Chhath Puja?","answer":"While both festivals honor the Sun God, their fasting practices differ significantly. In Ratha Saptami, the focus is primarily on performing rituals and offerings, with no specific mention of fasting requirements. In contrast, Chhath Puja involves rigorous fasting, with devotees observing a strict fast without water for 36 hours, and some even sleeping on the floor during the festival period.","context":["Click the 'Play' button to read out loud this webpage content\nRatha Saptami is an important day in Hinduism, which is celebrated as a festival. The festivity happens on the Saptami or the 7th day during the Shukla Paksha in the month of \"Magha\" as per the Hindu calendar.\nThis way, Ratha Saptami falls on the next day of Shri Panchami or Vasant Panchami. The day celebrates Sun God, and hence it is also known as Surya Jayanti or Magh Jayanti; it happens in Magha month. This is considered the birthday of the Lord Sun, which people celebrate to show gratitude toward the Sun that enlightens the whole world and makes life possible in the universe.\nOn the day of Saptami, Lord Sun is worshipped who sits on a golden chariot driven by seven white horses. This image denotes the glory of the Sun, which is not just limited to Earth but in heaven and the entire universe too.\nThere are many temples built to honor Lord Sun to praise him through gratitude. On this day, all these temples are decorated; special events and celebrations happen. People visit the sun temples; they thank the Lord for giving them light and energy, and they worship God. Several big temples like Tirumala Tirupati Balaji arrange special pujas and adornments for Sun, and it is majorly celebrated in Indian states like Maharashtra, Tamil Nadu, Karnataka, and Andhra Pradesh.\nTaking a bath on this day holds unique significance. Hence people wake up early in the morning and take a holy bath. The bath taken at the time of Sunrise is considered auspicious on this day. It is also considered to remove all the ailments and negativity while blessing the individual with good health and life. For this reason, the day is also known as \"Arogya Saptami.\" In Tamil Nadu, people use Erukku leaves in their bath to make it more sacred.\nOnce the devotees take a bath, they offer \"Arghyadan\" to Sun. During this, they offer water to Sun, slowly using a small Kalash. This is done by facing the Sun in standing position; people use copper Kalash for the offering. They chant Surya Mantra while offering the water; some people also chant twelve different names of Sun.\nOnce they have performed the water rituals, they worship the Lord Sun. For this, devotees light up a Ghee Diya and circle it in front of the Sun. They then offer red flowers by sprinkling in the direction of Sun. They light up some Kapoor and dhoop batti too. It is a belief that doing so blesses the devotee with the Sun's blessings, and he gets bestowed with a long and healthy life along with success.\nWomen of the household welcome and show gratitude to Lord Sun by drawing the image of Surya with the chariot. They make rangolis in front of their house as a welcome gesture. They then boil the milk in their courtyard in a mud vessel that faces the Sun. This boiled milk is then used to make the sweet rice to offer the Lord Sun.\nChanting Surya Mantra or Gayatri Mantra is considered very auspicious on this day.\nIn Hinduism, Sun is considered God who provides us with light and energy that makes life possible on Earth. Since the Ratha Saptami is considered the Sun's birthday, worshiping him on the day brings blessings of the Sun in one's life. In Hinduism, Sun is also a giver of long life, good health, a positive mind, and success. Hence worshipping Sun on a specific day sanctifies the devotee with all these aspects; plus, it also helps in flushing away his sins committed in the lifetime, so he gets a better life in the next birth and the current birth.\nThe day of Ratha Saptami also denotes the movement of the Sun in the northern hemisphere. The Winter ends, and the Summer season starts; this is also an indication of a rising temperature that is more notable in India's Southern states. The occasion signifies the beginning of the harvesting season too. The farmers harvest their crops and express gratitude to Sun God Surya and nature for providing them the right environment to let the crops grow. This is also the beginning of the New Year in some places.\nThe sacred scriptures prescribe fire ceremony, Abishekam (hydration ceremony), Pooja/Archana (Light and Sound ceremony), Yantra and Mantra (special sounds) as the ways to offer your prayer to the divine. Out of these, fire ceremony is the most evolved spiritual technology on earth to connect with the divine beings. Participate in the remedial services to clear your karma and receive the blessings.","A popular festival of North India, Chhath Puja starts on the sixth day of the Hindu month of Kartik in the Shukla Paksha (waxing moon), and is celebrated for four days. It is a Hindu thanksgiving festival devoted to the Sun, or Surya Deva, and his wife Usha or Chhathi Maiya. Since the Sun is considered to be God of energy and the source of healing in Hinduism, people seek his protection and blessings during the puja. The festival also encourages abstinence from food and water and the worshippers perform several rituals. But what is the story behind celebrating the festival, and why is it celebrated? Let’s have a look into the history, significance, and rituals of Chhath Puja.\nWhat is the History Associated with Chhath Puja?\nWhile the exact origin of Chhath puja is ambiguous, it is believed that it even predates the ancient Vedas texts. There are many references and hymns in Rigveda, Mahabharata, and Ramayana.\nAs per Ramayana, when Lord Ram returned after the exile, Lord Ram and Sita observed a fast in honour of the Sun God and the ritual evolved as Chhath puja.\nOn the other hand, as per Mahabharata, Karna was the child of the Sun, and it is said that he used to offer prayers to the Sun God while standing in the water and later distribute food among the needy. From there, the ritual of Chhath puja started.\nIt is believed that once a family has started performing Chhath puja, it is the duty of them to perform it every year for generations. However, if there is a death in the family, they can skip performing it for that particular year.\nWhat is the Significance of Chhath Puja?\nSun also stands as the symbol of stability & prosperity, and thus, people worship the Sun god for longevity and overall prosperity of the family and loved ones. The festival is celebrated during sunrise and sunset because these are two important periods of the day when a human body can safely get solar energy without any harm.\nIt is believed that Chhath Puja offers mental calmness and reduces the frequency of anger and negative emotions in a person. People also consider that the rituals lay a great detoxification effect on the mind and the body. This, in turn, helps in maintaining the flow of Prana, the life energy of a person, making the devotees more energetic.\nWhat Rituals are Performed on Chhath Puja?\nThis festival is considered to be eco-friendly as all the rituals are performed with natural things such as ‘soop,’ which is made from Bamboo. The fervour around the puja includes fasting, taking dips into the holy water of Ganga. Some devotees sleep on the floor and observe fasting without water for 36 hours. Apart from these, there are different rituals for the four days of Chhath puja; these are:\n1. Nahay Khay – First Day:\nOn the first day of Chhath Puja, the devotees do not eat before taking a bath.\n2. Lohanda and Kharna – Second Day:\nThe second day of Chhath Puja concludes with the Kharna Puja and offering food to the gods. Once the fast gets over, the devotees eat a combination of jaggery-laden kheer and puris.\n3. Sandhya Arghya – Third Day:\nOn this day, the devotees observe a rigid fast, where they do not consume water and food. The day is marked with folk songs and devotees taking a dip in the holy waters of holy rivers like the Ganga, Kosi, etc. They also offer Prasad to the setting sun at the riverbank to receive blessings from the Sun God.\n4. Usha Arghya – Fourth Day:\nThis is the last day of the festival celebration, and the devotees have to go to the riverbank early in the morning to make offerings to the rising Sun. After offering the prayers, they break their fast by having Prasad.\nWith that said, it is important to follow the rituals properly to get the blessings of the Sun god."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:4f631de3-4148-424b-9d1c-49cd75f2f654>","<urn:uuid:868b8dd9-ee76-4fdb-90ef-4085bd27d053>"],"error":null}
{"question":"How do philosophical views on the relationship between mind and body differ in rationalist and immanence-based approaches?","answer":"The rationalist approach, as demonstrated by Descartes, presents a clear dualistic view where the mind and body are separate, with animals being viewed as complex organic machines whose actions can be explained without reference to mind or thinking. In contrast, the perspective of radical immanence envisions humans as inseparably bonded bodies and minds thrown into a world consisting of other bodies and minds, with nothing else beyond these. While the rationalist view emphasizes the distinction between material and immaterial thinking substances, the immanence-based approach focuses on the inherent unity and inseparability of body and mind.","context":["Peeling Potatoes or Grinding Lenses\nSpinoza and Young Wittgenstein Converse on Immanence and Its Logic\nPublication Year: 2012\nPublished by: University of Pittsburgh Press\nDownload PDF (107.8 KB)\nDownload PDF (132.4 KB)\nThe National Technical University of Athens, where I started studying and where I continue teaching, has always been liberal with leaves of absence and helpful in many other ways. I am grateful to those who people and have peopled it—students, colleagues, secretaries, chairpersons, rectors—for unfailing support and encouragement...\nNote on References and Abbreviations\nDownload PDF (112.6 KB)\nCoordinates of a Conversation\nDownload PDF (294.4 KB)\nThroughout the following pages I argue that Wittgenstein’s Tractatus Logico-Philosphicus (TLP) and Spinoza’s Ethics (E) both pursue the same end. We can profitably take each as aiming to establish that there cannot be any position outside the world, thought, and language, that there can be no overarching standpoint from which anyone...\nChapter 1. Mutual Introductions\nDownload PDF (255.1 KB)\nNietzsche acknowledged that God might well not be the traditional God of monotheism, a superperson endowed with all the proper anthropomorphic attributes elevated to the superlative. He understood perfectly that God is fundamentally a transcending court of ultimate appeal and an overarching position put there to collect whatever might satisfy humanity’s...\nChapter 2. Purposes and Ends\nDownload PDF (316.7 KB)\nWhether it is spelled out ontologically, logically, or otherwise, the perspective of radical immanence leaves no room for any higher power to which one can appeal in case of need or any higher authority one can obey in case of doubt. To go on with one’s life, one can work only with what the world at large can provide for the purpose, or more specifically, with...\nChapter 3. Grammar\nDownload PDF (341.0 KB)\nAlmost 300 years of historical distance separates Spinoza from Wittgenstein. Hence the question inevitably surfaces: why are their works so strikingly similar? Were the associated historical changes not important enough, or is the relevant philosophical activity capable of ignoring—or eradicating—historical change? How does it take account, if it does...\nChapter 4. Strategies\nDownload PDF (339.3 KB)\nThe difference in the philosophical spaces available to Spinoza and Wittgenstein gave Spinoza’s strategy a narrower range of possibilities for unfolding; the wider range Wittgenstein enjoyed let him pursue a more intricate strategy. For this reason, understanding Wittgenstein’s strategy may help in understanding the perspective of radical immanence overall...\nChapter 5. Organizing Content\nDownload PDF (322.7 KB)\nIf the first movement of Wittgenstein’s strategy is subsumed under the second, then the propositions Wittgenstein advances along the first movement should be set forth in a way that makes them all self-destruct when the second movement is completed. Everything identifying and connecting those propositions within the body of that work—the way they...\nChapter 6. Metaphysics\nDownload PDF (460.3 KB)\nTo answer these questions, recall how the scientific upheavals of his day determined how each man set out to accomplish his task. My discussion of this, however, was restricted and in a sense anachronistic: it highlighted only the conceptual and hence grammatical aspects of those upheavals and aimed only at displaying how awareness—sharp in the case of Wittgenstein...\nChapter 7. Matching Content\nDownload PDF (289.2 KB)\nThe minimalist metaphysics we have been examining envisions us as inseparably bonded bodies and minds thrown into a world that consists of other bodies and other minds and, in the last analysis, nothing else. It remains to see how Spinoza and Wittgenstein take account of these absolutely fundamental facts, as well as whether and to what extent their...\nChapter 8. Matching Form\nDownload PDF (453.5 KB)\nWe just saw how Wittgensteinian facts and their pictures match Spinozistic extended modes and their ideas taken one by one. But Spinoza holds that both extended modes and ideas are “ordered and connected,” with the order and connection identical in the corresponding two Attributes. The task at this juncture is thus to clarify this relationship and to examine...\nExodus: Toward History and Its Surprises\nDownload PDF (297.5 KB)\nEvidence provided by his biographers shows that Spinoza had neither the opportunity nor the inclination to return to the Ethics after finishing it. As is well known, he had interrupted its composition to write his Theologico-Political Treatise, and after going back to complete the Ethics, he started composing the Political Treatise, a work his death interrupted...\nDownload PDF (326.7 KB)\nDownload PDF (189.7 KB)\nDownload PDF (430.4 KB)\nPage Count: 312\nPublication Year: 2012","|Philosophy Pages||Dictionary||Study Guide||Logic||F A Q s|\nLife and Works\n. . Method\n. . Animals\n. . Doubt\n. . Cogito\n. . God\n. . Error\n. . Extension\n. . Dualism\n. . Cartesianism\nThe first great philosopher of the modern era was René Descartes, whose new approach won him recognition as the progenitor of modern philosophy. Descartes's pursuit of mathematical and scientific truth soon led to a profound rejection of the scholastic tradition in which he had been educated. Much of his work was concerned with the provision of a secure foundation for the advancement of human knowledge through the natural sciences. Fearing the condemnation of the church, however, Descartes was rightly cautious about publicly expressing the full measure of his radical views. The philosophical writings for which he is remembered are therefore extremely circumspect in their treatment of controversial issues.\nAfter years of work in private, Descartes finally published a preliminary statement of his views in the Discourse on the Method of Rightly Conducting the Reason (1637). Since mathematics has genuinely achieved the certainty for which human thinkers yearn, he argued, we rightly turn to mathematical reasoning as a model for progress in human knowledge more generally. Expressing perfect confidence in the capacity of human reason to achieve knowledge, Descartes proposed an intellectual process no less unsettling than the architectural destruction and rebuilding of an entire town. In order to be absolutely sure that we accept only what is genuinely certain, we must first deliberately renounce all of the firmly held but questionable beliefs we have previously acquired by experience and education.\nThe progress and certainty of mathematical knowledge, Descartes supposed, provide an emulable model for a similarly productive philosophical method, characterized by four simple rules:\nWhile engaged in such a comprehensive revision of our beliefs, Descartes supposed it prudent to adhere to a modest, conventional way of life that provides a secure and comfortable environment in which to pursue serious study. The stoic underpinnings of this \"provisional morality\" are evident in the emphasis on changing oneself to fit the world. Its general importance as an avenue to the contemplative life, however, is more general. Great intellectual upheavals can best be undertaken during relatively calm and stable periods of life.\nIn this context, Descartes offered a brief description of his own experience with the proper approach to knowledge. Begin by renouncing any belief that can be doubted, including especially the testimony of the senses; then use the perfect certainty of one's own existence, which survives this doubt, as the foundation for a demonstration of the providential reliability of one's faculties generally. Significant knowledge of the world, Descartes supposed, can be achieved only by following this epistemological method, the rationalism of relying on a mathematical model and eliminating the distraction of sensory information in order to pursue the demonstrations of pure reason.\nLater sections of the Discourse (along with the supplementary scientific essays with which it was published) trace some of the more significant consequences of following the Cartesian method in philosophy. His mechanistic inclinations emerge clearly in these sections, with frequent reminders of the success of physical explanations of complex phenomena. Non-human animals, on Descartes's view, are complex organic machines, all of whose actions can be fully explained without any reference to the operation of mind in thinking.\nIn fact, Descartes declared, most of human behavior, like that of animals, is susceptible to simple mechanistic explanation. Cleverly designed automata could successfully mimic nearly all of what we do. Thus, Descartes argued, it is only the general ability to adapt to widely varying circumstancesand, in particular, the capacity to respond creatively in the use of languagethat provides a sure test for the presence of an immaterial soul associated with the normal human body.\nBut Descartes supposed that no matter how human-like an animal or machine could be made to appear in its form or operations,\nit would always be possible to distinguish it from a real human being by two functional criteria.\nAlthough an animal or machine may be capable of performing any one activity as well as (or even better than) we can, he argued,\neach human being is capable of a greater variety of different activities than could be performed by anything lacking a soul.\nIn a special instance of this general point, Descartes held that although an animal or machine might be made to utter sounds resembling human speech in response to specific stimuli,\nonly an immaterial thinking substance could engage in the creative use of language required for responding appropriately to any unexpected circumstances.\nMy puppy is a loyal companion, and my computer is a powerful instrument, but neither of them can engage in a decent conversation.\n(This criterion anticipated the more formal requirements of the Turing test.)\n|History of Philosophy|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:69ea5414-af30-44e6-95b9-c931cbfea449>","<urn:uuid:33524e9c-29b6-4b89-b813-8e98302851b1>"],"error":null}
{"question":"How do the cardiovascular health implications differ between consuming almond milk versus coconut products?","answer":"Almond milk and coconut products have different cardiovascular health implications. Almond milk, particularly unsweetened varieties, is lower in calories (30-60 calories per cup) compared to whole milk (148 calories) and contains zero saturated fat, making it a potential choice for those watching their caloric intake. Meanwhile, coconut products have shown mixed effects on cardiovascular health. While coconut's medium-chain fatty acids may decrease cholesterol levels in the short-term by reducing bile absorption, research from 21 studies indicates that coconut oil generally elevates total/LDL cholesterol more than non-tropical vegetable oils, though less than butter. Long-term consumption of diets high in saturated fat, including coconut products, can increase serum triglycerides, which are important indicators of cardiovascular disease risk.","context":["Almond milk is flying off the shelves.\nIt's now America's favorite alternative to dairy milk by a massive margin, easily overshadowing choices like soy milk, coconut milk and rice milk. According to Nielsen, almond milk sales grew by 250 percent between 2011 and 2016.\nCow's milk is as American as apple pie, but as plant-based diets continue to become more mainstream in our culture, many have abandoned it. Almond milk looks and tastes similar to cow's milk, and almonds have long been associated with a plethora of health benefits. But is the healthiness of almond milk grossly overestimated by the general public?\nWhen it comes to almond milk brands, White Wave is king. The company—which produces both the Silk and So Delicious lines of almond milk—did nearly $94 million of almond milk sales in 2016. That makes them the most popular almond milk company in America.\nWith that, let's dive into the nutrition facts for Silk Original Almondmilk. A one cup serving contains 60 calories, 2.5 grams of fat, 0 grams of saturated fat, 35mg of potassium, 8 grams of carbohydrate, less than a gram of fiber, 7 grams of sugar and 1 gram of protein. It also contains 45% of your daily calcium, 25% of your daily vitamin D and 20% of your daily vitamin E.\nSilk Organic Original Almondmilk possesses almost identical nutritional facts (albeit with significantly less calcium). Silk Unsweetened Almondmilk also possesses similar nutritional facts, but with 30 calories, 0 grams of sugar and less than a gram of carbohydrate per serving.\nSuch varieties of almond milk contain significantly less calories than cow's milk (a cup of whole milk contains 148 calories while a cup of skim milk contains 83 calories). If you simply begin using a cup of unsweetened almond milk with your morning cereal in place of a cup of whole milk, you'll instantly cut out 826 calories a week.\nHigh amounts of calcium, vitamin D and vitamin E are all pluses. Calcium is crucial for building strong bones and maintaining healthy heart, nerve and muscle function. Vitamin D plays an important role in bone health, but it also has a big impact on athletic performance (and many people are unknowingly deficient in it). Vitamin E helps prevent plaque from developing inside your arteries and helps the body make red blood cells.\nBut almond milk is low in one important nutrient commonly associated with cow's milk—protein. Protein is a powerhouse nutrient. It can help curb overeating by keeping you fuller for longer. High-quality protein provides the amino acids muscles need to repair and rebuild, allowing you to recover from exercise and get stronger over time. The body can also use protein as a source of energy.\nMost almond milks contain just 1 gram of protein per serving (both whole and skim milk contain 8 grams per serving). Kinda strange considering that almonds themselves are known as an awesome source of protein, right? Well, here's the thing—almond milk actually contains very little almonds. A serving of 23 almonds (1 oz.) contains 6 grams of protein. That means that one serving of almond milk (which has just 1 gram of protein) contains less than four actual almonds. In reality, almond milk is largely composed of filtered water.\nAside from protein, sugar can be a concern when it comes to almond milk. If you're looking for the absolute healthiest form of almond milk, you'd be wised to go for an unsweetened, unflavored variety. Unsweetened almond milk contains zero grams of sugar. Therefore, we can conclude that the sugar content in sweetened almond milk is of the \"added\" variety. Sweetened, unflavored almond milks usually contain about 7 grams of added sugar per serving. Not terrible, but not ideal. But flavored varieties (such as vanilla or chocolate) of almond milk often contain 16-22 grams of added sugar per serving—a downright dreadful amount.\nAdded sugar is perhaps the biggest issue in the modern American diet. \"Added sugars contribute additional calories and zero nutrients to food,\" the American Heart Association states. \"Over the past 30 years, Americans have steadily consumed more and more added sugars in their diets, which has contributed to the obesity epidemic.\" Diets high in added sugar have been linked to obesity, Type 2 diabetes, heart disease, tooth decay and even cancer. The AHA recommends a daily limit of 24 grams of added sugar per day for women and 36 grams per day for men, but the average American consumes a whopping 88 grams of added sugar per day. If your almond milk is packed with added sugar, it's not a healthy choice.\nOne other concern to keep in mind is carrageenan, a potentially dangerous ingredient we've covered previously. Some almond milks do contain carrageenan, but many do not. To check out what products do and do not contain carrageenan, check out this guide from the Cornucopia Institute.\nAlmond milk does possess a clear advantage over cow's milk for the many people who are unable to properly digest the latter. Issues such as lactose intolerance, a milk allergy or sensitivity to A1 casein are all reasons why an otherwise healthy person may struggle to digest dairy milk. For such folks, products like almond milk can be a godsend.\nSo, is almond milk actually healthy? It's hard to give a definitive answer on this one. If you're looking to cut down on calories and added sugar and aren't overly concerned about protein content, unsweetened almond milk can certainly be considered a healthy beverage option. It's also high in calcium, vitamin D and vitamin E. Is it better for you than cow's milk? That's debatable, but it really depends on the individual.\nHowever, flavored varieties stuffed with added sugar just aren't a wise choice. Not only do they make it easy to consume large amounts of added sugar quickly, but they don't have enough protein or fiber to help balance things out.\nIf you're someone who's looking to move away from cow's milk, there's no harm in giving almond milk a shot. Just make sure you're checking out the sugar content before you buy, and realize that you need to find additional protein sources to make up for what you're missing by nixing cow's milk.\nPhoto Credit: bhofack2/iStock, pashapixel/iStock\n- Is Halo Top Ice Cream Actually Healthy?\n- Got (Almond) Milk? How 6 Popular Milk Alternatives Measure Up?\n- 6 Reasons You Should Eat Almonds Every Single Day","Cuckoo for coconut: Is it as healthy as you think?\nIs coconut milk good for you, or just another source of saturated fat? Let’s bust some coconut myths and see what the science has to say about any health benefits (or risks) of coconut.\nClaim #1: Coconut is good for heart health.\nResearch studies show that a high intake of saturated fat tends to be linked to a higher risk of cardiovascular disease.1,2\nSo, why the claim that coconut oil could actually be good for the heart?\n- This idea is based on limited evidence that medium-chain fats don’t have the same negative effects as saturated fats on the body’s lipid profile, i.e., various types of cholesterol and triglycerides.\n- Most of the fat in coconut oil is medium-chain fatty acids or medium-chain triglycerides, and these have been seen to decrease cholesterol levels in the short-term by reducing bile absorption.3\n- Over the long-term, however, any diet high in saturated fat can increase serum triglycerides, which are arguably a more important indicator for cardiovascular disease risk.\nIn a review of 21 research papers, including eight clinical trials and 13 observational studies, coconut oil was found to generally elevate total/low-density lipoprotein cholesterol more than non-tropical vegetable oils but less than butter.4\nSo, is coconut good for heart health?\nVerdict: Well, these studies show that while coconut oil may be better for some people than butter or other animal fats, unsaturated fats are far better for reducing risk factors for cardiovascular disease. This would mean that olive, avocado, and fish oil are better choices.\nClaim #2: Coconut is rich in antioxidants.\nA diet high in antioxidants can help prevent oxidative damage to tissues in the body that can lead to disease and may also help prevent premature signs of aging.5\nCoconut, like many plant-derived foods, is a source of antioxidants. Coconut milk, for instance, provides a greater level of antioxidants than cow’s milk and many plant milks. What’s more, these antioxidants seem to survive cooking, meaning that coconut milk dishes, such as a veggie-rich green curry, could be a good way to boost your antioxidant intake.3\nHowever, it has to be said that coconut is also a lot fattier and, therefore, provides a lot more calories than fruits and vegetables.\nVerdict: While coconut does supply antioxidants, it’s highly calorific and shouldn’t be your preferred source of antioxidants. Don’t use it and crowd out other antioxidant-rich foods in your diet.\nClaim #3: Coconut is a traditional health food, so it’s good for you.\nThose touting the health benefits of coconut often fall back on limited epidemiological evidence from indigenous populations where coconut is a normal part of the diet.6 The trouble is that the kind of coconut products eaten in such cultures are usually either the flesh of fresh coconut or freshly squeezed coconut cream.\nIn most countries where coconut has increased in popularity in recent years, fresh coconut is rare.\n- Consumers rely on processed coconut products, such as coconut oil, canned coconut milk, and coconut meal.\n- Research shows that when coconut products are stored for more than two months, levels of short- and medium-chain fatty acids and aldehydes increase due to peroxidation of fats, i.e., the fats get damaged.7\n- The longer the storage time, the less nutritional value in the coconut milk and other products.\nIt should also be noted that in populations where large amounts of coconut are consumed, and where this doesn’t seem to negatively affect cardiovascular health, people tend to eat less processed food in general and are more socially connected and active.\nVerdict: Well, it’s difficult to draw meaningful conclusions about diet from large population studies, and not all coconut products are the same, especially fresh versus processed. So, while coconut might be part of a healthy diet in some places, those health benefits don’t always translate to other groups of people.\nClaim #4: Coconut combats candida.\nSome of the fatty acids in coconut have demonstrated antifungal activity in laboratory studies, leading to the idea that coconut might help tackle candida infections.\nStudies in humans are limited, but a study in mice suggests that compared to a diet rich in beef fat or soybean oil, a coconut-oil-rich diet did help reduce colonization of the gastrointestinal tract with Candida albicans.8 Additionally, switching the beef-fat-diet mice to coconut oil helped to reduce C. albicans colonization.\nIn one small study looking at preterm babies, supplementing milk with coconut oil fatty acids also appeared to reduce colonization with C. albicans.9\nVerdict: Maybe! There’s a reasonable expectation that the antifungal fatty acids in coconut oil could help keep candida and other fungal infections in check. For more serious fungal overgrowth, however, coconut oil might not be sufficient alone – there just isn’t the evidence to support dietary amounts of coconut as having a meaningful antifungal benefit.\nCrisp, refreshing, and packed with probiotics that support gut health to help control candida: Have you tried CocoBiotic yet?\nClaim #5: Coconut is good for your skin.\nAside from possibly being useful topically for isolated fungal overgrowth, coconut creams and lotions are often proclaimed to be a natural way to improve skin health.10,11\nIn one study, virgin coconut oil helped decrease the severity of atopic dermatitis in children and improved skin barrier function by reducing transepidermal water loss.12 Virgin coconut oil has also been seen to support speedier epithelization, meaning that wounds heal faster.11 And, coconut oil has antioxidant and anti-inflammatory effects that may help protect the skin against damage from ultraviolet light, although it shouldn’t be relied on as a sunscreen.13,14\nIn addition, one of the fatty acids in coconut oil, monolaurin, has an antimicrobial effect against the bacteria that cause acne, Propionibacterium acnes, as well as against Staphylococcus aureus and Staphylococcus epidermidis.15\nVerdict: Coconut oil does appear to have a range of benefits when applied topically. It should be stressed, however, that most coconut-based products are not the same as pure virgin coconut oil. Instead, lotions, creams, shampoos, and other coconut-based products often contain very little actual coconut, meaning their benefits for skin health will also be limited.\nFinal thoughts: What’s the right kind of coconut?\nIf you’re curious about blood type and diet, Dr. Peter D’Adamo does not recommend the meat, oil, or coconut milk for any of the four blood types (A, O, B, AB).16 However, all blood types do well on coconut water — and fermenting it to remove the sugar is best!\nAll in all, while coconut may have a few limited benefits as a food and when used topically, too much focus on this one ingredient could stop you from including other healthy foods or taking other positive steps that are far better for your health.\nIf you’re going to include coconut in your diet, the water right out of the coconut is your best bet. Or, look for cultured coconut products (like this). You can also use Body Ecology’s Kefir Starter to ferment coconut water. When coconut water is “cultured” or fermented, the nutrients are preserved and enhanced.\nFor topical applications, consider using straight coconut oil rather than expensive moisturizers and other products that likely contain added ingredients that can actually damage your skin. Coconut oil can also be used as a carrier for essential oils.\nAs always, remember the Body Ecology Principle of Uniqueness — what matters most is what works for you. Eat the foods that most benefit your body at this present moment. Keep an open mind as you listen to your body’s rhythms and changing needs.\n- 1. Jaike Praagman, Linda E.T. Vissers, Angela A. Mulligan, Anne Sofie Dam Laursen, Joline W.J. Beulens, Yvonne T. van der Schouw, Nicholas J. Wareham, Camilla Plambeck Hansen, Kay-Tee Khaw, Marianne Uhre Jakobsen, Ivonne Sluijs. Consumption of individual saturated fatty acids and the risk of myocardial infarction in a UK and a Danish cohort. International Journal of Cardiology, 2018; DOI: 10.1016/j.ijcard.2018.10.064.\n- 2. Jun Li, Qi Sun. Consumption of saturated fatty acids and coronary heart disease risk. International Journal of Cardiology, 2019; DOI: 10.1016/j.ijcard.2019.01.022.\n- 3. Karunasiri, Asiri N. Antioxidant and Nutritional Properties of Domestic and Commercial Coconut Milk Preparations. Int J Food Sci. 2020; 2020: 3489605. Published online 2020 Aug 1. doi: 10.1155/2020/3489605.\n- 4. Eyres L, Eyres MF, Chisholm A, Brown RC. Coconut oil consumption and cardiovascular risk factors in humans. Nutr Rev. 2016 Apr;74(4):267-80. doi: 10.1093/nutrit/nuw002. Epub 2016 Mar 5. PMID: 26946252; PMCID: PMC4892314.\n- 5. Anne M. Walk, Caitlyn G. Edwards, Nicholas W. Baumgartner, Morgan R. Chojnacki, Alicia R. Covello, Ginger E. Reeser, Billy R. Hammond, Lisa M. Renzi-Hammond, Naiman A. Khan. The Role of Retinal Carotenoids and Age on Neuroelectric Indices of Attentional Control among Early to Middle-Aged Adults. Frontiers in Aging Neuroscience, 2017; 9 DOI: 10.3389/fnagi.2017.00183.\n- 6. Vanga SK, Raghavan V. How well do plant based alternatives fare nutritionally compared to cow’s milk? J Food Sci Technol. 2018 Jan;55(1):10-20. doi: 10.1007/s13197-017-2915-y. Epub 2017 Nov 2. PMID: 29358791; PMCID: PMC5756203.\n- 7. Tinchan P, Lorjaroenphon Y, Cadwallader KR, Chaiseri S. Changes in the profile of volatiles of canned coconut milk during storage. J Food Sci. 2015 Jan;80(1):C49-54. doi: 10.1111/1750-3841.12730. Epub 2014 Dec 22. PMID: 25533179.\n- 8. Gunsalus KT, Tornberg-Belanger SN, Matthan NR, Lichtenstein AH, Kumamoto CA. Manipulation of Host Diet To Reduce Gastrointestinal Colonization by the Opportunistic Pathogen Candida albicans. mSphere. 2015 Nov 18;1(1):e00020-15. doi: 10.1128/mSphere.00020-15. PMID: 27303684; PMCID: PMC4863630.\n- 9. Arsenault AB, Gunsalus KTW, Laforce-Nesbitt SS, Przystac L, DeAngelis EJ, Hurley ME, Vorel ES, Tucker R, Matthan NR, Lichtenstein AH, Kumamoto CA, Bliss JM. Dietary Supplementation With Medium-Chain Triglycerides Reduces Candida Gastrointestinal Colonization in Preterm Infants. Pediatr Infect Dis J. 2019 Feb;38(2):164-168. doi: 10.1097/INF.0000000000002042. PMID: 29596218; PMCID: PMC6604858.\n- 10. Varma SR, Sivaprakasam TO, Arumugam I, Dilip N, Raghuraman M, Pavan KB, Rafiq M, Paramesh R. In vitro anti-inflammatory and skin protective properties of Virgin coconut oil. J Tradit Complement Med. 2018 Jan 17;9(1):5-14. doi: 10.1016/j.jtcme.2017.06.012. PMID: 30671361; PMCID: PMC6335493.\n- 11. Nevin KG, Rajamohan T. Effect of topical application of virgin coconut oil on skin components and antioxidant status during dermal wound healing in young rats. Skin Pharmacol Physiol. 2010;23(6):290-7. doi: 10.1159/000313516. Epub 2010 Jun 3. PMID: 20523108.\n- 12. Evangelista MT, Abad-Casintahan F, Lopez-Villafuerte L. The effect of topical virgin coconut oil on SCORAD index, transepidermal water loss, and skin capacitance in mild to moderate pediatric atopic dermatitis: a randomized, double-blind, clinical trial. Int J Dermatol. 2014 Jan;53(1):100-8. doi: 10.1111/ijd.12339. Epub 2013 Dec 10. PMID: 24320105.\n- 13. Kim S, Jang JE, Kim J, Lee YI, Lee DW, Song SY, Lee JH. Enhanced barrier functions and anti-inflammatory effect of cultured coconut extract on human skin. Food Chem Toxicol. 2017 Aug;106(Pt A):367-375. doi: 10.1016/j.fct.2017.05.060. Epub 2017 May 28. PMID: 28564614.\n- 14. Korać, Radava & Khambholja, Kapil. (2011). Potential of herbs in skin protection from ultraviolet radiation. Pharmacognosy reviews. 5. 164-73. 10.4103/0973-7847.91114.\n- 15. Preuss HG, Echard B, Enig M, Brook I, Elliott TB. Minimum inhibitory concentrations of herbal essential oils and monolaurin for gram-positive and gram-negative bacteria. Mol Cell Biochem. 2005 Apr;272(1-2):29-34. doi: 10.1007/s11010-005-6604-1. PMID: 16010969.\n- 16. D’Adamo, Peter. Eat Right 4 Your Type: Revised Edition. ARROW (YOUNG), 2017."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c6151da3-c492-45aa-a2ed-3084a489316f>","<urn:uuid:751509bf-8e26-46ab-a96e-1cbad7708c66>"],"error":null}
{"question":"How do Ethernet's CSMA/CD and BBR TCP differ in preventing data transmission conflicts?","answer":"CSMA/CD (Carrier Sense Multiple Access/Collision Detection) prevents conflicts by having devices check if the network is idle before transmitting, and if a collision occurs, devices wait a random time before retrying. BBR TCP, on the other hand, prevents conflicts by measuring the network's actual data delivery rate and using that to build a model of network capabilities, which determines the appropriate transmission rate without causing congestion or data loss.","context":["1. About Ethernet\n2. Ethernet at Data Link Layer\n3. Ethernet at Physical Layer\n4. IEEE standards\n5. Features of ethernet\n○ Port mirroring\nBob Metcalfe, first invented the Ethernet network on May 22, 1973 to\ninterconnect advanced computer workstations.\nEthernet is a network protocol referred as IEEE 802.3 protocol.It was\nintroduced in 1980 and then standardized in 1983 as IEEE 802.3\nIt is a family of computer networking technologies that controls how data is\ntransmitted over a LAN and MAN network\nThe protocol has evolved and improved over time and now it can deliver\nspeed up to gigabits per second.\nIt is used to connect computers and devices.It connect any type of computer\nto its network if that device has an Ethernet adapter or network card.\nEthernet Works on Layer 1 and Layer 2 of OSI model.\nHow data is transmitted over Ethernet cable?\nEthernet is a shared medium for sending packets of data. There are rules to\navoid conflicts and to protect data integrity over Ethernet.\nDevice check when the network is available for sending packets.\nIt is possible that two or more devices at different locations will attempt to\nsend data at the same time. When this happens, a packet collision occurs.\nIn order to eliminate collisions Ethernet uses a protocol called Carrier Sense\nMultiple Access/Collision Detection (CSMA/CD).\nCSMA/CD is a protocol which defines how to respond when two devices\nattempt to transmit packets simultaneously.Any device can try to send a\npacket at any time in the network.\nEach device senses the line that it is idle and available for use. Then the\ndevice start to transmit its first packet.\nIf then another device tried to send at the same time, a collision is said to\noccur and the packet is discarded. Each device then waits a random amount of\ntime and retries until successful in getting its transmission sent.\nEthernet at Data link Layer:\nEthernet at data link layer is responsible for ethernet addressing ,Hardware\naddressing or MAC addressing.\nAt data link layer, Ethernet encapsulates packets that are coming from\nNetwork Layer and form frames and then transmit over physical media.\nEthernet MAC addresses are in hexadecimal format. It consists of 6 byte (48\nbits).Here first three bytes tells the identity of the vendor and the last three\nbytes tells node identity.\nAt Data Link layer, no modification is required to the MAC address when going\nbetween different physical layer interfaces, such as from Ethernet to Fast Ethernet\nA MAC address (Media Access Control address) is a unique code which is\nstored on network interface cards or ethernet cards. There are two types of\n● Universal administered MAC addresses assigned to devices by their\n● Organizationally Unique Identifiers (OUI) are administered by the IEEE\nEthernet Addressing: MAC address format\n● The first three bytes tell about the manufacturer also known as the\nOrganizationally Unique Identifier (OUI).\n● The 1st bit of the first byte is referred to as unicast or multicast bit.\nIf the bit is set to 0 (zero), the frame is meant to reach only one receiving\nNIC. This type of transmission is called unicast.\nIf the bit is set to 1, the frame will still be sent only once to specific nodes\nin network; This is called multicast addressing.\n● The 2nd bit of the first byte is referred to as the U/L bit (Universal/Local)\nwhich identifies how the address is administered i.e. Globally unique or\nEach NIC card/ethernet adapter contain a unique code which is provided by the manufacturer, that unique code is known as Media Access Control address or Ethernet Address.\nEthernet at Data link Layer\nMac address = 04-00-00-00-00-00\nThe first byte in binary form is 00000100, where the first bit is 0.\nThis define that the address is unicast address.\nHere the second bit is also 0\nThis define that the address is a universally administered address.\nThe main function of data link layer is to convert bits into bytes and then bytes\nIt encapsulates the packet that are coming from network layer to frame and\nthen transmit it to the physical layer for transmission.\nThe MAC frame format for Ethernet, IEEE 802.3 used within the 10 and 100\nMbps systems is shown as\nEthernet at Data link Layer\nThe frame consists of seven parts divided into three main areas:-\n○ Preamble (PRE) – This is seven bytes long. It consists of a pattern of\nalternating ones and zeros, and this informs the receiving stations\nabout the start of frame and enable synchronisation. (10 Mbps\n○ Start Of Frame delimiter (SOF) – This consists of one byte. It contains\nan alternating pattern of ones and zeros but ending in two ones.\n○ Destination Address (DA) – This field contains the address of station for\nwhich the data is intended. The leftmost bit indicates whether the\ndestination is an individual address(0 bit) or a group address(1 bit).\n○ Source Address (SA) – The source address consists of six bytes, and it\nis used to identify the sending station.\n○ Length / Type – This field is two bytes in length. It gives MAC\ninformation and indicates the number of client data types that are\ncontained in the data field of the frame.\n○ Data – This contains the payload data and it size ranges up to 1500\nbytes. If the length of the field is less than 46 bytes, then the padding\ndata is added to fill the data block\n○ Frame Check Sequence (FCS) – This is four bytes long. It contains a 32\nbit Cyclic Redundancy Check (CRC) that is generated over the\nDestination Address, Source Address, Length / Type and Data fields.\nEthernet at Physical Layer\nThe Ethernet at physical layer defines the electrical or optical properties of the\nphysical connection between a device and the network or between network\nIt tells about the medium of transmission used between the network devices\nIt speed ranges from 1 Mbit/s to 100 Gbit/s.\nThe physical medium can be a coaxial cable , twisted pair cable or optical\n1. Standard Ethernet (10mbps)\n○ 10Base5 (Thick coaxial wire, Bus topology)\n○ 10base2 (Thin coaxial wire, Bus Topology)\n○ 10BaseT (UTP cable, Star Topology)\n○ 10BaseF (Optical Fiber Cable, Star Topology)\n2. Fast Ethernet (100mbps)\n○ 100BaseTX (2 UTP wire)\n○ 100BaseFX (2 Fiber wire)\n○ 10Base T 4 (4 UTP wire)\n3. Gigabit Ethernet (1gbps)\n○ 1000Base SX (Short wave,Optical Fiber)\n○ 1000Base LX (Long wave,Optical Fiber)\n○ 1000Base CX (STP wire, 2 Copper wire)\n○ 1000Base T (STP wire, 4 Copper wire)\n4. Ten Gigabit Ethernet (10gbps)\nTo understand standard Ethernet code, we have to understand the meaning of\n10 – at the beginning indicate the speed of network i.e. 10Mbps.\nBASE – indicates the type of signaling used i.e. baseband.\n2 or 5 – at the end indicates the maximum length of cable in meters.\nT – at the end stands for twisted-pair cable.\nX – at the end stands for full duplex-capable cable.\nFL – at the end stands for fiber optic cable.\nIEEE 802.3 is a working group which define the physical layer and data link\nlayer (MAC) of wired Ethernet.\nEthernet is defined under a number of IEEE standards and each standard\ndefine a different type of Ethernet.\nEach of the Ethernet IEEE 802.3 standards can be uniquely identified.\nThese IEEE standards are being updated as a new Ethernet cable is evolved.\nSome of them are also defined below.\nStandards Year Description\n802.3a 1985 10Base-2 (thin Ethernet)\n802.3c 1986 10Mb/s repeater specifications (clause 9)\n802.3d 1987 FOIRL (fiber link)\n802.3i 1990 10Base-T (twisted pair)\n802.3j 1993 10Base-F (fiber optic)\n802.3u 1995 100Base-T (Fast Ethernet and auto-negotiation)\n802.3z 1998 1000Base-X (Gigabit Ethernet)\n802.3ab 1999 1000Base-T (Gigabit Ethernet over twisted pair)\n802.3ac 1998 VLAN tag (frame size extension to 1522 bytes)\n802.3ae 2002 10-Gigabit Ethernet\n802.3at 2005 Power over Ethernet Plus\nFeatures of Ethernet\nPower over Ethernet (PoE)\nPower over Ethernet (PoE) is a networking feature which is defined by the IEEE\n802.3af and 802.3at standards.\nIt is a solution in which electric power is pass along with data on twisted pair\nEthernet cabling. This allows a single cable to provide both data connection\nand electric power to device such as wireless access points, IP cameras and\nThus there is no need of an extra AC power cord to be attached to device.\nThis minimizes the amount of cable required and reduce the difficulties and\ncost of installing extra outlets\nThe IEEE standards for PoE require category 5 cable for high power levels and\nallow using category 3 cable for low power level.\nFor example, a digital security camera which requires two connections to be\nmade when it is installed. One for network connection, in order to be able to\ncommunicate with video recording and display equipment and other for power\nsupply. So here PoE is used.\nPort mirroring is used on a network switch to send a copy of network packets\nseen on one switch port to a another switch port in network. This is used for\nmonitoring network traffic.\nPort mirroring on a Cisco Systems switch is known as Switched Port Analyzer\n(SPAN) or Remote Switched Port Analyzer (RSPAN). Other vendors have\ndifferent names for it, such as Roving Analysis Port (RAP) on 3Com switches.\nNetwork engineers or administrators use port mirroring to analyze and debug\ndata or diagnose errors on a network.","What is TCP?\nSince the beginning of the its existence, the internet has expanded in scope, traffic, content, and a myriad of other ways. The protocols that make up the internet’s backbone have mostly remained the same since they were developed in the 1980s. The Transmission Control Protocol (TCP) was one of the first networking protocols defined during the internet’s development, and specifies how data should be transmitted and received. TCP implementations, initially developed in the 1980’s, attempted to discover the right rate at which to send data by constantly trying to send more until reaching the point that not all of the data arrived at its destination, and then backing off on the amount being sent. Multiple TCP connections would share links, because each connection’s attempts to use more and more bandwidth would end up with each claiming part of the available bandwidth. For over 30 years, TCP has been how everyone connects to services on the internet, and its canonical implementation has been held up as the gold standard method for how network capacity can be shared amongst competing users. Recently, a group of Google researchers, including TCP pioneer Van Jacobson, developed a better algorithm for congestion control. BBR, or Bottleneck Bandwidth and RTT (Round-Trip Time), directly measures and models the network bottleneck in order to determine the right rate at which to send data. It uses analytical math, instead of the more traditional “guess and check” method used by older implementations of TCP. This modeling-based approach can more fully exploit existing network capabilities, leading to increased connection bandwidth without needing to upgrade network hardware. This represents a critical upgrade for today’s internet which handles exponentially increasing amounts of data.\nHow Does BBR Make the Internet Work Better?\nBBR prevents traffic congestion by measuring how fast the network can actually deliver data, and then using that rate to construct a model of the network’s capabilities. This model is then used to determine how much data can be sent at a time. By using this model (and repeatedly re-measuring to verify that the model remains correct) BBR-based TCP can transmit data at the right rate without causing congestion and loss of in-flight data. Even more, this model indicates that there is a lot of spare network capacity that older implementations of TCP are unable to utilize. By discovering this available capacity and making it available for use, BBR unlocks network capabilities that have always existed, but have never before been easily used. Using BBR TCP means that the right rate of traffic transmission is much more quickly discovered and that less traffic has to be re-sent. Additionally, the BBR TCP rate is frequently faster than the rate discovered by older TCP implementations. This makes for a faster everything – measurement, connection, etc., and increases accuracy of measurement. Additionally, this update can be implemented on just the server side, and so doesn’t require patches for individual browsers or smartphones. The move to BBR-based TCP has already started to happen without most of us even noticing.\nHow Does BBR Affect Measurement Lab\nM-Lab is already planning to do the work on the platform side to support BBR based TCP measurement, which will allow for more accurate and faster speed tests at the scale that M-Lab already supports.\nA BBR-based version of our NDT speed test should allow users to more accurately determine their network download capacity, and should allow them to do so in network contexts (high-latency networks, networks with high packet loss) that were previously infeasible for accurate testing. The more accurate version of NDT should be rolled out transparently to users - the main difference they will notice is that their download speed might improve. We will also be working with partners to develop new testing clients which should be able to determine correct download speeds in much less time than the 10-second test interval that NDT currently requires. We do this in an effort to continue to measure the end-to-end capacity available to internet users - the rollout of BBR-based TCP on servers across the internet means that we need it on our servers as well. Because BBR-based TCP implementations have different internals from older TCP implementations, the schema for our saved TCP data will change correspondingly. We will work with our data partners to ensure a smooth transition from the old-style results to the new ones. Right now, there are more than 250,000 download speed tests performed against M-Lab’s software infrastructure every day. Thanks to BBR, those tests are all going to get an upgrade."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:1bd61113-f6d5-408d-8d37-35f566ec1171>","<urn:uuid:54505ef1-09da-46d0-8898-56cfa621f496>"],"error":null}
{"question":"What naming controversies exist around ancient Southwest sites, and how has terminology evolved over time?","answer":"Several naming controversies exist around ancient Southwest sites and peoples. The term 'Anasazi' has been replaced by 'Ancestral Puebloans' because Anasazi is a Navajo word meaning 'enemy ancestors' and was deemed inappropriate since these peoples were not necessarily enemies. Similarly, some site names are historically inaccurate, such as Montezuma's Well and Montezuma Castle, which have nothing to do with Montezuma or the Aztecs but were misnamed by their discoverers. Even at Whitney Pocket, the name simply derives from an original landowner's surname, while 'Pocket' refers to the geological formations in the sandstone.","context":["This page last updated on 03/14/2018\nBack to [Gold Butte National Monument - Summary Page]\n|Directions: From Las Vegas, drive north on Interstate-15 for about 70 miles to Exit 112 (Highway 170 to Bunkerville). This exit is about 5 miles south of (before) Mesquite. The official Gold Butte Backcountry Byway starts on the Interstate 15 off-ramp.After leaving the I-15 and driving south for approximately 30 miles on Gold Butte Road, a roughly paved road that climbs steadily to an elevation of 3,100 feet, the pavement ends at a location called Whitney Pocket (Fig. 02) where it passes through a break in the mountains. This large parking area (Fig. 18 below) makes for a great place to act as a staging area for any exploration of the Gold Butte region.|\n|Description: The Whitney Pocket, located at the base of the Virgin Mountains is a palette of sandstone color. ‘Whitney” is the surname of an original landowner; \"Pockets\" is due to the pockets of red Aztec sandstone that has been exposed by the erosion of the lower fringes of the Virgin Mountains. This grouping of yellow, tan and rusty red sandstone outcrops exhibit unusual erosion patterns full of cave-like holes and bowls throughout the monoliths (Figs. 01, 03 & 04). Rainwater that pools in the depressions here has been a valued resource for desert travelers going back in time as far as early Native Americans (the Anasazi). |\n|Whitney Pocket Dam: Between 1933 to 1942, the Civilian Conservation Corp (CCC) built a concrete dam (Fig. 06) on the north side of the road at a crevasse (Fig. 05) in the sandstone in an attempt to catch water. (Figs. 07 & 08) show the front and back side of the dam. There is a pipe with a water flow turn on/off that leads from the base of the dam, under ground to a livestock watering trough near the mouth of the crevasse (Fig. 09). A nearby cave was walled in by the CCC. Parts of the walls are now disintegrating rapidly and a sign urges the public to assist in conserving what remains. In spite of CCC and cattlemen’s efforts to harvest the scarce water supplies, this region does not favor successful ranching. The summers are unbearably hot and often the winters are quite harsh. The area is loaded with other CCC artifacts, along with native petroglyphs and evidence of their civilization. |\n|04/25/2017 Trip Notes: Harvey and I visited the dam again, this time with our friends Bob Croke and Jim Herring. While they climbed to the top of the dam, I stayed below and took pictures (Fig. 10). Though both Jim and Bob used different methods to descend the high steps of the dam (Figs. 11 & 12), both agreed that is was a little \"hairy\".|\n10/22/2015 Trip Notes: For four out of my five visits to the Gold Butte region, I have used the large parking area (Fig. 18) at Whitney Pocket as a staging area for our exploration of the area. This visit was my 2nd visit to the dam. My first visit, chronicled in the 02/05/2014 trip notes below, was with Harvey Smith. Today’s visit on a trip with the Rock hounds from the Henderson Senior Center was with my hiking partner, Blake Smith. After surveying the trough and the area we both ascended the steep steps built into the right side of the dam (Fig. 13). Once we reached the top (Fig. 14), we were presented with a nice view looking back to the main Whitney parking area (Fig. 15). We also found that there was another step of stairs leading down the back side of the dam; even though we decided not to descend it (Figs.16 & 17). We then decided to continue walking around these outcrops looking for any signs of petroglyphs and the walled in cave (Figs. 18 & 19). We found neither. The last picture (Fig. 20) was looking north at the mountains on the back side of the outcrop.\n|06/10/2015 Trip Notes: The purpose of this visit was to locate Kirk’s Grotto and Little Finland. We used the large parking area at Whitney Pocket as a staging area for this trip (Fig. 21). Upon our return from these areas, we toured Whitney Pocket until we found the dam (Fig. 22). Just south of Whitney Pocket the sandy desert is dotted by Joshua trees as well as various types of cacti. These rocks, weathered into rounded boulders, cavities, deep fissures and other formations, contain beautiful shades of orange and white mixed with the usual red sandstone, stained by iron compounds (Figs. 23-25).|","Throughout the southwest, ancient people left their mark but mysteriously abandoned their villages between 750 and 1350 AD/Current Year (CE). The ruins are beautiful reminder of their heritage.\nRecently, the term Anasazi has been replaced by Ancestral Puebloans, referring to people who lived in the Colorado Plateau roughly 2000 to 700 years ago. Why? Anasazi is a Navajo word that translates to “enemy ancestors”, and the peoples were not necessarily enemies.\nThe Dwellings of Puebloan People\n- Pit-houses were the earliest Ancestral Pueblo residences. Subterranean wood and earth structures with roof entryways were cool in the summer and warm in the winter.\n- Pueblos, above ground stone-masonry structures, later replaced pit-houses.\n- Cliff Dwellings were compact, masonry-walled pueblos located in cliffs. Ladders used to access the dwellings were pulled up for protection.\n- Kivas are round, semi-subterranean structures used for ceremonial purposes or other large gatherings.\nVisiting the Ruins\nPalatki Heritage Site, Sedona.\nCliff dwellings and rock art from the Sinqua people from 1150 to 1350 AD/CE. Sinqua means “without water”.\nPalatki is a Hopi word for Red House; however, the Hopi had no specific name for this site. It was named by an archaeologist.\nSince its discovery in the 1900’s, about 70 to 90% of the structure has disappeared due to looting and misuse. It has since been designated a World Heritage Site.\nMontezuma’s Well, Sedona\nAn oasis/sinkhole with cliff dwellings for the Sinaqua in the 1100’s. Despite less than 13″ of annual rainfall, the well is replenished daily with 1.5 million gallons from an underground spring percolating through miles of rock.\nDespite the name, the site has nothing to do with Monetzuma or the Aztecs, the name is based on an inaccurate assumption by those who discovered the ruins.\nMontezuma Castle National Monument, Sedona.\nBuilt 90 feet up a cliff face, possibly to protect from flooding, this is a five-story structure with 20 rooms for 30 to 50 Sinaqua. 1100-1450 AD/CE. (Montezuma is a misnomer).\nTuzigoot National Monument, Sedona.\nTuzigoot is a word that means “crooked water”. The pueblo ruins are two to three stories with 110 rooms that housed 250 Sinaqua.\nWupatki National Monument, north of Flagstaff.\nWupatki is Hopi for “Tall House”: By 1182, 85 to 100 Puebloans lived in Wupatki Pueblo with 100 rooms, a community room and a ball court. The community room was likely used for trading and interaction with the larger community. Within a day’s walk, a population of several thousand lived in separate villages. Wupatki, the largest building within 50 miles, and the nearby hamlets were mysteriously abandoned circa 1250 AD/CE.\nPlayball!: There are over 200 ball courts in southern Arizona, common from 750 to 1200 AD. The Wupatki people intermingled with their neighbors between villages, (a days walk).\nThe Blowhole – A crevice in the earth’s crust that connects to an underground passage in the village. Archaeologists have no idea why. There was a nice warm updraft that I appreciated on a freezing day in May.\nWukoko a neighboring hamlet\nWukoko Pueblo, an elegant hamlet 3 miles from Wupatki Pueblo, is built on a giant rock and stands three stories tall. Pieces of wood beam remain.\nThe Sad Truth\nThe Visitor Center displays testimonials from Navajo families that called this place home, but were evicted due to competition and conflict with ranchers, the railroad, and later the National Park Service.\nCoombs Excavation Site, Boulder, UT,.”Anasazi” State Park\nA village of almost 100 rooms occupied between 1050 and 1175 AD/CE. The excavation site shows evidence of early pit houses and masonry pueblos. The people departed in 1175 AD and the village was burned, possibly by the inhabitants. One artifact on display, the Atlatl (Spear Thrower) is 2,000 years old.\nPetroglyphs are carvings or etchings in the rock. Pictographs are painted figures.\nPalatki Rockart (1150 to 1350 AD), Sedona. The rock art predates the ruins and are thousands of years old.\nNewspaper Rock, Near Needles/Canyonlands UT. Petroglyph panel recording 2000 years of early human activity. Etchings on the rock date from BC time to 1300 AD/CE by Ute, Navajo, Europeans, Americans (and some recent high school grads).\nPetroglyphs Fremont Cliffs Capitol Reef. 600 to 1300 AD/CE\nResources and Information\nSubscribe to receive emails for new posts\nAt A Glance: Click Here for Archived USA Posts – Click Here for Arizona Posts\n6 thoughts on “AZ-The Ancients, Native Americans”\nSo awesome! I have seen the sign for Montezuma’s Castle several times but never stopped. Next time I am in the area I will definitely visit! Thanks for sharing!!!\nI will definitely go!!!\nInteresting images those Petroglyphs Fremont Cliffs Capitol Reef. 600 to 1300 AD/CE.\nLove this post! Seeing these dwellings together is awe-inspiring mixed with some sad truths. Thanks for the gorgeous pics and history lesson. xo\nComments are closed."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1b91b2ea-11bc-4faf-8e8d-dd37a696d4d9>","<urn:uuid:700bc875-7701-4267-8885-ac096f530d13>"],"error":null}
{"question":"Hi! I am interior designer working on sustainable spaces. Can you compare painting furniture vs vertical farming for eco-friendly indoor solutions - benefits and implementation challenges?","answer":"Painting furniture offers economic benefits and durability, requiring basic materials like paint, sandpaper, and protective gear. Implementation challenges include proper ventilation, multiple coating steps, and potential lead paint risks in older furniture. For vertical farming, benefits include water efficiency (using less water than conventional farming), space optimization through vertical growth, and reduced pest issues in controlled environments. However, implementation challenges include high energy requirements, substantial initial investment costs, and difficulties in scaling beyond community-level operations. Both solutions can be implemented indoors, though vertical farming requires more sophisticated technology and monitoring systems.","context":["One economical way to revitalize your dining room is to paint tired or badly worn furniture. Paint is also a great tool for creating a stylistically unique dining set that can become the centerpiece of your decor. Especially for parents with growing children, painting your dining room furniture creates durable kid-friendly surfaces for family meals and celebrations. Choose a favorite color and use it to brighten your dining-room table and chairs.\nDon work clothes and protective gear. Cover your work surface with the drop cloth.\nSand unfinished furniture to remove any surface barriers to adhesion of new paint. Rub all surfaces of unfinished furniture with 220-grit sandpaper, including the undersides of chair seats and the underside of the table top. The surface should feel smooth but not slick to the touch. Use a hand-held orbital sander or sanding block on large surfaces. Wrap sandpaper around rungs, spindles and legs if necessary.\nSand previously finished furniture in several stages, using 100-, then 150-grit paper, finishing with 220-grit. Remove any peeling or blistered paint and smooth lines between remaining paint and any patches of bare wood. Any remaining paint should be smooth but not slick. On previously varnished, lacquered or oil-stain-finished furniture, sand through finish to expose as much bare wood as possible.\nRemove sanding residue with soft cloths or a hand vacuum cleaner.\nCover all surfaces to be painted with a thin even coat of primer paint. For the best job, start by turning chairs upside down, to cover the bottoms of chair seats and the easily-missed bottom edges of chair rungs. On the table, include all surfaces except the underside part of the table top concealed from view by the molding around the edge. Double-check rungs, spindles and legs and wipe off any paint drips with a rag or paper towel. Let the primer dry a minimum of two hours before applying the first top coat.\nSand the primer coat lightly only if the specific brand you have chosen suggests sanding. Most primers are designed to improve paint adhesion without extra effort.\nBrush a thin even coat of topcoat paint over the primed surfaces. For the most durable job, this is the first of two coats on most surfaces, the first of three on the chair seats and the table top. Use long, even strokes and avoid going back over painted areas to produce the smoothest job. Any small streaks or missed spots can be covered by the second coat. Check and wipe any drips. Let the first coat dry at least two hours and preferably longer for full drying.\nSand the fully dried top coat surfaces lightly with 220-grit paper. Wipe sanding residue off thoroughly. Apply a smooth even second top coat, using long even strokes to create the smoothest possible surface. Let paint dry completely.\nLightly sand chair seats and the table top with 220-grit paper. Apply a third thin even coat of topcoat paint. This prolongs the surfaces that receive hardest use. Keep furniture out of use for 24 hours so that paint dries completely and forms a tight bond with the wood surfaces.\nApply a single coat of clear polyurethane if you want further protection from hard use. Keep furniture out of use for an additional 24 hours.\nThings You Will Need\n- Well-ventilated workspace\n- Drop cloth\n- Work clothes, gloves, protective eyewear and face mask\n- Orbital hand sander or sanding block\n- 100-, 150- and 220-grit sandpaper\n- Soft cloth rags, paper towels and/or hand vacuum cleaner\n- Wood primer paint\n- Latex satin or high-gloss topcoat paint (enough for 2 to 3 coats)\n- Small roller\n- Water-based clear satin- or high-gloss polyurethane, optional\n- While numerous coats of surface protection seem time-consuming, this emulates commercial furniture painting, which often covers paint with a lacquer coating that resists chipping and scratching.\n- Before embarking on sanding old previously painted furniture, be aware that, according to Country LIving columnist Helen Fendelman, the highest incidence of lead-based paint on furniture is not on truly antique (18th to 19th-century) furniture but rather on pieces manufactured during the early to mid-20th century. Use a hardware-store lead-paint test kit or contact your local county Extension service for lead-safe alternatives to sanding.\n- Ventilate your work space and wear eye and breathing protection whenever sanding or refinishing wood.\n- Jupiterimages/Photos.com/Getty Images","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:bb30ccdc-9328-4131-aecb-96b6d80403e0>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"Which US prez had shorter active service - Garfield 200 days or Hayes Civil War years?","answer":"Hayes had a much longer period of active service. He served throughout the Civil War from 1861 to 1865, being officially mustered out on June 8, 1865. In contrast, Garfield's presidency lasted only 200 days before he was assassinated. Hayes even called his years of military service during the Civil War 'the best years of his life' and achieved the rank of Major General.","context":["James A. Garfield ranks among the less remarkable U.S. presidents, owing largely to the fact that he was assassinated just 200 days into his first term. For that reason, Garfield is less remembered for his policy achievements than for his personal quirks.\nThe president was both bilingual and ambidextrous. It's rumored that he entertained guests by writing in Greek with one hand and Latin with the other simultaneously. Garfield was also known for his wisdom and restraint. \"Right reason is stronger than force,\" he once said. He wasn’t the kind to lose his cool. As it happens, research suggests these characteristics are related—learning a second language can improve self-discipline. Studies show that bilingualism is associated with better executive function, a term used to describe several higher-order cognitive skills, including the ability to control our own behavior. Executive function allows us to play a game of chess or follow the plot of “Mad Men”, but it’s best understood as what allows us to make plans, get organized, and stay on task. Learning a new language can help cultivate these skills. “I speak Korean and English,” says Yang Hwajin, a professor of psychology at Singapore Management University. “When I speak English, I have to inhibit thoughts about Korean grammar and focus on English grammar, as the two languages do not share any grammatical structure. Speaking these two languages has trained me to inhibit distractions and focus better.\"\nThis skill, known as task switching, is related to executive function. To do it well, one ought to focus on one task (speaking English) and then another (speaking Korean), ideally without jumbling words or syntax between the two different languages. This entails working in one mode and then the other. Infants reared in bilingual households often see greater cognitive development. Even very young children who grow up hearing two different languages are able to sort one from the other—a difficult mental task. These benefits are apparent as children continue to grow and evolve. Students in dual-language classrooms, for example, tend to perform better in some areas in school. “What we have found in the last three decades is that bilingualism has substantial impact on cognitive function—the way that we think, make decisions, perceive things, solve decisions, and so on,” Hwajin says. It’s worth noting that some of this research is contested. Scientists have yet to reach a consensus on how bilingualism shapes cognitive development, but they generally agree that learning a new language is a workout for the brain. And this reaps measurable benefits well into old age.\nLearning a new language tones the brain by strengthening neural networks. While some parts of the brain control a specific task—the visual cortex processes sight, the auditory cortex processes sound—there is no part of the brain that specifically controls language. Speaking a language—even just one language—is such a complex task that it engages the entire brain. One must understand the meaning of thousands of words, the way they fit together, and the cultural context that offer weight and meaning. These processes must happen simultaneously and in conjunction, each a moving part in a complex machine. Learning a new language tightens the gears. Bilingual seniors tend to stay sharp as they age, are more likely to recover from a stroke, and have shown signs of dementia and Alzheimer’s four or five years later than their monolingual peers. The bilingual brain features stronger connections between disparate parts, allowing bilingual seniors to harness resiliency others may lack. These benefits are immensely valuable, and speaking additional languages can come in handy. It allows you to communicate with a wider range of people, navigate other parts of the world, and better understand different cultures. It’s useful anywhere, from the bodega to the boardroom. More than half the population is onto this. “One language sets you in a corridor for life,” says psycholinguist Frank Smith. “Two languages open every door along the way.”","Rutherford B. Hayes Month\nHayes & the Civil War\nPart VII: Conclusion\nHayes called his years of military service during the Civil War the best years of his life. This now seven-part series in honor of his birthday will visit four locations associated with Hayes’ service to the Union.\nAs Hayes remarks in his letter to his wife after Cedar Creek, fighting was generally over in the Shenandoah. And Cedar Creek was Hayes’s last battle. In January 1865, he was promoted to brigadier general. In March of 1865, Hayes was brevetted a Major General \"for gallant and distinguished service during the campaign of 1864 in West Virginia and particularly at the battles of Fisher's Hill and Cedar Creek, Virginia\". But in April 1865, Hayes was supposed to lead a raid on Lynchburg, Virginia, however Confederate General Robert E. Lee surrendered to Grant on April 9, 1965. And Lincoln was assassinated on April 14. And there was no need for the raid. The fighting of the Civil War was basically over.\nHayes was officially mustered out of service on June 8, 1865. Having been elected to the U.S. House of Representatives from Cincinnati, he prepared to go to Washington to begin the next phase of his life.\nRutherford B. Hayes, taken 1861, from the Hayes Presidential Center\nAnd thus ends Rutherford B. Hayes Month.\nHere is a summary of all of the 'Hayes & the Civil War' posts:\nPart I: Camp Chase\nPart II: Battle of South Mountain\nPart III: Battle of Buffington Island\nPart IV: Battle of Second Kernstown\nPart V: Battle of Third Winchester\nPart VI: Battle of Cedar Creek\nPart VII: Conclusion\nThere is so much more that could have been said about Hayes and his military service during the Civil War. I left much out, including many interesting details.\nFirst and foremost, I left out many battles in which Hayes fought with the 23rd Ohio Volunteer Infantry. This includes his first battle, Carnifax Ferry, West Virginia, and a number of other battle sites in present day West Virginia. A whole ‘Hayes in West Virginia’ trip may be in order. (Though ‘The Education of Rutherford B. Hayes’ is certainly on the list at some point as an excuse to visit Harvard again.)\nSo I also left out how he had been appointed Colonel of the 79th Ohio Infantry Volunteers in 1862, but he never assumed that position and stayed with the 23rd Ohio once appointed their Colonel.\n23rd Ohio Volunteer Infantry Regimental Flag from the Ohio Historical Society Exhibits\nBut I also left out a bunch of little details . . .\n- Like I simply find it fascinating that his wife Lucy and the children would come visit him while the 23rd was in camp, mostly in West Virginia. A man at Cedar Creek indicated that for ranking officers it was easier for family to come to the camp instead of leaving so regimental business could continue.\n- Hayes and Lucy even conceived a child, George, while the war was going on. (He would died in 1866.) That child was named after George Crook his commander. And one child, Joseph, died while visiting Camp White near Charleston with his mother. (Joseph & George are buried in Spring Grove. Check out this post from last year.)\n- Another interesting side-notion was it never really donned on me that fighting sort of took a break in wintertime.\n- And I simply find it fascinating that Hayes would have been 42 by the time fighting had ended in 1865. Yes, 42! President William McKinley for instance was 18 when he enlisted and 22 when the war ended.\nRaise the Rutherford! is a continuing, slightly humorous series to raise awareness of Rutherford B. Hayes and erect a statue of him in Cincinnati.\nThe Rutherford B. Hayes Presidential Center\nIncluding the Dairy and Letters of Rutherford B. Hayes\nFight for the Colors, The Ohio Battle Flag Collection. Ohio Historical Society Exhibits"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:eede8854-4ded-49e3-b370-0947a4b246f8>","<urn:uuid:f8bf434c-758e-4f66-945e-699cfd875f9e>"],"error":null}
{"question":"What was the purpose of European Union's support program for Eswatini in 2001, and how does it help local businesses?","answer":"The EU is supporting Eswatini through the 'Support Programme to the EU-SADC Economic Partnership Agreement (EPA) in the Kingdom of Eswatini'. This initiative specifically supports trade and export development to create EPA opportunities and supports trade and investment promotion bodies and small businesses. As part of this program, the EU is working with the International Trade Centre to provide small businesses and private-sector associations in Eswatini access to trade analysis tools and capacity training.","context":["Providing better trade and export development for Eswatini\nThe European Union partners with the International Trade Centre, a joint agency of the United Nations and the World Trade Organization to support the Government of the Kingdom of Eswatini in export market analysis.\n(Kingdom of Eswatini) The International Trade Centre’s market analysis tool Trade Map will support export market access and drive sustainable growth in the Kingdom of Eswatini.\nTrade Map is an online database of international trade statistics that has been providing useful indicators on export performance, international demand, alternative markets, and the role of competitors from different global markets since 2001.\nIt covers the annual trade flows of over 220 countries and over 5,300 products. It also offers trade indicators such as values, trends, quantities, trends, market share as well as monthly and quarterly data from both developed and developing countries.\nTo give small businesses and private-sector associations in Eswatini access to this tool, the International Trade Centre (ITC) is working closely together with the European Union (EU) and the Government.\nThe EU is supporting the initiative under its project ‘Support Programme to the EU-SADC Economic Partnership Agreement (EPA) in the Kingdom of Eswatini’. The project specifically supports trade and export development to create EPA opportunities and support trade and investment promotion bodies and small businesses.\nThe EU and ITC are currently running capacity trainings to equip business support institutions with knowledge and tools for analytical outputs and the EU-SADC-EPA as well as other international trade agreements. Participants were drawn from the Eswatini Investment Promotion Authority (EIPA) and the National Agricultural Marketing Board (NAMBoard), Eswatini Water and Agricultural Development Enterprise (ESWADE), Eswatini Economic Policy Analysis and Research Centre (ESEPARC), Eswatini Leather Association, COMESA Federation of Women in Business (COMFWB) Eswatini chapter, and Eswatini Multipurpose Cooperative Union (ESWAMCU).\n‘This tool will allow us to share information with the business community and puts us in a better position to advise on available market opportunities. It also offers an analysis of trends in terms of products that are doing well or that are being discontinued in global markets. Best of all, it’s readily available to everyone.’\nBongani Ntshangase, Executive Head of Trade, Eswatini Investment Promotion Authority\n‘We see this free-market access tool as a contribution to driving growth. It will help further facilitate business and trade relationships across continents and help micro, small, and medium-sized enterprises to increase their visibility and market opportunities.’\nLuis Miguel Pascoal, EU Programme Officer\nAbout the International Trade Centre - The International Trade Centre is the joint agency of the World Trade Organization and the United Nations. ITC assists small and medium-sized enterprises in developing and transition economies to become more competitive in global markets, thereby contributing to sustainable economic development within the frameworks of the Aid-for-Trade agenda and the United Nations’ Sustainable Development Goals.\nFor more information, visit www.intracen.org.\nFollow ITC on Twitter | Facebook | LinkedIn | Instagram | Flickr\nSenior Strategic Communications Officer\nInternational Trade Centre\nE: pak [at] intracen.org\nT: +41 22 730 0651\nM: +41 79 667 4660"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:9e40f11d-58ff-4f67-b5fc-310d4f31f4d4>"],"error":null}
{"question":"What are the key differences between planning lighting for patios versus planning the patio space itself?","answer":"When planning a patio space, you should calculate the desired size and then add an extra meter to each side for generous proportions. For patio lighting, the approach is more nuanced - it requires multiple medium-voltage small lights rather than a single floodlight to create a warm, lively feeling. Patios benefit from several lighting fixtures around the perimeter, and bulbs should not exceed 100 watts to avoid creating a 'black hole' effect or disturbing neighbors. Additionally, while patio planning focuses on physical space allocation, lighting planning must consider both safety aspects and aesthetic effects.","context":["Redesigning Your Garden\nMaking a Plan\nBefore you begin, check with your local planning office to see whether your garden is covered by any planning regulations. This may be the case if, for instance, you live in a conservation area. Mature trees may be protected by a Tree Preservation Order. If your house is a listed building or there is one nearby, you may also encounter restrictions on layout and height in the garden. If you need to apply for permission for any changes, apply as soon as possible, as sometimes the process can be slow.\nWhat is on your garden wish list?\nThink about what you want your garden to do. It should reflect your lifestyle, the people who will be enjoying it and the time you plan to spend in it, whether it is relaxing, playing or tending the plants.\nIf you have small children and they enjoying playing ball games, a large lawn is a good choice, space allowing. Fill borders with robust evergreen shrubs and grasses rather than delicate flowers.\nIf you enjoy entertaining in your garden, lay a patio surrounded by fragrant, low-maintenance borders.\n- It’s best to plan your project on paper first. You can always refine your ideas by developing several versions, but date them so you know which one is most recent.\n- Map out the existing garden. Make sure you include everything in it. A plan made to a scale of 1cm:1m (1:100) is a workable scale for most gardens. If you have a small garden, you may be able to use a scale of 1cm:0.5m (1:50).\n- Identify your boundaries. Mark whether they are fences, walls or hedging. Make a note of hedging, fence or wall style too and for hedging show the spread of foliage. Boundaries can look almost insignificant on a plan, but they can be a strong visual presence within the garden, completely changing the intended design.\n- Mark out the house. Include all major structures and any areas of hard landscaping. Then draw in details like paths, sheds, compost bins, pergolas, and water features.\n- Note the existing borders. Draw in flower beds, herb or vegetable gardens, and other large areas of planting.\n- Mark the orientation of the space. Note down North and mark areas where shade is cast at different times of day.\n- Remember utilities. Mark the position of any utilities, including water, sewage and electricity. It may be useful to show underground services and overhead services using different colours. The location from underground services can often be identified using the position of inspection hatches and above ground piping.\nUsing a Professional\nYou might decide to employ a professional designer. They can bring a more polished finish to the final design and may have creative, practical ideas on how to make the most of your space. If you do hire a professional, be very clear what you are looking for in the brief and ask for a written contract if one is not provided. Most problems with professional design work arise from the designer misunderstanding what the client wants, or the client changing their mind about what they want half way through the process. A written agreement can help to avoid this problem.\nChoose a qualified professional who is willing to spend time in discussion with you at the planning stage and who will take into account your wishes and your needs. But remember that they are the expert and it makes sense to listen to their advice.\nA Full-Size Plan\nOnce you have created your plan on paper, the best way to test how well it will work in practice is to mark it out on the ground and live with it for a while.\nThe simplest method is to use a hosepipe, length of rope or brightly-coloured spray paint to mark out the shape of the borders, lawns, and other large areas. Use stakes and twine to represent the exact shape and height of the hedges.\nOutline the paths with stakes and heavy string that is clearly visible – brightly-coloured string or twine is idea.\nMake a Note of Which Direction Your Garden Faces\nIf you understand the orientation of your garden, you will be much better able to make the most of your space. You will also be able to identify the best position for a patio, garden pond and other important features. Selecting plants to suit your space is also important – some plants love full sunshine, others prefer semi-shade and still others thrive in cooler, evening sun. Take into account shadows thrown by hedges and large trees when planting, as they can be just as solid as the shadows created by buildings.\nAdd Lighting to Your Plan\nStart thinking about where to position garden lighting in the early stages of planning. Too often, lighting is little more than after-thought, added after everything else has been positioned. By including lighting in your plan from the beginning, you willbe able to optimise its use within the garden.\nA light at each entrance, and several positioned along pathways and on the patio, will aid visibility – and add extra life and interest – late into summer evenings and on dull winter afternoons. It’s a good idea to illuminate any steps, raised areas and doors leading into the house.\nPlan to have a number of lighted areas, not just one. It is much more effective to have several medium-voltage small lights rather than a single floodlight: the lights will not dazzle and, at night, your garden will feel lively and warm. A bulb brighter than 100 watts will accentuate the ‘black hole’ of unlit space behind and may also annoy your neighbours!\nConsider the View\nDon’t forget to look beyond your garden to its immediate surroundings. There may be something you would like to hide from view – electricity pylons, telegraph poles, unattractive buildings – while optimising the nicest views.\nWork Out the Size of a Terrace or Patio\nCalculate the size you think you need – and then add another 1 metre to each side. Size is difficult to judge exactly, so be as generous as you can when planning.\nPlanning for a Water Feature\nThink carefully about the kind of water feature that is fitting for your style of garden.\n- You may consider a raised or sunken pond, a waterfall, a natural swimming pool, or a small bubble feature, depending on the space you have available and your priorities for the garden.\n- The site itself will have an impact on your choice: a running stream or waterfall is more natural on a sloping site, whereas a pond needs a level area where the ground is easy to excavate. If you have rock close to the surface or a high water table, a sunken pond will be impractical.\n- Remember to include electrical provision for a pump or lighting, if needed.\nNext: see Making the Most of Your Space","Landscape Lighting Design Guid\nA little knowledge and planning will help you attain landscape lighting that looks like it was done by a professional. Landscape lighting should include pathways, decks, patios, doorways, drives and other beautifully enhanced features to create a visible picturesque scene of your home. If you are rather unfamiliar with this topic, you may not feel too comfortable with handling the job yourself. You may even feel the need to hire a professional landscape lighting designer. To help you overcome this obstacle, this landscape lighting design guide will offer up some basic information and tips you can apply. Once you are finished reading it, you can shop our online catalog of outdoor light fixtures and buy from us with confidence.\nBenefits of Landscape Lighting\n- Enhances and reveals the beauty of your home and landscape after dusk.\n- Extends the amount of time spent together with loved ones while enjoying outdoor activities, relaxing, breathing fresh air and smelling the roses.\n- Adds dramatic lighting effects to a landscape.\n- Creates a heightened artistic scenic view with high and low points of interest.\n- Increases security and wards off intruders looking for an easy target.\n- Minimizes accidents by lighting up stairs, paths, drives and other traffic areas.\n- Polished and beautifully created landscape lighting escalates the financial value of a home.\nDesigning and Planning your Landscape Lighting\n- Don't over light your yard like a stadium with exterior lighting.\n- Create high and low areas of interest with high and low intensity lighting variations. Overall there should be more low focal points of light than high.\n- Incorporate different types of lighting fixtures into your landscape lighting as they will have different lighting effects and your landscape will be more interesting to look at.\n- Decorative outdoor lights have artistic flair and are designed to bejewel and adorn a landscape and home. Ideally decorative lighting fixtures should be placed more prominently. Other types of outdoor lighting, such as low voltage well lights and spot lights, are best concealed as their purpose is to create interesting lighting effects and the light fixture itself is not designed to draw attention.\n- Don't aim lights directly at windows, including your neighbor's.\n- Place decorative lights in and around pathways, doorways and other activity areas.\n- Pay extra special attention to the front entrance of your home, the welcoming place, which should be one of your high focal points of interest. The type of landscape lighting design you employ here is very important.\nCommon Landscape Lighting Techniques\n- Up Lighting - Light is aimed upwards for dramatic effect. Used commonly to light up a tree, sculpture, or walls. Lighting from below or upfront creates interesting shadows against surfaces behind the lighted objects.\n- Down Lighting - Casts light down and may create interesting effects by way of shadowing. Especially useful in lighting up very dark areas to heighten security and safety.\n- Moonlighting - Simulates moonlight by positioning light fixtures very high above trees and larger plants or objects to create enchanting shadow effects.\n- Accent Lighting - An intense beam, or spotlight, creates high focal points in your landscape. When the light is aimed at the leaves of shrubs, plants or flowers it can create a fantastic glittering effect.\n- Grazing - Placing the light fixture close to a surface to achieve the effect of the light traveling and enhancing the lighted area. Grazing is done against textured walls or beautiful wood. It's also used to highlight a very rough texture of a wider tree.\n- Backlighting - Silhouettes a sculpture, tree or plants.\n- Cross Lighting - Enhances a three-dimensional view of a voluminous plant, tree or object.\nTypes of Lighting Fixtures\nAlways choose quality lighting fixtures and you will never be disappointed. Quality lighting fixtures are time tested for durability and reliability. Their timeless designs and illumination will create a luxurious landscape.\n- Outdoor Lanterns and Wall Lights and Sconces – An excellent choice for doorways, garages, and windows. Used to enhance walled areas or square pillars.\n- Outdoor Ceiling Fixtures – Perfect for porches, patios, breezeways, and covered areas or walkways.\n- Post Lights – Majestically light up ponds, lakes, pools of water, main driveways and walkways. Create a picturesque scene by placing in a garden near outdoor benches. Some cast interesting light patterns.\n- Diffusers and Spreads – Low voltage lighting fixtures that give off a softer light and are used to outline borders.\n- Cylinders, Square and Bullet Shaped Lights – These are designed to focus a beam of light.\n- Well Lights – Hidden from view, they flush with the ground. Mostly used for up lighting on plants, trees and walls. On highly textured surfaces they give interesting shadow effects.\n- Accent Lighting– Very versatile fixtures that are used for up lighting, moonlighting, accent lighting, grazing, and down lighting.\nTypes of Energy Efficient Landscape Lighting\nAlways choose quality outdoor lighting fixtures and you will never be disappointed. Quality lighting fixtures are time-tested for durability and reliability. Their timeless designs and illumination will create a luxurious landscape lighting design that you can be proud of.\n- Outdoor Lanterns and Wall Lights and Sconces - These exterior light fixtures are an excellent choice for doorways, garages, and windows. Used to enhance walled areas or square pillars.\n- Outdoor Ceiling Fixtures - Perfect for porches, patios, breezeways, and covered areas or walkways.\n- Post Lights - Majestically light up ponds, lakes, pools of water, main driveways and walkways. Create a picturesque scene by placing in a garden near outdoor benches. Some cast interesting light patterns.\n- Diffusers and Spreads - Low voltage lighting fixtures that give off a softer light and are used to outline borders.\n- Cylinders, Square and Bullet Shaped Lights - These are designed to focus a beam of light.\n- Well Lights - Hidden from view, they are flush with the ground. Mostly used for up lighting on plants, trees and walls. On highly textured surfaces they give interesting shadow effects.\n- Accent Lighting- Very versatile fixtures that are used for up lighting, moonlighting, accent lighting, grazing, and down lighting.\nLandscape Lighting Easy Installation\nLandscape lighting is easy to install for the do it yourselfers. There are two types of voltage for landscape lighting: 12 volt and 120 volt. Landscape lighting fixtures that are 120v can be wired or plugged into your home’s existing electrical supply. Outdoor fluorescent lighting fixtures come equipped with a ballast which limits the current to its proper value and thus can be wired directly to your home wiring. Twelve volt (12-volt), low voltage landscape lighting requires an outdoor lighting transformer to step down the volts. Low voltage lighting is easy and safe to install and you do not need to be an electrician. Most landscape lighting designs include both line and low voltage landscape lighting.\nOutdoor Lighting Installation\nOutdoor lights that are 120 volts do not require a landscape lighting transformer and can be directly plugged into an outdoor electrical outlet. Low voltage landscape lighting and LED requires the use of an outdoor lighting transformer to step down the current from a house of 120 volts to a safe 12 volts. Do not hook up a 12 volt lighting fixture directly into an outside electrical unit, as homes are typically equipped with 120 volts.\n- If you require a lighting transformer, calculate the total amount of wattage by summing up the watts found on the bulbs that will be used for each light fixture. Purchase an outdoor lighting transformer that supports the summed wattage of the landscape lighting plus add an additional watt for every 10 feet of power cord. We recommend you buy a lighting transformer that supports more wattage to anticipate future changes and additions. However, for optimal performance use at least half of the transformer’s wattage rating.\n- Plug the transformer into a GFCI-protected outdoor electrical outlet that has a protective plastic box covering the power cord. For safe and convenient operation, the transformer should be mounted at least 1.5’ above the ground.\n- Lay out where the cord will run and allow extra cord to give you flexibility in moving the wires around. A #12-2 cable is recommended for low voltage landscape lights. It is recommended that the cable runs do not exceed 100 feet. If you have a very large area you may install more than one transformer and run cable between them or separate from each other.\n- It is not recommended that you put more than 100 watts on one line. If the wattage exceeds 100 watts make a t-connection. You may mix wattages as long as they don’t exceed the wattage of the light transformer. Read about the types of low voltage cable layouts below.\n- Connect the light fixtures to the cable. Be sure not to place light fixtures where they may be easily damaged. Use a Direct Burial (DBr) splice connector to connect the light fixtures to the cable. They are the highest quality. Direct Burial Connections are tightly sealed and prevent moisture invasion, minimizing future repairs and corrosion. All low voltage connections must be waterproof and tightly sealed.\n- Test your voltage and spacing and direction of lights. Stand at different angles and distances to be sure that you have properly spaced and placed your lights.\n- Between the light fixtures, where the cord will run dig a trench three to six inches deep and bury the cord. You may encase the cables in PVC pipe to minimize accidental damage.\n- Seal any loose ends with electrical tape.\nLow Voltage Cable Layouts\nLow Voltage lighting cable is available in #12-2, #10-2, and #8-2, the lower the number the thicker the cable. A thicker cable will reduce the amount of voltage drop.\n- Straight Installation – The lighting fixtures are sequentially run in one line from the transformer with only one end connected to the transformer. This is the easiest to install and requires the least amount of effort.\n- Star or Split Load Installation – More than one cord is run directly from the transformer, and run in different directions, hence a \"star\" formation which lends to the name.\n- Loop Installation – Minimizes the drop of voltage and outputs more uniform light among the light fixtures. Connect both wire ends to the transformer. Low voltage polarity is maintained by connecting the same wire leads to the proper transformer terminals. Mark on one side of the cable to note the ridge and help you make the proper connections.\n- T Installation – Two cords are run from a heavier gauge cord, (use either #8 or #10 gauge), that is directly run from the transformer and forms a “T”."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:fbaf5dad-6917-4528-88e4-36e1eea4bcdb>","<urn:uuid:7fc4907d-f5c0-4c26-a43c-a28a0ba9c0c2>"],"error":null}
{"question":"Being an international student, I need to know how does TOEFL assess academic writing skills compared to what's required in a dissertation's methodology chapter?","answer":"TOEFL writing assessment focuses on task completion, development, and language accuracy within short timed tasks, with scores reported on a scale of 5 and converted to a 30-point scale. In contrast, a dissertation's methodology chapter requires much more detailed academic writing, where you must thoroughly describe your research approach, data collection methods, analysis tools, and justify your methods. The methodology chapter needs to allow readers to assess the validity of your research, which is a much more complex and comprehensive form of academic writing than TOEFL's time-constrained tasks.","context":["The TOEFL® Test\nTOEFLiBT is an internet based test designed to assess the language ability of candidates who want to study or work where English is the language of communication.\nThe maximum score a candidate can get is 120. Each institution that uses TOEFL scores sets its own minimum level of acceptable performance. The standards vary from one institution to another depending on factors such as field of study or level of study. Test takers can view their scores online within 15 business days after the test.TOEFL scores have a recommended two year validity period.\nThe Main Features\n- It tests all four language skills that are important for effective communication: speaking, listening, reading and writing.\n- It is delivered via the Internet in secure test centres throughout the year.\n- Some tasks require test takers to combine more than one skill.\n- Read, listen and then respond to a question\n- Listen and then speak in response to a question\n- Read, listen and then write in response to a question.\nThe test is 4 hours long.\nThe TOEFL Scaled Score is as follows\n- Reading 0-30\n- Listening 0-30\n- Speaking 0-30\n- Writing 0-30\nTotal Score 0-120\nTiming: Approximately 60-100 min\nTest Material: There are 3-5 passages, 12-14 questions per passage. The passages are about 700 words long.\nQuestions: A variety of multiple choice questions types is used, chosen from the following: Factual Information questions, Inference questions, vocabulary questions, reference questions, sentence paraphrasing questions, insert text question, prose summary and fill in the table. A variety of voices and native-speaker accents is used. The text and the questions appear on the screen (side by side)\nEach correct answer carries one mark. The last question (which could be a summary question or a fill in the table question) could be for up to 4 marks.\nRaw Scores on 39 are converted into scaled score on 30.\nTiming: 60 – 90 min\nTest Material: There are 2-3 lectures (6 questions each), 2-3 discussions (6questions each) and 2-3 conversations (5 questions each) There are three types of Listening materials, conversations, discussions and lectures. Each are 4-6 min long. You should take notes during the listening as the questions will appear after the listening is over.\nQuestions: A variety of multiple choice questions types is used, chosen from the following: Gist- Content questions, Gist-Purpose questions, Detail, Understanding the function of what is said, understanding the Speaker’s attitude, understanding organization, connecting content, making inference. Most questions have one right answer. However, some may require you to choose two answer choices out of the four, or some may ask you to put order events or steps.\nMarking: Each correct answer carries one mark. The last question (which could be a summary question or a fill in the table question) could be for up to 4 marks.\nRaw Scores on 34 are converted into scaled score on 30.\nNOTE! The official TOEFL iBT tests have either a longer Reading Section i.e. 5 passages instead of 3 or a longer listening section i.e. 2-3 extra listening material (lecture / conversation). The extra part on each test contains experimental questions that will not be graded as part of your score. You will need to do your best on all of the questions because you will not know which questions are experimental.\nTiming: 20 minutes\nTasks: There are 6 tasks, 2 independent and 4 integrated. You will be asked to speak on a variety of topics that draw on personal experience, campus based situations, and academic- type content material. The stimuli are presented online and the responses are recorded online.\nTask 1 and Task 2 are independent tasks because they require you to draw entirely on your own ideas, opinions, and experiences when responding. You are asked for opinions on certain matters. Preparation time is 15 seconds and Recording time is 45 seconds.\nTask 3, 4, 5, 6 are integrated tasks. You will listen to a conversation or excerpts from lectures, or read a passage and then listen to a brief discussion or lecture excerpt, before you are asked the question. These questions are called integrated tasks because they require that you integrate your English-language skills – listening and speaking, or listening, reading and speaking. In responding to these questions you will be asked to base your spoken response on the listening material or on both the listening and the reading passage together. Preparation time is 20-30 seconds – Recording time – 60 seconds.\n- Task accomplishment\n- Language Use\n- Topic Development\nThe Speaking tasks are scored on a scale of 0-4 and then the raw score is converted into a scaled score.\nTiming: 50 minutes\nTasks: Task 1 – Integrated Writing and Task 2 Independent Writing.\nTask1: Integrated Writing Task (20 min)\nYou will read a passage about an academic topic for three minutes and then you will hear a lecture related to the topic. Then you will be asked to summarize the points in the listening material and explain how they relate to the specific points in the reading passage.\nYou will have 20 min to plan and type your response. You will be judged on the basis of the quality of your writing and on how well your response presents the points in the lecture and their relationship to the reading passage.\nTypically, an effective response is 150-225 words.\nTask 2: Independent Writing Task (30 min)\nYou are asked to give your opinion on an issue. You are required to state your opinion and provide supporting arguments for it.\nAn effective response is typically 300 words. There is no strict limit on the number of words. You may wish to write as much as you wish in the time allotted, but quality is generally preferred to quantity.\nCandidates are assessed on their performance according to four criteria:\n- Task accomplishment\n- Development – clear explanations, exemplifications and details.\n- Language accuracy\nScores are reported on a scale of 5 as a raw score and then converted into a scaled score on 30.","How to structure a dissertation\nA dissertation or thesis is a long piece of academic writing based on original research, submitted as part of a doctoral, master’s, or bachelor’s degree.\nYour dissertation is probably the longest piece of writing you’ve ever done, and it can be intimidating to know where to start. This article helps you work out exactly what you should include and where to include it.\nTable of contents\nDeciding on your dissertation’s structure\nNot all dissertations are structured exactly the same – the form your research takes will depend on your location, discipline, topic and approach.\nFor example, dissertations in the humanities are often structured more like a long essay, building an overall argument to support a central thesis, with chapters organized around different themes or case studies.\nBut if you’re doing empirical research in the sciences or social sciences, your dissertation should generally contain all of the following elements. In many cases, each will be a separate chapter, but sometimes you might combine them. For example, in certain kinds of qualitative social science, the results and discussion will be woven together rather than separated.\nThe order of sections can also vary between fields and countries. For example, some universities advise that the conclusion should always come before the discussion.\nIf in doubt about how your thesis or dissertation should be structured, always check your department’s guidelines and consult with your supervisor.\nThe very first page of your document contains your dissertation’s title, your name, department, institution, degree program, and submission date. Sometimes it also includes your student number, your supervisor’s name, and the university’s logo. Many programs have strict requirements for formatting the dissertation title page.\nThe acknowledgements section is usually optional, and gives space for you to thank everyone who helped you in writing your dissertation. This might include your supervisors, participants in your research, and friends or family who supported you.\nThe abstract is a short summary of your dissertation, usually about 150-300 words long. You should write it at the very end, when you’ve completed the rest of the dissertation. In the abstract, make sure to:\n- State the main topic and aims of your research\n- Describe the methods you used\n- Summarize the main results\n- State your conclusions\nAlthough the abstract is very short, it’s the first part (and sometimes the only part) of your dissertation that people will read, so it’s important that you get it right. If you’re struggling to write a strong abstract, read our guide on how to write an abstract.\nTable of Contents\nIn the table of contents, list all of your chapters and subheadings and their page numbers. The dissertation contents page gives the reader an overview of your structure and helps easily navigate the document.\nAll parts of your dissertation should be included in the table of contents, including the appendices. You can generate a table of contents automatically in Word if you used heading styles.\nList of Figures and Tables\nIf you have used a lot of tables and figures in your dissertation, you should itemize them in a numbered list. You can automatically generate this list using the Insert Caption feature in Word.\nList of Abbreviations\nIf you have used a lot of abbreviations in your dissertation, you can include them in an alphabetized list of abbreviations so that the reader can easily look up their meanings.\nIf you have used a lot of highly specialized terms that will not be familiar to your reader, it might be a good idea to include a glossary. List the terms alphabetically and explain each term with a brief description or definition.\nIn the introduction, you set up your dissertation’s topic, purpose, and relevance, and tell the reader what to expect in the rest of the dissertation. The introduction should:\n- Establish your research topic, giving necessary background information to contextualize your work\n- Narrow down the focus and define the scope of the research\n- Discuss the state of existing research on the topic, showing your work’s relevance to a broader problem or debate\n- Clearly state your research questions and objectives\n- Give an overview of your dissertation’s structure\nEverything in the introduction should be clear, engaging, and relevant to your research. By the end, the reader should understand the what, why and how of your research. If you need more help, read our guide on how to write a dissertation introduction.\nLiterature review / Theoretical framework\nBefore you start on your research, you should have conducted a literature review to gain a thorough understanding of the academic work that already exists on your topic. This means:\n- Collecting sources (e.g. books and journal articles) and selecting the most relevant ones\n- Critically evaluating and analyzing each source\n- Drawing connections between them (e.g. themes, patterns, conflicts, gaps) to make an overall point\nIn the dissertation literature review chapter or section, you shouldn’t just summarize existing studies, but develop a coherent structure and argument that leads to a clear basis or justification for your own research. For example, it might aim to show how your research:\n- Addresses a gap in the literature\n- Takes a new theoretical or methodological approach to the topic\n- Proposes a solution to an unresolved problem\n- Advances a theoretical debate\n- Builds on and strengthens existing knowledge with new data\nThe literature review often becomes the basis for a theoretical framework, in which you define and analyze the key theories, concepts and models that frame your research. In this section you can answer descriptive research questions about the relationship between concepts or variables.\nThe methodology chapter or section describes how you conducted your research, allowing your reader to assess its validity. You should generally include:\n- The overall approach and type of research (e.g. qualitative, quantitative, experimental, ethnographic)\n- Your methods of collecting data (e.g. interviews, surveys, archives)\n- Details of where, when, and with whom the research took place\n- Your methods of analyzing data (e.g. statistical analysis, discourse analysis)\n- Tools and materials you used (e.g. computer programs, lab equipment)\n- A discussion of any obstacles you faced in conducting the research and how you overcame them\n- An evaluation or justification of your methods\nYour aim in the methodology is to accurately report what you did, as well as convincing the reader that this was the best approach to answering your research questions or objectives.\nNext, you report the results of your research. You can structure this section around sub-questions, hypotheses, or topics. Only report results that are relevant to your objectives and research questions. In some disciplines, the results section is strictly separated from the discussion, while in others the two are combined.\nFor example, for qualitative methods like in-depth interviews, the presentation of the data will often be woven together with discussion and analysis, while in quantitative and experimental research, the results should be presented separately before you discuss their meaning. If you’re unsure, consult with your supervisor and look at sample dissertations to find out the best structure for your research.\nIn the results section it can often be helpful to include tables, graphs and charts. Think carefully about how best to present your data, and don’t include tables or figures that just repeat what you have written – they should provide extra information or usefully visualize the results in a way that adds value to your text.\nFull versions of your data (such as interview transcripts) can be included as an appendix.\nThe discussion is where you explore the meaning and implications of your results in relation to your research questions. Here you should interpret the results in detail, discussing whether they met your expectations and how well they fit with the framework that you built in earlier chapters. If any of the results were unexpected, offer explanations for why this might be. It’s a good idea to consider alternative interpretations of your data and discuss any limitations that might have influenced the results.\nThe discussion should reference other scholarly work to show how your results fit with existing knowledge. You can also make recommendations for future research or practical action.\nThe dissertation conclusion should concisely answer the main research question, leaving the reader with a clear understanding of your central argument.\nIn some academic conventions, the conclusion refers to a short section that comes before the discussion: first you directly state your overall conclusions, then you discuss and interpret their meaning.\nIn other contexts, however, the conclusion refers to the final chapter, where you wrap up your dissertation with a final reflection on what you did and how you did it. This type of conclusion often also includes recommendations for research or practice.\nIn this section, it’s important to show how your findings contribute to knowledge in the field and why your research matters. What have you added to what was already known?\nYou must include full details of all sources that you have cited in a reference list (sometimes also called a works cited list or bibliography). It’s important to follow a consistent citation style. Each style has strict and specific requirements for how to format your sources in the reference list.\nTo save time creating the reference list and make sure your citations are correctly and consistently formatted, you can use our free APA Citation Generator.\nYour dissertation itself should contain only essential information that directly contributes to answering your research question. Documents you have used that do not fit into the main body of your dissertation (such as interview transcripts, survey questions or tables with full figures) can be added as appendices.\nEditing and proofreading\nMaking sure all the sections are in the right place is only the first step to a well-written dissertation. Leave plenty of time for editing and proofreading. Grammar mistakes and sloppy formatting errors can drag down the quality of your hard work.\nYou should plan to write and revise several drafts of your thesis or dissertation before focusing on language mistakes, typos and inconsistencies."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:fda938d7-7aaf-447a-940f-134a1c23dde0>","<urn:uuid:c4faa034-f0c6-4e1b-9c7e-5a358cdcb4d4>"],"error":null}
{"question":"Between modern bodybuilding competitions and early physical culture events, how has the recognition of bodybuilding achievement evolved?","answer":"The recognition of bodybuilding achievement has evolved significantly from early physical culture events to modern practices. In 1904, the first large-scale American bodybuilding competition at Madison Square Garden offered a $1,000 prize to Al Treloar for being declared 'The Most Perfectly Developed Man in the World.' This was primarily based on physical appearance. Today, the recognition is more scientifically grounded, with understanding of specific mechanisms like mechanical tension, metabolic stress, and muscle damage that contribute to muscle development. Modern bodybuilding focuses on specific training protocols and repetition ranges to target different aspects of muscle growth, representing a more sophisticated approach to the sport.","context":["On January 16, 1904, the first large-scale bodybuilding competition in America took place at Madison Square Garden in New York City. The competition was promoted by Bernarr Macfadden, the father of physical culture and publisher of original bodybuilding magazines such as Health & Strength. The winner was Al Treloar, who was declared \"The Most Perfectly Developed Man in the World\". Treloar won a $1,000 cash prize, a substantial sum at that time. Two weeks later, Thomas Edison made a film of Treloar's posing routine. Edison had also made two films of Sandow a few years before. Those were the first three motion pictures featuring a bodybuilder. In the early 20th century, Macfadden and Charles Atlas continued to promote bodybuilding across the world. Alois P. Swoboda was an early pioneer in America.\nWhat started as an alternative to standard-issue military conditioning quickly grew into a life-changing career as Anthony \"Flama Blanca\" Fuhrman discovered his knack for lifting heavy and moving fast could catapult him to the top of his sport. Find out how this world-class Strongman and Titan Games competitor uses pop music and a larger-than-life persona to conquer the toughest lifts in competition. July 02, 2019 • 42 min read\nThe earliest extant systematic account of yoga and a bridge from the earlier Vedic uses of the term is found in the Hindu Katha Upanisad (Ku), a scripture dating from about the third century BCE[…] [I]t describes the hierarchy of mind-body constituents—the senses, mind, intellect, etc.—that comprise the foundational categories of Sāmkhya philosophy, whose metaphysical system grounds the yoga of the Yogasutras, Bhagavad Gita, and other texts and schools (Ku3.10–11; 6.7–8).\nAccording to Zimmer, Yoga philosophy is reckoned to be part of the non-Vedic system, which also includes the Samkhya school of Hindu philosophy, Jainism and Buddhism: \"[Jainism] does not derive from Brahman-Aryan sources, but reflects the cosmology and anthropology of a much older pre-Aryan upper class of northeastern India [Bihar] – being rooted in the same subsoil of archaic metaphysical speculation as Yoga, Sankhya, and Buddhism, the other non-Vedic Indian systems.\"[note 6]\nWeight training aims to build muscle by prompting two different types of hypertrophy: sarcoplasmic and myofibrillar. Sarcoplasmic hypertrophy leads to larger muscles and so is favored by bodybuilders more than myofibrillar hypertrophy, which builds athletic strength. Sarcoplasmic hypertrophy is triggered by increasing repetitions, whereas myofibrillar hypertrophy is triggered by lifting heavier weight. In either case, there is an increase in both size and strength of the muscles (compared to what happens if that same individual does not lift weights at all), however, the emphasis is different.\nMuscle growth is more difficult to achieve in older adults than younger adults because of biological aging, which leads to many metabolic changes detrimental to muscle growth; for instance, by diminishing growth hormone and testosterone levels. Some recent clinical studies have shown that low-dose HGH treatment for adults with HGH deficiency changes the body composition by increasing muscle mass, decreasing fat mass, increasing bone density and muscle strength, improves cardiovascular parameters, and affects the quality of life without significant side effects.[unreliable medical source?]\nAn influential text which teaches yoga from an Advaita perspective of nondualistic idealism is the Yoga-Vāsiṣṭha. This work uses numerous short stories and anecdotes to illustrate its main ideas. It teaches seven stages or bhumis of yogic practice. It was a major reference for medieval Advaita Vedanta yoga scholars and before the 12th century, it was one of the most popular texts on Hindu yoga.\nVajrayana is also known as Tantric Buddhism and Tantrayāna. Its texts were compiled starting with 7th century and Tibetan translations were completed in 8th century CE. These tantra yoga texts were the main source of Buddhist knowledge that was imported into Tibet. They were later translated into Chinese and other Asian languages, helping spread ideas of Tantric Buddhism. The Buddhist text Hevajra Tantra and Caryāgiti introduced hierarchies of chakras. Yoga is a significant practice in Tantric Buddhism.\n^ Andrew J. Nicholson (2013). Unifying Hinduism: Philosophy and Identity in Indian Intellectual History. Columbia University Press. p. 26. ISBN 978-0-231-14987-7., Quote: \"From a historical perspective, the Brahmasutras are best understood as a group of sutras composed by multiple authors over the course of hundreds of years, most likely composed in its current form between 400 and 450 BCE.\"\n^ Mangano, Kelsey M.; Sahni, Shivani; Kiel, Douglas P.; Tucker, Katherine L.; Dufour, Alyssa B.; Hannan, Marian T. (February 8, 2017). \"Dietary protein is associated with musculoskeletal health independently of dietary pattern: the Framingham Third Generation Study\". The American Journal of Clinical Nutrition. 105 (3): 714–722. doi:10.3945/ajcn.116.136762. PMC 5320406. PMID 28179224 – via ajcn.nutrition.org.\nThe origins of yoga have been speculated to date back to pre-Vedic Indian traditions; it is mentioned in the Rigveda,[note 1] but most likely developed around the sixth and fifth centuries BCE, in ancient India's ascetic and śramaṇa movements.[note 2] The chronology of earliest texts describing yoga-practices is unclear, varyingly credited to Upanishads. The Yoga Sutras of Patanjali date from the first half of the 1st millennium CE, but only gained prominence in the West in the 20th century. Hatha yoga texts emerged sometimes between the 9th and 11th century with origins in tantra.","YOUR FITNESS BLOG\nWhat Is The Science Behind Muscle Building?\nMuscle building is an objective shared by both male and females that engage in resistance exercise (i.e. free weights, cable pulleys, resistance machines). It is well accepted that the process of muscle growth involves the signalling of anabolic pathways that promote protein synthesis (building of new muscle protein) and prevent muscle breakdown .\nCurrent research indicates that three primary mechanisms are involved in the muscle building process [5, 7]. These mechanisms include mechanical tension, metabolic stress and muscle damage . A basic understanding of these mechanisms can help to inform exercise programme design to maximise muscle growth.\nMechanisms of Muscle Building\nPrevailing research indicates that mechanical tension is the primary driver of muscle hypertrophy . Mechanical tension relates to the force and stretch generated by muscles during resistance exercise [7, 8]. During weight lifting, muscle tension develops when the weights are lowered (eccentrically), which in turn increases the tension on the contractile components of muscle (actin and myosin), resulting in a hypertrophic response .\nAlso, muscle tension during resistance exercise has been shown to interrupt the integrity of the working muscles . This interruption brings about mechanical and chemical signals in the muscle that activate anabolic pathways and promote muscle growth . Greater muscle tension achieved by increasing the load lifted can elicit a stronger anabolic stimulus . However, an upper threshold exists whereby continually increasing tension will have diminishing returns (i.e. overtraining) .\nFrom a practical application standpoint, utilising a repetition range of 1 to 5 repetitions and loads of 90 to 100 percent of 1-repetition maximum (1RM) would be appropriate to enhance mechanical tension . Training in this lower rep range and with heavier weights will generate improvements in strength as the nervous system becomes better coordinated and synchronised .\nMetabolic stress manifests as a results of metabolites that build up in the bloodstream from resistance training . These metabolites include lactate, hydrogen ion, inorganic phosphate, creatine, and others. The specific mechanisms why metabolic stress increases muscle hypertrophy is completely understood. However, there are several suggested theories:\nType I and II Fiber Type Stimulation\n- All muscles are made up of muscle fibers that contract when stimulated. The two basic fiber types being fast-twitch (type II) and slow-twitch (type I) [3, 5]. Fast-twitch fibers show a greater potential for growth compared to slow-twitch fibers (about 50% more), yet increased activation of both types will maximise muscle hypertrophy . Research shows that metabolic stress enhances muscle hypertrophy by activating and fatiguing slow-twitch muscle fibers then forcing activation of fast-twitch fibers .\n- The feeling of muscles being “pumped” during resistance exercise is a phenomenon known as cell swelling . Research shows that cell swelling promotes muscle growth by stimulating protein synthesis and decreasing protein breakdown [6, 7]. Cell swelling occurs because of metabolic stress and primarily because of the lactate build-up in the blood [4, 6]. Lactate encourages water to be drawn into the muscle cell, which in turn places pressure on the cell wall [6, 7]. The muscle cell perceives this pressure as a threat to its structure and in response, sends anabolic signals that strengthens the cell [6, 7].\n- Muscle tissue produces a number of growth enhancing substances . Metabolic stress training is shown to increase a specific growth factor called mechano-growth factor (MGF), which is found to be important in muscle hypertrophy . Other growth factors including interleukin and fibrobrast growth factors have been found to activate from metabolic stress, which can further contribute to muscle building .\nFrom a practical application standpoint, metabolic stress is achieved by using higher repetitions (15-20 reps) range and loads of 60 percent or less of 1RM . Training in this higher repetition range will target more muscular endurance with little effect on muscle building because of the minimal activation of fast-twitch muscle fibers .\nThe familiar muscles soreness felt approximately 24 hours to 2-3 days following resistance training is known as delayed onset muscle soreness (DOMS) . DOMS is caused by small tears in the working muscle at a micro level [1, 2]. In response to damaged muscle tissue, immune cells (i.e. macrophages) migrate to the area to remove debris and repair the cell structure [1, 7]. When this occurs, macrophages produce signalling molecules called cytokines [1, 2, 7]. These cytokines release certain growth factors that promote muscle building [5, 7]. The end results is a stronger cell structure that can better withstand future muscle damage [5, 7].\nMuscle soreness is not required for muscle building . The more an individual trains at higher intensities, the better the tolerance to muscle soreness . However, even in the absence of muscle soreness, damage to the fibers still occurs . A point exists where too much muscle damage can be detrimental to muscle building. If muscle soreness is too great, this can delay the recovery of working muscle, which is essential for muscle building .\nFrom a practical application standpoint, a moderate repetition range between 8 to 12 repetitions and loads of between 65 to 85 percent of 1RM is optimal to create muscle damage . This repetition range allows for greater muscle tension to be sustained long enough to enhance muscle damage and fatigue . This moderate repetition range can also generate accumulation of metabolite, which can further contribute to enhanced muscle building .\nA common goal in exercise is the development of lean muscle. The great majority with this objective use moderate ranges (8-12 repetitions) with resistance training. However, research shows that certain gym programmes will encourage muscle building more than others. An examination of the research reveals three primary mechanisms that maximise muscle building. These mechanisms include mechanical tension, metabolic stress and muscle damage. Activation of each mechanism is best achieved by varying repetition range and load. Given this, individuals that want to maximise muscle building should try to vary their resistance training programmes to integrate low repetition (1 to 5) with heavy loads (90% to 100% of 1RM), moderate repetitions (8 to 12) with moderate loads (65% to 85% of 1RM), and finally high repetitions (15 to 20) with light loads (<60% of 1RM).\nFor more information on our personal training services please click here to read more.\n MacIntyre, D. L. et al. 1995. Delayed muscle soreness. The inflammatory response to muscle injury and its clinical implications. Sports Medicine. July. Vol. 20, No. 1, pp. 24-40.\n Mizumura, K. & Taguchi, T. 2016. Delayed onset muscle soreness: Involvement of neurotrophic factors. The Journal of Physiological Sciences. January. Vol. 66, No. 1, pp. 43-52.\n Schoenfeld, B. J. 2016. Science and Development of Muscle Hypertrophy. Australia: Human Kinetics.\n Schoenfeld, B. J. 2013. Potential Mechanisms of a Role of Metabolic Stress in Hypertrophic Adaptations to Resistance Training. Sports Medicine. March, Vol. 43, No. 3, pp. 179-194.\n Schoenfeld, B. J. 2013. The M.A.X. Muscle Plan. Leeds: Human Kinetics.\n Schoenfeld, B. J. 2013. The Role of Metabolic Stress in Muscle Growth [Online]. [Viewed 25 December 2016]. Available from: http://www.lookgreatnaked.com/blog/the-role-of-metabolic-stress-in-muscle-growth/\n Schoenfeld, B. J. 2010. The Mechanisms of Muscle Hypertrophy and their Application to Resistance Training. Journal of Strength and Conditioning Research. October, Vol. 24, No. 10, pp. 2857-2872.\n Toigo, M and Boutellier, U. 2006. New fundamental resistance exercise determinants of molecular and cellular muscle adaptations. European Journal of Applied Physiology. August, Vol. 97, No. 6, pp. 643–663.\nDid you find this content valuable?\nAdd yourself to our community to be notified of future content."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2c547a10-7fea-4a32-9073-98743afa0cba>","<urn:uuid:4dffdac6-8021-432e-b9be-f6b26f850ec2>"],"error":null}
{"question":"How does the thermal design of the Tonsley Main Assembly Building optimize air circulation?","answer":"The thermal design of the building was developed through complex thermal modeling to assess prevailing wind movements through the precinct. This analysis determined that additional roof openings were needed to allow air to move around more readily. The design also considered daylight, mapping the main circulation spaces to create a mix of clear and solid roofs.","context":["When the government of South Australia asked Woods Bagot to transform a former Mitsubishi Motors manufacturing site into a vibrant, mixed-use employment precinct, their team knew people-centric design was key.\nWoods Bagot and Tridente Architects worked closely with Renewal SA on a comprehensive design for the adaptive reuse project that would support advanced manufacturing employment in the Australian city of Adelaide.\nTo say their efforts were a success is an understatement. The Tonsley MAB (Main Assembly Building) and Pods have been recognized with many awards, from the Australian Institute of Architects David Oppenheim Award for Sustainable Architecture 2016 to the WAN Adaptive Reuse Award 2015. We recently talked with Woods Bagot’s Thomas Masullo and Milos Milutinovic about the project and the firm’s approach in general. Woods Bagot is a global design and consulting studio with more than 850 experts across 17 studios in six regions.\ngb&d: What is your design philosophy?\nWoods Bagot: Woods Bagot is a People Architecture company that places human experience at the center of the design process to deliver engaging, future-oriented projects that respond to the way people use space.\nPeople Architecture is both the process for design as well as Woods Bagot’s philosophy of a global practice. At its heart, People Architecture is a people-centric design approach that allows each project to be underpinned by deep analysis of user experience. It is delivered through the confluence of cross-sector knowledge informed by human behavior and community mapping. The vision of the firm culminates in projects that are built around the needs of users.\nSustainability was paramount throughout the design, with a planned 4-megawatt solar array on the roof producing affordable, sustainable energy for tenants and communications infrastructure providing connected technology as a basis for a smart grid energy system.\ngb&d: What was most exciting to you about this project?\nWoods Bagot: Sustainability was paramount throughout the design, with a planned 4-megawatt solar array on the roof producing affordable, sustainable energy for tenants and communications infrastructure providing connected technology as a basis for a smart grid energy system. Four urban forests inside the Main Assembly Building (MAB) provide naturally shaded green spaces, cool the air, and reduce the sun’s thermal load on the roof. “The Tonsley redevelopment represents a landmark South Australian project that is world-class in its ambitions, setting a new benchmark for sustainable urban regeneration,” says Woods Bagot Director Thomas Masullo.\nWoods Bagot Senior Associate and Project Leader Milos Milutinovic said sustainability was also a key driver of the design. “Our choice to retain and rejuvenate some 50,000 square meters of existing roof structure has saved approximately 90,000 tons of carbon—which is equivalent to taking 25,000 average cars off the road for one year. This also resulted in significant time, cost, and a reduction in decontamination and construction waste by extending the economic life of the existing structure.”\nThe broader Tonsley precinct is a public place where water sensitive urban design principles will support sustainability and reduce running costs. It also provides connected walking and cycling paths for easy access to public transport.\nThe existing roof and walls mediate between the exterior conditions and the interior semi-conditioned and conditioned spaces. The roof cladding can block or allow solar gains to enter the space. Options were evaluated to select the optimum combination of open, glazed, baffled roof sections. Four urban forests inside provide naturally shaded green spaces, cool the air, and reduce the sun’s thermal load on the roof.\ngb&d: What was the project’s biggest challenge?\nWoods Bagot: Woods Bagot’s design activates land that would otherwise be of low value, provides economic and environmental benefits, and creates a strong brand identity for a building rich in historical importance for South Australia. The project respects the strong social connections to the building while firmly establishing its place in the future of the state’s economy.\nTonsley is now a vibrant, mixed-use employment precinct supporting clean technologies, sustainable industries, advanced manufacturing, education, and research. The Main Assembly Building design became part of the demonstration of what a new industrial employment precinct would look like. The “umbrella” of the existing structure celebrates the industrial heritage of the building, creates a unique public destination, and delivers a clear layout with a highly flexible work environment. The design also incorporates a “pod” approach that is adaptable, flexible, and highly functional.\ngb&d: What is one lesson you learned during the process?\nWoods Bagot: Recognizing the design solution needed to be a complex urban environment with a series of streets and laneways for services and equipment, the team conducted complex thermal modeling to assess the way prevailing winds would move through the precinct, determining that more roof openings were required to allow air to move around more readily. Considering daylight, mapping the main circulation spaces to create a mix of clear and solid roofs.\nLearn more about at Woods Bagot."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c80ce9a7-f591-4731-b27d-bff3e5c43413>"],"error":null}
{"question":"How does Mother Earth School's outdoor education approach compare to the plant blindness problem affecting modern society?","answer":"Mother Earth School's approach directly addresses the plant blindness problem in modern society. While studies show Americans can identify over 1,000 corporate logos but fewer than 10 plants, Mother Earth School provides year-round outdoor immersion experiences focused on nature connection, homesteading skills, and forest exploration. The school's programs help children develop deep relationships with the natural world, countering the trend of plant blindness where people are unable to notice or appreciate plants in their environment. Their approach aligns with research showing that early exposure to nature and plant mentorship is crucial for developing environmental awareness and counteracting the effects of urbanization and indoor-focused lifestyles.","context":["One early May day some years ago I was walking with a friend in a preserve near her home. I had discovered the trail the day before and was so delighted with the abundance of wildflowers I wanted to share them with her. She was equally delighted. And then, as we finished the trail, said, “I’m so glad I came with you because I would never have noticed the flowers if you hadn’t shown me.”\nI was shocked. The flowers pictured here were everywhere, in vivid purple, white, yellow, orange, extra luminous under a pale gray sky. To my eyes, they were not only impossible to miss, but jumping at me. My friend is a woman of great intelligence and compassion. Her home is a warm haven, full of her exquisite sensitivity to color and texture. She wears beautiful blends of pinks, magentas, purples, turquoises. She sends me pictures of gorgeous sunsets from all over the world.\nThis is a perfect example of plant blindness. It’s a common phenomenon and a dangerous one: as a culture, we won’t take care of what we don’t see. Though 57% of endangered species are plants, there is a reason the World Wildlife Fund uses a panda as its symbol. We are much more acutely attuned to animals than plants. And the more like us — those with eyes facing front, for example — the more we identify with them, a trait we share with other primates. Thus, despite that endangered 57%, and the fact that 28,000 species are used in medicine, only 4% of federal endangered species funding goes to plants. But if you don’t save the bamboo forest, there is no way to save pandas. Extinctions don’t just happen — the ecology and habitat around the animal or plant change faster than a species can adapt.\nBotanists Elisabeth Schussler and James Wandersee coined the term plant blindness in the 1990s. The students they tested — in the fourth to seventh grade, all in the U.S. — were twice as interested in animals as plants. Only 7% expressed interest in the science of plants. Since that time so few people are studying botany that university departments are closing globally and funding is drying up. The students that do exist are more interested in molecular botany, designing engineered plants for commercial applications. This means that there are and will be fewer and fewer people who can educate people about plants. At the federal level, there is one botanist for every 20 million acres of public land.\n“We define plant blindness as (a) the inability to see or notice the plants in one’s environment; (b) the inability to recognize the importance of plants in the biosphere and in human affairs; (c) the inability to appreciate the aesthetic and unique biological features of the life forms that belong to the Plant Kingdom; and (d) the misguided anthropocentric ranking of plants as inferior to animals and thus, as unworthy of consideration.”\nSchussler and Wandersee hypothesize that we are wired for this bias. Millions of bits of information hit our retinas every second and need to be drastically sorted to be processed by the brain. Thus movement, common to animals, will attract our attention. Faces are compelling. Danger gets an instantaneous response. Things that are easily differentiated are preferred. Our brains are busy, and the quiet, static nature of most plants, mostly green, easily fades them into the background.\nIt’s easy to imagine, as our ancestors began to walk the African savannas, that evolution would have prepared us to respond to animals as sources both of danger and nourishment. But we also evolved to see red and green preferentially and to see more different shades in the green spectrum than in others. Theories suggest that we needed to distinguish new, edible shoots from more mature, potentially toxic leaves and stalks. And to contrast the ripeness of red against the green of fruits not yet sweet enough. So while we may have evolved to respond to animals, we also evolved to respond to plants: to finding them and making fine distinctions among them. And further to preparing them, eventually to nurturing them as a source of steady nourishment and medicine. Our ancestors, even recent ones, had to know a great deal about plants to feed and heal themselves.\nSo what happened? How did we, at least those reared in western traditions, lose our sensitivity to the plant world? Urbanization? Suburbanization? Loss of small, local farms? More and more time spent indoors and on screens, in a virtual world? Badly designed cities without enough green spaces? Homogenous zoning laws that make everywhere look like everywhere else? The culturally driven idea that humans are at the top of the evolutionary ladder and plants at the bottom, therefore barely worth thinking about? Moving, generation on generation, farther away from the grandparents and great grandparents who were intimate with plants?\nYes! All of it, and more. A Worldwatch Institute study found that Americans can identify over 1,000 corporate logos on sight, but fewer than 10 plants. And yet without plants, we wouldn’t exist. Couldn’t eat, couldn’t breathe. Would never have evolved. We are entirely dependent on them. The loss of these straightforward insights is what drove Schussler’s and Wanderslee’s campaign for more education focused on plant biology.\nBut beyond the narrow lens of our self-interest is a broader view. Plants are our elders by hundreds of millions of years. They are full of deep wisdom and spiritual resonance. They are the primary inhabitants of the earth, both in expanse and biomass. They grew up with the earth, were formed by evolution, and further evolved all that came with and after them. They are the most crucial beings on our planet. How do we not know them?\nThere are certainly plant and medicine people and societies who are still deeply connected, but most children in the U.S. today don’t interact much with plants. Even children in rural areas, according to Richard Louv’s The Last Child in the Woods, are not out among plants. They are often living the same time-structured, mostly indoor, screen-bound life that city kids tend to live. My love of plants came from my earliest childhood, when I lived for five years in a green paradise that I could roam freely in. Even when we moved from there to a more typically suburban neighborhood, I was still in a place full of trees and flowers, along with small but precious wild areas, and a river nearby.\nI learned the mechanics of plant biology in high school, but neither my parents nor my teachers were plant people. What mattered the most was that I was able to spend time among them at an age when their spiritual nature made perfect sense to me. I couldn’t have put this into words then, of course, but I knew I was alive among beings that were not only friends and family but were taking me in hand, telling me their deep truths about our profound connections.\nWe live in a world already close to 60% urbanized, and rapidly becoming more so. 80% of the U.S. population lives in metropolitan areas. So we may not be able to offer many children the opportunities I had. But we can certainly do infinitely better than we are now, both in the classroom and, even more important, outside of it. According to Louv’s studies, many of the things we say we value — mental and physical health, the ability to concentrate, to learn, to cooperate — are fostered when both children and adults have access to nature. He named our plight ‘nature deficit disorder’, an epidemic that flies below the radar because so few people are aware that they’re suffering from it.\nThe oldest Homo sapien fossils are 300,000 years old. We only began clustering into cities and towns 10,000 years ago. Until well into the 20th century, most people still lived in small villages and rural areas. So for 99.9% of the time since we evolved, we were immersed in the green breathing of plants. They formed us. Literally in the nourishing of our bodies and large brains. And also more deeply: our psyches and souls were created by constant interaction with the natural world. We were not designed for the choices we’ve been making.\nRight now we face equally great challenges and opportunities. Climate change and species loss require us to green our world as quickly as possible. That means preserving the vast areas of forests and savannas that still exist, but it also means bringing more green closer to home. Re-greening our cities and suburbs. Knitting together biodiversity corridors with our gardens and parks. Creating green roofs and green walls. Planting streets and meridians with appropriate plants. As we face redesigning our cities for a low carbon future we will have many chances to create green space as long as we’re not too plant blind to seize the moment. It’s a huge job, but also an exciting and ultimately beautiful one.\nSchussler’s and Wandersee’s plant blindness campaign aimed largely at expanding botanical education. But they also made a simple and rather heartwarming suggestion: become a plant mentor. Children who have people in their lives connected to plants become connected themselves. Children who grow up knowing the green world are our future environmentalists. If my own young relatives are anything to go by, as soon as you start talking about plants they become curious. It’s as if you’ve switched on a current, opened the channel to the long years of plant immersion we are descended from. We carry that deep knowledge in our bones and souls. It’s there to be reawakened. It’s longing to be reawakened.\nI’d love to have you join me! If you add your email address, I’ll send you notices of new monthly posts.","This is a new series of posts in which I interview Portland personalities that are making a difference in connecting children of this region to nature. Richard Louv, the premier voice in the children and nature movement, has written several times about our natureful city. In Last Child in the Woods, he remarked on Portland’s Greenspaces program’s “call for the creation of a regional system of parks, natural areas, greenways, and trails for both wildlife and people;” PSU students’ research on possible greenroof design in downtown; and the 40-Mile Loop. In The Nature Principle, Louv commented on research in Portland on the health benefits of nature (outdoor “prescriptions”); profiled a teacher who worked as an assistant for a study of small mammals in an urban green space (Marshall Park); described Mike Houck’s work to make room for nature in a big city at a time when the consensus was that nature and wildlife were elsewhere, not within where people dwell; and how the Tualatin River National Wildlife Refuge provides access to nature via the bus system. There are other mentions of Portland, too. Patrick, Catherine, and I feel lucky to live in a city that values what the Earth gives to it. Moreover, we feel privileged to share this city with folks who strive to not only instill a love of nature into its citizens’ minds and hearts, but in providing better access to that nature.\nPlease contact me at firstname.lastname@example.org if you have a suggestion for someone to be interviewed for this series.\nToday I talk to Kelly Hogan, co-founder at Mother Earth School.\nHi Kelly, welcome to Exploring Portland’s Natural Areas. Would you please share with us your background – education, jobs, etc. that relate to nature in some way?\nI am the co-founder of Mother Earth School, an all-outdoor preschool, kindergarten, 1st & 2nd grade located on forested urban farms in SE and SW Portland. We are about to start our 6th year of programming and we just completed our 5th year of summer camps that serve ages 4-12, including rite of passage work with the pre-teenagers. I have completed rite of passage training with an organization called Rite of Passage Journeys, an Art of Mentoring training at the Wilderness Awareness School, I participated in the Wilderness Survival immersion program through Trackers NW, I have my Waldorf teaching certificate and a Permaculture Design certificate. I lived and volunteered at Tryon Life Community Farm sustainability education center (the site of our early childhood programs) for 5 years where I focused on homesteading skills (specifically animal husbandry).\nDescribe for us how you connect Portland kids to nature. Tell us about your programs.\nI teach preschool at Mother Earth School. The programs at Mother Earth School are outdoor immersion experiences year-round that focus on homesteading skills, nature survival skills, community building, sensory integration and forest exploration. The children are nurtured in an environment full of awe, discovery, connectedness, adventure and celebration. We honor the seasons through song, storytelling, puppetry, craft projects, skill building and play. This relationship that the children develop with the natural world is the foundation of a future generation of environmental advocates. The mission of Mother Earth School is to support the healing of humanity by transforming education with reverence to the wisdom inherent in nature.\nDo you have any moments in nature from your childhood that left an impression on you?\nI grew up playing outside every day in the forest behind my house. All of my childhood memories center around nature. I have one very clear memory of running through my yard, fully immersed in an imaginative experience. Then, all of a sudden, it was as if I “snapped out of it” and caught a glimpse of my surroundings from a perspective that was not based in imagination. I stopped for a moment, glanced around at the forest which in that moment looked simply like scenery, and then made the conscious decision to re-enter my imaginary world, in which I was much more connected to the essence of what was around me, rather than just how things looked. It is a difficult moment to describe in words, but is quite a profound look at the level of engagement of the “child’s mind”. I want to nurture that in children and encourage its expression.\nDo you have children yourself? If so, tell me about their relationship to nature.\nI have 2 children. Talon is 12 and Yarrow is 10. They also grew up engaged in outdoor adventures and playing freely in nature. My son is a pre-teen and currently prefers skateboarding and basketball to hiking or fishing, but he doesn’t hesitate to spend a week camping or a day rafting. My daughter is constantly making herbal remedies (mostly in the form of tea). She loves plants and animals and enthusiastically participates in goat milking and other types of animal tending. She writes songs and stories about homesteading and loves to come with me to primitive skills gatherings throughout the year.\nDescribe for us something about Portland’s focus on nature that you think is valuable for the city’s youth.\nI love the diversity in the types of programming offered for youth in the Portland area! Mother Earth School offers an experience that is media-free, dreamy (in a good way!!) and idyllic whereas Trackers uses media trends to offer programs that are attractive to children that might otherwise prefer playing video games. I value the experiences on both ends of the spectrum, as well as everything in-between. We get contacted almost every week by an individual, school or organization that is looking for guidance in how to incorporate more nature into their programming. From public school garden classes and higher education credits for outdoor education training, to child care facilities that are committing to using all-natural materials, it is truly heartening to see that this movement is so quickly permeating the Portland area (and beyond!).\nIs there anything missing that you would like to see happen in Portland?\nI would like to see more access to these types of experiences for low-income families. Mother Earth School is a tuition-based school (necessarily so, because in order to be a registered school with public funding, we would have to change aspects of what is so foundational to our educational philosophy and practices). With more private funding, we would be able to reduce our tuition as well as to offer scholarships to families that can’t afford to pay for school. It seems that so many enriching experiences are costly, including nature immersion, which is really ironic. Either the government and insurance sectors are going to have to take a leap of faith, or the movement needs more people willing to invest money to support the accessibility of connecting consistently with nature as a means of education.\nDo you have any advice for parents looking to connect their kids to nature?\nYes, my advise is to prioritize it. It is easy for the busyness of urban life to displace outdoor experiences to occasional weekend outings (especially in the rainy season, which is most of the year in this area). Invest in some sturdy outerwear and long underwear (utilize the amazing second-hand market here in Portland to keep costs low) and play outside along with your children! P.S. – adults, it is still fun to splash in puddles and climb trees!! :)\nWhat is you favorite natural space in our city?\nI absolutely adore Tryon Creek State Park. It is such a magical forest that is so appreciated by its patrons that I feel like it is bursting with joy. Mother Earth School has special names for so many of the places that we love there. One of our favorites is the ‘owl tree’ where baby barred owls are born every spring. The baby owls are actually flirtatious!! The parents of our school children will hear all about the baby owls and want to take a walk after school to see the owl family. They will ask me for directions to the tree and all I have to say is “your child can show you the way” (and they will likely also introduce their parents to all the edible plants along the way as well).\nThank you for sharing your thoughts, Kelly! Patrick is just about to start first grade. Perhaps when Afton goes to preschool (she’s not even two weeks old yet!), she will go to Mother Earth School."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:8bcb5772-dda4-4883-896b-47145393c03b>","<urn:uuid:88783ea9-0a02-400b-9567-a61dfe38d77b>"],"error":null}