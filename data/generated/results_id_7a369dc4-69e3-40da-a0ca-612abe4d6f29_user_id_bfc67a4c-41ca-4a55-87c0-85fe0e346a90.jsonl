{"question":"In the Western Indian Ocean region, how many ppl rely on coral reefs 4 their livelihoods + what specific benefits do the reefs provide to coastal communities?","answer":"In the western Indian Ocean, approximately 60 million people depend on coral reefs for their livelihoods. The reefs provide multiple benefits to coastal communities: they support local fisheries that provide both food and income, create barriers that protect coastlines against ocean storms, and protect lagoons during monsoon seasons, allowing fishers to continue fishing in the lagoons.","context":["Coral reefs are the backbone of ocean biodiversity, as well as industries that support over 500 million people worldwide.\nThey are, however, some of the most threatened habitats on earth. Vulnerable to temperature changes that destabilise their delicate ecosystems, some scientists estimate that up to 90% of the world’s coral could be gone by 2050 owing to climate change. This would have devastating consequences for ocean plants and animals, as well as the millions of communities who rely on fishing for their sustenance and livelihoods.\n‘Global warming has caused unprecedented increases in sea surface temperatures, which kills coral,’ says Dr Melita Samoilys, a research fellow at Oxford’s Department of Zoology. ‘Corals cannot survive even just a few degrees of temperature rise because of their remarkable symbiotic relationship with microscopic algae called zooxanthellae. These zooxanthellae feed the coral and enable them to form into reefs. Zooxanthellae can only survive in a narrow temperature range, so cannot cope with temperature increases caused by climate change. When the water gets too warm, the algae, which allow the coral to survive and give them their beautiful colours, desert the coral – hence “coral bleaching” occurs.’\nCoral bleaching, so called because of the ghostly white colour taken on by dead coral, has increased rapidly over the last few years. Dr Samoilys says: ‘Changes in weather patterns, such as El Niño and La Niña, have been connected to climate change and cause a rise in sea surface temperatures. Over the last few years, we have seen summer sea surface temperatures exceed corals’ comfort zone almost every year.’ Intense summer temperatures in 2016 and 2017 alone are blamed for the coral bleaching of nearly half of the Great Barrier Reef, a reef structure so large that it can be seen from outer space.\nThe mass coral bleaching occurring on enormous scales globally is a startling and accurate indicator of how much our climate is changing\n‘Massive coral mortality, such as in the Great Barrier Reef, has a dramatic effect on marine life,’ says Dr Samoilys. ‘Coral reefs are one of the most biodiverse ecosystems on the planet, and their loss has a major impact on other invertebrates, vertebrates and plants. We are seeing a disappearance of fish species that are highly dependent on live coral for food and shelter. Reef sharks, too, are rapidly disappearing in developing countries such as Kenya and Tanzania, where they are at a critical level of depletion. Large predatory fish such as sharks are a critical component of a reef’s food chain and their loss precipitates a cascade of disruption throughout reefs.’\nNot only is ocean biodiversity under threat, but so are the coastal communities that rely on the ocean to survive. ‘In the western Indian Ocean alone, we estimate there are 60 million people dependent on coral reefs for their livelihoods,’ says Dr Samoilys. ‘Local fisheries are vital to the economies of developing countries, for food as well as income. Reefs also create a barrier protecting coastlines, which is essential to defend villages against ocean storms. They protect lagoons against seasonal weather like monsoons, which can last several months, allowing fishers to continue to fish in the lagoon. This is critical for their livelihoods.’\nDespite this bleak picture for coral reef survival, researchers are continuing to look for ways to save and conserve reefs. ‘When a reef dies, the reef structure is still there, in carbonate rock form. And there are usually a few isolated coral colonies still alive,’ explains Dr Samoilys. ‘What we are working on is trying to understand if it is possible for coral larvae to settle and grow when the reef is no longer functioning – if it has the potential to return.’\nGoing forward, the future of coral reefs is uncertain. Dr Samoilys says: ‘Reefs’ responses to elevated sea surface temperatures are highly variable and not always predictable; some species are very sensitive, while others are more tolerant. Thus, as we go forward into a future with warmer seas, the community of coral species on a reef will change.\n‘The response of reef fishes, too, is also highly variable. Some species that are highly dependent on live coral are likely to go locally extinct. Others may survive a changed ecosystem – these are often larger, more important species in fisheries, which is a good thing for people dependent on fishing. It also means, however, that these species’ populations need to be safeguarded now so they survive into the future.\n‘The mass coral bleaching occurring on enormous scales globally is a startling and accurate indicator of how much our climate is changing.’\nIn light of the increasing damage wrought by climate change, controlling for other factors that affect reefs is becoming even more important. ‘Maximising the resilience of reefs to hot water is a primary objective of conservationists,’ says Dr Samoilys. ‘There is strong evidence that the healthier the general state of the reef, in terms of its fish populations and the surrounding water, the more resilient it will be to climate change.’ This means combating pollution and overfishing, in order to give reefs the healthy environments they need to bounce back.\n‘But ultimately the most important thing we can do to preserve coral reefs is to address climate change. Governments need to get serious about changing policies across all sectors to reduce greenhouse gas emissions. Emissions need to be central to all domestic policies; there is no “later” for climate change.’"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:3ce8110e-e5fd-4397-b9e6-ff3447b0b16e>"],"error":null}
{"question":"Hey! I'm interested in some real numbers & data, could you compare the environmental impact stats between maritime shipping and vertical farming? Looking for water usage, carbon emissions etc!","answer":"Maritime shipping currently produces nearly a billion tons of carbon emissions annually, which exceeds Germany's yearly emissions. The industry handles 90% of global goods transportation. On the vertical farming side, the technology demonstrates significant environmental improvements, achieving 95% reduction in water usage compared to conventional agriculture, while providing 20-30 times higher crop yields. Both industries are working to improve their environmental impact - shipping aims for a 25% improvement in fuel efficiency and carbon footprint by 2025, while vertical farming's controlled environment technology enables significant resource conservation, particularly in water-scarce regions.","context":["Revealing how efficiently the world’s fleet operates with a free-to-access ship operational efficiency portal that identifies areas for improvement by showing how efficiently an individual vessel has operated relative to itself and its peers over the past year.\nWe aim to drive a 25% improvement in the fuel efficiency and carbon footprint of global commercial shipping fleets by 2025.\nWhat is Shipping Efficiency?\nWe are improving the fuel efficiency and carbon footprint of the global commercial shipping fleet by bridging gaps in market information and influencing key stakeholders to embed efficiency into their decision making.\nWhy It Matters\nMaritime shipping transports 90 percent of the world’s goods and much of the raw materials and agricultural products needed to make those goods. But transporting huge volumes of products across the globe comes at a cost. The shipping industry emits nearly a billion tons of carbon per year—more than Germany’s annual emissions—and there are no globally mandated regulations to abate these emissions.\nWe are working with BetterFleet, financial institutions, ship owners and charterers, and governing bodies to bring an unprecedented level of transparency to shipping performance and show how energy efficient vessels improve value and cut costs.\nWhat We’re Doing\nA tool that allows charterers to compare a ship’s design efficiency to peer vessels using a simple A–G scale, increasing demand for more efficient vessels by making the potential for fuel savings transparent.\nWorking with financial actors to understand and manage climate risk in the global maritime industry.\nWorking with the International Maritime Organization (IMO), member states, and industry organizations to ensure that the shipping industry will thrive in a future low-carbon economy.\nWhat We’ve Accomplished\n- In December 2016, we launched BetterFleet, the first publicly available and comprehensive online portal revealing the operational efficiency of ships throughout the world.\n- In 2016, over 2 billion tons of cargo moved on efficient ships selected with the GHG Emissions Rating we developed with RightShip.\n- HSH Nordbank and KfW IPEX-Bank now use energy-efficiency data in deciding which vessels will receive financing.\n- Carbon War Room’s direct engagement with the White House prompted the U.S. to submit a paper, with CWR’s input, that strongly influenced IMO’s development of a climate “road map.”\n- We helped fund a multitechnology efficiency retrofit, and identified a profit-sharing business model that enabled profitable investment in energy efficiency technologies, with shipowner Hammonia Reederei and charterer Intermarine. The deal was recognized by Business Green as the Energy Efficiency Project of the Year, and shortlisted for Lloyd’s List Deal of the Year Award.\n- In August 2016, our report Revealed Preferences for Energy Efficiency in the Shipping Market exposed market dynamics and failures that are key to designing effective pathways and regulations for reducing carbon emissions.\n- In 2014, Canada’s Prince Rupert Port Authority and Port Metro Vancouver became the first ports in the world to offer incentives for the most efficient ships.\nShipping 'progressives' call for industry carbon emission cuts—The Guardian\nNavigation Decarbonization—An approach to evaluate shipping's risks and opportunities associated with climate change mitigation policy, 2017\nRevealed Preferences for Energy Efficiency in the Shipping Markets, 2016\nDead in the water: an analysis of industry practices and perceptions on vessel efficiency and stranded ship assets, 2016\nProfit-sharing shipping retrofit deal wins business green award.\nGreen finance for dirty ships—The Economist\nNew Ship Efficiency Portal Launched—The Maritime Executive\nFind more resources at ShippingEfficiency.org.","Vertical Farming: Location a Key Factor to Success, Says IDTechEx\nVertical farming, the practice of growing crops indoors on vertically stacked layers, has received no small amount of interest over the last few years. Vertical farms commonly tout impressive numbers, such as using 95% less water and providing crop yields 20-30 times that of conventional agriculture. These claims, among many others, have seen many vertical farming start-ups being founded alongside large amounts of industry funding; funding for the industry reached a record high in 2021, with over US$1 billion being raised across the entire industry. The recent IDTechEx report, \"Vertical Farming 2022-2032\", details the economic and technological factors shaping this rapidly growing industry.\nWith crops being grown indoors under controlled environments, a selling point used by multiple vertical farms is that they can grow crops anywhere – even in the heart of a city. This has led to proponents of the industry envisioning \"smart cities\", where vertical farms in city skyscrapers help feed the urban population. While this is achievable in principle, the truth is that the choice of location for vertical farming is much more involved and intricate than it may appear from these claims alone. Choosing an ideal location can be one of the most important factors in determining the success of a vertical farm.\nSome vertical farms may choose to set up their facilities in pre-existing facilities, such as abandoned warehouses. In these cases, identifying the suitability of the venue is the first point of consideration: vertical farms are very energy intensive, and it is important to ensure the facilities chosen can support these energy loads. In addition, the ergonomics of the facility is also important; should the layout not be given proper consideration, this can impede workers and decrease worker efficiency. As labor costs are typically among the largest sources of expenditure for a vertical farm, improving labor efficiency to reduce these costs is of paramount importance.\nWhile growing crops in the center of a city may seem ideal, the reality is that this may be counterproductive. Obtaining and maintaining such a location is expensive and can contribute significantly to the operating expenditure of a vertical farm while presenting logistical challenges in distributing produce; the \"last mile\" of food distribution is often the hardest. Having a farm right next to the consumers themselves may also be less ideal than instead choosing a location near food distribution centers, as this allows for more efficient delivery of produce. As distribution centers are typically located on the outskirts of cities, the cost of land is also much cheaper. This is the approach chosen by UK-based Jones Food Company, which chose Scunthorpe as a location for its vertical farm – this is a relatively low-cost location located near food distribution centers and a network of motorways that could still reach many consumers in a day, even if it isn't right in the middle of the capital city. Vertical farms should carefully consider their place in the supply chain before establishing a base.\nOn a larger scale, vertical farms may prove more profitable in different geographical regions. Vertical farms can reduce water usage significantly over conventional agriculture, and the high degree of control over the growing environment allows them to grow crops in extreme climates – where such crops may not otherwise be able to grow. In return, vertical farms demand more energy to carry out growing operations. To maximize their potential, vertical farms would ideally be located in regions of water scarcity, such as Sub-Saharan Africa and the Middle East, or in areas with extreme climates, such as in Scandinavian countries, where the low amounts of sunlight and high costs of regulating greenhouse environments single out vertical farms as an optimal solution. The amount of agricultural land available is also an important factor – regions looking to increase food security and reduce reliance on imports while facing challenges in acquiring sufficient agricultural land would find vertical farms to be ideal. A particularly prominent example of such a country is Singapore, which has demonstrated much interest in vertical farming over the last few years.\nBeyond the considerations of water scarcity and temperature, the general availability of fresh produce and the distribution networks of given countries should also be considered. Vertical farms use the added freshness and higher quality of their crops as a primary selling point, but these are typically offset by higher prices. Should there already be a large supply of high-quality produce made available at lower costs, vertical farms will find it hard to distinguish their own produce and may struggle to establish a significant market share. The converse would also be true; should a country lack easy access to fresh produce, vertical farms are expected to see much demand for their produce. An example of such a region would be the Middle East: leafy greens typically travel several thousand miles to reach stores, resulting in consumers facing high prices and low-quality products. The high price of conventionally farmed leafy greens, alongside government subsidies, makes it easier for vertically farmed produce to approach price parity while providing much fresher, higher-quality products.\nWhile the choice of location is an important consideration, it is only one of many others that must be given proper thought. Only through proper optimization of growing operations to improve efficiency and reduce costs can vertical farms reach their true potential. In the IDTechEx report, \"Vertical Farming 2022-2032\", many further important factors for consideration are discussed in detail, and the future of vertical farming is evaluated through 10-year market forecasts.\nIDTechEx guides your strategic business decisions through its Research, Subscription and Consultancy products, helping you profit from emerging technologies. For more information, contact research@IDTechEx.com or visit www.IDTechEx.com.\nThis post does not have any comments. Be the first to leave a comment below.\nPost A Comment\nYou must be logged in before you can post a comment. Login now."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:109ad513-d773-4137-b905-fd217361730e>","<urn:uuid:98c0eed7-8d91-42cd-b0d1-cc7c105f9a4d>"],"error":null}
{"question":"What are the key differences between traditional voice communication systems and modern ACARS in terms of their communication capabilities?","answer":"Traditional voice communication has been the main method of ground-to-air communication for decades, while ACARS is a digital data link system that has been in use since 1978. ACARS offers more diverse communication options, including VHF channels, SATCOM (except in polar regions), and HF data link for polar communications. Additionally, ACARS supports both automatic and manual message initiation, and can handle various types of content including load sheets, weather information, technical performance data, and status updates.","context":["Vol.5 No.2 (February 2015)\nDevelopment Status of Data-Link Communication System in the Next Generation Air Transportation System\nIn order to adapt to the rapid growth of air traffic flow, Data-link Communication System was put forward as an important communication resource in the next generation. The ground-to-air com-munications system was generally called “Data-link”, which was used to build a bidirectional data communication between air craft and ground system. Over the past few decades, the ground-to-air communication has been mainly completed by voice system. Compared with the traditional voice communication, Data-link Communication System has many significant advantages. However, it still has a lot of shortcomings in using process. Although current study on Data-Link Communication System doesn’t have a consistent conclusion, there are still evidences which show that if the data link communications completely replace the voice communication, it can prevent 37% com-munication errors; if additional system is designed to determine whether the pilot understands relevant information, it can reduce 30% errors. The data link has brought a huge challenge to the air traffic control system and the pilot. The present study focuses on overcoming technical and potential safety problems to implement the next generation air transportation system completely.\n杜 晖 , 张成玉 (2015) 数据链在下一代航空运输系统中的发展现状。 心理学进展， 5， 105-111. doi: 10.12677/AP.2015.52015\n AC-121-FS-2008-16R1 (2008) Standards and guidelines of aviation operators to use the air-ground data communication system.\n Billings, C. E. (1997). Aviation automation: The search for a human-centered approach. Hillsdale, NJ: Law-rence Erlbaum Associates.\n Corwin, W. H., & McCauley, H. W. (1990). Considerations for the retrofit of datalink. Paper Presented at the SAETech. Rep. No. 901886, Warrendale, PA.\n Gibson, J., Orasanu, J., Villeda, E., & Nygren, T. E. (1997). Loss of situation awareness: Causes and consequences. Paper Presented at the Proceedings of the Eighth In-ternational Symposium on Aviation Psychology Columbus, OH.\n Goteman, O. (2010). Flight crew cooperation during live controller-pilot datalink communication trials. CPDLCpaperhufasrev1.doc.\n Hawkins, F. H. (1993). Human factors in flight (2nd ed.). Aldershot: Ashgate Publishing Co.\n Helleberg, J. R., & Wickens, C. D. (2003). Effects of data-link modality and display redundancy on pilot performance: An attentional perspective. The International Journal of Aviation Psychology, 13, 189-210.\n Kraut, J. M., Kiken, A., Billinghurst, S., Morgan, C. A., Strybel, T. Z., Chiappe, D., & Vu, K. P. L. (2011). Effects of data communications failure on air traffic controller sector management effectiveness, situation awareness, and workload. In M. J. Smith, & G. Salvendy (Eds.), Human Interface and the Management of Information, Interacting with Information (pp. 493-499). Berlin: Springer Berlin Heidelberg.\n Lee, A. (1989). Data link communication in the national airspace system. Proceedings of the 33rd Annual Meeting of the Human Factors Society, Denver, 778-782.\n Lin, C. J., Lin, P.-H., Chen, H.-J., Hsieh, M.-C., Yu, H.-C., Wang, E. M.-Y., & Ho, H. L.-C. (2012). Effects of controller-pilot communication medium, flight phase and the role in the cockpit on pilots’ workload and situation awareness. Safety Science, 50, 1722-1731.\n Maryland Aeronautical Radio, Inc. (2005). Airlines Electronic Engineering Committee. ARINC Characteristics 758-2: Com- munication management Unit (CMU) Mark 2.\n McGann, A., Morrow, D., Rodvold, M., & Mackintosh, M. A. (1998). Mixed-media communication on the flight deck: A comparison of voice, data link and mixed ATC environments. The International Journal of Aviation Psychology, 8, 137- 156.\n Navarro, C., & Sikorski, S. (1999). Datalink communication in flight deck operations: A synthesis of recent studies. The International Journal of Aviation Psychology, 9, 361-376.\n Shelton, K. J., Prinzel III, L. L. J., Arthur III, J. T. J., Jones, D. R., Allamandola, A. S., & Bailey, R. E. (2009). Data-link and surface map traffic intent displays for NextGen 4DT and equivalent visual surface operations. In SPIE Defense, Security, and Sensing (pp. 73280C-73280C). Bellingham, WA: International Society for Optics and Photonics.\n Stedmon, A. W., Nichols, S. C., Cox, G., Neale, H., Jackson, S., Wilson, J. R., & Milne, T. J. (2003). Framing the flight deck of the future: Human factors issues in free flight and datalink. Proceedings of the 10th International Conference on Human-Computer Interaction, Crete, 22-27 June 2003, 193-204.\n Stedmon, A. W., Sharpies, S., Littlewood, R., Cox, G., Patel, H., & Wilson, J. R. (2007). Datalink in air traffic management: Human factors issues in communications. Applied Ergonomics, 38, 473-480.\n Williams, J., Hooey, B. L., & Foyle, D. C. (2007). Pilot conformance to time-based taxi clearances: Implications for advanced surface traffic management systems. Human Centered System Lab Technical Report (HCSL-07-02), December 2007.","Aircraft Communications, Addressing and Reporting System.\nACARS (pronounced AY-CARS) is a digital data link system for the transmission of messages between aircraft and ground stations, which has been in use since 1978. At first it relied exclusively on VHF channels but more recently, alternative means of data transmission have been added which have greatly enhanced its geographical coverage. There has also been a rapid trend towards the integration of aircraft systems with the ACARS link. Both have led to rapid growth in its use as an operational communications tool.\nModern ACARS equipment now includes the facility for automatic as well as manual initiation of messaging. ARINC guidelines have been defined for all the various avionic components of ACARS.\nACARS messages may be of three types based upon their content:\n- Air Traffic Control (ATC)\n- Aeronautical Operational Control (AOC)\n- Airline Administrative Control (AAC)\nATC messages include aircraft requests for clearances and ATC issue of clearances and instructions to aircraft. They are often used to deliver Pre-Departure, Datalink ATIS and en route Oceanic Clearances. However, whilst the ACARS system is currently fulfilling a significant 'niche' role in ATC communications, it is not seen as a suitable system for the more widespread ATC use of datalink referred to as Controller Pilot Data Link Communications (CPDLC).\nAOC and AAC messages are used for communications between an aircraft and its base. These messages may be of standard form or as defined by users, but all must then meet at least the guidelines of ARINC Standard 618. Any message content is possible including such examples as:\n- upload to the aircraft of final load and trim sheets;\n- upload of weather or NOTAM information;\n- download from the aircraft of status, position, eta, and any diversion;\n- download of spot weather observations from aircraft sensors:\n- download of technical performance data including automatically triggered exceedance or abnormal aircraft system status information, and\n- 'housekeeping' information such as catering uplift requirements, special passenger advice and ETA.\nFree Text messaging is also possible.\nThe ACARS System\nWhen ACARS was first developed as an ATN component, it was modeled on the existing Telex System. As a consequence, the system architecture is based on three main components:\nThe Aircraft Equipment\nACARS equipment onboard an aircraft is called the Management Unit (MU) or, in the case of newer versions with more functionality, the Communications Management Unit (CMU). This functions as a router for all data transmitted or received externally, and, in more advanced systems internally too. The ACARS MU/CMU may be able to automatically select the most efficient air-ground transmission method if a choice is available. A flight deck printer will be provided and a cabin crew terminal may also be available. Flight Crew access to the ACARS system is usually via a CDU which, in more advanced systems, can be used to access up to seven different systems such as the FMS, besides the MU/CMU. Each system connected to the CDU generates its own display pages and accepts keyboard input when selected. Some EFBs may be used as a substitute for access via the CDU.\nThe Service Provider\nA Datalink Service Provider (DSP) is responsible for the movement of messages via radio link, usually to/from its own ground routing system. ACARS messages are transmitted using one of three possible data link methods:\n- VHF or VDL (VHF Data Link) which is line-of-sight limited\n- SATCOM which is not available in polar regions\n- HF or HFDL (HF Data Link) which has been added especially for polar region communications\nThe main primary DSPs are ARINC and SITA. Until quite recently, each part of the world was covered by a single DSP but competitive offerings are now increasingly available.\nThe Ground Processing System\nGround System provision is the responsibility of either a participating ANSP or an Aircraft Operator. Aircraft Operators often contract out the function to either DSP or to a separate service provider. Messages from aircraft, especially automatically generated ones, can be pre-configured according to message type so that they are automatically delivered to the appropriate recipient just as ground-originated messages can be configured to reach the correct aircraft."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:b3e89850-5b68-4bf0-9c9f-2b1985080b0c>","<urn:uuid:b908988b-00b8-4b88-ad87-e37aa29f6531>"],"error":null}
{"question":"How does epilepsy impact both physical health and stress levels? 🤔 Really worried about this!","answer":"Epilepsy physically manifests through seizures, which occur when abnormal electrical activity in the brain causes involuntary changes in body movement, sensation, awareness, or behavior. There are different types of seizures, including tonic-clonic (involving loss of consciousness and muscle jerking), focal (causing confusion), and absence seizures (characterized by rapid blinking or staring). When combined with stress, the condition can lead to additional physical symptoms like increased heart rate, rising blood pressure, muscle tension, and upset stomach. Stress can also worsen the overall situation by affecting sleep patterns and potentially triggering more seizures, creating a cycle that impacts both physical and mental well-being.","context":["Epilepsy and Seizures\nEpilepsy is a general term for more than 30 types of seizures. People diagnosed with epilepsy have had more than one seizure, and they may have more than one kind of seizure. About 2.9 million people in the United States have some form of epilepsy.\nA seizure happens when abnormal electrical activity in the brain results in an involuntary change in body movement or function, sensation, awareness, or behavior. It can last from a few seconds to minutes.\nThe signs and symptoms of seizures depend on the location of the abnormal electrical activity in the brain. Some types of seizures may be frightening to onlookers.\n- Tonic-clonic (grand mal): A person having a tonic-clonic seizure may make a sound (due to air being forced out of the lungs), lose consciousness, and fall to the ground. The seizure might cause their muscles to stiffen and jerk.\n- Focal (complex partial): A person having a focal seizure may appear confused or dazed. They might be unable to respond to questions or direction.\n- Absence (petit mal): A person having an absence seizure may blink rapidly or stare into space. This type of seizure may be brief and not noticed by others. These symptoms, sometimes, are the only clue that the person is having a seizure.\nAre you a writer or producer working on a current TV or film project? Contact the program for technical assistance.\nPeople with epilepsy often feel ashamed or different because of their condition. This stigma is compounded by the public’s lack of knowledge and their attitudes about epilepsy and appropriate seizure response. Their unawareness might result in their avoiding people with epilepsy. People with uncontrolled seizures face driver’s license restrictions further limiting their ability to work, go to school, get health care or spend time with friends. These barriers cause adverse professional, social, and employment consequences for people with epilepsy—limiting life opportunities. Those who have seizures only occasionally must decide whether to talk about it or hide it.\nEpilepsy is one of the most common disabling neurological disorders. Under certain conditions, anyone—young or old—can have a seizure.\nSometimes epilepsy can be prevented. The most common ways to reduce risk of developing epilepsy include having a healthy pregnancy; protection from brain injury, stroke, and heart disease; staying up-to-date on vaccinations; handwashing and preparing food safely.\nEpilepsy can be treated. Current treatment methods control seizures for most people with epilepsy. Anti-seizure drugs are the most common form of treatment. When medication is not effective, surgery may be. In children and some adults with certain types of seizures, a special high-fat, low-carbohydrate diet may reduce seizures when other treatments don’t work.\nRead more about epilepsy prevention.\nFirst aid for seizures involves keeping the person safe until the seizure stops by itself.\n- EDUCATE viewers to stay with the person until the seizure ends and he or she is fully awake. After it ends, they should help the person sit in a safe place. Once they are alert and able to communicate, they should tell them what happened in very simple terms.\n- ADVISE viewers to comfort the person and speak calmly.\n- INFORM viewers to check to see if the person is wearing a medical bracelet or other emergency information.\n- REMIND viewers to keep themselves and other people calm.\n- ENCOURAGE viewers to offer to call a taxi or another person to make sure the person gets home safely.\nLearn more about seizure first aid, including what to do in the event of a tonic-clonic (grand mal) seizure.\nAllen, an employee at a marketing company, has epilepsy. His seizures are usually controlled by medicine, but occasionally he blanks out and gets forgetful. One day, Allen forgot to run a routine software back-up for a buyer. As a result, some files were lost during a network outage. The office manager warned Allen that if he can’t pay attention or forgets to run his weekly back-up again, he’ll be fired. Allen is upset and worries he’ll be fired if he tells his manager about his seizures.\nTwo months later, Allen has another seizure which causes another mistake at work. The manager fires him the next day. With nothing left to lose, Allen tells the manager about his condition. The manager listens and understands, and works with Allen to set-up a checklist to help him keep track of his tasks. This allowed Allen to keep his job and feel more comfortable talking about epilepsy.\n- Page last reviewed: September 15, 2017\n- Page last updated: September 15, 2017\n- Content source:\n- Centers for Disease Control and Prevention\n- Page maintained by: Division of Public Affairs (DPA), Office of the Associate Director for Communication (OADC)","How Does Stress Affect The Body: Symptoms And Solutions\nUpdated August 27, 2020\nMedically Reviewed By: Lori Jones, LMHC\nAre you exhausted all the time? Do you have a lot of muscle tension and pain? Is your head or stomach bothering you? It could be stress. There is a long list of symptoms that falls under the question, “How does stress affect the body?” And, learning how to identify the symptoms can help you find the right solution.\nStress levels have been on the rise in Americans over recent years. It’s impacting people of all ages and spans a wide range of worries and concerns.\nThe Impact Of Chronic Stress\nEveryday stress can have a negative impact on multiple areas of your life. However, when the stressful situation passes, you may find that things return to normal even if you didn’t do anything to address your stress. This isn’t the healthiest way to get through stress, but it happens this way for some people.\nHowever, if you’re experiencing chronic stress, it’s not going to just go away. It may not be tied to a specific situation in your life. Instead, it might be the result of poor habits or not knowing how to deal with past trauma. It will not just go away if left untreated.\nThe Effect Of Stress On The Body\nStress can wreak havoc on your body if it’s left unchecked. Not only does occasional stress show up in your body, but chronic stress can also have long-term negative consequences for your physical health. When you are feeling stressed, you may experience:\n- Increased heart rate\n- Rising blood pressure\n- Muscle tension\n- Upset stomach\n- Lack of sexual desire\n- Change in appetite\nAnd these are just a few of the symptoms that you may experience. If you suffer from chronic stress, the symptoms above can start to turn into more serious health consequences.\nChronic stress can lead to health problems such as heart disease, high blood pressure, gastrointestinal problems, heart attack, and strokes, among others. These are clear indicators that allowing chronic stress to continue in your life can be detrimental to your physical health and well-being.\nHow Stress Affects Mental Health\nStress also impacts your mental health and wellness. It can lead to you experiencing many different negatives and difficult emotions such as sadness, anger, frustration, and fear.\nSome of the mental health symptoms that you may notice in your life from stress include:\n- Lack of motivation\n- Irritability and anger\n- Lack of concentration and focus\nThese are serious symptoms that should not be taken lightly. If you experience chronic stress, you may begin to think that these symptoms are just a normal part of life. But, they’re not. All of these symptoms can grow into more serious problems if you don’t work on addressing them.\nHow Stress Affects Behavior\nStress can also impact your behavior. If you look at the symptoms listed above under physical and mental health, it can be easier to understand how stress changes your behavior. If you’re living under constant overwhelm and anxiety and experiencing things like frequent headaches or stomach aches, it can be easy to lose your temper with your loved ones, for example. Here are some of the other behavioral changes that you may experience in your life as a result of stress:\n- Angry outbursts\n- Eating too much or not enough\n- Substance use or abuse\n- Social withdrawal\nThese behaviors can have a negative spiral effect on your life. For example, as your withdrawal from friends and family because of stress, you may find that you struggle even more to cope with stress in your life. This can lead to additional problems which keep you away from a social activity even more. This is why it’s important to learn to recognize and healthily address your stress.\nStress Management Tips To Overcome Chronic Stress\nThankfully there are many things that you can do to address your chronic stress and learn to overcome it. This doesn’t mean that you’ll never experience stress again. Instead, it means that when you do go through stressful situations, you’ll have tips and strategies that you can use to relieve stress and handle it healthily.\nSome of the stress management solutions you may benefit from include:\nLearn to identify your stress triggers\nWhen you start to feel stressed, it can be helpful to take time to identify where the feelings are coming from. This allows you to begin investigating what you can do to make to address it.\nWhile there will be some things causing you to stress that you can’t do anything about, there will be some things that you can address. For example, if a family member’s behavior is causing you to feel stressed, you probably aren’t going to be able to control how they are behaving. But you may be able to establish boundaries in your life that stop the other person’s behavior from having as large of a negative consequence on you.\nThere will be some things that you find are short term stressors. But there also might be habits that you identify that are causing you unnecessary stress. When you learn where the stress is coming from, you can start to take your first steps to address or removing it.\nPractice Deep Breathing\nWhen you’re starting to feel the stress and tension build up within your body, deep breathing can help to break up some of the physical symptoms that you’re experiencing. For example, you may notice that you start to breathe faster as your frustration grows. This can cause your heart to race, as well. And, as your heart beats faster, your blood pressure rises. These physical symptoms can continue to build and even lead to things like full-blown panic attacks.\nDeep breathing can help to stop your physical symptoms from progressing. As you start taking slow, deep breaths in and out, you may notice that it feels like your blood pressure is lowering, and your heart rate is returning to normal.\nYou may also find that deep breathing can help you to slow your thoughts. Your mind will be forced to temporarily shift from your stress and worry to the breathing technique that you’re using. This can help you to regain mental clarity and look for solutions to the stressful situation or problem that you’re facing.\nThere are multiple types of breathing techniques that you can use, so practice a few of them to find what works best for you. It can also help to practice them when you’re not under stress, so when you find your stress starting to build, you will know how to put the breathing exercise to use without too much thought.\nNot getting enough sleep can make it even harder to deal with stress. You may find that you struggle to be patient with others, and you cannot think clearly to look for solutions. If you’re having problems falling asleep or staying asleep due to stress, it’s an important symptom to address.\nMany different things may help improve sleep troubles. A few that you could try include:\n- Keeping a strict sleep schedule\n- Cutting out caffeine\n- Not exercising too close to bedtime.\n- Sleeping in a dark, cool room\n- Using white noise\nHowever, if you’re continuing to struggle, don’t be afraid to talk with your doctor to explore additional options.\nGet More Physical Activity\nPhysical activity and exercise can help you release tension that has built up from chronic stress. It also releases chemicals in your brain that work to boost your mood. But these chemicals also act as natural pain killers, which can help reduce some of the physical symptoms you’re experiencing.\nThere are other ways that physical activity and exercise can help with stress. You may find that you sleep better when you exercise. And, you may experience a boost in your self-esteem as well.\nThe Anxiety and Depression Association of America shares that you may start to experience these positive mental boosts after just five minutes of physical activity. So, if you’re feeling stressed, you don’t need to feel like you have to get in a full workout. Simply getting moving for a few minutes can start to help.\nTalk To Someone\nHaving a trusted person to turn to for support can help when you’re going through stressful situations or experiencing chronic stress. This could be a friend or family member. It could also be a support group. For example, if you’re under stress as a result of losing a loved one, you may benefit from connecting in a group for others experiencing grief from losing someone.\nIf you don’t have anyone to turn to or could use additional support in handling your stress, a licensed therapist is an effective option to consider. Not only can they listen as you talk through the stress in your life, but they also have education on how to help you overcome it. A therapist, like those at BetterHelp, can assist you in finding stress-relieving strategies that work for your specific situation.\nPrevious ArticleHow Stress Can Lead To Emotional Breakdowns And What You Can Do To Avoid It\nNext ArticleAre You Under Too Much Stress? Symptoms, Treatment And Tips\nLearn MoreWhat Is Online Therapy? About Online Counseling\nAbuse ADHD Adolescence Alzheimer's Ambition Anger Anxiety Attachment Attraction Behavior Bipolar Body Dysmorphic Disorder Body Language Bullying Careers Chat Childhood Counseling Dating Defense Mechanisms Dementia Depression Domestic Violence Eating Disorders Family Friendship General Grief Guilt Happiness How To Huntington's Disease Impulse Control Disorder Intimacy Loneliness Love Marriage Medication Memory Menopause MidLife Crisis Mindfulness Monogamy Morality Motivation Neuroticism Optimism Panic Attacks Paranoia Parenting Personality Personality Disorders Persuasion Pessimism Pheromones Phobias Pornography Procrastination Psychiatry Psychologists Psychopathy Psychosis Psychotherapy PTSD Punishment Rejection Relationships Resilience Schizophrenia Self Esteem Sleep Sociopathy Stage Fright Stereotypes Stress Success Stories Synesthesia Teamwork Teenagers Temperament Tests Therapy Time Management Trauma Visualization Willpower Wisdom Worry\nFeeling Overwhelmed? Learn These Stress Management Strategies How To Stop Stressing: 7 Tips To Find Balance And Relax How Stress Can Lead To Emotional Breakdowns And What You Can Do To Avoid It Are You Under Too Much Stress? Symptoms, Treatment And Tips 7 Tips On How To Handle Stressful Situations Stress Management That Works: How To Be Less Stressed"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:925800e0-ce1c-4d08-9a2d-3c32de268470>","<urn:uuid:1fe21db8-8096-462d-a9b3-85cbac331785>"],"error":null}
{"question":"Hey brainiacs! What's the deal w/sampling methods & their accuracy? Like how does sampling without replacement work vs sampling w/replacement, and what kinda errors mess up the results? Need 2 know ASAP!","answer":"Sampling without replacement means the probability of choosing items changes after each draw, and sample size cannot exceed available items. In contrast, sampling with replacement maintains constant probability for each item and allows unlimited sample size. Regarding accuracy, survey errors fall into two categories: sampling and nonsampling errors. Sampling error occurs only in sample surveys and can be quantitatively estimated - it decreases with larger samples. Nonsampling errors include specification error, coverage error, nonresponse error, measurement error, and processing error, which can occur at any stage of the survey process and are harder to measure.","context":["Many simulation and resampling tasks use one of four sampling methods. When you draw a random sample from a population, you can sample with or without replacement. At the same time, all individuals in the population might have equal probability of being selected, or some individuals might be more likely than others. Consequently, the four common sampling methods are shown in the following 2 x 2 table.\nIn SAS, the SURVEYSELECT procedure is a standard way to generate random samples. The previous table names of the four sampling methods, summarizes how to generate samples by using the SURVEYSELECT procedure in SAS/STAT, and shows how to use the SAMPLE function in SAS/IML.\nThe documentation for the SURVEYSELECT procedure uses terms that might not be familiar to programmers who are not survey statisticians. To help eliminate any confusion, the following sections describe the four common sampling methods and the corresponding METHOD= option in PROC SURVEYSELECT.\nSampling without replacement\nWhen you sample without replacement, the probability of choosing each item changes after each draw. The size of the sample cannot exceed the number of items.\nSimple random sampling (SRS) means sampling without replacement and with equal probability. Dealing cards from a 52-card deck is an example of SRS. Use the METHOD=SRS option in PROC SURVEYSELECT to request simple random sampling.\nProbability proportional to size (PPS) means sampling without replacement and with unequal probability. The classic example is counting the colors in a sample of colored marbles drawn from an urn that contains colors in different proportions. Use the METHOD=PPS option in PROC SURVEYSELECT to request PPS sampling and specify the relative sizes (or the probability vector) of items by using the SIZE statement.\n- PPS Example: Use PROC SURVEYSELECT as in this example, but change the option to METHOD=PPS.\n- Example: Use the SAMPLE function in SAS/IML for PPS sampling.\nSampling with replacement\nWhen you sample with replacement, the probability of choosing each item does not change. The size of the sample can be arbitrarily large.\nUnrestricted random sampling (URS) means sampling with replacement and with equal probability. Rolling a six-sided die and recording the face that appears is an example of URS. Use the METHOD=URS option in PROC SURVEYSELECT to request unrestricted random sampling.\n- URS Example: Use PROC SURVEYSELECT (or the DATA step) for URS.\n- Example: Use the SAMPLE function in SAS/IML for URS.\nProbability proportional to size with replacement means sampling with replacement and with unequal probability. An example is tossing two dice and recording the sum of the faces. Use the METHOD=PPS_WR option in PROC SURVEYSELECT to request PPS sampling with replacement. Use the the SIZE statement to specify the relative sizes or the probability vector for each item.\n- PPS_WR Example: Use PROC SURVEYSELECT or the SAMPLE function for PPS sampling with replacement\n- Example: Use PROC SURVEYSELECT (or the DATA step) for generating samples from the multinomial distribution, which is equivalent to PPS sampling with replacement\nThese four sampling methods are useful to the statistical programmer because they are often used in simulation studies. For more information about using the SAS DATA step and PROC SURVEYSELECT for basic sampling, see \"Selecting Unrestricted and Simple Random with Replacement Samples Using Base SAS and PROC SURVEYSELECT (Chapman 2012).\" PROC SURVEYSELECT contains many other useful sampling methods. For an overview of more advanced methods, see \"PROC SURVEYSELECT as a Tool for Drawing Random Samples\" (Lewis 2013).","Appendix | Methodology\nAccurate information is a primary goal of censuses and sample surveys. Accuracy can be defined as the extent to which results deviate from the true values of the characteristics in the target population. Statisticians use the term “error” to refer to this deviation. Good survey design seeks to minimize survey error.\nStatisticians usually classify the factors affecting the accuracy of survey data into two categories: nonsampling and sampling errors. Nonsampling error applies to administrative records and surveys, including censuses, whereas sampling error applies only to sample surveys.\nNonsampling error refers to error related to the design, data collection, and processing procedures. Nonsampling error may occur at each stage of the survey process and is often difficult to measure. The sources of nonsampling error in surveys have analogues for administrative records: the purposes for and the processes through which the records are created affect how well the records capture the concepts of interest of relevant populations (e.g., patents, journal articles, immigrant scientists and engineers). A brief description of five sources of nonsampling error follows. For convenience, the descriptions refer to samples, but they also apply to censuses and administrative records.\nSpecification error. Survey questions often do not perfectly measure the concept for which they are intended as indicators. For example, the number of patents does not perfectly quantify the amount of invention.\nCoverage error. The sampling frame, the listing of the target population members used for selecting survey respondents, may be inaccurate or incomplete. If the frame has omissions, duplications, or other flaws, the survey is less representative because coverage of the target population is inaccurate. Frame errors often require extensive effort to correct.\nNonresponse error. Nonresponse error can occur if not all members of the sample respond to the survey. Response rates indicate the proportion of sample members that respond to the survey. Response rate is not always an indication of nonresponse error.\nNonresponse can cause nonresponse bias, which occurs when the people or establishments that respond to a question, or to the survey as a whole, differ in systematic ways from those who do not respond. For example, in surveys of national populations, complete or partial nonresponse is often more likely among lower-income or less-educated respondents. Evidence of nonresponse bias is an important factor in decisions about whether survey data should be included in Indicators.\nManagers of high-quality surveys, such as those in the U.S. federal statistical system, do research on nonresponse patterns to assess whether and how nonresponse might bias survey estimates. Indicators notes instances where reported data may be subject to substantial nonresponse bias.\nMeasurement error. There are many sources of measurement error, but respondents, interviewers, mode of administration, and survey questionnaires are the most common. Knowingly or unintentionally, respondents may provide incorrect information. Interviewers may influence respondents’ answers or record their answers incorrectly. The questionnaire can be a source of error if there are ambiguous, poorly worded, or confusing questions, instructions, or terms or if the questionnaire layout is confusing.\nIn addition, the records or systems of information that a respondent may refer to, the mode of data collection, and the setting for the survey administration may contribute to measurement error. Perceptions about whether data will be treated as confidential may affect the accuracy of survey responses to sensitive questions, such as those about business profits or personal incomes.\nProcessing error. Processing errors include errors in recording, checking, coding, and preparing survey data to make them ready for analysis.\nSampling error is the most commonly reported measure of a survey’s precision. Unlike nonsampling error, sampling error can be quantitatively estimated in most scientific sample surveys.\nSampling error is the uncertainty in an estimate that results because not all units in the population are measured. Chance is involved in selecting the members of a sample. If the same, random procedures were used repeatedly to select samples from the population, numerous samples would be selected, each containing different members of the population with different characteristics. Each sample would produce different population estimates. When there is great variation among the samples drawn from a given population, the sampling error is high, and there is a large chance that the survey estimate is far from the true population value. In a census, because the entire population is surveyed, there is no sampling error, but nonsampling errors may still exist.\nSampling error is reduced when samples are large, and most of the surveys used in Indicators have large samples. Typically, sampling error is a function of the sample design and size, the variability of the measure of interest, and the methods used to produce estimates from the sample data.\nSampling error associated with an estimate is often measured by the coefficient of variation or margin of error, both of which are measures of the amount of uncertainty in the estimate."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:d780046b-ef1c-4832-8118-8f4e0acc1820>","<urn:uuid:30042e25-3da1-4250-b136-724a5a2482fa>"],"error":null}
{"question":"How much funding did the NSW Government recently allocate for Dr Euan McCaughey's research on electrical stimulation for spinal cord injury patients?","answer":"Dr Euan McCaughey received $2.4 million to evaluate whether electrical stimulation of the abdominal muscles can reduce ventilator dependency time and improve respiratory complications and bowel function in spinal cord injury patients.","context":["Neuroscience Research Australia (NeuRA) has today opened Australia’s new Spinal Cord Injury Research Centre.\nThe Centre will house the latest cutting edge technology and accelerate Australia’s research into better treatment for those with a spinal cord injury.\nAt the Centre’s opening, it was announced NeuRA researchers will receive $6.4 million in NSW Government funding for projects aimed at providing better treatments for people with spinal cord injuries.\nThe Centre’s projects involve using the latest research breakthroughs, such as virtual reality and electrical stimulation to restore feeling, movement and function after a devastating injury.\nNeuRA CEO Professor Peter Schofield AO said the Centre will advance treatment methods and offer new hope to those living with a spinal cord injury.\n“NeuRA is at the forefront of spinal cord injury research in Australia. The Spinal Cord Injury Research Centre and these research projects will dramatically improve Australia’s understanding of how to best treat people living with these life-long injuries,” he said.\n“NeuRA thanks the NSW Government for its funding of NeuRA’s spinal cord research projects. We also are deeply grateful for the tireless advocacy of SpinalCure Australia who have helped fund the new Centre as part of their mission to improve the quality of life for people with a spinal cord injury.”\nSpinal Cord Injury Research Projects funded by the NSW Government include:\n- Therapeutic acute intermittent hypoxia to restore voluntary function after spinal cord injury\nA project led by Senior Principal Research Scientist, Professor Jane Butler, received $1.5 million to advance the effectiveness of therapeutic acute intermittent hypoxia (AIH), which may help people to improve their breathing and recover movement after a spinal cord injury.\nProfessor Butler will study how this therapy affects people with a spinal cord injury to optimise treatment and better predict those who may benefit most from AIH.\n“Therapeutic acute intermittent hypoxia is a cutting-edge treatment that has the potential to restore function to muscles paralysed due to a spinal cord injury by changing the way the brain and spinal cord connect,” she said.\n“Our aim is to identify the best way to apply this treatment clinically in a targeted and tailored manner for people who have chronic and acute spinal cord injuries to improve their quality of life.”\n- Electrical abdominal stimulation to improve breathing and bowel function\nMore than half of the those who experience a spinal cord injury each year experience a condition known as tetraplegia, which often results in them requiring a mechanical ventilator to help them breathe, as well as respiratory and bowel complications.\nSenior Research Scientist Dr Euan McCaughey will receive $2.4 million to undertake a program of work evaluating whether electrical stimulation of the abdominal muscles can reduce the length of time people with a spinal cord injury require the assistance of a mechanical ventilator, and whether this technology can reduce respiratory complications and improve bowel function.\n“People with a spinal cord injury are up to 150 times more likely to get pneumonia than the general public and over half of them have bowel problems” Dr McCaughey said.\n“This program of work greatly expands our previous research and could significantly improve the lives of those living with a spinal cord injury,” he said.\n- Virtual reality treatment to restore touch and feeling in people with paraplegia\nAssociate Professor Sylvia Gustin from NeuRA and UNSW School of Psychology will receive $2.5 million from NSW Health for her RESTORE Project.\nThis project uses virtual reality in a way it has never been used before.\nA/Prof Gustin’s team will develop and test the world’s first immersive virtual reality interface that simultaneously enhances surviving spinal nerve fibres and touch signals in the brain in an effort to help people regain a sense of touch and feeling throughout their body.\nThis project builds on a recent discovery by her team that 50 per cent of individuals with complete spinal cord injury still have surviving spinal somatosensory nerve fibres. Contrary to previous belief, these findings showed that the brain is still receiving messages from areas of the body where the sense of feeling or touch has been lost.\n“It’s very exciting that we can explore how virtual reality can be used to help people regain feeling in their limbs. The outcomes of our research could lead to a cultural and scientific shift in terms of how we treat people with spinal cord injuries, and what they can expect from life after experiencing such a devastating injury,” A/Prof Gustin said.\nAdditional collaboration with UTS: New strategies to control urinary tract infections\nNeuRA Research Fellow and Senior Staff Specialist at the Prince of Wales Hospital Spinal Unit, Dr Bonne Lee, is a co-investigator on a collaborative grant led by A/Prof Diane McDougald from the ithree institute of The University of Technology Sydney (A/Prof Diane McDougald and Prof Iain Duggin) and the Singapore Centre for Environmental Life Sciences Engineering (SCELSE), Nanyang Technological University, Singapore, (A/Prof Scott Rice) has received $2 million.\nThe team will be investigating metagenomics-based diagnostics to look at new strategies to control urinary tract infections in people with a spinal cord injury.\n“Our team is looking at new diagnostic techniques that may help relieve the burden of catheterisation in people with spinal cord injury. Long-term, this work aims to produce better diagnostic decisions in the treatment of urinary tract infections and to reduce antibiotic use,” Dr Lee said."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ec9ed957-0a20-4295-91dd-82a0f1010fb8>"],"error":null}
{"question":"What is the maximum amount covered by FDIC insurance per depositor at Personal Capital Cash compared to a standard FDIC-insured bank account?","answer":"Personal Capital Cash accounts are FDIC insured up to an aggregate of $1.25 million, which is significantly higher than the standard FDIC insurance coverage of $250,000 per depositor, per FDIC-insured bank, per ownership category at regular banks.","context":["Founded in 2009, Personal Capital is among the initial electronic financial investment managers and our ideal robo-advisor for profile administration. Obtained by Empower Retirement in 2020, the firm distinguishes itself with a thorough range of free money management tools for investment management, retirement planning, as well as cash flow. Personal Capital Cash enables any U.S. citizen to access a high rates of interest interest-bearing account with a lineup of cost-free banking services.\nPersonal Capital Advisors is a complete investment manager as well as needs a $100,000 minimum with fees starting at 0.89% AUM. While the free tools appropriate for anyone curious about tracking their funds, greater total assets individuals and also family members will value the customized investment administration solutions together with holistic riches monitoring as well as Certified Financial Planner gain access to Personal Capital supplies. We’ll take an thorough take a look at Personal Capital’s functions to help you determine whether it is a suitable for your monetary requirements.\n- Personal Capital is our best robo-advisor for portfolio administration.\n- Personal Capital offers cost-free net worth tracking as well as retirement preparation software application for anybody.\n- Every paying Advisory customer can access a financial expert.\n- Personal Capital provides extensive tax-minimization techniques including tax-loss harvesting.\n- Stocks and personal equity investments are offered for some Advisor clients.\n- Free money management tools\n- Personalized economic preparation with CFP( R) consultants\n- Obtainable real-time phone client service\n- Easy to use, easy to browse internet site\n- High account minimum of $100,000.\n- High investment administration charge of 0.89% AUM.\n- Calls for a phone call from the firm.\n- Account Minimum$ 100,000.\n- Costs$ 100,000 – $1 million: 0.89% AUM First $3 million: 0.79% AUM Next $2 million: 0.69% AUM Next $5 million: 0.59% AUM Over $10 million: 0.49% AUM.\n- Objective SettingWork with a financial consultant to tailor objectives.\n- Readily Available Assets ETFs, stocks, bonds, personal equity for accounts more than $500,000.\n- Rate Of Interest Paid on Cash Balances0 .75%.\n- Adjustable PortfolioYes.\n- Sight Portfolio before FundingYes.\n- Consumer ServicePhone support and also email.\n- Financial Advisor AvailableYes. $100,000 to $200,000: team of experts. $200,000+: 2 committed monetary experts.\n- Cash Money ManagementYes: Personal Capital Cash.\n- Tax Obligation Loss HarvestingYes.\n- Outside Account Sync/ConsolidationYes.\n- Mobile AppAndroid as well as iOS.\nTo register with Personal Capital you’ll supply an e-mail, select a password, checklist your contact number, and also day of birth. You’re needed to set up a phone call with a monetary consultant whether you have an interest in just the totally free devices or the advising service. You’ll get a brief biography of your possible monetary expert.\nThe next step enables you to link your accounts. This procedure is safe and secure as well as quick . You can add most economic accounts consisting of:.\n- Financial debt.\n- Credit Card.\n- By hand entered classifications.\n- Home Mortgage.\nAfter syncing your accounts, you’ll have accessibility to the cost-free financial devices and reports defined in detail listed below. Anybody can access the free tools, although only those with a minimum of $100,000 certify to utilize the Personal Capital Advisors financial investment administration with monetary coordinators.\nTo register for Personal Capital Advisors, you answer consumption inquiries which cover your economic circumstance, goals, and also risk appetite. Your expert after that evaluates your present financial investments and then offers you with a personal monetary approach. Next, you’ll open your account at custodian Pershing as well as fund it with money or transfer protections.\nEven if you’re using the complimentary devices, you can expect a phone call from the firm. This may be thought about an aggravation by some, yet it’s a little price to spend for access to the robust financial logical software application.\nGoal preparation is distinct at Personal Capital Advisors as well as surpasses marking a box qualified retired life, college planning, or holiday. The goal setting process is customized as well as starts with a conversation with your economic consultant after you’ve responded to the brief survey. Although a lot of investors include retirement as a objective, any type of goal can be taken into consideration such as saving for college tuition or money for a second home.\nThe customizable Personal Capital Retirement Planner is readily available for both paid customers and also totally free devices users. This simulator projects your income and costs during both accumulation as well as distribution stages of life to come up with a affordable assumption of meeting your long-term retired life needs.\nAs your goals as well as scenario develop, your financial consultant exists to direct you and adjust your portfolio as needed.\nPersonal Capital uses banking with no minimum quantity and Personal Capital Advisors provides all the services offered at a full-service monetary planning company. While margin is not offered, two revolving lines of credit report can be accessed.\nThe Financial Roadmap is one-of-a-kind to Personal Capital Advisors as well as takes your account and car loan details to produce a personalized to-do list of economic jobs. Subjects might consist of items like insurance coverage analysis, retired life cash flow preparation, company strategy evaluation, debt management evaluation, and extra. The list evolves along with your specific circumstance.\nPersonal Capital’s financial preparation software application is readily available to anybody who enrolls in the platform, whether they use the Advisory solutions or not. There is no minimum needed to sync your accounts as well as make use of every one of the tools.\nTools and calculators consist of:.\n- Net Worth Tracker.\n- Cost savings Planner.\n- Retirement Planner.\n- Financial investment Check up.\n- Charge Analyzer.\n- Education Planner.\nThe cost analyzer analyzes whether you could be saving money on financial investment fees, while the investment exam analyzes your possession appropriation as well as suggests means to increase returns as well as reduced risks. The retired life coordinator enables you to change inputs as well as examine the success of numerous retired life scenarios. We located the calculators and devices to be incredibly helpful and also, thanks to account syncing, they supplied actionable as well as really customized insights.\nPersonal Capital Cash is a complimentary savings item without any minimums. The program is provided via UMB Bank. After opening an account, your cash is positioned in taking part member banks. The account is FDIC insured up to an aggregate of $1.25 million, well above the conventional $250,000 a solitary bank offers.\n- Readily available functions consist of:.\n- High return interest bearing savings account.\n- Individual as well as joint accounts.\n- Bill pay by connecting to your checking account.\n- Straight deposit.\n- Endless down payments and withdrawals.\nThe Personal Capital Advisors Personal Strategy is their customized financial investment procedure, individualized for each and every customer. The portfolio is driven by the client’s actions to the preliminary survey and also interview with the monetary consultant.\nThe formula creates the portfolio by incorporating the complying with variables right into its calculation:.\n- Threat resistance.\n- Time perspective and also objectives.\n- Health status.\n- Tax obligation legislations.\n- Legacy objectives.\n- Net worth as well as income.\n- Retirement age.\n- Marital status.\n- Expected withdrawal rate, and much more.\nSo Personal Capital takes a extremely detailed and also deep consider your financial resources and specific information when it makes the suggestion. The data synced to Personal Capital and also extracted via the call with the financial advisor provides Personal Capital more to work with than the majority of robo-advisors have, as well as the design this feeds right into seeks to provide even more actual diversity than other robo-advisors with an algorithm based off Modern Portfolio Theory (MPT) alone. The asset allowance used in the Personal method includes differing allotments to the list below asset classes:.\n- U.S. Stocks, consisting of differing styles, dimension, and markets.\n- International Stocks, consisting of established as well as emerging markets.\n- U.S. Bonds, consisting of diverse federal government as well as business of differing credit score high quality.\n- International Bonds of varying credit top quality.\n- Alternatives, including realty and also products.\n- Cash money.\nThe Smart Weighting asset allocation differs from various other automated advisors’ market capitalization possession course weighting, where profile fields reproduce those in market-cap weighted indexes like the S&P 500. The company’s clever weighting equally weights possessions based upon dimension, style, and field . This profile building and construction makes every effort to provide a greater risk-adjusted return than the marketplace cap-weighted allocation.\nInvestors can choose the core profile or a socially liable investing (SRI) option where firms as well as ETFs that represent solid Environmental, social, and also administration (ESG) variables change similar funds and also equities.\nReadily available Assets.\n- Private StocksYes.\n- Shared FundsNo.\n- Fixed IncomeYes.\n- Socially Responsible or ESG OptionsYes.\n- Non-Proprietary ETFsYes.\n- Crypto, Forex No.\nModification is an location where Personal Capital succeeds. Driven by the customer and also monetary expert, the personalization accommodates your tax obligation circumstance and also one-of-a-kind preferences. Clients can ask for unique property appropriations, the socially liable choice, ETFs only in lieu of individual supplies, restricted buy or sell lists, substitute ETFs or stocks, special tax-influenced strategies, and more.\nWith the father of Modern Portfolio Theory, Harry Markowitz, on the board of advisers, it’s not a surprise that profile’s are created along the efficient frontier. Simply put, each capitalist profile is crafted to give the highest returns for the given danger degree.\nThe portfolio administration is created to fit the customers objectives and also preferences. Funds and also stocks are assessed to guarantee that they optimize returns, decrease danger, and also contribute to the portfolio’s diversification. The portfolio is likewise determined for field and nation focus, which adds to the profile’s goal of making best use of returns for any provided risk level. The tax optimization method consists of suitable property area in the most tax-efficient accounts, tax obligation loss harvesting, and selecting tax-efficient funds. Local bond funds as well as private safety and securities can help in tax obligation minimization.\nRebalancing is assisted by modern technology informs which informs supervisors to change the client’s property mix. Rebalancing is triggered when a customer’s monetary scenario modifications, properties wander as well far from objective, or a tactical shift in the economy.\nThe progression towards goals is assessed regularly between the client and economic advisor and changes are made when needed.\n- Automatic RebalancingYes, As required.\n- Coverage FeaturesDashboard, monthly declarations, tax obligation documents, and also custom reports.\n- Tax Obligation Loss HarvestingYes.\n- Exterior Account Syncing/ConsolidationYes-all external accounts consisting of financial investment, custom, financial institution, as well as debt are offered.\nDesktop computer Experience:.\nPersonal Capital’s platform is amongst one of the most user friendly we’ve examined. The navigating is user-friendly with the significant classifications provided on the menu including Banking, Investing, Planning, and also Wealth Management. The Wealth Management vertical is the client’s link to the paid Personal Capital Advisors features with quick accessibility to your advisor and also the academic blog.\nThe summary tab results in a thorough dashboard sight with net worth, accounts, investing, and transactions. The investing and planning classifications consist of every one of the complimentary money management devices. Financial, as one would expect, is the door to Personal Capital Cash.\nThe mobile app is available for iOs and also Android and supplies account management, Personal Capital Cash, and many of the portfolio evaluation tools. Like the desktop variation, customers can track cash flow, financial investments, and also use the retirement preparation functions.\nA couple of desktop functions are doing not have on the app. The Investment Checkup isn’t offered on mobile, as well as individuals that desire to open an account demand to do so utilizing the desktop computer version. On top of that, there are some problems with account sync and also lack of exact existing information reporting, particularly on the android application.\nClient service is above average at Personal Capital. Customers can get in touch with the company by phone and additionally using email. Advisory clients can email and phone their investment representatives to arrange either in-person or online meetings. Additionally, the business has workplaces in California, Denver, Dallas, and Atlanta for in-person economic advisor meetings. The FAQs are well-categorized as well as detailed . Chat is not offered.\n- In-person and also digital conferences with advisors.\n- Physical offices in California, Denver, Dallas, and Atlanta.\n- Comprehensive FAQs.\n- No online chat.\nThe system uses the highest security protocols. Client login information is not available and encrypted to firm employees.\nSecurity protocols consist of:.\n- Two-factor authentication.\n- Firewall softwares and border protection comparable with financial institutions and also settlement cards.\n- 256-bit AES information security.\n- Daily transaction e-mails so users can confirm their own task.\n- Personal Capital brings Securities Investor Protection Corporation (SIPC) insurance coverage approximately $500,000 that includes $250,000 for money. Lots of various other companies supply excess insurance to cover business impropriety and/or insolvency. As is common throughout the sector, there is no sort of insurance policy that can shield your account from losses due to the financial investment markets.\nEducation and learning.\nThe educational portal blog site is called Daily Capital and also includes hundreds of well-written academic posts as well as guides.\nThe subject categories consist of:.\n- Personal Finance.\n- Investing and Markets.\n- Financial Planning.\n- Client Stories.\n- Guides and also Reports.\nA lot of the articles are of the actionable range, addressing typical inquiries like ” exactly how to reduce gross income” or taking market events and dovetailing them right into a favorite topic of diversification or concentration danger. They do, naturally, typically finish the analysis with a plug for the service. The robo-advisor likewise publishes evaluation of the 401K prepares offered by some huge companies like Amazon as well as Walmart that are very interesting. Among this lighter material, nevertheless, are white paper posts that go deeper into the thinking behind Personal Capital’s strategy. Advanced financiers will value the Personal Capital research as well as deeper coverage of investment topics.\nFees & payments .\nPersonal Capital’s finance software program is offered totally free and is well worth looking into.\nPersonal Capital Advisors bills a decreasing charge routine, based upon properties under monitoring. There are no compensations or purchase charges.\n- $ 100,000 to $1 million0 .89%.\n- First $3 million0 .79%.\n- Next $2 million0 .69%.\n- Following $5 million0 .59%.\n- Over $10 million0 .49%.\nThe AUM costs are more than the majority of rivals with economic experts like Betterment, Schwab Intelligent Advisors Premium, and also Vanguard Personal Advisors.\n- Management costs for $5,000 accountN/A.\n- Monitoring charges for $25,000 accountN/A.\n- Administration fees for $100,000 account$ 890.00 AUM each year.\n- Termination feesUnknown.\n- Cost ratios Not published.\n- Common fundsN/A.\nPersonal Capital is ideal for high-net-worth capitalists seeking a well-researched financial investment method, certified economic consultants, and also comprehensive economic preparation. Capitalists that might otherwise employ a full-service conventional economic expert can speak with a Personal Capital financial expert to learn more about their methods to minimize taxes and outperform a traditional market cap weight investment portfolio.\nThe huge account minimum places Personal Capital out of reach for numerous as a robo-advisor. That stated, the cost-free portfolio administration devices are the best we’ve located and also ideal for any investor who seeks to analyze and handle their own investments.\nEvery little thing You Need to Know About Robo-Advisor.\nOur objective at Investopedia is to provide investors with testimonials and ratings of robo-advisors that are thorough and honest . Our group of scientists and experienced authors, led by Michael Sacchitello, spent months examining all aspects of a robo-advisor’s system, consisting of the account setup process, objective planning devices, account solution alternatives, profile building offerings, profile administration, mobile and also desktop computer individual experience, instructional content, fees, and also safety. As part of this examination, we extract vital information factors that are weighted by our measurable version that creates a effective star-scoring system.\nWith the private investor in mind, we’ve made a extensive ranking technique to find the very best overall robo-advisors and the best robo-advisors across nine crucial classifications. Each expert is after that scored across numerous variables to price efficiency in every appropriate classification. Ball game for the total award is a heavy average of the categories.\nTake Pleasure In The Seamless Trading Experience.\nIntrigued in trading cryptocurrencies? Begin Trading CFDs on prominent cryptos such as Bitcoin, Dogecoin, Ethereum, Shiba Inu as well as much more with ease. Capital.com’s prize-winning platform has leading academic products and insights. Profession on 6k+ markets and also 200+ crypto CFD sets as well as access wise danger monitoring devices. 0% fees as well as affordable spreads. Start with a FREE trial account. When trading CFDs with this supplier, 83.45% of retail capitalist accounts lose money.\nPersonal Capital Cash allows any kind of U.S. local to access a high interest rate savings account with a lineup of totally free banking solutions.\nPersonal Capital uses banking with no minimum amount and also Personal Capital Advisors gives all the solutions offered at a full-service financial planning company. The Financial Roadmap is distinct to Personal Capital Advisors as well as takes your account as well as funding info to develop a personalized to-do checklist of financial tasks. The data synced to Personal Capital as well as attracted out via the phone call with the monetary advisor provides Personal Capital more to function with than a lot of robo-advisors have, and the design this feeds into looks to offer more actual diversity than various other robo-advisors with an algorithm based off Modern Portfolio Theory (MPT) alone. The mobile application is offered for iOs and Android and also supplies account monitoring, Personal Capital Cash, and many of the portfolio evaluation tools.","The Federal Deposit Insurance Corporation (FDIC) was formed in 1933 as part of the Banking Act of the same year. The formation of the FDIC was in response to the many banks that failed during the Great Depression.\nThe FDIC became an independent government corporation through the Banking Act of 1935.\nThe role of the FDIC is to cover FDIC-insured bank deposits, up to a limit, in the event of a bank failure.\nThis is what you know it for - it's supposed to safeguard your money if a bank goes under. FDIC Deposit Limits have become a popular topic with the rise of online banks and hybrid checking bank accounts. Is your money secure? Here's what you need to know!\nFDIC insurance covers deposits at banks that have FDIC insurance. From the FDIC website, deposits include the following:\n- Checking accounts\n- Negotiable Order of Withdrawal (NOW) accounts\n- Savings accounts\n- Money Market Deposit Accounts (MMDAs)\n- Time deposits such as certificates of deposit (CDs)\n- Cashier’s checks, money orders, and other official items issued by a bank\nStandard FDIC deposit insurance includes coverage up to $250,000 per depositor, per FDIC-insured bank, per ownership category. This limit applies to the total for all deposits owned by an account holder.\nIf you have multiple accounts, they are added together and insured to the limit. For example, if you have a $100,000 account, a $150,000 account, and a $50,000 account, equaling $300,000 in total, only $250,000 of this total will be insured.\nIf you have a joint account (with, say a spouse), your limit is combined - so $500,000.\nFDIC insurance does cover earnings on deposits, assuming the overall account value does not exceed the $250,000 insurance limit. If you have $200,000 in an account that has earned $5,000, the full $205,000 is insured since it does not exceed the $250,000 limit.\nTo better understand the various scenarios that deposits are covered under, check this interactive graph provided by the FDIC. Additionally, you can use the FDIC Electronic Deposit Insurance Estimator (EDIE). The EDIE lets you input your specific deposits and shows amounts that are FDIC-insured.\nTo receive FDIC insurance, you simply need to deposit funds with an FDIC-insured bank and one of the account types mentioned above. No application needs to be filled out to receive coverage.\nIt is critical that the bank has FDIC insurance to receive any coverage. To find out if a bank has FDIC insurance, check that the FDIC seal is present, which is usually on the bank’s door, or ask a bank representative. You can also use the FDIC’s BankFind tool.\nWhat Isn’t Covered\nOnly deposit products are covered by FDIC insurance. The following account types are not covered:\n- Stock investments\n- Bond investments\n- Mutual funds\n- Life insurance policies\n- Municipal securities\n- Safe deposit boxes or their contents\n- U.S. Treasury bills, bonds or notes\nYou can see a comparison chart here. What does this mean for your 401(k)? This will depend on what’s in the 401(k). Is there a cash or money market component to the 401(k)? If so, those assets may be covered but check with your 401(k) administrator to be sure (and they may be covered by another organization called the SIPC).\nFor those with over $250,000 of eligible funds, leaving them at one bank will exceed the insurance limit. But there are ways around the limit. Most involve distributing funds to multiple banks, each with a maximum of $250,000. This means someone with $1 million can distribute funds across four banks to receive full coverage.\n“If you wanted to do that directly at a bank, you’d have to set up differently titled accounts or have your funds literally placed in different banks,” says Erik Lind, vice president of cash management products at Fidelity Investments.\n“A more convenient way to gain the expanded coverage may be to open one account at a brokerage provider that can automatically cascade your assets throughout its bank network coverage, rather than having separate deposits in a bank or multiple banks,” says Lind.\nThese accounts are called cash management accounts, and Fidelity offers a great one that we list on our list of the best free checking accounts.\nWhat Happens to My Money When a Bank Fails?\nWhile rare, banks do fail. If an FDIC-insured bank fails and your money is in an insured account, rest assured that you are covered up to $250,000. Accountholders are insured dollar for dollar. Depositors are usually paid their insurance within only a few business days after the bank’s closing and often by the next business day.\nThere are two ways in which depositors can receive insurance when a bank fails:\n- The FDIC moves all insured accounts within the failed bank to another FDIC-insured bank.\n- The FDIC sends accountholders a check equivalent to their insured account value.\nThere can be delays for some of the more complex account types such as trusts and accounts opened by a third-party broker. These accounts need further review to determine how much is insurable.\nThe FDIC will become the receiver of a failed bank and sell off the bank’s assets. For depositors that have account values in excess of $250,000, proceeds from the bank’s assets are used to pay back uninsured funds. Although, it can take years to sell all of a failed bank’s assets. It is also unlikely those depositors will receive 100% of their uninsured funds.\nWhen checking out a new banking product, make sure that you understand whether your deposit is insured or not. We rely on banks to keep our money safe. This is how they are different than an investment product.\nYour bank account should not lose value as the result of the organization's management. That's why the FDIC exists. Make sure you know if your money is protected."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:62eaa020-4098-450d-83b4-9f7422dff748>","<urn:uuid:d8521166-ba8d-4cc4-b823-5ee7ead243e3>"],"error":null}
{"question":"What are the artistic techniques used for Mother's Day crafts, and how can recycled materials be incorporated into these projects?","answer":"For Mother's Day crafts, several artistic techniques can be employed, including wet-in-wet watercolor painting for butterfly designs, ensuring symmetrical wings by drawing on folded paper, and decorative painting using stick painting techniques inspired by Aboriginal art. For surface decoration, tools like dowel rods, cotton swabs, or mat board edges can be used. As for incorporating recycled materials, items like old picture frames can be transformed into trays using papier mache made from newspaper strips, and juice bottles can be repurposed for making recycled paper. The recycled paper can be enhanced with various materials like glitter, foil, candy wrappers, flowers, herbs, beads, or ribbon.","context":["Mothers Day Crafts\n1. You can use your imagination to create a fantasy butterfly, as we did, or study photos of butterflies to learn how to make more realistic insects. Butterflies occur in a variety of shapes and colors, and their wings have beautiful markings. One thing they have in common, however, is the symmetry of their wings. That is, the size and shape of one side or half matches the other.\n2. The easiest way to make sure that your butterfly will be symmetrical is to draw one-half of the insect on a folded piece of paper, with the body touching the fold. Keeping the paper closed, cut out the wings. Trace this shape on cardboard, and cut it out.\n3. Our fantasy butterfly was made by first painting a wet-in-wet watercolor. Before starting, protect the table with a layer of newspapers, and place a sheet of wax paper on top. Put a piece of rice paper on top of the wax paper, and soak it with water. Paint the paper, letting the colors flow into each other. Be sure to use lots of paint, because watercolor tends to dry lighter than it appears as you work.\n4. When the paper is dry, turn it face down, and trace the cardboard butterfly. Now lay the pattern on another area, trace it again, and add a margin of at least 1/2\" all the way around the shape. Clip the margin at 1/2\" intervals up to the traced shape. Put a thinned coating of glue on the pattern, and paste it to the wrong side of this paper. Now, one at a time, put glue on each tab and fold it over so that you cover the edge of the cardboard. Glue the other piece of painted paper to the bottom of the butterfly. Gently shape the wings while the cardboard and paper are damp.\n5. To make the body, paint the clothespin black. When it's dry, make a hole in the underside with a drill or awl, and slide it over the wings. To keep it in place, pack the underside opening of the pin with small scraps of cardboard. Choose a dark colored pipe cleaner for the antennae. Fold it in half, and twist the center around the head of the clothespin to fasten it. Shape the antennae.\n6. After the butterfly is finished, you're ready to attach it to the base with the coat hanger. Decide how tall you want the mobile to be, and use pliers to remove the hook portion and some additional length from the hanger. If you wish, cover the wire with floral tape, and bend it into a spiral or zigzag shape. Insert one end of the wire in the body, and staple the other end onto the block. Paint the wood a color which complements the butterfly, and if you wish, decorate the base with dried moss and flowers. Place the butterfly in a breezy area, and surprise your mom for Mother's Day!\nMake a Tray for Mother's Day\nIf you'd like to make mom a special gift, you can recycle a small picture frame to make an attractive tray. It will be just the right size to keep small items organized on her vanity.\nWe'll make the tray by using papier mache, a technique in which strips of newspapers are pasted onto the frame. For centuries making paper was limited and expensive, so recycling the material to make new products was a practical application of papier mache.\n1. Making a small tray is a great way to recycle an old or damaged picture frame. Since the entire frame will be covered, no one will notice the flaws. If you don't have a frame available, look for one at garage sales or try your local thrift store. If the glass is still in place, have an adult remove it and set it aside. Measure the rabbet or groove inside the frame, and cut pieces of mat board to fit. You'll need many pieces, because it will be necessary to fill the frame from front to back. Measure the back of the frame and cut one larger piece of mat board to cover it.\n2. Glue all the smaller pieces of mat board together, and put rubber bands around them to hold them in place for a few minutes. Remove the bands, and glue the boards inside the frame where the glass would normally fit. If necessary, add more mat board to fill the frame. Finally, glue the larger sheet of mat board onto the back of the frame, and add some strips of masking tape all around to help hold it in place. Protect the table with newspapers, and work on a sheet of wax paper to prevent gluing your project to the newspapers.\n3. Mix the wallpaper paste according to manufacturer's directions. Tear the newspaper into small squares or short strips, and begin pasting them to the tray form. Dip the paper into the mixture, and remove the extra paste by running the strip between your fingers and thumb. Apply one layer over the entire tray, front and back. When dry, apply a primer coat of gesso to the sides, top, and inside of the form. It's not necessary to prime the back, because it will be covered with felt.\n4. Painting a design on the tray is the fun part of this activity. You can paint the entire form one color, and then apply surface decoration on top of this base coat. To make the tray more interesting, however, you might want to use several colors. Before you start, look at the form to see if there are any definite shapes or borders suggested by the picture frame beneath the paper surface. You can paint the large area in the center one color and paint the frame portion one or more colors, for example. Also, paint a small strip of color all around on the back of the tray to hide any paper which the felt may not cover.\n5. After the base coat is dry, you're ready to apply the surface decoration to the entire frame. Rather than painting something realistic, consider using a design. It's easy to get ideas on good design elements by looking at patterns in printed fabric and paper such as those found in drapery and wallpaper. Another way is to study other cultures and use their painting techniques to inspire your work. Can you imagine painting without a brush? The aborigines of Australia have been painting beautiful dot designs with sticks for thousands of years!\n6. You can try your hand at stick painting by using short lengths of dowel rods, cotton swabs, or similar materials. To begin, look at the tray to see if there are any \"natural\" shapes or borders that could be decorated by stick painting. Dip the stick into a color, and apply it to the tray. Repeat until this area is covered with a design. Another way to decorate the surface is to dip the edge of a small piece of mat board into paint and apply it to the form. When you've finished painting, set the tray aside to dry. Complete the project by attaching felt to the back with fabric glue.\nPotholder for Mother's Day\nThis would make a sweet gift for your darling mom.\nWhite pot holders\nColored Stick Ons\nFlower Ribbon Pin Craft\nRibbon - 1 ½ inches wide\nGreen chenille stems\nPompoms or fun foam.\nBouquet in Hand Craft\nThis is a simple hand craft item which can really create a good piece with the use of paint, markers or construction paper.\nLight-colored construction paper for the background\nSkin colored construction paper for hand","Large mouth juice bottles are the best way to start your recycled paper project. Add shredded paper until the bottle is ¾ of the way full of paper.\nFill the bottle just over half way with warm water.\nMeanwhile, make sure that a workspace and supplies are ready. I spread out thick layers of denim fabric to help soak up water.\nScreens made from simple wooden frames screwed together with screen stapled on the side are all I use. Eric and I put a whole class set together in a matter of a few hours. These are placed over buckets that just fit the screen. In a class of about 15, I used three screens and it seemed to work out that there was always a screen available.\nI keep plenty of bottles and extra caps available. That way, if a student needs more time the next class period, he/she can simply use masking tape and a marker to label his or her bottle.\nThis is the bottle and its contents after about 10 minutes of shaking. The paper starts to break down into paper pulp quite quickly. The partners work well because the triceps may start to get a little tired – especially if the bottle is fuller.\nIf you like, you can speed up the process with a blender. I have a few donated old blenders, but I have found that the cheap brand new ones actually work the best.\nCover tightly. I keep plenty of rags and a few Sham Wows around to mop up spills. I used this time to teach a little science to the kiddos about water and electricity not mixing. Blend until smooth.\nTo change the color of the paper, you can use a variety of methods – dye, colored paper, or plants. Here, I used black construction paper. This is a great time to revisit the color wheel and the mixing of colors. We revisit the color wheel and I always remind my students that if you mix all the colors of shredded paper together you will get mud. Still, there are always a few who complain about their ‘vomit’ paper.\nEmbellishments are another fun way to make recycled paper more interesting. A few things to add but maybe not all at once: glitter, foil, candy wrappers, bits of flowers or herbs, small beads, or cut up ribbon or string. Here, I added strips of aluminum foil with the paper pulp.\nI requested my colleagues save foil chocolate wrappers for me – a great time to ask is before Christmas, Halloween or Valentine’s Day. I was always delighted to find baggies of foil wrappers in my box – I am sure some colleagues think I am quite the pack rat! When we start making paper the kids inevitable want to know if I like chocolate. 🙂\nAfter your paper is blended into a smooth pulp, pour it into a screen set up in one of the bins. Gently press some of the water out into the bucket below. Be careful not to press too hard, or your screen could come detached from the frame.\nPress out as much remaining water as possible on a flat surface, such as the denim fabric, felt, or newspaper.\nPress even more water out.\nThis would be a good time to say that making your stamp before you start your paper making is smart planning. Create embossing stamps out of Styrofoam. I like to rescue clean trays from the cafeteria to repurpose into stamping materials. These two stamps were simply drawn with ballpoint pen and then cut with scissors. I pressed them into the top of the wet paper.\nA simple, yet interesting, relief stamp can be made from multiple layers of Styrofoam, such as this smiling face. Cut out the shapes, glue, and allow to dry before pressing into the wet paper.\nI have found that the stamps work best if the stamps are placed on the fabric and then the paper is turned on top of the stamps. This way, you can press the paper onto the stamps and create a better impression.\nOne of my more favorite embellishments can be found in your kitchen. Onion peels – yellow or red can be added to paper pulp for an interesting dye technique.\nWhen initially blended, the color appear quite faint.\nOnce the paper is fully dry, the onion color shows up much more vibrantly. These are two examples of the the embossing stamps once dry.\nThe possibilities for recycled paper projects are endless. Here are a few ideas:\n- Paint them for more depth.\n- Use recycled paper in place of plaster or clay for hand prints.\n- Turn them into greeting cards.\n- Make recycled jewelry.\n- Make recycled paper bowls.\n- Turn two sheets into the front and back of a book.\n- Use the paper pulp as a sculpting material, make a 3D object, and then brush with watered down glue to hold it together.\nCheck back soon for more projects involving recycled paper."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:50b738c8-e067-4b4d-afb4-d7235f11cfca>","<urn:uuid:4498195d-34a2-4bb4-af22-e0b6ae3071d4>"],"error":null}
{"question":"If we compare the diversity of participants in early baseball games with the biodiversity found along the Mississippi River, which shows greater variety? 请比较这两个环境中的多样性。","answer":"The Mississippi River ecosystem shows greater diversity. While baseball had impressive variety in its participants - including teams of police, firemen, bartenders, dairymen, teachers, physicians, Chinese teams, Native American teams, women's teams, and even teams of amputees - the Mississippi River's biodiversity is even more extensive. The river system supports 260 fish species (25% of all North American fish species), 326 bird species (60% of all North American birds), 38-60 mussel species, over 50 mammal species, and at least 145 species of amphibians and reptiles. Additionally, 40% of the nation's migratory waterfowl use the river corridor during migrations. This natural diversity along the Mississippi significantly exceeds the human diversity seen in early baseball, despite baseball's notably inclusive nature for its time.","context":["The birth of a sport and a pastime\nExcerpted from A Clever Base-Ballist: The Life and Times of John M. WardBY BRYAN DI SALVATORE\nBaseball spread across America like the smell of fresh-baked bread. By 1856, the most serious New York metropolitan teams played 53-game seasons lasting from March to November. There were perhaps 50 teams in Manhattan, as well as another 60 youth clubs.\nBy 1859, there were 130 teams in New Jersey, including 78 in Newark and Jersey City alone. In 1858, 1500 people paid a hefty 50 cents each (a good part of a laborer's day's wages) to watch a game between New York and Brooklyn played on a Long Island race track.\nPolicemen formed teams. Firemen formed teams. Bartenders formed teams. Dairymen formed teams. Schoolteachers formed teams. Physicians formed teams.\nThere were teams in Syracuse and Albany and Buffalo and Philadelphia and Washington D.C. and Detroit and San Francisco and Los Angeles and Minnesota and Maine and the Oregon Territory and New Orleans. New England proper was playing a variation of baseball-the \"Massachusetts\" game-that allowed among other differences, \"soaking,\" that is, putting a runner out by hitting him with a thrown ball. The Massachusetts game's proponents struggled valiantly, but their version withered away.\nThe Civil War generally spread the baseball's gospel and temporarily stunted the game's growth. The U.S. Sanitary Commission sanctioned the game, seeing it as a morale booster. Idle soldiers of both armies as well as their prisoners of war played the game whenever possible.\nAfter the war ended, the game's glowing embers blew into high flame.\nUp through the end of the century-by which time the preposterous popularity of the game settled into merely steady and more rational growth-people jumped on the game like people do the Internet today. Baseball was the country's computer network. You could love it, hate it or not give a damn about it, but you could barely get through a normal day without hearing about it.\nAllopaths played homeopaths; fat men (Jumbos) played lean men (Shadows); bachelors played benedicts; maidens played \"matrons\"; Yale University and other colleges played professional teams; the New York Herald played the New York World; New York bartenders played Providence bartenders; New York post office clerks played Park National Bank clerks; lawyers played doctors in Mexico City; actors played lawyers; syrup brokers played sugar brokers; the KKK played \"nine carpetbaggers\" in Fayetteville, Tenn.; baseball players played cricket against cricket players and cricket players played baseball against baseball players; telegraph operators played railroad accountants; railroads played other railroads; people played games in lakes with fielders in knee-deep water and the pitcher and batter standing waist deep; people played on ice skates and roller skates and bicycles and horses and snowshoes; Chinese teams played each other; in California, Digger Indians played each other; the Dolly Vardens- \"artistically dressed ladies of color\"-played each other in \"red and white calico dresses of remarkable shortness\"; Negroes played each other; in Macon, Georgia, white prisoners were put in a special room to watch ball games in adjoining fields.\nWhen people got married, the headlines sometimes read that the couple had \"doubled up\"-like oxen or unlucky baserunners; in 1883, in Fort Wayne, two teams played a game under lights; two thousand Philadelphians watched a game played indoors in 1888; fifteen hundred people watched an 1883 game between blondes and brunettes; women played women all the time, thereby allowing sportswriters to write things like, \"With the exception of a mouse, there is probably nothing more dangerous than a base-ball flying through the air directly at some unprotected girl.\"\nBy 1888 people were playing \"old-timer's\" games, under the rules of 1868. In what may be the weirdest game of the century, two teams \"made up entirely of maimed men\" played each other: the Snorkeys (each with one arm) vs. the Hoppers (each with one leg). The New York Times could not withhold its admiration for these men, pointing out that all the members of both nines, \"excepting Fay, Connors, Cahan and Bogan,\" were married, with children.\nPeople were constantly trying to improve the game: one man posited that, instead of runs, the winner should be determined by the total number of bases reached by each team. Another futile campaign-this one by the century's most famous baseball writer, Henry Chadwick-posited that, since players in those days tended to stay close to their respective bags, the presence of a shortstop made the field uneven. Chadwick suggested a second shortstop, between first and second base. Bats flattened on one side were popular for years.\nIn 1885, W. Williams wrote to the weekly Sporting Life with a list of his inventions, each of which, he insisted, would \"transform\" the game. One was a paper bat, which, Williams said was \"more durable, of greater diameter of batting surface, of less weight, more elasticity and more friction on the surface and less affected by water\" than any other bat manufactured in the country. His second idea was an electric scoreboard; his third, fourth and fifth ideas were electric signal systems that would alert the umpire, without the slightest inaccuracy, whether a ball was in the strike zone, whether a pitcher stepped out of his box, or whether the runner or the ball reached the base first.\nNewspapers began running scores and game reports and, after they were invented by Chadwick, box scores. There were weekly sporting papers: the Spirit of the Times; the New York Clipper; Sporting Life; The Sporting News; the American Chronicle of Sports and Pastimes, as well as annual baseball guides and instruction books. There were baseball board games and baseball trading cards.\nThere grew up a huge sporting goods industry. The avid fan or player could buy any of seven types of scorebooks from the Spalding Company alone, ranging in price from ten cents to $3. Bats (15 cents-60 cents) came in polished and unpolished models, plain or oiled, trademarked or plain, in ash, cherry, basswood, willow or ash-and-willow models, for boys or for men.\nUniforms went for anywhere from $7 to $20 in an astonishing variety of models. There were many different models of catchers' gloves and, after 1875 or so, catchers' masks, and web belts and stockings and shoes and spikes and caps. Spalding alone offered eleven different grades of balls: all the way from the \"Official League Ball\" for $15 a dozen, down through the King of the Diamond, the Grand Duke, the Eureka, the Rattler, the Boss, to the lowly Nickel ball.\nPeople had great fun naming their teams, just like musicians have fun naming their bands: there were Live Oaks, Joyfuls, Mutuals, Athletic, Browns, Standards, Alerts, Ironside, Domestic, Defiance, Dauntless, Acmes, Resolutes, Actives, Monumental, Experts, Quicksteps, Crickets, Flyaway, Redemptor, High Boys, Young America (which was a slogan of the Democratic party), Asteroid, Mentor, Forester, Dreadnoughts, Romance (of Rome, N.Y.), Nameless, Lifeguard, Amateurs, Misfits, the Nine Orphans, Black Nines, Ebony Nines, Colored Mutuals and Buckeyes.\nThere were, in 1883, over one hundred different teams named the Oscar Wildes.\nMissoula-based writer Bryan Di Salvatore has written for The New Yorker, Outside and many other national magazines. Known to some as Cine-Man, Di Salvatore's first book, A Clever Base-Ballist, is due out February 1999.\nCanoeing a grand canyon, a trip down the Green River\nSTORY AND PHOTOS BY TIM WESTBY\nI grew up in the southern Arizona desert and the combination of free-flowing water and Southwestern landscapes-occuring so rarely together- has always called to me.\nIn 1996, just before I moved to Missoula, my father and I took a 120-mile canoe trip down the lower Green River in eastern Utah. In my parents' Mad River fiberglass canoe, we paddled from the town of Green River to the confluence of the Colorado and Green deep in Canyonlands National Park.\nThe lower Green cuts through some of the remotest and most pristine desert country left in the U.S., offering one of the longest backcountry wilderness canoe opportunities in the Lower 48.\nIn six days on the river, we saw only a handful of people. And the canyons of Utah present an intimacy that the postcard vistas of the Grand Canyon (I dare say) can't match. Since that trip, I have longed to return to that aquatic anomoly in the desert.\nA year ago, over the phone, I bought sight unseen a battered 16-foot rental canoe from an outfitter in Moab. Then I set about persuading my friend Karen Barton into fitting such an excursion into her hectic schedule.\nThe simple act of paddling with another person, slicing through water effortlessly and almost silently for an extended trip, can create an endurable bond. And so, last month-with our respective universities holding their spring breaks-Karen and I rendezvoused in Moab to canoe that ribbon of green running through the unocuppied canyons of Utah.\nThe morning is cloudless and warm. A fine day to start a week of paddling. A group of four kayakers are putting in at the same time as us. They are talkative and friendly, but Karen and I don't say much.\nWe both just want to be gone.\nCrystal Geyser is the remnant of an oil-test well drilled in the mid-1930's. It erupts about every 14 hours and does so while we're loading, shooting a column of odoriferous water high into the air. I'm too restless to pay much attention.\nThis first day, the river winds through flat, sagebrush desert. The corridor is choked with pesky exotic tamarisk, a stout green leafy bush lining the river like an imported jungle. We spend the day intermittently paddling, drifting and talking.\nI spot a dead cow lying half submerged. Karen wants to get close to it until the stench of decay overtakes us.\nWe camp above Ruby Ranch, the last chunk of private land on the river. We hike the nearest hill, truly one big rock which overlooks an old mining site, and watch the sunset. A Great Blue heron stands tall and stock still on a sandbar, looking as elegant as a champagne glass.\nIt's cold, and we linger in the warmth of multiple layers of clothes and sleeping bags. For reasons not at all clear this morning, I insisted on bringing the dry bags containing our clothes into our cramped tent the night before. When Karen wakes up, she starts where she left off the night before with a barrage of wise cracks about our crammed sleeping quarters and my analness.\nWhile she fixes breakfast, I take down the tent and break one of the three tent poles in the process. With just two poles, the tent has a lopsided stance like a house that's been rocked from its foundation, but it's still functional. I tell Karen she'll have to sleep outside from now on because there isn't room enough now for both of us and the dry bags.\nOn the river, still on different wave lengths, we paddle out of synch. We enter Labyrinth Canyon, which will continue for the next 62 miles, and come upon a gaggle of Canada geese. A handful honk and squawk as we approach and then take flight. They land several hundred yards down river only to repeat this escape process three more times as we approach. Finally the geese figure out that flying toward us will actually get rid of us.\nRounded canyon walls of Navajo Sandstone rise up around us like sand dunes frozen into stone. They are more yellow than red and modest compared to what we will see later. I bring the canoe up close to the cool rock surface.\nKaren, already awed like a little kid by the scenery, asks, \"Will the canyons get three times as high as this?\"\nWind whistles through a side canyon called Hey Joe. I look back to watch the placid surface of the river turn into windswept ripples hundreds of yards upstream. The wind and then the ripples overtake us in minutes.\nI look upstream again and see rain causing the water to boil. We've been moving along the western edge of the river, but the shoreline is so thick in tamarisk that we can't find a place to pull out. We decide to make for the other side where an old mining road lies just above the river. We pull hard.\nHalfway across the rain reaches us and a flash of lightning seems to surround us. We pull harder. I imagine the canoe rising above of the surface from our strong stokes.\nThe rain stops as soon as we stumble our way up to the road. We walk for a mile looking for an appropriate campsite. The rain will return. We don't find one, but we do come across an intact Bighorn sheep carcass picked clean. When the sun comes out we decide to make for a campsite I vaguely remember a couple miles downstream.\nWe find the spot just before raindrops start smacking us again. I cook dinner in the rain as Karen attempts to make our dysfunctional tent rain proof by lining the bottom and sides with garbage bags. She sticks her head out and asks jokingly how I can think of food at time like this.\n\"My stomach comes before all else,\" I respond.\nInside the tent we huddle in the center, rain pounding our enclosure as the wind swirls and screams in nearby slot canyons. Karen cuts the tension with another barrage of smart-ass comments over our predicament. We pass the time eating, talking and giving each other back rubs to ease the aches from three days of paddling the river.\nThis is the halfway point of the trip in terms of days, but we're only little more than a third the distance. We spend the day paddling hard to catch up.\nWe take a break to climb a ridge at Bowknot Bend. The river sweeps back and around in a loop flowing seven miles around this bend, but advancing only a half-mile as the crow flies. Atop the 800-foot high saddle, we look down to where the canoe is tied. On the other side, we can see a section of river it will take us two hours to reach by water, but we could climb down to in 15 minutes.\nAlthough we're seeing more people than my dad and I, we still feel like we have the river to ourselves. On the water again, the canyon walls rise as high as 1,300 feet above us. Rosy rocks the size of apartment buildings loom overhead. Later, we enter Canyonlands National Park.\nWe see one, maybe two, other groups a day. And when we do, we avoid all contact, choosing instead to protect our solitude at the expense of social niceties.\nI wake early and stumble out of the tent. The ground is speckled with a hard frost. I stand, look around, curse the cold and crawl back into the relative warmth of my sleeping bag. Karen, sleeping in six layers of clothing, a knit hat and a cocoon bag, mumbles in her sleep. I drift in and out of consciousness thinking up secrets to tell her she's been revealing.\nThe day turns out warm-almost hot-and we fall immediately and effortlessly into the synchronized paddling rhythm we've been striving for. We pull out at Fort Bottom, named for an Indian lookout on a pinnacle above. We hike up to it, stopping along the way at an old cabin said to have been used by Butch Cassidy.\nLate in the afternoon, we take a break from paddling. The water swirls and boils with small dimples around us. The relaxed current has picked up a notch or two as we near the confluence. Our paddling slows. It's not just fatigue.\nWe both know the end is near, and we're not quite ready for it.\nAnother mile or so we'll be at the confluence, having done close to 30 miles today and 116 over the last six. We'll drift into the confluence like a race car driver taking a victory lap. Half a mile or so down the Colorado we'll camp. We'll fix a dinner of black beans and rice, get drunk on a bottle of Zinfandel and a flask of Maker's Mark and sit around a fire talking like two old friends who haven't seen each other in years even though we've been inseparable all week.\nTomorrow we'll be picked up by a jet boat for a three-hour ride back to Moab, where we'll spend our time in a motel room because we feel too awkward being around humanity again. The day after that we'll get into our respective pickups and drive off in opposite directions.\nRight now, drifting on the river in a chilly dusk-tinted light, none of that matters. In the long shadows that come early to these canyons, it's that moment of the day where time feels like it's taking a breath and things will never change.\nEventually, I dip my paddle into the metallic water. Karen follows suit. We say nothing. The only sound: our paddles breaking the surface and coming back out. I watch Karen watch the arc of water that drips from her paddle every time she lifts it from the river.\nWe fall into rhythm, pushing to the end.\nTaking a break from paddling, the canoe filled with gear, Karen (shown here) and Tim had the river to themselves.\nThe view from Fort Bottom, 40 miles from the confluence of the Colorado and Green rivers in Canyonlands National Park.","Mississippi River Facts\nMississippi River Overview\nThe Mississippi River is one of the world’s major river systems in size, habitat diversity and biological productivity. It is the second longest river in North America, flowing 2,350 miles from its source at Lake Itasca through the center of the continental United States to the Gulf of Mexico. The Missouri River, a tributary of the Mississippi River, is about 100 miles longer. Some describe the Mississippi River as being the third longest river system in the world, if the length of Missouri and Ohio Rivers are added to the Mississippi’s main stem.\nWhen compared to other world rivers, the Mississippi-Missouri River combination ranks fourth in length (3,710 miles/5,970km) following the Nile (4,160 miles/6,693km), the Amazon (4,000 miles/6,436km), and the Yangtze Rivers (3,964 miles/6,378km). The reported length of a river may increase or decrease as deposition or erosion occurs at its delta, or as meanders are created or cutoff. As a result, different lengths may be reported depending upon the year or measurement method.\nFor reasons mentioned above there are competing claims as to the Mississippi’s length. The staff of Itasca State Park at the Mississippi’s headwaters say the main stem of the river is 2,552 miles long. The US Geologic Survey has published a number of 2,300 miles, the EPA says it is 2,320 miles long, and the Mississippi National River and Recreation Area suggests the river’s length is 2,350 miles.\nAt Lake Itasca, the river is between 20 and 30 feet wide, the narrowest stretch for its entire length. The widest part of the Mississippi can be found at Lake Winnibigoshish near Bena, MN, where it is wider than 11 miles. The widest navigable section in the shipping channel of the Mississippi is Lake Pepin, where it is approximately 2 miles wide.\nAt the headwaters of the Mississippi, the average surface speed of the water is about 1.2 miles per hour – roughly one-third as fast as people walk. At New Orleans the river flows 3 miles per hour on average.\nMississippi River Watershed\nThe Mississippi River watershed is the fourth largest in the world, extending from the Allegheny Mountains in the east to the Rocky Mountains in the west. The watershed includes all or parts of 31 states and 2 Canadian Provinces. The watershed measures approximately 1.2 million square miles, covering about 40% of the lower 48 states.\nCommunities up and down the river use the Mississippi to obtain freshwater and to discharge their industrial and municipal waste. We don’t have good figures on water use for the whole Mississippi River Basin, but we have some clues. A January 2000 study published by the Upper Mississippi River Conservation Committee states that close to 15 million people rely on the Mississippi River or its tributaries in just the upper half of the basin (from Cairo, IL to Minneapolis, MN). A frequently cited figure of 18 million people using the Mississippi River Watershed for water supply comes from a 1982 study by the Upper Mississippi River Basin Committee. The Environmental Protection Agency simply says that more than 50 cities rely on the Mississippi for daily water supply.\nAgriculture has been the dominant land use for nearly 200 years in the Mississippi basin, and has altered the hydrologic cycle and energy budget of the region. The agricultural products and the huge agribusiness industry that has developed in the basin produce 92% of the nation’s agricultural exports, 78% of the world’s exports in feed grains and soybeans, and most of the livestock and hogs produced nationally. Sixty percent of all grain exported from the US is shipped on the Mississippi River through the Port of New Orleans and the Port of South Louisiana.\nIn measure of tonnage, the largest port district in the world is located along the Mississippi River delta in Louisiana. The Port of South Louisiana is one of the largest volume ports in the United States. Representing 500 million tons of shipped goods per year (according to the Port of New Orleans), the Mississippi River barge port system is significant to national trade.\nShipping at the lower end of the Mississippi is focused on petroleum and petroleum products, iron and steel, grain, rubber, paper, wood, coffee, coal, chemicals, and edible oils.\nTo move goods up and down the Mississippi, the U.S. Army Corps of Engineers maintains a 9-foot shipping channel from Baton Rouge, LA to Minneapolis, MN. From Baton Rouge past New Orleans to Head of Passes, a 45 foot channel is maintained to allow ocean-going vessels access to ports between New Orleans and Baton Rouge.\nAt Lake Itasca, the average flow rate is 6 cubic feet per second. At Upper St. Anthony Falls, the northern most Lock and Dam, the average flow rate is 12,000 cubic feet per second or 89,869 gallons per second. At New Orleans, the average flow rate is 600,000 cubic feet per second.\nThere are 7.489 gallons of water in a cubic foot. One cubic foot of water weighs 62.4 pounds. A 48 foot semi-truck trailer is a 3,600 cubic foot container.\nAt Lake Itasca, it would take 10 minutes for one semi-trailer of water to flow out of the lake into the Mississippi.\nAt St. Anthony Falls, the equivalent of 3 semi-trailers full of water go over the falls every second.\nAt New Orleans, the equivalent of 166 semi-trailers of water flow past Algiers Point each second.\nThe Mississippi River and its floodplain are home to a diverse population of living things:\nAt least 260 species of fish, 25% of all fish species in North America;\nForty percent of the nation’s migratory waterfowl use the river corridor during their Spring and Fall migration;\nSixty percent of all North American birds (326 species) use the Mississippi River Basin as their migratory flyway;\nFrom Cairo, IL upstream to Lake Itasca there are 38 documented species of mussel. On the Lower Mississippi, there may be as many as 60 separate species of mussel;\nThe Upper Mississippi is host to more than 50 mammal species;\nAt least 145 species of amphibians and reptiles inhabit the Upper Mississippi River environs.\nContact the Park\n111 E. Kellogg Blvd., Suite 105\nSaint Paul, MN 55101\nThis is the general phone line at the Visitor Center, which is staffed every day except Mondays. Please leave a voicemail if we miss your call and expect a return call within 1 day, often sooner.\nMississippi River Facts, Bridges, Tributaries, History and …\nDescription : Leather Boots on Mississippi-River.netExcerpt found on www.mississippi-river.netThe Mississippi River, derived from the old Ojibwe word misi-ziibi meaning.\nFactbox: Five facts about Mississippi River flooding (Reuters …\nDescription : Here are five facts about the Mississippi River: MIGHTY MISSISSIPPI: The Mississippi begins at Lake Itasca in Minnesota, curls south for more than 2300 miles through the United States and empties into the Gulf of Mexico. …\nNervous residents watch rising Mississippi River at Interesting facts\nDescription : Memphis, Tennessee The swollen Mississippi River rolled south Wednesday as communities along its delta braced for flooding, and vast farms remained under threat as it left a trail of submerged homes. …\nprofile-Facts: Mississippi River\nDescription : Mississippi River is the largest river system in North America. About 2320 miles (3730 km) long, the river originates at Lake Itasca, Minnesota, and flows slowly southwards in sweeping meanders, terminating 95 miles (153 km) by river …\nMississippi River to crest Tuesday at Interesting facts\nDescription : The crisis stems in large part from a deluge of rainfall over the past month further up the Mississippi River. Reichling said that, over one two-week stretch, there was about 600% more precipitation than usual. This contributed to a …\nIncoming search terms:\n- mississippi river\n- where is the mississippi river located"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:c014339d-1dd8-45ff-a1b3-be511092d4dd>","<urn:uuid:d0b9d68e-1b7e-4a12-99b3-e7b5f62fe445>"],"error":null}
{"question":"Hello, could you please explain how to develop effective metrics for tracking cloud migration progress?","answer":"To develop effective metrics for cloud migration progress, you need to create simple dashboard measurements that show success criteria. For data center migrations, use burn-down charts of servers moved or shut-down. For cost reduction goals, develop baseline measurements and burn-down charts to track future reductions in license costs and identify trigger points for staff reductions. You may need detailed tracking of database licenses separate from data center server licenses due to cost differentials. These metrics should be used to communicate status and risks to stakeholders.","context":["Don’t blow the budget nor disrupt the business. PM Kinetics offers guidance on cloud security solutions to improve your success rate. The keys to a successful cloud migration strategy are:\n- Ensure that the business applications face minimal disruption or long-term outage,\n- Migration budget does not get blown out with a wide variance.\nAn obvious objective to be sure and one that can be achieved by following these steps:\n- Develop a set of principles that can be shared across the enterprise on how and why the migration is being done.\n- Develop an overall strategy and approach.\n- Develop some key metrics to measure progress and marks toward success.\n- Develop/maintain strong project management and budget discipline during execution.\n- Develop and faithfully execute a Communication plan.\nPrinciples are foundational concepts that will guide decision-making as the company strives to achieve its future state architecture. A principle is defined as “a statement of organizational position that can be argued by rational people”. Principles will provide a framework to shape the key decisions to achieve the future state.\nThe value of a “Principle-based” Cloud Architecture:\n- Ensures that an organization’s position is determined by conscious decision making at the highest level.\n- Aids in gaining alignment from all affected organizations and enables common goals to be achieved.\nSome examples might be:\n- Use an existing firewall infrastructure and forgo what might be available with a cloud vendor. The same might apply to whatever you use for malware and DLP to access into/out of the cloud.\n- Use key encryption management within the cloud versus what you might use on-premises today.\n- Extend your preferred Identity Access Management (IAM) to the cloud versus trying to develop a federated model.\nKeep the number to whatever are the most critical from a risk profile, technical profile and business critical and incorporate into your communication delivery package. Please note that they do not have to be exclusively technical in nature. It might cover your approach to SaaS or business self-service models. They guide decision-making during the more detailed project execution phase.\nSome key questions to address in the strategy planning should cover:\n- More than likely your approach will not use a single cloud vendor. So how will you separate functions, and will a common identity access and authentication mechanism be used? Will there be any level of integration or will they be treated as separate computer silos?\n- Although there are native services provided by the cloud providers for migrating data, these solutions might require significant restructuring of the application during a “lift-and-shift” operation or they may not be as fast, taking days or months to move the data. What provisions can be planned for application outage?\n- Will a governance group be established to review and monitor how decisions are made on when to use SaaS, IaaS, PaaS and for which cloud vendor, versus letting things remain status-quo?\n- What security model will you adopt to ensure data does not get accessed by individuals that are not supposed to gain access? Both NIST and OWASP have great frameworks to review, as does PCI. Who will track adherence to this standard and how will the migration get tested for compliance?\n- Privacy and security may be also governed by HIPAA and GDPR may loom large in how you conduct business, regardless of whether you are US-based or EU-based.\n- There are six basic strategies that could be followed:\n- Rehosting — Otherwise known as “lift-and-shift.”\n- Simply moving from on-premise virtual environment to another cloud without implementing any cloud optimizations, could save roughly 30 percent of its costs by rehosting. If you have a larger mix of physical servers, the saving could potentially be much larger.\n- Most rehosting can be automated with tools (e.g. AWS VM Import/Export, Racemi), although some might prefer to do this manually as they learn how to apply their legacy systems to the new cloud platform.\n- Replatforming — Sometimes called “lift-tinker-and-shift.”\n- You might make a few cloud (or other) optimizations to achieve some tangible benefit, but no changes to the core architecture of the application. Migrating to a database-as-a-service platform may be a consideration or how you perform snapshot backups.\n- Migrating from a Java application container (requires an expensive license) to Apache Tomcat, an open-source equivalent could add additional savings and agility.\n- Repurchasing — Move to a different product.\n- Refactoring / Re-architecting — Re-architect the application, typically using cloud-native features.\n- Driven by a strong business need to add features, scale, or performance and usually for new initiatives.\n- Might be driven to a service-oriented (or server-less) architecture to boost agility or improve business continuity. This approach tends to be the most expensive, but, if you have a good product-market fit, it can also be the most beneficial.\n- Retire — Get rid of the app\n- Once you have completed an application to a server to owner mapping you might find as much as 10% of an enterprise IT portfolio is no longer useful and can simply be turned off. This can supplement your business case and lessen the surface area you must secure.\n- Retain — Do nothing (for now).\n- You should only migrate what makes sense for the business; and, as the gravity of your portfolio changes from on-premises to the cloud, you’ll probably have fewer reasons to retain.\n- Rehosting — Otherwise known as “lift-and-shift.”\nDevelop a set of metrics to manage progress through your journey that can also be used to demonstrate your success criteria.\n- If the end game is to shut down an on-premise data center and/or leave an existing cloud vendor, what are the measurements to show success? These metrics need to be in some simple dashboard form that can be used to communicate status and risks. This might be a burn-down chart of servers moved or shut-down.\n- If the end game is to reduce staff costs or hardware/software license costs, do you have the baseline to measure this? Do you have a burn-down chart develop to track future reductions in license costs? Do you know the trigger point for any staff reductions? Detailed tracking of database licenses separate from data center server license might be important due to the cost differentials.\nCloud Migration Steps: Planning\nOne of the first steps to consider before migrating data to the cloud is to determine the use case that the public cloud will serve. One area of possible significant change is disaster recovery. Whether completely shifting to the cloud or a hybrid approach will impact how you continue to plan and execute disaster recovery. Some approaches to cloud migrations make this simpler. Another consideration to keep in mind is meeting ongoing performance and availability benchmarks to ensure your RPO and RTO objectives may change.\nIn terms of SLA numbers, Cloud Volumes HA can help you achieve a recovery point objective (RPO) of zero and a recovery time objective (RTO) of less than 60 seconds. The Multiple Availability Zone deployment model helps protect against Availability Zone failures. These features ensure that your cloud environment is resilient, safe from service disruptions, and able to host critical workloads as well as data migration processes without requiring expensive HA setup on the application side.\nA word of caution, however, is that if you use Multiple Availability Zones then data replication timing will become a key component and an additional cost.\nDetermine the factors that will govern the migration, such as critical application data, legacy data, and application interoperability. Do you have data that needs to be resynced regularly, data compliance requirements to meet or non-critical data that can possibly be migrated during the first few passes of the migration? Some legacy data could be ported to a simple read-only archive that involves fewer computing costs.\nAuthentication and access for those individuals setting up the cloud foundation need to be planned and might entail using multi-factor authentication. If you presently use Active Directory, establishing separate read-only and full-rights access is merited. This ensures complete logging and integrates with whatever you are doing on-premises.\nMapping applications to servers and mapping communication flows between servers is a critical step in planning. This will drive how application “waves” are created and the respective hosts provisioned in the cloud.\nEstablish timelines for applications and application waves that when coupled with the tracking metric dashboards create a complete communication package to your stakeholders.\nCloud Data Migration Execution\nThe main challenge in execution is how to carry out your migration with minimal disruption to normal operation, at the lowest cost, and over the shortest period. If your data becomes inaccessible to users during migration, you risk impacting your business operations. The same is true as you continue to sync and update your systems after the initial migration takes place. Plan out a mitigation plan and a roll-back plan in the event a migration must be postponed.\nEvery workload element individually migrated should be proven to work in the new environment before migrating another element. You’ll also need to find a way to synchronize changes that are made to the source data while the migration is ongoing. Both AWS and Azure provide built-in tools that aid in migrations.\nTest plans should have been built as a product of your planning phase with acceptance criteria from your stakeholders.\nThe miscellaneous pieces of information developed:\n- List of Principles and their impact\n- Summary of overall strategy and approach summary\n- List of tracking key metrics to measure progress\n- Plan and budget summary\nThese become the key components as to what you discuss with your stakeholders. The type of meetings (governance versus project versus finance) will dictate the level of details. But as with most large-scale projects, you simply cannot over communicate!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:52c8eb57-c4bf-4f1d-882a-af60168e2723>"],"error":null}
{"question":"What's new in robot vision systems for manufacturing, and what inspection requirements exist?","answer":"The latest robot vision systems include eight cameras per cell with advanced Cognex technology, featuring In-Sight 7802M for part measurement, In-Sight 9912M for circuit board measurement, and 3D-A5060 surface-scan cameras with 3D LightBurst technology for component positioning. The system uses data matrix codes for component identification and enables automatic placement model generation without programming. Regarding inspection requirements, OSHA inspectors are being trained to identify robot-related hazards and require comprehensive risk assessments before system operation. If a risk assessment hasn't been conducted, companies can face willful violation citations, and OSHA regularly contacts equipment manufacturers to verify safety standards compliance.","context":["Glaub Automation PCB Assembly Final Step\nPrinted circuit board assembly in electronics manufacturing is largely automated. Robots of various designs undertake steps such as SMD placement, soldering and automated optical inspection (AOI). Until now, an exception has been the through-hole mounting of so-called wired components such as capacitors, power coils and connectors. This is still mostly done by manual assembly, as it is a complex process that cannot be easily automated.\nThis is precisely the challenge that presented itself to the engineers from Glaub Automation & Engineering GmbH in Salzgitter. As CEO Niko Glaub says, “A client asked us concretely whether we might not be able to find a solution for automating this step.”\nThis challenge was the major incentive to developing a solution. Glaub's team enthusiastically set to work and the solution is now already being used — and it is highly unconventional. The assembly is done by a dual-arm collaborative YuMi robot from ABB, which — thanks to its two arms — can assemble the circuit boards twice as fast as a conventional robot.\nThe automated process in the ultra-compact “GL-THTeasy” robot cell takes place as follows: The robot is provided with blister packs with capacitors, for example, via a conveyor line. A data matrix code on the blister pack allows the item to be identified. YuMi then grips one capacitor after another from the blister pack and places it precisely on the printed circuit board. Alternatively, it can also remove the electronic components from an ESD container, for example, or a vibratory conveyor. This is immediately followed by soldering from below. Empty blister packs are removed by the installation via a recirculation system and full ones are then automatically fed in.\nTargeted bin picking\nThis all sounds quite logical so far, and we may wonder why this assembly process was not automated before now. The answer: robotics was unable to cope with either the high variability in the component feeding or the slight inaccuracies in positioning the components. This meant that the few previous robotic solutions were extremely complex, elaborate in terms of programming, and not particularly reliable in practice.\nHowever, Glaub's new robot cell uses smart cameras for the first time, boasting state-of-the-art image processing technology from Cognex, the market leader in industrial image processing. The position of the components in the blister packs is detected using 3D surface sensors — which also allow targeted picking from the bin or the vibratory conveyor, in order to then further measure the capacitors and printed circuit boards with the help of 2D cameras.\nA smart combination of robotics and image processing\nImage processing thus plays a crucial role in the success of this concept. Glaub’s engineers worked together with Wendeburg-based M-VIS Solutions GmbH to choose the cameras suitable for this application. Cognex’s solutions partner M-VIS developed a solution with several cameras that both capture the data matrix codes on the blister packs and precisely measure and locate each individual component.\nVitali Burghardt, CEO of M-VIS Solutions, explains: “With the 100% absolute measurement of components and printed circuit boards, GL-THTeasy compensates for every inaccuracy in terms of components, gripping, workpiece carriers and conveyor belts.” This means that components that do not fit exactly are immediately eliminated.\nEight cameras per cell, including two 3D cameras\nAs part of a feasibility study, M-VIS — supported by Cognex — chose eight cameras, four for each robot arm. An In-Sight 7802M vision system measures the parts and provides the necessary information to correct the position of the gripper. A further system from the In-Sight 9912M series measures the circuit board and if necessary corrects the gripper's movement when it is placing the component on the board. The 3D surface-scan camera 3D-A5060 with patent-pending 3D LightBurst technology and integrated VisionPro image processing software “sees” the position of parts in the feed line.\nNiko Glaub explains this step in more detail: “In each process step, the cameras capture the actual position of the component, the gripper and the circuit board in relation to the electronic component. In other words, the ‘legroom’ of the components is aligned with the actual dimensions of the assembly positions. First of all, this allows the component to be automatically found and removed, and then enables totally accurate through-hole mounting on the basis of actual position data.”\nThis approach offers a further advantage: since the movements are controlled based on cameras, the operators can generate a new placement model without programming. The camera images produced serve as a basis for this. This simplifies and accelerates not only assembly but also conversion. The GL-THTeasy robot cell is thus a prime example of flexible automation offering a smart solution for both current and future requirements.\nShort cycle time, rapid amortization\nABB YuMi's two arms work simultaneously round the clock, allowing 24/7 operation at high speed with a very short cycle time, which can be under three seconds depending on the components to be installed and the feed. The amortization period is also impressive, coming to around fourteen months from GL-THTeasy's first use. The new robot cell therefore scores points on several counts, which support its use: innovation, reliability, efficiency and future viability. With Glaub and M-VIS, there is no doubt that this smart solution will persuade many other electronic manufacturing companies in the future to automate the process step for printed circuit board assembly with this flexible and efficient method","- This editorial is filed under:\n- Automotive Component\n- Consumer Goods/Appliances\n- Life Sciences Pharma Biomed Medical Devices\n- Rubber & Plastics\n- Off-Road/Heavy Equipment\n- Robot Manufacturing\n- Building Products/Materials\n- Fabricated Metals\n- Printing & Publishing\n- Arc Welding\n- Wood Products\nAre You at Risk of OSHA Citations for Robot Safety?\nConversion Technology Posted 03/12/2019\nRobots have been a part of the industrial landscape for decades. As the world of industrial automation progresses, the number of employees and robots working in close quarters with each other continues to grow. With the increase in automation and the use of mobile and industrial robots, regulations are being updated to address the potential hazards posed by the changes in equipment and routine and non-routine tasks around robots in the workplace. The Occupational Safety and Health Administration (OSHA) is training their inspectors to be aware of these regulatory changes and become familiar with industrial robot use. Not being aware of your facility’s requirements or of the changes in robot regulations could cost you.\nRecently, CTI attended the annual International Robotics Safety Conference, hosted by the Robotics Industries Association (RIA). At the conference, we spoke with representatives from OSHA, who explained the plans and actions put in place to train inspectors to better identify the hazards around industrial robots, as well as a plan to work on updating regulations to better protect employees from these identified hazards.\nRobots are machines, and as such must be safeguarded in ways like those presented for any hazardous remotely controlled machine. As with any other machine, there are countless hazards that could be present in and around a robot system. These hazards could vary depending on the design of the robot cell, placement inside the facility, level of interaction with employees, program or software being run, or end effector being used. Some common hazards are slip, trip, and fall inside the cell, contact with moving parts, dropped parts or end effectors, being pinned by the robot arm, etc. These hazards could be magnified while in Teach mode, unless mechanical and engineering safeguards are in place. The most effective way of identifying hazards is by conducting a comprehensive risk assessment before the system is operational, and after all parts, guards, and work practices are in place.\nARE THERE ANY REGULATIONS FOR ROBOTS?\nWhile there is currently no OSHA standard specifically covering industrial robots, there are several consensus standards, that OSHA refers to, covering safeguarding performance criteria, risk assessment methodologies, and general safety requirements. Consensus standards are voluntary standards developed through the cooperation of multiple parties, typically governing agencies and industry groups, who have an interest in participating in the development and/or use of the standards. OSHA commonly refers to consensus standards when there is no specific regulation covering the topic (e.g. NFPA standards on combustible dust). OSHA is very aware that they do not know everything about every subject for every industry. RIA and the American National Standards Institute (ANSI) have put together the current robot safety standards and have partnered with the National Institute for Occupational Safety and Health (NIOSH) to further promote and update regulations on the topics related to robot safety.\nIf OSHA were to arrive at your facility and inspect a robot or robot system, the first thing the inspector would ask for is a copy of the last risk assessment conducted on the system. If a risk assessment has not been conducted on the system, they could push for a willful violation, as the risk assessment is required by law. Despite no standard in 29 CFR 1910 governing industrial robots, violations and citations can and have been issued on robot system. The primary regulations to be cited for violations with a robot system are Lockout/Tagout and the Control of Hazardous Energy (1910.147), Machine Safeguarding (1910.212), and the General Duty Clause (Section 5(a)(1)). OSHA does also regularly contact original equipment manufacturers (OEMs) and robot system integrators to determine the level of safety provided at installation of the equipment as compared to the hazards present at the time of the inspection or injury. As is the way of industrial safety, the employer is the party with the legal responsibility to recognize and mitigate hazards in the workplace.\nHOW DOES OSHA SEE ROBOT SAFETY?\nOSHA’s view on robot safety is that if the employer is meeting the requirements of the consensus standards, specifically ANSI/RIA R15.06 – Safety Requirements for Robots and Robot Systems, then there will not be any issues. However, one of the primary findings from inspections is that, while machine safeguarding and the control of hazardous energy are typically front of mind for employers, comprehensive risk assessments are not being conducted or revised after the installation of new equipment. A risk assessment is required by R15.06 and specified further in ANSI/RIA B11.0 – Safety of Machinery – General Requirements and Risk Assessment. Some facilities have them done by the robot integrators and installers, but fail to conduct them after changes to equipment, policies or procedures, or tooling and layout specifications.\nMany robot accidents and violations do not usually occur during normal operation and practices. These incidents typically occur during non-routine operating conditions (e.g. programming, maintenance, setup, part/tool changes, and while in Teach mode). It is imperative to select an effective safety system for your robots that is based on all jobs and tasks conducted by the robot and within the robot system. This can be done through safety controls, limiting boundaries, safeguards, etc. Through a comprehensive risk assessment all tasks and corresponding hazards can be identified, hazard ratings applied, and corrective actions can be determined and prioritized.\nHOW TO ENSURE COMPLIANCE\nCTI has years of experience conducting both qualitative and quantitative risk assessments on robots and robot systems. CTI is also a member of the R15.06 rule making committee and is in contact with OSHA representative on how the updating of rules and regulations impact employers and industry sectors. Contact CTI for more information on risk assessments and how to ensure your facility’s industrial robots and robot systems are in compliance with all governing standards and regulations."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:58ba5710-220d-4a3f-b12c-7454b345de67>","<urn:uuid:19933ddd-652b-4e96-a35c-746807e8197e>"],"error":null}
{"question":"Do gastroparesis and celiac disease both impact nutrient absorption in the digestive system, and if so, how do their mechanisms differ?","answer":"Both conditions affect nutrient absorption but through different mechanisms. In gastroparesis, the stomach's slow emptying impairs the normal 'bucket brigade' process of passing food contents along the digestive tract, which can interfere with proper nutrient processing. In celiac disease, gluten triggers an immune response that damages the small intestine's villi (finger-like projections), directly leading to poor absorption of food and nutrients, resulting in various nutritional deficiencies including iron deficiency.","context":["Whether you have heartburn (GERD), bloating with meals or symptoms commonly attributed to hypoglycemia, you may not know it but one of the most common causes for all of these conditions is a gastrointestinal disorder known as gastroparesis.\nGastroparesis is a Latin term for ‘paralyzed stomach’ and is used to describe various symptoms attributable to slow emptying of the stomach. Gastroparesis can lead to symptoms such as heart burn and GERD (gastroesophageal reflux disease) because the when the normal and persistent production of stomach continue throughout the day without being emptied, stomach acid builds up and refluxes or splashes back up into the esophagus causing the symptoms we commonly describe as heartburn or reflux.\nSometimes the excessive collection of acid in the stomach will cause people to misinterpret the symptoms as low blood sugar or ‘hypoglycemia’. This is because the excessive stomach acid will make the person feel shaky, weak, hungry and even nauseated, and when the person eats some food the symptoms resolve. The association of these symptoms and their resolution with food understandably but mistakenly attributed to low blood sugar.\nThe excessive stomach acid is actually causing the patient to experience some low grade nausea. The resolution of the symptoms is because food stimulates the stomach to increase it’s emptying action and also acts as a excellent acid neutralizer. We have experienced this before when we have been nauseated or vomiting from the flu or during pregnancy – we will eat some toast, soup or crackers to help us deal with the nausea we experience.\nThe other important fact is that our physiology is designed to help us maintain normal blood sugar levels even if we go without food for extended periods of time. We were in essence designed as Stone Age humans who for millions of years were constantly exposed to unstable food supplies and would often go for extended periods of time without eating. If we were to find ourselves in the unfortunate circumstances where we went without food for a weeks at a time, we might feel very hungry but our blood sugar levels would remain constant for many weeks without food.\nOther Symptoms Associated with Slow Stomach Motility\nWhen the stomach’s grinding and emptying motility action is impaired, other portions of the intestinal tract may also be impaired. Under normal conditions, the stomach is constantly emptying its contents into the small intestine From the upper esophagus all the way through to the colon, the intestinal tract is operating like one long bucket brigade with one portion of the GI tract passing its contents along to the next portion.\nSometimes patients will experience food (especially when it involves liquids) seeming to stick in the esophagus shortly after ingestion. They will feel a fullness or pressure in the chest often below the breastbone (sternum).\nThey an also experience some slowing of intestinal contents in the colon which results in constipation.\nThe slowed motility may also result in occasional frequent of loose stools because of overgrowth of some of the normal bacteria that colonize the small intestine and the colon. There is a mixture of upwards 400 different species of bacteria that normally live within our intestinal tract and when the motility of the intestine slows down, a portion of the bacteria grow out of proportion of the others and results in diarrhea or an urgency to have a bowel movement shortly after eating.\nAutonomic Dysfunction is the Underlying Cause Gastroparesis\nI am convinced the vast majority of the patients with heartburn or misinterpreted sense of hunger or low blood sugar have gastroparesis because of underlying autonomic nervous system (ANS) abnormalities. The ANS is responsible for operating and coordinating the function of all of the bodies organs such as the lungs, intestinal tract, cardiovascular system (heart, arteries and veins) and bladder.\nDysfunction of the ANS can cause the slowing of the intestinal tract motility and cause the bloating, heartburn or constipation discussed above. The most common causes for the autonomic dysfunction can be due to Insulin Resistance from obesity or excessive carbohydrate intake, a physically stressful event such as surgery pregnancy or a major illness. Some patients give a history of bloating, constipation or heartburn from early childhood.\nFortunately, the autonomic dysfunction can be accurately diagnosed and reversed with a short course of medications, changes in their diet or both.","WHAT IS CELIAC DISEASE?\nCeliac disease is an autoimmune disease, which means the body’s immune system mistakenly attacks itself when it senses a particular trigger. Experts believe that the cause of celiac disease involves interaction between a person’s genetic background and the environment.\nWith celiac disease, the trigger is a protein called gluten, which is commonly found in wheat, rye, barley, triticale, malt, brewer’s yeast, wheat starch and their by-products (see Table 1 for more information). It is estimated to affect 1 in 100 people worldwide and up to 3 million Americans. Children and adults can develop celiac disease at any age. In children, this can be as young as around six months, which is about the time solid foods containing gluten are introduced.\nThe lining of the small intestine is covered with small, finger-like projections called villi. If you or your child has celiac disease, eating foods with gluten inflames and damages the villi and the small intestine, via an immune system response to the gluten protein.\nOne of the consequences of the damage to the small intestine is that food and nutrients are absorbed poorly. This can result in bowel symptoms and various nutrition deficiencies, including iron deficiency. Inflammation also results in problems that can affect the bones, joints, skin and other organs.\nImportantly, appropriate treatment with a strict avoidance of gluten in the diet leads to small bowel healing, resolution of symptoms, and a reduction in the risk of complications. Untreated celiac disease and poor management can lead to chronic poor health.\nHOW IS CELIAC DISEASE TREATED?\nRight now, there is no cure for celiac disease, but fortunately it can be managed by following a strict, lifelong, gluten-free diet. Sticking to a gluten free diet is very important considering even trace amounts of gluten can damage the small intestine, with or without obvious signs or symptoms.\nSomeone with celiac disease needs to be a gluten super sleuth, and check food labels and ingredient lists carefully to make sure there are no gluten containing ingredients. See Table 1 for an example list of gluten- containing grains.\nGLUTEN-CONTAINING GRAINS AND THEIR DERIVATIVES\nVarieties and derivatives of wheat such as:\n- Wheat berries, Durum wheat, Emmer, Semolina, Spelt, Farina, Farro, Graham, KAMUT® Khorasan wheat, Einkorn wheat\n- Wheat Starch that has not been processed to remove the presence of gluten to below 20 ppm and adhere to the FDA Labelling Law Rye\nMalt in various forms including:\n- malted barley flour, malted milk or milkshakes, malt extract, malt syrup, malt flavouring, malt vinegar\n- Brewer’s Yeast\n- Wheat Starch that has not been processed to remove the presence of gluten to below 20ppm and adhere to the FDA Labeling Law*\n*According to the FDA, if a food contains wheat starch, it may only be labeled gluten-free if it has been processed to remove gluten, and tests show it is below 20 parts per million gluten. If a product labeled gluten-free contains wheat starch in the ingredient list, it must be followed by an asterisk explaining that the wheat has been processed sufficiently to adhere to the FDA requirements for gluten-free labeling.\nList adapted from: Celiac Disease Foundation. Gluten-Containing Grains and Their Derivatives.\nFor comprehensive lists of foods and ingredients to avoid when managing celiac disease, see the Celiac Disease Foundation website www.celiac.org/live-gluten-free/glutenfreediet/sources-of-gluten.\nPerhaps more importantly, for a comprehensive lists of foods that you can eat when living with celiac disease, see the Celiac Disease Foundation website for ‘What Can I Eat?’\nCELIAC DISEASE LONG-TERM MANAGEMENT\nAdjusting to the gluten free diet may seem difficult at first, but as knowledge and confidence grows, managing the diet becomes easier. When first diagnosed, a visit to a registered nutritionist or dietitian who specializes in gluten free diets is advised. They will help to explain and plan your individual gluten-free diet, give you information about how to read food labels, and will help you to plan to get all the right nutrients your body needs. This may involve considering the need for any dietary supplements like iron or calcium. A dietitian may also give you some nice recipe ideas.\nOnce gluten is eliminated, the small intestine can heal and you or your child should start to feel better, usually within a few days or weeks.\nREADING LABELS ON ALL FOODS FOR GLUTEN CONTAINING FOODS\nThere are many foods that could contain gluten, often in hidden or unexpected ways, so remember to always read packaged food labels to check for hidden gluten sources. Some tips include:\n- In America, a gluten-free claim can be made if the food contains no detectable gluten\n- Most packaged foods must declare ingredients derived from a gluten-containing grain (i.e. wheat or rye) on the food label’s ingredient list\n- If “gluten-free” is not specified on a food label, always read the label very carefully\nBe careful, as many products may appear to be gluten-free, but are not. For instance, products labeled wheat-free are not necessarily gluten-free. They may still contain spelt (a form of wheat), rye or barley-based ingredients that are not gluten-free. To confirm if something is gluten-free, read the product ingredient list.\nTo learn more about label reading as it relates to identifying gluten in food, see the Celiac Disease Foundation advice on Label Reading and The FDA www.celiac.org/live-gluten-free/glutenfreediet/label-reading.\nOTHER CONSIDERATIONS WITH CELIAC DISEASE\nPreschool or school:\nHaving a child’s school involved may help in the day-to-day management of maintaining a gluten free diet. Some helpful tips:\n- Talk with your child’s preschool or school and let them know that your child needs to follow a strict gluten-free diet\n- Discuss with your child’s class room teacher whether you and your child may have some ‘circle-time’ with the class, to explain the basics about celiac disease. This is a nice way to build education and tolerance, all at once\n- Exercise care around bake sales, classroom parties and snacks outside of the cafeteria.\nKeeping gluten-free food separate at home:\nIt’s important to keep gluten-free food separate from gluten-containing food, to avoid accidental exposure to gluten. Some helpful tips include:\n- Prepare and store all gluten-free foods away from foods with gluten. Use separate chopping boards and utensils when preparing or cooking gluten-free foods\n- Clean utensils and appliances that might have gluten-containing foods, even crumbs, on them\n- A separate toaster for gluten-free bread is best practice.\nTaking care when you eat out\nEating out is always a treat and having celiac disease is no reason to stop. However, a little extra care is needed to make sure gluten containing food is not eaten by mistake:\n- Pay attention to choose gluten-free menu items\n- Inform restaurant staff that you or your child can only eat food that is strictly gluten-free\n- Avoid condiments and food that may have been cooked with hidden (i.e. soy sauce)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:e1ca9e67-985d-4601-a586-9dfac971591c>","<urn:uuid:90ae3cb8-b15f-4493-aa7f-909ac60e016c>"],"error":null}
{"question":"What are the key differences between Botticelli's painting 'The Birth of Venus' and Leonardo da Vinci's 'The Last Supper' in terms of their artistic techniques and preservation status?","answer":"The Birth of Venus and The Last Supper employed different artistic techniques and have faced distinct preservation challenges. Botticelli's Birth of Venus (c. 1485) featured International Style elements like ornamental patterns and unrealistic stylization, such as flat sea waves. In contrast, Leonardo da Vinci's Last Supper (1495-1498) was painted using an experimental technique on dry plaster with a double layer of dried plaster sealed with gesso, pitch, and mastic, topped with white lead to enhance brightness. While The Birth of Venus has survived to become one of Botticelli's best-known works, The Last Supper began deteriorating almost immediately after completion, with the painted plaster flaking off the wall. The Last Supper has faced numerous challenges including structural damage from a door installation in 1652, damage from Napoleon's troops, World War II bombing exposure, and controversial restoration attempts, with critics arguing that recent restoration removed much of Leonardo's original work.","context":["Thursday, February 5th, 2009\nA change in taste?\nI am continually surprised and saddened when I think of the Early Renaissance artist Sandro Botticelli and his decision to burn some of his paintings.\nSandro Botticelli is arguably the most famous painter of the Early Renaissance. Today, he is probably best known for his mythological paintings The Birth of Venus (c. 1485, shown to the right) and Primavera (c. 1478). These paintings were influenced by International Style, as shown by the ornamental patterns (e.g. Flora’s dress on the right) and unrealistic stylization (e.g. the flat sea waves). Both paintings were completed during the beginning of Botticelli’s career, when he was highly successful and enjoyed the patronage of important families such as the Medici and Vespucci.\nHowever, it appears that Botticelli’s career took an interesting turn with the rise of Girolamo Savonarola, an Italian friar and preacher. Savonarola’s fundamentalist ideas regarding politics and religious art had considerable influence on Florence. In fact, many people in Florence considered Savonarola to be a prophet and type of savior for the city.1 In regards to art, Savonarola condemned the worldly character of religious paintings and criticized artists for using identifiable people as models for holy figures. Savonarola “complained that the images in the churches of the Virgin, St Elizabeth and the Magdalene were painted like nymphs in the likenesses of the young women of Florence, thundering: ‘You have made the Virgin appear dressed as a whore’. In his sermons he upbraided the young women of Florence for wearing such dress and castigated painters for representing them in sacred guise, urging everyone to burn their copies of the Decameron and all lascivious images.”2 There is no doubt that Botticelli’s mythological paintings would have been viewed as “lascivious” by Savonarola, particularly The Birth of Venus, which was the first painting of a Classical female nude since antiquity.\nSavonarola convinced many people to burn their worldly items. He is particularly associated with two public burnings took place on 7 February 1497 and and 27 February 1498. These fires are known as the bruciamenti delle vanità (bonfires of the vanities). At these public bonfires, cosmetics, false hair, playing cards, profane books and paintings were destroyed. It’s shocking to think of what precious works of art were destroyed at this time – especially works by the talented Botticelli. According to the biographer Vasari, Botticelli was affected by the teachings of Savonarola and reportedly threw some of his own paintings on pagan themes into the flames. How tragic! I have often wondered what those paintings were like and what type of contribution they might have made to the field of art history.\nOf course, Savonarola did not condemn all types of art. He felt that art could be utilized as a type of moral instruction and encouraged his illiterate followers to ponder the life of Christ by looking at paintings (no doubt, paintings which met his level of standard). At this same time, and probably not coincidentally, Botticelli’s artistic style began to change. He began to reject the courtly, ornamental style of his earlier paintings and turned to a more somber, simplistic style which mimicked the sentiment and style of earlier religious paintings. One such somber painting is Botticelli’s Lamentation over the Dead Christ with St. Jerome, St. Paul and St. Peter (c. 1490). Not only is the extense grief and mourning observed in the faces of the figures, but the sweeping lines of the dead corpse and kneeling figures suggest to me an added level of heaviness, weight, and grief.3 I cannot help but conclude that this shift in style was influenced by Savonarola’s apocalyptic preaching.4\nUnfortunately, Botticelli’s career faltered near the end of his life, which is especially disappointing since the artist was originally one of the influential painters in developing the new Renaissance style. I find the demise of Botticelli’s career partially linked to Savonarola and the artist’s subsequent change in style.\nI wonder if Botticelli held any regrets about burning his paintings or his change in artistic taste. Savonarola’s followers were often referred to as piagnoni (“snivellers” or “weepers”) because “they were given to loud and weepy repentence for their sins.”5 It is known that Botticelli’s brother was a piagnone and there is other evidence that Botticelli was also associated with this mournful group.6 If I were Botticelli and had burned some of my beautiful art, it seems like I’d have an additional reason to weep.\n1“Savonarola, Girolamo.” In Grove Art Online. Oxford Art Online, http://www.oxfordartonline.com.erl.lib.byu.edu/subscriber/article/grove/art/T076215 (accessed February 5, 2009).\n2 Charles Dempsey. “Botticelli, Sandro.” In Grove Art Online. Oxford Art Online, http://www.oxfordartonline.com.erl.lib.byu.edu/subscriber/article/grove/art/T010385 (accessed February 5, 2009).\n3 For more analysis of this painting and Savonarola’s influence on Botticelli, see Barbara Deimling, Botticelli, (New York: Taschen, 2000), 69-72. These pages can also be read online here.\n4 Not all historians agree on Savonarola’s influence on Botticelli. For some discussion on the art historical debate between Savonarola’s influence on Botticelli, see Ingrid Drake Rowland, From Heaten to Arcadia: The Sacred and the Profane in the Renaissance, (New York: New York Review of Books, 2005), 80-81.\n5 Ibid, 80.\n6 Vasari records that Botticelli was “extremely partisan to [Savonarola's] sect.” See Dempsey, “Botticelli, Sandro.”","Located on the wall of the dining room of the former Dominican convent of Santa Maria delle Grazie in Milan, exactly in the refectory of the convent, the Last Supper, a late 15th-century mural painting by the great Italian artist Leonardo da Vinci, is one of the most famous and fascinating, most studied and reproduced and the subject of many legends and controversies. Commissioned by Ludovico Sforza, the Duke of Milan and painted by the master artist between 1495 and 1498, the coloured plaster of the enormous fresco measuring 15 by 29 feet (4.6 x 8.8 meters), covers the entire wall of the refectory, although the room was not a refectory at the time that Leonardo painted it.\nInstead of using tempera on wet plaster, the usual method of fresco painting, Leonardo painted it on dry plaster, as he sought a greater detail and luminosity than could be achieved with traditional fresco and it resulted in a more varied palette. In fact, traces of gold and silver foils have been found which testify to the artist's eagerness to make the figures much more realistic. He chose to seal the stone wall with a double layer of dried plaster, composed of gesso (gypsum prepared with glue), pitch, and mastic. After that, he added an undercoat of white lead to enhance the brightness of the oil and tempera that was applied on top. Unfortunately, his experiment did not work, as the painted plaster began to flake off the wall almost immediately. Various authorities have struggled to restore it ever since.\nThe layout of the fresco is largely horizontal. All the figures are set behind the large table, which is seen in the foreground of the image. The painting is also largely symmetrical with the same number of figures on either side of Jesus.\nThe Last Supper is the visual interpretation of the artist of an event chronicled in all four of the Gospels, depicting the evening before Christ was to be betrayed by one of his disciples. He gathered them all together to eat and to tell them that he knew what was going to happen soon. The fresco depicts the next few seconds in this story after Christ dropped the bombshell that one of his disciples would betray him before the sunrise and the different reactions of horror, shock and anger of the twelve apostles. In Leonardo da Vinci’s interpretation, the moment represents the moment before the birth of the Eucharist, with Jesus reaching for the bread and a glass of wine that would be the key symbols of the Christian sacrament.\nIn recent years, much of the interest in the painting has centered on the details hidden within the painting. Many scholars have discussed about the hidden meaning of the spilled salt container near an elbow of Judas. The spilled salt has been said to represent his betrayal or could symbolize bad luck, loss, religion or Jesus as the salt of the earth. It is also claimed by some biblical scholars that the disciple, painted at the right hand of Jesus, is John. However, many art and biblical scholars believe that actually the person is a woman, since he/she is the only person in the picture wearing a necklace or a charm, which is not a small one and in all probability, she is Mary Magdalene.\nIt is quite evident from the painting that, many of the figures on the right side of the table seem to be talking to or at Mary, while Jesus appears to be engaged with those on the left, who reflect the concern the disciples must have felt when told one among them would betray Christ. A closer look will at ‘John’ will also reveal a resemblance to Mona Lisa, which the master artist painted five years later.\nUnfortunately, in the ensuing centuries, the Last Supper had been a victim of abuse for several times. In 1652, a new door was cut in the wall of the deteriorating painting, which removed a chunk of the artwork showing the feet of Jesus. The painting endured additional irreverence in the late 18th century, when the invading troops of Napoleon Bonaparte used the refectory as a stable and further damaged the wall with projectiles.\nDuring World War II, the roof and one wall of the refectory collapsed due to the bombing of the Allied forces. Though the painting survived, it was exposed to the elements for several months before the space was rebuilt.\nThe Last Supper was subject to numerous restoration attempts and it underwent an extensive and controversial 20-year restoration that was completed in 1999. However, many critics argued that the restorers had removed so much of the painting that very little was left of Leonardo’s original work."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:e4ba8147-a5fd-44ae-823d-493d01147d85>","<urn:uuid:09052286-1942-48fe-b88a-587975f8ab99>"],"error":null}
{"question":"I need to understand the difference between the final inspection phase of buying a business and the process of evaluating a franchise before purchase. Can you compare these two processes and what they involve? 🤔","answer":"The final inspection phase when buying a business and the franchise evaluation process are quite different. The final inspection phase of a business purchase involves reviewing updated financials since initial reports, checking representations and warranties changes, conducting title/lien/judgment searches, and physically inspecting assets, inventories, and properties. In contrast, the franchise evaluation process involves 6 steps: obtaining the Franchise Circular (UFOC), talking to current and former franchisees, analyzing all costs (upfront fees, royalties, marketing fees), reviewing recent lawsuits against the franchiser, checking out the competition, and comparing franchise costs versus independent business operation costs.","context":["Are Your \"Attorneys' Fees & Costs\" Provisions Effective?\nNovember 24, 2014\nHow to Franchise My Business, in 10 Steps\nDecember 9, 2014\nHow to Buy or Sell a Business, in 5 Phases\nDecember 9, 2014\nBy Joshua Logan, Esq.\nAs an attorney who has conducted several dozen business purchase and sale transactions, it has been my experience that the likelihood of a successful completion of the transaction increases considerably after the conclusion of each step of the transaction. Each of those phases of the transaction are described here so you can understand what to expect during the business acquisition or divestiture process, and understand the status of where your transaction stands. Here are the five major phases of a business purchase and sale transaction:\n1) Letter of Intent, Confidentiality & CNDAs\nAfter the identification of a target business to purchase or an interested\npotential buyer, the Letter of Intent, or “LOI,” sets out the most broad, intended terms of the transaction. Usually these terms of the LOI are:\nDetermine Asset Sale vs. Interest Sale\nWhat to do with the Seller's Contracts\nDetermine the Sales Price & any Pricing Formula\nExclusions from the Sale\nWhat to do with Employees\nConfidentiality and Non-Disclosure Agreements (\"CNDAs\")\nFor a description of each of the above-components to an LOI, read the post, What is an M&A Letter of Intent?here\nIf the initial terms are agreeable to the parties, the LOI is signed by authorized representatives.\n2) Due Diligence Phase\nAfter the agreement of the intended major terms of the transaction, through an LOI/CNDA, and before entering a Purchase & Sale Agreement, the Seller will work with the Buyer in providing and facilitating with information gathering. The information gathered by the buyer is essential in order to make a determination whether or not to go forward with the next phase of the transaction.\nThe Due Diligence inspections include review of the selling company’s financial documents, such as the balance sheet, revenue reports, loan documents, leases, copies of ongoing performance contracts, employment agreements, and verification of authority to enter into a binding contract to complete the intended transaction. This stage is a great time to get a business valuation from an experienced business broker or accountant.\nIf the information provided to and uncovered by the Seller is satisfactory in order to make a final offer as to the major terms of the buy/sell transaction, the parties will then enter into a Purchase & Sale Agreement.\n3) Purchase & Sale Agreement\nA well-drafted Purchase & Sale Agreement document (the “PSA”) will provide proper protections for all parties, the necessary flexibility to allow the deal to close or be terminated if appropriate, but rigidity to help ensure the transaction will close as the parties originally intend.\nTypical essential components of the PSA will include:\nProper identification of what is to be transferred;\nThe purchase price and how it is to be allocated;\nFormulas for determining certain costs and prices for inventories;\nAssigned and assumed contracts, benefits & liabilities;\nRepresentations and warranties;\nContingencies to completing the transaction;\nTerms of the due diligence inspections;\nWhat will become of the deposit;\nDetails regarding the intended closing including deliveries to be made;\nHow the intended transaction may be terminated and any rights and obligations due to termination; and\nPre- and post-closing obligations.\nOnce the PSA is properly executed by all parties, the next phase will be to enter into the Final Due Diligence period, and for the parties and their representatives to work to ensure contingencies to Closing are satisfied, to the extent they are in the control of the parties.\n4) Final Inspections\nThe Final Inspections should always include:\nProvision, and review, of financials as reporting periods that come due since initial reports were provided;\nThe provision, and review, of representations and warranties as they change;\nTitle, lien and judgment searches to determine if any actions can and should be taken to remove encumbrances on the Seller and the Seller’s assets; and\nPhysical inspection of the assets, inventories, real property, land and personal property.\nAdditionally, during this time and after, efforts should be taken in order to help ensure all contingencies are met in order to satisfy conditions to Closing, most especially, necessary third-party approvals (like those necessary to the assignment and assumption of necessary contracts, obligations and benefits); and obtaining any financing necessary to fund the deal.\n5) The Closing\nPrior to scheduling the Closing, all contingencies that can be satisfied prior to Closing should be satisfied, or there should be a strong reasonable belief that such conditions will be satisfied. Certain contingencies are not expected to be met until immediately prior to or during the Closing process, such as a final check for relevant encumbrances and of course, payment of the purchasae price.\nAt the Closing, the following documents should have been prepared, and the form of which agreed upon by the parties, and these documents will need to be executed and tendered to the proper party, or parties, to the transaction:\nA Bill of Sale;\nAn Assignment & Assumption Agreement;\nCertified Copies of Resolutions showing approval of the transaction;\nA Power of Attorney to allow for any post-closing actions to transfer titles or to assign & assume contracts;\nThe Closing Statement; and\nProof of Funding.\nAs you can see, there are many, many components to buying or selling a business. A proactive lawyerexperienced in such transactions can be the difference between getting a deal done and spinning wheels (along with money). Always make sure you understand which phases your transaction is in, and who is responsible for taking the next steps in order to move closer to closing.\nJoshua Logan of Achieve Legal is a Florida-licensed Franchise and M&A Attorney, serving clients throughout Florida from Orlando, and nationwide through association with local counsel. You can call him at (407) 502-2580, or he can be reached by e-mail at JLogan@AchieveLegal.com","This episode features an excerpt from the Nolo book “How to Run a Thriving Business: Strategies for Success and Satisfaction,” by Attorney Ralph Warner.\nShould You Buy a Franchise?\nAlmost every franchise presentation emphasizes that nation-wide franchise businesses take in about 50% of the retail sales dollar. What isn’t said is that the great majority of these dollars come from just a few categories: automobiles, gasoline, lodging, and fast food. Beyond these mega-buck fields, only a small percent of the money retailers take in goes to franchised operations.\nSome Exceptions to the Rule:\nAlthough I’ve become convinced that sinking money in a franchise is generally a bad investment, and no way to get your start in business, let me start with a couple of exceptions to my own rule. The first exception involves franchises built around continent-spanning communications networks, such as national hotel and motel groups, which maintain 800 phone numbers, and web sites allowing travelers to easily book reservations. This is not to say, of course, that any particular hotel, motel, or auto rental franchise is a good deal, only that, unlike many other franchises, they do sell something of real value. Second, franchises with brands that really are famous and highly regarded can sometimes be worth the high cost. Franchisees, especially those who bought in years ago at good locations, have made big profits in McDonald’s, Pizza Hut, Motel 6, and other world-famous franchises. But it’s been decades since an ordinary person could afford to purchase and build out one of the relatively few guilt-edged franchises.\nThe High Cost of Franchising\nThe biggest problem with many, if not most, franchises is depressingly simple: they charge too much for a business that doesn’t have enough value to justify the high upfront and ongoing costs. To help understand why this is true, answer these three simple questions: how hard is it to make a sandwich? How hard is it to clean a house? How hard is it to put grout in tile? If your answer is “not very,” then I have another question for you: why pay a franchise operator a large sum to teach you how to do one of these or other simple tasks, when you could learn to do it on your own for far less?\nHow Much Will You Have to Pay?\nFirst, you’ll pay an upfront franchise fee, which might be $30,000 to $70,000 or more, for a little known housecleaning service. Typically, you’ll also be required to pay the franchise 3-6% of your monthly gross revenue. Big name fast food operators such as Wendy’s, McDonald’s, Burger King, and Subway, typically charge between 8% and 11%, plus a few cents on the dollar for a franchiser’s marketing effort. Put these fees together and it means that in addition to paying the upfront fee for the franchise, you’ll usually have to pay the franchiser six to ten cents or more of every dollar of revenue, and if the franchiser requires that you buy goods or services either directly from it, or from an approved supplier, your costs will probably be higher, because franchisers commonly charge substantially more than do suppliers on the open market. Let’s assume that the extra cost would amount to two cents out of every dollar of revenue. Add it all up, and you’ll likely pay the franchiser ten cents of every dollar you take in. This is a huge burden to your long-term profitability.\nBut Won’t a High Volume of Sales Compensate for These Costs?\nConsider that the entire profit margin of many small businesses is less than ten cents on the dollar, and few businesses do much better. And, of course, your franchise fees don’t get your business open. If you want to open a business that has a high startup cost, such as a restaurant, you’ll need to build or remodel a physical space, purchase equipment, and train employees, things that are usually more expensive when you must conform to a franchiser’s many specifications. But if you don’t get a leg up from a franchiser, how will you get the knowledge and skills you need to open a successful business? Chances are, you can learn on your own for free. For example, if you’re interested in opening a lock shop, nail salon, or coffee shop, get a job in one for a few months, instead of buying a franchise. Not only will you learn much about how the business works, but you’ll be paid to do it.\nBut What About All the Benefits of the Franchiser’s Marketing Efforts?\nNational franchise outfits rarely do a good job of promoting their local franchises, in part because they typically rely on broadcast and print media campaigns, which for small businesses are usually an inefficient way to use precious marketing resources. Even worse, because franchisers are usually headquartered outside a franchisee’s area, they’re not equipped to implement the many types of low-cost local marketing that can be extremely effective.\nBut Won’t the Franchiser teach me How to Run My Business?\nOf course, to many prospective purchasers, the big appeal of buying a franchise is that someone else has figured out how to run the business. Just pay your money, and the franchiser will explain in great detail exactly how to make a donut, or wash a car, or sell sneakers. Often overlooked is the fact that operating a business by following an instruction manual can also be a big negative. Instead of having a chance to exercise your creativity and imagination to improve and change your products and services, you’ll be sentenced to endlessly repeating someone else’s recipe. Some people think they won’t mind running a boring, uncreative business, as long as it’s solidly profitable. Well, maybe. But the truth is that you are limited from actively using your intelligence and creativity to adjust the business to fit local circumstances, or take advantage of what you learn, which is almost sure to make the business less likely to succeed.\nWatch Out for Franchise Contracts\nAnother reason to avoid a franchise is that franchise contracts are stacked against you. These contracts, which typically run fifty pages or more, are written and rewritten by skilled lawyers, to be sure the franchiser remains firmly in control of the relationship. Like buying a car or an insurance policy, you have no chance to negotiate a change to even one word of these agreements, which by itself should tell you all you need to know about the one-sided nature of your future relationship. Here are just a few of the ways the fine print of these contracts benefit the franchiser:\nYou Can’t Compete.\nShould you wish to close a franchise and open a similar independent business, you’re typically prohibited from doing so for at least three to five years.\nYou’ll Need Approval to Sell the Franchise\nTo sell your franchise sometime in the future, you’ll probably have to get the franchiser’s approval. Not only can this make the sales process more difficult -- the franchiser might reject a purchaser you consider well-qualified -- but it means the prospective purchaser will have to agree to the terms in the then-current franchise contract.\nYou’ll Have to Travel to the Franchiser’s Home State for Legal Disputes\nIf you get into a legal dispute with a franchiser, the franchiser may require that you file your lawsuit on the opposite side of the country, and be subject to the law of the state whose courts are most favorable to the franchiser.\nYou Must Buy Goods and Services From the Franchiser\nAnd, of course, the contract may require that you buy supplies, goods, and even services, such as marketing and advertising services, from the franchiser. Although this sometimes make sense – all chicken sold at Big Ben’s Bird House should look and taste the same – often it’s just another way that franchisers take money away from franchisees.\nHow do you evaluate a Franchise?\nWell, despite my anti-franchise arguments, you may remain convinced that a particular franchise really does have such a valuable name and reputation that buying in might be a good deal. If that’s so, I recommend you go through the following steps:\nStep 1: Get the Franchise Circular\nBefore you do anything else, ask the franchiser for a copy of its Uniform Franchise Offering Circular (UFOC), a federally mandated document that contains loads of information about the franchise company’s history, operations, franchise network, rules, and costs.\nStep 2: Talk to Franchisees\nIf you talk to a number of people who already own a franchise in the outfit you’re attracted to, I can virtually guarantee you’ll learn many interesting things the salespeople somehow never told you. Check out the list of franchisees who have left the system in the last year, and call some of these people. If the franchiser suggests that you talk to particular franchisees, don’t bother. One way or another, these people are part of the franchiser’s sales team, and are unlikely to give you fully objective information.\nStep 3: Look at All the Costs\nCarefully study sections five and six of the UFOC for answers to these questions: how much is the upfront franchise fee? How much money do you have to pay the franchiser by way of a monthly fee, often called a royalty? Is there an additional marketing or advertising fee? Are there other fees, for such things as travel, training, audits, and attorneys? How much will it cost you to actually get into business – that is, to construct a building or buy equipment?\nStep 4: Find Out About Recent Lawsuits Against the Franchiser\nYou’ll want to know whether any unhappy franchisees have sued the company recently. Any such lawsuits should be listed near the beginning of the UFOC. If you find a history of litigation, contact the people involved to get their side of the story.\nStep 5: Check Out the Competition\nOpen the phone book, and count the competitors in the particular market niche. In any popular field, chances are there will be a number of other competing franchise operations, as well as many independents. Do the franchise operators really have an advantage over the well-run independents?\nStep 6: Analyze Your Options.\nFinally, compare the cost of a franchise to the cost of opening and operating a similar independent business for a year. If, for example, you’d save $50,000 by operating independently, pretend you invest this money in United States bonds, and leave it there until you retire. Obviously, it makes sense to invest in the franchise only if you are pretty sure you would earn more than you would take in running an independent business plus your annual investment income.\nThe material you have just heard is excerpted from the Nolo book, “How to Run a Thriving Business: Strategies for Success and Satisfaction,” by Attorney Ralph Warner.\nCopyright by Nolo. For over thirty years, Nolo has published reliable plain English books, software, and forms. Check it out at www.nolo.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:845bc5a2-36b0-4621-ba32-e2202334bcfc>","<urn:uuid:f5e09be0-02bb-4a68-b50e-08150caab3b5>"],"error":null}
{"question":"Good morning! What are the growth patterns and mature sizes of Aeonium and Rosemary plants? I'd like to understand how they compare.","answer":"Aeoniums can vary significantly in size, with some large specimens reaching 5 to 6 feet in height, while others are compact and form mounds of short-stemmed rosettes. They grow actively during fall and spring. Rosemary shrubs, on the other hand, can grow from 1 to 6 feet tall and spread out to 4 to 6 feet depending on the variety. They come in both shrub and groundcover forms, and don't require regular pruning, though they can be lightly sheared for a more formal appearance.","context":["What Is the Aeonium Plant?\nSucculents can be interesting and fun plants to grow, thriving outdoors in areas with dry summers and warm winters. In regions where winters are cool, they appreciate summer sun outdoors and then can grow indoors as houseplants when weather cools. Aeonium plants (Aeonium spp.) make up a group of succulents that are native to Africa and typically have leaves radially arranged in rosettes.\nAlthough dozens of plants belong to the genus Aeonium, they share a number of common characteristics. All have leafless stems topped by leaves that are radially arranged in a structure called a rosette. Leaves in the plant's rosettes are thick and have a waxy outer covering, and stems tend to be thick and resemble branches on larger specimens. Some large aeoniums can reach a mature height of 5 feet or more, while others are compact and form mounds of short-stemmed rosettes. All aeoniums are frost-tender, although several can tolerate a few hours of freezing temperatures. Generally, they are suitable for growing outdoors in U.S. Department of Agriculture plant hardiness zones 9 through 11.\nLight, Soil and Potting\nAeoniums thrive in full sun and need at least six hours of full sun each day. They can tolerate shade for a few hours, but many types will not develop their full leaf colors without full sun. They generally prefer a sandy soil mix with good drainage appropriate for cacti. When potted, they do best in terra cotta pots that allow soil to dry readily between waterings. Aeoniums cannot tolerate soil that remains moist for long periods; stems will soften and eventually rot when plants are kept under wet or soggy conditions.\nFeeding and Maintenance\nMost aeoniums grow actively during fall and spring, and respond to fertilizing every four to six weeks with a balanced, 10-10-10 formula while growing. When plants slow their growth during winter and summer, you should withhold fertilizer and cut back on watering. Aeoniums are unusual because rosettes tend to die after they flower. You can either cut these spent rosettes back when they fade, or to prevent flowering, take cuttings of terminal stems and plant these in moist soil. This helps control the size of your plant and stops it from blooming, preventing dieback of terminal rosettes and helps increase the size of your aeonium planting.\nDozens of aeonium species exist, each containing many cultivars. Some of the best choices include the black rose aeonium (A. arboreum \"Zwartkop\"), which has nearly black rosettes and can reach a height of 5 or 6 feet in warm climates; purple aeonium (A. arborerum \"Atropurpureum\"), with purple-red rosettes on a 2- to 3-feet tall plant; and green rose buds (A. aureum \"Green Rose Buds\"), a plant that forms up to 50 rosettes that are 4- to 10-inches wide. They have an apple green color on a compact plant that tolerates partial shade. Some aeoniums are multi-colored, such as Haworth's aeonium (A. haworthii \"Variegated\"), with red-edged green leaves, and the cultivar called sunburst aeonium (A. arboreum \"Luteovariegatum\"), a cold-tolerant, bushy plant with green leaves outlined in pale yellow.\nJoanne Marie began writing professionally in 1981. Her work has appeared in health, medical and scientific publications such as Endocrinology and Journal of Cell Biology. She has also published in hobbyist offerings such as The Hobstarand The Bagpiper. Marie is a certified master gardener and has a Ph.D. in anatomy from Temple University School of Medicine.","How to Grow a Drought Tolerant Rosemary Shrub\nA rosemary shrub isn't just for cooking—rosemary is also a great ornamental plant that does well in the landscape, including drought tolerant gardens.\nBenefits of Rosemary Shrubs\nWhen people look for an attractive evergreen shrub, rosemary is not what probably comes to mind right away. But when you think about it, why not use a rosemary bush as an ornamental shrub in the garden? A rosemary shrub has a lot of the things people look for when selecting a new plant for their garden—evergreen foliage, an attractive growth habit and of course, pretty flowers!\nDiscover the top 15 drought-tolerant plants that can handle dry weather.\nThe scientific name of rosemary (Rosmarinus officinalis), is Latin for “dew of the sea.” It was said to adorn the goddess, Aphrodite, when she emerged from the sea. This aromatic shrub is native to the Mediterranean region and does especially well in drought tolerant gardens. The foliage consists of dark green, needle-like aromatic foliage, which is evergreen. In spring and fall, small blue flowers form, transforming these evergreen shrubs into something beautiful.\nDiscover 5 attractive drought tolerant shrubs for your garden.\nHow to Grow Rosemary\nA rosemary shrub can grow from 1 to 6 feet tall and spread out to 4 to 6 feet depending on the variety. This makes it a versatile plant in the landscape. If you are looking for a plant for an informal hedge or if you want an attractive groundcover that spills over a wall or planter, rosemary is a great choice. Rosemary can also be planted in groups of 3 to 5 in an informal arrangement, spaced at least 6 feet apart.\nDiscover 15 ways to conserve water in the garden.\nIf your garden is located in zones 8 and above, you are fortunate that you can grow rosemary shrubs outside all year. However, if you live in a colder region, don’t despair—you can grow rosemary, too! Simply plant rosemary in a pot and bring it inside when winter temperatures dip down into the 20s. Learn how plant zones work and how to find yours.\nLike most herbs, rosemary isn’t a fussy shrub and does best when left alone. What they do need is an area with well-drained soil and a sunny exposure. While they can be grown in light shade, the foliage will be sparse and it may not flower. Water deeply and allow to the soil to dry out before watering again. Rosemary doesn’t like ‘wet feet’, which makes them a great choice for the drought tolerant garden. They do equally well in slightly acidic or slightly alkaline soils. Rosemary shrubs don’t need regular pruning. Although they can be lightly sheared for a more formal appearance.\nTypes of Rosemary to Grow\nRosemary plants are propagated by cuttings and are generally found in two growing forms: as a shrub or a groundcover. There are several varieties available that offer variations in growth habit, foliage and flower color. Low-growing varieties include ‘Irene’ or ‘Prostratus’. If you like multi-colored foliage, you may want to try ‘Aureus’ with its speckled foliage. ‘Golden Rain’ has streaks of yellow. While the predominant flower color is blue for most varieties of rosemary, there are white ‘Albus’ and pink ‘Roseus’ flowered varieties available.\nRosemary has much more to offer than simply the flavor its leaves add to our favorite foods. Why not utilize it to its fullest extent as both an herb and an ornamental plant!\nNext, check out drought resistant trees and plants to grow for birds."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f4fef257-c053-4976-a403-cf65e0115b9b>","<urn:uuid:4ab9d91c-2f74-42ed-87fd-70768119f53f>"],"error":null}
{"question":"How did Mohammad Hatta's early education and experiences in the Netherlands shape his political activism and leadership?","answer":"Mohammad Hatta received Dutch-language secondary schooling in Jakarta and later studied at the Rotterdam School of Commerce in the Netherlands. During his ten-year stay in the Netherlands (1922-1932), he emerged as the overseas leader of the Indonesian nationalist movement and developed connections with other independence movements, including meeting Indian leader Jawaharlal Nehru. In 1927, he was imprisoned for writing allegedly treasonous articles but successfully defended himself in court with a rousing speech that, when published in Indonesia, inspired the independence movement. These experiences in the Netherlands established him as one of the foremost intellectuals in the Afro-Asian anti-colonial movement.","context":["Mohammad Hatta (1902-1980), one of the foremost intellectuals in the Afro-Asian anticolonial movement, was a leader of the Indonesian nationalist movement leading to its independence in 1945. He was a champion of non-alignment and of socialism grounded in Islam.\nMohammad Hatta was born in Bukittinggi, West Sumatra, Indonesia, on August 12, 1902. Although his father died while he was an infant, he was raised in a secure, well-to-do family environment which encouraged scholarly achievement and faithfulness to Islam. These characteristics became his signature during his career as one of the foremost intellectuals in the Afro-Asian anti-colonial movement.\nEducation and Political Activism\nAs a child Hatta received the best education available in the Netherlands Indies, including Dutch-language secondary schooling in Jakarta. By the time he left for the Netherlands to continue his studies at the Rotterdam School of Commerce he had already developed a keen interest in political affairs, having served while still in his teens as an officer of youth organizations in West Sumatra and Jakarta. Shortly after arriving in Rotterdam he became treasurer of the Indonesian Union (Perhimpoenan Indonesia) at a time when it was adopting explicitly political programs.\nHatta did not return to Indonesia, as he and his nationalist compatriots called the colony, until 1932. During these ten years he emerged as the overseas leader of the Indonesian nationalist movement and became acquainted with counterparts representing other independence movements, including the Indian leader Jawaharlal Nehru. In 1927 Hatta was accused by Dutch authorities of writing treasonous articles. After being imprisoned for a year and a half, Hatta successfully defended himself and his associates in a rousing, uncompromising courtroom speech which, upon publication in Indonesia, set a militant tone for the independence movement.\nAfter his return to Indonesia Hatta and his compatriot Sutan Sjahrir sought to join forces with other nationalist leaders, including Sukarno. The organizational efforts of these men were thwarted by the repressive policies of the colonial state. First Sukarno was arrested and exiled to Flores; in 1934 Hatta and Sjahrir were arrested and eventually imprisoned in the much harsher prison camp at Digul, Western New Guinea. They later were relocated to the island of Banda, where they continued to formulate their ideas and express them in articles which were circulated in many Indonesian cities. When the Dutch in the Far East surrendered to Japan in early 1942, Hatta, back in Jakarta, assumed a new leadership role.\nOccupation and Independence\nTogether with Sukarno and Sjahrir, Hatta participated in the Japanese occupation government. He remained in communication with underground elements of the nationalist movement, and he used his position as vice-chairman of Putera—a mass organization created in 1943—to continue political preparations for independence. With the collapse of Japan's imperial ambitions, Indonesia became independent on August 17, 1945. The days leading up to this event were tumultuous and included a brief \"kidnapping\" of Sukarno and Hatta by youths who were pressing for dramatic action on the part of their leaders. Sukarno and Hatta signed the proclamation of independence and quickly were designated president and vice-president, respectively, by the provisional parliament.\nThe next four years were a period of armed struggle against the Dutch, who were intent upon regaining control of the East Indies. For Hatta it was a time of intense political activity which included another detention by the Dutch and a deepening rift with Sukarno. As negotiations leading to the December 1949 Dutch cession of sovereignty proceeded, Hatta's international stature and his ease and competency in dealing with Europeans were instrumental in determining the outcome. Among other things he successfully opposed imposition of a federal system designed by the Dutch.\nPolitical Ideas and Influence\nAlthough Hatta subsequently was overshadowed by the more flamboyant and aggressive Sukarno, many of his positions became important not only in Indonesia but internationally. He was an articulate champion of non-alignment and of socialism based mainly on cooperatives and decentralization. He also believed that Indonesian socialism should be firmly grounded in Islam.\nBecause he was from Sumatra and was so different in personal style from Sukarno, a Javanese, the two came to be regarded as thoroughly complementary. But these same differences caused their partnership to break down, and when Sukarno abandoned parliamentary processes in favor of \"guided democracy\" in the later 1950s the gulf between them became unbridgeable. The collapse of Sukarno's regime in confusion and disrepute in 1966 did not lead to Hatta's return to a formal political position, however; the successor government under Suharto was dominated by the army, an organization which Hatta regarded as corrupt, inefficient, and unsuited for governance under any circumstances.\nMohammad Hatta remained in the background of Indonesian politics throughout the 1970s except for a brief period in 1978 when he agreed to serve as general chairman of the Foundation for the Institute of Constitutional Awareness. The foundation provided a forum for bold expressions of criticism from a wide range of opponents to the Suharto government. It was unable to weaken significantly the army's control of public institutions, however, and it lost much of the momentum which it may have been gaining when Hatta died on March 14, 1980.\nFurther Reading on Mohammad Hatta\nIn view of the long and complex relationship between the two men, it should be no surprise that biographies of Sukarno are a good source of information on Hatta. See, for example, J. D. Legge, Sukarno: A Political Biography (1972). George Kahin's study of Nationalism and Revolution in Indonesia (1952) is a thorough account of the movement in which Hatta was absorbed. The best source for the post-independence period (not covered by Kahin) is Herbert Feith, The Decline of Constitutional Democracy in Indonesia (1962). A brief but insightful tribute to Hatta by Kahin appears under the title, \"In Memorium: Mohammad Hatta (1902-1980), \" in Indonesia (1980). A selection of Hatta's writings may be found in Herbert Feith and Lance Castles, editors, Indonesian Political Thinking, 1945-1965 (1970).\nAdditional Biography Sources\nHatta, Mohammad, Mohammad Hatta: memoirs, Jakarta: Tintamas Indonesia, 1979.\nHatta, Mohammad, Mohammad Hatta, Indonesian patriot: memoirs, Singapore: Gunung Agung, 1981.\nRose, Mavis, Indonesia free: a political biography of Mohammad Hatta, Ithaca, N.Y.: Cornell Modern Indonesia Project, Southeast Asia Program, Cornell University, 1987."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:69ac8387-dc46-45d6-9580-49cebe43848d>"],"error":null}
{"question":"How do comparative genomics and machine learning techniques contribute to gene prediction, and what role does odorant binding protein OBP2 play in mosquito-host interactions?","answer":"Comparative genomics approaches to gene prediction leverage the fact that functional elements undergo mutation at a slower rate due to natural selection, allowing detection of genes by comparing genomes of related species. Machine learning techniques like neural networks and support vector machines are used alongside other methods to predict biological features such as splice sites. Programs like CONTRAST use machine learning in a two-step process - first classifying splice sites and then constructing a full gene model. Regarding OBP2's role, it shows significantly higher expression in female mosquito antennae compared to males and belongs to a family of proteins that bind and transport odor molecules from sensillae to G-protein-coupled receptors in olfactory sensory neurons. Since the olfactory system is crucial for host-seeking behavior in mosquitoes, OBP2 likely plays an important role in female mosquito feeding behavior.","context":["In computational biology gene prediction or gene finding refers to the process of identifying the regions of genomic DNA that encode genes. This includes protein-coding genes as well as RNA genes, but may also include prediction of other functional elements such as regulatory regions. Gene finding is one of the first and most important steps in understanding the genome of a species once it has been sequenced.\nIn its earliest days, \"gene finding\" was based on painstaking experimentation on living cells and organisms. Statistical analysis of the rates of homologous recombination of several different genes could determine their order on a certain chromosome, and information from many such experiments could be combined to create a genetic map specifying the rough location of known genes relative to each other. Today, with comprehensive genome sequence and powerful computational resources at the disposal of the research community, gene finding has been redefined as a largely computational problem.\nDetermining that a sequence is functional should be distinguished from determining the function of the gene or its product. Predicting the function of a gene and confirming that the gene prediction is accurate still demands in vivo experimentation through gene knockout and other assays, although frontiers of bioinformatics research are making it increasingly possible to predict the function of a gene based on its sequence alone.\nMany aspects of structural gene prediction are based on current understanding of underlying biochemical processes in the cell such as gene transcription, translation, protein–protein interactions and regulation processes, which are subject of active research in the various Omics fields such as Transcriptomics, Proteomics, Metabolomics, and more generally structural and functional genomics.\nIn empirical (similarity, homology or evidence-based) gene finding systems, the target genome is searched for sequences that are similar to extrinsic evidence in the form of the known expressed sequence tags, messenger RNA (mRNA), protein products, and homologous or orthologous sequences. Given an mRNA sequence, it is trivial to derive a unique genomic DNA sequence from which it had to have been transcribed. Given a protein sequence, a family of possible coding DNA sequences can be derived by reverse translation of the genetic code. Once candidate DNA sequences have been determined, it is a relatively straightforward algorithmic problem to efficiently search a target genome for matches, complete or partial, and exact or inexact. Given a sequence, local alignment algorithms such as BLAST, FASTA and Smith-Waterman look for regions of similarity between the target sequence and possible candidate matches. Matches can be complete or partial, and exact or inexact. The success of this approach will be limited by the contents and accuracy of the sequence database.\nA high degree of similarity to a known messenger RNA or protein product is strong evidence that a region of a target genome is a protein-coding gene. However, to apply this approach systemically requires extensive sequencing of mRNA and protein products. Not only is this expensive, but in complex organisms, only a subset of all genes in the organism's genome are expressed at any given time, meaning that extrinsic evidence for many genes is not readily accessible in any single cell culture. Thus, in order to collect extrinsic evidence for most or all of the genes in a complex organism, many hundreds or thousands of different cell types must be studied, which itself presents further difficulties. For example, some human genes may be expressed only during development as an embryo or fetus, which might be difficult to study for ethical reasons.\nDespite these difficulties, extensive transcript and protein sequence databases have been generated for human as well as other important model organisms in biology, such as mice and yeast. For example, the RefSeq database contains transcript and protein sequence from many different species, and the Ensembl system comprehensively maps this evidence to human and several other genomes. It is, however, likely that these databases are both incomplete and contain small but significant amounts of erroneous data.\nNew high-throughput Transcriptome sequencing technologies such as RNA-Seq and ChIP-sequencing open opportunities for incorporating additional extrinsic evidence into gene prediction and validation, and allow structurally rich and more accurate alternative to previous methods of measuring Gene expression such as Expressed sequence tag or DNA microarray.\nMajor challenges involved in gene prediction involve dealing with sequencing errors in raw DNA data, dependence on the quality of the Sequence assembly, handling short reads, Frameshift mutations, Overlapping genes and incomplete genes.\nIn prokaryotes it's essential to consider Horizontal gene transfer when searching for gene sequence homology. An additional important factor which is somewhat underutilized in current gene detection tools is existence of gene clusters - Operons in both prokaryotes and eukaryotes. Most of the popular gene detectors treat each gene in isolation, independent of others which is not biologically accurate.\nAb initio methods\nAb Initio gene prediction is an intrinsic method based on gene content and signal detection. Because of the inherent expense and difficulty in obtaining extrinsic evidence for many genes, it is also necessary to resort to Ab initio gene finding, in which genomic DNA sequence alone is systematically searched for certain tell-tale signs of protein-coding genes. These signs can be broadly categorized as either signals, specific sequences that indicate the presence of a gene nearby, or content, statistical properties of protein-coding sequence itself. Ab initio gene finding might be more accurately characterized as gene prediction, since extrinsic evidence is generally required to conclusively establish that a putative gene is functional.\nIn the genomes of prokaryotes, genes have specific and relatively well-understood promoter sequences (signals), such as the Pribnow box and transcription factor binding sites, which are easy to systematically identify. Also, the sequence coding for a protein occurs as one contiguous open reading frame (ORF), which is typically many hundred or thousands of base pairs long. The statistics of stop codons are such that even finding an open reading frame of this length is a fairly informative sign. (Since 3 of the 64 possible codons in the genetic code are stop codons, one would expect a stop codon approximately every 20–25 codons, or 60–75 base pairs, in a random sequence.) Furthermore, protein-coding DNA has certain periodicities and other statistical properties that are easy to detect in sequence of this length. These characteristics make prokaryotic gene finding relatively straightforward, and well-designed systems are able to achieve high levels of accuracy.\nAb initio gene finding in eukaryotes, especially complex organisms like humans, is considerably more challenging for several reasons. First, the promoter and other regulatory signals in these genomes are more complex and less well-understood than in prokaryotes, making them more difficult to reliably recognize. Two classic examples of signals identified by eukaryotic gene finders are CpG islands and binding sites for a poly(A) tail.\nSecond, splicing mechanisms employed by eukaryotic cells mean that a particular protein-coding sequence in the genome is divided into several parts (exons), separated by non-coding sequences (introns). (Splice sites are themselves another signal that eukaryotic gene finders are often designed to identify.) A typical protein-coding gene in humans might be divided into a dozen exons, each less than two hundred base pairs in length, and some as short as twenty to thirty. It is therefore much more difficult to detect periodicities and other known content properties of protein-coding DNA in eukaryotes.\nAdvanced gene finders for both prokaryotic and eukaryotic genomes typically use complex probabilistic models, such as hidden Markov models (HMMs), in order to combine information from a variety of different signal and content measurements. The GLIMMER system is a widely used and highly accurate gene finder for prokaryotes. GeneMark is another popular approach. Eukaryotic ab initio gene finders, by comparison, have achieved only limited success; notable examples are the GENSCAN and geneid programs. The SNAP gene finder is HMM-based like Genscan and attempts to be more adaptable to different organisms, addressing problems related to using a gene finder on a genome sequence that it was not trained against. A few recent approaches like mSplicer, CONTRAST, or mGene also use machine learning techniques like support vector machines for successful gene prediction. They build a discriminative model using hidden Markov support vector machines or conditional random fields to learn an accurate gene prediction scoring function.\nAmong the derived signals used for prediction are statistics resulting from the sub-sequence statistics like k-mer statistics, Isochore (genetics) or Compositional domain GC composition/uniformity/entropy, sequence and frame length, Intron/Exon/Donor/Acceptor/Promoter and Ribosomal binding site vocabulary, Fractal dimension, Fourier transform of a pseudo-number-coded DNA, Z-curve parameters and certain run features.\nIt has been suggested that signals other than those directly detectable in sequences may improve gene prediction. For example, the role of secondary structure in the identification of regulatory motifs has been reported. In addition, it has been suggested that RNA secondary structure prediction helps splice site prediction.\nNeural networks are computational models which excel at machine learning and pattern recognition. Neural networks must be trained with example data before being able to generalise for experimental data, and tested against benchmark data. Neural networks are able to come up with approximate solutions to problems which are hard to solve algorithmically, provided there is sufficient training data. When applied to gene prediction, neural networks can be used alongside other ab initio methods to predict or identify biological features such as splice sites. One approach involves using a sliding window, which traverses the sequence data in an overlapping manner. The output at each position is a score based on whether the network thinks the window contains a donor splice site or an acceptor splice site. Larger windows offer more accuracy but also require more computational power. A neural network is an example of a signal sensor as its goal is to identify a functional site in the genome.\nPrograms such as Maker combine extrinsic and ab initio approaches by mapping protein and EST data to the genome to validate ab initio predictions. Augustus, which may be used as part of the Maker pipeline, can also incorporate hints in the form of EST alignments or protein profiles to increase the accuracy of the gene prediction.\nComparative genomics approaches\nAs the entire genomes of many different species are sequenced, a promising direction in current research on gene finding is a comparative genomics approach.\nThis is based on the principle that the forces of natural selection cause genes and other functional elements to undergo mutation at a slower rate than the rest of the genome, since mutations in functional elements are more likely to negatively impact the organism than mutations elsewhere. Genes can thus be detected by comparing the genomes of related species to detect this evolutionary pressure for conservation. This approach was first applied to the mouse and human genomes, using programs such as SLAM, SGP and TWINSCAN/N-SCAN and CONTRAST.\nTWINSCAN examined only human-mouse synteny to look for orthologous genes. Programs such as N-SCAN and CONTRAST allowed the incorporation of alignments from multiple organisms, or in the case of N-SCAN, a single alternate organism from the target. The use of multiple informants can lead to significant improvements in accuracy.\nCONTRAST is composed of two elements. The first is a smaller classifier, identifying donor splice sites and acceptor splice sites as well as start and stop codons. The second element involves constructing a full model using machine learning. Breaking the problem into two means that smaller targeted data sets can be used to train the classifiers, and that classifier can operate independently and be trained with smaller windows. The full model can use the independent classifier, and not have to waste computational time or model complexity re-classifying intron-exon boundaries. The paper in which CONTRAST is introduced proposes that their method (and those of TWINSCAN, etc.) be classified as de novo gene assembly, using alternate genomes, and identifying it as distinct from ab initio, which uses a target 'informant' genomes.\nComparative gene finding can also be used to project high quality annotations from one genome to another. Notable examples include Projector, GeneWise and GeneMapper. Such techniques now play a central role in the annotation of all genomes.\nPseudogenes are close relatives of genes, sharing very high sequence homology, but being unable to code for the same protein product. Whilst once relegated as byproducts of gene sequencing[disambiguation needed], increasingly, as regulatory roles are being uncovered, they are becoming predictive targets in their own right. Pseudogene prediction utilises existing sequence similarity and ab initio methods, whilst adding additional filtering and methods of identifying pseudogene characteristics.\nSequence similarity methods can be customised for pseudogene prediction using additional filtering to find candidate pseudogenes. This could use disablement detection, which looks for nonsense or frameshift mutations which would truncate or collapse an otherwise functional coding sequence. Additionally, translating DNA into proteins sequences can be more effective than just straight DNA homology.\nContent sensors can be filtered according to the differences in statistical properties between pseudogenes and genes, such as a reduced count of CpG islands in pseudogenes, or the differences in G-C content between pseudogenes and their neighbours. Signal sensors also can be honed to pseudogenes, looking for the absence of introns or polyadenine tails. \nMetagenomic gene prediction\nMetagenomics tools also fall into the basic categories of using either sequence similarity approaches (MEGAN4) and ab initio techniques (GLIMMER-MG).\nGlimmer-MG is an extension to GLIMMER that relies mostly on an ab initio approach for gene finding and by using training sets from related organisms. The prediction strategy is augmented by classification and clustering gene data sets prior to applying ab initio gene prediction methods. The data is clustered by species. This classification method leverages techniques from metagenomic phylogenetic classification. An example of software for this purpose is, Phymm, which uses interpolated markov models, and PhymmBL which integrates BLAST into the classification routines.\nMEGAN4 uses a sequence similarity approach, using local alignment against databases of known sequences, but also attempts to classify using additional information on functional roles, biological pathways and enzymes. As in single organism gene prediction, sequence similarity approaches are limited by the size of the database.\nFragGeneScan and MetaGeneAnnotator are popular gene prediction programs based on Hidden Markov model. These predictors account for sequencing errors, partial genes and work for short reads.\n- geneid, SGP2\n- Glimmer, GlimmerHMM\n- StarORF — A multi-platform and web tool for predicting ORFs and obtaining reverse complement sequence\n- Maker - A portable and easily configurable genome annotation pipeline\n- \"An overview of the current status of eukaryote gene prediction strategies\". Gene 461. 2010. doi:10.1016/j.gene.2010.04.008.\n- \"A beginner's guide to eukaryotic genome annotation\". Nature Reviews Genetics 13: 329–342. May 2012. doi:10.1038/nrg3174.\n- Korf I. (2004-05-14). \"Gene finding in novel genomes\". BMC Bioinformatics 5: 59–67. doi:10.1186/1471-2105-5-59. PMC 421630. PMID 15144565.\n- Rätsch, Gunnar; Sonnenburg, S; Srinivasan, J; Witte, H; Müller, KR; Sommer, RJ; Schölkopf, B (2007-02-23). \"Improving the C. elegans genome annotation using machine learning\". PLoS Computational Biology 3 (2): e20. doi:10.1371/journal.pcbi.0030020. PMC 1808025. PMID 17319737.\n- Gross, Samuel S; Do, CB; Sirota, M; Batzoglou, S (2007-12-20). \"CONTRAST: A Discriminative, Phylogeny-free Approach to Multiple Informant De Novo Gene Prediction\". Genome Biology 8 (12): R269. doi:10.1186/gb-2007-8-12-r269. PMC 2246271. PMID 18096039.\n- Schweikert G, Behr J, Zien A, et al. (July 2009). \"mGene.web: a web service for accurate computational gene finding\". Nucleic Acids Res. 37 (Web Server issue): W312–6. doi:10.1093/nar/gkp479. PMC 2703990. PMID 19494180.\n- Saeys Y, Rouzé P, Van de Peer Y (2007). \"In search of the small ones: improved prediction of short exons in vertebrates, plants, fungi and protists\". Bioinformatics 23 (4): 414–420. doi:10.1093/bioinformatics/btl639. PMID 17204465.\n- Hiller M, Pudimat R, Busch A, Backofen R (2006). \"Using RNA secondary structures to guide sequence motif finding towards single-stranded regions\". Nucleic Acids Res 34 (17): e117. doi:10.1093/nar/gkl544. PMC 1903381. PMID 16987907.\n- Patterson DJ, Yasuhara K, Ruzzo WL (2002). \"Pre-mRNA secondary structure prediction aids splice site prediction\". Pac Symp Biocomput: 223–234. PMID 11928478.\n- Marashi SA, Goodarzi H, Sadeghi M, Eslahchi C, Pezeshk H (2006). \"Importance of RNA secondary structure information for yeast donor and acceptor splice site predictions by neural networks\". Comput Biol Chem 30 (1): 50–7. doi:10.1016/j.compbiolchem.2005.10.009. PMID 16386465.\n- Marashi SA, Eslahchi C, Pezeshk H, Sadeghi M (2006). \"Impact of RNA structure on the prediction of donor and acceptor splice sites\". BMC Bioinformatics 7: 297. doi:10.1186/1471-2105-7-297. PMC 1526458. PMID 16772025.\n- Rogic, S (2006). The role of pre-mRNA secondary structure in gene splicing in Saccharomyces cerevisiae (PDF) (PhD thesis). University of British Columbia.\n- Neelam Goel, Shailendra Singh, Trilok Chand Aseri (2013). \"A comparative analysis of soft computing techniques for gene prediction\". Analytical Biochemistry. doi:10.1016/j.ab.2013.03.015.\n- \"Splice Site Prediction Using Artificial Neural Networks\". Computational Intelligence Methods for Bioinformatics and Biostatistics. Lec Not Comp Sci 5488. 2009. doi:10.1007/978-3-642-02504-4_9.\n- Alexander, Roger P.; Fang, Gang; Rozowsky, Joel; Snyder, Michael; Gerstein, Mark B. (2010). \"Annotating non-coding regions of the genome\". Nature Reviews Genetics 11 (8): 559–71. doi:10.1038/nrg2814. PMID 20628352.\n- Svensson, Örjan; Arvestad, Lars; Lagergren, Jens (2006). \"Genome-Wide Survey for Biologically Functional Pseudogenes\". PLoS Computational Biology 2 (5): e46. doi:10.1371/journal.pcbi.0020046. PMC 1456316. PMID 16680195.\n- Zhang, Zhaolei; Gerstein, Mark (2004). \"Large-scale analysis of pseudogenes in the human genome\". Current Opinion in Genetics & Development 14 (4): 328–35. doi:10.1016/j.gde.2004.06.003. PMID 15261647.\n- Kelley, D. R.; Liu, B.; Delcher, A. L.; Pop, M.; Salzberg, S. L. (2011). \"Gene prediction with Glimmer for metagenomic sequences augmented by classification and clustering\". Nucleic Acids Research 40 (1): e9. doi:10.1093/nar/gkr1067. PMC 3245904. PMID 22102569.\n- Huson, D. H.; Mitra, S.; Ruscheweyh, H.-J.; Weber, N.; Schuster, S. C. (2011). \"Integrative analysis of environmental sequences using MEGAN4\". Genome Research 21 (9): 1552–60. doi:10.1101/gr.120618.111. PMC 3166839. PMID 21690186.","- Short report\n- Open Access\nFemale Anopheles gambiae antennae: increased transcript accumulation of the mosquito-specific odorant-binding-protein OBP2\n© Hoffman et al; licensee BioMed Central Ltd. 2012\n- Received: 12 January 2012\n- Accepted: 6 February 2012\n- Published: 6 February 2012\nNew interventions are required to optimally and sustainably control the Anopheles sp. mosquitoes that transmit malaria and filariasis. The mosquito olfactory system is important in host seeking (transmission) and mate finding (reproduction). Understanding olfactory function could lead to development of control strategies based on repelling parasite-carrying mosquitoes or attracting them into a fatal trap.\nOur initial focus is on odorant binding proteins with differential transcript accumulation between female and male mosquitoes. We report that the odorant binding protein, OBP2 (AGAP003306), had increased expression in the antennae of female vs. male Anopheles gambiae sensu stricto (G3 strain). The increased expression in antennae of females of this gene by quantitative RT-PCR was 4.2 to 32.3 fold in three independent biological replicates and two technical replicate experiments using A. gambiae from two different laboratories. OBP2 is a member of the vast OBP superfamily of insect odorant binding proteins and belongs to the predominantly dipteran clade that includes the Culex oviposition kairomone-binding OBP1. Phylogenetic analysis indicates that its orthologs are present across culicid mosquitoes and are likely to play a conserved role in recognizing a molecule that might be critical for female behavior.\nOBP2 has increased mRNA transcript accumulation in the antennae of female as compared to male A. gambiae. This molecule and related molecules may play an important role in female mosquito feeding and breeding behavior. This finding may be a step toward providing a foundation for understanding mosquito olfactory requirements and developing control strategies based on reducing mosquito feeding and breeding success.\n- Anopheles gambiae\nFactors that influence mosquito fitness, especially host seeking and mate finding are complex and modulated by multiple cues, of which olfactory cues are most important [1–4]. Detection of odor molecules requires odorant binding proteins (OBPs) that are abundant in antennal chemosensilla [5, 6]. OBPs are low molecular weight soluble proteins that bind and transport odor molecules from sensillae to G-protein-coupled receptors in olfactory sensory neurons . The finding of receptor AgamOBP1 binding to its ligand indole demonstrated the significance of OBPs in odor recognition . Understanding olfactory function could lead to development of malaria control strategies based on repelling Plasmodium sp. carrying Anopheles mosquitoes or attracting them into a fatal trap. A first step is assessment of expression of olfactory system associated genes [7–10]. There is sexually dimorphic expression of OBPs in Anopheles mosquitoes and Drosophila melanogaster[11–13]. We are focusing on identifying OBPs in antennae of Anopheles gambiae, because in Africa A, gambiae is the most important vector of Plasmodium falciparum, a major vector of Wuchereria bancrofti, which causes lymphatic filariasis , and a vector of O'nyong-nyong virus . In this study, based on results of a screening microarray (unpublished) and previous microarray studies [9, 11], we hypothesized that the OBP, OBP2 (AGAP003306), would have increased transcript accumulation by quantitative reverse transcription PCR (qRT-PCR) in female as compared to male A. gambiae antennae.\nCollection and Processing of RNA\nWe studied AGAP003306, which had 2 fold greater expression in RNA isolated from antennae of 4 day old A. gambiae (Keele strain from Johns Hopkins) females than males in a microarray experiment (unpublished). In another microarray study of RNA from antennae of 5-7 day old Pink-eye A. gambiae, expression of AGAP003306 (OBP2) was 1.4 times higher in females as compared to males, but no qRT-PCR was done . In yet another microarray study of RNA isolated from 3 day old whole mosquitoes (Pink-eye strain A. gambiae) there was approximately 3 fold increased expression in females vs males .\nIn this study, A. gambiae sensu stricto (G3 strain) were from the same batch of eggs (each batch giving rise to a mosquito lot) and were raised to adulthood under standard insectary conditions, and fed ad libitum with 10% sugar water . We studied the antennae under controlled conditions of age and exposure to food. Adults of both sexes were collected exactly 4 days after emergence. The mosquitoes were immobilized by exposure to -20°C for 15 minutes, males and females separated, and antennae removed by manual dissection over dry ice, placed into separate 1.5 mL centrifuge tubes and homogenized using a pestle, each in 300 μL of Trizol reagent (Invitrogen, CA). RNA was isolated following manufacturer's instructions and purified using RNeasy mini column (Qiagen). The RNA was then assessed for quality and quantity using NanoDrop (ND-1000). The mosquito antennae that generated the RNA for the qRT-PCR experiments were isolated in July 2009 (mosquitoes from the University of Maryland), and January 2010 and June 2010 (mosquitoes from the National Institutes of Health).\nAs an endogenous control, and foundation for the qRT-PCR analysis, we used the S7 ribosomal RNA gene of A. gambiae. As another control we analyzed AGAP009629, which did not have differential expression in antennae of females vs. males by microarray, but had increased expression in antennae of unfed vs. blood-fed females (unpublished).\nPrimers used in qRT-PCR\nMultiple sequence alignments were built using the KALIGN program , followed by manual adjustments on the basis of profile-profile and structural alignments. Phylogenetic analysis was conducted using an approximately-maximum-likelihood method implemented in the FastTree 2.1 program under default parameters .\nExpression of AGAP003306 (OBP2) and AGAP0099629 (control) relative to expression of S7 in antennae of female and male A. gambiae in three biological replicates (experiments 1, 2 and 3).\nExpression of AGAP003306 (OBP2) relative to expression of S7 in antennae of female and male A. gambiae in two technical replicates.\nOBP2 belongs to an OBP super family that includes the insect pheromone binding proteins . Another member of this family, Agam OBP1, mediates indole recognition in antennae of female A. gambiae. The olfactory receptors of terrestrial animals exist in an aqueous environment; yet detect odorants that are primarily hydrophobic. The aqueous solubility of hydrophobic odorants is thought to be greatly enhanced via OBPs, which exist in the extracellular fluid surrounding odorant receptors. This family includes proteins that specialize in binding insect pheromones (PBPs) and others that bind general odorants (GOBPs) . Prior phylogenetic analysis has suggested that evolution of the OBP superfamily has evolved primarily through the process of lineage-specific expansion . Thus, the majority of the OBPs in a given lineage such as Diptera, Hymenoptera, Lepidoptera or Coleoptera tend to cluster with others from the same lineage to the exclusion of those from other lineages. The genome of A. gambiae itself contains about 72 members of the OBP family.\nNew interventions are needed to control the mosquitoes that transmit the parasites that cause malaria [27, 28] and lymphatic filariasis. Despite exciting scientific advances during the past few decades, no new approaches to mosquito vector control have been translated into widely used effective interventions. Sequencing the A. gambiae genome  and transcriptomics have provided a foundation for an approach to developing new interventions based on identifying genes and gene products that are important in transmission and mate-seeking. Stable genetic knockouts have not been generated in A. gambiae. However, transient knockdown by injection of sRNAi can be done and used to confirm the functional importance of OBP2 and other genes. This will be one of the next steps in our work.\nWe particularly appreciate Dr. Peter Billingsley's support and guidance with data analysis and illustration and with manuscript finalization. We are grateful to Dr. George Dimopoulos, Johns Hopkins School of Public Health for initiation of this project, guidance and support. We thank Yuemei Dong, and Antonio M. Mendes, Johns Hopkins School of Public Health, for support with our unpublished microarray studies. We thank Robert Harrel, Dr. David O'Brochta, and Robert Alford of the University of Maryland Biotechnology Institute and Dr. Tobi Lehmann from NIAID, for providing A. gambiae for the qRT-PCR studies. We thank Dr. Anthony James for review of the manuscript. We are indebted to the team at Sanaria Inc., especially Yonas Abebe, Solomon Conteh, Dr. Abraham Eappen, Dr. Adriana Ahumada, Dr. Anusha Gunasekera, and Benjamin Hoffman for logistical and technical support and discussions.\n- Zwiebel LJ, Takken W: Olfactory regulation of mosquito-host interactions. Insect Biochem Mol Biol. 2004, 34: 645-652. 10.1016/j.ibmb.2004.03.017.PubMed CentralView ArticlePubMedGoogle Scholar\n- Rutzler M, Zwiebel LJ: Molecular biology of insect olfaction: recent progress and conceptual models. J Comp Physiol A Neuroethol Sens Neural Behav Physiol. 2005, 191: 777-790. 10.1007/s00359-005-0044-y.View ArticlePubMedGoogle Scholar\n- Chen XG, Mathur G, James AA: Gene expression studies in mosquitoes. Adv Genet. 2008, 64: 19-50.PubMed CentralView ArticlePubMedGoogle Scholar\n- Maekawa E, Aonuma H, Nelson B, Yoshimura A, Tokunaga F, Fukumoto S, Kanuka H: The role of proboscis of the malaria vector mosquito Anopheles stephensi in host-seeking behavior. Parasit Vectors. 2011, 4: 10-10.1186/1756-3305-4-10.PubMed CentralView ArticlePubMedGoogle Scholar\n- Vogt RG, Riddiford LM: Pheromone binding and inactivation by moth antennae. Nature. 1981, 293: 161-163. 10.1038/293161a0.View ArticlePubMedGoogle Scholar\n- Pelosi P, Zhou JJ, Ban LP, Calvello M: Soluble proteins in insect chemical communication. Cell Mol Life Sci. 2006, 63: 1658-1676. 10.1007/s00018-005-5607-0.View ArticlePubMedGoogle Scholar\n- Biessmann H, Andronopoulou E, Biessmann MR, Douris V, Dimitratos SD, Eliopoulos E, Guerin PM, Iatrou K, Justice RW, Krober T: The Anopheles gambiae odorant binding protein 1 (AgamOBP1) mediates indole recognition in the antennae of female mosquitoes. PLoS One. 2010, 5: e9471-10.1371/journal.pone.0009471.PubMed CentralView ArticlePubMedGoogle Scholar\n- Pitts RJ, Fox AN, Zwiebel LJ: A highly conserved candidate chemoreceptor expressed in both olfactory and gustatory tissues in the malaria vector Anopheles gambiae. Proc Natl Acad Sci USA. 2004, 101: 5058-5063. 10.1073/pnas.0308146101.PubMed CentralView ArticlePubMedGoogle Scholar\n- Biessmann H, Nguyen QK, Le D, Walter MF: Microarray-based survey of a subset of putative olfactory genes in the mosquito Anopheles gambiae. Insect Mol Biol. 2005, 14: 575-589. 10.1111/j.1365-2583.2005.00590.x.View ArticlePubMedGoogle Scholar\n- Xu W, Cornel AJ, Leal WS: Odorant-binding proteins of the malaria mosquito Anopheles funestus sensu stricto. PLoS One. 2010, 5: e15403-10.1371/journal.pone.0015403.PubMed CentralView ArticlePubMedGoogle Scholar\n- Schymura D, Forstner M, Schultze A, Krober T, Swevers L, Iatrou K, Krieger J: Antennal expression pattern of two olfactory receptors and an odorant binding protein implicated in host odor detection by the malaria vector Anopheles gambiae. Int J Biol Sci. 2010, 6: 614-626.PubMed CentralView ArticlePubMedGoogle Scholar\n- Zhou S, Stone EA, Mackay TF, Anholt RR: Plasticity of the chemoreceptor repertoire in Drosophila melanogaster. PLoS Genet. 2009, 5: e1000681-10.1371/journal.pgen.1000681.PubMed CentralView ArticlePubMedGoogle Scholar\n- Pitts RJ, Rinker DC, Jones PL, Rokas A, Zwiebel LJ: Transcriptome profiling of chemosensory appendages in the malaria vector Anopheles gambiae reveals tissue- and sex-specific signatures of odor coding. BMC Genomics. 2011, 12: 271-10.1186/1471-2164-12-271.PubMed CentralView ArticlePubMedGoogle Scholar\n- Himeidan YE, Elzaki MM, Kweka EJ, Ibrahim M, Elhassan IM: Pattern of malaria transmission along the Rahad River basin, Eastern Sudan. Parasit Vectors. 2011, 4: 109-10.1186/1756-3305-4-109.PubMed CentralView ArticlePubMedGoogle Scholar\n- Dunyo SK, Appawu M, Nkrumah FK, Baffoe-Wilmot A, Pedersen EM, Simonsen PE: Lymphatic filariasis on the coast of Ghana. Tran R Soc Trop Med Hyg. 1996, 90: 634-638. 10.1016/S0035-9203(96)90414-9.View ArticleGoogle Scholar\n- Lanciotti RS, Ludwig ML, Rwaguma EB, Lutwama JJ, Kram TM, Karabatsos N, Cropp BC, Miller BR: Emergence of epidemic O'nyong-nyong fever in Uganda after a 35-year absence: genetic characterization of the virus. Virology. 1998, 252: 258-268. 10.1006/viro.1998.9437.View ArticlePubMedGoogle Scholar\n- Marinotti O, Calvo E, Nguyen QK, Dissanayake S, Ribeiro JM, James AA: Genome-wide analysis of gene expression in adult Anopheles gambiae. Insect Mol Biol. 2006, 15: 1-12. 10.1111/j.1365-2583.2006.00610.x.View ArticlePubMedGoogle Scholar\n- Benedict MQ: Care and maintenance of anopheline mosquito colonies. The Molecular Biology of Insect Disease Vectors: A Methods Manual. Edited by: Crampton JM, Beard CB, Louis C. 1997, London: Chapman and Hall, 1: 3-12.View ArticleGoogle Scholar\n- Salazar CE, Mills-Hamm D, Kumar V, Collins FH: Sequence of a cDNA from the mosquito Anopheles gambiae encoding a homologue of human ribosomal protein S7. Nucleic Acids Res. 1993, 21: 4147-10.1093/nar/21.17.4147.PubMed CentralView ArticlePubMedGoogle Scholar\n- Rozen S, Skaletsky H: Primer3 on the WWW for general users and for biologist programmers. Methods Mol Biol. 2000, 132: 365-386.PubMedGoogle Scholar\n- Livak KJ, Schmittgen TD: Analysis of relative gene expression data using real-time quantitative PCR and the 2(-Delta Delta C(T)) Method. Methods. 2001, 25: 402-408. 10.1006/meth.2001.1262.View ArticlePubMedGoogle Scholar\n- Schmittgen TD, Livak KJ: Analyzing real-time PCR data by the comparative C(T) method. Nat Protoc. 2008, 3: 1101-1108. 10.1038/nprot.2008.73.View ArticlePubMedGoogle Scholar\n- Lassmann T, Frings O, Sonnhammer EL: Kalign2: high-performance multiple alignment of protein and nucleotide sequences allowing external features. Nucleic Acids Res. 2009, 37: 858-865. 10.1093/nar/gkn1006.PubMed CentralView ArticlePubMedGoogle Scholar\n- Price MN, Dehal PS, Arkin AP: FastTree 2--approximately maximum-likelihood trees for large alignments. PLoS One. 2010, 5: e9490-10.1371/journal.pone.0009490.PubMed CentralView ArticlePubMedGoogle Scholar\n- Lespinet O, Wolf YI, Koonin EV, Aravind L: The role of lineage-specific gene family expansion in the evolution of eukaryotes. Genome Res. 2002, 12: 1048-1059. 10.1101/gr.174302.PubMed CentralView ArticlePubMedGoogle Scholar\n- Mao Y, Xu X, Xu W, Ishida Y, Leal WS, Ames JB, Clardy J: Crystal and solution structures of an odorant-binding protein from the southern house mosquito complexed with an oviposition pheromone. Proc Natl Acad Sci USA. 2010, 107: 19102-19107. 10.1073/pnas.1012274107.PubMed CentralView ArticlePubMedGoogle Scholar\n- Breman JG, Alilio MS, Mills A: Conquering the intolerable burden of malaria: what's new, what's needed: a summary. Am J Trop Med Hyg. 2004, 71: 1-15.PubMedGoogle Scholar\n- WHO: The global malaria action plan for a malaria free world. Geneva: Roll Back Malaria Partnership (RBMP). WHO. 2008, 274:Google Scholar\n- Holt RA, Subramanian GM, Halpern A, Sutton GG, Charlab R, Nusskern DR, Wincker P, Clark AG, Ribeiro JM, Wides R: The genome sequence of the malaria mosquito Anopheles gambiae. Science. 2002, 298: 129-149. 10.1126/science.1076181.View ArticlePubMedGoogle Scholar\nThis article is published under license to BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:be42ac9b-405d-445b-a8a8-7f5acd566315>","<urn:uuid:ae741678-03c6-46ff-9801-d890db8bbb1e>"],"error":null}
{"question":"How should authors structure their book's beginning for maximum impact, and what market analysis techniques can help ensure the content meets reader demands? 🤔","answer":"For a book's beginning, authors should avoid starting in the middle and instead think back to what they knew before their research. They should start by presenting their overarching question, provide a preview of what's to come, and lead readers through necessary introductory information to understand the stakes. As for market analysis, authors can use customer surveys to identify current needs and wants, analyze demographic data from sources like the U.S. Census Bureau, and study psychographic characteristics of target readers to ensure their content resonates with their audience. Understanding consumers' motivations helps tailor the message to convince readers that the book provides exactly what they're looking for.","context":["What Are Book Editors Looking for?\nChronicle of Higher Education, July 21, 2006\nBy Dedi Felman\nAs an editor for a major publishing company, I am occasionally asked to give talks on what editors are “looking for” in books. It’s always struck me as a curious question. It presumes that we know what we are looking for; that blessed with foresight, we anticipate the Next Big Thing and then instigate a full-bore search for the perfect prepackaged book and author.\nNot unheard of, I suppose. But much more often we wander, slightly dazed, through campus visits or a steady stream of summer submissions, hoping that good ideas and even better writers will find us — and that we will find them. Then, once we’ve located the creative kernel or thinker that sets us popping, an even-longer negotiation usually ensues over how exactly to get from idea to book.\nDespite the latest headlines about Google Print or the e-book, our mission remains remarkably stable. We’re looking for the same thing we’ve always looked for — solid, readable, provocative, and important works of scholarship with clear ideas at their core. And if you know some of the basics of good writing, you have already improved your starting score.\nWhat do you need to know when approaching a publisher? The first thing to remember is that all editors are different: We work in various divisions of different-sized companies with different mandates. We come from different backgrounds (some with advanced degrees, some without); we’ve set up different series and carved out different niches in the field. Our tastes vary widely. Some of us go wild over a rich and textured narrative. Some of us prostrate ourselves before the altar of “the big idea.” Some of us live to hoist yet another plaque onto the house’s prize-lined walls, some of us simply want a paycheck, and some of us want it all. So find the editor most likely to get enthusiastic about your work. Then listen to that editor’s advice.\nTo get you started, here are some basic first steps.\nIdentify the question driving your book. What is it about? Before you got lost trying to track every bit of information that exists on your topic, you had a question. Reclaim it.\nAre you wondering what social conditions led hundreds to die in a heat wave? (See Eric Klinenberg’s Heat Wave.) Are you wondering whether the media stereotypes about black men — their morality, their civility, or lack thereof — truly hold up? (See Mitchell Duneier’s Slim’s Table.)\nThe central question that you started with could become the beginning of a narrative that documents change, one that contradicts conventional wisdom, or one that merely explains. But rediscover your starting point and write it down, using no more than a few sentences to explain what motivated you, and now your book.\nIdentify why that question matters. The next key to your success lies in that well-oiled mantra from elementary school: Who cares? If you can’t tell your reader why they should care, you probably don’t have a question that motivates an entire book.\nThat’s easier to do in some cases than in others. We know why we should care about global warming or suicide bombing. Even if we’re not historians, we can see why gaining a clear understanding of the aftermath of the Civil War matters. It’s a lot harder to show why people should care about what Spinoza said (although Matthew Stewart in his recent work The Courtier and the Heretic did just that).\nStill, you know why you cared. And if you can sniff out what interests people when you explain your project to them and build on that to tell us why it all matters, you’ve got a question that can sustain a reader’s interest for 300 pages.\nCreate a narrative structure. Or, how to think like an architect. First and foremost, your book needs a logical architecture or frame. And that frame must actually support the house.\nIs your story an academic mystery in which the answer, through a steady accumulation of evidence, will gradually be revealed? Or perhaps it’s a dramatic conflict with two plausible storylines (yours and the conventional wisdom?) battling it out until a deus ex machina comes on stage to resolve all? Or a chronological narrative where we come to grasp a shift that has played out over time?\nPrepare an annotated outline of the entire book, including the introduction (how will you grab the reader’s interest?) and the conclusion (where do we go from here?). Then be prepared to justify your building plan. Does Chapter 4 naturally follow from Chapter 3? Perhaps Chapters 5 and 6 should be combined? Are we hearing the same point over and over? If it’s a complicated story, break the book down into three or four basic parts (often, “the what,” “the so-what,” and the “now-what”) and try organizing the chapters from there.\nDon’t cling to your first outline. Put it away for a week, and then re-ask yourself the same questions. Outline it to friends over a few drinks and see if they get it. If not, it could be their inebriation, but more likely, you need to try again.\nMake the story your own. The best books are the ones where writers seized control and told the story they wanted to tell. It’s hard to do that in your first book. Finding your own point of view is a lifelong process, and spelling it out with a distinctive voice and verve often takes a second or third book. But hold the despair. There are steps you can take to hurry the process along:\n- Throw out all traces of the literature review. Yes, you painstakingly put together a comprehensive overview for your committee, but now is the time to find your own voice. Like it or not, a book is an act of ego. Do not quote or explain others’ philosophies at length, or you will put your reader to sleep. Don’t let others grab your center stage.\n- Eliminate those endless block quotes. Never use someone else’s words to make a crucial point for your argument. Quote others when their rhetoric is powerful and you absolutely, positively couldn’t say it better yourself. But in most cases paraphrase.\n- Avoid jargon at all costs. You have probably heard that one before. But doesn’t jargon make me sound smarter? The answer is no. Jargon just makes your prose mushy and obscures your points. Ask yourself if your reader will understand how you are using a word. Then ask yourself if you truly understand how you are using that word. Then get rid of it.\n- Use examples. It’s not just the novelists who know that it’s more effective to show people what you would otherwise tell them. Follow flamboyant or intriguing characters through your narrative. Choose striking metaphors to express your central ideas. Once you’ve alighted on an indelible image or character, remember that your carefully chosen example isn’t superfluous to your argument, it is your argument. Show your reader something they won’t forget. Startle them.\nAvoid Abstraction. I know, I know, you can’t avoid abstraction. But make an effort to unearth the reality that underlies your theory, and return to it as often as you can. Share with your reader the real-life problem that makes your abstract argument concrete. For just two great examples of that, see Michael Walzer’s Just and Unjust Wars or Robert Frank and Philip Cook’s The Winner-Take-All Society.\nUnderstand the true beginning of your story. When it comes to the opening of the book, you must fight the temptation to begin in the middle. Think back to what you knew before you knew any of what you now know, and then back up even further. Don’t start the story where you would if you were talking to the six other people in your workshop who already know the disciplinary questions by heart.\nBegin by asking your overarching question. Then allow the reader a sneak peek of what’s to come. Finally, lead the reader quickly through the introductory information that they need in order to understand why the question matters and what’s at stake. Then and only then, after they’ve been properly prepped, are you ready to let loose.\nUnderstand the end of your story. After all the hard work of plowing through a book, there’s no greater disappointment than to have it drift off, either repeating the themes stated in the introduction or veering into irrelevant tangents.\nSeize the opportunity to point the way forward for the rest of us. If it’s a chronological tale, find the natural ending for the era that you have been describing and an anecdote that expresses the spirit of that point in time. If it’s a policy-oriented work, avoid ending with pie-in-the-sky proposals; tie your suggestions to the actual discussions in the book. If you’ve introduced characters in the book, return to them and wrap up their stories. The best works of both fiction and nonfiction open up worlds and ideas even as they tell a story that has a definite end.\nBe fair. We live in a time when bestsellers engage in shouting matches and have titles like Lies and the Lying Liars Who Tell Them. But I’m a respectable scholar, you protest. I’m nothing like those pop polemicists. You may not think you are, but are you examining the unquestioned and thickly encrusted crevices of your thought? Just because all your friends accept that unions are a force for good doesn’t mean that perspective is unquestioningly right. And if you reflect that point of view rather than truly argue it in your book, you aren’t being fair.\nYou don’t need to submerge your argument in mights, perhaps, and coulds, until the book flounders in equivocation. But you do need to be fair to all sides. Give your book to someone who you know disagrees with you, and ask him or her if you have presented that person’s views fairly. And take their critique seriously. The best arguments engage with and demonstrate the pitfalls in the other side’s logic. Be fair, and the reviewers will be fair to you.\nGive your book a pithy title. You might think that is industry folly, but it’s a premium exercise for conveying (and selling) your argument. The Republic. Bowling Alone. The Lonely Crowd. The Time Bind. Streetwise. Gideon’s Trumpet. The View from Nowhere. What do all those titles have in common? They illustrate an idea with an image. They don’t use jargon. And they express the author’s thesis in five words or less.\nFinally, remember: You’re not Tom Friedman (or David McCullough). And no one expects you to be. Yours is a narrative with a thought-provoking thesis, not a journalistic account. And though the more journalistic techniques that you can incorporate, the better your writing will read, don’t overworry this. Especially for a first book.\nInternalize Strunk and White and maybe even William Zinsser. But don’t twist yourself into a New York pretzel trying to write for Punch Sulzberger. You are bringing the rigor of logic, the surprises of great empirical inquiry, and the revelations of hard-won research to others. That’s a large-enough task.","Market Analysis for Small Businesses\nFor a small-business owner, market analysis refers to the process of obtaining information about customers, competitors and the industry in which the company operates. Market analysis is conducted to help entrepreneurs decide whether the market is large enough to justify taking the risk of starting the business. Established companies conduct market analysis on an ongoing basis to shape the development of strategies to grow the company.\nIdentifying a Market Need\nCompanies succeed by supplying products and services that meet the current most-pressing needs and wants of customers. A business owner must constantly monitor the market to identify when these needs change and adapt his product or service offering to be a better fit with these changed needs. A business owner can gather information about customer needs through conducting customer surveys and asking prospective customers what features of products or services are most important to them. The market need could arise from a problem that customers seek to solve or a personal or professional goal they seek to achieve. Consumers’ goal of saving money on energy bills, for example, has given rise to a host of solutions including more energy-efficient appliances and retrofitting homes so heating and cooling systems operate more efficiently.\nDefining a Market Opportunity\nA market need becomes an opportunity for the business owner if he can create and sell a superior solution for the need. The company must have the capability to deliver its products and services at the high level of quality the customer expects. The business owner must make a realistic assessment of what his company excels at in order to determine whether he can effectively compete in the market. Another way to define market opportunity is to identify where competitors are falling short of customer expectations and determine how your company can do better. Look at the major competitors in your market and analyze their strengths and weaknesses in relation to yours. An unmet need in the marketplace becomes your company’s opportunity.\nEstimating Market Size\nAccurately estimating the current size and projected growth of the market for the company’s products and services is critical, because there must be enough customers to support the sales level required for the company to become and remain profitable. Industry trade organizations publish data on the current size and projected growth rates for their industries. The National Restaurant Association, for example, publishes forecasts of restaurant revenue growth and analysis of industry trends. For consumer products and services, an effective way to analyze market size is to obtain information on demographic characteristics -- the populations of consumers in your area broken down by categories such as age and income level. The U.S. Census Bureau publishes demographic data on local communities.\nChoosing Target Markets\nTarget markets are the groups most ready to buy your products or services -- the customer groups that have the most urgent need to purchase what you are offering. Narrowing the large overall market down to target markets is necessary, because target markets have different motivations for making purchases. Consumers’ lifestyles, beliefs, values and attitudes shape their buying habits -- the kinds of products and services they are most interested in purchasing. These purchase behaviors are referred to as psychographic characteristics. The Bureau of Labor Statistics publishes a Consumer Expenditure Survey that analyzes the buying habits of American consumers. Understanding consumers’ motivations helps the business owner tailor his marketing message to resonate with each target group -- his message convinces them that his company is providing exactly what they are looking for.\nBrian Hill is the author of four popular business and finance books: \"The Making of a Bestseller,\" \"Inside Secrets to Venture Capital,\" \"Attracting Capital from Angels\" and his latest book, published in 2013, \"The Pocket Small Business Owner's Guide to Business Plans.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:9b5c6443-82c8-41de-b814-25cc131da2b3>","<urn:uuid:9dfda346-50c0-4592-b0f0-61a0eaf765b3>"],"error":null}
{"question":"What age restrictions apply for enrolling in the tamoxifen breast cancer trial versus the sickle cell ambrisentan study?","answer":"The breast cancer tamoxifen trial requires patients to be ≥ 18 years with no upper age limit, while the sickle cell ambrisentan study has more specific age requirements, limiting participation to those between 18-65 years old.","context":["View Cancer Clinical Trials\nPhase II Prospective Trial Correlating Progression Free Survival with CYP2D6 Activity in Patients with Metastatic Breast Cancer Treated with Single Agent Tamoxifen\n|North Coast Cancer\n- Primary Objective\n- To correlate CYP2D6 score (0 vs. 1+2) and progression-free survival.\n- Secondary Objectives\n- To correlate CYP2D6 score (0 vs. 1 vs. 2) and progression-free survival.\n- To correlate CYP2D6 score (0 vs. 1 + 2) and proportion of patients who are progression-free at 6 months.\n- To correlate endoxifen concentration with response.\n- To correlate CYP2D6 score with response.\n- To correlate the presence of candidate estrogen receptor (ESR) 1 and 2 variant alleles, UGT7, SULT1A1, other candidate genes and biomarkers to progression-free survival and other tamoxifen related outcomes.\n- Must have estrogen and/or progesterone receptor positive histologically confirmed adenocarcinoma of the breast.\n- Patients must have measurable or non-measurable Stage III/locally advanced or metastatic carcinoma of the breast where surgery is not possible, as defined in Section 6.1.1. Lesions must be evaluated within 4 weeks prior to registration.\n- Age ≥ 18 years.\n- Women must not be pregnant or breast-feeding due to harmful effects of tamoxifen. Note: All females of childbearing potential must have a blood test within 2 weeks prior to registration to rule out pregnancy. A female of childbearing potential is any woman, regardless of sexual orientation or whether they have undergone tubal ligation, who meets the following criteria: 1) Has not undergone a hysterectomy or bilateral oopherectomy; Or 2) Has not been naturally postmenopausal for at least 24 consecutive months (i.e., has had menses at any time in the preceding 24 consecutive months).\n- Women of childbearing potential and sexually active males must be strongly advised to use an accepted and effective method of non-hormonal contraception. Acceptable contraception includes barrier methods (e.g., condoms or diaphragm) or intrauterine devices or IUDs (these may include low-dose hormones at the discretion of the Study Chair).\n- ECOG performance status of 0-2.\n- Patient must not have had chemotherapy or trastuzumab (Herceptin) for metastatic disease. Chemotherapy or trastuzumab or bevacizumab in the adjuvant setting is allowed but must have been completed at least 6 weeks prior to study registration and discussed with the study PI. Prior investigational agents in the metastatic setting are not allowed. Other prior investigational agents in the adjuvant setting must have been completed at least 6 weeks prior to study registration and should be discussed with the study PI.\n- Prior tamoxifen or other agents that modulate or downregulate the estrogen receptor (e.g. raloxifene, fulvestrant) are not allowed.\n- Prior aromatase inhibitor (up to 2 agents) (e.g. anastrozole, letrozole, exemestane, aminoglutethamide) is allowed in the adjuvant or metastatic setting and must be discontinued at study registration.\n- Non-protocol concurrent hormonal therapy is not allowed.\n- Concurrent chemotherapy is not allowed.\n- Patients, must have adequate hematologic and renal function at the discretion of the treating physician and hepatic function as defined by the following within 4 weeks prior to registration:\n- Total bilirubin ≤ 1.5 x upper limit of normal\n- SGPT (ALT) ≤ 2.5 x upper limit of normal.\n- SGOT (AST) ≤ 2.5 x upper limit of normal.\n- Patients with a history of central nervous system metastasis are allowed provided they have been treated (surgery, radiation, or radiosurgery) at least 4 weeks prior to initiating study drug and do not require medication(s) to control symptoms. Patients with known leptomeningeal disease are not eligible.\n- Patients may receive concurrent radiation therapy to painful sites of bony disease or areas of impending fracture as long as the radiation therapy is initiated prior to study entry and sites of measurable and non-measurable disease outside the radiation therapy port are available to follow. Patients who have received prior radiation therapy must have recovered from toxicity of the prior radiation therapy.\n- Patients must not take the following medications that are strong to moderate inhibitors of CYP2D6 and may alter tamoxifen metabolism: paroxetine (Paxil), fluoxetine (Prozac), buproprion (Wellbutrin) and quinidine (Cardioquin) within 2 weeks of registration.\n- Patients must not suffer from medical or psychiatric conditions that would interfere with protocol compliance, the ability to provide informed consent, or assessment of response or anticipated toxicities.\n- Patients must be disease-free of prior invasive malignancies for ≥ 5 years with the exception of curatively-treated basal cell or squamous cell carcinoma of the skin or carcinoma in situ of the cervix.\n- Patients may not initiate bisphosphonate therapy while receiving treatment on this study. Patients who have begun receiving bisphosphonate therapy prior to registration may continue at the same intervals used prior to study registration.\nExclusion Criteria Not Available\nCancer Answers & Appointments\nSpeak with a cancer nurse specialist for appointment assistance and for answers to your questions about cancer locally at 216.444.7923 or toll-free 1.866.223.8100.\nMonday through Friday from 8 a.m. – 4:30 p.m. (ET).\nResources for medical professionals\n- Outpatient appointment referrals: 216.444.7923 or 866.223.8100\n- Inpatient hospital transfers: 800.553.5056\n- Referring Physician Concierge: 216.444.6196 or 216.312.4910.\nSearch available cancer clinical trials by disease, hospital, phase or number.","The Role of Endothelin-1 in Sickle Cell Disease\nThe Role of Endothelin-1 in Sickle Cell Disease\nLead Sponsor: Augusta University\nThe primary goal of the study is to determine the safety and tolerability of ambrisentan. It is also expected that ambrisentan will improve blood flow in the lungs, decrease inflammation, and reduce pain in sickle cell patients. An additional goal is to evaluate the use of select biomarkers in evaluating sickle nephropathy.\nThe purpose of this study is to test the hypothesis that endothelin antagonist (ETA) receptor blockade using ambrisentan is safe, tolerable, and improves kidney function/albuminuria in patients with sickle cell disease (SCD). The investigators anticipate a reduction in proteinuria, a decrease in tricuspid regurgitation jet (TRJ) velocity, reduction in inflammatory markers, improvement in forearm blood flow, and improvement in nociception/pain.\nThe primary efficacy endpoint was chosen as the effect of ETA receptor blockade on the reduction of microalbuminuria based upon findings in diabetic nephropathy, which has a similar pathogenesis to sickle nephropathy. A 30% reduction in proteinuria is rather conservative and realistic based upon the results of studies of ETA receptor blockade in diabetic nephropathy that consistently resulted in 40-45% reduction in proteinuria. Pre-clinical data has shown that changes can be detected in urinary nephrin excretion before overt proteinuria is observed in a model of chronic ET-1 elevation. Furthermore, tubular injury that can occur as a result of an intrarenal vaso-occlusive crisis (VOC) should be expected to increase the level of renal tubular injury markers, neutrophil gelantinase-associated lipocalin (NGAL), kidney injury molecule-1 (KIM-1), and netrin. Treatment with ambrisentan would be expected to attenuate these changes. Dr. Jennifer Pollock, has extensive experience with all of the biomarker assays including ET-1. Dr. Pollock runs bio-analytical core labs for two PO1s and has published extensively on these and similar methods in human and animal samples.\nAdditional rationale for this project is based on recent 24-hour urine results for creatinine clearance and total protein excretion from 97 patients with sickle cell anemia (Hb SS) and sickle beta zero thalassemia (SB0-thalassemia) followed in the Augusta Sickle Cell Clinic. These patients ranged in age from 19-63 years (52 females, 45 males). Of these, 17 (18%) had microalbuminuria as defined by a 24-hour albumin excretion of 150-300 mg, 34 (36%) had macroalbuminuria (>300 mg albumin excretion/24-hours), thus 54% of the patients had microalbuminuria or macroalbuminuria and only 46% were normo albuminuric. 58 patients (58%) were on chronic hydroxyurea therapy. 9 patients were on angiotensin converting enzyme inhibitors (ACEi) and 20 were on angiotensin receptor blockers (ARBs) (total 30%). 55 patients (57%) had glomerular hyperfiltration (creatinine clearance of >120 ml/min). There was an age related decline in the glomerular filtration rate (GFR) observed. The number of patients with normo albuminuria shows a sharp decline by age indicative of the progression of sickle nephropathy. Recent studies of pain in SCD have led to a change in perception. Approximately 50% of the patients reported experiencing daily, chronic pain. In addition, issues related to centralization and neuropathic pain in this patient population has begun to gain increasing attention. A large body of evidence suggests that endothelin-1 plays an important role in enhancing pain stimuli and nociception in various conditions. This is mediated by ETA receptor. Berkeley SCD transgenic mice do have hyperalgesia as shown in the rationale preliminary data section of Aim 2 and ETA receptor blockade reverses this hyperalgesia towards normal. As part of an ongoing National Institute on Minority Health and Health Disparities (NIMHD) funded study of pain and its genetic correlates (study reference: 1 P20 MD003383-01), the investigators have found that SCD patients display significant hyperalgesia as measured by pressure pain algometer testing. The testing was done at three different anatomic sites in patients (n=38) and controls (n=20) and showed that SCD patients perceived pain at significantly lower pressures compared to controls (masseter: 157.7 kilopascal (kPa) for patients, 214.4 kPa for controls, p=0.017; ulna: 299.1 kPa for patients, 477.5 kPa for controls, p=0.0018; and trapezius: 290.1 kPa for patients, 462.8 kPa for controls, p=0.02). These data suggest that hyperalgesia is present in the vast majority of SCD patients and will constitute an objective parameter to monitor during the study.\nProtocol-required assessments performed at every study visit include: physical exam, vital signs, con medication review, study drug accountability, adverse events review, pain diary completion, and pregnancy testing if applicable.\nProtocol-required assessments performed at specified study visits include: collection of blood and urine for safety and efficacy testing; electrocardiogram (ECG); echocardiogram (echo); Quantitative Sensory Testing (pressure pain threshold via algometer, cutaneous mechanical pain via monofilament, and thermal testing via Q-Sense Small Fiber Test); transcranial Doppler (TCD); forearm blood flow measurement; skin blood flow measurement; and completion of quality of life questionnaires.\nDescription of Procedures:\nSensory Testing: Patients will also undergo sensory testing at baseline and at the end of the study period. Sensory testing will include: 1) Pressure pain threshold will be measured by a hand-held, digital algometer as described by Fillingim et al at three different anatomical sites: center of right upper trapezius, right masseter, and the right ulna; 2) Cutaneous mechanical pain testing- Assessment of temporal summation of mechanical pain will be done using nylon monofilaments tested on the dorsum of the hand.; and 3) Thermal sensory testing will be performed using Q-Sense Small Fiber Test, which offers a scientifically validated measure of warm, cool and heat-pain thermal sensory thresholds.\nPain Diaries/Questionnaires: Patients will be asked to complete a daily pain diary as described in the PISCES study. The evaluation of pain diaries will be performed by Dr. Robert Gibson and the research staff. In addition, investigators will also incorporate two pre/post measures that will examine the effects of pain on functional performance and quality of life. The Assessment of Motor and Process Skills (AMPS) is a well-established functional assessment. This assessment has demonstrated the ability to discriminate between healthy controls and those with wide spread chronic pain. It also demonstrates the ability to measure change following intervention. The investigators will also measure patient's quality of life with the Adult Sickle Cell Quality of Life Measurement Information System (ASCQ-Me). This quality of life assessment has been specifically developed for sickle cell populations. Dr. Robert Gibson has training and experience with the ASCQ-Me and is certified to administer the AMPS.\nEchocardiogram for tricuspid regurgitation jet (TRJ) Velocity Measurement: A transthoracic echocardiogram for the measurement of TRJ velocity and pulmonary artery systolic pressure will be performed at baseline (unless obtained clinically within 6 months of Day 1) and at the end of the study.\nElectrocardiogram: An electrocardiogram (ECG) is a test that measures the electrical activity of the heart. An ECG will be done as a safety measure at baseline, Day 8 visit, and end of study, although ambrisentan is not known to prolong corrected QT interval (QTc).\nForearm Blood Flow Measurement: Brachial artery flow-mediated dilation (FMD) will be performed in accordance to the recent tutorial for ultrasound assessment of FMD. Patients will be tested under fasting conditions in a controlled environment at baseline and at the end of the study period.\nSkin Blood Flow Measurement: Skin blood flow will be measured non-invasively using a low frequency laser imaging camera and/or a small skin probe. Ring-shaped probes that are about 1.5 inches wide will be placed on the forearm with sticky tape. One ring will be filled with approximately 2 ml (about 1/2 teaspoon) of water for local heat and the others will be used for iontophoresis. For local heat, investigators will heat the water in the ring to between 42°C (107°F) and 44°C (111°F) and wait approximately 30 minutes for the increase in skin blood flow to become stable. Iontophoresis is a technique that delivers a vasoactive drug about 1 mm deep into the skin using a low level electrical current. The advantage of this technique is that it is non-invasive and the drug does not go into the body, it just remains at the surface of the skin. We will use a 1-2% solution of either acetylcholine, sodium nitroprusside, or N-nitro-L-arginine methyl ester (L-NAME) (vasoactive substances) to increase or decrease skin blood flow. Multiple short currents (10-180 seconds) will be performed approximately 1 minute apart. Investigators will monitor skin blood flow through the ring during the forearm arterial blood flow test described above as well as during local heat and iontophoresis.\nBlood and Urine Measurement: Lab tests to be performed for safety/ efficacy assessments at specified study visits include complete blood count (CBC), complete metabolic panel (CMP), spot urine for albumin-creatinine ratio, 24-hour urine, pregnancy testing, biomarkers, and inflammatory markers.\nTranscranial Doppler (TCD): Pre- and post-treatment TCD will be performed to assess velocity of cerebral blood flow. This non-invasive test is performed while the subject is lying down and involves use of sound waves to assess macro-circulation in the head. A transducer is used to hear and record the results.\n|Start Date||September 2015|\n|Completion Date||November 30, 2019|\n|Primary Completion Date||May 30, 2019|\nIntervention Type: Drug\nIntervention Name: Ambrisentan\nDescription: Ambrisentan 5 milligrams a day or a placebo (sugar pill) for twelve weeks\nArm Group Label: Treatment\nOther Name: Letairis\nIntervention Type: Drug\nIntervention Name: Placebo\nDescription: One inactive pill daily for twelve weeks\nArm Group Label: Placebo\nOther Name: \"Sugar Pill\"\nInclusion Criteria: 1. SS or Sβo-thalassemia 2. Age 18-65 years 3. Microalbuminuria (24-hour albumin 150-300 mg) or macroalbuminuria (24-hour albumin >300 mq) OR random urine albumin-creatinine ratio (MA Random) ≥ 30 µg/ mg creatinine 4. Subjects can have Stage 1, II, or III chronic kidney disease (CKD) 5. Subjects can be on hydroxyurea, ACE inhibitors (ACEi), or angiotensin receptor blockers (ARBs) for a period of 3 months or greater 6. Females of child bearing potential must agree to use two forms of birth control with one being a barrier method; abstinence is an acceptable form of birth control Exclusion Criteria: 1. Other genotypes of SCD 2. History of renal transplant 3. Chronic kidney disease (Stage IV and V including patients on hemo dialysis or peritoneal dialysis) 4. Patients on chronic transfusion therapy 5. Uncontrolled/poorly controlled hypertension or history of hypertension pre-dating proteinuria or 6. Known history of HIV, Hepatitis C, and/or diabetes 7. Peripheral edema 8. History of congestive heart failure or pulmonary edema 9. Recent history of coronary artery disease 10. Pregnant or breast feeding 11. Alanine Aminotransferase (ALT) or aspartate aminotransferase (AST) >3-fold upper limit of normal 12. Albumin <2.5 gm/dl 13. Hemoglobin < 6 gm/dL 14. History of non-compliance with medications and clinic visits; or Inability to give informed consent; or Patient deemed ineligible or unsuitable in the judgment of investigators\n1. SS or Sβo-thalassemia\n2. Age 18-65 years\n3. Microalbuminuria (24-hour albumin 150-300 mg) or macroalbuminuria (24-hour albumin >300 mq) OR random urine albumin-creatinine ratio (MA Random) ≥ 30 µg/ mg creatinine\n4. Subjects can have Stage 1, II, or III chronic kidney disease (CKD)\n5. Subjects can be on hydroxyurea, ACE inhibitors (ACEi), or angiotensin receptor blockers (ARBs) for a period of 3 months or greater\n6. Females of child bearing potential must agree to use two forms of birth control with one being a barrier method; abstinence is an acceptable form of birth control\n1. Other genotypes of SCD\n2. History of renal transplant\n3. Chronic kidney disease (Stage IV and V including patients on hemo dialysis or peritoneal dialysis)\n4. Patients on chronic transfusion therapy\n5. Uncontrolled/poorly controlled hypertension or history of hypertension pre-dating proteinuria or\n6. Known history of HIV, Hepatitis C, and/or diabetes\n7. Peripheral edema\n8. History of congestive heart failure or pulmonary edema\n9. Recent history of coronary artery disease\n10. Pregnant or breast feeding\n11. Alanine Aminotransferase (ALT) or aspartate aminotransferase (AST) >3-fold upper limit of normal\n12. Albumin <2.5 gm/dl\n13. Hemoglobin < 6 gm/dL\n14. History of non-compliance with medications and clinic visits; or Inability to give informed consent; or Patient deemed ineligible or unsuitable in the judgment of investigators\nMinimum Age: 18 Years\nMaximum Age: 65 Years\nHealthy Volunteers: No\nType: Principal Investigator\nInvestigator Affiliation: Augusta University\nInvestigator Full Name: Abdullah Kutlar\nInvestigator Title: Professor of Medicine\n|Has Expanded Access||No|\n|Number Of Arms||2|\nDescription: Ambrisentan 5 mg PO daily\nType: Placebo Comparator\nDescription: One inactive pill PO daily\n|Study Design Info||\nIntervention Model: Parallel Assignment\nPrimary Purpose: Treatment\nMasking: Triple (Participant, Care Provider, Investigator)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7df08f8c-5f56-4a72-a6cc-cddf0132bc7b>","<urn:uuid:4d7b03a3-5f56-4810-a368-070d8fbd63e3>"],"error":null}
{"question":"Could you please compare how Xu Bing and Carme Pigem approach the relationship between their art and the environment? I'm interested in understanding how these creators integrate their surroundings into their work.","answer":"Xu Bing focuses on recreating and transforming cultural elements, particularly through his work with 'new' Chinese and English characters, as well as 3D books, showing how art can reconstruct existing cultural concepts. On the other hand, Carme Pigem emphasizes creating a link with the existing environment, whether it be city, landscape or building, focusing on communicating feelings to people through spaces. Both creators emphasize connection with their surroundings, but while Xu Bing works primarily with linguistic and cultural elements, Pigem concentrates on physical and emotional connections to spatial environments.","context":["Samsara of All Things: Boundless in Art and Design between East and West\nThe theme comes from two thoughts: one is “All things are equal”, originated from Zhang Zai’s ideology, who was a thinker from Song Dynasty, thinking that all people and all things between heaven and earth are our fellows and we should treat other people like brothers, and love things just as we love people; and the other comes from samsara, the transmigration idea from Indian Buddism, which holds the belief that the subject of life take turns in different spheres of existence, and the wheel of death and rebirth is almost endless.\nBased on the above two ideas, we believe that samsara is not only meant for living things, but also for non-living things. Human beings and things are equal, and we should be kind to all things, which could be anything in the material world, and the nonmaterial including cultures, concepts, and skills, etc. Art and design can achieve the reconstruction and transformation of ancient cultures, concepts and skills, etc. That is what we call samsara of all things, and the theme for the 2019 Arts Bridge forum at Frankfurt Book Fair.\nAll the explorations and practices by Xu Bing (Artist of the Year), Jamy Yang (Designer of the Year), Zhu Zheqin’s KANJIAN (Honour of the Year), and Xu Gang (Manufacturer of the Year) verify such a theme.\nXu Bing’s “new” Chinese characters and “new” English characters are the art recreation of characters, and such spirit is also embodied in his exploration of 3D books.\nJamy Yang has integrated Chinese philosophical thinking and humanistic concepts into the industrial design of buildings, furniture, daily objects, etc. His design expresses his goodwill to human nature.\nMs. Zhu Zheqin has been devoted to the field investigation of music and folk art in China’s remote and minority areas for many years. She uses KANJIAN, the brand she has founded, to retain the traditional folk art in a modern design way and make it presented in a brand new way.\nThe BENTU team, led by Xu Gang, treat all things equally, and make true the modern reconstruction of waste materials by walking among ruins and wastes of agricultural civilisation and pre-industrial relics, thus realising samsara of all things in the secular world.\nAll these Chinese positions on art and design will be presented, and linked to Western traditions.\nOrganisers: ACC Art Books, Guangxi Normal University Press (Shanghai) Co., Ltd., Museum Angewandte Kunst, Frankfurt a.M. Hochschule für Gestaltung Offenbach a.M., Institute for Design Exchange (IDEe) and Northwestern Polytechnical University Mingde College Moderators: Wang Zijian (from China) and Prof. Dr. Klaus Klemp (from Germany)\n(The conference language is English)\nIn the first half\n- Speech by Prof. Xu Bing (artist) 10 mins\n- Speech and performance by Axel Malik (artist) 10 mins\n- Dialogue between Xu Bing and Axel Malik 10 mins\n- Speech by Ms. Zhu Zheqin (crossover artist) 10 mins\n- Speech by Prof. Matthias Wagner K (experimental curator) 10 mins\n- Dialogue between Zhu Zheqin and Matthias Wagner K 10 mins\n- Tea break with brief cold buffet (cakes, tea, coffee, fruits, etc.)\nIn the second half\n- Speech by Mr. Jamy Yang (industrial designer) 10 mins\n- Speech by Mr. Xu Gang (product designer) 10 mins\n- Speech by Prof. Dr. Klaus Klemp (design theorist) 10 mins\n- Dialogue between Mr. Jamy Yang, Mr. Xu Gang and Prof. Dr. Klaus Klemp\n- Resume by Prof. Dr. Klaus Klemp\n- Come together\nTotal time: max. 3 hours\n17th October, 2019, 2 p.m. - 5 p.m.\nMuseum Angewandte Kunst, Frankfurt am Main","At the forum’s second session, set to take place Tuesday at 1 p.m. at The Shilla Seoul’s Dynasty Hall, three global designers will expound on their approach to design.\nThe speakers of the session, titled “Design Thinking,” spoke with The Korea Herald prior to the event to answer questions on the fundamental meaning of design and its lasting value. –Ed.\nCommunicating feelings through spaces: Carme Pigem\nWorld-renowned Spanish architect Carme Pigem is a member of architectural firm RCR Arquitectes, which won the prestigious Pritzker Architecture Prize this year. In this edited excerpt, Pigem discusses the relationship between human beings and their environment.\n|Carme Pigem (Herald Design Forum)|\nQ: Who or what do you think has the power to lead trends in the global design industry? What role do you think design will play in the “fourth industrial revolution”?\nA: I believe that the value of the product in itself has the power to lead trends. Design’s role in our new world of Internet of Things should be to further emphasize the beauty beyond the utility of any tool, whether it be equipment or furniture. But tools should also understand the way daily lives have changed.\nQ: What is your ‘driving force’? What drives you to pursue and try out new designs?\nA: Our driving force is to communicate feelings to people through spaces. We want to research essential meanings and be linked with the existing environment, whether it be city, landscape or building. And we strive to work hard until the answer to the question is beauty.\nQ: What is design that gains value over time?\nA: Design that is timeless. When a space or a work of art, has quality, it keeps its value through time and you can still enjoy it even if its initial functional purpose changes.\nQ: Architecture is greatly influenced by social issues. Some notable architects, for instance, have ties with politics or support a certain political party. Europe has recently faced growing social controversy over refugees. How do you think refugee-related issues could be alleviated using architecture?\nA: It is a very difficult question. It is a big problem when people must run away from their countries. When large numbers of people arrive in other countries, there are two possible ways to manage their arrival, considering their number and time constraints. The first would be to prepare a quick venue where people could stay for a short period of time, by using abandoned buildings for example. The countries could also try to absorb the new people into their existing structures.\nQ: When it comes to architecture, the quality of material is said to effect users’ emotions as much as design.\nA: I think that is true when the new material has some kind of relation to the existing ones. Then the relation and combination established between the two make the design unique.\nQ: A word of advice for aspiring architects dreaming of becoming a world-renowned creator?\nA: You should pursue your dreams, work hard, believe in yourself, be honest with yourself and live -- and work (it’s the same thing) -- with passion.\nChanneling the European sensitivity: Seok Yong-bae\nKorean designer Seok Yong-bae has worked with various global apparel brands to helm their shoe collections. In 2008, Seok served as head of design for Dolce & Gabanna’s casual and sneaker collections. Now running his own studio and brand, Camotartan, Seok partners with brands including Bally, Geox and Diesel.\n|Seok Yong-bae (Herald Design Forum)|\nQ: You started out as an automobile designer, then transitioned to shoe design. What kind of similarities do the two types of design share?\nA: I started as an automobile designer in 2001 at the Italian company Pininfarina. I had the opportunity to participate in a project with Fila Italia, as a consulting shoe designer, and that is how I came to be where I am today. The similarity between automobile and shoe design is that both are tools that serve the function of transportation, helping humans move conveniently.\nShoes, like cars, go through an extensive design process. Apart from the outer appearance such as color, the technical elements such as the sole, the liner, material come together. Only when everything is aligned can the shoe give both pleasure and convenience to the consumer. I have been sought out by many companies as a consultant, and many prospective projects with European luxury brands are in the works.\nQ: The Italian fashion industry is known for being entirely talent- and merit-based, but also seen as closed off to foreign talent. What was your experience?\nA: That is true. The Italian fashion industry seems very open but it is difficult and rare for foreign designers to rise to director positions. To put it briefly, only effort and proven talent prevail. Personally, I believe I was able to overcome these difficulties because I am able to understand the Italian and European sensibility, because I am able to understand them and communicate with them regardless of ethnicity or nationality. Now, they do not regard me as a Korean or Asian designer but purely as designer Seok.\nQ: What kind of design is timeless?\nA: Design that gains value over time is a piece of work created by a designer who did not cling to the fad of the era, but created based on the designer’s own philosophy or aesthetics. One example can be found in Italy’s Ferrari cars. The cars strive for formative perfection, making them transcend time through beauty, and increase in price as time goes by.\nQ: What do you think the role of design should be in the face of the fourth industrial revolution?\nIn product design, I believe the product’s inherent function should not be lost. It should become more convenient while reflecting the era’s style. Currently, design acts as a connecting chain between humanity and technology by giving birth to products that possess both functionality and beauty.\nCareful attention to parents’ needs: Aernout Dijkstra-Hellinga\nAernout Dijkstra-Hellinga is the senior lead designer and vice president of sustainability at Bugaboo, a Dutch mobility company that specializes in strollers and luggage. Dijkstra-Hellinga explains in detail the creative process behind some of the company’s most renowned products such as the Bugaboo Donkey, a convertible stroller that attaches two seats together.\n|Aernout Dijkstra-Hellinga (Herald Design Forum)|\nQ: What is the most important feature of mobility? How are you blending such features to your design?\nA: Mobility for us means transportation of everything (your kids, stuff, etc.) from a to b. We like that our products are visible in public spaces. That’s where other people see our users interact with our products.\nQ: Bugaboo is a leading design company in smart mobility. What is your definition of smart mobility? What is the role of design in the industry?\nA: We create products with smart design, meaning they are very intuitive to use, engineered to last and provide value for money. This is what we consider smart design.\nQ: What inspired you to design the Bugaboo Donkey stroller? Why did you feel the need to create such a design? Could you give a brief explanation of the process?\nA: We had a lot parents asking for a solution for two or more children. We could have easily given parents what they asked for, a dedicated duo stroller. But at Bugaboo we always like to think about the need behind the question. We figured out that people wanted a stroller that could adapt to their needs. Sometimes you want to transport a single child, sometimes even three. So we needed to come up with a stroller that you could easily change from mono to duo -- within seconds.\nNext to that, we felt that it was important to give both children the same space, the same interaction with their parents. Lastly, the stroller should offer easy one-handed steering, even when fully loaded. By looking at all the needs we quickly realized that the only sensible configuration was to have a side-by-side stroller. The challenge was to make it narrow enough to fit through a standard door opening, while at the same time give the children enough space in which to sit comfortably.\nQ: What is the ‘luggage system’ suggested by Bugaboo? How do you think it will change our daily life?\nA: If you walk through any airport, train or bus station, you’ll see people struggling with their heavy luggage. We designed this luggage system to take away that burden and turn it into a smooth experience. It gives your arms and back a break because you can push it along with very little effort. Details make traveling more efficient -- interlocking cases which keep everything in place; an organizer for electronics can be clicked off at airport security.\nBy Rumy Doo (firstname.lastname@example.org)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:a0d1bb7e-24e7-46a6-b2c7-56e35d2465b4>","<urn:uuid:36ac5a17-f013-4e45-8b0b-0af6cd49ca30>"],"error":null}
{"question":"What are the proper mixing and rising steps for whole wheat hamburger buns, and how do whole grains contribute to weight management and digestive health?","answer":"The mixing and rising process involves first combining yeast, honey, and warm water, letting it stand for 5 minutes until foamy. Then add flours, salt, egg, and butter, mixing until no dry flour remains. The dough needs a first rise for 1 hour until doubled in size, followed by shaping into 8 balls and a second 30-minute rise. Regarding health benefits, whole grains aid in weight management because they're more filling, requiring more chewing time which helps prevent overeating. They also support digestive health due to their high fiber content, which promotes regular bowel movements and reduces the risk of bowel cancer. The fiber is also beneficial for gut bacteria and overall gut health.","context":["These easy homemade whole wheat hamburger buns are perfect for summer grilling!\nFor the dough:\n- 9 grams active dry yeast (2 1/4 teaspoons, or 1 packet)\n- 12 grams honey (1 Tablespoon)\n- 240 grams warm water (about 1 cup)\n- 200g All-Purpose Flour (about 1 2/3 cups)\n- 200g Whole Wheat Flour (about 1 1/2 cups)\n- 9 grams kosher salt (about 1 1/2 teaspoons)\n- 1 large egg, lightly beaten\n- 2 Tablespoons unsalted butter, melted\n- 2 Tablespoons unsalted butter, melted\n- 2 Tablespoons sesame seeds (optional, for topping)\nMIX THE DOUGH AND FIRST RISE (1 HOUR)\n- Stir yeast, honey, and warm water together in a large mixing bowl (you could also use a stand mixer fitted with a dough hook). Let stand for 5 minutes until the yeast is active and the mixture is foamy.\n- Add all purpose flour, whole wheat flour, kosher salt, egg, and melted butter to the yeast mixture. Use a wooden spoon or a sturdy spatula to stir everything together until no dry bits of flour remain. If it gets tough to stir the dough with a spoon, use clean hands to knead the dough together until it’s well mixed.\nNote: You can continue to knead this dough by hand (5-10 minutes) or with the dough hook on a stand mixer (3-5 minutes) until it’s smooth and elastic if you like; this will develop more gluten for a smoother dough and a bit more chew. If you’re short on time or just don’t feel like putting in that much elbow grease, just mix the dough until no dry bits of flour remain (it’s ok if it’s still a little shaggy and sticky!) We have tested this recipe with and without additional kneading and it’s good either way, so feel free to adjust based on your personal preference! In the photos above, we mixed the dough until it was just combined (with no additional kneading).\n- When dough is mixed, cover the bowl with a clean tea towel and let dough rise at room temperature for one hour, until it’s about doubled in size.\nSHAPE THE DOUGH & FINAL RISE (30 MINUTES)\n- When dough has risen, tip it out onto a lightly floured cutting board.\nTip: If the dough is sticking to the sides of the mixing bowl, run your hand under cold water to prevent sticking as you pull the dough out.\n- Knead the dough a few times to work out any large air pockets, then pull the dough into a round shape (this makes it easier to divide).\n- Use a bench scraper or a knife to divide the dough into 8 evenly sized pieces.\n- Add more flour to your cutting board if you need it, then shape each piece of dough into a ball. To shape, pull the corners of each dough piece in towards its center, then pinch the edges together and roll the dough across the cutting board to form a round shape.\n- Place the dough balls on a baking sheet lined with a nonstick baking mat, leaving 2 inches between each piece.\n- Gently press down on each ball to flatten it about halfway. Cover shaped buns with a clean tea towel and let rest for another 30 minutes while you heat the oven.\n- Heat the oven to 375° Fahrenheit.\nBAKE (15-18 MINUTES) & COOL (30-60 MINUTES)\n- When the oven is hot, use a pastry brush to brush each burger bun with a bit of melted butter. Top with a sprinkle of sesame seeds (optional).\n- Bake at 375° F for 15-18 minutes until golden brown.\n- Remove buns to a wire cooling rack. Let cool completely (for at least 30 minutes) before slicing and serving. Note: These buns might feel a little stiff right when you take them out of the oven; the crust will soften as they cool.\nWhat kind of flour should I use? We always recommend using King Arthur Flour when baking. They're incredibly consistent from bag to bag, and King Arthur All Purpose flour has a high protein content (which means better gluten development, which means the dough is easier to work with). Bob's Red Mill flours are also a good option for baking.\nSubstitutions. Use sugar, brown sugar, or molasses instead of honey. Use bread flour in place of some or all of the other flours in this recipe. Use additional all purpose flour in place of some or all of the wheat flour if you like. We do not recommend increasing the amount of whole wheat flour in this recipe (see the FAQ above or our Everyday Artisan Bread course for more on flour substitutions!)\nHow warm should my “warm water” be? A good rule of thumb is “warm to the touch.” Too-hot water (typically 130-140 degrees F and above) will kill your yeast, but use too-cold water and your yeast will take too long to activate. We usually shoot for a water temperature between 95° and 110° Fahrenheit. You should be able to comfortably hold your hand under water running at about 100 degrees – it should feel quite warm, but not hot. Use a kitchen thermometer for a precise measurement if you like!\nPlease use a kitchen scale if you can! Measuring by weight with a kitchen scale is much more accurate than measuring by volume (with cups). Because each person measures a cup of flour a little bit differently, it's easy to end up using way too much flour in a recipe - which results in dry, dense bread! When you measure your flour in grams with a kitchen scale, you can measure flour perfectly every time.\nMix this dough by hand OR with a stand mixer - it works well either way! If you use a stand mixer, fit it with the dough hook and add the flour a little bit at a time to ensure the mixer incorporates everything evenly. Let the mixer run an additional 3-5 minutes once all the flour is incorporated to develop the gluten in the dough more fully for a stronger, smoother dough.\nAdjusting the rise time. If your kitchen is especially chilly, you may need to let the dough sit another 15-30 minutes during the first rise.\nAdding toppings and mix-ins. Right after you brush the buns with melted butter, sprinkle on some sesame seeds, flakey sea salt, shredded cheese, or fresh herbs. You can also add mix-ins like roasted garlic, fresh herbs, or shredded cheese directly to the dough when you first mix it.\nAdjusting the size of your burger buns. Cut your dough into 8 pieces for standard-sized burger buns, 10 pieces for small burger/large slider buns, or 12 pieces for small slider buns. If you make smaller sized rolls, you will need to adjust the cook time (for small buns, start checking for doneness at about the 8-10 minute mark). We have also used this dough for hoagie/sub rolls.\nFor more ideas and instructions about flour types, dough shapes, and other substitutions and techniques, be sure to enroll in our Everyday Artisan Bread course.\nServe these buns with your favorite burger recipe! We love using homemade hamburger buns with our Smoked Gouda Beer Burgers, Easy Salmon Burgers, or Black Bean Burgers.\nKeywords: summer, grilling, bbq","We all know whole grain breads, cereals and pasta are good for us, but many of us might not know why.\nOn top of that, without knowing what whole grains are or what whole grain products actually look like, it can be difficult to know if it's the right choice.\nTo better understand the importance of including them in our diets, we spoke to Simone Austin, accredited practising dietitian and spokesperson for the Dietitians Association of Australia.\nWhat is a whole grain?\n\"A whole grain has the germ (the inside part of the grain with good fatty acids), the endosperm (the middle layer with the carbohydrate and protein) and then the bran (the outer layer with a lot of fibre, vitamins and minerals),\" Austin told The Huffington Post Australia.\nWhole grains are a form of complex carbohydrates and come in a huge range we can enjoy. These include:\n- Brown, black, red and wild rice\n- Quinoa (although technically a seed)\nWhile whole grain products include these grains as a 'whole', white flour products (such as white bread, white rice, white pasta, noodles and many cookies, breakfast cereals, snacks and crackers) are considered to be 'refined' as they have had the bran and germ layers removed. This leaves only the endosperm and contains much less fibre, minerals, vitamins and phytochemicals.\nWhole grains fill you up more, which is part of the reason why they help people maintain a healthy weight.\n\"Opting for a whole grain product is best as it has all of those great, nutrient-rich parts in it,\" Austin said.\nWhole grains are found in whole grain cereals (such as muesli, Weet-Bix, Cheerios and bran), some breads (typically rye, spelt or wholemeal bread) and pasta (for example wholemeal, rye, spelt and kamut).\nWhen looking at bread in particular, there is a bit of confusion about the difference between multigrain and whole grain bread.\n\"Multigrain is often a white flour based bread that has a few whole grain thrown in. Whereas a whole grain bread typically has wholemeal flour plus extra grains thrown in,\" Austin said.\nWhile multigrain is still a better choice than plain white bread, Austin said the quality of multigrain breads can differ greatly.\n\"Some multigrains at least have a large amount of grains in it, but others are very sparse in how many grains they include,\" she said.\nCompared to white flour, whole grains contain more fibre, protein and important vitamins and minerals such as folate, thiamin, riboflavin, niacin, iron, vitamin E, zinc, magnesium and phosphorus.\n\"As whole grain contains more fibre, they help you go to the toilet more regularly,\" Austin told HuffPost Australia. \"Including whole grains and dietary fibre regularly also reduces your risk of bowel cancer.\n\"The fact that it's high in fibre, which is great for feeding bacteria in your gut and for overall gut health, is really important.\"\nStudies also show that people who include whole grains in their diet are less likely to gain weight.\n\"Whole grains fill you up more which is part of the reason why they help people maintain a healthy weight,\" Austin said.\n\"This is partly because you have to chew whole grains more so you take longer to eat it, meaning your stomach has more of a chance to tell your brain it's full, which can help reduce the risk of overeating.\"\n\"Plus, whole grains taste good. A really nice quality whole grain bread tastes amazing.\"\nHow to check if a product is whole grain\nChoosing from the list of whole grains above is a good guide -- however, finding whole grain products like bread and cereal require a bit of quick investigating.\n1. Check the ingredients list\nIf you pick up a cereal or bread, check that the first ingredient in the list is wholemeal or whole grain wheat/flour/oats/etc. If the ingredient is: flour, enriched flour, degerminated, bran or wheat germ, it is likely to not be a whole grain.\n\"Check that wholemeal flour is the first ingredient. Otherwise it could be mostly white flour with just a little bit of wholemeal sprinkled in,\" Austin said.\nTips for choosing whole grain products:\n- Look for words like 'whole grain' or 'wholemeal'.\n- Some 'multigrain' breads are made with white flour and various whole grains added.\n- 'Wholemeal wholegrain' bread is made with wholemeal flour plus whole grains and has more fibre and nutrients than wholemeal, wholegrain or white breads.\n2. Feel the weight\nThis applies primarily to bread but it's an easy way to see whether it's a whole grain bread.\n\"You can often tell it's a whole grain bread as it's heavier and denser,\" Austin said. \"White and multigrain bread is often lighter and fluffier.\"\n3. Look at the product\n\"If you can see that it's got grains, seeds and nuts it's more likely to be whole grain,\" Austin said.\n\"The seeds and grains you can usually see in a whole grain bread include soy, flax seeds, pumpkin seeds, sunflower seeds, chia seeds and oats.\"\nHow much to eat\nFor Australian adults, it's recommended we eat at least 4-6 serves of grain foods per day, with two-thirds of this coming from whole grains. Here's what one serve equates to:\n- 1 slice (40g) bread\n- ½ medium (40g) roll or flat bread\n- ½ cup (75-120g) cooked rice, pasta, noodles, barley, buckwheat, semolina, polenta, bulgur or quinoa\n- ½ cup (120g) cooked porridge\n- ⅔ cup (30g) wheat cereal flakes\n- ¼ cup (30g) muesli\n- 3 (35g) crispbreads\n- 1 (60g) crumpet\n- 1 small (35g) English muffin or scone"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ea57634e-cede-4fdc-aa9c-beb1ec8fac75>","<urn:uuid:ad7627a8-5cba-4441-adc4-5b724e113221>"],"error":null}
{"question":"What are the different fermentation characteristics of inulin compared to insoluble fiber?","answer":"Inulin and insoluble fiber have very different fermentation characteristics. Inulin is highly fermentable and acts as a prebiotic, feeding beneficial bacteria like Bifidobacteria and promoting the production of important substances like short-chain fatty acids and vitamins B and K. In contrast, insoluble fiber is poorly fermented and primarily acts by bulking up the stool, supporting movement out, and cleaning the gut by helping to slough off dead cells.","context":["First of all I want to start off with a fun fact, not all fiber is the same, nor does it work the same in the gut. Fiber can be complicated. However, today I am going to try and break it down for you. Dietary fiber refers to plant carbohydrates that we humans cannot digest because we lack the digestive enzymes to break them down. These fibers include cellulose, hemicellulose, pectin, lignin, gums, beta-glucans, fructans, galactooligosaccharides, and resistant starches. Each part of the plant (stem, root, leaf, etc) and its age all are factor into what fibers are present upon consumption. See Table 2 below for more info on each type of dietary fiber.\nSOLUBLE VS INSOLUBLE\nThere are two main categories generally used to categorize dietary fiber – soluble fiber and insoluble fiber. We need both for optimal gut health. Soluble fiber dissolves in water and often has a high water holding capacity, creating a gel like (viscous) substance. This gel like substance can firm up and soften stool, and therefore can help reduce severity of diarrhea while also softening hard stools. However, it also slows down the speed of which food/fibers travels through the gut and can reduce the absorption of nutrients. Soluble fiber is also highly fermentable by gut bacteria. Insoluble fiber on the other hand, does not dissolve in water, does not create a viscous gel, and generally is poorly fermented. Instead it acts like straw, bulking up the stool, supporting movement out, and cleaning the gut along the way helping to slough off dead cells. Soluble fiber is more gentle and can be better tolerated if someone has gastritis or intestinal inflammation, while insoluble fiber is more rough and can be irritating to inflamed tissue.\nAlthough both soluble and insoluble fibers can be degraded by colonic bacteria, soluble fibers are fermented at a much higher degree. Fermentation of fibers usually happens in the first part of the colon, where bacteria breakdown the fibers that we cannot digest and produce hydrogen gas as a byproduct. Therefore, fermentable fibers are known as prebiotics, or food for good gut bacteria. The most fermentable dietary fibers include fructans, galactooligosaccharides (GOS), pectin, gums, beta-glucans, and resistant starch type 3. Fermentation produces energy for the beneficial microbes and provides humans with short chain fatty acids (mostly acetic acid, propionic acid, and butyric acid). Short chain fatty acids provide humans with a lot of different health benefits including 1) they stimulate mucous production in the colon which aids in elimination and encourages the growth of akkermansia (a keystone species of gut health), 2) they decrease the pH of the colon which prevents the growth of pathogenic bacteria, 3) they provide fuel for the colon cells, 4) they enhance immune function, 5) they help prevent colon cancer, and 6) they can inhibit cholesterol synthesis in the liver. Each unique bacteria preferentially ferments different fibers and therefore produces different types of SCFAs. Therefore, in order to reap these benefits, we need to eat a wide variety of plant fibers.\nViscosity, or gelling power, varies amongst fiber types. Some soluble fibers are viscous and all insoluble fibers are not. The higher the viscosity, the greater the gelling power, and the higher the water holding ability. Viscous fiber SLOWS stomach emptying and stool transit time. Therefore, it can help increase satiety and provide relief when struggling with loose stools. However, if soluble fiber is also fermentable (most soluble fiber), then it does not provide constipation relief. Since fiber is fermented in the first part of the colon, it loses its water holding capacity, and may therefore contribute more towards constipation (especially in supplemental forms like wheat dextrin). However, if the soluble fiber is highly viscous and NOT fermentable, like psyllium, it can increase stool water content throughout the entire length of the colon, providing relief for both loose stools and constipation.\nViscous fiber can also reduce absorption of nutrients by binding to them and reducing digestive enzyme function. Therefore, highly viscous soluble fiber is found to be helpful at reducing cholesterol and blood sugar levels. However, if someone is already struggling with malnutrition or nutrient deficiencies, too much soluble fiber may further impair nutrient absorption. BUT, if stools are loose, slowing them down with soluble fiber, will increase the time allowed for nutrient absorption, thereby increasing the ability to absorb nutrients from the diet.\nSoluble and viscous fibers include pectins (apples, citrus, berries), gums (oats, barley, legumes), beta-glucans (oats, barley, rye, and mushrooms), and psyllium. They are all highly fermentable, except psyllium. The more processed (heat & pressure), the less they retain their viscosity. Therefore, muesli eaten cold with yogurt is going to retain more of its viscosity than Cheerios. Another example would be a functional fiber, partially hydrolyzed guar gum (PHGG). This functional fiber has been chemically altered to remove the viscosity of raw guar gum. Therefore it no longer has gel forming properties but remains soluble and fermentable.\nNON-FERMENTABLE, NON-VISCOUS INSOLUBLE FIBERS\nNon-fermentable, non-viscous, insoluble fibers not only provide benefit by increasing fecal bulk and speeding up transit time, they also support detoxification. Since insoluble fibers help support regularity, they are essential for eliminating environmental toxins, chemicals, and excess hormones. If stool stays stagnant in the colon for too long, we can reabsorb these chemicals, toxins, and hormones, leading to increased inflammation and hormonal imbalances, and eventually leaving us feeling crummy, fatigued, and inflamed. Can anyone relate? If you don't have your daily poo, you don't feel like you?? Insoluble fibers also help increase stool bulk which aids in the laxation of the colon and reduces straining. Interestingly, research shows that the larger the fiber particle, the more potent the laxative effect. Therefore coarse wheat bran, which is high in insoluble fiber, is a much more effective laxative than fine wheat bran.\nYOUR LIKELY NOT EATING ENOUGH FIBER\nUnfortunately, 95% of Americans are not getting enough fiber. That means only 5% of Americans are actually meeting the daily recommended intake of 26-38g per day for women and men respectively. And although you may think you are getting enough fiber, research shows that only 1 in 20 actually do! With the rise of grain-free, bean/legume free, and gluten free diets, daily fiber intake can take a significant fall even in health conscious individuals. Ensuring adequate fiber intake is important because it helps eliminate waste products via stool, increases satiety and supports a healthy weight, and feeds good gut microbes. By eating a variety of plant foods, you will get adequate amounts of both soluble and insoluble fiber and plenty of colorful polyphenols to boot! A general rule of thumb is to aim for ~10g of fiber per meal. If you are not sure how much fiber you are getting each day, track it to get a rough estimate, then make intentional changes.\nBut wait! If you have been eating a low fiber diet, tummy rumbles and gas may be more pronounced initially as the bacterial composition and stool viscosity changes. Therefore, to avoid discomfort make sure to gradually increase fiber over time, drink plenty of fluids, and move your body daily to help support proper elimination. Unfortunately, certain types of fibers may exacerbate GI symptoms for some individuals. For example, if you are struggling with diarrhea, you may want to limit your intake of insoluble fiber rich foods until the diarrhea is resolved. Alternatively, if you struggle with constipation, slowly increasing your insoluble fiber intake can be helpful. If you struggle with SIBO you likely need to be careful with some fermentable fibers, to reduce gas and bloating, until SIBO is addressed. In general, a healthy gut thrives off of a variety of plant fibers. Therefore, just because you cannot tolerate one type of fiber right now, doesn't mean you may not be able to tolerate it again in the future. It just means that you need to be mindful of which fibers you can or cannot tolerate. Ideally, work 1:1 with a gut health dietitian to help you navigate dietary and supplemental fibers and address the root cause of your digestive woes.\nWhen using fiber supplements, you first have to ask yourself why do I need a fiber supplement? Is your diet lacking in fiber, then definitely start here first, gradually increasing fiber rich foods in your diet. If struggle with diarrhea you start by increasing soluble fiber rich foods and reducing insoluble fiber rich foods until loose stools improve. If this doesn't work, then try psyllium or Benefiber for some immediate support. Whereas if you struggle with constipation, you may consider adding in coarse wheat bran or using psyllium husk for relief. Finally, if you have SIBO then you may want to avoid inulin/FOS fiber supplements and try Sunfiber (PHGG). The table above will help guide you towards a fiber supplement that may help address your current needs. Ideally, work with a gut health dietitian to navigate all the other fiber options available and help you select a fiber supplement to fit your unique needs. And remember, if your symptoms worsen on a fiber supplement, please stop taking it, and ask your dietitian how to proceed.\nFor some fiber rich recipes check out the links below:\nTurmeric Quinoa Porridge\nFestive Massaged Kale Salad\nHearty Curry Vegetable Soup\nWhole Grain Harvest Cornbread\nLemon Curry Four Bean Salad\nFeedMe Belly Bowl\nApple Banana Breakfast Muffins\nChickpea Skillet Flatbread\n4. Gropper and Smith. Advanced Nutrition and Human Metabolism, 6th Edition.\nLike to read? Then get your evidence based nutrition information here! All posts written by Selva Wohlgemuth, MS, RDN Functional Nutritionist & Clinical Dietitian","What is Inulin?\nInulins are a group of naturally occurring polysaccharides produced by many types of plants, including agave, wheat, onion, bananas, garlic, asparagus, Jerusalem artichoke, and chicory, industrially most often extracted from chicory in the Europe and Jerusalem artichoke in China.\nInulin is a type of oligosaccharide called a fructan. Fructans are a chain of fructose (sugar) molecules strung together. Inulin is fermented by bacteria that normalize the colon and is considered a prebiotic. Prebiotics may improve gastrointestinal health as well as potentially enhance calcium absorption. This prebiotic dietary fiber is a type of carbohydrate that’s not digested in your body, but is used as “food” by your gut’s good bacteria. Inulin promotes the abundance of good bacteria that keep your gut healthy and happy like Bifidobacteria, probiotic microbes that help maintain balance in the microbiome and deter invaders.\nThis is also true of butyrate-producing bacteria — butyrate is a special short-chain fatty acid that fuels gut cells, maintains the gut lining, and combats inflammation.\nInulin is also a type of soluble fibre, which is important for general digestive health. Soluble fiber absorbs water in the gut to form a gel-like substance, which softens stool, reduces hunger, improves motility, and relieves constipation.\nInulin is frequently used as a fat substitute or sweetener. It can be used as an alternative to eggs in baked goods and added to ice cream to prevent ice crystals and reduce the amount of fat. Inulin is also available in capsule, gummy, tablet and powder forms, too.\nPrebiotics boost the abundance of health-promoting bacteria in your gut and increase their activity. Inulin is a prebiotic that can help diversify the gut microbiome (which is a sign of gut health) and provide fuel for good bacteria to make important substances, like short-chain fatty acids and vitamins.\nBy eating inulin-rich foods, you can help your bacteria perform activities that keep your gut healthy. For example, Bifidobacteria, a group of probiotic microbes that feed on inulin foods and inulin supplements, do many valuable jobs for your gut:\nMaintaining gut pH by producing lactate and acetate\nFeeding other bacteria with metabolites they produce\nDeterring pathogens and opportunistic microbes\nModulating the immune system\nMaking vitamins like B and K\nInulin and Constipation\nInulin is a soluble fibre that can relieve constipation. Soluble fibre absorbs water in the gut, creating a gel-like texture and softening stool. This normalises bowel transit and eases constipation.\nThe study showed that 10g of chicory root inulin per day improved both stool frequency and consistency in middle-aged people with mild, chronic constipation.\nInulin and Diabetes\nInulin food supplements reduced both fasting blood sugar and HbA1c levelssignificantly. A review of many studies using inulin fiber confirms that this prebiotic may be helpful for managing blood sugar and blood lipid problems in elderly people with diabetes type II.\nInulin and Weight Loss\nInulin may help with weight loss by making you feel full.\nSoluble fiber helps curb hunger and increases the feeling of fullness after meals. There is some research to show that inulin soluble fiber may promote weight loss, particularly in prediabetic individuals.\nImproved Calcium and Magnesium Absorption\nInulin fiber improves the absorption of specific minerals in the gut, namely calcium and magnesium. Magnesium is a mineral that the body needs for many biochemical reactions that maintain nerve and muscle function, blood sugar levels, and blood pressure. Calcium is just as important. It supports healthy bones and helps the nervous system to communicate with the body."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2e7b6a29-5533-45ca-b6d7-0173518d371f>","<urn:uuid:02f70359-5ba7-43e4-b440-086a72fcf531>"],"error":null}
{"question":"How did the Dreyfus brothers revolutionize the fabric industry with cellulose acetate, and what chemical processes affect the deterioration of cellulose acetate materials today?","answer":"The Dreyfus brothers revolutionized the fabric industry by developing the first commercially successful method of cellulose acetate fiber production. They overcame significant challenges, including adapting traditional knitting and weaving machinery for acetate yarn and creating entirely new dyes specific to acetate materials. They also focused on producing high-quality fabrics under the Celanese brand name, which represented 20% of their company's total sales. Regarding deterioration, cellulose acetate undergoes a specific chemical process where acetyl groups break from their molecular bonds when exposed to moisture, heat, or acids. This releases acetic acid and causes polymer chains to break into smaller pieces, leading to material shrinkage and brittleness. The degradation progresses through stages involving the release of plasticizers and can cause crystalline deposits or liquid-filled bubbles to appear on the surface.","context":["Camille and Henry Dreyfus: Two Pioneers and the Foundation that Honors Them\nPresentation at the 220th Annual\nAmerican Chemical Society Meeting\nAugust 21, 2000\nThe story of the Foundation begins in 1946, when a Swiss organic chemist and businessman, Dr. Camille Dreyfus, decided to create an organization devoted exclusively to providing support to talented chemical scientists. His goals for the Foundation reflected his conviction that chemistry held the keys to a better world. In his words, the Foundation was created \"to advance the science of chemistry, chemical engineering, and related sciences as a means of improving human relations and circumstances around the world.\"\nBut behind this public statement of purpose was a personal motivation - to honor the memory of his much loved brother Henry, also an organic chemist, with whom he had shared years of struggles and triumphs in their development of the first commercially successful method of cellulose acetate fiber production, the basis of the Celanese fiber and chemical companies. Henry had died in London two years before Camille created the\nFoundation, which was first called simply the Dreyfus Foundation. After Camille's death in 1956, Foundation directors voted to change the name to honor both men. (These photos were taken in the 1930s, by the way, and I think they capture something of the very different personalities of these men - Camille the aggressive businessman, and Henry quite content to stay in the lab.)\nThe technical and business achievements of the brothers which made the Foundation possible began in a small laboratory in a comer of their father's garden, in the city of Basel, Switzerland. It's from that city that the Foundation's logo derives, as you see it here. It's an interpretation of the official symbol of the City of Basel, which is a bishop's crook.\nBoth Camille and Henry had a fine education, having received their Ph.D. degrees with highest honors from the University of Basel, Camille in 1901 and Henry three years later. Their initial resources included some chemistry books, a few test tubes, a great many ideas - and above all, an unswerving determination to apply their science to a great project With full respect for the basic science he had studied at Basel, Camille at the same time was frustrated by what he experienced as its limitations. What he wanted was applied science, and in time he set out to apply it to everybody who wore clothes!\nIt took many years, and it was a momentous struggle, but Camille and Henry succeeded in running their backyard laboratory into a vast industrial empire that thrived in the U.S., Great Britain, and Canada. Camille Dreyfus was founder and chairman of Celanese Corporation of America and Canadian Celanese Ltd., while Henry headed their European operation, British Celanese Ltd.\nThe brothers developed their interest in cellulose acetate during postdoctoral study at the Sorbonne, at a time when the substance was little more than a laboratory curiosity. They set up a small laboratory on the Rhine, where their first breakthrough was development of a cellulose acetate motion picture film offered as an alternative to cellulose nitrate, a highly flammable and explosive compound that was used at the time, with often tragic results: periodically, projectionists were seriously burned when the heat of the projection lamp caused the film to burst into flames. The Dreyfuses' new cellulose acetate film was quickly accepted by the fledgling movie industry.\nIn exploring the potential for a cellulose acetate compound, the Dreyfus brothers early on identified the possibility of using it to make s ilk-like artificial fibers and fabrics. But their work on this new idea was interrupted by World War I. With hardly a missed beat, they found themselves at the center of international demand for cellulose acetate dope to be used in aircraft production.\nAt the time, the wooden wings of aircraft were covered with canvas that tended to tear and flap during flight; more important, the nitrocellulose dope then used as a coating to strengthen the aircraft frame was highly inflammable and easily ignited by incendiary tracer bullets. On learning of the Dreyfus brothers' work with cellulose acetate, the British government invited Camille and Henry to build an acetate plant in England, which they did, and in turn U.S. Secretary of War Newton Baker issued a similar invitation. As a result, early in 1918 the American Cellulose & Chemical Manufacturing Company opened a plant in Cumberland, Maryland, supplying initially 1.5 million pounds of nonflammable acetate to coat the wings and fuselages of American aircraft.\nNo sooner had this new venture hit its stride than the war ended, a scant seven months after the Dreyfuses had begun production. Returning to their laboratory, the brothers resumed in earnest their development work on cellulose acetate. \"Let us look around to really make something big,\" they said to each other. Their investigations explored the celluloid industry as well as the so-called artificial silk industry, whose chief product at the time was rayon fibers and fabrics.\nThey say knowledge and money is a combination that's hard to beat. Camille Dreyfus knew he had half the combination when he and his brother had completed the last of some 20,000 experiments from 1910 to 1921 on how best to produce a commercial form of cellulose acetate. With youth and knowledge on his side, the fight for money and power never took on the aspect of anything more than a game - a game he played for big stakes, gambling millions and millions of dollars, sometimes his own, sometimes others', to prove to the world, as he said, that \"man could make a better fiber than nature.\"\nCamille and Henry Dreyfus encountered a great many obstacles and disappointments in their journey from a specimen fiber to full commercial production. One of their earliest problems was the discovery that the new cellulose acetate yam they had created - using their own method of downward dry spinning - was not amenable to standard knitting and weaving machinery. Their answer? Learn all they could about knitting and weaving, until they knew enough to adapt the traditional apparatus to acetate yarn. To reach their great goal of commercial production, they didn't hesitate to subsidize knitters and weavers to work on the new equipment initially.\nHard on the heels of the fabrication problems came the discovery that cellulose acetate could not be dyed using conventional dyes. The Dreyfuses were at first understandably frantic and headed home to Basel, where they conferred with the leading dyers of tile time, who treated the brothers to their darkest prediction: no known dyes would ever take. \"Then,\" responded the brothers, \"new dyes must be invented.\" A group of Britain's leading dyestuff manufacturers and dyers, driven at fever pitch by the Dreyfus brothers, aided them in creating an entirely new series of dyes for acetate yarn. For his work in this area, Henry Dreyfus was awarded the Perkin Research Medal, the highest honor of the Society of Dyers and Colourists of Great Britain. He received that honor in 1938.\nMarketing their new yams and the fabrics textile manufacturers made from them presented another set of problems for the Dreyfus brothers. Despite growing acceptance of their yarns, Camille and Henry soon realized that to offer the market the quality fabrics they had in mind - ones that would command higher retail prices and stand distinct from the \"bargain basement\" fabrics that dominated synthetics - they would simply have to enter the fabric business themselves. This they did, producing fabrics under the brand name Celanese that impressed for their quality of both weaving and styling.\nCamille Dreyfus, a great merchandiser, succeeded in creating demand at the retail level first, an uncommon approach in the textiles industry. He could be seen on many an occasion walking up Fifth Avenue to pay a visit to apparel buyers at the best retail stores, bolts of Celanese fabric under his arm. As a measure of success in this area of their work, for many years the fabric business represented a full 20 percent of total sales of the Celanese companies.\nThese chemical pioneers struck new ground in a number of other areas which changed the face of the synthetic fibers industry. For example, their acetate products had solubility characteristics unlike any previously obtained and with much higher viscosities. Camille and Henry were the first to realize the importance of the acetyl content of cellulose acetate as it affects the properties of the material, and they showed that viscosity, high tensile strength, and elasticity are complementary. They also found out how to avoid cellulose degradation, long before the polymeric nature of cellulose was discovered many years later.\nCamille Dreyfus was imaginative and determined; above all, he was an optimist. He and his brother were fully involved in every aspect of Celanese operations - product development, production, financing, marketing, sales, and research. On the marketing and research side, Camille was notoriously aggressive. One anecdote typifies his consuming interest in the performance of Celanese fabrics. It's said he and his wife were going to a dinner one evening. \"She had a new black-and-white dress,\" he related, \"and I said it was wonderful. 'It's a pity you can't make something like this,' she told me. 'It's a Paris gown.' I said, ' Let me see that material. ' I put my lighted cigarette through the dress - and the material curled up in a knob around the hole. It was nonflammable - it was our own Celanese.\" One can't help wondering how the story would have gone if the fabric hadn't been Celanese ... ! But no doubt Camille was pretty certain of the outcome of that particular experiment.\nIn 1987, Celanese merged with the giant German chemical firm Hoechst and has since undergone a series of product shifts as has been the case generally in chemicals. Last year Celanese was spun off from Hoechst, with sales at $5.3 billion, and the firm is once again traded on the New York Stock Exchange.\nOver the fifty-four years since the Foundation's beginnings, its directors and operating staff have sought to perpetuate Camille Dreyfus' abundant confidence in the future of chemistry. His interest and belief in young scientists and scientists-to-be was reflected in one of the Foundation's first initiatives, the support of the National Merit Scholarship program for high school students during its earliest years.\nA broad range of institutions and chemical endeavors have received Foundation support over the years. In the early days, the Foundation made two sizable grants to support the construction of facilities that promised to strengthen the chemical enterprise: the Camille Eduoard Dreyfus Chemistry Building at MlT, and the Camille Dreyfus Laboratory of Polymer Chemistry at Research Triangle Institute - the first building of the new RTI complex, incidentally.\nThree endowments were also made in the early years - to MlT, to Stanford University and to The Rockefeller University, for chaired Camille Dreyfus professorships.\nCurrently, the Foundation conducts seven award programs designed to meet a range of needs and opportunities in the chemical sciences. These are all described in detail on our Web site, and in the literature package that's available on the table near the door. Today, the focus of our symposium will be the work and the insights of some of the chemists and chemical engineers we have been privileged to support through one or more of these programs.\nIn closing, I'd like to quote from a letter in the Foundation archives from Jean Dreyfus Boissevain, Camille Dreyfus' widow, who succeeded him as life president of the Foundation, as he specified in his will. To a young chemist friend, she wrote of her husband, \"The Doctor was completely dedicated to his chosen profession. He was a man of simple tastes, great charm and kindness, with the heart of a pioneer .... He faced many vicissitudes, struggles and disappointments in his development of cellulose acetate processes, but never doubted his ability to overcome all the obstacles. A writer in Fortune magazine described him well when he said, 'Dr. Camille Dreyfus is the sort of man who would find trouble in paradise - and probably beat the proprietor to a solution!'\"\nHer letter continues, \"He was always interested in the young people and at the end of his life, remarked that he would like a few more years to devote himself to chemistry and spend his time among the young scientists who, he said, amazed him by their great knowledge, enthusiasm, and curiosity.\" No doubt he would have been delighted to be among the present company.\nAugust 21, 2000","Cellulose acetate film\nCellulose acetate film, or safety film, is used in photography as a base material for photographic emulsions. It was introduced in the early 20th century by film manufacturers as a safe film base replacement for unstable and highly flammable nitrate film.\nBeginning with cellulose diacetate in 1909, this innovation continued with cellulose acetate propionate and cellulose acetate butyrate in the 1930s, and finally in the late 1940s, cellulose triacetate was introduced, alongside polyester bases. These less flammable substitutes for nitrate film were called safety film.\nThe motion picture industry continued to use cellulose nitrate supports until the introduction of cellulose triacetate in 1948, which met the rigorous safety and performance standards set by the cinematographic industry. The chemical instability of this material, unrecognized at the time of its introduction, has since become a major threat for film collections.\nDecay and the \"vinegar syndrome\"\nThe first instance of cellulose triacetate degradation was reported to the Eastman Kodak Company within a decade of its introduction in 1948. The first report came from the Government of India, whose film was stored in hot, humid conditions. It was followed by further reports of degradation from collections stored in similar conditions. These observations resulted in continuing studies in the Kodak laboratories during the 1960s.\nBeginning in the 1980s, there was a great deal of focus upon film stability following frequent reports of cellulose triacetate degradation. This material releases acetic acid, the key ingredient in vinegar and responsible for its acidic smell. The problem became known as the \"vinegar syndrome.\"\nThe progression of degradation\nIn acetate film, acetyl (CH3CO) groups are attached to long molecular chains of cellulose. With exposure to moisture, heat, or acids, these acetyl groups break from their molecular bonds and acetic acid is released. While the acid is initially released inside the plastic, it gradually diffuses to the surface, causing a characteristic vinegary smell.\nThe decay process follows this pattern:\n- Acetic acid is released during the initial acetate base deterioration, leading to the characteristic vinegar odor. This signal marks the progression of deterioration.\n- The plastic film base becomes brittle. This occurs in the advanced stages of deterioration, weakening the film and causing it to shatter with the slightest tension. These physical changes happen because cellulose acetate consists of long chains of repeating units, or polymers. When the acetic acid is released as these groups break off, the acidic environment helps to break the links between units, shortening the polymer chains and leading to brittleness.\n- Shrinkage also occurs during this process. With the cellulose acetate polymer chains breaking into smaller pieces, and with their side groups splitting off, the plastic film begins to shrink. In advanced stages of deterioration, shrinkage can be as much as 10%. A 1% reduction in size renders motion picture film unusable.\n- As the acetate base shrinks, the gelatin emulsion of the film does not shrink, because it is not undergoing deterioration. The emulsion and film base separate, causing buckling, referred to by archivists as 'channelling.' Sheet films are often severely channelled in the later stages of degradation.\n- Crystalline deposits or liquid-filled bubbles appear on the emulsion. These are evidence of plasticizers, additives to the plastic base, becoming incompatible with the film base and oozing out on the surface. This discharge of plasticizers is a sign of advanced degradation.\n- In some cases, pink or blue colors appear in some sheet films. This is caused by antihalation dyes, which are normally colorless and incorporated into the gelatin layer. When acetic acid is formed during deterioration, the acidic environment causes the dyes to return to their original pink or blue color.\nTesting for degradation\nA testing product developed by the Image Permanence Institute, A-D, or \"acid-detection\" indicator strips change color from blue through shades of green to yellow with increasing exposure to acid. According to the test User's Guide, they were \"...created to aid in the preservation of collections of photographic film, including sheet and roll films, cinema film, and microfilm. They provide a nondestructive method of determining the extent of vinegar syndrome in film collections.\"  These tools can be used to determine the extent of damage to a film collection and which steps should be taken to prolong their usability.\nPreservation and storage\nCurrently there is no practical way of halting or reversing the course of degradation. While there has been significant research regarding various methods of slowing degradation, such as storage in molecular sieves, temperature and moisture are the two key factors affecting the rate of deterioration. According to the Image Permanence Institute, fresh acetate film stored at a temperature of 65°F (18°C) and 50% relative humidity will last approximately 50 years before the onset of vinegar syndrome. Reducing the temperature 15°, while maintaining the same level of humidity, delays the process by 150 years. A combination of low temperature and low relative humidity represents the optimum storage condition for cellulose acetate base films, however, in practice temperatures of 55°F (12°C) and a relative humidity of 35% are now being used.\nMicroenvironments—the conditions inside an enclosure—can also have an impact on the condition of cellulose acetate film. Enclosures that are breathable or that contain an acid absorbent are instrumental in reducing the rate of decay due to vinegar syndrome. Sealed metal containers can trap the decay products released by the film, promoting the spread of vinegar syndrome.\nRescuing damaged film\nDuring early stages of decay, the film content can be rescued by transferring it to new film stock. Once the film becomes brittle it cannot be copied in its entirety. Because the gelatin emulsion usually stays intact during the degradation process, it is possible to save the image on sheet film using solvents to dissolve the emulsion away from the shrunken base. Once the emulsion has been freed from the shrunken support, it can be photographed or transferred to a new support. Because of the solvents used, this is a delicate and potentially hazardous procedure and is an expensive process for a large collection. Degraded motion picture film cannot be restored in this way, but sheet films often can.\nWhile digitization would be an ideal way to preserve the contents of cellulose acetate film, current standards do not allow for scanning at sufficient resolutions to produce a copy of the same picture and sound quality as the original. Currently, the National Film Preservation Institute advocates film-to-film transfer as the best method for film preservation, with the copies stored in proper environmental conditions.\nCellulose acetate film is also used to make replicates of materials and biological samples for microscopy. The techniques were developed for metallographic needs to examine the grain structure of polished metals. Replication can be used to understand the distribution, for example, of different types of iron in carbon steel samples, or the fine distribution of damage to a sample subject to mechanical wear.\n- ^ National Film Preservation Foundation. The Film Preservation Guide: The Basics for Archives, Libraries, and Museums. San Francisco: National Film Preservation Foundation, 2004, 9.\n- ^ Ram, A. Tulsi. “Archival Preservation of Photographic Film-A Perspective.” Polymer Degradation and Stability 29 (1990), 4.\n- ^ Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. \"Stability of Cellulose Ester Base Photographic Film: Part I-Laboratory Testing Procedures.\" SMPTE Journal 101 no.5 (1992): 336.\n- ^ James M. Reilly. \"Basic Strategy for Acetate Film Preservation.\" Microform and Imaging Review 31 no.4 (2002), 117.\n- ^ Image Permanence Institute. User's Guide for A-D Strips: Film Base Deterioration Monitor. Rochester: Image Permanence Institute, 2001.\n- ^ Allen, N.S., M. Edge, C.V. Horie, T.S. Jewitt, and J.H. Appleyard. “Degradation of Historic Cellulose Triacetate Cinematograph Film: Influence of Various Film Parameters and Prediction of Archival Life.” The Journal of Photographic Science 36 no. 6 (1998), 194.\n- ^ Reilly, James M. IPI Storage Guide for Acetate Film; Instructions of Using the Wheel, Graphs, and Table; Basic Strategy for Film Preservation. Rochester: Image Permanence Institute, 1993.\n- ^ Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. “Stability of Cellulose Ester Base Photographic Film: Part II-Practical Storage Considerations.” SMPTE Journal 101 no. 5 (May 1992): 353.\n- ^ \"Film and Media Storage\". http://www.bonded.com/storage.php.\n- ^ J.L. Bigourdan and J. Reilly, “Effectiveness of Storage Conditions in Controlling the Vinegar Syndrome: Preservation Strategies for Acetate Base Motion-Picture Film Collections”, Image Permanence Institute, Rochester Institute of Technology.\n- ^ Reilly, James M. “Basic Strategy for Acetate Film Preservation.” Microform and Imaging Review 31 no. 4 (2002): 118.\n- ^ www.filmpreservation.org/preservation/film_guide.html\n- ^ [http://www.sciencedirect.com/science/article/pii/0026080084900028 Measurement of the interlamellar spacing of pearlite ]\n- ^ Mechanisms of wear of the metal surface during fretting corrosion of steel on polymers\n- Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. “Stability of Cellulose Ester Base Photographic Film: Part I-Laboratory Testing Procedures.” SMPTE Journal 101 no. 5 (May 1992): 336-346.\n- Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. “Stability of Cellulose Ester Base Photographic Film: Part II-Practical Storage Considerations.” SMPTE Journal 101 no. 5 (May 1992): 347-354.\n- Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, and C.J. Erbland. “Stability of Cellulose Ester Base Photographic Film: Part III-Measurement of Film Degradation.” SMPTE Journal 104 (May 1995): 281-291.\n- Adelstein, P.Z., J.M. Reilly, D.W. Nishimura, C.J. Erbland, and J.L. Bigourdan. “Stability of Cellulose Ester Base Photographic Film: Part V- Recent Findings.” SMPTE Journal 104 no. 7 (July 1995): 439-447.\n- Allen, N.S., M. Edge, C.V. Horie, T.S. Jewitt, and J.H. Appleyard. “The Degradation and Stabilization of the Historic Cellulose acetate/ Nitrate Base Motion-picture Film.” The Journal of Photographic Science 36 no.3 (1988): 103-106.\n- Allen, N.S., M. Edge, C.V. Horie, T.S. Jewitt, and J.H. Appleyard. “Degradation of Historic Cellulose Triacetate Cinematograph Film: Influence of Various Film Parameters and Prediction of Archival Life.” The Journal of Photographic Science 36 no. 6 (1998), 194-198.\n- Allen, N.S., M. Edge, C.V. Horie, T.S. Jewitt, and J.H. Appleyard. “The Degradation Characteristics of Archival Cellulose Triacetate Base Cinematograph Film.” The Journal of Photographic Science 36 no. 6 (1998), 199-203.\n- Allen, N.S., M. Edge, T.S. Jewitt, and C.V. Horie. “Initiation of the Degradation of Cellulose Triacetate Base Motion Picture Film.” The Journal of Photographic Science 38 no. 2 (1990): 54-59.\n- Allen, N.S., J.H. Appleyard, E. Edge, D. Francis, C.V. Horie, and T.S. Jewitt. “The Nature of the Degradation of Archival Cellulose-Ester Base Motion-Picture Film: The Case for Stabilization.” The Journal of Photographic Science 36 no.2 (1988): 34-39.\n- Allen, N.S., M. Edge, T.S. Jewitt, and C.V. Horie. “Stabilization of Cellulose Triacetate Base Motion Picture Film.” The Journal of Photographic Science 30 no.1 (1990):26-29.\n- Bigourdan, Jean-Louis and James M. Reilly. “Effectiveness of storage Conditions in Controlling the Vinegar syndrome: Preservation Strategies for Acetate Base Motion-Picture Film Collections.” In Michelle Aubert and Richard Billeaud. Archiver et communiquer l'image et le son :les enjeux du 3ème millenaire : actes du Symposium Technique Mixte—JTS Paris 2000, 14-43. Paris: CNC, 2000.\n- Edge, M. and N.S. Allen. “Fundamental Aspects of the Degradation of Cellulose Triacetate Base Cinematograph Film.” Polymer Degradation and Stability 25 no. 2-4 (1989): 345-362.\n- Horvath, David G. (1987). The Acetate Negative Survey Final Report. Louisville, KY: Ekstrom Library Photographic Archives, University of Louisville. \n- Meyer, Mark-Paul and Paul Read. “Restoration and Preservation of Vinegar Syndrome Decayed Acetate Film.” In Michelle Aubert and Richard Billeaud. Archiver et communiquer l'image et le son :les enjeux du 3ème millenaire : actes du Symposium Technique Mixte—JTS Paris 2000, 54-65. Paris: CNC, 2000.\n- National Film Preservation Foundation. The Film Preservation Guide: The Basics for Archives, Libraries, and Museums. San Francisco: National Film Preservation Foundation, 2004.\n- Ram, A.T. “Archival Preservation of Photographic Films-A Perspective.” Polymer Degradation and Stability 29 no. 1 (1990): 3-29.\n- Ram, A.T., D.F. Kopperl, and R.C. Sehlin. “The Effects and Prevention of Vinegar Syndrome.” The Journal of Imaging science and Technology 38 no. 3 (1994): 249-261.\n- Reilly, James M. “Basic Strategy for Acetate Film Preservation.” Microform and Imaging Review 31 no. 4 (2002): 117-130.\n- Reilly, James M. IPI Storage Guide for Acetate Film; Instructions of Using the Wheel, Graphs, and Table; Basic Strategy for Film Preservation. Rochester: Image Permanence Institute, 1993.\nWikimedia Foundation. 2010.\nLook at other dictionaries:\nCellulose acetate — Cellulose acetate, first prepared in 1865, is the acetate ester of cellulose. Cellulose acetate is used as a film base in photography, and as a component in some adhesives; it is also used as a synthetic fiber.Acetate fiber and triacetate… … Wikipedia\ncellulose acetate — n. chemical used in photographic film and the yarn and textile inustry … English contemporary dictionary\nCellulose triacetate — Cellulose triacetate, also known simply as triacetate, is manufactured from cellulose and acetate. Triacetate is typically used for the creation of fibres and film base.It is similar chemically to cellulose acetate, with the distinguishing… … Wikipedia\nacetate — ► NOUN 1) Chemistry a salt or ester of acetic acid. 2) fibre or plastic made of cellulose acetate. 3) a transparency made of cellulose acetate film … English terms dictionary\nFilm preservation — Stacked containers filled with reels of film stock. The film preservation, or film restoration, movement is an ongoing project among film historians, archivists, museums, cinematheques, and non profit organizations to rescue decaying film stock… … Wikipedia\nacetate — [ asɪteɪt] noun 1》 Chemistry a salt or ester of acetic acid: lead acetate. 2》 textile fibre or plastic made of cellulose acetate. ↘a transparency made of cellulose acetate film. ↘a direct cut recording disc coated with cellulose acetate … English new terms dictionary\nfilm — [ film ] n. m. • 1889; mot angl. « pellicule » 1 ♦ Pellicule photographique. Développer un film. Rouleau de film. ♢ (1896) Plus cour. Pellicule cinématographique; bande régulièrement perforée. Film de 35 mm (format professionnel). Films de format … Encyclopédie Universelle\nCellulose — is an organic compound with the formula chem|(C|6|H|10|O|5|)|n, a polysaccharide consisting of a linear chain of several hundred to over ten thousand β(1→4) linked D glucose units.cite book author=Crawford, R. L. title=Lignin biodegradation and… … Wikipedia\nfilm — /fɪlm / (say film) noun 1. a thin layer or coating. 2. a thin sheet of any material. 3. Photography a. the sensitive coating, as of gelatine and silver bromide, on a photographic plate. b. a strip or roll of cellulose nitrate or cellulose acetate … Australian English dictionary\nFilm base — A film base is a transparent substrate which acts as a support medium for the photosensitive emulsion that lies atop it. Despite the numerous layers and coatings associated with the emulsion layer, the base generally accounts for the vast… … Wikipedia"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5f97f53f-0d70-4342-9d98-eae8961bd610>","<urn:uuid:acc84dc5-1fbf-469d-a0db-cbc58b28cbd5>"],"error":null}
{"question":"Could you explain how non-coding RNAs are regulated in breast cancer, and what new editing technologies are being developed to target them?","answer":"Non-coding RNAs (ncRNAs) in breast cancer are regulated through various epigenetic modifications, including six types of histone modifications (H3K4me1, H3K4me3, H3K27ac, H3K36me3, H3K9me3 and H3K27me3) and DNA methylation. These modifications work synergistically to modulate ncRNA transcription. As for targeting ncRNAs, a new technology called epigenome editing has been developed. This technology uses chimeric proteins with DNA binding domains (like zinc finger proteins, TALE effectors, and CRISPR-Cas9) combined with epigenetic factors to modify the epigenetic marks. Additionally, there are compounds called antagomirs that can prevent the binding of non-coding RNAs to their targets, providing a way to inhibit their action in epigenetics.","context":["Non-coding RNA Expression is Synergistically modulated by Various Types of Epigenetic Modifications\nIt is well known that most of the mammalian genome is transcribed, producing large amounts of non-coding RNAs (ncRNAs). The two main types of ncRNA are micro RNAs (miRNAs) and long non-coding RNAs (lncRNAs). MiRNAs are a recently discovered and well-characterized class of ncRNAs that are frequently dysregulated in different cancer and cancer subtypes. In contrast, the molecular basis of the functions of many lncRNAs is just emerging. However, substantial evidence indicates that lncRNAs play intricate roles in the regulation of a wide variety of biological processes. Although many studies have demonstrated the dysregulation of ncRNA expression in various types of cancer, the epigenetic regulation of ncRNAs remains poorly understood in different cancers and cancer subtypes.\nHere, building on the results of our recent studies, we carried out a series of integrative analyses to compare the alterations in six types of histone modifications (H3K4me1, H3K4me3, H3K27ac, H3K36me3, H3K9me3 and H3K27me3) and DNA methylation in the promoters of miRNAs and lncRNAs between two cell lines representing different breast cancer subtypes (luminal and basal). Widespread epigenetic alterations of miRNAs and lncRNAs were observed in the two subtypes.\nIn contrast to protein-coding genes, the majority of epigenetically dysregulated ncRNAs were shared between breast cancer subtypes, but a subset of transcriptomic and corresponding epigenetic changes were occurred in a subtype-specific manner. In addition, our results suggest that various types of epigenetic modifications might synergistically modulate the transcription of ncRNAs. Our observations further highlight the complementary dysregulation of epi-modifications, particularly of miRNA members within the same family, which produce the same directed alterations as a result of diverse epi-modification alterations. Functional enrichment analysis also revealed that epigenetically dysregulated ncRNAs were significantly involvarious types of epi-modifications might synergistically modulate the transcription of ncRNAsved in several hallmarks of cancers. Finally, our integrative analysis of epigenetic modification-mediated miRNA regulatory networks revealed that cancer progression was associated with specific miRNA-gene modules in two subtypes.\nIn summary, our analyses demonstrate the suitability of integrating multiple “omics” data sets to identify novel epigenetically dysregulated ncRNAs (lncRNAs and miRNAs) in different breast cancer subtypes. This study presents the aberrant epigenetic patterns of ncRNAs, thus providing a highly valuable resource for further investigation of the epigenetic regulation of breast cancer subtypes.\nXu J, Wang Z, Li S, Chen J, Zhang J, Jiang C, Zhao Z, Li J, Li Y, & Li X (2016). Combinatorial epigenetic regulation of non-coding RNAs has profound effects on oncogenic pathways in breast cancer subtypes. Briefings in bioinformatics PMID: 27742663\nLi Y, Li S, Chen J, Shao T, Jiang C, Wang Y, Chen H, Xu J, & Li X (2014). Comparative epigenetic analyses reveal distinct patterns of oncogenic pathways activation in breast cancer subtypes. Human molecular genetics, 23 (20), 5378-93 PMID: 24871326\nLi Y, Zhang Y, Li S, Lu J, Chen J, Wang Y, Li Y, Xu J, & Li X (2015). Genome-wide DNA methylome analysis reveals epigenetically dysregulated non-coding RNAs in human breast cancer. Scientific reports, 5 PMID: 25739977","Epigenome editing, a new anti-aging technology for longevity\nEpigenetics includes all the mechanisms that regulate gene expression, namely DNA methylation, histone modifications and chromatin remodelling. These mechanisms are introduced into the genome by non-coding enzymes and RNAs. With age, epigenetic errors occur, causing many diseases such as cancers. However, epigenetic changes are reversible and it is possible to target their origins to reverse them. Handling these errors makes it possible to understand their role in aging and the development of pathologies, but also to develop new therapies. Recently, the epigenome editing technology, literally meaning “genome editing technology“, has been developed, whose tools target the non-coding enzymes and RNAs responsible for the modifications, thus allowing the epigenome to be manipulated.\nEpigenome Editing : what is that?\nIn the term “epigenome editing”, the epigenome refers to the epigenetic state of a cell, while the term “editing” refers to the manipulation of this state to modify the epigenetic marks present. Epigenetic editing is therefore a technology for revising the epigenetic state of the genome, thus modifying the expression profile of a cell’s genes. It is currently the only technology that can modulate the epigenome.\nThe tools used to modify epigenetic marks are chimeric proteins, composed of two parts: a DNA binding domain, allowing proteins to associate with DNA, as its name suggests, and a domain comprising an epigenetic factor (enzyme or other) that will modify DNA. The DNA binding domains are of three different types: zinc finger proteins (or ZFP for Zinc Finger Protein), TALE effectors (Transcription Activator-Like Effectors) and the CRISPR-Cas9 system. These chimeric proteins are powerful tools for creating isogenic cells (“clones”) and transgenic animals (which may or may not express a specific gene) making epigenetic publishing a powerful tool for research, biotechnology and medicine.\nRevising epigenetic changes which can lead to age-related diseases offers great potential for therapeutic development against these diseases. Epigenetic editing is a technology that is very applicable to anti-aging research. It allows the introduction of epigenetic variations in vitro or in vivo, useful to understand and explain the development of pathologies, the severity of certain symptoms and the role of the environment. This approach complements the knowledge on the development of pathologies that cannot be explained by an approach based solely on the study of the DNA sequence. Epigenome editing can induce an epigenetic mark, but it can also remove it, as is the case, for example, in the study of cancers. Indeed, cancer cells can be induced in vitro by introducing epigenetic errors into the DNA of normal cells, and then these errors can be removed from cancer cells.\nEpigenome editing: what does it target?\nThis technology targets molecular effectors that produce epigenetic marks. Action is achieved by inhibiting enzymes that bring or suppress epigenetic changes, or by preventing the binding of non-coding RNAs with their targets. DNA methyltransferases (DNMT) are the enzymes responsible for establishing and maintaining DNA methylation, and one of the main targets of epigenetic editing. DNMT inhibitors are the most advanced in clinical applications of epigenome editing. At the same time, there are currently at least 232 enzymes capable of modulating post-translational changes in histones, and epigenetic therapies based on HDAC (Histone deacetylase: an enzyme that suppresses histone acetylation) are already in the clinical phase of testing. By targeting the enzymes responsible for epigenetic markers, epigenome editing makes it possible to target different types of epigenetic mutations such as loss of function mutations (where a gene is no longer expressed) and function gain mutations (where a gene is too expressed).\nEpigenome editing : epigenetic alterations and epi-medicine\nEpigenetic alterations are one of the major causes of aging and many pathologies. This is particularly the case for cancers, whose malignancy often results from epigenetic errors leading to the expression of pro-cancer genes, called oncogenes. Cancer epigenetics is one of the most important research topics in this field and the mechanisms of carcinogenesis are now better understood and controlled. It is now possible to induce or suppress cancer by epigenetic editing in cellular and animal models, with the hope of transferring this to humans in the next few years. An example of these mechanisms is the enzyme G9a, a methylase that will modify a specific histone, and, depending on the methylation state of this histone, will either induce or suppress cancer in an in vitro model. Another example is DNMT3A, a methyltransferase responsible for modifying DNA on a particular gene, called SOX2, and whose absence of expression (no protein production from the gene) will lead to suppression of tumor growth.\nEpi-medicine (or epigenetic drugs) rely on these mechanisms, sometimes beneficial, sometimes deleterious, to modify them in the right direction. They are compounds capable of targeting epigenetic enzymes to correct marks on the genome. Interest in these epigenetic modulators is growing, particularly in the treatment of cancers. By 2016, six epi-drugs had received FDA (Food and Drug Administration) approval. In addition, other drugs are in clinical development, including DNA methylation inhibitors and histone deacetylase enzyme (HDAC) inhibitors, overexpression of which is a common feature of human pathologies. Inhibiting HDACs would address some of the characteristics of cancers such as cell proliferation, angiogenesis and cell differentiation. More recently, it is the acetyltransferase histone inhibitors (HATi), the enzymes responsible for post-translational histone acetylation, that have gained interest thanks to promising results on solid tumours. Oncology is not the only field that can benefit from epigenome editing technology: the administration of a non-specific HDAC inhibitor, trichostatin, for example, has reversed neurodevelopmental disturbances in adult rats.\nFinally, there is a class of compounds capable of preventing the binding of non-coding RNAs to their targets: antagomirs. They are used to inhibit the action of non-coding RNAs in epigenetics. Treatment with the microRNA 181a antagomir following a stroke has been shown to reduce stroke severity, neural deficits, inflammation and provide neuroprotective effects in mice. In addition, the study demonstrated that reducing microRNA 181a before a stroke protects brain cells from ischemic damage in vivo and in vitro. However, epic drugs and antagonists are non-specific compounds that can affect any type of tissue and organ. This characteristic highlights the risks of more or less serious adverse reactions that can be observed with such treatments.\nEpigenome editing is an innovative technology whose powerful tools have revolutionized biomedical research. It offers unprecedented opportunities to study the epigenetics of aging and develop therapies against related diseases. Although methodological and conceptual breakthroughs are still needed to apply epigenome editing in routine and clinical settings, a large number of publications have already demonstrated the great potential of this technology.\nSee all our articles on the epigenetics of aging and longevity\nWith epigenetics, a new technology has come to light: epigenome editing, made possible by technologies such as the use of CRISPR-Cas9.\n Shota Nakade, Takashi Yamamoto, Tetsushi Sakuma, Cancer induction and suppression with transcriptional control and epigenome editing technologies, Journal of Human Genetics (2018) 63:187–194. https://doi.org/10.1038/s10038-017-0377-8\n Pratiksha I. Thakore, Joshua B. Black, Isaac B. Hilton and Charles A. Gersbach. Editing the Epigenome: Technologies for Programmable Transcriptional Modulation and Epigenetic Regulation, Nat Methods. 2016 February ; 13(2): 127–137. doi:10.1038/nmeth.3733.\n Cia-Hin Lau and Yousin Suh. Genome and Epigenome Editing in Mechanistic Studies of Human Aging and Aging-Related Disease, Gerontology. 2017 ; 63(2): 103–117. doi:10.1159/000452972.\n Graça et al. Clinical Epigenetics (2016) 8:98. DOI 10.1186/s13148-016-0264-8\n Tim J. Wigle, Promoting Illiteracy in Epigenetics: An Emerging Therapeutic Strategy, Current Chemical Genomics, 2011, 5, (Suppl 1-M1) 48-5.\n Sophia Xiao Pfister and Alan Ashworth. Marked for death: targeting epigenetic changes in cancer, Nature Reviews, Drug Discovery, Volume 16 (April 2017), 241-263.\n Carlos Lopez-Otin, Maria A. Blasco, Linda Partridge, Manuel Serrano and Guido Kroemer. The Hallmarks of Aging, Cell 153, June 2013, 1194-1217.\n Alfonso Dueñas-González, J. Jesús Naveja, José L. Medina-Franco, Introduction of Epigenetic Targets in Drug Discovery and Current Status of Epi-Drugs and Epi-Probes, Epi-Informatics. http://dx.doi.org/10.1016/B978-0-12-802808-7.00001-0\n Moshe Szyf. Prospects for the development of epigenetic drugs for CNS conditions, Nature Reviews, Drug Discovery Volume 14 (July 2015), 461-474.\n Li-Jun Xu, Yi-Bing Ouyang, Xiaoxing Xiong, Creed M Stary, and Rona G Giffard. Post-stroke treatment with miR-181 antagomir reduces injury and improves long-term behavioral recovery in mice after focal cerebral ischemia. Exp Neurol. 2015 February ; 264: 1–7. doi:10.1016/j.expneurol.2014.11.007.\nAnne is studying medicine science at the Institute of Pharmaceutical and Biological Science in Lyon and she has graduated with a Bachelor’s degree in molecular and cellular biology at the University of Strasbourg.\nMore about the Long Long Life team\nAnne étudie les sciences du médicament à l’Institut des Sciences Pharmaceutiques et Biologiques de Lyon. Elle est titulaire d’une licence en biologie moléculaire et cellulaire de l’Université de Strasbourg.\nEn savoir plus sur l’équipe de Long Long Life"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:ca98ab5f-9561-4f39-a1dc-09574fc35529>","<urn:uuid:7a1dfcae-e6d5-4cfa-a2c7-d24edf91fd49>"],"error":null}
{"question":"How do the OECD's TiVA initiative and UNCTAD's role in global trade complement each other in addressing international economic development?","answer":"The OECD's Trade in Value-Added (TiVA) initiative and UNCTAD complement each other in addressing international economic development through different approaches. The TiVA initiative focuses on measuring the value added by each country in globally consumed goods and services, providing insights into commercial relations between nations. Meanwhile, UNCTAD serves as a permanent intergovernmental body that aims to maximize trade and investment opportunities for developing countries and helps them integrate into the world economy on an equitable basis. Together, these initiatives provide both analytical tools and policy frameworks for understanding and promoting international economic development.","context":["International production, trade and investments are increasingly organised within so-called global value chains (GVCs) where the different stages of the production process are located across different countries. Globalisation motivates companies to restructure their operations internationally through outsourcing and offshoring of activities.\nFirms try to optimise their production processes by locating the various stages across different sites. The past decades have witnessed a strong trend towards the international dispersion of value chain activities such as design, production, marketing, distribution, etc.\nThis emergence of GVCs challenges conventional wisdom on how we look at economic globalisation and in particular, the policies that we develop around it.\nThe OECD provides a broad range of work to help policy makers understand the effects of GVCs on a number of different topics:\n|Trade in Value-Added||Trade policy and GVCs\n||Initiative on GVCs, Production Transformation and Development|\n|The goods and services we buy are composed of inputs from various countries around the world. However, the flows of goods and services within these global production chains are not always reflected in conventional measures of international trade. The joint OECD – WTO Trade in Value-Added (TiVA) initiative addresses this issue by considering the value added by each country in the production of goods and services that are consumed worldwide. TiVA indicators are designed to better inform policy makers by providing new insights into the commercial relations between nations||Global value chains (GVCs) have become a dominant feature of world trade, encompassing developing, emerging, and developed economies. The whole process of producing goods, from raw materials to finished products, is increasingly carried out wherever the necessary skills and materials are available at competitive cost and quality. Similarly, trade in services is essential for the efficient functioning of GVCs, not only because services link activities across countries but also because they help companies to increase the value of their products. This fragmentation highlights the importance of an ambitious complementary policy agenda to leverage engagement in GVCs into more inclusive growth and employment and the OECD is currently undertaking comprehensive statistical and analytical work that aims to shed light on the scale, nature and consequences of international production sharing.||The OECD Initiative on Global Value Chains (GVCs), Production Transformation and Development is a platform for policy dialogue and knowledge sharing between OECD and non-OECD countries. It aims at improving evidence and identifying policy guidelines to promote development by fostering participation and upgrading in Global Value Chains.|\n|Inclusive Global Value Chains: Policy options in trade and complementary areas for GVC Integration by small and medium enterprises and low-income developing countries\nThis joint OECD and World Bank Group report focuses on the challenge of making GVCs more “inclusive” by overcoming participation constraints for Small and Medium Enterprises (SMEs) and facilitating access for Low Income Developing Countries (LIDCs).\nResults suggest that SME participation in GVCs is mostly taking place through indirect contribution to exports, rather than through exporting directly, and that a holistic approach to trade, investment and national and multilateral policy action is needed to create more inclusive GVCs.\nThe findings were presented to G20 Trade Ministers by OECD Secretary General Angel Gurria at the annual ministerial meeting that took place in Istanbul on 6 October 2015.","United Nations Conference on Trade and Development\nThe United Nations Conference on Trade and Development (UNCTAD) (French Conférence des Nations unies sur le Commerce et le Développement (CNUCED)) was established in 1964 as a permanent intergovernmental body.\nUNCTAD is the principal organ of the United Nations General Assembly dealing with trade, investment, and development issues. The organization's goals are to: \"maximize the trade, investment and development opportunities of developing countries and assist them in their efforts to integrate into the world economy on an equitable basis.\"\nThe primary objective of UNCTAD is to formulate policies relating to all aspects of development including trade, aid, transport, finance and technology. The conference ordinarily meets once in four years; the permanent secretariat is in Geneva.\nOne of the principal achievements of UNCTAD has been to conceive and implement the Generalised System of Preferences (GSP). It was argued in UNCTAD that to promote exports of manufactured goods from developing countries, it would be necessary to offer special tariff concessions to such exports. Accepting this argument, the developed countries formulated the GSP scheme under which manufacturers' exports and some agricultural goods from the developing countries enter duty-free or at reduced rates in the developed countries. Since imports of such items from other developed countries are subject to the normal rates of duties, imports of the same items from developing countries would enjoy a competitive advantage.\nThe creation of UNCTAD in 1964 was based on concerns of developing countries over the international market, multi-national corporations, and great disparity between developed nations and developing nations. The United Nations Conference on Trade and Development was established to provide a forum where the developing countries could discuss the problems relating to their economic development. The organisation grew from the view that existing institutions like GATT (now replaced by the World Trade Organization, WTO), the International Monetary Fund (IMF), and World Bank were not properly organized to handle the particular problems of developing countries. Later, in the 1970s and 1980s, UNCTAD was closely associated with the idea of a New International Economic Order (NIEO).\nThe first UNCTAD conference took place in Geneva in 1964, the second in New Delhi in 1968, the third in Santiago in 1972, fourth in Nairobi in 1976, the fifth in Manila in 1979, the sixth in Belgrade in 1983, the seventh in Geneva in 1987, the eighth in Cartagena in 1992, the ninth at Johannesburg (South Africa) in 1996, the tenth in Bangkok (Thailand) in 2000, the eleventh in São Paulo (Brazil) in 2004, the twelfth in Accra in 2008 and the thirteenth in Doha (Qatar) in 2012.\nCurrently, UNCTAD has 194 member states and is headquartered in Geneva, Switzerland. UNCTAD has 400 staff members and a bi-annual (2010–2011) regular budget of $138 million in core expenditures and $72 million in extra-budgetary technical assistance funds. It is a member of the United Nations Development Group. There are non-governmental organizations participating in the activities of UNCTAD.\nAs of October 2012, 194 states are UNCTAD members: all UN members and the Holy See. UNCTAD members are divided into four lists, the division being based on United Nations Regional Groups with six members unassigned: Armenia, Kiribati, Nauru, South Sudan, Tajikistan, Tuvalu. List A consists mostly of countries in the African and Asia-Pacific Groups of the UN. List B consists of countries of the Western European and Others Group. List C consists of countries of the Group of Latin American and Caribbean States (GRULAC). List D consists of countries of the Eastern European Group.\nThe lists, originally defined in 19th General Assembly resolution 1995 serve to balance geographical distribution of member states' representation on the Trade Development Board and other UNCTAD structures. The lists are similar to those of UNIDO, an UN specialized agency.\nThe full lists are as follows:\n- List A (100 members): Afghanistan, Algeria, Angola, Bahrain, Bangladesh, Benin, Bhutan, Bosnia and Herzegovina, Botswana, Brunei Darussalam, Burkina Faso, Burundi, Cambodia, Cameroon, Cape Verde, Central African Republic, Chad, China, Comoros, Côte d'Ivoire, Republic of Congo, Democratic Republic of Congo, Djibouti, Egypt, Equatorial Guinea, Eritrea, Ethiopia, Fiji, Gabon, Gambia, Ghana, Guinea, Guinea-Bissau, India, Indonesia, Iran, Iraq, Israel, Jordan, Kenya, Kuwait, Laos, Lebanon, Lesotho, Liberia, Libya, Madagascar, Malawi , Malaysia, Maldives, Mali, Marshall Islands, Mauritania, Mauritius, Micronesia, Mongolia, Morocco, Mozambique, Myanmar, Namibia, Nepal, Niger, Nigeria, North Korea, Oman, Pakistan, Palau, Papua New Guinea, Philippines, Qatar, South Korea, Rwanda, Samoa, Sao Tome and Principe, Saudi Arabia, Senegal, Seychelles, Sierra Leone, Singapore, Solomon Islands, Somalia, South Africa, Sri Lanka, Sudan, Swaziland, Syria, Thailand, Timor-Leste, Togo, Tonga, Tunisia, Turkmenistan, Uganda, United Arab Emirates, Tanzania, Vanuatu, Viet Nam, Yemen, Zambia, Zimbabwe.\n- List B (31 members): Andorra, Australia, Austria, Belgium, Canada, Cyprus, Denmark, Finland, France, Germany, Greece, Holy See, Iceland, Ireland, Italy, Japan, Liechtenstein, Luxembourg, Malta, Monaco, Netherlands, New Zealand, Norway, Portugal, San Marino, Spain, Sweden, Switzerland, Turkey, United Kingdom, United States.\n- List C (33 members): Antigua and Barbuda, Argentina, Bahamas, Barbados, Belize, Bolivia, Brazil, Chile, Colombia, Costa Rica, Cuba, Dominica, Dominican Republic, Ecuador, El Salvador, Grenada, Guatemala, Guyana, Haiti, Honduras, Jamaica, Mexico, Nicaragua, Panama, Paraguay, Peru, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Suriname, Trinidad and Tobago, Uruguay, Venezuela.\n- List D (24 members): Albania, Azerbaijan, Belarus, Bulgaria, Croatia, Czech Republic, Estonia, Georgia, Hungary, Kazakhstan, Kyrgyzstan, Latvia, Lithuania, Montenegro, Poland, Moldova, Romania, Russia, Serbia, Slovakia, Slovenia, Macedonia, Ukraine, Uzbekistan.\n- Not assigned countries (6 members): Armenia, Kiribati, Nauru, South Sudan, Tajikistan, Tuvalu.\nThe inter-governmental work is done at five levels of meetings:\n- The UNCTAD Conference – held every four years:\n- UNCTAD VIII in Cartagena, Colombia on 8–25 February 1992\n- UNCTAD IX in Midrand, South Africa on 27 April – 11 May 1996\n- UNCTAD X in Bangkok, Thailand on 12–19 February 2000\n- UNCTAD XI in São Paulo, Brazil on 13–18 June 2004\n- UNCTAD XII in Accra, Ghana on 21–25 April 2008\n- UNCTAD XIII in Doha, Qatar on 21–26 April 2012\n- The UNCTAD Trade and Development Board – the Board manages the work of UNCTAD between two conferences and meets up to three times every year;\n- Four UNCTAD Commissions and one Working Party – these meet more often than the Board to take up policy, programme and budgetary issues;\n- Expert Meetings – the commissions will convene expert meetings on selected topics to provide substantive and expert input for Commission policy discussions.\nIn response to developing country (Least Developed Country, LDC) anxiety at their worsening position in world trade, the United Nations General Assembly voted for a 'one off' conference. These early discussions paved the way for new IMF facilities to provide finance for shortfalls in commodity earnings and for the Generalised Preference Schemes which increased access to Northern markets for manufactured imports from the South. At Geneva, the LDCs were successful in their proposal for the conference with its Secretariat to become a permanent organ of the UN, with meetings every four years.\nNew Delhi, 1968\nThe New Delhi Conference, held in February and March 1968, was a forum that allowed developing countries to reach agreement on basic principles of their development policies. The conference in New Delhi was an opportunity for schemes to be finally approved. The conference provided a major impetus in persuading the North to follow up UNCTAD I resolutions, in establishing generalised preferences. The target for private and official flows to LDCs was raised to 1% of the North's GNP, but the developed countries failed to commit themselves to achieving the target by a specific date. This has proven a continuing point of debate at UNCTAD conferences.\nThe Santiago Conference, April 15, 1972, was the third occasion on which the developing countries have confronted the rich with the need to use trade and aid measures more effectively to improve living standards in the developing world. Discussion centred on the international monetary system and specifically on the South's proposal that a higher proportion of new special drawing rights (SDRs) should be allocated to LDCs as a form of aid (the so-called 'link'). In Santiago, substantial disagreements arose within the Group of 77 (G77) despite preconference meetings. There was disagreement over the SDR proposal and between those in the G77 who wanted fundamental changes such as a change in the voting allocations in the South's favour at the IMF and those (mainly the Latin American countries) who wanted much milder reforms. This internal dissent seriously weakened the group's negotiating position and led to a final agreed motion which recommended that the IMF should examine the link and that further research be conducted into general reforms. This avoided firm commitments to act on the 'link' or general reform, and the motion was passed by conference.\nNairobi, 1976 and Manila, 1979\nUNCTAD IV held in Nairobi May 1976, showed relative success compared to its predecessors. An Overseas Development Institute briefing paper of April 1979 highlights one reason for success as being down to the 1973 Oil Crisis and the encouragement of LDCs to make gains through producers of other commodities. The principal result of the conference was the adoption of the Integrated Programme for Commodities. The programme covered the principal commodity exports and its objectives aside from the stabilisation of commodity prices were: 'Just and remunerative pricing, taking into account world inflation', the expansion of processing, distribution and control of technology by LDCs and improved access to markets.\nUNCTAD V in the wake of the Nairobi Conference, held in Manila 1979 focused on the key issues of: protectionism in developing countries and the need for structural change, trade in commodities and manufactures aid and international monetary reform,technology, shipping, and economic co-operation among developing countries. An Overseas Development Institute briefing paper written in 1979 focuses its attention on the key issues regarding the LDCs` role as the Group of 77 in the international community.\nThe sixth UN conference on trade and development in Belgrade, 6–30 June 1983 was held against the background of earlier UNCTADs which have substantially failed to resolve many of the disagreements between the developed and developing countries and of a world economy in its worst recession since the early 1930s. The key issues of the time were finance and adjustment, commodity price stabilisation and trade.\nUNCTAD produces a number of topical reports, including:\n- The Trade and Development Report \n- The Trade and Environment Review \n- The World Investment Report \n- The Economic Development in Africa Report \n- The Least Developed Countries Report \n- UNCTAD Statistics \n- The Information Economy Report \n- The Review of Maritime Transport \n- The International Accounting and Reporting Issues Annual Review \n- The Technology and Innovation Report \nIn addition, UNCTAD conducts certain technical cooperation in collaboration with the World Trade Organization through the joint International Trade Centre (ITC), a technical cooperation agency targeting operational and enterprise-oriented aspects of trade development.\nUNCTAD is a founding member of the United Nations Sustainable Stock Exchanges (SSE) initiative along with the Principles for Responsible Investment (PRI), the United Nations Environment Programme Finance Initiative (UNEP-FI), and the UN Global Compact.\nList of Secretaries-General and Officers-in-Charge\n1 September 2013.\n- International trade\n- Foreign direct investment\n- List of countries by received FDI\n- Global System of Trade Preferences among Developing Countries (GSTP)\n- United Nations Guidelines for Consumer Protection\n- from official website\n- Membership of UNCTAD and membership of the Trade and Development Board\n- \"UNCTAD VI: background and issues\". ODI Briefing Paper. Overseas Development Institute. Retrieved 19 July 2011.\n- \"The UN Conference on Trade and Development\". ODI Briefing Paper 1. Overseas Development Institute. Retrieved 27 June 2011.\n- \"ODI Briefing Paper\". UNCTAD III, problems and prospects. Overseas Development Institute. Retrieved 27 June 2011.\n- \"UNCTAD 5: A preview of the issues\". ODI briefing paper No.2 1979. Overseas Development Institute. Retrieved 28 June 2011.\n- \"UNCTAd VI: background and issues\". ODI Briefing Paper. Overseas Development Institute. Retrieved 19 July 2011.\n- \"UNCTAD: A preview of the issues\". ODI briefing paper 1979. Overseas Development Institute. Retrieved 28 June 2011.\n- Paul Berthoud, A Professional Life Narrative, 2008, worked with UNCTAD and offers testimony from the inside.\n|Number||Secretary-General||Dates in office||Country of origin||Remarks|\n|3||Gamani Corea||1974–1984||Sri Lanka|\n|5||Kenneth K.S. Dadzie||1986–1994||Ghana|\n|8||Carlos Fortin||2004–2005||Chile||Officer-in-Charge||9||Supachai Panitchpakdi||1 September 2005 – 30 August 2013||Thailand|\n|10||Mukhisa Kituyi||1 September 2013 – Present||Kenya|\n|Wikimedia Commons has media related to United Nations Conference on Trade and Development.|\n- United Nations Conference on Trade and Development\n- UNCTAD member states\n- Research Guide about UNCTAD (UN Library at Geneva)\n- International Trade Centre\n- UNCTAD: Time to Lead (Focus on the Global South)\n- Global Policy Forum – UNCTAD\n- International Trade Debates\n- ODI Briefing Papers on the UNCTAD"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:b4c57ad2-ada3-4ecf-b7aa-3488ec7de2fd>","<urn:uuid:d71f517d-d76a-4e1e-b046-c59530e55487>"],"error":null}
{"question":"Could you please compare how anorexia nervosa affects women's physical bodies according to both medical and social perspectives? What are the comprehensive impacts on women's health and their relationship with food?","answer":"Anorexia nervosa has severe physical impacts on women's bodies while also deeply affecting their relationship with food. Physically, women with anorexia experience cold sensitivity, frequent illness, hair loss, skin dryness, temperature deregulation, brittle nails, sleeplessness, and development of soft baby-like hair called lanuga. They may become malnourished and very thin, potentially reaching skeletal appearance, and their menstruation stops. Beyond physical symptoms, there's a complex psychological relationship with food where women feel constant hunger but view this as a form of control over their lives. This condition exists within a broader social context where it's become norm for women to feel dissatisfied with their bodies and worry about eating. The condition can be life-threatening, with anorexia having the highest mortality rate among eating disorders, particularly due to risks of starvation, electrolyte imbalance, or unusually low heart rate (bradycardia).","context":["An eating disorder is an obsession with food and weight that harms a person's well-being. Although we all worry about our weight sometimes, people who have an eating disorder go to extremes to keep from gaining weight. There are 2 main eating disorders: anorexia nervosa and bulimia.\nAnorexia nervosa is an illness that usually occurs in teenage girls, but it also can occur in teenage boys, and in adult women and men. People who have anorexia are obsessed with being thin. They don't want to eat, and they are afraid of gaining weight. They may constantly worry about how many calories they take in or how much fat is in their food. They may take diet pills, laxatives or water pills to lose weight. They may exercise too much. People who have anorexia usually think they're fat even though they're very thin. They may get so thin that they look like they're sick. Anorexia isn't just a problem with food or weight. It's an attempt to use food and weight to deal with emotional problems.\nBulimia is eating a lot of food at once (called bingeing), and then throwing up or using laxatives to remove the food from the body (called purging). After a binge, some bulimics fast (don't eat) or overexercise to keep from gaining weight. People who have bulimia may also use water pills, laxatives or diet pills to \"control\" their weight. People who have bulimia often try to hide their bingeing and purging. They may hide food for binges. People who have bulimia are usually close to normal weight, but their weight may go up and down.\nSource: National Association of Anorexia\nThe following are possible warning signs of anorexia and bulimia:\nMore serious warning signs may be harder to notice because people who have an eating disorder try to keep it secret. Watch for these signs:\nDoctor don't know exactly. Possible causes include feeling stressed out or upset about something in your life, or feeling like you need to be \"in control.\" Society also puts a lot of pressure on people to be thin. This pressure can contribute too.\nThe reason some people get anorexia isn't known. People who have anorexia may believe they would be happier and more successful if they were thin. They want everything in their lives to be perfect. People who have this disorder are usually good students. They are involved in many school and community activities. They blame themselves if they don't get perfect grades, or if other things in life are not perfect.\nYes. For people who have anorexia, the first step is getting back to a normal weight. If you're malnourished or very thin, you may be put in the hospital. Your doctor will probably want you to see a dietitian to learn how to pick healthy foods and eat at regular times. For both people who have anorexia and bulimia, family and individual counseling (talking about your feelings about your weight and problems in your life) is helpful.\nTreatment of anorexia is difficult, because people who have anorexia believe there is nothing wrong with them. Patients in the early stages of anorexia (less than 6 months or with just a small amount of weight loss) may be successfully treated without having to be admitted to the hospital. But for successful treatment, patients must want to change and must have family and friends to help them.\nPeople who have more severe anorexia need care in the hospital, usually in a special unit for people who have anorexia and bulimia. Treatment involves more than changing the person's eating habits. Anorexic patients often need counseling for a year or more so they can work on changing the feelings that are causing their eating problems. These feelings may be about their weight, family problems or problems with self-esteem. Some anorexic patients are helped by taking medicine that makes them feel less depressed. These medicines are prescribed by a doctor and are used along with counseling.\nThe most important thing that family and friends can do to help a person who has anorexia is to love them. People who have anorexia feel safe, secure and comfortable with their illness. Their biggest fear is gaining weight, and gaining weight is seen as loss of control. They may deny they have a problem. People who have anorexia will beg and lie to avoid eating and gaining weight, which is like giving up the illness. Family and friends should not give in to the pleading of the anorexic patient.\nIt's healthy to watch what you eat and to exercise. What isn't healthy is worrying all the time about your weight and what you eat. People who have eating disorders do harmful things to their bodies because of their obsession about their weight. If it isn't treated, anorexia can cause the following health problems:\nIf it isn't treated, bulimia can cause the following health problems:\nGirls and women who have anorexia may feel cold all the time, and they may get sick often. People who have anorexia are often in a bad mood. They have a hard time concentrating and are always thinking about food. It is not true that anorexics are never hungry. Actually, they are always hungry. Feeling hunger gives them a feeling of control over their lives and their bodies. It makes them feel like they are good at something--they are good at losing weight. People who have severe anorexia may be at risk of death from starvation.\nWritten by familydoctor.org editorial staff","Women, Food, and Eating Disorders\nMaking Peace with Food\nWomen have related intimately with food since time began, as feeders and nurturers, harvesters, gatherers, and cooks. But in recent decades, this relationship has grown troubled. It can be said, in fact, that very few women today feel completely comfortable with food, eating, and the bodies their diets should nourish. Research has confirmed what any of us could have guessed – it actually is the norm in this country for women to be dissatisfied with their bodies, to worry about how much they eat, and to believe they should be dieting. What does this mean, and can we change it?\nThinking in the worst possible terms, this mindset implies that eating disorders, some of which are life-threatening and most of which are soul-torturing, are here to stay. Although the modern quest for thinness does not, in and of itself, automatically lead to eating disorders, dieting does precede most eating disorders. Consequently, this could also mean that the diet industry will continue to thrive while women who are not skinny will continue to feel depressed or inadequate.\nThinking a little more optimistically, we could anticipate an increasing awareness of the dangers posed by our diet-obsessed culture. More people could be alerted to the roots and results of ongoing body dissatisfaction and frequent dieting. In fact, such things are beginning to occur. Many individual women, however, continue to feel drained of at least some self-esteem and creative energy as a result of remaining fixed on the elusive goals of a perfect body and perfectly-regulated (never gluttonous) eating.\nUnderstanding eating disorders as well as more \"normal\" kinds of unhappiness with eating and the body challenges us. These are complex matters that touch on our emotions, our physiology, our family histories, and our social and political context. This article lays a groundwork that will serve to help us achieve this understanding – and start, I hope, to help us make peace with food, our natural hungers, and the amazing bodies we are fortunate to possess.\nI do not mean to exclude men from these discussions. I do, however, address these words to women directly, as women have much higher rates of eating disorders, as well as lesser forms of body dissatisfaction. Many men do suffer from similar ailments, though, and all are certainly invited to read, talk back in future chat rooms, and to ask their questions.\nDefining Eating Disorders\nPeople often wonder, when does \"normal\" dieting, or \"normal\" overeating, stop being normal and cross the line into an eating disorder? It is important to recognize that many, many people suffer from conflicted relationships with their eating. However, there are degrees of suffering and degrees of danger to health, with clinically diagnosable eating disorders inflicting the most of each. Eating disorders assume a few different forms.\nAnorexia nervosa is a condition in which a person literally starves the body of the nutrients it needs. People with anorexia often claim they are not hungry, strive to eat very little (even to the point of counting out flakes of cereal or individual grapes), and have an exaggerated, irrational fear of becoming fat. The fear of fat exists despite actual body size; in fact, the person afflicted may be very skinny or even skeletal. To be diagnosed with anorexia, one must be 15% below normal weight.\nCommon behaviors include denial of how serious the condition is, secretiveness about how much has been eaten, the wearing of baggy clothes to hide thinness, avoidance of social events where food will be present, and obsessions with cooking or feeding food to others. In women, menstruation stops. Physical symptoms can include hair loss, skin dryness, temperature deregulation (feeling cold all the time), brittle nails, sleeplessness, hyperactivity, the development of obsessions, and the development of soft, ba by -like hair on the body called \"lanuga.\" Some people who self-starve will occasionally binge eat and then get rid of the \"damage\" by purging or overexercising. People who are underweight and undereating to the point of anorexia also distort information and perception (as part of the disorder, not necessarily on purpose), so that no amount of \"talking sense\" – listing health dangers, noting the person’s boniness – seems to make a difference.\nBulimia nervosa refers to the condition in which large quantities of food are consumed in a way that feels out-of-control and is not normal for the situation (for instance, eating a lot at Thanksgiving is not necessarily binging). The food binge can consist of thousands of calories, most often carbohydrates and fats. The person ingesting all this food then tries to get rid of it by vomiting, overexercising, taking laxatives, or some other means. A person with bulimia can be normal, below normal, or overweight. Menstruation does not necessarily stop, although it can.\nEating is usually done in isolation, and the individual often feels very ashamed and out-of-control with this behavior. Like an addictive substance, however, the food binge is often looked forward to and protected by the person as a source of short-term relief or good feelings. People with bulimia usually fear getting fat, as in anorexia. They can develop dental problems, throat irritations, swelling around the base of the jaw, lesions in the esophagus, gastrointestinal problems, and heart problems (including heart emergencies) from electrolyte imbalance or the use of Ipecac to induce vomiting.\nBinge eating disorder involves eating in quantities similar to bulimia, but the purging afterward does not occur. People with binge eating disorder are more likely to be overweight than those with bulimia, but are not always so. Health problems are usually fewer than those found in the other eating disorders, although individuals can be at risk for those conditions associated with high calorie and fat intake generally.\nLess common forms of clinical eating disorder involve variations on the themes already discussed. For example, some people purge what they eat even if it wasn’t a binge or large amount of food. Some people develop the behaviors and thinking of the anorexic, but may be overweight or may not have stopped menstruating.\nWhile all of the eating disorders carry health risks, anorexia has the highest mortality rate and the highest risk of sudden death (from electrolyte imbalance or bradycardia, an unusually low heart rate). Anorexia is less common than bulimia and most often afflicts women beginning at age 13 through the early 20s. People usually develop bulimia somewhat later, around age 15 or 16 through the early 30s. Men, as well as women who are older or younger than these ages, can also develop these syndromes.\nI hope this article will help people to begin thinking about their own relationships with food and how they might like to change them. Your questions and comments are, of course, always welcomed.Date published: 3/14/00 6:09:45 PM\nLast reviewed: By John M. Grohol, Psy.D. on 30 Apr 2016\nPublished on PsychCentral.com. All rights reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:9c70f9f2-1884-4f90-8e71-32116f2722e3>","<urn:uuid:5c0dde98-15c1-4e09-875d-e93bc0c2e6b4>"],"error":null}
{"question":"When comparing historical events on February 21st, which happened first: Fangio's kidnapping in Cuba or Malcolm X's assassination?","answer":"Juan Manuel Fangio's kidnapping by Communist guerrillas in Havana, Cuba occurred on February 21, 1958, while Malcolm X was assassinated at the Audubon Ballroom in New York City on February 21, 1965. Therefore, Fangio's kidnapping happened 7 years earlier.","context":["Learning Opportunities! Today in History: February 21st\nare a list of events that took place today in history.\nHave you studied or will you be studying any of these events?\nShare with us!\n- 362 – Athanasius returns to Alexandria.\n- 1245 – Thomas, the first known Bishop of Finland, is granted resignation after confessing to torture and forgery.\n- 1440 – The Prussian Confederation is formed.\n- 1543 – Battle of Wayna Daga – A combined army of Ethiopian and Portuguese troops defeats a Muslim army led by Ahmed Gragn.\n- 1613 – Mikhail I is elected unanimously as Tsar by a national assembly, beginning the Romanov dynasty of Imperial Russia.\n- 1804 – The first self-propelling steam locomotive makes its outing at the Pen-y-Darren Ironworks in Wales.\n- 1808 – Without a previous declaration of war, Russian troops cross the border to Sweden at Abborfors in eastern Finland, thus beginning the Finnish war, in which Sweden will lose the eastern half of the country (i.e. Finland) to Russia.\n- 1842 – John Greenough is granted the first U.S. patent for the sewing machine.\n- 1848 – Karl Marx and Friedrich Engels publish The Communist Manifesto.\n- 1862 – American Civil War: Battle of Valverde is fought near Fort Craig in New Mexico Territory.\n- 1874 – The Oakland Daily Tribune publishes its first edition.\n- 1878 – The first telephone book is issued in New Haven, Connecticut.\n- 1885 – The newly completed Washington Monument is dedicated.\n- 1913 – Ioannina is incorporated into the Greek state after the Balkan Wars.\n- 1916 – World War I: In France, the Battle of Verdun begins.\n- 1918 – The last Carolina Parakeet dies in captivity at the Cincinnati Zoo.\n- 1919 – Kurt Eisner, German socialist, is assassinated. His death results in the establishment of the Bavarian Soviet Republic and parliament and government fleeing Munich, Germany.\n- 1921 – Constituent Assembly of the Democratic Republic of Georgia adopts the country's first constitution.\n- 1921 – Rezā Shāh takes control of Tehran during a successful coup\n- 1925 – The New Yorker publishes its first issue.\n- 1937 – Initial flight of the first successful flying car, Waldo Waterman's Arrowbile.\n- 1937 – The League of Nations bans foreign national \"volunteers\" in the Spanish Civil War.\n- 1945 – World War II: Japanese Kamikaze planes sink the escort carrier Bismarck Sea and damage the Saratoga.\n- 1947 – In New York City, Edwin Land demonstrates the first \"instant camera\", the Polaroid Land Camera, to a meeting of the Optical Society of America.\n- 1948 – NASCAR is incorporated.\n- 1952 – The British government, under Winston Churchill, abolishes identity cards in the UK to \"set the people free\".\n- 1952 – The Bengali Language Movement protests occur at the University of Dhaka in East Pakistan (now Bangladesh).\n- 1958 – The Peace symbol, commissioned by Campaign for Nuclear Disarmament in protest against the Atomic Weapons Research Establishment, is designed and completed by Gerald Holtom.\n- 1965 – Malcolm X is assassinated at the Audubon Ballroom in New York City by members of the Nation of Islam.\n- 1970 – Swissair Flight 330: A mid-air bomb explosion and subsequent crash kills 38 passengers and nine crew members near Zürich, Switzerland.\n- 1971 – The Convention on Psychotropic Substances is signed at Vienna.\n- 1972 – President Richard Nixon visits the People's Republic of China to normalize Sino-American relations.\n- 1972 – The Soviet unmanned spaceship Luna 20 lands on the Moon.\n- 1973 – Over the Sinai Desert, Israeli fighter aircraft shoot down Libyan Arab Airlines Flight 114 jet killing 108.\n- 1974 – The last Israeli soldiers leave the west bank of the Suez Canal pursuant to a truce with Egypt.\n- 1975 – Watergate scandal: Former United States Attorney General John N. Mitchell and former White House aides H. R. Haldeman and John Ehrlichman are sentenced to prison.\n- 1995 – Steve Fossett lands in Leader, Saskatchewan, Canada becoming the first person to make a solo flight across the Pacific Ocean in a balloon.","Discover the momentous motor sports events that took place this weekend in history……..\n1926: Pete DePaolo won the inaugural AAA sanctioned race with a $30,000 purse was run on the 1.25 mile board Miami-Fulford Speedway, in front of 20,000 spectators. DePaolo’s Duesenberg averaged 129.29 mph in the 300 mile AAA Championship race on the track which had 50 degree banking. Tommy Milton turned a lap of 142.93 mph in qualifying. The track was designed by Ray Harroun, winner of the first Indy 500, and financed by Carl Fisher, who was involved in building the Indianapolis Motor Speedway. It would be the only race on the speedway as it was destroyed by a hurricane on the 17th September 1926, the wood and material being used to rebuild the town of Miami Beach.\n1959: Daytona International Speedway, the “World Center of Racing,” hosted the first Daytona 500. A field of 59 cars took the green flag for the start of the 500 mile race in front of a crowd of over 41,000. There were no caution periods in the race; making it one of the few “perfect games” in NASCAR history, though it would occur in three of the first four Daytona 500s, as the Daytona 500 also went caution-free in both 1961 and 1962. This would be repeated ten years later with the 1969 running of the Motor Trend 500. Welborn led the early laps in the race but his race ended after 75 laps (of 200) with engine problems. Other leaders in the first 22 laps of the race were “Tiger” Tom Pistone and Joe Weatherly. Fireball Roberts took over the lead on lap 23, leading the next 20 laps before dropping out on lap 57 due to a broken fuel pump. When Roberts went to the pits on lap 43, Johnny Beauchamp, running in second place, became the leader. On lap 50,Piston took over first place and Jack Smith moved into second; Beauchamp was third and Lee Petty was fifth. From lap 43 to 148 the race leaders were Piston, Smith, and Beauchamp. Although Smith and Pistone led most of these laps, Beauchamp led a few times, for example records show he led on lap 110. There is print information about the details of the race, including the leaders of the race in five lap intervals. Pistone and Jack Smith both had dropped out of contention by lap 149 and Beauchamp took over first place. 100 miles (160 km). Richard Petty also had to retire from the race with an engine problem and earned $100 ($821.58 when adjusted for inflation) for his 57th-place performance. Lee Petty battled with Beauchamp during the final 30 laps of the race, and they were the only two drivers to finish on the lead lap. Petty took the lead with 3 laps left, and led at the start of the final lap. Petty and Beauchamp drove side by side across the finish line at the end final lap for a photo finish. Beauchamp was declared the unofficial winner by NASCAR officials, and he drove to victory lane. Petty protested the results, saying “I had Beauchamp by a good two feet. In my own mind, I know I won.” Beauchamp replied “I had him by two feet. I glanced over to Lee Petty’s car as I crossed the finish line and I could see his headlight slightly back of my car. It was so close I didn’t know how they would call it, but I thought I won.” Early leader Fireball Roberts, who was standing by the finish line, said “There’s no doubt about it, Petty won.” It took NASCAR founder Bill France, Sr. three days to decide the winner the following Wednesday. In the end, with the help of photographs and newsreel footage, Petty was officially declared the winner. The official margin of victory was two feet. The controversial finish helped the sport. The delayed results to determine the official winner kept NASCAR and the Daytona 500 on the front page of newspapers.\n1963: Junior Johnson and Johnny Rutherford won the 100 mile NASCAR GN qualifying races at Daytona International Speedway. The relatively unknown Rutherford had run a record 165.183 mph in the Smokey Yunick Chevy and beat Rex White’s Chevy by 3 car lengths in winning race 2, his first GN start.\n1969: Racer Don MacTavish (36) died when his Mercury Comet crashed during a race at Daytona Beach, Florida.\n1970: Pete Hamilton driving a Plymouth Superbird won the 1970 NASCAR Grand National Series Daytona 500 in front of 103,800 spectators at the Daytona International Speedway. Hamilton passed David Pearson with nine laps to go and won by three car lengths. A grand total of 24 lead changes were made with an average green flag run of 22 laps. Exactly 23% of the race was held under a caution flag; blown engines were the primary culprit behind the yellow flags. The race car drivers had to commute to the races using the same stock cars that competed in a typical weekend’s race through a policy of homologation (and under their own power). This policy was in effect until 1975.\n1998: Chuck Etchells defeated Ron Capps in the Funnycar finals of the Atsco Nationals at Firebird Raceway in Arizona, US – the first all-Camaro Funnycar finals in NHRA history.\n1998: Chuck Etchells defeated Ron Capps in the Funnycar finals at the Atsco Nationals at Firebird Raceway in Chandler, Arizona, USA; the first all-Camaro Funnycar finals in NHRA history.\n2001: Jaguar boss Bobby Rahal told his driver Eddie Irvine to curb his criticism of the team ahead of the new season. Irvine had openly slammed the car’s lack of pace at a test a couple of weeks earlier and Rahal was keen to put a lid on his brutally honest driver. “I think he was trying to tweak us a bit so we know what we ought to be doing for him,” said Rahal. “I don’t know if the criticisms are Eddie’s way of motivating the team. I’ve seen other drivers do that, but I’ve never felt it was a very positive way of motivating people.”\n2001: Ferrari signed a new contract with cigarette company Philip Morris to carry Marlboro sponsorship until 2006. Rumours that the EU-imposed ban on tobacco sponsorship would put Philip Morris off extending its deal were proved unfounded and the branding remained on the car. The two companies then raised even more eyebrows by agreeing on another deal that would extend the sponsorship to 2011. Rumoured to be worth US$1 billion over seven years, the ban meant that from 2006 the car didn’t even carry the Marlboro brand but a series of stripes instead.\n1947: The Stockholm Grand Prix at Vallentuna was won by Reg Parnell in an ERA A-Type.\n1953: Ernesto Ceirano (79) of Fiat, winner of the 1911 and 1914 Targa Florio, died in Turin, Italy.\n1958: Communist guerrillas in Havana, Cuba, one day before the second Havana Grand Prix, kidnapped Argentine racing champion Juan Manuel Fangio. Revolutionary Manuel Uziel, holding a revolver, approached Fangio in the lobby of his hotel and ordered the race-car driver to identify himself. Fangio reportedly thought it was a joke until Uziel was joined by a group of men carrying submachine guns. Fangio reacted calmly as the kidnappers explained to him their intention to keep him only until the race was over. After his release to the Argentine Embassy, Fangio revealed a fondness for his kidnappers, refusing to help identify them and relaying their explanation that the kidnapping was a political statement.\n1958: Paul Goldsmith drove Smokey Yunick’s Pontiac to victory in the 160-mile NASCAR Grand National race on Daytona’s Beach-Road course. The event was the final NASCAR race staged on the picturesque 4.1-mile course on the shore.\n1964: Driving a potent Plymouth with the new Hemi engine, Richard Petty led 184 of the 200 laps to win the 1964 Daytona 500. Plymouths ran 1-2-3 at the finish. The triumph was Petty’s first on a super-speedway. Petty drove his number 43 to victory in 3 hours and 14 minutes. There were three caution flags that slowed the race for 19 laps. The Chrysler teams debuted their brand-new 426 ci Chrysler Hemi engine in this race; NASCAR ordered the teams who had it to sandbag it during practice and qualifying due to their superiority. During the race itself, Richard Petty, who at the time was known best for his skill on short tracks, led 184 of the 200 laps (a Daytona 500 record that stands to this day) and Chrysler teams took four of the top five spots. The transition to purpose-built racecars began in the early 1960s and occurred gradually over that decade. Changes made to the sport by the late 1960s brought an end to the “strictly stock” vehicles of the 1950s; most of the cars were trailered to events or hauled in by trucks. Bobby Marshman would retire from NASCAR Grand National Series racing after the conclusion of this event. For some drivers, this would be their last Daytona 500, as the 1960s were an especially brutal era for NASCAR. Jimmy Pardue was killed later in the year in a test crash. Billy Wade was killed in a tire test in January 1965. Bobby Marshman killed in a test crash in late 1964 at Phoenix. Fireball Roberts died in July from injuries inflicted while racing in the World 600 and Joe Weatherly was killed at Riverside early that year.\n1969: Lee Roy Yarbrough chased down Charlie Glotzbach who had an eleven second lead to win the Daytona 500. Yarbrough passing Glotzbach on the final lap. It was the first Daytona 500 that was won on a last lap pass.\n1969: The Stardust International Raceway, Las Vegas, Nevada held it last race. It featured a flat, 3-mile (4.8 km), 13-turn road course, and a quarter-mile drag strip. It was built in 1965 by the Stardust Hotel and Casino to attract high rollers to the hotel. In 1966 it began hosting the season finale of the Can-Am championship and two years later staged a race in the USAC Championship Car series. The hotel was sold in 1969, and the new owners largely abandoned the track. Real estate developers Pardee Homes bought the land and built the Spring Valley community on it.\n1975: Richard Petty drove his Dodge to a convincing win in the ‘Richmond 500’ NASCAR GN race. Lennie Pond’s Chevy finished second, 6 laps behind Petty. Point leader Bobby Allison, Buddy Baker and Cale Yarborough all skipped the race.\n1983: The Peugeot 205 Turbo 16 4WD rally car was launched on the same day as its two-wheel-drive road version.\n1986: At the Miller High Life 400 rivals Darrell Waltrip and Dale Earnhardt battled for the win on the half-mile short track for the better part of the race. In the final five laps, Waltrip rode on the back bumper of Earnhardt, bumping and rubbing the whole way. Waltrip finally snuck underneath exiting turn two with three laps to go. Going into turn 3, Earnhardt spun Waltrip out, but lost control himself and both cars crashed hard. The wreck collected Joe Ruttman (3rd place) and Geoff Bodine (4th place), allowing 5th place Kyle Petty to slip by and take his first-career Cup victory. The incident drew a fine for Earnhardt, raised tempers throughout the garage area, and gave Earnhardt the “Ironhead” nickname. The incident was dramatized in the movie 3.\n1992: The Jaguar XJR-14 (cove image) made its race debut in the IMSA Camel GT race at Miami, Florida, USA. Davy Jones started it on pole but finished sixth behind the winning Nissan NPTI-91 of Geoff Brabham.\n2008: Kyle Busch won the NASCAR Craftsman Truck race at the Auto Club Speedway , Fontana, California, US."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:73a3c9fa-0cf5-473e-8df3-93a65fd8e3c6>","<urn:uuid:86d59707-92c3-4451-a860-7185a613e732>"],"error":null}
{"question":"Could you please compare the temporal resolution capabilities between the magnetic sense in pigeons and functional MRI (fMRI) technology? I'm particularly interested in understanding which one can detect changes more quickly.","answer":"The magnetic sense in pigeons can detect changes in magnetic fields in real-time through electromagnetic induction in the semicircular canals, while fMRI has a relatively low temporal resolution of hundreds of milliseconds to seconds. Additionally, fMRI's BOLD signal peaks up to 6 seconds after neuronal activity, and the hemodynamics act like a low pass filter, which further smears out changes in local electrical activity.","context":["Using maps, compasses, and sextants, mariners in the early 1500’s developed the first methods to navigate the open sea; heralding an age of exploration as humanity set sail for thehorizon. Yet long before this time evolution had equipped life on the planet with a biological global positioning system that was far superior to those early navigational tools – the Magnetic Sense. While there is unequivocal behavioural evidence demonstrating that this faculty exists, it is the least understood of all senses. The location of the primary sensors, the underlying biophysical mechanisms, and the neurological basis of the sense are unknown. Currently, there are three ideas that aim to explain how magnetosensation might work: (1) magnetite based magnetoreception; (2) a light sensitive radical pair based model; and (3) electromagnetic induction (See Figure).\nOur goal is to identify the molecules, cells and circuits that underlie the magnetic sense in pigeons. To achieve this objective we employ an assay that assesses neuronal activation within the pigeon brain, following exposure to magnetic fields generated by Helmholtz coils. These experiments have confirmed that magnetic stimuli results in the activation of neurons in the vestibular nuclei, implicating the inner ear of pigeons in the magnetic sense. We have shown experimentally and by physical calculations that magnetic stimulation can induce electric fields in the pigeon semicircular canals that are within the physiological range of known electroreceptive systems. This in turn led to the discovery of a splice isoform of a voltage-gated calcium channel (CaV1.3) in the pigeon inner ear that has been shown to mediate electroreception in skates and sharks (Nimpf et al, Current Biology 2019). These data have led us to propose that pigeons detect magnetic fields by electromagnetic induction within the semicircular canals that is dependent on the presence of apically located voltage-gated cation channels in a population of electrosensory hair cells.\nAre hair cells the primary magnetosensors? Having established a physiologically relevant readout for the magnetic sense, we can now ask if hair cells are the primary sensors. To do so we are employing anatomical perturbations (e.g. hair cell ablation with antibiotics), which we expect will be superseded by genetic methods.\nWhat neuronal circuits process magnetic information? To gain further insight into the underlying circuitry that processes magnetic information we have established an iDISCO clearing protocol for the pigeon brain that results in a translucent brain that can be stained with markers such as c-fos followed by light sheet microscopy (Fig. 2). We are employing this technology to identify the circuits that process magnetic information and to interrogate the underlying biophysical mechanisms.\nHow is magnetic information encoded in the avian brain? To address this question brain we have built a 2-photon microscope that permits in vivo calcium imaging while exposing pigeons to precise magnetic stimuli. Coupled with a genetically encoded calcium indicator (e.g GCaMP6) delivered by an adeno associated virus we are able to study which components of the magnetic field elicit neuronal activity (i.e. intensity, polarity and inclination) and how this information is integrated into existing neuronal networks. Are there magnetic place cells?\nFigure 2: Image showing a whole pigeon brain prior to the iDISCO clearing protocol (left), after bleaching (middle), and following the removal of lipids (right). Coupled with immunostaining and light sheet microscopy this method allows us to globally assess neuronal activation in the pigeon brain.\nCollaborators: Dr Jeremy Shaw (University of Western Australia), Dr Matthew Mason (University of Cambridge), Dr Michael Eisterer (Technical University Vienna).\nFunded by: ERC Starting Grant – The Cellular and Molecular Basis of Magnetoreception, ERC Consolidator Grant – The Neurological Basis of the Magnetic sense.","Functional imaging is the study of human brain function based on analysis of data acquired using brain imaging modalities such as Electroencephalography (EEG), Magnetoencephalography (MEG), functional Magnetic Resonance Imaging (fMRI), Positron Emission Tomography (PET) or Optical Imaging. The aim is to understand how the brain works, in terms of its physiology, functional architecture and dynamics. The framework for the conduct of these studies includes classical techniques of neuroanatomy, neurophysiology, and experimental psychology and the cognitive neurosciences, as well as more theoretical approaches, based on perspectives from computational neuroscience and statistics.\nModern functional imaging has two main advantages over the multi/single-unit recordings used to study the electrophysiology of neurons. The first is that it is generally non-invasive, and is therefore applicable routinely in humans. This allows for the study of unique human attributes such as language. The second is that it can provide a wide field of view. Rather than recording information about a single or small number of neuronal cells, an image may be gathered summarizing simultaneous activity across the whole brain. This provides a different yet complementary perspective on neural coding (see e.g., functional integration, below). A disadvantage, however, is that functional imaging provides only an indirect measure of the quantities of primary interest to neuroscientists e.g., firing rates and membrane potentials. Current research is aimed at bridging this gap using a combination of experimental and mathematical modelling approaches.\nCurrent imaging modalities include the Electroencephalogram (EEG) which records electrical voltages from electrodes placed on the scalp and the Magnetoencephalogram (MEG) which records the magnetic field from SQUID sensors placed above the head. Both MEG and EEG have a high temporal resolution (milliseconds), capable of detecting e.g., the 40Hz Gamma response implicated in object representation (Tallon-Baudry and Bertrand, 1999). Their spatial resolution is, however, usually of the order of centimeters rather than millimeters. This varies a great deal, depending on the nature of the neuronal activity one is trying to localize. It depends in particular on the number of sources that is activated at the time data is recorded. In practice this implies that e.g. for isolating subtle cognitive components, a lower resolution is to be expected, whilst the stronger early components of an auditory response can be localized to within millimeters in the brainstem.\nIn contrast, functional Magnetic Resonance Imaging (fMRI) has low temporal (hundreds of milliseconds or seconds) but relatively high spatial (millimeters) resolution. Increases in neural activity cause variations in blood oxygenation, which in turn cause magnetization changes that can be detected in an MRI scanner. This Blood Oxygenation Level Dependent (BOLD) signal peaks up to 6s after neuronal activity. Moreover, the hemodynamics act like a low pass filter (Logothetis, 2001), smearing out changes in local electrical activity.\nSimultaneous recordings of EEG and fMRI (Ritter and Villringer, 2006) have the potential to localise neuronal activity with both high temporal and spatial resolution. Other important imaging modalities are PET and Optical Imaging. PET's spatial resolution typically falls somewhere between that of fMRI and MEG/EEG. In addition, PET has very low temporal resolution (tens of seconds to minutes) and requires injection of a trace amount of radioactivity. This limits the number of measurements that can be made on any one individual. But a great advantage of PET is that it is particularly useful in the study of brain neurophysiology and neurochemistry e.g., one can image glucose uptake and the activity at serotonin and dopamine receptors, in systems of importance to those studying anxiety, depression and addiction. Optical Imaging  or Near Infrared Spectroscopy (NIRs) can also detect BOLD signals from changes in the amount of reflected light. This is an economical alternative to fMRI but is limited to imaging the cortex.\nFunctional imaging is also closely related to structural imaging, in which MRI is used to provide high resolution images with high contrast between e.g., white matter and gray matter. These detailed anatomical images have recently been complemented with data from Diffusion Tensor Imaging (DTI) which can show the direction of white matter fibres.\nThere are two key themes in the analysis of functional imaging data. They reflect the long-standing debate in neuroscience about functional specialization versus functional integration in the brain (Cohen and Tong, 2001). The first is brain `mapping’ where three-dimensional images of neuronal activation are produced showing which parts of the brain respond to a given cognitive or sensory challenge. This is also known as the study of functional specialization and generally proceeds using some form of Statistical Parametric Mapping (SPM). A classic example here is the identification of human V4 and V5, the areas specialized for the processing of color and motion.\nSPM is a voxel-based approach, employing classical statistics and topological inference, to make comments about regionally specific responses to experimental factors. PET or fMRI data are first spatially processed so that they conform to a known anatomical space, in which responses are characterized statistically typically using the General Linear Model (GLM). For fMRI data the GLM embodies a convolution model of the hemodynamic response. This accounts for the fact that BOLD signals are a delayed and dispersed version of the neuronal response. GLMs are fitted at each voxel and inferences are made about which parts of the brain are active, in a statistical sense. To accommodate the spatial nature of the imaging data (and account for the multiple statistical comparisons made) SPM techniques make use of Random Field Theory (RFT) (see Fig 1) and/or other statistical procedures, e.g., False Discovery Rate.\nThe SPM approach can also be used with structural data to find brain regions containing a higher gray matter density. This is known as Voxel-Based Morphometry (VBM) (Ashburner and Friston, 2000) and has been used, for example, to show that the posterior hippocampus, useful for spatial navigation, is enlarged in taxi drivers.\nFor MEG or EEG, data can be analyzed in sensor space, furnishing a crude spatial mapping of brain function. Functions can, however, be more accurately localized using source reconstruction methods (Baillet et al. 2001). These work by specifying a forward model describing how a current source in the brain propagates to become an MEG or EEG measurement, using Maxwell's equations (http://www.scholarpedia.org/article/Volume_Conduction). These models are then inverted using statistical inference. Data from sensory systems is often analyzed using an averaging procedure. The data immediately following a sensory event, e.g., hearing an auditory tone, is averaged over multiple events to produce an Event Related Potential (ERP). Components of the ERP can then be localized to different parts of the brain. Other cognitive components, however, are not easily isolated using this ERP approach. For these, a time-frequency characterization may be more appropriate (Tallon-Baudry and Bertrand, 1999). See also Makeig et al. 2002 for a recent critique of the averaging procedure.\nThe second theme is ‘functional integration’, where models are used to describe how different brain areas interact. A classic example is the use of models to find increased connectivity between dorsal and ventral visual streams after subjects learn object-place associations. A wide range of statistical techniques are being used to measure inter-regional connectivity. Both unsupervised (e.g., Independent Component Analysis , ICA) and supervised techniques (e.g., support vector machine, SVM) are used. Other models seek to directly measure \"causal\" connectivity based on static, statistical constraints (e.g., Structural Equation Modelling, SEM) or dynamic, more bio-physically motivated assumptions (e.g., Dynamic Causal Modelling, DCM). A challenge for functional integration models is to bridge the gap between the large-scale, statistical models of the whole brain, and the small number of highly constrained spatial regions needed to be able to apply SEM and/or DCM.\nDCM for fMRI uses a forward model in which neural activity generates BOLD signal changes via a `Balloon' model of vascular dynamics. The model is then inverted to provide estimates of changes in connectivity between brain regions. In DCM for ERPs, neural activity is described using neural-mass models, which then give rise to observed EEG or MEG data using Maxwell's equations (see above). Inversion of the model then allows one to make inferences about changes in long-range excitatory connections among different brain areas.\nThe above analysis approaches are implemented in various software packages such as SPM  (SPM is the name of a software package as well as a methodology), FSL , EEGLAB , BrainVoyager , or AFNI . They are also described in a recent textbook (Friston et al. 2006).\nThe applications of functional imaging are diverse and multitudinous. PubMed, for example, returns over 32,000 articles. The functional imaging journal `NeuroImage' classifies research articles under 'Anatomy and Physiology', 'Methods and Modelling', 'Systems Neuroscience' or 'Cognitive Neuroscience'. Additionally a number of applications in clinical and experimental medicine are emerging.\nFunctional imaging has been applied to all systems of the brain; whether visual, auditory, sensorimotor, emotional, memory, language, attention or control. Overviews of research findings are available in recent textbooks (Frackowiak et al. 2003, Gazzaniga 2004). Recent high-profile (and arbitrarily selected) applications of fMRI in these areas include a study of the effect of sleep on human memory performance (Yoo et al. 2007) and a study of the neuronal and cognitive components of altruism (Tankersley et al. 2007).\nImaging is also used for the study of basic brain anatomy and physiology. For example, DTI has recently been used to identify three regions of human parietal cortex based on their connectivity patterns with other brain areas (Rushworth et al. 2006). Imaging is also used clinically: The best established application is the use of fMRI for pre-surgical mapping to localize cerebral functions in tissue within or near regions intended for neurosurgical resection (Matthews et al. 2006).\nAuthors web page: http://www.fil.ion.ucl.ac.uk/~wpenny/\nR.S.J. Frackowiak, K.J. Friston, C. Frith, R. Dolan, C.J. Price, S. Zeki, J. Ashburner, and W.D. Penny (2003) Human Brain Function. Academic Press, UK, 2nd edition.\nC. Frith (2007) Making up the mind: How the brain creates our mental world. Blackwell Publishing.\nK. Friston, J. Ashburner, S. Kiebel, T. Nichols and W. Penny (2006) Statistical Parametric Mapping: The Analysis of Functional Brain Images. Elsevier, London.\nGazzaniga, M.S., Ivry, R., & Mangun, G.R. Cognitive Neuroscience: The Biology of the Mind. W.W. Norton, 2002. 2nd Edition\nM.S. Gazzaniga (2004). The Cognitive Neurosciences III. MIT Press, New York.\n- Jan A. Sanders (2006) Averaging. Scholarpedia, 1(11):1760.\n- Valentino Braitenberg (2007) Brain. Scholarpedia, 2(11):2918.\n- James Meiss (2007) Dynamical systems. Scholarpedia, 2(2):1629.\n- Paul L. Nunez and Ramesh Srinivasan (2007) Electroencephalogram. Scholarpedia, 2(2):1348.\n- Seiji Ogawa and Yul-Wan Sung (2007) Functional magnetic resonance imaging. Scholarpedia, 2(10):3105.\n- Mark Aronoff (2007) Language. Scholarpedia, 2(5):3175.\n- Anthony T. Barker and Ian Freeston (2007) Transcranial magnetic stimulation. Scholarpedia, 2(10):2936.\nJ. Ashburner and K.J. Friston. Voxel-Based Morphometry - The Methods. NeuroImage, 11:805-821, 2000\nS. Baillet, J.C. Mosher and R.M. Leahy (2001) Electromagnetic brain mapping. IEEE Signal Processing Magazine, pages 14-30.\nJ.D. Cohen and F. Tong (2001) The face of controversy. Science, 293, 2405-2407.\nN. Logothetis, J. Pauls, M. Augath, T. Trinath and A. Oeltermann (2001) Neurophysiological investigation of the basis of the fMRI signal. Nature 412, 150-157.\nS. Makeig, M. Westerfield, T Jung, S. Enghoff, J. Townsend, E Courchesne and T. Sejnowski (2002) Dynamic brain sources of visual evoked responses Science, 295, 690-694.\nP.M. Matthews, G.D. Honey and E.T. Bullmore (2006) Applications of fMRI in translational medicine and clinical practice. Nature Reviews Neuroscience, 7, 732-744.\nM.F. Rushworth, T.E. Behrens and H. Johansen-Berg (2006) Connection patterns distinguish 3 regions of human parietal cortex. Cerebral Cortex, 16(10):1418-30.\nC. Tallon-Baudry and O. Bertrand (1999) Oscillatory gamma activity and its role in object representation. Trends in Cognitive Sciences, 3(4), 151-162.\nP. Ritter and A. Villringer (2006) Simultaneous EEG-fMRI. Neuroscience and Biobehavioural Reviews. 30(6), 823-838.\nD. Tankersley, C J Stowe and S A Huettel (2007) Altruism is associated with an increased neural response to agency. Nature Neuroscience 10, 150 - 151.\nS.S. Yoo, P T Hu, N. Gujar, F A Jolesz and M P Walker (2007). A deficit in the ability to form new human memories without sleep. Nature Neuroscience 10, 385 - 392."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:1a788c16-88fb-455a-8ea8-879d2331bedf>","<urn:uuid:9129a73e-9969-48ea-a807-09e74e27c94a>"],"error":null}
{"question":"What's the difference between spider mites and aphids when it comes to controlling them in gardens? 🌿","answer":"Spider mites and aphids have different control methods due to their distinct characteristics. For aphids, control options include squishing them directly (as they have soft bodies with no protection) or using a homemade spray mixture of water, mineral oil, dish detergent, and peppermint oil. Lady beetles can also be attracted to the garden as natural aphid predators. For spider mites, control methods include pruning heavily infested leaves, spraying plants with a strong stream of water, and treating with neem oil every 3 days to destroy eggs. Both pests can be controlled with insecticidal soap, but spider mites require additional attention to prevention through adequate water and fertilizer for plant health.","context":["Who doesn’t want to grow the perfect tomato? After all, tomatoes are the quintessential summer food that we're all eager to get on our tables. Unfortunately, tomato pests are equally as excited to eat the delicious fruits.\nTomatoes are one of the most popular garden crops. My friends and family have an ongoing contest to see who can get them to the table first. Not only does this mean I need to get an early start, but I have to make sure my tomatoes are growing pest free.\nKnowing what type of tomato pest is attacking your garden is the first step to tackling the problem. This guide will help you figure out what's pestering your plants and give you a few methods for getting the situation under control.\nCommon Insect Pests That Affect Tomatoes\nAt some point, every gardener encounters aphids. The tiny, green, pear-shaped bugs leave a sticky honeydew secretion behind them. They're notorious sapsuckers of not only tomatoes but many other vegetables. A few aphids aren't a problem, but when you start seeing clusters of them, it's time to take action. An infestation will interfere with nutrients and water flowing up to your tomatoes leaves.\nFortunately, getting rid of aphids is not hard. They have soft bodies and no way to protect themselves. You can squish them with your thumb like a demi-God if you need an ego boost. Or you can take a more traditional route. Several OMRI and NOP sprays kill aphids. However, they're mostly soap and fats, something you can mix up yourself at home on the cheap.\nAme’s Homemade Bug Spray\nHere's the mixture that I use. The mineral oil suffocates the aphids while the dish detergent dehydrates them. The peppermint oil acts as a natural repellent.\n- Two cups of water\n- Two tablespoons of food grade mineral oil\n- Three drops of dish detergent\n- Three drops of peppermint oil\nMix the ingredients in a spray bottle and apply to your plants. Shake the mixture frequently while applying because the oil will separate.\nNatural Aphid Controls\nDo you want Mother Nature to assist you with killing those aphids? Then encourage lady beetles. Lady beetles (or ladybugs) eat aphids when they are in the larvae stage. You can attract ladybugs to your garden by planting dill, chives, cosmos, marigolds, and yarrow. They also need places like trees and shrubs to hide from predators.\nFlea beetles are tiny black insects with a hard exoskeleton. They can jump (hence the name flea beetle) and spread through your garden rapidly.\nFlea beetles are my kryptonite! I'm convinced they spend all winter daydreaming of sucking the life out of my plants. They especially love to eat tomatoes and their cousins, eggplant and pepper.\nYou can identify flea beetle damage by the irregularly shaped pits and holes in the leaves of your tomatoes. Extensive damage makes the plant's leaves look like lace. It sounds pretty, but it can destroy your plant.\nFlea beetles lay eggs in the soil. The larva live underground and eat the plant's roots. They typically don't cause serious damage at this stage. As adults, they move above ground and eat the plant's leaves. This stage is when they can cause monumental destruction.\nControlling Flea Beetles\nYou can help prevent flea beetles from attacking seedlings by using row covers when you transplant. Make sure to secure the row cover at ground level.\nSticky traps are an excellent way to control adults. I've seen diatomaceous earth (DE) recommended for flea beetles, but my experience is that it's not effective. DE works best on soft-bodied pests. Diatomaceous earth consists of aquatic fossils. These ground-up organisms feel like flour in your hand. However, the microscopic jagged edges are lethal knives to soft-bodies critters. You can also t\nFlea beetles are attracted to radishes, which you can plant as a trap crop and then use traps or an insecticide to kill the tomato pests.\nRoot Knot Nematodes\nThere are thousands of varieties of nematodes, but the ones that mutilat your tomatoes are root-knot nematodes. These microscopic roundworms cause bumps or galls on the roots of your plants.\nThese galls interfere with the plant's ability to take up nutrients. They may cause the plant to stop growing and turn yellow from lack of nutrients. Nematodes are widespread, but they're more common in the south and coastal areas with warm winters.\nPrevention of Nematodes\nThe best prevention is a strict crop rotation system. Nematodes can't travel far, so removing hosts plants lowers the population. Leave three years between planting tomatoes in one spot.\nYour grandmother, like mine, may have told you to plant marigolds with your tomatoes. That's because marigolds act as a trap crop to attract nematodes. Research shows that the French types such as lemon drop, yellow boy, and tangerine are the most effective.\nYou can also purchase tomato varieties that are resistant to nematodes. When looking in catalogs, you may see VFN after a tomato variety. The N stands for nematode and means that type will be resistant.\nPrevention of nematodes starts in the fall, so be sure to take steps to stop them from becoming a problem.\nWhiteflies are small flying insects. They feed on the sap of your plants much the way aphids do. Whiteflies spread many types of fungal diseases among your plants.\nThe nymphs look like small white pods and hang out on the underside of the leaves. Adults are yellow colored insects with white wings. Whiteflies tend to be a greater problem in greenhouse production of tomatoes because they love a warm, moist environment.\nThe best prevention for whitefly is weeding. Whiteflies breed and lay eggs in decaying plant matter.\nYou can also try silver colored mulch. Research shows that silver mulch confuses whiteflies so that they don't\nFinally, try an insecticidal soap such as Safer, or Ames Homemade recipe above.\nThe tomato hornworm strikes fear in even the bravest of gardeners. The larvae are large, typically three to four inches long, with a black “horn” on their butt. These caterpillars will undergo metamorphosis into the Carolina sphinx moth, also known as the tobacco hawk moth.\nThe brown pupae overwinter and emerge as adults in the spring. The adults mate and lay eggs which hatch into caterpillars. They produce two generations each growing season, so you need to keep on top of things if these show up in\nNatural Hornworm Control\nThe best control is hand picking these tomato pests off your plants. You can dispose of them in a bucket of soapy water. Better yet, feed them to your poultry for a treat.\nEnlist your children in the task. When I was a kid, my father paid us a penny for each hornworm we nabbed. You can also recruit your poultry flock. Chickens have sharp eyes and are willing participants. You will have to keep an eye on them so that they don't eat the tomatoes, though. Ducks are an efficient caterpillar control as well.\nSprinkle DE on the ground around your young plants when you transplant them. As they grow, sprinkle the leaves to protect them.\nBe sure to till your garden in the fall. Tilling\nIf your infestation is severe, you can use commercial organic products. Both Safer Garden Dust and Monterey Insect Spray will kill the young caterpillars.\nColorado Potato Beetle (CPB)\nThe Colorado Potato Beetle doesn't only live in Colorado, and they munch on more than potatoes. They also love tomatoes and plants in the Solanaceae family. It's one of the most destructive tomato pests out there.\nThe adults emerge in spring and lay clusters of orange eggs on the underside of plants. The eggs can survive for several years. Larvae will hatch dependent on weather conditions.\nThe Colorado Potato Beetle starts its life as a caterpillar-like larva. They are a dark red with black spots and gradually become a lighter red color. The adults are orange colored and oval shaped. They have a hard exoskeleton which protects them.\nBoth the larvae and adults can quickly defoliate your plants. Adults can fly and move around your garden.\nHow To Protect Your Tomatoes From Colorado Potatoes Beetles\nHandpick CPB and d\nFor organic controls, azadirachtin (neem oil) and spinosad (Monterey Insect Spray) both help control populations. Neither of these will outright kill the adult CPB as their hard shell protects them, but they can destroy the larva.\nTomato fruitworm, also known as corn earworm and cotton bollworm, usually first shows up at as a black hole at the base of tomato fruits. Once you cut into the tomato, you'll see tunnels throughout the fruit. The larvae have tan heads and striped bodies. As adult moths, they are olive in color with a dark spot and bands on their wings.\nGetting Rid of Tomato Fruitworm\nDispose of any infested fruit and avoid planting near corn because having two food sources can be an even more enticing lure for tomato fruitworm. Parasitic wasps can help control populations, and you can also sprinkle plants with diatomaceous earth. If all else fails, you may need to turn to chemical pesticides.\nCutworms are large gray or brown caterpillars with black or yellow spots. As adults, they are mottled brown or gray moths. If you notice large holes in your tomatoes, cutworms could be the culprit. Cutworms have smooth skin and reach about 2-inches at their full size. They are easiest to spot at night when they are active. During the day, they like to hide in soil or under debris around plants.\nThese aggressive tomato pests can mow down an entire garden overnight, so if you have them, get serious about control.\nKeep the area around your plants clean and till the soil before planting. You can also place cardboard collars around your plants to prevent the worms from being able to nibble. Diatomaceous earth is effective against them, and you can head out to your garden in the evening with a flashlight to spot and destroy them. Moth traps are effective for catching the adults.\nTo prevent them from taking hold, be sure to rotate your crops and till the soil in the fall.\nGreen and Brown Stink Bugs\nThere are several different kinds of bugs that attack tomatoes, but the damage they do and the method of controlling them are similar. Stink bugs are about 1/2-inch long and have a distinct shield-shaped body and are usually brown or green. They emerge in the garden in the spring. If you don't spot the critters first, you'll notice pinpricks on your tomatoes surrounded by lighter colored areas that eventually turn yellow. They can carry diseases, including mold, that can destroy your garden. If you suspect you have an infestation, shake your tomato plant and examine the ground for the bugs.\nControlling Stink Bugs\nTo get a handle on these tomato pests, remove the weeds that stink bugs prefer to hide or overwinter in, like thistle, bramble, mustards, and mallow. Spray plants with water every day to knock them off your tomatoes and then hand pick them off the ground. If all else fails, try treating plants with kaolin. Try not to use commercial insecticides because they can kill the beneficial insects that will help keep your garden healthy.\nSpider mites are teeny-tiny arachnids that suck the life out of plants. Although they are small, a heavy infestation can wreak havoc on your garden. You'll probably first see their delicate webbing all over your tomato plants. If you look on the underside of leaves, you'll spot clusters of them hanging out. They can cause leaves to turn yellow and fall off. This can reduce yields.\nGetting Rid of Spider Mites\nPrune off heavily infested leaves and then spray the plant with a strong stream of water. Treat tomato plants with neem oil every 3 days to destroy the eggs. You can also use insecticidal soap if things are bad.\nTo help prevent them from returning, make sure your tomatoes get ample water and fertilizer because healthy plants can better fight off these pests. You also want to encourage beneficial insects to hang out in your garden, because they love to snack on aphids.\nTortoise beetles like to nibble on the underside of tomato leaves. They can kill off seedlings, and a massive infestation can reduce yields by filling with tiny holes. The beetles are pretty distinct at about 1/4-inch long with a round or oval body that looks similar to a ladybug at first. The difference is that a tortoise beetle shell extends over its head. They come in all colors, from red and orange to metallic green and blue.\nControlling the Tortoise Beetle\nHandpick beetles if you spot them on your plants and drop them into soapy water. Then, scour your plants weekly for the clusters of eggs that the females lay. Scrape them off or snip off the host leaf. If things are still bad, use insecticidal soap to spray plants. Keep tomato plants well watered and fed, because healthy plants can fight off a small infestation.\nSlugs and Snails\nSlugs will eat just about anything in the garden and tomatoes are no exception. You probably know what a slug looks like, but if you don't catch them in the act, you'll know you have them if you see a hole nibbled into the fruit with surface feeding around the hole.\nControl Slugs in your Tomato Garden\nDematiaceous earth is an effective first-line defense against slugs. Sprinkle it around your plants. You can also put lava rock around plants.\nAdjusting your watering schedule can do wonders. Water in the morning, so the soil is dry by the evening, which is when moisture-loving slugs are most active.\nIf you want to tackle your slug problem aggressively, head outside at night with a flashlight and some salt. Sprinkle every slug you see with some salt. Be cautious not to use too much, or you can harm your plants. You can also try commercial slug repellants.\nTarnished Plant Bug\nThe tarnished plant bug is a small insect – about 1/4-inch big. It's mottled yellow and brown, with a black tip on each wing. These tomato pests suck the juice from plant stems, leaves and fruits. You'll often first know you have them when you see black spots all over your plant. They'll also cause\nControl Tarnished Plant Bug\nThere are five generations of this bug each year, so you need to keep on top of things. Your first line of defense is to keep weeds like mustard, pigweed, mullein, and ragweed away from your garden. Then, be sure to till your garden before planting.\nUse floating row covers and white sticky traps around your garden at 2-feet above the ground. Make sure the covers are sealed at the soil level. Spray plants with a garlic spray while the bugs are active. You also want to encourage parasitic wasps.\nDiagnosing Your Tomato Pests\nAre you still stumped as to what's killing your tomatoes? It's well worth the time to spend studying some articles or websites to help you figure out which tomato pest is plaguing your plants. That way you can act quickly to nip it in the bud so to speak.\nOnline insect guides can be a great asset in your arsenal. You can look up your state and extension office to see what is available in your area. Here are a few resources based on location:\n- For those on the west coast The Pacific Northwest Pest Management Handbook\n- For the Northeast, the University of Maine has an extensive insect guide\n- The deep south offers some unique challenges to bug control. University of Florida Extension has a guide to help.\nThe best way to protect your plants is to be an observant gardener. I like to take evening walks in my garden. I examine the plants and make notes of what needs weeding, spraying or harvesting. In this way, I keep up with my garden chores before things get out of hand.","Spider Mites - Tetranychus\nBy SCMG Coby Lafayette-Kelleher\nBaby it’s cold (and wet) outside. Too cold, one would think, for any self-respecting insect. It’s mollusk weather, so we expect to see worms, snails and slugs busy turning last season’s plant material into compost. But insects? This time of the year, they are keeping a low profile.\nDon’t let your guard down, though. They’re out there, waiting. This is especially true of our bug of the month—Spider Mites. A point of clarification before we go much further: Spider Mites aren’t really bugs or insects; they are actually tiny little Arachnids. Thus, the “Spider” part of Spider Mites. Just so we’re clear.\nAnyway, up close, really close, these tiny mites do look rather spider-y with their eight bristly legs and tear-drop shaped bodies. And, while you can see them clearly with a hand lens, it’s the evidence of their presence that usually gets your attention. Your first clue to the presence of Spider Mites is probably going to be a fine, silk-like webbing on leaves, twigs, and fruit of infested plants. They are particularly fond of vines, berries, and fruit trees, although ornamentals are vulnerable too.\nSpider Mites feed on their host plant by piercing plant leaf cells and sucking out the contents. So, it isn’t surprising that leaf stippling is actually the first sign of damage to your plant/s. Stippling is subtle and easily overlooked. Consequently, most people don’t “discover” a Spider Mite infestation until the webbing starts showing up.\nSpider Mites are colonial Arachnids, meaning that they occur in large groups; generally, an infestation suggests itself by the presence of mite activity on the under-side of plant leaves. Strong, healthy plants can usually hold their own against a few colonies of mites. But, bear in mind that in favorable conditions, mites can complete an entire life cycle in just one week. Really.\nSo, when all evidence points to the presence of Spider Mites, it’s time to take action. With moderate infestations, cultural controls can be quite successful in reducing and discouraging these mites. They like it dusty, so keep areas around plantings moist. A more direct method of controlling Spider Mites is the tried-and-true forceful blast of water on infested leaves and twigs.\nIf cultural controls fail or the infestation is severe, consider biological controls. Turns out Spider Mites have a number of natural enemies. The most significant are predatory mites in the Genus Phytoseiulus; common name Western Predatory Mite. These mighty mites are not much bigger than Spider Mites, but are extremely good at their job and once established tend to stick around as long as they have something to eat.\nPredatory Mites are generally available these days through most nurseries. But, if your nursery doesn’t sell them, don’t despair. The California Department of Pesticide Regulation has a list of “Suppliers of Beneficial Organisms in North America” available on their website here.\nHere in California, Rincon-Vitova Insectaries carries a number of Predatory Mites.\nLet it be said though, that when it comes to Spider Mites, an ounce of prevention is worthy of consideration. These critters, like most “pests,” are opportunists. So, keep your plants healthy, well-watered and free of insect infestations. It’s a good idea too, to routinely look for stippling and check the under-sides of leaves for evidence of Spider Mites, such as webbing or colonial aggregations.\nBut don’t let your guard down, because while you are waiting out the winter, so are Tetranychus, safely tucked into leaf litter somewhere in your garden.\nOne more thing: a much longer and more detailed version of the material shared in this feature can be found at the University of California’s Online Integrated Pest Management Website."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:35e7f537-afff-4178-9f2e-6fa7600e7371>","<urn:uuid:0164876a-2f15-4576-b7c7-0fabbfb25423>"],"error":null}
{"question":"I'd love to know how AI and IoT are revolutionizing smart cities, but also what measures should be taken to protect citizens' data? 🌆","answer":"In smart cities, IoT devices like thermostats, lights, and security cameras collect data on residents' preferences and usage patterns, while AI leverages this data to automate energy consumption, enhance security, and optimize traffic flow. For protecting citizens' data, several crucial measures must be implemented: strict access controls with strong authentication mechanisms, comprehensive data encryption both at rest and during transmission, regular system audits and monitoring for suspicious activities, thorough staff training on data security, and well-defined incident response plans. Organizations must also conduct regular risk assessments, develop robust security policies, and maintain disaster recovery capabilities to ensure the confidentiality and integrity of collected data while preventing unauthorized access.","context":["Artificial Intelligence (AI) and the Internet of Things (IoT) are two transformative technologies that, when combined, have the potential to revolutionize numerous industries and aspects of our daily lives. AI and IoT are often referred to as complementary technologies, as they can work together to collect, analyze, and utilize data in ways that were previously unimaginable. Let’s explore the synergy between AI and IoT and their applications in various domains.\n1. Data Collection and Analysis: IoT devices are capable of collecting vast amounts of data from the physical world, such as temperature, humidity, pressure, location, and more. AI can process and analyze this data in real-time, extracting valuable insights and patterns. For instance, in agriculture, IoT sensors can monitor soil conditions, while AI can process this data to optimize irrigation, crop management, and predict yield.\n2. Predictive Maintenance: In industrial settings, IoT sensors on machinery can continuously monitor performance and detect anomalies. AI algorithms can then predict when maintenance is needed, reducing downtime and minimizing costly breakdowns. This concept is known as predictive maintenance and is essential in manufacturing, transportation, and infrastructure management.\n3. Smart Homes and Cities: IoT is integral to the development of smart homes and smart cities. Smart devices like thermostats, lights, and security cameras collect data on residents’ preferences and usage patterns. AI can leverage this data to automate energy consumption, enhance security, and optimize traffic flow in smart cities.\n4. Healthcare: IoT devices, such as wearables and medical sensors, collect real-time health data from patients. AI can process this data to monitor chronic conditions, make early diagnoses, and even predict disease outbreaks. Telemedicine and remote patient monitoring are growing applications of AI and IoT in healthcare.\n5. Environmental Monitoring: AI-powered IoT systems can monitor and analyze environmental data, such as air quality, water quality, and wildlife behavior. This is crucial for conservation efforts, disaster management, and pollution control.\n6. Supply Chain and Inventory Management: IoT sensors can track the location and condition of goods in transit. AI can optimize inventory management, forecast demand, and reduce waste by ensuring that products are stored and transported under optimal conditions.\n7. Autonomous Vehicles: IoT sensors in autonomous vehicles provide real-time data on road conditions, traffic, and vehicle performance. AI algorithms process this data to make split-second decisions for safe and efficient navigation.\n8. Energy Management: AI and IoT can work together to optimize energy consumption in buildings and industrial facilities. Smart grids and smart meters enable efficient electricity distribution, while AI algorithms can manage energy usage based on real-time demand and cost factors.\n9. Retail and Customer Experience: Retailers use IoT devices and AI to collect data on customer behavior, preferences, and inventory levels. This data can be used to personalize marketing, optimize stock levels, and enhance the in-store and online shopping experience.\n10. Security and Surveillance: IoT-enabled security systems can detect anomalies and intrusions, while AI can analyze this data to distinguish between false alarms and genuine threats, improving overall security.\nHowever, the integration of AI and IoT also raises important considerations regarding data privacy, security, and ethical concerns. Ensuring that IoT devices and AI systems are developed and deployed responsibly is paramount.\nIn conclusion, the combination of AI and IoT has the potential to bring about profound changes in how we live and work. These technologies enable us to collect and leverage data in ways that were previously impossible, leading to more efficient, sustainable, and interconnected systems in a wide range of industries. As these technologies continue to evolve, it is crucial to address the associated challenges to fully unlock their potential for the benefit of society.","In today’s digital landscape, the importance of data security and privacy in the healthcare industry cannot be overstated. With the increasing use of electronic health records (EHRs) and the need to exchange health information seamlessly, healthcare organizations must prioritize compliance with regulatory standards. One such standard is the National Health Authority’s UAE Health Data and Information Flow (NABIDH) compliance framework. This article explores the best practices for achieving NABIDH compliance and safeguarding sensitive patient data.\nUnderstanding NABIDH Compliance\nNABIDH compliance refers to the adherence to the standards and guidelines set by the National Health Authority (NHA) of the United Arab Emirates (UAE) for the secure flow of health data and information. It aims to ensure the confidentiality, integrity, and availability of healthcare data while facilitating interoperability between different healthcare systems.\nImportance of NABIDH Compliance in Healthcare\nComplying with NABIDH standards is crucial for healthcare organizations for several reasons. Firstly, it helps protect patient privacy and prevent unauthorized access to sensitive health information. Secondly, it promotes data integrity, ensuring that the information exchanged between healthcare providers is accurate and reliable. Thirdly, NABIDH compliance enhances the overall security posture of healthcare organizations, reducing the risk of data breaches and cyber threats.\nKey Components of NABIDH Compliance\nTo achieve NABIDH compliance, healthcare organizations need to focus on the following key components:\n1. Implementing Secure Access Controls\nEffective access controls are essential for limiting data access to authorized individuals. Healthcare organizations should implement strong user authentication mechanisms, such as two-factor authentication, to ensure that only authorized personnel can access sensitive patient data.\n2. Ensuring Data Encryption\nData encryption plays a vital role in safeguarding patient information during storage and transmission. Encryption techniques like Advanced Encryption Standard (AES) should be employed to encrypt data at rest and in transit, reducing the risk of unauthorized access.\n3. Regular System Audits and Monitoring\nContinuous monitoring and auditing of healthcare systems are crucial for identifying vulnerabilities and detecting any suspicious activities. Robust logging mechanisms should be in place to track system events and generate alerts in real-time.\n4. Training and Awareness Programs\nHuman error is one of the leading causes of data breaches. Healthcare organizations should invest in comprehensive training and awareness programs to educate employees about the importance of data security, safe handling of patient information, and best practices for maintaining NABIDH compliance.\n5. Incident Response and Disaster Recovery\nHealthcare organizations must develop an incident response plan to effectively respond to security incidents. This plan should outline the steps to be taken in the event of a breach or a cyber attack, including containment, investigation, mitigation, and recovery. Additionally, organizations should have a robust disaster recovery plan in place to ensure business continuity and minimize the impact of any disruptions.\nChallenges in Achieving NABIDH Compliance\nAchieving NABIDH compliance can be challenging for healthcare organizations due to various factors:\n1. Complex Regulatory Landscape\nThe healthcare industry operates under multiple regulatory frameworks, and navigating through these complexities can be overwhelming. Organizations must stay updated with the evolving NABIDH standards and ensure alignment with other relevant regulations, such as the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA).\n2. Technical and Infrastructure Requirements\nMeeting the technical requirements for NABIDH compliance may require significant investments in technology infrastructure. Organizations need to assess their existing systems and infrastructure to identify any gaps and implement the necessary upgrades to meet the compliance standards.\n3. Budgetary Constraints\nImplementing the necessary measures for NABIDH compliance can be financially demanding for healthcare organizations, especially smaller clinics or facilities with limited budgets. Allocating adequate resources for cybersecurity initiatives becomes a critical challenge.\nBest Practices for Achieving NABIDH Compliance\nTo overcome the challenges and achieve NABIDH compliance, healthcare organizations should follow these best practices:\n1. Conducting a Comprehensive Risk Assessment\nPerform a thorough risk assessment to identify potential vulnerabilities and risks to patient data. This assessment should cover all aspects of the organization’s information systems, including networks, hardware, software, and human factors. Based on the assessment findings, develop a risk management plan to prioritize and address the identified risks.\n2. Developing a Robust Security Policy\nEstablish a comprehensive security policy that outlines the organization’s commitment to data protection, defines roles and responsibilities, and provides guidelines for secure data handling and access. The policy should be regularly reviewed and updated to reflect changing threats and compliance requirements.\n3. Establishing Access Control Mechanisms\nImplement strict access controls to ensure that only authorized individuals can access patient data. This includes using strong passwords, multi-factor authentication, and role-based access controls. Regularly review and update access privileges to maintain data integrity and minimize the risk of unauthorized access.\n4. Implementing Data Encryption\nEncrypt sensitive data at rest and in transit to prevent unauthorized access. Implement robust encryption algorithms and ensure that encryption keys are securely managed. This helps safeguard patient data even in the event of a breach or unauthorized access.\n5. Regularly Auditing and Monitoring Systems\nImplement a robust system monitoring and auditing mechanism to detect any unusual activities or potential security breaches. Regularly review logs, conduct vulnerability scans, and perform penetration testing to identify and address vulnerabilities proactively.\n6. Providing Ongoing Training and Awareness\nEducate employees about the importance of data security and their roles in maintaining NABIDH compliance. Conduct regular training sessions to raise awareness about phishing attacks, social engineering techniques, and safe data handling practices. Empowering employees with the necessary knowledge and skills is crucial in mitigating risks.\n7. Creating an Incident Response Plan\nDevelop a detailed incident response plan that outlines the steps to be taken in the event of a security incident. This plan should include procedures for reporting incidents, containment measures, forensic investigation processes, and communication protocols.\n8. Ensuring Disaster Recovery Capability\nImplement a robust disaster recovery plan to minimize the impact of disruptions and ensure the continuity of healthcare services. Regularly backup critical data, test recovery procedures, and establish off-site storage to protect data in case of natural disasters or system failures.\nAchieving NABIDH compliance is essential for healthcare organizations to protect patient data, maintain regulatory compliance, and enhance overall cybersecurity. By implementing the best practices outlined above, healthcare organizations can significantly improve their chances of achieving and maintaining NABIDH compliance. It is crucial to prioritize data security, invest in technology infrastructure, and educate employees about their roles in maintaining compliance.\nHowever, achieving NABIDH compliance is not a one-time effort. It requires ongoing monitoring, updates, and adaptability to keep up with evolving threats and regulatory changes. Healthcare organizations should stay informed about the latest industry standards, collaborate with cybersecurity experts, and regularly assess and enhance their security measures.\nBy prioritizing NABIDH compliance, healthcare organizations can build trust with patients, ensure the confidentiality of sensitive health information, and protect themselves from costly data breaches and regulatory penalties.\n- What is NABIDH compliance? NABIDH compliance refers to adhering to the standards and guidelines set by the National Health Authority (NHA) of the United Arab Emirates (UAE) for the secure flow of health data and information. It ensures the confidentiality, integrity, and availability of healthcare data while promoting interoperability.\n- Why is NABIDH compliance important in healthcare? NABIDH compliance is crucial in healthcare as it protects patient privacy, ensures data integrity, and enhances overall security. It helps prevent unauthorized access to sensitive health information and reduces the risk of data breaches and cyber threats.\n- What are the key components of NABIDH compliance? The key components of NABIDH compliance include implementing secure access controls, ensuring data encryption, conducting regular system audits and monitoring, providing training and awareness programs, and establishing incident response and disaster recovery plans.\n- What are the challenges in achieving NABIDH compliance? Challenges in achieving NABIDH compliance include the complex regulatory landscape, technical and infrastructure requirements, and budgetary constraints. Healthcare organizations need to navigate through multiple regulations, invest in technology infrastructure, and allocate adequate resources for cybersecurity initiatives.\n- What are the best practices for achieving NABIDH compliance? Best practices for achieving NABIDH compliance include conducting comprehensive risk assessments, developing robust security policies, establishing access control mechanisms, implementing data encryption, regularly auditing and monitoring systems, providing ongoing training and awareness, creating incident response plans, and ensuring disaster recovery capability.\nRemember, achieving NABIDH compliance is an ongoing process that requires commitment, vigilance, and continuous improvement to protect patient data and ensure regulatory compliance in the ever-evolving healthcare landscape."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:f05e6f29-7cee-4e82-9758-5f8da212511a>","<urn:uuid:cea91547-19c9-4e96-9307-c652f1a37926>"],"error":null}
{"question":"What's the latest research on controlling odors in composting operations, and what stuff can I actually throw in my compost pile?","answer":"Research shows that odor control in composting operations is linked to pH management and temperature control. When composting food waste, keeping initial temperatures below 40°C allows pH to rise to neutral levels, which significantly reduces odors and volatile organic compounds. Uncontrolled high temperatures can lead to sour odors and incomplete decomposition. As for acceptable materials, you can compost tree-trimmings, grass clippings, garden refuse, kitchen food wastes, sawdust, manure, wood ashes, hay, and straw. However, meat and dairy products should be avoided. For best results, these materials should be shredded to accelerate decomposition. Disease-infected or pest-infested materials should not be added as they can contaminate the pile.","context":["Tim O’Neill and Ryan Costello\nFood waste is increasingly being diverted to composting facilities that have traditionally managed green waste. Among other challenges, the high biodegradability and low pH inherent in food waste have tended to increase the generation of odors and volatile organic compounds and lengthen the time to achieve a stable product. In a series of peer-reviewed publications, Celia Sundberg, a compost researcher, explained the mechanism by which food waste rich (50%) feedstocks inhibit composting and give rise to these issues (Sundberg, 2004; 2008; 2013). Her research found that when temperatures are kept below 40°C (104°F) for the first 48 hours of active composting, the mesophilic bacteria could degrade the organic acids in the food waste, which in turn allowed the pH to rise above 6.5 (7.0 is neutral) and eliminate the inhibitory effects of low pH.\nSundberg’s work, however, did not look at some of the nuances that come with working with typical source separated organics at full scale. These nuances impact both the design and operation of a facility. Questions that remained to be answered include: What about lower concentrations of food waste? How does increasing early phase temperatures over a broader range inhibit biodegradation? What happens if temperatures are allowed to increase to typical high thermophilic conditions after a mesophilic initial period? And perhaps most importantly for composting facility designers, what peak aeration rate is required to provide an initial mesophilic period? The study reported on in this article aimed at answering some of these questions.\nEngineered Compost Systems’ (ECS) Compost Process Lab is equipped with three instrumented Aeration Demand Testers (ADTs) that provide controlled compost process conditions and measure the real-time levels of carbon dioxide (CO2) and oxygen (O2), the primary indicators of composting activity. ADTs aerate a 12-liter feedstock sample while controlling temperatures to within +1.0°C (+1.8°F) and airflow to within 2% of full-scale. By adjusting the rate of airflow the range of oxygen levels can be controlled as well. Standard feedstock and compost product tests (weight, moisture content, volatile solids, pH, density, and stability) are carried out as needed. These data provide insight on how process conditions and mix characteristics impact the rate of biodegradation.\nEach ADT is a sealed stainless-steel vessel nested within another steel vessel containing a stirred water bath that is maintained at a selected temperature. Sensors monitor the temperature in the headspace (T1), compost sample (T2), and in the water bath (T3), as illustrated in Figure 1.\nA series of cooling and reheating coils reduce the relative humidity in the exhaust air prior to exposure to the CO2 and O2 sensors. All sensor data is logged every 10 minutes.\nFeedstock Mix and Test Conditions\nFor this trial, a mix was created from common acidic feedstocks — food waste and conifer wood waste — to further examine the interaction between pH and temperature in composting process efficiency. The raw mix was created at a ratio of 1:3 food waste to sawdust by volume and had the characteristics shown in Table 1.\nThe food waste was comprised of blemished produce from a grocery store (tomatoes, strawberries, melons, and citrus) that was slurried in a food processor. It was amended with relatively fresh coarse fir sawdust. Water was added during mixing until mix bulk density was approximately 900 lbs/cubic yard (yd3) by bucket test, and the “squeeze test” was able to produce a few drops of free water. (At full-scale facilities using wood chips in the mix, the larger particles would have decreased the short-term bioavailable carbon but demonstrate the same pH inhibition effect.)\nADT 1, 2, and 3 were maintained at 40°C (104°F), 55°C (131°F), and 65°C (149°F) respectively for the first 4 days to examine the difference between mesophilic, moderate thermophilic, and high thermophilic composting temperatures on composting efficiency. At 4 days, the temperatures in ADT 1 and 2 were also adjusted to 65°C to expose the samples to higher thermophilic temperatures that are common in full-scale systems.\nThe pH in the feedstock mix was measured by saturating a small sample with deionized water, stirring, allowing the mix to sit for about 1 minute, and then inserting color-indicator pH paper strips from Hydrion. Moisture was assessed by weighing wet and then oven-drying samples at 100°C (212°F) for 24 hours. Volatile solids (organic matter) were determined using a 550°C (1,022°F) oven for two hours to combust the dry samples. Stability was assessed using a Solvita test kit with an optical reader at the end of the trial (8.5 days total).\nFigure 2 (a, b, and c) and Table 3 indicate that, for the acidic feedstocks tested, temperature had a strong impact on the rate and the total CO2 evolution during the trial period. ADT 1 (40°C) produced significantly more CO2 both within the first two days, and throughout the test period, than either ADT 2 (55°C) or ADT 3 (65°C). After 40 hours, CO2 production from ADT 2 began to increase, but by the end of the trial at 8.5 days it had only produced 40% of the CO2 produced by ADT 1. ADT 3 appeared to be strongly inhibited; by the end of the trial, it had only produced 5% of the CO2 produced by ADT 1.\nThe pH also appears to be affected by temperature and, as shown by Sundberg, impacts generation of CO2. The mix started at pH 4. After 4 days the mesophilic ADT 1 had reached a neutral pH of 7, whereas the thermophilic treatments remained at pH 4. By the end of the trial the moderately thermophilic ADT 2 increased to pH 5, while the highly thermophilic ADT 3 was still at pH 4.\nTable 3 also shows the expected correlation between CO2 generation, pH, and stability measured on the Solvita Index. Solvita compost stability test (1=very unstable, 8 = very stable) results were measured after 8.5 days of composting. The sample from ADT 1 achieved an Index of 6 (ready curing phase). The other two treatments produced Solvita Indexes of 3 and 4, for ADT 2 and ADT 3, respectively (still in an active composting phase). Figure 3 shows cumulative CO2 generation from ADT 1, 2, and 3, along with the midway and final pH.\nFigure 4 of the samples at the end of the trial shows that ADT 1 appeared darker than ADT 2 and 3, which looked relatively unchanged from the raw mix. The material from ADT1 had a very light earthy odor whereas the other samples emitted a sour odor.\nAn uninhibited active phase composting process produces significantly more CO2, and thus achieves a more stable product much more quickly than a process that is inhibited by low pH resulting from high initial temperatures. Our qualitative observations, as well as findings in the research, indicate that uninhibited composting generates less odors and volatile organic compounds. When composting acidic waste feedstocks, providing a mesophilic (<40°C) phase at the beginning of the process allows the pH to quickly rise to the neutral range. This pH shift from acidic to neutral appears to not be reversed when temperatures are subsequently allowed to rise significantly higher (65°C). Thus, the benefits of achieving near neutral pH continue through the remainder of the composting process.\nThese findings should be considered when developing composting facility design and operational plans for food waste, or other low pH feedstocks such as MSW and some green waste. Biosolids, digestate and manures all tend towards the alkaline side of the pH spectrum where low pH inhibition isn’t an issue. The CO2 data generated in the ADT trials provides the basis to calculate unit heat generation (Watts per kg). In Table 4, typical ambient air conditions (30°C, 20% relative humidity (RH)) and the peak CO2 generation rates during the mesophilic and thermophilic phases in ADT 1 are used to calculate required airflow to match the rate of heat generation.\nThe calculated result is a very high aeration rate for the mesophilic phase. Typical industry aeration rates vary from 0.2 to 5.0 cfm/cy. Fortunately, the pH shift isn’t reversible; the temperature can be allowed to rise once near neutral pH has been achieved. Once the pH shift occurs, the aeration rates in the thermophilic phase can be drastically downsized due to both the much lower rate of CO2 (heat) generation following an uninhibited mesophilic phase, and the much higher energy content of the 65°C exhaust air.\nIn practice more effective cooling can be achieved in a 2 to 4 day mesophilic phase by initially building shorter piles that increase the unit aeration rate and take advantage of increased heat loss through convection and conduction by increasing the surface area/mass ratio of the pile. Material could then be combined into standard depth piles for the remainder of the active composting period. The mix used in this trial had a high food waste content (45%). Mixes with a more common food waste content in the 5% to 20% range tend to produce lower rates of heat generation, but in our experience, they also require higher aeration rates than industry standards to limit temperatures to mesophilic levels.\nThe results from these trials have provided insights into more efficient composting of acidic feedstocks, but they have also raised questions on how to further optimize facility design. These questions include: How short can the mesophilic period be? How does reducing the percentage of food waste from 45% in these tests impact the aeration requirement to maintain a mesophilic period? Our hunch is that moderately acidic feedstocks (say an initial pH of 5.5) will require a shorter mesophilic period than more strongly acidic feedstocks, but that peak aeration requirements will always need to consider the biodegradability of the whole mix.\nTim O’Neill, president and founder of ECS (Engineered Compost Systems), has an MS in Mechanical Engineering and leads research and development at ECS. He is on the board of trustees of the Compost Research Education Foundation and frequently teaches classes at operator trainings that involve the application of compost process science to understanding and improving facility performance. Ryan Costello, Compost Scientist at ECS, earned an MS in Science in Soil Science and Horticulture from Oregon State University. He has a decade of experience working as an organic farm certification officer.","Creating a compost pile as a classroom project will demonstrate\nto students that natural materials can be recycled.\nA location for the project, organic waste materials, a garden\npitchfork, soil, water. (Lime, manure, nitrogenous fertilizer,\nmaterials to construct an enclosure, a ½inch mesh screen,\nand a soil thermometer are optional.) Volunteers must be available\nto construct and maintain the compost pile.\nCompost is an inexpensive and effective soil conditioner that\nrecycles organic waste materials. Nutrients in plant material\nare returned to the soil through the breakdown of organic material\nby the action of microscopic fungi and aerobic bacteria. Organic\nwastes are decomposed, and the result is a material useful as\na natural fertilizer.\nIn Japan, Europe, and recently in the United\nStates, municipalities have established large-scale solid municipal\ncomposting facilities. The volume of organic material composted\nis diverted from other disposal facilities. Municipalities have\nfound that composting leaves collected in the fall can save disposal\nWhen properly managed, a compost pile will\nnot produce odors or attract pests. The finished product can be\nready for use as a garden mulch in as little as six weeks with\nproper management. Compost has proven valuable for use in land\nreclamation efforts where erosion or earth moving activities have\ndisturbed the topsoil.\n1. Find a suitable outdoor site to locate the compost pile. The pile should be exposed to rainfall, but may work best in a shaded location. Proximity to a water source is suggested.\nA good time to start a compost pile is whenever\norganic materials are available. The fall of the year is quite\nsuitable, since composting can serve as an alternative to the\nburning or landfilling of leaves. Tree-trimmings, grass clippings,\ngarden refuse, kitchen and lunchroom food wastes, sawdust, manure,\nwood ashes, hay and straw are among the organic wastes suitable\nfor composting. Meat and dairy products should be avoided.\n2. Develop a plan of operation that outlines\nthe procedures for conducting the composting project. Present\nthe plan to the school principal. Permission and support from\nadministrative and maintenance personnel must be obtained before\ninitiation of the composting project.\n3. An easy to manage compost pile can be enclosed on three sides by utilizing wooden pallets, used concrete block, fencing, snow-fencing, or hay bales. The fourth side should be accessible to permit turning the pile. A 4'x4'x4' enclosure can yield a ton of compost.\nA compost pile can be constructed without\nan enclosure. A shallow pit may be excavated and the organic material\nsimply piled. The excavated soil will be added to the pile.\n4. Begin the compost pile with a layer of branches or cornstalks to help promote ventilation and drainage. The compost pile is then built with successive eight-inch layers consisting of a six-inch layer of organic material moistened with water and covered with two inches of soil, lime, manure or nitrogenous fertilizer. Shredding the organic materials will accelerate the decomposition process.\nThe eight-inch layers are repeated until the\npile is four feet high. Each layer should be moistened, but not\nsoaked. Materials in the compost pile should always remain as\ndamp as a squeezed sponge. A depression created at the top center\nof the pile will collect precipitation. (Layering of materials\nis not essential to the process.)\n5. The compost pile is now ready for decomposition.\nDuring this phase the temperature within the pile may reach 175o F.\nThe heat is effective in eliminating most disease organisms, insects,\nand weed seeds. Diseased or infested materials should not be added\nto the compost pile.\nThe pile should be turned over and mixed every\nfew weeks to move outer materials to the center. Less frequent\nturning will delay decomposition. A steady decrease in the temperature\nat the center of the pile will signal the end of the fermentation\nprocess. When the compost is finished, it will have a dark color\nand a crumbly soil-like texture.\n6. Maintain a record of the composting process.\nEnter the date of compost pile construction, the organic materials\nadded to the pile, the days the compost pile is turned, the date\nthe compost is ready, how the compost is used and other observations.\n7. A soil thermometer can be used to monitor\nthe temperature of the pile. Create a compost pile temperature\nchart plotting thermometer readings over the term of the project.\n8. The finished compost may be sifted through\na ½\" mesh screen with rejected particles returned\nto the compost pile, or the compost may be added directly to garden\nsoil. Applied as a mulch or top dressing around plants shrubs\nand trees, the compost will provide soil nutrients, retain moisture,\nand inhibit weed growth. Look for uses for compost around the\nschool grounds. Consider marketing sifted compost as a fund-raising\n9. Prepare a report that will describe the\ncomposting project. Refer to the project log (Item 6) for\nkey information. Present the report to the school principal. Consider\npublicizing the project in the school and community newspaper."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7d244b77-2ff1-4e2d-bb99-62b730e3f08e>","<urn:uuid:a4d1319d-16de-4f65-848c-8a99bf12a053>"],"error":null}
{"question":"How do battery-based grid-tied PV systems and emergency standby generator systems compare in terms of their backup power capabilities during utility outages - what are their key operational differences and limitations?","answer":"Battery-based grid-tied PV systems and emergency standby generator systems have distinct operational characteristics and limitations. Battery-based grid-tied systems typically provide limited backup power - a typical bank of four 100 amp-hour sealed batteries only delivers 4 kilowatt-hours of usable power before requiring recharge, which can be insufficient during extended outages, especially at night or during storms. In contrast, emergency standby generator systems can provide continuous backup power as long as fuel is available. The generator system uses an automatic transfer switch (ATS) that detects utility power loss and commands the generator to start, with a UPS providing battery backup during the generator's startup period of several seconds to minutes. While battery-based PV systems are slightly less efficient than batteryless systems due to battery charging requirements, standby generators are simpler in configuration with a single utility source, single generator source, and one ATS, though they lack redundancy if the generator fails to start properly.","context":["Batteryless grid-tied PV systems are in many ways the essence of simplicity. Reliable and essentially maintenance-free if skillfully installed, they are ideally suited to homes accustomed to utility power. But the chief limitation to batteryless PV systems is lack of backup power during utility outages. If you want backup power for “critical” loads, such as refrigeration or lighting, there are three common approaches.\nThe first approach is to install a battery-based grid-tied system. While a battery backup system works well if designed and installed well, it will add cost and complexity to the system. It will be slightly less efficient than a batteryless system, as it must balance AC production with charging and maintaining batteries, rather than solely maximizing AC production into the grid. It also will usually supply only enough stored power to support selected house loads during a typical short-duration utility outage, though RE-generated energy will extend the length. Off-the-shelf packages are available to support a modest load profile during an outage of short duration: A typical battery bank of four 100 amp-hour sealed batteries only provides 4 kilowatt-hours of usable power before it must be recharged; this would be a hard limit for most households at night or during a snowstorm.\nAdding a generator to a battery-based grid-tied PV system will extend the system’s backup duration indefinitely—until the fuel runs out or the residents get tired of the noise. With good design and execution, this approach can work well. However, the system quickly reaches complexity beyond the skills of most homeowners and novice installers. It must be set up and programmed to work reliably and automatically, and must be tested and exercised regularly if it’s to be relied upon. Plus, not all manufacturers’ equipment is capable of integrating both generator and grid inputs.\nThe third approach is to install a generator and transfer switch as a backup source during outages without tying the systems together. The disadvantage is that switchover to the backup source is not instantaneous, and with batteries eliminated, the generator must run continuously whenever backup power is needed. Advantages include the simplicity of separate, conventional systems, each supported as necessary by its separate provider and each optimized to a single task. An entire market has developed for generators for residential standby use.\nAll three approaches are considerably more costly than batteryless systems, for two main reasons. The first is simply the added hardware and complexity. The second is the cost of modifications to a home’s wiring. Since none of these approaches typically has the capacity to run an entire home, key loads are identified as critical to operate during an outage. These usually include some lights, a heating source, refrigerator and freezer, and communications (computer, television, and Internet). Key circuits must be identified and moved from the home’s main breaker panel to a “critical loads” panel to be fed by the backup source. Unless specifically planned during the home’s original construction, this is a labor-intensive job for an experienced electrician.\nIf a grid outage is no more than an inconvenience, install a batteryless system and enjoy the occasional meal by candlelight when the grid goes down. If power during an outage is critical and typical outages are infrequent and of short duration, a grid-tied system with battery backup is a good choice. If an outage would be potentially catastrophic, such as a winter ice storm that can cause power to be lost for days in subfreezing weather, the battery and PV recharge capacity of a battery-based grid-tied system will be insufficient, and a backup generator will be a wiser choice. Consult with a reputable local RE installer or generator dealer to determine the best approach for your needs.","Standby Generator Basics\nStandby generators are a critical element to any high availability power system. Data centers, network closets, hospitals, campuses, manufacturing floors – all of these loads require maximum uptime. Uninterruptable Power Supply (UPS) systems provide battery power for a few minutes, but on-premise power generation is required for long-duration outages. In most cases, that power source is a standby generator.\nThree Most Common Generator Types\nThe first step in selecting a generator is to understand which type of generator you need. The three most common types of generators are Standby, Prime, and Continuous.\nStandby power generators are most commonly used in emergency power applications when another continuous power source, such as the utility, is the primary source of power. As the name suggests, the generator is on standby ready to take over in the event that the utility or other primary power source is no longer available. Standby generators are recommended for use only during a power outage or for required maintenance and testing. Standby generators are the most common generators for Tier I and Tier II data centers.\nPrime generators are considered to have an unlimited run time and can be used in both standby applications or as the primary source of power. Prime generators usually include an overload capacity of around 10% for short durations and are durable enough to provide maximum power to a variable load for an unlimited number of hours. Tier III and Tier IV data centers require prime generators.\nContinuous power generators are similar to prime, but generally supply power to a constant load. Continuous generators are not rated for overload conditions or variable loads. Because of these limitations, continuous power generators are rarely used in data center or critical applications where utility power is available.\nThe Standby Generator System\nThe generator system, often referred to as the genset, includes the standby generator, the automatic transfer switch (ATS), and the output power distribution. The ATS is fed by both the utility and the generator with the utility as the preferred source. When the ATS detects a problem with the utility power source, it automatically commands the generator to start and switches the power feed to generator.\nBecause the generator requires anywhere from a few seconds to several minutes to supply consistent power to the load, an uninterruptable power supply (UPS) protects the load with battery back up from the time utility power is lost until the generator can supply clean and consistent power.\nHow a Standby Generator System Works\nWhen the UPS detects an interruption in power, within milliseconds, it provides immediate battery back-up to the load. So, as far as the protected equipment is concerned, there is no power interruption and existing processes and computation are not interrupted. Meanwhile, the ATS has detected the utility outage, has commanded the generator to start and has switched the power source to generator. Once the UPS recognizes the generator as a stable source, it starts to transfer the load off battery and onto the generator. This is done in sequence and carefully managed by both the switchgear and UPS firmware, so the load is not simply dumped onto the generator risking overload. When the utility power is restored, the ATS and switchgear transfer the load back to utility and the generator is commanded to power down.\nThree Most Common Standby Generator Systems\nIn most cases, the standby generator system will fall into one of three categories – Emergency Standby Generator System, Multiple Isolated Standby Generators, Multiple Generators Operating in Parallel with Utility System.\nEmergency Standby Generator System\nThe emergency standby generator system is the simplest type of genset. There is a single utility source, a single generator source, and one ATS. In this case, the load is either supplied from the utility or the generator. The generator and the utility are never connected together. This type of generator system improves reliability but does not provide any redundancy in the event the generator doesn’t start properly. This basic emergency standby generator system may or may not include a UPS and is most common in smaller, less critical applications.\nMultiple Isolated Standby Generators\nIn critical applications when redundancy is required, the multiple isolated standby generator system is more common. In this configuration, multiple generators are connected to a paralleling bus feeding multiple transfer switches. This configuration is often used when N+1 or 2N redundancy is required. Multiple generator systems have a more complex control mechanism as the units need to share the load under normal operating conditions and be synchronized and paralleled together. This is typically done through programmable logic controllers (PLCs) in the switchgear. Because of the high criticality of the IT load, multiple isolated generator systems almost always include at least one UPS.\nMultiple Generators Operating in Parallel with Utility System\nWhile the emergency standby generator system and multiple isolated standby generator systems both require that power be supplied by either the utility or the generators through operation of one or more ATSs, when multiple generators are operating in parallel with the utility system, the generators are in parallel with the utility. This is most common in applications where load shedding or peak shaving is required. The electric utility will likely have strict interconnection requirements for paralleling with the utility and the system requires a complete and complex protection and control scheme.\nHow to Choose a Generator for a Data Center\nWhile you will need an electrical engineer to assist you in determining the exact specifications for your generator, ATS and/or switchgear, in most data center applications, you will find either standby or prime diesel generators in either single or multiple isolated systems. In other words, not in parallel with the utility. While gasoline, natural gas, liquid propane, steam and other fuels are available, diesel generators remain the top choice because of the quick start-up and power of the internal combustion engine. Additionally, diesel remains one of the most readily available and economic fuel types.\nFactors to consider when choosing a generator – How to size a generator\nIn most commercial applications, the general rule of thumb is to size the generator at 2x the UPS load while factoring in the PUE. The rationale is that the generator capacity beyond what is required for the load connected to the UPS will provide enough power for the mechanicals, cooling, and other critical building systems.\nMore specifically, start by calculating the target power usage effectiveness (PUE) of the data center. This is the ratio of power delivered to the facility to power delivered to the IT load. A PUE of 1.2 is considered very efficient. Let’s say the site has a 500kVA UPS with a PUE of 1.2. If you take the UPS load and multiple times 2, you get 1000kVA. Multiply that by the PUE of 1.2, you have 1200kVA. From there, you would round up to the next nominal generator capacity which may vary by manufacturer, but in this case, would likely be 1250kVA.\nFactors to consider when choosing a generator – Generator System\nWhen deciding whether to use a generator system with a single generator, or one with multiple generators, the biggest question to ask is whether or not you need redundancy. While generally this is a question left to the data center operator and senior management, some facilities such as hospitals or college campuses have regulatory rules in place than mandate a certain level of redundancy.\nIf a single generator system is adequate, it is worth considering sizing the battery capacity of the UPS for some additional run time to allow extra time to initiate graceful shut-down of non-critical loads in the event the generator doesn’t come online as quickly as expected.\nFactors to consider when choosing a generator – Generator Type\nIf the design calls for a single emergency generator, then a standby generator is likely the most appropriate for your application. For highly critical loads and redundant configurations, a prime generator may be preferred or even required depending again upon industry regulations and the Uptime Institute data center tier you are striving for. Tier III and Tier IV data centers require prime generators and a minimum of N+1 redundancy.\nFuel source is also an important consideration. As mentioned earlier, many fuel types are available, but diesel remains the gold standard for data center applications because of its short start-up time, power, and reliability. For highly critical applications, duel redundant fuel sources may be considered.\nFactors to consider when choosing a generator – Environmental\nAside from choosing the generator and generator system itself, other factors to consider when designing a power system are:\n- Exhaust and local EPA regulations\n- Noise and local noise ordinances\n- Aesthetics and any zoning regulations that may require hiding the generator from plain site\n- Foundation or padding size and type\n- Genset derating due to high altitudes or excessive ambient temperatures\n- Fuel storage tanks and hazardous waste considerations\n- Environmental hazards that can block or contaminate the intake and/or exhaust such as heavy snow or salty air near the coast\nStandby Generator Basics: Conclusion\nThere are many different things to consider when selecting a generator. Each configuration is unique and requires careful planning and consideration. It’s worth emphasizing that this article is intended as a general guideline only. An electrical engineer familiar with the building power distribution system is the most qualified professional to properly size the generator for your requirement. At a minimum, the engineer will require an electrical one-line of the power system to begin sizing and configuration of the generator and ATS or switchgear.\nPower Solutions LLC can assist with the process. Our team has successfully engineered, furnished and installed generators and power systems for customers all over the country. Power Solutions LLC is a national vendor-agnostic solutions provider of power products and services for IT, manufacturing facilities, and telecommunications applications. As a value-added reseller for more than 20 different manufacturers, we help you configure the best solution for your specific application. With access to such a wide range of power products and services for both AC and DC applications, we offer customized turn-key solutions that simplify project execution and procurement with a single point of contact."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:30e1a249-2d6c-48be-b931-6fcdd2d9273d>","<urn:uuid:25212dc9-dcb7-42ba-89a8-8e22c8a59536>"],"error":null}
{"question":"Good morning! What two bacterial species were studied in the subgingival biofilm model using Raman microscopy?","answer":"The study examined two early colonizing species of bacteria: Actinomyces denticolens and Streptococci oralis. 300 spectra were collected from a mono-species biofilm for each bacterial strain.","context":["In order to aid their survival, microorganisms form biofilms on surfaces. Gingivitis and other periodontal infections can be caused by bacterial accumulation and biofilm formation in subgingival regions. Oral disease mechanisms can be understood better by studying microbial composition and biofilm architecture.\nResearchers at Procter & Gamble and the Fraunhofer Institute for Interfacial Engineering and Biotechnology used the Renishaw inViaTM Raman microscope, and employed confocal Raman microscopy and multivariate analysis to predict and split the two-subgingival bacteria species in a biofilm model.\nSpectra were collected, by researchers, from two early colonizing species of bacteria, namely Actinomyces denticolens and Streptococci oralis. 300 spectra were collected from a mono-species biofilm for each bacterial strain, which showed differences between the species in terms of peak intensities. A principal component analysis (PCA) was then employed in order to separate the two species into two distinct clusters.\nAverage Raman spectra for A. denticolens and S. oralis mono-species biofilms. Spectral differences are represented in the absolute difference spectrum. The PCA score plot shows separation of the two bacterial species. Image Credit: Renishaw plc – Spectroscopy\nAnalysis of A. denticolens and S. oralis distribution. Morphology analysis and Raman analysis were directly compared using the same sample region. Image Credit: Renishaw plc – Spectroscopy\nThe researchers next collected Raman maps from a sample containing\nA. denticolens and S. oralis, once they had clearly separated the two bacterial species using PCA. On a slide, the two species of bacteria were grown adjacent to each other, and confocal Raman spectroscopy was used to map the interface region between them. The bacteria were identified and separated using cluster analysis, which then gave rise to a well-defined edge at the interface between the two species.\nThe researchers also performed a morphological analysis on the same sample regions for validation of Raman mapping. The bacteria were then separated based on their shape (rod-shaped vs round) by this technique. The same clearly-defined boundary between the two bacterial species was revealed by morphology analysis species, validating the Raman analysis.\nConfocal Raman spectroscopy was then used by the researchers to analyze 15 random regions of an artificially grown bacterial biofilm. Cluster analysis was then used to separate A. denticolens and S. oralis in each region and then used to visualize the distribution of each species. The separation of the two bacterial species was validated by comparing the morphological analysis data with the Raman data.\nThe next step was to overlay the images from the two analyses: areas that could not be classified by morphological, or Raman analysis were labeled in blue.\nThe major clusters were correctly identified across all sample regions despite some discrepancies at the edges of the bacterial clusters. It was therefore confirmed that Raman spectroscopy could determine bacterial coverage in a dual-species biofilm.\nAnalysis of A. denticolens and S. oralis distribution in dual-species biofilms. Morphology analysis and Raman analysis were directly compared. Layover images show areas where species classification differed between morphology and Raman analysis (blue). Image Credit: Renishaw plc – Spectroscopy\nIn conclusion, a non-destructive, affordable method for studying biofilms is presented by confocal Raman spectroscopy. Information about biofilm composition and architecture is provided by the chemical fingerprint generated with Raman. Spectra can be effectively separated based on small differences thanks to subsequent multivariate analysis. This therefore shows how Raman spectroscopy could further ameliorate our understanding of oral disease progression.\nThis piece is a summary based on the piece Mapping of a Subgingival Dual-Species Biofilm Model Using Confocal Raman Microscopy, by Kriem Lukas Simon, Wright Kevin, Ccahuana-Vasquez Renzo Alberto, Rupp Steffen. This piece was originally published in Frontiers in Microbiology, 12, 2021; the original piece can be found by following this link: https://www.frontiersin.org/article/10.3389/fmicb.2021.729720\nThe content provided by this article is therefore a summary based on interpretations of the original article made by Renishaw employees. The content should therefore be interpreted as merely summaries and should not be read as a true representation of the author's original work. The licensor does not endorse Renishaw PLC in any way.\nThe original article is copyright © 2021 Kriem, Wright, Ccahuana-Vasquez and Rupp. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY).The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted, which does not comply with these terms.\nThis information has been sourced, reviewed and adapted from materials provided by Renishaw plc - Spectroscopy.\nFor more information on this source, please visit Renishaw plc - Spectroscopy."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:e1bd1314-ce26-4b62-a09f-2dec1d74323a>"],"error":null}
{"question":"How do modern understanding and treatment approaches differ between altitude-related suicide risk and adolescent depression?","answer":"Modern understanding of altitude-related suicide risk focuses on the possible effects of altitude-related hypoxia on mood disorders and mitochondrial dysfunction, with research showing strong correlations between higher altitudes and increased suicide rates. In contrast, the understanding of adolescent depression has evolved to recognize distinct symptoms in youth versus adults, with research showing that combination treatment (medication and psychotherapy) works best for most depressed teens. While altitude appears to be an environmental risk factor for suicide, depression treatment focuses on individual interventions including antidepressants and specialized psychotherapy.","context":["Suicide rates in the United States have shown consistent regional variations and have generally been higher in western states (1). One possible explanation for this relationship is an adverse effect of altitude-related hypoxia in persons with mood disorders. The peak altitudes within each state as well as the altitude of each state capital have been shown to be strongly associated with state suicide rates (2). However, alternative explanations for the significant association between suicide and altitude include relatively higher rates of gun ownership in the western United States (3, 4) and the low population density (5), which is exemplified by the rural nature of the intermountain West (6).\nMood disorders have been strongly associated with suicide in the developed world (7). While the etiology of mood disorders is still being studied, both major depression and bipolar disorder have been associated with mitochondrial dysfunction (8—10). Likewise, individuals with mitochondrial disorders often have an elevated incidence of depressive symptoms (11, 12). Life at higher altitudes provides a mild hypoxic challenge, and exposure to hypobaric hypoxia has been associated with worsening mood (13) that can persist for up to 90 days (14).\nOver time, a number of compensatory changes follow moving to a higher altitude (15, 16). However, we propose that some individuals with mood disorders may not have the same capacity to adapt to the metabolic stress of high altitude. For example, exposure to low glucose media tended to cause down-regulation of electron transport gene expression in lymphocytes of patients with bipolar disorder, whereas these conditions caused up-regulation of electron transport gene expression in lymphocytes from individuals without bipolar disorder (17). Changes seen following reduced glucose may parallel those seen following hypoxia. Animals exposed to hypoxia have demonstrated rapid increases in brain mitochondrial number and protein (18).\nIn this study, we assessed the relationship of county and state altitude in the United States with total age-adjusted suicide rates, firearm-related suicide rates, and non-firearm-related suicide rates. To the extent that altitude of residence was an independent risk factor for suicide, we hypothesized that both firearm and non-firearm-related suicide rates would be correlated with county and state altitude. In addition, with state-level data, we performed a multiple regression analysis of suicide rates with mean altitude, gun ownership rates, and population densities. We hypothesized that altitude would be significantly associated with suicide rate.\nThe Centers for Disease Control and Prevention (CDC) WONDER database (19) was used to extract data sets related to suicide rates in the continental United States collected over a 20-year period (1979—1998). All data sets were age-adjusted, were reported as deaths per 100,000, and included only persons at least 5 years of age. The data sets abstracted data on total suicide rate, firearm suicide rate, and population. Each data set was abstracted on the county level as well as the state level.\nThe county-level abstraction (overall suicide rate, firearm suicide rate, and population) included data sets of suicide rates from each county in the continental United States for which elevation data were available (N=3,108 counties for each data set; four counties were excluded for lack of elevation data). The overall suicide data set included all recorded suicides by county regardless of mechanism of suicide. The CDC identified 490 counties as having unreliable total suicide rates, defined as counties with 20 or fewer cumulative suicide deaths.\nThe firearm suicide data set included only suicides completed using firearms. This data set enabled us to calculate the non-firearm suicides for each county. Forty-one percent of reported suicides in the CDC data were non-firearm suicides. Of the 3,108 counties, 740 were identified as having unreliable data because they had 20 or fewer firearm suicide deaths.\nThe state-level CDC WONDER abstraction also included overall suicide and suicide by firearms data sets (the 48 contiguous states plus the District of Columbia). All firearm suicide data abstracted from the CDC WONDER database were identified as reliable by the CDC—that is, each state had more than 20 suicide deaths over the observed period.\nGun ownership rates for each state were taken from the CDC's Behavioral Risk Factor Surveillance System, which included survey data from 201,881 respondents nationwide (20). For gun ownership, data were available only on a state level.\nPopulation density for each county was calculated from county populations provided by the CDC WONDER database, as reported in the 2000 census (19), and the area in square miles of each county from the Shuttle Radar Topography Mission (SRTM) data set. Since some counties with a high mean altitude may have a low population, we also evaluated the relationship between adjusted suicide rates and the altitude of the most populous county in each state.\nThe SRTM elevation data set, developed by the National Geospatial-Intelligence Agency and the National Aeronautics and Space Administration, was used to calculate the average elevation of each county. SRTM is a global data set created in February 2000 with a spatial resolution of approximately 0.1 km. The spatial extent for this data set covered only the contiguous United States. Four counties were not included in this data set: one county each in Montana and Georgia and two counties in Virginia. These counties were removed from the analysis, which resulted in a total of 3,108 counties. The average elevation of each of the counties was evaluated using zonal statistics in an ArcGIS/ArcInfo 9.3 environment (ESRI, Redlands, Calif.). ArcGIS/ArcInfo is geographical information system software used to calculate data related to mapping and querying geographical databases. The data from the SRTM provided mean elevation calculations for each square kilometer in each county. County outlines provided by ArcGIS/ArcInfo were then overlaid on the mean spatial data to obtain mean county elevation for each county. Data inputs for this analysis used a 1:500,000-scale U.S. counties vector data set and a mosaiced digital elevation model of ∼0.5 km spatial resolution derived from the SRTM data set. State elevations were calculated by taking the mean of the county elevations for each state.\nPearson correlation coefficients were computed to investigate the association between elevation and suicide rate. In addition, a two-tailed multiple regression analysis was performed to analyze the dependent variable, age-adjusted suicide rate. This regression was run with the independent variables defined as the average state elevation, the state population density, and the state gun ownership rate.\nThe following equation describes our model of the multiple regression analysis of state elevation, population density, and gun ownership with respect to suicide rate:\nYsuicide_rate = β1 Xelevation + β2 Xpopulation_density + β3 Xgun_ownership + c,\nwhere β1, β2, and β3 are regression coefficients and c is a constant.\nStatistical significance was defined at an alpha level of 0.05 and two-tailed. R version 2.10 (http://www.R-project.org) was used for the computation.\nTo increase our confidence in the results we present here, we conducted two additional analyses. First, we looked at the relationship between suicide and altitude in the context of a number of possible covariates. We analyzed the following county-level variables: altitude; total number of child psychiatrists; total number of psychiatrists; number of psychiatric, patient care, and hospital full-time staff; percentage of persons in poverty; unemployment rate for persons age 16 and older; per capita income; percentage of persons age 25 or older with less than 9 years of school; percentage of persons age 25 or older with at least high school diploma; percentage of persons age 25 or older with 4 years or more of college; population density; male population ratio; white male population ratio; white female population ratio; and divorced female population ratio. These data were obtained from the Area Resource File 2008 (U.S. Department of Health and Human Services, Health Resources and Services Administration, Bureau of Health Professions, Rockville, Md., 2009) and analyzed using a normalized regression method.\nSecond, in order to address possible cultural factors, we analyzed suicide rates by county, as a function of elevation, in South Korea. South Korea was selected not only because it is culturally quite different from the United States, but also because it has both a variable geography with many mountainous areas and a relatively high suicide rate (N. Kim and P. Renshaw, unpublished 2010 data). Elevation data were extracted from SRTM (21). County boundary data were obtained from the Korean National Geographic Information Institute (21). Suicide data for a 4-year period (2005—2008) were obtained from Statistics Korea (22). All 233 South Korean counties were included in this analysis.\nAs shown in Figure 1 and Table 1, there were significant positive correlations between age-adjusted suicide rate and county elevation in the United States. Both firearm suicide rate and non-firearm suicide rate were positively correlated with county altitude. All correlations were run using the CDC WONDER data set restricted to counties with reliable values as well as the complete data set. The reliable values and the complete data set correlations did not differ significantly. A correlation analysis based on state-level data showed that elevation (r=0.79, df=47, p<0.001) had a more significant association with age-adjusted suicide rate than did gun ownership (r=0.49, df=47, p<0.001). A negative correlation was observed between suicide rates and population density (population per square mile) on a state level (r=−0.36, df=47, p=0.010), which is consistent with results from previous studies (6).\nScatterplots of Suicide Rates and Average Elevation of Counties in the Continental United Statesa\na Includes the 48 contiguous states and the District of Columbia; excludes counties with fewer than 20 cumulative suicide deaths and counties for which elevation data were not available (N=4).\nb Overall suicide rate by elevation (r=0.51, N=2,618, p<0.001).\nc Suicide rate of death by firearms by elevation (r=0.40, N=2,368, p<0.001).\nd Suicide rate of non-firearm deaths by elevation (r=0.32, N=2,368, p<0.001).\nCorrelations Between Elevation and Age-Adjusted Suicide Rates in U.S. States and Counties\n| Add to My POL\n|Age-adjusted suicide rateb||0.51||<0.001||0.79||<0.001|\n|Age-adjusted firearm suicide ratec||0.41||<0.001||0.64||<0.001|\n|Age-adjusted non-firearm suicide ratec||0.32||<0.001||0.53||<0.001|\nFor the most populous county in each state, there was a strong association between suicide rates and altitude (r=0.72, df=47, p<0.001). This observation suggests that the relationship between suicide rate and altitude does not derive solely from data from counties with high altitudes and low populations. For the 50 counties with the highest suicide rates, there was only a weak, nonsignificant association between suicide rate and altitude (r=0.07, df=48, p<0.626). This is most likely due to the limited range in suicide rates across this small sample. In terms of size, these counties tend to be smaller and to have a lower population density (Table 2). This underscores the importance of including population density as a covariate in these analyses. Since the CDC WONDER criteria for reliable suicide data included a minimum number of deaths per county, counties with very small populations were excluded from the analysis.\nU.S. County Population and Population Densitya\n| Add to My POL\n|Counties||Population||Population Density (Population/Square Mile)|\n|The 50 counties with the highest suicide rate||380,000||720,000||228||690|\n|The most populous county in each state (N=49)||19,100,000||25,900,000||45,100||95,100|\n|All countiesb (N=2,618)||1,470,000||4,800,000||4,032||27,000|\nThe multiple regression analysis using state elevation, population density, and gun ownership as independent variables showed that elevation was a significant factor when based on state-level data (r=0.004, p<0.001). Surprisingly, in this multiple regression, population density was not significant and gun ownership was barely significant (r=0.062, p=0.017). The overall model, though, had an adjusted R2 value of 0.687 (p<0.001), suggesting that these three variables captured much of the variance in suicide rates across states.\nTable 3 presents normalized correlation coefficients from a multiple regression model. In this analysis, county altitude had one of the highest normalized beta values among the variables included in the model. Notably, the number of psychiatrists per county was not a significant risk factor in this model. In addition, divorced white women appeared to be at especially high risk of suicide, as has been previously demonstrated (23). Estimates of suicide rate for this analysis were derived from relatively brief sampling periods (3—6 years), which likely explains the somewhat lower adjusted R2 value for the model.\nNormalized Correlation Coefficients of Covariates (β Coefficients) for Suicide Rates in U.S. Counties\n| Add to My POL\n|Measure||Overall Suicide Rate, 1998—2000 and 2003—2005a (2,631 Observations)||3-Year Suicide Rate, 2003—2005a (2,781 Observations)||3-Year Suicide Rate, 1998—2000a (2,770 Observations)|\n|Total number of child psychiatrists, 2005||−0.087||−0.069||−0.047|\n|Total number of psychiatrists, 2005||0.034||0.027||0.024|\n|Number of psychiatry, patient care, and hospital full-time staff, 2005||0.073a||0.043||0.046|\n|Percentage of persons in povertya||0.142*||0.075*||0.100*|\n|Unemployment rate for persons age 16 and oldera||−0.027||0.010||−0.057*|\n|Per capita incomea||0.117*||0.058||0.057*|\n|Percentage of persons age 25 or older with less than 9 years of school, 2000||−0.228*||−0.167*||−0.107*|\n|Percentage of persons age 25 or older with at least a high school diploma, 2000||−0.135*||−0.136*||−0.066|\n|Percentage of persons age 25 or older with 4 years or more of college, 2000||−0.139*||−0.032||−0.128*|\n|Male population ratioa||−0.030||0.011||−0.041|\n|Female population ratioa||−0.047||0.039||−0.009|\n|White male population ratioa||−0.202||0.050||−0.005|\n|White female population ratioa||0.306*||0.043||0.082|\n|Divorced female population ratio, 2000||0.534*||0.492*||0.637*|\nThe analysis of suicide data and county altitude in South Korea showed a strong association between suicide rate and altitude (r=0.39, df=231, p<0.001).\nOur results are consistent with previous reports of increased suicide rates among gun owners (4, 24, 25) and rural residents (6). In addition, the association we observed between altitude, county of residence, and overall suicide rate replicates and extends earlier observations (2). Unique to this study is an evaluation of the relationship between overall suicide rates, firearm-related suicide rates, and non-firearm-related suicide rates and their relation to altitude of residence. Data on gun ownership were not available on a county basis (4). However, when altitude, gun ownership, and population density were considered as predictor variables for suicide rates on a state basis, altitude appeared to be a significant risk factor.\nDespite the very strong associations between suicide and altitude of residence, the data we present here should be interpreted cautiously. As recently reviewed by Hawton and van Heeringen (26), suicide rates are known to vary with age, sex, ethnicity, socioeconomic factors, psychiatric illness, and family history. Tondo and colleagues (27) have also emphasized the important role that health care access plays in variation in suicide rates. However, in our analyses, we found that altitude remains a significant risk factor even when additional risk factors were included in the model.\nCultural factors, too, are known to influence suicide rates (28). However, in this context, it is notable that suicide rates in South Korea also demonstrated a strong association with local altitude. Nonetheless, given these multiple risk factors, ecological studies of suicide tend to provide weak evidence for causal association. Most people who commit suicide have psychiatric disorders, especially mood and substance use disorders (29), and further research will be needed to clarify the effects of altitude on the course of mental illness.\nThe authors thank Mendel E. Singer for his critical review of this manuscript.","Depression in Children and Adolescents\nAre our children depressed? About 11 percent of adolescents have a depressive disorder by age 18 according to the National Comorbidity Survey. Girls are more likely than boys to experience depression. The risk for depression increases as a child gets older. According to the World Health Organization, major depressive disorder is the leading cause of disability among Americans age 15 to 44. Because normal behaviors vary from one childhood stage to another, it can be difficult to tell whether a child who shows changes in behavior is just going through a temporary “phase” or is suffering from depression.\n• People believed that children could not get depression. Teens with depression were often dismissed as being moody or difficult.\n• It wasn’t known that having depression can increase a person’s risk for heart disease, diabetes, and other diseases.\n• Today’s most commonly used type of antidepressant medications did not exist. Selective serotonin reuptake inhibitors (SSRIs) resulted from the work of the late researcher Julius Axelrod, who defined the action of brain chemicals (neurotransmitters) in mood disorders.\n• We now know that youth who have depression may show signs that are slightly different from the typical adult symptoms of depression. Children who are depressed may complain of feeling sick, refuse to go to school, cling to a parent or caregiver, or worry excessively that a parent may die. Older children and teens may sulk, get into trouble at school, be negative or grouchy, or feel misunderstood.\n• Findings from large-scale trials are helping doctors and their patients make better individual treatment decisions. For example, the Treatment for Adolescents with Depression Study found that combination treatment of medication and psychotherapy works best for most teens with depression.\n• The Treatment of SSRI-resistant Depression in Adolescents study found that teens who did not respond to a first antidepressant medication are more likely to get better if they switch to a treatment that includes both medication and psychotherapy.\n• The Treatment of Adolescent Suicide Attempters study found that a new treatment approach that includes medication plus a specialized psychotherapy designed specifically to reduce suicidal thinking and behavior may reduce suicide attempts in severely depressed teens.\n• Depressed teens with coexisting disorders such as substance abuse problems are less likely to respond to treatment for depression. Studies focusing on conditions that frequently co-occur and how they affect one another may lead to more targeted screening tools and interventions\n• Although antidepressants are generally safe, the U.S. Food and Drug Administration has placed a warning label on all antidepressant medications. The warning says there is an increased risk of suicidal thinking or attempts in youth taking antidepressants. Youth and young adults should be closely monitored especially during initial weeks of treatment.\n• Studies focusing on depression in teens and children are pinpointing factors that appear to influence risk, treatment response, and recovery. Given the chronic nature of depression, effective intervention early in life may help reduce future burden and disability.\n• Multi-generational studies have revealed a link between depression that runs in families and changes in brain structure and function. This research is helping to identify early indicators that may lead to better treatment or prevention.\nKeep in Mind\nWith medication, psychotherapy, or combined treatment, most youth with depression can be effectively treated. Youth are more likely to respond to treatment if they receive it early in the course of their illness.\nSource: National Institute of Mental Healthhttp://www.nimh.nih.gov"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:8a8ee738-d991-45ed-96f2-8ef49cedc60f>","<urn:uuid:9489143f-8c16-4460-88f3-764f373546af>"],"error":null}
{"question":"What are the early warning signs that an ash tree is infested with emerald ash borers?","answer":"The noticeable signs of emerald ash borer infestation include loss of foliage and new shoots appearing on the trunk of the tree. Later signs include bark cracking open or bulging near where the larvae have been feeding.","context":["Here in Montreal, there have recently been a lot of tree losses due the emerald ash borer (EAB). When walking around my neighbourhood, my heart breaks when I see big, beautiful ash trees flagged for removal because of the wrath of the EAB. All this had me wondering: how did it all start and how do we stop it? Logically, if it is killing these trees at such an alarming rate, then it can’t possibly be natural, or at least not natural for this part of the world.\nThe EAB has been a problem in North America since it was first noticed, in Detroit and Windsor back in 2002, but of course, the insect probably arrived on the continent, several years earlier. The pest is native to Asia, living in and feeding off trees of similar genus or family to the ash tree. It is not considered a pest in Asia because Asian ash trees tend to be more resistant and the pest occurs at lower densities, however, its behaviours and impacts are much different in North America. The EAB most likely arrived in solid wood packing material that was being imported to North America. Finding the ash tree as a suitable host, it was able to establish and multiply, causing increasingly devastating losses to ash tree populations. This is particularly problematic for urban areas because ash trees are frequently planted in urban forests due to their shaded-providing and aesthetic properties.\nThe adult EABs are not the ones that cause significant harm the trees, it is actually their larvae. They live underneath the bark, feeding off the internal fibres of the tree, specifically, the phloem and the cambium of the tree, impeding water and nutrient supply to outer parts of the tree, causing it to die within 3 years.\nIt is difficult (unless you are a professional) to notice if an ash tree is infested before it is too late for treatment. Some noticeable signs are loss of foliage and new shoots appearing on the trunk of the tree, which is the tree’s natural response to loss of foliage. Closer to the end of its life, the bark of the tree will crack open or bulge near to where the larvae have been feeding.\nOnce a tree is infested, in can either be treated or cut down. Treatment is only possible for larger ash trees (15 cm or more in diameter) that have lost less than 30% of their foliage. In urban areas, smaller trees and trees that are too far gone, must be cut down as there is no other way to save it and they are at risk of falling down suddenly and harming someone.\nCutting down all infested trees, is not an effective solution to eliminating the EAB problem, as the EAB will just find another host tree. There is still a lot of research being done on this front, and it is suspected that biological controls and natural tree resistance can be possible contributors to the control or eradication of the EAB in North America. Biological controls have a long history of causing more problems than they solve, so it is possible that with population control via treatment of infested trees and regulations on transported materials, the problem will eventually be solved as ash trees gain natural resistance.\nHere are some pictures I’ve taken of local ash trees that have been seriously infested:"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:7512ce80-7c9f-4a8b-b04e-13cbfef499aa>"],"error":null}
{"question":"What's the deal with zoning exceptions in cities - how do they handle unique cases, and what impact does this have on urban sprawl? 🤔","answer":"Cities handle unique zoning cases through a system of exceptions and specific codes. For instance, in Ontario, applicants can request lifting of prescriptive rules, leading to 'zoning inflation' - the increasing creation of unique zoning rules. Ottawa's GIS system tracked an increase from 3821 to 3997 unique zoning codes in just 10 months. As for urban sprawl impact, this relates to how cities expand. The Halifax Regional Municipality demonstrates how uncontrolled zoning can contribute to sprawl, with only 16% of population growth occurring in the urban core versus 56% in suburbs. This sprawl pattern results in higher municipal costs, requiring more infrastructure over larger areas and increasing service delivery expenses. The financial impact is significant - in Halifax's case, changing urban growth targets could save $1.7 billion over 20 years.","context":["An interesting feature (bug?) of Ontario's land planning rules is how prescriptive our zoning rules can be. For instance as the applicants for D08-02-16/A-00037 (at Committee of Adjustment) have discovered, yes, you can have a reception area in a commercial building but no, it cannot be within 6 metres of the front wall of the building.\nIf we had fixed rules for fixed reasons that would be the end of the line. But we don't have fixed rules for fixed reasons. We have zoning. This is not the end of the line; the next step is to ask for the very prescriptive rule to be lifted. I can't see how D08-02-16/A-00037 will be denied and I'm fine with that.\nBut then what happens? Well the thing that happens is a thing called zoning inflation: the rate at which unique and single-instance zoning rules are created.\nThe good news is the City of Ottawa has a fantastic GIS system that can keep track of all of these zoning exceptions. The bad news is it has more and more unique zonings to keep track of. Is that a good thing?\nIn April 2015 I obtained a snapshot of the zoning database and there were 3821 unique zoning codes. Today there are 3997, a 4.6% increase in 10 months. Let's randomly look at a few of these new zoning codes.\nThis is a traditional mainstreet (TM) zoning specific to the Wellington Street Subzone (11) with exception 2297 \"No broadcasting antenna or external public address system is permitted in association with a broadcasting studio\", created by By-law 2015-329 after Council approved the rezoning application.\nThis is a residential 4th density (R4) zoning specific to the Retirement Home & Rooming house subzone (E) with exception 1137, but I can't figure out what the \"-c\" means. The exception is below. I can't easily find how this unique combination of zoning rules was concocted.\nminimum southerly interior side yard setback for the existing building and any addition thereto must be 0.6 m plus the sum of: (i) 0.7 m per storey over the second storey; and (ii) 0.2 m per metre of building length in excess of 21 m parallel to the interior side lot line, provided that where access to parking is provided in an interior side yard, the minimum width of the said interior side yard must be the greater of 3 m or the above combination of building height and length\nI didn't pick these zoning codes for any reason; they just sorted themselves to the top of the spreadsheet I'm working from that compares the frequency and total area of the zoning codes between April 2015 and today.\nThis story isn't about any particular changes, or their merits. At the individual level all of the requests are likely reasonable and result from individuals asking for a specific rezoning either at council or Committee of Adjustment.\nThis story is about what happens at the aggregate level.\nTo the database-mobile!\nBiggest loser: the zoning code that lost the most, in terms of total area moved to another use, is Rural Countryside Zone (RU), though it only lost 0.94% of its total. This sounds like sprawl in action. I'll be interested in seeing the march of RU being converted to other things over time (which is why I grab snapshots from time-to-time).\nBiggest winner: meanwhile Environmental Protection Zone 3 (EP3) gained the most in absolute area, with an increased coverage of 2.3%. People can still build homes in EP3. I wonder what the previous zoning was on these lands? I could check, but that's a story for another day.\nBiggest newbie: looking at the new zoning codes, the one with the largest total area is RU[806r]. I'm going to guess, in lieu of actually confirming, that this land was previously just RU and someone carved out a new exception. On the Rural Exception list 806r is listed as:\nFor a period of one day, being June 27, 2015 the following applies:\n(i) the uses and provisions of exception 286r apply; and,\n(ii) theatre is a permitted use.\n-On expiration of the temporary zoning, the lands subject to exception [806r] are rezoned back to RU[286r].\nHa ha ha. This story started with a boring title, but after stumbling across this beautiful nugget, I have just renamed it \"Zoning inflation: the Shania Twain Effect\". Ottawa Citizen has the explainer.\nAnother fun fact is the zoning death is written into the enacted by-law already. So if I do this story again sometime (now I have to), we can expect to find RU[806r] listed as one of the biggest losers in total area as the zoning reverts to whatever it was before.\nBiggest zombie: looking at zoning codes that have disappeared, the one with largest total area is RU[286r]. So it looks like the Twain lands came from 286r in the first place? I'll skip to the next one so it's not All Twain All the Time.\nThe second biggest dissappearing zone code is RH[787r]-h, which is Rural Heavy Industrial Zone (RH) exception 787r and no idea what \"-h\" means. It concerns a solid waste disposal facility. I'll spare you the cut-and-paste of the exception which you can read here. My guess is the design of whatever they wanted to build didn't meet the setback requiements, so they carved out new setbacks FTW.\nOh the complex web we weave.\nLet's step back for a second and marvel at how the city purrs along doing its thing.\nPeople want to build things or do things and discover they aren't within the rules. They apply for changes. The changes are approved. Staff write the by-laws that give effect to the approvals. Those by-laws receive three readings at a subsquent Council meeting. News of this makes its way down into the bowels of government and people peck at keyboards updating the published zonings as well as updating the GIS systems and who knows what else.\nNobody should ever say government is broken or it doesn't work. It hums very nicely doing it's thing. It's a marvel.\nDestiny or fate?\nI'll end the story with, well, no opinion.\nIs marching towards ever more granular zoning definitions a normal and positive outcome of good planning systems? Or should the city (and probably the province) introduce some corrective measures, perhaps allowing that re-zoning applications should be applied more broadly, saving future applicates time and money?\nIf a reduced setback of 0.6m is good for the goose, maybe apply it automatically to the gander, and save the gander $15k?\nI don't know, but it's food for thought.\nHow long will it be until every spec of land in Ottawa is its own special zoning snowflake?\n Zoning codes sometimes present with a \"*_\" prefix which I've eliminated for the purpose of this review, so \"*_R4\" is treated as just \"R4\" in my calculations.","It has become painfully obvious to many if not most observers that Halifax has a sprawl problem. The Halifax Regional Municipality covers an area almost the size of Prince Edward Island, and the city seem to be spreading out rapidly to fill those borders. The Halifax Regional Municipality has been experiencing significant population growth in its suburbs, while population in the downtown cores of Halifax and Dartmouth continues to stagnate.\nThe sheer size of the HRM makes our population density unusually low, at 71 persons per square kilometer. However, even when we look at urban areas of HRM alone, the density is 1106.4 people per square kilometre, which is still significantly lower than comparable cities like Kitchener-Waterloo, London (Ontario) and Regina, slightly less than Victoria, and far below densities of comparable great worldwide. If Halifax aspires to be a great city, then we need to start thinking like a great city, and growing in a more focused, sustainable way.\nHRM's Regional Plan, which is supposed to set the framework for sustainable growth in the HRM for the next 25 years, adopted a target that 25% of population growth should take place in the urban core (essentially Halifax, Dartmouth and Bedford) while 50% should take place in the suburbs, and 25% in rural areas. While some would argue that set the bar too low for urban growth, in the first five years of the plan we fell short of even that modest goal, with only 16% of population growth taking place in the urban core, while 56% took place in the suburbs and 28% in rural areas. In other words, the city is expanding into the surrounding countryside at a rate that is not meeting even a modest target for sustainable growth. This makes Halifax a textbook example of sprawl.\nWhat is sprawl and why is it undesirable?\nSprawl has been described as a \"land use pattern of single-use zones,\ntypically made up of subdivisions, office parks, shopping centres’\nstrung together by arterials and highways.\" While it is sometimes referred to as \"suburban sprawl\", the problem is not suburbs themselves, but rather the separation of residential, commercial and retail areas into large, isolated, single-use zones which can only be reached by driving, rather than compact mixed-use walkable neighbourhoods. It is typically characterized by lower population density.\nSprawl is problematic from a number of different standpoints, notably its financial and economic costs, its health and environmental impacts and its social dimensions.\nIn terms of financial costs, sprawl requires a municipality to provide services to the same number of taxpayers over a larger area. This requires not only more infrastructure (more pipes, more roads, etc.) but increases soft costs as well: police officers, garbage haulers, and buses, all have to cover a larger area, resulting in increased costs to the taxpayer.\nThis has a very real impact on the municipal budget. A recent study commissioned by the HRM shows that changing the urban growth target from 25% to 50% of the population increase would save the municipality $1.7 billion over the next 20 years.\nIn other words, to continue to service sprawl, the HRM will either have to charge higher taxes, or decrease service levels just to break even. Sprawl imposes such high costs, that observers have linked it with municipal bankruptcies in the US and elsewhere. It has been further linked to higher housing costs and even as a partial cause of the foreclosure crisis in the US.\nSprawl also imposes economic costs. For example, because of it's dependence on single-occupant vehicles, sprawl is also associated with gridlock, which costs the Canadian economy $10 billion a year.\nSpawl is also associated with a number of environmental and health problems. An ever expanding city can result in loss of wildlife habitat and greenspace, which is essential for both environmental and human health. It contributes to depletion and degradation of water sources. The focus on single occupant vehicles increases reliance on fossil fuels and greenhouse gas emissions. And it increases air pollution and sedentary lifestyles, leading to rising health problems and costs.\nFinally, sprawl is associated with certain social problems, including loss of community, income inequality, and diminished social services. Sprawl has been accused of lacking \"quality of place\" and creating social disconnection.\nWhat is density and why is it desirable?\nOur HRM Alliance defines density as \"the number of people per area\". Statistics Canada defines an \"urban density\" as 400 people per square kilometer. Spacing Magazine identifies some of the benefits of density as follows: \"Residential density[...] is one of the most important characteristics of urban areas. High densities create vibrant streets, support main street commercial areas, and encourage walking, biking and transit use.\"\nDenser cities avoid many of the problems created by sprawl, as detailed above. Municipalities are able to keep costs (and taxes) down by providing services over a smaller area. More walkable cities avoid many of the costs of gridlock, and reduce air pollution and greenhouse gases. Wild spaces and water can be preserved, providing habitat for wildlife, as well as recreational opportunities for people. Finally, denser cities often provide tighter, more closely knit communities and neighbourhoods. This has led some observers to link density with increased innovation.\nWhy is sprawl becoming a hot-button issue in Halifax?\nAddressing the issue of sprawl has created strange political bedfellows. 42 groups as diverse as the Downtown Halifax Business Commission, the Ecology Action Centre, Fusion Halifax, the Heart and Stroke Foundation, the Halifax Trails Association, and the YWCA have banded together as the Our HRM Alliance, dedicated to making the HRM a more livable and sustainable place. In particular, the group has been working through the Regional Plan five year review process to try and strengthen the regional plan to more comprehensively address sprawl through seven solutions, including green belting. They believe that implementation of these solutions will address the problem of sprawl, and help Halifax grow more densely and sustainably. Groups like the Alliance have been doing a great deal to raise the profile of sprawl as a topic for public discussion.\nThe issue of sprawl came to a head recently when regional council rejected the application for the 48 story Skye Halifax development. If built, the two towers would both have stood 150 metres (492 feet) high, making them the by far the tallest buildings in Atlantic Canada. Council rejected the development on the advice of staff and the design review committee, who found that it did not meet the development rules set out in the HRM by Design rules for downtown development.\nMany opponents of council's decision to nix the proposed building suggested that allowing Skye to proceed would have been one way of addressing sprawl. Many questioned the HRM by Design rules, and suggested Halifax needs to \"grow up\" and build taller buildings. Despite this, many vocal opponents of sprawl, and proponents of downtown development, such as the Downtown Halifax Business Commission, supported council's decision. So while many agree on the need to densify and combat sprawl, there is a lack of agreement on whether skyscrapers are part of the solution.\nWon't taller buildings help?\nWhile it would seem obvious that building taller buildings is one way to increase density and address sprawl, real-world experience suggests that is not necessarily the case.\nOn the one hand, New York City, which has 5,818 high-rise buildings, of which 92 are over 600 feet, is quite a dense city by North American standards, with 10,518.60 persons/square kilometer By contrast Paris, which has only 14 buildings over 492 feet (of which only 8 are over 600 feet). Paris has used height restrictions to confined high-rise development to specific areas of the city: in most of Central Paris, there are few buildings over six stories high, with the notable except of the Eiffel Tower. Yet, Paris, with 21,196 persons per square kilometer is around twice as dense as New York.\nHouston, which has far fewer development restrictions than most large cities, has 360 high-rises, and 31 skyscrapers over 492 feet. Yet despite having more tall buildings than Paris, Houston has a major problem with sprawl, with only 1,505 persons per square kilometer. Similarly, Calgary has few development restrictions and has 14 buildings over 492 feet. Yet Calgary as a City has a lower density than Houston, at 1,329 person/square kilometer. It's urban density is not much higher, at 1,554.8 persons per square kilometer. Calgary is having serious problems meeting the costs of servicing an ever expanding area. The problem is so serious, that Mayor Nenshi and the fire department have said the City is having difficulty maintaining municipal fire services to meet national standards.\nCloser to home, Halifax's North End is one of the densest neighbourhoods in Atlantic Canada, with a density of 5,888 people/square kilometer, despite having very few tall buildings at all. Another one of Halifax's densest neighbourhoods is Schmidtville, one of its oldest, and most historic. Schmidtville is a compact, mixed use neighbourhood\nSo density is not merely a function of height, and there are ways to achieve density without building skyscrapers. Some experts have suggested that mid-rise development, and not towers, are the key to densification. Toronto has achieved significant population growth in its urban core in the last several years, due in part to significant mid-rise development, although this has not been without controversy in some neighbourhoods. However, the question becomes how high we go, and what other considerations we want to balance height against. Is the sky the limit, or is there an optimal height for achieving density?\nThere is no question that if Halifax is to increase density and combat sprawl, taller buildings will be part of the mix. However, the experience of other cities suggests that skyscrapers are not necessarily a panacea: the solution will have to be more comprehensive. Many cities have been able to develop densely without many tall buildings, and in some cases with significant restrictions on height. Many cities that have allowed tall buildings still have a major problem with sprawl. Many mid-rise buildings may make a greater contribution than a few high-rises.\nGenerally speaking, it is the cities that place the least rules around development (like Calgary and Houston) that have the biggest problems with sprawl, while cities with clear, considered development guidelines (like Paris) tend to develop more densely. Great cities aren't just grown; they are planned. Whether Halifax becomes the great city it has the potential to be will depend how well our regional plan, downtown plan, and development rules address issues like height and density.\n(This post is the first in a series, and is cross-posted with Spacing Atlantic. In future posts on development in Halifax, I will look at: the rules governing development in Downtown Halifax; the rules around suburban and rural development in Halifax; business and industrial parks like Bayers Lake and Burnside; commercial taxation; and other related topics)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:807e8e6e-d4b0-494b-bc5f-345dff6be386>","<urn:uuid:0eadd55b-5a56-4d82-b545-6e8b29ad0bc4>"],"error":null}
{"question":"Could you please explain how temperature regulation issues in autism compare to sleep regulation problems in narcolepsy? I'm particularly interested in how both conditions affect the body's chemical processes and daily functioning.","answer":"Both conditions involve disrupted regulation mechanisms in the body, though through different pathways. In autism, temperature regulation issues stem from neural feedback mechanisms in the hypothalamus not functioning properly, leading to constant discomfort and difficulty managing body temperature. People with autism may feel too hot or too cold regardless of actual temperature, and their blood circulation may be affected, causing cold extremities. In narcolepsy, the disruption occurs due to low levels of hypocretin in the brain, which affects the sleep-wake cycle. This chemical imbalance prevents proper regulation of sleep patterns, causing random sleep attacks and irregular sleep cycles. Both conditions can significantly impact daily functioning - autism's temperature issues can lead to sensory overload and meltdowns, while narcolepsy causes excessive daytime sleepiness and difficulty completing basic tasks.","context":["#Temperature Changes & #Autism\nThe weather is changing and you may find that the attitude of your favorite person with autism is changing too. Hypo or hyper sensitivity to temperature can affect those on the spectrum. A writer for the Spirit of Autism says, “My son’s refusal to wear a coat in winter I thought was either driven by stubbornness or laziness. But many people with ASD may feel ‘hot’ all the time, even when it’s ‘cold’. Or, they can feel ‘cold’ all the time, even when it’s ‘hot’ weather.”\nIf their nervous system is on ‘high alert’, they may suffer from poor or underdeveloped circulation. Their hands and feet are always cold because their blood leaves their extremities to deliver oxygen to the internal organs and muscles. Unfortunately, the body is unable to regulate and may become overheated or always have cold hands and feet due to thermoregulation issues.\nDavide-Rivera (2015) writes that 'temperature regulation' issues can put a person in a state of constant discomfort. This can lead to sensory overload, shutdowns, and even meltdowns. The body’s core temperature is regulated by neural feedback mechanisms in the brain, which operate through the hypothalamus. A very neutral temperature may be insisted upon by the ASD person’s bath, their food, and even for the room they’re in. Even when taking even moderately hot showers, those who are temperature sensitive may report feeling dizzy, light-headed, or seeing spots. And then when cooling down, they may feel the cold much faster than you would expect.\n< My Thoughts > “…mechanisms in the brain…”\nSometimes when the endocrine system is ‘out of whack’, it’s easy to feel these same kinds of sensations. This may be hard to diagnose without very thorough and expensive tests, which #clinicians and insurance companies tend to want to avoid. Just saying.\nDavide-Rivera (2013) – One of the first things I learned about my #Asperger’s #diagnosis was that there were three core deficits that accompany this condition – the lack of theory of mind, executive dysfunction, and weak central coherence. There were also a host of other issues like #sensory processing difficulties – being #hyper or #hypo sensitive to outside stimuli like to heat, cold, or pain.\n< My Thoughts > “…a host of other issues…”\n#Parents may take years to find out that their ‘wild child’ has really painful sensory issues. Often, children really don’t want to give you a bad time… they’re just having a bad time.\nvan Someren (2016) – Clinicians use the ETSRS questionnaire to narrow down temperature regulation issues. Questions cover differentiating experiences of the individual’s body ‘heating-up’ and ‘cooling-down’, unexpectedly. Some persons with sensory issues only complain about problems with their extremities, and only under certain conditions. Determining whether this happens to them during the daytime or the nighttime. Whether the person is in an upright or supine position, awake or asleep. If they are inside or outside, in summer or in winter; and so on is helpful. These questions are designed to determine which parts of the body seem to be affected. Answering a variety of these types of questions further narrows down the specific #regulation issue.\nDavide-Rivera, J. (2013). Twirling Naked in the Streets & No One Noticed: Growing Up with Autism; eBook Edition.\nDavide-Rivera, J. (2015). Why Does My Child Refuse to Wear a Coat?; Autism Answers, Traits/Behaviors; Retrieved online from – http://aspiewriter.com/2015/07/temperature-regulation-why-does-my-autistic-child-refuse-to-wear-a-coat.html\nvan Someren, E., Dekker, K., et al.(2016). Experienced Temperature Sensitivity & Regulation Survey (ETSRS); Journal of Sleep Research; V25, p125.","Narcolepsy, or “falling asleep disorder,” appears in pop culture more often than it does in day to day life. This does not mean, however, that is a condition that is easy to dismiss.\n- 1 What is Narcolepsy?\n- 2 Narcolepsy Symptoms\n- 3 What Causes Narcolepsy?\n- 4 Diagnosing Narcolepsy\n- 5 Narcolepsy Treatment\n- 6 Conclusion\nWhat is Narcolepsy?\nNarcolepsy is a sleep condition that affects .05% of people in the world, making it a rare condition without reducing its substantial impact on those who suffer from it.\nFolks with narcolepsy are said to suffer from attacks of sleep – which is a simple way of addressing the condition’s impact on a person’s day to day life.\nMore specifically, those who have narcolepsy fall into REM sleep seemingly at random over the course of the day when, a few moments earlier, they could have been entirely awake. It doesn’t matter what that person was doing beforehand, they can’t manage to stay awake.\nYou may find it funny if one of your close friends happens to fall asleep in the middle of your working lunch, but for folks with narcolepsy, even simple tasks like eating or driving can become difficult.\nSometimes, the narcoleptic victim will continue the activity that they were undertaking even while they sleep – such is the case with eating, which is considered an automatic activity. However, if the person in question was driving, they probably won’t be so lucky.\nFolks who experience narcolepsy are also prone to depression, excessive sleepiness, sleep paralysis, hallucinations, and the occasional loss of muscle control. Narcolepsy also increases the risk that a person may suffer from increased anxiety or stress.\nThese conditions (excluding depression, which is a more complex topic) stem from a lack of hypocretin in the body.\nHypocretin keeps folks awake and works along with our circadian rhythms in order to keep us up and going during the sunnier hours of the day.\nNarcolepsy symptoms are said to develop when you are a child, but they can be hard to pin down, as said symptoms can be affiliated with a number of other conditions.\nThat said, if you are narcoleptic, you’ll have experienced a number of the following since you were rather young, and you may have been misdiagnosed in the past.\nIrregular Sleep Cycle\nThough it may seem like an obvious symptom, irregular sleep cycles are part and parcel of life with narcolepsy.\nWhile you may find that you fall asleep at seemingly random times of the day, you may also find that you are unable to sleep peacefully through the night.\nWhether this is due to nightmares or simply because you wake up at odd hours, the lack of sleep and the irregularity of a narcoleptic’s sleep cycle contributes to the overall sleepiness its victims may have to deal with.\nThis disrupted sleep cycle comes as a result of narcolepsy’s disruption of a person’s day to day REM cycle.\nREM, it should be noted, is the fourth stage of sleep and the deepest; the acronym stands for “Rapid Eye Movement,” or the twitching of your eyes that occurs when you start to dream.\nWithout this level of sleep, your health is likely to be compromised, and you’ll have an exceptionally difficult time feeling well rested.\nSpeaking of which, one of the other signs of narcolepsy is extreme sleepiness.\nOver the course of a day, a person with narcolepsy may find that they’re moving through something like a mental fog and that completing their responsibilities at work, around the home, or in school is, in turn, quite difficult.\nThis lack of energy is often attributed to laziness, but with narcoleptics, it is a lacking that exceeds the norm.\nThis sleepiness may also result in trouble with a person’s memory, as short terms memories will be more difficult to form due to the victim’s inability to pay attention to their surroundings.\nCataplexy is, in a number of ways, similar to sleep paralysis, a condition which this article will touch on momentarily.\nHowever, cataplexy is far more an extreme version of muscle weakness. Narcoleptics are likely to experience muscle weakness and cataplexy as symptoms of their condition, especially when they’re experiencing strong emotions.\nThis brand of muscle weakness is said to arise when a person is laughing uproariously, crying, or when a person’s mood swings abruptly.\nPerhaps one of the scarier symptoms of narcolepsy is the potential for a victim to start hallucinating. These hallucinations will seem especially real and can impact a person’s sense of smell, taste, sight, and their ability to hear.\nOften, hallucinations are paired with sleep paralysis, but the lack of sleep that a narcoleptic experiences may also result in hallucinations appearing while a person is walking down the street or going about their day to day responsibilities.\nSleep paralysis is the inability to move or speak when awake. This kind of paralysis often occurs when you’re wide awake and can be scary, especially, as mentioned, if it’s paired with hallucinations.\nFolks who suffer from sleep paralysis also report feeling as though there’s a weight resting on their chest.\nFits of sleep paralysis are somewhat unpredictable and can last anywhere from a few seconds to several minutes.\nWhat Causes Narcolepsy?\nUnderstanding narcolepsy can be somewhat difficult, but luckily, all of its potential causes are rooted in a chemical imbalance, making treatment a little easier to handle.\nLow Levels of Hypocretin\nAt the most basic level, narcolepsy is caused by low levels of hypocretin in the brain. Hypocretin, as has been mentioned, is the chemical that keeps us awake and enables us to be active members of society.\nWhen your body doesn’t produce enough hypocretin, or when the receptors in your brain are unable to accept the hypocretin when it arrives, the result is an altered sleep pattern and, in more severe cases, narcolepsy.\nWhat impacts whether or not your body produces enough hypocretin, though, or what prevents your brain from properly assessing the chemical’s influence on your body? Some additional causes are listed below.\nSome sources, including the National Heart, Lung, and Blood Institute, claim that environmental influences like the presence of heavy metals, pesticides, and weed killers can make it more difficult for your brain to process the chemicals that your body makes.\nIn the case of narcolepsy, it’s been proposed that exposure to these kinds of influences would explain why it would be more difficult for a narcoleptic’s brain and body to produce and interpret hypocretin.\nIt has also been proposed that narcolepsy is a genetic disorder, passed on from parent to child. Research in this area is limited, however.\nIt has been determinedly said, though, that genetics alone do not cause narcolepsy – there must be some other influence at play to bring about the inability of a person’s brain or body to work adequately with hypocretin.\nAs such, if you have narcolepsy and are worried about passing it along to your child, don’t put too much weight on the influence of genes.\nReceiving a concussion, developing a tumor, or having a stroke can all limit the brain’s ability to interpret the purpose of hypocretin once it’s been delivered to the brain.\nThese brain injuries are often more environmentally-oriented, of course, so if you’re playing football or trying your hand at parkour, you may want to be cautious.\nBe sure that while seeking out a diagnosis for what you suspect might be narcolepsy, you take into account your family’s history and see whether or not blood clots or different types of cancers run in your family.\nWhile brain injuries are one explanation for narcolepsy, they are more often than not the result of something far more sinister.\nInfections will naturally make you more tired, as your body will be trying to fight off intruding bacteria and thereby using its resources differently.\nHowever, if you find that you have a tendency to become ill frequently or have been diagnosed with any autoimmune diseases, then you may find that narcolepsy will plague you as a secondary condition.\nIt’s possible, should you suffer from frequent infections or an autoimmune disease, that your body won’t be able to produce the sorts of chemicals it needs to function appropriately – or that the white blood cells in your body will begin to attack more than just the invading bacteria.\nIf you find that you are both excessively sleepy and prone to illness, check in with your doctor in order to get a better understanding of what’s going on.\nLower Levels of Histamines\nIn addition to a lack of hypocretin, though, you may find that you have lower levels of histamines in your blood.\nHistamines, like hypocretin, help you stay awake over the course of a day, and their lack can, in turn, result in an increased need for sleep and an inability to focus.\nHere’s a video explaining more on narcolepsy.\nAs has been mentioned, narcolepsy can look, in terms of its symptoms, like a number of other conditions. As such, it can take between ten and fifteen years to properly diagnosis – not in the least because the disorder is so rare.\nMany doctors may misdiagnose narcolepsy as depression, infection, or other sleep disorders – as well as, in children, a learning disorder, seizures, or general laziness.\nIn order to accurately determine whether or not you have narcolepsy, your doctor needs to consider the aforementioned symptoms, your family medical history, and the impact that your environment may have had on your body.\nMost importantly, a doctor will need to assess how much hypocretin your body produces and whether or not your brain can process the chemical.\nSleep studies tend to be exceptionally helpful during the diagnostic stage, though you may be referred to a sleep specialist and have to stay overnight in a medical facility if that is the case.\nThese sleep studies allow medical professionals to get a better understanding of how much REM sleep you are getting and to what degree your body fluctuates from the norm while you rest.\nDetermining a narcolepsy treatment can be somewhat difficult, as the disorder does not have a cure.\nThe symptoms of narcolepsy, however, can be treated, as is discussed below.\nIt may not be a stretch to encourage you to incorporate caffeine into your diet, but folks who suffer from narcolepsy may need more than the average office employee.\nStimulants like caffeine and other prescription medications will help you stay awake over the course of the day – but make sure to stop taking them at least three hours before you want to fall asleep!\nYou may also want to ask your doctor about hypocretin supplements. After a proper diagnosis, it may be possible for you to try and tend to your body’s inability to produce hypocretin through an artificial substitute.\nAs such, you’ll be able to force your brain to acknowledge the presence of hypocretin in your body and thereby stay awake for a more significant part of the day.\nWhile an odd suggestion, some medical professionals believe that because depression and narcolepsy have similar symptoms, the two can be treated with similar medications. As such, your doctor may prescribe anti-depressants in order to treat your narcolepsy.\nThe physical symptoms of depression (as opposed to the mental) mirror narcolepsy and anti-depressants may allow you to find the energy in your day to see to all of your responsibilities.\nSetting a Routine\nAn adjustment to your lifestyle may also help you combat narcolepsy.\nMake sure that you limit your exposure to electronics before bed, and that you try to incorporate some kind of relaxing activity into your nightly routine.\nCoping With Wakefulness\nIf you find that you’re still waking up at odd times in the middle of the night, do your best to stay in bed and try to fall back asleep for at least fifteen minutes.\nIf this does not work, then feel free to get out of bed and engage in some sort of relaxing activity, like reading, or making yourself a cup of warm milk in order to coax yourself back toward sleepiness.\nWhatever you do, though, don’t have your coffee at 3 am – caffeine will be useful to you during the day, but it’s the last thing you want at night!\nHere’s a video explaining more about diagnosing narcolepsy.\nEven though it is rare, narcolepsy is a serious and complex disorder that impacts a significant number of people.\nWhile an entirely effective treatment is yet to be determined, folks who suffer from this disorder can take comfort in understanding it in more detail and by adjusting their lives accordingly."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5c65db55-a10a-42f1-933f-0d193be950f3>","<urn:uuid:b76b1036-b1c3-46f9-94d4-d1830fc73601>"],"error":null}
{"question":"Could you please compare the origins of Arabic and Akkadian as Semitic languages? I am curious about their historical development.","answer":"Arabic and Akkadian are both Semitic languages but emerged at different times. Arabic is considered the youngest of the Semitic languages and derives from ancient Semitic languages. It was introduced by Arab conquerors and exists in three forms: classical, modern standard, and spoken. Akkadian, on the other hand, emerged much earlier and became prominent around 2,500 BC when the Akkadians, a Semitic-speaking people north of the Sumerians, adopted the cuneiform writing system. Akkadian became the primary language of Mesopotamia around 2,300 BC and evolved into later forms known as Babylonian and Assyrian.","context":["In English, many things are named after a particular country – but have you ever wondered what those things are called in those countries?\n- The alphabet used to write Arabic from the 6th century to the present day is a form of Aramaic script that belongs to the family of Semitic scripts whose ultimate origins go back to Egyptian hieroglyphs.\n- Aramaic was the language of Semitic peoples - the Assyrians, Chaldeans, Hebrews and Syrians.\n- Some are ethnically Arab, which means that they are Semitic, and speak Arabic.\n- Like the vestry that surrounded ancient anointed Semitic kings, the dove was symbolic both of God's approval and the installation of power to his servant for service.\n- Arabic, a Semitic language, was introduced by the Arab conquerors and has three different forms: classical, modern standard, and spoken.\n- Moreover, the heart of the Islamic domain has always been the region closest to Europe… Arabic and Hebrew are Semitic languages, and together they dispose and redispose of material that is urgently important to Christianity.\n- The rest of the population speaks languages drawn from Indo-European, Ural-Altaic, or Semitic language families.\n- Whereas Hebrew and Arabic are Semitic languages, Israeli is a Semito-European language.\n- It is called Geez, an ancient Semitic language used in the Coptic Christian Church.\n- It is a Semitic language somewhat related to Arabic and Hebrew.\n- Arabic, on the other hand, belongs to the Semitic language family.\n- Arabic is a Semitic language related to Hebrew and Aramaic.\n- Arabic, the language of the majority and the official language of the country, is a Semitic tongue related to Hebrew, Aramaic, and Amharic.\n- Diligently, he traces each letter from the ancient Semitic and Phoenician alphabets through Old English and Norman French, exploring shapes and pronunciation.\n- The Semitic mind speaks in terms of beings, beings that embody good and evil.\n- It's a Semitic language, totally unlike English.\n- Aramaic belongs to the Semitic language group historically centred in the Middle East.\n- Dialects much changed in the new surroundings, needless to say - in Amharic, the ancient Semitic word for ‘almond’ now refers to the African groundnut.\n- Arabic derives from the ancient Semitic languages.\n- The Sumerian signs were used to write the Old Akkadian language which was Semitic.\n- For the ancient Semitic world, of course, this watery chaos was the home of the great sea monster, the forces of death and destruction.\n- Both studied in Germany and spent time studying and teaching Hebrew and other Semitic languages; both were especially fascinated by the significance of Arabic for the study of ancient Semitic languages.\n- Arabic, like other Semitic languages, including Hebrew, is based on triliteral roots.\n- Arabic is the youngest of the Semitic languages.\n- Had they come to Israel at the end of the 19th century, Israeli would have been a very Semitic language, just like Arabic and Hebrew.\nEnglish has borrowed many of the following foreign expressions of parting, so you’ve probably encountered some of these ways to say goodbye in other languages.\nMany words formed by the addition of the suffix –ster are now obsolete - which ones are due a resurgence?\nAs their breed names often attest, dogs are a truly international bunch. Let’s take a look at 12 different dog breed names and their backstories.","The Rise and Fall of Sumer and Akkad\nThe Sumerians were the first known people to settle in Mesopotamia over 7,000 years ago. Located in the southernmost part of Mesopotamia between the Tigris and Euphrates rivers (modern day Iraq), Sumer was often called the cradle of civilization. By the 4th millennium BC, it had established an advanced system writing, spectacular arts and architecture, astronomy, and mathematics. The Akkadians would follow the Sumerians, borrowing from their culture, producing a new language of their own, and creating the world’s first empire.\nWho Were the Sumerians?\nThe origin of the Sumerians remains a mystery till this day. They called themselves Saggiga (the \"black-headed\" or \"bald-headed ones\") and their country, Kengi (\"civilized land\"). Some believe they came from around Anatolia or modern day Turkey. Others suggest they might have come from India and were Caucasian in origin. They were established in southern Babylonia, in what is now Iraq, by at least 3500 BC.\nThe origin of the Sumerians remains a mystery till this day. ( swisshippo /Adobe Stock)\nLocated in what the ancient Greeks called Mesopotamia, meaning \"the land between the rivers,\" Sumer was a collection of city-states or cities that were also independent nations, some of which endured for 3,000 years. Beginning around 3500 BC, the Sumerians began to build walled cities, including Ur, the capital of the civilization. Each of these cities contained public buildings, markets, workshops, and advanced water systems, and were surrounded by villages and land for agriculture. Political power originally belonged to the citizens, but as rivalry between the various city-states increased, each adopted the institution of kingship.\n- The Sumerian King List still puzzles historians after more than a century of research\n- Sargon of Akkad: Familiar and Legendary Tales of a Famous Mesopotamian King\n- A Dusty Demise for the Akkadian Empire (New Study)\nThe Sumerian Temple and Key Inventions\nEach city-state was believed to be under the rule of a local god or goddess and their temples dominated the towns architecture. The most famous temple, the Ziggurat of Ur was a three-storied, 15m (49 ft) high building constructed from mud bricks in the form of pyramidal graduated terraces. It formed a complex of temples and included the royal palace. On top of the structure was a shrine dedicated to the god of that city.\nThe nature of the famous Ziggurat of Ur has been described in a previous Ancient Origins article:\n“The Great Ziggurat of Ur was dedicated to the moon god Nanna, who was the patron deity of the city. As the Mesopotamian gods were commonly linked to the eastern mountains, the ziggurat may have functioned as a representation of their homes. Thus, the people of Ur believed that their ziggurat was the place on earth where Nanna chose to dwell […] The people of ancient Mesopotamia believed that their gods had needs just like their mortal subjects. Hence, a bedchamber was provided for Nanna in the shrine on top of his ziggurat. This chamber was occupied by a maiden chosen to be the god’s companion. On the side stairway of the ziggurat’s north western part is a kitchen, which was likely used to prepare food for this god. The god’s mortal servants had to be provided for as well, and the outer enclosure of the ziggurat contained a temple storehouse, the houses of the priests and a royal ceremonial palace.”\nThe Sumerians were among the first known cultures to develop many benchmarks that are used to define a \"civilization”. They are credited with the establishing codes of law, the plow, the sailboat, and a lunar calendar. They also developed a numerical system, based on the number 60 that is still used to measure seconds and minutes. However, probably the most famous legacy is their writing system.\nThe Sumerians devised one of the earliest writing system known as cuneiform or wedge-shaped symbols. The earliest known cuneiform inscriptions were found in the lower Tigris-Euphrates Valley in what is now southeastern Iraq and date from about 3,000 BC. Writers made the symbols by pressing a pointed instrument called a stylus into wet clay tablets .\nThe tablets were then dried in the sun to preserve the text. Hundreds of thousands of these tablets have survived, providing a window into Sumerian culture, economy, law, literature, politics, and religion. Their writing system would influence the style of scripts in the region for the next 3,000 years.\nAn example of a Cuneiform Tablet. ( homocosmicos/Adobe Stock)\nWhile the cuneiform writing system was created and used at first only by the Sumerians , it didn’t take long before neighboring groups adopted it for their own use. By 2,500 BC, the Akkadians, a Semitic-speaking people that dwelled north of the Sumerians, starting using cuneiform to write their own language.\nHowever, it was the ascendency of the Akkadian dynasty in around 2,300 BC that positioned Akkadian over Sumerian as the primary language of Mesopotamia. While Sumerian did experience a short revival, it eventually became a dead language used only in literary contexts. Akkadian would continue to be spoken for the next two millennium and evolved into later forms known as Babylonian and Assyrian.\nWhere is Akkad?\nThe Sumerians may have been one of the first known civilizations, but it was the Akkadians that formed one of the first known empires. A Semitic group, they moved into southern Mesopotamia during the early part of the third millennium and gained political control of the area.\nThe civilization was founded by Sargon the Great , and was a collection of city states under the control of Sargon’s city, Akkad. Sargon reigned from approximately 2334-2279 BC and conquered all of southern Mesopotamia as well as parts of Syria, Anatolia, and Elam (now western Iran) establishing the region's first Semitic dynasty.\nBronze head of a king of the Old Akkadian dynasty, most likely representing either Naram-Sin or Sargon of Akkad. Unearthed in Nineveh (now in Iraq). In the National Museum of Iraq, Baghdad. ( Public Domain )\nSargon is known almost entirely from the legends that followed his reputation through 2,000 years of Mesopotamian history, but not from documents written during his lifetime. This lack of contemporary record is explained by the fact that the capital city of Akkad, which he commissioned, has never been located and excavated. It was destroyed at the end of the dynasty that Sargon founded and was never inhabited again, at least under the name of Akkad.\nThe Curse of Akkad\n“The Curse of Akkad ” was written within a century of the empire’s fall and attributes Akkad’s downfall to an outrage against the gods after the temple of Enlil was plundered:\nFor the first time since cities were built and founded,\nThe great agricultural tracts produced no grain,\nThe inundated tracts produced no fish,\nThe irrigated orchards produced neither syrup nor wine,\nThe gathered clouds did not rain, the masgurum did not grow.\nAt that time, one shekel’s worth of oil was only one-half quart,\nOne shekel’s worth of grain was only one-half quart. . . .\nThese sold at such prices in the markets of all the cities!\nHe who slept on the roof, died on the roof,\nHe who slept in the house, had no burial,\nPeople were flailing at themselves from hunger.\nThe Rise and Fall of a Mesopotamian Empire\nIn 2350 BC, Sargon conquered all the Sumerian city-states, uniting them under his rule, creating the first Mesopotamian Empire. He defeated the armies of Sumer in two battles and captured Lugalzagesi, the Sumerian king who had united (or conquered) all of Sumer and earned the title of “King of Kish”.\nFor the next two centuries the Akkadians would rule Sumer, during which time the cities carried out many revolts against them. Around 2,100 BC, as Akkad declined, the city of Ur took its place rising to prominence a century after its fall, and the city-states once again became independent.\n- The origins of human beings according to ancient Sumerian texts\n- The great Flood through the Sumerian Tablets\n- Explaining the Fall of the Great Akkadian Empire\nThe Akkadian empire collapsed sometime after 2,200 BC. Historians blame its downfall on tribes of mountain people called Gutians who conquered many parts of Sumer, or possibly point at administrative incompetence, a poor harvest, provincial revolt, or perhaps even a giant meteor. Climate change has of course been thrown into the mix as well and evidence of a long drought, and changing winds and ocean currents have been provided to support that idea.\nIn a 2019 study , fossil coral records from Oman were given as new evidence that frequent winter shamals, or dust storms, and a prolonged cold winter season also contributed to the collapse of the ancient empire . Assyria and Babylon would grow to dominate the area afterwards.\nFrom 2112 to 2004 BC, a dynasty based at the city of Ur revived Sumerian culture to its greatest height, even though the Sumerian language had begun to fall out of use. By 2,000 BC, it was no longer being used and was replaced as a spoken language by Semitic Akkadian.\nWeight of ½ mina dedicated by King Shulgi and bearing the emblem of the crescent moon: it was used in the temple of the moon-god Nanna at Ur. Ur III period. ( Public Domain )\nAlthough Sargon’s dynasty only lasted around 150 years, it created a model of government that influenced all of Middle Eastern civilization and left a permanent imprint on Mesopotamian civilization for the millennia that followed.\nTop Image: Illustration of Mesopotamia. Source: Jeff Brown Graphics\n\"THE BIG MYTH - the Myths.\" THE BIG MYTH - the Myths. http://mythicjourneys.org/bigmyth/fullversion/password011/myths/english/eng_sumerian_culture.htm\n\"Ancient Scripts: Sumerian.\" Ancient Scripts: Sumerian. http://www.ancientscripts.com/sumerian.html\n\"Tall Al-'Ubayd | Archaeological Site, Iraq.\" Encyclopedia Britannica Online. http://www.britannica.com/EBchecked/topic/612242/Tall-al-Ubayd\n\"Apocrypha: The Sumerians and Akkadians.\" The Sumerian Culture. http://lost-history.com/apocrypha1.html\n\"Ur, Sumeria.\" Ur, Sumeria. http://www.ancient-wisdom.co.uk/iraqur.htm\n\"Life in Sumer.\" Ushistory.org. http://www.ushistory.org/civ/4a.asp\n\"A Brief Introduction to the Sumerians.\" Sumerian Shakespeare. http://sumerianshakespeare.com/21101.html\n\"Apocrypha: The Sumerians and Akkadians.\" ART HISTORY WORLDS. http://arthistoryworlds.org/"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:188a15ba-aa35-4e70-a510-e736d6ea899d>","<urn:uuid:6e49cf80-19bd-4c36-8830-2e286cf9e774>"],"error":null}
{"question":"How does AI contribute to international R&D collaboration in food production, and what security measures are necessary to protect these collaborative systems?","answer":"AI contributes to international R&D collaboration in food production through various applications, including AI for design in food production, improving food safety solutions, and enhancing nutritional value development. These collaborative projects can use AI to accelerate the design of materials, devices, and processes in food production. However, to protect these collaborative systems, comprehensive security measures are essential. Organizations need AI-based cybersecurity systems that can continuously gather and analyze data across enterprise information systems, perform correlation of patterns across millions of signals, and provide real-time threat detection. The security system should also include IT asset inventory management, threat exposure monitoring, controls effectiveness assessment, and breach risk prediction to protect the collaborative food production systems from cyber threats.","context":["UK businesses can apply for a share of up to £5 million to develop innovative technologies targeting global marketing with Canadian partners.\nThis competition provides innovation research and development (R&D) grant funding for projects aimed at enhancing industrial productivity.\nYour must focus on one of these 2 themes:\n- Enabling technologies\n- Enhanced productivity\nYour proposal must demonstrate:\n- a clear game-changing and/or disruptive innovative idea leading to new products, processes or services\n- a strong and deliverable business plan that addresses market potential and needs\n- a team, business arrangement or working structure with the necessary skills and experience to run the project successfully and complete on time\n- awareness of the main risks the project will face (including contractor or equipment failure) with realistic management, mitigation and impact minimisation plans for each\n- sound, practical financial plans and timelines\n- good value for money\n- a clear, evidence based plan to deliver economic impact, return on investment (ROI) and growth through commercialisation, as soon as possible after project completion\n- considerate potential to positively impact both the UK and the Canadian economy\n- the benefit to participants from the 2 countries working together\nInnovate UK are looking to fund a portfolio of projects, across a variety of technologies, markets, technological maturities and research categories to build a portfolio of projects that:\n- are high quality\n- target opportunities that benefit both the UK and Canadian economy and/or productivity in a positive way\n- demonstrate sufficient innovation, potential return on investment and degree of technical risk\n- demonstrate value for money, including the potential impact of the project relative to its cost, and the cost of other projects under consideration\nTheme 1: Enabling technologies\nThese are collaborative R&D projects to demonstrate artificial intelligence for design as applied to advanced manufacturing or food production. These projects must use AI to accelerate the design of materials, devices or processes.\nYour project can cover:\n- AI for design\n- AI for supply chains\n- Applying AI and vision systems\nTheme 2: Enhanced productivity\nThese are collaborative R&D projects addressing global competitiveness by improving or adding to products service or process delivery. This can include enabling technologies such as Internet of Things, blockchain and cybersecurity.\nEnhanced productivity projects de-risk and integrate technologies into operationally ready pre-commercial solutions. These must address an identified business need in the area of advanced manufacturing and/or food production. While solutions that include AI or machine learning are welcome under this themes, this is not mandatory.\nFor Theme 1 the projects that will be funded should address:\n- the use of machine learning and other AI techniques such as deep reinforcement learning, deep learning and active learning, to help designers invent new advanced materials, devices and complex systems. Projects would propose new AI-driven design tools to create improved designs for advanced manufacturing or food processing in less time, with faster simulations and overcoming human design bias.\n- the application of AI to improve trust in manufacturing supply networks by tracking the provenance, safety and timeliness of delivery of parts and products\n- the integration into factory processes of AI-driven automated design or visual inspection for the non-destructive testing of parts\nFrom theme 2 the following wo9uld be considered:\nThis can include:\n- additive manufacturing processes, materials and machinery\n- AI, machine learning, robotics, and IoT for manufacturing\n- advanced materials, composites design & manufacturing solutions\n- biorefinery including circular economy waste solutions\n- clean energy technologies for industrial applications\n- digital manufacturing or industry 4.0\nFood quality and processing technologies for improved productivity\nThis can include:\n- enhancing nutritional value\n- novel ingredient development\n- sustainable protein development\n- food safety solutions\n- authenticity and traceability systems\n- processes for improved shelf life and reducing food waste\n- smart packaging\n- lean manufacturing or improved efficiency\nTo find out more about this programme, please follow the link provided on the right hand side.","The enterprise attack surface is massive, and continuing to grow and evolve rapidly. Depending on the size of your enterprise, there are up to several hundred billion time-varying signals that need to be analyzed to accurately calculate risk.\nAnalyzing and improving cybersecurity posture is not a human-scale problem anymore.\nIn response to this unprecedented challenge, Artificial Intelligence (AI) based tools for cybersecurity have emerged to help information security teams reduce breach risk and improve their security posture efficiently and effectively.\nAI and machine learning (ML) have become critical technologies in information security, as they are able to quickly analyze millions of events and identify many different types of threats – from malware exploiting zero-day vulnerabilities to identifying risky behavior that might lead to a phishing attack or download of malicious code. These technologies learn over time, drawing from the past to identify new types of attacks now. Histories of behavior build profiles on users, assets, and networks, allowing AI to detect and respond to deviations from established norms.\nArtificial Intelligence vs. Data Analytics\nUnfortunately, AI is a very popular, often misused buzzword at the moment. Not unlike big data, the cloud, IoT, and every other “next big thing”, an increasing number of companies are looking for ways to jump on the AI bandwagon. But many of today’s AI offerings don’t actually meet the AI test. While they use technologies that analyze data and let results drive certain outcomes, that’s not AI; pure AI is about reproducing cognitive abilities to automate tasks.\nHere’s the crucial difference:\n- AI systems are iterative and dynamic.They get smarter with the more data they analyze, they “learn” from experience, and they become increasingly capable and autonomous as they go.\n- Data analytics (DA), on the other hand, is a static process that examines large data sets in order to draw conclusions about the information they contain with the aid of specialized systems and software. DA is neither iterative nor self-learning.\nUnderstanding AI Basics\nAI refers to technologies that can understand, learn, and act based on acquired and derived information. Today, AI works in three ways:\n- Assisted intelligence, widely available today, improves what people and organizations are already doing.\n- Augmented intelligence, emerging today, enables people and organizations to do things they couldn’t otherwise do.\n- Autonomous intelligence, being developed for the future, features machines that act on their own. An example of this will be self-driving vehicles, when they come into widespread use.\nAI can be said to possess some degree of human intelligence: a store of domain-specific knowledge; mechanisms to acquire new knowledge; and mechanisms to put that knowledge to use. Machine learning, expert systems, neural networks, and deep learning are all examples or subsets of AI technology today.\n- Machine learning uses statistical techniques to give computer systems the ability to “learn” (e.g., progressively improve performance) using data rather than being explicitly programmed. Machine learning works best when aimed at a specific task rather than a wide-ranging mission.\n- Expert systems are programs designed to solve problems within specialized domains. By mimicking the thinking of human experts, they solve problems and make decisions using fuzzy rules-based reasoning through carefully curated bodies of knowledge.\n- Neural networks use a biologically-inspired programming paradigm which enables a computer to learn from observational data. In a neural network, each node assigns a weight to its input representing how correct or incorrect it is relative to the operation being performed. The final output is then determined by the sum of such weights.\n- Deep learning is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Today, image recognition via deep learning is often better than humans, with a variety of applications such as autonomous vehicles, scan analyses, and medical diagnoses.\nApplying AI to cybersecurity\nAI is ideally suited to solve some of our most difficult problems, and cybersecurity certainly falls into that category. With today’s ever evolving cyber-attacks and proliferation of devices, machine learning and AI can be used to “keep up with the bad guys,” automating threat detection and respond more efficiently than traditional software-driven approaches.\nAt the same time, cybersecurity presents some unique challenges:\n- A vast attack surface\n- 10s or 100s of thousands of devices per organization\n- Hundreds of attack vectors\n- Big shortfalls in the number of skilled security professionals\n- Masses of data that have moved beyond a human-scale problem\nA self-learning, AI-based cybersecurity posture management system should be able to solve many of these challenges. Technologies exist to properly train a self-learning system to continuously and independently gather data from across your enterprise information systems. That data is then analyzed and used to perform correlation of patterns across millions to billions of signals relevant to the enterprise attack surface.\nThe result is new levels of intelligence feeding human teams across diverse categories of cybersecurity, including:\n- IT Asset Inventory – gaining a complete, accurate inventory of all devices, users, and applications with any access to information systems. Categorization and measurement of business criticality also play big roles in inventory.\n- Threat Exposure – hackers follow trends just like everyone else, so what’s fashionable with hackers changes regularly. AI-based cybersecurity systems can provide up to date knowledge of global and industry specific threats to help make critical prioritization decisions based not only on what could be used to attack your enterprise, but based on what is likely to be used to attack your enterprise.\n- Controls Effectiveness – it is important to understand the impact of the various security tools and security processes that you have employed to maintain a strong security posture. AI can help understand where your infosec program has strengths, and where it has gaps.\n- Breach Risk Prediction – Accounting for IT asset inventory, threat exposure, and controls effectiveness, AI-based systems can predict how and where you are most likely to be breached, so that you can plan for resource and tool allocation towards areas of weakness. Prescriptive insights derived from AI analysis can help you configure and enhance controls and processes to most effectively improve your organization’s cyber resilience.\n- Incident response – AI powered systems can provide improved context for prioritization and response to security alerts, for fast response to incidents, and to surface root causes in order to mitigate vulnerabilities and avoid future issues.\n- Explainability – Key to harnessing AI to augment human infosec teams is explainability of recommendations and analysis. This is important in getting buy-in from stakeholders across the organization, for understanding the impact of various infosec programs, and for reporting relevant information to all involved stakeholders, including end users, security operations, CISO, auditors, CIO, CEO and board of directors.\nSome early AI adopters\nGoogle: Gmail has used machine learning techniques to filter emails since its launch 18 years ago. Today, there are applications of machine learning in almost all of its services, especially through deep learning, which allows algorithms to do more independent adjustments and self-regulation as they train and evolve.\n“Before we were in a world where the more data you had, the more problems you had. Now with deep learning, the more data the better. Elie Bursztein, head of anti-abuse research team at Google\nIBM/Watson: The team at IBM has increasingly leaned on its Watson cognitive learning platform for “knowledge consolidation” tasks and threat detection based on machine learning.\n“A lot of work that’s happening in a security operation center today is routine or repetitive, so what if we can automate some of that using machine learning?” – Koos Lodewijkx, vice president and chief technology officer of security operations and response at IBM Security.\nJuniper Networks: The networking community hungers for disruptive ideas to address the unsustainable economics of present-day networks. Juniper sees the answer to this problem taking shape as a production-ready, economically feasible Self-Driving Network™.\n“The world is ready for autonomous networks. Advances in artificial intelligence, machine learning, and intent-driven networking have brought us to the threshold at which automation gives way to autonomy.” Kevin Hutchins, Sr. VP of strategy and product management.\nBalbix BreachControl (now called Balbix Security Cloud) platform uses AI-powered observations and analysis to deliver continuous and real-time risk predictions, risk-based vulnerability management and proactive control of breaches. The platform helps make cybersecurity teams more efficient and more effective at the many jobs they must do to maintain a strong security posture – everything from keeping systems patched to preventing ransomware.\n“Enterprises need to build security infrastructure leveraging the power of AI, machine learning, and deep learning to handle the sheer scale of analysis” – Gaurav Banga, Founder and CEO.\nAI Use by Adversaries\nAI and machine learning (ML) can be used by IT security professionals to enforce good cybersecurity practices and shrink the attack surface instead of constantly chasing after malicious activity. At the same time, state-sponsored attackers, criminal cyber-gangs, and ideological hackers can employ those same AI techniques to defeat defenses and avoid detection. Herein lies the “AI/cybersecurity conundrum.”\nAs AI matures and moves increasingly into the cybersecurity space, companies will need to guard against the potential downsides of this exciting new technology:\n- Machine learning and artificial intelligence can help guard against cyber-attacks, but hackers can foil security algorithms by targeting the data they train on and the warning flags they look for\n- Hackers can also use AI to break through defenses and develop mutating malware that changes its structure to avoid detection\n- Without massive volumes of data and events, AI systems will deliver inaccurate results and false positives\n- If data manipulation goes undetected, organizations will struggle to recover the correct data that feeds its AI systems, with potentially disastrous consequences\nIn recent years, AI has emerged as required technology for augmenting the efforts of human information security teams. Since humans can no longer scale to adequately protect the dynamic enterprise attack surface, AI provides much needed analysis and threat identification that can be acted upon by cybersecurity professionals to reduce breach risk and improve security posture. In security, AI can identify and prioritize risk, instantly spot any malware on a network, guide incident response, and detect intrusions before they start.\nAI allows cybersecurity teams to form powerful human-machine partnerships that push the boundaries of our knowledge, enrich our lives, and drive cybersecurity in a way that seems greater than the sum of its parts.\nFrequently Asked Questions\n- How is AI used in cybersecurity?\nAI is used in cyber security to quickly analyze millions of events and identify many different types of threats – from malware exploiting zero-day vulnerabilities to identifying risky behavior that might lead to a phishing attack or download of malicious code. This technology is a self-learning system that automatically and continuously gathers data from across your enterprise information systems. This data is then analyzed and used to perform a correlation of patterns across millions to billions of signals relevant to the enterprise attack surface to identify new types of attacks.\n- Will AI take over cybersecurity?\nWith today’s ever-evolving cyber-attacks and proliferation of devices, analyzing and solving cybersecurity posture is not a human-scale problem anymore. Artificial Intelligence (AI) based tools for cybersecurity have emerged to help information security teams reduce breach risk by providing real-time monitoring of the attack surface, improving security threat detection, and prompting action so your security team can “keep up with the bad guys”. Given its benefits, AI-based cybersecurity posture management systems are growing in popularity as more organizations are recognizing their ability to automate threat detection and predict cyberattacks with matchless precision.\n- What is AI in cybersecurity?\nAI in cybersecurity is the process of analyzing numerous quantities of risk data and the relationship between threats in your enterprise information systems to identify new types of attacks. The result is new levels of intelligence feeding human teams across diverse categories of cybersecurity, including IT asset inventory, threat exposure, controls effectiveness, breach risk prediction, incident response, and improved communication around cyber security within the organization. With their ability to quickly analyze millions of events and identify many different types of threats, AI technologies can help security teams reduce breach risk and improve their security posture efficiently and effectively."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:da04ad54-ac58-467f-8c4b-ced723a4de04>","<urn:uuid:32285b50-7e5a-468c-a29a-9c07251582ab>"],"error":null}
{"question":"What are the recommended physical exercises for back rehabilitation, and how does sleep quality influence pain management during recovery?","answer":"For back rehabilitation, several exercises are recommended: lying on your back and stretching one leg with a towel for 30 seconds, stretching the piriformis muscle by crossing legs while lying down, pushing up the upper body while lying on stomach, leg raises while prone, hip raises while supine, and opposing limb raises while on hands and knees. Each exercise should be repeated 3-5 times. Additionally, 20-30 minutes of low-impact cardio 3-4 times per week is beneficial. As for sleep's influence on pain management, getting less than 7 hours of sleep is associated with developing pain conditions and increased pain sensitivity. Poor sleep can create a vicious cycle, as it heightens pain and delays healing. Research shows that sleep impairments are actually a stronger predictor of pain than pain is of sleep problems, making sleep management crucial for recovery.","context":["Depending upon a patient’s individual injury and level of pain, the exercise and rehabilitation program may vary. Please consult with your doctor of chiropractic prior to starting a new exercise or rehabilitation program, especially when associated with low-back pain or discomfort. He or she can help develop an individualized exercise program and provide instruction on the proper stretching techniques.\nExercises to Safeguard Your Back\nLie on your back with both legs straight. Extend one leg straight up in the air. Loop a towel over the arch of the lifted foot, and gently pull on the towel as you push against it with your foot until a stretch is felt in the back of the thigh. Hold 30 seconds. Relax. Repeat 3 times on each side.\nThe piriformis muscle runs through the buttock and can contribute to back and leg pain. To stretch this muscle, lie on the back and cross one leg over the other; gently pull the knee toward the chest until a stretch is felt in the buttock area. Hold 30 seconds. Relax. Repeat 3 times.\nLie on your stomach. Use your arms to push your upper body off the floor. Hold for 10 seconds. Let your back relax and sag. Repeat 10 times.\nLie on your stomach. Tighten the muscles in one leg and raise it from the floor. Hold your leg up for a count of 10 and return it to the floor. Do the same with the other leg. Repeat 5 times with each leg.\nLie on your back with your knees flexed and your feet flat on the floor. Keep the knees together. Tighten the muscles of the lower abdomen and buttocks so as to flatten your lower back against the floor. Slowly raise your hips up from the floor and hold for a slow count of 10. Repeat this exercise 3 times. If you cannot raise your hips from the floor, merely tighten the belly, the abdominal and buttock muscles, and wait until you can raise the hips.\nKneel on mat on hands and knees, with palms directly under shoulders and knees hip-width apart. Slowly raise right arm, and extend it forward parallel to floor. (Balance by contracting your abdominal muscles.) Keep right palm parallel to the floor, then lift the left leg, and straighten it behind you. Hold opposing limbs off the ground for 30 to 60 seconds without arching your back. Switch sides. Repeat 3 to 6 times.\nThe Cardio Component\nEngaging in cardiovascular exercise can help aid in rehabilitation of the spine and help prevent future injury. Most health care professionals recommend 20 to 30 minutes of cardio exercise three to four days per week to improve cardiovascular endurance and help lose weight. Until you’ve recovered from back pain, select low-impact activities that burn calories, but won’t place undue stress on your joints. Consider alternating cardio exercise and strength training to get the most from your workouts and to allow your muscles time to recover.\nTo get the maximum benefit from stretching, proper technique is essential. The American Chiropractic Association offers the following tips:\n• Warm up your muscles before stretching by walking or doing other gentle movements for 10 to 15 minutes.\n• Slowly increase your stretch as you feel your muscles relax. Don’t bounce.\n• Stretch slowly and gently only to the point of mild tension, not to the point of pain.\n• Don’t hold your breath. Inhale deeply before each stretch and exhale during the stretch.\n• As your flexibility increases, consider increasing the number of repetitions.\n• Stop immediately if you feel any severe pain. For more health tips, visit the ACA’s Web site at www.acatoday.org/patients\n*Type of Exercise\n**Avg. Calories Burned per 30 min.\n***Walking in a shallow pool can provide weightless conditioning, which minimizes stress on the back. However, for some patients swimming and water aerobics may cause too much rotation of the spine. Be sure to first consult with your health care provider.\n***Focus on standing upright and maintaining good posture. Don’t lock your knees.\n***Walking is very gentle on the back. Avoid walking on concrete or uneven terrain.\n*Stationary Recumbent Bike\n***A safe form of cardio because you press your lower back against the seat rest at all times.","What is good sleep?\nThe recommended sleep for adults is between 7-9 hours of sleep per night.\nWhy is sleep so important?\nConsistently getting less than 7 hours of sleep per night is associated with greater risk of:\n- Developing a pain condition Finan, Goodin, & Smith 2013\n- Heightened pain sensitivity Schuh-Hofer et al 2013\n- Accidents and sporting injuries Copenhaver & Diamond 2017\n- Reduced memory and learning Diekelmann & Born 2010\n- Poorer cardiovascular health and disease Covassin and Singh 2016\n- Mental health disorders such as depression and anxiety Freeman et al 2017\n- Decreased immune system and increased susceptibility to infectious agents Asif et al 2017\n- Dementia and Alzheimer’s disease Spira et al 2014\n- Some cancers von Ruesten 2012\n- Weight gain and obesity Kobayashi et al 2011\nConversely getting the recommended 7-9 hours’ sleep consistently will improve your mood, energy levels, productivity and reduce your risk of all the above health problems.\nThe sleep, pain, mental health vicious cycle and how to break it\n- Poor sleep will heighten pain, delay healing and negatively impact on our mental health.\n- Poor mental health (stress, depression and anxiety) will impact on our sleep and pain.\n- Pain can negatively impact on our sleep quality and mental health.\nThe result of the interplay of poor sleep, mental health and pain that is not managed well can cause a vicious cycle that leads to a drastically reduced quality of life. Overcoming this is a vital part of improving your quality of life.\nBreaking the cycle\nAny strategies that target mental health, pain or sleep can be effective for breaking this cycle. Research shows that sleep impairments are a stronger, more reliable predictor of pain than pain is of sleep impairments. Therefore, targeting sleep impairments can be a more effective tool to breaking this cycle than targeting pain. The next section will give you tools you can use to do this.\nStrategies for improving sleep quality\n- Prioritise sleep as a vital part of your week and take practical steps to ensure it happens.\n- Make a plan to improve your sleep.\n- Set your body clock:\n- Get regular with your wake time and bedtime even on the weekends no matter how poor your sleep was.\n- If you need to catch up on sleep it is better to go to bed earlier rather than try to sleep in.\n- Get out into natural light as soon as possible in the morning.\n- Wind down pre-bedtime:\n- Develop a relaxed bed routine that includes avoiding devices, work or other mentally stimulating activities in the hour before bed.\n- Write down a ‘to-do’ list or a journal to get thoughts out of your head before bed (this can be done during the night if you wake and are struggling to get back to sleep).\n- Ensure your bedroom is dark, cool and quiet (eye masks, earplugs, fans or air con if required).\n- Bed is for sleeping- limit activities such as phones, laptops and TV’s in bed.\n- If you can’t sleep after 20 minutes, get up and do something boring until you feel tired, then try again.\n- too many caffeinated drinks during the day (but especially in the afternoon/evening).\n- Alcohol- even if it helps you get to sleep alcoholic drinks at night impact on the quality of your sleep.\n- Identify life stress that may be impacting on sleep and make a plan to manage this:\n- Prioritising a good work life balance.\n- Say ‘No’ to some things if you are over-committed.\n- Prioritise stress reducing exercise during the day.\n- Use relaxation techniques such as mindfulness and diaphragmatic breathing during the day and before bed.\n- Take a twenty to thirty minute catch up nap during the day. Avoid longer than\nthirty minutes during the day to limit impact on sleep quality at night.\n- Get treatment for pain, depression, anxiety and low mood as these conditions\nare shown to impact on sleep quality.\nUseful resources on sleep\nTwo good websites to explore more info on sleep are:\nFor a review of various sleeping apps to help with sleep see:\nFor a simple breathing exercise or a guided imagery exercise see:\nFor a good blog site (by one of NZ’s best-known pain academic and clinician) with some good articles around sleep (as well as many other things pain related) see:\nAsif, N., Iqbal, R., and Chaudhry, F. 2017 Human immune system during sleep. Am J Clin Exp Immunol.; 6(6): 92–96.\nCopenhaver EA, Diamond AB. 2017. The Value of Sleep on Athletic Performance, Injury, and Recovery in the Young\nAthlete. Pediatr Ann. Mar 1;46(3):e106-e111. doi: 10.3928/19382359-20170221-01.\nCovassin and Singh 2016. Sleep Duration and Cardiovascular Disease Risk: Epidemiologic and Experimental Evidence\nSleep Med Clin. Mar; 11(1): 81–89.\nDiekelmann, S. and J. Born (2010). The memory function of sleep. Nature Reviews Neuroscience 11: 114.\nFinan, P., Goodin, B., & Smith, M. 2013. The association of sleep and pain: An update and a path forward. J Pain. 2013\nDecember; 14(12): 1539–1552. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4046588/pdf/nihms521705.pdf\nFox, J.L., Scanlan, A.T., Stanton, R. et al. 2020. Insufficient Sleep in Young Athletes? Causes, Consequences, and\nPotential Treatments. Sports Med 50, 461–470. https://doi.org/10.1007/s40279-019-01220-8\nFreeman et al 2017. The effects of improving sleep on mental health (OASIS): a randomised controlled trial with\nmediation analysis. The Lancet VOLUME 4, ISSUE 10, P749-758, OCTOBER 01, 2017\nKobayashi, D., Takahashi, O., Deshpande, G.A. et al. (2012). Association between weight gain, obesity, and sleep duration: a large-scale 3-year cohort study. Sleep Breath 16, 829–833. https://doi.org/10.1007/s11325-011-0583-0\nSchuh-Hofer S, Wodarski R, Pfau DB, et al. 2013. One night of total sleep deprivation promotes a state of generalized\nhyperalgesia: A surrogate pain model to study the relationship of insomnia and pain. PAIN®. 154:1613-1621.\nSpira, A1,2 Lenis P. Chen-Edinboro, 1 Mark N. Wu, 3 and Kristine Yaffe4 2014. Impact of Sleep on the Risk of Cognitive\nDecline and Dementia. Curr Opin Psychiatry. Nov; 27(6): 478–483. doi: 10.1097/YCO.0000000000000106\nVon Ruesten A, Weikert C, Fietze I, Boeing H. 2012. Association of Sleep Duration with Chronic Diseases in the\nEuropean Prospective Investigation into Cancer and Nutrition (EPIC). Potsdam Study, PLoS One, 2012, vol. 7 pg. e30972"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:2934a956-d9a6-44d6-abaf-6caa57c85900>","<urn:uuid:591ac915-3ce9-441c-931f-305c2ecfcb9d>"],"error":null}
{"question":"What are the main differences in job search resources available for boilermakers versus general Wisconsin job seekers through the DWD?","answer":"For general Wisconsin job seekers, the Department of Workforce Development (DWD) offers JobCenterofWisconsin.com, a free 24-hour online labor exchange with advanced skills matching features, Skill Explorer tools to expand job searches, and local Job Centers with in-person services. For boilermakers specifically, job opportunities typically come through apprenticeship programs, with the Bureau of Labor Statistics projecting about 1,400 new vacancies annually between 2019-2029, and employment options are primarily found through construction companies or private services.","context":["Wisconsin employers turn to the Office of Skills Development (OSD) for effective talent development solutions to recruit, train and retain highly skilled workers. Let us connect you with industry and workforce development system experts across the state to help your business:\n- Identify and implement workforce training needs.\n- Make local talent development connections.\nFind Skilled Labor Solutions\nOSD works quickly to review and resolve the skilled labor needs of Wisconsin employers, including facilitating introductions to and discussions with regional business and economic development organizations and industry-recognized training providers. Assistance is provided in coordination with these workforce development system partners:\n- Wisconsin Department of Workforce Development – Office of Economic Advisors\n- Wisconsin Economic Development Corporation\n- Wisconsin Technical College System\n- Wisconsin Workforce Development Association\nHelp Shape Future Grant Opportunities\nOSD uses your business insight and current labor market data to develop worker training grant opportunities. Share the following business and industry knowledge with us to help shape future Grant Program Announcements (GPAs):\n- Innovative strategies for building human capital, skills and talent\n- Collaborations focused on improving talent development outcomes\n- Unmet talent development needs due limited training opportunities\n- Challenges with attracting and retaining qualified workers\nBuild Your Talent Pipeline\nExplore these innovative programs to build a robust talent pipeline with your business, sector or community:\n- Youth Apprenticeship: DWD's Youth Apprenticeship (YA) program integrates school-based and work-based learning, providing students with instruction in employability and occupational skills as defined by Wisconsin industries. Employ YA students to help develop your local workforce and create a recruitment pipeline that is loyal to your business and community.\n- Registered Apprenticeship: Registered Apprenticeship (RA) enables employers to build a customized, on-the-job skills training system to developing highly skilled workers by immersing them in company culture while earning good-paying, family-supporting wages as they learn a high demand trade. Explore how this proven workforce readiness solutions can help your business address the skills gap.\n- Career and Technical Education: All Wisconsin school districts must provide Career and Technical Education (CTE) to students in grades 6 – 12. School districts are actively identifying opportunities to partner with local businesses to help demonstrate and expose students to different occupations.\nRecruit Qualified Talent\nAccess talent recruitment tools to broaden your reach for qualified workers:\n- JobCenterofWisconsin.com: Operated by DWD, JobCenterofWisconsin.com (JCW) is Wisconsin's online public labor exchange that connects employers and job seekers at no cost 24 hours a day. Logon to access advanced skills matching features and integrated labor market information to make informed employment decisions now.\n- Skill Explorer: Let DWD's Skill Explorer tools help you expand your search for qualified job candidates by discovering other occupations that utilized the same skills you need in open positions.\n- Local Job Centers: Conveniently located across the state, Wisconsin's Job Centers offer a variety of job seeker and employer services. Many employers find talent to fill current job openings by connecting with thousands of job seekers who visit Job Centers daily. Job Centers also assist employers with: using advanced employment tools on JobCenterofWisconsin.com, accessing local Labor Market Information, and connecting to education and training resources.\nThe Wisconsin Fast Forward (WFF) Internship program coordinates efforts between the Department of Workforce Development (DWD), the University of Wisconsin System (UWS), the Wisconsin Association of Independent Colleges and Universities (WAICU) and the Wisconsin Technical College System to increase the number of internship opportunities available to Wisconsin college students.\nCollege students who intern for a Wisconsin company are more likely to stay in the state after graduation, making internships a valuable workforce retention strategy. Additional benefits include:\n- Providing employers with an opportunity to observe students before extending a permanent job offer.\n- Enabling students to demonstrate their skills in a work environment.\nHelp Shape the Internship Program\nDWD and higher education partners are meeting with business owners and leaders, workforce professionals and educators across the state to obtain feedback on what businesses and educators need to create successful internship opportunities for Wisconsin college students. Input will be used to explore the:\n- Development a robust, user-friendly internship website, enabling employers post and students to search available opportunities.\n- Creation of a resource library for businesses of any size or scope that are interested in providing internships.\nLearn more about program activities here: http://wisconsinfastforward.com/pdf/icp_waw_wrapup_1702.pdf","How to Become\nBest Education Tracks, Key Skills, and Top Certifications\nA boilermaker occupies an important position within the field of construction. These workers are indispensable to the success of any project. This career has a good job outlook and high salaries compared to other jobs.\nIf you want to learn how to become a boilermaker in 2021, this guide will show you the way. Read on for information about the job description, average salaries, educational options, and licenses needed to work as a professional boilermaker.\nWhat Is a Boilermaker?\nA boilermaker is a person in charge of everything related to boiler systems in a company, houses, shopping centers, or other construction projects. A boilermaker maintains and repairs containers that contain gases or liquids.\nIron, copper, and stainless steel boilers require manual and professional maintenance. For these jobs, a boilermaker uses his theoretical and practical skills to maintain the optimal functioning of the boiler system.\nWhat Type of School Should You Attend\nto Become a Boilermaker?\nTo work in this position, you usually need a high school diploma or its equivalent. You can then access an apprenticeship program to reinforce your knowledge and gain professional experience. For this job, you can offer private services or belong to a construction company.\nBest Boilermaker Education Tracks\nBoilermakers have intense job training. Most jobs have extended hours and require you to be in excellent physical condition. Technical training is the most common way to learn this trade, usually in an apprenticeship program rather than a trade school.\nIf you want to know about the educational options to become a boilermaker, pay attention to the points below.\nThe most common way to start your career as a boilermaker is through an apprenticeship program. This preparation requires approximately four years. During the training, you will learn theoretical concepts and practical activities for your occupation.\nYou will be taught about metals and installation techniques, welding, plan design, how to repair boilers, plus the tools and equipment required for each task, among other subjects. One of the advantages of these programs is professional experience. Currently, there are many organizations across America where you can access an apprenticeship program.\nBoilermakers Local 433 is an organization in Tampa, Florida affiliated with the Boilermakers National Apprenticeship Program (BNAP) and the International Brotherhood of Boilermakers.\nAccording to the Boilermakers Local 433 website, members pay dues of $43 each month during training. Additionally, apprentices need to meet physical standards, work full-time schedules, and be able to work in challenging environments.\nCommunity colleges offer two-year associate degrees and four-year bachelor’s degrees. However, not all community colleges offer a bachelor’s degree. You should check your preferred school to see which track you can take. Community colleges are more convenient and affordable than universities.\nSome of the best community colleges for graphic design are Nassau Community College and Waubonsee Community College. These and many other colleges offer world-class training at low prices. Tuition varies, but it will typically be less than a private university.\nThe typical way to obtain career preparation is with hands-on practice. However, many online courses offer valuable training for this profession. Both theory and critical thinking are tools for success as a boilermaker.\nIn addition, welding and blueprint reading are key skills for this profession. Journeymen and journeywomen who have taken courses in these subjects have a greater chance of succeeding in the industry.\nHow to Become a Boilermaker: A Step-by-Step Guide\nThere is no specific path to becoming a boilermaker. However, some options help you increase your chances of getting better jobs and higher wages. If you want to start your way as a boilermaker, pay attention to the five steps below.\nAssess your aptitude\nBefore choosing any career, you need to assess your chances of success. To be a boilermaker, you must be fit and strong, and be prepared to understand the health and safety aspects of the trade.\nAnalyze the market\nIf you have the profile of a boilermaker, then the next task is to assess the market and the demand for boilermaker jobs. Check the average salaries, the places where you can learn this career, and the job benefits.\nEnroll in an apprenticeship program\nThis is the principal educational requirement. In these programs, you will learn the fundamental practices of the profession. A diploma from an apprenticeship program will help you get to a company faster and be better prepared.\nGet a job\nAfter your time as an apprentice, you must get a job. Training programs connect you with your first job opportunities, but your performance will be the key to maintaining good work relationships and advancing in the industry.\nExpand your skills\nThe bigger your resume, the better your chances of being successful. The industry is extensive and requires versatile professionals with the skills to perform tasks of all kinds.\nKey Boilermaker Skills\nTo get a job opportunity in this industry, you must have a professional profile. The experience and certification help you improve your chances of achieving success as a boilermaker.\nWhat skills are most likely to get you hired as a boilermaker? Pay attention to the points below.\nStrength and physical endurance are crucial competencies for a boilermaker. A boilermaker needs optimal fitness and strength to carry heavy components to workplaces. This is one of the reasons these tasks pay out large sums of money.\nDuring your training in the apprenticeship program, you will learn the best techniques to use your strength at work correctly. This will help you avoid injuries and optimize your time.\nTraining and hands-on activities make you a boilermaker ready for any challenge. All equipment repairs, maintenance, and testing are entirely manual.\nCompanies hire professionals who work with tools such as try-squares, box levels, rulers, protractors, and contour gauges, among others. In addition, you must know general construction techniques, possess safety awareness and welding experience, and be able to drive a forklift.\nAbility to Adapt to Different Work Environments\nAs a boilermaker, your jobs will be varied. Workspaces can be high up in a building, in tight basements, boilers, and much more. You can work in very hot or cold areas, depending on the project.\nBoilermaker Salary and Job Outlook\nA 2020 Bureau of Labor Statistics (BLS) report says that a boilermaker earns an average salary of $65,360. These figures depend on the state, your level of experience, and the project. Most boilermakers are paid on the job.\nBLS also projects that the demand for boilermakers will grow one percent between 2019 and 2029, slower than other related jobs. This works out to about 1,400 new vacancies every year during the mentioned period.\nEntry-Level Boilermaker Job Requirements\nEntry-level is the standard starting position if you join a company. To get a job at this level, you must understand how to build, repair, and maintain boiler systems, and how to weld. You must also have general craft experience.\nAn apprenticeship program diploma qualifies you for an entry-level position as a professional boilermaker. According to PayScale, an average entry-level boilermaker makes about $22 per hour, which is equal to about $49,000 per year.\nWhat Does a Boilermaker Do?\nA boilermaker has several tasks on their schedule. Remember, like all jobs, your activities depend on your position, experience, and type of project. The more years you have in the company, the more functions you can take on.\nBelow we list some key responsibilities of a boilermaker.\nYour main role is to take care of a boiler system in a building, company, house, boat, or any environment that uses these machines. You must install boiler tanks using welding techniques and various kinds of work equipment.\nMonitors and Repairs Boilers\nAs a boilermaker, you have the skills to verify the operation of the boiler. You must analyze its operability, inspect for faults such as overheating, and be able to detect gas leaks. Additionally, you must apply your technical knowledge to repair these systems using the correct tools. Remember that these repairs can be in tight spaces, high areas, or outdoors.\nReading blueprints is a constant activity in this job. The plans include the location and conditions of the boiler system at the specific facility. Also, in case of a construction query, you can suggest the best place to install the boiler.\nAn apprenticeship program can be your only training before you start working as a professional. However, a certification allows you to achieve a higher status, ideal for endorsing your knowledge and skills with specialized organizations in this industry.\nBoilermaker licenses also include welding programs and related jobs. Many companies and states require employees to have these certifications. Below you will see a description of each program.\nBoilermaking Four-Level Certification\nThe National Center for Construction Education and Research (NCCER) is an organization that provides special licenses and training for workers in the construction industry in the United States.\nWith the NCCER Boilermaking four-level certification, you will become a journey-level boilermaker, qualified to work on any project with these characteristics. In the program, you will test your skills in the maintenance, exchange, and installation of industrial boilermaking.\nCertified Welder Program\nThe American Welding Society (AWS) is an organization that offers certifications for welding workers and related tasks. The Certified Welder program evaluates your performance as a welder in many areas of work, including boilermaking.\nThis credential verifies your skills in these tasks and allows you to present yourself as a professional in any company in the United States.\nHow to Prepare for Your Boilermaker Job Interview\nBefore joining a company, you must ace your job interview. In this process, the employer will evaluate your critical thinking and, above all, your practical skills as a boilermaker. Some questions may include theoretical content, but your physical condition and key aptitudes will be assessed, too.\nBoilermaker Job Interview Practice Questions\n- Why did you choose this company to work for?\n- How many types of welding can you do?\n- Can you work in harsh conditions?\n- Are you physically fit to lift heavy objects?\n- What projects have you undertaken as a boilermaker?\nHow Long Does It Take to Become a Boilermaker?\nThe time it takes to become a boilermaker depends on your effort and skills. However, an apprenticeship program can last between four and five years.\nIf you want to be a qualified boilermaker, you can apply for official credentials. The process to obtain the essential certifications can take a year. Then, when you find a job, it will take a year or two to get promoted as an official boilermaker. So, you could become an expert in approximately seven years.\nShould You Become a Boilermaker in 2021?\nYes. If you think that you have what it takes to become a boilermaker, you should take this career path. Boilermaking is not a job for everyone, but it’s a good choice if you like physically demanding jobs.\nAnother reason to be a boilermaker is that you can easily access key resources to build up your theoretical knowledge. Currently, online colleges, courses, and thousands of books are available on the Internet for all students.\nHow many people work as boilermakers in America?\nAccording to a study by BLS, approximately 15,900 people worked as boilermakers in the United States in 2019.\nIs it dangerous to work as a boilermaker?\nAlthough boilermakers work in harsh environments or outdoors, the risk of serious injury is not that high. In the 21st century, technologically advanced tools, improved health and safety practices, and better working conditions make this, and many other construction jobs, safer than ever before.\nHow popular are apprenticeship programs?\nIn 2020, a report from the Department of Labor (DOL) said there were 26,000 apprenticeship programs registered in the United States. That year, about 3,143 new programs were created. So, the signs are positive that apprenticeship programs will continue to gain in popularity.\nIs it difficult to enter an apprenticeship program?\nNo. Many people enter these programs every year. DOL figures show that in 2020, despite COVID-19 and a 12 percent decline in new students, apprenticeship programs recorded the third-highest revenue in their history."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"}],"document_ids":["<urn:uuid:d3aef51e-fbf2-4adc-b390-90452ef88511>","<urn:uuid:51f364d0-0a5c-4853-9275-95a0a2ed0f64>"],"error":null}
{"question":"How do employment-related injuries differ from child supervision injuries in terms of what types of incidents are not covered by legal protections?","answer":"For employment-related injuries, workers' compensation does not cover injuries sustained while committing a crime, self-inflicted injuries, injuries sustained off-the-job, or injuries sustained while violating company policies. In child supervision cases, injuries are not covered when there is no duty of supervision connecting the child and caretaker - for example, if a child is injured while in the vicinity of a stranger who has no obligation to supervise them. Additionally, in child supervision cases, the supervisor's liability may be reduced or eliminated if the child was sufficiently developed to understand and avoid the harm, particularly with older children like fourteen-year-olds.","context":["Suing for Injuries Caused to Your Children by a Negligent Supervisor\nSep 14, 2018 - Personal Injury\nThough many parents might want to be able to supervise their children at all times, there are times when children must come under the supervision of another adult, whether in a school setting or through some other arrangement (i.e., childcare center).\nChildren are naturally curious, and as such, they tend to “attract” injuries. These tendencies often give rise to situations where children sustain injuries while under the care of a non-parental adult. Generally speaking, for minor injuries — cuts and scrapes and scratches — this is a non-issue, but in cases where your child has suffered severe injuries, Pennsylvania law may give you a right of action to secure damages on behalf of your child.\nWhat Qualifies as Negligent Supervision of a Child?\nLiability for negligent supervision of a child will attach only if there is a duty of supervision that connects the child and the caretaker.\nFor example, if your child runs off in the supermarket and hurts themselves crawling on a shelf while in the vicinity of a stranger, you cannot hold that strange liable for failing to supervise or otherwise “negligently” supervising your child, as the stranger has no duty to supervise your child.\nOn the other hand, if you paid a nanny to supervise your child, and he or she crawled on a shelf and injured themselves, then you might be entitled to hold the nanny liable for negligent supervision — the nanny has a clear duty of supervision.\nFailure to meet the standard of care — thus violating the duty of supervision and committing negligence — is dependent on the circumstances of the case. The defendant will only be found to have violated their duty if another caretaker (in the same or similar circumstances) would have acted differently, thus preventing the harm.\nVariable Understanding of the Avoidance of Harm\nIn many negligent supervision cases involving an injured child, the defendant may argue that they cannot be held liable — or that their liability should be reduced — due to the child’s inability or unwillingness to avoid the harm at-issue. If the child could have and should have avoided the injury-causing harm, then the defendant might thereby minimize their potential liability.\nChildren cannot necessarily be expected to avoid harms, however, and certainly, children may vary significantly in terms of their developed intelligence and discipline. In Pennsylvania, as in other states, the courts will determine whether the child’s particular age and development was at a sufficiently high level that the defendant cannot be held liable for the harm.\nFor example, if your caretaker is supervising your five-year-old child, and negligently allows the child to play on a barbed wire fence, then the caretaker defendant would have quite a difficult time convincing the court that your child “should have” avoided the harm. 5-year-olds are not at a level of development that one can expect them to avoid such harms — even obvious ones.\nBy contrast, if a friend of yours is supervising your fourteen-year-old child, and your child plays on a barbed wire fence, then the court will likely reduce liability or even absolve the defendant of liability, as a fourteen-year-old can be expected to understand the harm and to make reasonable efforts to avoid it.\nContact a Skilled Stroudsburg Personal Injury Attorney to Arrange a Free and Confidential Consultation\nHere at Drake, Hileman & Davis, PC, our attorneys boast decades of experience advocating on behalf of those who have sustained injuries due to the negligence or wrongful misconduct of another.\nWe are a results-oriented firm, and as such, we work closely with clients throughout the litigation process so that we can provide personalized representation that is specifically developed to secure maximum compensation. This approach has helped us generate a consistent track record of success, with significant case results.\nCall (215) 348-2088 today to arrange a free and confidential consultation with an experienced Stroudsburg personal injury attorney at Drake, Hileman & Davis, PC.","As an employee, you’re focused on doing your job and doing it well. Regardless of how well you do your job, there’s always the potential to sustain an injury. Nearly 8 million people suffer from workplace injuries every year, according to the Bureau of Labor Statistics. If you suffer an on-the-job injury, you should know there’s a chance you’re eligible for workers’ compensation.\nWorkers’ compensation laws are determined at the state level. If you’re injured at work in Illinois, it’s important to understand the ins and outs of the laws, so you can receive the compensation you deserve. Our South Illinois workers’ compensation lawyers have the knowledge and drive to help you ease your financial burdens with workers’ compensation benefits.\nWhat Is Workers’ Compensation and Why Is It Awarded?\nAccording to the United States Department of Labor (DOL), over 6,500 workers’ compensation claims have been filed in Illinois, as of July 2018. While not every claim was awarded compensation, injured employees have received a combined total of over $213 million.\nIt’s not just the individuals working in high-risk jobs like construction or healthcare who are susceptible to workplace injuries. People working in office jobs are just as likely to sustain injuries deserving of workers’ comp. Companies are required by law to have a workers’ comp system in place.\nIn Illinois, the decision to award is made by the Illinois Workers’ Compensation Commission if there is a dispute between an employer and employee. This state agency manages the court proceedings between the two parties. How the proceedings go is established by the Illinois Workers’ Compensation Act. This act discusses the specific injuries that are to be covered by workers’ comp benefits, as well as scenarios where employees are not eligible for injury-related benefits.\nThe following injuries are covered under the Illinois Workers’ Compensation Act:\n- Injuries sustained after the repetitive use of body parts for an extended period of time\n- Heart attack or stroke sustained because of work conditions\n- Pre-existing conditions exacerbated because of work\n- Other physical injuries, illnesses, or disease caused by work\nThe following injuries are not covered under the Illinois Workers’ Compensation Act:\n- Injuries sustained while committing a crime\n- Injuries sustained off-the-job\n- Injuries sustained while in violation of the company’s policies or procedures\n- Self-inflicted injuries\nWhat Kind of Benefits Are Available for Workers’ Compensation Claims in Illinois?\nThe minimum and maximum workers’ comp amounts are reevaluated and published every six months by the Illinois Department of Employment Security. These amounts are referred to as the statewide average weekly wages (SAWW). Eligibility for workers’ comp benefits is determined by the type of injury sustained, and which disability category that injury falls under.\nTemporary Partial Disability Benefits\nTemporary partial disability (TPD) benefits cover the period of time when an injured employee is healing. The employee is still able to work, either full-time or part-time; however, their duties are medically required to be lightened. At this time, an employee will earn less than they did prior to being injured; however, the employer is required to pay the employee two-thirds of their weekly wages until they have recovered or are deemed able to return to their regular work status.\nTemporary Total Disability Benefits\nTemporary total disability (TTD) benefits are awarded to employees who are temporarily unable to work. TTD benefits also apply if an employee is cleared for lighter duties, but an employer is unable to provide them. TTD benefits are not paid for the first three days of missed work unless the employee is out for 14 days or longer. An employee receiving TTD benefits is legally entitled to two-thirds of their average weekly pay for the extent of their leave.\nPermanent Partial Disability Benefits\nIllinois breaks permanent partial disability (PPD) benefits down into four categories: scheduled awards, nonscheduled awards, wage differential, and disfigurement.\nScheduled awards are provided to employees who have sustained a disability to their ears, eyes, arms, hands, legs, or feet. An employee is generally entitled to 60 percent of their weekly wages for a period of weeks determined by the Illinois Department of Employment Security.\nIf an injury is sustained to a body party not determined to be eligible for scheduled awards, an employee may be eligible for nonscheduled awards. This often applies to internal injuries, like to the spine or organs. An employee receiving nonscheduled awards is also eligible for 60 percent of their original weekly wages.\nIf an employee has suffered a permanent impairment, they may be eligible for wage differential. Wage differential is awarded in situations where a person is required to take on a new position as a result of their injury. For five years, or until the age of 67, they’ll be entitled to two-thirds of the difference between their original average weekly wage and what they make in their new position.\nIn the event an employee suffers from serious and permanent disfigurement to a visible area of the body, they are eligible for up 60 percent of their average weekly wages for a little over three years.\nPermanent Total Disability Benefits\nPermanent total disability (PTD) benefits are awarded for the most serious of injuries, often involving the amputation of multiple limbs or loss of eyesight or hearing. If after your treatment your doctor determines have you sustained a permanent disability, you are eligible to receive two-thirds of your weekly average pay, or whatever you were being awarded for TTD benefits, for the rest of your life.\nAdditional Benefits and Limitations\nIn addition to recovering a portion of the lost wages, injured employees may be eligible for medical benefits to cover treatments and mileage reimbursement for traveling to and from the hospital. In the event they are unable to return to their previous position, some may be eligible for vocational training.\nIf a loved one sustains a fatal workplace injury, the family may be entitled to assistance with funeral expenses and survival benefits. The survival benefits are equivalent to two-thirds the deceased person’s average weekly wages.\nEmployees seeking workers’ compensation benefits cannot recover payment for pain and suffering in Illinois.\nHow Can a Southern Illinois workers’ Compensation Lawyer Help Me Prove My Claim?\nTo file a workers’ comp claim, the employee needs to prove the following:\n- The employee worked for the employer when the accident happened\n- The employee notified the employer within the required timeframe of the incident leading to the injury\n- The employee was not violating any company policies when they were injured\n- The injury or disease was sustained on-the-job\n- The injury or disease was sustained because of employment\n- The injury or disease was made worse because of employment\nIf you’re the victim of an on-the-job injury, pursuing workers’ compensation benefits can ease your financial burden while you recover. While a medical professional’s testimony is likely to play a large roll in whether or not you’re eligible for benefits, that may not be enough. Your employer is likely to have a strong legal team supporting them who may argue about the extent of your injury, the necessity of medical treatment, or your average wages.\nWithout proper legal representation, you may not receive fair compensation. Our Marion, IL workers’ compensation lawyers have a proven track record of helping individuals receive benefits after sustaining an on-the-job injury in Marion and the surrounding Southern Illinois areas. Contact us today for a free evaluation of your claim."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:61caee0e-96f9-49fd-9528-ae62782f409e>","<urn:uuid:0799c4a9-bd30-4876-9037-640e8bf5b437>"],"error":null}
{"question":"What criteria were used to evaluate apps in the US Surgeon General's health app competition?","answer":"Apps were evaluated based on several key criteria: new thinking and creativity in health promotion; evidence-based or data-driven approach using scientific evidence; user-friendliness and interactive capabilities, including accessibility for people with disabilities; potential to help all Americans, particularly underserved and hard-to-reach communities; and the 'fun factor' in promoting healthy behaviors. Bonus points were awarded for apps that could integrate with other health applications and Personal/Electronic Health Records.","context":["The Surgeon General is challenging developers to create apps that provide tailored health information and empower users to engage in and enjoy healthy behavior.\nThe U.S. Surgeon General’s Healthy Apps Challenge will encourage the development and submission of technology applications that will complement and enhance two key aspects of the Surgeon General’s prevention agenda: The Surgeon General’s Vision for a Healthy and Fit Nation (www.surgeongeneral.gov/library/obesityvision/obesityvision2010.pdf) and the nation’s first National Prevention Strategy (www.healthcare.gov/prevention/nphpphc/strategy/report.pdf). Specifically, the challenge will highlight the ability of innovative new technologies to: (1) provide health information tailored to the needs of the user; and (2) empower users (the general public) to regularly engage in and enjoy health promoting behaviors related to fitness and physical activity, nutrition and healthy eating, and/or physical, mental and emotional well-being. This challenge is being conducted in collaboration with the Office of the National Coordinator for Health IT.\nHow to enter\nEntrants are asked to develop software applications (apps) in the following categories:\nFitness/physical activity: This category is focused on applications particularly aimed at recruiting and retaining those people who are not currently regularly exercising.\nNutrition/healthy eating: This category is focused on applications aimed at quickly prepared home meals, eating out sensibly, and getting healthy food when travelling (e.g. in airports) or out and about.\nIntegrative health: This category is focused on applications aimed at integrating multiple aspects of wellness (healthy sleep habits, boosting mental/spiritual health, lifestyle behavior change, social health, family health, community health, etc.).\nSubmissions can be existing applications or applications developed specifically for this challenge. A free version of the application must be available for consumer use. The applications should not require the purchase of additional products to be fully operational.\nAbility to empower users to engage in health promoting behaviors related to fitness and physical activity; nutrition/healthy eating; or physical and mental well-being. Apps must provide health information tailored to the needs of the user.\nEach entry will be rated for the degree of new thinking and creativity it brings to applications focusing on the health promotion, disease prevention, and wellness in the three categories outlined.\nEvidenced-Based or Data-Driven Approach\nDegree to which scientific evidence or empirical data to help assess and modify health behaviors and wellness outcomes is used. The entry must include a description of how research and science is incorporated into the evidence base underpinning the app.\nUser-friendliness and interactive capabilities. Entries should be applicable and attractive to people who are not early adopters and are not “high tech.” Additional consideration will be given for usability by people with handicaps or disabilities.\nPotential to help all Americans, esp. those who do not engage in health promoting behavior, to improve their health and fitness. Appeal to those in underserved, hard-to-reach, and Cultural and Linguistically Diverse (CALD) communities will be assessed.\nBonus points are available if entry can download personal data and integrate into other health applications, including Personal/Electronic Health Records. Entries must specify information type available (e.g. running log) and measurement unit (calories).\nFun Factor and Health “Lagniappe”\nThe “fun factor” it brings to users who are engaged in health promoting behaviors and on whether it provides “lagniagge” (something extra or a bonus) in health for the user to enhance their personal health, fitness, and/or wellness goals."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:27f4e35a-44a6-4eba-94e7-550bf544ab90>"],"error":null}
{"question":"What are the main responsibilities of Conservation Districts in Montana when it comes to stream management, and how do they help landowners protect natural water resources?","answer":"Conservation Districts in Montana have dual responsibilities in stream management: they administer the Natural Streambed and Land Preservation Act (310 Law) and provide support to streamside landowners. Under the 310 Law, they issue permits for any work affecting the bed or banks of perennial streams to maintain them in their natural condition and minimize sedimentation. They also offer crucial resources and guidance to streamside property owners, helping them make informed decisions about bank stabilization and riparian habitat management. For instance, they can advise on 'soft' bank stabilization techniques that use natural processes and native plants to protect streambanks, rather than harmful 'hard' stabilization methods like rip rap. Additionally, Conservation Districts coordinate watershed planning efforts and promote conservation education, offering various resources and technical assistance to help landowners maintain healthy stream systems while protecting their property.","context":["Episode 2 - How can I be a good neighbor to a stream?\nHere in the Bitterroot we are lucky to have the beautiful river and its tributaries running through our valley.\nI was once told that there are more tributaries entering the main stem of the Bitterroot River per river mile than any other major river in Montana. With that abundance of streams crossing our valley, there are thousands of properties in the Bitterroot that are streamside or riverfront. We have seen that owning streamside property in the Bitterroot can be peaceful or thrilling and comes with added responsibility.\nWe talked to one stream and riverside landowner about his experience. Howard Eldredge and his wife, Patti, live in Victor, and their property has about three tenths of a mile of Sweathouse Creek down to its confluence with the Bitterroot.\nHoward said of having streamside property, “…there is a great responsibility for taking care of the resource. I’m not sure that I realized that when I started here. We had some problems with bank stabilization.”\nMoving here in the early 1990s, Howard bought the property and planned out his house with the views of the stream he wanted to capture in mind. Now, he knows that they built the house too close to the stream and he wishes it was set further back from the water’s edge.\nThree large trees came down during high water and changed things on the property, with the stream moving towards Howard’s house. Using those downed trees, he had a “soft” bank stabilization project put in. “Soft” bank stabilization takes advantage of the natural processes of riparian habitat and the roots of plants help hold the banks together. They installed the root wads of those trees in the bank and planted native shrubs between them.\nAs a streamside landowner it is best to understand natural stream processes, and it can save you headaches and expense in the long run. Streams move as a part of their healthy process. When movement is inhibited by unnatural means like “hard” bank stabilization, large angular rock used to armor the bank, sometimes known as rip rap, it can cause other issues upstream and downstream of the area.\n“Some people like to have human contact right to the edge of the stream. It’s not good for the stream. It’s bad habitat for fish. The roots of a plant tend to stabilize the bank. If you look at nature, nature puts bushes alongside the banks. It’s called the riparian area. So, when you disturb the vegetation, you’re potentially causing yourself problems,” said Eldredge.\nThe riparian area is the “green zone” of water-loving vegetation found along streams and it is vital. Streamside vegetation should be preserved because it protects streambanks from erosion, protects or improves water quality by buffering the stream from pollutants, provides shade to the stream helping to reduce water temperatures, slows water flows aiding in replenishing groundwater, and provides habitat for fish and wildlife.\n“Streams are seldom all by themselves, they have systems around them. Maintaining that system is pretty important,” says Howard.\nIn dealing with his bank stabilization issues he became interested in learning more. Being retired he says, “I have time to follow that interest with service on the Conservation District which supervises the 310 law. The gist of that law says that anytime you do a project, anything you do to the bed or banks of a perennially flowing stream, you need to get a permit. To newcomers it can feel like government overreach but it’s really a resource of helpful information.”\nConservation Districts are units of local government designed to help citizens conserve their soil, water, and other renewable natural resources. In Montana, CDs have the additional responsibility of implementing the Natural Streambed and Land Preservation Act, also known as the 310 law. This law requires a permit from the local CD before any work is done in Montana’s waterways. You must apply for a free permit to do any work disturbing the bed or banks of streams. For more information contact the Bitterroot Conservation District at 406.361.6181 or firstname.lastname@example.org.\nUltimately, Howard says you can’t “fix” a streambank, and managing it may involve leaving the natural processes of a stream in place or using natural means to improve the situation. Local resources like the Conservation District and the Bitter Root Water Forum can help.\nIn response to a growing number of real estate transactions around streamside properties, and to raise awareness of how to responsibly live near streams, the Water Forum is partnering with the BCD to launch a streamside landowner resource packet. Information will first be mailed to new streamside landowners, and made available at the Chamber of Commerce, BCD, and Water Forum offices. Eventually, the groups hope to connect all streamside landowners to resources that can assist them in making management decisions that simultaneously benefit their properties and natural resources.","Stillwater Conservation District works to help private landowners conserve natural resources in Stillwater County, Montana.\nStillwater CD is also charged with administering the Natural Streambed and Land Preservation (310) Law within our district. That means that we are responsible for issuing 310 Permits for work in or near streambeds. You can read more about what constitutes a project, what permits are needed for streambed work, and download the forms for a permit application on our website, or come into the office.\nWhat Do Conservation Districts Do?\nThe 310 Law: The Natural Streambed and Land Preservation Act, also known as \"The 310 Law\", is administered by the conservation districts. The purpose of the law is to keep rivers and streams in as natural or existing condition as possible, to minimize sedimentation and to recognize beneficial uses. Any individual or corporation proposing construction in a perennial stream, must apply for a 310 Permit through the local conservation district.\nWater Reservations: Conservation districts in Montana are able to reserve water for future beneficial use. Currently, 31 conservation districts hold water reservations throughout the Yellowstone, Little Missouri, and Missouri River basins. Each of these conservation districts administers its reservation for use by individuals within the district. Applications for reserved water use can be obtained from the applicable conservation district.\nYouth Education: Conservation districts work with schools to develop conservation education curricula and outdoor classrooms by:\n- Coordinating technical and financial assistance, provide teaching aids; and,\n- Sponsoring youth conservation field days and annual camps including Natural Resource Youth Camp, Montana Youth Range Camp, Montana Range Days and Montana Envirothon.\nSaline Seep Reclamation: Conservation district supervisors in 33 counties make up the membership of the Montana Salinity Control Association. This internationally recognized organization headquartered in Conrad, Montana provides expert technical assistance in the reclamation and control of saline seeps in agricultural areas. What is a saline seep? You may have seen white, powdery-looking spots in the low areas of fields. These spots are seeps, and they have adverse effects on water quality, wildlife, agriculture production, and other resources.\nConservation Equipment/Supplies: Many conservation districts rent out a wide array of equipment for conservation practices to land users, including:\n- Tree Planters;\n- Fabric Layers;\n- Weed Sprayers;\n- Weed Badgers;\n- Conservation Tillage Drills;\n- Grass Seeders; and,\n- Tree Chippers\nIn addition, many conservation districts sell trees for conservation plantings, provide landowner maps, and provide a host of other services for conservation purposes.\nWatershed Planning: Watershed groups are locally led and work on local and regional natural resource management issues on a river basin or watershed basis. Conservation districts are often instrumental on drawing people and resources together to assist the development of these groups.\nUrban Conservation: With the rapid increase in subdivided acreages, and the resource issues associated with these small tracts, conservation districts have recently taken on a new role. Conservation districts may operate recycling programs, create and maintain interpretive trails, sponsor water projects, or provide education in natural resource management in an urban setting.\nConservation Projects/Planning: Conservation districts promote voluntary, education and incentive-based approaches to conservation. Planning and local input is an important aspect to this approach. Many projects throughout the state are undertaken to demonstrate the latest methods of riparian management, soil health improvements, water quality improvements, river and stream restoration, irrigation efficiencies, and range management. In addition, conservation districts host local conservation education events on the latest farming practices, urban conservation, weed control, and other current topics."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"}],"document_ids":["<urn:uuid:be47e751-136e-49e2-abf2-f132271b7af3>","<urn:uuid:8e3970b9-953b-4735-8b5f-f6c9e1fdedf1>"],"error":null}
{"question":"What is the force generation capacity of individual nematodes in water, and how extensive is their pest control effectiveness in soil environments?","answer":"Individual nematodes are capable of generating thrust forces of more than 28 nN in water, as measured through analysis of their movement and acceleration. In soil environments, their pest control effectiveness is remarkably extensive - they can control a broad range of soil-inhabiting insects and above-ground insects in their soil-inhabiting stage. They are particularly effective because they can actively seek out hosts, with several generations of nematodes living and breeding within a single dead insect. Each infected insect can contain between 5,000 to 10,000 nematodes, which then emerge to seek new hosts when their food source is depleted. They are so effective that they can work in the soil to kill immature stages of garden pests before they become adults.","context":["This study shows an straightforward and inexpensive technique which allows the measurement of physical properties such as for example position, velocity, acceleration and forces mixed up in locomotory behavior of nematodes suspended within a column of water in response to single wavelengths of light. The common descent was 1.5 mm/sec 0.1 mm/sec for both the inactive and live worms using 633 nm coherent light. The next example is certainly a research study of go for individual changing path through the descent within a vertical drinking water column. Drive and Acceleration are analyzed within this example. This research study shows the range of various other physical properties that may be examined using SWSI while analyzing the behavior using one wavelengths within an environment that’s not available with traditional microscopes. Employing this evaluation we approximated a person PSI-6130 nematode is certainly with the capacity of thrusting using a drive more than 28 nNis a free-living helpful soil nematode that is clearly a effective model organism for learning systems of gene legislation, advancement and more for understanding sensory biology and behavior recently. Despite having just 302 neurons, can handle complicated locomotory patterns, reproductive behaviors, navigation, chemotaxis and several various other behaviors. possess mechanoreceptors, chemoreceptors as well as identify PSI-6130 blue wavelengths of light (Ward in true period8. For the initial demo of our technique, we monitor the horizontal placement, x, as well as the vertical placement, y, of the freely swimming within a drinking water column, more than a distance around a centimeter. Specifically, we want in the vertical movement since gravity acts vertically also. The slope of the linear suit towards the vertical placement provides vertical swiftness, vy, from the nematode since it descends in water column: ? ? ?(1) The main mean square from the mistake (RMSE)9?indicates the grade of the suit and indicates if the descending swiftness is normally constant. The vertical speeds are averaged for every species and deceased worms then. Using these total results, the move, that your worms experience could be approximated. For the next demo of our technique, we chosen that didn’t descend at PSI-6130 a continuing rate unlike a lot of the worms noticed. The selected worms either turned around and swam or hovered for some time before continuing the descent up-wards. Physically, this full research study implies that the thrust of the going swimming microorganism could be calculated. Newton’s laws and regulations dictate a body that adjustments directions accelerates, which suggests a net drive, , is certainly functioning on that body10: ? ? ?(2) where may be the linear momentum and t is normally period. The acceleration from the worm is certainly directly proportional towards the drive functioning on the worm because the mass from the worm continues to be constant. As a total result, the vertical net drive is certainly: (3) where m may be the mass of the worm and ay represents the vertical acceleration. The web force in PSI-6130 the vertical direction represents the worm thrust in the same direction then. The full total thrust could be computed by firmly taking the horizontal component into consideration. Protocol 1. Planning Prepare petri plates of youthful adult as defined in previous tests involving suspension system of within a liquid filled cuvette11. On the entire time from the video evaluation, get live youthful adult nematodes right into a cuvette filled up with deionized straight, distilled drinking water utilizing a platinum get as defined in step two 2. Prepare inactive with chloroform publicity. Continue by following procedure defined for choosing live nematodes defined in step two 2. 2. Optical Set up for the Video Evaluation Assemble the experimental set up to make shadow pictures as proven in Body?1. The surveillance camera could be positioned at any length from the display screen so long as with the ability to catch a Mouse monoclonal antibody to Hexokinase 2. Hexokinases phosphorylate glucose to produce glucose-6-phosphate, the first step in mostglucose metabolism pathways. This gene encodes hexokinase 2, the predominant form found inskeletal muscle. It localizes to the outer membrane of mitochondria. Expression of this gene isinsulin-responsive, and studies in rat suggest that it is involved in the increased rate of glycolysisseen in rapidly growing cancer cells. [provided by RefSeq, Apr 2009] frontal watch from the display screen. An excellent place is certainly next towards the cuvette facing the display screen. Using at least two mirrors to steer the tunable Helium-Neon laser beam output right into a Galilean beam expander so the beam is certainly extended to a size of 12 mm. Place a couple of pinholes.","Beneficial Nematodes to eliminate grubs and insect pests\nSafe Biological Pest Control\nAsk a Question Home Nematodes Biology fact sheet 520-298-4400 Order Your Nematodes Here! email@example.comIntegrated Pest Management IPM\nNATURAL PEST CONTROL WITH BENEFICIAL NEMATODES\nBiological Control Of Pest Insects With Nematodes. Beneficial Nematodes naturally occur in soil and are used to control soil pest insects and whenever larvae or grubs are present. Like all of our products, it will not expose humans or animals to any health or environmental risks. Beneficial nematodes only attack soil dwelling insects and leave plants and earthworms alone. The beneficial nematodes enters the larva via mouth, anus or respiratory openings and starts to feed. This causes specific bacteria to emerge from the intestinal tract of the nematode. These spread inside the insect and multiply very rapidly. The bacteria convert host tissue into products which can easily be taken up by the nematodes. The soil dwelling insect dies within a few days. Beneficial nematodes are a totally safe biological control in pest insects. The Beneficial nematodes are so safe the EPA has waived the registration requirements for application.NATURE'S BEST WAY OF KILLING Grubs and Japanese Beetles. We ship Beneficial Nematodes to USA and Canada. Call 520-298-4400 for information to place order for shipment to Canada. For USA visit our website www.buglogical.com\nThough they are harmless to humans, animals, plants, and healthy earthworms, beneficial nematodes aggressively pursue insects. The beneficial nematodes can be used to control a broad range of soil inhabiting insects and above ground insects in their soil inhabiting stage of life. More than 200 species of pest insects from 100 insect families are susceptible to these nematodes. When they sense the temperature and carbon dioxide emissions of soil-borne insects, beneficial nematodes move toward their prey and enter the pest through its body openings. The nematodes carry an associated bacterium (Xenorhabdus species) that kills insects fast within 48 hours. The bacteria is harmless to humans and other organisms and cannot live freely in nature. Several generations of nematodes may live and breed within the dead insect, feeding on it as a food source. When the food source is gone, they migrate into the soil in search of a new host. When the pest population is eliminated, the beneficial nematodes die off and biodegrade. Beneficial nematodes are so effective, they can work in the soil to kill the immature stages of garden pests before they become adults.\nBeneficial nematodes infest grubs and other pest insects that are known to destroy lawns and plants.\nThe Nematodes are effective against grubs and the larval or grub stage of Japanese Beetles, Northern Masked Chafer, European Chafer, Rose Chafer, Fly larvae, Oriental Beetles, June Beetles, Flea beetles, Bill-bugs, Cut-worms, Army worms, Black Vine Weevils, Strawberry Root Weevils, Fungus Gnats, Sciarid larvae, Sod Web-worms, Girdler, Citrus Weevils, Maggots and other Dip-tera, Mole Crickets, Iris Borer, Root Maggot, Cabbage Root Maggot and Carrot Weevils.\nBeneficial nematodes belong to one of two genera: Steinernema and Heterorhabditis are commercially available in the U.S. Steinernema is the most widely studied beneficial nematode because it is easy to produce. Heterorhabditis is more difficult to produce but can be more effective against certain insects, such as th white grubs, and Japanese beetles.How beneficial nematodes work: The life cycle of beneficial nematodes consists of six distinct stages: an egg stage, four juvenile stages and the adult stage. The adult spends its life inside the host insect. The third juvenile stage, called a dauer, enters the bodies of insects (usually the soil dwelling larval form. Some nematodes seek out their hosts, while others wait for the insect to come to them. Host seeking nematodes travel through the soil the thin film of water that coats soil particles. They search for insect larvae using built-in homing mechanisms that respond to changes in carbon dioxide levels and temperature. They also follow trails of insect excrement. After a single nematode finds and enters an insect through its skin or natural openings, the nematode release a toxic bacteria that kills its host, usually within a day or two. In less than two weeks the nematodes pass through several generations of adults, which literally fill the insect cadaver. Steinernema reproduction requires at least two dauer nematodes to enter an insect, but a single Heterorhabditis can generate offspring on its own. The nematodes actively searches for insect larvae. Once inside the larva the nematodes excretes specific bacteria from its digestive trac before it starts to feed. The bacteria multiply very rapid and convert the host tissue into products that the nematodes take up and use for food. The larva dies within a few days and the color changes from white-beige to orange-red or red-brown. The nematodes multiply and develop within the dead insect. As soon as the nematodes are in the infectious third stage, they leave the old host and start searching for new larvae. Infected grubs turn color from white-beige to red brown 2-4 days after application and becomes slimy. After a few weeks, dead larvae disintegrate completely and are difficult to find.\nBeneficial nematodes are also very effective against termites, German cockroaches, flies, ant, and fleas.\nAPPLICATION: Beneficial Nematodes are very easy to use. Mix with water and spray or sprinkle on the soil along garden plants or lawn. Put the contents of the Beneficial nematodes in a bucket of water and stir to break up any lumps, and let the entire solution soak for a few minutes. Application can be made using a water-can, irrigation system, knapsack or sprayer. On sprayer use a maximum pressure to avoid blockage, all sieves should be removed. The sprayer nozzle opening should be at least 1/2 mm. Evenly spread the spraying solutions over the ground area to be treated. Continuous mixing should take place to prevent the nematodes from sinking to the bottom. After application keep the soil moist during the first two weeks for the nematodes to get establish. For a small garden the best method is using a simple sprinkling or water can to apply the Beneficial nematodes to the soil. Apply nematodes before setting out transplants; for other pest insects, Japanese Beetles and grubs, apply whenever symptomatic damage from insects is detected. Best to apply water first if soil is dry.Application and amount for 50 and 100 Mil. Nematodes. The 50 Mil. + nematodes are packed in an inert carrying material that will dissolve in water when mixed. You can use a watering can, pump sprayer; hose end sprayer and irrigation system, backpack sprayers, or motorized sprayer. The 50 and 100 Mil. Nematodes mix ½ teaspoon per gallon of water. The Large yard size: 1/2 Acre Size (50 Million) you can use up to 50 Gallons of water The Acre size 100 Mil. Nematodes you can use up to 100 Gallons of water. o For covering up to 800 square feet, place approximately 1½ teaspoons of dry nematodes in the hose end sprayer container, and then fill to the 3 gallon mark. (for Garden size) o Evenly spread the solution over the ground areas to be treated. o Continuous mixing should take place to prevent the nematodes from sinking to the bottom of the container. To avoid blockages, remove all filters from the sprayer. o You can sprinkle the soil with water again after application to move the nematodes into the soil. o Apply nematodes as soon as possible for best product performance. o Keep the soil most for the first week after application. Application for the 10 Mil. garden size Nematodes. The 10 Mil. nematodes are placed in a sponge. Place the entire sponge in a gallon of water, squeeze for a few minutes to get the nematodes out of the sponge and into the water. Discard the sponge and pour the gallon of water into the sprayer or water can and apply to the soil.Proper storage and handling is essential to nematode health. Always follow the package instructions for the best method of mixing nematodes. Formulations vary depending on the species and target insect. Nematodes can be stored in the refrigerator up to a month (not the freezer) before they are mixed with water, but once the nematodes are diluted in water, they cannot be stored. Also, nematodes shouldn’t be stored in hot vehicles, or left in spray tanks for long periods of time.\nNematodes need moisture in the soil for movement (if the soil is too dry or compact, they may not able to search out hosts) and high humidity if they are used against foliage pests. Watering the insect-infested area before and after applying nematodes keeps the soil moist and helps move them deeper into the soil. Care should be taken not to soak the area because nematodes in too much water cannot infect.Exposure to UV light or very high temperatures can kill nematodes. Apply nematodes in the early evening or late afternoon when soil temps are lower and UV incidence is lower as well (cloudy or rainy days are good too). Nematodes function best with soil temperatures between 48Fº and 93Fº day time temperatures.\nApplication is usually easy. In most cases, there is no need for special application equipment. Most nematodes species are compatible with pressurized, mist, electrostatic, fan and aerial sprayers! Hose-end sprayers, pump sprayers, and watering cans are effective applicators as well. Nematodes are even applied through irrigation systems on some crops. Check the label of the nematode species to use the best application method. Repeat applications if the insect is in the soil for a longer period of time. There is no need for masks or specialized safety equipment. Insect parasitic nematodes are safe for plants and animals (worms, birds, pets, children). Because they leave no residues, application can be made anytime before a harvest and there is no re-entry time after application.How to use beneficial nematodes: For the home gardener, localized spraying is probably the quickest and easiest way to get the nematodes into the soil. Producers ship beneficial nematodes in the form of dry granules, powder type clay, and sponges. All of these dissolve in water and release the millions of nematodes. Each nematode ready to start searching for an insect in your lawn or garden. Nematodes should be sprayed on infested areas at the time when pests is in the soil. Timing is important, or else you will have to repeat the application. Northern gardeners should apply the nematodes in the spring, summer and fall, when the soil contains insect larvae. Most of the beneficial nematodes are adaptive to cold weather. In fact , the very best time to control white grubs is in the spring and fall. If your in a warmer climate, beneficial nematodes are most effective in the summer.\nFertilizers should be avoided roughly 2 weeks prior to and after nematode application, because they may be adversely affected by high nitrogen content.\nSome pesticides work well with nematodes when their mutual exposure is limited while other pesticides may kill nematodes. Check labels or specific fact sheets to find out. Some chemicals to avoid are bendiocarb, chlorpyrifos, ethoprop, and isazophos. Fungicides to avoid are anilazine, dimethyl benzyl, ammonium chloride, fenarimol, and mercurous chloride. The herbicides, 2,4-D and trichlopyr and nematicide, fenamiphos, should be avoided as well.\nDuring hot weather release nematodes in the evening or afternoon when temperature is cooler. Release once or twice a year or until infestation subsides. Nematodes are shipped in the infectious larvae stage of their life cycle and can be stored in the refrigerator for up to 4 weeks. Always release very early in the morning or late in the late afternoon.\nWhy are these organisms beneficial?\nBeneficial nematodes seek out and kill all stages of harmful soil-dwelling insects. They can be used to control a broad range of soil-inhabiting insects and above-ground insects in their soil-inhabiting stage of life.\nParasitic nematodes are beneficial for eliminating pest insects. First, they have such a wide host range that they can be used successfully on numerous insect pests. The nematodes' nonspecific development, which does not rely on specific host nutrients, allows them to infect a large number of insect species.\nNematodes enter pest bugs while they are still alive, then they multiply inside the bugs (which eventually die) and finally burst out of the dead bodies. The number of nematodes inside a single bug (depending on the species) ranges from 5,000 to 10,000. Although you can barely see one young nematode with your naked eye, large groups of these tiny wigglers pouring out of the dead insects are easy to see. Then the nematodes wriggle off to find other insects to \"invade,\" starting the whole cycle all over again.\nSecond, nematodes kill their insect hosts within 48 hours. As mentioned earlier, this is due to enzymes produced by the Xenorhabdus bacteria.\nAlso, the infective juveniles can live for some time without nourishment as they search for a host.\nFinally, there is no evidence that parasitic nematodes or their symbiotic bacteria can develop in vertebrates. This makes nematode use for insect pest control safe and environmentally friendly. The United States Environmental Protection Agency (EPA) has ruled that nematodes are exempt from registration because they occur naturally and require no genetic modification by man. Beneficial nematodes can be an excellent tool in the lawn and garden to control certain pest insects. They can be used with organic gardening and are safe for kids and pets.\nWhat is a nematode? Nematodes are microscopic, whitish to transparent, unsegmented worms. They occupy almost every conceivable habitat on earth, both aquatic and terrestrial, and are among the most common multicelled organisms. Nematodes are generally wormlike and cylindrical in shape, often tapering at the head and tail ends; they are sometimes called roundworms or eelworms. There are thousands of kinds of nematodes, each with their particular feeding behavior -- for example, bacterial feeders, plant feeders, animal parasites, and insect parasites, to name a few.\nInsect-Parasitic Nematodes. Traditionally, soil-inhabiting insect pests are managed by applying pesticides to the soil or by using cultural practices, for example, tillage and crop rotation. Biological control can be another important way to manage soil-inhabiting insect pests. A group of organisms that shows promise as biological control agents for soil pests are insect-parasitic nematodes. These organisms, which belong to the families Steinernematidae and Heterorhabditidae, have been studied extensively as biological control agents for soil-dwelling stages of insect pests. These nematodes occur naturally in soil and possess a durable, motile infective stage that can actively seek out and infect a broad range of insects, but they do not infect birds or mammals. Because of these attributes, as well as their ease of mass production and exemption from EPA registration, a number of commercial enterprises produce these nematodes as biological \"insecticides.\"\nHow to order Beneficial Nematodes: All nematodes are not the same. Buglogical nematodes are more tolerant of high tempertures than any other brands. It is best to order biological control nematodes and have them delivered directly to you from a reliable source.. This helps insure that the nematodes you are buying are still alive. Nematodes do not live very long in storage. Therefore, buying nematodes that are stocked on a store shelf is very risky.\nSuppliers: Buglogical Control Systems, Inc. PO Box 32046, Tucson, AZ 85751-2046 Phone: 520-298-4400\nBedding, R.A. and L.A. Miller. 1981. Use of a Nematode, Heterorhabditis heliothidis to Control Black Vine Weevil, Otiorhynchus sulcatus, in Potted Plants.Ann.Appl.Biol. 99:211-216.\nDavidson, J.A., S.A. Gill, and M.J. Raupp. 1992. Controlling Clearwing Moths with Entomopathogenic Nematodes: The Dogwood Borer Case Study. J. of Arboriculture. 18(2):81-84.\nGeorgis, R. and G.O. Poinar. 1989. Field Effectiveness of Entomophilic Nematodes Neoaplectana andHeterorhabditis.Pages 213-224, In A.R. Leslie and R.L. Metcalf (eds.). Integrated Pest Management for Turfgrass and Ornamentals.United States Environmental Protection Agency, Washington, DC.\nGill, S., J.A. Davidson, and M.J. Raupp. 1992. Control of Peachtree Borer Using Entomopathogenic Nematodes. J. of Arboriculture.18(4):184-187\nKaya, H.K. 1985.Entomogenous Nematodes for Insect Control in IPM Systems. Pages 283-303, In M.A. Hoy and D.C. Herzog (eds.).Biological Control in Agricultural IPM Systems,New York: Academic Press.\nKaya, H.K. and L.R. Brown. 1986.Field Application of Entomogenous Nematodes for Biological Control of Clear-Wing Moth Borers in Alder and Sycamore Trees. J. of Arboriculture. 12(6):150-154.\nOwen, N.P., M.J.Raupp, C.S. Sadof, and B.C. Bull. 1991. Influence of Entomophagus Nematodes and Irrigation on Black Vine Weevil in Euonymus fortunei (Turcz.) Hard. Mazz.Beds.J.Environ.Hort.9(3):109-112.\nPoinar, G.O. 1986. Entomophagous Nematodes. Pages 95-121, In H.Franz(ed.).Biological Plant and Health Protection, Fortschritte der Zoologie, Bd.32.G.Fischer Verlog, Stuttgart, New York. Reprint.\nRutherford, T.A., D. Trotter, and J.M. Webster. 1987. The Potential of Heterorhabditid Nematodes as Control Agents of Root Weevils. The Canadian Ent. 119:67-73.\nShetlar, D.J. 1989. Entomogenous Nematodes for Control of Turfgrass Insects with Notes on Other Biological Control Agents.Pages 225-253, InA.R. Leslie and R.L. Metcalf (eds.) Integrated Pest Management for Turfgrasses and Ornamentals. United States Environmental Protection Agency, Washington, DC."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:21392b4d-a806-4afa-9e86-677a95b51dd8>","<urn:uuid:0ed9232d-2cef-49aa-97cd-48e2b47d3b63>"],"error":null}
{"question":"How long should you hold the standing calf stretch position when doing the exercise?","answer":"The standing calf stretch position should be held for 15 to 30 seconds before returning to the starting position. This exercise should be repeated 3 times and performed several times each day.","context":["- Prone hip extension: Lie on your stomach with your legs straight out behind you. Fold your arms under your head and rest your head on your arms. Draw your belly button in towards your spine and tighten your abdominal muscles. Tighten the buttocks and thigh muscles of the leg on your injured side and lift the leg off the floor about 8 inches. Keep your leg straight. Hold for 5 seconds. Then lower your leg and relax. Do 2 sets of 15.\n- Side-lying leg lift: Lie on your uninjured side. Tighten the front thigh muscles on your injured leg and lift that leg 8 to 10 inches (20 to 25 centimeters) away from the other leg. Keep the leg straight and lower it slowly. Do 2 sets of 15.\n- Towel stretch: Sit on a hard surface with your injured leg stretched out in front of you. Loop a towel around your toes and the ball of your foot and pull the towel toward your body keeping your leg straight. Hold this position for 15 to 30 seconds and then relax. Repeat 3 times.\n- Standing calf stretch: Stand facing a wall with your hands on the wall at about eye level. Keep your injured leg back with your heel on the floor. Keep the other leg forward with the knee bent. Turn your back foot slightly inward (as if you were pigeon-toed). Slowly lean into the wall until you feel a stretch in the back of your calf. Hold the stretch for 15 to 30 seconds. Return to the starting position. Repeat 3 times. Do this exercise several times each day.\n- Heel raise: Stand behind a chair or counter with both feet flat on the floor. Using the chair or counter as a support, rise up onto your toes and hold for 5 seconds. Then slowly lower yourself down without holding onto the support. (It's OK to keep holding onto the support if you need to.) When this exercise becomes less painful, try doing this exercise while you are standing on the injured leg only. Repeat 15 times. Do 2 sets of 15. Rest 30 seconds between sets.\n- Step-up: Stand with the foot of your injured leg on a support 3 to 5 inches (8 to 13 centimeters) high --like a small step or block of wood. Keep your other foot flat on the floor. Shift your weight onto the injured leg on the support. Straighten your injured leg as the other leg comes off the floor. Return to the starting position by bending your injured leg and slowly lowering your uninjured leg back to the floor. Do 2 sets of 15.\n- Balance and reach exercises: Stand next to a chair with your injured leg farther from the chair. The chair will provide support if you need it. Stand on the foot of your injured leg and bend your knee slightly. Try to raise the arch of this foot while keeping your big toe on the floor. Keep your foot in this position.\n- With the hand that is farther away from the chair, reach forward in front of you by bending at the waist. Avoid bending your knee any more as you do this. Repeat this 15 times. To make the exercise more challenging, reach farther in front of you. Do 2 sets of 15.\n- While keeping your arch raised, reach the hand that is farther away from the chair across your body toward the chair. The farther you reach, the more challenging the exercise. Do 2 sets of 15.\nIf you have access to a wobble board, do the following exercises:\nWobble board exercises\nStand on a wobble board with your feet shoulder-width apart.\n- Rock the board forwards and backwards 30 times, then side to side 30 times. Hold on to a chair if you need support.\n- Rotate the wobble board around so that the edge of the board is in contact with the floor at all times. Do this 30 times in a clockwise and then a counterclockwise direction.\n- Balance on the wobble board for as long as you can without letting the edges touch the floor. Try to do this for 2 minutes without touching the floor.\n- Rotate the wobble board in clockwise and counterclockwise circles, but do not let the edge of the board touch the floor.\nWhen you have mastered the wobble exercises standing on both legs, try repeating them while standing on just your injured leg. After you are able to do these exercises on one leg, try to do them with your eyes closed. Make sure you have something nearby to support you in case you lose your balance.\nDeveloped by RelayHealth.\nPublished by RelayHealth.\nCopyright ©2014 McKesson Corporation and/or one of its subsidiaries. All rights reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:bec89a67-cde6-4dc4-ad48-65bbb97e3f3f>"],"error":null}
{"question":"list all the security features available in modern luxury boat tenders","answer":"The security features in modern luxury boat tenders include bulletproofing with Polyethelene hull lining and bulletproof glass, remote secondary helm stations that create a citadel when under threat, trackers with monitoring systems for engines and equipment with a 'go home' function, and wireless kill cords with occupant sensors that alert in man overboard situations.","context":["With the ever-growing tech bubble that shapes our modern world comes a whole new scope for weird and wonderful features that make the new age of tenders a sight to behold. Gone are the days when tinted windows or plush seats are enough, it has now become a challenge to produce a tender that is beyond what anyone has seen before, with gadgets that even the wealthiest among us could not imagine. After all, impressing an owner that has seen everything, is no mean feat. The philosophy of a 21st century custom tender manufacturer is whatever you want, and it is then created. The production boat has its place on many yachts but for owners who want the best there is an increasing emphasis on coming up with personalised individual tenders, often matching the mothership. The mind-set that anything is possible opens an unlimited number of doors and means that although it can be claimed that yachts and tenders have been behind other forms of transport in the technology race, that is not necessarily any longer the case. As this goes to press Superyacht Tenders and Toys have 10 fully custom boats in build for clients, where they have been designed from the ground up to meet the client’s requirements with many features that push the boundaries. With this experience we have put together some of the latest trends and requests for readers interest.\nWhen it becomes difficult to think of what can’t be placed in a boat you know you are headed in the right direction. With underwater lights, electric swim ladders and mega sound systems becoming a regular on the spec. sheet, where do you go next? When you sit down in the office one day and start having a joke about what you might want in a tender that then turns into proposal of ideas ranging from wireless kill cords to air-conditioned seating, you start to realise what can be achieved. Matching what can actually be built and be reliable is key here but for the brainstorming there are no stupid idea’s. If an interactive deck head in the cabin of a 40ft Limousine tender doesn’t impress, then it is by no means the bottom of the barrel. As the size of your average superyacht gets larger so does the size of the tenders that service them and this often allows more room for creativity, which there is no shortage of in a world where money is no object. Even, what has become a relatively mundane feature, is now being pimped into something more. The likes of JL and Fusion creating monster speaker systems is now not only a feature in wakeboard boats but can be seen in a variety of custom tenders. One recent project had tower speakers coming up out of the hulls of a catamaran landing craft, which rotated to the beach, so the client could create a DJ booth and entertain guests on the beach for a party!\nThere are also some great features being passed over from homes, offices and cars where there are large R & D budgets to come up with new products and features. Your standard fridge on board a tender is no longer a cooler box in an under-seat locker but can be anything up to a fully stocked interactive wine fridge with media screen, and when coupled with chilled cup holders a cool drink on a hot day is never far away. One current project we even have titanium chilled champagne glasses embossed with the client’s logo as an example.\nWith all of this obviously comes a price tag and considerations into design, fabrication and support of using emerging tech and products. As well as this, as we know, custom has a price tag and thus on the other production or semi-custom tenders we have in build full custom is not appropriate for every client. There is a lot to be said for having proven reliable technology with minimal opportunity for breakdowns and getting this balance is critical.\nEmerging tech and trends\nAutomatic Volume control – The Boost Box\nThe Boost Box automatically adjusts your stereo volume when pulling riders on wakeboard boats. When you accelerate to pull a rider, the Boost Box automatically turns your stereo up to compensate for the roar of the engine and the rush of the wind.\nVentilated seats perform the opposite task of heated seats, providing three levels of cooling, rather than heating. Cool air gently circulates through perforations in the seats, keeping you cool, which is especially nice after the summer sun has been baking those black leather seats. These seats will help adjust your comfort levels quickly, so you get to your next destination without getting overheated.\nConcept seat design\nFrench car seat manufacturer Faurecia has produced a concept design for a state of the art seat. Sensors in the seat detect a driver’s heart rhythms and breathing patterns and the data is used to trigger fans (right) to blow out air and motors to give the occupant a massage. The ‘Active Wellness’ seat has built-in biometric sensors to detect if an occupant is lacking energy or is under stress – and responds by offering a suitable massage and blowing air through its ventilation system.\nInterior Deck-head Design\nOn previous tender projects, we have used the deck-head and roof as a design feature of the interior of the Limousine. Having an interactive screen on the walls, roof or even floor is a great feature able to change the mood and look of a tender according to the use. You can have rivers running along the floor, mountains on the walls or whatever lighting, including starry skies at night for instance. The only constraint is the designs you want to have.\nAn emerging application in the automotive industry is the temperature-controlled cup holder. Based on a switch setting, thermoelectric devices will preserve the warmth or chill of a beverage by temperature regulation of the cup holder. Further advantages of thermoelectric in this application include a built-in condensation management system\nOn previous tender projects we have built in LED lighting into the cup holder which can change colour according to the mood.\nThere are so many options here, that it is hard to quantify these, however some of the ones SYTT have recently been requested to do have included:\n- Automatic door sensors\n- Crew call buttons\n- Trackers with monitoring systems for engines/drives and equipment, with a go home function\n- Wireless charging in arm rests for phones and tablets\n- Hidden cleats, ensuring a 100% flush deck\n- Retractable mooring lines on springs built into the side of the boat\n- WASSP sonar sea bed sonar with recording function to use when in unknown or uncharted destinations. This remotely records charts from the tender onto the mothership\n- Bulletproofing, including lining the hull with Polyethelene and bulletproof glass\n- Remote secondary helm for in bullet proof boats to create a citadel when under threat\n- Retractable tender chocks, so that the tender can sit on deck without the need to have a separate chock. This also ensures that the tender can sit on any dock for maintenance periods or storage without the need for a cradle\n- Wireless kill cords and occupant sensors alerting any man over board instances\n- RGB lighting able to be any colour, including running lighting into teak caulking\n- Underwater speakers for clients to listen to music underwater\nSuperyacht Tenders and Toys lead the way in yacht tender sales, and also project management of tenders in build, and have a wealth of knowledge into the best designers, builders, products and services to suit a clients’ budget and requirements. We are involved in ordering and managing the build process of a wide range of tenders. Whether it is a 16m chase boat, a 10m limousine tender, a classic mahogany tender or a RIB the added value of having a specialist overseeing the build process and quality is a huge benefit to yachts. With the value often so high on custom tenders getting it right first time is very important, knowing where to push the boundaries and not. Whether purchasing just one or multiple custom tenders, using SYTT is a great way to ensure that all tenders exceed expectations and avoid any pitfalls during the build process, backed up by excellent after sales support and warranty handling."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"}],"document_ids":["<urn:uuid:dae51597-8dbe-4a15-b90f-8548ed8a95b9>"],"error":null}
{"question":"hey! what happens when u shoot in RAW format - does it save the pic settings like white balance n stuff or nah?","answer":"When you shoot in RAW format, there is usually an embedded JPEG contained within the file. The JPEG will include the camera settings like resolution, white balance, and picture style. However, when editing the RAW file, you can still adjust all these settings since the editing software applies adjustments to the RAW data before displaying the image.","context":["Cameras normally do a good job of setting white balance but how “good” this is depends on what kind of image you want. White balance can be adjusted to cope with tungsten lighting (I hope no one has any of those left), flourescent, flash, daylight, shade, cloudy etc. and in these circumstances the results can be ok.\nBut as I said above “it depends”. You might not want bright whites to show as bright whites in your photo, you might want to show them a little warmer. Let’s say you are at a gig and the lighting is coloured in a much more extreme way than using tungsten lighting. If the lighting is red then it is likely that the image you get will be red. But how red should it be? Do you want white to show as white or do you want it to be red? We already know that the camera can adjust the image so that white appears white irrespective of orange light from tungsten bulbs or the blue light of a flash. But at the gig we may want to keep the red lighting in the image, and in fact the camera does not adjust white balance in those situations to show white as white.\nOne situation where auto white balance in camera gives results where whites are no longer white is at night under street lighting.\nThe photo above (taken from Brook Hill, Baildon in November 2010!!) is the RAW file exported as a JPEG with no additional processing done to it. The white balance, sharpness, contrast, saturation and tone are based on the camera settings: auto white balance, standard picture style. Auto exposure was also used – 2s at f:3.5. ISO800. EF-S 18-55mm kit lens at 18mm.\nTo produce this image I loaded the RAW file into darktable and selected an area of snow in the foreground to set the white balance. I know snow is white. It has created a very different photo.\nIn this example we have an area that we know is white. Not all subjects have something that you know is white. In these circumstances you can add something to the shot that is white or take a photo of something white and apply the settings from that to shots taken with the same lighting. This can be done in camera or later by applying the same white balance to all images.\nThe sharp eyed of you may notice a couple of other changes. I told the software to apply changes based on the lens I used. This brightened the corners of the image to counteract the vignette from using f:3.5 with that lens. It also tweaked the image to counteract the distortion to shapes introduced by the lens.\nIf you shoot in RAW then there is usually an embedded JPEG contained within the file. The JPEG will be saved after applying the camera settings of resolution, white balance, picture style etc. The picture style will apply levels of sharpness, contrast, saturation, and color tone dependent on which style has been selected.\nWhen viewing the RAW file what you see can depend on what software you are using. Most viewing software will simply display the embedded JPEG. Editing software will apply all the adjustments recorded in the RAW file, or an associated sidecar file, before displaying it. If it is the first time viewing the file it may look similar to the embedded JPEG – it depends on what the software is programmed to do. All are acceptable ways of doing it as long as you know which way it works. It can be surprising to set the camera to black and white, or monochrome, yet when working on the RAW file later all colours are still available to work with. The editing software lets you adjust the white balance, sharpness, contrast, saturation and tone. These adjustments are applied to the RAW data before displaying the image.\nOnce all the edits have been made it is usual to export a JPEG that then locks in all the changes and is a much smaller file. As part of the editing process it is acceptable to create staging images that lock in changes so that heavy processing doesn’t have to be done every time a tweak is made. This may involve exporting to a DNG RAW file or a TIFF that does not apply the lossy compression used in JPEG."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:5d406114-edc5-4bde-bdfc-3f1f74e29264>"],"error":null}
{"question":"What's the difference between American Oak and European Oak when it comes to aging whisky and rum?","answer":"American Oak (Quercus Alba) and European Oak (which includes Quercus Robur and Quercus Petraea) have different characteristics that affect spirit aging. American White Oak has high vanillin content and is fast-growing. European Oak, particularly Quercus Robur, is high in tannins and fast-growing, while Quercus Petraea grows slower, has finer tannins, and also has high vanillin content. Both types are used in spirit maturation, with American Oak being commonly used for bourbon barrels that later age rum and whisky, while European Oak is often used for cognac and sherry casks.","context":["Zacapa's aging and blending facility near Quetzaltenango is where Lorena spends most of her time. Here is where the white oak barrels seek shelter, and where the rum will lie dormant throughout the aging period. This facility is called The House Above the Clouds and it is an important part of the complex ageing system of Zacapa.\nThis place is situated on the mountainside about 2300 meters (7,655 feet) above sea level. Several atmospheric conditions contribute to this aging process: Here the days are warm and the nights are cool.\nDisclaimer: The information contained within this article is as accurate as possible at the time of writing and is only as accurate as the information provided by Zacapa. As such some inaccuracies may be present in the text.\nThe cold climate of this city is perfect for a slow aging process and according to Lorena; the cooler altitude here helps Zacapa develop its characteristic colour, body, flavour and aroma.\nThis is in contrast with most Caribbean regions, where the rums age quickly due to their proximity to sea level. It is for this reason that Zacapa ages their rums at a high altitude thus allowing the rum and the wood sufficient time to produce what is technically known as high altitude aging.\nThe purpose of aging rum is for the wood and the rum to exchange their virtues; the aromas and flavours in the blend, creating a fusion of sensations. The oak and the rum impart their own traits and character which gives birth to a perfectly balanced spirit. During the aging process, the following occurs:\nAn extraction process during which the rums acquire the aromas and flavours of the wood barrels and also the substances impregnated on them.\nAn oxidation reaction in which chemical changes take place. The beginning of the aging process is the period when the most oxidation occurs, because there is a higher concentration of dissolved oxygen. In this process there is an increase in the aldehyde and acetic acid content.\nEtherification is one of the most important chemical reactions that take place during the aging process. This is when alcohol and acids react. This reaction gives esters to the rum and these esters are the key of the final product. This etherification happens during the ageing, but all basic ingredients start the process during fermentation\nAlcohol + Acid <------> Esters and Water.\nTime is the perfect companion for the ageing and blending, and it also varies according to the rum being created. The thin air helps the rum to age slower. To age, rum like whiskey needs oxygen. The cool climate with an average temperature of 16.7 degrees Celsius (a bit like Scotland) also helps to slow down the pace of the ageing. The slower the ageing, the finer and more interesting the body, flavour and aroma will become. Lorena said:\n“It ages slower here in the mountains than it does on hot islands. Think of simmering food slowly. You extract more flavours than you do with quick cooking. We call it Real Highland Slow Ageing.”\nBecause of the humidity in this area, the barrels never dry out on the outside (this saves angels share and alcohol).\nZacapa has a complex of warehouses that hold more than 160,000 barrels of rum. In one of them some 10.000 barrels are stacked four to six levels high in the style they use in Spain for the Sherry’s. Lorena explains that to make Zacapa as it is, they use several types of barrels in the ageing process to add as much colour, mellowness, taste and aroma as possible.\nAccording to the history books, the Celtic people were the first to experiment with bending wood by the use of heat. This gave rise to the production of barrels and the traditional curvature of the staves. This characteristic shape gives these containers their superior resistance and strength. Therefore each barrel, puncheon or cask is a work of art, a perfect balance between the harmony of the object and the simplicity of its shape.\nOak is the common name used for over 250 different species of tree from the Quercus genus. There are two species which are of special interest to the rum industry, Quercus Alba (American White Oak) and Quercus sessiliflora (French Oak).\nMost of the species of the Quercus genus have spirally arranged leaves, with a lobed margin. Their wood is hard and compact, with a yellowish tinge. Their impact in the presence of rum, however, is very different.\nThe barrel is where rum remains for many years throughout the maturation process until it becomes an aged rum. It is assembled by hand by a person known as a cooper.\nAfter being assembled, it undergoes a firing on the inside. This process is done in order to reactivate the wood that will be in contact with the rum in a marriage ritual, where they promise to remain together until they become the perfect couple. The barrels are then sealed with leaves of tul, and aquatic plant found in Lake Atitlan. These leaves will seal each crevice and prevent evaporation or leakage.\nThe reason for using the tul leaves is because they are inert and don’t impart a smell or flavour to the rum. Since they have always been in constant contact with water, they do not decompose.\nEvery time an aging cycle is completed, the barrels are inspected prior to being used again. If necessary, they are toasted again or else discarded. For the people at Zacapa, every barrel is a living being which must be nurtured and cared for in optimal conditions so that it may continue to develop.\nFor the production of rums of Zacapa, different types of barrels are used. Some are made from American White Oak and were previously used to store American Whiskey, Sherry or Pedro Ximenez Wines. Others are made from French oak that stored Cognac.\nThey each have different degrees of toasting, depending on the final characteristics that are being sought for the different rums. This stage of the production of rums allows Lorena to vary the complexity of the profile that she is seeking in the rum.\nMost of the rums from other parts of the world use a static aging process. This process consists of filling the barrels with rum, and leaving it in place for an indeterminate period of time.\nFrequently, the older barrels don’t contribute much to the aging process anymore, because the wood has been spent. Not here, because the ageing at Zacapa has much similarities with the ageing of Sherry in Spain. It is called the Solera System because the barrels containing the most aged rums are closest to the ground (= suelo) hence the name.\nThe system is not really the same, so forget what you know about the Sherry Solera System. At Zacapa the name of this system only refers to the blending of several ages in smaller and bigger barrels. Whatever you refer to, the Solera System is a dynamic method:\nIn the simplest form the Solera System are a set of barrels with liquid of the same type but different ages (Rum, Sherry) stacked on top of each other. The oldest liquid is on the bottom and the youngest is on top. This creates a system for blending.\nA fraction of the oldest Rum from the barrel on the bottom is removed and bottled. Then this barrel is filled-up with the next oldest Rum, from the barrels above. This goes on and on until the barrel on top with the youngest rum needs to be filled up with fresh new rum. The barrels are then left to age until the process is repeated.\nThis system results (by partial removal of the rum from the barrel) in uniformity of the characteristics of the rum. There are a variable number of ‘criaderas’, which are the barrels containing the younger rums. The frequency of the transfers varies according to type of rum that is being prepared. The amount of ‘criaderas’ can vary according to the type of rum being made. When younger and older rums mix, the younger one takes over the characteristics of the older one.\nLorena explains to me that the Solera System that they use is a very good system for ageing rums. She thinks this system gives better result to Zacapa than any other system. This system gives sweetness to the rum, whereas others don’t. This sweetness is derived from the different types of barrels and not from caramel (a product they don’t use by the way). Even if they did use caramel, this type is only used to give colour and not taste to the product!\nBy using a system like the Solera System (blending different ages into one fine blend) you easily loose track of the ages. The oldest part keeps on getting older while the youngest stays the same. Officially Zacapa should therefore put a different age statement on their bottles each year.\nNo matter what age the Rum is, with each step that Zacapa takes in the Blending process, there is a man from the government present. He/she makes sure that the people of Zacapa are not adding anything to their rums.\nThey also make sure that all taxes are correct. The alcohol percentage of the rum changes slowly and each year it will be a little bit less than the year before. The humidity helps to keep this to normal levels.\nZacapa Solera System\nThe speed of the Solera System depends on the type of rum that is ageing and of the development inside the barrel. Some will stay in a barrel for only 1 year (because it is a new barrel and the ageing happens quicker), others will stay in the barrel for 2 years (because the ageing happens slower or the rum needs more flavours).\nIn the drawing of the Zacapa Solera System that I made (with the original hand drawing of Lorena as guide) you see several barrels and a part of the alphabet.\nThey all represent steps in a large system. Each type of Zacapa rum goes through a system like this, but not at the same speed.\nThe Master Blender already knows from the beginning which spirit will become Zacapa 15, 23 or XO. Zacapa 15 will do all steps a bit quicker than Zacapa 23 or Zacapa XO. Zacapa XO will have an extra step to make it special and different from the others.\nA: This is the basic of the system. Here you see the old Rums in the warehouse. They are al blended together to maintain a continuing quality. Young and old Rums are blended to one product: Old Reserve.\nB: This is the fresh spirit of the last harvest. This is put into American Bourbon barrels. When you put a spirit in a used Bourbon barrel you call it “First Fill Bourbon Barrel” (FFBB). It is not the first product in the barrel, because then the name would be New American Oak Barrel. FFBB means this is the first spirit after the Bourbon, so it is the second product in this barrel. This will take between 1½ and 2 years.\nC:C, E, G, I, J and L are big wooden barrels/containers that are capable to blend several barrels. These containers have a capacity of 17.000 18.000 litre and are made of American Oak. Here in “C” you will find a blend of all “B” barrels. All barrels are fully emptied. In this big container also some Old Reserve (“A”) is added.\nD: During the storage in C, the American Bourbon Barrels (from which this rum originates) are burned again (= re-charring). This is done very heavily and they call the result as “Alligator Skin.” This extra burning is important, since now it is easier for the rum to get into the wood and take out the chocolate and vanilla flavours. It also mellows and sweetens the rum. This heavy burning makes it necessary for the workers to wear fireproof suits.\nE: The barrels are emptied in a big container again, and again there is some Old Reserve added. The re-charred barrels are no longer used.\nF: The new mixture is put in Sherry Butts. Sherry barrels give a lot of the Zacapa sweetness and fruitiness.\nG: The barrels are emptied in a big container again, and yet again some Old Reserve is added.\nH: The new mixture is put in Sherry Butts again, but this time it is a PX Sherry Butt. This type of Sherry is very sweet and it is an important component that gives the final Zacapa product lots of its honey sweetness. Pedro Ximenez barrels give notes of raisins and dried fig.\nI: Now the rum can go into 2 directions. Zacapa 15 and Zacapa 23 varieties, which will (separately) go into a big container again. After the barrels are emptied into the container again there is some Old Reserve added. Soon after that, they will add water until the alcohol percentage is 46%, ready for consumption. This lowering of the alcohol will take 6 months. No caramel is added and no chill filtering is done. That’s why the alcohol has to stay at 46%; to make sure the rums do not go blind. At 40% this would be the case.\nJ: The rums that are meant to become Zacapa XO will go into a container as well. They will also be mixed with some extra Old Reserve. But this is not the final station.\nK: This pre-XO mixture will be put in Cognac barrels made of French Limousin Oak. This way they get an extra dimension in their flavour. The XO is really different from Zacapa 15 and 23. The French oak is only used for the XO and contributes dry elegance and subtlety.\nL: Finally this Rum is mixed in a container to make sure all bottles have the same mixture. No caramel is added and no chill filtering is done.\nM: Because there is a lot of Old Reserve Rum used in the mixture, this needs to be replenished. Some of the final mixture goes into the warehouses to be used in some of the next batches of Zacapa. Which one it is, depends on the quality of the new batch.\nIf we take a closer look to the schedule, the fastest system will take approximately 6 years. This is the rum that will be used for Zacapa 15 and Zacapa 23. The quickest XO tour will be at least 8 years. All these ages are the minimum. Most ages are higher.\nWhen the blending stage is finished, the rum is taken to the bottling facilities, where the alcoholic strength is adjusted. All Zacapa rum is aged at an alcoholic strength of 60% alcohol by volume. This aging process is always supervised and controlled by the government of Guatemala. During the ageing the alcohol percentage will go down slowly, so at the end the alcohol percentage is not at 60 anymore.\nThe rum is filtered and polished and then sent to the automatic bottling line.\nDuring the entire ageing, Lorena and her team do partial tastings. The content of each barrel is analyzed for taste, aroma and colour. They do this on cask strength. They test it cask strength because they taste the Rum straight from the barrel when they walk in the warehouses. Here they make a first selection. Then they take some samples to the lab and dilute these to 20% abv.\nBefore the rum is bottled it needs to be filtered. This is done with cellulose on room temperature. This makes it necessary to keep the alcohol percentage at 46% or higher. When they lower the alcohol under 46%, the rum can become cloudy in colder places. This doesn’t mean the rum went bad, it just means the rum is cloudy. Nobody will notice a change in the flavour when this happens. During the entire process, critical control points are analyzed by the Quality Control Department which ensures that each one of Zacapa’s rums meets the high standards they have established.\nRené van Hoven (Rumpages.com)\nRefined Vices Rum Reviews:\nRon Zacapa Centenario 23","This is by no means the Oxford Dictionary of Whisky, but at any rate it is a very thorough layman’s whisky glossary.\nABV – Alcohol By Volume (percentage)\nACE – Additional Cask Enhancement. (See Cask Finish).\nAge statement – years of maturation Aldehydes – Grassy, leathery aromas\nAmerican Oak – White Oak or Quercus Alba. Fast growing wood with high vanillin content commonly used for whisky maturation\nAmylase – Enzyme used to convert starch into maltose\nAngel’s Share – whisky lost to evaporation, normally about 2% per annum\nBackset – American: alcohol-free liquid left at the bottom of the still after distillation and is added to mash tun and washback to ward off bacterial contamination.\nBalance – Overall composition of a whisky. How well the flavours mesh and complement each other\nBarley – Cereal grain used in the production of whisky\nBarrel (Cask) – Large oak vessel used to store maturing spirit.\nBere Barley – The oldest variety of barley in the British Isles, mainly to be found on the Isle of Orkney.\nBlending – The act of combining one liquid with another. Used extensively to make all whiskies except single casks. Very important in bringing various desirable characteristics to the final product.\nBlended Malt – Blend of malt whiskies from more two or more distilleries\nBlended whisky – A blend of whiskies. Typically 40% malt 60% grain\nBody – Mouth feel of a whisky\nBonded Storage – Storage facilities where liquor is held prior to excise levies\nBourbon – A type of American whiskey made from a minimum of 51% corn, distilled to no more than 80% ABV and matured in new charred American oak barrels at no more than 62.5% ABV.\nBottled in bond – An American term describing quality assurance stipulations laid out in the “Bottled-in-bond Act of 1897”. Whiskey Bottled-in-bond must come from one distillation season (January to December), one distiller at one distillery, be stored in a US bonded warehouse for 4 years minimum and bottled at 50% ABV\nBrewing – The process of adding yeast to wort and fermenting this into wash\nBurr Oak – Species of white oak native to North America used for whisky maturation\nButt – 477.3 liter (105 gallon) Sherry cask – roughly twice that of a hogshead\nCask Strength – whisky bottled at barreling strength, typically between 55 – 70% ABV\nCasks – oak barrels used for maturation\nCask finish – The practice, in maturation, of transferring near mature whisky to another cask to capture some of that cask’s characteristics. E.g. Port or sherry finish.\nCharring – Burning the inside of a barrel to ‘activate’ the wood. The char acts as a filter and imparts flavours to the liquid.\nChill-filtration – A form of filtration used to “clear” (remove flock) whisky by lowering the temperature of the liquid between -10 and 4 degrees celcius and forcing the liquid through a fine filter. A necessary evil to produce huge volumes, but scorned for stripping whisky of it’s viscosity and colour, thereby necessitating the need for spirit-caramel\nColumn still – type of still used for grain whiskies. Capable of distilling continuously by re-heating of the liquid. Very efficient and can distill to as high as 96% in a single batch, vs a pot still that only does about 25 – 30% at a time.\nCondensation – Change of the physical state of matter from gas to liquid\nCongeners – Substance produced during fermentation of alcoholic beverages said to provide the majority of taste characteristics present in whisky\nCooper – the man who makes casks for a living\nCorn – A grain used in whisk(e)y production. Very popular in America where it forms the basis of Bourbon. Out of all the grains, corn yields the highest level of alcohol.\nDistillation – physical process of separating the alcohol and flavours from water in the wash\nDram – Measurement of whisky (glass)\nDramming – Drinking whisky\nDressing – The removal of detritus or ‘combings’, principally rootlets form the malted grain\nDrum Malting – Modern alternative to floor malting of barley. Basically a large, constantly turning drum\nDunnage Warehouse – Traditional style warehouse, with a slate roof, earthen floor and thick stone or brick walls. Expensive to run, but believed to produce a better whisky due to better airflow and a higher humidity.\nEnzymes – are large biological molecules responsible for the thousands of metabolic processes that sustain life. Malting increases the incidence of enzymes which convert starch into fermentable sugars.\nEsters – Fruity, flowery aromas\nEuropean Oak –\nType 1. Quercus Robur also known as Pedunculate Oak. Widely used for brandy and sherry, fast growing and high in tannins\nType 2. Quercus Petraea or Sessile Oak. Slower growing than Quercus Robur, with finer tannins and high vanillin content.\nEvaporation – Change of the physical state of matter from liquid to gas.\nExcise Duty – Government tax imposed on beverage alcohol.\nFeints (Tails or Aftershots) – Unusable end of the second distillation run. Mainly made up of fusel alcohols and highly unpalatable.\nFermentation – A metabolic process where yeast, a living organism, feeds on sugar creating acids, gases and/or alcohol as by products\nFinish – Aftertaste of a whisky, measured in length, the longer the better, provided that it is in fact a nice taste\nFirst fill – A barrel that is being used for the first time for whisky maturation. It would have been used in a different industry to hold another liquid previously, sherry, port or bourbon for example. These casks impart a very strong flavor on the spirit because there is still some of the previous liquid soaked into the wood. The cask’s effect on the spirit diminishes every time that it is used as there is less and less of the original liquid present in the wood.\nFloor malting – The traditional method used to malt grain on a malt house floor. Replaced in modern times (since the 1940s) in favour of more efficient processes such as drum malting.\nForeshots (Heads) – Methanol rich first part of the second distillation run.\nGorda – Large sherry cask\nGrain Whisky – whisky made from barely (malted or unmalted), wheat, maize or rye and distilled in a column still\nGreen Malt – Germinated barley that has yet to be dried\nGrist – Course flour produced by finely grinding malted barley to produce wort\nGrist Mill – used to make grist\nHeart – Highly desirable middle part of second distillation. Contains sweet ethanol, perfect for whisky\nHogshead – 238.7 litre (52.5 gallon) cask IB – Independent bottling\nKiln – Used to dry green malt, thereby halting germination\nLow Wines – the produce of the first distillation or “wash run”\nMadeira pipe – A 418 litre (92 gallons) cask used to mature madeira.\nMalt – germinated barley that has been dried by heat\nMalt Whisky – Whisky made from malted barley and water, distilled in a pot still, matured in oak casks.\nMalting process – process of inducing and halting germination in barley grain\nMalt Kiln – used to halt the germination process with heat\nMarrying – consolidation of several casks for further maturation\nMashing – the process of preparing the malt for brewing. Involves milling the malt into grist and steeping the grist to form wort\nMash Bill – Like a Mash Invoice, list of grains and proportions thereof used in American whiskey production. Typically a mix of corn, wheat, rye and barley\nMash Tun – A vessel used to convert the starches in crushed grains (grist) into sugars for fermentation\nMaturation – The whisky is left a minimum of three years but usually between 8 and 25 years in wooden barrels to mature\nNAS – No Age Statement\nNew Make – Un matured “new” spirit. Basically unfiltered grain vodka at this point.\nNose – refers to both the smell of a whisk(e)y and the act of smelling a whisk(e)y\nOB – Official / owner bottling\nOctave – 63 litre sherry cask\nOverproof – ABV higher than 46%\nPalate – refers to the taste of a whisk(e)y\nPeat – An accumulation of partially decayed vegetation formed in wetlands and used traditionally used a fuel. Used in whiskey as part of the kiln drying process in malting to impart smoky characteristics to the malt which later come out in the whisky. Islay is well known for its heavily peated whisky.\nPeated Malt – Malted barley that tastes strongly of peat smoke\nPhenols – Peaty, smoky aromas also known as carbolic acid. Represent the chemical presence of peat in whisky\nPot still – a traditional type of still used for malt whisky and some American whiskeys. Made of copper and only capable of doing single batches at a time as opposed to a continuous still. Based on the Alembic still, created by Arabian alchemist Jabi ibn Hayyan in the eight century\nProof – American system of measuring ABV\nPuncheon – 450 litre Sherry cask\nPort Pipe – 477.3 litre (105 gallon) cask initially used to mature port\nPPM – Parts per million. Used to note the phenolic content of a substance (phenol parts per million).\nPure Malt – Another word for blended or vatted malt whisky.\nPure Pot Still – A variety of Irish whiskey referring to whisky, as that name suggests, that is distilled in a pot still and is not blended. Barely is used, both malted and unmalted.\nRefill – This refers to casks that have been used a number of times to mature whisky and no longer imparts any character to the spirit\nReflux – The alcoholic vapour that doesn’t make it out of the still because it condenses and falls back into the still before reaching the condenser. Higher reflux produces a lighter, more delicate spirit\nRegions – This refers to a geographical area in which whisk(e)y is produced. Each region imparts its unique influences on its whisky\nRye – A grain used in whisk(e)y production. Very popular in America and said to bring spicy characters\nQuaich – Traditional drinking cup\nScotch – whisky made in Scotland\nShell and Tube Condenser – The most widely used method of condensing alcoholic vapours. It is a copper tube surrounded by small copper pipes that are fed with cool water.\nSingle Malt – malt whisky from one single distillery that has not been blended with whisky from another distillery\nSkalk – First dram of the morning\nSlainte – Health! Gaelic drinking toast\nSMSW – Single Malt Scotch Whisky\nSolera – A process for aging liquids by fractional blending. Basically the oldest barrel is part emptied for bottling and topped up with liquid from the second oldest barrel. This process continues all the way back to the first barrel which is filled with new spirit. Eventually each barrel becomes a blend of various ages.\nSpirit – distilled alcoholic beverage with an ABV above 20%\nSpirit Caramel (E150) – tasteless liquid used to colour whisky, giving it an older appearance. Popular with whiskies that have been chill-filtered\nSpirit Safe – Locked container through which to view spirit leaving the still. Locked because of excise reasons, so that every drop that comes out of the still is accounted for and taxed. In the UK the key is held by HMRS.\nSpirit Still – The second still, used to re-distil the low wines produced by the wash still. Called a spirit still because it distils spirit.\nStraight Bourbon – American term referring to bourbon aged for at least two years and distilled to no more than 80% ABV.\nStraight Rye – Same as above, but this time it is whiskey made from at least 51% rye, distilled to no more than 80% ABV and aged for at least two years\nStraight Wheat – Same as Straight Rye, only wheat this time.\nStraight Whiskey – You should be able to guess this by now…American term for Whiskey distilled from no more than 51% of any one grain, aged for at least two years and distilled to no more than 80% abv.\nSnifter – Nosing glass\nSpirit thief – Also known as a valenche is a large pipette used to draw spirit from a barrel\nTumbler – Not to be used to appreciate whisky unless you are a Philistine\nTerroir – A wine term used to describe the combined effect that a region’s location (climate, topography, aspect, altitude etc.) has on the wine. The same applies to whisky.\nTriple Distillation – Distilling the liquid three times instead of the standard double distillation. The Irish and the Lowlands favour this method and it delivers a smoother, lighter spirit.\nVatting – Blend of different whiskies\nVatted Malt – A blend of single malts from two or more distilleries\nVintage – Year of distillation\nVirgin oak – Oak that has not yet been used to mature alcoholic beverage. While it is rarely used for Scotch, all Bourbon has to be matured in virgin oak\nWash – A rudimentary beer brewed from fermenting cereal grains. A liquid can only be distilled if there is alcohol present, thus the production of beer is the first stage in the production of whisk(e)y and this beer is called wash\nWash Receiver – A vessel used to collect the wash prior to distillation in the Wash Still.\nWash Still – The pot still used to distil the wash\nWashback – The fermenter is which the wash is brewed\nWheat – A very common cereal grain used to make bread, flour and beer. Used to make grain whisky and American whiskies and ads smooth sweetness to the whisky\nWhite Dog – American word for New Make\nWorm Tub – A condenser made of a large tub of water in which a coiled copper tube is submerged. This cools the alcoholic vapour and causes them to condense into liquid\nWort – Fermentable solution containing maltose, made from steeping grist\nYeast – A microorganism from the Fungi kingdom central to alcohol production. Yeast causes fermentation by feeding on sugar and creating carbon dioxide and alcohol as by products\nYO – Years Old"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:96b154a6-7a73-437d-9228-66859eb96d01>","<urn:uuid:629d8c07-977b-400c-bf31-cc6db6c67dd0>"],"error":null}
{"question":"Que puede decir sobre los door snakes/what was the Door Snake Project about?","answer":"The Door Snake Project was a collaborative art installation featuring 14 local and international artists who each created their own version of a door snake (traditionally used to block drafts under doors). The artists mailed their creations to Melbourne, where they were installed in 15 glass cabinets with fabric environments, creating a surreal underground reptile enclosure.","context":["Stage 1: Survival and Adaptation (detail installation view)\n(plants, water, beakers, labels)\nIn preparation for her upcoming solo show at Platform in June, Melbourne artist and amateur botanist Bernadette Trench-Thiedeman created an experiment in the infamous leaky cabinet. The second last cabinet in the subway leaks after every big rain pretty much making in unusable for artists but a few brave artists have ventured to use the space. So after applying for a show to create an underground garden, we offered Bernadette the chance to experiment with plant survival in a space without sunlight. Installing a series of plant clippings partly immersed in science beakers and test tubes of water, Bernadette made Stage 1 of her project to test how the plants would survive. Having been set in the cases at the end of February, not only have they survived for two months without sunlight but the plants have even grown under the flouroscent tubes.\nWe showed (and even sold) some work at Art Melbourne from April 18-20 held at the Melbourne Exhibition Buildings. We were invited by our patrons – the City of Melbourne Arts & Culture (with thanks to Candy Mitchell) – to share a stall with Kings ARI and the newly independent Sticky Institute. We invited some of our favourite Platform artists to show at Art Melbourne including Sylvia Jeffries, Rus Kitchin, Contextual Villains, Anna Nilsson and Daniel Dorall.\nEnd of The Beginning (installation view)\n(photographic prints, spray paint, texta, gaffa, paper, televisions, laptops, dvds)\nRus Kitchin, our special guest artist for April, pulled together a vast body of reconstituted photographic and digital work which he used to wallpaper the large Vitrine space. Rus painted over the surface of the photo-wall in a graf style for which he first earned his rep on the local art scene. Installing two large televisions and four laptop computers, the imagery framing the back wall was then re-presented in digital looping formats while the remnants of the work were left aesthetically randomised across the floorboards. A wildly expressive show from one of Melbourne's hottest young artists, End of the Beginning is one of our favourite Vitrine shows.\nErica Tarquinio & Madelaine Farrugia\nThe Book of Proverbs (detail)\n(pencil on paper)\nTwo young emerging artists, Erica Tarquinio and Madelaine Farrugia, installed a series of drawings in the Sample window during April. The works on paper took their inspiration from sometimes popular (sometimes clichéd) proverbs taken from around the world. Erica and Madelaine then interpreted the proverbs in a literal and playful illustration style, installing the pieces like pages fallen from a children's book.\nFresh Fields (photographic detail)\n(dry-point print on paper)\nMelbourne printmaker Dominique Mitchelson took time out of her busy teaching schedule to install a series of beautifully detailed prints of magical 'anthropomorphs' in the Majorca Building cases for April. Dominique says of her latest installation:\n\"This current artwork is inspired by old fashioned illustration and a love for fine detail. I have re-created textures and designs of fabric within the imagery. The prints are based on my interest in animals and birds. The fox would have to be one of the most exquisite and common of animals – they are incredibly smart and very dedicated to looking after their families. The swallow is also a common bird, but it is so quick and fleeting in flight which makes it hard to see how truly beautiful these birds really are. With this work, I hope that the viewer can be transported from the city to another environment, where a fox goes exploring with a lantern light or swallows play on a maypole.\"\nDominique will be showing soon with Outré Gallery on Elizabeth Street Melbourne.\nThe Door Snake Project (installation detail view)\n(mixed media incl. fabric, foam, cardboard, paint)\nThe Door Snake Project stemmed from an initiative by Lori Kirk, a Melbourne based artist who developed the April collaborative art installation at Platform with 14 local and international contemporary artists. The artists each created their own unique version of the pop-crafty door snake, traditionally used underneath doorways to block unwanted drafts. The completed door snakes were posted from each artist’s country of residence, arriving in Melbourne to be installed at Platform. Lori then created the accompanying fabric environments within the 15 glass cabinets, creating a surreal underground reptile enclosure.\nThe Door Snake Project Artists\nAltynai Osoeva, Lauren DiCioccio, Julia Adzuki, Ben Griffiths, Lori Kirk, Ruth Fleishman, Massimo Palombo, Cynthia Johnston, Elizabeth Temple, Kellie Lyler, Isabell Walsh, Judy Oakenfull, Rachel Carlisle, Helen Brooker and Becky James."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"}],"document_ids":["<urn:uuid:f1901ca7-5bf1-46f0-b780-61aa9d1ec825>"],"error":null}