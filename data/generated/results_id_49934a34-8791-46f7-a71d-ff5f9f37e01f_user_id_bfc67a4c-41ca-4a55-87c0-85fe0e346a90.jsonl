{"question":"How do tide tables help with beach activities, and what natural forces affect tide patterns?","answer":"Tide tables help with beach activities by outlining dates, times, and tide predictions for an area, allowing people to plan activities like swimming, surfing, and beachcombing, and helping prevent getting stranded during shore walks. They show high/low tide times and heights, wave intervals, and swell directions. As for natural forces affecting tides, they are primarily influenced by the Moon's gravity causing ocean bulges on sides facing and opposite to it, creating two daily high tides and two low tides every lunar day (24 hours and 50 minutes). The Sun's gravity also affects tides, causing spring tides during new/full moons and neap tides when Sun and Moon are perpendicular. Local factors like wind, weather, and coastal geography can further impact tide patterns.","context":["Have you ever suffered from the utter devastation of watching the merciless ocean creeping forward to consume your half-finished sandcastle? Have you ever eagerly arrived at the beach, surfboard under arm, only to find wimpy waves or perilously exposed rocks?\nIf you have endured such hardships at the hands of the ever changing tide, allow me to introduce you to your new best friend, the tide table.\nWhat is a tide table?\nThis handy tool outlines the dates, times, and tide predictions for an area. Tide tables are useful for determining when tide changes will happen on a certain day or for finding the best tidal conditions for your favourite water activity. Tide tables can also keep you safe by making sure that you don’t get stranded during a walk along the shore when the tide comes in.\nKnowing how to interpret the data on a tide table is helpful for anyone who ventures to the water. Swimmers, surfers, anglers, beachcombers, boaters, and many others will benefit from learning to read a tide table.\nHow to read a tide table\nAlthough the format and appearance of tide tables may differ, most will include the same three basic pieces of information: date, time, and tide height, organized in a grid-like structure.\nStep 1: Know what you’re looking for\nThere are two main ways you can find information on a tide table. You can either look for the date and time you’d like to know the tides for, or you can look for the type of tide you want (or do not want) to encounter when you get to the beach.\nFor instance, if you want to go swimming on Saturday afternoon, you can find out when the high and low tides will happen on that day and time. Alternatively, if you want to go surfing but don’t have a specific day in mind, then you can find out when there will be good tide conditions for surfing in the near future and choose a day and time that works for you.\nStep 2: Understanding chart datum\nAn important part of reading a tide table is fathoming–pun intended–chart datum. It’s a key concept in reading a tide table, because it’s the figure that tidal heights are measured against. Fear not, it isn’t as complex as it sounds!\nPut simply, chart datum is the average of high and low tides. Tide charts note the depth of the water at a given time, then add it to the Chart Datum to create this average of tide heights. The heights you will find on a tide table are the measures of the water above or below the chart datum, which is usually the lowest average tide. The tide will seldom fall further than this point.\nStep 3: Noting the tides and times\nFind out what time high tide and low tide will occur and observe the difference in height between the two. The height will be in meters (m) or feet (ft).\nThe high tide value will be the greater number of the two, and it will tell you how high the tide will be above the chart datum at its highest point. Sometimes, the high tide will have a plus sign (+) in front of it.\nThe low tide value will indicate how low the tide will be below the chart datum at its lowest point. To determine the low tide, look for the smaller, or sometimes even negative (-), number.\nThe time may be stated in military time. If it isn’t, make sure you’ve discerned whether a tide is in the morning or in the evening to avoid unwanted surprises!\nOther information you may find on a tide table\nSome tide tables will supply additional data, such as the interval of the waves or the direction of a swell. This information is particularly valuable to swimmers and surfers.\nThe wave interval tells you, on average, how many seconds pass between each wave. This is calculated by measuring how long it takes waves to hit a specific marker, usually a buoy.\nSwell direction lets you know what angle waves will approach from. Swell direction is marked on a tide table with a letter. For example, ‘W’ means that waves will come from the west.\nThings to keep in mind when you read a tide table\nTides change daily, so make sure that you’re using an up-to-date tide table. Tides can also differ greatly among sites that are very near one another, so ensure that your tide table is as area-specific as possible.\nFinally, tide predictions are just that… predictions! Aside from the moon, sun, and gravity, other factors affect tides, such as local weather and wind patterns. Be aware that the tides on the beach may not follow the tide table perfectly, and always exercise caution in or by the water.\nArmed with your new knowledge about tide tables, you’re ready to face the water without worry. Never again will your sandy creation be thwarted by unanticipated tides!\n© SWIM DRINK FISH CANADA, 2011 - 2020","What Ocean Movements Are Affected by the Moon's Gravity?\nWhile the Moon can make for interesting viewing at dusk, dawn or during the night -- it also has a major effect on the motions of our planet's oceans. More specifically, the Moon's gravity is the major influence on tides seen in coastal areas. Tides start in the middle of the ocean and move out toward the continents.\nThe Moon's Pull\nThe Moon's gravitational power is largely responsible for movements of the ocean that are seen through the rising and falling tides. The Moon affects the oceans on the sides of the planet facing and facing away from it -- causing them to bulge away from the Earth. On one side, the ocean bulges toward the Moon; on the other side, the ocean bulges away from the Earth because the planet is slightly being pulled toward the Moon. These bulges translate into high tides on the sides facing and opposite the Moon. Conversely, the oceans on the other sides of Earth, which are not facing or opposite the Moon, do not bulge, and are said to be at low tide. Taken together, this means that each day is marked by two high-tide bulges and two low-tide non-bulges as the Moon rotates around the Earth.\nMotions of the Moon\nAs the Moon circles the Earth, its angle relative to the equator changes over the course of the year. This phenomenon, known as declination, also affects the tides as their intensity tends to follow the angle of the Moon. Tides are also affected by the fact that the Moon takes slightly longer than one day to revolve around the Earth -- 24 hours and 50 minutes, or one lunar day. This means that coastal areas see two high tides and two low tides every lunar day, with the oscillation from high to low tides set at 6 hours and 12.5 minutes.\nEffect of the Sun\nThe gravity of the Sun also has an effect on coastal tides. When the Sun and Moon are aligned, as they are during a new or full Moon, the Sun exaggerates the tides: causing high tide to be extra high and low tide to be especially low -- a phenomenon called spring tides. However, a week later -- when the Sun and Moon are perpendicular to each other -- the Sun has a reductive affect on the lunar tides, producing what are called neap tides. Two sets of spring tides and neap tides take place each lunar month.\nOther Factors That Affect Tides\nDue to the position and irregular shapes of the continents, tidal patterns vary from coast to coast -- with some areas seeing two high tides and low tides each day while other areas see only one high tide and low tide per day. On a local scale, tides can be affected by the depth and configuration of a coastal area. Inlet bays and estuaries can significantly affect the severity of lunar tides. Wind and weather patterns can also play a significant role.\n- Discovery.com: How Does the Moon Affect the Ocean Tides?\n- National Oceanic and Atmospheric Administration: Changing Angles and Changing Tides\n- National Oceanic and Atmospheric Administration: Frequency of Tides - The Lunar Day\n- National Oceanic and Atmospheric Administration: Tidal Variations - The Influence of Position and Distance\n- National Oceanic and Atmospheric Administration: Types and Causes of Tidal Cycles – Diurnal, Semidiurnal, Mixed Semidiurnal; Continental Interference\n- National Oceanic and Atmospheric Administration: What Affects Tides in Addition to the Sun and Moon?\n- National Oceanic and Atmospheric Administration: What are Tides?\nBrett Smith is a science journalist based in Buffalo, N.Y. A graduate of the State University of New York - Buffalo, he has more than seven years of experience working in a professional laboratory setting."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:fa4421fc-c17b-4c30-ba29-4d96a409cfc5>","<urn:uuid:d15670fc-8cfe-4012-9c01-3565b9408002>"],"error":null}
{"question":"Can you compare how algorithm bias manifests in facial recognition versus predictive policing applications?","answer":"In facial recognition systems, algorithm bias emerges from training data that doesn't properly represent diverse user populations, potentially leading to discrimination based on factors like skin tone. For predictive policing, algorithm bias manifests when data systems create feedback loops that stigmatize poor communities - deploying more resources to designated 'hot spots' can lead to discovering more minor crimes in these areas, reinforcing the original bias. Both cases demonstrate how unconscious biases in machine learning can perpetuate discrimination, though facial recognition bias primarily affects individual identification while predictive policing bias impacts entire communities.","context":["- The potential to misuse facial recognition software (FRS) has raised fundamental questions around ethical use, privacy and accessibility.\n- Those developing facial recognition software can develop standards to help ensure ethical and responsible technology design.\nFacial recognition software (FRS) has hit the headlines of late and rightly so. Its potential for misuse has raised fundamental questions around ethical use, privacy and accessibility. In some cases, major companies have even halted development and some NGOs have called for total bans.\nSuch moves have moved some to push for more standardization in the development of facial recognition technology as many realize that it can be used to conduct unacceptable infringement of liberties. The chief of police in one major US city even pointed out that FRS’ low level of accuracy could lead authorities to misidentify someone 96% of the time if officers were to leverage the technology just by itself.\nTo help mitigate these fears, those developing facial recognition software can develop standards to help ensure ethical and responsible technology design. Such human-centered standards can help forge trust and protect against bias or misapplication.\nWhat is algorithm bias?\nAlgorithm bias, also called machine learning bias, is a phenomenon in which algorithms can act in a discriminatory or prejudiced manner due to misplaced assumptions during the learning phase of their development.\nUnconscious biases regarding gender, race and social class can make their way into the training data fed by programmers into “machine-learning algorithms”, systems which constantly improve their own performance by including new data into an existing model.\nThese biases can be observed in the algorithm’s output: erroneous reflected assumptions that can result in embarrassing news coverage.\nSome recent stories about accidental algorithm bias include:\nOver the past year, the World Economic Forum Artificial Intelligence team found that responsible limits can be set on facial recognition technology. Its AI team worked alongside public and private sectors to develop a unique FRS framework anchored in responsible development for engineers and deployment for policymakers.\nThe framework is the first to go beyond general principles and to operationalize commercial use cases under the banner of ethical and responsible AI. It comprises four simple steps:\n1. Draft principles that define responsible use.\nFacial recognition has gained special importance as COVID-19 has pushed contactless services as a safe way to verify identities. While touchless and passwordless technology is the path ahead, companies can pick the direction of travel.\nTo guide that journey – and prevent misuse – companies can draft a set of ‘Principles for Action’ that define what constitutes responsible facial recognition technology. These principles should focus on 7 key areas to protect users from any misuse of the technology and be developed in consultation with industry players, policymakers, advocacy groups and academics with the goal of reaching a high level of consensus between these stakeholders. These areas include:\n- Privacy: Protecting end user’s personal data\n- Risk assessment: Building processes to mitigate risks of errors and its consequences\n- Proportional use of technology: Assessing the trade-off between usage risks and opportunities\n- Accountability: Defining the responsibility of platform providers and organizations using the technology\n- Consent: Naming the rules for free consent by end-users\n- Accessibility: Making sure the technology accommodates differently-abled people\n- Adaptability: Offering fallback options as situations change and having humans on deck for oversight\nTo ensure the effective adoption of these Principles for Action, they must be embedded at the core of business operations through internal processes and oversight to lessen potential risks.\nTaking these points into consideration can ensure a high level of security and respect of data privacy. Their application could manifest mechanisms such as autodeleting biometric data after 24 hours, conducting impact assessments among designers to identify any potential bias, or designing public signage so passers know when an FRS system is running.\n2. Design systems to support product teams.\nIn developing ‘responsible by design’ technologies, product teams will need special support, best practices, and systems for testing or quality control. Factors to consider when designing systems to provide this support will include:\n- Facial recognition justification\n- A data plan matching end-user characteristics\n- Bias risk mitigation\n- Methods to inform end-users\nTo take each factor into account, strong collaborations between the organization using the technology and the platform provider will help ease any risks. For example, in bias risk mitigation, algorithms provided by platform providers must be trained with data that will truly represent the users of the service and the system tested to ensure it acts properly before release. To do so, organizations should assess the diversity of its end-users and share it with the platform provider to train the algorithm accordingly.\nHave you read?\nWhile acknowledging that biases may still occur, organizations should anticipate this risk by building fallback systems that are robust enough to provide the same level of service to anyone and thus diminish any form of discrimination due to algorithm biases.\n3. Auto-assess your work.\nBest practices and principles cannot exist in a vacuum. Auto-assessing is a necessary step to check the Principles of Action are being respected and to identify potential blind spots. To that end, an assessment questionnaire can help groups test how well the systems they designed matched the standards they’d set.\nFor example, when it comes to assessing their proportional use, organizations can check if they have considered alternatives to FRS and documented the reasons they have rejected them. They can also analyse the level of false negatives and false positives and determine if this level is suitable for the use case they deploy. They can also compare their results between skin tones to attest that the system doesn’t produce any form of discrimination.\nWhen gaps are found, the assessment process can drive teams to refer to their best practices to take internal actions to bridge gaps and improve their level of accountability and trustworthiness for their customers.\n4. Validate compliance through a third-party audit.\nToo often, companies only rely on home-made labels to build transparency among users. The quality of those labels can sometimes be questionable, and also pose a systemic risk of mistrust for the industry that could consequently undermine wider efforts to build transparency.\nBeing audited by a third and independent party to deliver certification in line with the Principles for Action could be one path forward. Lessons from the accounting industry can be applied, adding additional transparency and safety through independent and accredited agencies. [For example, to draft our FRS framework, the World Economic Forum Artificial Intelligence team partnered with AFNOR Certification, the French leader of certification and assessment services.]\nThese evaluations should occur right after the deployment of systems for end-users and be conducted regularly to attest to the respect of standards over time. By doing so, certified organizations will be able to communicate among their customers to show their compliance across a range of requirements.\nWhile certification is widely used in many industries and services, facial recognition, despite its high level of scrutiny, is still deployed without any existing certification. We believe that transparency and trust in this field can only be achieved with such mechanisms.\nThese four steps can help inform the design of responsible systems for flow management use cases in FRS. They can also ensure that their designers and users are effectively compliant with these achievable principles.\nCertification is a reachable step towards the regulation of FRS. The cooperation between industry actors, policymakers, academics and civil society on this policy project has shown a strong willingness for standards for the commercial use of FRS.\nYet, Governments have to step in to adopt bills that will ensure a sustainable regulation of FRS along with international standards defining what a responsible use of the technology should be. They also need to address the thorny question of FRS for law enforcement and determine the right levels of oversight and accountability for its related use-cases.\nThe recent call for regulation by organizations such as IBM, Microsoft and Amazon should be followed by new regulations or guidance or we’ll likely see a deployment of FRS that will lead to mistrust and consumer avoidance.","When you browse online for a new pair of shoes, pick a movie to stream on Netflix or apply for a car loan, an algorithm likely has its word to say on the outcome.\nThe complex mathematical formulas are playing a growing role in all walks of life: from detecting skin cancers to suggesting new Facebook friends, deciding who gets a job, how police resources are deployed, who gets insurance at what cost, or who is on a “no fly” list.\nAlgorithms are being used—experimentally—to write news articles from raw data, while Donald Trump’s presidential campaign was helped by behavioral marketers who used an algorithm to locate the highest concentrations of “persuadable voters.” But while such automated tools can inject a measure of objectivity into erstwhile subjective decisions, fears are rising over the lack of transparency algorithms can entail, with pressure growing to apply standards of ethics or “accountability.”\nData scientist Cathy O’Neil cautions about “blindly trusting” formulas to determine a fair outcome. “Algorithms are not inherently fair, because the person who builds the model defines success,” she said.\nO’Neil argues that while some algorithms may be helpful, others can be nefarious. In her 2016 book, Weapons of Math Destruction, she cites some troubling examples in the United States:\nPublic schools in Washington, D.C. in 2010 fired more than 200 teachers—including several well-respected instructors—based on scores in an algorithmic formula which evaluated performance.\nA man diagnosed with bipolar disorder was rejected for employment at seven major retailers after a third-party “personality” test deemed him a high risk based on its algorithmic classification.\nMany jurisdictions are using “predictive policing” to shift resources to likely “hot spots.” O’Neill says that depending on how data is fed into the system, this could lead to discovery of more minor crimes and a “feedback loop” which stigmatizes poor communities.\nSome courts rely on computer-ranked formulas to determine jail sentences and parole, which may discriminate against minorities by taking into account “risk” factors such as their neighborhoods and friend or family links to crime.\nIn the world of finance, brokers “scrape” data from online and other sources in new ways to make decisions on credit or insurance. This too often amplifies prejudice against the disadvantaged, O’Neil argues.\nHer findings were echoed in a White House report last year warning that algorithmic systems “are not infallible—they rely on the imperfect inputs, logic, probability, and people who design them.”\nThe report noted that data systems can ideally help weed out human bias but warned against algorithms “systematically disadvantaging certain groups.”\nZeynep Tufekci, a University of North Carolina professor who studies technology and society, said automated decisions are often based on data collected about people, sometimes without their knowledge. “These computational systems can infer all sorts of things about you from your digital crumbs,” Tufekci said in a recent TED lecture. “They can infer your sexual orientation, your personality traits, your political leanings. They have predictive power with high levels of accuracy.”\nSuch insights may be useful in certain contexts—such as helping medical professionals diagnose postpartum depression—but unfair in others, she said. Part of the problem, she said, stems from asking computers to answer questions that have no single right answer. “They are subjective, open-ended and value-laden questions, asking who should the company hire, which update from which friend should you be shown, which convict is more likely to reoffend.”\nFrank Pasquale, a University of Maryland law professor and author of The Black Box Society: The Secret Algorithms That Control Money and Information, shares the same concerns. He suggests one way to remedy unfair effects may be to enforce existing laws on consumer protection or deceptive practices.\nPasquale points at the European Union’s data protection law, set from next year to create a “right of explanation” when consumers are impacted by an algorithmic decision, as a model that could be expanded. This would “either force transparency or it will stop algorithms from being used in certain contexts,” he said.\nAlethea Lange, a policy analyst at the Center for Democracy and Technology, said the E.U. plan “sounds good” but “is really burdensome” and risked proving unworkable in practice. She believes education and discussion may be more important than enforcement in developing fairer algorithms.\nLange said her organization worked with Facebook, for example, to modify a much-criticized formula that allowed advertisers to use “ethnic affinity” in their targeting.\nOthers meanwhile caution that algorithms should not be made a scapegoat for societal ills. “People get angry and they are looking for something to blame,” said Daniel Castro, vice president at the Information Technology and Innovation Foundation. “We are concerned about bias, accountability and ethical decisions but those exist whether you are using algorithms or not.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:e17b4735-6a29-4050-a875-5f038355be65>","<urn:uuid:6ac4c8f9-4485-43b7-b764-b48a249516fb>"],"error":null}
{"question":"How do traditional smell-based entertainment systems like Smell-O-Vision compare with modern olfactory sensor technology in terms of accuracy and implementation?","answer":"Traditional smell-based entertainment systems like Smell-O-Vision from the 1950s used mechanical methods to pump scents through theater vents or release them from beneath seats, but faced significant implementation challenges including difficulty removing smells between showings and causing allergic reactions or nausea in audiences. In contrast, modern olfactory sensor technology uses more sophisticated approaches like rotational spectroscopy implemented on CMOS chips, which can detect specific molecules with high accuracy and at a much lower cost. While traditional systems like Smell-O-Vision could only deliver pre-selected scents, modern sensors can analyze and identify multiple gases simultaneously with sharp delineations between frequencies, making them more suitable for practical applications in medical and industrial fields.","context":["Hello Domeng and thank you for your interesting question.\nThe American film industry pioneered olfactory entertainment in the\nlate 1950s with \"Aroma-Rama\" and \"Smell-O-Vision,\" pumping smells\nthrough theatre vents or releasing them from beneath audience seats.\nSmell-O-Vision was a process invented by a Swiss chemist who combined\nsmell with vision. His objective was to stimulate the audience with\nthe sense of the smell, as well as the visual representation.\nHere is a timeline of the films that made use of this kind of\nDuring a newsreel of a Pasadena Rose Bowl game, a Family Theater in\nForest City, Pennsylvania dipped cotton wool in a rose essence and\nplaced it in front of an electric fan.\nFor the film Lilac Time, a theater in Boston put lilac scent in the\nventilating system for the opening credits of the film.\nWhen Broadway Melody premiered in New York, an orange blossom perfume\nwas sprayed from the ceiling\nOn December 2, 1959, in New York, Aroma-Rama was introduced in the\nfilm \"Behind the Great Wall\" a travelogue about China with 52 smells.\nMike Todd, Movie Producer and Elizabeth Taylor's husband, premiered\n\"The Scent of Mystery\" with Smell-O-Vision in Chicago on January 12,\n1960. Thirty smells were included. Some scents included garlic, paint,\nboot polish, ocean ozone, bread, coffee and perfume. They were piped\nin to each seat and spread by a special smell track on the film.\nOdorama scratch-and-sniff cards were created by John Water's \"Smelling\nOdorama was used for the film \"Polyester\". Movie goers were given a\nscratch pad with 10 places. When a number appeared on the screen, the\naudience would scratch that part with a coin to release the smells.\nGolden Harvest Pictures, China, scheduled a UFO film scented movie\nArticle by Richard von Busack at Metro Publishing Website\nTake a whiff of smelly movies July 2000 - by Winnie Chung\nApril 2001 interview by Cindy Ahuna with David Libby, Director of\nPublic Relations at DigiScents\nTime Online Edition Website\nHans Laube, a Swiss professor of osmics (the study of smells)\ndiscovered how to reproduce odors in a movie theater in the 1950s. The\ninvention was known as Smell-O-Vision and was introduced in the 1960\nfilm Scent of Mystery.\nHow did smell-o-vision work?\nIt worked like this: tiny plastic hidden tubes under your seat pumped\nout smells like garlic, pipe smoke, and shoe-shine wax from a\ncentralized \"smell brain.\"\nSource: Retrofuture Website\nAccording to the Weird Science Fact File # 1415, In the 1950's,\nSwiss professor Hans Laube invented Smell-O-Vision, a machine\ninstalled in movie theatres that emitted puffs of specific odors in\nsynchronization with the action on the screen. These aromas were\npumped into the theatre through a network of hidden plastic tubes\nattached underneath each seat.\nSource: First Science Website\nIn 1959 , Hollywood, still having trouble with the menacing medium of\ntelevision, introduces Aroma-Rama, a scenting system that is developed\nby inventor Charles Weiss to add a sense of smell to the documentary\nfilm Behind the Great Wall by filtering Oriental scents into the\nauditorium trough the air-conditioning system. It competes with\nSmell-O-Vision, which pipes odors into the individual seats.\nSource: Thinkquest Library\nEric Lefcowitz esplains the workings of Aroma-Rama.\nLike Smell-O-Vision, AromaRama used a \"scent track\" to trigger the\nfilm's odors. But there was a crucial difference: AromaRama spread its\nodors through the theater's air conditioning system with Freon gas\nused to diffuse the smells. Unfortunately, it didn't diffuse all that\nwellpungent aromas often hung malodorously in the air in a\nSource: Archived version of the Retrofuture Website\nHow do \"scratch-'n'-sniff\" cards work?\n3M Company invented the S&S, Scratch and Sniff process, also called\nMicro-Fragrance Coatings. It was developed to spray daffodils and\nbuttercups scents. 3M emulsified the liquid and mixed it up with oil\nand water. The end product was suspended in a liquid form. The bubbles\nare transmitted into glue, which were applied to paper. The end\nproduct was the scratch and sniff books. When you press down with your\npen, the bubbles are released.\nYou may see a magnification of a scratch and sniff paper here:\nWhy olfactory entertainment failed:\nOne reason for the failure of Smell-o-vision was the problem getting\nthe smells out of the theater after each showing.\nTodd's Smell-o-vision and the others barely lasted past their\npremieres. None worked well, and there was a problem getting the\nsmells out of the theater after each showing. Todd's Scent of Mystery\nwas released in most theaters as Holiday in Spain minus the smells.\nSource: Take a whiff of smelly movies July 2000 - by Winnie Chung\nSome people were aleregic or got nauseous from the smells.\nThese experimental systems were mainly novelties and not very\nsuccessful, with reactions from the audiences reaching from allergic\nreactions to nausea.\nSource: Smelly interfaces - A brief review of the application of smell\nin user interfaces by Kari Hamnes\nDate: 20. February 2002\nAnother reason for failure was the implementation of the scratch and\nIn 1981, filmmaker John Waters launched his movie ''Polyester'' with\na scratch-and-sniff card dubbed ``Odorama.'' But American noses\nremained aloof to the idea that smell could be part of a fun evening\nout. One of the big problems with the past has been implementation,\nSmith said. If you are asking someone to do scratch and sniff, it is\nnot going to be as compelling as if it is automated.\nSource: Wired News article 1999\nThe critics said the following about smell-o-vision:\nBosley Crowther of the New York Times dismissed the \"novel\nstimulation\" as \"bunk.\" Time magazine warned: \"Customers will probably\nagree that the smell they liked best was the one they got during\nintermission: fresh air.\"\nSource: Retrofuture Archives Website\nCharles Weiss Aroma-Rama\nI hope you find this helpful and if there is anything that I've\nwritten that needs clarification, please ask before you rate this","As data gathering becomes more pervasive, what else can be done with this technology?\nSensor technology is beginning to change on a fundamental level as companies begin looking beyond a human’s five senses, on which early sensors were modeled, to what can be done with those sensors for specific applications.\nIn some cases, sensors don’t have to be as accurate as the sight, smell, touch, taste and hearing of a person. In others, they can be augmented to far exceed human limitations. And while the human brain remains more efficient and effective at certain operations, such as adding context around sensory data, sensors connected to digital logic can react more quickly and predictably to known stimuli.\nMost early vision technology, for example, came out of medical research. The primary goal was to cure blindness or compensate for impaired vision. Machine vision has a different purpose. Rather than striving for visual acuity that is as good or better than a person’s eyesight, current efforts add the ability to sense objects in the non-visible spectra, such as infrared imaging, or radar to detect objects around corners or other objects that are not visible to people.\n“We limit ourselves if we think of this as human vision,” said Lucio Lanza, managing partner of Lanza techVentures. “Once you start thinking of this as machine vision, which is perception of different phenomena, it opens up a whole new level of opportunity.”\nThere has been much work in embedded vision in the automotive and robotic sectors. Each has radically different goals than human vision. But they also have distinctly different goals from each other. A robot needs to recognize patterns in a person’s face and the surroundings in order to distinguish a human from a statue, or a ledge or irregularity in a floor versus a carpet. A car needs to recognize the speed at which a car is moving, other objects that might cross its path, and how to react in a tiny fraction of a second.\nThis becomes harder with conventional vision approaches. Craig Forest, CTO of Arteris, noted that sensors can be blinded, just like people, with the worst problems occurring at dusk. “A lot of work is required to keep data reliable. If there is an error, you need to make sure you propagate that back to the ingress point.”\nBut people can put on sunglasses, put up their hand, and still comprehend what’s going on. A machine cannot unless that is programmed into the hardware and software and the issues are taken into account at the architectural level. So automatic garage door openers and interactive traffic lights regularly process faulty data when blinded by the sun, while a person would not be fooled.\nThe solution is to add more intelligence into image sensors, and this is where many semiconductor companies are focusing their efforts. For one thing, it’s one of the most lucrative areas for chips these days, and margins continue to hold due to high demand and constant changes in the technology.\n“There is massive growth in this area for surveillance and automotive applications,” said Wally Rhines, chairman and CEO of Mentor Graphics. “It now represents 3% of the total available market for semiconductors. That could exceed 5%.”\nThe upside is based not only on the need for more image processing, but the need for surrounding logic that is sophisticated enough to recognize patterns.\n“This is why there is so much interest by Google, Facebook and Microsoft in computer vision and convolutional neural networks,” said Jen-Tai Hsu, vice president of engineering at Kilopass. “The IoT is a revolutionary trend. The entire view of technology is different. It’s not just a memory or a processor, and it’s not just about computing in speed or power.”\nWhat exactly is that smell?\nResearch is underway for all five electronic senses, but the real money so far has been in the image processing arena. Olfactory sensing until very recently was given short shrift, largely because what has existed in the past was good enough for industrial purposes such as detecting gas leaks. That is changing as new opportunities open up in in the industrial medical fields.\n“A dog feels an earthquake before a human,” said Lanza. “It also can smell cancer. If you can sense what’s going on with human skin, from the smell or sweat, you can get enormous amounts of very valuable information.”\nThe existing methods for analyzing odors rely primarily on mass spectrometry. Air or gas samples are ionized and then run through a magnetic field and down a column to separate them out based upon their mass-to-charge ratio. Like most mechanical approaches, the equipment is large and cumbersome and hasn’t changed significantly in decades.\nResearchers are now working on a different approach called rotational spectroscopy, which can be implemented on CMOS instead of requiring a special machine.\n“With rotational spectroscopy, you collect the molecules in a gas state and vibrate and spin them,” said Kenneth O, professor at the University of Texas at Dallas and director of the Texas Analog Center of Excellence, which is funded by Semiconductor Research Corp., Texas Instruments and Samsung. “Depending on the shape of the molecule, there is a preferred axis of rotation and energy state. If you change the energy state, you can transition from one rotational state to another.”\nThat generates electromagnetic waves, which unlike mass spectrometry can be measured in extremely narrow bands. The result is that with mixes of multiple gases, the delineations between frequencies are sharp enough to be able to pick out multiple gases rather than just one.\n“The goal here is specificity at an affordable cost,” said O. “Right now it costs about $150 per line (single gas identification) and machines run as much as $80,000. If we can implement this on CMOS, we can sell chips for as little as $500 to $1,000 using sensors tuned for one molecule. And this isn’t leading-edge technology. We’ve built prototypes at 65nm.”\nHe said that with mass spectrometry, even the best equipment has trouble detecting different molecules because the lines between them are too wide. That leads to overlap and some guesswork. “If you have many different molecules, it’s difficult to detect one. But with diseases or indoor air monitoring, for example, that’s exactly what you need to do.”\nThe technology proved so accurate that one student went to a tavern and consumed a couple drinks to analyze his blood alcohol content (BAC), and the researchers discovered the tavern was diluting its liquor because his BAC was below what it would have been if he had consumed a single beer. That same capability can be used to measure everything from blood glucose levels to how much marijuana a driver has consumed and when.\nThe brain behind the senses\nSimilar work is underway to digitize the other senses so the data can be effectively processed and mined. But the bigger question is what else can be done with all of this data.\n“There are a lot of places for growth in the near-term,” said Mentor’s Rhines. “Longer term, there will be an inevitable infusion of knowledge downward. This has been the pattern from mainframe to minicomputer to terminals, and over time to peripherals like network controllers and optical drives. It always starts with the intelligence being centralized, which is why we have massive data centers and why everyone is designing a gateway. But that’s an intermediate intelligence. Ultimately, it will diffuse down to semiconductors and actuators. When we finally get to the IoT, there will be a lot of device intelligence.”\nThat will be required for a couple reasons. For one thing, it’s faster to process data locally rather than send everything to the cloud. And second, much of this data needs to be mobile, where connections are not always guaranteed.\n“There are times when you’re in a good, fast low-latency network and times when you are not,” said ARM CEO Simon Segars. “It doesn’t all go into the cloud. There’s a tradeoff between what’s remote and what’s being done offline. Nothing is more frustrating than when you’re talking into your device and there is no network connection. But it does require more compute power, different types of processing. We’ve spoken about moving a big CPU to a more distributed computing environment where you have accelerators for specific functions. That’s how you get power efficiency.”\nThat puts a lot of pressure on chip architects to continue finding ways to make processing more efficient at every level, including the sensor. Sensors will play a key role in this shift, and the interaction between sensor, processor, software and overall architecture will need to evolve in ways to make all of this more efficient and more portable.\n“If a sensor is always on, you can customize it to poll the data,” said Amy Wong, director of marketing for Marvell’s IoT Business Unit. “So if you have a sensor library, you write that to an API and you can customize a wearable or a sensor for something like calculating blood pressure. It’s not just about the chip, though. It’s integration, framework and software development, and it’s being able to do some of the work in batch. So if you look at a watch, it’s idle 95% of the time. The CPU and other pieces can be designed to do things differently. If you calculate that over the battery life, you can extend the battery life by at least 20%. This is a process, and you need to understand the process as well as the architecture.”\nContext and issues\nThere are enough new capabilities being developed in sensors to generate plenty of new ideas, but the reality is they have to run in the context of a system, which in turn needs to function in the context of other systems.\n“We have moved from connecting computers to connecting people, which is what’s happening today,” said Lanza. “At a certain point, we are going to be connecting things and we will need a set of rules to find which things are good things and what security is needed based on what we think is the right way to do things. We will have a lot of work to make a society of things consistent.”\nThis is a massive job, and it affects how data is collected and shared and what is ultimately done with that data.\n“The proliferation of sensors and the growth rate of data will be enormous,” said Steven Woo, vice president of enterprise solutions technology and distinguished inventor at Rambus. “This is more data than can be moved back to the data center, though. It will require more edge computing, where there will be filters or pre-processing. So you basically can have simple processing to get to more meaningful data.”\nWoo said that could require a different way of looking at data for edge devices, as well. “You may start to see more machine learning in the end points, where you scan information and learn the important events about that data and send along consolidated information. There are ways you can do that with reasonable security back and forth over the air.”\nDesigning sensors inside of systems isn’t a straight line, either. There is no clear roadmap for how this technology will be used and what can be done with it. That will require massive innovation at every level, and while that makes system engineering much more interesting, it also adds levels of uncertainty into the tech business that haven’t been seen since the introduction of the PC.\n“In the old days, Intel said what its next processor would be and the rest of the industry followed,” said Kilopass’ Hsu. “In the future, people will define technology, not the high-tech companies. IoT is the next big thing, and we will find more and more applications that will need high tech. But IoT is also difficult to unify, so it will be very difficult for today’s dominant players to fill that market.”\nThe Trouble With MEMS\nSevere price erosion is putting this whole sector under pressure at a time when demand is growing.\nFlexible Sensors Begin Ramping\nTechnology opens up new possibilities for the Internet of Everything.\nSensors Enable ADAS\nAdvanced driver assistance requires a careful and complex balance of hardware, software and security."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:9c74017f-be45-4740-9afe-a7ac36158519>","<urn:uuid:41cdb118-808b-44cb-a5ca-7e9211faed7f>"],"error":null}
{"question":"How did the transition from empire to nation-state differ between post-WWI Central Europe and post-WWII Germany in terms of peace treaty implementation and stability?","answer":"The transition after WWI in Central Europe was marked by prolonged violence and civil wars, despite the Treaty of Versailles formally ending the war. The region between Russia and Germany became a contested space where new borders had to be drawn, leading to continued fighting in 1919 even as peace was being established in Paris. Local populations, mainly peasants, struggled to adapt to new national identities, and neighboring communities were suddenly divided by new borders. In contrast, after WWII, there was no single comprehensive peace treaty like the Treaty of Versailles. While the Treaty of Versailles had imposed heavy penalties on Germany after WWI that contributed to WWII, the post-WWII approach aimed to learn from these mistakes, as evidenced by the Potsdam conference where delegates specifically sought to avoid repeating the errors of the Versailles Treaty.","context":["The Great War ended the age of empires in continental Europe. National narratives of the successor states have it that they materialized like the proverbial jack-in-the-box in 1918. In reality, the transition from empire to nation state was a process that lasted years, and thus prolonged the violence of the World War long into the postwar area.\nIt is only logical, if one thinks about it: in Central Europe, a vast area between Russia and Germany was turned into a tabula rasa on which now new borders had to be drawn. It certainly does not come as a surprise that this was not achieved by peaceful means. In 1919, while peace was established in Paris, fighting went on in Central Europe.\nThis continuation of conflict was a tragedy for a region that had witnessed some of the fiercest battles of the Eastern Front. Wartime destruction, famine, and epidemics were ubiquitous. After 1918, its population was still suffering, starving, dying. And yet peace was only a distant dream. The immediate postwar experience of the people of Central Europe contrasted highly with the national bravado voiced by the elites in their respective capitals.\nA large majority of the new citizens of emerging Central European countries—predominantly peasants—had no clear idea what the new ethnic nation states were supposed to mean. Central Europe was populated by people speaking Lithuanian, Ukrainian, Russian, Yiddish, Czech, Slovak, or German, with no clear boundaries running between them. In prewar censuses, they even had difficulties to grasp the concept of nationality, and in consequence, an alternative category—tutejsi (locals)—had to be added. As subjects of empires until 1918, the idea that they should suddenly be citizens of different states came to them as a shock: “Other times had come,” a peasant noted in his memoir on Polish independence in 1918. “The village woke up because everything was shaking all around. They tell us that there’ll be a Poland and it’s already taking shape, though it’s still a bit weak, but slowly getting stronger. The peasants don’t want to believe it, because we’ve always been told that this here’s Russia and Russia it will be, and now, all of a sudden-hocus-pocus-it’s Poland.”\nThe immediate postwar experience of the people of Central Europe contrasted highly with the national bravado voiced by the elites in their respective capitals.\nFrom late 1918 onwards, cities, towns, and villages far from the state centers that struggled for survival on a daily basis were drawn into the new contest for national supremacy. Neighbors that had been living together—not conflict-free, but by and large peacefully—under imperial rule for ages overnight were turned into enemies by competing politicians and agents of nationality which claimed them as their citizens. Did they embrace the struggle for national independence with enthusiasm? Were they sending their sons unhesitatingly to the diverse fronts that opened when the arms of the Great War fell silent? Far from it: in the contested borderlands of Central Europe, the passage from war to peace was experienced as a fratricidal civil war. Another peasant in the Lithuanian-Polish borderlands complained: “Now, this is Lithuania, and that’s Poland. It used to be one, but now there’s a border between Bereźniki and Ogrodniki; there’s a war on. Is that how things should be? Don’t we all go to the same church? Isn’t it a disaster that brothers are divided and fighting?”\nGeographically in the eye of the cyclone, the emerging ethnic Polish nation state claimed territories that hosted minorities of almost all nations involved in the Central European Civil War. Between 1918 and 1921, it was in a permanent state of declared or undeclared war on literally all frontiers except the Romanian.\nFor political scientists, the notion of the diverse postwar conflicts in Central Europe as a civil war might be provocative, since the term usually refers to conflicts below state level, while here, different states were involved. The counter-argument is that those very states were not fully developed yet, since their borders and population still had to be defined, and their state institutions—above all a functioning army—had to be built. Furthermore, the local people experienced those conflicts as a civil war, not as wars between nations. With the words of historian David Armitage: “Civil war is, first and foremost, a category of experience; the participants usually know they are in the midst of civil war long before international organizations declare it to be so.” To bring both arguments together: The Central European Civil War marked the transition from empires to nation states. In its fires, state institutions were built and state citizenship was defined.\nFeatured image credit: “Map showing the political divisions of Europe in 1919 after the treaties of Brest-Litovsk and Versailles” by unknown author. Public Domain via Wikimedia Commons.","Even though the treaty of versailles ended world war i, the severe consequences given to germany set events in motion that would eventually lead to world war ii league of nations the treaty of versailles also created the league of nations. Find out more about the history of treaty of versailles, including videos, interesting world war i officially ended with the signing of the treaty of versailles they deferred certain fundamental issues for so long that no formal peace treaty was ever written to end world war ii david. More than 60 years after world war ii ended, russia and japan have restarted negotiations over a still-unsigned peace treaty that would finally bring a formal end to the war between the two countries. The treaty of versailles was an agreement signed to end _____ aworld war i bworld war ii cthe cold war dthe vietnam conflict. World war i ended with an armistice signed on november 11, 1918, followed shortly by the signing of the treaty of versailles these documents ended four years of bloody battles that left 10 million. Ending the war to end all wars compare their treaty of versailles with the actual treaty and then make predictions about its effects on the post- how did the treaty of versailles contribute to world war ii duration one 90 minute class period 2.\nThe treaty of versailles, which was the official end of world war i, was the main cause that led to world war ii. World war one was brought to its official end with the signing of the treaty of versailles in 1919 it was drawn up by the victors of the war, which included great britain, the united states, italy, and france. Best answer: the treaty of versailles is one of five separate treaties that ended world war i in 1919 the treaty of versailles became the most well-known of the five because of its punitive nature and especially harsh sanctions against its primary offender, germany. The 1919 treaty of paris ended world war i, but imposed heavy penalties on germany. World war ii: the start of the second world war the armistice that ended world war i went into effect at 11 am on november 11, 1918 in summary, the treaty of versailles left the world with a shaky peace.\nPotsdam: the end of world war ii and the the delegates arrived at potsdam determined to learn from the mistakes their predecessors made in the treaty of versailles and the soviet union discussed how to end world war ii and lay the groundwork for a post-war peace perhaps more. Instructor name date the treaty of versailles made world war ii inevitable world war ii claimed more lives and involved more countries than any war that. How the treaty of versailles caused wwii home significant treaty but by the time world war ii rolled around many germans remembered this clause of the treaty and it turned them away from the allies towards the end of world war i and during the signing of the treaty russia's government. The beginning events of world war ii hannah peace was needed so finally after the war ended the leaders meet for a treaty the treaty of versailles produced german anger and the treaty of versailles contributed to the territorial losses that germany received after world war i. The largest and most influential of the treaties ending world war i, the treaty of versailles had long been debated to have caused the world war ii would talk for hours on end in private. The treaty of versailles ended wwi there was no single peacetreaty ending wwii, such as the treaty of paris which ended thefrench and indian war.\nWorld war i: treaties and reparations related articles related links comments a peace without victory to end the war to end all wars revision of the versailles treaty represented one of the platforms that gave radical right wing parties in germany. Did the treaty of versailles make world war ii inevitable january 8, 2008 student id: 081404078 abstract this essay analyses the origins of the second world war by briefly summarizing the events from 1919-1939 treaty of versailles was the end of world war i. The treaty of versailles was much unfair on germany article 231, often known as the war guilt clause, was the opening article of the reparations section of the treaty of versailles, which ended the first world war between the german empire and the allied and associated powers.\nThe treaty of versailles ended _____ a world war ii bthe anglo-boer war cworld war i dthe crimean war. Analyze how the stipulations of the treaty of versailles that ended world war i, along with the great depression of the 1930s, contributed to the outbreak of world war ii. The big four dominated the proceedings that led to the formulation of the treaty of versailles, a treaty that ended world war i and allowed for the trial of kaiser wilhelm ii and a number of other high-ranking german officials as war criminals. Despite germany instigating world war ii, the causes that led to the war are more intricate reparations on germany from the treaty of versailles in 1919 in the end, germany pulled out. I understand there is no definitive answer but i'm wondering if there is any consensus amongst historians on this issue if world war ii is.\nThe treaty of versailles (french: trait de versailles) was the most important of the peace treaties that brought world war i to an end the treaty ended the state of war between germany and the allied powers (in contrast to the situation following world war ii."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:92554d37-97fb-4b15-b76b-05d7ed5c6a5f>","<urn:uuid:58cf1460-1402-4cad-8775-9b6d4eff056e>"],"error":null}
{"question":"Hey there! I'm doing research on urban economic growth - what's the relationship between city size and productivity, and how does this vary between developed and developing nations? 🤔","answer":"The relationship between city size and productivity is complex and varies significantly between developed and developing nations. In developed countries, there is only a weak relationship between city size and productivity - larger cities don't necessarily guarantee higher productivity. In fact, in Britain, smaller and medium-sized cities have often outperformed larger cities. However, in developing nations, large cities tend to have much higher productivity compared to their national averages. While metros in developed countries typically have productivity ratios of 1 to 1.25 times their national average, metros in developing economies often have ratios of 2 or higher. For example, cities like Bangkok, Delhi, and Shanghai have productivity ratios of 2.4-2.7 times their national averages, with some Chinese cities reaching ratios of 4 or more. This pattern suggests that as countries become more developed, the productivity advantage of urban areas over their national averages tends to decrease.","context":["Posted by Andy Pike, 1st March 2018\nCity centrism has become an enduring, dominant and familiar narrative and policy mix. Portrayed as an account of necessarily connected events, it has been generalised into an international recipe for urban prosperity via a myriad of identifiable and high profile academic gurus, international agencies, and think-tanks. Simplified and repeated into messages that can be understood and implemented by national and city policymakers. In short, fix the city centre and the economic growth will trickle-down throughout the wider city-region. City centres are interpreted as the principal engines of economic growth and dynamism for their wider city-regional and, ultimately, national economies.\nEconomic logics and policy repertoires\nThe economic logic is based upon ‘external economies of agglomeration’ generated by thick labour markets of educated and skilled labour, spill-overs of innovations and knowledge, market size effects, and linkages amongst associated goods and services activities as the route to higher productivity, economic growth, innovation, incomes and prosperity in cities. Drawing upon urban economics, especially its US varieties thought-up and derived from the empirical experiences of US cities, primacy is given to urban scale, density and economic integration between cities and their hinterlands to reduce geographical and social frictions and enable rational sorting of people and capital over time and space. Policies that work with the grain of this dominant logic are favoured including planning liberalisation, education and skills upgrading, facilitating labour mobility and bringing places closer into their central city growth orbits, often through improved ‘connectivity’ and intra-city transport connections, and business and political leadership and devolved powers and resources to the city-regional scale of ‘functional economic areas’.\nCity centrism isn’t everything for urban growth\nEmergent research from the ESRC-funded city evolutions research project into the urban predicament in British cities since the early 1970s raises some difficult issues for city centrism . Demonstrating markedly divergent pathways in output and employment, southern cities have grown faster than northern cities and large cumulative growth gaps have emerged since the early 1970s. While London’s turnaround since the 1990s is a particular and atypical case, elsewhere smaller and medium-sized cities have outperformed larger cities. City centrism, then, doesn’t appear to work for all cities everywhere all the time. Taking a longer-term evolutionary approach can help us understand when, where and why agglomeration as well as other factors such as their functions and non-spatial public policies may be important for certain economic activities and cities. It can also prompt us to look again and more closely at the relationships and transmission mechanisms of city centre growth to adjacent and outlying areas. For some city-regions, these seem to have been intermittent, weak or close to non-existent since the early 1970s. As the prospects and prosperity of British cities continue to diverge, the difficulties intensify of addressing those cities struggling to adapt and the people and places ‘left behind’.\nProductivity growth is not guaranteed with services and scale\nOn city productivity growth, northern cities led during the 1970s and 1980s through manufacturing specialisation until their struggle with deindustrialisation led to a switch to southern cities from the 1990s https://www.camecon.com/2017/11/28/geographical-dimension-productivity-problem/. Yet the more service-based growth of southern cities has been less productive, slowing national average productivity growth, despite their relatively higher levels of skilled workforces and knowledge intensive business services. City centrism in Britain appears somewhat skewed towards a paler, lower productivity variant than elsewhere. As Figure 1 shows, there is only a weak relationship between city size and productivity . Scale and density are not always and everywhere the key determinants of city growth.\nFigure 1: City size and labour productivity, 2015\nSource: ESRC City Economic Evolutions project\nTransition to a particular kind of urban service economy has been integral to this productivity slowdown, and cities have largely left any remaining urban manufacturing to fend for itself. Rather than relative neglect, analysis could be much more open to the possibility that the role of diseconomies of agglomeration may have been underplayed in this uneven city productivity performance as unbalanced spatial concentration has stoked more rapid price inflation of factor inputs in land and labour markets, negative externalities in congestion and pollution, and rising costs of infrastructure provision at least partly due to escalating urban land values.\nHigh skilled employment and agglomeration are necessary but not sufficient for city growth\nOn city skills, city growth and high-skilled occupations are closely related. Skilled workforces are critical to growth with higher levels and faster growth in southern cities. Northern cities experienced slower growth of higher-skilled and greater hollowing-out of middle-range occupations. But the city centrism propositions of ‘smart cities becoming smarter’, and a positive relationship between agglomeration and high skilled employment growth do not appear to apply to British cities. Other factors have been more important and newer, smaller cities have led the pattern of high-skill growth. The urban service economy has become profoundly polarised in occupational terms. Yet, policies are sometimes overly focused on the higher-end jobs, occupations and people when they will only ever constitute a relatively small, albeit highly productive, part of the city economy. While the lower-end of the labour market which employs lots of people but has weak productivity is relatively neglected when its potential for productivity growth could be at least examined more thoroughly. Issues of job (re)design, progression and rotation in sectors such as leisure, retail, and social care could readily be addressed. Reducing the reliance upon thinking that is conceived from a particular US perspective and draws heavily upon empirical data from US cities might be a way to recover traditions of urban and regional analysis better able to explain the particularities of the British urban system.\nTime for some fresh thinking?\nSome deeply invested in city centrism may dismiss or fail to engage with this more finely drawn picture of city economic evolution in British cities and its challenge to the existing policy repertoire. Some will no doubt continue to argue and marshal evidence to show that it has not failed but has simply not been applied effectively, consistently or strongly enough: that most British cities are still too small and not dense enough, that insufficient emphasis has been given to attracting highly skilled people, that less skilled people have not been given sufficiently sharp incentives to sort themselves (out) and move to where the opportunities are located, that intra and inter-city transport networks are still not extensive enough, and that planning should have been further relaxed to encourage private investment.\nYet, the magnitude of urban concerns in Britain after twenty-five years of city centrism suggests a more self-critical and reflective response, even a little soul searching. Is it time, then, to ask whether an overly narrow and singular focus upon city centrism has reached its limits? In the same way that we can look back at the other US imports of urban motorways and suburbanisation rife in the 1960s as historic and not entirely successful recipes for urban development in Britain, will we consider city centrism in the same way in the years to come? Can we think more broadly and imaginatively about different geographies of urban and regional development that better reflect the emergent international patterns and dynamics of city economic evolutions in urban archipelagos, patchworks and mosaics rather than simple and binary cores and peripheries? Countries across the world are experimenting with new metropolitan and other wider scale, pan-city-regional geographies in attempts to tackle the thorny problems at hand. Finding better ways to think about and design policies that support the combination of the dynamism of city centres with their hinterlands to the benefit of both is a pressing challenge for those thinking and doing city economy and policy.\nAndy Pike is the Henry Daysh Professor of Regional Development Studies at the Centre for Urban and Regional Development Studies (CURDS), Newcastle University, UK. He is part of the ESRC-funded project on City Economic Evolutions led by Professor Ron Martin at the University of Cambridge and involving other colleagues in Cambridge and the Universities of Aston and Southampton, and Cambridge Econometrics.","Half the world's population lives in cities today, a figure that will increase to 70 percent by 2050. In that same time period, McKinsey Global Institute projects that the economic output of the 600 largest cities and metro areas is projected to grow $30 trillion, accounting for two-thirds of all global growth.\nEconomists and urbanists have long noted the connection between urbanization and economic development. As Harvard University economist Edward Glaeser points out, “if you compare countries that are more than 50 percent urbanized with countries that are less than 50 percent urbanized,\" he writes, \"incomes are five times higher in the more urbanized countries and infant mortality rates are less than a third in the more urbanized countries.”\nBut it has been difficult to get at the precise ways that global cities relate to productivity and economic development, mainly because of the lack of comprehensive, systematic, and comparable data. Aside from estimates of their populations, none of the major statistical agencies — the United Nations, the World Bank, or others — collect comparable economic data for the world's urban areas.\nFortunately, the Brookings Institution's Global MetroMonitor has compiled data on GDP per capita for the world’s 300 largest metropolitan economies, most recently through 2012. These metros account for nearly one-half (48 percent) of global output, while being home to less than one in five (19 percent) of its people. Undertaken in collaboration with researchers from the London School of Economics, these data are based on assessments from Moody's Analytics and Oxford Economics.\nNot surprisingly, the blue dots, which indicate metros with between $45,000-$90,000 in per capita output, are concentrated in the \"Global North.\" Even though the data do not cover metros in the poorest parts of the world, the yellow dots (metros with between $3,000-$15,000 in economic output per person) are concentrated in in the \"Global South.\" The poorest metros, according to this metric and data, are in India, China, Brazil, and South Africa.\nLet’s take a closer look:\n$45,000-$90,000 per person: There are 91 metros in this group, found mainly in the United States, Canada, Northern Europe and Scandinavia, Australia, and Japan. These are the peaks of the world economy — places like greater New York, London, Paris, Singapore, Hong Kong, Washington, D.C., San Jose (Silicon Valley), Boston, Pittsburgh, Denver, Sydney, Amsterdam, Geneva, Zurich, Frankfurt, Munich, Vienna, Brussels, Dublin, Oslo, and Helsinki. The list also includes oil-rich rich regions like Calgary and Houston as well as Abu Dhabi and Kuwait City in emerging economies. These very high output metros are home to 3.8 percent of the world’s population and 20.1 percent of the population of the top 300 metros (those covered in the study), but they account for 18.2 percent of global economic output and 37.2 percent of the output of the top 300 metros.\n$30,000-$45,000 per person: 103 metros fall into this category. They are sprinkled acrossthe U.S., Canada, Europe, Australia, Japan, South Korea and Taiwan. They include places like Detroit, Miami, Toronto, Vancouver, Brasilia, Melbourne, Brisbane and Adelaide; Barcelona and Madrid; Milan, Rome and Turin; Copenhagen, Glasgow, Berlin, Budapest, Moscow, and Lisbon as well as Tel Aviv and Haifa; and Seoul, Taipei, and several metros in China. These well-performing metros are home to 4.8 percent of the world’s population and 25.7 percent of the population of the top 300, and account for more than 16 percent of the world’s total economic output and 32.8 percent of the total economic output produced by the top 300 metros.\n$15,000-$30,000 per person: There are 75 metros in this group. These include metros in Europe, Japan and South Korea as well as Brazil, China, Russia, Mexico, Argentina, Colombia, Kazakhstan, Malaysia, Indonesia and South Africa. The group includes metros like Liverpool, Cardiff, Sheffield, Lille, Leipzig, Auckland, Naples, Seville, Valencia, Naples, Istanbul, St. Petersburg, Krakow, Buenos Aires, Sapporo, Shanghai, Beijing, Bangkok, Mexico City, Guadalajara, Sao Paolo, Rio de Janeiro, Santiago, Bogota, Lima, Cape Town, and Dubai. These middle performing metros are home to 6 percent of the world’s population and 32.2 percent of the population of the top 300 metros, and they account for 11.4 percent of global economic output and 23.3 percent of the output of the top 300 metros.\n$3,000 to $15,000 per person: There are 31 metros in this category. Of course, many more metros around the world would likely fall into this group, but they are not covered in the Brookings data. These metros are found mainly in Global South, including China, India, Brazil, Colombia, Egypt, and Indonesia. They include some of the largest cities and metros in the world, like Manila, Jakarta, Cairo and Alexandria, Durban, Medellin and Cali, Mumbai, Kolkata, Delhi and Bangalore, as well as numerous metros in China. These lower output metros are home to roughly 4.1 percent of the world's population and 21.9 percent of the population of the top 300 metros, but they account for just 3.2 percent of total global economic output overall and 6.7 percent of the output of the top 300 metros.\n• • • • •\nWhile metros in the advanced countries remain more affluent than those in the developing and emerging economies, growth is accelerating in the latter. \"Three-quarters of the fastest-growing metropolitan economies in 2012 were located in developing Asia, Latin America, and the Middle East and Africa,\" the Brookings authors write. \"By contrast, almost 90 percent of the slowest-growing metro economies were in Western Europe and North America.\"\nWhat’s even more interesting is how the data enable us to examine to what extent metro economic growth is related to national growth. As the report notes:\nCompared to their countries, more than half of metro areas outperformed on employment growth in 2012, but only 40 percent achieved faster GDP per capita growth. Fifty-six metro areas were pockets of growth in their countries, with both GDP per capita and employment expanding at a faster pace than national averages.\nTo further explore this relationship, I developed a metric I call the urban productivity ratio. The ratio simply compares the per capita economic output of metros to the per capita economic output of their nations. My MPI colleague Charlotta Mellander calculated the urban productivity ratios for these 300 metros based on the Brookings data for metros and World Bank data for nations, using data from 2012. Zara Matheson mapped the results.\nAs can clearly be seen on the map, the urban productivity ratios tend to be highest in the metros of the developing and emerging economies. The typical large metro in the United States or Europe or Japan has an urban productivity ratio of 1 to 1.25, indicating productivity on par or perhaps 25 percent greater than the national average. At 1.6, San Jose – Silicon Valley – has one of the very highest in the advanced world. Paris is 1.5; Boston and San Francisco 1.4; New York, L.A., Dublin, Frankfurt, Stockholm and Helsinki 1.3; Tokyo, Rome and Hamburg 1.2; Chicago, Toronto, Seoul, and Sydney 1.1; Singapore, Hong Kong, Copenhagen and Dusseldorf around 1.\nThe urban productivity ratios for metros in the developing and emerging economies is much greater. More than 40 metros spanning China, Brazil, Russia, Poland, Thailand, the Philippines and other developing economies have ratios greater than 2. These include Bangkok (2.7), Delhi (2.6), Shanghai (2.6) Beijing (2.4), and Sao Paolo (2.0). Another 18 – mainly in China but also in Brazil and the Philippines – have ratios of 3 or greater; 10, all in China, have ratios of 4 or more; and 3 have ratios of 5 or more.\nWhile this is the dominant pattern, it does not hold across the board. Mumbai, Cairo, Bogota, and Jakarta – with ratios around 1.6 – are on the high side of what are found in advanced nations. Rio, Cape Town and Abu Dhabi have ratios of 1.4, about the same as Munich; Mexico City and Santiago come in at roughly 1.2, about the same as Minneapolis; Hyderabad and Bangalore around 1.1; and Guadalajara around 1.0; Kolkata 0.9 and Dubai 0.5.\nDespite these exceptions the general pattern is clear: Metros in the developing and emerging economies have higher urban productivity ratios than their counterparts in the advanced world. As countries become more developed, the productivity edge that urban regions have over their countries declines. This makes sense, development leads to urbanization which leads, eventually, to the convergence of the productivity of metro areas and the country as a whole.\n• • • • •\nThe Brookings MetroMonitor data are limited in that they exclude many of the poorest and most disadvantaged cities of the Global South (the maps above show few cities in Africa).\nTo get a better handle on the urban productivity differentials in the world’s poorest cities, I used data on economic output for a broader sample of cities based on satellite images of the world at night. I developed this measure of \"light-related regional product\" for 680 metro areas around the world in collaboration with Timothy Gulden and Charlotta Mellander. While questions about the precision of these satellite data remain, the recent American Economic Review study \"Measuring Economic Growth from Outer Space\" by economists J. Vernon Henderson, Adam Storeygard, and David Weil, notes their utility in just these kinds of circumstances. They write:\n[W]e can use night lights data to tell us about economic growth in circumstances where we have no measures of income growth. Most compelling is that night lights data are available at a far greater degree of geographic fineness than is attainable in any standard income and product accounts.\nThe graph below, from MPI’s Zara Matheson, charts the urban productivity ratio using this light-emissions metric. Notice the predominance of green and blue dots — marking metros with high or very high urban productivity relative to national productivity — in Africa, India, China and Southeast Asia.\nBased on this larger data set, we found roughly 125 global metros — mainly in China and India, Southeast Asia, Africa and South America with urban productivity ratio of 3 or above. We found 40 global metros — mainly in Asia and Africa — with ratios greater than 5. And we found seven — again in Asia and Africa — with urban productivity rations in excess of ten times their national average.\n• • • • •\nWhile our analysis is provisional, it suggests that many cities in emerging and developing nations have a considerable productivity advantage over their countries, and also have higher relative productivity than their counterparts in the advanced world.\nUnderstanding the positives and negatives associated with this outpaced development will require better and more systematic data on the economies of global cities. But it remains one of the most important questions in the study of global economic development. Urbanization has become a key part of economic growth in today's world. In many places, cities have provided a critical spur to overall economic growth. But the benefits that come from urbanization have been uneven. In too many parts of the Global South, mega-slums and persistent poverty remain disturbing facts of life. My next two posts will deal with how harnessing the full potential of these great conurbations to spur further development and rising living standards throughout the developing world remains the grandest challenge of our time.\nAll maps by MPI's Zara Matheson."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:74023449-9766-4f0c-87e3-a9c4b661e8d7>","<urn:uuid:fda033d0-a9a7-47f0-9916-1525bf860587>"],"error":null}
{"question":"What were the preservation approaches used at Circlestone compared to Mesa Grande?","answer":"Mesa Grande had a systematic and organized preservation effort, including its listing on the National Register of Historic Places in 1978, purchase by the City of Mesa in 1988, and subsequent stabilization work by the Southwest Archaeology Team focusing on repairing damaged areas and minimizing new excavations. In contrast, Circlestone's preservation history is less structured - while its existence has been known since territorial days, the site was not even recognized as a Native American archaeological site until the 1970s, and the documents don't indicate any formal preservation efforts for this structure.","context":["Copyright Tom Kollenborn, 1999\nThe Superstition Wilderness Area is a significant treasure trove of ancient archaeological sites belonging to the Hohokam and Salado cultures. Mixed among the Hohokam and Salado cultures are the more contemporary Apaches and Yavapais.\nThe Apache and Yavapais’ use of the Superstition Wilderness Area was more superficial than that of the Hohokam and Salado. Both the Hohokam and Salado cultures built mud and stone structures. Remnants of these structures can still be found throughout much of the area. Circlestone, Roger’s Canyon Cliff Dwelling and Garden Valley are classic examples of these types of structures.\nIt is difficult to estimate when man first occupied these lands in what is known today as the Superstition Wilderness. Many archaeologists suggest ancient cultures were using the region for gathering and hunting subsistence as early as 350 BC. There are lithics or stone tools that suggest a primitive hunting culture may have existence in this area 8,000 – 10,000 years ago. The Salado probably arrived on the scene around 800 AD. These architects of mud and stone left several excellent examples of their work in the region. You might say their architectural ingenuity created structures that have survived the ravages of time.\nMore damages have occurred to the Salado cliff dwellings in Roger’s Canyon during the past thirty years than in the previous eight hundred years. This damage is the result [of] modern man’s ignorance to the fragility of these ancient structures. The ruins were in perfect shape at the turn of the century. When I first visited the site in 1948 with my father it appeared as if its inhabitants had just moved out the day before.\nAnother interesting prehistoric ruin in the region is Circlestone. This 136-foot in diameter circular stone wall still defies complete explanation. The ruin is located on a grassy knoll. The elevation of the knoll is 6,010 feet above sea level. Knowledge of the ruin’s existence has been with us since the territorial days.\nTheories associated with this structure are numerous, however actual explanations with supporting documentation are unavailable. Early visitors to the site believed Circlestone was a Spanish corral or fortress. Others believed the site was once an early U.S. Army heliograph station connecting the military post of Arizona Territory in the early 1870’s. Not until the 1970’s was the site accepted as an ancient Native American archaeological site. Today Circlestone remains as one of the major enigmas of the Superstition Wilderness Area.\nThe discovery of surface potsherds and fetishes has created interesting speculation about Circlestone. Mr. Sam Henderson, an earlier superintendent at Casa Grande National Monument, has suggested the site may have been used as a trading center or even a special ceremonial site. Other archaeologists have suggested the site may have been celestially oriented and used for ceremonial purposes.\nGarden Valley was formed by a small group of Hohokams probably a 1,000 years ago when the climat[e] conditions were more favorable. This large valley of flat has more than 200 acres of arable land when there is a sufficient supply of water. Today mesquite and chain colla have become the climax vegetation in the area because cattle growers over-grazed the area for the past 100 years.\nA ruin was located in the center of the valley. This structure probably housed twenty-five to thirty individuals, while small caves on the fringe of the valley contained other families. Prior to 1930, the valley floor was literally covered with stone tools used by the ancient inhabitants who cultivated this special parcel of land.\nLate in November of 1931, the Arizona Republican co-sponsored an archaeological expedition led by the City of Phoenix archaeologist Odds Halseth. The expedition undertook selective collecting of surface artifacts and documented the location of each artifact before it was removed. Halseth, Harvey Mott and other members of the archaeological expedition made a cursory inventory of surface artifacts they did not collect. Several hundred lithics were inventoried on the surface, recorded, and left in place. Today, none of these lithics remain on the floor of Garden Valley. They all have been carried off by collectors during the past sixty-nine years. My father and I use[d] to walk through Garden Valley on our way to Second Water. Sometimes my father would take a side trip and show me matates and manos. They were still quite numerous in the late 1940’s if you looked closely for them. The lithics and potsherds of Garden Valley were indicative of Hohokam culture.\nThere is considerable evidence to suggest the Pimas gathered and foraged in the area long before 1500 AD. The Pimas gather[ed] the seeds of many plants common [to] the Superstition Mountain region including the cacti fruit and seedpods of various legumes such as the mesquite, ironwood, palo verde, and acacia.\nThe Apaches and Yavapais probably moved into these rugged mountains around 1500 AD. The Apaches and Yavapais both constructed temporary rancherias or farmsteads in locations such as Garden Valley, Frog Tanks, Dismal Valley, Rock Tanks, Reavis Valley, along Tortilla Creek and many of the tributaries draining into the Salt River.\nMost of these rancherias and farmstead[s] were destroy[ed] during the U.S. military campaigns against the Apaches and Yavapais between 1864-1868.\nThe Apaches and Yavapais used many of the caves and undercuts along Fish Creek Canyon, Tortilla Creek, La Barge and Boulder Canyon when they [were] pursued by the Army from Fort McDowell. The Native Americans often stole cattle, horses, sheep, goats, and mules and took them to the more accessible caves to be slaughtered for food. Bones of these animals have been found in caves along La Barge Canyon and Tortilla Canyons. It is estimated there are more than 2,500 archaeological sites within the boundaries of the Superstition Wilderness Area. During the past forty-five years I have recorded hundreds of sites on several maps.\nThe Superstition Wilderness Area is a fantastic artifact and heritage trove of ancient cultures that existed here for the past 1,000 years or so. It will take archaeologist[s] a century or more to sift through the archaeological history of the Superstition Wilderness and develop a systematic history of the area. Until that time the National Wilderness Act will help to preserve this valuable resource for future archaeologist[s] and scientist[s] to study. Hopefully visitors to the area will understand the importance of not disturbing archaeological resources within the wilderness area. Educating the public about the significance of this resource is a very important mission of the forest service.Wilderness Treasure","An Ancient Treasure in Modern Mesa\nMesa Grande Cultural Park opens January 19, 2013\nMesa Grande by air from the northwest.\nThe ancient Hohokam, ancestors of todayâ€™s Oâ€™odham people, built and used the\nMesa Grande platform mound between AD 1100 and 1450. The mound was the\npublic and ceremonial center for a one of the largest Hohokam villages in\nthe Salt River Valley, a residential area that extended for over one mile\nalong the terrace overlooking the river.\nThe Hohokam were the only cultural group in prehistoric North America to\nrely on massive canal systems, irrigating up to 110,000 acres of corn, beans\nand squash. Archaeologists from the Arizona Museum of Natural History\nexcavated one prehistoric canal that measured 15 feet deep and 45 feet wide.\nThese irrigation systems represented monumental efforts of labor and\nengineering. In the late 1800s farmers rebuilt and opened the brilliantly\nengineered Hohokam irrigation systems â€“ some remain in use today.\nMesa Grande was one of the two largest temple mounds of the Hohokam. Its\nsister mound, Pueblo Grande, is preserved as a museum and cultural park by\nthe City of Phoenix.\nMesaâ€™s First Historic Preservation Project\nThe drive to preserve Mesa Grande and open it to the public was Mesaâ€™s\nfirst historic preservation project. When this effort started is unknown but\nthe first public event was a parade down Main Street organized by the\nchamber of commerce in 1927. This was the year that Pueblo Grande, the other\ngreat mound of the Hohokam, opened to the public.\nMany such efforts followed and community support for a public facility has\nremained very strong through the years. In the early 1950s Frank and Grace\nMidvale organized the Mesa Grande Archaeological Society to promote the\nopening of the mound. This organization was transformed in 1955 into the\nMesa Archaeological and Historical Society. The new group held its first\norganizational event at Mesa Grande where over 200 members joined.\nA major force in the community, the Mesa Archaeological and Historical\nSociety attracted prominent speakers including governors and legislators\nincluding Barry Goldwater. Today, this is the Mesa Historical Society which\noperates the Mesa Historical Museum. Those with archaeological interests\nbegan what is now the Southwest Archaeology Team, which is affiliated with\nthe Arizona Museum of Natural History and continues to work on the Mesa\nGrande platform mound.\nPioneers of Preservation\nAnn Madora Barker\nMadora Barker and her husband purchased the land containing Mesa Grande in\n1916. Following the untimely death of her husband, Widow Barker and her boys\npreserved the Mesa Grande.\nOf her, Omar Turney wrote:\nâ€œNeeding the revenue which she might have obtained from pot hunters, both\nthe scientific and the unscientific, she has steadfastly refused to permit\nthem to destroy the fine old ruin of Pueblo de Lehi (Turneyâ€™s name for Mesa\nGrande taken from the Book of Mormon). Due to the self-sacrifice of this\nlone widow there remains just one ancient building on the south side of the\nriver which has not been torn open and its broken remnants used as highway\ndirt. The people of her faith should honor this woman: her faith will remain\npermanent in the country as long as it embraces women as true as she.â€\nTo preserve the mound, Madora Barker sold it to archaeologist Frank Midvale\nFrank J. Midvale\nFrank Midvale at the platform mound at La Ciudad.\nMidvaleâ€™s Mesa Grande logo.\nFrank Midvaleâ€™s intense interest in archaeology began at a very young age\nand carried through his entire life. Funding his work through teaching and\nother jobs, Midvale roamed the Arizona desert recording Hohokam sites and\nmapping the prehistoric canal systems. His notes on file at ASU preserve\nvaluable information on sites now long destroyed by modern construction.\nFollowing his early experiences with archaeology in the 1920s, Midvale\ndirected excavations of a platform mound at the site of La Ciudad covered\ntoday by Saint Lukeâ€™s Hospital, for Dwight Heard, a wealthy Phoenix business\nman and founder of the Heard Museum. Thousands of visitors toured the\nexcavations at La Ciudad from 1929 to 1936. Having earned a B.A. in\nanthropology, Frank was completing his masterâ€™s degree at the University of\nArizona when the United States entered World War II. He then served his\ncountry as a member of the Army Air Corps. He finished his formal career in\narchaeology in the late 1960s as an interpreter at Casa Grande Ruins\nNational Monument, a job he loved.\nIn 1927 Midvale purchased Mesa Grande from Ann Madora Barker to preserve the\nsite. With his wife Grace, he founded a group that ultimately became the\nMesa Historical and Archaeological Society, which originally was dedicated\nto the preservation of Mesa Grande. Unable to open Mesa Grande to the\npublic, Midvale transferred the mound to Jack and Acquanetta Ross in 1962.\nMidvale hoped that they had the influence to accomplish his dream of opening\nan archaeological park. The preservation of Mesa Grande and his\nirreplaceable notes on Hohokam sites stand as Frank Midvaleâ€™s lasting\nAcquanetta was one of the most colorful people in the history of Mesa\nGrande. Acquanetta was a well-known movie actress, billed in Hollywood as\nthe Venezuelan Volcano. She is perhaps best known for playing the title role\nin Tarzan and the Leopard Woman with Johnny Weissmuller, but she\nappeared in many other films. Acquanetta married Jack Ross, a three\ntime gubernatorial candidate and owner of a car dealership. Acquanetta\nappeared in television ads for the dealership and became a beloved local\nAcquanettaâ€™s mother was Native American and Acquanetta grew up in the\nArapaho community in Montana. She had strong feelings for Mesa Grande and\nworked for many years to preserve the mound and to open it to the public. In\nthe 1970s, she worked tirelessly with the Mesa Historical and Archaeological\nSociety and the City of Mesa to open the mound to the public. Having failed\nto get adequate support for the project, she played the key role in the\n1980s in getting Mesa Grande into public ownership.\nMesa Grande was listed on the National Register of Historic Places on\nNovember 21, 1978. In the 1980s, new preservation efforts were finally\nrewarded when, in response to significant community support, the City of\nMesa purchased the mound in 1988 for $1.1 million.\nSince then, the Arizona Museum of Natural History, through their\naward-wining avocational archaeology group, the Southwest Archaeology Team,\nhas carried out stabilization, reconstruction and excavation efforts. In\n1994, the museum developed a comprehensive master plan for Mesa Grande.\nInitial studies focused on defining the condition of the mound, identifying\nprocesses of erosion, and providing the information needed to prepare a\nstabilization plan. The stabilization plan was in place the following year,\nand volunteers began implementing the plan. The emergency stabilization was\ncompleted within a few years.\nThe Mesa Grande project is based on a strong preservation ethic that focuses\nupon preserving the mound for the future. While archaeological excavation\nprovides an important window on the past, it is also a destructive process.\nOnce a site has been excavated, it cannot be excavated again and much of the\ninformation it holds can be removed. On all archaeological excavations, the\nrecording and preservation of information is critical.\nThe approach taken at Mesa Grande has been to re-excavate and stabilize\nareas that were disturbed by digging in the last 100 years. When first\nexcavated, these areas began to erode and many such areas were still eroding\nat the beginning of the Mesa Grande archaeological project. Archaeologists\nre-excavated damaged areas of the mound, primarily removing soil that built\nup during the historic period. The archaeology team stabilized and restored\nportions of the mound and prepares the areas for interpretation for visitors\nwhen the park is opened. This process allows us to solve current erosional\nproblems, create interpretive areas, avoid new damage to the site, and\ngather significant new information about the mound.\nSWAT volunteers excavating an ancient wall at the north end of the\nMesa Grande platform mound. In 1887, famed anthropologist Frank Hamilton\nCushing excavated this area, which was in need of repair by the 1990s.\nPreservation of Tradition\nThe Hohokam have been gone since around 1450, but the descendants of the\nHohokam are our neighbors today. Some are known as the Akimel Au-Authm\nor Oâ€™othom, the River People, also known as Pima. Near Mesa Grande,\npeople from these traditions live in the Salt River Pima-Maricopa Indian\nCommunities and the Gila River Indian Community. Representatives of\nboth communities were crucial members of planning team for the Mesa Grande\nproject, for which we are most appreciative.\nstory of the Salt River Pima-Maricopa Indian Community\nRead the story of the Gila River Indian Community\nOther descendants of the Hohokam are the Tohono Oâ€™odham, the Desert\nPeople, also known as Papago.\nA New Interpretive Plan for Mesa Grande\nEfforts to open Mesa Grande to the public were redoubled in 2005 and\ngreatly facilitated by a generous grant from the Salt River Pima-Maricopa\nIndian Community. The museum assembled a team of archaeologists,\nmuseum professionals, Native Americans, educators and others to plan an\ninterpretation plan for Mesa Grande.\nThe team recognized that Mesa Grande is a sacred cultural heritage site in\nthe middle of a vast urban landscape. The interpretive plan is based upon\nthe unique and sacred nature of the site itself. The modern footprint\non the site will be the minimum necessary to tell the story of Mesa Grande\nand the Hohokam, and allow other places and formats to carry portions of the\ninterpretive load, such as the Southwest exhibitions at the Arizona Museum\nof Natural History and electronic formats such as this website.\nMesa Grande Planning Team\nThomas H. Wilson, Director, AzMNH\nJerry Howard, Curator of Anthropology, AzMNH\nSuzanne Fish, Hohokam archaeologist, University of Arizona\nBill Doelle, Director, Center for Desert Archaeology\nDave Abbott, Hohokam archaeologist, Arizona State University\nMichael Erwin, Professor, University of Advancing Technology\nRoger Lidman, Director, Pueblo Grande Museum\nSalt River Pima-Maricopa Indian Community Team\nKelly Washington, SRP-MIC Cultural Resources Director\nShane Anton, Cultural Preservation Program Supervisor\nAngela Garcia-Lewis, NAGPRA Coordinator\nThomas Wright, Staff Archaeologist\nGila River Indian Community\nBarnaby Lewis, Cultural Resource Specialist\nRon Peters, AIA, AICP, Historic Streetscapes\nMary Ann Modzelwski, AIA, WHPacific\nCity of Mesa Team\nSteve Stettler, Engineering\nStephen Ganstrom, Engineering\nAzMNH Exhibitions and Education Staff\nTim Walters, AzMNH Exhibitions Coordinator\nMichael Ramos, AzMNH Graphic Designer\nKathy Eastman, AzMNH Curator of Education\nMargaret Barton, Collections & Archaeology\nSouthwest Archaeology Team (SWAT)\nJim Britton, Southwest Archaeology Team representative\nMesa Grande Interpretive Trail\nThe Mesa Grande Cultural Park trail system.\nThe Welcoming Center and Gathering Place are at the orange square bottom\nThe red circles along the trail mark the interpretive kiosks.\nThe yellow triangles mark locations of future erosion control canopies.\nThe layout of the interpretive trail is based upon tours given by the\nSouthwest Archaeology Team (SWAT) since 1986. There are nine stations\non the trail that correspond to significant places on the mound. There\nare four themes at each station.\nThe Hohokam: Explores Hohokam origins,\nirrigation, village structure, trade, daily life, what happened to the\nHohokam and other subjects.\nOld Explorer: The first non-Native\nAmerican visitors to the area or site, explorers and archaeologists such as\nFather Eusebio Kino, John Russell Bartlett, Frank Hamilton Cushing, Adolf\nBandelier, Emil Haury and others.\nArchaeology Today: The state of our knowledge\nbased upon the best modern scientific understanding of Mesa Grande,\nincluding mound construction, Room D, shooting gallery, entryway, eastern\nplaza, Cushing excavations, back door, ballcourt, and top of mound.\nLook Closely: Invites you to examine the\nscience and natural history of Mesa Grande, including the symmetry of mound\nerosion, erosion gullies, wall erosion, caliche, and the plants and animals\nof the site.\nWelcoming Center and Gathering Place\nWelcoming Center and Gathering Place\nfrom the North\nWelcoming Center and Gathering Place from the Northeast\nVisitors will first arrive at the Welcoming Center, where they will\npurchase tickets and receive orientation to the Hohokam and to Mesa Grande.\nThe viga and latilla ceiling and banded masonry of the building suggest\nNative American building techniques, and the rammed earth tapered walls\nflanking the center on two sides reference the mound itself.\nThe Gathering Place is a semicircular courtyard paved with flagstones and\nshaded with fabric canopies. Benches surround and further define the\nspace, which can be used for group orientation or events. A focal\npoint of the Gathering Place is the lifesize bronze sculpture Hohokam\nby Antonio Pazzi.\nHohokam, by Antonio Pazzi.\nGift of Dennis R. and Mary L. Pollard."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:bd708746-c8e6-4f52-a362-31a78d810918>","<urn:uuid:f5530d94-a3a3-4d11-9fc8-d949a96819f5>"],"error":null}
{"question":"What are the main differences between a standard vulnerability assessment and a penetration test?","answer":"A vulnerability assessment and a penetration test differ significantly in their depth and execution. A vulnerability assessment simply involves using a tool to scan the environment for possible vulnerabilities in operating systems and applications. In contrast, a penetration test goes further by employing a qualified professional who possesses similar knowledge and skills as a real attacker. This professional uses the same techniques and tools as actual attackers to not only identify but also exploit security vulnerabilities, determining potential damage from an actual attack. The penetration tester must think creatively and 'out of the box' to identify various exploitation possibilities, utilizing both commercial and publicly available tools to achieve accurate and consistent results.","context":["Penetration Tests, or ethical hacking, are an important part of any security program. Our methodology, based largely on the Open Source Security Testing Methodology (OSSTM), is designed to validate the security surrounding externally connected systems from the Internet, as well as within a Corporate Network, depending on your needs.\nBoth External and Internal Penetration Tests are designed to detect weaknesses in a system, network or application that could allow host or information compromise and safely exploit these weaknesses to evaulate the impact on business operations.\nExternal Penetration Testing involves finding and exploiting known and unknown vulnerabilities from the perspective of an outside attacker. External pentests can also test an organization's monitoring and incident response capabilities. Our testing is designed to mimic a variety of scenarios including a casual hacker who infiltrates a system for spam or other unrelated malicious purposes, dedicated hackers targeting with a specific goal and rogue employees or disgruntled ex-employees who may have privileged access or the ability to bypass certain access controls.\nUsing the Open Source Security Testing Methodology, our tests are comprehensive examinations of both network and application layer vulnerabilities. Our analysts use commercial and publically available tools - the same tools as the bad guys, in many cases, to achieve accurate and consistent results.\nInternal Penetration Testing examines the security surrounding internally connected systems, typically within a corporate network. Just as External Penetration Testing tests the security of externally connected systems from over the Internet, Internal Penetration Testing involves the finding and exploitation of known and unknown vulnerabilities from the perspective of an inside attacker, whether that be a guest within the premises of an organization or disgruntled employee.\nOur internal pentests are also designed to address advanced threats such as layer 2 and 3 protocols that if successfully leveraged, can be exploited to gain additional access to a corporate network.\nPenetration Testing must be conducted to achieve compliance with a multitude of regulations and standards that industries face including the Payment Card Industry Data Security Standard (PCI DSS) [PDF]. These regulations require that penetration testing to be performed once a year and after any significant application modification or network upgrade.\nWhile not required for compliance testing, many of our customers wish to evaluate the security surrounding their wireless networks - whether for guests or employees. Dara Security will test not only the encryption of your wireless network, but also perform a site survey, identify any rogue access points, and test to ensure proper segmentation is applied. Visitor or guest wireless networks without proper segmentation could allow an outsider access into your business environment.\nSocial engineering can be performed in a multitude of ways, however our preferred method is through electronic means, such as email and telephone. In a social engineering exercise, our analysts will attempt to solicit employees to supply privileged information, such as usernames or passwords, trade secrets or personal identifying information (PII). We can also passively collect information regarding users who open email attachments or visit potentially malicious websites controlled by Dara Security. Combined with a penetration test, this can offer a valuable evaluation of the human factor of your security program.\nDara Security's analysts have over a decade of experience in various Profiling and Penetration Testing techniques. Our team's certifications include CISSP, GPEN, GXPN, GWAPT and certified web application penetration testing certifications. Our analysts are constantly studying and working to stay at the forefront of penetration testing and security assessment techniques as well as business trends through training, education, and speaking.","Unlike a vulnerability assessment, in which a tool scans the environment to determine possible vulnerabilities in operating systems and applications, the pentest goes further. It has a qualified professional with similar knowledge and skills as a real attacker, who uses the same techniques and tools to identify and exploit security vulnerabilities and find out what kind of damage could be caused by an actual attack.\nThis penetration testing professional must have the ability to think \"out of the box\" and use their creativity to identify the various possibilities exploitation of a corporate network.\nWhat are the main categories of a penetration test?\nBlack Box - In this type, the professional contracted for the penetration test does not have any information about the network that will be analyzed, it is the approach that happens when a real attack occurs. The attacker develops research and then identifies the best way to compromise the network.\nGray Box - Here the professional pentester receives some basic information about the environment to be tested, this information can include a valid IP address or user credential for the environment, very useful to identify what type of attack could be carried out by an internal user or employee the company.\nWhite Box – In this scenario, the test is conducted by a contracted professional who receives all the pertinent information for the network to be analyzed, including network topologies and IP addresses of computers. It is a useful method to identify what type of attack could be carried out by a user with administrative rights and to determine the faults in any isolated internet networks, but which can still be compromised in other ways, such as using a USB stick malicious content on a workstation.\nWhat are the benefits of pentesting to my company?\nThe practice of regular penetration tests is mandatory for some security standards (such as PCI-DSS) and creates many advantages for your company, including:\n- Test your environments and defenses against attacks by a qualified professional who knows the same methodologies and tools used by real attackers;\n- Identify what types of data can be stolen from your company;\n- Identify the risks to the business and, consequently, to the reputation of your brand, avoiding financial damages that go beyond the loss of data;\n- Validate your security and compliance policies and ensure that they are adequate to ensure network and asset security;\n- To plan investments in Information Security more effectively with a penetration test it is possible to discover that a smaller investment in a specific tool or process may be more useful for improving the security of information in the company than the acquisition of that equipment that costs $1 million that you were planning to buy.\nAn impressive ROI, right? Have you already tested to see if your company is protected against attackers and cyber threats?\nRed Team vs Blue Team Exercises\nAnother method that is part of ethical hacking is the \"Red Team versus Blue Team.\" In this service, a team of attackers (the red team) attempts to attack the company to test the effectiveness of the company's Security Operation Center (the so-called blue team) in response to the attack. The methodologies, tools, and tactics used by the red team are, in general, like the Black Box.\nThe result in any of the methods used is the issuance of a detailed report describing the types of vulnerabilities that were successfully exploited in the intrusion test, which data could be accessed, the servers and workstations that were compromised, in addition to possible damage which could have been caused by a real attack. The report considers the criticality of each of the flaws about the business processes and business continuity and, depending on the contracted scope, includes a patch management plan for the vulnerabilities encountered."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:9a3a6f56-6ccf-46c5-af35-cd774b3eaf38>","<urn:uuid:96fa3c76-5fe7-4b70-b092-bf94ec2096bf>"],"error":null}
{"question":"How do the noise reduction approaches differ between Altered Studio's morphing technology and Rega DAC-R's digital audio processing?","answer":"These systems take different approaches to noise reduction. Altered Studio focuses on preventing background noise and room reverb at the input stage, requiring clean raw audio without effects for optimal speech synthesis quality. For the Rega DAC-R, noise reduction occurs at multiple stages of digital processing - it employs special attention to minimizing control data noise between ICs, uses dedicated power supplies for the receiver/PLL, and implements total isolation from host computers through transformers on the USB input stage. The DAC-R also uses audio grade capacitors bypassed with MMK polyester capacitors and separate power supplies for digital and analog stages to minimize noise.","context":["1. Morphing audio files in Altered Studio is the process of using Speech-to-Speech synthesis to change your input voice into a new ‘target’ voice.\n2. You can morph the whole file, a single block, or a selection from the waveform.\n3. Before you can morph audio on the desktop version you may need to install your desired voices from the Voice Library (refer to Using the Voice Library).\n4. Always start with clean, raw audio, without any effects or filters applied. The speaker should be close to the microphone and background noise or room reverb should be avoided as these can interfere with the synthesis quality. Refer to Tips for Best Recording Performance for further guidance.\n5. If you want to morph different sections of audio file into different voices (e.g. for different speakers in a conversation etc), then you should add a new Morph effect layer for each section / character. Each Morph effect layer only synthesises the audio from the section of audio that the Morph effect was applied to, so if you want to morph a different part of the file then you should create a new Morph effect for that section.\n6. By using the block controls in the transcript panel, you can easily add Morph effects to different parts of your file to create a conversation between a multi-cast of characters (refer to Working with the Transcription Panel for information on how to morph a block).\n7. You can also select your desired section of the waveform and use + Effect to add a new Morph effect. If no selection is made, then the Morph effect will be applied to the whole file.\n8. Once the Morph effect layer has been created, use the (+) button the Morph Properties panel to open the Voices selection screen. From here choose one or more Voices which you would like to use and click OK.\n9. Each Voice has a number of options as outlined below, take care to set these before proceeding. Refer to Working with Presets for instructions on how to save Voice configuration settings for future use.\n10. Once you have chosen your Voice, move onto the voice model selection and voice settings.\nVoice Model Selection\n1. The Model dropdown allows you to choose between the different models available for each Voice. These will vary depending on your subscription level and you may not have access to all types. Different models are designed for different use cases (see below).\n2. Timbre (cross-lingual) models are designed for non-English inputs. These models are the only ones which don’t convert accents and are best suited to preserve characteristics of the accent, sounds and emotes from your input performance. Age, Gender and Loudness shifts are now available in Timbre.\n3. Performance (English) models allow a closer reproduction of your input performance and dynamics, with the additional Prosody setting for greater performance range.\n4. Flexi (English) models have additional settings such as Age, Gender and Loudness shifts to provide more variety and range.\n5. Clone (English) models are designed to sound more like the target voice than the Performance voices do, and are less influenced by the input speaker than the other voices.\n6. Narration (English) models have greater constraint over dynamics and performance characteristics, to give a more typically narrative feel.\n7, In addition, each model type has a “Fast” version which provides quicker, but lower fidelity synthesis, which can be useful for rapid prototyping.\nUsing Voice Settings\n1. The Morph panel contains several other settings that will change the morph output. The settings available may change depending on which model is used.\n2. Use 48kHz generates the output of the synthesis at 48kHz, if this is not selected then the output will be generated at 24kHz by default.\n3. Decreak corrects minor vocal fry/creak in the source file, to provide a clearer synthesis output. Turn this setting off if you prefer these sounds to come across in the performance from the source audio.\n4. Pitch Shift is used to change the pitch of the morphed sample from the target voice’s natural pitch. The best results come from using a range of +/- 2 semitones, however the setting allows for a wider range if you wish to experiment. This setting can improve the synthesis where there is a large difference in pitch between the input and target voices so it is worth trying different settings to get the best output.\n5. Target Prosody is used to adjust the weighting between the input voice and the target voice’s natural performance characteristics. A higher Prosody (closer to 100%) will reduce the likeness to the input performance and take more performance from the target voice model.\n6. Text Reinforcement is available on certain subscription levels for some models. It can be used to correct minor mispronunciations on the morph output by forcing the spoken words to match the transcript. If the transcript is wrong you should edit the text before you add a morph effect in order for the Text Reinforcement functionality to be available.\n7. Loudness Shift changes the output between whispering (dial to the left) and loud speaking (dial to the right).\n8. Age Shift changes the output between younger sounding (dial to the left) and older sounding (dial to the right).\n9. Gender Shift changes the output between highlighting more masculine qualities (dial to the left) or more feminine qualities (dial to the right) in the voice.\n10. Power Envelope can be used to smooth out dynamic changes in longer morphs. If you find the synthesis is performing inconsistently across the file, then turning this ON can help to improve your morph outputs.\n11. Post-Processing sharpens the sound and reduces artefacts and noise in the synthesis. By default this is set to 80% however higher or lower settings may be appropriate for your audio. This setting is not available on the Fast models due to the nature of the synthesis process.\nSynthesising a Morph Sample\n1. Once you have finalised your settings click Synthesize to create a Morph sample.\n2. The Speech-to-Speech synthesis is non-deterministic, so there may be slight differences in read, emphasis etc. between outputs generated with the same settings. For this reason we recommend generating a few samples so you can choose your preferred one. NB this principle does not apply to the Fast models which are deterministic and will give consistent synthesis outputs each time.\n3. Click on your chosen sample to select it as active on the waveform, you can listen to the input audio for a Morph effect at any time by pressing ▶ on the Input Audio row in the sample bay.\n4. The Mixing panel will show options for mixing, loudness and crossfade for the morphed audio with the rest of the waveform. Refer to Adding Audio Effects for more information on using these effects.\n5. Once a morph sample is selected, subsequent edits and effects applied to this section of the waveform will be based on the morphed audio.\n6. If you add a morph effect which overlaps a previous morph effect layer in the History then you will see a yellow area above the voice selection panel which says “Input for this effect already contains morphed audio”. This indicates the overlap exists, check “Use original audio” to avoid morphing the original morph again, which may decrease quality.\n7. Using the blocks in the Transcription Editor is a simple way to avoid any overlapping sections.\n8. When generating synthesis you can add as many morphs as you want, and you do not need to wait for each to finish before starting the next. As noted above, some models will give slight variations in read and performance, so it is often beneficial to click the Synthesise button multiple times and quickly add additional samples for processing to find your favourite output.\n9. Refer to Morph Quota Guidance for more information on how quota is consumed during synthesis.\n10. You can export an individual morph sample to disk by right-clicking and choosing Export as. Alternatively, you can export all the files in the sample bay at once by using the ⬇ download icon in the top right corner of the Sample panel.\nAdditional Morph Sample Options\n1. By right-clicking on a morph sample you can perform the following functions.\n2. Open the sample as a new tab to edit it separately.\n3. Copy the sample to the Clipboard for future use in the same session (refer to Working with the Clipboard).\n4. Export Preset will create a new preset with the settings used to create this morph sample.\n5. Retake Sample will create a new sample with the same settings.\n6. Export as… lets you export an audio file of the sample to disk.\n7. Delete will delete the sample","Developed to be simple to set up and use, the Rega DAC-R is designed to optimise performance from any two channel PCM digital audio source.\nWith the PC now widely accepted as a credible medium for storing and streaming music,the use of high quality lossless files such as WAV, FLAC and ALAC offer performance through the DAC-R equal to, and in some cases better than, Red Book CD. Great care has been taken to remove noise generated by the PC and other input sources. (During development this was identified as a major drawback with many DAC’s on the market today).\nThe DAC-R is housed in a custom aluminium and steel case and boasts a pair of Wolfson DAC IC’s, three user selectable digital filters and one isolated asynchronous USB, two isolated Co-axial inputs & two Toslink SPDIF inputs. The DAC-R has been designed and engineered to integrate perfectly into any system and achieve the highest performance in its class. We hope you enjoy this Rega product for any years to come. The optical and coaxial input stage comprises a Wolfson digital receiver with a high stability, low jitter clock driving the receiver PLL. The receiver and PLL have their own dedicated power supplies.\nThe DAC stage comprises a pair of parallel-connected Wolfson WM8742 DAC’s, that are driven via a buffer stage, which ensures the integrity of the data being fed to the DAC IC’s similar to the arrangement used in the Isis reference CDP. The USB input stage is comprised of a bit perfect XMOS USB Audio 2.0 with asynchronous clocking. The USB input stage feeds the Optical and Coaxial input stage via an isolating transformer giving total isolation from the host computer.\nThe output amplifier employs a discrete differential, multiple feedback filter and output amplifier, with a high cut-off frequency for use with higher sample rates. We decided not to use a sample rate converter and process the data at the incoming sample rate which keeps the signal processing to a minimum. Jitter was minimised by synchronously locking the digital data with our receiver PLL (removing any jitter from the input signal).\nAll the capacitors associated with the analogue signal path are audio grade bypassed with MMK polyester capacitors, and low impedance conductive polymer capacitors are used for DAC decoupling. The power supply utilizes a toroidal transformer, fast rectifier diodes and again audio grade capacitors. There is a power supply for the control micro controller, separate from the digital & analogue audio stages. Special attention being paid to the inter IC control signals ensures the control data noise is kept to a minimum.\nDAC 2x Wolfson WM8742\nFrequency Response (100KΩ load) Filter 1 selected\nLow data rate 44.1/48kHz = 10Hz -0.05dB to (44.1K) 20.02kHz (48K) 21.7kHz -0.03dB\nMedium data rate 88.2/96kHz = 10Hz -0.05dB to (88.1K) 28.7kHz (96K) 31.2kHz -3dB\nHigh data rate 176.4/192kHz = 10Hz -0.05dB to (176.4K) 44.1kHz (192K) 47.7kHz -3dB\nMaximum output level = 2.175V into 100KΩ load\nBit resolution (all inputs) 16 to 24bit\nSupported data rates = 32, 44.1, 48, 88.2, 96, 176.4, 192kHz\nTotal Harmonic Distortion all inputs (24bit 96kHz) = 0.006% @ 1kHz\nSignal to Noise Ratio -105dB (relative to maximum output level with a 100Hz to 22kHz bandwidth)\nUSB Asynchronous Isolated (24bit 44.1/48/88.2/96/176.4/192kHz)\nInput 1 Optical/Toslink (24bit 32/44.1/48/88.2/96/176.4/192kHz)\nInput 2 Optical /Toslink (24bit 32/44.1/48/88.2/96/176.4/192kHz)\nInput 3 Isolated 75Ω Co-axial (24bit 32/44.1/48/88.2/96/176.4/192kHz)\nInput 4 Isolated 75Ω Co-axial (24bit32/44.1/48/88.2/96/176.4/192kHz)\nDigital SPDIF outputs (via receiver & PLL)\nIsolated 75Ω Co-axial\nDimensions W 215mm x D 320mm x H 80mm\nRemote NEC system (6E91)\nWeight 4.0 Kg\nWhat can I plug into my DAC?\nDAC's will usually have a number of inputs. Coaxial and Toslink Optical are the most common, while USB is still also very common.\nThe Optical and Coaxial connections can have a CD player, network streamer, television, dvd/blu-ray disc player, game console, and some computers.\nThe USB input can handle pretty much any kind of computer, or a phone with the appropriate adapter (OTG for android, Camera Connect Kit for iOS devices).\nWhat is a DAC?\nEvery digital device with an analog output has a Digital to Analog Converter of some kind. In the case of phones, televisions, and computers, they are usually stuck on as an afterthought, or just to do the job of creating analog sound to be amplified and played through speakers.\nHaving a standalone DAC is a worthwhile investment in any modern Hi-Fi system, essentially all the circuitry involved in creating that signal you can hear from the 1's and 0's is housed in its own box, with usually a much better design than what you would find in your television or computer.\nA DAC can be useful to connect digital devices to an analog amplifier if the amplifier doesn't have digital inputs. Or if you are after a cleaner, more refined sound.\nThings to listen for in higher quality DAC's are separation of sounds (how well you can hear a single voice or instrument in the mix of all the others), \"textures\" of sounds (raspyness of a bow being drawn across the strings of a double bass), or the clarity of echo's and quiet noises among loud instruments."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:4dceb0a9-4187-4ae3-b545-5c370aa7cd0e>","<urn:uuid:711204e7-173d-4c48-b758-1c722eb98711>"],"error":null}
{"question":"What are the risks associated with excessive hydration in dancers?","answer":"Over-hydrating can lead to a dangerous condition called hyponatremia, which is characterized by increased water in the blood. Early symptoms can include nausea and headache, though these symptoms are not specific to hyponatremia. Additionally, if a dancer is urinating clear fluid frequently and in large amounts, it may indicate they are drinking too much fluid.","context":["Updated: Feb 16\nAdequate hydration is critical to an Irish dancer’s overall health and performance. It is important to understand that as individuals, each dancer has their own hydration requirements. Some individuals sweat extensively, have more surface area by which they lose fluids or simply are older (and thus larger) individuals and have higher fluid needs. It is critical to be well-hydrated prior to, during and after dance class, strength or cross-training and any physical activity.\nIt is well documented that dehydration can lead to lethargy, poor athletic performance, muscle cramping, poor concentration and focus. In some cases, dehydration can put an adult athlete’s health and safety in considerable risk especially during extended vigorous physical activity in warm or hot environments. While the adverse effect on young athletes are not as extensively researched, the negative effects are most likely related to decreases in cardiovascular system functioning, thermoregulation and central fatigue.\n“In adult athletes, loss of 2% body weight due to dehydration has been shown to have detrimental effects on performance. In children, the negative effects of fluid loss begin to occur at 1% decrease in body weight”.\nDespite this and long-standing beliefs otherwise, The American Academy of Pediatrics (AAP) noted in their revised policy statement that children can tolerate and adapt to exercise in heat as well as similarly fit adults, when adequate hydration is maintained. Children and adolescents are particularly at risk as they may not notice the signs of dehydration nor correlate them to dehydration. Symptoms of dehydration may include: noticeable thirst, fatigue, nausea, headache, dizziness, muscle cramping, irritability, weakness, dark yellow urine or no “need” to urinate, decreased performance, inability to focus.\nAdequate hydration is critical to dancers overall health but there is also a risk in over-hydrating. Drinking too much fluid too quickly can lead to increased water in the blood, a condition called hyponatremia, that can lead to much more dangerous outcomes. Early signs of hyponatremia can include nausea and headache (though these are not specific to hyponatremia) along with more severe symptoms.\nSo how much fluid is appropriate for each individual dancer?\nHow can I tell if I’m hydrated?\nThe Pee Test – check out your pee.\nI realize how strange this sounds but next time you go to the bathroom, check out the color of your pee. (see chart)\nIf it is more like a lemonade color, you’re on the right path of hydration.\nIf it is dark in color (like apple juice or darker), drink up!! (Urine is notably darker right after waking so don’t panic, but get drinking☺\nIf you are urinating clear, often, and large amounts, you might be overdoing it. Slow it down a bit.\nHow can I tell if I’m drinking enough fluids during dance class/cross-, strength-training/ physical activity?\nWeigh yourself (with little clothing) before and after class. Keep track of how much fluid you drank during class. It is important that you’ve done your pee test prior to weighing yourself. If you are not well-hydrated before going to dance class, you are putting yourself at risk.\nYour weight should not decrease more than a pound during that time. If it does, you need to bump up your fluid intake 16 ounces of fluid for every pound lost.\nIf your weight increases, you may be drinking too much. Try to cut back a bit so that your pre- and post- activity weights are similar.\nSome variance in those two numbers is normal\nFollow the AAP Hydration Recommendations :\nProvide and promote consumption of readily accessible fluids at regular intervals before, during, and after activity to offset sweat loss and maintain adequate hydration while avoiding over-drinking. Generally, 100 to 250 mL (3 to 8 oz) every 20 minutes for 9- to 12-year-olds and up to 1 to 1.5 L (34 to 50 oz) per hour for adolescents is enough to minimize sweat-induced body-water deficits during exercise as long as pre-activity hydration status is good.\nInterested in working with Jess to target the specific needs you have to better fuel your body for Irish dance?\nCheckout the Target Training Nutrition Consultation details and get started today!\nINTERESTED IN IRISH DANCE-SPECIFIC TRAINING VIDEOS?\nReceive 14 DAYS FREE @ the Target Training Online Institute\nIrish dance strength & conditioning, ANYTIME, ANYWHERE! Join the Target Training Online Institute today and get a FREE 14-day trial of the Trainer subscription, no code needed.\nGet started at institute.targettrainingdance.com\n1. Van Pelt, J. Hydration in Young Athletes. Today’s Dietitian. 2015; 17 (4) page 28. https://www.todaysdietitian.com/newarchives/040715p28.shtml.\n2. Council on Sports Medicine and Fitness and Council on School Health, Bergeron MF, Devore C, Rice SG; American Academy of Pediatrics. Policy statement — climatic heat stress and exercising children and adolescents. Pediatrics. 2011;128(3):e741-e747. https://pediatrics.aappublications.org/content/128/3/e741.\n3. Council on Sports Medicine and Fitness and Council on School Health, Bergeron MF, Devore C, Rice SG; American Academy of Pediatrics. Policy statement — climatic heat stress and exercising children and adolescents. Pediatrics. 2011;128(3):e741-e747. https://pediatrics.aappublications.org/content/128/3/e741."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:e6b8a3ee-31b5-4f0a-b98b-5d2b9030fb24>"],"error":null}
{"question":"I study finance and I'm researching credit card features. Could you explain what makes Wyndham Rewards unique among hotel loyalty programs, and what are the advantages and disadvantages of retail store credit cards in list format?","answer":"Wyndham Rewards is unique because it has a flat redemption rate of 15,000 points for a free night across all hotel categories, unlike competitors like Marriott, Starwood, and IHG which require more points for higher category hotels. This applies to any property in their network of over 7,000 hotels across 14 brands. For retail store credit cards: Advantages include: special offers and discounts for cardholders, opportunity to build credit with small purchases, and exclusive cardholder rewards and rebates. Disadvantages include: higher interest rates compared to standard credit cards, potential credit score drop from new account opening, lower credit limits that can lead to high utilization ratios, and each application causing approximately five points drop in credit score due to inquiry.","context":["The Wyndham Rewards program is one of the most popular hotel rewards programs, and one of the easiest and most straightforward compared to loyalty programs from competing hotel brands. From booking free nights to obtaining frequent flyer miles, you have a number of options to save through the use of your points. Based on our research, we found the overall average value of the Wyndham point to be 0.6 cents . However, depending on the redemption, that value can rise to as high as 3.40 cents per point to as low as 0.29 cents per point.\nValue of Wyndham Rewards by Redemption Type\nThere are four main ways you can redeem Wyndham rewards for: hotel reservations, booking airline tickets, gift cards and transferring to airline partners for miles. Below you will find, in descending order, the redemptions that get you the most value per point to the least.\n|Redemptions||Value per point (cents)|\n|Highest Priced Hotels in Major Cities||1.42|\n|Median Priced Hotels in Major Cities||0.64|\n|Booking Airline Tickets||0.43|\n|Lowest Priced Hotels in Major Cities||0.38|\nAs you can see, not all redemptions for rewards, or even hotel nights, are created equal. For more information on the pros, cons, and math behind each of these:\nWyndham offers over 7,000 hotels through its 14 brands making it one of the most encompassing hotel networks in the world. What makes the Wyndham program so different from other hotel rewards programs is the flat 15,000 points for a free night across all categories of hotels. As opposed to Marriott, Starwood, and IHG, which increase the amount of points needed for higher category hotels, Wyndham will allow you to spend 15,000 points for any hotel, be it the most luxurious or most budget-minded hotel around. Due to the nature of the flat 15,000 per night rate, the value you get will fluctuate based on the different cities you redeem in, as well as the cost of a night's stay at the property. We explored the rates for free nights throughout major cities in the U.S and abroad obtaining the average price per night in each and calculated where you will get the most value per point.\n|Location of Property||Average Price||Average Point Value (cents)|\n|New York City||$172||1.14|\nOf the seven domestic and three international cities we studied, we found the average for reservations to be $96 or 0.64 cents per point. However because of the flat 15,000 point redemption, you will get a much higher value for your points by booking in more expensive cities at more expensive hotels. For example the average value for New York City properties is $0.009 or 0.90 cents per point, 0.26 cents higher than the worldwide average. The average highest price our research identified in these cities is $213/night or 1.42 cents per point, but in other places such as Los Angeles you can get a hotel that normally costs $510 a night, giving your points a value of 3.43 cents. You should be aware, though, that there are only a handful of high-tier hotels per city and they may fill up fast. Additionally, it may be more beneficial to book a lower-tier hotel to capitalize on Wyndham's cash + points option. In a handful of hotels in every city, you will be given the option to use only 3,000 points plus cash to make up the difference for the reservation. We find this to be a generally good deal so long as the cash required is less than $75 (which is mostly the case for lower-tier hotels).\nFor example, you can pay $85 for a night’s stay in a Wyndham hotel in LA, or redeem rewards for a free night using 3,000 points and paying $45. The question then becomes whether redeeming 3,000 points is worth saving $40 on the cost of that night. To figure this out, you can divide the 3,000 points required by the $40 savings, for a point value of 1.3, which means your points go a lot further in this scenario. Put another way, if you divide the average value for a Wyndham Point, 0.60cents into the $45 dollars, you will get 7,500 points, essentially making this reservation worth 10,500 points. That's a discount to the 15,000 points that would have been required to redeem the free night otherwise.\nAnother example, in Boston, a room costs $179 but can be redeemed for 3,000 points and $150. This is a less than optimal because the $150 is worth 25,000 points. Generally speaking, if they are asking for more than $75, pay the 15,000 points instead. Essentially for all cases, you should strive to surpass 0.64 cents/point for hotel reservations specifically, or 0.6 cents overall. Anything below is a bad deal.\nWhere are Wyndham Properties Located?\nWyndham and all of its brands is one of the most accessible hotel chains in the world. The United States has a Wyndham in every major city and most minor ones making this program great for domestic travellers. Internationally, there are more Wyndham properties than Starwood or Marriott. This is particularly true in South America, where frequent visitors will find much more value in this program than other hotel rewards programs. The only region that is less served by Wyndham compared to other hotel chains is Africa, where only six Wyndham properties are located throughout the entire continent (none of which are in South Africa or Egypt).\nWyndham also boasts several branches within its network around the world. Below you can see which hotels are a part of the Wyndham family.\n|Brands||# of Properties|\n|Wyndham Hotels and Resorts||75|\nWyndham points can also be used to either book air travel directly through their portal, or transferred for miles to 14 different airline partners. Of the two, booking directly through Wyndham is the better value. Below we sampled a few itineraries from JFK where we obtained an average point value of 0.43 cents.\n|American Airlines Trip||Price||Points Required||Value (cents)|\n|JFK to SFO||$213||40,618||0.52|\n|JFK to MIA||$249||54,663||0.46|\n|JFK to LHR||$1,120||324,944||0.40|\nThe number of points and price may vary, but on average are 0.43 cents per/point. It may be difficult to find something higher than 0.55 cents which is the maximum we found (although we would encourage you to explore all possible options if you have a flight in mind). Starwood and Marriott offer airline ticket bookings at around the same points value as well. Compared to the potential of a free night stay however, this is not a good redemption. If you do opt for this, we implore you to check that you are getting at least a points value of over 0.43 cents, if value maximization is important to you.\nAs for miles transfer, there is a flat, five points for one mile rate, with a minimum of 6,000 points for 1,200 miles being needed to redeem. Generally, airline miles are valued between 1 cents and 2 cents, thus at an average value of 0.60 cents, you would be trading 3¢ worth of points (5 points x 0.006) for between 1 cents and 2 cents worth of miles - a poor deal. Recall how we said above that you should strive for your points to be worth greater than 0.60 cents per point. As you can see below, transferring for miles will devalue your points below that line.\n|Airlines||Ratio (min points: 6,000; 16,000; 30,000)||Devalued Wyndham Point (cents)|\n|Saudia Arabian Airlines||5:1||0.24|\nWe would only recommend using this redemption option if you absolutely must book a flight or travel somewhere. The savings you will make on a hotel room however should convince you that you should probably never make this redemption.\nReservations for hotels ordinarily more than $213/night (1.42 cents per point) were the maximum values we found for Wyndham Rewards points. There is usually at least one of these properties in every major city, and thus should be the property you choose if there are available rooms for the best value.\nReservations for hotels ordinarily more than $105/ night (0.64 cents per point ) are the next best deal to get with your points. Many times the high-end hotel in the city you are booking in will be filled and thus you may need to settle for a cheaper hotel; just try to make sure you are getting above 0.64 cents per point. You could also get a good redemption by settling for a lower tier hotel and utilizing the points + cash option; just remember to keep the cash below the $75 level.\nHotel Reservations that typically cost less than $105/night (0.64 cents per point) are one of the lowest value redemption you can make. If that is the only option available to you, by all means go for it. But if you are booking a low tier hotel, without the points + cash option, it's going to be a suboptimal use of points considering you can be booking a higher tier hotel for the same amount of points. The other bad redemptions are: airline ticket bookings (0.43 cents per point), Gift Cards (0.40 cents per point), and Miles transfer (0.30 per point).\nComments and Questions\n* See the online credit card application for details about terms and conditions. Reasonable efforts are made to maintain accurate information. However all credit card information is presented without warranty. When you click on the \"Apply Now\" button, you can review the credit card terms and conditions on the issuer's website.\nInfo about the following cards: American Express® Gold Card, Delta Reserve® Credit Card from American Express, Platinum Delta SkyMiles® Credit Card, Starwood Preferred Guest® Credit Card from American Express, Hilton Honors Surpass® Credit Card, American Express® Blue Sky, American Express® Green Card, The Amex EveryDay® Preferred Credit Card from American Express, The Plenti® Credit Card from Amex, Blue from American Express®, Platinum Card® from American Express Exclusively for Mercedes-Benz, Bluebird, Centurion® Card from American Express, Hilton Honors American Express Aspire Card, and Starwood Preferred Guest® American Express Luxury Card has been collected independently by ValuePenguin. The issuer did not provide the details, nor is it responsible for their accuracy.\nDisclaimer: This content is not provided or commissioned by the credit card issuer. Opinions expressed here are author's alone, not those of the credit card issuer, and have not been reviewed, approved or otherwise endorsed by the credit card issuer. This site may be compensated through the credit card issuer Affiliate Program.\nThese responses are not provided or commissioned by the bank advertiser. Responses have not been reviewed, approved or otherwise endorsed by the bank advertiser. It is not the bank advertiser's responsibility to ensure all posts and/or questions are answered.\nHow We Calculate Rewards: ValuePenguin calculates the value of rewards by estimating the dollar value of any points, miles or bonuses earned using the card less any associated annual fees. These estimates here are ValuePenguin's alone, not those of the card issuer, and have not been reviewed, approved or otherwise endorsed by the credit card issuer.","A retail credit card from your favorite store is enticing. If you enjoy shopping and the ability to finance your purchases, you might find yourself eager to submit the application. You don't need perfect credit to get approved for a retail credit card, which can make applying even more tempting. Some store cards will approve applicants with scores as low as 550, which is considered poor. Opening a line of credit at a retail store offers advantages, but opening several retail credit cards can negatively affect your credit score.\nWhile most stores don't post their minimum credit score requirements for their store card, CreditDonkey reports that Walmart has a 550 minimum for their store card.\nTypically, retail credit cards have more relaxed qualification requirements. If you are just building your credit, you might choose to start with a retail card. Retail cards with a Visa or MasterCard affiliation are issued by a bank instead of the store's financing department. For example, Capital One issues the Saks Fifth Avenue credit card and Citibank issues the Sears card. These stores tend to follow the issuing bank's underwriting criteria.\nCredit card issuers do not specify the average score necessary to obtain a retail credit card. There is no defined range for a fair credit score, but typically a FICO score between 580 and 669 are considered fair. For a Walmart credit card, a score of 660 or higher is recommended, but those with scores as low as 550 have reported success when applying for the card. Best Buy credit cards, which are issued by Citibank, require a credit score of at least 660.\nFactors Affecting Approval\nRetail credit card companies consider more than a credit score when reviewing applications. Your employment status, ability to pay, willingness to pay and credit score are all factors used to determine approval. A person with a low credit score because of lack of credit could be more likely to gain approval than an applicant who has a low score because of missed payments or an account in collection.\nIf a store chooses to extend you credit, your credit score and income are generally used to calculate your credit limit. Credit card companies may increase your limit after you display a dependable payment history. The economy also plays a role in your ability to qualify for a retail credit card. Creditors tend to tighten lending standard in a sluggish economy. When the economy is booming, standards are often more relaxed.\nDisadvantages Associated With Store Cards\nA retail credit card has some disadvantages for consumers with established credit. Retail credit cards often carry higher interest rates. If your credit score is high enough to qualify for a standard credit card, you might save money in interest charges. Simply applying for a retail credit card can hurt your score. Every application you submit appears on your credit report as an inquiry. A single inquiry lowers your score approximately five points.\nBecause retail cards might be easier to get, avoid stacking too many. According to MyFICO, the type of credit you have makes up 10 percent of your score. Ideally, a consumer should have a combination of both revolving and installment accounts. Because retail credit cards tend to have lower limits, even an inexpensive purchase can generate a high credit-utilization ratio. Your utilization ratio is calculated by comparing the balances on your credit cards with the credit limits. New credit cards are also scored lower than older accounts. When you open a retail account, your score may drop slightly simply because the account is new.\nAdvantages Associated With Store Cards\nRetail credit cards offer rewards and incentives to attract customers. These cards allow you to take advantage of special offers, sales and discounts reserved exclusively for cardholders. Many stores send coupons to cardholders and even offer rebates on store purchases. A retail credit card gives you the opportunity to make small purchases that you can pay off when it's convenient for you. If used properly, you can improve your credit score while saving some cash.\n- MyFICO: What’s in my FICO Scores\n- Experian: What Is a Good Credit Score?\n- Saks Fifth Avenue: SaksFirst Store Card\n- Sears: Save More at Sears and KMart\n- Credit Karma: Are Retail Cards the Easiest Credit Cards to Get Approved For?\n- Bankrate: Are Store Credit Cards a Good Deal?\n- Credit Donkey: Walmart Credit Card Review\n- Finder: Is the Best Buy Credit Card Worth Applying For?\n- Credit Cards for People Trying to Build Credit\n- What Is the Guarantor on a Credit Card?\n- Are Store Credit Cards Good or Bad?\n- How Does a Credit Card That the Balance Has to Be Paid Every Month Differ from Other Credit Cards?\n- Does Paying During a Grace Period Affect the Credit Score?\n- Does Closing One Card & Opening Another Hurt Your Credit?\n- How to Establish Credit After Having Judgments\n- How Many Points Can Secured Credit Cards Raise Your Credit Score?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:c8a7b23e-1b31-461c-994d-4a5a2c8d7f33>","<urn:uuid:700aa279-a8c3-435b-b75d-f8931f2b8a4a>"],"error":null}
{"question":"Is Roman chamomile scientifically proven effective for treating inflammation and skin conditions?","answer":"While Roman chamomile is traditionally used for treating inflammation and skin conditions, there is not enough reliable research in humans to definitively support these uses. The existing evidence for skin inflammation shows chamomile may provide little if any benefit, although animal studies support its anti-inflammatory properties. For skin conditions, while the German Commission E authorizes topical chamomile use, there is limited research proving its effectiveness for problems like eczema. More rigorous scientific studies are needed before drawing firm conclusions about its efficacy for inflammatory and skin-related uses.","context":["Chamomile is an alternative herbal treatment for anxiety and tension, various digestive disorders, muscle pain and spasm, and menstrual cramps. Learn about the usage, dosage, side-effects of Roman Chamomile.\nBotanical Name:Chamaemelum nobile\nCommon Names: Roman Chamomile\n- Plant Description\n- What's It Made Of?\n- Available Forms\n- How to Take It\n- Possible Interactions\n- Supporting Research\nThere are two plants known as chamomile: the more popular German chamomile (Matricaria recutita) and Roman, or English, chamomile (Chamaemelum nobile). Both belong to the Asteraceae family, which also includes ragweed, echinacea, and feverfew. Both have been used traditionally to calm frayed nerves, to treat various digestive disorders, to relieve muscle spasms and menstrual cramps, and to treat a range of skin conditions (including minor first degree burns) and mild infections. Chamomile can also be found in a variety of face creams, drinks, hair dyes, shampoos, and perfumes.\nMost research on chamomile has been done with the closely related plant, German chamomile, which has similar, but not identical, active ingredients. Roman chamomile has not been used in studies of people as much as German chamomile, so claims about its use for specific health conditions are based on clinical experience and will have to be verified through future research. Nevertheless, Roman chamomile is an ingredient in many teas, ointments, and other types of medicinal preparations.\nTraditionally, Roman chamomile has been used to treat nausea, vomiting, heartburn, and excess intestinal gas that can happen when feeling nervous. It is widely valued for its tension-relieving properties. As legend has it, Peter Rabbit's mother used Roman chamomile tea to calm him down after his adventures in Mr. MacGregor's garden. This herb may also reduce inflammation associated with cuts or hemorrhoids, and may ease the discomfort associated with conditions such as eczema and gingivitis (swollen gums). The traditional uses of Roman chamomile, again while not studied scientifically are quite similar to the uses for German chamomile.\nRoman chamomile originates in northwestern Europe and Northern Ireland, where it creeps close to the ground and can reach up to one foot in height. Gray-green leaves grow from the stems, and the flowers have yellow centers surrounded by white petals, like miniature daisies. It differs from German chamomile in that its leaves are thicker and it grows closer to the ground. The flowers smell like apples.\nChamomile teas, ointments, and extracts all start with the white and yellow flower head. The flower heads may be dried and used in teas or capsules or crushed and steamed to produce a blue oil, which has medicinal benefits. The oil contains ingredients that reduce swelling and may limit the growth of bacteria, viruses, and fungi.\nRoman chamomile is available as dried flowers in bulk, tea, tinctures, and in creams and ointments.\nThere are no known scientific reports regarding the appropriate pediatric dose of Roman chamomile. For this reason, children should not take this herb.\nRoman chamomile can be taken a number of ways. A cup of hot chamomile tea may help soothe an upset stomach or help those who suffer from insomnia. The oral dosages listed below should help relieve stomach discomfort; chamomile has also been used for reducing menstrual pain and the swelling of gums in the case of gingivitis. The ointment and bath recommendations are for skin conditions.\n- Tea: Pour one cup of boiling water over 1 heaping tablespoon of dried herb, steep 10 to 15 minutes.\n- Liquid extract (1:1, 70% alcohol) 20 to 120 drops, three times per day\n- Bath: Add two teabags or a few drops of Roman chamomile essential oil to a full tub of bathwater to soothe hemorrhoids or skin problems\n- Cream/Ointment: Apply cream or ointment containing 3% to 10% chamomile content\nThe use of herbs is a time-honored approach to strengthening the body and treating disease. Herbs, however, contain active substances that can trigger side effects and interact with other herbs, supplements, or medications. For these reasons, herbs should be taken with care, under the supervision of a practitioner knowledgeable in the field of botanical medicine.\nChamomile is considered generally safe by the FDA. Roman chamomile contains an ingredient, anthemic acid, which can induce vomiting if taken in high doses. Highly concentrated tea may therefore cause vomiting.\nThose who are allergic to ragweed or other plants in the Asteraceae family (including echinacea, feverfew, and chrysanthemums) should avoid chamomile. Allergic reactions are somewhat common, actually, and may include stomach cramps, tongue thickness, swollen lips and eyes (called angioedema), itching, hives, throat tightness, and even shortness of breath. The latter two symptoms are medical emergencies and medical care should be sought urgently.\nIf you are currently being treated with any of the following medications, you should not use Roman chamomile without first talking to your healthcare provider.\nBecause of its calming effects, chamomile should not be taken in conjunction with sedative medications (particularly those that belong to a class called benzodiazepines such as alprazolam and lorazepam) or alcohol.\nPatients taking blood-thinning medications such as warfarin should use Roman chamomile only under the careful supervision of a healthcare practitioner. Although not proven scientifically, this herb may, in theory, enhance the effects of the medication.\nback to: Herbal Treatments Homepage\nBlumenthal M, ed. The Complete German Commission E Monographs. Boston, Mass: Integrative Medicine Communications; 1998:320-321.\nBriggs CJ, Briggs GL. Herbal products in depression therapy. CPJ/RPC. November 1998;40-44.\nCauffield JS, Forbes HJM. Dietary supplements used in the treatment of depression, anxiety, and sleep disorders. Lippincott's Primary Care Practice. 1999;3(3):290-304.\nErnst E, ed. The Desktop Guide to Complementary and Alternative Medicine: An Evidence-Based Approach. New York, NY: Mosby;2001:110-112.\nFoster S, Tyler VE. Tyler's Honest Herbal. New York, NY: The Haworth Herbal Press; 1999:105-108, 399.\nHeck AM, DeWitt BA, Lukes AL. Potential interactions between alternative therapies and warfarin. Am J Health Syst Pharm. 2000;57(13):1221-1227.\nLeung A, Foster S. Encyclopedia of Common Natural Ingredients Used in Food, Drugs, and Cosmetics. 2nd ed. New York, NY: Wiley & Sons; 1996.\nMcGuffin M, Hobbs C, Upton R, Goldberg A. American Herbal Products Associations's Botanical Safety Handbook. Boca Raton, Fla: CRC Press; 1996:27.\nMiller L. Herbal medicinals: selected clinical considerations focusing on known or potential drug-herb interactions. Arch Intern Med. 1998;158(20):2200-2211.\nNewall CA, Anderson LA, Phillipson JD. Herbal Medicines: A Guide for Health Care Professionals. London, England: The Pharmaceutical Press; 1996:72 73.\nO'Hara M, Kiefer D, Farrell K, Kemper K. A review of 12 commonly used medicinal herbs. Arch Fam Med. 1998:7(6):523-536.\nRobbers JE, Tyler VE. Tyler's Herbs of Choice: The Therapeutic Use of Phytomedicinals. New York, NY: The Haworth Herbal Press;1999:69-71.\nRotblatt M, Ziment I. Evidence-Based Herbal Medicine. Philadelphia, Penn:Hanley & Belfus, Inc. 2002:119-123.\nback to: Herbal Treatments Homepage\nStaff, H. (2008, December 11). Roman Chamomile, HealthyPlace. Retrieved on 2021, December 4 from https://www.healthyplace.com/alternative-mental-health/herbal-treatments/roman-chamomile","Anthemis arvensis, Anthemis cotula, Anthemis nobile, Anthemis nobilis, Anthemis xylopoda, apigenin, Asteraceae/Compositae (family), baboonig, babuna, babunah, babunah camomile, babunj, bunga kamil, camamila, camamilla, camomile, camomile sauvage, camomilla, Camomille Allemande, Campomilla, chamaemeloside, Chamaemelum nobile L., chamomile flowers, Chamomilla, Chamomilla recutita, chamomillae ramane flos, chamomille commune, classic chamomile, common chamomile, double chamomile, Echte Kamille (Dutch), English chamomile, feldkamille (German), fleur de chamomile (French), fleurs de petite camomille (French), Flores Anthemidis, flos chamomillae, garden chamomile, German chamomile, Grosse Kamille, Grote Kamille, ground apple, Hungarian chamomile, Kamille, Kamillen, kamitsure, kamiture, Kleine, kleme kamille, lawn chamomile, low chamomile, manzanilla, manzanilla chiquita, manzilla comun, manzanilla dulce, matricaire, Matricaria chamomilla, Matricaria maritime (L.), Matricaria recutita, Matricaria suaveolens, matricariae flos, matricariae flowers, may-then, Nervine, pin heads, rauschert, Romaine, romaine manzanilla, Roman chamomile, Romische Kamille, single chamomile, STW 5 (containing Iberis, peppermint, chamomile), sweet chamomile, sweet false chamomile, sweet feverfew, true chamomile, whig-plant, wild chamomile.\nChamomile has been used medicinally for thousands of years and is widely used in Europe. It is a popular treatment for numerous ailments, including sleep disorders, anxiety, digestion/intestinal conditions, skin infections/inflammation (including eczema), wound healing, infantile colic, teething pains, and diaper rash. In the United States, chamomile is best known as an ingredient in herbal tea preparations advertised for mild sedating effects.\nGerman chamomile (Matricaria recutita) and Roman chamomile (Chamaemelum nobile) are the two major types of chamomile used for health conditions. They are believed to have similar effects on the body, although German chamomile may be slightly stronger. Most research has used German chamomile, which is more commonly used everywhere except for England, where Roman chamomile is more common.\nAlthough chamomile is widely used, there is not enough reliable research in humans to support its use for any condition. Despite its reputation as a gentle medicinal plant, there are many reports of allergic reactions in people after eating or coming into contact with chamomile preparations, including life-threatening anaphylaxis.\nThese uses have been tested in humans or animals. Safety and effectiveness have not always been proven. Some of these conditions are potentially serious, and should be evaluated by a qualified healthcare provider.\nChamomile is not well-known for its cardiac effects, and there is little research in this area. Large, well-designed randomized controlled trials are needed before a firm conclusion can be made.\nIn early study, inhaling steam with chamomile extract has been reported to help common cold symptoms. Further research is needed to confirm these results.\nDiarrhea in children\nPreliminary study reports that chamomile with apple pectin may reduce the length of time that children experience diarrhea. Further research is needed before a strong recommendation can be made.\nThe German Commission E authorizes the use of topical chamomile for diseases of the skin. However, little research has been done on topical chamomile for eczema and further research is needed.\nChamomile is used traditionally for numerous gastrointestinal conditions, including digestion disorders, \"spasm\" or colic, upset stomach, flatulence (gas), ulcers, and gastrointestinal irritation. However, currently there is a lack of reliable human research available in any of these areas. Additional study is needed.\nHemorrhagic cystitis (bladder irritation with bleeding)\nPreliminary study reports that the combination of chamomile baths plus chamomile bladder washes and antibiotics is superior to antibiotics alone for hemorrhagic cystitis. Additional research is necessary before a conclusion can be reached.\nPreliminary study reports that chamomile ointment may improve hemorrhoids. Better evidence is needed before a strong recommendation can be made.\nChamomile is reputed to have anti-spasmodic activity, but there is little research to substantiate this claim. Additional research evaluating chamomile alone is needed.\nMucositis from cancer treatment (mouth ulcers/irritation)\nPoor-quality studies have used chamomile mouthwash for the prevention or treatment of mouth mucositis caused by radiation therapy or cancer chemotherapy. Results are conflicting, and it remains unclear if chamomile is helpful in this situation.\nQuality of life in cancer patients\nA small amount of research suggests that massage using chamomile essential oil may improve anxiety and quality of life in cancer patients. However, this evidence is not high quality. Additional study is needed before a firm conclusion can be reached.\nTopical chamomile preparations have traditionally been used to soothe skin inflammation. The existing human evidence shows that chamomile may be of little, if any, benefit while animal studies support its anti-inflammatory action. Additional human research is needed in this area.\nSleep aid / sedation\nTraditionally, chamomile preparations, such as tea and essential oil aromatherapy, have been used for insomnia and sedation (calming effects). Better research is needed before a recommendation can be made.\nVaginitis (inflammation of the vagina)\nVaginitis may involve itching, discharge, or pain with urination. Chamomile douche may improve symptoms of vaginitis with few side effects. Because infection (including sexually transmitted diseases), poor hygiene, or nutritional deficiencies can cause vaginitis, medical attention should be sought by people with this condition. Better research is needed before a conclusion can be drawn regarding the role of chamomile in the management of vaginitis.\nThere is promising preliminary evidence supporting the topical use of chamomile for wound healing. However, the available literature is not adequate to support a recommendation either for or against this use.\nPost-operative sore throat/hoarseness due to intubation\nChamomile spray has not been found to prevent post-operative sore throat and hoarseness any more than normal saline.\n*Key to grades:\nA: Strong scientific evidence for this use;\nB: Good scientific evidence for this use;\nC: Unclear scientific evidence for this use;\nD: Fair scientific evidence against this use (it may not work);\nF: Strong scientific evidence against this use (it likely does not work).\nThe below uses are based on tradition, scientific theories, or limited research. They often have not been thoroughly tested in humans, and safety and effectiveness have not always been proven. Some of these conditions are potentially serious, and should be evaluated by a qualified healthcare provider. There may be other proposed uses that are not listed below. Abdominal bloating, abortifacient, abrasions, abscesses, acne, anorexia, antibacterial, anticoagulant, antifungal, antioxidant, antipruritic, antispasmodic, antiseptic, anxiety, aromatic, arthritis, asthma, back pain, bedsores, bladder disorders, blood purification, bruises, burns, cancer, canker sores, carpal tunnel syndrome, catarrh, chicken pox, constipation, contact dermatitis, cough, Crohn's disease, croup, delirium tremens (DTs), diaper rash, diaphoretic, diuretic (increasing urination), diverticulitis, dry skin, dysmenorrhea (painful menstruation), ear infections, eye disorders (blocked tear ducts), eye infections, fatty liver, fever, fistula healing, frostbite, gallstones, gingivitis, gout, hay fever, headaches, heartburn, hives, hypoglycemia (low blood sugar), hysteria, impetigo, inflammatory conditions, insect bites, insomnia, intestinal cramps, irregular menstrual cycles, irritable bowel syndrome, kidney disorders, leg ulcers, liver disorders, low back pain, malaria, mastitis (breast inflammation), menopause, menstrual cramps, menstrual disorders, morphine withdrawal, motion sickness, muscle strength, nasal inflammation, nausea, nervous stomach, neuralgia (nerve pain), nightmares, oral hygiene (mouthwash), osteoporosis, parasites/worms, peptic ulcers, perineal trauma, poison ivy, post-natal depression, psoriasis, rash (heat), respiratory inflammatory, restlessness, rheumatism, Roehmheld's syndrome, sciatica, seizure disorder, sinusitis, stomach cramps, sunburn, sunstroke, teething pain (mouth rinse), tension, tics, toothache, travel sickness, tuberculosis, ulcerative colitis, ulcers, uterine disorders, uterine stimulant, uterine tonic, vaginal infections, viral infection (flu-like symptoms or polio), vomiting, vomiting/nausea during pregnancy.\nThe below doses are based on scientific research, publications, traditional use, or expert opinion. Many herbs and supplements have not been thoroughly tested, and safety and effectiveness may not be proven. Brands may be made differently, with variable ingredients, even within the same brand. The below doses may not apply to all products. You should read product labels, and discuss doses with a qualified healthcare provider before starting therapy.\nAdults (18 years and older)\nCapsules/tablets containing 400 to 1,600 milligrams in divided doses have been taken by mouth daily. As a liquid extract (1:1 in 45% alcohol), 1 to 4 milliliters three times daily has been taken by mouth. As a tincture (1:5 in alcohol), 15 milliliters three to four times per day has been used. As a mouth rinse, a 1% fluid extract or 5% tincture has been used.\nChamomile is frequently consumed as tea, and 1 to 4 cups of chamomile tea taken daily (from tea bags) is a common dose.\nThere are no standard doses for chamomile used on the skin. Some natural medicine publications have recommended paste, plaster, or ointment containing 3% to 10% chamomile flower heads. Chamomile has been also used as a bath additive and as a douche.\nChildren (younger that 18 years)\nThere is not enough reliable scientific data available to recommend the safe use of chamomile products in children.\nThe U.S. Food and Drug Administration does not strictly regulate herbs and supplements. There is no guarantee of strength, purity or safety of products, and effects may vary. You should always read product labels. If you have a medical condition, or are taking other drugs, herbs, or supplements, you should speak with a qualified healthcare provider before starting a new therapy. Consult a healthcare provider immediately if you experience side effects.\nThere are multiple reports of serious allergic reactions to chamomile taken by mouth or as an enema, including anaphylaxis, throat swelling, and shortness of breath. Skin allergic reactions have been frequently reported, including dermatitis and eczema. Chamomile eyewash can cause allergic conjunctivitis (pinkeye).\nPeople with allergies to other plants in the Asteraceae (Compositae) family should avoid chamomile. Examples include: aster, chrysanthemum, mugwort, ragweed, and ragwort. Cross-reactions may occur with celery, chrysanthemum, feverfew, tansy, and birch pollen. Individuals with allergies to these plants should avoid chamomile. Contact skin allergy has been reported.\nImpurities (adulterants) in chamomile products are common and may cause adverse effects. Atopic dermatitis (skin rash) has been reported.\nChamomile in various forms may cause drowsiness or sedation. Use caution when driving or operating heavy machinery. In large doses, chamomile can cause vomiting. Due to its coumarin content, chamomile may theoretically increase the risk of bleeding. Caution is advised in patients with bleeding disorders or taking drugs that may increase the risk of bleeding. Dosing adjustments may be necessary. Increases in blood pressure are possible.\nPregnancy and Breastfeeding\nIn theory, chamomile may act as a uterine stimulant or lead to abortion. It therefore should be avoided during pregnancy. There is not enough scientific data to recommend the safe use of chamomile while breastfeeding.\nMost herbs and supplements have not been thoroughly tested for interactions with other herbs, supplements, drugs, or foods. The interactions listed below are based on reports in scientific publications, laboratory experiments, or traditional use. You should always read product labels. If you have a medical condition, or are taking other drugs, herbs, or supplements, you should speak with a qualified healthcare provider before starting a new therapy.\nInteractions with Drugs\nChamomile interactions are not well studied scientifically.\nChamomile may increase the amount of drowsiness caused by some drugs. Examples include benzodiazepines such as lorazepam (Ativan©) or diazepam (Valium©), barbiturates such as phenobarbital, narcotics such as codeine, some antidepressants, and alcohol. Caution is advised while driving or operating machinery.\nIn theory, chamomile may increase the risk of bleeding when used with anticoagulants or antiplatelet drugs. Some examples include aspirin, anticoagulants (\"blood thinners\") such as warfarin (Coumadin©) or heparin, anti-platelet drugs such as clopidogrel (Plavix©), and non-steroidal anti-inflammatory drugs (NSAIDs) such as ibuprofen (Motrin©, Advil©) or naproxen (Naprosyn©, Aleve©).\nChamomile may interfere with the way the body processes certain drugs using the liver's \"cytochrome P450\" enzyme system. As a result, the levels of these drugs may be increased in the blood and may cause increased effects or potentially serious adverse reactions. Patients using any medications should check the package insert and speak with a healthcare professional including a pharmacist about possible interactions.\nBe aware that many tinctures contain high levels of alcohol and may cause vomiting when taken with metronidazole (Flagyl©) or disulfiram (Antabuse©).\nAn extract containing Matricaria chamomile, Sideritis euboea, Sideritis clandestine, and Pimpinella anisum was associated with selective estrogen receptor modulator (SERM) properties against osteoporosis. Theoretically, chamomile may interact with SERM drugs like raloxifene (prescription drug used for osteoporosis) or tamoxifen (a prescription drug used for cancer).\nConstituents in chamomile may alter blood sugar or blood pressure. Patients taking medications that affect blood sugar or blood pressure should be cautious.\nChamomile may have anti-inflammatory effects. Theoretically, use of chamomile with other anti-inflammatory drugs, such as NSAIDs or ibuprofen, may have additive effects.\nChamomile may interact with medications that act as cardiac depressants, central nervous system depressants, calcium channel blockers, cardiac glycosides, and respiratory depressants.\nChamomile may also interact with antibiotics, antifungals, antihistamines, diuretics, as well as drugs for high cholesterol, ulcers, diarrhea, or gastrointestinal disorders.\nInteractions with Herbs and Dietary Supplements\nChamomile may increase the amount of drowsiness caused by some herbs or supplements. Caution is advised while driving or operating machinery.\nIn theory, chamomile may increase the risk of bleeding when taken with other products that are believed to increase the risk of bleeding. Multiple cases of bleeding have been reported with the use of Ginkgo biloba, and fewer cases with garlic and saw palmetto. Numerous other agents may theoretically increase the risk of bleeding, although this has not been proven in most cases.\nChamomile may interfere with the way the body processes certain drugs using the liver's \"cytochrome P450\" enzyme system. As a result, the levels of other herbs or supplements may become too high in the blood. It may also alter the effects that other herbs or supplements possibly have on the P450 system. Patients using any medications should check the package insert and speak with a healthcare professional including a pharmacist about possible interactions.\nChamomile may have anti-estrogenic effects and interact with herbs and supplements like red clover or soy.\nBased on preliminary study, constituents in chamomile may alter blood sugar or blood pressure. Patients taking herbs or supplements that affect blood sugar or blood pressure should be cautious.\nChamomile may have anti-inflammatory effects. Theoretically, the use of chamomile with other anti-inflammatory herbs and supplements may have additive effects.\nChamomile may interact with herbs and supplements that act as cardiac depressants, cardiac glycosides, respiratory depressants, or spasmolytics.\nChamomile may also interact with antibacterial, antifungal, antihistamine, or diuretic herbs and supplements, as well as herbs and supplements used for high cholesterol, ulcers, diarrhea, or gastrointestinal disorders.\nThis information is based on a systematic review of scientific literature edited and peer-reviewed by contributors to the Natural Standard Research Collaboration (www.naturalstandard.com).\nNatural Standard Bottom Line Monograph, Copyright © 2011 (www.naturalstandard.com). Commercial distribution prohibited. This monograph is intended for informational purposes only, and should not be interpreted as specific medical advice. You should consult with a qualified healthcare provider before making decisions about therapies and/or health conditions.\nWhile some complementary and alternative techniques have been studied scientifically, high-quality data regarding safety, effectiveness, and mechanism of action are limited or controversial for most therapies. Whenever possible, it is recommended that practitioners be licensed by a recognized professional organization that adheres to clearly published standards. In addition, before starting a new technique or engaging a practitioner, it is recommended that patients speak with their primary healthcare provider(s). Potential benefits, risks (including financial costs), and alternatives should be carefully considered. The below monograph is designed to provide historical background and an overview of clinically-oriented research, and neither advocates for or against the use of a particular therapy.\nThe information in this monograph is intended for informational purposes only, and is meant to help users better understand health concerns. Information is based on review of scientific research data, historical practice patterns, and clinical experience. This information should not be interpreted as specific medical advice. Users should consult with a qualified healthcare provider for specific questions regarding therapies, diagnosis and/or health conditions, prior to making therapeutic decisions.\n- Aertgeerts P, Albring M, Klaschka F, et al. [Comparative testing of Kamillosan cream and steroidal (0.25% hydrocortisone, 0.75% fluocortin butyl ester) and non-steroidal (5% bufexamac) dermatologic agents in maintenance therapy of eczematous diseases]. Z Hautkr 2-1-1985;60(3):270-277.\n- Benetti C, Manganelli F. [Clinical experiences in the pharmacological treatment of vaginitis with a camomile-extract vaginal douche]. Minerva Ginecol 1985;37(12):799-801.\n- de la Torre MF, Sanchez MI, Garcia Robaina JC, et al. Clinical cross-reactivity between Artemisia vulgaris and Matricaria chamomilla (chamomile). J Investig Allergol Clin Immunol 2001;11(2):118-122.\n- Glowania HJ, Raulin C, Swoboda M. [Effect of chamomile on wound healing—a clinical double-blind study]. Z Hautkr 9-1-1987;62(17):1262, 1267-1271.\n- Hormann H, Korting H. Evidence for the efficacy and safety of topical herbal drugs in dermatology: part 1: anti-inflammatory agents. Phytomedicine 1994;1(2):161-171.\n- Ikram M. Medicinal plants as hypocholesterolemic agents. J Pak Med Assoc 1980;30(12):278-281.\n- Konig GM, Wright AD, Keller WJ, et al. Hypoglycaemic activity of an HMG-containing flavonoid glucoside, chamaemeloside, from Chamaemelum nobile. Planta Med 1998;64(7):612-614.\n- Kyokong O, Charuluxananan S, Muangmingsuk V, et al. Efficacy of chamomile-extract spray for prevention of post-operative sore throat. J Med Assoc Thai 2002;85 Suppl 1:S180-S185.\n- Maiche A, Grohn P, Maki-Hokkonen H. Effect of chamomile cream and almond ointment on acute radiation skin reaction. Acta Oncol 1991;30:395-397.\n- Maiche A, Maki-Kokkonen H, Grohn P. [Comparative trial of chamomile cream in radiotherapy]. Suomen Laakarilehti 1991;46(24):2206-2208.\n- Patzelt-Wenczler R, Ponce-P©schl E. Proof of efficacy of Kamillosan cream in atopic eczema. Eur J Med Res 2000;5:171-175.\n- Rycroft RJ. Recurrent facial dermatitis from chamomile tea. Contact Dermatitis 2003;48(4):229.\n- Saller R, Beschomer M, Hellenbrecht D, et al. Dose dependency of symptomatic relief of complaints by chamomile steam inhalation in patients with common cold. Eur J Pharmacol 1990;183:728-729.\n- Weizman Z, Alkrinawi S, Goldfarb D, et al. Efficacy of herbal tea preparation in infantile colic. J Pediatr 1993;122(4):650-652.\n- Wilkinson S, Aldridge J, Salmon I, et al. An evaluation of aromatherapy massage in palliative care. Palliat Med 1999;13(5):409-417.\nCopyright © 2011 Natural Standard (www.naturalstandard.com)\nCopyright © 2011 Natural Standard (www.naturalstandard.com)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:c267a9fd-1e46-4b3b-9e77-8b4e97f28c58>","<urn:uuid:f9fc6c94-b923-49b6-b31d-6780d8cd7ea0>"],"error":null}
{"question":"What are the key differences between rehoming pets vs farm animals in terms of adoption requirements and screening process?","answer":"For pets, the key requirements include spaying/neutering the animal, taking photos, creating a biography of traits, and conducting interviews with potential adopters. For farm animals, the process is more extensive, requiring a three-stage adoption process including a prospective adoption form, telephone interview, and property inspection. Farm animal adopters must also commit to never breeding, selling, or consuming the animals, and must provide species-specific care including shelter from harsh weather and companionship of their own kind. Additionally, farm animal adoptions are restricted to permanent homes only - no schools, childcare facilities, or temporary placements are allowed.","context":["Finding a new home for your pet often takes time, patience and effort. However, it can be less stressful on the pet, and you will be able to choose the new home. PAWS suggests these steps to make the process easier:\n- Get your pet spayed or neutered. It’s much easier to find homes for altered animals. If cost is an issue, PAWS offers low-cost surgeries for qualified low-income households. You can also check our list of low-cost spay/neuter clinics or call 425.787.2500 x450 for information.\n- Make a list of your pet's traits. Brainstorm your pet’s appearance, personality and behavior. Do include the good and the bad. This will help you develop a short biography for your pet to give to potential adopters. Be honest so the person giving a home to your animal will know what to expect.\n- Take photos of your animal. Use the photos to make fliers to post at pet supply stores, veterinary clinics, places of worship, work bulletin boards, online bulletin boards, e-mails and in newsletters. Be sure to alert family and friends so they can tell the people in their community about your pet needing a new home.\n- If the animal is a purebred dog or cat you may be able to seek help from organizations like Seattle Purebred Dog Rescue or Cat Purebred Rescue.\n- When using social media or the internet, do not list the animal for free. Unfortunately there are people who acquire animals to resell them to research laboratories or place them in illegal dog fighting rings or other abusive situations. By asking the adopter for an adoption fee (even if it is a small donation to your local shelter), you help to ensure that the person who is adopting your pet has his best interest at heart and can financially provide for your pet.\n- Interview potential adopters. You want to make sure that the people adopting your pet are a good fit. Download and print this list of suggested interview questions (PDF 101KB).\n- Talk to your veterinarian. Some veterinary hospitals can help place owned animals or may have alternate solutions to medical or behavior issues.\n- Be patient and stay realistic. Older animals, sick animals, and those who have medical conditions or behavioral problems can be hard to place. Keep in mind that the very problem that is causing you to give up your pet may be the same reason someone else is reluctant to adopt her.\n- Military? If you are in the military and going on deployment, consider contacting Dogs on Deployment. This non-profit provides an online network for services members to search for volunteers who are willing to board their pets during their owner's service commitments.\nWhen a new home is not the solution\nSad as it is, there are times when medical conditions or behavioral problems cannot be resolved and you may want to discuss humane euthanasia with your veterinarian.\nWhile this choice is not always necessary, it is much better to provide the animal with a humane death than have her end up on the street because the new guardian was not told about the behavior or medical problems.","With numerous organisations doing great work to rehome cats and dogs and our wildlife carers providing a great network of coverage across the country, what becomes of farmed animals? These creatures are forgotten from our animal protection legislation and often forgotten from people’s thoughts, however they are never forgotten by Edgar’s Mission. In fact, it was the lot of one pig that changed the life of Edgar’s Mission Founder and Director, Pam Ahern, in 2003.\nStruck by the fact that farmed animals had nowhere to live lives where they were valued for who they were, not for what they could provide, Edgar’s Mission trotted into gear. Over the years, we here at Edgar’s Mission have come to realise just how much an organisation like ours is needed, not only to provide a voice for farmed animals but moreover to provide sanctuary for those animals who have become lost or abandoned with no identifiable owner. The latter of these reasons has been a driving force for securing our new forever home. But we have also come to acknowledge that whilst the animals think us super heroes, we are not, and sadly we cannot provide every single farmed animal in need with a forever place to call home sweet home, as well as continue to maintain the first class level of care to the animals already in our care, be good stewards to the land, be advocates for animals and last, but of equal importance, to give due diligence to our own physical and emotional wellbeing.\nThe task of rescuing animals in need, whilst often difficult, heart breaking and expensive, is something we will never shy away from. Rendering care, veterinary attention, rehabilitation along with all the kindness, love and tender mercy we can muster will always be our charter. But we need to ask your assistance to help share the love. So we now ask, “Will you be there?”\nWill you be there to provide a rescued animal with a forever home? A home where they will have the company of their own kind, with shelter and warmth through the coldest of days and shade and cool water under the harsh Australian sun. A home where these gentle souls will never want for anything, where they will never be bred with or from, one where they will never be on-sold, given away, or (perish the thought) killed for human or animal consumption. A home where they will never again be forgotten, where they will be valued for the unique and beautiful creature they are, not as a something. Will you be there to provide a rescued animal with a life truly worth living?\nIf you think you fit this lofty bill, we would love to hear from you as we seek to expand our base of adoptive ‘parents’. Through social media and publicity, the work of Edgar’s Mission reaches farther afield each day but so too do the number of calls from members of the public, animal rescue organisations, council pounds and even the police, to help farmed animals in need. We are deeply touched that so many have great faith in our work, and we take great pride that we are indeed able to provide a beacon of hope for farmed animals. Indeed this is something which we wish to continue providing.\nLike to find out more? Please check out prospective adoption form below for more information or email Pam.\nPlease note Edgar’s Mission is located in Lancefield, Victoria, Australia.\nA quick FAQ on our rehoming scheme\nWhich animals do you adopt out?\nThis is a really really difficult question. Animals who have resided at the sanctuary for several years, who have made bonds with their fellow kind or who we know will have on-going special needs will remain with us. We prefer to adopt animals in pairs or family groups to make their transition to their new forever home as stress free as possible.\nWhy so many questions in your prospective adoption form?\nPlease understand our first duty is always to the animal/s in our care, animal/s whose terrified ear we have whispered into that we would never let them be in harm’s way ever again. And whilst you may know you are good folk, we do not yet. Over the years we have received many an odd call from members of the public wishing to ‘obtain’ an animal from Edgar’s Mission for a less than satisfactory end. Whilst some may feel our questions seem a little personal, in all honesty the information is nothing a dear friend of yours would not know and after all, that is what we would like to be should you adopt an animal from Edgar’s Mission. Rest assured also that no information supplied is ever passed on to any third party and is always held strictly confidential.\nWe will also keep in touch with you and your new friend/s should the adoption proceed and we will always be on hand to offer advice should the need arise.\nHave you considered our three R’s of Rehoming?\nResearch – Make sure you do your homework to see what is involved in bringing a farmed animal into your home and your heart. Do you have the means and the ability to cater to their specific needs? Do you understand the species specific requirements for the animal you are looking to adopt?\nResponsibility – welcoming an animal into your world is a big responsibility and requires commitment for the entire natural lifespan of the animal.\nRemember – While you have may many interests in your life, you are the life source for your new friend and they need you to be there strong for them.\nSo how long do farmed animals live (approximately)?\nChicken 7-10 (note rescued battery hens and broiler birds have greatly reduced life spans because of the terrible lives they have endured and the un-natural breeding they have undergone)\nDucks: 10 years\nRabbits: 8-12 years\nSheep: 8-15 years (will greatly depend on the start in life they have had)\nGoats: 10 -15 years\nAlpacas: 15-20 years\nPigs: 8- 12 years (factory farmed or intensively bred pigs have shortened lives due to the terrible conditions they have endured and un-natural breeding they have undergone)\nCows: 15 -20 years\nHorses: 20-25 years\nBut I want an alpaca?\nAlpacas, due to their limited numbers in Australia and relatively small niche market tend to be very well cared for and as such, thankfully so, they are rarely in need of our assistance. And our current alpacas are much needed at the sanctuary.\nWhich animals are most in need of my help?\nThe animals most in need of forever homes are roosters, pigs, goats and sheep. Sorry we currently do not have any hens available for adoption. Due to our current numbers of roosters we need to keep a stable number of hens.\nWill the animal be healthy?\nAll animals are thoroughly health checked, treated for parasites and in the best of health they can be prior to placement.\nHow does the process work?\nOur adoption process consists of three stages. Stage one is the submission of your Prospective Adoption Form. Stage two involves a telephone interview with the final stage being a property inspection/verification.\nOnce an animal has been adopted to their new forever home, it won’t be the last you hear of us We will check in with you throughout the life of the animal to ensure all is going well and assist you with any questions you may have. And in the unlikely event your circumstance change and you can no longer provide the home the animal/s need, they come back to Edgar’s Mission. Animals are NEVER to be on-sold, given away or relocated to a property other than the one occupied by the adopter (unless express agreement have been received from Edgar’s Mission). Many thanks in anticipation of your understanding.\nCan anyone adopt an animal from Edgar’s Mission?\nEdgar’s Mission will only adopt rescued animals to people who can provide a forever home, not to those offering short term or foster care or those intending to rehome the animal themselves. If you are after a cheap lawnmover please try one of the many hardware stores or ebay. Please note we are not able to adopt animals to schools, child care facilities, aged care homes or community groups.\nOh and one last thing, if you are prepared to offer your heart and home to a rescued farmed animal please standby for one of the most rewarding times of your life. So if you are still with us and wish to be there for a rescued farm animal, start clicking."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:e5cc2637-5be1-4f48-ad01-d774ce729fe4>","<urn:uuid:1081e236-d15d-49ef-baa9-52935f459d81>"],"error":null}
{"question":"What role does light interaction play in the formation of both snowflakes and rainbows, and how does this affect their visual uniqueness?","answer":"Light interacts very differently in the formation of snowflakes and rainbows, leading to distinct visual characteristics. In rainbows, sunlight splits into VIBGYOR colors when passing through raindrops, with each drop acting like an independent prism that reflects and refracts light at specific angles to create the characteristic bow shape. In contrast, snowflakes' visual uniqueness comes from their physical structure, which begins with water molecules freezing in a hexagonal pattern due to hydrogen bonding. As they grow, snowflakes encounter different temperatures and humidity levels, causing unique crystalline patterns to form. No two snowflakes are alike because each takes a slightly different path through varying conditions. While rainbows show consistent colors and patterns when conditions are right, snowflakes exhibit infinite structural variations despite always having six sides.","context":["How full of the creative genius is the air in which these are generated! I should hardly admire more if real stars fell and lodged on my coat. Nature is full of genius, full of divinity: so that not a snowflake escapes its fashioning hand.\n-Henry David Thoreau, Journal, 1856\nThe wonder of how a snowflake forms is one that incites curiosity from both children and adults alike. All of us, at one time or another, have watched a snowflake land and taken an up-close view of the intricate latticework that embodies the snowflake. The form of each snowflake is a remarkable piece of artwork that nature provides us each winter. Even more remarkable, is the fact that nature never repeats the same design twice, as each snowflake is unique. The intricacy of each snowflake can be understood by digging into how a snowflake first forms, and then looking into what factors along the way contribute to the final result.\nFirst we must start by journeying up into the clouds where it all begins. Clouds are typically composed of a mix of ice crystals and supercooled water droplets (water that is colder then 32F, but doesn't freeze). Amazing as it sounds, water droplets tend to remain in the liquid form all the way down to temperatures of -38F. This is because in order for water to freeze at a temperature greater than -38F, it needs an ice nucleus. Without an ice nucleus, the water droplets will only be able to freeze if they get colder than -38F. Therefore, most of the time, ice nuclei are needed. Some examples of particulates that serve as ice nuclei are desert dust, volcanic ash, pollen, and even a certain strand of leaf bacteria.\nWhen the supercooled droplets freeze, they take on an initial form that is driven by the molecular structure of water. Water molecules inherently freeze in the form of a hexagonal structure (a six-sided shape). This initial shape is driven by the hydrogen bonding that exists within a water molecule. At this point, the ice crystal remains light enough that it remains suspended in the cloud. While suspended in the cloud, other supercooled droplets freeze onto the initial ice crystal. This allows the crystal to steadily grow in size. The additional supercooled water droplets that the ice crystal comes in contact with will tend to freeze on the outside edges of the initial hexagonal form, resulting in the growth of 6 branches that extend outward from the center of the snowflake. The image below depicts this process starting at the initial ice crystal formation, followed by the growth that occurs as surrounding supercooled water droplets continue to freeze onto it.\nAs the snowflake travels through different portions of the cloud, it continues to grow, encountering contrasting temperatures and humidity levels along the way. The specific temperature and humidity level present in the cloud layer correlates to the type of form the snowflake takes on. Snowflakes that form in cold and dry environments tend to take on a simpler form, as less moisture results in less intricate crystal growth. Conversely, when snowflakes grow in warmer and increasingly moist environments, their form tends to take on the intricate dendrite form (typical 6 branched snowflake) that we are all familiar with. The image below provides the varied snowflake shapes that occur when ice crystals grow in environments of differing moisture and temperature combinations (moisture level is the vertical or Y axis while temperature is the horizontal or X axis).\nOnce the snow crystal has grown heavy enough, it can no longer be supported by the weak updrafts present in the clouds (slow rising air currents). At this point, it will begin to fall from the cloud and begin its trek down to the surface. Along its way down, the snowflake's shape will undergo further modification due to collisions with other snowflakes. When snowflakes fall into a warmer environment (temperatures close to 32F), the edges of the snowflake will partially melt. When this happens, other snowflakes that come into contact with it will tend to stick to it. This process of snowflakes sticking together to form larger snowflakes is called aggregation. This is common when temperatures are near the freezing mark as the snowflake gets closer to the surface. Conversely, when snowflakes travel down into a very cold and well below freezing environment, the snowflakes take on a more brittle character and will fracture and break apart when they collide with one another. The last factor that can impact the eventual shape of the snowflake is wind. When high winds are encountered during the snowflakes descent to the ground, crystal fracturing will occur for both wet as well as the dry, brittle variety of snowflakes.\nWith all the variables that occur from the time an ice crystal first forms, to when the finished product of a snowflake reaches the ground, it is not hard to grasp the concept of no two snowflakes ever being alike. Each individual snowflake will travel in at least a slightly different path from each other, meaning it will travel through varied temperature and humidity levels at differing times from one another. The different temperatures and humidity levels that the ice crystal travels through, and how long it is subjected to each environment, directly impacts the type of form each crystal will take on. The end result of it all is one of nature's great pieces of artwork, lending to the sense of wonder that the viewing of a snowflake up-close creates.","How are Rainbows FormedPosted by monsterguide\nHow are Rainbows Formed\nA rainbow is a colorful bow appearing in the sky when it suddenly rains on a bright sunny day.\nWhat is a Rainbow?\nSunlight is composed of light of varying wavelengths. Whilst the shorter wavelength light appears as blue, violet and indigo in color, the longer wavelength light appears as red, orange and yellow in color. When this sunlight passes through a raindrop in the air, it takes a complicated path. Initially light splits into the ‘VIBGYOR’ colors. This light then reflects off the back surface of the raindrop and then goes back to the other side in which the light first passed, forming an arch. The light thus emerging from several of the raindrops create a rainbow.\nIt is not always that every time it rains we can witness this colorful rainbow in the sky. The prerequisite for rainbow formation is a brightly shining sun from one side and rain coming from the other direction.\nWhen you look at a rainbow, you would see that the sun is behind you. There is a fixed critical angle where the light from the sun is reflected back to you. If there is a water droplet in the right place and at the time of a rain, there will always be raindrops in the right place because drops falling behind will replace the earlier ones. A rainbow with the brightest colors is seen when the sun is quite low on a clear sky just behind us, and a heavy rain just in front of us.\nWhy should the rainbow be curved? Why can’t it be a straight line across the two horizons?\nA rainbow is always curved just for the reason that all the angles of the water droplet have to be just right for it to reflect some light back to you, standing on the ground. So when there is sunlight behind a viewer, only those water droplets that have the same angle formed by the viewer, the droplet, and the sun (which is approximately 42 degrees) will produce the rainbow effect. Other droplets send their light somewhere else, and if the person moves to a different location, a few more droplets are needed to form another rainbow that the viewer witnesses in the new place.\nThis is also the reason why it is said that we cannot reach the end of a rainbow. Wherever a person stands, it is only the faraway droplets of water that reflect and bend sunlight to result in a rainbow.\nA rainbow may look like a half circle when it is formed at sunrise or sunset because the light at that time travels almost horizontally towards the earth. However, as the sun rises high in the sky, the shape of the rainbow would be like an arch rather than a half circle.\nInterestingly, from an airplane a rainbow is seen as a full circle! And how do you like this?\nThe ‘VIBGYOR’ colors of a rainbow are due to the fact that each rain droplet behaves like an independent prism which splits sunlight into its constituent colors.\nA simple demo to form a rainbow under normal conditions!: Take a spray gun and fill it with water. Go out on a sunny day and – with the sun behind you – spray water in front of you along all directions to see rainbows popping all around.\nRainbow Image Gallery"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:cb8c850d-c9a4-46cd-afdd-8d51fba2fb0a>","<urn:uuid:5d540404-8812-44ea-ba41-c30d65248722>"],"error":null}
{"question":"As a guitar craftsman with decades of experience, I'm curious: do both archtop guitars and Stratocasters typically use wood-to-wood sound transmission in their bridge designs?","answer":"No, they use different approaches. Archtop guitars, as described, use solid wood bridges that provide direct wood-to-wood sound transmission for fuller sound and quicker response. In contrast, the Stratocaster uses a 2-point tremolo system bridge made of metal that connects to springs in the guitar body, which can affect tuning stability but allows for vibrato effects.","context":["With good reason, archtops are considered the greatest challenge in guitar building. And there’s nothing more beautiful than a hand-carved, elegantly arched top or back. Those curves! Those reflections! An archtop’s fusion of structure, sound, and elegance is an art that’s not easy to master. And it gets even more difficult when you’re departing from conventional shapes, for example by introducing tighter radiuses - nothing breaks easier than highly flamed wood being bent into shape! But we’re known for outsmarting obstacles, and often, all it takes is more time and patience - and occasionally, an extra set of sides or two. . .\nTime and patience are also required for an archtop’s details. Some of these are invisibly located on the inside of the guitar’s body, but even some that are on the outside, such as a solid ebony bridge, aren’t noticed right away. Meticulously shaped to be a perfect match to the top’s arch, a solid bridge yields a fuller sound and quicker response than the typical adjustable designs, where the strings’ vibrations have to travel through a set of screws, and then a separate, possibly flexible, base - no wonder that tone gets lost!\nSimilarly, our tailpiece differs from most standard designs, which tend to be mounted to either a metal hinge or a plastic string. Inspired by traditional Japanese wood joinery (as well as the viola de gamba), our tailpiece is mounted to a matching wood counterpart, resulting in sound transmission that goes from wood to wood, which, in our view, is the best and most honest tonal option. On guitars with pickups, we even include a ground connection to the strings to avoid issues with buzz and hum.\nLet’s not forget about our reversed pickguard, which is another of our innovations. Typically, the pickguard - often larger than necessary - is mounted with its largest section toward the center of the body. This makes sense from a historical perspective, as players used to have to dig in hard to be heard playing in a big band without amplification. But very few guitarists play that way today, with the result being that the pickguard’s function has largely moved from one of protecting the guitar’s top to being more of a finger-rest and guide for the picking hand. Accordingly, we turned the shape around, with the narrow portion toward the bridge, which not only harmonizes better with the shape of the tailpiece, but also allows for better tone by not covering up the f-hole.\nIt goes without saying that our pickguards attach to the fingerboard extension, rather than being mounted to the instrument’s top. On guitars with electronics, the pickguard also serves as a platform for the pickup (often with a cover made from the same wood as the pickguard itself) and virtually invisibly mounted controls.\nSpeaking of f-holes: Most jazz guitars have them, ours don’t. Originating with the family of bowed instruments, the shape was adapted for guitars early on, even though nothing is further removed from how an archtop guitar’s top is excited than a violin. Another reason is that traditional f-holes can easily be cut with a router, while the «f-holes» on our Massari model take two days to realize.\nA few words about the wood: We celebrated our 35th anniversary in 2015, so it’s only natural that we’ve become good friends with many wood suppliers. Many of them call us when they’ve come across something special, because they know that we’re only looking for the highest quality, and won’t haggle about the price. We simply want the very best, or at least what we consider to be the best, as it’s not always the «normal» pieces of wood that inspire us. As a result, we’ve ended up with an impressive collection!","I'm a guitarist and bassist with over 35 years of experience as a musician.\nStratocaster or Telecaster: Which Is Better?\nSome people will tell you that you can’t go wrong with either a Stratocaster or Telecaster, but that’s only partly true. It really depends on your playing style and musical taste. While they are both awesome guitars, each has its own vibe and has more firmly carved its niche in certain styles of music.\nThe Stratocaster and Telecaster are legendary Fender guitars, and two of the most popular instruments in the music industry. Both are loved by players in almost every imaginable genre of music, and both have been endorsed by numerous icons of the guitar world. One of these guitars is probably better suited to you.\nJust to make things even more confusing, countless versions of each guitar exist, both those offered by Fender and those created by players themselves. Stratocasters and Telecasters have been decked out with special pickups and other appointments to make them more usable in specific genres.\nIf you don’t know much about either, how do you decide? What’s the difference, anyway?\nFender is among the best guitar companies in the world. From its Squier line of guitars to custom shop models for pros, they build a Strat or Tele for every level of player. With all of the different models and options it can be confusing, and hopefully, this article will help you better sort out the differences between the two guitars.\nFor a lot of guitarists, the Fender MIM Player Telecaster and Stratocaster are more practical from a cost perspective. However, for our purposes here, and to hopefully relieve confusion, let’s consider the American Professional Series Strat vs the American Professional Tele.\nThe Difference Between the Stratocaster and Telecaster\nHere are a few things to consider if you are trying to decide between these two great guitars. Be sure to check out Fender's website for the latest information on their gear.\nFender Guitar Bodies\nBoth the Stratocaster and Telecaster typically feature an Alder body. Alder is a snappy tonewood that gives guitars a good bite to their sound.\nThe shapes of the guitar bodies are different, and due to the shape of the upper horn perhaps the Stratocaster allows a little more access to the higher registers, but the difference is minimal and probably doesn’t affect most players.\nIt's also worth noting that both guitars are available in a few different tonewoods besides the standard alder. For example, ash is another choice coveted by both Stratocaster and Telecaster players. Some players feel it adds a little depth and resonance to their sound.\nInterestingly, some early Telecasters were made of pine, which is an almost unheard of tonewood choice today. You can still get a pine Tele body in some high-end Fender models, and affordable Squier models.\nFender American Professional Stratocaster\nStrat vs Tele Neck\nThe necks of the Strat and Tele are both Fender’s “Deep C Shape”, and both are made of maple with the option for either a one-piece all-maple neck or a maple neck with a rosewood fingerboard. The choice between maple and rosewood fingerboards is up to the individual guitar player, and one isn't inherently better than the other. Maple fingerboards have a bit more bite, where rosewood will round out the notes a little more, and it is a matter of personal preference.\nBoth guitars feature 22 narrow-tall frets, the same 25.5” scale, and the same 1.685\" nut width and the same 9.5\" fretboard radius. The Stratocaster has a larger headstock. Some guitarists believe a heavier headstock allows for greater sustain and better tone. This is the kind of discussion that can cause massive arguments in guitar forums, but the truth is most players probably won’t notice a difference.\nEven though the necks are similar, most players will tell you there is a definite difference in the feel of each guitar.\nSo far both guitars seem pretty similar, but from here out the differences become more evident.\nStratocaster and Telecaster Pickups\nBoth guitars come with single-coil pickups specifically designed to get the most out of their sound. The Stratocaster has three V-Mod single-coil Strat pickups, where the Telecaster has a pair of V-Mode Tele pickups.\nThe Strat has a 5-way pickup selector switch, where the Tele has a 3-way. This means the Strat has more options and more available tones when it comes to switching between pickups, and employing different pickup configurations. These uniquely designed pickups also mean there is a noticeable tone difference between the two guitars.\nBoth guitars have a master volume control with treble bleed. The Strat has two tone controls, where the Tele only has one.\nNote that both the Strat and Tele come in versions equipped with humbucking pickups. For the Strat, a bridge position humbucker is most common, and Fender makes many versions of the guitar with a hum-single-single or HSS pickup configuration.\nFender American Professional Telecaster\nFender Electronics and Hardware\nWhen it comes to hardware, the bridge is perhaps the biggest difference here between the Stratocaster and Telecaster. Both bridges have six adjustable saddles, but where the Tele bridge is one piece that extends beneath the bridge pickup, the Strat bridge is a smaller 2-point tremolo system.\nThe Stratocaster bridge is attached to springs that connect with the body of the guitar and allow the player to increase or decrease tension on the strings, creating a vibrato effect. This makes the Stratocaster a bit tougher to keep in tune, but a good setup goes a long way toward maintaining the tuning stability of either guitar.\nMany players \"block\" the bridge of their Strats, effectively turning it into a fixed bridge more similar to the Telecaster. For heavier styles of music, some players prefer Stratocasters equipped with Floyd Rose tremolo systems.\nBoth guitars are fairly simple under the hood, sticking to the basic blueprint outlined by Fender so many years ago. Here again, the Stratocaster offers a little more flexibility when it comes to tone. Both guitars have master volume controls, but where the Telecaster has only one tone control, the Strat has one tone knob dedicated to the middle pickup, and one to the bridge pickup.\nThis, combined with the aforementioned 5-way versus 3-way switching makes the Stratocaster a little more versatile when it comes to the array of available sounds.\nStrat vs Tele Sound Difference\nStrat or Tele: Which to Choose?\nMaybe you read all of the above and you’re still confused. After all, what matters is figuring out which guitar is better for you, not knowing what the neck is made out of! The best thing to do is go out and play a bunch of each kind of guitar and let your ears and hands make the choice for you.\nMost experienced players will tell you, even though the two guitars might look similar on paper, the feel and sound of a Stratocaster versus a Telecaster are very different.\nRemember that the guitars compared above are the basic American Professional versions. If you decide you like the feel of a Telecaster but prefer the sound of a Strat, no doubt with minimal searching you can track down a Fender with pickups and specs more to your liking.\nFender really does offer a ton of different options for both guitars. Stratocasters might be more known for rock and blues. There are even some Strats made for metal.\nTelecasters, on the other hand, are known for rock and blues but regarded among the best electric guitars for country music.\nBoth guitars have been used successfully in every genre imaginable. Don’t let stereotypes make your decision for you. Go out and play guitars and decide for yourself what you like.\nStratocasters and Telecasters are both great guitars, but if you're like most guitarists you’ll eventually find yourself gravitating to one or the other. And after you've been playing for a while you'll come to know another secret about Fender guitars: You can never own too many of them! When it comes to the Stratocaster and the Telecaster, no matter which is their favorite many players simply prefer to have a few of each!\nStratocaster vs Telecaster\nQuestions & Answers\nQuestion: Is a hardtail Stratocaster considered a \"real\" Strat and is the sound different?\nAnswer: I certainly consider hardtail Stratocasters real Strats, and I think most guitar players would agree. You may notice a slight increase in sustain and resonance with a hardtail, where a Strat with a tremolo will sound a little tighter. Overall I think the difference is minimal, but some players swear by one or the other.\nMany Stratocaster players also “deck” their tremolos. This means they adjust the springs and the bridge itself to prevent it from easily moving. Another method is to “block” the tremolo. Here a wooden block is installed in the spring cavity, again preventing the tremolo from moving. Both of these methods, to some extent, can turn a Strat with a tremolo into a hardtail.\nSome players feel that a blocked or decked tremolo results in better tuning stability, as well as the aforementioned improved sustain and resonance. If you intend to try it and aren’t experienced with guitar setup and repair you’ll want to leave this job to a guitar tech.\nHowever, even if you block the trem you’ll still have a big cavity and lots of metal in the body of the guitar, instead of the solid wood body you’d get with a true hardtail. That, I think, is the root of any sound difference. Again, I think it’s minimal, but I suggest you try out both types of Stratocasters and decide for yourself.\nJordan Dyas on December 04, 2019:\nAs an older age beginner my idols (James Burton, Steve Cropper, even Jerry Garcia at one point). I'm also a '60's hippie who loves country music. Dancing to Bob Wills & favorite Colorado band, Dusty Drapes and the Dusters.\nChris on September 04, 2016:\nWhat about Rickenbacker guys?? Can't beat that jangle and good for slide too (ask Joe Walsh). Loved your article by the way but I would choose a Tele every time. Cheers Chris.\nMichael James (author) on November 09, 2014:\nThanks Matt! You make a great point, but you left out the rest of my sentence in the quote: \"...but the difference is minimal and probably doesn’t affect most players.\" For me, I've always felt like shape of the body of the bass-side made the Tele a little clumsier when fretting the highest notes. It's not so much where the body joins the neck, but the shape of the body itself.\nOf course that's just my opinion, based on my playing style. Other guitar players may notice no difference at all.\nMatt Morrison on November 07, 2014:\nReally good article, but the author is in error when he states: \"perhaps the Stratocaster allows a little more access to the higher registers\". It's just not true. The body joins the neck in exactly the same place on both guitars. The Strat gives the visual ILLUSION of a deeper cutaway because of its extended upper horn. But I have had many Strats and many Teles. Access to the higher registers is exactly the same."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:9aa5f05e-bdd2-4b7c-b864-2b0029d98416>","<urn:uuid:de4b4115-64be-4ae5-af1b-44d60525056f>"],"error":null}
{"question":"How has Iran's nuclear program development evolved since 2015, and what were the key developments after the US withdrawal from the JCPOA?","answer":"In 2015, Iran agreed to the JCPOA nuclear deal with major world powers, restricting its nuclear program to civilian use in exchange for sanctions relief. However, after US President Trump withdrew from the deal in 2018 and reimposed sanctions, Iran announced in 2019 that it would increase uranium enrichment. By August 2021, the International Atomic Energy Agency reported that Iran had accelerated its uranium enrichment to 60%. Negotiations to restore the 2015 agreement resumed in Vienna in late 2021.","context":["Due to Khamenei’s distrust of foreign powers, security is his main interest and the area where he has concentrated the most power. He has been the most active on this type of policy-making and, when he became Supreme Leader, the first thing Khamenei did was appoint a new generation of politicians with expertise in security and military issues.Khamenei has always distrusted of foreign powers, especially the USA, and security has been a key area for his own priorities and political survival. In order to maintain his leadership, Khamenei has appointed a new generation of politicians and a close inner circle in security and military issues and bolstered ties with other state and non-state actors with the same geopolitical interests as Iran.Due to Khamenei’s distrust of foreign powers, security is his main interest and the area where he has concentrated the most power. He has been the most active on this type of policy-making and, when he became Supreme Leader, the first thing Khamenei did was appoint a new generation of politicians with expertise in security and military issues.\nSecurity from Imperialism\nKhamenei’s security related decisions are based on his personal experiences during Mohammad Reza Pahlavi’s era (1941-1979). During the last years of Reza Pahlavi’s reign, Washington and London had the last word on Iranian internal and foreign policy. Consequently, Khamenei has had a constant fear that foreign countries, especially the US will make the key decisions in Iran’s domestic and foreign related topics.\nKhamenei views the world as a hostile environment and has a high distrust of others. This pessimism can be explained by his witnessing of different events such as the 1953 Iranian coup (Ajax operation), the Iran-Iraq war, the fall of the USSR, US interventions in Afghanistan in 2001 and in Iraq in 2003 and 2011 Arab Revolts. As well as his predecessor, Khomenei, Khamenei had little trust in the regular iranian army, as the army had supported the Shah before the triumph of the Islamic Revolution in 1979. Therefore, Khamenei has made a deal with the Islamic Revolutionary Guard Corps (IRGC). The IRGC is the second armed forces within Iran and this body was key to carry out the 1979 revolution and have historically protected the core principles of the Islamic Revolution, since Khomeini took power. Additionally, the IRGC is paramount for the regime’s survival, a second army with Iran for keeping the regime.\nKhamenei fears his own political survival. Thus, he has cooperated with foreign allies and non allies, when convenient. It is important to highlight that in the Middle East and among the Muslim world, there are 3 core branches: Sunni, Shia and Ibadi. During centuries these branches lived peacefully together. Nevertheless, the violence among Sunni and Shia will erupt since 1979.\nIran’s Islamic Revolution in 1979 gave Shia cleric Ayatollah Ruhollah Khomeini the opportunity to implement his vision for an Islamic government ruled by the “guardianship of the jurist” (velayat-e faqih), a controversial concept among Shia scholars that is opposed by Sunnis, who have historically differentiated between political leadership and religious scholarship. Khomeini tried to inspire further Islamic revival, preaching Muslim unity, but supported groups in Lebanon, Iraq, Afghanistan, Bahrain, and Pakistan that had specific Shia agendas. Sunni Islamists, such as the Muslim Brotherhood and Hamas, admired Khomeini’s success, but did not accept his leadership.\nAs the IRGC first deployed abroad in the Iran-Iraq War, it began sponsoring non-state armed groups in the region. The expeditionary Quds Force emerged as the IRGC’s de facto external affairs branch, and it has developed ties with armed groups in Afghanistan, Iraq, Lebanon, the Palestinian territories. The Quds Force provided these groups training and military advice to project its power abroad.\nAfter the Arab Revolts in 2011, the rivalry among Sunni and Shia got into a new level and since 2011. Shia groups supported by Iran have recently won important political victories and consequently, Saudi Arabia has been worried about their own grips on power in this region. Iran and Saudi Arabia have been in a war using proxies in Syria, Lebanon, Iraq and Yemen mainly.\nTo begin with, in Syria, the al-Assad family has ruled this country since the 1970’s. Al-Assad relies on Alawis, a heterodox Shia sect that makes up about 13 percent of Syria’s population. They dominate the upper reaches of the country’s military and security services and are the backbone of the forces fighting to support the Assad regime in Syria’s civil war.\nSecondly, in Iraq, since the 2003 invasion of Iraq unseated Saddam Hussein and instituted competitive elections, the Shia majority has dominated the parliament and produced its prime ministers.\nThirdly and most important in Lebanon and Yemen, Hezbollah, the Lebanese Shia militia and political movement, is the strongest party in Lebanon. Plus, the Houthis, Shia militants in Yemen tenuously linked to Iran, have toppled the country’s internationally recognized government. Iran, a majority Shia country, has seen its regional influence swell as its allies in these countries have accumulated power.\nThere are two main security concerns Khamenei has dealt with throughout his long tenure: regional power struggles and the nuclear conflict with the US.\nBecoming the regional power for Khamenei's survival\nSince Khamenei came to power, he has aimed at making Iran the leader of the Muslim world by uniting them that will increase Iran’s regional power and fortifying Khamenei’s leadership position through the use of nationalism. A clear example of this goal of uniting the Muslim world has been the ongoing Palestinian-Israeli conflict and up to this day, Iran has not only withdrew its recognition of the Israeli state. Although Khamenei has an ideological and theological preference for Shia majority countries, he has been flexible and pragmatic in his aspirations. For instance, Iran helps the terrorist group, Hamas (suni majority) against Israel. Nevertheless, this situation has approached the Gulf States and Israel against Iran. For instance, in 2020, the Abraham Accords were signed, between Israel, the UAE and Bahrain, with USA intervention. Additionally, under the table Israel and Saudi Arabia have improved their bilateral ties and thus, becoming Israel from the enemy among the Gulf Countries as an ally.\nUS intervention in Iraq and in 2011 Arab Revolts provided a key opportunity for Khamenei to increase Iran’s influence in the Middle East and to develop a Shi’a corridor towards the Mediterranean. Ali Khamenei has supported groups that share the same similar interests as Iran, such as: Hezbollah in Lebanon, Shi’a based groups in Iraq, Houthi in Yemen and was key to Al-Assad’s stay in power in Syria. Khamenei has also invested in Islamic economic, political and cultural centers throughout the region.\nEspecially since 2018 Iran has looked east and strengthened its ties with China and Russia, for Khamenei’s survival. These three states do not share U.S interests in containing Iran and stifling its economy. Moscow, Beijing and Tehran work together to undermine Washington’s interests in the Middle East. In late 2021, Iran was accepted into the Shanghai Coordination Organization and thus, enhances Tehran’s ties with China and Russia. In January 2022, Iran, Russia and China held joint naval exercises in the Gulf of Oman.\nNuclear development as deterrence\nKhamenei has continued with Iran’s nuclear programme, started in the 1960’s as a tool to bolster national pride, technological advancement and to meet Iran’s internal energy demand. This tool and nuclear modernization has allowed Khamenei to strengthen his power and his political position. Another key reason for Iran’s nuclear development is based on security concerns. Since the triumph of the Islamic Revolution in 1979, Iran relations with the US, Israel and Europe worsened. Under Khamenei’s rule these ties worsened even more. In order to contest US, Israel and other western powers leverage in the Middle East, the nuclear bomb development is key. Furthermore, Khamenei is concerned that Saudi Arabia and Turkey could also develop their own nuclear weapons.\nKhamenei has always kept a strategic ambiguity regarding if Iran has already developed its own nuclear weapons or not. In 2015, the US, European Union, France, Germany, the United Kingdom, Russia, the People’s Republic of China and Iran agreed on a nuclear deal, commonly known as JCPOA.\nThe JCPOA portrayed Iran’s willingness to collaborate with the West and ensured that its nuclear programme was barely restricted to a mere civilian use, in exchange of a sanction lift. Not all parties were happy with 2015’s Iran Nuclear Deal, such as Turkey, Saudi Arabia and especially, Israel. These three countries share the distrust and fear that Iran could become the regional power in the Middle East.\nIsrael lobbied against this deal and in 2018 US President Trump withdrew from the JCPOA and reimposed sanctions against Iran. This marked a new chapter of US-Iran open hostilities that culminated in the assassination of IRCG, General Soleimani. In 2019, Khamenei announced that Iran would start enriching more uranium, as a tool of pressure. In august 2021, the International Atomic Energy Agency announced that Iran accelerated its uranium enrichment to 60%.\nSince the end of 2021, negotiations have restarted in Vienna. These negotiations have the goal to restore the 2015 international agreement that limited Iran’s nuclear program in exchange for sanctions relief. All eyes are kept in Vienna and, even though Israel, Saudi Arabia and Turkey are not formally part of the diplomatic talks; they are making all the necessary cautious moves to safeguard their national interest to avoid a strong Iran that could minimize their footprint in the Middle East."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:1626f295-036c-4b89-84e4-2e5af0a21d7e>"],"error":null}
{"question":"Hey plant experts! What causes yellowing in young vs old leaves? I keep seeing conflicting info!","answer":"The age of leaves that yellow first helps identify different nutrient and deficiency issues. For young leaves, yellowing typically indicates either iron deficiency (showing as yellowing between leaf veins on plant tops and branch tips) or sulfur deficiency. In contrast, older leaves yellow first with nitrogen deficiency (showing broad yellowing that spreads outward), potassium deficiency (bright yellow leaf edges with green centers), and magnesium deficiency (yellow patches between veins while edges stay green longest). This pattern occurs because some nutrients are mobile within the plant and can be relocated from older to younger leaves, while others are immobile and deficiencies show up in new growth first.","context":["Sulfur is an essential nutrient that is required for plant growth. The need for additional sulfur in crop production has increased over the past 30 years.\nYellowing of plants, slow plant growth, and delayed maturity are signs of sulfur deficiency.\nTissue sampling should be done to assess the need for added sulfur.\nSulfur (S), a secondary macronutrient, is one of 18 essential nutrients that are required for plant growth and is ranked only behind nitrogen, phosphorus, and potassium in importance. Sulfur plays a key role in protein synthesis and is needed for many plant functions, including photosynthesis, chlorophyll formation, and nitrogen fixation. Deficiency can occur when sulfur is limited, resulting in light green coloration, reduced plant growth, and delayed maturity in corn and soybean.\nThe reduction of sulfur dioxide (SO2) in the atmosphere from anti-pollution laws, combined with higher yields and lower use of sulfur containing pesticides, has increased the need for additional sulfur in recent years. In fact, reports have shown that the amount of sulfur in rainfall throughout the Midwest has decreased by about 10-lbs per acre in the past 30 years (Figure 1).1 Since we can no longer rely on the atmospheric deposition, crops must get their sulfur through the mineralization of organic matter and supplemental fertilizer. The addition of sulfur in some areas can produce dramatic results, helping to increase yield and profits in corn and soybean production.4\nSulfur Mineralization- About 95 percent of the total sulfur in soils is found in organic matter, which remains unavailable to the plant until it is converted, or mineralized, to sulfate-sulfur (S04-S). Only 3 to 5-pounds of S are mineralized each year per percent of organic matter in the top 6-inches of soil. The rate of mineralization is dependent on soil moisture, temperature, and the carbon to sulfur (C:S) ratio. Conservation tillage systems can further slow the rate of mineralization due to a higher C:S ratio and lower soil temperatures.4\nCrop Removal and Leaching- Crop removal and leaching are the two main ways that sulfur leaves the soil. A 200-bu corn crop harvested for grain removes about 16-lbs S per acre (25 to 30-lbs for silage), half of which is taken up after tasseling.4 Roughly 10-lbs S per acre is removed from a 65-bu soybean crop, or, 0.16-lbs S per bushel of soybean.3 Like nitrate, S04-S is mobile in the soil and is subject to leaching out of the root zone. Excessive rainfall can accelerate leaching of sulfate through the soil, especially in sandy soils.4 The addition of sulfur is particularly beneficial on soils with under 2% organic matter and in coarse-textured soils. Sulfur typically does not need to be added in fine-textured soils or fields with manure history.\nSulfur deficiency- Deficiency is more likely to occur on sandy soils and areas with low organic matter. Sulfur deficiency in corn is displayed as an overall yellow appearance, similar to nitrogen deficiency. However, because sulfur is immobile in the plant, symptoms will first appear on younger leaves; whereas nitrogen deficiency will first show on older leaves. Striped leaves in corn is also a symptom of sulfur deficiency and may be confused with magnesium, manganese, or zinc deficiency. In soybeans, look for yellowing of the plant, starting in the upper canopy.1,3Crops that are deficient in sulfur will have slow plant growth and delayed maturity. Early season deficiency symptoms may occur under cool soil conditions before young plants have a fully developed root system; however, plants are likely to overcome this as soils warm and root mass and sulfur mineralization increases.3 If an S deficiency is misdiagnosed as an N deficiency, the application of fertilizer N will make the S deficiency worse; therefore, tissue sampling is recommended to positively identify which nutrient is deficient.\nTable 1. Sulfur Content of common Fertilizer\nSulfur Content (%)\n|Kaiser, D., Lamb, J., Vetsch, J. 2014. Suflur in Minnesota soils.\nAG-FO-00794-B. University of Minnesota Extension.\nTissue Sampling- Soil tests are generally considered unreliable for predicting sulfur availability. Instead, tissue tests should be used to assess sulfur needs. Samples should be collected from both areas of suspected deficiency and healthy areas of the field for comparison. Tissue tests evaluate the S concentration in the plant as well as the nitrogen to sulfur (N:S) ratio. Since proteins within the plant require an N:S ratio of 15:1, a deficiency can still occur when the S concentration is sufficient but the N:S ratio is off.1,5\nIn corn, sample young plants with less than four leaf collars by removing the whole plant from ½-inch above the soil surface, collecting 15 to 20 plants per area. For larger plants, collect 10 to 15 of the youngest collared leaves per sampling area. Similarly, collect 10 to 15 of the youngest fully developed trifoliate leaves per sampling area in soybeans. Samples should be placed in a paper bag and shipped to a testing laboratory.1\nFertilizer Considerations- Soil texture, organic matter, crop rotation, and manure history are all important factors when predicting the need for additional sulfur (S). Since sulfur is used by the crop throughout the entire growing season and is prone to leaching from the root zone, applications should be made as close to the crop need as possible. Sulfur should be applied annually in fields where a response is predicted, such as sandy soils and fields with low organic matter.\nBased on studies across the Midwest in corn production, 25-lbs S per acre broadcast, or 10 to 12-lbs S per acre applied in a starter fertilizer is sufficient in sandy soils. For medium to fine textured soils, 10 to 15-lbs S per acre broadcast is the suggested rate.1,4 Soybeans generally require less sulfur than corn; however, soybean response to sulfur application has been recently observed.5 The University of Minnesota suggests that 10 to 15-lbs S broadcast per acre should be adequate for soybeans in at-risk fields.5 There are several fertilizer options that can be used to supply sulfur (Table 1). Sulfate sulfur (S04-S) should be applied, as it is readily available for plant uptake. Liquid fertilizer containing S can be damaging to emerging crops and should not be applied directly on the seed.4 Manure is a good source of sulfur, fields with a history of manure application typically do not need additional sulfur.\n1 Camberato, J., Maloney, S., Casteel, S. 2012. Sulfur deficiency in corn. Purdue University. www.agry.purdue.edu\n2 Schwartz, J. 2016. Is sulfur the missing ingredient? 360 Yield Center. Yield 360. https://360yieldcenter.com\n3 Place, S., Kilcer, T., Ketterings, Q., Cherney, D., Cherney, J. 2007. Sulfur for field crops. Fact Sheet 34. Cornell University. http://nmsp.cals.cornell.edu.\n4 Kaiser, D., Lamb, J., Vetsch, J. 2014. Sulfur in Minnesota soils. AG-FO-00794-B. University of Minnesota Extension. On-Farm Network Summary Sheet 2012-NB06. Soybean Sulfur Status. http://www.iasoybeans.com\n5 Kaiser, D. 2013. Does my soybean crop need sulfur? University of Minnesota Extension. http://blog-crop-news.extension.umn.edu","Plants with lush green leaves brighten your day and adorn your surroundings, from living rooms to landscapes. It is possible for plant turning yellow. Plant leaves, however, can sometimes become yellow despite your best attempts.\nYellowing, also known as chlorosis, occurs when anything interacts with your plant’s chlorophyll, the plant pigment that gives it its attractive green color. The good news is that yellow leaves indicate that plants want assistance. Yellow leaves can be remedied and prevented by reading the signals and taking the appropriate steps.\nWhy Plants Turning Yellow; Improper Watering\nIN THIS ARTICLE:\nThe most common cause of yellow leaves is a lack of water – either too much or too little. Roots can’t breathe in too damp soil. They suffocate, shut down, and stop supplying plants with the water and nutrients they require. Drought, or underwatering, has a similar impact. Plants cannot absorb important nutrients if there is insufficient water. As a result, yellow leaves appear.\nStart with porous, well-draining soil to solve or prevent water problems. Choose pots with good drainage holes and keep saucers clear of excess water if you’re growing in containers. Avoid growing where rainwater or irrigation collects in your landscape. Incorporate organic matter into your soil, such as compost, to improve soil structure and drainage.\nYou can test the soil’s moisture before watering by inserting your index finger several inches into the soil. The soil should only be watered when it feels dry. You should then deeply and thoroughly water. The soil should feel cool and moist for a few days. Make sure the soil is slightly dry before watering again.\nWhy Plants Turning yellow; Root Damage\nRoot damage can be caused by a variety of factors, including errant digging, root rot, and other diseases. Roots that have been injured may struggle to provide what your plant requires. Compacted roots occur when container plants overflow their pots. Water, oxygen, and nutrient circulation are all stifled by compacted landscape soil outside. In some circumstances, the roots are unable to function effectively, resulting in yellow leaves.\nSlide your plant out of its pot gently to examine for damaged or compacted roots. (You’ll also get a good look at soil moisture.) The roots of healthy plants are yellowish yellow. Roots that are dark and decaying have a horrible odor. It’s time to choose a new plant if the roots are decaying and sick. If compaction is an issue, remove unhealthy roots, loosen roots gently, and repot in a larger container with well-draining soil.\nImprove soil compaction by aerating your lawn with landscaping plantings. Include organic mulch and organic stuff in your planting areas. Garden gypsum can also help keep foliage green by improving soil compaction, especially in hard clay soils.\nWhy Snake Plant Turning Yellow; Improper Soil pH\nIf you’re cultivating container plants and using premium plant fertilizers on a regular basis, soil pH isn’t likely to be the source of your plant’s yellow leaves. If your leaf problem is limited to landscaping plants, however, soil pH may be the answer.\nPlants’ ability to obtain nutrients is influenced by the pH of the soil. As the pH of the soil rises or falls, so does the availability of nutrients. Soil pH in the neutral to slightly acidic range, between 6.0 to 7.0, is ideal for most plants, including lawn grasses. Rhododendrons and blueberries, for example, demand soil with a pH of 4.5 to 6.0.\nWhen the pH of the soil falls outside of a plant’s ideal range, some nutrients become unavailable to it. Even if nutrients are there, plants are unable to absorb them, even if fertilizers are added. Until the pH issues are resolved, the leaves become yellow.\nA simple soil test may determine the pH of your soil as well as provide other useful information. Most testing labs also make recommendations for pH-balancing soil additives. As a result, nutrients become available again, and green leaves resume their normal course.\nWhy Plants Turning Yellow; Lack of Nutrients\nWhen yellow leaves appear in the presence of an optimum soil pH, a real nutritional deficit may be present. Some nutrients move around a lot. Nitrogen, for example, easily passes through soil and is washed away. Nitrogen deficits turn grass and plant foliage yellow or pale green unless soil nitrogen is regularly replenished by fertilizer treatments.\nIf you suspect nutrient inadequacies, premium plant foods and adequate fertilization can assist. Your county extension agent can also provide information on the individual nutrients in question. Identifying which leaves turn yellow first and how the yellowing begins can reveal common flaws1 such as the following:\nA broad yellowing indicates a nitrogen deficiency. The older, inner leaves are the first to turn yellow. Yellowing spreads outward as time goes on, eventually reaching young leaves as well.\nLeaf edges turn brilliant yellow, but the center leaf remains green, indicating potassium shortage. The symptoms appear first on older leaves, and the leaf edges quickly become dark.\nMagnesium shortage manifests itself in elder leaves as yellow patches between veins. As yellow creeps outward from the leaf core, the veins remain green. The leaf edges are the last to turn yellow.\nYellowing between leaf veins is another sign of iron shortage, but it affects young leaves on plant tops and branch tips first.\nSulfur insufficiency begins with the youngest leaves, which turn yellow.\nNutrient relationships in soil and plants are complicated. Potassium deficiency, for example, can make iron less accessible. 2 However, excess potassium binds calcium, magnesium, and nitrogen, resulting in nutritional deficits. 1 To keep your plant clear of yellow leaves, you’ll need to fertilize it with trustworthy, premium plant meals."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:bfbdbbf6-1d8c-46d5-894b-b9410de1a496>","<urn:uuid:5c9cf399-78eb-43d5-9dbf-ce9d6c533a38>"],"error":null}
{"question":"What's the connection between ethical behavior and economic decision-making in both environmental economics and IDEA Foundation's principles?","answer":"In environmental economics, ethical behavior is linked to economic incentives, as people are more likely to change their environmental behavior when they understand the long-term economic consequences of their actions. Similarly, IDEA Foundation combines ethical standards with economic decisions by requiring that all supported projects demonstrate long-term sustainable development while adhering to principles of honesty, integrity, and fair business practices. Both frameworks recognize that economic decisions have ethical implications for environmental and social welfare.","context":["Understand environmental economics by reading this article.\nThere is worldwide recognition that natural resources are finite. Hence, if used improperly, resources become scarce and finally get exhausted through time.\nAnything in this world that gets scarce becomes more important, much more valuable than when it was in great supply. There is a need, therefore, to manage scarce resources to maximize their utility. This is the realm of economics.\nThe same applies to natural resources. As human population increases, more natural resources are required to provide for their needs. Thus, natural resources become scarce. Scarce natural resources, therefore, should be managed to ensure their availability not only for this generation but in generations to come. Environmental economics aims to satisfy this goal.\nEnvironmental Economics Defined\nEnvironmental economics is a branch of economics that uses economic principles in the study of people’s behavior in relation to their environment. It examines the way people make decisions that may either lead to environmental destruction or environmental enhancements. It clarifies options for decision-making by using economic tools.\nThis goes to say that whatever environmental degradation that results nowadays is a result of man’s deliberate disregard of the value of maintaining a viable store of natural resources. This behavior is considered unethical or immoral as it negatively impacts on other people’s welfare. The question of equity arises.\nExamples of Unethical or Immoral Behavior Towards the Environment\nWhat are examples of decisions that threaten environmental integrity? The following is a list of things that people do due to lack of ethical standards and immoral behavior:\n- indiscriminate throwing of wastes into waterways,\n- use of dynamite in fishing,\n- clear-cutting of forests especially in steep mountain slopes,\n- use of fine-mesh nets in catching fish,\n- trading of endangered species of plants or animals,\n- cutting of mangroves for charcoal,\n- mineral extraction without rehabilitation,\n… and many others.\nWhy do people behave this way?\nIn the environmental economics perspective, the above behaviors occur because people have failed to see the value, or cost of their actions. This failure prevents them from making sound, rational decisions that work towards their advantage.\nFor example, had fishers known that if they have caught only the large or moderate-sized fish using the prescribed mesh size for their nets, there will still be enough fish to catch in the future. Enough fish populations are allowed to reproduce and the young allowed to grow to more valuable sizes, instead of just being dumped as by-catch. This means more profit for fishers as they don’t need to go farther away to catch migrating fish.\nWhy Use the Environmental Economics Approach?\nAppealing to people’s morality or adherence to ethics does not usually appeal to many people. Changing people’s attitude and behavior take time. Actions to save the environment may be too late when finally, behavioral change is instituted among those who directly interact with their environment to make a living.\nThe economic incentive is one of the main reasons why people behave the way they do. If they realize that their action will have long-term consequences on their livelihood, they will voluntarily exhibit behavior towards adopting a better alternative. Thus, environmental economics help make clear the options by offering tools to balance the costs and benefits of their action.\nField, B. C. and M. K. Field, 2006. Environmental economics: an introduction, 4th ed. London: McGraw-Hill Irwin. 503 pp.\n© 2014 June 7 P. A. Regoniel","Ethical guidelines for IDEA Foundation\nEthical guidelines and values - IDEA FoundationBackground and definitions\nA code of ethics is a set of principles of conduct within an organization that guide decision making and behavior. The purpose of the code is to provide members and other interested persons with guidelines for making ethical choices in the conduct of their work. Professional integrity is the cornerstone of credibility in a working community. Member of an organization adopt a code of ethics to share a dedication to ethical behavior and adopt this code to declare the organization's principles and standards of practice.\nViews on ethical considerations have changed greatly in recent years and are still evolving. Idea Foundation will work hard to meet our own and our stakeholders expectations to be at the forefront of this development. Establishing and implementing ethical standards and values for Idea will therefor be an ongoing and dynamic process, constantly under revision and improvement. The clear responsibility for this process lies with the board of directors. The board also holds the ultimate responsibility for implementation and monitoring of activities conducted by the Foundation it self, its partners, stakeholders and projects, supported by the Foundation.\nIdea Foundation is a non-profit organization and should not be compared with a traditional investor, maximizing his returns. Our ambition is never the less to influence and contribute to development of sustainable growth and prosperity through out all the projects and geographical areas where we are involved. To be able to successfully reach our long term ambitions The board of directors with Idea Foundation have set ethical standards, that as a minimum must meet the principles of ethical investment management, including endorsement of the UN principles for responsible investment (UN-PRI).\nThe UN Global Compact asks companies to embrace, support and enact, within their sphere of influence, a set of core values in the areas of human rights, labor standards, the environment, and anti-corruption:\nPrinciple 1: Businesses should support and respect the protection of internationally proclaimed human rights; and\nPrinciple 2: make sure that they are not complicit in human rights abuses.\nPrinciple 3: Businesses should uphold the freedom of association and the effective recognition of the right to collective bargaining;\nPrinciple 4: the elimination of all forms of forced and compulsory labor;\nPrinciple 5: the effective abolition of child labor; and\nPrinciple 6: the elimination of discrimination in respect of employment and occupation.\nPrinciple 7: Businesses should support a precautionary approach to environmental challenges;\nPrinciple 8: undertake initiatives to promote greater environmental responsibility; and\nPrinciple 9: encourage the development and diffusion of environmentally friendly technologies.\nPrinciple 10: Businesses should work against corruption in all its forms, including extortion and bribery.\nIn addition to the The Ten Principles of the UN Global Compact, the Idea Foundation has established a set of core values to be followed in all aspects of our behavior and in our relationships internally and externally. Idea Foundation has a clear ambition to take an active stand in implementing the same core values in all our projects and businesses established and developed on the basis of support from Idea Foundation.\nWe want to be a leader in every aspect of our involvement and our business. In the development of our team leadership skills at every level; in our management performance; in the way we design, build, and support our products and services; and in our financial reporting.\nWe are committed to practice the highest ethical standards, and by honoring our commitments. We will take personal responsibility for our actions and treat everyone fairly, with trust and respect.\nWe will strive for continuous quality improvement in all that we do, so that we will rank among the best in customer, employee, and community satisfaction.\nPeople working together\nOur strength and our competitive advantage is and always will be – people. We will continually learn, and share, ideas and knowledge. We will encourage cooperative efforts at every level and across all activities in our company.\nGood corporate citizenship\nWe will provide a safe workplace and protect the environment. We will promote the health and well-being of people and their families. We will work with local communities to support education and other worthy causes.\nFundamental equal rights and respect for all individuals Equal rights offering same opportunities to all, despite, race, religion, sex.\nThe Foundation will support business ideas and projects that can document a long term sustainable development.\nCorporate Social Responsibility\nNo matter how small a business is it can have a strong belief in social responsibility.\nThe ethical guidelines for the Idea Foundation are promoted through the following four measures:\nExercise of power to support proposed and established projects\nExclusion of companies and projects\nObservation of companies and projects\nThe role of the board and the investment committee is to provide evaluation on whether or not investment and support in specified companies and projects is inconsistent with the established ethical guidelines and The UN Global Compact. This work shall be based on the following principles:\nHonesty and Integrity: We act with honesty and integrity.\nProfessional Behavior: We operate within the letter and the spirit of applicable laws.\nCompetence: We apply appropriate skills and capabilities to every\ndecision we make\nObjectivity: We are objective in forming our professional opinion\nConfidentiality: We respect the confidentiality of information.\nFair Business Practices: We are committed to fair business practices.\nResponsibility to Society: We recognize and respect the impact we have on the world\nRespect and Fair Treatment:We treat all our colleagues with respect, courtesy and\nAccountability and Decision-Making: We lead by example, using our Core Values as"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:e8cb93ad-78cb-4503-83b5-b535c5b54719>","<urn:uuid:10159761-6257-4e01-b061-4205ae03e2e1>"],"error":null}
{"question":"Me encantan las variaciones en la cocina! ¿Qué diferentes formas hay de preparar el aloo palak? ¡Necesito ideas! 😊","answer":"There are several variations of aloo palak: you can make it as a dry dish or with gravy (following palak paneer recipe and adding boiled potatoes), you can use boiled potatoes instead of pan-fried ones to save time, you can substitute Indian spinach with other greens like methi (fenugreek) or amaranth, and you can add peas to the dish. You can even replace regular potatoes with sweet potatoes.","context":["Aloo palak recipe – Stir fried Indian spinach potato recipe. Aloo refers to potatoes & palak to spinach in Indian language. This aloo palak is a side dish from north Indian cuisine made by stir frying potatoes and spinach. It is a healthy and delicious vegan dish made with minimal use of spices. It can be eaten with roti, paratha, Poori or even with rice.\nThis aloo palak sabzi has always been their favorite and they love this stuffed in kathi rolls, wraps, bread sandwiches and even with lemon rice.\nWe try to include greens in our diet at least 3 to 4 times a week. I prefer not to over cook the leafy greens so just a few minutes of stir frying is what I do most times. This aloo palak is a great way to enjoy spinach as it is lightly sauteed with potatoes & spices.\nYoung and fresh spinach is best to make this dish tasty with mild flavors. I do not use any garam masala for this recipe as the natural flavors of aloo and palak are good enough.\nHow I made aloo palak\nTo make this simple aloo palak, I have first fried some sliced onions which can be skipped completely from the recipe. Then I fried the potatoes & cooked them until tender. Then I added some spice powders and sauteed the palak. That’s it. But here are some variations which I make other times.\nVariations to make aloo palak\n- Aloo palak can be made to a gravy or dry dish. Both the versions are commonly made in Indian households. To make the gravy version, I follow the same recipe of Palak paneer. I just add the boiled potatoes to the spinach gravy. The gravy tastes amazing with rotis or basmati rice.\n- Sometimes I also make aloo palak with boiled potatoes. I boil the cubed aloo along with rice in the cooker and then temper the aloo. Next add palak and saute. This method also saves a lot of time as you don’t need to cook the potatoes in the pan.\n- If you do not have indian spinach, for a change you can also make it with other leafy greens such as methi (fenugreek) or amaranth.\n- My kids love peas in this dish. You can also add some along with spinach to the pan.\nTips to make aloo palak\n- Palak or spinach tastes best with minimum spices especially in the powder form. Too much of spice powder alters the taste of the leafy greens.\n- Do not over cook spinach. Add it towards the end of making the dish (once the potatoes are cooked).\n- You can skip the onions from the recipe.\n- Ensure the potatoes are chopped to bite sized about ¾ inch in size. Otherwise they may not cook well & quickly.\n- To add a nutty taste add some coarse crushed roasted peanuts just before serving. Kids will love that. You can also sprinkle some toasted sesame seeds.\n- Lastly you can replace potatoes with sweet potatoes in this recipe.\nIf you are new to cooking, then these tips on how to prepare palak for the recipe may help you.\nPreparing spinach for aloo palak\n- I usually inspect each leaf for larvae and worms. Discard the leaf if I find any.\n- Then I spray vinegar and rest the leaves for 5 to 8 mins.\n- Pluck the leaves and discard the stems as most times they have a metallic or bitter taste. The stems of palak are usually fibrous and stringy even after cooking so I remove them if the leaves are not young.\n- Next rinse them a few times in a large pot of water.\n- Remove and put them in a colander. When the water drains completely, finely chop the leaves. You can also use a food processor.\nAloo palak recipe\nAloo palak recipe | Aloo palak sabzi | Spinach potatoes recipe\nIngredients (1 cup = 240ml )\nIngredients for aloo palak recipe\n- 2 tablespoons oil\n- ¾ teaspoon cumin or jeera\n- 2 cloves garlic (chopped or paste)\n- 1 pinch asafoetida or hing (optional)\n- 2 green chilies (deseeded and chopped)\n- 1 medium onion sliced or chopped (optional)\n- 2 cups potatoes or aloo cubed about 4 medium potatoes\n- 4 cups spinach or palak chopped\n- ¼ teaspoon Salt to taste (adjust to taste)\n- ¼ teaspoon red chili powder optional\n- 1/8 teaspoon turmeric or haldi\n- 1 tablespoon lemon juice\n- ¼ to ½ teaspoon garam masala (optional)\n- 1 teaspoon toasted sesame seeds (optional)\n- ¾ teaspoon kasuri methi or dried fenugreek leaves (optional)\nHow to make the recipe\nHow to make aloo palak\n- Heat 2 tablespoons oil in a pan.\n- When the oil turns hot add ¾ teaspoon cumin and let them splutter.\n- Then add 2 cloves of garlic finely chopped and 1 to 2 chopped green chilies.\n- Fry them for a minute. Sprinkle hing.\n- Next add medium sized chopped or sliced onion and fry until light golden.\n- Then add about 2 cups of cubed potatoes & fry for 2 to 3 mins.\n- You can just stir fry until the potatoes are fork tender or Pour 3 tablespoons water and mix well.\n- Cover and cook until the potatoes are completely cooked but not mushy.\n- Then sprinkle 1/8 teaspoon turmeric, ¼ teaspoon salt and ¼ teaspoon chili powder (optional).\n- Saute for a minute or two until the raw smell of chili powder goes away.\n- You can also cover and cook for 2 minutes.\n- Next add 4 cups of chopped palak.\n- Saute on a medium high flame for about 3 minutes or until the leaves wilt.\n- You can also sprinkle some garam masala and crushed kasuri methi (optional).\n- When the moisture evaporates turn off the stove.\n- You can sprinkle 1 tsp of sesame seeds towards the end. Serve aloo palak with roti, chapati, paratha or rice.\nVideo of aloo palak recipe\nPreparation for aloo palak\n1. Pluck spinach leaves and wash them in a large pot of water. Drain and wash a few times. I usually spray some vinegar and rest them for 10 mins, then wash them a few times.\n2. Drain them completely in a colander. There should be no water in the palak.\n3. Chop them just before adding to the pan.\n4. Wash potatoes and peel them. Cube them to ¾ inch cubes. Do not chop them large as they take long time to cook. You can also boil the potatoes first and add use them in the recipe.\n5. Add them to a bowl of water and keep aside until needed.\n6. Meanwhile slice onions, chop garlic and green chilies.\nMaking aloo palak\n7. Heat oil in a pan. Add cumin seeds.\n8. When they begin to splutter, add garlic and green chilies. I usually deseed the green chilies and use. You can skip chili if you have very young kids.\n9. Fry garlic for a minute, then add hing. Avoid browning them as they may burn later when sautéed with potatoes.\n10. Add sliced onions. You can also skip the onions.\n11. Fry them until lightly golden.\n12. Add the cubed potatoes and saute for 2 to 3 mins.\n13. Pour 3 to 4 tbsp water and mix well.\n14. Cover and cook until done.\n15. The potatoes must be just cooked but not mushy.\n16. Sprinkle salt, turmeric and red chili powder.\n17. Saute for 2 to 3 mins. Cover and cook just for a minute.\nFrying aloo palak\n18. Add in the chopped spinach.\n19. Stir fry on a medium high flame just for 3 to 4 mins until they wilt off. There must be no excess moisture left in the pan.\n20. If using garam masala sprinkle it now. I usually sprinkle a tsp of toasted sesame seeds and lemon juice towards the end.\n21. Serve aloo palak with puri, roti or chapati."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:7ef549a5-1563-421f-92c3-c8b667ba594d>"],"error":null}
{"question":"As a parent of a 4-year-old with ASD, I'm curious - what are the core symptoms of autism spectrum disorder, and which treatments are most effective at this age? 🤔","answer":"The core symptoms of ASD include persistent deficits in social communication/interaction and restrictive/repetitive patterns of behavior. Specific behaviors include difficulty making eye contact, trouble understanding others' feelings, delayed language learning, narrow interests, and sensory sensitivities. For a 4-year-old, Early Intensive Behavioral Intervention (EIBI) is considered the most effective treatment, consisting of 20-40 hours per week for 2-3 years. EIBI should be started before age 5 and can take the form of structured Discrete Trial Training or Natural Environment Teaching. The Early Start Denver Model (ESDM) is also effective, blending behavioral strategies with relationship-based approaches.","context":["Learn About Autism\nWhat Is Autism Spectrum Disorder (ASD)?\nAutism spectrum disorder (ASD) is a complex developmental disability with signs typically appear during early childhood and affect a person’s ability to communicate, and interact with others. ASD is defined by a certain set of behaviors and is a “spectrum condition” that affects individuals differently and to varying degrees. There is no known single cause of autism, but increased awareness, early diagnosis/intervention, and access to appropriate services/supports lead to significantly improved outcomes.\nSome of the behaviors associated with autism include delayed learning of language; difficulty making eye contact or holding a conversation; difficulty with executive functioning, which relates to reasoning and planning; narrow, intense interests; poor motor skills and sensory sensitivities. A person on the spectrum might exhibit many of these behaviors or just a few, and/or many others besides.\nNew to Autism\nThe Autism Society of Wisconsin is here to help guide families who have a child newly diagnosed with autism. Our Next Program can help your family understand what this new diagnosis means, find community resources, and find emotional support. Call our office today to receive a free resource package and/or to connect with other parents who have been down a similar path.\nA printed copy of our Next Step booklet is included in the resource package but is also available electronically.\nTrack Your Child's Developmental Milestones\nThe journey of your child's early years includes many developmental milestones for how he or she plays, learns, speaks, and acts.\nUse this map to learn what to look for in your child. Talk with your child's doctor about these milestones.\nNot reaching these milestones, or reaching them much later than other children, could be a sign on a developmental delay.\nWhat are some of the characteristics of ASDs?\nPeople with ASDs may have problems with social, emotional, and communication skills. They might repeat certain behaviors and might not want change in their daily activities. Many people with ASDs also have different ways of learning, paying attention, or reacting to things.\nMay not understand imagination or play “pretend” games (pretend to “feed” a doll)\nMay not point at objects to show interest (point at an airplane flying over)\nMay spin objects or themselves\nMay have trouble turning attention when directed to\nMay have trouble relating to others or not have an interest in other people at all\nMay avoid eye contact or prefer to be alone\nMay have trouble understanding other people’s feelings or talking about their own feelings\nMay avoid physical contact\nMay not imitate others\nMay appear to be unaware when other people talk to them but respond to other sounds\nMight be very interested in people, but not know how to talk, play, or relate to them\nMight repeat or echo words or phrases in place of normal language (echolalia)\nMay have trouble expressing their needs using typical words or motions\nMay have trouble adapting when a routine changes\nMight have unusual reactions to the way things sound, smell, taste, look, or feel\nMay have difficulty entering conversations and taking turns within conversations\nMay interpret language in the literal sense (may not understand figurative language such as “Let’s hit the road”)\nIf you are concerned about your child’s development, don’t wait – contact your health care provider for an evaluation.\nAutism Across the Lifespan\nLike everyone else, people with autism move through significant life changes. Their quality of life depends not only on the foundation provided in childhood, but also on ongoing supports that are specific to their educational, medical, social, recreational, family, and employment needs. The Autism Society supports people with autism and their families through five critical stages of life:\n- Birth to Five\n- School Age\n- Transition to Adulthood\n- Older Adult","Autism Spectrum Disorder (ASD) is a broad term used to categorize a group of neurodevelopmental disorders that are present from early childhood and affect many aspects of day-to-day functioning. The core features of ASD include\n- Persistent deficits in social communication and social interaction\n- Restrictive and repetitive patterns of behavior, interests or activities\nThese symptoms exist on a spectrum, and are diagnosed based on severity and intensity of support required (very substantial support, substantial support, and support). Currently, ASD is estimated to occur in approximately 1 in 68 individuals. ASD disproportionately affects boys; however, the expression of the disorder is often more severe in girls. Despite advances indicating a genetic component, the causes of ASD remain largely unknown at this time.\nWhat is the role of the word spectrum in ASD?\nThere is a saying that if you’ve met one individual with ASD, you’ve only met one individual with ASD. This is because each individual diagnosed with ASD may present differently and require a different level of intervention.\nIn the category of persistent deficits in social communication and interaction, some individuals with ASD “requiring very substantial support” may have little or no verbal language. These individuals rarely initiate social interaction, and may not respond to social overtures from others. On the other side of the spectrum, individuals in need of “support” may be able to speak in full sentences, but struggle to engage in reciprocal conversations or to initiate and maintain friendships.\nIn terms of patterns of behavior, individuals requiring “very substantial support” may engage in self-stimulatory behavior (i.e., hand flapping) that severely limit functioning throughout the day. Behavior that exists on the other end of this spectrum may include difficulty transitioning between activities, as well as organizing and planning.\nOverall, it is important to remember that the behaviors in each domain that make up ASD may exist with varying severity and impact on day to day functioning.\nWhat are the most effective and evidence based treatments for ASD?\nRegardless of the severity of ASD symptoms, early and intensive intervention is universally considered to be best practice in treatment. Research on ASD intervention is primarily focused in two areas: Applied Behavior Analysis (ABA) and Developmental Social Pragmatic (DSP).\n- ABA interventions are based primarily on the concept that difficulties associated with ASD can be addressed through the identification and reinforcement of specific target behaviors\n- DSP interventions are focused on improving the ability to jointly engage in activities with adults and peers.\nThe following is a more in depth look at evidence-based interventions for the treatment of ASD\n- The most notable treatment for ASD is Early Intensive Behavioral Intervention (EIBI). EIBI consists of 20-40 hours per week of treatment for 2-3 years. As previously indicated, earlier intervention is critical, and EIBI is recommended prior to 5 years of age. Comprehensive ABA programs are adult led, and focused on all areas of functional deficit. Most comprehensive programs begin with an assessment of age appropriate behavior milestones, which are then addressed by an individualized curriculum. Depending on level of support needed, therapy may take the form of Discrete Trial Training (DTT) or Natural Environment Teaching (NET). DTT is a highly structured teaching method in which skills are broken down and individually taught. Each trial is typically reinforced with a tangible reinforce (i.e., candy, toy). NET is primarily delivered in the natural environment, capitalizes on opportunities for incidental learning, and makes use of natural reinforcers in addition to tangibles.\n- Pivotal Response Training (PRT) – PRT is based on the principles of ABA, and primarily targets “pivotal” responses rather than individual target behaviors. Pivotal responses include: motivation, response to multiple stimuli, social interaction, as well as self-management. The overarching philosophy is that improvement of these pivotal responses will lead to improvement in a variety of other functional behaviors. As PRT is play based and child initiated, this intervention may be more effective for children who possess already established behaviors including: increased social initiation and toy play.\n- Picture Exchange Communication System (PECS) – PECS is an augmentative/alternative communication (AAC) intervention based on the principles of ABA. PECS encourages the initiation of communication. That is, the child must initiate a request by presenting a picture of a desired item to a communicative partner who then fulfills the request. As the child masters various stages, the communicative repertoire is expanded to include sentences, attributes, commenting, as well as the answering questions. PECS and other AAC devices are primarily used with minimally verbal children.\n- Early Start Denver Model (ESDM) – ESDM is a blend of both ABA and DSP approaches to treatment. ESDM integrates elements of a relationship-based approach with behaviorally based teaching strategies. Specifically, a therapist or parent is instructed to match their child’s affect and allow the child to lead the activity. The program is based on a developmental curriculum and outlines the skills that need to be taught at any given time. ESDM has been shown to be effective across a wide range of abilities.\nYou can click to learn more about our Applied Behavior Analysis services.\nIn addition to these treatments, there are a variety of treatments that are currently being thoroughly investigated. These interventions include various parent training techniques, DIR floortime, comprehensive classroom based approaches, as well as PCIT. It is important to keep an eye on the literature, as treatments are constantly evolving, and new randomized controlled studies are being completed.\nThe following article was utilized in the development of the above:\nTristram Smith & Suzannah Iadarola (2015) Evidence Base Update for Autism Spectrum Disorder, Journal of Clinical Child & Adolescent Psychology, 44:6, 897-922\nAs the variety of intervention strategies for ASD has increased significantly over time, this article was chosen, as it is the most recent evaluation of what is currently offered in the field. The future directions for research stated by the authors place value on treatments that are emerging as evidence based, supporting the notion that any review of evidence based treatment must be as recent as possible.\nWritten by Ariel Kornblum, PsyD, BCBA, LBA"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:09d2c661-1215-41f4-adac-54f2e70df4fd>","<urn:uuid:c430fd78-133a-47f8-8ded-888fbe4029e9>"],"error":null}
{"question":"How do World View's balloon flights and Virgin Galactic's SpaceShipTwo compare in terms of accessibility and physical demands on passengers?","answer":"World View's balloon flights are specifically designed for greater accessibility, featuring a gentle ascent and descent that is comparable to or even gentler than a commercial airplane's takeoff and landing. The experience is designed to be accessible for people of all physical abilities. Virgin Galactic's SpaceShipTwo, on the other hand, involves more intense physical experiences, with specially designed seats that recline to help passengers handle g-forces during launch and return, and passengers wear specially developed spacesuits by Under Armour. The cabin is also designed to accommodate floating around during zero-gravity conditions, suggesting more physical involvement during the flight.","context":["World View, a stratospheric ballooning company, announced an expansion of services that includes its new edge-of-space tourism experience, which takes place inside a high-tech hot air balloon.\nThe edge-of-space flights will lift eight participants and two World View crew members in a zero-pressure stratospheric balloon and pressurized space capsule to 100,000 feet, or nearly 23 miles into the stratosphere. In total, the experience will last for anywhere between six and 12 hours.\nThe journey begins at World View’s spaceports located across the globe. Guests will arrive ahead of their flight for a five-day experience that includes excursions so they can immerse themselves in the surrounding area. The spaceport locations include the Grand Canyon in the United States, the Great Barrier Reef in Queensland Australia, the Serengeti in Kenya, an Aurora Borealis spot in Norway, Amazonia in Brazil, the Giza Pyramids in Egypt, and the Great Wall of China in Mongolia.\nNow, here’s the best part. The flights are priced at $50,000 per person. Though still quite a bit more expensive than a regular plane ride, it’s a marked discount from other space tourism operations, some of which can run for nearly $500,000 per seat.\n“There are some really great space tourism companies that are building some amazing experiences to take their customers into space. We celebrate their accomplishments in pushing the space tourism industry forward,” Ryan Hartman, World View president and CEO, told Travel + Leisure. “We’ve opted to build a different type of space tourism experience, one that will allow more humans to view the curvature of Earth, the darkness of space, and the fragility of our planet. In order to increase humanity’s access to space tourism, we felt it was important to price our experience to be more affordable to more people.”\nBut, affordability isn’t the only goal. It’s also about accessibility for all.\n“We wanted to make sure that it was accessible for people of all physical abilities. We’ve designed the experience to feature a very gentle ascent and descent that might be comparable (or even gentler) than a takeoff and landing of a commercial airplane,” Hartman added. “In addition to increasing affordability and accessibility, we also want to maximize the value of the experience. This comes in two forms: time and place. From a time perspective, we want to make sure you have as much time as possible at apogee to truly appreciate this awe-inspiring vantage point. You’ll spend over six hours hovering 100,000 feet above Earth, giving you ample time to rediscover the beauty and fragility of our planet.”\nThose interested in joining the flight can put in a deposit starting today with just $500. World View’s first commercial flights are expected to begin in early 2024 with the non-profit Space for Humanity, which already secured the inaugural commercial flight.\n“We are very excited about securing World View’s first commercial capsule,” Rachel Lyons, executive director of Space for Humanity, said in the statement. “Our mission is to expand access to space to all and in doing so, support the transformation of our world’s most ambitious leaders so they can use their experience in space to create positive change here on Earth. This is a groundbreaking time for space tourism and we’re looking forward to giving more people the opportunity to experience it for themselves.”\nCheck out more about the experience and book your spot here.","As Virgin Galactic prepares for commercial service, the space tourism company has given aspiring astronauts a glimpse of what it will be like. Disney veteran Michael Colglazier recently assumed the role of Virgin Galactic CEO to ensure the experience, which will be centred around the view of Earth and the experience of weightlessness, is as magical as anticipated.\nVirgin Galactic gave the public a first look at what the interior of its SpaceShipTwo cabin will look like when it begins operating spaceflights from New Mexico’s Spaceport America. The renderings were revealed today on YouTube (video below), after which an augmented reality app was launched for both Apple and Android users.\nThe design process was guided by input from a variety of experts including doctors, astronaut trainers, pilots, engineers and Virgin Galactic’s future astronaut customers. “The very first thing we did…was talk to our customers about their expectations for what they wanted when they went to space,” said George Whitesides, chief space officer, Virgin Galactic, during the YouTube reveal. “It was those expectations around the rocket ride, the weightlessness, the view of the Earth from space and the reception of their astronaut wings upon the conclusion of the journey, that were the key drivers behind our entire design process.”\nThe cabin features six passenger seats, which recline both for comfort – especially during launch and return, when g-forces are in effect – and to maximize cabin space during the zero-gravity portion of the trip, according to Jeremy Brown, design director, Virgin Galactic. Each seat is equipped with a seatback screen, but the in-flight entertainment will be different from what airline passengers are used to. The displays will show live flight data including speed, g-force and remaining boost time.\nThe seats were created using aluminum and carbon-fiber manufacturing techniques, and breathable fabric provided by Under Armour, which also developed the spacesuits that passengers will wear. Since SpaceShipTwo is meant to perform tourism as well as research functions, each seat can easily be replaced with equipment, Whitesides said.\nA total of 17 windows (five for the pilots and 12 for passengers) perforate the cabin walls. Each one is bordered by foam that passengers can grasp, in order to get the best view while floating around the cabin during zero-g.\n“We’ll turn [the mood lighting] off completely whilst you’re in space, to let Earth provide the ultimate beautiful lighting.” – Jeremy Brown, Virgin Galactic\nThe desire to capture and share the experience was top of mind for the design team, which incorporated 16 cameras into the cabin (in addition to those in the cockpit and mounted externally). The cabin also features a large mirror that allows passengers to see themselves hovering over their seats, without waiting for the camera footage.\nVirgin aircraft are known for their mood lighting and this technology will also be present on SpaceShipTwo. “We work alongside several astronauts who tell us that space is deep black and Earth is extremely bright, giving us a natural light source on board. But we also know the importance of lighting in our environment and how we can use it to enhance your experience,” Brown said. “We use [multi-color LED] lighting functionally during boarding. We’ve also programmed it for different phases of the flight, using a calming color on your cruise up to release altitude and we can add more vibrancy through the energetic rocket boost phase. We’ll turn it off completely whilst you’re in space, to let Earth provide the ultimate beautiful lighting.”\nSeymourpowell developed the color palette, which features golden metallics, blue hues and teal tones to evoke desert sands, celestial spaces and the ocean, respectively. The London-based design firm created a concept in 2016 for Airbus A380 first-class suites, called First Spaces.\n[BREAKING] @virgingalactic's #SpaceShipTwo Unity interior will feature 6 passenger seats with digital seatback displays, 17 windows, 16 built-in cameras. Coverage of the reveal coming soon on https://t.co/nrsTwDlc8w. Full video: https://t.co/knPNKSMjjk #VirginGalactic pic.twitter.com/cRd9HCLHLV\n— Kristina Velan (@kristinavelan) July 28, 2020\n“It’s a big moment today … for anyone who has ever dreamt of flying to space including our … founder, Richard Branson,” said Michael Colglazier, who assumed the role of CEO of Virgin Galactic one week ago following 26 years at The Walt Disney Company, the final two of which he spent leading Disney Parks International as president and managing director.\nVirgin Galactic was hoping to send Branson, who turned 70 this month, to space to celebrate the milestone and the company was on track to do so prior to the pandemic. Before becoming the first space tourism company on the New York Stock Exchange in October, the company told investors that commercial flights would begin in the first or second quarter of this year, setting a target for 16 flights in 2020.\nThe company has not announced a new timeline but is currently accepting $1,000 refundable deposits to join its Spacefarer community, which will be given priority when new tickets become available. 600 aspiring astronauts form what Virgin Galactic calls its Future Astronaut community, which has already secured seats on board SpaceShipTwo by paying significant deposits toward the $250,000 price tag for seats reserved before the company’s first test flight in December 2018. The company plans to charge more for the second round of seat sales.\nBeth Moses, chief astronaut instructor, Virgin Galactic, was the first person to fly in the customer cabin of SpaceShipTwo, during the second test flight in February 2019. “We tested ways to get in and out of the seat, ways to improve the seat interfaces, translation aids, handling aids, how you move about the cabin,” she explained during the YouTube reveal. “[The cabin] both facilitates your movement … so you can get to any window or next to any person … while not distracting you from the view. The cabin interior will be successful when the … view of Earth from space is the star of the show, and the cabin is an almost invisible but perfect supporting act.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:d1821c61-0ef3-45ed-b02a-b137a8552e57>","<urn:uuid:2483f9b0-58c4-4890-a302-0c2a96eb756d>"],"error":null}
{"question":"How do gravitational waves and electromagnetic waves differ in their interaction with spacetime?","answer":"Electromagnetic waves and gravitational waves interact with spacetime differently. Electromagnetic waves (like light) travel through spacetime without a medium, having energy and momentum but no mass. They exist within the framework of space and time. In contrast, gravitational waves are actually ripples of spacetime itself, produced by the movement of massive objects like black holes or neutron stars. These waves represent changes in the structure of spacetime and can carry energy from one region of the universe to another, despite spacetime itself having no mass, energy, or momentum.","context":["Often when talking about gravity and general relativity the “fabric of spacetime” will be mentioned. This fabric is often visualized as a kind of rubbery sheet that can be bent and stretched by the presence of masses. We use this kind of imagery so often that one might imagine that’s actually what spacetime is. But if spacetime is a fabric, it is the same type of fabric used to make the Emperor’s new clothes.\nI’ve been teaching introductory electromagnetism this term, and the one thing students often struggle with is the abstraction of it all. We start by looking at forces between charged objects, which is clear enough. But then we talk about how each charge creates an electric field around itself, and other charges interact with that field to experience a force. A nice enough description, but then we shift our focus to the field itself. An electric field can cause charges to flow through a wire, which produces a magnetic field, but a changing electric field can produce a magnetic field without any charges at all. Of course such mutually oscillating electromagnetic fields are what we call light.\nWhat started as a mathematical construct for determining the force on a charge becomes a thing in and of itself. Electromagnetic fields are an abstract physical quantity. When we talk about waves of electromagnetism, we often use the analogy of water waves. But water waves travel through the medium of water, while light waves travel through no medium at all. They have energy, but no mass. They have momentum, but no unique force. Most are invisible to human eyes. But despite this abstraction, electric and magnetic fields are taken to exist within the framework of space and time. No matter how abstract electromagnetism seems, it is a thing that exists within spacetime.\nWhich is why relativity can be even more difficult for students to wrap their heads around. We often start with special relativity, where the constancy of light can be used to describe the strange effects of time dilation, length contraction and the like. Even though it’s stressed that this relative behavior of space and time is an inherent property, that even the concept of “now” is relative, many students still imagine a background framework against which all this relative stuff occurs.\nBut in general relativity the last vestige of that background is wiped away. Just as electromagnetic waves travel not through a medium, but on their own, the warps and wefts of spacetime occur not relative to some fixed background, but as an inherent behavior of space and time. This is why masses can curve space without bending it into some higher dimension, and why the universe can expand without expanding into anything. While it helps to imagine spacetime as a bendy-stretchy fabric, just as it helps to imagine light as waves of water, it’s only an analogy to help visualize the complexity of space and time.\nThis sometimes raises the question as to whether space and time are physical quantities. It’s an idea that’s been argued about for years. On the one hand, spacetime has no mass, energy, or momentum, and can only be quantified by observing the behavior of other physical objects. On the other hand, it is warped by masses, and through gravitational waves can carry energy from one region of the universe to another. Regardless of one’s position on the metaphysics, it is clear that we can describe the structure of spacetime and its effects on light and matter quite well mathematically.\nOf course this ambiguity is part of the reason quantum gravity can be so problematic, but that’s a story for another time.","Advancing Basic Science for Humanity\nWhat are gravitational waves?1\nGravitational waves are a key prediction of general relativity, a theory proposed by Albert Einstein in 1915 that is still our best explanation for the force of gravity. Einstein pictured space and time as interwoven aspects of the same underlying reality, known as space-time. Objects that possess mass, such as stars and planets, warp space-time much like how a heavy ball placed on a trampoline creates a bowl-like depression around itself. This curvature in the space-time trampoline, so to speak, is experienced by all matter in the universe as the force of gravity. Whenever any mass moves, it generates gravitational waves that swell through space-time like ripples radiating across a pond's surface. For these waves to be big enough to detect, however, extraordinarily massive, astronomical objects are required, such as accelerating black holes or neutron stars. The ability to measure the strength and frequency of gravitational waves is important because these measurements would provide vital details about the distant, exotic phenomena that unleashed the waves upon the cosmos.\nWhy are gravitational waves important for science?\nVirtually all of our knowledge about the universe has come to us in the form of light, or electromagnetic radiation. Gravitational waves, however, are an entirely different form of radiation, produced by some of the most violent events in the universe. The study of gravitational waves will reveal the inner workings of some of the most violent events in the universe, such as the collisions of black holes and the explosions of titanic stars. Gravity itself, as a force of nature, will be probed in new ways in these extreme astrophysical conditions that are un-reproducible on Earth in the laboratory. Gravitational waves should even let scientists see all the way back to the origin of the cosmos itself in the Big Bang. The opening of these unparalleled new vistas has a historical precedent. For nearly all of human history, we had to make do with the sunlight and starlight our eyes can see. However, visible light makes up a tiny sliver of the range of energies light can possess. In the 20th century, we learned how to detect the entire electromagnetic spectrum, from the highest-energy gamma rays on down to the lowest-energy radio waves. As a result, astronomers literally saw the universe in a whole new light. A slew of phenomena and happenings in the cosmos suddenly became evident to us, from the spasms of supermassive black holes in galaxy cores to the faint heat emanated by gas clouds where solar systems are born. Now, in the 21st century, a newfound ability to see in gravitational waves will further bolster our efforts to understand the universe.\nWhat is the history behind the search for gravitational waves?\nAlbert Einstein's general theory of relativity, published in full in 1916, revolutionized the understanding of gravity that had prevailed since Isaac Newton's time in the 17th century. Rather than treating space and time as absolute, independent entities as Newton did, Einstein's equations intertwined them into a \"space-time.\" The force of gravity, although described quite accurately by Newton's mathematics, was now conceived by Einstein to be a geometric property of space-time. According to general relativity, mass caused space-time to curve in a describable manner, manifesting as gravity. Application of Einstein's theory resolved numerous discrepancies with Newton's physics, such as tiny shifts in the orbit of the planet Mercury. Over the past century, the predictions of general relativity have been proven to extreme accuracy again and again, and have even made possible precision technologies such as the Global Positioning System.\nThe LIGO Laboratory operates two detector sites, one near Hanford in eastern Washington, and another near Livingston, Louisiana. This photo shows the Hanford detector site. (Credit: Caltech/MIT/LIGO Lab)\nOne of general relativity’s biggest unproven predictions is the existence of gravitational waves. Einstein's work held that no information could travel faster than the speed of light, including the positions of masses in the universe, which are communicated by gravity. When masses move, corresponding changes in the gravitational field move through the cosmos as gravitational waves at the speed of light, like ripples across a pond. But gravity is an extremely weak force so even the sources of the biggest gravitational waves, like the cataclysmic collisions of black holes, would only produce the tiniest of wiggles by the time they reached Earth. This movement induced by gravitational waves has been calculated to be thousands of times smaller than the nucleus of an atom. For many decades, the ability to measure on such a small scale was impossible.\nAgainst this challenge, strong, albeit indirect support for the existence of gravitational waves emerged in 1974. The astronomers Russell Hulse and Joseph Taylor were conducting research at the Arecibo Radio Observatory in Puerto Rico. They discovered the first binary pulsar star system, consisting of two rapidly spinning, neutron star remnants of once-giant stars, orbiting each other and sending out pulses of radiation. According to general relativity, the two pulsars would draw closer together over time as they radiated gravitational waves. Indeed, over eight years of observations, the pulsars' orbits decreased at exactly the rate called for by Einstein's equations. Hulse and Taylor were awarded the 1993 Nobel Prize in Physics for their discovery.2\nAs exciting as these and related pulsar findings have been, no gravitational waves had ever been directly detected on Earth. LIGO was built to finally test this prediction of general relativity and made the long-awaited detection in September 2015.\nWhat are the types of gravitational waves?3\nAny movement of a mass through space-time generates gravitational waves. But objects such as people or airplanes do not make waves that are anywhere near large enough to detect. Instead, scientists must look to the cosmos, where extremely massive objects capable of generating detectable waves are relatively commonplace.\nLIGO scientists have come up with four different types of gravitational waves that, depending on their origin, should each leave a distinctive signature on the experiment's detection equipment.\nContinuous gravitational waves\nThese waves should be produced by spinning, massive objects that are not perfectly spherical. An example of such an object is a neutron star—the dense, city-sized leftover of a colossal star that collapsed on itself, exploding as a supernova. As a neutron star whirls on its axis, any bumps on its surface will emit gravitational waves. Assuming the compact neutron star's spin rate remains constant, the ripples in space-time it sends out with also remain constant, or continuous.\nInspiral gravitational waves\nTwo massive objects in the universe can become locked gravitationally and will draw each other closer and closer in a winding, spiral-shaped dance. As the objects \"inspiral\" toward each other, their orbital distances decrease and their speeds ramp up, rather like how a spinning figure skater who draws in his or her outstretched arms whirls even faster. Huge gravitational waves are generated in this process, with the waves' frequency increasing until the two objects collide and merge into a single object. There are two kinds of massive astronomical bodies that should crank out detectable inspiral gravitational waves: neutron stars (described above); and even denser objects called black holes. So-called stellar black holes form in a similar manner as neutron stars, during the collapse of a giant star going supernova. But these remnant, compact masses contain so much mass, and thus generate so much gravity, that even light cannot escape their gravitational clutches. Any combination of neutron stars and black holes spiraling in toward each other—whether two neutron stars, two black holes, or one of each—should make distinctive gravitational wave signatures.\nStochastic gravitational waves\nGiven all the possible sources of gravitational waves in the universe, scientists expect there to be a sort of background gravitational wave \"hum.\" The waves composing this background \"noise\" are expected to be small and hard to detect. Intriguingly, one of the sources of this particular kind of gravitational wave could be the Big Bang itself, 13.8 billion years ago when the universe came into existence. Stochastic gravitational waves from the Big Bang could offer scientists an unprecedented view into the earliest moments of the cosmos.\nBurst gravitational waves\nThis last category is reserved for the \"expected unexpected.\" Scientists think it is likely that LIGO and similar gravitational-wave detection experiments will discover types of waves that theorists have not yet completely described or anticipated. Supernovae, for example, as well as energetic, puzzling phenomena known as gamma ray bursts could each produce telltale gravitational waves."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:4f3a3469-5e89-47e2-8890-36b484596e4f>","<urn:uuid:6742c6db-d66f-4a04-aab2-724a28314749>"],"error":null}
{"question":"As someone struggling with anxiety, I'm curious: what are the health benefits of having an internal locus of control, and what are the risks of embracing victimhood mentality?","answer":"Having an internal locus of control is associated with better health outcomes and less stress. People with internal locus of control believe they are responsible for what happens to them and tend to be healthier, more politically active, and perform better academically. Additionally, an internal locus of control is a decisive factor in psychological resilience. Conversely, embracing victimhood and an external locus of control can have negative implications for mental health. It fosters a sense of helplessness, is correlated with anxiety disorders, and can lead to disengagement from life. When people perceive themselves as victims or permanently infirm, they may avoid facing life's challenges, which can result in increased anxiety and potentially a life of unfulfilled potential.","context":["Friday, February 26, 2016\nSome of the ideas put forth by social-cognitive theorists, including Bandura's concept of self-efficacy, are almost like traits that describe an individual's characteristic way of thinking. An example is Julian Rotter's concept of locus of control. A person can be described as having either an internal or an external locus of control. People with an internal locus of control feel as if they are responsible for what happens to them. For instance, they tend to believe that hard work will lead to success. Conversely, people with an external locus of control generally believe that luck and other forces outside of their own control determine their destinies. A person's locus of control can have a large effect on how a person thinks and acts, thus impacting their personality. A number of positive outcomes have been found to be associated with having an internal locus of control. As compared with externals, internals tend to be healthier, to be more politically active, and tend to do better in school. Of course, these findings are strictly based on correlational research (no cause & effect relationship), so we can't conclude that locus of control causes such differences.\nFriday, February 19, 2016\nRegular exercise is important for maintaining overall health and fitness but can also have a very positive effect on your mood. It has also been shown it may help prevent stress, depression, and mental health in general. Continual exercise has even been shown to produce euphoria or a “runners high”. Exercise can help tremendously with somebody struggling with self esteem issues, just 5 minutes of activity can enhance their mood and even make a stressful day wash away. Routine physical exercise can help improve a variety of bodily functions such as sleep and cardiovascular activity, it has also been known to help maintain a healthy weight, muscle strength, joint mobility and even strengthen the immune system. Exercise can also contribute to the prevention of many deadly diseases and conditions such as heart disease, diabetes, obesity and even stress. The way we perceive our bodies and health means a lot to some of us, not only our self image is important but our health too which is why exercise should be a routine part of anyone’s day so that their body and mind can function to its full potential. So whenever something has you down, instead of retreating to your bed in tears, lace up and see your day improve ahead of you.\nA drug is a substance that when inhaled, smoked, absorbed or consumed causes a physiological change in the body. Individuals use drugs for a variety of reasons such as an escape from reality or just to fit in although they can have harmful effects on our bodies and minds. Repeated use of a drug can cause the user to develop a tolerance because the body adapts to the presence of the substance requiring the user to ingest more of it for the same effect. Continued use of these substances can lead to dependency or addiction. Addiction occurs when a users body and mind is so used to functioning under the effect of a substance that when the drug is not active in their system they get sick or have withdrawals. Withdrawals or fear of not having enough of a substance to get high can lead to addicts stealing from friends or even family to support their habit. Not only can drugs harm the users mentality and body but also their family. Close friends and family can become depressed at the sight of their loved ones sick or in trouble and in even some cases, blame themselves for their addiction. Temptation and cravings are all around us in life and can be enjoyable as long as we don’t let them overwhelm or distract us from our priorities.\nThursday, February 4, 2016\nMemory is the process in which information is stored, organized, and retrieved in our brains. We use our existing memories everyday to remember when to wake up, what homework is due, and what classes we have to attend. Besides remembering daily routines we also have a long term memory which can store much larger amounts of information for potentially a whole life time. Our brains sometimes forget moments and past interactions just as often as it keeps them. Transience is the degrading of memories due to the passing of time while absentmindedness is the failure to retain a memory so it is impossible for your brain to retrieve them later. Other factors such as stress and sleep can affect how easily and effectively your brain encodes and stores information, sleep strengthens the neural connections in our brain making it easier to stabilize and retain memories. Memories can be complex and kept in our brains for long periods of time or simple such as looking at an item and recognizing it immediately after, this is not a cognitive control and is an automatic response of our brain taking note of our surroundings. Diseases associated with the mind can destroy memory and other important mental functions, Alzheimer disease causes brain cell connections to degenerate and die eventually discarding memory and many of the other brains' significant operations. Remembering things can be difficult and time consuming but without it we wouldn't be able to remember the important people, places, and information our vast world is constantly throwing at us.\nFriday, January 29, 2016\nSleep is a naturally occurring state of mind described by altered consciousness, partially inhibited sensory activity, inhibition of nearly all voluntary muscles and a big part of our day. Most people spend one-third of their lives sleeping and still scientists don't fully understand why we need sleep although it is known to help our brains retain information better. Our bodies require long periods of sleep to restore and grow muscle, repair tissue, and synthesize hormones. This is why children grow more over the summer, they also have more time to relax and do not have the stresses of the school year. Many people do not receive the amount of sleep necessary and some even suffer from sleep disorders or parasomnias, such as sleep apnea, narcolepsy, and night terrors. These disorders often disrupt the sufferers sleep cycle, prevent sleep in general, or cause intense nightmares that prompt them to wake up. Besides being able to lay in bed for hours at a time and forget all your problems, sleep is actually very important for you health mentally and psychically. Sleep is involved in the healing of the heart and repair of its blood vessels, lack of sleep is also contributed to a large amount of health problems such as heart disease, kidney failure, stroke, high blood pressure, and diabetes. Without a proper sleeping schedule you might find it harder to pay attention in school or even on the road! Sleeping rests your exhausted brain to prepare it for the next day, a person’s brain that did not receive enough sleep will lose brain cells and not work to its full capacity.\nFriday, January 22, 2016\nColors are powerful communication tools and can be used to signal action, influence mood, and cause physiological reactions. Certain colors have been associated with increased blood pressure, increased metabolism, and eyestrain. However, most colors have different representations depending on one's personal experiences and one's culture. For example, the color white is used in many Western countries to represent purity and innocence, yet it is seen as a symbol of mourning in many Eastern countries. Certain colors even affect your emotions, as well as breathing. The color red, for example, stimulates a faster heartbeat and breathing. The color blue, however, does the opposite in that it relaxes one, slows one's breathing. The color white, as commonly used by doctors, also implies sterility. Color influences perceptions because each color creates a specific type of perceptual capacity or level of intelligence. Often your favorite color might tell you what your favorite mood is and certain colors make you feel a certain way. This is how ads in the markets take advantage of the way certain colors make you feel, and use certain colors to sell you a product. The color Red-Orange is said to be the most appealing color to someone who is shopping for something. Colors were often used in several ancient cultures, including the Egyptians and the Chinese, to heal, through chromotherapy. Chromotherapy is referred to as light therapy or colourology and is still used today as a holistic or alternative treatment. In this treatment:\nwas used to stimulate the body and mind and to increase circulation.\nwas thought to stimulate the nerves and purify the body.\nwas used to heal the lungs and to increase energy levels.\nwas believed to soothe illnesses and treat pain.\nshades were thought to alleviate skin problems.","When Carl Jung was a 12-year-old schoolboy, he was shoved to the ground by another child, hitting his head on the pavement, and nearly losing consciousness. Instantly, he grasped the opportunities created by this attack.\nAt the moment I felt the blow, the thought flashed through my mind: “Now you won’t have to go to school anymore.” I was only half unconscious, but I remained lying there a few moments longer than was strictly necessary, chiefly in order to avenge myself on my assailant….\nFrom this point forward, Jung began having fainting spells whenever he returned to class or attempted homework. For six months, he did not attend school. His worried parents consulted doctors, and sent him away to convalesce. Jung described this period as “a picnic.” Beneath the giddiness, however, he sensed something was amiss.\nI frittered away my time with loafing, collecting, reading, and playing. But I did not feel any happier for it; I had the obscure feeling that I was fleeing from myself.\nEventually, Jung forgot how his infirmity arose. His invalid status was taken for granted, and he didn’t question it or concern himself with a remedy, until he overheard a conversation that shook him into awareness.\nThen one day a friend called on my father. They were sitting in the garden and I hid behind a shrub, for I was possessed of an insatiable curiosity. I heard the visitor saying to my father, “And how is your son?” “Ah, that’s a sad business,” my father replied. “The doctors no longer know what is wrong with him. They think it might be epilepsy. It would be dreadful if he were incurable. I have lost what little I had, and what will become of the boy if he cannot earn his own living?”\nI was thunderstruck. This was the collision with reality.\n“Why, then, I must get to work!” I thought suddenly.\nAt that moment, Jung became a “serious child.” He went straight to his father’s study and began working intensely on his Latin grammar.\nAfter ten minutes of this I had the finest of fainting fits. I almost fell off the chair, but after a few minutes, I felt better and went on working. “Devil take it, I’m not going to faint,” I told myself, and persisted on purpose. This time it took about fifteen minutes before the second attack came. That, too, passed like the first. “And now you must really get to work!” I stuck it out, and after an hour came the third attack. Still I did not give up, and worked for another hour, until I had the feeling that I had overcome the attacks. Suddenly I felt better than I had in all the months before. And in fact the attacks did not recur. From that day on I worked over my grammar and other schoolbooks every day. A few weeks later I returned to school, and never suffered another attack, even there. The whole bag of tricks was over and done with! That was when I learned what a neurosis is.1\nAn awkward and aggressive boy who was not well-liked by classmates or teachers, Jung must have welcomed the opportunity to escape from school. At childhood’s twilight hour, faced with the looming demands of adolescence, Jung withdrew from the world. For a while, his fate hung in the balance, as he drifted towards the possibility of permanent, self-imposed marginalization and infirmity.\nIn my therapeutic work with mothers of teens and tweens, I am a frequent second-hand witness to children who, seeking to avoid the developmental demands of approaching independence, cling to their frailties in much the same way 12-year-old Jung did. Negotiating such an impasse as a parent can be particularly difficult, as current cultural trends offer unwitting support for young people to claim oppression and illness.\nPersonal or collective attitudes that create an invitation to victimhood and infirmity can alter what we expect for ourselves. Embracing a status of oppression or affliction can be helpful, as it marshals needed care. However, when held onto too long, it can invite disengagement from life, and an avoidance of one’s fate. Worryingly, it also has negative implications for personal mental health, as it may foster a sense of helplessness.\nLocus of Control\nThinking of ourselves as oppressed or infirm may inadvertently cultivate what psychologists call an external locus of control. Locus of control is a psychological concept articulated in the 1950s by Julian Rotter. Those with an internal locus of control experience themselves as able to influence outcomes that affect them. Those with an external locus of control feel that most of what happens to them is beyond their ability to affect.\nThough both external and internal loci of control confer advantages and disadvantages, research has shown that having an internal locus of control is associated with less stress and better health, whereas having an external locus of control is correlated with anxiety disorders. Importantly, an internal locus of control appears to be a decisive factor in determining whether one will be psychologically resilient. As a society, therefore, it is in our interest to cultivate an internal locus of control, and indeed, the popular notions of grit and mindset are undergirded by locus of control theory. However, some environments are fostering its opposite.\nA mother in my practice recently shared that her child’s seventh grade year began with the teacher having students share their preferred pronouns. Immediately afterwards, this mother’s 12-year-old daughter began identifying as genderfluid and became preoccupied with her new status as a member of an oppressed minority. Though the teacher undoubtedly meant to communicate tolerance and acceptance, she inadvertently created an inducement to victimhood.\nSome current cultural trends award increased social status to those perceived as victims. Sociologists have posited that a new moral culture of victimhood is developing on college campuses. In such a culture, being a victim raises one’s standing and confers virtue, in part because it mobilizes protection and support from powerful third parties. The increased status of victimhood may account for the rise in “digital self-harm” that researchers have identified when teens cyber-bully themselves.\nVictimhood culture rewards us when we are aggrieved, helpless, and weak. It therefore encourages us to experience ourselves as being at the mercy of external forces beyond our control, which, as we have seen, may have negative consequences for mental well-being.\nIn addition to a moral culture of victimhood, a related tendency encourages us to think of ourselves as unwell. A recent piece entitled “Turning Childhood into a Mental Illness” in Spiked Online notes the trend to medicalize childhood by assigning diagnoses to ordinary distress, which encourages children to perceive themselves as ill:\nThe relationship between this new narrative of illness and its impact on young people is a dialectical one. The narrative doesn’t only frame the way children are expected to experience everyday problems – it also acts as an invitation to infirmity.\nJust as young Carl derived moral and practical gain from his infirmity, in certain subcultures today, having a mental health diagnosis brings with it perceived advantages. On Tumblr, there are communities of those who have diagnosed themselves with dissociative identity disorder. Many Tumblr users proudly list their mental health conditions in their profiles, including anxiety, depression, borderline personality disorder, and PTSD. Author Angela Nagle has named this Tumblr phenomenon “the cult of suffering, weakness, and vulnerability.” In addition to a moral culture of victimhood, a related tendency encourages us to think of ourselves as unwell.\nThe tendency towards self-diagnosis on Tumblr mirrors currents in the wider culture as the number of mental health disorders have proliferated. In the late ‘70s, the first Diagnostic and Statistical Manual contained roughly two dozen diagnostic categories. The most recent revision to this catalogue of modern maladies lists 265.\nA diagnosis carries with it a sense of absolution. It isn’t our fault that we have anxiety or depression. Forces beyond our control have conspired against us. Psychiatric diagnosis has myriad practical benefits. It can contextualize and normalize distress, reduce stigma, and point the way toward intervention and treatment. However, when our diagnosis becomes an important part of who we are, we are encouraged to abdicate responsibility for our plight. We are adrift on life’s turbulent currents, without blame, but also without agency. This fosters a sense of helplessness, which in turn can lead to increased anxiety.\nAn October 2017 New York Times article entitled “Why Are More American Teenagers Than Ever Suffering From Extreme Anxiety?” looked at the rising tide of teen anxiety in the United States. Increasing academic pressures, the advent of smart phones, and ubiquitous social media use were explored as potential contributors to increasing teen anxiety, but the article implicated another factor as well – school cultures that enable young people to avoid those things that make them uncomfortable. Special educational 504 plans address student anxieties by allowing kids to leave class early, use special entrances, and seek out safe spaces when they are feeling overwhelmed. A therapist interviewed for the Times article worries that these kinds of “avoidance-based” accommodations only make anxiety worse by sending the message to kids that they are too fragile to handle things that make them uncomfortable.\nEssentially, such adaptations to anxiety cultivate an external locus of control, teaching young people that they are not capable of handling challenge, and encouraging them to believe that the world around them ought to be altered to meet their needs. This primes people to expect life to conform to their expectations, and to feel crushed or outraged when it doesn’t. It promotes fragility, as young people wait helplessly to be acted upon.\nThe Times article profiles a New Jersey high school that has developed a dedicated program to meet the needs of anxious students. It relates an encounter between Paul Critelli, one of the program’s teachers, and a withdrawn, anxious student who claimed he had nothing to do.\nCritelli looked at him incredulously. “Dude, you’re failing physics,” Critelli said. “What do you mean you don’t have anything to do?”\n“There’s nothing I can do — I’m going to fail,” the student mumbled.\nCritelli’s student evidences an extreme external locus of control. He has collapsed utterly into victimhood, to the point that he is not able to imagine a way to advocate for himself or affect the outcome of his grade.\nAvoiding Our Fate\nIf anxiety is our chief malady, avoidance is its coddling nurse, always ready to assure us we need not risk confrontation with that which makes us uncomfortable. When we heed our fear, we stay safe, but we also stay out of life. Jung never forgot about the dangers of avoidance. Some 25 years after his period of school refusal, Jung wrote the following:\nLife calls us forth to independence, and anyone who does not heed this call because of childhood laziness or timidity is threatened with neurosis. And once this has broken out, it becomes an increasingly valid reason for running away from life and remaining forever in the morally poisonous atmosphere of infancy.3\nI’ve seen the adults that teens who withdraw from the life’s arena become. In my consulting room, they speak of lives unlived, and suffering unredeemed. It isn’t just that the world misses out on their talents and productive capacity. (Though that is no small loss – imagine if 12-year-old Carl hadn’t overhead his father’s conversation that day.) It’s that the story they came into the world to tell doesn’t get told.\nThe Times piece profiled an appealing teen who, like Jung, struggled with school avoidance. Unlike Jung, however, this teen eventually dropped out of school after failing to overcome her anxiety. According to the Times, she spends most of her days at home alone texting friends, relieved never to have to set foot in a high school again. The issue here isn’t just about kids who can’t get to class. The stakes are higher, and have to do with a life of meaning and purpose on its way to being forfeited.\nJung noted that “a neurosis is always a substitute for legitimate suffering.”4 His childhood fainting spells served as a substitute for the very legitimate suffering of finding his way into adolescence and facing his fate as a poor clergyman’s son who would need to establish a profession and make a living. The word “suffer” comes from a Latin word meaning to bear, to carry, or to endure. When we suffer our fate rather than avoiding it, we become actors in our own drama. Suffering becomes part of our personal story, that with which we must wrestle. In the words of Rilke, it is a “harsh hand that kneads us,” changing us and leaving us “proud and strengthened,” even in defeat. When, on the other hand, we externalize and medicalize our pain, we run the risk of becoming its hapless victim.\nThe Hero’s Task\nThousands of years before anyone spoke of an “internal locus of control,” the poets and bards of earlier epochs knew the decisive importance of walking toward one’s fate. The one who did this was known as the hero. Whoever daily confronts uncertainty and fear, no matter how mundane the gesture, is heroic in the psychological sense. “We each have an appointment with ourselves, though most of us never show up for it,” writes Jungian analyst James Hollis. “Showing up, and dealing with whatever must be faced in the chasms of fear and self-doubt, that is the hero’s task.”5\nIn contrast to the avoidant teens profiled in the Times, consider the words of Marcus Aurelius. During a military campaign against Barbarian invaders, the emperor and stoic philosopher wrote the following lines to himself nearly two thousand years ago:\nAt first day’s light have in readiness, against disinclination to leave your bed, the thought that, “I am rising for the work of man.” Must I grumble at setting out to do what I was born for, and for the sake of which I have been brought into the world? Is this the purpose of my creation, to lie here under the blankets and keep myself warm?6\nCreating a society in which we are encouraged to confront anxiety and face difficult realities matters not just for the mental health of individuals, but also for our collective well-being. In the world that soon awaits us, humankind will desperately need those individuals willing to rise from their beds. The challenges that loom ahead will require us to set aside timidity, weakness, and victimhood and claim instead agency and boldness, no matter how grim the odds.\nLisa Marchiano is a clinical social worker and Jungian analyst in private practice in Philadelphia, PA. Her writing on parenting issues can be found at motherhoodtransformation.com. Follow her on Twitter @lisamarchiano\n2 Nagle, Angela. Kill all normies: online culture wars from 4chan and Tumblr to Trump and the alt-Right. Zero Books, 2017, p. 73.\n3 Jung, C. G. (1970). Symbols of transformation: an analysis of the prelude to a case of schizophrenia. 2nd ed. transl. by R.F.C. hull. Princeton, NJ: Princeton University Press, para. 461.\nListen to this article"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:db6cae5f-445c-406a-ab06-87e716771b51>","<urn:uuid:61b723ee-27e0-4647-a75a-bb03c8bd3d29>"],"error":null}
{"question":"As a musician interested in both social and environmental impact, I wonder: how do jam sessions promote inclusive music communities, and what lessons can be learned from EV charging infrastructure regarding equitable access?","answer":"Jam sessions generally promote inclusive communities by being welcoming to newer and shy players, offering gentle encouragement to participate, and following structured protocols that give everyone a chance to play. Most jams tend to be inviting and provide opportunities for all skill levels to participate, with friendly encouragement helping players push beyond their comfort zones. However, examining EV charging infrastructure reveals important lessons about equitable access - current deployment shows significant disparities, with facilities being 2-2.3 times more likely to be located in majority-White areas and areas with higher education and income levels. This suggests that even well-intentioned community resources can end up disproportionately serving certain demographic groups if equity isn't explicitly prioritized in their implementation.","context":["Jam sessions are one of the best ways to improve your skills as a guitarist and have a great time playing with other musicians. However, the prospect of a jam session can feel imposing to many players.\nFrom calling tunes to comping over other players’ songs and improvising solos, jam sessions are complex endeavors. Beginners may struggle to take everything in at first — or worse, feel left out and give up on the session entirely. This guide aims to explain everything you’ll need to know to shorten your learning curve and help you take full advantage of your next jam.\nNo matter the genre or style, many jam sessions involve long rhythmic vamps, individual solos, and collective, “traded solos” throughout the course of a tune.\nBasics of Jam Sessions\nFirst and foremost, no two jam sessions are the same. A variety of factors can influence the character of any particular jam, from the time and place to the genre of music and regular crowd of musicians. Even two different dates of a periodic organized jam may be significantly different.\nThe first takeaway from this is that you shouldn’t get easily discouraged by a bad jam. The next jam may be better. If it isn’t, chances are you can find other jam sessions around that may offer a much more enjoyable experience.\nWith that being said, most jams do tend to follow a similar structure, especially those at clubs, bars, or other music venues. Players will rotate on and off stage, and each one will generally get a chance to call a song to play.\nDepending on the format, that song may be a popular hit, a jazz standard, a basic blues progression, or anything in between. While tune-callers are generally expected to give some tips on the basics of their song for those who don’t know, you may often need to keep rhythm off of their instructions alone, or play along with changes from a sheet in real time.\nSoloing is an essential part of jamming — after all, part of the fun of playing with other people is showing off your skills! Protocol for soloing may vary from jam to jam; the size of the session or skill of those involved can dictate the soloists and order of solos. Most jams will structure the songs to allow multiple solos from those on the bandstand.\nA good rule of thumb is that if you call a tune, you’ll be given space for a solo. If the jam rotates guitarists (as most do), you may be able to solo for each tune when you’re on stage. However, you’ll probably only get to play for two or three songs at a time, so it’s a definite trade-off!\nMost jams tend to be inviting for newer and shyer players. If you would prefer to just play rhythm, you can let the other players know. Don’t be surprised, though, if they gently push you to give soloing a shot. Jams are a great way to improve your skills, and friendly encouragement is often the way to push a bit beyond your comfort zone.\nMany guitarists find jam sessions through other local musicians or online musician’s groups.\nHow To Find Jam Sessions\nBefore you can play at a jam session, you’ve got to find one first! The easiest way is to do a quick Google search for jam sessions in your local area. Many bars or clubs may host weekly or semi-weekly jam nights in a variety of genres.\nLocal musicians are another great avenue to find jams. Check for any online groups (especially on Facebook). If you don’t see any, don’t be afraid to ask any musician friends or even take a trip to a local guitar store. Odds are any staff or musicians there will be happy to tell you if they know of any jams in the area.\nThankfully, your first jam session won’t involve an audience like this.\nHow to Prepare for a Jam Session\nBefore you go to a jam session, take a bit of time to prepare so you’re as relaxed as possible at the event.\nDo some online research: look up the venue’s website, see if they have a dedicated page for jam information, get a feel for the style and character of the place. If there’s no information for the jam, you can try some musician’s forums or give the venue a call to ask a few questions.\nSome jams will provide a backline for guitarists, meaning you only need to bring your instrument and plug into their amps, while others expect you to bring your own amplification as well. A quick Google search or phone call can save you from ending up with no amp to play through, so don’t forget this step!\nYou should also have a fair idea of the musical style before you show up to the jam session. If the website or sign-up page (if there is one) doesn’t give enough detail, do some more research! Videos from past jams or information from other players who attend could be helpful.\nWhile you don’t need to show up with a full setlist in mind, it’s always a good idea to think of a few songs that you enjoy and can play, are simple to explain, and offer opportunities to solo.\nDepending on the theme of the jam you could find a few jazz guitar standards or popular rock songs to suggest. Try to be original with your choices — no, “Smoke On The Water” or “Smells Like Teen Spirit” don’t count.\nIn any situation, a blues is a great backup plan. Whether you’re at a blues-focused session or someone just stole your song idea, calling a blues form is a simple and entertaining way to please everyone else on the bandstand. Switch up the key or move to a 16-bar form (rather than the standard 12) to keep things interesting, and avoid overplayed keys like E and A in particular.\nMake sure that you can play smooth rhythm guitar without slowing others down! Before jamming, practice playing songs with just a chord sheet in front of you. Knowing your different chord shapes is essential for this skill.\nIf you have no idea of a song at the jam, you can always ask the person who called it for a quick explanation, but your goal should be to play through the session without stumbling or holding the whole band up. Thankfully, most songs will be popular enough or simple enough you should be able to “comp” (play rhythm) as you go without serious issues.\n- Think of 3-5 potential songs to call\n- Work on your rhythm playing\n- Practice soloing over backing tracks\n- Research the venue to determine the required equipment\nA great example of communicating with other players while up on the bandstand.\nWhat to Do At a Jam Session\nWhen you arrive at the venue, take a few minutes to check in and talk with the other musicians there. It’s always helpful to get there early, as it gives you a bit of extra time to settle in, calm any nerves, and get ready to play when the jam begins.\nIf there’s a dedicated MC or organizer, don’t be afraid to introduce yourself. A little introduction will help him or her pair you on the bandstand with other musicians who share your skill level and interests.\nPlaying Tips For Jam Sessions\nOnce the jam session gets underway, the organizer or MC will typically direct traffic. At some sessions, there may be a house backing band that plays a distinct set then invites jammers up to join, or the jam may start out of the gate.\nAt some point during your time on stage, you will be expected to call a song for everyone to play. If you’ve thought through some ideas before the jam, this shouldn’t be too difficult. Just make sure to explain the changes for everyone, and specify any tips about the structure or melody of the tune. If for example you just want to play the verses while people solo, remember to tell people when you call out the tune.\nMake sure everyone is clear before you begin playing! Usually all it takes is a quick glance at each player for a thumbs-up or similar sign of approval, but be prepared to clarify any questions other players might have. Once you’re sure everyone’s ready to go, you can count in the song and get playing!\nRhythm playing is just as important as soloing at a jam. Finding partial chords higher on the neck is a great way to differentiate yourself from other guitarists also playing rhythm at the same time and add some character to any tune.\nPlaying Rhythm At a Jam\nThe fastest way to ingratiate yourself to other players at a jam session is to keep great rhythm. If that’s not reason enough to practice your rhythm guitar, nine times out of ten you’ll spend the majority of the jam playing rhythm anyway, so it’s something you might as well get used to beforehand.\nIn all seriousness, rhythm playing is an essential guitar skill to master. A steady, accurate rhythm part fuels a song and anchors the groove for soloists and other instrumental players.\nIt’s the tune caller’s responsibility to make sure you know the form and general chords to the song they call. If you’ve never heard the song or their explanation makes no sense, try to find a chord sheet to play from, or pick up the chords by ear as the tune progresses. If you really feel lost, you can always choose to take a rest for the tune — if you’re not confident you can make it through the form, this can be a great way to tune up your guitar, enjoy a quick drink, and simply take in the atmosphere before the next song.\nWhen playing rhythm, the most important thing is keeping the initial tempo and locking down a steady pattern. As a guitar player, it can be especially hard to resist the urge to speed up or to over-embellish! Instead, follow the lead of the other musicians and keep things relaxed. Turn down your volume a bit when playing rhythm to give the soloists some space to shine.\nOnce you’ve locked into the groove and feel comfortable playing with the chord structure, don’t be afraid to add a few timely accents to liven up the beat. If multiple guitarists are playing rhythm parts, you can also find different chord voicings or play partial chords. A bit of creativity and coordination can take a tune from mediocre to rock-solid.\nJust make sure you don’t overdo the accents, or sacrifice the groove to make your part busier. Jam sessions are all about cooperation. Be a team player and take extra care not to hog the limelight or ignore your job when you’re not soloing.\nJam Session Rhythm Tips:\n- Focus on the groove, not the accents\n- Stay on tempo\n- Don’t step on other players’ toes (play different voicings)\n- Make the soloist sound as good as possible!\nDoing this during your solo may be the quickest way to not be invited back to the next jam.\nSoloing is the essence of any jam session. And while it may seem imposing for new players, with a few simple guidelines you should be able to deliver effective solos and keep up at any jam session.\nIf you’ve never practiced improvising a solo or wouldn’t feel comfortable soloing in any case at a jam session, don’t go just yet. Practice your scales (major, minor, pentatonic, modes, and arpeggios) through the circle of fifths and improvise some solos over backing tracks or with a looper pedal. Listen to some of your favorite solos and note what licks and techniques the players use to guide you. You can also check out our article on essential guitar improvisation techniques to help focus your practice in this area.\nIf you’re a more intermediate player with some soloing experience, keep practicing! Pentatonic soloing should work well for most jam-session songs. If it’s not a standard blues form, you can use major pentatonic to play over the chords of a given form. If the changes come fast in a particular section, focus on outlining the chords — that is, use the notes from the chord shape you played for rhythm to guide your improvisation.\nFor advanced players, focus on developing your improvisational vocabulary and delivering focused, articulate solos. Stay away from tapping and shredding (nobody likes a show-off) and strive for clear, tasty licks with feel. Your goal should be to improve your own playing and work on improvisational methods you’ve never used before, rather than showing up every other player at the jam. That advice is especially important if you want to be invited back to the next session.\nNo matter your skill level, you’ll need to keep a few logistical tips in mind when soloing. If you’ve called the tune, you’ll be expected to take the first solo, so be prepared! When going into your solo, turn your volume up a tad so you can be heard over the mix. Just don’t drown out the rhythm section and remember to turn back down when you’re finished.\nA rule of thumb regarding solo length: every solo gets one chorus, a good one earns two, and only great ones get three choruses or more. In a jam session environment, everyone deserves an opportunity, but nobody should dominate. Keep your solos short and sweet! As you near the end of each chorus, look around the stage and communicate with the other musicians; if they encourage you to keep soloing, go for it! Otherwise, get ready to pass it off to the next soloist.\nJazz Session Soloing Tips:\n- Prioritize feel and ideas over speed\n- Outline chords over more difficult changes\n- Keep your solos focused and communicate with other players\nRemember to turn down your amp when trading fours so you don’t overpower the other guitarists!\nOn certain tunes (especially at jazz jams), players will “trade fours” after each person has taken a solo. This means that multiple players will take turns delivering a four-bar lick and building off each other. The trade-off can go for one chorus or many, but the general idea remains the same.\nThe order of players will usually go based on the solo order from earlier in the song. Remember your place in line and the rest is pretty simple. Once again, just be considerate of other players — don’t cut in early or let your licks run a bar long — and you should be fine.\nIf you’ve never jammed before, jam sessions can feel imposing or out of your league. It’s easy to convince yourself that you’ll never understand the process or keep up with the musicians.\nBut with some common etiquette and a bit of practice, jam sessions can become a unique opportunity to improve your chops and enjoy making music with other people. We hope this guide has been helpful and given you all the tools you need to take advantage of your next local session. Enjoy the jam!\nJazz is one of the more popular genres at many jam sessions. Time to brush up on your extended chords!","As electric vehicles (EVs) roll onto the roads in large volumes across the U.S, there has been a corresponding demand for more robust charging infrastructure. Despite the appealing environmental and economic benefits of EVs, the convenience of charging stations heavily influences adoption. And there begins the problem. While electric vehicle use is growing rapidly in well-to-do, predominantly White communities, minority neighborhoods have largely been left behind to date.\nThe U.S. government has emphasized the importance of equity when planning infrastructure investments in bills like the Build Back Better plan, and has incentivized a large portion of EV infrastructure funding in programs like NEVI and policies that seek to ensure EV charging infrastructure is deployed equitably.\nBut how well are these policies performing?\nTaking a granular approach to assess the current state of EV infrastructure\nIn an effort to understand the current state of equity in charger deployment, identify gaps that may exist, and add to the body of knowledge surrounding EV infrastructure deployment – we decided to leverage our vast data core, analytical expertise, and powerful software platform to conduct an analysis of Public EV Level 2 chargers in Columbus, Ohio.\nIn the sections below, we explore where access to EV chargers is most prevalent based on variables like population density, various socio-demographic statistics, and with different definitions of what constitutes a charging gap.\nEV Charging Infrastructure Basics\nFor EVs to achieve broad adoption and utilization, drivers need easy access to charging infrastructure. While many EV owners charge at home, people with longer commutes or irregular driving habits are unlikely to see themselves in EV ownership without excellent access to public charging stations.\nThere are three levels of charging equipment, determined by charging speed.\n- Level 1 (L1) – less than 2% of public EV chargers in the U.S. are L1.\n- Level 2 (L2) – the most common type of public EV chargers, accounting for more than 80% of public EV chargers in the U.S.\n- Level 3 (L3) – more than 15% of public EV chargers in the U.S. are L3.\nDue to the overwhelming preference for and majority of public charging stations being Level 2, we focused our analysis on L2 chargers.\nThe Landscape of EV Charging Infrastructure in Columbus\nAt first glance, you see that Columbus has Level 2 EV chargers spread across the city. A high concentration of chargers are located in the downtown area. So during our study, we took population density into consideration when drawing any conclusions regarding charger placement.\nRacial Factors and public EV Charger Locations\nCloser inspection of the distribution of public chargers in Columbus reveals disparities when comparing majority White areas to majority non-White areas. This is in line with other recent studies on public EV charging distribution. For example, Axios did an analysis of 35 U.S. cities and found that majority-White census tracts are 1.4 times as likely to have a charger.\nGiven UrbanFootprint’s unique ability to aggregate data across all census resolutions, we looked a level of granularity deeper, analyzing census block groups in Columbus using our Analyst application. It revealed that the EV charging locations in our study area are even more heavily skewed towards majority-White areas than what Axios had found in other cities around the country.\nIn Columbus, majority-White block groups are 2 times more likely to have a charger, and 2.3 times more likely to have at least three chargers.\nBut we knew there were likely other factors that may be more strongly correlated with the prevalence of charging stations than race and ethnicity.\nRelationship between EV Charger Presence and Educational Level\nOur study found an even stronger correlation between high education levels and the presence of EV chargers. It was immediately apparent from looking at the map that the median US educational attainment level (36% with at least a bachelor’s degree) was a tipping point for whether a block group would likely contain an L2 charger.\nTo be specific, around 80% of L2 chargers in our study were located in block groups with above the US median for bachelor’s degree attainment – and those block groups were 3.55 times as likely to contain a charger than those below the median.\nThe really shocking piece of this statistic is that these same block groups accounted for only 51% of population and only 40% of area.\nMedian Income and its Impact on EV Charger Distribution\nMedian income also appears to play a significant role in charger presence. Within our study area, block groups with chargers had median incomes 1.1 times higher than those without. Moreover, block groups with average incomes above the US median for household income ($68,703) were 2.24 times as likely to have a charger.\nThis finding supports the argument that historically, charger placement has favored higher income areas, and raises concerns for how access to EV infrastructure will lead to income-based disparities in future EV usage.\nEquitable Access to EV Chargers\nThe end goal of public EV charging infrastructure is to serve the public. That is, having enough chargers in a given block group to satisfy demand. We wanted to get an overall picture of who is being “served” versus “unserved” in Columbus. We chose 4 L2 chargers as a threshold for which to consider a block group “served.”\nWhen we looked at the data through this lens, all three ‘metrics’ (education level, median income, and racial composition) showed equity-related differences.\nConclusions and Recommendations\nOverall, our findings indicate that education level and median income are the most closely related metrics to the distribution of EV chargers in Columbus, Ohio. While racial factors are not quite as strong, there is still a trend for chargers to be more present in predominantly White block groups – in line with studies of other major cities across the country.\nAdditionally, the results were progressively more compelling as we peeled back all of the layers of data. For example, when limiting the analysis to the areas of the city with the highest population density, and increasing the threshold of what counts as a charging gap, the trends are magnified significantly.\nPolicymakers, local governments, utilities, and private companies should consider these findings when incentivizing, funding, planning, and placing future EV infrastructure. By focusing on ensuring equitable access to chargers, they can support the wider adoption of EVs. This includes prioritizing charger installation in diverse neighborhoods and areas with lower educational attainment and income levels.\nBy leveraging the right data at the intersections of climate, community, and the built environment, we can surface the actionable insights that will ensure a more equitable distribution of EV chargers – that will ultimately contribute significantly to higher EV adoption rates, pushing us closer towards a more resilient, sustainable, and inclusive future. Resilience Insights, when paired with our comprehensive Analyst application for data visualization, provide answers to many complex questions related to Infrastructure & Mobility for any location in the United States. Contact us if you want to learn more!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:d58be001-a8a1-472d-ad59-a8135dc1249b>","<urn:uuid:5fbf5a0f-b48f-437c-a774-25770944f520>"],"error":null}
{"question":"How do pre-fight massage rules and injury prevention guidelines differ between Thailand and the West?","answer":"In Thailand, pre-fight massages with Thai boxing oil are a standard practice, being performed at all venues from prestigious Bangkok stadiums to countryside events. However, in the West, particularly the US, Thai oil use is often prohibited or restricted, with some athletic commissions only allowing limited testing. While the Association of Boxing Commissions approved Thai oil use in 2011, many states still have confusing or restrictive rules. Regarding injury prevention, Western guidelines emphasize basic safety measures like proper warm-up and stretching for 15 minutes, staying hydrated, stopping immediately when feeling pain, and using first-aid treatment. These safety protocols are standardized and explicitly outlined, unlike the traditional Thai massage preparation methods.","context":["Photo of Original Lumpinee Stadium Bangkok, Thailand by Jacob Klensin\nIt’s a stifling day in Thailand. And inside one of the Muay Thai stadiums, a fighter lays face down on an old but sturdy wooden table in a pre-fight area of the concrete colossus. The fighter’s corner-men, a mix of old trainers and young fighters, one of whom wears a fresh cut across his brow, vigorously massage the fighter’s body. They take turns scooping out globs of Vaseline from a jar and mixing it with Thai boxing oil before working it into the skin and muscles of the fighter.\nJust across the hallway in a separate room his opponent is spread out on a similar wooden table. And his cornermen attack his body in the same methodical way. One trainer massages his right leg. One trainer massages his left leg. And yet another trainer massages his back and neck. They take turns quickly rubbing the fighter’s muscles and bending his limbs in different ways.\nAlthough both fighters come from different camps and come from different parts of Thailand, they share something similar. Like every Muay Thai fighter in Thailand, they go through the same process before the step into the ring, a process that involves a vigorous pre-fight massage.\nIt doesn’t matter if you’re at one of Bangkok’s prestigious Muay Thai stadiums, a tv studio, or in the rice fields of the countryside, where ever a Muay Thai fight takes place in Thailand, you’ll witness the pre-fight massage.\nThe pre-fight massage is a staple of the Thai martial art. Petch Tho, one of the head trainers at Sitjaopho gym in Hua Hin, says pre-fight massages help Muay Thai fighters in three ways. First, they relieve the soreness a fighter experiences from hard training and weight cutting before a fight. Second, pre-fight massages “wake up” the muscles and get them ready for the fight ahead. And third, pre-fight massages get a fighter’s muscles and joints moving efficiently before a fight.\nIt seems that Thai trainers and fighters have known the benefits of pre-fight massages for decades. But what does science tell us about this ritual? Do pre-fight massages actually benefit fighters?\nThe Science Behind the Pre-fight Massage\nMax Muay Thai Stadium in Pattaya, Thailand by Matt Lucas\nWhen fighters are massaged, bio-mechanical changes happen quickly. The vigorous rubbing of the fighter’s body increases the fighter’s range of motion, speeds up blood flow, and raises the temperature of the fighter’s muscles. All of which warmup a fighter before the Muay Thai fight. This explains why, in Thailand, pre-fight massages are done in place of pre-fight pad work, the preferred warmup method for most Western fighters. And this also explains what Peth Tho refers to as the waking up of the fighter’s muscles. But pre-fight massages affect the fighter’s body in other ways too.\nWhen a fighter is massaged before a fight, the fighter’s heart rate and blood pressure may drop as well, causing the fighter to feel more physically relaxed. You might think that feeling too relaxed would be a bad thing, especially before for a Muay Thai fight. But the Thais value being relaxed. You won’t see too many Thais wasting energy before a fight. After preparing for their fight, they calmly sit waiting for their turn. Sometimes next to their opponent. Sometimes next to the ring. And once they’re in the ring, everything they do leading up to the fight is an extension of that calmness, from performing the wai kru to removing the mongkon.\nIf a fighter’s body is relaxed, it’s only a matter of time before his mind is going to relax too. Psychologically, pre-fight massages reduce anxiety and mentally relax a fighter. Brett Hlavacek, professional Muay Thai fighter from New York, says pre-fight massages are a form of meditation. The activity helps the fighter escape the chaos of the pre-fight staging area, especially in the West.\nThe Importance of Pre-fight Massages with Thai Oil\nBut for a pre-fight massage to have the most effect, a trainer can’t kneed the skin and underlying muscles of a fighter that is bone dry. This is where Thai boxing oils come into play. When fighters get their pre-fight massage, their cornermen often douse the fighter’s body with Thai oil until the fighter is soaked in the slippery yellow liquid.\nFor five or ten minutes, trainers and fellow fighters systematically attack each part of the fighter’s body, vigorously massaging the Thai boxing oil into the legs, arms, chest, neck, back, and stomach. They clamp their hands around the limbs of the fighter and push forward and pull backward on the muscles. On the back and chest, they use their palms to circle the boxing oil into the fighter’s muscles. When massaging the fighter’s stomach, they put their hands together like they’re doing CPR, and drop most of their weight down on the fighter’s midsection. Afterward, they’ll stretch the fighter’s arms and legs.\nSome fighters, like recently retired Muay Thai pugilist Dorian Price, say their is a balance to the pre-fight massage, a balance that swings between being helpful and hurtful. But the Thais, who have been squeezing out pre-fight massages since the Golden Era, seem to have perfected the touch. They know how to apply just enough pressure just enough to keep a fighter limber, but not too much as to cause any damage.\nBy John Wolcott\nJohn is a freelance writer and MAX Muay Thai ringside commentator living in Thailand. Originally from New Jersey, in 2014 he left a 15-year long career in corporate America for a more adventurous life in Thailand.\nAdvocating for Pre-fight Massages with Thai Oil in America\nIn the US, though, the use of boxing liniment in Muay Thai is still prohibited in many places. This is because the rules that govern what happens before a Muay Thai fight were adapted from combat sports like Western Boxing or amateur wrestling. Many athletic directors come from these combat sport backgrounds and may not understand the benefits of using Thai oil before a Muay Thai fight.\nAthletic directors, for example, don't allow wrestlers to use anything on their skin before or during competition. There have been other times when boxers have used products similar to Thai oil, but if an official smells or sees something they’re not familiar with, it could cause issues for the boxer and their team.\nVideo of Mike Tyson having a Warmup Cream Applied before a Fight\nIn some cases, state athletic commissions let approved Muay Thai sanctioning bodies set the rules for the sport. Unfortunately, though, sometimes the officials at events are from the athletic commissions and are unfamiliar with Thai oil, thus prohibiting a fighter from using it.\nIn 2011, Muay Thai saw a leap forward a few years ago when the Association of Boxing Commissions and Combat Sports (ABC) approved the use of Thai Oil in Muay Thai under the Approved Unified Rules of Muay Thai.\nAthlon Rub Founder presenting on Thai Oil - 2013 Annual Conf. Association of Boxing Commissions and Combat Sports (ABC)\nChaired by Nick Lembo, the ABC developed these rules after several years of work. The expert panel consisted of John E. Baker, Vladimir Borodine, Mick Doyle, Clint Heyliger, Samasek Kanthawong, Coban Lookchaomaesaithong, Tony Moore, Tony Myers, Siraphop Ratanasuban, Jr., Kaensak Sor Ploenchit, Stephen Strotmeyer, Manuel R. Taningco, and Kumron Vaitayanon.\nAlthough many state athletic commissions choose to adopt the ABC Unified Approved Rules of Muay Thai, or those of the Muay Thai sanctioning bodies they have approved in the state, some state athletic commissions like Nevada choose to follow their own rules.\nAfter years of lobbying the Nevada Athletic Commission (NAC) and having them vet everything submitted to them, the NAC said they would test the use of Thai oil only on the upper torso of fighters. But right before testing, they decided to test Thai oil only on the lower part of the body. The tests were only supposed to last a few months, but since the start of testing, it’s been seven years to date.\nAnd the end result? It’s still confusing. Although the NAC listed the approval of Thai oil on their Q&A page, they failed to list it on their rules page, making it confusing for fighters and their team, as well as NAC officials, to understand the NAC’s stance on the use of Thai oil. Ideally, the goal is to have the approved use of Thai Oil before competition explicitly written within all governing body rules to avoid any ambiguity or confusion backstage. Now more than ever with the inclusion of the Muay Thai into the World Games and the recent IOC recognition, it is important to ensure the sport of Muay Thai retains its identity with minimum to no alterations\nInstruction Video - Pre-Fight Massage with Thai Oil\nPre-fight massages are a staple in Muay Thai throughout Thailand. And Thai boxing oil is a staple of the pre-fight massage. For decades, the Thais have been using the pre-fight massage and Thai oil to help usher fighters into the ring in a state of both mental and physical preparation. Is it time we start taking this practice more serious in the West?\nAthlon Rub - Founder // www.AthlonRub.com","10 Muay Thai Safety Tips for Beginners\nTraining Muay Thai can be extremely risky. An accident can happen anytime especially in clinching and sparring. That’s why you need to know some basic tips to prevent the unnecessary accident.\n1. Warm up and stretching before and after training\nWarm up and stretching for at least 15 minutes before and after training. It helps blood circulation system, increasing heart-rate and relax your key muscle and tendon, which will reduce the chance of getting a cramp or tearing muscles.\n2. Stay hydrate\nYou will sweat a lot so stay hydrate all the time (before, during and after the session). Dehydration causes low blood pressure, brain malfunction, seizure, and faint. The best way is to sip the water continuously, not drink the whole cup spontaneously which will cause colic during the training.\n3. Stop training immediately if you feel any pain\nIf you feel any pain in any part of your body, stop training and tell your trainer immediately to get first-aid treatment then go to see the doctor later. Do not continue training even it just a small pain because it can get worse if you don’t treat the injury correctly.\n4. Look for the first-aid kit and ice pack\nLook for the first-aid kit in the gym and also ask the trainer where to get the ice pack. The ice pack can be used as a first-aid treatment to reduce pain and bruises within 48 hours, after that you will need a warm water instead.\n5. Don’t rush to come back training after injury\nDon’t rush to come back training after injury, wait until you completely heal. It so easy to get the same injury and it will be even worse which is not worth it.\n6. Always wear a mouth-guard and head-guard\nAlways wear a mouth-guard and head-guard, especially when sparring because Muay Thai has a lot of impact moves. It will keep you safe from a concussion, dislocating jaw and a broken tooth.\n7. Trim your toe and hand nails.\nTrim your toe and hand nails. Dust and germs can easily stick to longer nails, especially Muay Thai training mostly with bare feet. Trim your nails to keep yourself clean and prevent accidentally scratching your sparring partner.\n8. Tie your hair before training\nIf you have a long hair, please tie your hair before training. It can distract your training and sweep on your friend’s face which is very annoying.\n9. Dress right and wear the perfectly fit equipment\nDress right and wear the perfectly fit equipment. It’s not ok to wear shoes in the training are if it’s not your gym shoes. Always wear shorts that you can kick easy. The gloves and head-guard are not too tight or too big for your palm or it might fall down or cause an injury.\n10. Stay clean and sanitized before and after training\nStay clean and sanitized before and after training. Always clean your training clothes so they won’t smell when you come to the gym. Wear some deodorant before training, wash your feet and take a shower after training. No-one wants to train with smelly partners."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:aebff902-5982-48e4-9ee6-e679e3373b5d>","<urn:uuid:c81facea-8e62-4844-a15c-3f494475489e>"],"error":null}
{"question":"What are the historical challenges of transmitting visual content over long distances, and what modern security measures are recommended for remote data transmission?","answer":"Historically, transmitting visual content faced significant technical challenges, as evidenced by the moon landing broadcast which required a special 'slow-scan' narrowband TV camera operating at 10 frames per second to save bandwidth, resulting in poor picture quality that had to be re-recorded and redistributed. Today's remote transmission challenges focus on security, requiring multiple protective measures including strong password hygiene with multi-character combinations, two-step authentication, email encryption, antivirus software, web filtering, and firewalls to protect data being transmitted across networks.","context":["It was, after all, only a boot-crunching dust. You wouldn’t think the sight would affect so many or change so much.\nBut we saw it, on TV. We saw it and were changed, along with the human world around us.\nLike many readers, I was sitting in a room with my family July 20, 1969, when Neil Armstrong hopped off that ladder onto the moon. I nearly missed it, plumb forgot, but then I happened into the living room, where my parents and huge assortment of sibs were sitting in a half-circle, watching.\nThere had to be TV. The United States wanted to show the world proof we’d beaten the Russians in the space race. More, it was a TV age: Many of us had watched Lee Harvey Oswald gunned down live in 1963, watched a president’s funeral, watched “Our World” in 1967, the first live global television link, featuring, among many other things, a studio Beatles performance from London of “All You Need Is Love.” We believed TV shrank the world and put it in our pockets.\n(And that is a measure of what’s changed. Back then, “the power of TV” was praised all the time, and we could feel it, when Beatles played and men moonwalked live. Today, that power is a given, unremarked, invisible.)\nWhen the TV picture snapped to the audiovisual link from the moon, it was hard to see at first. Shadow silhouetted Armstrong, behind him the black blank of space, slashed with the arc of the lunar horizon. The contrast settings were wrong; on some TVs you could barely see Armstrong. It was surreal; we had to adjust, to “make it out.”\nWhy was the picture so poor? It was a picture of a picture. The TV camera on the Apollo 11 Lunar Module was a “slow-scan,” narrowband TV camera, used in early space exploration because it saved bandwidth and could be transmitted over voice channels. Problem: At a pokey 10 frames a second, it wasn’t easily compatible with broadcast TV.\nSo a broadcast-TV camera was pointed at a 10-inch direct monitor. That signal, lossy and noisy, was encoded, processed, sent to Houston, and thence to the world.\n“We’re really seeing it,” arose in the room, either Mom or … or one of my eight brothers and sisters. That was the point: Bad picture or not, it was enough to realize we were seeing it, to connect. That live image reached 600 million people that evening. I can’t have been the only one with my scalp prickling. My family’s certainly were not the only smiles in the world, realizing history in front of us.\nSo the picture was garbled, and Armstrong’s rehearsed line, “One small step for (a) man, one giant leap for mankind,” was garbled. What wasn’t was the reception in each mind watching. If you want to be moved, look up some of the pictures of the TV-watching crowds across the world at that moment, some small, some large, just … gazing.\n(Michael Collins, orbiting the moon, in the Columbia command spacecraft, had no TV. The capsule communicator in Houston quipped, “I guess you’re about the only person that doesn’t have TV coverage of the scene.”)\nThat was, and remains, among the longest-ever direct communications links among human beings, vaulting 239,000 miles in seconds, ping-ponging as described above, and reaching so many minds. It remains a metaphor for the amazing things people can do. It came at a troubled time: When I think of it, the troubles come back to mind. And so does the astonishment. Armstrong descending is an image of what’s possible.\nA very few people don’t think it ever happened. And, to be sure, with war abroad and social upheaval across the land, this country had enough work on earth without busying itself in the heavens. But it signaled a change in the world, a different consciousness of our planet and ourselves. It is a communal memory, too, shared by this race of gifted creatures, video, vivid, telling and retelling.\nJohn Timpane is a staff writer for the Philadelphia Inquirer. Readers may send him email at firstname.lastname@example.org.","Everyone's working remotely these days, yet security risks remain. Here are 10 ways you can combat online security threats.\nShare this article\nThere can’t be many businesses today that don’t use remote working to some extent throughout the working day. Even those without a culture or need to offer remote working will have employees or directors taking work home, or working from hotel suites, conference venues and public transport at times.\nThis more casual form of remote working, one that may not be accounted for when analysing how business IT networks are used, is often missed in cyber security policies and procedures. However, it is one important factor that can put organisations at risk of cyber attacks and data breaches.\nRemote working, whether a formalised arrangement between a business and an employee, or an ad hoc ‘needs must’ requirement to get work done, can leave your business IT network, systems and devices vulnerable.\nThe first step for managing security and remote workers is to understand where your business is at risk. This should be followed with an awareness raising campaign within the organisation so that all employees understand how their actions may compromise security and what steps they must take to protect company networks and systems.\nCyber security policies need to include the specific risks associated with remote working, with procedures and guidance in place for working away from the office. This will also need to explain what actions need to take place if a remote worker believes they have exposed the company to a cyber attack, and any disciplinary measures that may be taken.\nThe following top tips provide an excellent starting point:\n1. Keep mobile devices and laptops safe\nLost and stolen mobile devices and laptops are easy pickings for cyber criminals if insufficient security measures are in place. The first line of defence is to look after these business assets: keep them with you and in sight at all times, and never leave them in hotel safes, cars etc.\nNext up is securing the devices themselves with good password hygiene and encryption on laptops. Finally, installing mobile device management apps such as AirWatch and MaaS360 give employees a chance of securing and recovering lost mobiles or tablets.\nRemember: it's not just cyber crime that can disrupt your IT\n2. Excellent password hygiene\nStrong passwords will not only protect your devices and systems being accessed if a mobile or laptop is lost or stolen, they also protect businesses from hackers. Good password hygiene includes using long passwords with multi-characters, two-step authentication processes, and unique passwords for different systems and logins.\n3. Ensure up-to-date security protection is in place\nAny devices that are owned by the organisation should be properly protected with antivirus, web filtering, firewalls, device encryption and other preventative software, but so too should your employees’ own devices if they are using them for remote working.\nThis can be a difficult area to negotiate as your employee may feel this impinges on the personal use of their device: Your cyber security policies will need to address issues like these, either restricting staff from using their own devices for certain business critical activities, providing secure company owned devices, or making your cyber security protection mandatory.\n4. Use of public wifi\nPublic wifi can be vulnerable to malicious attack, presenting issues for those employees who may need to work from a hotel or conference. While it is good advice to only connect to trusted networks this is not always feasible.\nTherefore, your remote working / cyber security policy should stipulate that employees should not use public wifi for any sensitive, business critical activities. It is advisable to draw up some guidelines that explain what systems and activities staff can and cannot access when using public wifi.\n'Quick question Dave: where's all our money gone?\n5. Email encryption and best practice\nEmail is perhaps the most used digital technology by staff members who are away from the office, and one that can open a backdoor to cyber criminals. Encryption and robust management of corporate email is a must.\nThe installation of applications such as Mimecast is a no brainer, but raising awareness of the vulnerabilities of email will also help embed best practice in your organisation. This can include training in spotting cyber threats like phishing emails, and also policies on what information should not be communicated in an email – for example logins and passwords.\n6. Using public computers\nWhile the majority of people will have their own laptop or mobile device that they use for remote working, occasionally someone may need to use a public computer such as in a business suite in an airport.\nEmployees should be aware of the security implications of this and adhere to the following guidance: keep screens private (position them away from other people), don’t use public computers for any sensitive information, use ‘private browsing’ where possible, never use ‘remember me’ or ‘save information’, and clear your browsing history and delete any downloads before closing the browser.\n7. Using devices when out and about\nEmployees should also be aware of physical threats when using devices when in public places like cafes, hotels, airports etc. Just as you would hide your PIN when using an ATM, employees should be discreet when keying in passwords and logging into systems.\nThey should also be aware of the risk of snooping and eavesdropping, not just online, but also from other people in the vicinity. Can someone see and potentially grab a discreet photo of company sensitive information while they work in a public space?\n8. Removable devices\nUSB sticks and other removable devices can be a source of malware and should be checked first. Many conferences hand out USB sticks that may be infected, often unbeknown to the organisers. Also don’t allow anyone to plug in a USB device into your computer, for example to share information in a meeting. Always get your IT department to security check removable devices.\n9. Monitoring and policy enforcement\nTwenty four-seven network monitoring and security will help your organisation identify threats and monitor users on your networks. Remote workers and their mobile devices can be monitored using this solution to protect your organisation’s network.\n10. Negligence and accidental risks in the home\nEven when your employees are working from home using your secure VPN, VDI or remote desktop, there can be other risks that need to be considered. Children and pets can be a surprising threat.\nCats have a habit of jumping on computer keyboards and inquisitive minds might press a few keys when a laptop is unattended. These kinds of risks should be addressed in your remote working / security policies to ensure that your staff take every feasible step to protect your systems at all times."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:555c8585-c40d-41b7-a02f-c370c9ffe2c3>","<urn:uuid:02598588-2ea6-4379-b2e0-6b47df581446>"],"error":null}
{"question":"I'm studying printing technologies evolution. Which came first historically - screen printing with its paper stencils or 3D printing with plastic extrusion? And how did each technology initially develop?","answer":"Screen printing emerged earlier, with paper stencil techniques being used in Philadelphia around 1973 by companies like AMPRO. They used blank screens with paper stencils placed under the screen to print on shirts and sweatshirts. Meanwhile, 3D printing was developed later, in the early 1980s, initially using plastic extrusion through nozzles to build objects layer by layer. The early 3D printers were very expensive and mainly used by large corporations for prototyping parts. Both technologies have evolved significantly - screen printing moved from paper stencils to liquid emulsion and Rubylith film, while 3D printing expanded from single-color plastic to multiple colors and even different materials like chocolate.","context":["Ask Nick: Applications for 3-D printing are endless\n- Is 3-D printing perfect%3F No. But there are some pretty exciting things in development.\n- 3-D printing has gone from printing in one color to two colors and now even using chocolate.\nQuestion: What is this 3-D printing I keep hearing about? What is it for?\nAnswer: In regular computer printing — the kind with which we're all familiar — you basically take an image on the screen of a document you've created, and recreate that image on the paper in the printer. Essentially, you're just reproducing the thing you see on the screen, onto the paper. That's 2-D (two dimensional) printing. 3-D (three dimensional) printing is pretty much the same idea, but adds that third dimension so that you print a solid, physical object — something that you can hold in your hand, like a statue — rather than just an image on a piece of paper.\nAs you can imagine, you can't just use a word processor to create the image that you print. You need a specialized software program called a Computer Aided Design (or CAD) package to design the image on your computer screen. When these things were first designed, in the early 1980s, both the CAD programs and the actual 3-D printers themselves were very expensive. They were mostly used by large corporations to design and prototype new parts. Once those prototypes were known to work properly, the parts were then mass manufactured by traditional manufacturing methods.\nThe 3-D model in the computer can also be created by using a 3-D scanner to scan an object that already exists. This is not tremendously accurate, and often requires a lot of touching up before the model can be printed.\nOnce that 3-D model is in the computer, it can be sent to the 3-D printer. There are a lot of different types of 3-D printers. The most common ones (including the one on the International Space Station!) use a certain type of plastic heated and forced (extruded) through a nozzle to lay down a thin layer of plastic in the right shape. It then goes back and does the next layer, and the next, and the next, until a solid 3-D thing is built up. Others use a similar build-up process, but use lasers to melt metal instead of plastic. There's a Chinese company that has developed a 3-D printer that uses what they're calling \"liquid metal,\" which sounds a little too Terminator for my taste.\n3-D printing is starting to come down in cost to the point where small companies and many individuals are buying them. There are a lot of companies making 3-D printers that cost less then $2,000. Some even cost less than $1,000. Companies like MakerBot (http://www.makerbot.com/), Cubify (http://cubify.com/cube), and even Dremel (https://3dprinter.dremel.com/) offer printers that you can buy directly from them, from places like Amazon, and some are even available from Home Depot.\nWhat can you make from them? Well, pretty much anything you can imagine. Toys, replacement game pieces, rings — whatever. If you're not terribly good with a CAD program yet, you can always download pre-created designs from places like Shapeways (http://www.shapeways.com/) or Thingiverse (http://www.thingiverse.com/). You'll find some pretty clever things being designed by some pretty clever people.\n3-D printing isn't perfect. It's slow, you're limited as to the size of the finished piece, and you may have to do a bit of sanding and smoothing of the finished product, but remember, these are early days yet. The printers get better with every iteration, and I think we'll be seeing some pretty exciting things. We started out with single plastic extruders, so everything you printed was just one color (of course, you could paint it). Now there are two-color printers, and even 3-D printers that print using chocolate!\nIf a 3-D printer is a little rich for your blood, you can dip your toe in with a 3-D pen. 3doodler (http://the3doodler.com/), LIX (http://lixpen.com/), and CreoPop (http://www.creopop.com/) are just a few of the companies making them. 3doodler has it right; they're more for doodling in 3-D than for making serious, usable things, but they're an awful lot of fun!\nAt the opposite end of the spectrum are the companies making cars (http://3dprint.com/53532/chinese-3d-printed-car/) and houses (http://www.businessinsider.com/3d-printed-houses-construction-industry-neighborhoods-2015-3) with 3-D printers. There's a lot of potential in 3-D printing for our future.\nIf you want to see 3-D printing in action, look into Rochester's own Hackerspace, Interlock Rochester (http://www.interlockroc.org/). This is a dynamic, interesting, eclectic group of people — do yourself a favor, and check it out.","The History of Screen Printing\nBy: Steph Shea\nPushing ink through a screen is the most oversimplified way to explain screen printing. Before digital artwork was in common use, there were many ingenious methods developed.\nIn Philadelphia, screen printing was also known as Process Lettering, and in 1973, Walt Huddell, Gary Huddell, and Geoff Traub developed an incredibly efficient way of process lettering; they used blank screens and paper stencils that they would lay UNDER the screen as they printed shirts and sweatshirts, and thus was born American Process Lettering—later to become AMPRO. The initial process used a cut paper stencil that would be sent down the conveyor belt of their homemade dryer alongside the t-shirt with wet ink on it. The process was an improvement in efficiency, especially for individual numbering, but still incredibly labor-intensive compared to more modern methods.\nFrom there, printing evolved into a more traditional method where the silk screen has a liquid emulsion dried onto it (think of it like a wax) and then an image is burned into the emulsion using a film positive made of an arcane material called Rubylith film. “Ruby” as it was called, eventually evolved into a film positive which was printed on a digital printer with a specialized toner. But in the beginning, full color images were often printed in a 4-color process using painstakingly hand-cut and layered ruby positives. Our artist, John Cosgrove, hand-cut our ruby for over 25 years before the first of many digital revolutions. He is best known for his photographic wildlife prints developed under Zooper Sportswear; he made beautiful full color images of tree frogs, tropical fish, and whales- all made with hand cut film positives.\nAs the art and pre-production processes were evolving, the equipment for the industry was also becoming more modern. Originally, our printing rigs were hand-made with angle iron and spring-loaded hinges. Those rigs were lovingly named “Junkyard Dogs,” and they still pull a pretty mean 1-color print to this day. However, as the company and industry was growing, an equipment manufacturer called Precision developed an automatic machine called the Multiprinter. Known for it’s dramatic screen indexing movement, those machines were incredibly productive, durable, and designed to print many colors in perfect registration. From there, M&R Inc. developed pneumatic, industrial multicolor presses that overtook the Precision equipment because of their deep understanding of the art of printing. The settings and configuration capabilities helped printing evolve from relatively crude stencils to fine art.\nThe biggest change to the industry in the ‘80s was a legal one. Prior to 1981, it was legal to print college logos on apparel and sell them. Crewneck sweatshirts were fashionable, and the apparel industry was selling beautiful, high quality, locally sourced sweatshirts with the names of local schools printed on them. However, once collegiate licensing was conceived, the apparel industry was reigned in and college sweatshirts were made through a small handful of channels, leaving smaller printers to history. The apparel industry was still growing like crazy, so the onset of licensing was only a minor blow to larger domestic printers.\nOnce the United States had a flourishing apparel industry replete with large-volume screenprinting shops, fabric mills, cotton farms and\nclothing manufacturers, and then NAFTA was signed in 1993. Though globalization may have been\ninevitable, NAFTA was a particularly cruel piece of legislation for many manufacturers. Apparel printing and manufacturing is\nextremely labor intensive. Many companies packed up their equipment and set up\nshop in Mexico, decimating the apparel producing towns throughout the East\nCoast. As the apparel manufacturers\nmoved, so did the printing. It started\nin Mexico and gravitated further south as more trade deals were signed,\neventually settling in Honduras, Mexico, El Salvador and Nicaragua.\nBut… the domestic printing industry adapted, shrank and\npersisted. Then, Abercrombie developed a\nbusiness model that was favorable to printers. They imported their blank shirts\nbut didn’t commit to the decorating until they extensively tested each design\nat their flagship stores and through their catalogs. Pioneering a new business model, their nimble\napproach to fashion (before the phrase “fast fashion” was coined) blew their\ncompetitors out of the water. Their print\npreferences were simple and athletic, but preppy and vintage at the same\ntime. The quality of their shirts was outstanding;\n7 oz ringspun cotton fabric constructed with a back yoke and double needle\neverything. Their dyes were beautifully\ndone; their shirts were laundered after printing, and they developed\nproprietary techniques for distressing their prints, which you would never\nexpect to find in the United States.\nThey taught the entire domestic apparel industry how to really “vintage”\na print through art, meshes, inks, and washing.\nThe industry went from making their prints as bright and indestructible\nas possible to making them look like they had already been washed hundreds of times.\nThe next big earth-shattering disruption for domestic\nprinters was the re-introduction of polyester.\nJust when we thought we had put polyester clothing behind us, too! Under Armour rocked the boat with the\nintroduction of mass-market performance apparel. They had developed some proprietary fabrics that\ncompressed muscles, kept athletes warm, and wicked away sweat. While these garments were great for athletes,\nthey were very rough for printers.\nPolyester fabrics are notorious for bleeding; the dyestuffs sit on top\nof the (plastic) polyester fibers and can release into the air in a dryer. That doesn’t sound so terrible, except that\nthe dyes release up through the inks we print with, turning them pink, blue or\nsome other “off” color. Performance\nfabrics were challenging, but polyester was also being blended back into\neverything. Triblends, biblends, and CVC\nfabrics came onto the scene, and while they are dreamy fabrics off the shelf,\nthey don’t tolerate printing or washing that well. Rayon also made its way into our t-shirts (now\ncalled Triblends) and since Rayon is a substitute for silk, it changed the\nlongevity of the shirts. They pill\nfaster than cotton/poly blends, and can’t withstand the heat needed to print\nAs polyester was working its way back into our wardrobes, digital\nprinting and Web companies really started to put the “Old School” printers out\nto pasture. Struggling to adapt to the\nchanging client expectations and struggling to compete with innovative\nweb-driven companies that know how to sell online (but may not consider\nthemselves fine art printers), many printers have been swallowed up by venture\ncapital firms. There are precious few\nlarge-volume independent screenprinting shops left- most have been purchased by\nlarger companies who excel at converting business through the Internet.\nThe newest technique\nfor apparel decorating is digital printing also called “direct to garment\nprinting.” Also known as DTG, this new method is beginning to displacing\ntraditional screenprinting, but there are some significant hurdles before digital\nprinting equals screenprinting in speed and quality.\nWhy is screenprinting\nan art, and how does digital printing compare?\nDigital printing is an emerging technology that may soon\ndisplace screenprinting… but digital printing is rigid and slow. Traditional domestic screenprinting presses\nhave controls and settings that would rival a Boeing 747. Digital printers have far fewer adjustable\nfeatures- the ink has to be thin enough to pass through the print head, so\nnuanced printing with different color opacities to create dimension is impossible. This may be the future of the industry, but\nit cannot rival traditional screenprinting… yet.\nScreenprinting, on the other hand, is an artform that can be\nmanipulated and adjusted from the art department through to the print\ntechnician. Dozens of small decisions\nare made before the first shirt gets printed, starting with the color\nseparations. To screenprint a\nmulti-color design, the artwork has to be separated by each color for each\nindividual silkscreen. When a human artist can influence the color separation\nprocess, they achieve the spirit of the design better than an automated digital\nprinter ever could. Aside from choosing\ncolors, they are making an extraordinary number of tiny decisions to influence\nthe final product in ways that computers could not understand.\nStarting in the art department, the design is evaluated\nholistically alongside the garment to be sure we can render the print to the\nclient’s expectations. The use of the\ngarment is also taken into consideration; if a product will be used outdoors,\nor with heavy commercial washing, we protect the print using a variety of\nmethods. The design is then color\nseparated using a combination of technology and manual manipulation. Color choices, color order, and flashing (lightly\ndrying the print to “tacky” between silk screens on press) are specified and\nprepared for production.\nFrom the artist, the design is handed over to “pre-press”\nwhere the technician decides what type of machine the product should be printed\non and hand-picks each mesh count for every color. Most screenprints require an “underbase,”\nwhich typically prints in a slightly lower (finer) mesh than the rest of the\ncolors. “Mesh” is a reference to how big\nthe holes are in the silkscreen itself- bigger mesh holes allow for thicker ink\ndeposits. Finer/higher mesh counts can\nonly allow thinner, more transparent inks to pass through. High meshes are ideal for printing halftone\nimages, blending colors, and vintage printing.\nThe higher the mesh, the softer the print. Print softness is always a high priority,\nthough some garments require a slightly heavier ink deposit to protect the\nprint from bleeding. Many printers,\nAMPRO included, have proprietary ink softening techniques that work in tandem\nwith the mesh count decisions made by Pre-press. Careful selection of meshes ensures the best\npossible reproduction of the art.\nAfter the silkscreens are made by prepress, the ink\ndepartment begins its selection process.\nAs with the other areas of pre-production, inks are critical to the\nquality of the final product. Most\nlarge-scale print shops blend their inks in house, giving them total control\nover the viscosity (thickness) and accuracy of the colors. Across most industries, Pantone Color Systems\nare used for matching color; by standardizing the colors from shop to shop, a\ncompany can safely print products at several shops with matching results. Pantone color books are even installed into\nAdobe Photoshop and Illustrator so that artists can specify the correct Pantone\nink color in the digital file. Choosing\nink colors is more nuanced than people may realize. Pantone standardized the colors, but that\ndoes not mean that all ink/Pantone colors print alike. Ink shades that are bright or fluorescent do\nnot withstand the rigors of the printing process as well as high-opacity colors\nlike white. White has a very high pigment\nload, as do many shades of grey, so they perform differently on a print run\nthan fluorescent pink. Fluorescent\ncolors often need to be dried on press or printed last so they don’t lift off\nwhen the next color prints on top. Depending\non the way a piece of art prints, it’s sometimes necessary to choose a\ndarker/lighter Pantone shade so that the end result matches the requested\ncolor. Some inks are “shaped” prior to\nprinting to make them thicker or thinner depending on the purpose of the color\nin the art and the endurance needed on the print run. Ink color, order and viscosity influence the\nart at least as much as the color separations and mesh counts.\nequipment utilizes only thin inks of the same viscosity, so it’s not possible\nto take advantage of all the amazing things that can be done to ink before it\ngoes into the screens. Hybridized\nDigital Printing, a combination of regular screen printing and digital\nprinting, has the potential to overcome some of the ink transparency issues-\neven allowing for the use of spot colors like Fluorescent highlights, metallic\nhighlights, or reflective areas.\nThe art all comes together out on the production floor; the\nscreens are married up with the inks and the goods, and the printers get to dig\nin. Printers have a variety of choices\nand adjustments that allow them to take a group of screens and inks and render\nit into ART.\nScreenprinting variables and adjustments:\nSqueegee selection is the first step; technicians\nchoose the durometer of each squeegee based on the color going into the\nscreen. For “hard to print” colors like\nFluorescents and process inks, they typically use triple durometer squeegees\nthat have a soft printing rubber on the face, but a stiffer middle layer that\ngives the squeegee enough rigidity to push the ink through the screen.\nSqueegee angles are set at the same time the\nsqueegees and floodbars are set in. Controlling\nthe angle gives them another way to control the (ink deposit) opacity of each\nindividual color. More angle, more ink;\nless angle, less ink. If the squeegee is\nstanding straight up and down, it will skim the image in the silk and produce a\nsoft film of ink.\nPressure on the squeegee is often the first\nadjustment that is made to each print head as the design is set up and is crucial\nto success. Similar to angle, the more\npressure, the more ink will be pushed through the screen. Applying pressure\nthat is too heavy pressure can cause the\nlifting off other wet inks, “pancaking” the print, distorting\nregistration, leaving marks on the fabric outside of the print area, or losing\ntransparencies and dimension in the art.\nRegistration is another key component to\nrendering the “art” of a print. In this\none area, digital printing typically has the advantage, but that does not\ncompensate for its other limitations. Domestic apparel screenprinters have been\nsuccessfully registering full color images for more than half a century. Registration is the process where all the\nscreens are lined up to make a print, and each color has to be perfectly\naligned with all the other colors to make this possible. Innovations in the industry, such as Tri-loc has\ngreatly improved the registration process over the past 25 years.\nOnce the screens are registered and the\nsqueegees are set, the speed of the print head is set, color by color. In a production environment, you want your\nprint heads to print at their maximum possible speed- but the overall quality\nand dimension of the print are the top priority. Setting your head speed faster will keep the\nsqueegee’s contact with other wet ink colors to a bare minimum though some inks\nneed slower head speeds to physically “clear” or push through the screen.\nOff-contact is also adjusted along with the head\nspeed. The purpose of off-contact is to get the screen to peel and snap off the\nwet ink it has just printed. Too much\noff-contact can distort registration, whereas not enough off-contact can\nflatten the image and make the inks mottle.\nTemperatures are also a huge factor in printing;\nthe fabric is printed between the machine pallet (the t-shirt is loaded with a light\nadhesive onto a pallet so that it remains stable while moving from one print station\nto another) and the screen. The pallet\nitself retains warmth from the flash cure units, which apply a bright burst of\nheat to lightly dry the inks while printing.\nPallets must always be warmed prior to production so that printing\nresults are consistent from the first printed t-shirt to the last. Inks also warm up and thin during the\nprocess, so inks have to be managed carefully to keep the print looking\nconsistent. Printing technicians make\nconstant adjustments to head speeds and pressures to accommodate the\ntemperature changes in pallets and inks.\nPrinters are artisans who wield a tremendous amount of influence on the\nfinal result. Printers can understand\nthe “spirit of the art” and what the product is trying to achieve more\nintuitively than a software program attached to a bunch of print heads. Perfectly composed designs require a human touch\nwhen they are reproduced, which is why preserving the art of domestic\nscreenprinting is so vitally important.\nDigital printing is the Kraft Mac & Cheese of our industry; it’s\nvibrant, it tastes good, it’s quick and easy- but it doesn’t compare to the mac\n& cheese our moms made. While digital\nand hybridized digital garment printing techniques are important innovations in\nthe domestic apparel market, digital printing is highly limited and should be\napproached with a degree of cynicism for full color, photographic or fine art\nprinting. Precious few high-volume\nindependent screenprinting operations are left in the United States; most have\nbeen relocated to Mexico or further South- but we are committed to our\nUnderstanding the differences between digital apparel\nprinting technologies and screenprinting should shape the discussion about\npreserving this art form. On the\nsurface, digital printing is able to reproduce at a higher resolution than a\nscreenprint, but the lack of control over softness, opacity, dimension, and\ncolor cannot be overstated. Picture Bob\nRoss at his canvas, starting his background… what if he had to print his\nbackground strokes at 100% opacity before he started on his happy little\ntrees? I can almost picture these\nbeautiful Liquidtex acrylics- Burnt Sienna, Cerulean Blue, Cadmium Red getting\nslathered onto the canvas at full opacity; it would be difficult to bring forth\ndimension in a landscape using every ink fully opaque.\nEven after painstakingly extolling the virtues of domestic\nscreenprinting, we haven’t even delved into the sentimental value of t-shirts,\nand their significance to us personally!\nT-shirts are the comfort food of our wardrobe as well as a vehicle to\ndeclare love for a favorite team, school, musician, or (God help us) political\npreference. Making art on t-shirts is\nthe best business in the world, so let’s preserve our art and keep the domestic\napparel business vibrant and robust!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:d9dc751f-7596-488b-830d-0c8a9a83f091>","<urn:uuid:8fbfc235-d946-46fd-8c1e-207593fb551e>"],"error":null}
{"question":"whats difference between storage of roquefort cheese vs wine - temp and darkness requirements?","answer":"Roquefort cheese and wine have distinct storage requirements. Roquefort cheese requires natural, fresh, humid cellars with a constant temperature of 7-8°C (about 45°F). For wine, while 55°F is ideal, the key is maintaining constant temperature rather than achieving a specific number. Both products need darkness - Roquefort cheese matures in dark caves with natural ventilation through rock cracks, while wine must be protected from bright light and sunlight which can damage it during aging. Wine can be stored in simple dark places like closets or basements, while Roquefort specifically requires the caves of the Combalou mountain.","context":["Roquefort cheese: Introduction.\nRoquefort cheese is the most consumed cheese in France after Comté, it is also one of the most renowned of the blue cheeses along with Gorgonzola and Stilton. Roquefort cheese melts fully under the palate and leaves a light salty taste and a very pleasant taste of mould. It's character is very typical of that of a great French cheese.\nThe ripening of Roquefort cheese lasts for a duration of at least 3 months in the natural, fresh, humid cellars of the mountain of \"Cambalou.\" The cracks on the rocks serve as natural ventilation. Roquefort cheese is formed in a cylindrical shape of 20cm diametre and 9cm long. It weighs between 2.5 and 2.9kg. The penicillium roqueforti, the famous mould, can only be found in the caves of Roquefort. The mould lives in the ground and contains the property which ferments the cheese.\nIn hand-made production, rye bread and wheat are stored, and after 6 weeks, the mould is collected and pricked in to the the Roquefort cheese. This is done with the use of a needle. After 4 weeks, the cheeses are enveloped in aluminum foil to avoid parasites.\nExterior appearance of Roquefort cheese: clean rind\nInterior appearance of Roquefort cheese: evenly veined\nFeel of Roquefort cheese: firm without hardness\nTaste of Roquefort cheese: pronounced ovine flavour\nHistory of Roquefort cheese\nRoquefort cheese is probably the cheese with the richest history. In the 9th book of his Natural History series, already Pliny the Elder cites the cheese as \"cheese of paus des Gabales,\" at the same time he cites another cheese of Gévaudan. In his account, he wrote that the monk of Saint-Gall, who was the secretary to Charlemagne towards the end of his life, recounted an anecdote according to which, during the return home from the campaign he had led against the Saracens, the emperor stopped at the monastery of Vabres, where he was served a cheese of which he removed the green with the end of his knife, when the abbot told him \" Sire, you are removing the best part.\" It can only have been Roquefort cheese that he was eating.\nIn the year 1070, a donation at the monastery of Conques mentions among the revenues, two cheeses supplied from each of the cellars of Roquefort. However, it is not until 1411 that Charles VI acknowledges to the inhabitants of Roquefort the monopoly of the maturing process \" like is was practiced from a time immemorial in the caves of this village.\" Later, many royal charters confirm this monopoly and give to the inhabitants many priviliges of jurisdiction, franchises and rights to asylum. The rulings of the court of the parliament of Toulouse protects the cheese against imitations and confirms the rights accorded for the protection of \"this strong rock\" where \"maturing\" of the cheeses takes place.\nThe term of origin, defined by the law of July 1925, is protected by the international framework as outlined by the Stresa convention, signed the 1st June 1951. The countries who have not signed nevertheless must respect the term \"Roquefort cheese\" the mark having been registered in the national institutes of these countries. It is not the place that the production takes place that is protected by the term of origin, but the place of maturing. For centuries, the production of ewe's milk destined for the fabrication of Roquefort was limited to the limestone plateaux around the village, a territory where, to cite a charter from the 15th century \" neither grows vines nor wheat.\" The milk is therefore collected from the other regions of Rouergue (Basin of Camarès, mountains of Lacaune, causse Comtal, Lévezou) and to those adjacent, the Lozère, the Gard, Tarn and Aude.\nFinally it iwas in the 1950s that the departments of the Pyrénées-Atlantiques and Corsica came to join the \"range\" of regions from which the milk is collected. As already specified by a ruling of the parliament of Languadoc in 1666, the pasture should be so, that it is the same food as in the Causses. Dairy production of Roquefort reaches its maximum in April and May, each ewe giving an average of 100 to 150 litres of milk by \"campaign,\" and around 4.5litres of milk is needed to obtain 1kg of Roquefort. When made by a cheese-maker, it is obligatory for the cheese to be matured at the \"rock of Combalou,\" a zone in which the limits are fixed by a decision of the justice in the application of the law of the term of origin .\nIn this region of Karstic relief, the limestone plateaux are in effect ripped apart by a terrain of cavities and galleries of which the inhabitants of Roquefort have made good use of by creating the caves for maturing the cheese. Dispersed into several stacked levels (of which the number varies from 4 to 11) which are ventillated by naturally formed chimneys which can be up to 100m long (the fleurines), this makes an immense pool producing cold air and a chemical-biological filter. In effect a particular cycle of condensation and evaporation is maintained all year round at a constant temperature of 7 to 8 °C. As for the fine mist which saturates the air, this is the environment where the Penicillium roqeuforti is developed naturally, this is the fungus from which Roquefort gets its beautiful blue veins.","Many people think that if they’re going to store wine at home then they need a cellar. But the word “cellar” conjures up images of dark, cavernous chambers cut out of bedrock, or slick, temperature and humidity-controlled rooms lined with mahogany wine racks. All very nice, but not at all necessary. We recommend you interpret “cellar” somewhat loosely.\nThere are four main things to consider when storing wine: temperature, light, vibration and keeping the cork wet.\nTemperature: Both red and white wine likes to be kept cool. 55°F is ideal, but more important than this magic number is that the temperature doesn’t fluctuate. Better a constant 65°F than 40° one day and 80° the next.\nLight: Bright light and sunlight can damage wine as it ages in bottle, so the darker the room, the better. Total darkness is easily achieved by simply closing the lid of the case or the closet door.\nVibration: Areas subject to heavy foot traffic (or vacuum cleaners) should be avoided as wine, unlike martinis, should be neither shaken nor stirred.\nKeep the cork wet: Laying your bottles down on their sides keeps the wine in contact with the cork, which in turn prevents the cork from drying out. Dry corks contract, allowing air to pass into the wine and wine to leak out. If air gets in, it renders the wine dull and lifeless and it will taste more like old sherry than wine.\nIf you keep these basic requirements in mind, you’ll find it remarkably easy to find a place to store your wine, and you won’t need a cellar at all. A corner of the basement, a closet in a spare bedroom, your shipping box or the cupboard under the stairs will all do nicely. And remember, the longer you plan to store your wine, the more important these factors become. If a newly-purchased wine is to be drunk in a day or two, it really doesn’t matter too much where you keep it, but if the wine is to be kept for weeks or months then find it a nice cool, dark spot.\nNow, some wines require not months but many years, even decades, of bottle aging before they’re ready to drink. This is a small percentage of all the wines made, but nonetheless, it is an important one. Where you store these high quality (and often expensive) wines designed for long aging takes on a special importance if your investment is to be protected. In this case you may want to consider one of the commercially available wine storage units, which come in a variety of sizes and finishes. Another alternative is off-site storage, where you rent a locker in a temperature and humidity-controlled wine storage facility. This option is great for wines that you don’t plan to drink for some years and has the added advantage of being out of reach; a real bonus during those weak moments.\nAs your collection of wine grows you’ll need to keep track of it. An old-fashioned cellar book where you record each new wine that goes into your cellar and cross them off as you take them out, works just fine. These days, however, there are also numerous cellar software programs that make it easy and fun to manage your wine collection."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:29b4f8fe-2a1a-4bda-883e-c8dac888a42d>","<urn:uuid:894311c3-40f9-481e-9b9e-3a9ed34c8672>"],"error":null}
{"question":"What role does optimization strategy selection play in handling both function evaluation constraints and non-differentiable loss functions?","answer":"Optimization strategy selection is crucial for handling both challenges. For function evaluation constraints, strategies include using approximation models with evolution control, where only certain individuals or generations are evaluated with the original fitness function. For non-differentiable loss functions, options include evolutionary algorithms that don't require gradients, or adapting reinforcement learning approaches like Actor-Critic methods for supervised learning. The Actor-Critic adaptation uses two networks - an actor network that makes predictions and a critic network that evaluates performance - allowing training without requiring differentiable loss functions while managing computational resources.","context":["Satisfying the function evaluation constraints in Evolutionary Algorithms\nFunction evaluation will mean both Objective function evaluation and Constraint function evaluation\nIn a recent work for the IEEE CEC 2010 competition on Constrained Real parameter optimization problems, we (along with Prof. Deb and Ritu ) coupled an idea from the Gradient projection (GP) method for handling equality constraints with our Genetic Algorithms (GA) based algorithm for finding the global optima of the problems. The approach worked wonderfully well for us. There was a problem, however. There were limits on the maximum number of function evaluations and our algorithm did not perform well for some of the problems, with this constraint! (As if the actual constraints were not enough!! :D). So what we did was, instead of applying the GP method every generation, we started applying it only after some generations. We achieved decent results with this within the specified max. number of function evaluations. In every generation, we applied the GP to every individual. There could be a form of evolution control  in play here, which we didn’t explore during the course of our work because of time constraints.\nThis is a common problem when you are working with EAs such as a GA. Since there is a population of solutions being processed at each generation, the function evaluation count goes up every generation. Besides the huge number of function evaluations, sometimes in complex problems, the time taken for each evaluation is so high that parallel methods have to adopted in order to solve them. (In a very recent playing-around, the resident CUDA guru here, just parallelized the objective function evaluation in the Innovization guru’s code and they achieved a huge speedup).\nUsing approximation methods: Since parallelization is not always the best option, one way to avoid expensive function evaluations (or comply to the function evaluation limits – parallelization won’t help here ofcourse) is to use approximation models (or meta-model). That is, using a set of true data (samples) build up a model, and then use this model to obtain the approximation to an unknown input sample, thus saving the actual function evaluation(s). The important consideration, however is how true the approximation model, also called the fidelity of the model. If the model is not a very good approximation, then the EA might only be able to return a sub-optimal result which is not what we want.\nFollowing are some papers I recently read and will be useful to anyone looking to investigate this field:\n- Yaochu Jin , Markus Olhofer , Bernhard Sendhoff, “A Framework for Evolutionary Optimization With Approximate Fitness Functions“\n- El-Beltagy, M.A. and Keane, A.J. (1999), “Metamodeling Techniques For Evolutionary Optimization of Computationally Expensive Problems : Promises and Limitations”\n- Y. Jin. “A comprehensive survey of ﬁtness approximation in evolutionary computation.”\n- Won, Ray, Tai, “A Framework for Optimization Using Approximate Functions”\nSome notes on  above:\n- individual and generation based evolution control– evolution proceeds based on fitness evaluations using not only the approximate fitness model but also the original fitness function\n- interactive evolutionary computation and usage of approximate models therein\n- use in multi-modal test function optimization: smoothen the rough fitness function, and thus prevent stagnation in a local optimum\n- Two approaches of evolution control: (1) Every generation, only certain individuals are evaluated using the original fitness function (2) All individuals only every N generations per M generations are evaluated- determination of the number of individuals in (1) and generations in (2) has been answered via simulations\n- frequency at which the original function is called and the approximate model is updated should be determined by the local fidelity of the approximate model.The lower the fidelity, more frequently the original function should be called. *local fidelity model* is stressed\nI hope to experiment with approximation models for the work I mentioned at the beginning. The challenge would be to find the perfect evolution control strategy.","Breaking the Gradient: Supervised Learning with Non-Differentiable Loss Functions\nZian (Andy) Wang\nSupervised learning is a fundamental machine learning task involving training a model to make predictions based on labeled data. The process typically starts with inputting data into the model and using it to generate predictions. These predictions are then compared against the ground-truth data through a function that quantifies the prediction error, which we refer to as the loss/objective function. To improve the model's performance, we need to update its parameters. We then take the gradient of the loss function with respect to the network's parameters, which allows us to determine the direction of parameter update that will \"nudge\" the model towards the global minima. This prediction, evaluation, and update cycle is repeated until the model converges. The above process only generalizes to Deep Learning approaches since not all classical machine learning models follow the same training scheme.\nAside from the model, the next crucial component in the training process is the loss function. In supervised learning, the loss function assumes the role of an \"optimizee.\" At the same time, the optimizer, an algorithm outlining how parameter updates are executed, optimizes the loss function through its computed gradients. The gradient of the loss function can be seen as a tool that \"sculpts\" out the loss landscape; without it, the model would be no better off than a blind man traveling tens of thousands of mountain ranges because the model cannot even determine which way is up or down!\nThe chosen loss function must therefore be differentiable and accurately reflect the model's performance. However, in the ever-so-intricate world of today, not every problem can be described and optimized by a differentiable function, and not every carefully crafted objective function is differentiable.\nThe Problem With Most Loss Functions\nTo illustrate, in the scenario of biomedical image segmentation, the usual dice loss and Jaccard Index (IoU) might not suffice. The model is often required to identify small objects in a complex medical image, which can span hundreds or even thousands of pixels in resolution. Using only the amount of overlap between the predicted mask and the ground truth (using the dice loss) to measure error can be problematic. This is because it doesn't give the model any feedback about how close its predictions are to the location of the ground truth when there is no overlap between the pixels. Other cases in which the loss function may be non-differentiable are when the model's predictions are used as an intermediate stage in a pipeline that generates labels. For example, the model may predict specific characteristics of a 3D structure, but the actual labels can only be generated once the model predictions are fed into third-party rendering software. In such cases, the loss function cannot be calculated directly as the network cannot \"back propagate\" through the third-party software.\nThe truth is, there are many more real-world situations where there are no concrete roadblocks preventing engineers from crafting a differentiable loss function, but rather that the function needs to articulate more information to the model for it to improve and converge.\nGenetic and Evolutionary Based Algorithms\nSearching for no-gradient optimization techniques is one easy way to train Deep Learning models with non-differentiable loss functions. Due to the absence of backpropagation and gradient computation in traditional evolutionary and genetic algorithms, networks with non-differentiable loss functions can be trained. These optimization techniques typically treat the loss function or optimization objective as a \"black box,\" and they \"tune\" the model based on the feedback they get from the \"black box environment.\" These algorithms are heavily inspired by the evolutionary process of nature or the behavior of specific organisms. Particle Swarm Optimization, Genetic Algorithm, and Ant Colony Optimization are a few of the well-liked options. These algorithms rely on fascinatingly simple rules and guidelines to adjust parameters and eventually reach convergence iteratively. Although discussing these algorithms' inner workings is beyond this article's scope, most evolutionary/genetic algorithms can be adapted to train neural networks. One clear advantage of no-gradient optimization training schemes is that they are less likely to become \"stuck\" in local minimums than conventional gradient-based optimizers because they modify the model based on environmental feedback rather than mathematically minimizing a function.\nHowever, in some cases, the drawbacks of these algorithms outweigh their advantages. Notably, evolutionary algorithms are proven to take absurdly long periods to train compared to standard Backpropagation. This characteristic arises from the fact that their training mechanisms rely more on the notion of \"trial and error\" rather than mathematically calculating the optimal parameters. Another caveat to these approaches is that they usually don't have out-of-the-box implementation that can work with neural networks nor ready-to-use libraries that can utilize GPUs to speed up the training process.\nSuppose you're not interested in writing algorithms from scratch and speeding it up manually by writing low-level code for GPUs. In that case, you might look for methods that can be implemented in the already existing popular Deep Learning frameworks. We then might turn back to the concepts used by evolutionary algorithms–treating the loss function as a black box environment–but \"modernize it\" with Deep Learning. For those that are familiar, this sounds a lot like Reinforcement Learning. Specifically, we can try to adopt the Actor-Critic method for Supervised learning.\nAdapting Actor-Critic to Supervised Learning\nReinforcement learning is a subfield of machine learning where an agent is trained to interact with an environment by performing actions based on policies to maximize a reward signal. The agent learns by adjusting its policies based on the feedback it receives from the environment. The ultimate goal is to learn a policy that maximizes the expected cumulative reward over time. Reinforcement Learning is commonly employed in areas where a series of dependent and sequential actions are required to optimize the problem (self-driving cars, games, etc.), unlike supervised learning, which only has an input and a prediction.\nTo illustrate, imagine a game of chess where you are the \"agent\". At each turn, you can make a move, and each action's result will grant you a reward. The \"environment\" would be your opponent, whether a computer or a human. The environment would respond to your moves by moving its pieces. For instance, you could gain a reward of +1 point for each move you make, while an action that results in your opponent capturing one of your pieces would cost you -5 points.\nAdditionally, you could receive a reward of +5 points for capturing one of your opponent's pieces and +10 points for achieving a check. The design of the reward function can be as complicated as you'd want, and different reward functions may encourage agents to take different strategies under the same environment. Now, there are different approaches to optimizing an agent's policy, and our interest in this article lies in the Actor-Critic method.\nTo its name, the Actor-Critic method consists of two function approximators, or just neural networks, with one taking on the role of an \"actor\" while the other being the \"critic\" of the actor. The actor network approximates the policy: it receives the agent's current state as input and outputs an action. The critic network approximates the action-value function–an indicator of how \"good\" the action the actor performs.\nThe action-value function maps a state-action pair to the expected cumulative reward that the agent can obtain by taking that action in that state and then following a given policy. The action-value function discounts future rewards by multiplying them by a factor known as the discount rate. This factor determines how much weight is given to future rewards compared to immediate rewards.\nThe actor network seeks to maximize the action-value function that the critic network represents. The action from the state-action pair input to the critic network is generated by the policy or the actor network. The actor network then backpropagates through the critic network based on the value it produces. On the other hand, the critic network aims to predict the cumulative reward more accurately based on the current reward received and an estimation of future rewards. The details of Actor-Critic are much more than what is presented here, but these concepts can inspire us to adapt it for supervised learning.\nSupervised learning simplifies matters a lot since, without the need to account for future rewards or the past mistakes that the model may have made, Actor-Critic is readily adaptable to supervised learning. Instead of estimating the cumulative return from the future, we can let the critic network predict the current reward or the value of our loss function. The objectives of the actor and critic network will then be the following.\nIn practice, the actor and critic networks can be updated alternately. Specifically, the \"Actor-Critic for supervised learning\" approach can be outlined as follows.\nInitialize the actor and critic networks.\nInput the features to the actor network, and record its predictions.\nInput the predictions and features to the critic network, and record the \"goodness\" that it outputs.\nCompare the output of the critic network with the actual value obtained by the loss function, and train the critic network using any regression-based loss.\nTrain the actor network by maximizing the output of the critic network and, in other words, using the critic network output as the actor network's loss value.\nRepeat steps 2 to 5 for the desired number of iterations/epochs.\nRemember that this approach is not limited to only outputting \"discrete actions\" like a typical Actor-Critic algorithm. In fact, the actor network can be any predictor as long as the critic network can accept the output format of the actor network. We can also easily adapt the above outline to batched training by storing states, actions predicted by the actor network, values predicted by the critic network, and the actual loss value in a \"memory\" and only update the networks when the number of samples seen is greater than or equal to the batch size.\nImplications and Potentiality\nAdopting concepts from Reinforcement Learning for complex supervised learning problems presents an exciting array of possibilities. Although Actor-Critic is not necessarily the optimal algorithm for Reinforcement Learning, the adaptations we made to apply it to supervised learning can be utilized to modify other RL algorithms, such as Proximal Policy Optimization, a usually much superior choice over Actor-Critic. Although a relatively less popular area and consideration of recent times, the idea of training networks without needing a differentiable loss function has been explored, whether directly or indirectly.\nThe paper \"Learning to Learn: Meta-Critic Networks for Sample Efficient Learning\" by Flood Sung et al. proposes a framework based on Actor-Critic methods for meta-learning in supervised and semi-supervised learning environments. Furthermore, the exciting forward-forward algorithm explored by Gregory Hilton recently completely throws the notion of \"loss functions,\" as we know it, out of the window. The algorithm trains each layer of the network individually. By more closely resembling the activities of biological neurons rather than mathematically optimizing a loss function, forward-forward can pipeline sequential data through a neural network without storing neural activities or propagating error derivatives.\nAs modern ML problems grow in complexity, the demand for flexible and adaptable algorithms beyond Backpropagation will increase. Hilton notes in the paper on the Forward-Forward algorithm, \"backpropagation remains implausible despite considerable effort to invent ways in which it could be implemented by real neurons,\" and the backpropagation algorithm lacks biological relevance. The advancement of efficient optimization frameworks for neural networks may be geared towards methods similar to Reinforcement Learning, where the learning environment and strategy are similar to the actions performed by us or imitating how biological systems and neurons propagate their learning through our brains, like the forward-forward algorithm."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:1a13f6ef-062b-4861-88e7-6a265f9a21e7>","<urn:uuid:fdddec7f-4266-4c96-a99a-b4999a3d4ce5>"],"error":null}
{"question":"¿Qué dificultades técnicas presentan el Little Pamir Trek y el Everest Three High Pass Trek para los excursionistas? ⛰️","answer":"The Everest Three High Pass Trek is described as very demanding, requiring significant hours of continuous walking at high elevation, with risks of altitude sickness above 3,500 meters and is not recommended for people with heart problems, lung issues, serious asthma, high blood pressure, or acute diabetes. The Little Pamir Trek presents challenges such as wading through icy rivers and crossing difficult terrain, with the documents mentioning situations where even yaks have trouble crossing certain rivers and dealing with snowy conditions.","context":["1. the last place where you can buy food for camping is Ishkashim. Even better to get your supplies in Dushanbe (more choice).\n2. Bring enough dollars, there are no cash machines...\n3. I brought along an extra tent for the guide and yak man. That gave us more freedom to camp anywhere rather then having to go to a settlement.\n4. Dress respectfully (no skin and women should cover up the hair)\nLittle Pamir Trek\n- Distance: 182km\n- Total altitude difference: 4750 metres\n- Highest point: 4880 metres\n- Highest camp: 4600 metres\nSarhad-e-Broghil (3260m)-Baharaq (3510m)\nTowards the Dalriz Pass\nBaharaq (3510m) - Langar (3630m)\n↑ 400 metres\n↓ 280 metres\nThe easy way...\nLangar (3630m)-Khachigosh (4080m)\n↑ 450 metres\nApproaching the Little Pamir Plateau\nKhachigosh (4080m) - Chaqmaqtin Lake (4030m)\nWading through icy rivers\nChaqmaqtin Lake (4030m)\nGreat things to do around the like is visiting one of the Kyrgyz settlements. Long strolls along the shores of Chaqmatin is another thing to do. And what about exploring one of the side valleys to see the hanging glaciers from up close!\nThe stunning mountains of the Hindu Kush\nChaqmaqtin Lake (4030m) - Khachigosh (4080m)\nKachigosh (4080m) - Showr (4300m)\nOur yak reluctant to venture the snowy conditions\nShowr (4300) - Barnoz (4400)\nStruggling up the 4880m high Varm Pass\nBarnoz (4400m) - Dara River (3985m)\nCrossing this river is not as easy for the yak...\nOur yak man Safdar warming up\nDara Bik (4600m) - Sarhad (3250m)\nDara Bik plateau\nMura Pass Trek\n- Distance: 58km\n- Total altitude up: 2825m\n- Total altitude down: 1460m\n- Highest point: 3780m\n- Highest camp: 3100m\nThe beckoning mountains that can be seen from Tajikistan's capital Dushanbe scream to be explored. From the hot plains south of the Hissar Range the trek follows the fertile banks of the Karatag River. The path follows the raging whitewater river through juniper woodland and alpine meadows. The higher up the Mura Pass you get, the more rugged the surroundings become.\nEnd of the tar road – Hakimi\nCamp before Mura Pass at 3100 metres\nView on the other side of the Mura Pass\nDukdon Pass Trek\nThe Dukdon Pass Trek is a more cultural trek, passing through numerous mountain villages and summer settlements. It's the best walk in this book to get a closer look into Tajik mountain life. After the Zurmech Pass you descend into a lively valley and you are likely to be invited in for a few cups of tea, yoghurt and bread. Once down this valley you will follow the Archamaiden river all the way up towards the foot of the Dukdon Pass. At times this wild river plunges through steep, narrow gorges while a few kilometres later it reveals breathtaking views of the glaciated peaks of the Dukdon Range. The approach to the Dukdon Pass is gentle, though partly covered in snow. The descent towards Sarytag is challenging, with steep scree sections. Arriving in Sarytag you will think back on the generousity of the locals with a big smile on your face.\nChukurak Lake enroute to Zurmech Pass\nView from the Zurmech Pass towards the Archmaiden Valley\nLocals having fun\nCamp spot with Dukdon Pass in the distance!\nOn the Dukdon Pass!! (3810m)\nThis is short, but possibly the most technical trek in the guidebook. And it has all to do with the north side of the Kaznok Pass. With 4040m it's quite high, but this is not the crux. The steep, icy slopes on the north side require crampons and maybe even an axe. The walk itself is a north-south traverse of the Fann Mountains. North of the Kaznok Pass you will camp at the brilliant Alaudin and Mutnyi Lakes. The rocky and dry landscape is high alpine terrain, surrounded by numerous 5000m+ peaks. The south side is green and lush with beautiful pastures and juniper forests.\nThe last sun rays of the day light up the Chimtarga summit\nThe approach to the Kaznok Pass (we were doing the route north bound, much more difficult!)\nAlong Kaznok River\nTotal altitude difference: 3145 metres\nHighest point: 3775 metres\nHighest camp: 2850 metres\nArtush Village (1770m) – Kulikalon Lake (2850m)\nKulikalon Lake (2850m) – Alaudin Lake (2780m) via Laudan Pass (3640m)\nCamping at Alaudin Lake\nAlaudin Lake (2780m) – Artush Village (1770m) via Alaudin Pass (3775m)\nDonkeys struggling up Alaudin Pass\n1st day and what an approach!\nCampfire and a single malt whisky, life is sweet\nThe Tabaspin Pass within reach\nCrossing one of the side rivers\nOverview of the Direct Route with left the Yagnob crossing\nCrossing snowbridge, High Route\nChilling out at a fab campsite\nView of the Zamin Karor Massif\nWhere a mucky river meets the Ghund River, Bachor\nSmall tent, big mountains\n5000m+ peaks all around\nPicture perfect Uch Kul, including some local wildlife\nThe last translucent mountain lake before the final descent\nThe last hurdle: big moraines\nThe bridge, the gateway back to Bachor\nLangar – Plateau at base Pik Engels\n13km (one way)\n5 hours (one way)\nApproach Pik Engels\nAdmiring Pik Engels","Everest Three High Pass Trek\nThere are numerous treks in Everest area. Most people try Everest Base camp trek and that is one of the best trek in Everest area. Besides the base camp if you are making three high passes you get an incredible view of the whole area. Three high pass trekking is challenging and adventurous too. This demanding trek gives you opportunity to observe many of the 8000m peak including Choyu, Lhotse and Makalu besides Everest. This is an ideal trek to observe the Everest from different angles. It totally depends on the weather condition but it does not need any technical climbing.\nEverest three high pass trek at a glance\nMaximum elevation: 5,550 meter\nNumber of Days: 16 days of walking which include two days for acclimatization\nDifficulty level: Strenuous or demanding, walking in high altitude for significant days\nAccommodation: Tea house during the trek and hotel in Kathmandu\nTransportation: Private transfer and by flights\nPrice: $1680 per person\nBest time to go: March- May and September-November\nHighlights of Everest Three High Pass Trek\nThe most adventurous trek of Everest area which take to the three important and high pass of the area, namely Renjo-La (5,465 m.), Cho-La (5,420 m.) and Kongma-La (5,5535 m.),\nSuperb view of Everest from Gokyo-Ri and Kalapathhar with the view from high passes also,\nAchieve the thrilling adventure while crossing the all high passes in the area,\nA scenic flight to and fro to Lukla and landing at the most thrilling airport of the world,\nGives opportunity to discover series of glacial lakes which are normally seems turquoise blue in color due to reflection of light and the lake is important even for the aspect of biodiversity because of migrating bird during summer,\nA day to discover the world famous monuments in Kathmandu,\nAn extra contingency day or free day after the trip if weather matters, accommodation included for that day also and the day can be utilized to do shopping, to take rest or optional sightseeing,\nGives opportunity to observe Sherpa culture and visit one of the biggest monasteries of the area, Tyangboche monastery,\nThe best view of Everest from different angles and perspectives,\nCan sight musk deer, Himalayan Thar, Nepal’s national bird pheasant, and national flower rhododendron during the trek\nWhy I have to do Everest Three High Pass trek?\nThe Everest Three High pass Trek is only complete trek that covers all the attraction of Everest area. The major attraction of this trek is not only the high passes of Renjo-La, Cho-la and Kongma-La but the best view points of Everest like Kalapathhar and Gokyo-Ri are included in the trip. The combined view of Gokyo lakes and Everest range from top of Renjo-La is an unforgettable experience for everybody who reaches Renjo-La.\nAll “must do” in the Khumbu region like Gokyo lake, Kalapathhar, Namche Bazar, Everest Base Camp, Tyangboche monastery and Chhukung a gateway to island peak is included in the itinerary.\nKongma-La pass offers the best view of Mt. Amadbalam, Mt. Thamserku and Mt. Kangtenga.\nDifficulty level of the Everest Three High Pass Trek\nThe Everest Three High Pass Trek is very demanding trek and one must be very fit to complete the trek successfully. Previous exposure to higher elevation is very beneficial. The trek needs significant hours of continuous walking in the higher elevation especially during the days in which the high passes are included. One can suffer from acute mountain sickness or high altitude sickness even from 3,500 meters above sea level. If you develop a symptom of high altitude sickness you stop for few days or go lower elevation and when you find yourself in good condition you can continue. Rescue helicopters are available; make sure your insurance will cover helicopter and medical expense after that. We have 90% success rate for Everest Three High Pass trek. Our motive is to complete your trips successfully; we are not scammers of the helicopters. We do not recommend Everest Three High Pass trek for the people with heart problems, lungs problem, serious asthma, high blood pressure and acute diabetes. Even this trek is not good for pregnant trekkers. Access your medical history and consult your doctor before choosing to do Everest Three High Pass trek."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:c2dfcb7b-fe2c-4b5f-a885-1def2caa3014>","<urn:uuid:36797c45-4ac5-49a9-9e6d-404a30abfb85>"],"error":null}
{"question":"What are the key differences between Gas Chromatography (GC) and High Pressure Liquid Chromatography (HPLC) in terms of their separation mechanisms and applications?","answer":"Gas Chromatography (GC) and High Pressure Liquid Chromatography (HPLC) differ in their separation mechanisms and applications. GC uses helium or a nonreactive gas as the carrier in the mobile phase and is specifically used to test compounds that can vaporize without decomposition. It helps laboratories test chemical purity, separate specific mixtures, and identify compounds. In contrast, HPLC uses a pressurized liquid solvent to separate high boiling point mixtures. It works by passing the sample through a column filled with solid adsorbent material, where different components interact differently with the adsorbent, leading to separation. HPLC is particularly useful for analyzing pharmaceuticals, organic acids, industrial chemicals, cosmetics, and food products.","context":["Autosamplers: What Role Do They Perform in Gas Chromatography?\nPosted On April 28, 2021\nChromatography is a logical interaction comprising of numerous research facility procedures used to isolate a blend. A blend is first broken down in a liquid or gas and afterward conveyed to another material to make a fixed stage. This interaction isolates the combination by working with a particular speed of movement alongside different elements. Chromatography might be utilized as a refinement strategy or for estimating analyte extents in a combination. An analyte is the genuine substance being submitted to the partition cycle. The thought is to make one piece of the combination fixed or stable and the other portable to cause partition. An autosampler is frequently utilized in these lab settings to build exactness during the example inclusion part of the partition cycle. This instrument embeds the example into the contraption gulfs during a test for expanded effectiveness just as simpler multiplication. It has become the most well-known strategy for finishing different kinds of chromatography methods.\nLogical Instruments Involved With Compound Separation Lab Techniques\nGas chromatography (GC) is quite possibly the most well-known applications for testing logical instruments. It is utilized to test any compound fit for disintegrating without going through a deterioration interaction. GC can help research centers test substance immaculateness, separate explicit blends, recognize compounds, and make an unadulterated adaptation from an answer. Helium or a nonreactive gas goes about as the transporter in the versatile stage though a fluid inside a section is available for the fixed stage. The gas compound responds with the fixed stage material inside the segment making the mixtures elute at various maintenance times. Uniquely planned instruments permit researchers to look at the maintenance times for logical purposes.\ngas chromatography is a scientific instrument used to help with complex example compound partition. It utilizes the segment to send the example through a gas stream at a particular pace to make an ideal collaboration with the pre-owned segment filling during the fixed stage. The fixed stage works with detachment of the dissolvable mixtures with every one leaving the section at different maintenance times. Gas stream rate, temperature, and segment length might be modified to change the detachment request or maintenance time. A predetermined gas or fluid volume is embedded into the segment either physically or naturally with uniquely planned hardware. Movement is made by particle absorption which straightforwardly decides the pace of atom movement. The fixed stage materials, absorption strength, and particles types influence movement.\nAn autosampler is an instrument used to accomplish addition into the section naturally. The addition point is frequently alluded to as a delta and can be as a split, split less, on-segment, cleanse and-trap, gas exchanging valve, or PTV injector. Manual inclusion leaves space for blunder which can bargain the test outcomes and furthermore takes substantially more time. Most labs today utilize programmed infusion to acquire more dependable outcomes just as streamline testing time. Samplers are grouped dependent on the example limit, scientific reason, or the sort of innovation. They are utilized for different partition measures including the estimation of unstable natural mixtures. Programmed inspecting frameworks have enormously improved the interaction of gas chromatography just as different types of compound division.","The CASL Service Center offers a full range of chemical testing services for academia and industry. With capabilities that cover a wide range of instrumental and classical wet chemistry techniques, CASL’s services provide solutions to problems aimed at raw materials testing, chemical impurities, solvent testing and more.\nRoutine tests performed at CASL include:\n- Environmental assessment of effluent streams, air quality and water quality\n- Identification of chemicals or contaminants in products for quality control\n- Thermo-analysis of wood, thin films, foams, gels, surfactants, biopolymers\n- Rheometric measurements of stress and strain in solid polymers\n- Dynamic mechanical analysis of the viscoelastic properties of materials\n- Klason lignin analyses of pulps and other lignocellulosic samples\n- Carbohydrate compositional analyses of pulps and other polysaccharides-based materials\n- Quality control of paper products\n- Molecular weight distribution determinations\nThe Chemical Analysis and Spectroscopy Laboratory is located on NC State University’s main campus at:\n431 Dan Allen Drive\nRoom 3228 Pulp and Paper Labs\nRaleigh, NC 27695\nPulp and Paper Labs – Campus Location Map\nFacilities ID: 113B, 35.782488, -78.678226\nFor inquiries about specific capabilities and available tests or for use of CASL facilities, please contact:\nDr. David Tilotta, Professor and Director\nDr. Jie Liu, Lab manager\nThe instruments and capabilities in CASL currently include the following:\nFourier Transform Infrared Spectrometery (FTIR) – Perkin Elmer Frontier\nDescription: This instrumental technique allows for the detection of functional groups and the characterization of covalent bonding. It can provide information on the molecular structure of your material.\nUse: Common applications of molecular spectroscopy include:\n- Chemicals and Materials: Troubleshoot manufacturing problems; identify product contaminants; analyze fuels; gain deeper insights into the properties of novel and advanced materials.\n- Pharmaceuticals: Analyze product formulations and package coatings; rapidly screen the quality of raw materials, intermediates and formulated products; qualify nutraceuticals.\n- Food: Screen for known and unknown adulterants.\nDescription: UV-Vis is an analytical technique that measures the absorbance of visible or ultraviolet light at a specific wavelength or over a spectral range. By measuring the absorbance of the compound in a solution and calculating its molar extinction coefficient, the concentration of it can be quantified.\nUse: UV-Vis analysis on materials include consumer care products, pigments, coatings, organic conjugates (proteins, DNA, RNA) and metallic ions.\nGas Chromatography (GC) – HP6890 (FID Detector), Agilent6890N (FID Detector), Agilent7820A (Mass Detector)\nDescription: Gas chromatography is an analytical technique used for the determination of organic components that can be volatized. Several detectors are available including a Flame Ionization Detector (FID) that is capable of easily quantifying separated analytes and a mass spectrometer which is useful for identifying components.\nUse: With our expertise in Gas Chromatography Method Development we can analyze a broad range of sample types including volatiles, hydrocarbons, glycols, fatty acids, flavors and fragrances or develop a GC or GC/MS method for your unique analysis needs.\nDescription: Ion chromatography (or ion-exchange chromatography) is a chromatographic process that separates ions and polar molecules based on their affinity to an ion exchanger. It works on almost any kind of charged molecule and the two types of ion chromatography available are anion-exchange and cation-exchange. The water-soluble and charged molecules bind to moieties which are oppositely charged by forming ionic bonds to the insoluble stationary phase. T\nUse: The CarboPac PA1 column, which is a specialized anion-exchange capillary column, can be used to determine a wide pH range from 0 to 14 at all concentrations of buffer salts. It is ideal for the separation of neutral and acidic monosaccharides.\nDescription: Shimadzu has introduced a GPC system designed specifically to provide superior data reliability and ease of use. Gel permeation chromatography is essential in polymer chemistry for measuring the distribution of molecular weights. The Shimadzu GPC consits of an LC-20AD, CTO-20A, SPD-20A (Diode Array Detector), RID-20A (Refractive Index Detector), and LabSolutions GPC Software.\nUse: The technique is often used for the analysis of the molecular weights of polymers dissolved in THF.\nHigh Pressure Liquid Chromatography – Agilent1200 and Agilent1220\nDescription: High-performance liquid chromatography (HPLC, formerly referred to as high-pressure liquid chromatography) is a technique in analytical chemistry used to separate, identify, and quantify components of high boiling point mixtures. It relies on pumps to pass a pressurized liquid solvent containing the sample mixture through a column filled with a solid adsorbent material. Each component in the sample interacts slightly differently with the adsorbent material, causing different flow rates for the different components and leading to the separation of the components as they flow out the column.\nThe Agilent HPLC consists of a pump, autosampler, thermostatted column compartment, and a multiwave/refractive index detector detector that allows for the various analyses of liquid samples.\nUse: The primary use for HPLC is the assay of active ingredients in your product. This analysis provides assurance that the product meets labeled specifications and is useful in monitoring the efficacy of the product over time. With our expertise in HPLC, we can analyze a wide range of sample types including pharmaceuticals, organic acids, industrial chemicals, cosmetics, dietary supplements, food and beverages or develop an HPLC method for your unique analysis needs.\nDescription: TGA is a method of thermal analysis in which changes in physical and chemical properties of materials are measured as a function of increasing temperature (with constant heating rate), or as a function of time (with constant temperature and/or constant mass loss).\nUse: It is an especially useful technique for the study of polymeric materials, including thermoplastics, thermosets, elastomers, composites, plastic films, fibers, coatings and paints. Common applications of TGA include:\n- Materials characterization through analysis of characteristic decomposition patterns\n- Studies of degradation mechanisms and reaction kinetics\n- Determination of organic content in a sample\n- Determination of inorganic (e.g. ash) content in a sample, which may be useful for corroborating predicted material structures or simply used as a chemical analysis.\nDescription: DSC is a thermoanalytical technique in which the difference in the amount of heat required to increase the temperature of a sample and reference is measured as a function of temperature. Both the sample and reference are maintained at nearly the same temperature throughout the experiment. Generally, the temperature program for a DSC analysis is designed such that the sample temperature increases linearly as a function of time. The reference sample should have a well-defined heat capacity over the range of temperatures to be scanned.\nUse: Determination of melting points, glass transition temperatures, crystallinity and crystallization temperatures of primarily polymeric compounds. Isothermic runs can also provide info about kinetics for extrusion experiments.\nDescription: DMA is a technique used to study and characterize material properties as a function of temperature, time, frequency, stress, atmosphere or a combination of these parameters. This technique applies a constant static force to a material and monitors the material change as temperature or time varies.\nUse: DMA is useful for studying the viscoelastic behavior of polymers. A sinusoidal stress is applied and the strain in the material is measured, allowing one to determine the complex modulus. The temperature of the sample or the frequency of the stress are often varied, leading to variations in the complex modulus; this approach can be used to locate the glass transition temperature of the material, as well as to identify transitions corresponding to other molecular motions. Important used include measuring glass transition temperatures and determining the composition of polymers\nDescription: A rheometer is a laboratory device used to measure the manner in which a liquid, suspension or slurry flows in response to applied forces. It is used for those fluids which cannot be defined by a single value of viscosity and therefore require more parameters to be set and measured than is the case for a viscometer. It measures the rheology of the fluid. Cone and plate mode for provides a wide variety of shear rates and viscosity ranges, which can be further extended by the use of interchangeable cone spindles. Different models can be selected to meet the specific range of viscosities and shear rates required.\nUse: The major uses of rheometry include:\n- Acquisition of steady-state flow curves\n- Measuring the impact of particle size, volume fraction and polydispersity on dispersion flow properties\n- Determination of polymer melt rheology\n- Determination of time-dependent flow behavior and the yield of stress dispersions and its relation to Zeta Potential, thixotropy, and structure recovery"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:cb43b661-c9e3-48d2-b813-97915a0ff205>","<urn:uuid:c2bd95b8-01d1-4036-b085-8b5e1861a0ab>"],"error":null}
{"question":"How do medical professionals diagnose insomnia, and what treatment approaches are available at modern healthcare facilities?","answer":"Medical professionals diagnose insomnia through complete medical evaluations, physical examinations, and tools like the Elsworth Sleepiness Scale. They may request a two-week sleep diary tracking sleep patterns, exercise, and substance intake. For treatment, healthcare facilities like Kaiser Permanente offer various approaches. These include medication options (both over-the-counter antihistamines and prescription nonbenzodiazepine hypnotics) and behavioral treatments. Modern facilities particularly emphasize CBTi as the first line of treatment, offering 8-week group therapy sessions that include both in-person and phone consultations.","context":["Insomnia: Sleep Education\nAn estimated 40 million Americans experience some type of sleep disorder each year. Insomnia is the most common sleep disorder that affects people today. Insomnia is characterized by the following symptoms; difficultly falling asleep on a regular basis, regularly waking up several times during the night and not being able to fall back to sleep, and always waking up after a night time rest exhausted and never feeling well rested. Insomnia can last anywhere from one to several nights. More severe cases of insomnia can last anywhere from a month to years.\nThere are three types of insomnia known as transient insomnia, acute insomnia, and chronic insomnia. Transient insomnia is also referred to as short term insomnia. It is characterized by the inability to sleep or sleep well three or more nights in a week for a period of one week. Transient insomnia is often times caused by a temporary situation in a person’s life like; a short term illness or injury, job shift changes, or timing of sleep, or certain medications. Acute insomnia is a longer term insomnia that a person suffers from. This can last from a consistent inability to sleep well for at least three nights a week. The causes of acute insomnia can be the result of environmental factors like noise, extreme temperatures, health problems, side effects from medications, a life stress such as job change, or divorce, and physical or emotional discomfort. Long term insomnia which is often referred to as chronic insomnia is the inability to sleep for at least three nights a week for a period lasting longer than one month. Long term insomnia can causes problems for a person during the day which include lack of energy, trouble with concentration, mental fatigue, and irritability.\nThe most common causes of insomnia are triggered by physiological and physical factors. The physiological causes include; stress, anxiety, and depression. Stressful life events such as the death or illness of a loved one, divorce or job loss can keep the mind active at night making it difficult to fall asleep. Depression can cause chemical imbalances in the brain and can keep the body from relaxing enough to get a good night’s rest. The physical causes include medications, medical conditions, caffeine, and changes in environment. Prescription drugs like antidepressants, heart and blood pressure medications, allergy medications, weight loss products, and many over-the-counter medications contain caffeine and other stimulants which can interfere with sleep. Medical conditions linked with insomnia include arthritis, cancer, heart failure, lung disease, Parkinson’s disease and Alzheimer’s disease. Coffee, tea, alcohol, and other caffeinated drinks are well-known stimulants that can prevent deeper stages of sleep or cause you to wake up in the middle of the night. A change in environment like travel or work shifts can disrupt the body’s circadian rhythms which act as internal clocks and guide your wake-sleep cycle.\nIn the event where a person experiences the symptoms for insomnia, a medical practitioner can run tests to find out if insomnia is the major issue. A complete medical evaluation along with a physical examination must be completed before a diagnosis can be determined. Common causes like snoring or recent weight gain, routine medications, alcohol and drug use, and work/physical activity or all considered in the diagnosis process. A medical practitioner can use tools like the Elsworth Sleepiness Scale, a questionnaire used to analyze daytime sleepiness to assess insomnia. The practitioner can sometimes request a sleep diary for the patient to write down the times they go to bed, fall asleep, awake from sleep, awake in bed, get up in the morning, daily exercise, alcohol, caffeine intake, and medication. The sleep diary is often times requested to analyze the patient’s alertness at various times of the day in a two week period.\nInsomnia can be treated without the use of prescription drugs. There are behavioral methods that can help a person to achieve their goal of getting a well-rested sleep each night. Behavioral methods include stimulus control, cognitive behavioral therapy, and relaxation training. Stimulus control therapy focuses on training a person’s mind to regain the idea that the bed is for sleeping. Stimulus control suggests maintaining a regular wake-time and avoiding naps. Cognitive behavioral therapy focuses on observing and changing negative thoughts about sleep. The goal is to change or correct misconceptions about the ability to fall and stay asleep. Cognitive behavioral therapy focuses on addressing the anxiety around the sleep disorder. Relaxation training focuses on breathing and guided imagery techniques. Muscle relaxation techniques are practiced frequently during this course of therapy.\nThere are non-prescription and prescription medicines for insomnia today. Many over-the-counter sleeping medications use antihistamines, which cause drowsiness. Nonbenzodiazepine Hypnotics, are prescription drugs that bind to a specific receptor in the brain called omega-1 thereby inducing sleep.\nInsomnia is resolved when the underlying issue is removed or corrected. The main treatment for insomnia is directed at finding the cause. Once the root of the insomnia is discovered, the sleeping disorder can be cured.\nHere are links to further your understanding of insomnia:\nFounder & Editor-in-Chief\nDerek Hales is the Founder and Editor-in-Chief of Sleepopolis. Derek has tested over 100 of the most popular online mattresses, and scores of other bedding accessories. His testing approach is grounded in objective criteria and personalized in-depth research. Derek lives in Phoenix, Arizona with his wife Samantha and dog Tibbers.","Cognitive therapy at Kaiser Permanente in the Santa Clara area is helping hundreds of insomniacs learn how to get a good night’s sleep.\nA staggering 10 percent of adults in the U.S. now suffer from chronic insomnia. That’s when we have trouble falling or staying asleep at least 3 days a week for at least 3 months.\nWhile probably most people have the occasional rough night resulting in feeling foggy the following day, chronic insomnia can lead to lack of focus, irritability, and even depression.\n“If you are too tired for too long, it results in lack of productivity, which can result in feelings of helplessness,” explained Janice Raj, PsyD, a psychologist at Kaiser Permanente’s Milpitas Medical Office. “It’s a vicious cycle.”\nFor Kaiser Permanente patients in the Santa Clara area, though, there is help. It comes in the form of a series of Cognitive Behavioral Therapy for Insomnia (CBTi) sessions designed by Dr. Raj and led by her and colleagues at the Santa Clara, Milpitas, Mountain View, and Campbell locations.\nInsomnia Is Common\n“Insomnia is considered the most common sleep-related disorder in the U.S.,” Dr. Raj explained. “The condition is greater in women and older adults.”\nThe commonality as well as her interests led her to help insomniacs.\n“I’m fascinated by the bridge between the mind and the body, or psychology and physiology, and how the two interact with one another,” she said.\nTwo years ago, Dr. Raj began creating CBTi group therapy sessions for patients suffering from chronic insomnia. She first trained by going to a seminar by Colleen E. Carnie, PhD, a leading insomnia researcher. “My protocol is based on her work,” Dr. Raj explained.\nToday the 8-week sessions, which include 10 to 30 patients per group, are facilitated at Kaiser Permanente by Dr. Raj in Milpitas, Patricia Eaton, MFT, in Campbell; Keely Hirsch, PsyD, in Santa Clara; and Sarah Pack, MFT, in Mountain View.\n“I see patients from as far away as Santa Cruz,” Dr. Raj said, explaining that patients are referred by Kaiser Permanente physicians, including primary care physicians, specialists, neurologists, and therapists.\nAdvice from an Expert\nCBTi helps insomniacs identify and address thoughts and behaviors that cause sleep problems and replace them with habits that promote sound sleep. Techniques help target struggles with circadian rhythm, sleep drive, and hyperarousal.\nFive of the 8 sessions cover the participants’ issues, teach them about how sleep and insomnia each work, their goals for their own sleep, and behavior techniques.\nWhen it comes to the latter, Dr. Raj said that a “normal” sleeper thinks of bed as sleep, cozy, warm, and other positives. Someone with insomnia thinks of tossing and turning, frustration, and anxiety. “We are helping them to retrain their brain, so they return to think of bed as a place of sleep and relaxation.”\nThe remaining 3 sessions are on the phone, in order to answer last questions or provide personalized resources to help the individual. Some of the most popular tools her patients use are mindfulness apps such as Headspace, which put the body in a restful space and can be a bridge to connect people to sleep.\nDr. Raj emphasized that people should only go to bed when they are sleepy. “If you’re unable to sleep in 20 minutes, get out of bed. Do not put effort into sleep or your stress mechanisms kick in. Sleep is a biological process. Take pressure off and you end up sleeping.”\n“While sleep medications may be beneficial for certain cases, behavioral techniques can be used now, 5 years later, or 10 years later,” Dr. Raj said of the sessions. “Research shows behavioral change has longer-lasting benefits. CBTI is now considered the first line of treatment by research.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:70222256-e7cc-463a-aa0f-c3c9a2b728f8>","<urn:uuid:85368c6c-4484-49c6-b883-e4f0b647008c>"],"error":null}
{"question":"What was the standard training procedure for torpedo aircraft pilots during World War I?","answer":"Pilots initially trained at Vernon before moving to East Fortune. They practiced drops against T.B.Ds., making their first three or four drops using dummy torpedoes while flying near a drifter whose mast helped judge drop height. A track angle of 90° was determined to be best. Final training involved concerted attacks on a destroyer from the Grand Fleet using live torpedoes set to run under the target. Photographs were used to record drop altitudes.","context":["Torpedo Planes (land-based aeroplanes, floatplanes or seaplanes) were envisioned before the Great War and played an ancillary part in the war itself as small numbers of increasingly capable platforms were fielded and employed. The damage they inflicted in the war was slight, but it presaged the impact this weapon system would carry into World War II.\nThis article is just a stub, to start, and is primarily based upon an extract of a work entitled History of Development of Torpedo Aircraft which appeared in in the Annual Report of the Torpedo School for 1918.\nThe British felt that the first tests were made by Italian Captain Guidoni in 1911, when he released a 352-lb. torpedo from an 80 HP \"Farman\" aeroplane. In America, in 1912 Admiral Fiske advocated using seaplanes to carry torpedoes.\nIn 1913, the torpedo training school H.M.S. Vernon began experiments in conjuction with the Royal Naval Air Service employing a seaplane with sawn-down wings to operate as a hydroplane which could perform high speed drop tests as it skimmed along the surface. Later in the year a 200 HP Sopwith Canton-Unne seaplane carried a 14-in torpedo aloft at Calshot, and in September a torpedo seaplane was specially ordered for the purpose.\nIn mid 1914, a 160 HP Short-Gnome seaplane was experimenting with a 14-in torpedo, and finally a 225 HP Short seaplane carried one weighing 810 lbs. Felixstowe was home to much of the early experimentation.\nMid-1915 marked the Royal Navy's move to 18-in torpedoes for aerial work, as they had ordered two 310 HP Short seaplanes able to lift a 14-in Mark IX torpedo which had been especially developed for the purpose. This weapon weighed 1,000 pounds including its 170 pound T.N.T. warhead.\nDevelopment of the Arm\nThe 1919 Progamme provided for creation of seven Torpedo Aeroplane Squadrons, each of eighteen Sopwith T.1s capable of operating from land of from Eagle or Argus, the latter of which was completed in April 1918.\nInitial pilot training was at Vernon, but this soon moved to East Fortune. The Royal Navy struggled to deliver enough torpedo officers to support the air arm, and eventually the R.A.F. had to train its own men.\nPilots practiced drops against T.B.Ds., their first three or four drops using dummy torpedoes while flying near a drifter whose mast helped them judge a good drop height. It was eventually decided that a track angle of 90° was best. Short range drops were the goal, and when concerted attack was possible, a combined first attack was to be followed by other planes saving their weapons to see if the enemy evaded with a turn. Final training would entail concerted attack on a destroyer lent from the Grand Fleet using real torpedoes set to run under the target. Photographs of the training drops were used to help record the altitude of the drops.\nAttacks in the War\nThe first operational chance to use the weapon was lost in May 1915 during the a German coastal raid (which raid is being referred to here, I have no idea), as the plane was loaded with a bomb instead of its customary torpedo. Success would soon follow in Turkish theatre.\nOn 12 August 1915, a seaplane flown by Flight Commander C. H. Edmonds was launched from Ben-my-Chree to attack a ship that had been sighted in the Sea of Marmora the day before. Edmonds climbed to 800 feet and crossed the Isthmus of Bulair in the face of desultory rifle fire to discover the 2,000 ton steamer off Injeh Bornu. A successful drop was made at 350 yards, striking her amidships, but later review indicated that the ship may have already been abandoned after beaching herself after an earlier attack by the submarine E 14\nOn 17 August, Ben My Chree launched a second raid of two seaplanes to target transports which were resupplying Al Bashi Liman on Gallipoli. Edmonds flew one plane, and Flight Commander G. B. Dacre the other. The isthmus was crossed at 1,000 feet, and the enemy aerodrome at Galata was overflown with the cover of the vanishing night. Flying down the Strait toward the target from the direction of Galata would make the planes seem perhaps German, and put the sun behind them as they struck. Edmonds found three overlapping steamships and dropped his torpedo, hitting the middle vessel. Dacre's machine suffered from poor engine performance which required him to set it down on the Straits before reaching Al Bashi Liman. He spotted a steam tug on the Asiatic shore and puttered over to her, taxiing in to drop his torpedo in the face of rifle fire. Thus unburdened, Dacre was able to coax his machine back into the air for a nape-of-the-earth flight back to his ship.\nUnfortunately, it was deemed that these attacks demonstrated that the climatic conditions off Gallipoli were such that newer machines were required to safely carry out torpedo attacks, and so further efforts were set aside.\n- Annual Report of the Torpedo School, 1918. pp. 89-96."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:18c69bfb-9415-489d-a915-ca167a2cda41>"],"error":null}
{"question":"What are the essential physical training factors needed for peak performance in tae kwon do?","answer":"For optimum peaking, tae kwon do athletes need to specifically train their aerobic and anaerobic capacities, muscle strength, neuromuscular coordination, speed, and recovery. Physical conditioning, especially aerobic capacity, depends on four elements: maximal oxygen consumption, anaerobic threshold, work economy, and recovery.","context":["volume 16 number 3\nAncient documents and archaeological evidence convince us that aromatherapy was developed over 5,000 years ago in ancient Egypt. Its utilization was first recorded by Herodotus around 425 BC. The Greek historian described the use of distillation to extract essential oils from plant-based materials. Later, during the Middle Ages, the pharmaceutical properties of certain aromatic plants were exploited against infectious diseases such as the plague. In the First World War, René-Maurice Gattefossé, a French chemist, used aromatic plants to treat soldiers’ wounds. Since then, aromatherapy has been utilized to ease aches and pains, heal injuries, manage stress and anxiety as well as depression, and restore well-being through relaxation, sedation, and stimulation (Cook & Ernst, 2000; Leach, 2004; Motomura, Sakurai, & Yotsuya, 2001).\nTae kwon do is a full-contact, free-sparring sport in which points are awarded for head contact. Tae kwon do competition consists of three 3-min rounds with a 1-min break between rounds. Most points accumulated determines the winner. There are appropriate methods of training for peak performance in tae kwon do that equip the athlete with the physical endurance needed for continued training and competition—and success.\nChiefly, the tae kwon do athlete’s cardiorespiratory function, energy expenditure, and blood lactate system must be well controlled if peak performance is to be reliably achieved. Peaking, or the ability of an athlete to perform at his or peak capacity when competitions occur, is dependent upon proper physical training of several physiological factors (Hiroyuki et al., 1999). For optimum peaking, tae kwon do athletes should train, specifically, their aerobic and anaerobic capacities, their muscle strength, their neuromuscular coordination, their speed, and their recovery. It is well known that physical conditioning—aerobic capacity in particular—depends on four elements: maximal oxygen consumption, anaerobic threshold, work economy, and recovery (Howley, Bassett, & Welch, 1995).\nIt could be said that the world’s best athletes are always in control. It can be defined as mental control, as their recognition of specific situations within the game and how they can appropriately respond. Or as emotional control, their ability to ignore the things that don’t matter and focus on the things that do make a difference in the game’s outcome. But, certainly, the ability to control one’s physical self is a major issue for athletes. They must control their sports skills and execute them properly, despite numerous obstacles that are very physical in nature. An obstacle could be a rushing defensive lineman, in football; a tall, long-armed center, in basketball; or a quick-moving goalkeeper, in soccer. In all cases, a high level of physical control—more specifically, balance and stability—is required.\nBalance and stability: The words may seem interchangeable, but they are not. The words are closely linked, of course, but they aren’t a two-way street, because it is possible for an athlete to have one but not the other.\nMany studies have described the benefits of yoga exercises. Many scientific studies have found that mind-body interventions, including yoga, are effective in treating stress-related mental and physical disorders (Becker, 2000; Benson, 1996; Brown & Gerbarg, 2005). Brown and Gerbarg (2005) indicated that yogic breathing is a unique method for balancing the autonomic nervous system and influencing psychological and stress-related disorders.\nThe integration of yoga exercises into daily life can have great advantages (Scott, 2006). Evidence has shown that yoga enhances functioning of the body’s operating systems (Dinsmore-Tuli, 2002; Fronske, 2005; Gilmore, 2002; Heaner, 2001; Latona & Shelton, 2002; McGarvey, 2003; McGinnis, 2006). Heaner (2001) indicated that yoga increases self-control, self-discipline, and self-confidence. Yoga enables a person to manage both body and mind well.\nSeveral issues are involved in the heated debate on whether student-athletes should be paid by their institutions for their athletic services. Some believe that student-athletes receive more than enough compensation through their awarded scholarships. Others believe that student-athletes should be rewarded for hard work and the revenue they bring to their colleges and universities. To further the debate, the authors would like to review a few comments from both proponents and opponents of pay for collegiate student-athletes, to help readers gain a better understanding.\nThose who think student-athletes should not be paid provide several arguments. Their primary concern is that, once student-athletes start receiving benefits in monetary form, they will no longer be amateur athletes: When monetary rewards are given, the athlete is then a professional. In addition, cash payments could also impose unsportsmanlike conduct among players and university sport programs. When athletes accept scholarships, they are provided tuition, books, meals, housing, and sometimes graduate assistantships. At some colleges and universities, such support may reach a value of $200,000 over a four-year period. Student-athletes may also receive special treatment when it comes to academic issues, for example priority scheduling, tutoring assistance, and excused absences. Aren’t student-athletes, then, well-compensated already?\nJust as advances in Internet and digital technology have created new opportunities for collegiate athletic departments, they have also produced new challenges. Perhaps the most commonly encountered such challenge has been the advent of online communities such as Facebook and MySpace that give users virtual carte blanche to express themselves on the worldwide information superhighway. Athletic departments are learning that every student-athlete with a Facebook account is a potential public relations disaster."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:f26a4ec0-77dc-4808-b0dd-d95b1049b9ba>"],"error":null}
{"question":"How can you explain the process of orbital decay in X-ray binary systems?","answer":"In X-ray binary systems, orbital decay occurs due to the loss of mass in the companion star. This process can be observed and measured through long-term changes in the orbital period of the binary system. For example, in the MXB 1658-298 system, researchers found that the orbital period decay occurs with a time scale of approximately 6.5 x 10^7 years. This decay can be detected by studying the mid-eclipse timing records of the system over extended periods, which show an overall decrease in the orbital period.","context":["This article is published in collaboration with Raman Research Institute (RRI)\nResearchers from Raman Research Institute (RRI) and University of Delhi have discovered a massive extrasolar planet (a planet located outside our solar system with a mass that is about 25 times the mass of Jupiter or approximately 8000 times the mass of Earth).\nWe normally associate planets with objects orbiting ordinary single stars like our sun. However, that is a very limited view. Astronomers have discovered planets orbiting a variety of cosmic entities including exotic ones like compact neutron stars (the remnants of stars that have exploded as a supernova, compact because of their small size but relatively very high density).\nBINARY STAR SYSTEM\nThere are also planets that are in orbit around two stars, which in turn are locked in orbit around each other. Such a two-star system is known as a binary star system. A subclass of binary star system are the X-ray binaries which as the name indicates emit X-rays. One of the stars in X-ray binaries is invariably a neutron star or a black hole which due to its very intense gravitational pull gobbles up star stuff from the unfortunate second component of the binary releasing X-rays in the process.\nNow, the natural question that might arise in the mind of the reader is the following: How are astronomers able to discover a planet that is orbiting a binary system located far away?\nIf the orbital plane of an X-ray binary star system happens to be close to our line of sight towards it, the X-ray emitting star will be eclipsed by the companion star once every binary orbit. The middle of this eclipse, i.e. when the two stars are most aligned along our line of sight, can be measured with high accuracy.\nA careful analysis of this mid-eclipse time history helps astronomers to understand the binary system in question. When researchers at RRI studied three distinct X-ray outbursts spanning 40 years from one particular X-ray binary system named MXB 1658-298 they were surprised to find that in addition to orbital decay (due to loss of mass in the companion star) there was also a distinct sinusoidal signature in the eclipse timing residual that is indicative of a third body orbiting the binary system.\nBased on calculations involving the masses of the neutron star and the companion star along with their orbital parameters enabled the researchers to estimate the mass of the planet to be about 8000 times that of our earth.\nUNIQUE ASPECTS OF THE DISCOVERY\nProf. Biswajit Paul from RRI says “There are several aspects that make this discovery unique:\n1. First planet around an X-ray binary stellar system (one of the stars being a neutron star or a black hole emitting X-rays).\n2. It is the most massive among the circumbinary planets, about 8000 times that of Earth.\n3. This is the most compact of all binary systems around which planets have been found.\n4. This is the first planet found using X-ray observations.\n5. This discovery is made with a completely new technique, that of periodic delay in X-ray eclipses.”\nResearchers presented an X-ray timing analysis of the transient X-ray binary MXB 1658-298, using data obtained from the RXTE and XMM-Newton observatories.\nProf. Biswajit Paul says, “We made 27 new mid-eclipse time measurements from observations made during the two outbursts of the source. These new measurements have been combined with the previously known values to study long-term changes in the orbital period of the binary system. We have found that the mid-eclipse timing record of MXB 1658-298 is quite unusual.”\nAccording to the study “the long-term evolution of mid-eclipse times indicates an overall orbital period decay with a time scale of — 6.5(7) x 10^7 year. Over and above this orbital period decay, the O-C residual curve also shows a periodic residual on shorter timescales.”\n“This sinusoidal variation has an amplitude of ~9 lt-sec and a period of ~760 d. This is indicative of the presence of a third body around the compact X-ray binary. The mass and orbital radius of the third body are estimated to lie in the range, 20.5-26.9 Jupiter mass and 750-860 lt-sec, respectively.”\nIf true, then it will be the most massive circumbinary planet and also the smallest period binary known to host a planet.\nFor more details visit https://arxiv.org/abs/1703.04433"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"sensitive"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:a8d8dd62-6b91-4f42-986d-6e7807a5012b>"],"error":null}
{"question":"What are the differences between Islamic and Jewish rules about food dedicated to other deities?","answer":"Both religions prohibit consuming food dedicated to other deities, but with slightly different emphases. Islamic law forbids 'that which has been dedicated to other than Allah' and specifically mentions 'that which is sacrificed on stone altars.' Jewish law addresses this issue through the tithing system, requiring followers to bring their food offerings to 'the one place where the Lord your God has chosen to be worshiped' and specifically instructs them to eat there 'in the presence of the Lord your God.' While both religions prohibit idolatrous food practices, Islamic law focuses on forbidden foods, while Jewish law emphasizes the proper place and manner of food dedication.","context":["Welcome to the Quranic Arabic Corpus, an annotated linguistic resource for the Holy Quran. This page shows seven parallel translations in English for the third verse of chapter 5 (sūrat l-māidah). Click on the Arabic text to below to see word by word details of the verse's morphology.\nChapter (5) sūrat l-māidah (The Table spread with Food)\nSahih International: Prohibited to you are dead animals, blood, the flesh of swine, and that which has been dedicated to other than Allah , and [those animals] killed by strangling or by a violent blow or by a head-long fall or by the goring of horns, and those from which a wild animal has eaten, except what you [are able to] slaughter [before its death], and those which are sacrificed on stone altars, and [prohibited is] that you seek decision through divining arrows. That is grave disobedience. This day those who disbelieve have despaired of [defeating] your religion; so fear them not, but fear Me. This day I have perfected for you your religion and completed My favor upon you and have approved for you Islam as religion. But whoever is forced by severe hunger with no inclination to sin - then indeed, Allah is Forgiving and Merciful.\nPickthall: Forbidden unto you (for food) are carrion and blood and swineflesh, and that which hath been dedicated unto any other than Allah, and the strangled, and the dead through beating, and the dead through falling from a height, and that which hath been killed by (the goring of) horns, and the devoured of wild beasts, saving that which ye make lawful (by the death-stroke), and that which hath been immolated unto idols. And (forbidden is it) that ye swear by the divining arrows. This is an abomination. This day are those who disbelieve in despair of (ever harming) your religion; so fear them not, fear Me! This day have I perfected your religion for you and completed My favour unto you, and have chosen for you as religion al-Islam. Whoso is forced by hunger, not by will, to sin: (for him) lo! Allah is Forgiving, Merciful.\nYusuf Ali: Forbidden to you (for food) are: dead meat, blood, the flesh of swine, and that on which hath been invoked the name of other than Allah; that which hath been killed by strangling, or by a violent blow, or by a headlong fall, or by being gored to death; that which hath been (partly) eaten by a wild animal; unless ye are able to slaughter it (in due form); that which is sacrificed on stone (altars); (forbidden) also is the division (of meat) by raffling with arrows: that is impiety. This day have those who reject faith given up all hope of your religion: yet fear them not but fear Me. This day have I perfected your religion for you, completed My favour upon you, and have chosen for you Islam as your religion. But if any is forced by hunger, with no inclination to transgression, Allah is indeed Oft-forgiving, Most Merciful.\nShakir: Forbidden to you is that which dies of itself, and blood, and flesh of swine, and that on which any other name than that of Allah has been invoked, and the strangled (animal) and that beaten to death, and that killed by a fall and that killed by being smitten with the horn, and that which wild beasts have eaten, except what you slaughter, and what is sacrificed on stones set up (for idols) and that you divide by the arrows; that is a transgression. This day have those who disbelieve despaired of your religion, so fear them not, and fear Me. This day have I perfected for you your religion and completed My favor on you and chosen for you Islam as a religion; but whoever is compelled by hunger, not inclining willfully to sin, then surely Allah is Forgiving, Merciful.\nMuhammad Sarwar: It is unlawful for you to consume the following as food: an animal that has not been properly slaughtered, blood, pork, an animal slaughtered and consecrated in the name of someone other than God, an animal killed by strangulation or a violent blow, an animal killed by falling down, an animal which has been gored to death, an animal partly eaten by a wild beast before being properly slaughtered, an animal which has been sacrificed on the stone blocks (which pagans worshipped), and any flesh divided by casting superstitious and gambling arrows (a pagan tradition), which is a sin. Today, the unbelievers have lost hope about your religion. Do not be afraid of them but have fear of Me. On this day I have perfected your religion, completed My favors to you, and have chosen Islam as your religion. If anyone not (normally) inclined to sin is forced by hunger to eat unlawful substances instead of proper food, he may do so to spare his life. God is All-forgiving and All-merciful.\nMohsin Khan: Forbidden to you (for food) are: Al-Maytatah (the dead animals - cattle-beast not slaughtered), blood, the flesh of swine, and the meat of that which has been slaughtered as a sacrifice for others than Allah, or has been slaughtered for idols, etc., or on which Allah's Name has not been mentioned while slaughtering, and that which has been killed by strangling, or by a violent blow, or by a headlong fall, or by the goring of horns - and that which has been (partly) eaten by a wild animal - unless you are able to slaughter it (before its death) and that which is sacrificed (slaughtered) on AnNusub (stone altars). (Forbidden) also is to use arrows seeking luck or decision, (all) that is Fisqun (disobedience of Allah and sin). This day, those who disbelieved have given up all hope of your religion, so fear them not, but fear Me. This day, I have perfected your religion for you, completed My Favour upon you, and have chosen for you Islam as your religion. But as for him who is forced by severe hunger, with no inclination to sin (such can eat these above-mentioned meats), then surely, Allah is OftForgiving, Most Merciful.\nArberry: Forbidden to you are carrion, blood, the flesh of swine, what has been hallowed to other than God, the beast strangled; the beast beaten down, the beast fallen to death, the beast gored, and that devoured by beasts of prey - excepting that you have sacrificed duly -- as also things sacrificed to idols, and partition by the divining arrows; that is ungodliness. Today the unbelievers have despaired of your religion; therefore fear them not, but fear you Me. Today I have perfected your religion for you, and I have completed My blessing upon you, and I have approved Islam for your religion. But whosoever is constrained in emptiness and not inclining purposely to sin -- God is All-forgiving, All-compassionate.","14 “You are the people of the Lord your God. So when you mourn for the dead, don't gash yourselves or shave the front of your head, as other people do. 2 You belong to the Lord your God; he has chosen you to be his own people from among all the peoples who live on earth.\nClean and Unclean Animals\n3 “Do not eat anything that the Lord has declared unclean. 4 You may eat these animals: cattle, sheep, goats, 5 deer, wild sheep, wild goats, or antelopes— 6 any animals that have divided hoofs and that also chew the cud. 7 But no animals may be eaten unless they have divided hoofs and also chew the cud. You may not eat camels, rabbits, or rock badgers. They must be considered unclean; they chew the cud but do not have divided hoofs. 8 Do not eat pigs. They must be considered unclean; they have divided hoofs but do not chew the cud. Do not eat any of these animals or even touch their dead bodies.\n9 “You may eat any kind of fish that has fins and scales, 10 but anything living in the water that does not have fins and scales may not be eaten; it must be considered unclean.\n11 “You may eat any clean bird. 12-18 But these are the kinds of birds you are not to eat: eagles, owls, hawks, falcons; buzzards, vultures, crows; ostriches; seagulls, storks, herons, pelicans, cormorants;[a] hoopoes; and bats.\n19 “All winged insects are unclean; do not eat them. 20 You may eat any clean insect.\n21 “Do not eat any animal that dies a natural death. You may let the foreigners who live among you eat it, or you may sell it to other foreigners. But you belong to the Lord your God; you are his people.\n“Do not cook a young sheep or goat in its mother's milk.\nThe Law of the Tithe\n22 “Set aside a tithe—a tenth of all that your fields produce each year. 23 Then go to the one place where the Lord your God has chosen to be worshiped; and there in his presence eat the tithes of your grain, wine, and olive oil, and the first-born of your cattle and sheep. Do this so that you may learn to honor the Lord your God always. 24 If the place of worship is too far from your home for you to carry there the tithe of the produce that the Lord has blessed you with, then do this: 25 Sell your produce and take the money with you to the one place of worship. 26 Spend it on whatever you want—beef, lamb, wine, beer—and there, in the presence of the Lord your God, you and your families are to eat and enjoy yourselves.\n27 “Do not neglect the Levites who live in your towns; they have no property of their own. 28 At the end of every third year bring the tithe of all your crops and store it in your towns. 29 This food is for the Levites, since they own no property, and for the foreigners, orphans, and widows who live in your towns. They are to come and get all they need. Do this, and the Lord your God will bless you in everything you do.\nYou'll get this book and many others when you join Bible Gateway Plus. Learn more\nYou must be logged in to view your newly purchased content. Please log in below or if you don't have an account, creating one is easy and only takes a few moments. After you log in your content will be available in your library.\nStep 1 - Create an account or log in to start your free trial.\nStarting your free trial of Bible Gateway Plus is easy. You’re already logged in with your Bible Gateway account. The next step is to enter your payment information. Your credit card won’t be charged until the trial period is over. You can cancel anytime during the trial period.\nClick the button below to continue.\nStep 1 - Create an account or log in to start your subscription.\nYou’ve already claimed your free trial of Bible Gateway Plus. To subscribe at our regular subscription rate of $3.99/month, click the button below.\nUpgrade to the best Bible Gateway experience! With Bible Gateway Plus, you gain instant access to a digital Bible study library, including complete notes from the NIV Cultural Backgrounds Study Bible and the New Bible Commentary. Try it free for 30 days!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:da67a6fd-643a-4578-99c8-864e5ba117e0>","<urn:uuid:04a1c48e-fe2a-4dfa-99f9-bfb56565a2d3>"],"error":null}
{"question":"How do man-in-the-middle attacks and SQL injection attacks compare in terms of data theft methods?","answer":"Man-in-the-middle attacks steal data by intercepting network traffic, using packet capture tools to reconstruct data streams after compromising DHCP servers and redirecting client traffic through the attacker. In contrast, SQL injection attacks steal data by manipulating database queries through malicious input in web forms or URLs, allowing attackers to bypass authentication and directly extract database contents by injecting unauthorized SQL commands.","context":["This article covers popular Layer 2 & Layer 3 network attacks with a focus on DHCP Starvation Attacks, Man-in-the-Middle attacks, unintentional rogue DHCP servers and explains how security features like DHCP Snooping help protect networks from these attacks. We explain how DHCP Snooping works, cover DHCP Snooping terminology (trusted, untrusted ports/interfaces) and more. Finally we talk about the importance and purpose the DHCP Snooping Binding Database also used by Dynamic ARP Inspection to prevent ARP Poisoning and ARP Spoofing attacks.\nTopics covered include:\n- DHCP Starvation Attack, Man-in-the-Middle Attack, DHCP Hijacking & Reconnaissance Attacks\n- Rogue DHCP Servers – A Major Security Threat & Source of Network Disruptions\n- DHCP Snooping Support for Cisco Catalyst and Nexus Switches. Licensing & Features\n- How DHCP Snooping Works – DHCP Snooping Concepts - Trusted, Untrusted Ports/Interfaces\n- Traffic Dropped by DHCP Snooping, DHCP Snooping Violations – Syslog Messages\n- The IP DHCP Snooping Binding Database – Dynamic ARP Inspection\n- DHCP Snooping Option-82 Data Insertion\nDHCP Starvation attack is a common network attack that targets network DHCP servers. Its primary objective is to flood the organization’s DHCP server with DHCP REQUEST messages using spoofed source MAC addresses. The DHCP server will respond to all requests, not knowing this is a DHCP Starvation attack, and assign available IP addresses until its DHCP pool is depleted.\nAt this point the attacker has rendered the organization’s DHCP server useless and can now enable his own rogue DHCP server to serve network clients. DHCP Starvation is often accompanied by a Man-in-the-Middle attack as the rogue DHCP server distributes fake IP address parameters, including Gateway & DNS IP address, so that all client traffic passes through the attacker for inspection.\nTypical Man-in-the-Middle attack. Client data streams flow through the attacker\nUsing packet capture and protocol analysis tools the attacker is able to fully reconstruct any data stream captured and export files from it. In fact the process so simple it only requires a basic level of understanding of these type of network tools.\nIn other cases the Man-in-the-Middle attack can be used as a reconnaissance attack with the objective to obtain information about the network infrastructure, services but also identify hosts of high interest such as financial or database servers.\nIt should be by now evident how a simple attack can become a major security threat for any organization. The above attacks are examples on how easy hackers can infiltrate the network and get access to valuable information by simply connecting an unauthorized/untrusted device to an available network port effectively bypassing firewalls and other levels of security.\nRogue DHCP servers are a common problem within enterprise organizations and are not always directly related with an attack. Rogue DHCP Servers tend to appear out of nowhere thanks to users who connect consumer-grade network devices to the network infrastructure unaware that they have connected an unauthorized device with a rogue DHCP server enabled.\nThe Rogue DHCP server then begins assigning IP addresses to hosts within the network therefore causing network connectivity problems and in many cases – major service disruptions. In a best case scenario DHCP clients are served with an invalid IP address disconnecting them from the rest of the network. Worst case scenario would be the clients been assigned an IP address used by network infrastructure devices e.g the VLAN interface on the Core switch or a firewall interface, causing serious network disruptions and conflicts.\nWhile many organizations enforce security policies that do not allow 3rd party or unauthorized devices to be connected to their network, there are still incidents where users who do not understand (or care about) the security implications continue to connect these devices to the network infrastructure without consulting their IT Department.\nEducating users and enforcing security policies can be extremely challenging which is why security mechanisms need to be in place to help mitigate these incidents and is where DHCP Snooping comes into the picture.\nDHCP Snooping is considered a standard security feature and does not require any additional licensing for the older Catalyst IOS, newer Catalyst IOS XE and Nexus NS-OS operating systems, therefore the feature is available and readily configurable on all switches.\nExamples of Cisco Catalyst switches that support DHCP Snooping are: Cisco Catalyst 2960S, 2960-X, 3560, 3750, 3750-X, 3850, 4500, 6500, 9300, 9400 and 9500 series.\nExamples of Cisco Nexus switches that support DHCP Snooping are: Nexus 2000, 3000, 5000, 7000 and 9000 series.\nDHCP Snooping can be enabled globally and on a per-VLAN basis. This means you can enable it for all VLANs (globally) or only for specific including VLAN ranges e.g VLANs 1-20 & VLANs 45-50.\nDHCP Snooping is a Layer 2 security switch feature which blocks unauthorized (rogue) DHCP servers from distributing IP addresses to DHCP clients. In fact Cisco was the first vendor to implement DHCP Snooping as a security feature in its network switches and other vendors have since then followed with similar features.\nNOTE: It is important to note that DHCP SNOOPING is an access layer protection service – it does not belong in the core network.\nThe way DHCP Snooping works is fairly straight forward. DHCP Snooping categorizes all switchports into two simple categories:\n- Trusted Ports\n- Untrusted Ports\nA Trusted Port, also known as a Trusted Source or Trusted Interface, is a port or source whose DHCP server messages are trusted because it is under the organization’s administrative control. For example, the port to which your organization’s DHCP server connects to is considered a Trusted Port. This is also shown in the diagram below:\nAn Untrusted Port, also known as an Untrusted Source or Untrusted Interface, is a port from which DHCP server messages are not trusted. An example on an untrusted port is one where hosts or PCs connect to from which DHCP OFFER, DHCP ACK or DHCPNAK messages should never be seen as these are sent only by DHCP Servers.\nWhen enabling DHCP Snooping the switch will begin to drop specific type of DHCP traffic in order to protect the network from rogue DHCP servers. Here is a list of the type of traffic DHCP Snooping will drop:\n- DHCP Snooping will drop DHCP messages DHCPACK, DHCPNAK, DHCPOFFER originating from a DHCP server that is not trusted – that is, connected to an untrusted port.\n- DHCP Snooping will drop DHCP messages that release or decline an offer if these messages are not originating from the port where the original DHCP conversation was held. This stops attackers from trying to terminate or decline a DHCP offer on behalf of the actual DHCP client.\n- A DHCP relay agent forwards a DHCP packet that includes a relay-agent IP address that is not 0.0.0.0, or the relay agent forwards a packet that includes Option 82 information to an untrusted port. In-depth analysis in upcoming DHCP Option 82 article.\n- DHCP Snooping will drop DHCP messages where the Source MAC address and client MAC address are not identical (see DHCP_SNOOPING-5-DHCP_SNOOPING_MATCH_MAC_FAIL below).\nWhen DHCP Snooping detects a violation the DHCP packet(s) triggering the event is dropped and a message is logged in the switch’s log. The message can contain one of the following entries:\n- %DHCP_SNOOPING-5-DHCP_SNOOPING_UNTRUSTED_PORT: DHCP Snooping has detected DHCP server messages from an untrusted port. This is a serious violation and usually points to a rogue DHCP server operating on an untrusted port.\n- %DHCP_SNOOPING-5-DHCP_SNOOPING_MATCH_MAC_FAIL: DHCP Snooping has detected the Source MAC address of the Ethernet frame and Client MAC address in the DHCP message are not identical (see image below).\nWhen DHCP Snooping is enabled it will begin to build a dynamic database containing an entry for each untrusted host with a leased IP address if the host is associated with a VLAN that has DHCP Snooping enabled. No entries are created for hosts connected to trusted interfaces.\nEach entry in the binding database contains the following information:\n- MAC address of the untrusted host\n- Leased IP address of the untrusted host\n- Lease time\n- Binding type\n- VLAN number & interface the untrusted host is associated with\nAs untrusted hosts are assigned IP addresses from the trusted DHCP server the switch will automatically create new entries, update and cleanup the DHCP Snooping Binding Database.\nFor example, when an IP address lease expires or the switch receives a DHCPRELEASE message from the untrusted host, it will remove the specific entry from the database. On the other hand an entry will be created in the database if the switch sees a DHCPACK message from the trusted DHCP server acknowledging the assignment of an IP address to an untrusted host.\nThe show ip dhcp snooping binding command displays all entries inside the DHCP Snooping Binding Database:\nCat3560-Firewall.cx# show ip dhcp snooping binding\nMacAddress IpAddress Lease(sec) Type VLAN Interface\n------------------ --------------- ---------- ------------- ---- --------------------\nD0:76:58:0C:BB:80 192.168.4.50 85228 dhcp-snooping 4 GigabitEthernet0/5\nTotal number of bindings: 1\nThe DHCP Snooping Binding Database is also used by other Layer2/3 security features such as Dynamic ARP Inspection which help protect the network against ARP Poisoning & ARP Spoofing attacks.\nNote: IP DHCP Snooping configuration for Cisco Catalyst and Cisco Nexus switching platforms will be covered extensively in an upcoming technical article.\nNote: Dynamic ARP Inspection, ARP Poisoning, ARP Spoofing attacks will be covered in an upcoming security article.\nThe DHCP Option 82, aka Relay Agent Information Option, was originally created by RFC 3046 to allow the DHCP relay agent (e.g switch or router) to identify itself and the DHCP client that sent the DHCP messages. DHCP Option 82 is used in large metropolitan Ethernet-access deployments where DHCP is required to centrally manage the IP addresses for a large number of subscribers.\nWhen DHCP Snooping is enabled on a Cisco Catalyst or Nexus switch, it will insert the Option 82 field into the client’s DHCP message:\nDHCP Option 82 is not often used within organizations but it does provide an additional layer of protection if the DHCP server supports it. For example the DHCP Server on Windows Server 2012 or 2016 supports Option 82 allowing administrators to create DHCP Policies that control the assignment of IP addresses to specific switches within the network.\nAnalyzing the structure of DHCP Option 82 is out of this article’s scope but will be covered in great depth in an upcoming article.\nMan-in-the-Middle attacks and network disruptions from rogue DHCP servers is a serious network security threat organizations are faced to deal with on a daily basis. In this article we explained how Man-in-the-Middle attacks allow attackers to gain visibility of your network and can potentially lead exposing sensitive data flowing between servers and clients. We explained what DHCP Snooping is, examined how DHCP Snooping works and how it can effectively protect the network from these attacks. We looked at the type of traffic dropped by DHCP snooping, violation warnings and also explained the purpose and operation of the DHCP Snooping Binding Database. Finally we touched on the DHCP Snooping Option 82.","How to Prevent SQL Injection\nSQL injection attacks are one of the top threats to the security of websites and web applications. In fact, the SQL injection threat has been number one on OWASP’s Top 10 list since it was publicly disclosed more than 10 years ago.\nSQL injection attacks have caused historic havoc. They have been used to breach the World Health Organization (WTO) database and steal data from internal staff, and also to attack U.S. government agencies, including the U.S. Army and NASA. SQL injection has also been used to steal the personal data of close to 157,000 TalkTalk customers and this is only the tip of the iceberg.\nIn the same way that SQL injection attacks are relatively straightforward and anyone with some SQL knowledge or access to tools could launch them, their prevention and protection strategies are also straightforward.\nIn this post, we’ll go through the process of an SQL injection, the types of SQL injections and why they are exploitable. We’ll also highlight the prevention and protection mechanisms for all types of SQL injections.\nThe SQL Injection: What Is It and How Does It Work?\nAn SQL (Structured Query Language) injection is a type of cyberattack that aims to substitute a legitimate website’s input, such as a username, password, or URL, with an SQL statement that injects code.\nHackers perform an SQL injection, by entering fake SQL statements into a website input form view bits and pieces of information or alter a database table. In other words, hackers use unauthorized SQL code to “piggyback” on an authorized path (an input form) to reach and interfere with a site’s SQL database.\nWhere Is the Vulnerability?\nA site with an SQL injection vulnerability has forms of entry, such as input application forms or URL strings, that are unable to differentiate between a user’s input and the native commands in the site’s code.\nText fields, such as the username or password areas on login pages that allows a user or hacker to interact with the backend SQL database are at risk.\nFor example, when a user inputs a username and password, the information is compared with the website’s database using SQL statements.\nAs shown in the screenshot below, a SELECT statement is the one used to select data from a database and return results to a result table. This statement could be exploited to steal data.\nRefer to the screenshot above. When someone types a username in the text input, the SELECT statement will search in the database’s table users by utilizing this input from the user which according to the example code above, relies on a variable (“ + txtUserId;).\nIf there is no protection or way to prevent a user from entering invalid inputs, the user could enter calculated SQL statements. For example, an unknown visitor could enter a “crafted” input in the username field, such as “105 OR 1=1”.\nWith this crafted input, the SQL statement would change, but still be valid.\nNormally, the variable (+ txtUserId, as stated above) would be replaced with a normal UserId, such as “Diego”.\nHowever, with this crafted input, something else happens. The invalid input “105 OR 1=1” was designed by the hacker to intentionally create a brand new query. For example, OR 1=1 means “always TRUE”. It makes the database return all row values from the Users table.\nAlthough calculated statements like these can be dangerous, there are far sneakier ways hackers can steal, gain control, and even destroy entire databases.\nFor example, by entering something like “or 1=1–” in the password field, a new SQL statement is created that could return all password values for a Users table.\nWhy Certain Types of SQL Injections Are Exploitable\nAlthough SQL injections are generally easy to execute, hackers tend to be really persistent, especially if they want to get something out of an SQL injection.\nSQL injections can be classified based on the server’s response and the channel extraction method. Based on the server’s response, an SQL injection can be either performed by error or blind (inferential), and based on the communication channel, an SQL injection can be either in-band or out-of-band.\nA. In-band Error-based SQL injection\nAn in-band SQL injection occurs when a hacker uses the same communication channel to launch the attack and collect results. In the in-band error-based SQL injection, the hacker attempts to find and exploit visible error results given by the database server, to retrieve information, such as table names and contents. This intention behind this type of SQLi attack is used to learn as much as possible about the structure of the database.\nAn error-based SQL injection is exploitable due to:\n- User-supplied data (input) is not filtered by the web application.\n- Dynamic SQL queries (non-parameterized calls).\nUnsanitized inputs can lead to error-based feedback that provide too many useful clues for the attacker.\nB. In-band UNION-based SQL injection\nAn in-band UNION-based SQL injection is another type of in-band SQL injection in which a hacker uses the same communication channel to attack and gather results. This SQLi is based on the UNION SQL operator, which extends the results provided by the original query. For example, it could combine the results of two (or more) SELECT statements into a single result.\nA UNION-based SQLi is exploitable due to:\n- User-supplied data (input) is not sanitized by the web application.\n- Dynamic SQL queries (non-parameterized calls).\nC. Blind Attacks (Inferential SQL injection)\nAs the name suggests, hackers who perform blind SQL injection attacks have no idea what is on the other end. The application or website is still vulnerable to SQL injection, but the HTTP response does not come with clues about the SQL query or database error. The hacker can launch this type of attack by blindly reconstructing the database structure with the response and behavior of the server. A blind SQL injection can be Boolean or Time-based.\nA blind SQLi is exploitable due to:\n- Lack of parameterized queries or prepared statements.\n- Dynamic SQL queries using their procedures.\nD. Out-of-Band (OOB) SQLi\nThis type of SQLi occurs when an attacker exploits two different channels, one for launching the attack and the other for collecting data. This attack uses an OOB channel, such as DNS or HTTP protocol to exfiltrate data.\nAn OOB SQLi is exploitable due to:\n- Lack of sanitized or validated inputs from web applications.\n- Lack of (or poor) network security measures that allow the listening database server to initiate outbound requests (HTTP or DNS) to the public.\n- The right permissions to run the necessary function that starts the outbound request.\nHow to Prevent an SQL Injection\nAn SQL injection is one of the easiest to perform and most effective forms of cyber attacks. On the other hand, it is also relatively easy to defend against an SQL injection. The best way to prevent an SQL injection attack is to separate data from queries and commands using prepared statements.\nHow to defend against an SQL injection\n- User Prepared Statements\n- Validate User Inputs\n- Enforce Least Privilege\n- Perform Penetration Testing\n- Set Up a a WAF (Web Application Firewall)\nA. Use Prepared Statements with Parameterized Queries\nPrepared statements guarantee that user inputs passed to SQL statements are safe. The idea is that a user’s input cannot directly influence the SQL commands that control and access an SQL database. The database will treat all inputs (including malicious SQL statements) as ordinary inputted data and not as commands.\nIn a prepared statement, the variables in a query are always separated from the rest of the query. In other words, when a developer defines the code for SQL queries (prepared statements) the user input and code are separated.\n- Parameterize stored procedures.\n- Avoid dynamic SQL or at least parameterize them.\n- Prepared statements also help escape to evade user-supplied inputs. Configure user inputs to a specific function to ensure that characters, such as the single quote (‘), are not passed along to an SQL query as instructions.\nB. Validate User Inputs\nAnother way to defend a website’s front line against SQL injections is to validate user inputs. With this method, whatever a user inputs to the web application, there is always a validation (filtering) process allowing/disallowing inputs.\nTo validate user inputs, identify and define the essential SQL statements. Establish an SQL statement whitelist that helps filter out all unvalidated statements and accepts valid inputs.\nAdditionally, user data input can also be defined by context. For example input fields can be filtered for email addresses by allowing only the characters used in the email, such as the @ symbol or phone number inputs, which can be filtered by only allowing numbers.\nC. Enforce Least Privilege\nAs a second line of defense, the level of access and user’s privileges to a database should be limited. It is important to restrict access rights to the specific users, accounts, or computer processes that need to perform a specific activity on the database. For example, if a website or application only needs to use SELECT statements for the database, there should not be any user privileges to use INSERT or DELETE.\nEnforcing the least privileged account strategy will help reduce the level of damage in case a user account gets compromised. If a hacker steals a user’s account information, they will also have limited access to the database.\nD. Perform Penetration Testing\nA great way to prevent an SQL injection is to know the website’s flaws and to perform regular assessments of the website’s defenses by auditing suspicious activities, user privileges, and using pen-testing tools.\nSQL injection hackers and even script kiddies are beginning to use automation for time-consuming and repetitive tasks. Examples are Havij, an automated SQL Injection tool, that helps penetration testers (or bad actors) to find and exploit SQL Injection vulnerabilities. Another is SQLMap, an open-source penetration tool used to automatically scan, detect, and perform SQL injections and take control of databases.\nE. Set Up a WAF\nUse a Web Application Firewall (WAF) to protect web applications with access to databases. A WAF can filter out malicious traffic data by using a set of security rules to monitor and filter suspicious traffic behavior.\nCloudBric’s Cloud-Based Smart Web Application Protection (SWAP) is one of the most comprehensive WAF solutions against SQL injections, and other common threats. SWAP uses a patented logic (including pattern matching, semantics, and heuristic analysis) and a ruleset to automate attack detection and mitigation.\nSQL injections pose a great threat to web apps and sites. Hackers can steal, modify, and even destroy entire databases.\nThe best way to prevent an SQL injection is to parameterize statements. This ensures that all parameters (like inputs) that are passed into SQL statements are treated safely. Another way to defend against these threats is to validate or filter all user inputs.\nTo stay ahead of the game, we also recommend enforcing your user privileges and access rights, using an advanced WAF, and always pen-testing the website’s defenses."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:be470bd8-e549-4734-9245-c04889e41923>","<urn:uuid:744f28a5-65f6-4866-99a0-409456138607>"],"error":null}
{"question":"As a film student studying sound design, I'm curious about how film scores build tension and what the different components of a film's soundtrack are. Can you explain both aspects?","answer":"Film scores can build tension through strategic musical elements, as demonstrated by James Newton Howard's work in Signs where he uses ascending string figures, tone clusters, and a repeated 'fate theme' with disorienting tritone intervals to gradually increase tension. The soundtrack of a film consists of three main components: dialogue (which can be recorded live on set or in post-production through ADR/looping), music (either specifically written for the film or pre-existing), and sound effects (which are often created by foley artists and effects editors in post-production).","context":["Hollywood Records (2061-62368-2)\nRelease Date: July 30, 2002\nConducted by Pete Anthony\nThe Hollywood Studio Symphony\nMusic from this album has been used in 4 trailer(s). Click to view which ones!\nAverage Rating: 4.67 stars (3 users)\nBest of 2002: Best Soundtrack\n|2.||First Crop Circles||3:17|\n|5.||In The Cornfield||5:40|\n|8.||Throwing A Stone||5:35|\n|9.||Boarding Up The House||3:05|\n|10.||Into The Basement||5:19|\n|12.||The Hand of Fate - Part I||5:34|\n|13.||The Hand of Fate - Part II||3:37|\n|Total Album Time:||45:27|\n|by Andrew Granade\nAugust 24, 2003\nLet's get one thing straight and out in the open: Hitchcock and Herrmann would be proud. I know that you are thinking, \"Well, that's an obvious opening, isn't it? He must have read at least one review of the movie to come up with that one.\" To some degree you are correct in that assessment, but bear with me for a few more lines.\nThe press has dubbed M. Night Shyamalan \"the next Hitchcock\" and \"the next Spielberg.\" Those monikers seem to have come about from the director's clean, clear storytelling style, distinctive atmosphere, love of sharing the common man's tale, and, above all, infatuation with MacGuffins.\n\"MacGuffin\" is a Hitchcockian term that describes the red herring a director puts in front of an audience to drive the plot forward and to keep attention away from a movie's real subject matter. Shyamalan' The Sixth Sense famously kept our attention on the psychologist's need to redeem himself and his relationship with the young boy so we would not notice the obvious facts of his existence until the end. With Signs, crop circles and their meaning keep our attention. These are obvious and have been pointed out countless times, but I believe there is another MacGuffin most have accepted, including you reading this review – Shyamalan, along with composer James Newton Howard, are invoking Hitchcock and Herrmann to receive initial praise and acceptance for emulating those masters. This clever MacGuffin allows the fresh, new elements they incorporate to blindside the audience who has come expecting a Hitchcockian thriller, creating even more suspense as viewers are thrown off balance. It is easy to do what you want when people are looking the other way.\nHoward's score for Signs is his best collaboration with Shyamalan to date. The director's recent style has been deliberate, quiet, and creepy and the composer has supported that feel by creating atmosphere (remember the human breaths in his Sixth Sense score?) while, for the most part, staying out of the way. Howard's score for Signs continues that tradition by slowly creeping under the skin, barely noticed. Indeed, for much of the movie there is absolutely no underscoring. Yet each time a cue appears, there is a little more motion, a little more sound, steadily generating enough tension to power the theatre until it finally blossoms into revelation and catharsis during the final cues, \"The Hand of Fate - Parts I and II\".\nThe movie opens with simple title cards announcing the production's major players. Howard's score opens with a nod to two of his greatest influences, Ligetti and Penderecki, with strident string tone clusters, much like the former's Atmospheres. After this 30 second homage, one that Herrmann, who had an interest in the European texturalists, would have appreciated, Howard launches immediately into an ascending string figure that strikingly recalls Herrmann's work for Psycho in instrumentation and motion. The string tone clusters are never far away, however, and rest underneath this figuration, eventually being overtaken by brass and percussion that sweep the ascending figure into a frenzy that eventually ceases, rather than really ending. This short opening cue is one of the year's most effective main titles.\nSigns then moves on to the task of building up audience expectation. Howard accomplishes this by utilizing a post-minimalist language reminiscent of John Adams' work from the late 1970s, the period of Shaker Loops. For example, consider the second cue, \"First Crop Circles\", which establishes the movie's main theme (one I call the \"fate\" theme). It is a deceptively simple theme in C, consisting of three notes and two intervals: a tritone followed by minor second. A rest of equal value to the notes follows the last note and the figure is then repeated, and repeated, and repeated, and finally modulated. The continuous resolution of the tritone is disorienting while it is comforting, a wonderful metaphor for the entire movie. Shifting chords and tone clusters not unlike the opening one accompany this fate theme and help shift its nebulous mood according to onscreen needs. (Ed. - the shifting chords are reminiscent of Howard's work in Grand Canyon.)\nEvery other theme in the movie can be seen as an extension of this fate theme. Listen to the beginning of \"Brazilian Video\". The opening theme is the fate theme again, only with the first interval descending and the second one remaining an ascending one. Another slight alteration occurs when the family looks to the skies, as in \"Baby Monitor\". The fate theme is opened up past a perfect fifth to encompass a major sixth, providing a palpable feeling of airiness. \"Into the Basement\" combines a variation of the fate theme with the original three-note theme over string and trumpet tone clusters in two distinct tonalities. This combination of two familiar thematic variations in two separate keys creates a genuinely unsettling atmosphere out of association. Howard's incredible reliance on one theme and its complete exploitation is one of the best examples of musical minimalism I have heard in a movie score, yet he uses percussion to accentuate the action and key relationships to establish tension and release so well that it rises above any pedantic classification I could provide.\nThe MacGuffin seems to be working. There is a huge temptation for me to tell you how Howard, like Herrmann, places every note so that it perfectly complements onscreen occurrences and reflects the director's style in every aspect; how he uses the orchestra just like Herrmann did in this Hitchcock movie or that one. Yet I have tried to show you the ways in which Howard is using the musical language of his era in his own unique way. The best advice I can give you, though, is to go out, buy this score, and look past the surface similarities to plumb its depths. Signs is certainly one of Howard's best and one of the best this year. So go, give it a listen, and try not to get too caught up in what Mr. Herrmann - I mean Howard - expects us to hear.\nEnter your e-mail address to receive weekly soundtrack and film score news:\nIf any information appears to be missing from this page, contact us and let us know!","Home > Preview\nThe flashcards below were created by user\non FreezingBlue Flashcards.\na class or type of film. Films belonging to a particular genre share narrative, visual, and/or sound conventions.\nA musical in which some or all musical numbers are not motivated by the narrative; for example, characters sing and dance throughout the film but at least some performance are not stages for an onscreen audience.\n(Motion Pictures Producers and Distributors of America)\nan organization designed to help the industry regulate its own content. It ensured that the American film industry kept supposedly subversive content in check.\ntheater managers would allow Micheaux to screen films for black audiences at midnight.\nan alternative to continuity editing, this style of editing was developed in silent Soviet cinema, based on the theory that editing should exploit the difference between shots to generate intellectual and emotional responses in the audience.\nterm coined by Maori filmmaker and theorist Barry Barclay to describe the filmmaking practices of indigenous people, also called aboriginal people or First Nations. Fourth World filmmakers have been committing their experiences to film, garnering an international audience in the process.\nDocumentaries that rely exclusively on interviews are often called \"talking heads\" documentaries.\npresent contemporary or historical events rather than fictional stories.\nDocumentaries that trace the lives of individuals and, as a result they resemble fiction films with characters, goals, and obstacles.\n(Man on Wire)\nalso called cinéma vérité, a documentary style in which the filmmaker attempts to remain as unobtrusive as possible, recording without obvious editorial comment.\na fiction film (often a comedy) that uses documentary conventions on fictional rather than real-world subject matter.\nMan Ray, Walter Ruttmann, and Hans Richter established a different tradition, one of abstract filmmaking. Their films abandoned human figures altogether. Their animated, geometrical forms reflect their creators' idea that filmmaking was painting with motion.\nSound recorded on a set, on locations or, for documentary film, at an actual real-world event, as opposed to dubbed in post-production through ADR or looping.\nTranslates as \"author.\" A term for film directors that was popularized by French film critics and refers to film directors with their own distinctive style.\nThat combined with the dark lighting and characteristics in films designed to bring the viewer into the story, it is difficult for people to break away from the ideology that is being portrayed in a movie.\nPractice of self-censorship by Hollywood. It was how Hollywood would monitor and control the content of movies.\nA long shot in which the film frame resembles the proscenium arch of the stage, distancing the audience.\nAudiences assume that the sounds that accompany images are true to life.\nsound played outside the world the characters inhabit.\nA single take that contains an entire scene.\nEverything audiences hear when they watch a sound film. The soundtrack is the composite of all three elements of film sound: dialogue, music, and sound effects.\nSystem of beliefs, values, and opinions that are influenced by family, cultural background, personal experiences, education, and popular culture.\nA standard shot pattern that dictates that a shot of one character will be followed by a shot of another character, taken from the reverse angle of the first shot.\nExtremely low budget, DIY American independent filmmaking, beginning in the early 2000s. Often compared to the French New Wave because of its improvisational style and personal subject matter.\nEarly films that documented everyday events, such as workers leaving a factory.\nA process of blending the free elements of the sound track (dialogue, music and effects) in post-production.\nWriters, directors and producers design a project with a specific star in mind, hoping to capitalize on audience expectations.\nMaterials intentionally released by studios to attract public attention to films and their stars. Promotion differs from publicity, which is information that is not (or does not appear to be) intentionally disseminated by studios.\na business model adopted by the major studios during the Hollywood studio era, in which studios controlled all aspects of the film business, from production to distribution to exhibition.\nAn outlawed studio era practice, where studios forced exhibitors to book groups of films at once, thus ensuring a mart for their failures along with their successes.\nThe practice of Hollywood studios contracting out post-production work to individuals or firms outside the U.S.\nan assortment of images joined together in a sequence.\nA technique of cutting back and forth between action occurring in two different locations, which often creates the illusion that they are happening simultaneously. Also called \"cross cutting.\"\nThe reverse angle of the original shot. Usually used in conversation or dialogue between two characters.\n(automatic dialogue replacement) a technique for recording synchronized dialogue in post-production, using a machine that runs forward and backward. Also called \"looping\" because it is achieved by cutting several identical lengths of developed film and having actors record the dialogue repeatedly.\nA musical accompaniment written specifically for a film.\nA technique of depicting two layered images simultaneously. Images from one frame or several frames of film are added to pre-existing images, using as optical printer, to produce the same effect as a double exposure.\nKnow about the Hollywood studios: their filming practices, their use of sets and stars, how many major studios there were and the names of the top ones, where they make most of their profits.\n- Studio System - hollywood effiencent mode of production, distribution and exhibition.\n- Relied on a division of labor to generate products rapidly and cheaply.\n- Producers used that same production team to ensure the speed.\n- Used the same camera placement, reused sets, and used stars as marketing devices.\n- Five major studios - MGM, Paramount, Warner Brothers, Twienthieth Century Fox, RKO\n- Would make most of profits internationally.\nKnow about the uses of recorded dialogue - live and in postproduction.\n- During production:\n- a boom operator extends the mic to record dialogue on set\n- During Post:\n- foley artists and effects editors experiment with making sound effects.\n- the ADR supervisor rerecords dialogue\n- supervising sound editor makes sure everything is in sync with the film\nIn No Country for Old Men, what links the carnage of the drug war to the O.K. Corral shoot out?\na historical reference in the dialogue\nWhen were female directors in Hollywood most numerous?\nDuring the 1910s and 1920s. The earliest years of cinema.\nKnow all about genres - the origin of the term, why Hollywood liked making them, how they influence audiences, what characteristics do films of the same genre share.\n- Origin of genre traced back to greek philosopher Aristotle.\n- Genres allowed studios to conceptualize, produce, market, and distribute their products effecitnely and rapidly.\n- They could reuse sets, story lines, costumes.\n- Romance films have the same lovey couple, action films have the same male superhero.\n- Audiences that love horror movies will return to see the newest ones same with romance films.\n- Audiences are looking to see if they can surprise or break their idea of a romance.\nWhat is the Hollywood standard for editing?\n- Continuity editing or invisible editing.\n- Cutting is so seamless from one shot to the next that audiences in the theater are not even aware that they are seeing a sequence of images.\nWhere was the center of commercial filmmaking until the 1950s?\nWhat is the most common manufactured sound in films?\nKnow about the Hollywood star system, how were stars paid, what kind of contracts did they have.\n- Stars are actors who the studios create an image for. They have a fan base, a specify reputation, are seen in the media and play similar roles. They were originally held in multi-year contracts but now they are free agents.\n- Stars are paid with percentages of profits and cash break zeros. They make a small amount of money up front and then a portion of the profits.\nWho were the early supporters of sound technology?\nAt first, filmmakers Eisenstein, Pudovkin, and Alexandrov were not supporters of the sound technology. They thought movies wouldn't convey their artist meaning. Eventually, they became supporters.\nMurnau's film Sunrise, was one of the masterful in using sound technology to create meaning in film. With no dialogue, the film used music and horns to give meaning.\nHow did black audiences react to the depiction of aggressive African Americans?\nThey created the Coalition against Blaxploitation and protested the films.\nWhen dec the musical reach its creative and economic peak?\n1940s and 1950s.\nWhat event influences the film industry's depiction of disabled characters?\nThe Americans with Disabilities Act of 1990, film scholars have addressed the ideologies implicit in cinematic representations of disability.\nWhat sound do classical Hollywood films privilege over other sounds?\nDescribe abstract films - what technologies do they employ?\nThey abandoned human figures all together.\ngraphic form, rhythmic editing, anything.\nWhen and why did film critics start to value genre films?\n- Origin of Genre can be traced to greek philosopher Aristotle.\n- Before the 1960s, genre films were thought to be repetitive entertainment. Critics overlooked their intellectual potential. Now genre films inspire popular criticism.\nSince the 1920s, who has dominated the inter national film market?\nWhat are the three modes of filmmaking?\nWho directed the first African-American feature film?\nOscar Micheaux - Within Our Gates.\nHow many women have won Oscard for Best Director? Who are they and when did they win?\nOne, Kathryn Bigelow. Won for Hurt Locker in 2010.\nWhat signaled the end of the studio era in Hollywood?\n1948 Supreme Court Decision that called the Paramount Consent Decree - ordered Hollywood studios to cease monopolistic businesses.\nWhat feature length narrative film was released in 1915 and demonstrated cinema's artistic and commercial potential?\nBirth of a Nation. D.W. Griffith.\nWhy was film noir popular initially?\nThe cynicism following World War II.\nWhat does an editor do? Why is it important? How does it impact the film?\n- An editor pieces together all of the film footage to create the final piece. They need to collaborate with the director and their vision to make the movie.\n- Its is important because the wrong cut, the wrong angle can immediately distance the audience from the story world.\n- Insured continuity\nGive a specific and detailed example of how editing was used in either Jaws or Bonnie and Clyde. What was the impact of the editor on the story?\nI counted a total of 59 shots in the beach scene. The editing in the scene must have been complicated. Verna Fields had to use good timing to make sure that each time someone walked in front of Brody, the camera zoomed in closer to his concerned face. The pacing of the scene started out with long shots. Each one watched each person in the water and then would cut back to Brody watching them; the woman floating, the dog - pipet, the couple, Alex on his raft, and Brody's son and friends. When people started to get in Brody's face and distract him, something happened with each person to alarm the viewers and Brody; the woman floating had the old man with the grey cap swim underneath her, looking like a shark and the couple were horsing around and the girl screamed. As it flips to each person and shows the viewers what each person is doing, it builds the excitement because we don't know who is going to be killed. Allowing the viewers to see the ordinary people doing ordinary activities on the beach, it not only lets the viewer get emotionally attached to each person but the viewers can relate to what everyone is doing, making the thought of getting attacked by a shark, that much more scary. As the shots get closer to the attack, the shorten, telling the viewers that its coming. It isn't until we're looking at the underwater view that we know the shark is here. Once Alex is killed, the shots flip from the front to the back of the panic and people running. I think this allows the viewer to be involved with the chaos. Finally, at the end of the scene, the shots get longer and quiet as Alex's mother discovers her son is dead.\nWhat is sound design? What are the various components of sound in a film? Give an example of how sound is used from the Live of Others.\n- Sound design is the planning out of how sound will be portrayed in a film.\n- Dialogue, Sound effects, and Music\n- COME BACK\nWhat is a genre? How are films classified in various genres? Using either The Shining or Alien, give specific examples of how you can tell that those films belong to their respective genres.\n- Genre refers to a group of films that share a set of narrative, stylistic, and thematic characteristics or conventions.\n- Subgenre - smaller clusters of films that adicitonal conventions come into play.\n- Horror films - low lighting, monsters, dramatic music.\n- Westerns - open space, white settlers, whiskey, horses, cowboy hats, spurs, extreme long shots.\n- Detective Film - mysteries, police officer\n- Action film - thrill, male hero\n- Science Fiction - space travel, futuristic societies,\n- Musical - highly choreographed dance routines,\nWhat are the different types of documentaries? What type of documentary is Man on Wire? Why?\n- narrative documentary - trace lives of individuals and resemble fiction films\n- voice of authority - voice over narration with images (Planet Earth)\n- Talking heads - rely mostly on interviews\n- Director-Participant - director is involved in the film (Supersize me)\n- Direct Cinema - observational style (cinema verite)\n- Self-reflective documentary - includes process of filmmaker as part of subject matter\n- Mockumentary - Fiction films that pose as documentaries (Cloverfield)\nMan on a Wire was a narrative documentary that detailed the story of how he got to walking the wire across the twin towers.\nWhat impact does experimental film have? WHat is the value of experimental film? Give a specific example from one of the experimental films that we watched.\n- They evoke frustration because they confront us with a situation we were sure we understood.\n- Approach the medium as an aesthetic, philosophical and/or political means of expression.\n- Utopia One had a piece called The Stream which paralleled the meaning of a stream flowing into the ocean to life."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:a8dc48ce-a3ee-4d07-a76e-e659f4226ec9>","<urn:uuid:f6ddf670-b4a8-4137-8b9b-44bba099a96c>"],"error":null}
{"question":"How do the product concept and production concept differ in their approach to meeting consumer preferences?","answer":"The product concept and production concept take different approaches to consumer preferences. The product concept holds that consumers will favor products that offer the most quality, performance, and innovative features, with marketing managers focusing on continuous product development and making attractive designs. An example of this is Nescafe coffee by Nestle. In contrast, the production concept assumes that consumers will favor products that are widely available and highly affordable, with marketing managers focusing on sufficient supply, distribution, and keeping prices within buyers' capacity. Examples of organizations following the production concept include WASA and Bangladesh Railway.","context":["3 Tugas Kelompok : 1. Presentasi per chapter 2. Presentasi Marketing PlanBuku Wajib :Kotler, Keller, Marketing Management, edisi 14, 2012\n4 Penilaian Partisipasi : 10% Nilai Presentasi Individual : 20% Nilai Presentasi Kelompok : 10%UTS : 30%UAS : 30%\n5 All of the above, plus much more! What is Marketing…??Selling?Advertising?Promotions?Making products available in stores?Maintaining inventories?All of the above, plus much more!\n6 Marketing is the activity, set of institutions, and processes for creating, communicating, delivering, and exchanging offers that have value for customers, clients, partners, and society at large. (American Marketing Association, dalam Kotler dan Keller, 2012)\n7 Marketing is a societal process by which individuals and groups obtain what they need and want through creating, offering, and freely exchanging products and services of value with others (Kotler and Keller, 2012)\n8 Marketing management is the art and science of choosing target markets and getting, keeping, and growing customers through creating, delivering, and communicating superior customer value.\n9 Simple Marketing System CommunicationIndustry(a collectionof sellers)Marketof Buyers)Goods/servicesMoneyInformation\n11 What is Marketed? Experiences Events Properties Organizations PersonsExperiencesEventsPropertiesOrganizationsInformationIdeasServicesGoodsExperiences include a trip to Disney World, Fantasy baseball camp, a cruise.Events can include trade shows, the Olympics, Super Bowl, etc.Properties include real estate as well as stocks and bonds.Organizations use marketing to connect with their target market.Information is marketed by universities, textbook publishers, newspapers, etc.Ideas include “Friends don’t let friends drive drunk”Places\n12 What is Marketed?1. Goods – Physical products, consumer products, consumer durables, etc. 2. Services – Transport, repair & maintenance, legal, financial, consultancy, hotel, specialised skills, professionals, 3. Events – Trade shows, sports, world cups, vintage car rally, fashion shows, artistic performance, annual functions, event management, etc. 4. Experiences – Theatres, opera, Disney-world, trekking, mountaineering, ocean cruise, cinema, music concerts, etc. 5. Persons – Celebrity marketing, film stars, politicians, artists, performers, advertisers, and now also CEOs of companies. 6. Places – Cities, states, countries for tourism, leisure & place for industrialisation, real estate agents & business\n13 What is Marketed?7. Properties – Ownership of tangible properties like real estate, house, apartment, farm house, precious metals and intangible properties like financial portfolio of various securities, stocks, bonds. 8. Organisation – Building up identity, image, reputation, and brand value in the minds of consumers. 9. Information – It can be produced, packaged & marketed as a product – text books, encyclopaedias, magazines & journals on literature, science, technology, medicine info, available thru internet 10. Ideas – The concept regarding a utility, business opportunity, advertising / marketing ideas, scientific & technical, social, financial, psychological etc.\n14 Core Marketing Concepts Target Markets, Positioning, and SegmentationNeeds, Wants, and DemandsOfferings and BrandsValue and Satisfaction\n17 Core Concepts of Marketing Based on :Needs, Wants, Desires / demandProducts, Utility, Value & SatisfactionExchange, Transactions & RelationshipsMarkets, Marketing & Marketers.\n18 Needs, wants, and demands; Bab 1 Pemasaran: Mengatur Hubungan Pelanggan yang MenguntungkanNeeds, wants, and demands;Needs are states of felt deprivation.All humanity have the same needs;Physical needs: Food, clothing, shelter, safety.Social needs: Belonging, affection.Individual needs: Learning, knowledge, self-expression– food ( is a must )Wants; are needs shaped by culture and individual personalityWant – Pizza, Burger, French fry's ( translation of a need as per our experience )Demand – Burger ( translation of a want as per our willingness and ability to buy )1818\n19 Penawaran Pasar (Marketing Offer) Bab 1 Pemasaran: Mengatur Hubungan Pelanggan yang MenguntungkanPenawaran Pasar (Marketing Offer)consumer needs and wants are fulfilled through a marketing offer, which is some combination of products, services, or experiences offered to a market to satisfy a need.Valuethe value or benefits the customers gain from using the product versus the cost of obtaining the product.SatisfactionBased on a comparison of performance and expectations.Performance > Expectations => SatisfactionPerformance < Expectations => Dissatisfaction1919\n20 Exchange, transactions, and relationships The act of obtaining a desired object from someone by offering something in return.Transaction :consists of a trade values between two parties.Relationship :The goal of marketers is to build solid relationship with customers and retaining them by delivering superior value.MarketsThe set of all actual and potential buyers of a product. Sellers must search for buyers, both buyers and sellers are carried out by marketing\n21 Who is Responsible for Marketing? Entire OrganizationMarketing DepartmentChief Marketing Officer(CMO)CMOs must have strong quantitative skills, to accompany their qualitative skills. Must be entrepreneurial as well as a team player. However, the CMO nor the marketing department can be solely responsible for marketing. It must be undertaken by the entire organization.David Packard of Hewlett-Packard is quoted as saying: “Marketing is far to important to be left to the marketing department.”\n22 Marketing Concepts Production Product Selling Marketing Holistic QualityInnovationCreate, deliver, and communicate valueProductionProductSellingMarketingHolisticMass productionMass distributionUnsought goodsOvercapacityThe five distinct marketing concepts are: Production, Product, Selling, Marketing, and Holistic. These philosophies have evolved over time and began with the production concept. The evolution of a new marketing concept does not mean that all companies are changing. Many companies continue to operate under the production concept.Under a production philosophy the company will seek to mass produce products and to distribute them on a wide scale. The belief is that consumers prefer products that are widely available and inexpensive.The product concept proposes that consumers prefer products that have higher quality, performance, or are more innovative. Often, managers focus too much on the product (a better mousetrap) but this does not always equal success.The selling concept argues that members of a market will not purchase enough product on their own so companies use the “hard-sell” to increase demand. Typically used with unsought goods such as insurance or cemetery plots, or when companies face overcapacity.The marketing concept first emerged in the 1950’s and focuses more on the customer with a “sense-and-respond” attitude. Companies that have embraced the marketing concept have been shown to achieve superior performance than competitors.The holistic concept takes a philosophy that everything matters in marketing. Figure 1.3 (next slide) outlines the Holistic Marketing Concept.\n23 Company Orientations Towards the Marketplace Consumers prefer products that arewidely available and inexpensiveProduction ConceptConsumers favor products thatoffer the most quality, performance,or innovative featuresProduct ConceptConsumers will buy products only ifthe company aggressivelypromotes/sells these productsSelling ConceptFocuses on needs/ wants of targetmarkets & delivering valuebetter than competitorsMarketing Concept\n24 Pemasaran secara keseluruhan (holistic marketing) merupakan ide dimana pemasaran merupakan segalanya, dikatakan bahwa organisasi yang sukses harus memiliki pendekatan secara menyeluruh pada pemasaran dimana semua bagian dari organisasi memfokuskan untuk bagaimana meningkatkan nilai tambah bagi pelanggan dan bagaimana mengkomunikasikan nilai tersebut kepada pelanggannya.\n26 Relationship Marketing Relationship marketing seeks to build mutually beneficial, long-term relationship with key constituents in order to earn and retain their business. The four key constituents are: customers, employees, partners, and member of the financial community. Attracting a new customer can cost five times as much as retaining existing customers so building long-term relationships makes financial sense for the company.Marketing networks consist of the company and its supporting stakeholders who have built a mutually profitable business relationship.Build long-term relationshipsDevelop marketing networks\n27 Integrated Marketing Create, communicate, and deliver customer value Integrated marketing holds that all activities undertaken by the company should create, communicate, and deliver value. Further, all new activities should take into consideration all other marketing activities.\n28 Internal MarketingInternal marketing is the task of hiring, training, and motivating able employees to serve customers well. You can’t promise excellent service if you can’t deliver excellent service.\n29 Performance Marketing Social ResponsibilityMarketers must understand both the financial and nonfinancial returns to a business and society from marketing programs and activities. Financial accountability involves the justification of marketing expenditures in terms of financial returns. But they must also think about the ethical, environmental, legal, and social aspects of their activities.Financial Accountability\n34 Faktor-faktor yang Mempengaruhi Strategi Perusahaan TargetConsumersProductPlacePricePromotionImplementationMarketingPlanningControlAnalysisCompetitorsIntermediariesPublicsSuppliersDemographic-EconomicEnvironmentTechnological-NaturalPolitical-LegalSocial-Cultural\n35 10 dosa pemasaranPerusahaan tidak berfokus pada pasar dan tidak digerakkan oleh pelanggan pada tingkat yang memadaiPerusahaan tidak sepenuhnya memahami pelanggan sasarannyaPerusahaan perlu menentukan dan memantau pesaingnya dengan lebih baikPerusahaan belum mengelola hubungan dengan pemangku kepentingan (stakeholder)nya secara benarPerusahaan kurang lihai dalam menentukan peluang-peluang baruRencana pemasaran dan proses perencanaan perusahaan burukKebijakan-kebijakan produk dan jasa perusahaan memerlukan pengetatanKeahlian komunikasi dan pembangunan merek perusahaan lemahPerusahaan tidak terorganisasi dengan baik untuk menjalankan pemasaran secara efektif dan efisienPerusahaan tidak memaksimalkan pemanfaatan teknologi.\nYour consent to our cookies if you continue to use this website.","Discuss about the marketing management philosophies.\nWhat are the differences between marketing and selling concept?\nWhat are the differences between Customer and Consumer?\nAnswer: Marketing is the process of identifying needs and wants of target customers and delivering the product and service to customer for earning profit. There must have some logic behind every activity. This logic is called philosophy. When marketing activities are completed under these logic then it is called marketing philosophy.\nThere are five marketing management philosophies. These are given below:\n1. The Production Concept:\nThe product concept holds that consumers will favor products that are available and highly affordable. Such as; WASA, Bangladesh Railway etc. Marketing manager tasks:\na). Produce product with a view to sufficient supply and distribution.\nb). Price keep up within the capacity of buyers.\n2. The Product Concept:\nThe product concept holds that consumers will favor products that offer the most quality, performance and innovative features. Such as; Nescafe coffee of Nestle. Marketing manager tasks:\na). Development of product continuously.\nb). Make attractive design, color, packaging and form of product.\nc). Increase the effectiveness of product\n3. The Selling Concept: The selling concept holds that consumers will not purchase enough of the form’s products unless it undertakes a large-scale selling and promotion effort. Such as; Insurance Company of Bangladesh is following this concept. Marketing manager tasks;\na). Increasing promotional activities.\nb). Employed skilled and experienced salesman.\nc). Ensure customer satisfaction\nd). Taking planning shortly\n4. The Marketing Concept:\nThe marketing concept holds that achieving organizational goals depends on knowing the needs and wants of target markets and delivering the desired satisfaction better than competitors do. Such as; Pride, Dandy Dying, and Proctor and Grumble (P and G) are conducting their business under this concept. Marketing manager tasks;\na). Identity customer test, preference and demand.\nb). Product planning, development, pricing and distribution according to the demand of customers.\nc). Earning profit with customer satisfaction.\n5. The Societal Marketing Concept:\nThe societal marketing concept is the idea that a company’s marketing decisions should consider consumer’s want, the company’s requirements, consumer’s long run interest and society’s long-run interests. Such as; Uniliver Company ltd., Bata Shoe, Dutch Bangla Bank ltd. are doing their business under this concept. Marketing manager tasks:\na). Satisfying the needs of customer and welfare the society and society’s people.\nb). Participate various types of social welfare programs.\nc). Try to solve environmental problem, economic problem use our minimum resources properly.\nFrom the above discussion we can say that, marketer has to follow an appropriate philosophy for effective marketing programs.\n2. What are the differences between marketing and selling concept?\nAnswer: Marketing concept and selling concept are two different concepts. The basic differences of these two concepts are given below:\n|Point of Difference||Marketing Concept||Selling Concept|\n|1. Definition||Marketing is the process of creating value for customer and delivering goods and services for earning profit.||Selling is the process of encoring customers for increasing sales.|\n|2. Objective||The objective of this concept is to satisfy customer through goods and services.||The objective of selling concept is to increase sales of goods and services.|\n|3. Scope||The scope of marketing concept is wider.||The scope of selling concept is narrow.|\n|4. Emphasis||This concept emphasis on customer needs.||This concept emphasis on products or services.|\n|5. Profit||This concept earns profit through customer satisfaction||This concept earns profit through attractive sales and marketing promotion.|\n|6. Start||This concept starts with actual and potential customers||This concept starts with existing product.|\n|7. Market segmentation||This concept thinks about market segmentation deeply.||This concept never thinks about market concept.|\n|8. Marketing mix||This concept gives equal importance on marketing mix.||This concept gives importance on only promotion.|\n|9. Effectiveness||This concept is applicable in pure competition market.||This concept is useless in pure competition marekt|\n3. What are the differences between Customer and Consumer?\nAnswer: Though the meaning of customer and consumer are almost same, but it has some fundamental differences. The basic differences between customer and consumer are given below:\n|Point of Difference||Customer||Consumer|\n|1. Definition||Customer is a person or organization who purchases goods or services for further production.||Consumer is a person or organization who purchases goods or services for consumption.|\n|2. Nature||Customer may be business or organization.||Consumer may be person or family member.|\n|3. Objective||Business customer purchases goods for earning profit.||Consumer purchases goods for destroying utility of goods or services.|\n|4. Utility||Customer creates utility of goods or services||Consumer destroys the utility of goods or services.|\n|5. Number||The number of customers is fewer.||The number of consumer is larger.|\n|6. Purchase volume||Customer purchases huge amount of goods or services||Comparatively purchase lower amount of good or services|\n|7. Place||In distribution channel, customer stays before consumer.||In distribution channel, consumer stays after customer.|\n|contract||Customers have ability to make contract.||Consumers may have no ability to make contract.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:e6173b4f-359a-43c8-993b-e308d1b5e34a>","<urn:uuid:a8ad85e4-012e-453a-b4f2-100c3d989191>"],"error":null}
{"question":"How does the daily work of a penetration tester combine technical expertise with problem-solving abilities, and what are the main training approaches for developing these skills?","answer":"A penetration tester's work involves deep technical analysis of systems and devices, combining hacking, reverse engineering, and cryptography skills with creative problem-solving. The role requires developing a thorough understanding of how systems are built in order to break them. Training for these skills occurs through two main approaches: self-directed research via the internet and structured certification programs. Major certification paths include the EC-Council CEH, which provides hands-on hacking challenges and malware analysis, and Offensive Security's courses that emphasize critical thinking through their 'Try Harder' methodology. The field demands continuous learning, with professionals often needing to become 'experts in an afternoon' on new technologies to find vulnerabilities.","context":["|Computer Systems and Software Engineering|\n|Digital and IT services|\n|Large business (250+ employees)|\nLike this profile?\nAdd this profile to your favourites so you can return to it later from your account.\nA day in the life of a Penetration Tester in the United Kingdom\n>15 years in infosec, currently a hacker; in the past been an infosec policy consultant and ran my own business.\nWhat I do\nI'm an embedded penetration tester for BT. I attempt to break the security of products and devices that BT uses and sells.\nSkills I use and how I developed them\nHacking, reverse engineering, *nix systems administration, C, python, assembler (inc MIPS and ARM), hardware attacks, cryptography.\nMost skills were developed through own research via the Internet.\nWhat I like most\nI like having the time to develop a deep understanding of how an embedded device was put together, in order to break it.\nWhat I like least\nWhat surprised me most\nIt's a relaxed atmosphere that supports the development of new skills.\nMy career goals when I graduated\nI really just wanted to write computer games but my industrial placements indicated that there was fun to be had in systems integration in the defence sector.\nMy career history\nSponsored by Siemens Plessey Systems (2 summer placements).\nEmployed by Siemens Plessey Systems as a software engineer. Business unit was sold to British Aerospace which later became BAE SYSTEMS. Progressed to systems engineer and designer running a prototyping team. (3 years).\nEmployed by VEGA Group as an information security consultant. I spent two years working within the Defence Procurement Agency advising on security policy and then qualified as a penetration tester (CHECK Team Leader). (3 years).\nStarted and ran my own consultancy, supplying infosec policy and penetration testing services to the defence sector. (6 years).\nIn tandem, started and ran the leading street hypnosis training company, Head Hacking, with my friend Anthony Jacquin.\nSold my consultancy and joined BT as a penetration tester. I specialise in embedded device hacking.\nWhat has helped my career to progress\nDedication to completing tasks and attention to detail are the two things that provided the opportunities for progression. Knowing what I wanted and actively seeking them out provided the momentum for progression.\nCourses taken since graduation\nCoursera Stanford Uni Cryptography 1.\nCoursera Maryland Uni Cryptography.\nCoursera Maryland Uni Hardware Security.\nCryptography (often in the form of TLS) is appearing everywhere and there are many ways of getting it wrong, while still making it look right. These courses teach strong principles that are useful in multiple levels of crypto analyses.\nHow my studies have helped my career\nHacking is relatively unique within the IT industry as it is dependent on a mind set rather than a set of skills or qualifications. As such, this career requires continual learning and study, but not in the form of formal courses or reading recommended books.\nSometimes it is simply to keep abreast of developments, but often it is because we have to learn how something new works in a relatively short period of time, in order to find ways to break it. We refer to this as 'becoming an expert in an afternoon.'\nIt's fun because it is challenging.\nWhat surprised me about my career so far\n'Business' is just a more complex system with fewer controlled variables.\nWhere I hope to be in 5 years\nI'd happily still be breaking things for BT. Maybe I would move into management again. Ideally I'll be building cool art installations, directing films or writing for famous magicians and hypnotists, but I guess I'll just continue with that in my own time.\nMy advice to students considering work\nWork out what you'd ideally like to do and do that, even if it isn't in this field. It's far more important to enjoy your work than it is to get paid well or be respected by your peers.\nLife is about having fun and work is such a huge part of that, that it has to be fun for it all to work.\nMy advice about working in my industry\nIf you want to be a professional hacker then either a) you're already an amateur hacker and you know your way around gdb and NOP-sleds; b) you've just got interested in security and you're not afraid of learning assembly and giving up your social life in favour of research; or c) you're probably not going to make it, sorry.\nSecurity and hacking qualifications are worth less than hours in a darkened room reading the right bits of the Internet and trying things out.\nYou don't have to grow up to exist in the grown-up world. You don't even need to grow up to get a reasonably-well paid job that offers progression and development. You just need to be dedicated and competent. Happy people always get promoted first.\nI'll happily mentor students wanting to get into information security, particularly hacking. I'll also answer questions on anything to do with work.\nFor those interested, I present at Cyber Security Challenge occasionally and I presented at Securi-Tay at Abertay Uni in Feb 2015.\nIf you like the look of Kevin’s profile, the next steps are down to you! You can send Kevin a message to find out more about their career journey. If you feel you would benefit from more in-depth conversations, ask Kevin to be your mentor.","ZDNet independently tests and researches products to bring you our best recommendations and advice. When you buy through our links, we may earn a commission.Our process\n'ZDNet Recommends': What exactly does it mean?\nZDNet's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we’re assessing.\nWhen you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNet nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers.\nZDNet's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form.\nThe 6 best ethical hacking certifications: Hone your skills\nWhat is the best ethical hacking certification? EC-Council CEH is our top choice because it is globally recognized. Becoming a certified ethical hacker can be a rewarding career. Here are ZDNet's recommendations for the top certifications in 2022.\nHacking isn't necessarily about just having an in-depth knowledge of code: It's about enjoying a challenge and problem-solving.\nWhile understanding the bare bones of computing, specific programming languages, risk analysis, and networking before working your way up is valuable and may help you have a successful career in cybersecurity, the work opportunities vary based on your interests and the path you wish to pursue.\nOne path you can pursue is that of ethical hacking: Learning how to think like an attacker in order to find and remediate vulnerabilities before threat actors are able to exploit gaps in enterprise systems for illicit financial gain, cyberespionage, or to cause damage. Ethical hacking is also core to the concept of bug bounties -- finding vulnerabilities and security problems in services and software on behalf of vendors in return for credit and financial rewards.\nOne aspect of these courses is that they focus more on offense rather than defense, and topics covered often include penetration testing, malware analysis, exploit creation, learning how to use the programming languages which often provide the backbone for today's malware and a study of modern hacking tools.\nBelow, ZDNet has compiled a list of recommended courses to explore in the ethical hacking field.\nThe first recommendation, and perhaps the most well-known option today, is the EC-Council's Certified Ethical Hacker (CEH) qualification.\nCEHv11 teaches students about today's modern hacking techniques, exploits, emerging cybersecurity trends and attack vectors, and how to use commercial-grade tools to effectively break into systems.\nModules also include cyberattack case studies, malware analysis, and hands-on hacking challenges.\nHacking challenges are introduced at the end of each module to put theory into practice, pushing learners to apply their new knowledge of attacks to business settings. The course makes use of ParrotOS, too, an alternative security-based operating system to Kali.\nThis certification would suit a range of roles, including security analysts, pen testers, network engineers, and consultants.\nMost well-known option\nTeaches modern hacking techniques, exploits, emerging cybersecurity trends, and attack vectors\nIncludes e cyberattack case studies, malware analysis, and hands-on hacking challenges\nOffensive Security's Penetration Testing with Kali Linux (PEN-200) is the organization's foundation course in using the Kali Linux OS for ethical hacking.\nThe vendor's focus is hands-on learning rather than just lectures and academic study and encourages both critical thinking and problem solving with the \"Try Harder\" slogan.\nYou will need a solid grounding in network principles, and an understanding of Windows, Linux, and Bash/Python will help.\nIf you're serious about pursuing a career in ethical hacking but are looking for somewhere to start, the OSCP will give you a qualification well-received in the cybersecurity industry. You can also sign up for a subscription to earn your OSCP at a more relaxed pace.\nEncourages both critical thinking and problem solving\nThe OSCP will give you a qualification well-received in the cybersecurity industry\nYou need a solid grounding in network principles\nYou should have an understanding of Windows, Linux, and Bash/Python\nYou should consider another ethical hacking certification, the PEN 300 (OSEP). The course builds upon PEN 200 and offers more in-depth, advanced penetration testing training, fieldwork instruction, and studies in perimeter attack and defense.\nTopics include antivirus evasion, post-exploits, how to bypass network defenses and filters, and Microsoft SQL attacks. You are awarded the OSEP once you have passed the 48-hour exam.\n\"As a general rule, it will not specifically deal with the act of evading a blue team but rather focus on bypassing security mechanisms that are designed to block attacks,\" the vendor says.\nMore in-depth, advanced penetration testing training\nFieldwork instruction and studies in perimeter attack and defense\nTopics include antivirus evasion, post-exploits, how to bypass network defenses and filters\nThe SANS Institute also offers courses that are likely to be of interest to anyone pursuing a career in enterprise security, penetration testing, and ethical hacking.\nOne such course is SEC560, which focuses on on-premise systems, Azure, and Azure AD as a penetration tester. By learning about and exploiting real-world vulnerabilities, learners are taught how to think like a modern attacker and what security holes need to be looked out for when testing enterprise systems.\nThe course includes over 30 practical lab sessions and ends with a Capture The Flag exercise to test your new skills. SANS offers a six-day in-person course or remote learning.\nLearn how to think like a modern attacker\nOver 30 practical lab sessions and ends with a Capture The Flag exercise\nSANS offers a six-day in-person course or remote learning\nCREST is a course provider also of note as an organization that offers professional development qualifications in information security.\nCREST's certifications, accredited globally, are organized into three levels: practitioner, registered and certified. To reach the certified level, you can take exams in subjects including cybersecurity analysis, penetration testing, web applications, threat intelligence, and incident response. Prices vary.\nCREST's certifications are accredited globally\nThree levels offered: Practitioner, registered and certified\nEC-Council CEH is our top choice, but one size doesn't fit all\nIf you're looking at a certified ethical hacking course, you should consider what course is right for you in terms of career development. Cybersecurity professionals are in high demand, and while the career can be a lucrative one, you should have researched whether or not specific qualifications will benefit you in the future, whether at your current job or in a future role. While 'cybersecurity expert' is an umbrella term, the industry itself has distinct career opportunities ranging from penetration testers to compliance, legal, and auditing professionals.\nHow did we choose these certifications?\nOur recommendations are based on courses that offer learners instruction in different areas of ethical hacking: whether focused on offensive security, pen testing, or the aftermath of incidents and the means to investigate as a member of a cyberforensics team effectively.\nWhat roles can an ethical hacking qualification benefit?\nRecruitment paths vary country-to-country, but ethical hacking courses can be of use to those who want to become penetration testers, security analysts -- an umbrella term common in the field -- cyberforensics investigators, consultants, and members of red teams.\nAre there alternative certifications worth considering?\nIf you're serious about a career in ethical hacking, perhaps consider learning more about cybersecurity. It may help you to eventually move up the ladder. What is the best cybersecurity certification? In our review of the top choices, Cybrary is No. 1 because it's an excellent online resource for free and paid video-based courses."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:22cbdd85-a05f-4a21-a92a-c541e1a9c450>","<urn:uuid:55409598-1963-460c-a091-785cc229a2b6>"],"error":null}
{"question":"How do the Supreme Court's handling of racial issues in Kansas v. Glover compare with their approach to racial history in McCullen v. Coakley?","answer":"In Kansas v. Glover, the Supreme Court largely avoided discussing racial implications, despite concerns about racial profiling in police stops. Only Justice Sotomayor expressed concern about stops based on demographic profiles. In contrast, in McCullen v. Coakley, the Court focused on evaluating free speech rights without addressing any potential discriminatory aspects, ultimately ruling 9-0 in favor of McCullen based purely on First Amendment grounds. Both cases demonstrate the Court's tendency to avoid explicit discussions of racial implications, focusing instead on narrow legal interpretations of constitutional rights.","context":["The question in this case: Is the state of Massachusetts violating Eleanor McCullen’s 1st Amendment right to free speech by enforcing a law that prevents her and others from protesting inside a 35-foot buffer zone of the entrance, exits, and parking lots of abortion clinics?\n5-cent explanation: Eleanor McCullen (pictured above), a Massachusetts resident and pro-life protestor, is challenging a 2007 law that increased a buffer zone from 24 to 35 feet outside of abortion clinics. This buffer zone prevents anyone not receiving treatment at the clinics from protesting less than 35 feet outside the clinic and speaking to patients in the parking lot, sidewalks, or near an entrance or exit. McCullen believes this law is a violation of her free speech and expression rights protected by the 1st Amendment. Massachusetts’ claim is that the law is legitimized by a precedent in a similar case in 2000 (Hill v. Colorado), and ultimately protects patients from potential harassment. 10-cent explanation: This case involves 70-year old Eleanor McCullen, a pro-life activist who has brought this suit with six other pro-life 1st Amendment advocates. Their complaint is that a Massachusetts’s law barring anyone from crossing the buffer zone around abortion clinics violates their 1st Amendment free speech and expression rights to offer alternatives to patients getting an abortion. When the state first enacted a law in 2000, harassment of women entering and exiting abortion clinics wasn’t uncommon. The history of harassment outside of abortion clinics in Massachusetts is extensive and, in some cases, fatal. Patients endured bouts of shouting and screaming, were grabbed, had their license plates photographed, and in some cases had butyric acid thrown at them while they entered and exited the clinics. In 1994, John Salvi shot and killed two women, and injured 5 others, at two abortion clinics in Brookline, Massachusetts.\nThe 2000 law banned protestors from standing within six feet from an 18-foot radius to the center of an abortion clinic. The U.S. Court of Appeals for the 1st Circuit upheld the Massachusetts law when it was challenged in 2000 and was further supported in a 6-3 SCOTUS decision on a similar matter in Hill v. Colorado. In the Colorado case, the Court upheld an eight-foot buffer zone around health facilities. They reasoned that a speech restriction law that didn’t discriminate against an expressed viewpoint, was narrowly tailored, left open other means of expression, and was neither overbroad nor too vague passed constitutional muster. To simplify the decision: you have a constitutional right to say something you believe strongly in, but that right does not guarantee your intended audience must hear or listen to your speech. Similar laws that restrict the time, place, and manner of speech have been upheld by the courts and can be enforced by agents of government. Every Tuesday and Wednesday morning, from 7:00AM-11:00AM, you can find Eleanor McCullen outside of a Massachusetts abortion clinic pushing a stroller and handing out literature, including her personal phone number, to women she can reach going in or out of the clinic. Inside the stroller is a DVD player showing ultrasounds of the unborn and pamphlets on pregnancy medical centers nearby. McCullen considers her work “sidewalk counseling.” Instructed by her priest to activate a message she believes she received from the Holy Spirit eleven years ago, McCullen took up this method of outreach. The priest told McCullen that her “purpose was to ‘stretch herself’ out of her comfort zone in order to build up the kingdom of God.” He recommended she do this by protesting local abortion clinics. Eleanor McCullen has reached out to many women seeking abortions, offering them counseling, alternatives, even offering to host them baby showers, in which she will give them all the items new mothers and babies need. McCullen admits the showers are opportunities, or incentives, for the women not to have an abortion. She feels strongly in speaking to these women respectfully, kindly, and in a way that isn’t threatening, and has been openly critical of aggressive anti-abortion advocates. Her problem with the 2007 law is not simply that it exists but that it allows certain people to approach these women – employees, nurses, and doctors – while others are legally barred from doing the same. McCullen argues this practice constitutes “viewpoint discrimination.” Her attorney will argue the employees of the clinics could encourage women to have the abortion while the buffer zone prevents the opposing side from also encouraging the women from receiving the abortive procedure. McCullen raises a few fundamental questions about 1st Amendment rights one has to deliver speech and to protest to an intended audience. First, is Massachusetts’ 35-foot buffer denying some access to their intended audience (McCullen and the protestors) while simultaneously granting others (employees of the clinics) access? If so, this technically could be viewpoint discrimination and the law may be struck down. Second, is there a distance that provides opportunities for speakers to reach their intended audience while also protecting patients from harassment? If 35-feet is too much and 24-feet is too little, can the Court split the difference between the two to reach a compromise? It’s also worth noting two key points: Only two of the six who supported the 2000 Colorado law are still on the bench (Ginsburg and Breyer) while three who voted against it (Scalia, Thomas, and Kennedy) are not only still around but likely to garner support by justices Roberts and Alito. Finally, if the justices strike down the Massachusetts buffer zone as an unconstitutional violation of the free speech, will the Court reverse the current ban in place for anyone who wants to protest on the steps of the Supreme Court? Doubtful, but we’ll wait to see what happens.\nDecision: 9-0 for McCullen. Chief Justice John Roberts, writing for the majority, found McCullen’s sidewalk counseling to be the very non-hostile, unthreatening speech the 1st Amendment seeks to protect. He continued that her approach was more like that of a counselor than a protestor. Therefore, the 35-foot buffer zone around MA abortion clinics impairs her efforts and infringes on her right to speak freely with her target audience (patients coming in/out of the clinics) about their options. While McCullen won in this case it’s important to note that the Court did not strike down the legality of content-neutral laws like buffer zones. The decision was reversed and remanded back to the lower court where Massachusetts’ legislators will get to recreate buffer zones that both protect patients and civil liberties.","Supreme Court justices typically write opinions that say more than what is strictly necessary to decide the case before them. In those opinions, the justices also communicate with their colleagues, other courts, and the country about the issues, values, and people they deem especially important. When it comes to the possibility and history of racism, however, most of the current justices—with the important exception of Justice Sonia Sotomayor—tend to respond the way so many white people do: More often than not, they would rather just not talk about it.\nThat tendency was evident this past term in most of the cases potentially implicating the subject of race.\nIn Kansas v. Glover, the Court held that a police officer could lawfully stop a vehicle about which the officer knew only that its owner had a revoked driver’s license. In 2016, an officer in Lawrence, Kansas, ran a license-plate check on a moving pickup truck and found that the license of the registered owner—Charles Glover—had been revoked. The officer stopped the truck, which Glover was driving, and Kansas charged him with driving as a habitual violator. In his defense, Glover argued that the stop violated the Fourth Amendment, which protects people against “unreasonable searches and seizures,” and the Kansas Supreme Court ruled in his favor, arguing that the officer had not adequately justified the stop and instead had “only a hunch” that Glover was driving.\nOne wouldn’t know it from the several opinions written by the justices, but the defendant appears Black. Does racial profiling explain the officer’s decision to run the vehicle’s license plate? One cannot know.\nThe Fourth Amendment protects people’s reasonable expectations of privacy when they are subject to searches and seizures by the police, but it is difficult to argue that people have such an expectation for their license plates, which are readily viewable by others when an individual is driving. So the Court understandably declined to read the Fourth Amendment as requiring police officers to explain their decision to run a license plate. But the risk of racial profiling is significant, and the justices might have drawn attention to the problem. That the officer spends most of his time running license plates seems unlikely, and at no point in this case did he give an explanation for his action. The justices might have also mentioned that police departments could insist that officers explain their reasons for running a vehicle’s license plate, to help limit racial profiling. Only Sotomayor expressed concern about the police stopping vehicles “based on nothing more than a demographic profile.”\nThere was no claim of racial discrimination in this specific case, so the amicus briefs filed in Glover’s favor did not focus primarily on the issue of racial profiling. But some of them did mention racial and socioeconomic disparities in police stops across the country; one of them emphasized that Black drivers are more likely than white drivers to be pulled over by the police and pointed to “evidence that racial bias motivates many traffic stops.” The lack of more extensive discussion in the briefs may in part be a function of the Court’s long-standing resistance to discussing the subject in Fourth Amendment cases.\nGlover is not a one-off. It is extraordinarily rare for the justices—again, except for Sotomayor—to reference race or racism in cases involving police stops. For example, in Utah v. Strieff, in 2016, the Court held that when a police officer discovers a preexisting warrant for a person’s arrest, incriminating evidence seized pursuant to that arrest is admissible in court even if the officer’s stop of the individual was unconstitutional to begin with. The Court had nothing to say about whether it was enabling police to engage in racial profiling. Sotomayor wrote for herself alone that “it is no secret that people of color are disproportionate victims of this type of scrutiny.”\nThere are many other examples one could cite and very few counterexamples. Racism was mentioned briefly in a majority opinion in 1968, in Terry v. Ohio, which provided the constitutional basis for the stop that occurred in Glover. There was also a brief discussion of race-based selective enforcement of the law in 1996, in Whren v. United States, but there the Court actually said that issues such as racial profiling and pretextual reasons for traffic stops may not be evaluated under the Fourth Amendment. Occasionally a justice will mention concerns about racism in a dissent, but that’s pretty much it.\nThe Deferred Action for Childhood Arrivals (DACA) case (Department of Homeland Security v. Regents of the University of California) is another example from this term in which the majority might have at least acknowledged the freighted racial or ethnic context and potential implications of a case, regardless of whether doing so would have altered the outcome. Instead, the majority opinion, which held that the Trump administration had acted arbitrarily (and thus unlawfully) in rescinding the DACA program, limited itself to deciding whether the administration had violated procedural requirements set forth in the Administrative Procedure Act. Four of the five justices in the majority (and all four dissenters) rejected the argument of the respondents that the decision to end DACA was unconstitutional because it had been motivated by animus against the beneficiaries of the program, concluding that there was “no plausible equal protection claim.” Only Sotomayor talked about the possibility of racism given Donald Trump’s past public statements that Mexican immigrants are “people that have lots of problems,” “the bad ones,” and “criminals, drug dealers, [and] rapists,” and his statement likening undocumented immigrants to “animals” who are responsible for “the drugs, the gangs, the cartels, the crisis of smuggling and trafficking, [and] MS13.”\nIt should not be considered out of bounds for the justices to worry publicly about the possibility of racism just because the racist statements are made by the president of the United States. Such statements invariably affect federal officers further down the chain of command, regardless of whether there is sufficient evidence of such influence in a given case to change the result.\nThere were times this past term when most of the justices did better. In Ramos v. Louisiana, the Court held that the Sixth Amendment right to a jury trial requires a unanimous verdict to convict a defendant of a serious crime in both federal and state courts. As briefs on behalf of the defendant emphasized, the history of non-unanimous jury verdicts has everything to do with race and racism. Louisiana and Oregon originally permitted non-unanimous jury verdicts to negate the impact of Black and other minority jurors and thereby increase the likelihood of convicting Black people and other minorities.\nThe history in Louisiana is especially ugly. Reconstruction ended in 1876 as part of a compromise among white people that gave Republicans the White House and Democrats the end of military rule in the South. As Republicans tired of pursuing racial equality and joined the Democrats in courting business interests, former Confederate states were emboldened, and established an authoritarian, apartheid social order. Louisiana first embraced non-unanimous verdicts at its constitutional convention in 1898, when it declared white supremacy to be the official policy of the state and adopted numerous measures to disenfranchise Black people on a massive scale. Mississippi did much the same at its constitutional convention in 1890, and put the Confederate battle flag on its state flag in 1894.\nTo its credit, the majority in Ramos did emphasize the racist history of laws permitting non-unanimous jury verdicts. But it did so over the fierce objections of the three dissenters. Writing for them, Justice Samuel Alito accused the majority of using “ad hominem rhetoric” that “contribut[es] to the worst current trends” in public discourse. Alito also declined to concede the existence of this racist history, instead deeming the issue irrelevant to the outcome of the case. “If Louisiana and Oregon originally adopted their laws allowing non-unanimous verdicts for these reasons, that is deplorable,” he wrote, as if the point were fairly debatable, “but what does that have to do with the broad constitutional question before us? The answer is: nothing.”\n(Yet in another case this term, Espinoza v. Montana Department of Revenue, Alito wrote a concurring opinion in which he elected to emphasize the anti-Catholic history of certain laws prohibiting government aid to religious schools. It was not necessary for him to have done so to decide the case; he had already joined the majority opinion, which itself noted this history even though the case almost certainly would have come out the same way had the history of anti-Catholic bigotry been different. So it is not as if he disfavors talk of history in general.)\nIn another case, also this term, the Court again acknowledged some of the relevant history of racism in the United States, but it did not get the history quite right, making it sound better than it was. In Comcast Corporation v. National Association of African American–Owned Media, the Court held that a plaintiff who sues for racial discrimination in contracting under a federal law, 42 U.S.C. § 1981, has to show that race was not just a motivating factor, but a but-for cause of the plaintiff’s injury. It is much harder to prove but-for causation, which is why a major corporation, the Chamber of Commerce, and the Trump administration wanted this requirement, while civil-rights groups opposed it. This case obviously had to do with potential racism: It was expressly about proving certain claims of racial discrimination.\nWhat is less obvious is that Congress originally passed Section 1981 as part of the Civil Rights Act of 1866, after the Civil War, to protect certain rights of formerly enslaved people. Concerns about whether the Thirteenth Amendment (which ended slavery) permitted Congress to pass this law led to the passage of the Fourteenth Amendment (which, among other things, includes a guarantee of constitutional equality).\nWhat is also less obvious is how narrow was the understanding of racial equality embraced by the Congress that passed the Civil Rights Act of 1866. The Court in Comcast said that the law had been passed “to vindicate the rights of former slaves.” Sadly, the history is more complicated and depressing. The law was passed to protect the civil rights of formerly enslaved people, such as contracting, suing, and owning property. Civil rights for Black Americans at that time did not include political rights such as voting (hence the need for the Fifteenth Amendment) or social rights such as attending public schools with white children and marrying white people (rights—to America’s national shame—not protected by the Court until 1954 and 1967, respectively). These distinctions among civil, political, and social equality, which are not required by the text of the Constitution and no longer exist in U.S. constitutional law, were intended both to grant some measure of equality to the citizens who had been slaves and to strictly limit how much and what kind. By describing Section 1981 in sweeping terms, the Court failed to acknowledge the more sobering portions of the relevant history.\nIt would have been interesting to see the Court respond to a constitutional challenge to affirmative action or to the Voting Rights Act in the current political climate. But this term does illustrate why, in light of American history, racism and race relations remain the nation’s most enduring domestic crisis.\nThe impulse of most of the justices, most of the time, not to talk about race and racism is not excusable, let alone justifiable, but it is understandable. Why risk making oneself, one’s colleagues, and other audiences uncomfortable or even upset when it is not absolutely necessary to “go there”? When it comes to the subject of racism in this country, however, saying nothing often is saying something.\nThe general silence of the justices can have spillover effects that produce bad law in cases in which correct interpretation of the Constitution and statutes requires serious engagement with the long, tragic history of racism in this nation—and with its continued existence. For example, a Court more attuned to history and current social reality would not have been as likely to declare that “things have changed dramatically” and a key part of the Voting Rights Act is no longer constitutionally justified, which is what the Court said and held in Shelby County v. Holder in 2013. The Court’s equal-protection jurisprudence in the areas of affirmative action and disparate-impact liability might also look different. But if more often than not the justices can’t—or won’t—grapple with the nation’s racist history and present, then more often than not the law articulated by the Court will continue to be unworthy of the aspiration engraved over the front entrance to the building in which it sits: equal justice under law."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:27ce2cef-a32b-44ba-b3ae-ee0142ba3358>","<urn:uuid:83663ae7-e488-4716-b864-bda1c1c161e3>"],"error":null}
{"question":"Did King George III witness both the American Revolution and Bloody Sunday in Russia?","answer":"No. While King George III was alive during the American Revolution (which began in 1775 and ended with the Treaties of Paris in 1783), Bloody Sunday in Russia occurred in 1905, long after King George III's death in 1820. The event involved demonstrators being gunned down by the Imperial Guard while attempting to present a petition to Tsar Nicholas II.","context":["[What the opponents of Roe v. Wade seem unable or unwilling to understand - I mean, how could they, since a diet of strong rhetoric is antithetical to understanding - is that absolutely no one is pro-abortion... If they were I'd be the first person to oppose them. Being pro-choice is about not wanting strangers to be able to tell you what to do with your body; once these zealots get the right to force women to carry babies they don't want to full term, whose bodies are they going to go after next? I think I know. Will they allow me to force them not to go to church anymore? I didn't think so..]\n565 CE - Eutychius was deposed as Patriarch of Constantinople by his eventual successor, John Scholasticus, while celebrating the feast day of St. Timotheus; Byzantine Emperor Justinian the Great had masterminded the coup himself, but didn't live to see Eutychius return from his exile at Amasea a dozen years later and resume the role during the reign of Justin II.\n1506 - The first contingent of 150 Swiss Guards arrived at the Vatican.\n1863 - The January Uprising broke out in Poland, Lithuania and Belarus; the aim of the national movement is to regain Polish-Lithuanian-Ruthenian Commonwealth from occupation of Russia.\n1877 - Anglican clergyman Arthur Tooth was taken into custody at London's Horsemonger Lane Gaol after being prosecuted for using ritualist practices.\n1879 - During the Anglo-Zulu War Zulu troops defeated British troops at the Battle of Isandlwana while on the same day at Battle of Rorke's Drift 139 British soldiers successfully defend their garrison against an intense assault by four to five thousand Zulu warriors.\n1890 - The United Mine Workers of America was founded in Columbus, Ohio.\n1901 - Queen Victoria died; Edward VII became King.\n1905 - When unarmed, peaceful demonstrators marching to present a petition to Tsar Nicholas II were gunned down by the Imperial Guard in St. Petersburg the day became known as Bloody Sunday, and the event sparked the 1905 revolution.\n1924 - Ramsay MacDonald became the UK's first Labour Prime Minister.\n1931 - Sir Isaac Isaacs was sworn in as the first Australian-born Governor-General of Australia.\n1957 - New York City's 'Mad Bomber', George P. Metesky, was arrested in Waterbury, Connecticut, and charged with planting more than 30 bombs since 1940.\n1968 - Rowan & Martin's Laugh-In made its television debut on NBC.\n1973 - The US Supreme Court handed down its decision in the case of Roe v. Wade; the measure was opposed only by Byron White and William Rehnquist.\n1987 - R. Budd Dwyer, a state politician from Pennsylvania embroiled in controversy, committed suicide on live television during a press conference.\n1992 - Dr. Roberta Bondar became the first Canadian woman in space during NASA's Space Shuttle mission STS-42.\n1997 - Madeleine Albright became the first woman to serve as US Secretary of State.\n1999 - Australian missionary Graham Staines and his two sons were burned alive by a mob of radical Hindus while sleeping in their car in Eastern India.\n2002 - Kmart Corp became the largest retailer in American history to file for bankruptcy protection under Chapter 11.\n2007 - The jury portion of the trial against Robert Pickton, accused of being Canada's worst serial killer, opened in New Westminster, British Columbia; in December 2007 he was found guilty on six counts of second-degree murder, and still awaits trial on 20 more.\nshare on: facebook","King George III reigned from 25 October 1760 until 29th January 1820, a total of 59 years, 3 months and 2 days. From the date he ascended to the throne until 1 January 1801, he was King of Great Britain and King of Ireland.\nAfter 1801 George was King of the United Kingdom of Great Britain and Ireland until he died.\nHe was also Prince Elector of Hanover until October 1814, when he became King of Hanover, and was Duke of Brunswick-Luneburg.\nAlthough the third of the King Georges of the House of Hanover, he was the first to be born in the United Kingdom and to speak English as a first language.\nUnlike the other two, who spent most of their time in Hanover, he never actually visited the place at all. King George’s reign was tumultuous. The first British Empire came to an end when the American War of Independence led to the establishment of the United States of America.\nOn the other hand, in long running battles against France, Napoleon was finally defeated at the Battle of Waterloo in 1815.\nFamily and Childhood\nThe Hanover Kings had distinctly odd families. King George III was the grandson, rather than son, of the preceding monarch King George II.\nKing George II’s oldest son was Frederick Prince of Wales. George II disliked his eldest son the Prince of Wales and there was very little communication between George II and Frederick.\nKing George III’s parents\nPrince Frederick Louis was the eldest son of Prince George, later to be King George II, and George’s wife, Queen Caroline, born Wilhelmina Charlotte Caroline of Ansbach.\nPrince George August and Caroline married in August 1705 and their first child, Prince Frederick, was born in February 1707.\nWhen George I took the British throne in 1714, Prince George and Princess Caroline moved to the United Kingdom, leaving their 7 year old son Frederick behind in Hanover. They did not see him for another 14 years.\nA large number of younger children had been born to the couple by the time Frederick arrived in England, and George and Caroline referred to their eldest son as a foundling, and nicknamed him “Griff” or “Griffin”.\nIn 1736 Prince Frederick married 16 year old Augusta of Saxe-Gotha. The couple had a total of 9 children, the last, a daughter, being born posthumously.\nGeorge William Frederick was born on 4 June 1738 in London at Norfolk House. He was 2 months premature, but grew into a healthy although shy child.\nGeorge was, unlike many of the Hanover royal children, well-educated.\nHe could read and write in both English and German by the age of 8, and also studied astronomy, chemistry, physics, mathematics, French, Latin, history, music, geography, agriculture and constitutional law.\nWhen George’s father died suddenly in 1751, King George II decided to take an interest in his grandchildren for the first time. 3 weeks after Frederick died, George II created Prince George as the Prince of Wales.\nMarriage and Family\nKing George II died at the age of 76 on 25 October 1760.\nA wife was clearly needed for the new King George III, and on 8 September 1761 the King married Duchess Sophia Charlotte of Mecklenburg-Strelitz.\nAt the time of their marriage, George III was 24, and Charlotte was 17. The couple met for the first time on the day they married.\nA fortnight after the wedding, the King and Queen were crowned together at Westminster Abbey.\nDespite an extremely arranged marriage, the couple appeared to have been genuinely happy. George III is not known to have had a mistress at any time, unlike the vast run of Hanoverian royals who were knee deep in mistresses and illegitimate children.\nKing George III and Queen Charlotte had 15 children in total. 13 of these children survived to adulthood.\n1. George IV born 12th August 1762\n2. Frederick, Duke of York, born 16th August 1763\n3. William IV born 21st August 1765\n4. Charlotte, Princess Royal born 29 September 1766\n5. Edward, Duke of Kent born 2nd November 1767\n6. Princess Augusta Sophia born 8th November 1768\n7. Princess Elizabeth born 22nd May 1770\n8. Ernest Augustus I of Hanover, Duke of Cumberland born 5th June 1771\n9. Augustus Frederick, Duke of Sussex born 27th January 1773\n10. Adolphus, Duke of Cambridge born 24th February 1774\n11. Princess Mary, Duchess of Gloucester born 25th April 1776\n12. Princess Sophia born 3rd November 1777\n13. Prince Octavius born 23rd February 1779\n14. Prince Alfred born 22nd September 1780\n15. Princess Amelia born 7th August 1783\nThe two youngest sons died before they were 5 years old, and the other 13 children lived to adulthood.\nWhat those children didn’t do was marry and produce legitimate heirs for the British and Hanover thrones.\nKing George and Queen Charlotte were the first of the royal family to live in Buckingham House, which later became known as Buckingham Palace.\nIt was originally intended as a private retreat for the King and Queen, and was known as the Queen’s House.\nSt James’ Palace remained the official and ceremonial royal palace in London. 14 of the couple’s 15 children were born at Buckingham House.\nQueen Charlotte was possessive of her children, in particular her daughters, and kept them close by her side and refused to allow them to marry until they were in their 30s or 40s. None of the King and Queen’s daughters had children.\nKing George III’s Reign\nIt is unnecessary to write much of a summary of the most important political events of George III’s reign. Suffice it to say, that his reign saw the end of the first British Empire, when the American War of Independence began in April 1775.\nKing George III’s reign had, as the Declaration of Independence put it, “abdicated government here, plundered our seas, ravaged our coasts, burnt our towns, and destroyed the lives of our people.”\nAfter Lord Cornwallis’ surrender at the siege of Yorktown in 1781, King George III drafted an abdication notice, although he never acted upon it. The King accepted, finally, the defeat in North America and authorised peace negotiations.\nThe treaties of Paris were ratified in 1783 and the United States of America became an independent country recognised throughout the world.\nKing George III told John Adams, American Minister to Britain in 1785 that:\nI was the last to consent to the separation, but the separation having been made and having become inevitable, I have always said, as I say now, that I will be the first to meet the friendship of the United States as an independent power.\nWhat King George III was like\nGeorge III was extremely interested in agriculture and science.\nDuring his reign the agricultural revolution really kicked off, allowing the release of a huge number of previously agriculture workers to become the workforce for the industrial revolution, in which Britain led the world.\nHe was nicknamed by pamphleteers and caricaturists as, “Farmer George” but the nickname later became an affectionate one, especially as his sons ran up huge debts and failed to do anything very useful. By comparison, George III’s interests in agriculture and science came to be seen as positively virtuous.\nGeorge collected large numbers of scientific instruments, which can now be seen at the Science Museum in London, and funded the largest ever telescope built at that time, which was 40 feet.\nThe Madness of King George\nBy 1788, King George III was suffering from the first spell of mental illness, which later took over his life. It is now thought likely that the illness from which he suffered was Porphyria, a genetic illness.\nIn 1788, he began to suffer a particularly acute episode of the illness. At the end of the summer in 1788 he went to Cheltenham Spa. Although only 100 miles from London, this was the furthest King George III had ever ventured in his life.\nBy November George had become seriously ill, speaking for hours without pausing for more than breaths, foaming at the mouth, and becoming increasingly unwell.\nArguments between parliamentarians were underway and the Regency Bill was introduced in parliament in February 1789 authorising Prince George, the Prince of Wales, to act as Prince Regent whilst his father was incapacitated.\nHowever, before the Bill could be passed into an Act of Parliament George III recovered.\nKing George suffered another serious episode of mental illness in 1804, which lasted for approximately 9 months.\nIn 1810, King George III was almost blind, from severe cataracts, and also suffered from bad rheumatism which left him in constant pain. The death of his youngest daughter, Princess Amelia, triggered another severe episode of mental illness.\nThe Regency Act 1811 was passed, and Prince George, Prince of Wales (the future George IV) acted as Regent for the rest of George III’s reign.\nBy November 1811, King George III had become permanently insane. He lived at Windsor Castle, secluded from the public and the world, for the next 9 years.\nHis illness was so severe that he was unaware that he became King of Hanover in 1814, or that Napolean was defeated at Waterloo in 1815, and was similarly unaware that his wife died in 1818.\nHe was completely blind, almost completely deaf, and suffered severe mobility problems. He died in January 1820 at Windsor Castle.\nKing George III was followed on the throne by two of his sons, King George IV and King William IV, and both of whom who died without legitimate children.\nThe throne was then inherited in 1837 by King George III’s granddaughter, Queen Victoria, the child of George III’s fourth son, Edward Duke of Kent.\nLink/cite this page\nIf you use any of the content on this page in your own work, please use the code below to cite this page as the source of the content.\nLink will appear as Hanson, Marilee. \"King George III\" https://englishhistory.net/house-of-hanover/george-iii/, January 17, 2022"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:73609534-6f16-42e4-8a38-aa35165a2d36>","<urn:uuid:fdbbff0c-d0ff-4707-a591-2df741513137>"],"error":null}
{"question":"How do the manufacturing approaches for the FN 15 Tactical II barrel compare to the Adcor Defense B.E.A.R.'s rail system in terms of precision and durability?","answer":"The FN 15 Tactical II features a hammer-forged barrel manufactured through a process where hammers beat steel around a mandrel, resulting in precise internal dimensions, exact rifling, and work-hardened material that increases durability. Meanwhile, the Adcor Defense B.E.A.R.'s rail system is made from 7075-T6 material and uses an innovative key-locked design that mounts seamlessly to the upper receiver through a boss, spline, and groove system. The rail system ensures proper alignment and allows for easy separation of upper and lower rail guards with a push button, while maintaining rigid construction for mounting various accessories.","context":["The popularity of the AR is problematic for anyone looking to purchase one of these rifles. There are so many types at a wide variety of price points that it’s difficult to know what AR works best for any particular application. Since the answer is so often “general use,” this review highlights the FN 15 Tactical II.\nOther than Colt, FN is the only manufacturer that can claim they’ve made ARs in large quantities for the U.S. military. That fact matters because it means the company has a track record of producing rifles that meet required battle-ready performance specifications.\nPerhaps the two most important parts of any AR-type rifle are the bolt and the barrel. Both have to be made from quality materials and manufactured to offer performance and long life. The Tactical II offers both in the FN 15’s critical components.\nOne Tough Bolt\nFN has produced bolts for so long that they know how to make them correctly. If they didn’t, they would have quickly lost their military contracts. In the creation of the bolts, FN follows all of the specified handling protocols of magnetic particle inspection (MPI), peening, pressure testing, etc.\nAR-pattern bolts have two lugs that are prone to break under very high use (i.e., after several thousand rounds fired). The two lugs opposite the extractor are only supported on one side, so they are usually the first to break. One way to get to prolong their life span is to use a steel alloy, either Carpenter 158 (the military’s specified material) or 9310, and to get the heat-treat process correct.\nFN uses Carpenter 158 for its Tactical II bolts. However, heat-treating a small part like a bolt is difficult. Carpenter 158 steel was originally intended for large industrial gears. It is very tough, but it is remelted steel made from a process that uses two large electrical probes to burn excess sulfur and phosphorus out of the material.\nRemoving the sulfur and phosphorus from the steel alloy gives it a very predictable wear cycle, which is why Carpenter 158 is one of the recommended materials. Unfortunately, the absence of those materials makes bolts difficult to correctly heat-treat. Small variations in soak time and temperature can mean the difference between a part that lasts almost forever and one that breaks after a thousand rounds. But FN did it right, and their bolts can be depended upon.\nThe barrel FN uses on the Tactical II is the preferred length (16 inches) and type for the most common general-use AR. Most inexpensive ARs use button-rifled barrels because they are easy and inexpensive to make. FN chooses to use hammer-forged barrels instead.\nAny AR barrel should be able to handle a lot of rounds in a short time period and survive this tempo for several years. Hammer forging a barrel is very helpful in this regard. The process of making a hammer-forged barrel involves first inserting a mandrel through the center of a short, thick cylinder of steel. Hammers then beat the steel until it takes the shape of the finished barrel. This usually means the barrel is twice as long as the cylinder that started the process, so it has undergone significant pressure.\nAll that pressure gives this type of barrel a couple of very useful characteristics. The bore will be a mirror copy of the mandrel, so the internal and external dimensions are very precise and the rifling is exact. In inexpensive button-rifled barrels, that process can have irregular rifling because the button can slip when pulled through the bore.\nAnother significant advantage of hammer-forged barrels is the work-hardening that occurs during the manufacturing process. As those hammers beat the barrel into shape, the portion that sits directly against the mandrel gets very hard because it cannot move. Every time the hammer strikes, it squishes the barrel material up against the mandrel making it harder. This is a very useful phenomenon when talking about barrel bores. The harder the bore, the longer it will last because it can better resist the heat and pressure that comes when firing cartridges. FN has made hundreds of thousands of these barrels over a few decades and has long since perfected the process.\nIn keeping with the ideal AR theme, FN had the foresight to put a mid-length gas system on this rifle. Stretching the length of the gas tube that runs atop the barrel means the bullet has to move further down the bore before the action cycles. When the bullet moves down the barrel, the pressure inside the bore drops. A longer gas tube means lower operating pressures.\nARs are happiest when pressures are low. Residual pressure in the chamber makes it hard for the bolt to twist to unlock and is the prime reason why bolts break. Lowering the pressure in the bore adds a lot of life to the bolt by making it easier for it to unlock and extract the fired case. There is no good reason to have the shorter carbine-length gas system on a 16-inch barrel. Mid-length is always the right answer.\nGetting a Grip\nThe handguard found on the Tactical II is absolutely one of the best factory types made. It is 13½-inches long and beautifully machined. There are no sharp edges on it anywhere and the octagonal shape sits comfortably in the hand. Three of the eight sides are flat enough for M-Lok slots — U.S. Special Operations Command’s preferred attachment method — and help keep the rifle oriented in the support hand.\nThe rifle also features Magpul’s MOE grip and enhanced triggerguard. The grip is comfortable and the triggerguard protects the firing hand’s middle finger by filling the gap normally present between the triggerguard and grip, while also accommodating a gloved trigger finger.\nFarther back sits the adjustable Magpul MOE SL buttstock. It has a slender profile that puts plenty of meat against the firing shoulder. Its adjustment lever is tucked out of the way where accidental activation is unlikely. There is a metal flush cup on either side of the stock for use with quick-detach sling swivels and two locations where the sling can loop around the stock for direct attachment.\nMagpul is also one of the very few companies that beats the brakes off their products in testing. The MOE Slim Line (SL) buttstock is so durable that you could extend it and then repeatedly slam it to the ground without breaking the stock. It is highly likely that the positioning detent in the buffer tube will deform before the stock breaks. The SL is a fantastic addition and should be a serious consideration on other AR-15s as an aftermarket accessory.\nPerformance at the range was about what you’d expect from a quality hammer-forged, free-floated barrel. Five-shot groups at 100 yards hovered right around 1 MOA, which is above-average performance from any AR.\nFN got the details of this rifle right. They kept weight down with an excellent choice in barrel contour and then used a hammer-forged tube to get maximum life. All of FN’s tribal knowledge went into making sure they got the manufacturing details right as far as dimensions and heat-treat are concerned.\nFinally, FN made sensible choices on the furniture and handguard. There is no excess anywhere on the FN 15 Tactical II, but it absolutely has every relevant feature you’d want or need on a general-use, AR-pattern rifle.\nFN FN15 Tactical II\nType: Direct impingement, semiautomatic\nCartridge: 5.56 NATO\nCapacity: 20 or 30 rds.\nBarrel: 16 in.; 1:7-in. twist\nOverall Length: 33.7 in. (collapsed), 37 in. (extended)\nWeight: 6 lbs., 11 oz.\nStock: Magpul MOE SL\nGrip: Magpul MOE\nLength of Pull: 11 in. (collapsed), 14 in. (extended)\nFinish: Anodized, type III, hardcoat\nSafety: Two-position selector lever\nManufacturer: FN America, 703-288-3500,\nEnjoy articles like this?\nSubscribe to the magazine.\nGet access to everything Guns & Ammo has to offer.\nSubscribe to the Magazine","Adcor Defense 5.56 x 45mm Semi-Auto B.E.A.R. Brown Enhanced Automatic Rifle combines design innovation and precision manufacturing built upon the tried and true M4/M16 rifle platform. The Adcor Defense B.E.A.R. springs from the M4-M16 rifle platform but offers a completely new twist. Available in many configurations, it offers the operator many significant improvements:\nA Revolutionary Gas Piston System (Patent # 065111.00128)\nFree-floating barrel and increased accuracy: A gas-driven piston incorporated into the upper half of the forward rail system never touches the barrel. No carrier tilt: The operating rod is connected to the bolt carrier by hex head bolts reinforced with industrial grade Loctite 262 high strength threadlocker. They are then drilled and pinned to prevent loosening under adverse conditions. The operating rod then rides through a machined boss in the receiver rail system interface keeping the bolt carrier in a linear plane as it operates.\nNo fouling of chamber and action: Hot gases and carbon particulates are prevented from entering the receiver. Because heat reaching the upper receiver is diminished, lubricant lasts longer and maintenance is simplified.\nEasily detached lower rail system: No special tools needed for quick cleaning and easy maintenance.\nWear on critical parts is minimized: The operator can easily adjust the piston’s cyclical rate for a controlled rate of fire.\nNo need for piston rings: Extremely close machine tolerances of the piston eliminate the need.\nExclusive Port Dust Wiper with Cover\nPrevents entry of dust, sand or debris: The spring-loaded dust shield mounted on the bolt carrier allows the bolt carrier’s return to ready position; the port dust wiper moves into the ejection port opening flush with the inside geometry of the upper receiver.\nWithstands temperature extremes because of the highly durable, self-lubricating polymer shield.\nAmbidextrous Forward Placed Charging Handle: Simplicity is Beautiful.\nSimple design allows the operator to keep his or her strong hand on the firearm and use the other hand to clear the rifle.\nAmbidextrous handle detaches easily without special tools and returns to locked position once used. It folds forward, recessed, and easily swings back out in a single motion.\nNon-reciprocating handle only engages when the operator charges or clears.\nRail System ( 7075-T6 )\nInnovative, key locked, highly rigid rail system mounts seamlessly to the upper receiver: Design ensures proper alignment of the rail with a redesigned boss, spline and groove system.\nUpper and lower rail guards separate with the push of a button: No special tools necessary.\nFully compatible with optical, image intensification or thermal sights; laser targeting systems; bipods; tactical lights and Multiple Integrated Laser Engagement System (MILES).\nMack's sells firearms in accordance with Federal, State, and Local laws regarding firearm transfer and ownership. To purchase firearms online, please check your local and state laws and then complete the steps listed in our instructional guide on How to Purchase a Firearm Online . Guns must be purchased separately from all other items and transferred through an FFL dealer (no international shipments). Most gun orders ship within 24 business hours of MPW’s receipt of all necessary paperwork."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:78cc620a-5bf9-4bf2-84fe-e5ee1ea37de5>","<urn:uuid:22d08b91-459a-4fad-be22-2a59354f7fd5>"],"error":null}
{"question":"What equipment considerations are essential for safe hiking, and how can mindfulness practices improve the outdoor experience?","answer":"Essential hiking equipment includes proper footwear (such as waterproof hiking boots with good traction), appropriate layered clothing (base layer for wicking, mid layer for insulation, outer layer for protection), navigation tools (map and compass), and safety essentials like first aid kits, lighting, and fire starters. Proper hydration equipment and nutritious snacks are also crucial. To enhance the outdoor experience, mindfulness practices like focused meditation can be beneficial. This type of meditation helps build awareness, reduce stress, and increase concentration. It can be practiced anywhere quiet, requires no instructor, and involves focusing on specific sensory stimuli. Studies show it helps reduce anxiety and depression symptoms while rewiring neural networks in the brain, making it particularly valuable for maintaining mental clarity and emotional balance during outdoor activities.","context":["In this how-to program we will focus on hiking and all of the basic skills and hiking equipment, clothing and boots you will need to have for a great new outdoor experience with friends and family.\nINDEX: Quick links in this guide to learn how to:\nPlanning & Packing for a Hike\nChoose the Right Hiking Boot or Shoe\nHow & Why Dressing in Layers to Hike is Important\nHow to Use a Map and Compass\nHow to Stay Properly Hydrated\nPack the Best Hiking Snacks\nSafe Hiking Gear & Supplies\nWhere to Go for a Great Hiking Adventure\nHow to Keep Kids Motivated While Hiking\nBegin by Planning the Hiking Trip Early\nPlan for the weather: Is there a chance of rain and flooding? What are the high and low temperatures expected for the days you plan to be out?\nGain knowledge of the location: Let's assume you're planning to explore a national park. You can begin by visiting the website, social media, blogs, and asking local outfitters and others that have visited the location.\nCan you think of some things that would be beneficial to know about the location where you will be hiking or camping?\n- Are there marked trails?\n- Does the campground offer electric hook-ups?\n- Can I bring my dog?\nTell someone before you leave: As a safety precaution always tell someone where you will be staying and the route you will be hiking This is important just in case you were to get lost or hurt, someone would know where to begin looking. Another key safety tip is to invite someone to join you. Two hikers are safer than hiking alone.\nTip: Explore some of the most popular national parks or browse the full list of all the parks in the National Park System here\nRemember to Pack the Right Hiking Essentials\nJust like many outdoor outings, packing your hiking gear can make or break your experience particularly if your are taking the kids.\nOne of the first things to consider is the proper footwear. When you hit the trail, you don’t want to think about your feet. It’s all about taking in the view, the wonder of nature along the way, and the beauty of being out there. A hiking boot that hits all the marks when it comes to comfort, fit, and performance is the way to go.\nHow to Choose the Right Hiking Boot or Shoe\nWhen choosing your hiking footwear you need to consider 4 different things.\n1. Destination: Think about where you will be hiking and what type of terrain you will be in. Will it be desert, mountain, water, gravel trail or a paved trail?\n2. Boot Fit Considerations: The arch height, foot width, injuries or disabilities all factor into getting the most comfortable hiking boot fit possible. For example:\n- Low Cut Hiking Shoe or High Cut Hiking Boots - Consider wearing a mid-cut-hiking boot they are the go to shoe. This will help protect you from the elements and animals and help with stability, especially if carrying a heavy load.\n- Hiking Boot Mid-Sole - The mid-sole of a boot provides cushioning, buffers feet from shock and mostly controls the boot’s stiffness. You may think stiff boots can't be a good thing, but for long hikes on rocky, uneven ground they can mean better comfort so your feet won't wear out and stability.\n- Hiking Boot Outer Sole - Two more boot safety considerations for choosing a hiking boot or shoe. A snug fit at the heel. This enhances the heel brake which is the raised heel zone of your boot. This area helps to reduce the chance of sliding and falling on steep descents. Number two is the lug pattern and depth of your hiker. Lugs are the bumps on the outer-sole foot bottom. If you're backpacking or mountaineering you'll want deeper and thicker lugs for improved grip. Also, lugs spaced widely provide good traction and will shed mud more easily.\n3. The Type of Hiking Trip You've Planned: Are you doing an overnight hike and need a tent? Might you be exposed to wet conditions like rain or crossing rivers and streams? Day and night temperature highs and lows are very important for your physical comfort and hydration.\n4. Waterproof Boots and Shoes: Generally recommended if hiking on an overnight trip. Wet shoes and feet can ruin a trip.\n5. Always Try on Boots and Shoes: Try on at least two different pairs to compare the contrast in fit, comfort, and stability. Hiking boots should fit your foot snug everywhere and never tight plus plus they should have room to wiggle your toes. For a real feel, try them on with the socks you will be wearing. Best time to try on shoes is at the end of the day (after feet swell).\nTip: Another test of worthiness is to mimic some of hiking conditions by trying your boots on with a backpack and walking around the shoe department. See if you can find a few stairs to replicate hiking situations. Once you've decided, wear them often to break them in prior to your hike.\nOur Experts Recommend the Targhee Boot Collection From KEEN\nThe award-winning Targhee collection offers trail-tested favorites for men and women. These waterproof hiking boots and shoes are breathable, durable and designed to feel like a four-wheel drive on any trail. They have multi-directional outsole lugs to ensure a sure grip on rough trails. Outsole easily sheds dirt and debris. The dual density compression molded EVA midsoles provide support and cushion.\nWatch Video: CONSCIOUSLY CREATED FOR A BETTER PLANET by Keen\nHow and Why You Should Dress in Layers for Hiking\nLayering is wearing a combination of clothes worn on top of each other to regulate your body temperature, so you don't overheat or get too cold. Also, it helps with moisture wicking to keep the skin as dry as possible to help prevent blisters and chafing.\nDressing in layers must match the climate conditions you are in, your activity level, and your thermostat.\nYou might be wondering why not just wear 100% cotton? While cotton still a fabric of choice in casual wear, it has no real place for the outdoors anymore. Cotton has no wicking features; it stays wet longer, which can make you chilly, especially if it is windy. Instead, you need to look for synthetic fabrics \"blends\" of cotton and polyester. Today you can even find clothing with a UPF value (ultraviolet protection factor) for sun protection and insect repellent.\nLayered clothing breaks into three principal layers: Base, Mid, and Outer. These three layers act in unison, but individually each layer performs specific functions.\nHow Do You Layer Up for Hiking in Any Weather?\nLook for light colored clothing like white, khaki or tan that is loose-fitting synthetic fabric blends like nylon and polyester. For hot weather, cotton can be worn as well. Your clothing should allow for breathability for comfort from sweating in your clothes.\nLightweight hoodie, well ventilated lightweight waterproof and breathable rain jacket and pants. Plus waterproof hiking boots or shoes and waterproof gaiters. Consider base or mid-weight layers, your goal is to cover your skin with a stretch layer to keep a jacket off your skin.\nLayer 1: Base Layer - For Wicking Away Moisture\nThe base layer is the wicking layer that wicks moisture away from the skin. The base layer is the first layer of clothing your wear that directly contacts your skin. A base layer should fit snug. This fit lets the material quickly wick away moisture from your skin keeping you dry and warm. Base layers come in three different layer weights; lightweight, midweight and heavyweight. Light weight has better moisture wicking with less insulation, and used in warmer areas. The heavyweight more insulation less moisture weekend, used in colder climates.\nLayer 2: Middle Layer - For Insulation\nThe middle layer garments are for insulation and warmth by trapping the warm the body generates. Polyester fleece is a common middle layer that stays warm even if it gets damp, and it dries fast. Fleece also breathes well, so you’re less likely to overheat in it.\nLayer 3: Outer Layer - For Protection\nThe outer layer garments are designed to protect you from the weather. This layer is commonly called the shell as it protects against wind, rain, and snow. For example you would use a waterproof jacket for rain or windproof or down jacket for the cold.\nEasy Orienteering Tips Anyone Can Use\nIn this age of digital technology, stand-alone GPS devices and smartphones with GPS technology are increasingly people’s go-to navigational aids. But over-reliance on these devices is increasing the number of mountain rescue across the world.\nHow to Use a Topographical Map\n- When Hiking With a Map: To begin, you should orient to mimic the environment, which means if headed north, you will see the map right side up, while if hiking south, your map will be upside down.\n- The Attack Point: This point on your map is the location in which you are hiking and will help guide you—for instance, a river or trailhead.\n- The Topographical Lines on the Map: These lines represent the hills and valleys of the land. They also present you with some of the easiest-to-follow landmarks in any landscape. Knowing your starting location and ending location on the map, it is easy to choose routes that follow ridge-lines or valleys, or cross a certain number of ridges or hills, on the way to your end destination.\nHow to Use a Compass\nThe first thing you need to know about your compass is that the needle always points North.\nCompasses measure direction in degrees, which on a compass increase clockwise; East is at 90 degrees, South at 180, West at 270, and North at 360 or 0 degrees.\nPractice With Your Compass at Home or a Park.\nUse the compass to find the bearing by using an attack point. While hiking, this point could be a spot in your backyard or a special tree in the park. Point the arrow on the base plate toward the item. Then turn the dial of the compass until North aligns with the red tip of the compass needle. The arrow will point to a number in degrees on the dial; this is the bearing for the landmark.\nOn the trail, you need to be aware of the difference between true North and magnetic North and that it varies from place to place on the Earth’s surface. Topographical maps almost always note the declination in the map scale. While often only a minor deviation from true North, it’s worth noting on each map and making adjustments on a compass for long headings or if the declination is particularly significant.\nLearning how to read a map and compass can seem daunting. But with practice, this skill is hugely beneficial on the trail and if you're lost.\nStaying Properly Hydrated is one of the Keys to Comfort and Performance\nNo matter if you are going hiking, camping, or just playing in the playground, you should always bring water as it is essential to life.\nTo prevent dehydration, you need to drink water every day, and the hotter and harder you work, the more you will need to drink.\nParticipant Activity: Ask someone to come up and determine how much water you think they will need on a 4 hours hike in moderate temperature and terrain.\nPlan for a half of a liter of water per hour in moderate temperatures and terrain\nAnd if it is hotter or the terrain will be steep, then you will need to plan for more or bring something to purify your water. Even on a short hike, a few snacks are crucial to keeping everyone happy especially if you are taking the kids\nWhat is the Amount of Water to Bring on a Hike?\nModerate Hike: Suggested amount of water to bring on a moderate hike up to 5 miles. The weather is temperate or the elevation is low. Amounts are based upon 16 oz.\nStrenuous Hike: Suggested amount of water to bring on a strenuous hike up to 5 miles. The weather is hot or the elevation is high. Amounts are based upon 16 oz.\nHiking Snacks are a Must on the Trail\nPack calorie-dense foods to keep you going throughout the day and provide you with that much needed energy required to get to where you're going. You should also carry an extra portion in case you are out longer than expected.\nWhat are the Best Hiking Snacks to Pack?\nLong distance hikers need to consume more calories because their body will start to burn both fat and muscle. They need to consume enough protein plus, by combining protein with carbohydrates, your snacks will feel more fulfilling. So, what do you pack? Here are some tips on the different types of fuel for the body.\nLightweight: Water is the number one contributor to dense food. Packaging being the second. Prioritize \"dry\" food and feel free to repackage it if the cardboard boxes and bagging is excessive.\nReady-to-eat: Focus on foods that require minimal preparation (and cleanup). You will be hiking in the daytime and will not want to set up the stove to cook and dishes to clean. Make sure your meals and snacks are effortless so you can keep moving. Even for dinner at camp, most backpackers only cook \"one-pot-meals.\"\nHigh levels of nutrition: Hikers can burn up to 6,000 calories a day. In short, you need to consume a lot of macronutrients to balance out the high burn-rate. High-levels of calories, carbohydrates, protein, fats, fiber, and electrolytes (mainly sodium, calcium, potassium, and magnesium) are a must.\nTry to prioritize hiking food that provides at least 100 calories per oz.\nGood Hiking Snack Examples are:\n- Dried fruit like mango, apricots, dates.\n- Nuts like almonds, macadamia, peanuts, pecans.\n- Granola Bars\n- Trail Mix\n- Energy and Protein Bars\n- Dehydrated Meals\nSafe Hiking Essentials to Pack\nRemaining safe while outdoors is essential as lots of the areas in which you hike to have limited access to emergency services. Here are some essential items to pack.\n- Lighting or Flashlight\n- Fire Starter like Coghlan's Fire Starter\n- Safety Whistle\n- First Aid Kit\n- Insect Repellant\n- Knife or Multi-Tool\nNow that we have gathered all of the needed supplies let’s take look at a couple of backpacks.\nAscend Current Hydration Pack Features\n- Carry weight: 1 lb. Dimensions: 18\"x4\"x11\". Imported.\n- Handy 2L hydration system and gear transport for outdoor adventure\n- 244 cubic inch of storage for gear\n- Ventilated shoulder straps with sternum strap\n- Padded air mesh back panel with ventilation channels\n- 2 accessory pockets and 1 stretch stash pocket\n- Side compression straps\n- Rain Gear and dry fast\nAscend Folsom Daypack Features\n- Roomy and versatile daypack that handles weekdays or the weekend\n- Versatile and roomy 4-compartment design - 2,380 cubic inches of storage\n- Padded front compartment with organization pockets\n- Padded shoulder straps with sternum strap\n- Padded air mesh back panel with ventilation channels\n- Removable waist belt and upper and lower side compression straps for stability\n- Durable 420D nylon construction with YKK zippers\n- Two stretch mesh pockets on the sides\n- Padded haul handle on top\n- Reflective webbing and blinker light attachment point - increased visibility\nWhere Can you go for a Great Hiking Adventure?\nThere are 419 national park sites in the United States and 10,234 state parks . Our National Parks encompass more than 84 million acres in each state including Puerto Rico, the Virgin Islands, American Samoa, and Guam. National parks are owned by citizens of the United States and operated by the National Park Service, an agency of the Department of the Interior. The United States has 62 protected areas known as national parks and include \"National Park\" as part of the name. These are operated by the National Park Service, an agency of the Department of the Interior.\nState citizens own the state parks in the state where they live. Some of the most beautiful land in America is in our national and state parks.\nThere are also county parks within a state and city parks within cities.\nTip: Find your next adventure at www.recreation.gov\nHow to Keep Kids Motivated When Hiking\nHiking with the family can be a fantastic experience; here are some activity ideas to engage the kids while hiking in the outdoors.\n- Play a nature \"I spy\" game\n- Go birdwatching & take field guides\n- Create outdoor games\n- Go on a night hike\n- Identify animal tracks\n- Visit a nature center\n- Look for decomposers\n- Listen for animal sounds\n- Create a nature scavenger hunt\nThis is not an article. This is mini-course! Thanks a lot!","Focused Meditation and Its Benefits for Addiction Recovery\nFocused meditation contributes to our internal arsenal of meditation techniques, that can help us in so many ways during our journey of addiction recovery.\nThe art of meditation has been practised for thousands of years. Today, it serves as a form of alternative healing and its benefits are seemingly endless.\nFocused meditation helps to build more awareness of our thought patterns and come to understand that they do not define us. It can help us relax in moments of chaos.\nThe ability to observe our own thoughts is a major catalyst for increased growth during the recovery process. For instance, building ‘focused awareness’ can help us acknowledge the kinds of thoughts we have that trigger cravings.\nBy discovering what our thought patterns and internal monologues are, we can even step outside of ourselves and learn how to stop reacting to internal and external triggers. This is just one of the many benefits focused meditation offers when practised consistently.\nWhat Is Focused Meditation?\nFocused meditation is a type of meditation in which you place your focus on one particular sensory stimuli. Whether that be an object, sensation, or sound.\nTraditionally, when we think of what it means to meditate, we think of clearing our minds. With focused meditation, there is less focus on clearing the mind, and more focus on, well, focusing !\nThis form of meditation is great for beginners because it isn’t guided and doesn’t require facilitation. It also serves as a perfect introduction into meditation as it helps to slow down the mind with the assistance of a focal point, rather than having nothing to anchor on to.\nYou can practise this form of meditation at home, at work, at the park, and truly anywhere that isn’t overly crowded or noisy. It will be much easier to practise focused meditation in a place that is quiet and still.\nThis particular meditation can be practised for as long as you can maintain, or for as little as a few minutes. It is truly a highly adaptable form of meditation.\nThe Benefits of Focused Meditation\nThere are many benefits of this type of mindful meditation for the brain and the body. Research shows that focused meditation helps to reduce stress. One study even indicated that practising focused meditation was able to help relieve pain in patients experiencing chronic lower back pain\nAnother study showed that university students who practised focused meditation for six weeks experienced a decrease in symptoms of anxiety and depression. A follow-up conducted 6 months after the conclusion of the study revealed that the students who continued to practise the meditation continued to experience a decrease in symptoms.\nAfter 12 months, students still engaging in the practice were observed to have a continued reduction in their symptoms of anxiety and depression. The study concluded that focused meditation is a formidable psychosocial tool that can benefit university students greatly\nFocused meditation does wonders for the brain, and has the ability to rewire neural networks. Over time, it also actually causes areas of the brain to thicken, including the mid-prefrontal cortex and the mid-insular part of the brain. This can be invaluable for us as we work to heal our brains from the long-term effects of substance abuse and prolonged exposure to stress .\nThe benefits of focused meditation:\n- Great for beginners\n- Can be practised without an instructor\n- Can be practised anywhere\n- Rewires neural networks in the brain\n- Increased concentration levels\n- Helps increase patience\n- Fosters mindfulness\n- Reduces stress\n- Relieves pain\n- Decreases symptoms of anxiety\n- Decreases symptoms of depression\nStep-By-Step Focused Meditation for Beginners\nAre you ready to give focused meditation a try? Below is a step-by-step outline to help get you started.\nBefore the Meditation\nIn order to prepare for a focused meditation, be sure that you are in a calm environment, as it will be more difficult to practise focused meditation in loud or over-stimulating surroundings.\nRemove any distractions, such as the sound of the television, and switch your phone to silent. Wear comfortable clothing to encourage relaxation.\nDuring the Meditation\n1. Select a focus object.\nMany practitioners suggest starting with your breath as your initial focus object. If you would like to focus on something external, you can select a candle flame or a sound bowl. You can choose anything that engages your senses.\n2. Sit or lay in a comfortable position.\nIf you choose to sit for your meditation, you can sit on the floor or on the edge of a chair. Keep your back straight to maintain posture. Rest your hands wherever they naturally fall and feel comfortable. If you prefer to lay down, you can do this as well. Just be sure that you don't try this when you are feeling extra tired, or you may run the risk of falling asleep mid-meditation!\n3. Relax your body.\nOnce you find yourself in a comfortable position either sitting or laying down, close your eyes. Do a brief body scan to relax the muscles in your body. Start from the top of your body and work your way down, observing each body part, such as your arms and fingers, and releasing any tension you may feel. Drop your shoulders if they are raised and drop your tongue from the roof of your mouth. Do this for your entire body.\n4. Begin to focus.\nNow, we begin the focused aspect of the meditation. Let’s say you’ve decided to use your breath as your focus object. During the experience, you are observing your breath, how it feels when the air flows in through your nose, and how it feels when the air flows out of your nose. You can observe that your stomach rises and falls with each breath, that your chest rises and falls in the same fashion. It is important to not think about your breathing, but to simply be present and observe it.\n5. Let your thoughts pass.\nWhilst focusing on your breathing, you’ll undeniably begin to think about what you should eat for dinner tonight. Or maybe you’ll think about work, and an upcoming deadline that you have. This is completely normal! It isn’t easy to still the mind, even if there is a focus object involved. Thoughts will flow into your mind, and the important thing to do here is to acknowledge that that have happened. Acknowledging that you are experiencing a thought is a massive step, as it indicates that you’re practising mindfulness. Once you’ve observed the thought, let it go. You can place the thought on a cloud and watch it float away, and then continue to focus on your breathing. It is important to not feel resistance to thoughts coming in and out of your mind, but rather to acknowledge them, accept them, let them go, and continue focusing.\nAfter the Meditation\nGently move your upper body, and then gently move your lower body. Gradually transition from an inward focus to an outward focus. Mindfully pay attention to how you feel after the meditation. Do you feel calmer? More relaxed?\nWhile in this state, take time to reflect on what the experience of focused meditation was like.\nWas it difficult to maintain focus? Was it easier than you imagined? What kinds of thoughts were coming in?\nWhether you practised for 3 minutes, 10 minutes, or 30 minutes, you should commend yourself for doing it. The mind is like a muscle, and it will become easier to focus for longer intervals of time as you practise more.\nAdditional Tips for Practising Focused Meditation\n- Begin with short sessions. Instead of diving in headfirst and trying to practise focused meditation for 30 minutes on the first go, try to practise it for 5 minutes instead. It is important to start with shorter sessions so as to not overwhelm yourself. Small steps are key when developing a long-term, enjoyable and sustainable meditation practice.\n- Find a time of day that works best for you. Some people choose to practise meditation first thing in the morning before work or school. This helps set them up for the day with a clear and grounded mind. Others choose to practise at night in order to wind down and relax after a busy day. You can even have a pop-up, 5-minute focused meditation session in the middle of a hectic work day to re-center and slow down internally. This is one of the many positive benefits of focused meditation - it is completely flexible based upon you and your needs!\n- Be gracious with yourself. Developing the meditation muscle takes time and practise. Don’t beat yourself up for having those thoughts coming in and out (just keep putting them on clouds!), and don’t beat yourself up if you’re having trouble focusing. The whole point is to use this form of meditation to develop that ability, among so many other things as we’ve discussed. Start with short sessions and a schedule that is feasible for you, feels realistic to maintain, and doesn’t feel overwhelming. Always give yourself loving words of positive encouragement after a session, even if you feel you couldn’t keep focus for more than a few seconds. It truly does get easier! You will be able to practise focused meditation for longer intervals and longer periods of time as long as you keep practising.\nSources for Accessing This Meditation Practise\nOur Recoverlution Wellness hub is dedicated to increasing your wellbeing and offers many methods of mindfulness, breath work and meditation for you to use.\nRemember, the beauty of focused meditation is that you can practise anywhere at any time, and it doesn’t have to be guided. As always, Recoverlution is here to help if you have any questions!\nAuthor - Thurga\n- How To Start A Focused Meditation Practise - https://www.verywellmind.com/practice-focused-meditation-3144785\n- Science Direct - Effectiveness of Focused Meditation for Patients With Chronic Low Back Pain - https://www.sciencedirect.com/science/article/abs/pii/S0965229916300358\n- Science Direct - The Effect of a 6-week Focused Meditation Training on Depression and Anxiety Symptoms in Brazilian University Students with 6 and 12 months of Follow Up - https://www.sciencedirect.com/science/article/abs/pii/S0165032718310498\n- Mindfulness Meditation and Addiction - https://www.psychologytoday.com/us/blog/the-wise-open-mind/201004/mindfulness-meditation-addiction"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:e25fc74a-fd6e-4707-b091-8e5cd1156965>","<urn:uuid:6e58479b-65c0-472d-aea1-855a29303e40>"],"error":null}
{"question":"As a PhD student working with human subjects data, could you explain what are the ethical requirements for data sharing and the available tools to manage this process?","answer":"The ethical requirements for data sharing include obtaining informed consent from participants for future storage and reuse, ensuring data confidentiality, and following proper anonymization procedures. Tools available include DMPonline and DMPTool, which provide templates and guidance for writing data management plans. For sharing sensitive data, specialized repositories like the Australian Data Archive (ADA) can handle both qualitative and quantitative data while maintaining security. These repositories can mediate access requests and apply specific licensing conditions. Any data sharing plans must be approved by Human Research Ethics Committees, and successful examples like the Alzheimer's Disease Neuroimaging Initiative demonstrate that sensitive data can be shared ethically while requiring users to sign agreements preventing re-identification attempts.","context":["Effective data management starts when research is being designed. Planning should consider how data will be managed both during and after the research. This involves thinking critically about how data can be shared, what might limit or prohibit data sharing, and whether any steps can be taken to remove such limitations.\nBenefits of planning data management:\nIt is recommended that every research project, whether funded or non-funded, produces a data management plan (DMP) before starting the project.\nA DMP is a document that describes how research data will be collected, organised, stored, secured, backed-up, preserved, and where applicable, shared.\nA DMP should be concise and appropriate to the nature of the research, with more detailed plans for larger projects. Decisions about data management should be justified and the DMP should be implemented once the project begins.\nMany funders have specific DMP guidelines which you need to be aware of and the diversity of research means there is no single answer for every DMP. The following topics are typically covered:\nIf newly generated data cannot be shared, adequate justification should be given because funders require all strategies to enable data sharing to have been considered, and if not they may impose sanctions.\nGuidance is available on how to choose a data repository\nIn some instances, direct costs to manage research data can be requested as part of a grant application. Other funders expect the university to meet the costs from overheads, if these can be recovered as part of the grant. Please check with the Research & Enterprise Development Team if you are unsure about the rules for a particular funder/scheme. The UK Data Archive have developed a useful Costing Tool for data management in the social sciences.\nThere are a variety of tools, templates and checklists available to help write a DMP.\nDMPonline provides templates and embedded guidance for writing data management plans for all major UK funding bodies, some international funders, as well as a generic template. You can save plans and return to them, and also collaborate on them with partners. A short screencast is available on the homepage which demonstrates how to use the tool. Please note that registration is required, but you can link it to your Salford network username/password.\nDMPTool is similar to DMPonline, but includes templates and guidance based on the major US funding bodies.\nA summary of the UK funder expectations for data management plans, including links to their templates are available here.\nThe Digital Curation Centre maintains a list of example data management plans\nBelow are a few examples to download:\nMany research funders have data policies which specify the research data management practice expected from grant holders. In many cases submission of a data management plan (DMP) is part of the grant application process.\nTip: Always use the templates or guidance provided by the funder. Be aware that some funders provide multiple templates, requiring more detailed DMPs to be submitted after a project has been funded.\nLinks to the funder data policies and templates are in the table below.\n|Funder Data Policy||Plan requirement||Submit with grant application?||Length (A4 pages)||Guidance||Template/Example|\n|AHRC||Technical plan||Yes||Max. 4||Online guidance||Sample plan from University of Bristol, DCC annotated sample plan|\n|BBSRC||Data management plan||Yes||Max. 1||FAQs||Template available in DMPonline|\n|CRUK||Data sharing plan||Yes||No limit specified||FAQs\n|Template available in DMPonline|\n|EPSRC||Data management plan||Not peer reviewed as part of application but must exist locally for all data||No limit specified||Clarifications||Template available in DMPonline|\n|ESRC||Data management and sharing plan||Yes||No limit specified||Online guidance||Template and Example|\n|EU Horizon 2020||Data management plan||A plan is required for projects participating in the Open Research Data Pilot. Other projects are invited to submit a plan if relevant for their planned research||No limit specified||Online guidance and Factsheet||Template in Annex 1 of pdf|\n|Innovate UK||No data policy or guidance available|\n|Kidscan||No data policy or guidance available|\n|MRC||Data management plan||Yes||Max. 3||Online guidance||Template|\n|NERC||Outline data management plan for proposal stage, and a full DMP if grant awarded||Yes||Max. 1||Online guidance||Template|\n|NIH||Data sharing plan||Yes for scientists submitting grant, cooperative, or contract applications||No limit specified||FAQs |\n|NIHR||A statement on how underlying research materials can be accessed||Yes||No limit specified||Not available||Not available|\n|STFC||Data management plan||Yes||Max. 2||Online guidance||Template available in DMPonline|\n|Wellcome Trust||Data management and sharing plan||Yes – for large scale projects and proposals involving the generation of datasets that have clear scope for wider research use and hold significant long-term value||No limit specified||Online guidance |\nQ & A\n|Example from LSHTM\nTemplate available in DMPonline\nThe University requires research to be consistent with basic ethical principles, including:\nIt is important in the planning stages of a project to ensure that consent is obtained from participants for future storage, re-use and sharing of their data and/or tissues with third parties as it can be difficult to obtain consent retrospectively. Many funders now require data to be made openly available where possible.\nEthical arguments for sharing research data include:\nResearch data about people, including sensitive data, can be shared ethically and legally if the following strategies are used:\nDo not assume all research data should be destroyed after a certain time period.\nThe Medical Research Council (MRC) aims to maximise the value of human tissue samples and suggests that researchers establish a Research Tissue Bank or Research Database for the purposes of sharing. For information on managing access to, and transfer of, data or tissue samples, please visit the following MRC tool kits:\nQuick guide flowchart\nResearch Ethics Checklist Form to be completed as part of the Learning Agreement\nCodes of Ethics from professional bodies and research councils\nIf you are intending to carry out research projects in the NHS, you must also obtain NHS research ethics approval. The normal procedure is to apply for School Ethics approval before proceeding to NHS Research Ethics approval (Details of how to apply for NHS ethics approval can be found at the Integrated Research Application System (IRAS).\nThe Health Research Authority (HRA) provides two linked, online decision tools to assist you in determining whether your project is classified as research and whether it requires ethical review by a NHS REC. These tools are available at:\nCreating a data management plan will enhance your research planning and also help you to navigate the information governance requirements listed below.\nThe Data Protection Act 1998 regulates the holding and processing of personal data that is information relating to living individuals of any age. Its purpose is to ensure that personal information (personal data) is only processed in accordance with certain legitimate principles under specified conditions and to provide a set of rights to the individuals to whom the personal data relates.\nThe Freedom of Information Act 2000 came fully into effect in January 2005 and it aims to promote transparency and accountability in the public sector. It enables people to access to information they would otherwise not have access to in public funded organisations.\nEnvironmental Information Regulations (EIRs) establish an access regime which allows people to request environmental information from public authorities and those bodies carrying out a public function.\nFor queries relating to Data Protection, Freedom of Information and Environmental Information Regulations, please contact the Information Governance Team\nIntellectual Property (IP) is defined as “creations of the mind, such as inventions; literary and artistic works; designs and symbols, names and images used in commerce” and is protected in law to reward creativity.\nThere are four main types of IP:\nDuring a research project a variety of intellectual property will be created, the management of which is vital to both the success of the project and possible ‘impact’ opportunities, for example:\nThe EPSRC typically expects the IP generated as a result of a funded project to be owned by the institute that created it. In the event of a project with a commercial collaborator, the position on IP may be different but in either case, the research contract will set out the rights and responsibilities associated with IP.\nFor advice and support on IP, please contact the University Intellectual Property Manager:\nt: +44 (0) 161 295 2905","Publishing your data: the ethics question\n23 July 2013 1 Comment\nIn practice, this means looking at all of those legal and policy issues which have an impact on data sharing and use, such as copyright, licensing, ethics, Gov 2.0, etc and keeping an eye on developments overseas.\nANDS is building the Australian Research Data Commons: a cohesive collection of research resources from all research institutions, to make better use of Australia’s research data outputs.\nLast April, I published a short item in this blog about the importance of data citation if you are to get recognition for publishing your data and making it available to others.\nThere is increasing evidence that making your data available to others (or, to use a more familiar term, publishing your data) can enhance your research reputation. But consider the case of researchers whose work involves human subjects who might feel that their data cannot be published. This presents something of a challenge: how to handle sensitive data so that others can use it, while following ethical guidelines and making sure that the data cannot be wrongly used.\nData can be sensitive for a variety of reasons. Privacy considerations mean that personal, identified data cannot usually be made available to others. Security considerations might mean that you would be putting people at risk if you made some data available. You might also consider the possibility that your data describes the last remaining population of some rare species and, while most of the data might be fine to share, you don’t want anyone to put the species at risk by providing location information (think of the Wollemi Pine).\nAll is not lost.\nThe fact that your data might be sensitive does not mean that you cannot consider sharing it. However, there are ethical considerations to keep in mind, and it helps if you do your planning at the beginning of a project.\nFirst, there is the issue of consent when dealing with personal information. This means that you need to inform participants in your study how the research data will be stored, preserved, and used in the long term. You will need this for any study of this kind. You’ll need to inform your participants how confidentiality will be maintained. You’ll need informed consent, either written or verbal, for data sharing.\nSecond, you may need to consider the issue of anonymising any data that you plan to share so that individuals, organisations, or businesses cannot be identified. There are a number of techniques that can be used to anonymise data: removing direct identifiers such as names or addresses, aggregating variables such as replacing date of birth with age groups, and reducing the upper or lower ranges of a variable to hide outliers (such as subjects with very high salaries or advanced ages). There is really good information from the Australian Bureau of Statistics on the finer details of how to anonymise your data.\nAnonymisation is not always required. You may, for example, have conducted oral history interviews where it is customary to publish and share the names of people interviewed, if they have given their consent.\nIn addition, not all data can be anonymised. Audiovisual data is very difficult to anonymise, so it may not be worth it. Consider using transcripts of interviews, or accept that there is some data which simply cannot be shared.\nThird, you’ll need to consider how future users might access the data, and any conditions that you might want to apply to its use. You might, for example, want to limit access to people working in the same discipline, or wish to have an assurance that there will be no data-mining that might allow the data to be re-identified.\nThis is probably the trickiest aspect of sharing the data, as you need to have confidence that the data repository you choose has the capacity to keep your data secure and mediate requests for its use.\nIn Australia, there are several examples of facilities that might meet your needs. The Australian Data Archive (ADA) is well set up for this purpose as is its offshoot, the Aboriginal and Torres Strait Islander Data Archive (ATSIDA). Both can handle qualitative and quantitative data. There are other excellent facilities in other countries.\nAny access conditions you want to impose can be expressed by the licence attached to the data. If the data repository you choose does not have a standard licence, there is a template available through AusGOAL, the Australian Governments Open Access and Licensing Framework.\nAny plans you have to make your data available will have to be approved by your local Human Research Ethics Committee. Some members of the Committee may not be comfortable about your proposals. If they aren’t, you could tell them about one major data sharing initiative that has been a huge success in terms of making data available to improve our capacity to understand Alzheimer’s Disease.\nThe Alzheimer’s Disease Neuroimaging Initiative (ADNI) was established in California in 2004. It is a longitudinal study intended to improve our understanding of the progression from mild aging through to Alzheimer’s disease (AD). Central to the study has been the notion that all data should be shared, enabling researchers from different countries and disciplines to take part. By March 2013, there were 3,712 approved investigators accessing the data.\nAnyone seeking to make use of the de-identified data must sign an agreement, which stipulates that they will not attempt to re-identify or contact subjects involved in the study, disclose or redistribute the data, or attempt to contact ADNI principal investigators (PIs) or staff.\nAustralian ADNI is based on neuroimaging data from the Australian Imaging, Biomarker and Lifestyle Flagship Study of Ageing (AIBL) which has 12 partners around Australia, all sharing data.\nSo, who says it can’t be done?\nThere is a more detailed guide to the whole topic of ethics, consent and data sharing available via the ANDS website.\nThe Margaret Henty series on data management"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:5d446edb-6f7a-4eab-aef6-af473258a31f>","<urn:uuid:42bd4abc-848a-4d25-9b9d-791eeffd460a>"],"error":null}
{"question":"How do you set up a key light for family portraits to avoid harsh shadows?","answer":"To set up a key light for family portraits, place it at a 45° angle to the group, positioned just above their eye level. This positioning creates attractive lighting while avoiding harsh shadows. You should then adjust the distance between the light and the subjects based on your desired lighting brightness.","context":["There are several ways to set up lighting for family portraits. Using a 2 light setup for family portrait gives you more possibilities for illuminating small or large groups. It can create balanced lighting to ensure everyone appears good in the frame without harsh shadows on the face. In this article, we will break it down on how to achieve this setup, starting from the choice of equipment to some tips for better portrait images. Read on to get what you need.\nChoose two suitable lights for family portrait\nContinuous lights and strobes are two common lighting types that you can choose for the family portrait. Here we will focus on the continuous type and tell you what to consider when making the purchase.\nSufficient power output: Select the one that is bright enough to adequately illuminate the subjects and the scene. The power output will be determined by criteria such as room size, group size, desired depth of focus, and camera sensitivity (ISO).\nColor temperature consistency: Make sure the color temperatures of both lights match or are balanced. This helps to keep consistent and natural-looking skin tones across the photograph. You may accomplish this by using gels or LED lights of adjustable color temperatures. The COLBOR CL330, for example, is a good choice. Its color temperature is adjustable from 2700K to 6500K. In addition, it features the color balance function to adjust to red or green tint. This allows it to match your already-have light sources and gives the subjects natural skin tones if it is adjusted to red tint.\nAdjustable intensity: Continuous lights with adjustable intensity or power output are useful in 2 light setup for family portrait. This versatility allows you to manage the quantity of light that strikes the subjects and adjust for varied lighting conditions or creative effects.\nHeat management: Continuous lighting creates heat, thus heat control is essential to avoid discomfort or any risks throughout the shooting. Look for lights with built-in cooling systems, or use external fans as needed.\nCompatibility with softboxes or diffusion: Check if the continuous lights are compatible with softboxes, diffusers, or other light modifiers in order to generate soft and diffused lighting. This enables you to minimize harsh shadows on the family's faces and soften the light source.\nSturdy and adjustable: Select continuous lights that are both robust and flexible so you may place them at whatever height or angle you require. This adaptability enables you to get the 2 light setup for your family portrait.\nRemember that these factors might change based on your tastes, the available gear, and the location of the shoot. It's wise to investigate and try a variety of continuous lights to see which ones best meet your requirements.\nPosition the key light to illuminate family portraits without harsh shadows\nSet up the first light, which serves as the key light in this 2 light setup for family portrait. Place it at a 45° angle to the group, just above their eye level. This results in attractive lighting while avoiding harsh shadows. Adjust the distance between the subjects based on the desired lighting brightness.\nAdd a fill light to fill in shadows\nThe second light is a fill light for softening the shadows cast by the aforementioned light source. Position it opposite the key light, at a 45° angle to the subjects. Adjust the brightness to obtain a balanced lighting ratio. If you do not have a second light, you can use a reflector or diffuser instead.\nModify two lights for family portraits to get pleasing illumination\nTo provide softer, more diffused illumination, attach softboxes, diffusers, or umbrellas to the lights. These can eliminate harsh shadows and aid in the balanced distribution of light. To regulate the direction and dispersion of light, change the modifiers' location and angle.\nTest and adjust to see if all family members look good in the frame\nTo evaluate the lighting and make any required modifications, take a few test photographs. Analyze the portrait's exposure, shadows, and general appearance. Adjust the lighting position, intensity, or modifiers as necessary to get the desired outcome.\nMore tips for 2 light setup for family portrait\nWhen illuminating the family portrait with a two-light setup, you should pay attention to the lighting consistency, consider the background, avoid facial shadows cast by people in the group, create depth and dimension in the frame, and keep communicating with the family members. All these ensure better family portrait outputs. Read on to get more details.\nEnsure consistent lighting: Make sure the lighting is consistent across the whole group. Try your best to create even illumination, with no one individual looking considerably brighter or darker than the others. This requires you to place your two photography lights properly or make efficient use of natural light sources.\nConsider the background: Check the background and make sure that it is well illuminated to preserve a balanced composition and eliminate distractions. If required, use accent lights or spotlights to draw attention to certain objects or separate the whole family from the background.\nAvoid facial shadows: Be aware of any shadows that members of the group may cast on other members. To reduce unpleasant shadows on faces, adjust the lighting or utilize reflectors.\nCreate depth and dimension: Use back or hair lighting to make the family group stand out from the background. This will bring depth and dimension to the family portraits, keep the image from seeming flat, and help to create visual interest.\nCommunicate with family members: Communicate well with the family members throughout the photoshoot. Teach them where to stand relative to the light sources so that everyone is illuminated properly and appears good in the frame."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:bb16ed85-0d24-49f9-9394-975dc0634b76>"],"error":null}
{"question":"Do common rosefinches and common cranes both migrate?","answer":"Yes, both species migrate. Common rosefinches follow the Indo-European flyway, migrating between Europe and India, with specific patterns in spring and autumn migrations. They use straighter winds from India to Europe during spring for fast direct migration, while taking larger detours in autumn. Common cranes are long-distance migrants that travel between northern Europe/Asia and northern Africa, with autumn migration occurring from August to October and spring migration from March through May.","context":["Overcoming multiple setbacks, and teaming up with researchers from across Europe, finally brought insight into the previously mysterious travels of the common rosefinch.\nAbove: A male common rosefinch equipped with a 0.5 gram light-level geolocator\n(Photo credit: Benjamin Metzger)..\nTo me, the common rosefinch is still a rather exotic bird species. In the last decades the bird with its iconic song that is often rendered as “pleased to meet you” (check it out yourself), has expanded its huge breeding range westwards, and we have seen small populations emerging at new sites in Europe. During my undergraduate studies, my friend and co-author Roland Neumann was investigating a population at the Baltic Sea Coast. Since very little was known about their routes and their destinations, he was catching and banding individuals with the hope to get resightings from their wintering sites or from any location during their migratory journey. Unfortunately, the massive efforts didn’t pay off.\nEditors’ Choice article: (Free to read online for a year.)\nLisovski, S., Neumann, R., Albrecht, T., Munclinger, P., Ahola, M.P., Bauer, S., Cepak, J., Fransson, T., Jakobsson, S., Jaakkonen, T., Klvana, P., Kullberg, C., Laaksonen, T., Metzger, B., Piha, M., Shurulinkov, P., Stach, R., Ström, K., Velmala, W. and Briedis, M. (2021), The Indo-European flyway: Opportunities and constraints reflected by Common Rosefinches breeding across Europe. J Biogeogr. 48:1255–1266. https://doi.org/10.1111/jbi.14085\nWe were thus super excited that, in collaboration with the Swiss Ornithological Institute and co-funded by the German Ornithologists’ Society, we could deploy light-level geolocators on male rosefinches in 2011. These devices were still somewhat new in the sense that they have just gotten small enough to use on a 25g bird such as the common rosefinch. Those so called geolocators record light intensity that can be used after recapture and download of the data in the following year, to approximate their migratory journeys. At the same time, we were in close contact with colleagues from Czechia also working on this species and equally thrilled to deploy these loggers on some of the individuals from their Bohemian Mountain populations.\nFieldwork went exceptionally and we had almost an entire year to make plans for the next field season in which we needed to especially look out for returning birds with geolocators, recapture them and retrieve the data containing so much information and the answer to our main questions of the birds’ whereabouts outside the breeding season. Both in northern Germany and in Czechia, we recaptured around 30% of the individuals (a reasonable return rate and the sample size we hoped for) just to learn that none of the devices recorded any data! Obviously, quite a downer that required some time to digest and to even think about rosefinch migration again. Yet, we decided to give it another go and repeat the study in 2013. This time we succeeded and collected useful data from several individuals.\nRoland Neumann and Benjamin Metzger banding and measuring common rosefinches in the dunes of the Baltic Sea in Germany (left). Tomas Albrecht and Jaroslav Cepak preparing their mobile lab after a successful catch of rosefinches in the mountains of Bulgaria (right). (Photo credit: Simeon Lisovski, Pavel Munclinger)\nIn the meantime, we learned that other research groups from across Europe were also collecting migration data from rosefinches. Colleagues from Finland deployed geolocators on males breeding as far north as Oulu (65°N), a Swedish research team collected tracks from males in southern Sweden, and besides the German and Czech data, Bulgarian colleagues were also successful in tracking rosefinches from their southern European breeding site (41°N). Since all of us were dealing with rather small sample sizes (3-10 tracks), we were all easily convinced and excited to pool the data allowing us to not only investigate population specific migration pattern in isolation but extend our objectives towards a flyway wide approach. Now, we saw the opportunity to identify general migration routes, bottlenecks as well as barriers on the so far understudied Indo-European flyway.\nInspired, by previous large-scale research on the effects of wind on bird migration (Kranstauber et al. 2015 Ecology Letters) we constructed simple theoretical migration models that optimized the routes with respect to wind and resource availability during migration. These theoretical tracks revealed interesting patterns with seasonally specific bottlenecks such as the areas south of the Caspian Sea during spring migration. The comparison between the theoretical tracks and the collected empirical tracks also showed that rosefinches seem to make use of the straighter winds from India to Europe during spring, probably allowing for fast and direct migration to the breeding sites, while engaging in larger detours during autumn following the routes that provide highest security in terms of resources.\nTo me, this study shows how valuable even small and local studies can be, if seen as puzzle pieces helping to reveal the larger picture. Obviously, this needs an open and collaborative network of researchers which I am very glad to have encountered during this project. I would like to thank all of my co-authors for sharing not only their data but for providing their insights into the study systems, for fruitful discussion, for pushing me hard to explain the modeling approach and forcing me to meet empirical fieldwork and theory halfway!\nSimeon Lisovski, Postdoctoral researcher, Alfred Wegener Institute Helmholtz Centre for Polar and Marine Research, Potsdam, Germany","The Common Crane is a large, stately bird and a medium-sized crane. It is 100-130 cm (40-52 in) tall with a 180-240 cm (71-96 in) wingspan. The body weight can range from 3 to 6.1 kg (6.6 to 13.4 lb), with the nominate subspecies averaging around 5.4 kg (12 lb) and the eastern subspecies (G. g. lilfordi) averaging 4.6 kg (10 lb). Among standard measurements, the wing chord is 50.7-60.8 cm (20.0-23.9 in) long, the tarsus is 20.1-25.2 cm (7.9-9.9 in) and the exposed culmen is 9.5-11.6 cm (3.7-4.6 in). This species is slate-grey overall. The forehead and lores are blackish with a bare red crown and a white streak extending from behind the eyes to the upper back. The overall colour is darkest on the back and rump and palest on the breast and wings. The primaries, the tips of secondaries, the alula, the tip of the tail, and the edges of upper tail coverts are all black and the greater coverts droop into explosive plumes. This combination of colouration ultimately distinguishes it from similar species in Asia, like the Hooded and Black-necked Cranes. The juvenile has yellowish-brown tips to its body feathers and lacks the drooping wing feathers and the bright neck pattern of the adult, and has a fully feathered crown. Every two years, before migration, the adult Common Crane undergoes a complete moult, remaining flightless for six weeks, until the new feathers grow.\nHabitat and Distribution\nThis species is found in the northern parts of Europe and Asia. Formerly the species was spread as far west as Ireland, but about 200 years ago, it became extinct there. However, it has since started to return to Ireland naturally and there are now plans to help it return to Ireland on a greater scale. The Common Crane is an uncommon breeder in southern Europe, smaller numbers breeding in Greece, Yugoslavia, Romania, Denmark and Germany. Larger breeding populations can found in Scandinavia, especially Finland and Sweden. The heart of the breeding population for the species is in Russia, however, where possibly up to 100,000 cranes of this species can be found seasonally. In Russia, it is distributed as a breeder from the Ukraine region to the Chukchi Peninsula. The breeding population extends as far south as Manchuria but almost the entire Asian breeding population is restricted to Russia. The species is a long distance migrant predominantly wintering in northern Africa. Autumn migration is from August to October and spring migration is in March through May. In Europe, the Common Crane predominantly breeds in boreal and taiga forest and mixed forests, from an elevation of sea-level to 2,200 m (7,200 ft). In Northern climes, treeless moors, on bogs, or on dwarf heather habitats, usually where small lakes or pools are also found. In Sweden, breeders are usually found in small, swampy openings amongst pine forests while, in Germany, marshy wetlands are used. Breeding habitat used in Russia are similar, though they can be found nesting in less likely habitat such as steppe and even semi-desert, so long as water is near. Primarily, the largest number of Common Cranes are found breeding in wooded swamps, bogs and wetlands and seem to require quiet, peaceful environs with minimal human interference. They occur at low density as breeders even where common, typically ranging from 1 to 5 pairs per 100 km2 (39 sq mi). While winter, this species moves to flooded areas, shallow sheltered bays, and swampy meadows. During the flightless moulting period there is a need for shallow waters or high reed cover for concealment. Later, after the migration period, the birds winter regularly in open country, often on cultivated lands and sometimes also in savanna-like areas, for example on the Iberian Peninsula.\nThe Common Crane is omnivorous, as are all cranes. It largely eats plant matter, including roots, rhizomes, tubers, stems, leaves, fruits and seeds. They also commonly eat, as available, pond-weeds, heath berries, peas, potatoes, olives, acorns, cedarnuts and pods of peanuts. Notably amongst the berries consumed, the cranberry, is possibly named after the species.\nThis species usually lays eggs in May, though seldom will do so earlier or later. Like most cranes, this species displays indefinite monogamous pair bonds. In common with Sandhill Cranes (and no other crane species), Common Cranes `paint` their bodies with mud or decaying vegetation, apparently in order to blend into their nesting environment. The clutch of the Common Crane usually contains two eggs, with seldom one laid and, even more rarely, 3 or 4. If a clutch is lost early in incubation, the cranes may be able to lay another one within a couple of weeks. The incubation period is around 30 days and is done primarily by the female but occasionally by both sexes. If humans approach the nest both parents may engage in a distraction display but known ground predators (including domestic dogs) are physically attacked almost immediately.\nCalls and Songs\nIt has a loud trumpeting call, given in flight and display. The call is piercing and can be heard from a considerable distance. It has a dancing display, leaping with wings uplifted, described in detail below."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:1d71efff-6a0b-4ac0-bd2d-e08edd40f33e>","<urn:uuid:30bbac2d-30ed-4d24-bc0a-8b64f78f29f6>"],"error":null}
{"question":"How do VRF systems contribute to building decarbonization, and what's their expected lifespan compared to traditional water heaters?","answer":"VRF systems significantly contribute to building decarbonization by being up to 40% more energy efficient than conventional HVAC equipment and achieving coefficients of performance (COPs) of 3 and higher. They help reduce CO2 emissions by eliminating fossil fuel usage and requiring less ductwork, which reduces energy consumption from fan motors. Regarding lifespan, while VRF systems are designed for extended life cycles providing sustainable performance well into the future, traditional water heaters have different lifespans - electric tank water heaters last 10-15 years, while gas tank heaters last 8-12 years. However, with proper maintenance including annual flushing and anode rod replacement every 2-3 years, traditional tank water heaters can last up to 20 years or more.","context":["VRF technology divides a building into zones with individual set points customized for occupant usage, typical solar loads, energy efficiency, and comfort. With no need for fossil fuels, an all-electric VRF heat pump or heat-recovery system provides energy-efficient heating and cooling by cycling refrigerant between an outdoor unit and each zone’s ductless or ducted indoor unit. Up to 50 indoor units can connect to a single outdoor unit. By selecting VRF zoning systems and heat pump water heaters, engineers can help developers and owners future-proof buildings for compliance, tenant expectations, and market value.\nElectrify Everything: The Time Is Now\nStrategic electrification is the movement to reduce energy use and society’s dependence on fossil fuels by switching to energy-efficient, all-electric technologies. Energy-efficient mechanical systems draw less energy from electricity grids transitioning from fossil fuels. As grids generate more electricity from renewable hydroelectric, wind, and solar power sources, the grid becomes cleaner and stronger — all-electric systems that minimize surplus energy use, such as space and water conditioning technologies, only further strengthen the grid.\nEnergy efficiency and superior comfort experiences have made VRF systems one of the fastest growing segments of the commercial HVAC industry, but sustainability through electrification is increasing nationwide demand for the technology. Building owners and developers are navigating an evolving real estate market and regulatory environment where sustainability significantly impacts their properties' values, operating costs, and marketability. Strategic electrification is happening now, driven by a mix of public and private entities on the local, state, and national levels, and proceeding along voluntary and mandatory paths, including through legislation like the Inflation Reduction Act.\nUtilities, states, cities, agencies, and nonprofits have published studies and reports on how strategic electrification is the best way to achieve aggressive carbon emission reduction goals. An example is New Jersey’s 2019 Energy Master Plan: Pathway to 2050. The report contains an analysis of decarbonization pathways performed by Evolved Energy Research and RMI for the New Jersey Board of Public Utilities. Per the analysis, fully electrifying vehicles and building systems is the most sensible decarbonization pathway for New Jersey. Electric appliances and vehicles save money by using “roughly one-third the energy of gasoline vehicles or natural gas-fired water heaters and furnaces.” Electrified buildings give the state more options for energy generation and can lead to new quality jobs in the electricity and building industries, including HVAC and plumbing.\nNew Jersey’s report recommends wider adoption of heat pumps in residential and commercial buildings and suggests the state provide incentives for installing all-electric building systems. This recommendation is consistent with other reports on decarbonization and electrification, including The Action Plan to Accelerate Strategic Electrification in the Northeast and the Variable Refrigerant Flow (VRF) Market Strategies Report, published by Northeast Energy Efficiency Partnerships (NEEP), one of six regional energy efficiency organizations (REEOs) funded in part by the U.S. Department of Energy.\nGuidehouse Research suggests energy efficiency spending in North America will grow to nearly $11 billion per year over the next decade. Public and private initiatives toward decarbonization coupled with the cost of energy are driving more stringent building codes and legislation. This includes rebates and financial incentives to encourage more sustainable buildings and the adoption of high-performance heat pumps and VRF systems. Along with federal standards and programs, like Energy Star, tax credits and utility rebates will continue accelerating the adoption of energy-efficient alternates to fossil-burning systems.\nSolving Electrification Challenges with VRF Technology\nVRF systems are uniquely positioned to help building owners meet CO2 reduction goals and lower energy use and costs. They’re designed for extended life cycles providing sustainable performance well into the future.\nAcross the U.S., a decline in coal-fired electricity reduced CO2 emissions by 2.1% in 2019. The last large coal-fired power plant in New England closed in May of 2017, and the last of New York’s coal plants closed in 2020. Renewable energy is now cost competitive with fossil-fuel-based generation for utility companies.\nThe energy efficiency and intelligence of VRF systems will help utilities manage the challenges of peak loads, utility-scale storage, and variable generation from wind and solar power. As strategic electrification expands, this will be increasingly important. Depending upon electrification adoption, the annual nationwide demand for electricity may increase by up to 3,700 terawatt hours or 85% by 2050.\nEnergy efficiency is a prerequisite for decarbonization. In regions with carbon-intensive power grids, using more efficient systems and appliances helps reduce energy consumption as a good first step toward lowering carbon footprints. Variable-capacity VRF heating and cooling systems are up to 40% more energy efficient than conventional, fixed-capacity HVAC equipment. Most of these savings occur during partial-load conditions, as VRF systems use inverter-driven compressors to adjust capacity and energy consumption to precisely match each zone’s load.\nVRF systems regularly achieve coefficients of performance (COPs) of 3 and higher, meaning they can deliver much more heat than the power they consume in watts. Additionally, VRF systems require less ductwork than forced-air systems, further reducing the energy consumed by large fan motors.\nCommercial heat pump water heaters using similar inverter-driven compressor technology are now available to provide high-volume domestic hot water (DHW) for commercial buildings. DHW accounts for roughly 25% of annual energy usage in multifamily buildings, hotels, hospitals, senior living facilities, and other commercial spaces. This energy expenditure is costly for building owners and has a significant environmental impact due to greenhouse gas emissions. As a solution, Mitsubishi Electric recently introduced the all-electric, cold-climate Heat2O Hot Water Heat Pump to significantly reduce the environmental impact of DHW through energy-efficient operation and use of CO2 refrigerant.\nCO2, a natural and environmentally friendly refrigerant with a global warming potential (GWP) of 1 and an ozone depletion potential (ODP) of zero, helps commercial facilities qualify for rigorous sustainability certifications, such as passive house status. Using Heat2O reduces on-site carbon emissions in the production of domestic hot water.\nDecarbonized Water Heating\nThe Heat2O system achieves highly efficient heat exchange using a patented twisted spiral gas cooler, where three refrigerant lines are wrapped around a twisted water pipe. The CO2 refrigerant flows in the opposite direction of the water. Running the refrigerant lines along the pipe’s grooves increases the heat-conductive area, while the spiral helps create a vortex in the pipe, accelerating the turbulence effect of water and reducing pressure loss in the heat exchanger. Transferring thermal energy from outdoor air to potable water by cycling refrigerant, natural CO2 enables the system to supply hot water up to 176°F, even when outside temperatures are as low as minus 13°.\nThe inverter-driven scroll compressor in the heat pump further increases Heat2O’s energy efficiency by enabling the system to modulate refrigerant flow and heating capacity to match loads. Heat2O can provide over four times more energy as heat than the system consumes in electricity.\nWith a COP of up to 4.52, Heat2O offers energy savings of 60%-70% for building owners and tenants compared to electric-resistance water heaters. If employed at scale in the U.S. alone, the environmental impact could be significant. Energy savings combined with incentives from utilities can ultimately offset first costs.\nHeat2O is an engineered solution designed to ensure installation quality and optimal performance of the heat pump. Components include a heat pump water heater; hot water storage tanks; swing tanks, secondary heat exchangers; and variable-speed, secondary circuit pumps.\nAll-electric DHW is already in demand for commercial buildings looking to meet aggressive sustainability and energy-cost measures. Preliminary data from an installation at the Bayview Tower, a Seattle Housing Authority multifamily dwelling, indicates Heat2O will save the housing authority more than 136,875kWh/year (a 40% decrease). This equates to about $15,000 in operational savings annually (using $0.11/kWh electrical rate). Previously, the building relied on an aging electric boiler system to produce DHW for 100 one-bedroom apartments on 13 stories.\nHotel Marcel, a $50 million renovation of the historic Pirelli building in New Haven, Connecticut, is one of the first net-zero hotels in the U.S. A Heat2O system is installed to achieve the project’s sustainability goals, including LEED Platinum certification and being 60% more energy efficient than code requirements.\nHeat2O is available in select markets, including New York, Massachusetts, Washington, Oregon, and Northern California. A nationwide rollout is anticipated for 2023.","Whether you are buying a house or your water heater is not producing hot water, water heater replacement may be on the horizon. Often, homeowners are not prepared for water heater replacement. When a water heater quits, it can become a shock to us and our wallet.\nPart of preparing and budget for a water heater replacement is knowing how long a water heater will last.\nAn electric tank water heater lasts 10 to 15 years. A gas tank water heater lasts 8 to 12 years. With routine maintenance, tank water heaters can last 20 years or more. Tank water heater repairs cost $200 to $300 while replacement costs $1500 to $2000. Tankless water heaters can last 15 to 20 years. With more functional parts that can fail, tankless water heater repair costs about $575, and replacement costs $1500 to $3000.\nDo you need a Licensed Plumber? We can help!\nGet a free estimate from top-rated, screened, and licensed plumbers in your area!\nA well-maintained gas or electric water heater can last 20 years or more. Water heater maintenance includes annual water heater flushing maintenance and replacing the anode rod every 2-3 years to protect the inner tank lining.\nIn this article, we will focus on tank-style water heaters. You can get details on how long a tankless water heater lasts here. Many factors go into determining how long a water heater will last. The number one factor in extending the life of a water heater is proper maintenance. Even inexpensive water heaters can outlast more expensive models provided it’s properly maintained.\nHowever, if the water heater is not maintained well, you can expect the lifespan to be shorter, and you could risk sudden failure or damage to your house.\nMost inexpensive water heaters come with a 6-year manufacturer warranty. However, some better quality models can have manufacturer warranties of 10 to 12 years. In contrast, a few high-end models offer lifetime warranties on the water heater.\nWater heaters with a fiberglass tank tend to last considerably longer than water heaters with steel tanks. Regardless of type, it is essential to remember that all tanks eventually fail no matter how well it’s maintained.\nCost to Repair a Tank Water Heater\nIf caught in time, most water heater repairs are relatively inexpensive and easy to do. However, this may not always be the best option. Here are some things that you should consider:\nYou should consider the repair cost and whether you can do that repair yourself or if you need to hire a licensed plumber. A plumber will cost about $50 to $150 per hour and often carries a minimum trip charge. If the water heater is experiencing a higher maintenance level, a replacement water heater will be your best option.\nThere’s no point in throwing good money after bad. If your repair cost will be 50% the cost of a new water heater and will only provide a small amount of remaining life expectancy, then replacement would be your best option.\n|Water Heater Repair||Part Cost||Repair with Labor|\n|Thermocouple||$20||$70 to $170 (1 hour)|\n|Gas Control Thermostat||$80||$130 to $210 (1 hour)|\n|Heating Elements||$40||$210 to $360 (2 hours)|\n|Electric Thermostat||$20||Included in Heating Element Replacement|\n|Leaking||Varies||Up to $1000|\nCost to Replace a Tank Water Heater\nNow that you’ve determined it’s time to replace your water heater, there are some things you need to consider:\n- Will you be replacing the water heater with a similarly sized unit? When you decided that it’s time to replace your water heater, it may be an excellent time to make an upgrade or to switch to a tankless system. If your house is damaged by a leak or needs to move the water heater, replacement is recommended.\n- Do you need a larger tank to meet your family’s needs? If your family has grown since you installed the water heater last time, a larger water heater may be desired. However, there are limitations to size when choosing tank water heaters, depending on where you live.\n- Will you install the water heater yourself or hire a licensed plumber? It is always recommended that you hire a licensed plumber to install a new water heater. Licensed plumbers in your area will know local building codes that you may not be aware of. However, depending on your skill level, you can install a new water heater yourself.\nNew Water Heater Tank and Installation Costs\nAccording to HomeDepot.com, the average cost of a new water heater installation is approximately $1,308. Nationwide costs typically range from $952 to $2,098. This replacement estimate includes the following components:\n- Traditional tank water heater – typical water heater size is 40 or 50 gallons.\n- Permits to install new water (some locations do not require this).\n- Installation materials such as fittings, shut-off valves, piping, etc.\n- Installation labor by a local, licensed, and insured plumber.\n- Removal of the old unit following local laws.\nIf you choose to purchase and install a water heater yourself, you can expect to pay between $350 to $2000 depending on the size and type of water heater you decide to purchase.\n|Water Heater Type/Size||Average Cost||Installed|\n|Electric, 40 Gallon||$400 – $450||$600 – $1050|\n|Electric, 50 Gallon||$450 – $1000||$650 – $1600|\n|Electric Hybrid, 50 Gallon||$1000 – $1500||$1200 – $2100|\n|Electric Hybrid, 80 Gallon||$1500 – $2250||$1700 – $2850|\n|Gas, 40 Gallon||$460 – $550||$660 – $1150|\n|Gas, 50 Gallon||$560 – $1200||$760 – $1800|\n|Gas, 75+ Gallon||$1400 – $1900||$1600 – $2500|\nBe Aware of Hidden Installation Costs\nWe touched on building codes and number three in the previous section. When you replace a water heater, you will be required to meet all current building codes.\nThere may be costs greater than just the water heater itself. Some of the hidden costs you may see when installing a new water heater include:\n- Expansion tanks – expansion tanks are used to help control pressure inside and water heater.\n- Seismic strapping – depending on where you live, you may need seismic strapping if you live in an area prone to earthquakes.\n- Drain pans – drain pans are placed under the water heater to catch leaks and discharge water to the exterior to protect the structure of your home.\n- Plumbing pipe upgrades – Depending on the type of plumbing supply pipes you have in your house, you may need upgrades. For example, if your home has polybutylene piping, you will likely need to replace all accessible piping during installation.\nWhy Hot Water Heaters Fail\nA water heater may seem to fail out of nowhere, but that’s not the case. Water heaters fail over time. It is recommended you drain and flush your water heater annually. It would be best to replace the anode rod every 2-3 years. This sacrificial rod protects the interior tank lining.\nYou can often predict water heater failure if you are aware of the signs. There are two main reasons why tank water heaters fail:\nToo much pressure inside the tank\nAs the water heats up, it expands, creating pressure inside the tank. If the water inside the tank heats up to the point that it exceeds the tank’s pressure specification, it could explode. One of the ways to avoid this is keeping your hot water temperature setting under 140°F.\nModern water heaters are equipped with a temperature and pressure relief valve (T&P valve). The temperature and pressure relief valve open when the pressure inside the tank reaches the tank’s temperature threshold. This valve expels hot water through a discharge pipe to relieve the pressure inside the tank.\nAnother safety feature that you can add is an expansion tank. Expansion tanks help control pressure inside the water heater tank by providing a place for hot water to expand. In a 50 gallon tank, hot water expands to 52 gallons. A 2-gallon expansion tank can store the expanded water.\nOne of the problems with old water heaters is that they lack a T&P valve and an expansion tank to help control the water heater’s pressure. With nowhere to expand, the pressure inside the tank can cause the water heater to explode, turning many old water heaters into a ticking time bomb.\nFlush Sediment Buildup Inside the Water Heater\nThe second leading cause of water heater failure is sediment buildup inside the tank. Chemicals and contaminants found in the water supply cause corrosion inside the tank, eventually rusting the tank, causing it to leak.\nExplosions occur when a flaw develops inside the tank. The tank’s pressure builds and forces water through these weak areas.\nAs part of your annual maintenance on your water heater, you should drain and flush out any sediment inside the tank. Tank water heaters have a drain valve at the base of the tank to do this. Here’s a quick rundown of how you can drain and flush your water heater.\n- Turn off the water supply before working on the water heater.\n- Turn off power or gas to the water heater before starting work.\n- Attach the hose to the drain valve and run it to the exterior to drain freely and safely. Remember the water will be hot.\n- Open hot water tap at a nearby sink.\n- Open the drain valve on the water heater to begin draining.\n- Once the water heater has drained, turn the water supply back on the flush the tank.\n- Close the drain valve once the water heater has been flushed and water running from the hose is clear.\n- Allow the water heater tank to refill.\n- Turn the power or gas back on to the water heater back. If you have a gas water heater, you’ll likely need to light the pilot.\nFor a more detailed step by step guide see, Water Heater Maintenance Tips to 2X Your Tanks Lifespan.\nReplace Your Anode Rod\nWater heaters with glass liners are equipped with an anode rod that attracts contaminants in water. Over time the contaminants will eat away at the anode rod protecting the tank’s liner. Once the anode rod has deteriorated, contaminants begin attacking the liner, weakening the water heater. You can expect anode rods to last 3-5 years, depending on the water quality and the type of anode rod installed.\nAnode rods are made from magnesium, aluminum, or aluminum/zinc alloy. Replacement anode rods can be purchased at local home improvement stores and are relatively easy to install.\nIt would help if you replaced your anode rod every 3 to 5 years. You can do this during your annual flushing maintenance of the water heater. You may want to replace a low-quality anode rod with a better quality one that will last longer.\nFor a more detailed step by step guide see, Water Heater Anode Rods: What it is & How to Replace it\nSigns You Need to Replace Your Water Heater\nAge of the Water Heater\nAge is a significant factor in your water heater’s performance. If your water heater is over ten years old, you should expect to have higher maintenance costs or to replace it in the near future.\nIf you’ve properly maintained your water heater, you may get up to 12 to15 years of life from a gas water heater and 10 to 15 years from electric water heaters. Beyond that, replacement is recommended mainly due to tank condition, rapid advances in safety, and improvements in energy efficiency.\nEnergy efficiency is crucial when it comes to gas water heaters. Energy consumption is higher in older water heaters versus newer models, which will pay for itself over time.\nRusty Water From Water Heater\nIf you see rusty or reddish-brown water from your water heater, that’s an indication that rust is mixing with water inside your tank. It’s a clear indication that your water heater needs replacement.\nIf the water heater is not leaking, you may be able to buy some time by performing a drain and flush maintenance on the water heater to remove loose sediment and change your anode rod. However, depending on the condition of the tank’s interior, this service may not be sufficient.\nUltimately, if the tank is damaged or starting to leak, there is no other option but replacement.\nGurgling Sounds Inside Your Water Heater\nAs your water heater gets older, sediment begins to build up at the bottom of the tank. When the sediment is heated and reheated over time, it begins to harden.\nBanging, crackling or gurgling sounds from inside your water heater indicates a sediment buildup inside the tank. It is recommended that you try to flush and drain your water heater. Depending on the amount of sediment build-up inside the water heater, the drain valve may be clogged or blocked, preventing it from draining water properly. One option would be to try replacing the drain valve to clear sediment from the water heater.\nIf all attempts to drain and remove the sediment fails, then replacement is the best option. In this case, the water heater’s replacement is recommended before a catastrophic leak occurs, which could cost thousands of dollars of damage to your house.\nLeaking from the Bottom of a Water Heater Tank\nIf your water heater is leaking, that is a good indication that there is something wrong. See our How to Repair a Leaking Water Heater – Complete Step by Step Guide for steps to repair a leaking water heater. You need to contact a licensed plumber to have them examine your water heater for repair or replacement.\nSome leaks can be repaired while others can’t. Leaks located around the top of the water heater, such as around the supply pipe fittings or the T&P valve, are likely repairable. However, this is not always the case and still an indication that something is going wrong internally that needs to be addressed.\nIf you’ve noticed water on the floor around your water heater, then you could have a severe problem. You can’t repair internal leaks in tanks. If water is leaking from the bottom of your tank, you should plan on an immediate replacement and call a licensed plumber.\nThe Water Heater Stops Producing Hot Water\nHot water is a daily luxury we’ve all become accustomed to. No hot water can be an inconvenience. If the electric water heater has no hot water, many things could be the cause. One is there is likely a bad heating element or a bad electric thermostat.\nAnother possibility is a broken drip tube. The drip tube is a plastic pipe that runs from the cold water inlet to the tank’s bottom. If this pipe is broken, cold water and hot water mix resulting in lukewarm water from the tank.\nThese items do commonly fail over time and can be repaired by a licensed plumber. These items are relatively easy to replace for the handy do-it-yourself person."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:7cde58cc-d5a4-4ae7-be2e-9bd07c944890>","<urn:uuid:6bd65563-e38d-437b-9468-f0cdda09b846>"],"error":null}
{"question":"Hola! Soy entrenadora y quiero saber: ¿qué beneficios tiene el entrenamiento de equilibrio para los adultos mayores y qué evidencia científica hay sobre su efectividad? 🤔","answer":"Balance training offers multiple benefits for older adults. Studies show it can significantly increase walking speed and muscular strength. When combined with resistance training, it improves balance and gait measures beyond the improvements achieved from strength training alone. The American College of Sports Medicine recommends that both active and inactive older adults perform balance training at least three days per week. This type of training is especially important for fall prevention in older adults, which is a common cause of injury. Scientific evidence from randomized trials demonstrates that elders can make significant gains in muscular strength and walking speed through resistance training, and adding balance training further improves balance and gait measures.","context":["This study compared the dietary intakes of supplementing (SA) and non-supplementing athletes (NSA). Twenty-five female master athletes (mean age = 50.4 yr) participated in the study (SA = 16, NSA = 9). Four-day diet records were analyzed using Nutritionist V. Statistical significance (p < .005) was determined by independent t tests. No significant differences were observed in intakes of kilocalories (SA = 2079 ± 628 kcals, NSA = 2001 ± 435 kcals), protein (SA = 104 ± 75 g, NSA = 84 ± 35 g), fat (SA = 65 ± 39 g, NSA = 61 ± 22 g), or carbohydrates (SA = 269 ± 112 g, NSA = 277 ± 43 g). Mean intakes exceeded Dietary Reference Intake (DRI) guidelines for all micro-nutrients except calcium and vitamin E (NSA = 79% and 87% of DRI, respectively). SA had significantly greater total intakes than NSA for calcium (p = .0001), magnesium (p = .004), vitamin C (p = .003), and vitamin E (p = .001). Results suggest that female master athletes may rely on dietary supplements rather than nutrient-dense food choices to provide daily nutritional needs.\nDonna Beshgetoor and Jeanne F. Nichols\nDonna Beshgetoor, Jeanne F. Nichols and Inah Rego\nThe focus of this prospective, observational study was to determine the effect of sport-specific training and calcium intake on bone mineral density (BMD) in female master cyclists, runners and non-athletes. Thirty women (12 cyclists, 9 runners, 9 controls), mean age of 49.6 ±7.9 years, were assessed at baseline and 18 months for calcium intake (4-day records), current exercise activity (recall questionnaire), and BMD of the lumbar spine and hip (DXA). A three (cyclists, runners, controls) by two (0 and 18 months) repeated measures ANOVA demonstrated a significant interaction effect of sport and time at the femoral neck (p < .04). Tukey post hoc analysis indicated that the BMD of the femur was maintained in cyclists and runners but declined in controls (p < .05). A significant time effect was noted in BMD at the lumbar spine (p < .001) and the trochanter (p < .003). BMD of the lumbar spine was maintained in runners but declined in cyclists (p < .007) and in controls (p < .03), while trochanteric BMD declined in all groups (p < .01). No significant interaction effect of sport and dietary calcium intake was noted for BMD at any site.\nDeborah F. Verfaillie, Jeanne F. Nichols, Ellen Turkel and Melbourne F. Hovell\nThe purpose of this study was to compare the effects of resistance training alone or in combination with balance and gait training on balance and gait measures in seniors. Subjects, ranging in age from 65 to 83 years, were randomly assigned to a strength and balance/gait group (SB, n = 21 ) or a control group (S, n = 18) receiving strength and relaxation training. Both groups significantly increased their strength and gait speed over the 12-week training period, but step length remained unchanged. The results suggest that elders can make significant gains in muscular strength and walking speed through resistance training, and that adding balance and gait training to resistance training can significantly improve some balance and gait measures beyond improvements achieved from strength training alone. If replicated, these results set the stage for investigations of injury control benefits possible from balance training.\nJeanne F. Nichols, Karen P. Nelson, Katrina K. Peterson and David J. Sartoris\nThe purpose of this investigation was to determine the effects of high-intensity strength training on bone mineral density (BMD) of 34 non-estrogen-repleted, active women over 60 years of age. The study was designed as a randomized, nonblinded trial in which subjects were stratified into rank-ordered pairs by level of physical activity, then randomly assigned into either a weight training (WT) or a control (CON) group. BMD of the spine (L2–L4), hip, and total body was assessed at 0, 6, and 12 months by dual energy x-ray absorptiometry. Group-by-time repeated-measures ANOVA demonstrated no effect of weight training on BMD, despite marked gains in muscular strength for all exercises. The high-intensity weight training utilized in this study did not induce positive changes in BMD of the hip and spine of previously active, non-estrogen-repleted older women. However, the protocol was safe, enjoyable, and highly effective in increasing muscular strength.\nJeanne F. Nichols, Lori M. Hitzelberger, Jennifer G. Sherman and Patricia Patterson\nThis study examined the efficacy of a progressive resistance exercise program, using equal concentric/eccentric (CE) or greater eccentric/concentric (GE) workloads, for increasing strength and improving functional abilities of community-dwelling older adults. Sixty men and women were randomly assigned to one of three groups: CE, GE, or control. All strength testing and training took place on six Lifecircuit machines. Functional tests included a bas carry, weighted stair climb, shelf task 1-RM, and static balance. Significant interactions in strength tests were noted for the chest, back, and shoulder exercises. GE improved in shoulder strength more than CE and control For functional measures, all weight trainers were grouped and compared to controls. A significant interaction occurred for the stair climb and balance with the exercise groups decreasing stair climb time by 11% and increasing balance time by 26%. Relative improvements by weight trainers of 12% for the shelf task and 7% for the bag carry were not significant. These data indicate that a moderate intensity resistance program can have positive effects on tasks required for everyday function.\nJeanne F. Nichols, Mitchell J. Rauh, Michelle T. Barrack, Hava-Shoshana Barkai and Yael Pernick\nThe authors’ purpose was to determine the prevalence and compare associations of disordered eating (DE) and menstrual irregularity (MI) among high school athletes. The Eating Disorder Examination Questionnaire (EDE-Q) and a menstrual-history questionnaire were administered to 423 athletes (15.7 ± 1.2 y, 61.2 ± 10.2 kg) categorized as lean build (LB; n = 146) or nonlean build (NLB; n = 277). Among all athletes, 20.0% met the criteria for DE and 20.1% for MI. Although the prevalence of MI was higher in LB (26.7%) than NLB (16.6%) athletes (P = 0.01), no differences were found for DE. For both sport types, oligo/amenorrheic athletes consistently reported higher EDE-Q scores than eumenorrheic athletes (P < 0.05). Athletes with DE were over 2 times as likely (OR = 2.3, 95%CI: 1.3, 4.2) to report oligo/amenorrhea than athletes without DE. These data establish an association between DE and MI among high school athletes and indicate that LB athletes have more MI but not DE than NLB athletes.\nKatie J. Thralls, Jeanne F. Nichols, Michelle T. Barrack, Mark Kern and Mitchell J. Rauh\nEarly detection of the female athlete triad is essential for the long-term health of adolescent female athletes. The purpose of this study was to assess relationships between common anthropometric markers (ideal body weight [IBW] via the Hamwi formula, youth-percentile body mass index [BMI], adult BMI categories, and body fat percentage [BF%]) and triad components, (low energy availability [EA], measured by dietary restraint [DR], menstrual dysfunction [MD], low bone mineral density [BMD]). In the sample (n = 320) of adolescent female athletes (age 15.9± 1.2 y), Spearman’s rho correlations and multiple logistic regression analyses evaluated associations between anthropometric clinical cutoffs and triad components. All underweight categories for the anthropometric measures predicted greater likelihood of MD and low BMD. Athletes with an IBW ≤85% were nearly 4 times more likely to report MD (OR = 3.7, 95% CI [1.8, 7.9]) and had low BMD (OR = 4.1, 95% CI [1.2, 14.2]). Those in <5th percentile for their age-specific BMI were 9 times more likely to report MD (OR 9.1, 95% CI [1.8, 46.9]) and had low BMD than those in the 50th to 85th percentile. Athletes with a high BF% were almost 3 times more likely to report DR (OR = 2.8, 95% CI [1.4, 6.1]). Our study indicates that low age-adjusted BMI and low IBW may serve as evidence-based clinical indicators that may be practically evaluated in the field, predicting MD and low BMD in adolescents. These measures should be tested for their ability as tools to minimize the risk for the triad.\nJeanne F. Nichols, Hilary Aralis, Sonia Garcia Merino, Michelle T. Barrack, Lindsay Stalker-Fader and Mitchell J. Rauh\nThere is a growing need to accurately assess exercise energy expenditure (EEE) in athletic populations that may be at risk for health disorders because of an imbalance between energy intake and energy expenditure. The Actiheart combines heart rate and uniaxial accelerometry to estimate energy expenditure above rest. The authors’ purpose was to determine the utility of the Actiheart for predicting EEE in female adolescent runners (N = 39, age 15.7 ± 1.1 yr). EEE was measured by indirect calorimetry and predicted by the Actiheart during three 8-min stages of treadmill running at individualized velocities corresponding to each runner’s training, including recovery, tempo, and 5-km-race pace. Repeated-measures ANOVA with Bonferroni post hoc comparisons across the 3 running stages indicated that the Actiheart was sensitive to changes in intensity (p < .01), but accelerometer output tended to plateau at race pace. Pairwise comparisons of the mean difference between Actiheart- and criterion-measured EEE yielded values of 0.0436, 0.0539, and 0.0753 kcal · kg−1 · min−1 during recovery, tempo, and race pace, respectively (p < .0001). Bland–Altman plots indicated that the Actiheart consistently underestimated EEE except in 1 runner’s recovery bout. A linear mixed-model regression analysis with height as a covariate provided an improved EEE prediction model, with the overall standard error of the estimate for the 3 speeds reduced to 0.0101 kcal · kg−1 · min−1. Using the manufacturer’s equation that combines heart rate and uniaxial motion, the Actiheart may have limited use in accurately assessing EEE, and therefore energy availability, in young, female competitive runners.","Health and Fitness Benefits of Balance Training Exercises\nWhy is Balance Important?\nAll functional movements, from walking down the street, to dancing, to properly executing a squat, require a working balance system. A basic definition of balance is the body being in a still state of equilibrium. Even more important is dynamic balance, in which the body moves and changes directions in an agile, skilled way without falling over.\nBalance may be taken for granted as you go about your exercise routine; however, it’s a mistake to neglect training your balance mechanisms! Suboptimal balance puts you at increased risk of hurting yourself, as well as the development of muscle imbalances which affect your posture, putting you smack-dab in the cumulative injury cycle.\nIn addition to the balance training modalities described in this article, activities such as yoga and tai chi provide built-in balance training among myriad other benefits. The key is to find balance training workouts that are enjoyable and effective for you.\nWhat Types of Equipment are Used in Balance Training?\nSome balance exercises are done on the ground with no equipment at all. Other exercises may require simple equipment to further challenge your balance, such as half foam rolls, small\nbalance beams, balance disks, wobble boards, and BOSU balls. Many of these implements come in different sizes and materials to provide a progressive challenge.\nMost gyms have this type of equipment on hand, or you can buy balance training equipment online if you prefer to train at home.\nWho Should Add Balance Training to Their Workouts?\nBalance training is important when you begin working out, as it helps greatly in joint stabilization and developing coordination. Balance is also important as you progress into the more advanced strength levels of training; the exercises become more demanding on your neuromuscular system, and your body needs to have a trained balance mechanism to keep up. Additionally, advanced exercisers benefit from balance training due to the performance enhancements optimal balance offers.\nEveryone can benefit from balance training exercises; I encourage everyone from casual exercisers, runners, dancers, martial artists, and athletes of all other types to integrate balance exercises into their workout regimen.\nWhat Health and Fitness Benefits Does Balance Training Offer?\nAn excellent benefit provided by balance training is joint stabilization. Stabilization of joints involves training the strength of muscles themselves as well as training the sensory receptors in the nervous system that give your brain information about your body position. More refined information from these receptors decreases the chance of injury due to improper joint motion.\nJoint dysfunction is a major cause of lower back injuries, which affect 80% of the adult U.S. population. The more we can do to prevent joint dysfunction, the better.\nBalance training increases your coordination and kinesthetic awareness, allowing you to have more conscious control of your movements, as well as training your reflexive system to respond more appropriately to changing conditions without your having to think about it. This leads to increases in fitness and athletic performance.\nIn older adults, balance training helps prevent falls which are a common cause of injury. The American College of Sports Medicine recommends that both active and inactive older adults perform balance training at least three days a week.\nWhat are the Characteristics of an Effective Balance Training Program?\nBalance training exercises should challenge your limit of stability, or the distance outside your center of gravity you can move without falling over. Your limit of stability will become wider the more you train your balance. A good balance training program is progressive. You will want to start with easier exercises, and then move on to more difficult exercises as your balance improves over time. Don’t push too fast, or you may experience setbacks in the form of injury.\nYou should aim for controlled instability in your balance training sessions. This can mean employing simple equipment meant to provide a progressive challenge. For example, you begin a certain exercise on the floor, then as you improve in subsequent sessions, you may graduate to a balance beam or half foam roll, then onto a balance disk, then a BOSU ball, and onward.\nHow Does Balance Training Fit into a Workout Session?\nBalance training exercises are appropriate at various points throughout your workout. Consider the following suggestions and see what works best for you.\nThe National Academy of Sports Medicine (NASM) recommends doing balance exercises after your warm-up, but before the tougher part of your workout such as strength training. However, a recent study of soccer athletes showed more improvement in the balance of those performing balance exercises after their workout versus those who did balance exercises before working out.\nThe key is to DO the exercises, and do them regularly, whether you prefer to do them right after your warm-up or just before your cool-down. For maximum benefits, perform your sequence of balance exercises for 10 minutes three or four times per week for at least four weeks.\nWhat are Some Effective Exercises to Train Balance?\nBalance exercises range from very simple for beginners, to more complex for folks with more training. Here are just a few examples of balance exercises for beginning, intermediate, and advanced levels.\nBeginners Balance Training Exercise: Single Leg Stance Variations\nStanding in a neutral position with hands on hips, lift one leg in front of you as high as can be controlled. Hold here for a few seconds, then without letting the foot touch the ground, bring it back to center, then out to the side as high as can be controlled. Again, hold here for a few seconds, then bring the leg back to center, and then extend behind you. Hold for a few seconds and then repeat the whole sequence from the beginning.\nSwitch legs and repeat.\nTIP: Contract the glutes of the leg you’re balancing on to stabilize.\nThis exercise can be progressed by balancing on a balance disk, the flat side of a half foam roll, or other piece of equipment.\nIntermediate Balance Training Exercise: Single-Leg Squat Touchdown\nFrom a neutral position with feet and hips pointing straight ahead, lift one leg. Slowly squat down as if you’re going to sit in a chair. Without letting the lifted foot touch the floor, reach down with the hand on the same side as the lifted foot and touch the floor or the balance foot. Stand up using your glutes and abs. Without letting the foot touch down, repeat a few more times.\nSwitch legs and repeat.\nTIP: Focus on stability and optimal posture! Don’t let the knee cave in or bow out. If you find you can’t keep your knee from caving in, you need to focus on correcting the muscle imbalances likely at fault. Consult a personal trainer.\nAdvanced Balance Training Exercise: Single Leg Hop with Stabilization\nThis advanced balance exercise should only be attempted by folks with properly stabilized ankles and knees.\nStanding in a neutral position with hands on hips, lift one knee to up to a 90-degree angle. Hop forward, landing on the opposite foot and stabilizing for 5 seconds. Hop back onto the opposite foot, landing and stabilizing for 5 seconds again. You can also hop from side to side, or try rotating on a diagonal for more of a challenge.\nSwitch feet and repeat.\nTIP: Focus on a soft landing, distributing forces evenly to avoid stressing the joints too much. Keep your knee aligned over top of the second and third toes; don’t let in cave in or bow out.\nBuild a better, more integrated workout program by integrating balance training into your routine. Consult a physician if you find you struggle with balance, as several health conditions can lead to balance impairments. Enlist the help of a personal trainer to receive a workout plan tailored to your specific goals.\nClick here to buy balance training equipment to use at home!\nBalance Training Exercises: References\n1. Clark, M.A., Lucett, S.C., and Sutton, B.G., (Eds.). (2012). Balance training concepts. NASM essentials of personal fitness training (4th ed). Philadelphia: Lippincott Williams & Wilkins.\n2. Gioftsidou, A., et al. (2006). The effects of soccer training and timing of balance training on balance ability. European Journal of Applied Physiology, 96(6): 659-664.\n3. Heyward, V. (2010). Designing balance training programs. Advanced Fitness Assessment and Exercise Prescription (6th ed.). Human Kinetics.\n- BOSU Ball Exercises\n- BOSU Balance Trainer Review\n- Best Ball Exercises\n- Best Exercise Ball\n- BOSU Ball Workout\n- Top 5 Reasons to BOSU\n- Overpronation Foot Problems\n- Balance Exercises for Seniors"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:e12b68a8-50e3-456c-8f4e-ed066dd0eae0>","<urn:uuid:1350d0f7-e536-4aaa-ba28-defbb00175b4>"],"error":null}
{"question":"How do combined blood gas analyzers differ across hospital settings, and what training requirements exist for their safe operation?","answer":"Combined analyzers are available in various modalities including portable, benchtop, and laboratory versions for use in hospitals, clinics, ambulatory surgical centers, and point-of-care settings. While these devices are designed to be intuitive, specific training is crucial for safe operation. The lack of proper training has led to documented accidents and fatalities, particularly due to misinterpretation of readings and understanding of limitations. Critical aspects include understanding environmental influences, technological limitations, and specific operating procedures for each instrument's unique set of sensors to prevent hazardous incidents.","context":["Global Blood Gas and Electrolyte Market, By Product (Blood Gas Analyzers, Electrolyte Analyzers, Combined Analyzers, Consumables), Modality (Portable, Benchtop, Laboratory), End-User (Hospitals & Clinics, Ambulatory Surgical Centers, Point-of-Care, Laboratory & Institute, Others), Country (U.S., Canada, Mexico, Germany, Italy, U.K., France, Spain, Netherlands, Belgium, Switzerland, Turkey, Russia, Rest of Europe, Japan, China, India, South Korea, Australia, Singapore, Malaysia, Thailand, Indonesia, Philippines, Rest of Asia- Pacific, Brazil, Argentina, Rest of South America, South Africa, Saudi Arabia, UAE, Egypt, Israel, Rest of Middle East & Africa) Industry Trends and Forecast to 2028\nThe blood gas and electrolyte market is expected to gain market growth in the forecast period of 2021 to 2028. Data Bridge Market Research analyses the market to grow at a CAGR of 6.62% in the above-mentioned forecast period. Rise in the levels of patients under distress from chronic diseases drives the blood gas and electrolyte market.\nBlood gas is defined as a particular group of diagnostic tests which is carried out to determine pH, amount of oxygen and carbon dioxide present in a sample of blood. The sample is extracted generally from the artery of patients to determine any disorders with the lung as it helps if there is any disorder. Electrolytes are an essential part of the functioning of the human body, they are generally found in the cells of humans, electrolyte testing involves the detection of sodium, potassium, chloride and bicarbonate.\nRise in the number of patients admitted in hospitals, clinics, patient wards resulting in rise in demand for the product from these settings acts as a vital factor escalating the market growth, also rise in the advancements in the market leading to availability of point-of-care, rise in the demand for integrated systems and rise in the growth in the elderly population base in developed countries are the major factors among others driving the blood gas and electrolyte market. Moreover, rising technological advancements and modernization in the healthcare sector and rising research and development activities in the healthcare sector will further create new opportunities for blood gas and electrolyte market in the forecasted period of 2021-2028.\nHowever, increase in the complications in understanding and determining the results of blood gas and electrolyte analysis and rise in the usage of the analyzers by the unskilled professionals and technicians resulting in errors of the diagnostic results acts as the major factors among others which will obstruct the market growth, and will further challenge the growth of blood gas and electrolyte market in the forecast period mentioned above.\nThe blood gas and electrolyte market report provides details of new recent developments, trade regulations, import export analysis, production analysis, value chain optimization, market share, impact of domestic and localised market players, analyses opportunities in terms of emerging revenue pockets, changes in market regulations, strategic market growth analysis, market size, category market growths, application niches and dominance, product approvals, product launches, geographic expansions, technological innovations in the market. To gain more info on the blood gas and electrolyte market contact Data Bridge Market Research for an Analyst Brief, our team will help you take an informed market decision to achieve market growth.\nBlood Gas and Electrolyte Market Scope and Market Size\nThe blood gas and electrolyte market is segmented on the basis of product, modality and end-user. The growth amongst these segments will help you analyse meagre growth segments in the industries, and provide the users with valuable market overview and market insights to help them in making strategic decisions for identification of core market applications.\n- On the basis of product, the blood gas and electrolyte market is segmented into blood gas analyzers, electrolyte analyzers, combined analyzers and consumables.\n- Based on modality, the blood gas and electrolyte market is segmented into portable, benchtop and laboratory.\n- The blood gas and electrolyte market is also segmented on the basis of end-user into hospitals & clinics, ambulatory surgical centers, point-of-care, laboratory & institute and others.\nBlood Gas and Electrolyte Market Country Level Analysis\nThe blood gas and electrolyte market is analysed and market size insights and trends are provided by country, product, modality and end-user as referenced above.\nThe countries covered in the blood gas and electrolyte market report are U.S., Canada and Mexico in North America, Germany, France, U.K., Netherlands, Switzerland, Belgium, Russia, Italy, Spain, Turkey, Rest of Europe in Europe, China, Japan, India, South Korea, Singapore, Malaysia, Australia, Thailand, Indonesia, Philippines, Rest of Asia-Pacific (APAC) in the Asia-Pacific (APAC), Saudi Arabia, U.A.E, South Africa, Egypt, Israel, Rest of Middle East and Africa (MEA) as a part of Middle East and Africa (MEA), Brazil, Argentina and Rest of South America as part of South America.\nNorth America dominates the blood gas and electrolyte market due to rise in the presence of high practitioner and patient awareness levels, well-established healthcare infrastructure, and increase in the government support for research and development exercises in this region. Asia-Pacific is the expected region in terms of growth in blood gas and electrolyte market due to rise in the presence of high unmet clinical needs coupled with rapidly rising healthcare expenditures and patient awareness levels in this region.\nThe country section of the blood gas and electrolyte market report also provides individual market impacting factors and changes in regulation in the market domestically that impacts the current and future trends of the market. Data points such as consumption volumes, production sites and volumes, import export analysis, price trend analysis, cost of raw materials, down-stream and upstream value chain analysis are some of the major pointers used to forecast the market scenario for individual countries. Also, presence and availability of global brands and their challenges faced due to large or scarce competition from local and domestic brands, impact of domestic tariffs and trade routes are considered while providing forecast analysis of the country data.\nHealthcare Infrastructure Growth Installed base and New Technology Penetration\nThe blood gas and electrolyte market also provides you with detailed market analysis for every country growth in healthcare expenditure for capital equipment’s, installed base of different kind of products for blood gas and electrolyte market, impact of technology using life line curves and changes in healthcare regulatory scenarios and their impact on the blood gas and electrolyte market. The data is available for historic period 2010 to 2019.\nCompetitive Landscape and Blood Gas and Electrolyte Market Share Analysis\nThe blood gas and electrolyte market competitive landscape provides details by competitor. Details included are company overview, company financials, revenue generated, market potential, investment in research and development, new market initiatives, global presence, production sites and facilities, production capacities, company strengths and weaknesses, product launch, product width and breadth, application dominance. The above data points provided are only related to the companies’ focus related to blood gas and electrolyte market.\nThe major players covered in the blood gas and electrolyte market report are Abbott, OPTI Medical, Siemens, F. Hoffmann-La Roche Ltd, Medica Corporation, Radiometer India, Nova Biomedical, Erba Diagnostics, SENSACORE, Instrumentation Laboratory Company, Samsung Medison Co., Ltd., Accurex Biomedical Pvt. Ltd., Cerner Corporation, Edan Instruments, Inc., Bayer AG and Tecom Analytical Systems. among other domestic and global players. Market share data is available for Global, North America, Europe, Asia-Pacific (APAC), Middle East and Africa (MEA) and South America separately. DBMR analysts understand competitive strengths and provide competitive analysis for each competitor separately.\nCustomization Available : Global Blood Gas and Electrolyte Market\nData Bridge Market Research is a leader in advanced formative research. We take pride in servicing our existing and new customers with data and analysis that match and suits their goal. The report can be customised to include price trend analysis of target brands understanding the market for additional countries (ask for the list of countries), clinical trial results data, literature review, refurbished market and product base analysis. Market analysis of target competitors can be analysed from technology-based analysis to market portfolio strategies. We can add as many competitors that you require data about in the format and data style you are looking for. Our team of analysts can also provide you data in crude raw excel files pivot tables (Factbook) or can assist you in creating presentations from the data sets available in the report.","5 gas detection misconceptions\nIn recent years, portable gas detectors have shrunk in size and cost while many new features have been added: automatic time-weighted average (TWA) and short-term exposure limit (STEL) calculations, data logging, man-down alarm, and wireless capabilities to name a few. With increased usage due to reduced cost, enhanced safety awareness and tighter regulations, misconceptions regarding correct portable instrument usage have increased accordingly.\nMisconceptions become problematic when those ideas are gradually accepted as truth. Those safety industry misconceptions can not only be costly, but also potentially hazardous, injurious and even deadly. Five common safety industry misconceptions will be discussed here.\nMisconception #1: Gas detection is self-explanatory; no training is\nMost manufacturers design portable gas detectors to be as intuitive and as simple to use as possible, offering quick-start guides and electronic simulators; however, training related to the specific application is still necessary. While most employers provide adequate training programs, there are still many safety managers who provide little or no specific application training, assuming instead that workers will interpret the manufacturers’ limitations accordingly.\nA gas detector will do what it is designed to do: provide alarms that sound, light or vibrate when a gas hazard that the unit is equipped to detect is present. Of critical importance is the need to understand environmental influences and technological limitations in order to prevent informational misinterpretations, worker panic or major incidents concerning potentially hazardous gas exposures.\nExamples of accidents due to inadequate training range from fatalities occurring within confined spaces to tank explosions. An example of the latter: a documented incident concerning combustible gas readings taken prior to welding. The latest reading indicated levels well below the lower explosive limit (LEL). Improper instrument use plus the operator’s inadequate understanding of its limitations were cause of a fatality. Low oxygen level (<10 percent) resulted in an incorrect LEL reading interpretation. Other documented informational misinterpretation incidents relate to cross-sensitivity or sensor poisoning. Every instrument has its own set of sensors, operating procedures and instructions that must be understood by all users.\nGas detection education and application-specific training improves user comfort level, reduces misunderstanding, and reduces costs long-term. Most gas detection instrument manufacturers offer training classes to improve your knowledge base. However, the addition of comprehensive application-based training can be the difference between life and death.\nMisconception #2: Daily instrument bump tests are not required as long as you calibrate periodically.\nThis statement is commonly used in the field to win sales orders; however user manuals may contradict assertions concerning frequency of bump tests and full calibrations. There is no way to ensure that gas detectors work properly unless gas is applied and alarms and readings are verified. It doesn’t matter whether your fleet includes disposable alarming devices or direct reading instruments. Recent cases of O2 sensors ‘frozen’ on 20.9 percent Vol reading, or unresponsive CO sensors displaying stable zero readings are clear evidence. Poisoned combustible sensors can also display stable zero readings and not respond to applied combustible gas. After such industry-wide occurrences and resulting safety notices, the assertion above is thankfully less likely to be used.\nTo make life easier for users, manufacturers have developed bump and automatic calibration stations to simplify complex and time-consuming processes, minimizing necessary training for performing required instrument tests before each day’s use.\nWhile several manufacturers offer innovative ways to verify sensor life and health, there is no current automatic solution to determine that sensor outer membranes are clogged with invisible polymers or dirt, an issue that can prevent gas from reaching sensors.\nMisconception #3: All gas detectors are the same and provide the same safety level.\nNot all detectors are created equal. Most manufacturers use off-the-shelf sensors from known sensor suppliers, but only a few have the know-how and capability to manufacture their own sensors. While all gas detectors are approved to meet certain standards, only some meet stringent performance standards and will hold up to the harshest conditions.\nWhere costs are concerned, the devil is in the details; sensor life, response time, warranty, rugged design, total cost of ownership, etc. differ significantly when comparing competitive gas detectors. Faster-responding sensors not only alert you more quickly to potentially hazardous situations, but also save you gas and labor time with every calibration and bump test. Unreliable sensors and poorly constructed detectors can cause unnecessary downtime and shutdowns, costing you more than ever imagined.\nFeatures such as color-change display screens can be critical in conveying information regarding potentially hazardous situations at a glance and taking proper, immediate action. In isolated environments where co-workers are out of sight, features such as man-down alarms enhance worker safety. Instruments with significant battery run time allow for use of the same detector for two or more shifts, reducing downtime and related costs.\nMisconception #4: Gas detectors don’t require routine maintenance.\nMaintenance is part of life…get used to it. You need to maintain your car and your home; even your body needs a periodic checkup to ensure that all parts work properly. Why would you not expect to maintain a device that is designed to save your life?\nPeriodic maintenance is needed to keep your company operating and profitable by minimizing unexpected failures and possible accidents; gas detectors are no different in that respect. The European Technology Platform Industrial Safety (ETPIS) estimates that 10-15 percent of fatal work-related accidents and 15-20 percent of all accidents are related to lack of proper equipment maintenance.\nWhile maintenance is viewed as a necessary evil, a critical look at maintenance frequency and actual total cost can really pay dividends. Some manufacturers offer ways to reduce or eliminate portable instrument maintenance by offering total care programs built around leasing and renting instrument fleets to customers. Manufacturers provide instrument maintenance; consequential costs are simply included as part of leasing fees.\nActual long-term cost-savings are hidden in the warranty, durability, sensor life and response times. When optimized, those features can save you significant maintenance costs overall by reducing instrument and sensor replacement frequency, component replacement and calibration gas usage. Cost of ownership over several years typically far exceeds actual purchasing cost of detectors. Calculating total maintenance cost over a longer period can be a true eye-opener.\nMisconception #5: Gas detectors CAN accurately detect 1.0 ppm H2S.\nThe American Conference of Governmental Industrial Hygienists (ACGIH) has amended its H2S exposure recommendation by reducing H2S TWA level to 1.0 ppm; some European countries have changed their requirement to 1.6 ppm.\nLet’s consider a gas detector with a reading of 1.0 ppm H2S, calibrated with typical NIST-traceable calibration gas of 20 ppm H2S concentration. Cylinder accuracy is stated as ± 10 percent, meaning 2.0 ppm. Due to environmental factors (temperature, humidity, pressure, etc.) and H2S reactivity, the best electrochemical H2S sensor currently available offers 5 percent of full-scale accuracy and 0-30 ppm detection range or 1.5 ppm.\nTolerances of instrument and calibration gas values are additive, resulting in 2 ppm + 1.5 ppm = 3.5 ppm. As a result, a typical 1.0 ppm reading is correctly stated as 1.0 ± 3.5 ppm; in other words, the concentration may be anywhere between -2.5 ppm and 4.5 ppm.\nThe tolerances listed above, in addition to inherent electrical noise of sensors and associated circuits, may cause instrument manufacturers to employ noise dampening techniques that mask low-level readings. Consequently, some gas detectors may not provide such precise readings at 1.0 ppm and may not even display values until they reach 3.0 ppm.\nDue to the factors described above and depending upon environmental conditions, specific sensor type and manufacturer chosen, users with instrument H2S alarms set at 1.0 ppm may not be alerted to concentrations between 1.0 and 4.5 ppm and/or may encounter frequent false alarms. This is important for users required to follow this ACGIH limit, as opposed to the OSHA, NIOSH or other recognized H2S exposure limits. Choose an instrument that provides the appropriate detection capability.\nAssumptions are not necessarily evil. Research is built upon assumptions, but should subsequently be followed with proper empirical analysis to prove theories as correct or incorrect. Avoid misconceptions, as acting upon them can result in negative impact upon the well-being of your personnel and affect the bottom line. Verify, rather than assume. Reliable and trustworthy manufacturers will help you to work through your questions and guide you to safer product solutions.\n• European Technology Platform Industrial Safety (ETPIS)\n• Occupational Safety and Health Administration (OSHA)\n• Safety and Health Information Bulletins (SHIB)\n• American Conference of Governmental Industrial Hygienists (ACGIH), www.ACGIH.org/TLV/"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:ad24b556-4ebc-4bc1-a93c-9cdeb63c0db2>","<urn:uuid:7096024e-4640-451f-a560-a01bd9ea2f08>"],"error":null}
{"question":"Hey there, can you break down how the NPT and TPNW differ in their approach to nuclear weapons legitimacy? I know they're both important treaties but they seem pretty different.","answer":"The NPT and TPNW take fundamentally different approaches to nuclear weapons legitimacy. The NPT recognizes and legitimizes nuclear weapons possession by five states (US, UK, France, Russia, and China) while preventing others from acquiring them. It also encourages peaceful nuclear technology development. In contrast, the TPNW completely delegitimizes nuclear weapons by making them illegal and placing them in the same category as other banned weapons of mass destruction. The TPNW prohibits developing, testing, producing, manufacturing, acquiring, possessing or stockpiling nuclear weapons, while requiring states with nuclear weapons to destroy them according to a deadline if they join the treaty.","context":["A remarkable thing happened on October 24, 2020. It is a date that should go down in history. And yet, as far as I can tell there was barely a mention by the major news media.\nWhat is that momentous thing?\nNuclear weapons are now officially illegal – that’s what.\nAt least they will be on 22 January 2021 when the TPNW enters into force and becomes binding international law.\nThe TPNW is the UN Treaty on the Prohibition of Nuclear Weapons and it just reached the 50 ratifications needed to enter into force in 90 days\nWouldn’t you think the world would be celebrating? Wouldn’t you think the International Campaign to Abolish Nuclear weapons (ICAN) would be hailed as heroes, even nominated for another Nobel Peace Prize?\nInstead, the Treaty has been met with silence, apparent indifference or cynicism, and by some with overt hostility.\nThe cynics argue that the Treaty is meaningless and it won’t change anything because the nine nuclear powers were “excluded”.\nIn fact, they excluded themselves. The meetings and negotiations were open to all UN member and observer states but not one of the nuclear-armed states chose to attend.\nSo, does that make the Treaty meaningless? Certainly not. It will still serve to delegitimise nuclear weapons and that’s its main purpose. Nuclear weapons are now in the same unacceptable legal category as other weapons of mass destruction such as biological and chemical weapons.\nCritics say the Treaty lacks validity because it wasn’t agreed to by consensus. But the Netherlands (which hosts NATO nuclear weapons) blocked consensus. This forced the TPNW text to go to the vote. The result: 122 to 1 in favour with 1 abstention (Singapore).\nVoting is nothing new or outlandish, is it? Anyway, the two-thirds majority voting procedure followed is the standard for multilateral treaty negotiations, as per the Vienna Convention on the Law of Treaties.\nThe Nuclear Non-proliferation Treaty (NPT) and the Comprehensive Test Ban Treaty (CTBT) were both adopted through voting at the UN General Assembly despite the preference for reaching agreement by consensus.\nIn negotiations that require consensus with no option to vote, a single state can veto a text every other state agrees upon. If the negotiators had waited for consensus we can only guess what a watered down version of the Treaty would have looked like. Very likely it would then really have been meaningless.\nSome say the negotiations should have taken place within the framework of the Conference on Disarmament. It is true that forum and its predecessors have delivered important agreements such as the Non Proliferation Treaty (NPT), the Biological Weapons Convention (BWC), the Chemical Weapons Convention (CWC) and the Comprehensive Test Ban Treaty (CTBT). But the Conference on Disarmament is limited to 65 states and has been deadlocked for the last twenty years. That’s probably because it operates by consensus. Given the resistance to the TPNW by the nuclear powers and those under their nuclear umbrella, it is unlikely the initiating meetings would have happened at all, let alone the treaty negotiations themselves.\nAnother criticism is the lack of strong verification rules. But the TPNW does provide for both non-proliferation safeguards and disarmament verification.\nParties to the TPNW must have or adopt a safeguards agreement with the International Atomic Energy Agency (IAEA). The 132 states that have accepted the IAEA Additional Protocol (AP) that allows IAEA inspection of undeclared nuclear facilities will be legally committed to continue that agreement. And the TPNW also specifically adopts the safeguards of the Non Proliferation Treaty (NPT) plus extra safeguards in a legally binding treaty.\nRegarding disarmament verification, Article 4 (1) of the Treaty stipulates that nuclear-armed states that claim to have disarmed before joining the TPNW must cooperate with an institution mandated by the TPNW state parties to verify that it really has eliminated its nuclear-weapons program.\nArticle 4 (2) stipulates that nuclear-armed states that join the TPNW while possessing nuclear weapons must immediately remove their nuclear weapons from operational status, destroy their weapons as soon as possible but not later than a deadline to be determined by the first meeting of States Parties and lastly, eliminate their entire nuclear-weapons program, including the elimination or irreversible conversion of all nuclear-weapons-related facilities.\nThat sounds like some pretty stringent safeguards and disarmament provisions to me. But, of course, without participation by the nuclear-armed states it was not realistic to negotiate the finer verification details just yet.\nSome say the TPNW fails to make up for the weaker aspects of the Nuclear Non-proliferation Treaty (NPT) such as its failure to prohibit its parties from preparing to produce nuclear weapons including the transfer of parts, materials and technology. But it seems to me, the TPNW goes a long way towards doing just that.\nFor example parties to the TPNW must not develop, test, produce, manufacture, otherwise acquire, possess or stockpile nuclear weapons or other nuclear explosive devices; they must not receive the transfer of or control over nuclear weapons or other nuclear explosive devices directly or indirectly; and they must not seek or receive any assistance in any way, from anyone to engage in any activity prohibited to a State Party under the Treaty.\nOf course, then there is the predictable concern about upsetting the strategic balance by undermining the legitimacy of the nuclear deterrence doctrine that adherents claim has kept us safe for more than seven decades.\nBut we are not safe. We never can be while the spectre of nuclear annihilation hangs over the head of every living being on the planet.\nNot one of the nuclear powers has taken meaningful steps to phase out nuclear weapons with the aim to ultimately eliminate them altogether as required by the Non Proliferation Treaty (NPT).\nNATO member states, particularly those hosting nuclear weapons and states like Australia, under the “west’s” nuclear umbrella, have also shown little interest in nuclear disarmament.\nThe deterrence doctrine is nothing but an excuse not to disarm. Disarmament will simply not happen while nuclear weapons are considered acceptable, useful and legitimate.\nThe Treaty on the Prohibition of Nuclear Weapons (TPNW) seeks to change that. Will it really make a difference? Will it have any affect on international law?\nOnly time will tell but one thing is certain: the longer nuclear weapons continue to exist, the greater the chance that we won’t.\nIt is not deterrence that has saved us so far, just incredibly good luck.","The Nuclear Non-Proliferation Treaty (NPT) recognizes only five “nuclear states” – the U.S., UK, France, Russia and China. Since 1967, India, Pakistan, North Korea and Israel have also developed nuclear weapons, almost doubling the size of the group. South Africa stands alone as the only country to have developed atomic weapons and then give them up. While the number of nuclear states has increased, there has been a slow, steady reduction of the number of nuclear warheads in the world to between 10,000 and 25,000, with, according to some, 90% of nuclear warheads produced since 1945 now out of service. Nonetheless, there are still more than enough serviceable warheads to destroy human civilization as we know it. We live in a world with an increasing number of nuclear states, but the picture is not as bleak as it may seem. President John F. Kennedy predicted that by 1964 the world would see as many as 10 to 20 nuclear powers. There is not that many today over 50 years later. World leaders have shown renewed interest in nuclear disarmament since 2007. With a practical view of nuclear disarmament, proper control, verification and enforcement mechanisms, and the necessary political will, it may be possible to achieve a nuclear weapons-free world. However, would such a world actually be any more secure?\nAbolition v. Disarmament\nMichael O’Hanlon points out an important difference in vocabulary in what one means by “a world without nuclear weapons.” “Abolishing” nuclear weapons means not only dismantling all existing weapons, but outlawing their testing, use, reconstruction, development, proliferation and make them wholly illegal in all circumstances. This differs from “disarmament”; dismantling or demilitarizing all currently existing nuclear arsenals in the world and agreeing an international framework to monitor progress and fissile materials, verify disarmament and mediate disputes, but not totally outlawing them forever in all circumstances. Both would constitute a “nuclear weapons-free world”, but disarmament is more realistic and achievable.\nThe NPT attempts to prevent nuclear proliferation and elicits an agreement in Article VI from nuclear states to cease the nuclear arms race, dismantle their nuclear weapons and agree a treaty leading to “general and complete disarmament” under international control – something they have yet to do. Article I stops nuclear states from transferring nuclear weapons technology to non-nuclear states while Article II prevents signatories which did not already possess nuclear weapons from obtaining them. States which have developed atomic weapons after the NPT have done so as non-members of the treaty. The NPT does not seek to abolish nuclear technology in the world. It recognises at Article I the right of nuclear states – America, Britain, France, China and Russia – to possess them. NPT Article IV in fact recognizes and encourages the development and sharing of the “inalienable right” to peaceful nuclear technology among nuclear and non-nuclear states – something often forgotten in debates about the nuclear ambitions of states such as Iran.\nIndividuals and groups who call for the abolition of nuclear weapons stand on firm moral ground. However, groups such as Greenpeace and iCan focus on campaigning to motivate individuals at grassroots level to apply pressure on officials to abolish atomic weapons, which, while also an important part of the effort, does not address the perennial underlying political and security issues which cause states to cling to or seek to acquire nuclear weapons. These groups have been critical of the 1970 NPT as legitimizing a system of nuclear “haves and have-nots” by giving special recognition to the five original nuclear states charge it has been ineffective in applying pressure to disarm and point to a complete ban on nuclear weapons along the same path as treaties for landmines and biological weapons as a better goal. Nonetheless, even some critical of the NPT and piece-meal bilateral agreements acknowledge that forever “dis-inventing” nuclear weapons is not possible and that if one state reconstitutes a nuclear program, there may be no alternative but for others to do so to confront that threat.\nControl, Verification and “Virtual Deterrence”\nIn considering whether a nuclear weapons-free world is possible it must also be considered how any nuclear disarmament agreement, once reached, would be verified, how fissile materials would be controlled and how noncompliance would be confronted. An agreement without such teeth would be doomed from the start. Tracking and controlling nuclear material is the only way to ensure new atomic weapons cannot be developed. However, environmental, market and economic pressures have brought about the need for energy from a clean, reliable source such as nuclear power. The IAEA itself has called for 1,400 new nuclear reactors to be built worldwide by 2050 to meet world energy demand.\nCounterintuitively, none of the nine states known to possess nuclear weapons is currently subject to IAEA inspections. The five NPT nuclear states are exempt and four non-NPT states – India, Israel, Pakistan, North Korea – are not subject to them. This means the overwhelming bulk of nuclear weapons and material in the world is only tracked by these governments. We all have to take their word for it. Any comprehensive treaty would have to include control, verification and inspection measures by an organization, such as the IAEA, which apply to all countries equally. The IAEA, “should be given the authority to inspect any facility, at any time, and anywhere on the territory of every signature state.”\nNuclear weapons technology cannot be dis-invented and the threat will always exist. Nuclear disarmament is possible; abolition of nuclear weapons forever is not\nEven if all nuclear weapons are demilitarized, fissile material closely controlled and an inspection regime instituted, there will always be the chance that one or more states will reconstitute nuclear weapons at some point in the future. The first steps could be similar to those taken today against states such as North Korea and Iran – sanctions, negotiations and the thinly veiled threat of military action. However, such a process may take too long to stop or do nothing to prevent the reconstitution of nuclear weapons. Allowing it to be subject to a UN Security Council veto could further complicate matters. The fear a foe would secretly maintain nuclear weapons or rebuild them is an obstacle to convincing current nuclear-armed states to agree to give them up.\nThe concept of “virtual deterrence” has been put forward both to calm the security fears of current nuclear states regarding disarmament and as an enforcement and deterrence mechanism against rogue states that would start or reconstitute a nuclear weapons programme after achieving nuclear disarmament. Briefly, the idea is that current nuclear states, such as the P5, could maintain the threshold ability to reconstitute their nuclear programmes in a matter of months in such circumstances in order to confront the threat. Nuclear abolitionists would object, of course. However, nuclear weapons technology cannot be dis-invented and the threat will always exist. Nuclear disarmament is possible; abolition of nuclear weapons forever is not.\nIn 2007, U.S. senior policy leaders Henry Kissinger, Sam Nunn, William Perry and George Shultz breathed new life into the movement toward “Global Zero” beginning with a series of opinion pieces in major American newspapers as part of the Nuclear Security Project. They succeeded in making nuclear disarmament an issue in the 2008 U.S. presidential campaign, supported by both then-Senator Barack Obama and Senator John McCain.\nAs President, Barack Obama named nuclear disarmament one of the priorities of his administration and promised in his 2009 Prague Speech that America would take “concrete steps toward a world without nuclear weapons.” In October 2009, President Obama was awarded the Nobel Peace Prize for his commitment to eliminating nuclear weapons. In April 2010, the U.S. Department of Defense released a Nuclear Posture Review (NPR) in which it reiterated America’s commitment to the elimination of nuclear weapons, committed to “reducing the role of U.S. nuclear weapons in U.S. national security strategy” and to “maintaining strategic deterrence and stability at reduced nuclear force levels” as key priorities. In April 2010, President Obama also signed New START with Russian President Dmitry Medvedev, reducing and limiting both nuclear arsenals to 1,550 warheads and between 700-800 delivery platforms. In May 2010 the 5-yearly NPT review conference was held in New York. The final document included recommitment of members to the NPT, specific action-plans regarding nuclear disarmament and proposed steps for creating a “WMD-free” zone in the Middle East. The U.S. National Security Strategy, released in May 2010, also reiterated U.S. commitment to ‘pursue the goal of a world without nuclear weapons’ and strengthening the NPT.\nThese events of the past nine years have shown a renewed commitment on behalf of most of the major nuclear states to nuclear disarmament, but it has not all been easy or good news. The U.S. and Russia did sign New START in 2010, though it was only just barely ratified by Congress in 2011 over objections by hawkish Republican Senators, such as John McCain who had had claimed to support nuclear disarmament in his 2008 presidential campaign. After agreeing a 2008 deal with the P5+1 to end its nuclear program, North Korea destroyed the cooling tower of its nuclear facility at Yongbyon. However, it appears to have restarted work at the facility in recent years and last month the North claimed to have tested a hydrogen bomb, though this is disputed by several intelligence agencies.\nThe political will is there, but the attention and timing must also be there and they all must coincide in order for there to be any realistic chance of achieving a world free of nuclear weapons\nThe renewed push for nuclear disarmament since 2007 has been overtaken by other domestic and international events in key states. The 2008 economic downturn, economic crises in EU states, government austerity policies in the UK, fights over tax and fiscal policies and domestic political issues such as Obamacare, sequestration and the debt ceiling, as well as international crises such as the Russian invasion of Ukraine and the Syrian Civil War and ensuing refugee crisis continue to absorb much of the world’s political energy and capital. Since 2008 there have been changes in national leadership in the U.S., UK, France, Russia, China, North Korea, India, Pakistan and Israel – every nuclear state.\nThe 2012 conference called for in the 2010 NPT review to devise a WMD-free zone in the Middle Eastwas canceled as it was unclear whether Israel, having just begun a military incursion into Gaza, or Iran would fully participate and Arab states were angered by further delays in a discussion which began 15 years before at the 1995 NPT review. The reinvigorated campaign for nuclear disarmament shows that the political will to push forward the agenda for a nuclear-free world exists; however, it is also constantly being stretched to the limit and attention is divided by other world events. Timing is crucial on this issue and real progress may not be possible until some of the world’s other major problems which often cause security concerns which complicate the nuclear disarmament issue are resolved first. As President Obama acknowledged in his 2009 Prague Speech, “This goal will not be reached quickly – perhaps not in my lifetime. It will take patience and persistence. But we, too, must ignore the voices who tell us the world cannot change.” The political will is there, but the attention and timing must also be there and they all must coincide in order for there to be any realistic chance of achieving a world free of nuclear weapons.\nBut Do We Really Want a World Free of Nuclear Weapons?\nThough the accepted dilemma seems to be how to achieve a world free of nuclear weapons and whether it is possible, there is not much in-depth discussion as to whether a nuclear weapons-free world would provide security and peace as asserted. Arguably the most famous analysis of this question is the debate in book form between Scott Sagan and the late Ken Waltz.\nBriefly, as a Realist, Waltz believed that the world consists of states in global anarchy which must find ways to create security for themselves through the two separate ideas of defence – the ability to repel an attack – and deterrence – the ability to inflict enough punishment on an opponent to create a disincentive for them to attack. Simply put, possession of nuclear weapons greatly bolsters a state’s ability to defend and deter and when two nuclear states stand opposed to one another, these weapons have an equalizing effect which causes them to proceed in a much more considered manner because of what they risk in a nuclear exchange and the knowledge that the other side, rationally, must be proceeding in the same manner creates a more secure condition. He asserts nuclear weapons allow states to meet the need to provide their own security and secure states do not fight wars. When confronted with the question if Global Zero would be better, Waltz answers that, “Abolishing the weapons that have caused sixty-seven years of peace would certainly have effects. Such an action would, among other things, make the world safe for the fighting of World War III.” Since what Robert Jervis termed the “Nuclear Revolution” of 1945, there certainly has been no large-scale conventional war between great powers.\nAlso briefly, Sagan responds to Waltz by arguing that nuclear weapons make the world less secure because they are military objects and, institutionally, military organizations are concerned with achieving “military victory” by defeating opponents in war. Because nuclear weapons provide unmatched potential to do that, military organizations will always seek to obtain advantage by building more nuclear weapons and will, as all institutions do, seek ever more resources to do so. Many states do not have sufficient civilian control over their military apparatus to counter these tendencies. More states will seek nuclear weapons and states that already possess them will build more, the end state being that they will not provide security for anyone, just more nuclear weapons. This creates a dangerous spiral. He also shows empirically that political and military leaders cannot always be counted on to act rationally in the severe emotional circumstances high-level conflict creates. On the question of Global Zero, Sagan answers that the need for nuclear deterrence ended with the Cold War and the threat of loose nuclear weapons falling into the hands of terrorist organizations is greater because they do not fear nuclear retaliation, making the need to reach zero a pressing modern issue.\nAs the title of their book suggests, this is An Enduring Debate. The world was a violent place before nuclear weapons and will still be one if ever we are without them, as Sagan recognizes, and still be fraught with conflict and the state quest for security, as Waltz asserts. Unless and until the world arrives at that point, there can be no answer.\n[Photo: Flickr CC: Freedom II Andres]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:ed1140dd-2df2-4f64-b99a-ed7874dbb398>","<urn:uuid:f9a855bd-cccb-425d-b42b-24d5bb7ed381>"],"error":null}