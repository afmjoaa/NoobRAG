{"question":"Could you help me understand the compositional purity between cave pearls from Gruta de las Canicas and Hawk's-eye gemstones?","answer":"The cave pearls from Gruta de las Canicas are composed of extremely pure low-Mg calcite, containing 99.4-99.8% CaCO3 with only minor substitutions of Mg, Mn, Fe, and Sr. In comparison, Hawk's-eye consists of crocidolite fibers (asbestiform riebeckite) that have been permeated or partially replaced by microcrystalline quartz. The cave pearls show a more chemically pure composition focused on calcium carbonate, while Hawk's-eye has a more complex mineralogical makeup combining both silicate minerals (crocidolite and quartz).","context":["|2008 Joint Meeting of The Geological Society of America, Soil Science Society of America, American Society of Agronomy, Crop Science Society of America, Gulf Coast Association of Geological Societies with the Gulf Coast Section of SEPM|\n|Paper No. 304-3|\n|Presentation Time: 8:00 AM-6:00 PM|\nPetrology and Chemistry of Cave Pearls from Gruta De Las Canicas (Cave of the Marbles), Tabasco, Mexico\nHOUSTON, Shari, MOZLEY, Peter S., CAMPBELL, Andrew R., and BOSTON, Penny, Earth and Environmental Science, New Mexico Tech, Socorro, NM 87801, firstname.lastname@example.org|\nCave pearls are relatively common in caves, but are typically present in very low abundance. Gruta de las Canicas, a cave system developed in Cretaceous carbonates in Tabasco, Mexico, is highly unusual in that it contains a tremendous quantity of pearls (estimated 200 million pearls found on the cave floor to a depth of a meter or more). The mechanism for the formation of this vast quantity of pearls has not been determined. Here we provide the first report of the mineralogy, texture and chemistry of the Canicas pearls. The pearls were studied using standard optical petrographic and geochemical methods (electron microprobe and stable isotopes). The pearls can be subdivided into three distinct zones based upon cement texture, presence of impurities, and porosity. Clay-rich zones are enriched in clay-sized non-carbonate material, and typically contain little to no porosity. Cement-rich, non-porous zones contain mainly radially oriented spar, with lesser amounts of microspar, and also have little to no porosity. Cement-rich porous zones are similar to the non-porous zones, but contain up to 50% porosity, with the pores elongated in a radial manner parallel to the radial spar crystals. The three zone types alternate concentrically in an apparently random manner. Electron microprobe analysis indicates that the carbonate is extremely pure low-Mg calcite (99.4 – 99.8 % CaCO3), with only minor substitution of Mg, and to a lesser extend Mn, Fe, and Sr for Ca in the calcite structure. A microprobe traverse detected no statistically significant variation in elemental composition from pearl center to edge. Microprobe analysis of the non-carbonate clay sized material in the pearls shows that a variety of minerals are present, including quartz and apatite. The abundance and distribution of porosity suggests that a more soluble phase (e.g., aragonite, organic matter) was removed by dissolution.\n2008 Joint Meeting of The Geological Society of America, Soil Science Society of America, American Society of Agronomy, Crop Science Society of America, Gulf Coast Association of Geological Societies with the Gulf Coast Section of SEPM\nGeneral Information for this Meeting\n|Session No. 304--Booth# 120|\nSediments, Carbonates / Clastic (Posters)\nGeorge R. Brown Convention Center: Exhibit Hall E\n8:00 AM-6:00 PM, Wednesday, 8 October 2008\nGeological Society of America Abstracts with Programs, Vol. 40, No. 6, p. 479\n© Copyright 2008 The Geological Society of America (GSA), all rights reserved. Permission is hereby granted to the author(s) of this abstract to reproduce and distribute it freely, for noncommercial purposes. Permission is hereby granted to any individual scientist to download a single copy of this electronic file and reproduce up to 20 paper copies for noncommercial purposes advancing science and education, including classroom use, providing all reproductions include the complete content shown here, including the author information. All other forms of reproduction and/or transmittal are prohibited without written permission from GSA Copyright Permissions.","TIGER'S-EYE and HAWK'S-EYE\nB. Hawk's-eye \"teardrop \" (1.9 x 3.3\ncm) from South Africa. Being a fan of Northern Lights (aurora\nI especially like this photograph of this stone. (© photo\nA. Tiger's-eye round cabochon (diameter - 3.7 cm). (© photo courtesy www.mysticmerchant.com)\nC. \"Tricolor\" cabochon (3.76 x 2.8 cm) that exhibits both Tiger's-eye and Hawk's-eye, as well as reddish zones. Rough from Cape Province, South Africa. (© photo courtesy topgems.homestead.com)\nD. Pietersite (height - ~ 5.1 cm) from from Namibia (formerly South-West Africa). (© photo courtesy topgems.homestead.com )\nE. Fiber optics sphere (diameter - 5\ncm). A possible simulant for hawk's-eye. (© photo\nHawk's-eye consists of closely packed\nparallel crocidolite fibers that have been permeated and/or partially\nreplaced by virtually colorless microcrystalline quartz.\nthe name applied widely\nto asbestiform (i.e., fibrous)\nriebeckite, which is an amphibole.\nColor - diverse blue hues, commonly grayish, less commonly greenish, typically striped\nH. crocidolite - 6; quartz - 7\nTiger's-eye consists largely of\nmicrocrystalline quartz pseudomorphs after hawk's-eye. In\ntiger's-eye, however, the precursor\ncrocidolite fibers have been altered (chiefly oxidized) to\nhydrous iron oxide (\"limonite\"), and at least some of\nthe so-called fibers are cavities or cavity fillings in some specimens. In\naddition, the pseudomorphism of some specimens has been shown to have\nonly partial replacement.\nColor - various shades of golden or honey yellow and various brown hues, typically roughly striped\nH. (effective hardness) 6.5 - 7\nS.G. ~ 2.65 - 2.9.\nThe following names include terms that are no longer used or unfortunately applied (luckily not widely), as well as terms that have rather good standing, at least in the marketplace.\nUSES: Both tiger's-eye and hawk's-eye are used in jewelry -- usually as cabochons, but also as small spheres, diverse prisms (some with rounded edges), faceted forms, small carvings (scarabs, cameos, intaglios, etc.), and even tumbled chips. They also have been carved and fashioned into articles such as boxes, cane heads, eggs, hearts, obelisks, pyramids, snuff bottles, spheres and diverse figures. In addition, especially in the past, they were also used as eyes -- i.e., set into the eye sockets -- of carvings fashioned from other gemrocks, shell and wood.\nOCCURRENCES: As already noted, hawk's-eye is asbestiform crocidolite that has been so-to-speak permeated or partially replaced by silica. The precursor crocidolite asbestos -- at least that at the well-known tiger's-eye locality, west of Griquatown, South Africa -- consisted of fibers ranging up to a few centimeters long and only 0.001+ mm in diameter (Bauer and Schlossmacher, 1932, 673.). According to a summary given by Deer, Howie and Zussman (1992), tiger's-eye -- and, I suspect they also would include hawk's-eye -- occurs \"in iron formations of South Africa and western Australia where it occurs in seams conformable with the bedding of the ironstone. The composition of the crocidolite is closely comparable with that of the ironstone . . . crystallization of the amphibole, initially in the form of massive riebeckite, occurred with little or no addition of material and under conditions of moderate temperature and pressure consequent on the burial of the ironstones to moderate depths. . . . transformation of the riebeckite to the fibrous crocidolite may [have] result[ed] from the instability of the massive riebeckite during a period when the ironstones were subjected to shearing stress.\" Amstutz and Hälbich Stellenbosch (1992), however, have concluded that the \"pseudomorphism of tiger's eye is of near-surface origin...[and may] be considered a specific type of silcrete formation.\" (See also Sinclair, 1941) And, more recently, Heaney and Fisher (2003/2004) have concluded that the long and widely held idea that these gemrocks are quartz pseudomorphs after asbestiform crocidolite is incorrect; they present data derived from their investigation of specimens that they interpret to represent synchronous mineral growth of chiefly quartz and crocidolite as a consequence of crack-sealing, vein-filling processes. Whatever the correct origin(s?), at least in South Africa, where the gemrock material comprises relatively thick slabs, tiger's-eye appears to have been formed from hawk's-eye as the result of the breakdown -- chiefly oxidation -- of crocidolite, with preservation of the original fibrous nature of the overall mass. Furthermore, it seems worth repeating here that \"In some tiger-eye the crocidolite has been wholly destroyed, leaving hollow tubes that may be partly filled with hydrated iron oxide\" (Frondel, 1962).\nNOTEWORTHY LOCALITIES: The most important source of both tiger's-eye and hawk's-eye is South Africa. Commercial quantities have also been recovered from India; Myanmar (formerly Burma); Sri Lanka (formerly Ceylon); the Hamersley range of the Pilbara region and Mt. Brockman, West Australia; Brazil; and California. Pietersite, as mentioned, comes from Namibia (formerly South-West Africa), and from Nangang, Hunan Province, China. Localities for some of the varieties that have been given special names are given under the OTHER NAMES subheading.\nREMARKS: The names given these gemrocks were apparently based on their chatoyancy, which roughly resembles the eyes of tigers and hawks. I have been unable to date to find the first application of these terms to these gemrocks.\nAlthough it is generally thought that these gemrocks are only rarely enhanced, Anderson (1980) reports that \"grey varieties, and paler types . . . have been stained in a number of unlikely colours\"; Webster (1986, p.89) notes, “It appears that many pseudocrocidolite cabochons are stained –vivid greens, blues and blacks, which are obviously stained, have been seen. This staining may be carried out in a similar manner to that used for staining agates.”; [and] as already noted under the OTHER NAMES subheading, Hart (1927) lists the name \"Harlequin stone\" as a name applied to \"artificially colored crocidolite.\" Also, as already mentioned, some tiger's-eye has been heated to produce deep red and brownish-red tiger's-eye; a procedure is outlined on the web site topgems.homestead.com . And, some hawk's-eye and tiger's-eye have been acid treated to produce what is marketed as \"grey tiger eye.\" In addition, according to information given on the web site www.minerals-n-more.com: \"Soon after Tiger Eye's discovery in the late 19th century [this date is, by the way, not correct!], Idar-Oberstein lapidaries discovered they could bleach tiger-eye to an evenly colored light yellow . . . By using either hydrochloric or oxalic acid. [And,] When properly oriented and cut, this material could yield a sharp cat's-eye stone . . . reminiscent of 'real' cat's-eye.\" It is also reported (topgems.homestead.com) that open spaces in Pietersite and bighamite stones have sometimes been \"filled with wax, super glue or opticon in the last steps of sanding and polishing.\"\nSome jewelers and other marketers recommend cleaning these gemrocks with only a polishing cloth -- i.e., they say these gemrocks should not be submitted to such things as alcohol, steam, or an ultrasonic gem cleaner. Furthermore, they frequently (prudently!!) direct attention to the fact that when struck, these stones are more likely than many other gem materials to chip or break.Several praiseworthy virtues have been attributed to these stones. Although I have made a point not to give the purported attributes for most gemrocks, a few are given here as an example of the types of things that one can find recorded for just about all gemstones (both gem minerals and gemrocks): So far as physical and mental health, tiger's-eye is said to be \"a good general tonic [for] hypochondria, eye diseases, stubbornness, spleen, pancreas, digestive organs, blood purification, unwanted emotion\"; to \"be an aid to the treatment of ulcers. . . . [and] to improve night vision.\" (www.crystalbeauty.co.nz) Elsewhere it is noted, for example, that \"Tiger's eye is closely attuned to earth energies, but the shimmering, eye-catching yellow highlights are linked to the sun. Thus tiger's eye is a bridge between earth and sky, a link or tool representing a balance between the physical and the spiritual. [-- that is,] Its energies promote psychic ability by connecting the physical earth aspects of self with the non-physical spiritual aspects. . . . [Also,] As a link between Father Sky and Mother Earth, tige'sr [sic] eye can help balance the male and female energies that are within us all. [Indeed,] This stone's influence is one of harmony between yin and yang., and it's energies can help us to achieve this same balance (www.crystalbeauty.co.nz). In addition, \"Tiger's Eye . . . helps us recognize the resources within ourselves and [how to] use those resources for the attainment of our dreams. [And,] It helps us judge a situation and determine how best to approach it.\" (??). Furthermore, \"Tiger Eye can help you become practical as well as more grounded. It has been used to stimulate wealth and enhance the stability to maintain wealth.\" (www.blacktassel.com). Indeed, I have even been told that it may even be programmed (how was not revealed) to help one see different possibilities before acting. etc., etc. -- These, in my opinion, are examples of fabricated marketing ploys, but I have been cautioned \"who are you to make such an assumption(?). However, is it not interesting that even the recently discovered Pietersite has been assigned such attributes: For example, it is \"said to contain the 'keys to the kingdom of heaven,' dispelling illusion and assisting one in the recognition of the beauty of the soul. . . .[; to] exhibit[s] an energy conducive to the actualization of the loving characteristics of the \"brotherhood\" of humanity. ...[; to] bring[s] the potential of the individual to the perfection of the source of all being, stimulating dignified power and loving guidance. . . . [; and to] promote[s] loyalty to the self and to the ultimate experience of life. -- [And, it ] can be used to stimulate the pituitary gland to provide the proper regulation of the other endocrine glands and to produce, in the proper quantity, the hormones concerned with growth, sex, metabolism, blood pressure, and body temperature.\" (www.vjdesigns.com). Yikes!!! Also, for what it may mean or be worth, \"Tiger's Eye is a warm stone which is recognised as slightly masculine... [and] a good stone for those born during The Moon of Growth [i.e., under the zodiac sign of Taurus] - (20 Apr - 20 May).\" (www.crystalbeauty.co.nz).\nIn some circles, tiger's-eye is considered to be the gemstone for the ninth wedding anniversary.\nArizona tiger-eye - asbestiform, translucent, amber-colored\nserpentine that exhibits a fine chatoyance. The name is why\nI include it here -- I do not know of its ever been marketed as\ntiger's-eye per se. - [\nCalifornia tiger's eye - \"silica-impregnated, white-to buff-colored, massive fibrous tremolite ... [from] Iowa Hill in Placer County. It is sometimes called Placer County tigereye. ... It has a chatoyance similar to silkstone. (topgems.homestead.com) In the same way as the above listed simulant, the name is the reason I include it here -- I do not know of its ever been marketed as tiger's-eye per se. - [Appearance suffices.].\nFiber Optics - certain shades of blue and golden brown fiber optics fashioned as cabochons used in costume jewelry and for curios -- e.g., spheres marketed as scrying balls (i.e., so-called “crystal” or future-predicting balls) -- roughly resemble hawk's-eye and tiger's-eye, respectively. - [So far as I have been able to determine, the fiber-optics used for these purposes consist of strands of glass and/or plastic; consequently, their overall hardnesses are less than the effective hardness of either hawk's-eye or tiger's-eye; in addition, to me they have an overall artificial appearance (see Figure E).].\n\"Fire jade - looks like tiger's eye, mainly opal\"\n- the preceding \"bald statement\" appears on\nwww.gemscape.com/html/misnomer.htm. I have found no additional\ninformation about this material.\n***Forsterite (Mg-olivine) - this synthetic olivine has been sold as \"rodusite\" (Federov, 2002). - [said to exhibit melt features].\nasbestiform serpentine veins within massive serpentine(?) has been\nmarketed as pietersite (or Arizona pietersite). [resemblance is not\nthat good; fibers of asbestiform veins are virtually\nperpendicular to their marginal surfaces; S.G. is 2.50-2.58 versus the\n2.67-2.74 of true pietersite (Hu and Heaney, 2010) ].\n***Tiger's eye soaprock® - This glycerin soap consists of layers of browns, golden yellows and dark gray, the thicknesses and arrangements of which roughly resemble tiger's-eye (www.artisticdelights.com/tigerseye). - [Appearance suffices to distinguish it from tiger's-eye.].\n1986. For Pietersite: Hu and Heaney, 2010 & 2010a.\n| Top | Home |\nR. V. Dietrich © 2014\nLast update: 20 January 2012\nweb page created by Emmett Mason"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:8c0cbcd7-ccaa-43e4-a40c-58452804c616>","<urn:uuid:1d0ece36-a167-49a4-a6f2-25d14f8d4174>"],"error":null}
{"question":"¿Cuál es la diferencia entre lugares dependientes e independientes en sistemas de direcciones, y qué restricciones aplican a la edición de números de casa?","answer":"In Nominatim, dependent places (rank 30) include house numbers and POIs that don't have their own full address but are attached to a parent street or place. Independent places include streets, parks, water bodies, suburbs, and cities, which receive full addresses of their own. For editing house numbers in Waze Map Editor, there are several restrictions: they can only be edited at 100m/200ft zoom level or lower, cannot be added to unnamed segments, must contain numbers (though can include letters but cannot start with them), and cannot be edited when there are unsaved changes. Additionally, segments cannot have their names removed if house numbers are attached to them.","context":["In Nominatim, the word indexing refers to the process that takes the raw OpenStreetMap data from the place table, enriches it with address information and creates the search indexes. This section explains the basic data flow.\nAfter osm2pgsql has loaded the raw OSM data into the place table, the data is copied to the final search tables placex and location_property_osmline. While they are copied, some basic properties are added:\n- country_code, geometry_sector and partition\n- initial search and address rank\nIn addition the column\nindexed_status is set to\n1 marking the place as one\nthat needs to be indexed.\nAll this happens in the triggers\nThe main work horse of the data import is the indexing step, where Nominatim takes every place from the placex and location_property_osmline tables where the indexed_status != 0 and computes the search terms and the address parts of the place.\nThe indexing happens in three major steps:\nData preparation - The indexer gets the data for the place to be indexed from the database.\nSearch name processing - The prepared data is given to the tokenizer which computes the search terms from the names and potentially other information.\nAddress processing - The indexer then hands the prepared data and the tokenizer information back to the database via an\nINSERTstatement which also sets the indexed_status to\n0. This triggers the update triggers\nosmline_updatewhich do the work of computing address parts and filling all the search tables.\nWhen computing the address terms of a place, Nominatim relies on the processed search names of all the address parts. That is why places are processed in rank order, from smallest rank to largest. To ensure correct handling of linked place nodes, administrative boundaries are processed before all other places.\nApart from these restrictions, each place can be indexed independently from the others. This allows a large degree of parallelization during the indexing. It also means that the indexing process can be interrupted at any time and will simply pick up where it left of when restarted.\nThe data preparation step computes and retrieves all data for a place that might be needed for the next step of processing the search name. That includes\n- location information (country code)\n- place classification (class, type, ranks)\n- names (including names of linked places)\n- address information (\nData preparation is implemented in pl/PgSQL mostly in the functions\naddr:* tag inheritance\nNominatim has limited support for inheriting address tags from a building to POIs inside the building. This only works when the address tags are on the building outline. Any rank 30 object inside such a building or on its outline inherits all address tags when it does not have any address tags of its own.\nThe inheritance is computed in the data preparation step.\nSearch name processing\nThe prepared place information is handed to the tokenizer next. This is a Python module responsible for processing the names from both name and address terms and building up the word index from them. The process is explained in more detail in the Tokenizer chapter.\nFinally, the preprocessed place information and the results of the search name processing are written back to the database. At this point the update trigger of the placex/location_property_osmline tables take over and fill all the dependent tables. This makes up the most work-intensive part of the indexing.\nNominatim distinguishes between dependent and independent places. Dependent places are all places on rank 30: house numbers, POIs etc. These places don't have a full address of their own. Instead they are attached to a parent street or place and use the information of the parent for searching and displaying information. Everything else are independent places: streets, parks, water bodies, suburbs, cities, states etc. They receive a full address on their own.\nThe address processing for both types of places is very different.\nTo compute the address of an independent place Nominatim searches for all\nplaces that cover the place to compute the address for at least partially.\nFor places with an area, that area is used to check for coverage. For place\nnodes an artificial square area is computed according to the rank of\nthe place. The lower the rank the lager the area. The\ntables are there to facilitate the lookup. All places that can function as\nthe address of another place are saved in those tables.\nisin:* tags are taken into account to compute the address, too.\nNominatim will give preference to places with the same name as in these tags\nwhen looking for places in the vicinity. If there are no matching place names\nat all, then the tags are at least added to the search index. That means that\nthe names will not be shown in the result as the 'address' of the place, but\nsearching by them still works.\nIndependent places are always added to the global search index\nDependent places skip the full address computation for performance reasons. Instead they just find a parent place to attach themselves to.\nBy default a POI or house number will be attached to the closest street. That can be any major or minor street indexed by Nominatim. In the default configuration that means that it can attach itself to a footway but only when it has a name.\nWhen the dependent place has an\naddr:street tag, then Nominatim will first\ntry to find a street with the same name before falling back to the closest\nThere are also addresses in OSM, where the housenumber does not belong\nto a street at all. These have an\naddr:place tag. For these places, Nominatim\ntries to find a place with the given name in the indexed places with an\naddress rank between 16 and 25. If none is found, then the dependent place\nis attached to the closest place in that category and the addr:place name is\nadded as unlisted place, which indicates to Nominatim that it needs to add\nit to the address output, no matter what. This special case is necessary to\ncover addresses that don't really refer to an existing object.\nWhen an address has both the\naddr:place tag, then Nominatim\nassumes that the\naddr:place tag in fact should be the city part of the address\nand give the POI the usual street number address.\nDependent places are only added to the global search index\nthey have either a name themselves or when they have address tags that are not\ncovered by the places that make up their address. The latter ensures that\naddresses are always searchable by those address tags.","House Numbers in WME\nThis page covers editing house numbers in Waze Map Editor.\n- 1 Overview\n- 2 Things to remember (caveats/warnings)\n- 3 New roads must have House Numbers added\n- 4 Activating the interface\n- 5 Using the interface\n- 6 Errors\n- 7 House Number Tips\n- 8 Remember to save\nWaze keeps its own internal database of addresses, or what is called House Numbers. These addresses are used only when a specific address is searched from the Waze Map Editor or Waze client app. House Numbers are independent from any other address or mapping provider.\nHouse numbers are points on the map and can be adjusted at a fine level of detail to ensure Wazers are properly directed to the correct location for that address. It is therefore important to pay attention to all details regarding house numbers to ensure proper routing.\nThings to remember (caveats/warnings)\n- The House numbers interface cannot be activated when there are unsaved edits.\n- House numbers are editable only at the 100m/200ft zoom level and lower.\n- House numbers cannot be added to segments without a name.\n- A segment cannot have its name removed if there are house numbers attached to it.\n- House numbers can contain numbers and letters (But cannot start with a letter).\n- The Waze app address search will only use the Waze house number if it has been created in the editor, or has been modified (moved).\n- The Waze app does NOT automatically update position data for already-obtained destinations! Even after an address has been adjusted using WME and/or Google MapMaker, the app will retain the stale position data indefinitely and navigation will continue to fail. To eliminate stale position data, ask the reporter to remove any existing instances of the destination in the recents and favorites lists and then search for the destination again from scratch.\n- If an address is on an unnamed road (private, parking lot, etc.) and the marker is a long way from the actual house location, delete the Waze house number and add a private residence Place marker which contains only the address.\n- If the correct approach entrance to an address is from a street or segment other than the street the address is named after, then a Waze House Number for that address should not be used. Instead, add a private residence Place marker which contains only the address.\n- See the FAQ for more information\nNew roads must have House Numbers added\nWaze will not be doing any future imports or merges of external data for house numbers. It is therefore very important that any new roads you add must have house numbers added in order for Waze to provide accurate routing to those addresses.\nActivating the interface\nTo edit, update or delete house numbers on a street, select any segment of the road then click the Edit House Numbers button or tap the H keyboard shortcut.\n- Note: The House Numbers interface cannot be activated when there are unsaved edits or multiple segments selected.\nUsing the interface\nWaze will often select an additional segment or two connected to the segment you selected when they are the same city and name (same Street ID). You can pan the map along that selection to edit house numbers on more than just the single segment you selected.\nThe lighted area around the road is the default distance that house numbers are allowed from the street. If a house number must be located beyond that distance, you will need to Force the house number into place (discussed below).\n- Note: If you are zoomed out farther than 100m/200ft, you cannot edit house numbers, and there will be a message at the top center of the window which reads, \"Zoom in to edit\"\nWhen the house numbers interface is activated, a new toolbar appears with the standard complement of action buttons specific to working with house numbers.\nDescriptions of the house numbers toolbar buttons:\nClick to add a new house number\nClick to save current changes to house numbers\nClick to undo the last house number action or modification\nClick to redo the last house number action or modification\nClick to exit the house numbers interface\nThe two images to the right show what a house number looks like in the editor. Which one is used depends on the density of house numbers and how WME determines the marker should be flipped for best visibility and handling.\n- The red \"X\" is the delete button.\n- The blue dot on the other side is the drag handle\nThe house number itself should be placed the center of the rooftop of the house. For large buildings such as box stores which have a single address, and addresses within shopping malls, it is recommended to NOT add the house number to Waze. These stores are typically within parking lots and the stop point for house numbers is not a good location for navigation to end. For these locations, use a Place area or point.\n- To Edit a house number, click in the number and enter a new number.\n- To Move a house number, click in the number or on the blue drag handle, and then click and drag using the blue drag handle.\n- To Delete a house number, click in the number, and then click the red X.\nTo add a house number, click the Add button on the toolbar or tap h on the keyboard. Click on the location of the house or business address. Enter the house number.\nSetting the Stop Point\nThe Stop Point (sometimes referred to as the Segment Marker) is the location on the segment to which the house number is attached, and this is where the Waze navigation destination will be set in the client app. This is represented by a white dot with a blue outline attached to the street:\nWhen moving a house number or adding one, you will see a dashed white line leading to a white dot on the road you selected. This white dot is the Stop Point for navigation to that house number. It can be moved anywhere on the named segment for that address. It cannot be moved to any other segment including driveways (at this time).\nThe Stop Point should be placed:\n- where you would park if you wanted to park closest to the main entrance to the building or an entrance to the building parking lot.\n- where the driveway intersects the road. For houses with driveway which intersects the same road more than once or two different roads, put the stop point where the mailbox is located\nThink of it this way: \"If you can get the driver to this point and have the app display a directional marker flag and announce the correct side of the road the destination is on, the driver should be able to figure out which building it is.\"\nNote: if you have aerial images turned off, such that the background screen is white, you will not be able to see the dashed line. Because it's white.\n- The Waze development team is working on a way to specify a stop point on a street other than the named segment to which the address is attached. This is important for any house whose driveway may not be on named street, mall location, unnamed long private streets/drives with multiple addresses, or businesses which have an entrance from a parking lot off the named street. At this time, there is no way to guide the drivers within a parking lot.\nThere are two categories of errors related to House Numbers:\n- Errors you can force\n- Errors you cannot force\nErrors you can force\nWhen the error message pops up:\n- Dismiss it by clicking on Continue editing\n- Hover the mouse over the red-bordered House Numbers to review what the errors are\n- Fix the House Numbers which are in error, if they are indeed in error.\nThe following errors can be forced if you are a Level 3 or higher editor:\n- Number too far from segment : House Number is beyond the standard allowed distance from its segment.\n- Number out of sequence : House Number stop point doesn't align with existing house numbers on the segment.\n- Number already exists : House Number already exists somewhere on the same selected segments.\n- Wrong side : House Number doesn't align with other existing ones for odd/even numbering scheme.\nWhen you are fully confident there are no errors with any of your House Numbers after you have reviewed the House Numbers highlighted as being in error, click the Save button and then click the Force button.\nErrors you cannot force\nYou will need to fix the errors in order to save. The following errors cannot be forced:\n- Unsupported number format : House Number contains too many numbers, or has letters or other characters which do not align with the allowed format for the country.\nHouse Number Tips\n- Forcing of House Numbers is currently limited to Level 3 editors and higher.\n- The wrong side and out of sequence errors are often a problem in a cul-de-sac or court where stop points for houses tend to bunch up at the end of the street, and the exact position of the segment must align between the house number sequence or odd/even switch (in those countries which follow this method)\n- Another common out of sequence error occurs on circular roads with multiple segments. A junction node should be placed at the point where the sequence starts as the interface can only \"start\" a sequence at a node. For example, if a sequence of house numbers on a circular road runs from 50 to 250, then a junction node must be located between the stop points of 50 and 250 in order to avoid the need to force.\n- For an address which must be moved beyond the normal accepted distance, when you Add it, it must first be placed within the highlighted area, and once the number is entered, then you can drag it to the correct location and adjust the stop point as necessary.\n- If you have more than one which requires forcing, only one click of Continue editing will force them all. Be sure you are confident that all are correctly located.\n- House Number changes are not enough to force a Tile Update. Physically change something (preferably delete an unnecessary geometry node) so the Tile will update and the changes will publish. Search House Number Node in the forums for supporting info.\nRemember to save\nBefore you tap Esc on the keyboard or click Close, remember to Save your changes."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:4ee83bb9-badd-4966-b004-b1903a140e49>","<urn:uuid:8aabe18b-8ea5-43d7-9ba1-70bb16984631>"],"error":null}
{"question":"¿Cuáles son las principales diferencias en resistencia a la corrosión entre el acero inoxidable tipo 316 y las válvulas de Monel en ambientes con cloro?","answer":"Both materials have distinct corrosion resistance properties. Type 316 stainless steel shows excellent resistance to various corrosive environments, particularly in handling hot organic and fatty acids, and can handle waters with up to 2000 ppm chloride. However, it is susceptible to chloride stress cracking. Monel, while performing well in perfectly dry chlorine service, rapidly corrodes when exposed to any amount of moisture. When moisture contamination occurs, Monel surfaces develop problematic dendrite formations (crystal-like masses) that can damage valve seats and cause binding of moving parts. Due to these limitations, HastelloyC has become the preferred material over Monel for valve disc and stem construction in chlorine service applications.","context":["Dry chlorine gas is found within chlorine production, storage and transfer facilities, as well as in downstream processes such as production of vinyl chloride monomer (used in making polymer polyvinyl chloride) or phosgene (used in applications such as pharmaceuticals and organic compounds). Dry chlorine is generally understood to have less than 150 parts per million (PPM) water containment.1 Moisture contamination in any of these dry chlorine service areas, however, can result in the formation of dendrites, crystal-like masses that can cause problems.\nFor example, research shows that dendrite formation on critical sealing surfaces, such as valve seats and packings exposed to extremely harsh operating conditions, leads to a significant increase in potential leak paths because of erosion of seal surfaces that come into contact with the dendrites. Dendrites can also form on contacting or sliding surfaces, causing valves to freeze or to become abrasive to the soft seat, which can lead to severe seat damage. In fact, any amount of moisture can lead to dendrite formation and rapid corrosion of moving valve parts.\nRecent collaboration between valve manufacturers and end users has led to a clearer understanding of the factors that cause valve failure under these circumstances and factors that can increase service life of those valves. As part of that collaboration, researchers investigated construction materials that could combat moisture contamination and provide enhanced performance in harsh operating conditions, looking at each component of the valve for dendrite susceptibility. Suitable materials were identified, leading to designs that incorporate the appropriate combination of super alloys and stainless steel to allow for a cost-effective solution to these issues.\nThis article discusses the relationship between dendrite formation and valve life, and provides an overview of shortcomings that are now being overcome with these new designs and materials. Coincidentally, the improvements in design that resulted, combined with best practices for specifying features, has contributed to lower total life-cycle costs in the critical applications in dry chlorine services.\nTHE ISSUES INVOLVED\nTo understand the newer solutions and how they were derived, it is important to look at critical valve performance and maintenance issues that end users encounter in applications involving dry chlorine. Understanding the factors that can contribute to valve failure enables valve manufacturers to make corresponding design improvements, and the end user can use this knowledge to extend valve service life.\nFor these dry chlorine applications, end users articulated that real world applications were less than ideal—that these applications posed challenges that were too often severe and unforeseen, and that the end users could save money if they had access to valves that would tolerate the varied and harsh conditions involved. The consensus vision for an optimal design was that it would take into account demanding material requirements, ease of cleaning and packaging, and that external seals would limit emissions under severe conditions. Double packing and a monitoring port also were considered preferable.\nBy working through both typical and more unusual critical pain points, it became evident that dendrites often formed on the surface of Monel discs, causing damage to the soft seats. When such crystals form on frictional surfaces, they cause binding and seizing of mating parts. An example is simple components such as disc spacers that keep the disc centered. The shaft turns inside the bearings and packing so any crystal formation quickly deteriorates these critical components. When these crystals form on hard surfaces that make contact with soft seals, they erode the seals (working like sandpaper).\nFurthermore, while Monel performs well in dry chlorine service (when temperatures stay high enough to preserve chlorine’s “dry” form), any amount of moisture can lead to rapid corrosion of moving parts. For these two reasons, engineers settled on HastelloyC as the preferred material for disc and stem construction.\nFor the valve body, CF8M stainless steel has been identified as the new standard, but all other valve components (pin, gland, bolts, etc.) were also studied individually to determine the optimal choice of material. Using Hastelloy for key parts enables the valve to tolerate moisture. Using CF8M stainless steel for the body, meanwhile, also makes the exterior visibly cleaner and provides visual confirmation that the device is robust.2\nPamphlet 6 of the Chlorine Institute (page 5 of the May 2005 edition), recommends the use of carbon steel for piping for handling dry chlorine. Monel bodies and internals are recommended for dry chlorine, but on page 20, it suggests that consideration be given to materials suited for both wet and dry applications if there is a chance that moisture contamination could be present. For wet chlorine, HastelloyC internals are specified by The Chlorine Institute (note that dry chlorine can become “wet” through moisture contamination, or by sufficiently reduced temperatures).\nResearch indicates that even in dry chlorine service, systems are prone to moisture contamination during installation or whenever connections are made. Such contamination reacts with the chlorine and forms hard, microscopic metallic chloride crystals on metal surfaces. This occurs even on Monel surfaces, which are considered acceptable with perfectly dry chlorine.\nTo combat corrosion, the use of HastelloyC is the better option for dynamic surfaces that form between metal and soft parts. This is a very hard, resilient material that is more costly and more expensive to machine. However, it is indispensable for longer product life and reduced total lifecycle costs—reduced in-service operating costs (replacement, maintenance, spill/leak cleanup, emission/environmental hazards, etc.) more than compensate for higher up-front costs.\nWCB valve bodies are normally recommended for dry chlorine since stainless steel is susceptible to chloride stress cracking. CF8M bodies have demonstrated exceptional endurance in corrosive environments, while being structurally resilient against stress-cracking.\nEnd users also showed interest in valves featuring live-loading, double-packing with monitoring ports and Inconel spring (Belleville washers). Ideally, the packing gland is made from HastelloyC to prevent freezing in the packing bore, which would disable live loading. The packing gland is a simple part on the outside of a valve that might not appear imperiled by possible contamination. Experience indicates, however, that it can be exposed to considerable moisture and minute traces of chlorine vapor emanating or permeating from the system, often causing the gland to seize. Once this occurs, it no longer serves its function of pressurizing the packing, and leakage may result.\nBecause of these risks, Alloy 20 bolting can be used for packing gland studs and nuts to minimize corrosion on these stressed parts. At the same time, this mitigates stress cracking concerns that arise when stainless steel is used.\nA lantern ring between the series of packing allows leakage from the flow path to accumulate in an area adjacent to the second packing. A small sampling valve can be threaded to a monitoring port channeled to this area so it can be periodically checked for early signs of weakening. The lantern ring should be made from HastelloyC to ensure long term operation.\nFor seating, the use of Teflon provides a compliant, inert seal that provides bubble-tight shutoff. An axially pliant seat allows the use of reinforced PTFE seats to maximize resistance to abrasion while maintaining the chemical compatibility of the Teflon. This axially pliant seat flexes but is not subject to hoop stretch. Since Teflon does not have “memory,” it is important to avoid stretching the material.\nChlorine systems often encounter rust or scale so valve seats must be impurity-tolerant. Molecularly enhanced PTFE (TFM) has the disadvantage of performing poorly at low temperatures whereas the R-PTFE ensures valves perform well during temperature cycling and in cold environments.\nSystems often require bidirectional sealing as well as the ability to be used in end-of-line service with pressure contained behind a closed valve without the concern of which side will remain exposed to the medium being processed (which is accompanied by pressure from the piping system).\nBecause minimizing the potential for leak paths is essential in controlling fugitive emissions, using a closed bottom design is a good choice for inhibiting leakage that might occur if the shaft bore protruded through the bottom of the valve. This is the acknowledged best design for valves 12 inches and under. For larger valves, where it is not practical to bore the bottom shaft hole from the top of the valve, a bolted plate with a static PTFE seal is preferred over a threaded plug because a threaded plug tends to grab a seal and twist it during assembly.\nWith the foregoing realities in mind, valve manufacturers are either developing solutions or have already succeeded. The culmination of those efforts is a valve made with materials designed to withstand and be impervious to the effects of dendrite formation. Such is the case with some high-performance butterfly valves, which have incorporated the advances mentioned here in efforts to meet the needs of dry chlorine service. Such valves represent a breakthrough in design, as they specifically address the requirement of preventing valve damage from both moisture contamination and abrasion or erosion.\n1 The qualifying reading may not always be 150 PPM because chlorine at this and lower readings can become “wet” in sufficiently cool or cold conditions.\n2 For reference about recommended materials for valve components, please see the Valve Materials Selection Guide provided by The Chlorine Institute: www.cl2.com/index.php/.../139-pamphlet-6-edition-15-may-2005 table 4.8).\n- The Chlorine Institute, Inc. Pamphlet 6 – Piping Systems for Dry Chlorine (Edition 15)\n- Xomox Manufacturing Control Manual – Level III Cleanliness Requirements: Chlorine, Food, Std. Oxygen, and Vacuum Service Valves","Types 316 and 316L are more resistant to atmospheric and other mild types of corrosion than the 18-8 stainless steels. In general, media that do not corrode 18-8 stainless steels will not attack these molybdenum-containing grades. One known exception is highly oxidizing acids such as nitric acid to which the molybdenum-bearing stainless steels are less resistant.\nType 316 is considerably more resistant than any of the other chromium-nickel types to solutions of sulfuric acid. At temperature as high as 120° F (49° C), Type 316 is resistant to concentrations of this acid up to 5 percent. At temperatures under 100° F (38° C), this type has excellent resistance to higher concentrations. Service tests are usually desirable as operating conditions and acid contaminants may significantly affect corrosion rate. Where condensation of sulfur-bearing gases occurs, these alloys are much more resistant than other types of stainless steels. In such applications, however, the acid concentration has marked influence on the rate of attack and should be carefully determined.\nThe molybdenum-bearing Type 316 stainless steel also provides resistance to a wide variety of other environments. This alloy offers excellent resistance to boiling 20% phosphoric acid. It is widely used in handling hot organic and fatty acids. This is a factor in the manufacture and handling of certain food and pharmaceutical products where the molybdenum-containing stainless steels are often required in order to minimize metallic contamination.\nGenerally, the Type 316 grade can be considered to perform equally well for a given environment. A notable exception is in environments sufficiently corrosive to cause intergranular corrosion of welds and heat-affected zones on susceptible alloys. In such media, Type 316L is preferred over Type 316 for the welded condition since low carbon levels enhance resistance to intergranular corrosion.\nResistance of austenitic stainless steels to pitting and/or crevice corrosion in the presence of chloride or halide ions is enhanced by higher chromium (Cr), molybdenum (Mo), and nitrogen (N) content. A relative measure of pitting resistance is given by the PREN (Pitting Resistance Equivalent, including Nitrogen) calculation, where PREN = Cr+3.3Mo+16N. The PREN of Type 316 and 316L (24.2) is better than that of Type 304 (PREN=19.0), reflecting the better pitting resistance which Type 316 (or 316L) offers due to its Mo content.\nType 304 stainless steel is considered to resist pitting and crevice corrosion in waters containing up to about 100 ppm chloride. The Mo-bearing Type 316 alloy on the other hand, will handle waters with up to about 2000 ppm chloride. Although this alloy has been used with mixed success in seawater (19,000 ppm chloride) it is not recommended for such use. The Type 316 alloy is considered to be adequate for some marine environment applications such as boat rails and hardware, and facades of buildings near the ocean which are exposed to salt spray. Type 316 stainless steel performs without evidence of corrosion in the 100-hou, 5% salt spray (ASTM-B-117) test.\nType 316 is susceptible to precipitation of chromium carbides in grain boundaries when exposed to temperatures in the 800° F to 1500° F (427° C to 816° C) range. This “sensitized” steel is subject to intergranular corrosion when exposed to aggressive environments.\nFor applications where heavy cross sections cannon be annealed after welding or where low temperature stress relieving treatments are desired, the low carbon Type 316L is available to avoid the hazard of intergranular corrosion. This provides resistance to intergranular attack with any thickness in the as-welded condition or with short periods of exposure in the 800-1500° F (427-826° C) temperature range. Where vessels require stress relieving treatment, short treatments falling within these limits can be employed without affecting the normal excellent corrosion resistance of the metal. Accelerated cooling from higher temperatures for the “L” grade is not needed when very heavy or bulky section have been annealed.\nType 316L posses the same desirable corrosion resistance and mechanical properties as the corresponding higher carbon Type 316, and offers an additional advantage in highly corrosive applications where intergranular corrosion is a hazard. Although the short duration heating encountered during welding or stress relieving does not produce susceptibility to intergranular corrosion, it should be noted that continuous or prolonged exposure at 800-1500° F (427-816° C) can be harmful from this standpoint with Type 316L. Also, stress relieving between 100-1500° F (593-816° C) may cause some slight Embrittlement of this type.\nStress Corrosion Cracking\nAustenitic stainless steels are susceptible to stress corrosion cracking (SCC) in halide environments. Although the Type 316 alloy is somewhat more resistant to SCC than the 18 Cr-8 Ni alloys because of the molybdenum content, they still are quite susceptible. Conditions which produce SCC are: (1) presence of halide ion (generally chloride), (2) residual tensile stresses, and (3) temperatures in excess of about 120° F (49° C).\nStresses result from cold deformation or thermal cycles during welding. Annealing or stress relieving heat treatments may be effective in reducing stresses, thereby reducing sensitivity to halide SCC. Although the low carbon “L” grade offers no advantage as regards to SCC resistance, it is a better choice for service in the stress relieved condition in environments which might cause intergranular corrosion."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:3239124a-92eb-421d-bd65-dcc5664a7002>","<urn:uuid:5621a491-4f86-4a35-aaf7-03241bc73fdb>"],"error":null}
{"question":"How did environmental damage from the Yellowstone River and Mayflower oil spills differ?","answer":"The Yellowstone River spill affected approximately 85 river miles and floodplain, damaging fish, aquatic organisms, birds, wildlife, woody debris piles, aquatic habitat, terrestrial habitat, and recreational use. The Mayflower spill contaminated homes and yards in a neighborhood, as well as a nearby creek, wetlands, and a cove of Lake Conway, forcing some residents to evacuate their homes for an extended period.","context":["The second meeting of the Yellowstone River Recreation Project Advisory Committee will take place on Monday, June 12, from 7:00 PM to 9:30 PM (est.) at the Billings Senior Center at 901 South 30th St., Billings, MT.\nMembers of the Committee are:\nKenneth E. Olson, Jr.\nYellowstone County appointed:\nIn January 2017, the State of Montana and U.S. Department of the Interior issued the Final Programmatic Damage Assessment and Restoration Plan and Final Programmatic Environmental Assessment for the Exxonmobil Pipeline Company July 1, 2011 Yellowstone River Oil Spill. The restoration plan was prepared by the State of Montana through the Department of Justice Natural Resource Damage Program and the U.S. Department of the Interior, through the Bureau of Land Management and U.S. Fish and Wildlife Service. The restoration plan describes the natural resource injuries caused by the oil spill and restoration project types to compensate for those injuries.\nThe State of Montana and the United States entered into a $12 million natural resource damage settlement with Exxonmobil, which was approved by the Court in December. The State of Montana will be implementing almost $9.5 million in restoration projects on the Yellowstone River in the next few years. “The restoration plan includes a range of project types that address specific injuries associated with the oil spill, and in total will make the environment and public whole,” said Alicia Stickney, Natural Resource Damage Program Project Manager. “The plan will guide restoration of the Yellowstone River to improve natural and recreational resources of the river injured due to the spill.”\nTo assist with the development of recreation projects, the State has formed a locally-based ad-hoc Recreation Project Advisory Committee to prepare a draft Recreation Project Plan for how approximately $2.3 million will be spent on recreation projects on the Yellowstone River impacted by the spill. The draft Recreation Project Plan will be submitted to the Governor for approval. The Recreation Project Advisory Committee will solicit projects and input from the community.\nHISTORY OF THE SPILL\nOn July 1, 2011, a 12-inch diameter pipeline (Silvertip Pipeline) owned by ExxonMobil Pipeline Company ruptured near Laurel, Montana, resulting in the discharge of crude oil into the Yellowstone River and floodplain. The discharge is estimated to have been approximately 63,000 gallons (about 1,500 barrels) of oil. The discharge occurred during a high-flow event, affecting approximately 85 river miles and associated floodplain. Oil from the spill, along with the cleanup activities, harmed natural resources including fish and other aquatic organisms, birds (including migratory birds), wildlife, large woody debris piles, aquatic habitat, terrestrial habitat, recreational use, and the services provided by these natural resources. These public natural resources are under the Trusteeship of the State of Montana and the U.S. Department of the Interior under the Oil Pollution Act and other laws.\nTHE OIL POLLUTION ACT AND NATURAL RESOURCE DAMAGE ASSESSMENT AND RESTORATION\nThe primary goal of the Oil Pollution Act is to make the environment and public whole for injuries to natural resources and services resulting from a discharge of oil or other hazardous substances to the environment. In the restoration plan, the Trustees have presented their evaluation of injuries to the natural resources, restoration alternatives, and projects that benefit the same or similar resources injured by the oil spill.\nINJURED RESOURCES AND RESTORATION ALTERNATIVES\nOil from the spill, along with spill response and cleanup activities, harmed fish, wildlife and their habitats and other natural resources in and around the Yellowstone River. The spill also impacted the recreational use of the river and public sites along the river. Injuries included:\nThe Trustees evaluated a range of restoration alternatives that would provide resource services to compensate the public for losses pending natural recovery of resources injured by the oil spill. The Trustees have identified preferred restoration alternatives designed to address the resource injuries. The Trustees plan to work with project partners such as local, state, and federal agencies and nonprofit organizations and landowners to implement the projects.\nProject types include:\nLINKS AND DOCUMENTS","Arctic, Arkansas, clean water, corruption, crude oil, DOJ, environment, Exxon, Exxon Mobil, ExxonMobil Pipeline company, Kara Sea, Lake Conway, Mayflower Arkansas, NORM, oil and gas, oil spill, pipeline rupture, radioactive waste, resource extraction, Rex Tillerson, risk management, TENORM, water\n“Oil flowed through the neighborhood, contaminating homes and yards, before entering a nearby creek, wetlands and a cove of Lake Conway. Some residents were ordered to evacuate their homes after the spill and remained displaced for an extended period of time. The spill volume has been estimated at approximately 3,190 barrels, or 134,000 gallons” (USDOJ)\nMayflower Arkansas (suburban Little Rock) Exxon Mobil Pipeline oil spill, USEPA\nUS Secretary of State nominee, Rex Tillerson, wants sanctions dropped against Russia to export equipment to drill the Arctic but is “not well-equipped to deal with” clean-up? “Rex Tillerson, CEO of Exxon Mobil, testified that “when these things [oil spills] happen, we are not well-equipped to deal with them.” Over the last 20 years nothing has changed. The industry and even the government has substantially invested in new technologies to drill in deeper water and deeper into the Earth, but little has been invested in safety or oil spill response and clean-up.(US Senator Menendez, June 22, 2010). https://www.menendez.senate.gov/news-and-events/press/menendez-finds-that-mmss-oil-spill-response-research-tank-is-inoperable Furthermore, the Kara Sea, where they want to drill, is full of radioactive waste, some barrels of it have been leaking since the 1990s, since no one has figured out how to clean-up that even more lethal problem. Given the low price of oil and the high price of radioactive waste disposal one can but wonder if it’s really oil drilling at stake or nuclear waste disposal, even if it is illegal. Or, perhaps they have found a loophole. Isn’t there one related to offshore rigs and TENORM (technologically enhanced naturally occurring radioactive materials)?\nRussian oil companies have already worked out how to make messes without Exxon’s help!\nMayflower oil spill, US EPA\nFrom the US DOJ:\n“FOR IMMEDIATE RELEASE\nWednesday, April 22, 2015\nExxonMobil to Pay $5 Million to Settle U.S. and Arkansas Claims for 2013 Mayflower Oil Spill\nExxonMobil Pipeline Company and Mobil Pipe Line Company (ExxonMobil) have agreed to pay civil penalties, fund an environmental project and implement corrective measures to resolve alleged violations of the Clean Water Act and state environmental laws stemming from a 2013 crude oil spill from the Pegasus Pipeline in Mayflower, Arkansas, the Department of Justice and the Environmental Protection Agency (EPA) announced today.\nUnder a consent decree lodged today in federal court, ExxonMobil will pay $3.19 million in federal civil penalties and take steps to address pipeline safety issues and oil spill response capability. In addition, ExxonMobil will pay $1 million in state civil penalties, $600,000 for a project to improve water quality at Lake Conway, and $280,000 to the Arkansas Attorney General’s Office for the state’s litigation costs.\nThe oil spill occurred on March 29, 2013, after the Pegasus Pipeline, carrying Canadian heavy crude oil from Illinois to Texas, ruptured in the Northwoods neighborhood of Mayflower, Arkansas.\nOil flowed through the neighborhood, contaminating homes and yards, before entering a nearby creek, wetlands and a cove of Lake Conway. Some residents were ordered to evacuate their homes after the spill and remained displaced for an extended period of time. The spill volume has been estimated at approximately 3,190 barrels, or 134,000 gallons.\n“This settlement holds ExxonMobil accountable for this very serious oil spill and its disastrous impact on the Mayflower community and environment,” said Assistant Attorney General John C. Cruden for the Justice Department’s Environment and Natural Resources Division. “This agreement is also an excellent example of federal and state cooperation that will benefit public health and the environment for years to come and most importantly prevent future disasters by requiring better pipeline safety and response measures.”\n“Oil spills like this one in Mayflower, Arkansas have real and lasting impacts on clean water for communities,” said Assistant Administrator Cynthia Giles for EPA’s Office of Enforcement and Compliance Assurance. “Companies need to take the necessary precautions to make sure oil is transported safely and responsibly. This settlement puts in place essential pipeline safety and response measures that are important to make this industry safer for communities.”\n“The U.S. and the state of Arkansas have worked together since the first barrel of oil was spilled in 2013 to provide relief and assistance to the residents of Mayflower and Faulkner County and to hold ExxonMobil accountable for this serious spill,” said U.S. Attorney Christopher R. Thyer for the Eastern District of Arkansas. “This settlement does both. In addition to paying significant civil penalties, ExxonMobil will provide money for safety and water-quality projects to help ensure that the residents of the affected area never have to go through an ordeal like this again. This resolution to a terrible disaster is a testament to the partnership between our federal and state governments to protect the citizens of Arkansas.”\n“Pipeline companies have the responsibility to protect both our water resources and people from oil spills,” said Regional Administrator Ron Curry for EPA. “Today’s settlement will help protect the environment by preventing the high economic and environmental costs of future oil spills.”\nThe penalties owed by ExxonMobil under the consent decree are in addition to the money that the company has already paid to reimburse federal and state response efforts and comply with orders and directives issued by the Pipeline and Hazardous Materials Safety Administration (PHMSA). The segment of the Pegasus Pipeline that includes the rupture site has not been used since the March 2013 spill, and under the terms of the settlement agreement, ExxonMobil must comply with all PHMSA corrective action requirements before returning the pipeline to operation. The consent decree also requires ExxonMobil to take other important pipeline safety corrective action to help prevent future ruptures and improve its spill response capabilities by providing additional training to its oil spill first responders. In addition, ExxonMobil is required to establish caches of spill response equipment and supplies at three strategically-chosen sites along the pipeline, including one location near Mayflower in Faulkner County, Arkansas.\nThe Clean Water Act makes it unlawful to discharge oil or hazardous substances into or upon the navigable waters of the U.S. or adjoining shorelines in quantities that may be harmful to the environment or public health. The penalty paid to the U.S. for this spill will be deposited in the federal Oil Spill Liability Trust Fund managed by the National Pollution Funds Center. Those funds will be available to pay for federal response activities and to compensate for damages when there is a discharge or substantial threat of discharge of oil or hazardous substances to waters of the U.S. or adjoining shorelines.\nThe joint federal and state complaint in the case, filed June 13, 2013, in the U.S. District Court for the Eastern District of Arkansas, alleges that ExxonMobil discharged crude oil in violation of the Clean Water Act. The complaint also asserts state claims for civil penalties for improper storage of hazardous waste generated during the cleanup and for water and air pollution violations pursuant to the Arkansas Water and Air Pollution Control Act and the Arkansas Hazardous Waste Management Act.\nThe proposed consent decree, lodged in the Eastern District of Arkansas, is subject to a 30-day public comment period and court review and approval. A copy of the consent decree is available on the Department of Justice website at http://www.justice.gov/enrd/Consent_Decrees.html.”\nOil spill location exported from Wikipedia. US EPA Photos public domain via Wikipedia https://en.wikipedia.org/wiki/2013_Mayflower_oil_spill"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:0e11fe8d-b1d6-4c09-aff0-4d2818a8e71f>","<urn:uuid:1d2a3123-326b-4108-9859-ef53f56081f9>"],"error":null}
{"question":"How does modern farming technology improve animal welfare monitoring, and what irrigation control systems support efficient crop management?","answer":"Modern farming technology improves animal welfare monitoring through automated systems that track cattle behavior 24/7. Milking robots and ear tags with motion sensors monitor cows' activities, detecting issues like digestive problems through chewing patterns and potential health concerns through movement profiles. For irrigation control, fields use multiple sensors in each irrigation zone to obtain reliable soil moisture readings. Controllers and sensors are installed at each plot or group of sprinklers, performing individual irrigation schedules. Wireless Sensor Networks have made this system more cost-effective, with RFID sensors providing crucial data about soil moisture, temperature, and electrical conductivity for optimized crop management.","context":["Germany’s greatest innovations are found not on the autobahn, but on the country’s fields and farms: self-driving high-tech tractors, milking robots, and feeding machines are already standard equipment for many farmers. Smart Farming is the future of agriculture, with digital technologies enabling efficient and resource-saving farming.\nEvery Plant Counts\nDuring the first of half of 2017, nearly EUR 1 billion risk capital was invested in enterprises and start-ups in the agricultural technology sector. Agricultural machinery manufacturer John Deere took over Blue River Technology, a company which develops artificial intelligence (AI) to enable targeted treatment of individual plants. The AI establishes the amount of herbicide a plant needs and sprays accordingly. The AIʼs plant-recognition ability opens up completely new perspectives in the agricultural sector. Whole surface treatments are no longer necessary and the use of weed killers is reduced.\nThe same applies to fertilisation: using leaf colouring, geodata and soil maps, Precision Farming can calculate a plant’s individual fertiliser needs. This information is transferred to the tractor’s on-board computer. The tractor itself has no driver; cameras, lasers and GPS keep it on the right track. The spreader can then control the fertiliser dose for each individual plant in real time.\nEven sowing can be taken over by robots. The farmer plots the sowing using an app, then the spreader robots roll over the fields. They communicate via the cloud, document where and when a seed was planted and, based on that information, each plant receives individualised care.\nIn future, drones could also patrol the fields. The “Crop Watch” project at the University of Bonn is developing new data management systems to analyze plant growth, using cameras attached to drones and tractors to record plant populations in the fields. The image data is compared and analyzed, then combined with information on weather conditions or the state of the soil, to provide farmers with useful information. Based on the data collected, computer algorithms then assess how many plants cover the area, their state of health and the optimum time for harvesting. Drones can be of great help in animal welfare, too, enabling wild or young animals to be detected on the field before combine harvesters endanger them.\nNowadays, cattle farming is mostly automatic, with milking robots and feeding machines doing most of the work. Milking systems track the cows’ behavior and automatically send updates to farmers’ smartphones. This means that they receive around-the-clock information on their animals’ health. Ear tags enable farm animals to be cared for 24/7. For example, motion sensors record how long a cow chews the cud, which allows farmers to recognise digestive problems early on. Movement profiles can also reveal a lot about the animals’ wellbeing: if a cow moves more than usual, she could be on heat; if she is unusually lethargic, the vet should examine her.\nDigitalization Brings Producers and Consumers Closer Together\nDigitalization will bring consumers and farmers closer together. The extensive information available on plants and animals makes farming more transparent to consumers: for example, consumers can establish precisely where fruit of a particular seed batch was grown and treated. Possible scenarios include a direct webcam feed from the stables, or feedback from consumers to farmers. In the long run, smart farming will affect agricultural production: animals can be better cared for and fields can be treated in ways that are more friendly to plants and the environment as a whole.\nSmart Farms Need Fast Internet\nTo ensure that smart farming goes beyond being a vision and is widely adopted, modern agriculture needs extensive and fast data transfer technology as well as cloud connectivity. However, Germany’s rural areas lack broadband and network coverage. International benchmarking by the Bertelsmann Foundation and the Fraunhofer Institute for System and Innovation Research (ISI) showed that the extent of fibre coverage in Germany’s rural areas amounts to just 1.4%. Compared with its OECD peers, Germany has among the fewest fibre connections, ranking 28th out of 32 countries. If farms are to function as high-tech businesses, fast and robust data connections are indispensable.","As populations have grown, as food production has increased, as economic activity has developed and as societies have become more affluent, so demand for water has burgeoned. Climate change adds yet more pressure on our limited water resources. In very many places demand has far outstripped supply – this may be particularly so in seasons when supply may be severely limited or in years of drought, or at times when demand is particularly high, for example when there is great demand for water for irrigation.\nNot a new challenge\nThere have already been multiple approaches to optimizing processes in farming related activities. Some of the latest solutions include: monitoring local variations in soil, drainage and evaporation, in order to ensure uniform irrigation, accurate dendrometers, capable of measuring changes in a plant’s diameter of only a few micrometers to measure water intake or sensors that measure the conditions for photosynthesis.\nThese solutions are applicable both to open field and greenhouses, where it monitoring and controlling micro-climates is important. Gas sensors can also be of interest in certain environments.\nProne to human errors\nEfficient irrigation practices provide a consistent moisture supply to crops, water deficiencies can be overcome during periods of drought, more than one crop cycle per year can be achieved and the effective use of all production resources can be improved dramatically. The pressure on the diminishing water resources can also be alleviated and, as a result, more land can be put under irrigation.\nAs an example of a specific case, here is an overview of the process on cultivating in greenhouses. The agronomists, during their usual check visit, manually perform the activities of ground sensing and vegetables evaluation. In this step, they store on paper notes all qualitative information detected.\nCaptured parameters are then processed. Based on this information the agronomist takes decisions and applies actions on plants and grounds. They need to annotate all performed operations (i.e., irrigation, sowing and treatments with plant protection products) on paper that is later saved into a digital system. It is a management software tool that allows the farm to carefully store very important information about the operations carried out on plants, the adopted cultivation methods, and the use of approved cleaning products. Some activities, such as irrigation and temperature control, are executed in automated manner through remote terminal units and an advanced computerized control system.\nIntroducing wireless sensors in farming\nThe concept of using a soil moisture sensor to activate irrigation scheduling is well-known and has become a common practice. A large number of soil moisture sensors have become available after the introduction of dielectric soil water. The main parameters to monitor are soil volumetric water content (VWC) and the soil electrical conductivity (EC). For precision real-time irrigation control, controllers and sensors are installed at each plot or at least at every group of sprinklers or drippers in the field. Each controller performs an individual irrigation schedule which is set and reprogrammed on a regular basis.\nIn general, field require a large number of sensors in each irrigation zone to obtain a reliable soil moisture reading. Many controllers and sensors are involved and so far the cost for investment, installing wiring, maintenance and data-handling have been a bottleneck. This has forced growers to look for new improved and cost-effective monitoring and control systems.\nThis is the reason why Wireless Sensor Networks have been introduced in this industry. Installation and management cost is dramatically reduced while providing very similar results to users.\nPassive RFID sensor opportunities\nPassive RFID tags have always been a nice solution to track and monitor different items. The fact that we can now use sensors within battery-free tags opens a new field of opportunities in smart solutions for farming.\nIn particular, soil moisture, temperature and EC would be of great interest for the agriculture industry. A wireless battery-free RFID sensor tag providing information about soil moisture, soil temperature and soil EC would greatly help automate monitoring of soil conditions and data processing.\n- Soil moisture sensor. Knowing the moisture level of the soil before actually irrigating the specific area will lead to increased efficiency by saving up to 40% of water.\n- Soil temperature and ambient temperature sensors. These sensors are used for both humidity sensor compensation and to prevent the irrigation system from acting when the temperature would cause the water to freeze, which is not recommended in many situations.\n- Soil Electrical Conductivity. Knowing the nutrient composition of the soil water in advance allows the user to prepare the water nutrient concentration that will best serve the crops or plants each time the irrigation occurs.\nThis is how an RFID sensor system would work:\n- Battery-free RFID sensor tags are correctly located in the field. Specific location will depend on the field, crops and finite areas the farmer wants to monitor. Note that battery-free sensor tags are easy to move from one location to another.\n- RFID readers are used to gather ID and sensor data from the field. Sensor data is uniquely associated to the ID of the tag. This helps automating data collection and processing without introducing error prone activities such as documenting data by hand. When implemented in a more automated farm, RFID readers can be mounted on center pivot irrigation or similar systems to directly receive data from sensors while the machinery moves above the crops/plants.\n- A robust data processing software will divide water and nutrient requirements by field areas and decide the quantities of water and nutrients the system needs to deliver in each area for optimized growth.\n- The irrigation machinery will deliver the proper amount of supplies to the crops/plants according to the areas defined by the processing system.\nBy introducing wireless sensors in the farming industry, growing crops and plants can be greatly optimized. The main advantages of using an RFID sensor system are:\n- Water consumption is reduced.\n- Crops productivity is increased.\n- Water consumption is tracked and water tariffing is enabled.\n- Unauthorized water consumption is detected.\nBattery-free RFID sensor systems reduce the investment in sensor devices and maintenance costs are decreased because these do not require battery changes. Thus, these systems can be a very good fit for irrigation systems in the farming industry."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:48bc0a04-3afc-4508-aa38-a826922bf68f>","<urn:uuid:745d99e2-3586-4d9b-8388-6a6a21412b62>"],"error":null}
{"question":"I'm interested in making a smooth cranberry curd - what's the optional step mentioned to achieve this?","answer":"To achieve a perfectly smooth curd, there's an optional step of pressing the curd through a sieve over a large bowl after cooking it. You can discard the bits that remain in the sieve. If you don't mind bits of zest or cranberries, you can skip this step.","context":["Sometimes you want to make a recipe that is just big on 'wow' factor. Sometimes you want to make something that is fun and crowd-pleasing. These bars are both, and they make a 9 x 13 pan full so makes enough to share with a crowd.\nIf you've never made curd before, this is a fairly straightforward recipe, and does not involve baking the bars twice. Often bars made with a curd will require baking to set the crust and then again to set the filling. This one incorporates a good amount of butter in the curd to provide a lovely mouthfeel as well as setting abilities. Note the optional step of straining the curd after cooking it on the stovetop (over simmering water, a make-shift double burner). If you want the curd to be smooth, be sure to follow this step. If you don't mind bits of zest or cranberries, you can go ahead and skip it.\nI hope you enjoy this beautiful treat!\nyields one 9 x 13 pan of bars\n8 ounces gingersnap cookies + graham crackers (about half and half of each, but your preference)\n1 cup pecans, optional\n6 tablespoons unsalted butter melted\n3/4 cup white chocolate chips (or 6 oz high quality bar, chopped)\n1 12-ounce package fresh (or frozen thawed) cranberries\n1 cup + 1/2 cup granulated sugar\n2 large eggs\n4 large egg yolks\n1 teaspoon finely grated lemon zest\n1 teaspoons finely grated lime zest\n1/2 cup fresh lime juice\nPinch of kosher salt\n3/4 cup (1½ sticks) unsalted butter room temperature, cut into pieces\nWhipped cream (for serving)\nStart with the crust:\nPreheat oven to 350°. Pulse cookies in a food processor until very finely ground (you should have about 1 cup). Add pecans if using; pulse until finely ground. Add butter and brown sugar; pulse to combine. Transfer to a 9 x 13 pan. Using a measuring cup, press firmly onto bottom of dish. Bake until firm and slightly darkened in color, 10-15 minutes.\nOnce the crust is browned, sprinkle the white chocolate over the crust and return the pan to the oven for one minute. Once softened, gently spread the chocolate over the crust, making as even a layer as possible. Let cool. Do Ahead: Crust can be baked 1 day ahead. Store tightly wrapped at room temperature.\nNext make the filling:\nBring 12 oz. cranberries, 1 cup granulated sugar, and ¼ cup water to a boil in a large saucepan over medium-high. Reduce heat; simmer until cranberries burst and most of the liquid evaporates, 12-15 minutes. Let cool. Purée in a blender until very smooth.\nCombine purée, eggs, egg yolks, lemon zest, lime juice, salt, ½ cup sugar, and 1 tsp. lime zest in a heatproof bowl. Whisk well, and then set over a saucepan of simmering water (bowl should not touch water), stirring with a rubber spatula and scraping down sides of bowl often, until curd thickens and coats spatula, 8-10 minutes. Optional: If you want the curd to be perfectly smooth, press the curd through a sieve over a large bowl. Discard bits that remind in the sieve. Let cool until just warm.\nUsing an electric mixer on medium-high, beat curd, adding butter a piece at a time and incorporating after each addition, until curd looks lighter in color and texture, about 5 minutes. Scrape into crust, spread evenly, and chill until firm, about 2 hours.\nTo serve, cut into small bars. Enjoy!"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:4453ebf7-2ba6-438a-a9ce-cefc35eb6d3d>"],"error":null}
{"question":"What are the historical developments in machine translation methodologies, and how does Google's current translation system improve upon previous approaches?","answer":"Historically, machine translation alternated between statistical and grammar-based methods, with early approaches relying on simple word-to-word translation using vocabularies. The Grammatical Framework (GF) emerged as an approach using type theory for high-quality translation between multiple languages, combining grammar-based methods with statistical techniques. Google's current system represents a significant advancement by using a single neural network instead of separate models for each language pair. Their Neural Machine Translation System employs a sequence-to-sequence model with 8 LSTM-RNN layers that can handle 103 languages. This unified approach improves translation quality and enables translations between language pairs the system hasn't directly trained on, through what's called Zero-Shot Translation.","context":["Machine Translation and Type Theory\nThis paper gives an introduction to automatic translation via examples from the history of the field, where statistical and grammar-based methods alternate. Grammatical Frameword (GF) is introduced as an approach that uses type theory to provide high-quality translation between multiple languages. GF translation is fundamentally grammar-based but can be combined with statistical methods such as learning translation models from a corpus and ranking translation candidates by probabilities.\nKeywordsNoun Phrase Machine Translation Target Language Translation System Abstract Syntax\nPer Martin-Löf supervised my PhD thesis and taught me how to think about type theory and language. He also introduced me to the noisy channel model of Shannon. He wondered if statistical models were still considered useful in natural language processing, and they have ever since been a recurrent theme in our discussions. With his unique combination of insights in both statistics and logic, and his accurate knowledge of many languages, Per has continued to be a major resource for my work through the 21 years that have passed since my PhD. When I later started to look closer at statistical methods, I received inspiration and guidance from Joakim Nivre, Lluís Màrquez, and Cristina España. Lauri Carlson has helped me to understand the problems of translation in general. The model described in this paper has received substantial contributions from my own PhD students Peter Ljunglöf, Kristofer Johannisson, Janna Khegai, Markus Forsberg, Björn Bringert, Krasimir Angelov, and Ramona Enache. The insightful comments from an anonymous referee were valuable when preparing the final version of the paper. The research leading to these results has received funding from the European Union’s Seventh Framework Programme (FP7/2007–2013) under grant agreement n:o FP7-ICT-247914 (http://www.molto-project.eu).\n- Ajdukiewicz, K. 1935. Die syntaktische konnexität. Studia Philosophica1: 1–27.Google Scholar\n- Alshawi, H. 1992. The core language engine. Cambridge: MIT.Google Scholar\n- Angelov, K. 2009. Incremental parsing with parallel multiple context-free grammars. In Proceedings of EACL’09, Athens.Google Scholar\n- Angelov, K., and R. Enache. 2010. Typeful ontologies with direct multilingual verbalization. In CNL 2010, Controlled natural language, Marettimo Island, ed. N. Fuchs and M. Rosner. New Brunswick: ACL.Google Scholar\n- Angelov, K., O. Caprotti, R. Enache, T. Hallgren, I. Listenmaa, A. Ranta, J. Saludes, and A. Slaski 2010, 06/2010. D10.2 molto web service, first version. (D10.2).Google Scholar\n- Appel, A. 1998. Modern compiler implementation in ML. Cambridge/New York: Cambridge University Press.Google Scholar\n- Bar-Hillel, Y. 1964. Language and information. Reading: Addison-Wesley.Google Scholar\n- Bringert, B., R. Cooper, P. Ljunglöf, and A. Ranta. 2005. Multimodal dialogue system grammars. In Proceedings of DIALOR’05, ninth workshop on the semantics and pragmatics of dialogue, Nancy, 53–60.Google Scholar\n- Brown, P.F., J. Cocke, S.A.D. Pietra, V.J.D. Pietra, F. Jelinek, J.D. Lafferty, R.L. Mercer, and P.S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics16(2): 76–85.Google Scholar\n- Burke, D.A., and K. Johannisson. 2005. Translating formal software specifications to natural language/a grammar-based approach. In Logical Aspects of Computational Linguistics (LACL 2005), Lecture notes in computer science/Lecture notes in artificial intelligence, vol. 3492, ed. P. Blache, E. Stabler, and J. Busquets, R. Moot, 51–66. Berlin/New York: Springer. http://www.springerlink.com/content/?k=LNCS+3492.\n- Caprotti, O. 2006. WebALT! Deliver mathematics everywhere. In Proceedings of SITE 2006, Orlando March 20–24. http://webalt.math.helsinki.fi/content/e16/e301/e512/PosterDemoWebALT_e%ng.pdf.Google Scholar\n- Curry, H.B. 1961. Some logical aspects of grammatical structure. In Structure of language and its mathematical aspects: Proceedings of the twelfth symposium in applied mathematics, ed. R. Jakobson, 56–68. Providence: American Mathematical Society.Google Scholar\n- Donzeau-Gouge, V., G. Huet, G. Kahn, B. Lang, and J. J. Levy. 1975. A structure-oriented program editor: A first step towards computer assisted programming. In International computing symposium (ICS’75). Hsinchu: Nat Chiao Tung University.Google Scholar\n- Dowek, G., A. Felty, H. Herbelin, G. Huet, C. Parent, C. Paulin Mohring, B. Werner, and C. Murthy. 1993. The Coq proof assistant user’s guide: version 5.8. Research Report RT-0154, INRIA.Google Scholar\n- Dymetman, M., V. Lux, and A. Ranta. 2000. XML and multilingual document authoring: Convergent trends. In Proceedings of the computational linguistics COLING, Saarbrücken, 243–249. International Committee on Computational Linguistics.Google Scholar\n- Hallgren, T., and A. Ranta. 2000. An extensible proof text editor. In LPAR-2000, Lecture notes in computer science/Lecture notes in artificial intelligence, vol. 1955, ed. M. Parigot and A. Voronkov pp. 70–84. Berlin: Springer. http://www.cse.chalmers.se/~aarne/articles/lpar2000.pdf.\n- Hutchins, W.J., and H.L. Somers. 1992. An introduction to machine translation. London: Academic.Google Scholar\n- Johannisson, K. 2005. Formal and informal software specifications. Ph.D. thesis, Department of Computing Science, Chalmers University of Technology and Gothenburg University.Google Scholar\n- Jonson, R. 2006. Generating statistical language models from interpretation grammars in dialogue system. In Proceedings of EACL06, Trento.Google Scholar\n- Khegai, J., B. Nordström, and A. Ranta. 2003. Multilingual syntax editing in GF. In Intelligent text processing and computational linguistics (CICLing-2003), Mexico City, February 2003, Lecture notes in computer science, vol. 2588, ed. A. Gelbukh, 453–464. Springer. http://www.cs.chalmers.se/~aarne/articles/mexico.ps.gz.\n- Koehn, P., and H. Hoang. 2007. Factored translation models. In EMNLP-CoNLL, Prague, 868–876. ACL.Google Scholar\n- Ljunglöf, P. 2004. The expressivity and complexity of grammatical framework. Ph.D. thesis, Department of Computing Science, Chalmers University of Technology and Gothenburg University.http://www.cs.chalmers.se/~peb/pubs/p04-PhD-thesis.pdf.Google Scholar\n- Ljunglöf, P., G. Amores, R. Cooper, D. Hjelm, O. Lemon, P. Manchón, G. Pérez, and A. Ranta. 2006. Multimodal grammar library. TALK. Talk and Look: Tools for Ambient Linguistic Knowledge. IST-507802. Deliverable 1.2b. http://www.talk-project.org/fileadmin/talk/publications_public/delivera%bles_public/TK_D1-2-2.pdf.Google Scholar\n- Luo, Z., and P. Callaghan (1999). Mathematical vernacular and conceptual well-formedness in mathematical language. In Logical aspects of computational linguistics (LACL), Nancy, Lecture notes in computer science/Lecture notes in artificial intelligence, vol. 1582, ed. A. Lecomte, F. Lamarche, and G. Perrier, 231–250.Google Scholar\n- Luo, Z., and R. Pollack. 1992. LEGO proof development system. Technical report, University of Edinburgh.Google Scholar\n- Magnusson, L. 1994. The implementation of ALF – A proof editor based on Martin-Löf’s monomorphic type theory with explicit substitution. Ph.D. thesis, Department of Computing Science, Chalmers University of Technology and University of Göteborg.Google Scholar\n- Martin-Löf, P. 1984. Intuitionistic type theory. Napoli: Bibliopolis.Google Scholar\n- Montague, R. 1974. Formal philosophy. New Haven: Yale University Press. Collected papers edited by Richmond Thomason.Google Scholar\n- Nordström, B., K. Petersson, and J. Smith. 1990. Programming in Martin-Löf’s type theory: An introduction. Oxford: Clarendon Press.Google Scholar\n- Norell, U. 2007. Towards a practical programming language based on dependent type theory. Ph.D. thesis, Department of Computer Science and Engineering, Chalmers University of Technology, SE-412 96 Göteborg, Sweden.Google Scholar\n- Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In ACL, Philadelphia, 311–318.Google Scholar\n- Perera, N., and A. Ranta (2007). Dialogue system localization with the GF resource grammar library. In SPEECHGRAM 2007: ACL workshop on grammar-based approaches to spoken language processing, June 29, 2007, Prague. http://www.cs.chalmers.se/~aarne/articles/perera-ranta.pdf.\n- Pierce, J.R., and J. B. Carroll et al. 1966. Language and machines – Computers in translation and linguistics. ALPAC report.Google Scholar\n- Power, R., and D. Scott (1998). Multilingual authoring using feedback texts. In COLING-ACL 98, Montreal.Google Scholar\n- Ranta, A. 1994. Type theoretical grammar. Oxford: Oxford University Press.Google Scholar\n- Ranta, A. 2004. Grammatical framework: A type-theoretical grammar formalism. The Journal of Functional Programming14(2): 145–189. http://www.cse.chalmers.se/~aarne/articles/gf-jfp.pdf.Google Scholar\n- Ranta, A. 2007. Modular grammar engineering in GF. Research on Language and Computation5: 133–158. http://www.cs.chalmers.se/~aarne/articles/multieng3.pdf.\n- Ranta, A. 2009a. Grammars as software libraries. In From semantics to computer science. Essays in honour of Gilles Kahn, ed. Y. Bertot, G. Huet, J.-J. Lévy, and G. Plotkin, 281–308. Cambridge/New York: Cambridge University Press. http://www.cse.chalmers.se/~aarne/articles/libraries-kahn.pdf.\n- Ranta, A. 2009b. The GF resource grammar library. Linguistics in Language Technology 2. http://elanguage.net/journals/index.php/lilt/article/viewFile/214/158.\n- Ranta, A. 2011. Grammatical framework: Programming with multilingual grammars. Stanford: CSLI Publications. ISBN-10: 1-57586-626-9 (Paper), 1-57586-627-7 (Cloth).Google Scholar\n- Rayner, M., P. Estrella, and P. Bouillon. 2011. Bootstrapping a statistical speech translator from a rule-based one. In Proceedings of the second international workshop on free/open-source rule-based machine translation, Barcelona. http://hdl.handle.net/10609/5647.\n- Rosetta, M.T. 1994. Compositional translation. Dordrecht: Kluwer.Google Scholar\n- Shannon, C. 1948. A mathematical theory of communication. The Bell System Technical Journal27(1): 379–423, 623–656.Google Scholar\n- Stallman, R. 2001. Using and porting the GNU compiler collection. Cambridge: Free Software Foundation.Google Scholar\n- Tyers, F., and J. Nordfalk. 2009. Shallow-transfer rule-based machine translation for Swedish to Danish. In Proceedings of the first international workshop on free/open-source rule-based machine translation, Alicante. http://hdl.handle.net/10045/12024.","Machine Learning Translation and the Google Translate Algorithm\nToday, we’ve decided to explore machine translators and explain how the Google Translate algorithm works.\nBy Daniil Korbut, Statsbot.\nGoogle Machine Translation\nEvery day we use different technologies without even knowing how exactly they work. In fact, it’s not very easy to understand engines powered by machine learning. The Statsbot team wants to make machine learning clear by telling data stories in this blog. Today, we’ve decided to explore machine translators and explain how the Google Translate algorithm works.\nYears ago, it was very time consuming to translate the text from an unknown language. Using simple vocabularies with word-for-word translation was hard for two reasons: 1) the reader had to know the grammar rules and 2) needed to keep in mind all language versions while translating the whole sentence.\nNow, we don’t need to struggle so much– we can translate phrases, sentences, and even large texts just by putting them in Google Translate. But most people don’t actually care how the engine of machine learning translation works. This post is for those who do care.\nDeep learning translation problems\nIf the Google Translate engine tried to kept the translations for even short sentences, it wouldn’t work because of the huge number of possible variations. The best idea can be to teach the computer sets of grammar rules and translate the sentences according to them. If only it were as easy as it sounds.\nIf you have ever tried learning a foreign language, you know that there are always a lot of exceptions to rules. When we try to capture all these rules, exceptions and exceptions to the exceptions in the program, the quality of translation breaks down.\nModern machine translation systems use a different approach: they allocate the rules from text by analyzing a huge set of documents.\nCreating your own simple machine translator would be a great project for any data science resume.\nLet’s try to investigate what hides in the “black boxes” that we call machine translators. Deep neural networks can achieve excellent results in very complicated tasks (speech/visual object recognition), but despite their flexibility, they can be applied only for tasks where the input and target have fixed dimensionality.\nRecurrent Neural Networks\nHere is where Long Short-Term Memory networks (LSTMs) come into play, helping us to work with sequences whose length we can’t know a priori.\nLSTMs are a special kind of recurrent neural network (RNN), capable of learning long-term dependencies. All RNNs look like a chain of repeating modules.\nUnrolled recurrent neural network\nSo the LSTM transmits data from module to module and, for example, for generating Ht we use not only Xt, but all previous input values X. To learn more about structure and mathematical models of LSTM, you can read the great article “Understanding LSTM Networks.”\nOur next step is bidirectional recurrent neural networks (BRNNs). What a BRNN does, is split the neurons of a regular RNN into two directions. One direction is for positive time, or forward states. The other direction is for negative time, or backward states. The output of these two states are not connected to inputs of the opposite direction states.\nBidirectional recurrent neural networks\nTo understand why BRNNs can work better than a simple RNN, imagine that we have a sentence of 9 words and we want to predict the 5th word. We can make it know either only the first 4 words, or the first 4 words and last 4 words. Of course, the quality in the second case would be better.\nSequence to sequence\nNow we’re ready to move to sequence to sequence models (also called seq2seq). The basic seq2seq model consist of two RNNs: an encoder network that processes the input and a decoder network that generates the output.\nSequence to sequence model\nFinally, we can make our first machine translator!\nHowever, let’s think about one trick. Google Translate currently supports 103 languages, so we should have 103x102 different models for each pair of languages. Of course, the quality of these models varies according to the popularity of languages and the amount of documents needed for training this network. The best that we can do is to make one NN to take any language as input and translate into any language.\nThat very idea was realized by Google engineers at the end of 2016. Architecture of NN was build on the seq2seq model, which we have already studied.\nThe only exception is that between the encoder and decoder there are 8 layers of LSTM-RNN that have residual connections between layers with some tweaks for accuracy and speed. If you want to go deeper with that, take a look at the article Google’s Neural Machine Translation System.\nThe main thing about this approach is that now the Google Translate algorithm uses only one system instead of a huge set for every pair of languages.\nThe system requires a “token” at the beginning of the input sentence which specifies the language you’re trying to translate the phrase into.\nThis improves translation quality and enables translations even between two languages which the system hasn’t seen yet, a method termed “Zero-Shot Translation.”\nWhat means better translation?\nWhen we’re talking about improvements and better results from Google Translate algorithms, how can we correctly evaluate that the first candidate for translation is better than the second?\nIt’s not a trivial problem, because for some commonly used sentences we have the sets of reference translations from the professional translators, that have, of course, some differences.\nThere are a lot of approaches that partly solve this problem, but the most popular and effective metric is BLEU (bilingual evaluation understudy). Imagine, we have two candidates from machine translators:\nCandidate 1: Statsbot makes it easy for companies to closely monitor data from various analytical platforms via natural language.\nCandidate 2: Statsbot uses natural language to accurately analyze businesses’ metrics from different analytical platforms.\nAlthough they have the same meaning they differ in quality and have different structure.\nLet’s look at two human translations:\nReference 1: Statsbot helps companies closely monitor their data from different analytical platforms via natural language.\nReference 2: Statsbot allows companies to carefully monitor data from various analytics platforms by using natural language.\nObviously, Candidate 1 is better, sharing more words and phrases compared to Candidate 2. This is a key idea of the simple BLEU approach. We can compare n-grams of the candidate with n-grams of the reference translation and count the number of matches (independent from their position). We use only n-gram precisions, because calculating recall is difficult with multiple refs and the result is the geometric average of n-gram scores.\nNow you can evaluate the complex engine of machine learning translation. Next time when you translate something with Google Translate, imagine how many millions of documents it analyzed before giving you the best language version.\nOriginal. Reposted with permission.\nTop Stories Past 30 Days"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:b7a8f42f-a760-414d-9620-25a64ff92b56>","<urn:uuid:28f48368-d80c-4b2a-9c89-7a0829e3e1e1>"],"error":null}
{"question":"How do glazed porcelain tiles compare to polished marble tiles in terms of their resistance to liquids and maintenance needs?","answer":"Glazed porcelain tiles and polished marble tiles have different properties regarding liquid resistance. Glazed porcelain tiles are well-protected against excessive moisture due to their glass layer. Meanwhile, polished marble tiles, while less porous than other marble finishes, still have some porosity that makes them vulnerable to absorbing liquids. This makes marble tiles riskier to use in areas prone to spills, such as kitchens. For maintenance, marble requires more careful attention as its color can easily change due to spills or wrong cleaning agents, while glazed porcelain is more forgiving but needs protection against scratches from sand or small particles.","context":["Colorful tiles can make any part of the house look better. One of their biggest advantages is low-maintenance, but that is not the case for all types of tiles.\nDifferent kinds come with their specifics, and frequent use will inevitably leave their mark, regardless of the material’s durability. With proper regular care, though, you can extend the tiles’ lifespan. Still, it’s good to have the most efficient cleaning approach, and you can find out what that is below.\nCleaning Tips For The Most Popular Types Of Tiles\nQuite popular due to their long lifespan, stain-resistance, and fairly low maintenance, these tiles are a good choice for your bathroom or kitchen. When it comes to cleaning, once a week will be enough, but you can increase that number for high-traffic areas. The steps are pretty simple:\n- Use the vacuum cleaner to eliminate all debris; otherwise, you risk spreading them during the cleaning.\n- A few drops of dish detergent mixed in a bucket with lukewarm water will be enough to make your ceramic tiles sparkling again. If you wish to make the solution more efficient, then you can also add white vinegar. The latter can eliminate greasy stans quite fast, giving a nice shiny look to the ceramic surface.\n- If you are cleaning ceramic floors, then better do it with microfibre mop. It can evenly spread the water without trapping parts of it into the grout. Use a clean rug or microfiber cloth for walls and other areas.\n- You can dry with a clean towel for a perfect finish or let air dry if you’d prefer.\nTip: In case your ceramic tiles are visibly stained, and in need of a deeper cleaning, pretty much any all-purpose cleaner can be of use. The grout, however, usually requires some manual work and a little bit more patience. You can whiten it quickly with products like baking soda, hydrogen peroxide, or even toothpaste, scrubbed on gently with a soft brush.\nWhile both porcelain and ceramic tiles are mainly made of clay, the porcelain tiles are considered to be more refined and durable. Thanks to the similarities between the two kinds of tiles, they can both be cleaned with the same method. However, it is good to take into consideration what type of porcelain tiles you have to clean – unglazed, glazed, or textured.\nThe main difference between the first and the second is the added liquid glass layer, which makes the glazed porcelain tiles appear glossy and well-polished. As for the textured tiles, they can resemble marble, as well as other natural stone or hardwood.\nConsider the following tips when cleaning your porcelain tiles:\n- Unglazed tiles – Since they lack the layer of glass protection, they are prone to collect dust and other particles easily. Therefore, don’t forget to vacuum or dust everywhere prior to washing. Cleaning with mild liquid dish soap, vinegar, or other non-chemical solutions is fine, as long as you do not use too much water and dry the tiles after. Spills are also more likely to leave their mark, so it is good to take care of those right away.\n- Glazed tiles – You don’t have to worry about excessive moisture here, as glazed porcelain tiles are well-protected. However, that shiny glass surface can be easily scratched by sand or other small particles, so make sure those are always removed in full. Also, do not use wire or other types of hard brushes for scrubbing the surface. Both homemade and chemical cleaners are suitable, although it is good to check the label to be sure.\n- Textured tiles – Stick to non-abrasive types of cleaners to protect the structure of the tiles. It is alright to gently scrub stained areas with a softer brush. Wash well with water afterward, so all of the cleaners can be removed before drying.\nTerracotta tiles are not that hard to look after, and they can make any room feel cozy and warm. Just like the unglazed porcelain tiles, though, these types of tiles have a porous surface where dust, hair and other dirt can remain trapped. It will also be tough to get rid of stain marks over time, which makes terracotta tiles a bit harder to maintain. You can follow this simple routine for general cleaning:\n- Vacuum or sweep to get rid of the trapped dirt.\n- If the tiles look dingy and stained, you can use a special terracotta cleaning product. Otherwise, a ph neutral cleaner will be enough to clean the surface, such as dishwashing liquid. You can check for others in the store, as they are not hard to find.\n- Wash the surface several times afterward.\nNote: Terracotta tiles can also be sealed to make their surface more durable and easy to maintain. This allows for a deeper steam cleaning, which is best done by a professional who can provide and properly use the specialized equipment. Still, constant use can weaken the protective layer’s structure over time, even with the best cares. This is why it is good to consider resealing your terracotta floors every few years.\nNatural Stone Tiles\nNatural stone tiles are known to require a bit more maintenance and cares than other kinds of tiles. The material does not favor chemical-based cleaners, as well as some natural ones like vinegar or lemon juice. To avoid damages, you have to choose wisely by checking the label and always testing on a small area of the tile first.\nHere are some tips on how to best clean your natural stone tiles:\n- Marble – The beautiful tile color can easily change due to spills or even one treatment with the wrong type of cleaning agent. The porous structure increases the tiles’ ability to absorb all kinds of liquids, which makes it risky to add marble tiles in the kitchen. This also means that you need to be careful with the amount of water you use during the cleaning. To stay on the safe side, rely only on neutral washing agents. Dry and polish to avoid water streaks.\n- Granite – These types of tiles are solid, and they can endure a lot. While you won’t have to worry so much about a scratch or two, stains are another matter. Spills need to be blotted right away, especially from acidic liquids. As for general cleaning and maintenance, water and mild detergent will do the trick. Specialized natural stone cleaners will also be beneficial for granite tiles.\nAs you can see, it is not so hard to look after the different types of tiles. Given the higher price of natural stone and other similar materials, it is not a good idea to risk with untested products or techniques. Professional cleaning is always a safe option, which can help maintain the tile’s perfect look.","Marble Tile Glossary\nWhether you love it on the floor, on the wall, or in your shower, marble tile brings a sense of elegance and timelessness to any space. The variety of colors, sizes, and patterns makes marble a go-to material for all areas of your home. Check out the below glossary of terms to learn more about this beautiful natural stone.\nAccent tile: A secondary tile used to decorate and bring out the qualities of the field tile. An inlay, for instance, might show off marble tile mosaics in a checkered or repeating pattern.\nArtificial marble: Also known as cultured or reconstituted marble, this engineered product is made from a combination of stone particles and synthetic resins, which are cast and designed to look like authentic marble.\nBullnose: A piece of marble that has a convex or rounded edge. This edge type is used for stair treads, countertop edges, and trim.\nCarrara: One of the most popular types of marble for sculpture and building décor. Carrara marble is usually white, gray, or blue-gray, and features linear vein patterns.\nCaulk: Generally silicone based, this material is used to seal joints between tiles and other surfaces.\nCoefficient of friction: A calculation used to measure tile’s smoothness or slip-resistance. Textured marble is more slip-resistant than high-gloss tile.\nCure time: The time required for an adhesive to dry and set completely. For example, when installing marble tile you must wait for the grout to cure before you can seal it.\nField tile: The tile material chosen to cover most of a space, such as a floor. Accent tiles can be used for borders, mosaics, and other creative designs.\nGrout: A thin mortar that’s used to fill the joints between marble tiles. For joints 1/8” or smaller, use unsanded grout; sanded grout is commonly used for joints 1/8” or larger. Sanded grout can scratch polished marble, so install this type of tile with unsanded grout and joint widths smaller than 1/8”.\nHoned: A marble surface finish that produces a satin or matte (and not glossy) look and feel.\nLarge-format tile: A tile is deemed large format when at least one of its edges is longer that 15”. Large-format marble tile options include 12”x24”, 18”x18”, and 24”x24”.\nListello: Border tiles used to enhance a wall or other field.\nMarble: A metamorphic rock formed when limestone is exposed to intense heat and pressure. Other minerals present in the limestone, like clay, sand, or silt, give marble its characteristic vein patterns. It takes polish well.\nMolding (trim): Decorative edging applied to a marble tile field. Sometimes molding hides spaces between tiles and other flooring materials or between floors and walls.\nMosaic: A decorative design element/pattern created with small tiles in a specific area of a floor or wall.\nPolished: A smooth surface finish that gives marble tile high reflectivity and a glossy sheen.\nPorosity: The number of tiny holes on a surface that allow liquids to absorb into the material. All natural stone is porous to some extent; polished marble is less porous than other finishes, making it less vulnerable to absorbing liquids.\nSealer: A protective, topical coating applied to natural stone tiles to prevent liquids or other materials from absorbing into the surface. This will protect the stone from staining.\nSpacer (shim): A cross-shaped piece of plastic or rubber used during installation to create the grouted space between marble tiles.\nThinset mortar: A blend of cement, fine sand, and water retention compounds used to adhere tiles to a substrate.\nVariation: The changes in color and tone that occur on a piece (or multiple pieces) of marble tile.\nVeining: The lines of color that run through the marble’s dominant color."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:fe42dfda-e983-41ed-83ba-db092839ce77>","<urn:uuid:54ba1b5f-b629-44b3-b7f4-f2a9ecc30a8c>"],"error":null}
{"question":"How does inflammation affect diabetes complications in normal versus diabetic mice according to recent research?","answer":"Research shows significant differences in inflammatory response between normal and diabetic mice. In normal mice, inflammation resolves rapidly within three days after infection. However, diabetic mice experience prolonged inflammation that they cannot resolve quickly. This is particularly important because studies have shown that chronic inflammation, not just high blood sugar, is necessary for diabetes complications to occur. The inflammation allows glucose to enter and damage cells in the body, affecting areas like eyes, kidneys, fingers, toes, gums, stomach, and feet.","context":["Complications Most Challenging\nPricking your finger and taking diabetes medications on a daily basis is tedious but the most challenging and impactful part of this disease is the threat of complications.\nUnmanaged diabetes can especially threaten the wellbeing and function of your eyes, kidneys, fingers, toes, gums, stomach, and feet.\nA few years ago, research explained that high blood sugars alone are not the cause of diabetes complications. Instead, chronic inflammation must exist in order for damage to occur in the precious nerves throughout these areas of the body.\nSource of Inflammation?\nBut what causes that inflammation? Until recently, it was assumed that high blood sugar levels were to blame and that achieving healthier blood sugar levels was the key to reducing inflammation.\nPublished in Cell Metabolism by researchers from the University of Kentucky, the study reveals that changes in your mitochondria are actually the primary source of inflammation.\nMitochondria are considered the “power source” of energy for the cells in your body because they take in nutrients, breaks them down, and create energy-rich molecules for cell consumption.\nResearchers Barbara Nikolajczyk and Douglas Lauffenberger were initially studying a theory that immune cells in patients with type 2 diabetes could produce energy by burning glucose for fuel. While their theory proved false, Nikolajczyk said during that research they stumbled across evidence that damaged mitochondria and high levels of a particular type of fatty tissue were driving persistent inflammation.\nReducing inflammation could prevent complications\nThe significance of inflammation’s role in diabetes complications was first highlighted by a study published in 2014 when researchers found that without inflammation, glucose couldn’t enter and damage cells in the body.\nThis means that reducing inflammation could dramatically help protect against diabetes complications.\nThis is important because reducing inflammation may be easier than reducing blood sugars for patients with type 2 diabetes battling severe insulin resistance and beta-cell dysfunction.\nResearchers pinpointed that adding an inflammatory protein called “interleukin-1” into the bloodstream results in cells metabolizing excess glucose which in turn produces inflammation. Interleukin-1 had the same inflammatory effect in patients with and without existing diabetes.\nAfter administering an anti-inflammatory drug, the inflammatory response was blocked and the glucose was no longer able to enter and damage the cells.\nHow you can reduce inflammation naturally\nThere are actually many ways you can reduce inflammation in your body to reduce your risk of developing complications. Reducing inflammation also means reducing insulin resistance which will further help to lower your blood sugar levels.\nTurmeric (curcumin) supplements\nTurmeric has proven to be one the of most anti-inflammatory agents mother nature has to offer — even more so than some pharmaceutical drugs, as explained in this 2017 study.\nIt’s also proven to improve your body’s ability to produce insulin.\nA 9-month curcumin intervention in a prediabetic population significantly lowered the number of prediabetic patients who eventually developed type 2 diabetes, explained a 2012 study published by the American Diabetes Association. Curcumin treatment also improved the overall function of beta-cells with very few side-effects.\nThe study also determined that turmeric can also improve your body’s production of glucagon-like peptide 1 (GLP-1) which is responsible for the rate at which food is digested, and for signaling to your brain that you’re full which helps you stop eating sooner.\nExercise is one of the most powerful anti-inflammatory resources at your fingertips — and it doesn’t cost a thing!\n“Obesity and physical inactivity are associated with chronic low-grade inflammation, which provides the common soil from which a myriad of metabolic diseases develop,” explained the 2014 study.\nRegular exercise (like walking for 30 minutes a day during your lunch break, after work, or after dinner) not only helps to lower your blood sugars, it lowers inflammation levels throughout your entire body, too. As you reduce inflammation, you reduce insulin resistance, which means you can naturally improve your body’s ability to reduce your blood sugar levels, too.\n“That could be one of the reasons if we can get a diabetic to exercise and lose weight, they will have less damage to their blood vessels,” explained Mary Ann Bauman, MD.\nDon’t overthink it, just get moving.\nReduce your alcohol consumption\nWhile a glass or two of wine — or a few tasty microbrews — are no big deal for a person with diabetes, drinking alcohol more than one or two nights a week can contribute heavily to overall inflammation.\nAlcohol is a toxin. Period.\nIf alcohol is present in your body, it wreaks havoc on a variety of aspects of your health. The more you drink, the more potential damage alcohol can cause.\n“Heavy alcohol consumption contributes to systemic inflammation by interfering with the body’s natural defenses against the influx of gut microbiota and its products,” explained a 2010 study.\nThe study adds that alcohol seriously impairs your brain’s ability to regulate systemic inflammation.\nReduce inflammatory foods like dairy, gluten, and sugar\nIf you’re drinking cow’s milk, eating bread and pasta daily, and consuming sugar throughout your entire day, there’s a lot of potential for reducing inflammation simply by replacing these habits with healthier ones.\n- Swap unsweetened vanilla almond milk for cow’s milk.\n- Limit your gluten intake to once a day and replacing those calories with more whole-food gluten-free sources like fruit, vegetables, beans, and lean protein from chicken, turkey, and eggs.\n- Track how many grams of sugar you drink and eat every day.\n- Cut out all sugary beverages because you can easily cut 50 to 100 grams of sugar from your diet overnight by drinking more water, seltzer, and the occasional beverage sweetened with stevia instead.\n- Limit your sweet treats to once per day or every other day depending on how much sugar you get from food now.\nBy making just a few changes to your daily habits, you could drastically lower your body’s inflammation levels and reduce your risk of diabetes complications. You might eventually enjoy those healthier habits, too!","A new study sheds light on the response to infection in people with type 2 diabetes. These individuals develop diabetes associated with obesity. Findings from this study revealed that controlling a specific protein produced by the body, known as a cytokine, reduces the expression of other molecules and helps control inflammation. This is significant because many complications associated with diabetes trigger an inflammatory response. Right now, type 2 diabetes affects over 17 million people in the United States and impacts the health industry significantly on economic and individual levels .\nTumor necrosis factor (TNF), a type of cytokine, can cause inflammation and damage  in soft tissue infections, bite wounds and in periodontal disease. In a recent study published in the Journal of Investigative Dermatology, two groups of lab mice, one normal, the other diabetic, were injected with anaerobic bacteria, a germ present in “approximately one-third of bite wounds and … associated with the formation of abscesses and with relatively serious infections,” [3,4] to determine how type 2 diabetes affects the inflammatory response in surrounding tissue. Results from the tests demonstrated that the presence of diabetes prolongs inflammation. Following infection, the normal mice were able to rapidly resolve the ensuing inflammation within three days whereas the diabetic mice could not.\n“It may be particularly important in diabetics to consider the impact that prolonged inflammation might have on the course of events,” states contributing author Dr. Dana T. Graves. According to the study, diabetics are particularly susceptible to the detrimental effects of infection associated with inflammatory cytokines. Further, inflammation can often be a precursor to complications such as cardiovascular disease and poor wound healing. Dr. Graves concludes, “If excess TNF in diabetics is inhibited, the tendency for prolonged inflammation is reduced.”\nSharon Agsalda | alfa\nThe birth of a new protein\n20.10.2017 | University of Arizona\nBuilding New Moss Factories\n20.10.2017 | Albert-Ludwigs-Universität Freiburg im Breisgau\nUniversity of Maryland researchers contribute to historic detection of gravitational waves and light created by event\nOn August 17, 2017, at 12:41:04 UTC, scientists made the first direct observation of a merger between two neutron stars--the dense, collapsed cores that remain...\nSeven new papers describe the first-ever detection of light from a gravitational wave source. The event, caused by two neutron stars colliding and merging together, was dubbed GW170817 because it sent ripples through space-time that reached Earth on 2017 August 17. Around the world, hundreds of excited astronomers mobilized quickly and were able to observe the event using numerous telescopes, providing a wealth of new data.\nPrevious detections of gravitational waves have all involved the merger of two black holes, a feat that won the 2017 Nobel Prize in Physics earlier this month....\nMaterial defects in end products can quickly result in failures in many areas of industry, and have a massive impact on the safe use of their products. This is why, in the field of quality assurance, intelligent, nondestructive sensor systems play a key role. They allow testing components and parts in a rapid and cost-efficient manner without destroying the actual product or changing its surface. Experts from the Fraunhofer IZFP in Saarbrücken will be presenting two exhibits at the Blechexpo in Stuttgart from 7–10 November 2017 that allow fast, reliable, and automated characterization of materials and detection of defects (Hall 5, Booth 5306).\nWhen quality testing uses time-consuming destructive test methods, it can result in enormous costs due to damaging or destroying the products. And given that...\nUsing a new cooling technique MPQ scientists succeed at observing collisions in a dense beam of cold and slow dipolar molecules.\nHow do chemical reactions proceed at extremely low temperatures? The answer requires the investigation of molecular samples that are cold, dense, and slow at...\nScientists from the Max Planck Institute of Quantum Optics, using high precision laser spectroscopy of atomic hydrogen, confirm the surprisingly small value of the proton radius determined from muonic hydrogen.\nIt was one of the breakthroughs of the year 2010: Laser spectroscopy of muonic hydrogen resulted in a value for the proton charge radius that was significantly...\n17.10.2017 | Event News\n10.10.2017 | Event News\n10.10.2017 | Event News\n20.10.2017 | Information Technology\n20.10.2017 | Materials Sciences\n20.10.2017 | Interdisciplinary Research"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:98843eae-af5a-412c-9a3b-efb25410d006>","<urn:uuid:de7d6a91-d73b-439d-b5e8-509b756c1d06>"],"error":null}
{"question":"What are the main differences between integrated pest management (IPM) and organic farming in their approach to pest control?","answer":"Integrated pest management (IPM) and organic farming have distinct approaches to pest control. IPM focuses on accepting some level of pest presence and uses an action threshold to determine when intervention is necessary. It employs monitoring and proper pest identification before taking action, and uses a combination of prevention methods and controls that pose minimal risk, potentially including some pesticides if safer controls aren't effective. In contrast, organic farming completely prohibits synthetic pesticides and relies exclusively on naturally-derived substances from the Permitted Substances List. Organic farming takes a holistic approach that includes crop rotations, cover crops, and encouraging balanced host/predator relationships. Both approaches prioritize environmental protection and minimize chemical pesticide use, but organic farming has stricter regulations and completely eliminates synthetic pesticides, while IPM may use them as a last resort.","context":["Integrated bug control or IPM for brief is definitely an eco sensitive method of managing pest and disease within the garden. It’s the perfected choice when pest management is required. Instead of heading out and spraying each time pest or disease is located IPM concentrates on different control methods according to more environment friendly approach.\nUsing integrated bug control requires some knowledge of biological factors such as pest and disease in addition to non biological factors such as weather, soil condition, nutrients,and lightweight. The concept is the fact that by handling the growing conditions you are able to eliminate a few of the biological problems. Among the big premises of IPM is you need to be prepared to simply accept some loss and small infestations rather of attempting to manage various existence inside your garden.\nYou will find four primary parts to some effective integrated bug control program it’s not just one action but several actions that determine the requirement for control. They might be referred to as follows.\nSet An Action Threshold\nWhenever a single pest was discovered it’s not time for you to panic and do something. A place is made at what constitutes the best time for you to take actions. What will begin to cause economic injury to the crop. And to be considered may be the condition brought on by climate conditions as well as other outdoors influence before any pursuit is going to be considered.\nMonitor And Identify Unwanted pests\nBefore any pursuit could be taken you have to correctly find out the pest that’s allowing the problem. This is achieved by scouting which would be to go and check out the plants after which correctly find out the unwanted pests. Not every weeds, insects, or microorganisms are dangerous. Make certain that issue is correctly identified to ensure that wrong pesticide sits dormant. Also this might even see whether a pesticide is even needed.\nThe very first type of defense ought to always be prevention. By planting disease resident varieties, making certain proper cultivation, monitoring water and nutrients this helps to avoid problems from arising.\nThe ultimate step is control after you have achieve the the stage where monitoring and prevention not work you’ve arrived at the experience threshold. It’s time to evaluate and find out what the very best method posing minimal quantity of risk is. Generally this may be manual removal or biological like using pheromones to disrupt pest mating. If further monitoring,identification and action thresholds indicate the less dangerous controls aren’t working then additional pest controls could be used.\nWhile all pesticide me is not eliminated by practicing IPM that is certainly reduced significantly. By getting a much better knowledge of your plants needs and just how growing conditions can effect it you’re helping lessen the levels of unneeded pesticide within the atmosphere.\nWith integrated pest management, you can get rid of pests without relying on products and practices that damage the environment. By focusing on aspects of mechanical, biological, and chemical control, such services reduce pest problems considerably in homes and offices.","Introduction to Organic Farming\nOrganic farming is a method of crop and livestock production that involves much more than choosing not to use pesticides, fertilizers, genetically modified organisms, antibiotics and growth hormones.\nOrganic production is a holistic system designed to optimize the productivity and fitness of diverse communities within the agro-ecosystem, including soil organisms, plants, livestock and people. The principal goal of organic production is to develop enterprises that are sustainable and harmonious with the environment.\nThe general principles of organic production, from the Canadian Organic Standards (2006), include the following:\n- protect the environment, minimize soil degradation and erosion, decrease pollution, optimize biological productivity and promote a sound state of health\n- maintain long-term soil fertility by optimizing conditions for biological activity within the soil\n- maintain biological diversity within the system\n- recycle materials and resources to the greatest extent possible within the enterprise\n- provide attentive care that promotes the health and meets the behavioural needs of livestock\n- prepare organic products, emphasizing careful processing, and handling methods in order to maintain the organic integrity and vital qualities of the products at all stages of production\n- rely on renewable resources in locally organized agricultural systems\nOrganic farming promotes the use of crop rotations and cover crops, and encourages balanced host/predator relationships. Organic residues and nutrients produced on the farm are recycled back to the soil. Cover crops and composted manure are used to maintain soil organic matter and fertility. Preventative insect and disease control methods are practiced, including crop rotation, improved genetics and resistant varieties. Integrated pest and weed management, and soil conservation systems are valuable tools on an organic farm. Organically approved pesticides include “natural” or other pest management products included in the Permitted Substances List (PSL) of the organic standards. The Permitted Substances List identifies substances permitted for use as a pesticides in organic agriculture. All grains, forages and protein supplements fed to livestock must be organically grown.\nThe organic standards generally prohibit products of genetic engineering and animal cloning, synthetic pesticides, synthetic fertilizers, sewage sludge, synthetic drugs, synthetic food processing aids and ingredients, and ionizing radiation. Prohibited products and practices must not be used on certified organic farms for at least three years prior to harvest of the certified organic products. Livestock must be raised organically and fed 100 per cent organic feed ingredients.\nOrganic farming presents many challenges. Some crops are more challenging than others to grow organically; however, nearly every commodity can be produced organically.\nThe world market for organic food has grown for over 15 years. Growth of retail sales in North America is predicted to be 10 per cent to 20 per cent per year during the next few years. The retail organic food market in Canada is estimated at over $1.5 billion in 2008 and $22.9 billion in the U.S.A. in 2008. It is estimated that imported products make up over 70 per cent of the organic food consumed in Canada. Canada also exports many organic products, particularly soybeans and grains.\nThe Canadian Organic Farmers reported 669 certified organic farms in Ontario in 2007 with over 100,000 certified organic acres of crops and pasture land. This is an annual increase of approximately 10 per cent per year in recent years. About 48 per cent of the organic cropland is seeded to grains, 40 per cent produces hay and pasture and about five per cent for certified organic fruits and vegetables. Livestock production (meat, dairy and eggs) has also been steadily increasing in recent years.\nThe main reasons farmers state for wanting to farm organically are their concerns for the environment and about working with agricultural chemicals in conventional farming systems. There is also an issue with the amount of energy used in agriculture, since many farm chemicals require energy intensive manufacturing processes that rely heavily on fossil fuels. Organic farmers find their method of farming to be profitable and personally rewarding.\nConsumers purchase organic foods for many different reasons. Many want to buy food products that are free of chemical pesticides or grown without conventional fertilizers. Some simply like to try new and different products. Product taste, concerns for the environment and the desire to avoid foods from genetically engineered organisms are among the many other reasons some consumers prefer to buy organic food products. In 2007 it was estimated that over 60 per cent of consumers bought some organic products. Approximately five per cent of consumers are considered to be core organic consumers who buy up to 50 per cent of all organic food.\n“Certified organic” is a term given to products produced according to organic standards as certified by one of the certifying bodies. There are several certification bodies operating in Ontario. A grower wishing to be certified organic must apply to a certification body requesting an independent inspection of their farm to verify that the farm meets the organic standards. Farmers, processors and traders are each required to maintain the organic integrity of the product and to maintain a document trail for audit purposes. Products from certified organic farms are labelled and promoted as “certified organic.”\nIn June 2009, the Canadian government introduced regulations to regulate organic products. Under these regulations the Canadian Food Inspection Agency (CFIA) oversees organic certification, including accreditation of Conformity Verification Bodies (CVBs) and Certification Bodies (CBs). This regulation also references the Canadian Organic Production Systems General Principles and Management Standards (CAN/CGSB-32.310) and the Organic Production Systems – Permitted Substances List that were revised in 2009.\nThe Canadian organic regulations require certification to these standards for agricultural products represented as organic in import, export and inter-provincial trade, or that bear the federal organic agricultural product legend or logo. (Figure 1) Products that are both produced and sold within a province are regulated by provincial organic regulations where they exist (Quebec, British Columbia and Manitoba).\nFigure 1. Canadian Agriculture Product Legend (logo)\nThe federal regulations apply to most food and drink intended for human consumption and food intended to feed livestock, including agricultural crops used for those purposes. They also apply to the cultivation of plants. The regulations do not apply to organic claims for other products such as aquaculture products, cosmetics, fibres, health care products, fertilizers, pet food, lawn care, etc.\nFood products labelled as organic must contain at least 95 per cent organic ingredients (not including water and salt) and can bear the Canada Organic logo. Multi-ingredient products with 70 per cent to 95 per cent organic product content may be labelled with the declaration: “% organic ingredients”. Multi-ingredient products with less than 70 per cent organic content may identify the organic components in the ingredient list.\nExported products must meet the requirements of the importing country or standards negotiated through international equivalency agreements. Products exported to the U.S. must meet the terms of the Canada-U.S. equivalency agreement signed in June 2009. All products that meet the requirements of the Canada Organic Regime can be exported to the U.S. with the exception that agricultural products derived from animals treated with antibiotics cannot not be marketed as organic in the U.S. Canada is also exploring other international equivalency agreements with other trading partners to enhance trade opportunities for export and to assure the organic integrity of imported products.\nWhen considering organic certification, know the requirements and accreditation(s) needed in the marketplace where your products will be sold. When comparing certification bodies, make sure they have the certification requirements and accreditations needed to meet market requirements. As a minimum certification bodies should be accredited under the Canadian Organic Products Regulations. Some markets may require accreditation or equivalency agreements with countries in the European Union, or with the Japanese Agricultural Standard (JAS), Bio-Swisse or other international organic certification systems. As Canada develops international equivalency agreements the need for the certification body to have these international accreditations will diminish.\nFor more information on certification and links to Canadian regulations and standards see the Organic Agricultural section of the OMAFRA website at www.ontario.ca/organic or the CFIA website at www.inspection.gc.ca.\nThe first few years of organic production are the hardest. Organic standards require that organic lands must be managed using organic practices for 36 months prior to harvest of the first certified organic crop. This is called the “transition period” when both the soil and the manager adjust to the new system. Insect and weed populations also adjust during this time.\nCash flow can be a problem due to the unstable nature of the yields and the fact that price premiums are frequently not available during the transition since products do not qualify as “certified organic.” For this reason, some farmers choose to convert to organic production in stages. Crops with a low cost of production are commonly grown during the transition period to help manage this risk.\nCarefully prepare a plan for conversion. Try 10 per cent to 20 per cent the first year. Pick one of the best fields to start with and expand organic acreage as knowledge and confidence are gained. It may take five to 10 years to become totally organic, but a long term approach is often more successful than a rapid conversion, especially when financial constraints are considered. Parallel production (producing both organic and conventional versions of the same crop or livestock product) is not allowed. Use good sanitation, visually different varieties, individual animal identification and other systems to maintain separation and integrity of the organic and conventional products. Good records are essential.\nIn organic production, farmers choose not to use some of the convenient chemical tools available to other farmers. Design and management of the production system are critical to the success of the farm. Select enterprises that complement each other and choose crop rotation and tillage practices to avoid or reduce crop problems.\nYields of each organic crop vary, depending on the success of the manager. During the transition from conventional to organic, production yields are lower than conventional levels, but after a three to five year transition period the organic yields typically increase.\nCereal and forage crops can be grown organically relatively easily to due to relatively low pest pressures and nutrient requirements. Soybeans also perform well but weeds can be a challenge. Corn is being grown more frequently on organic farms but careful management of weed control and fertility is needed. Meeting nitrogen requirements is particularly challenging. Corn can be successfully grown after forage legumes or if manure has been applied. Markets for organic feed grains have been strong in recent years.\nThe adoption of genetically engineered (GMO) corn and canola varieties on conventional farms has created the issue of buffer zones or isolation distance for organic corn and canola crops. Farmers producing corn and canola organically are required to manage the risks of GMO contamination in order to produce a “GMO-free” product. The main strategy to manage this risk is through appropriate buffer distances between organic and genetically engineered crops. Cross-pollinated crops such as corn and canola require much greater isolation distance than self-pollinated crops such as soybeans or cereals.\nFruit and vegetable crops present greater challenges depending on the crop. Some managers have been very successful, while other farms with the same crop have had significant problems. Certain insect or disease pests are more serious in some regions than in others. Some pest problems are difficult to manage with organic methods. This is less of an issue as more organically approved biopesticides become available. Marketable yields of organic horticultural crops are usually below non-organic crop yields. The yield reduction varies by crop and farm. Some organic producers have added value to their products with on-farm processing. An example is to make jams, jellies, juice, etc. using products that do not meet fresh market standards.\nLivestock products can also be produced organically. In recent years, organic dairy products have become popular. There is an expanding market for organic meat products. Animals must be fed only organic feeds (except under exceptional circumstances). Feed must not contain mammalian, avian or fish by-products. All genetically engineered organisms and substances are prohibited. Antibiotics, growth hormones and insecticides are generally prohibited. If an animal becomes ill and antibiotics are necessary for recovery, they should be administered. The animal must then be segregated from the organic livestock herd and cannot be sold for organic meat products. Vaccinations are permitted when diseases cannot be controlled by other means. Artificial insemination is permitted. Always check with your certification body to determine if a product or technique is allowed in the Permitted Substances List and the organic standards. Organic production must also respect all other federal, provincial and municipal regulations.\nOrganic produce can usually qualify for higher prices than non-organic products. These premiums vary with the crop and may depend on whether you are dealing with a processor, wholesaler, retailer or directly with the consumer. Prices and premiums are negotiated between buyer and seller and will fluctuate with local and global supply and demand.\nHigher prices offset the higher production costs (per unit of production) of management, labour, and for lower farm yields. These differences vary with commodity. Some experienced field crop producers, particularly of cereals and forages, report very little change in yield while in some horticultural crops such as tree fruits, significant differences in marketable yield have been observed. There may also be higher marketing costs to develop markets where there is less infrastructure than for conventional commodities. Currently, demand is greater than supply for most organic products.\nOrganic farming can be a viable alternative production method for farmers, but there are many challenges. One key to success is being open to alternative organic approaches to solving production problems. Determine the cause of the problem, and assess strategies to avoid or reduce the long term problem rather than a short term fix for it.\nCOG – Canadian Organic Growers Inc.\n323 Chapel St., Ottawa ON K1N 7Z2\nPhone: (613) 216-0741, 1-888-375-7383\nEFAO – Ecological Farmers Association of Ontario\n5420 Highway 6 North,\nRR 5, Guelph, ON N1H 6S2\nPhone: (519) 822-8606\nOMAFRA – Ontario Ministry of Agriculture, Food and Rural Affairs\n1 Stone Road W., Guelph, ON N1G 4Y2\nAgr. Information Contact Centre\nOACC- Organic Agricultural Centre of Canada\nNova Scotia Agricultural College\nBox 550, Truro, Nova Scotia, B2N 5E3\nPhone: (902) 893-7256, Fax: (902) 893-3430\nGuelph Organic Conference\nFor information contact:\nTomás Nimmo, Box 116,\nCollingwood, ON L9Y 3Z4\nPhone: (705) 444-0923, Fax (705) 444-0380\nOCO - Organic Council of Ontario\nRR 5 Guelph, ON N1H 6J2\nPhone: (519) 827-1221, Fax: (519) 827-0721\n© Queen's Printer for Ontario, 2015\nThe information on this page was written and copyrighted by the Government of Ontario. The information is offered here for educational purposes only and materials on this page are owned by the Government of Ontario and protected by Crown copyright. Unless otherwise noted materials may be reproduced for non-commercial purposes. The materials must be reproduced accurately and the reproduction must not be represented as an official version. As a general rule, information materials may be used for non-profit and personal use."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:08be27f0-6f7e-415f-976c-743112e65713>","<urn:uuid:e6ddb9b3-68e1-422b-adc0-49ee5dd47471>"],"error":null}
{"question":"What is the International Trade Centre and who runs it?","answer":"The International Trade Centre is the joint agency of the World Trade Organization and the United Nations. It assists small and medium-sized enterprises in developing and transition economies to become more competitive in global markets, contributing to sustainable economic development within the Aid-for-Trade agenda and UN Sustainable Development Goals.","context":["Providing better trade and export development for Eswatini\nThe European Union partners with the International Trade Centre, a joint agency of the United Nations and the World Trade Organization to support the Government of the Kingdom of Eswatini in export market analysis.\n(Kingdom of Eswatini) The International Trade Centre’s market analysis tool Trade Map will support export market access and drive sustainable growth in the Kingdom of Eswatini.\nTrade Map is an online database of international trade statistics that has been providing useful indicators on export performance, international demand, alternative markets, and the role of competitors from different global markets since 2001.\nIt covers the annual trade flows of over 220 countries and over 5,300 products. It also offers trade indicators such as values, trends, quantities, trends, market share as well as monthly and quarterly data from both developed and developing countries.\nTo give small businesses and private-sector associations in Eswatini access to this tool, the International Trade Centre (ITC) is working closely together with the European Union (EU) and the Government.\nThe EU is supporting the initiative under its project ‘Support Programme to the EU-SADC Economic Partnership Agreement (EPA) in the Kingdom of Eswatini’. The project specifically supports trade and export development to create EPA opportunities and support trade and investment promotion bodies and small businesses.\nThe EU and ITC are currently running capacity trainings to equip business support institutions with knowledge and tools for analytical outputs and the EU-SADC-EPA as well as other international trade agreements. Participants were drawn from the Eswatini Investment Promotion Authority (EIPA) and the National Agricultural Marketing Board (NAMBoard), Eswatini Water and Agricultural Development Enterprise (ESWADE), Eswatini Economic Policy Analysis and Research Centre (ESEPARC), Eswatini Leather Association, COMESA Federation of Women in Business (COMFWB) Eswatini chapter, and Eswatini Multipurpose Cooperative Union (ESWAMCU).\n‘This tool will allow us to share information with the business community and puts us in a better position to advise on available market opportunities. It also offers an analysis of trends in terms of products that are doing well or that are being discontinued in global markets. Best of all, it’s readily available to everyone.’\nBongani Ntshangase, Executive Head of Trade, Eswatini Investment Promotion Authority\n‘We see this free-market access tool as a contribution to driving growth. It will help further facilitate business and trade relationships across continents and help micro, small, and medium-sized enterprises to increase their visibility and market opportunities.’\nLuis Miguel Pascoal, EU Programme Officer\nAbout the International Trade Centre - The International Trade Centre is the joint agency of the World Trade Organization and the United Nations. ITC assists small and medium-sized enterprises in developing and transition economies to become more competitive in global markets, thereby contributing to sustainable economic development within the frameworks of the Aid-for-Trade agenda and the United Nations’ Sustainable Development Goals.\nFor more information, visit www.intracen.org.\nFollow ITC on Twitter | Facebook | LinkedIn | Instagram | Flickr\nSenior Strategic Communications Officer\nInternational Trade Centre\nE: pak [at] intracen.org\nT: +41 22 730 0651\nM: +41 79 667 4660"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:9e40f11d-58ff-4f67-b5fc-310d4f31f4d4>"],"error":null}
{"question":"I study stellar physics - would the Sun and Betelgeuse have the same fate regarding their final stages of evolution?","answer":"No, the Sun and Betelgeuse will have very different endings. According to the documents, main sequence stars like our Sun will eventually burn out and become cold white dwarves. In contrast, Betelgeuse, being much more massive, will end its life in a spectacular supernova explosion when its core exhausts its silicon, becoming the brightest and most spectacular supernova visible from Earth in perhaps a million years.","context":["Stellar nucleosynthesis is the theory explaining the creation (synthesis) our sun produces 10% of its energy from the cno cycle. Whether ccsne or sne ia were the dominant sites of the nucleosynthesis of the p-nuclei in the sun is (2010) nucleosynthesis in thermonuclear supernovae. Nucleosynthesis: nucleosynthesis, a process that entails large-scale nuclear reactions including those in progress in the sun and other stars. Stellar nucleosynthesis refers to the synthesis of heavy element nuclei due to nuclear fusion this cycle dominates in stars more massive than the sun. Stellar nucleosynthesis creates heavier elements from hydrogen and helium the energy released during this process is what causes the sun.\nIf iron is the heaviest element made by nuclear fusion, then how do we have so many elements heavier than iron on earth ----- here's a rather cool. Nucleosynthesis, solar system nucleosynthesis, lecture notes - geochemistry such as c and n than the sun explosive nucleosynthesis the r-process. Stellar nucleosynthesis this would be the end of the story, except main sequence stars like our sun burn out and become cold white dwarves. Origin of elements •the big bang: h, d, 3,4he, li •all other nuclei were synthesized in stars •stellar nucleosynthesis ⇔ 3 key processes. Ep 107: nucleosynthesis: elements from stars the universe kind of resembled the inside of the sun that were going on during the big bang nucleosynthesis.\nStellar nucleosynthesis refers to the assembly of the natural abundances of the chemical elements by the prime energy producer in the sun is the fusion of. An introduction to the evidence for stellar nucleosynthesis george sivulka march 8, 2017 submitted as coursework for ph241 the spectral lines of the sun.\nStars with masses roughly ten times the mass of the sun die in violent explosions known as type ii supernovae element formation occurs in such massive stars both. If the star has more than about twice the sun's mass the heaviest elements of all are produced by explosive nucleosynthesis in supernova explosions. Cosmic nucleosynthesis but stellar fusion in main sequence stars like the sun haven't had enough time since the big bang to generate this much helium. Nucleosynthesis in the sun when we learned about the sun, we learned that in the core, four hydrogen atoms combine to form one helium atom.\nStars with masses roughly ten times the mass of the sun die in violent explosions known as type ii supernovae element formation occurs in such massive stars both during the pre-explosion. The origin of the elements by stuart surrey for the sun, beneath the stellar nucleosynthesis of the elements up to iron has involved exothermic nuclear fusion.\nStellar nucleosynthesis evolution and nucleosynthesis calculation of gamow peak assumes that the process is far interior of the sun. Nucleosynthesis - wikipedianucleosynthesis is the process that creates new atomic nuclei from pre-existing nucleons, of those with more than eight times the mass of. Educational interactive movies/games have you ever wondered what powers our sun and the other stars in the sky nucleosynthesis in the sun.\nStellar nucleosynthesis is the process by which the natural abundances of the chemical elements within stars vary due to nuclear fusion reactions in the. Stellar nucleosynthesis is the process by which the natural abundances of the chemical elements within stars change due to the sun itself has a core temperature. Nucleosynthesis in the sun nucleosynthesis in the sun stellar nucleosynthesis – wikipedia stellar nucleosynthesis is the process by which the natural. Have the elements evolved by stellar all theorists acknowledge that cno reactions in the sun lead to virtually no nucleosynthesis of atoms heavier than helium3. The proton-proton cycle operates in less massive and luminous stars like the sun nucleosynthesis the process by which elements are formed.\nElement synthesis and isotopes nucleosynthesis 2 hydrogen 10 20 30 40 50 60 70 80 90 the sun late sun large star as red giant. Nucleosynthesis definition nucleosynthesis is the process by which heavier the most prevalent reaction in smaller stars like our sun is the fusion of hydrogen. Stellar nucleosynthesis is the process by which the natural abundances of the chemical elements within stars change due to nuclear from the sun to nearly. The process is called nucleosynthesis are thought to be produced in stars that contain at least ten times as much matter as our sun our sun is currently. Start studying nucleosynthesis learn vocabulary, terms, and more with flashcards, games for a sun-sized star this increase in heat will cause the star to.","Betelgeuse is the ninth brightest star in the sky, and the second brightest in the constellation of Orion (it’s the red one, on the opposite side of the Belt from Rigel, which is the blue one, and the brightest).\nWith a mass of some 20 sols (= the mass of 20 Suns), Betelgeuse is evolving rapidly, even though it’s only a few million years old. It’s now a red supergiant, burning helium in a shell, and (very likely) burning carbon in another shell (closer to the nucleus), and (possibly) oxygen, silicon, and sulfur in other nested shells (like Russian dolls).\nBetelgeuse is enormous … if it were where the Sun is, all four inner planets would be inside it! Because it’s so big, and is only approx 640 light-years away, Betelgeuse appears to about 1/20 of an arcsecond in size; this made it an ideal target for optical interferometry. And so it was that in 1920 Michelson and Pease used the 100″ Mt Wilson telescope, with a 20 m interferometer attached to the front, to measure Betelgeuse’s diameter.\nThe Hubble Space Telescope imaged Betelgeuse directly, in 1995, in the ultraviolet (see above). Why the UV? Because ground-based telescopes can’t make such observations, and because the Hubble’s resolution is greatest in the UV.\nSince the 1920s Betelgeuse has been observed, from the ground, by many different optical interferometers, at many wavelengths. Its diameter varies somewhat, as does its brightness (Herschel is perhaps the first astronomer to describe its variability, in 1836). It also has ‘hotspots’, which are ginormous.\nBetelgeuse is also shedding mass in giant plumes that stretch to over six times its diameter. Although these plumes will certainly cause it to ‘slim down’, they won’t be enough to stop its core turning to iron (when the silicon there is exhausted, if it hasn’t already done so). Not long afterwards, perhaps within the next thousand years or so, Betelgeuse will go supernova … making it the brightest and most spectacular supernova visible from Earth in perhaps a million years. Fortunately, because we are not looking directly down on its pole, when Betelgeuse does go bang, we won’t be fried by a gamma ray burst (GRB) which may occur (while a core collapse supernova can cause one kind of GRB, it is not yet known if all such supernovae produce GRBs; in any case, such a GRB is one of a pair of jets which rip through the poles of the dying star).\nAAVSO has an excellent article on Betelgeuse, and COAST’s (Cambridge Optical Aperture Synthesis Telescope) webpage on its observations of Betelgeuse gives a good summary of one interferometric technique (and some great images too!).\nUniverse Today has many stories on just about every aspect of Betelgeuse, from its varying size (The Curious Case of the Shrinking Star), the bubbles it’s blowing and its plumes (Closest Ever Look at Betelgeuse Reveals its Fiery Secret), featured in What’s Up This Week, to the bow shock it creates in the interstellar medium (The Bow Shock of Betelgeuse Revealed).\nAstronomy Cast’s The Life of Other Stars is a whole episode on the evolution of stars other than the Sun."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:5c7baa7f-e02b-436a-bb0d-59d76cff1cdb>","<urn:uuid:f73a2ac7-896d-438f-b590-ab1fcc1cc6da>"],"error":null}
{"question":"As a medical student studying professional guidelines, I want to understand: how do disclosure policies for conflicts of interest compare between clinical policy statements and psychiatric diagnostic manuals in terms of their effectiveness?","answer":"Comparing disclosure policies between clinical policy statements and psychiatric diagnostic manuals reveals significant limitations in both contexts. For clinical policies, disclosure of conflicts allows readers to interpret the policy considering potential biases and permits other policy team members to evaluate contributions from potentially conflicted members. However, for psychiatric diagnostic manuals like the DSM, evidence suggests disclosure is not only ineffective but potentially harmful. Research shows that disclosure can lead to greater bias through 'strategic exaggeration' where more biased advice is provided to counteract anticipated discounting, and through 'moral licensing' where advisers feel legitimized because advisees have been warned. Additionally, while clinical policy statements require comprehensive disclosure of financial relationships, the DSM's policy has concerning gaps - it allows considerable amounts of funding ($10,000 annually and $50,000 in stock holdings), doesn't consider unrestricted research grants problematic, and permits participation in pharmaceutical speakers' bureaus without full transparency.","context":["Clinical policies of professional societies such as the American Academy of Pediatrics are valued highly, not only by clinicians who provide direct health care to children but also by many others who rely on the professional expertise of these organizations, including parents, employers, insurers, and legislators. The utility of a policy depends, in large part, on the degree to which its purpose and basis are clear to policy users, an attribute known as the policy's transparency. This statement describes the critical importance and special value of transparency in clinical policies, guidelines, and recommendations; helps identify obstacles to achieving transparency; and suggests several approaches to overcome these obstacles.\nThe mission of the American Academy of Pediatrics (AAP) is to promote the attainment of optimal physical, mental, and social health and well-being for all children. To aid in the accomplishment of this mission, the AAP develops clinical policies that are valued highly by members who provide direct health care to children, members of other organizations that share similar goals, and by parents, payers, and legislators. The utility of a policy depends, in large part, on the degree to which its purpose and basis are clear to policy users. This attribute is referred to as the policy's transparency. The purpose of this policy statement is to describe the critical importance and special value of transparency in clinical policies, guidelines, and recommendations; to identify obstacles to achieving transparency; and to suggest several approaches to overcome these obstacles. The term “policy” is used to refer generally to policies, guidelines, recommendations, and other similar statements.\nThe purpose of creating most clinical policies is to improve processes and outcomes of care by decreasing inappropriate variation in practice and increasing the implementation of effective strategies of health promotion and disease management. Such policies serve to guide clinical practice by summarizing the accumulated scientific evidence and combining it with the opinions of expert clinicians to define courses of action that are appropriate for patient care. Policies may provide guidance about:\neducating or counseling patients about their health and health care;\nimplementing effective strategies of health promotion and disease management;\nusing tests appropriately;\nmonitoring patient status;\ndefining criteria for diagnosis;\nprescribing medications and devices;\nreferring for specialized care;\ndocumenting in the medical record;\noutlining ethical behaviors;\ndefining an appropriate setting for care;\npromoting patient well-being;\npreparing clinicians and facilities to provide safe and effective care; and/or\nadvocating on behalf of patients1 and pediatricians.\nPolicies that are intended to influence the clinical actions of health care professionals should be based on the best available evidence and should include guidance regarding the application of such evidence to the individual patient.\nApplication of policy recommendations, like all clinical decision-making, is attended by some degree of uncertainty. Will this treatment be effective for this patient at this time? Will the patient suffer from an adverse effect that will interfere with successful treatment? Will the cost of the regimen make it impossible for the family to adhere to the prescribed intervention? Astute clinicians weigh the anticipated benefits of a policy recommendation against potential risks, harms, and costs viewed in the context of the patient's individual situation and preferences. Users' confidence that a given policy will result in particular benefits, risks, harms, and costs is enhanced when the policy is based on the best available clinical research and is free from bias and when the evidence and reasoning that support the policy are explicitly stated.\nTransparency requires explicit statements regarding the reasons for developing a policy and explaining how published scientific evidence, pathophysiologic reasoning, clinicians' experiences, and perceptions of society's and patients' values are weighed. It also requires disclosure of potential biases of policy authors. Transparency allows potential users to judge the credibility of a policy by observing how the policy authors arrived at a result; thus, it has the potential to promote acceptance of the judgments and choices that have been made.\nOPPORTUNITIES TO IMPROVE POLICY\nPolicies will decrease inappropriate variability in practice only when they are accepted and adhered to by clinicians. Obstacles to the successful acceptance and implementation of a policy include concern about lack of credibility and bias on the part of policy writers; poor understanding of the process of policy formulation; apparent or actual lack of scientific evidence to support policies; and poorly articulated or vague policy statements. Each of these obstacles has the potential to diminish the user's confidence in the policy and, thereby, the authority and utility of a statement.\nConfidence in Policy Makers\nUltimately, the authority accorded to a policy depends on the policy authors' credibility as child health experts. It is valuable for policies to include a concise summary of the breadth of skills and experience represented on the writing team. In general, national specialty societies, such as the AAP, and their policy-writing committees are accorded a high level of credibility by members and other users, because policy authors are considered to have considerable scientific knowledge and clinical expertise and to share and represent the values of stakeholders in the policy.2\nConcerns may also be raised about policy authors' potential biases. Despite the best intentions, policy authors may be influenced—consciously or unconsciously—by financial, personal, and intellectual conflicts in the development of policy. Potential conflicts of interest of all members of the formulating body must be declared. Disclosure of conflicts allows the reader to interpret the policy in light of those potential conflicts and permits other members of the policy-writing team to decide how to interpret contributions from a potentially conflicted team member. Public recognition of potential conflicts may also make policy authors more cognizant of otherwise-unrecognized biases.\nUnderstanding of the Process of Policy Formulation\nAn explicit statement of the purpose of the policy can help users to understand the values applied by the policy authors. For example, if the goal of a policy is to decrease inappropriate practice variation when scientific evidence supports a particular clinical practice, it may be interpreted differently from a policy with a goal of diminishing cost or influencing funding decisions.\nThe process of policy formulation should include a complete review of the available scientific evidence and formulation of guidance based on a combination of evidence and expert consensus. Because the validity of the guidance depends on these processes, an explicit statement of how evidence, expertise, and values were weighed by policy authors can help policy users to understand how best to apply the policy recommendations. The approved process of the AAP for creation of recommendations in evidence-based practice guidelines, for example, calls on policy authors to appraise evidence quality and make an explicit judgment regarding anticipated benefits, harms, risks, and costs.3 These declarations are summarized in a statement of evidence quality and strength of recommendation for each recommendation in a guideline.\nAvailability of Evidence\nFor many situations in pediatric health care, high-quality evidence is not yet available.4 Because evidence is often absent or conflicting, many statements will inevitably be based largely on expert opinion. This is entirely appropriate, provided the basis is readily apparent to the critical reader. Indeed, it is when evidence is lacking, scant, or conflicting that expert guidance is most often sought. In these situations, policy authors must rely on lower-quality evidence, such as reasoning based on basic principles or expert consensus, to formulate coherent recommendations.\nIt is particularly important that users of a policy be aware if the policy relies on lower-quality evidence so they may be alert to the publication of new information and so those to whom the policy is applied are aware of the relatively tenuous state of the supporting evidence. Moreover, an understanding of the quality of supporting evidence should influence the expectations of payers and those who define legal standards of care. When policies are written in a spirit of full disclosure, policy users will be aware of the potential for change when new evidence becomes available and will be more likely to understand, and accept, changes in policy. Moreover, expectations of adherence should be lower when evidence quality is limited or there is a balance between anticipated benefits versus harms, risks, and costs.\nVague Policy Statements\nThoughtfully crafted statements that reflect hours of travail by policy authors may be difficult to put into practice consistently because of lack of clarity.5,6 Ambiguous policy statements are those that are capable of being interpreted in more than one way. It seems obvious that policy statements that are intended to improve the consistency of clinical care should not be ambiguous. Yet, policy implementers regularly complain about the lack of clarity of published policies. A related problem is that authors often deliberately introduce vagueness into policy by using terms with meanings that lack precise boundaries.7\nReasons for intentionally creating vague recommendations include:\ninsufficient evidence (commonly, the scientific literature in pediatrics has not addressed critical topics, or the conclusions of published studies are suspect because of methodologic flaws);\ninability to achieve consensus among the authors regarding evidence quality, anticipated benefits and harms, or interpretation of the published literature;\nlegal considerations (ie, unwillingness to create a potential legal “standard of care”);\neconomic reasons (one approach is clearly best but may not be affordable or cost-effective); and\nethical/religious issues (such as attitudes about the “burden” or “futility” of care, premarital sex, or the use of blood products).\nAn explicit statement of the reasons for writing deliberately vague recommendations can help users interpret and apply them.\nThe Steering Committee on Quality Improvement and Management makes these recommendations to improve transparency and credibility of policy documents while recognizing the challenges involved in their full implementation.\nEnhance the credibility of policy-making groups. Policy-writing panels should seek input from major stakeholders who are likely to apply the policy themselves or be influenced by it and from experts in the area of interest. Policy authors should disclose potential and actual conflicts of interest that might affect their policy writing and describe how they are addressed.\nMake the process of policy formulation clear to users. A statement should include an explicit statement of the reason it was decided to make policy in this area and the intended goals of the statement. Authors should describe how the evidence was selected and assessed and what evidence exists to support the policy. Authors should clearly note when consensus and expert opinion have been required to formulate policy.\nImprove the clarity of recommendations to facilitate implementation. Policy authors should be explicit about the exact circumstances under which a recommended action should be performed (decidability) and should describe precisely how that action should be performed when those circumstances exist (executability). By improving the decidability and executability of policies, such precision would improve consistent application of recommendations. If vagueness is introduced intentionally, the rationale should be documented.\nTrain skilled policy authors. Effectively implementing these recommendations will require training of policy authors in critical appraisal of evidence; differentiating questions of evidence and expertise from questions of value; consensus building; and understanding the policy-formation process. Opportunities for such training should be developed and sustained.\nProfessional societies, government agencies, and other organizations have created a vast array of policies that are widely used and highly valued. When such policies advise actions that have significant clinical and resource implications, it is particularly important that the policy-writing process explicitly recognize uncertainty and include careful literature review; systematic appraisal of evidence quality; weighing of anticipated benefits, harms, and costs; and documentation of the process. Every policy should be subject to a scheduled periodic review with reaffirmation, revision, or retirement as possible outcomes.\nThe solutions proposed in this statement may be applied as new policies are created or when policies are revised. Such gradual implementation will also help to ensure both continuity and accurate transformation to more transparent formats. Ensuring a high degree of transparency by standardizing the process for assessing the quality of evidence and strength of recommendations and defining reasons for deliberate vagueness will facilitate the work of policy developers, achieve better methodologic consistency across the broad range of clinical and policy statements, and thereby sustain and enhance the credibility of an organization's policies.\nSTEERING COMMITTEE ON QUALITY IMPROVEMENT AND MANAGEMENT, 2007–2008\nPRIMARY AUTHORS AND EXPERTISE\nRichard N. Shiffman, MD (general academic pediatrics, medical informatics)\nEdgar K. Marcuse, MD, MPH (general academic pediatrics, quality improvement)\nVirginia A. Moyer, MD, MPH (general academic pediatrics, member US Preventive Services Task Force [evidence-based medicine])\nDaniel R. Neuspiel, MD, MPH (general academic pediatrics, epidemiology)\nElizabeth Susan Hodgson, MD, Chairperson\nGordon Glade, MD\nNorman Harbaugh, Jr, MD\nMarlene R. Miller, MD, MSc\nXavier Sevilla, MD\nLisa Simpson, MB, BCh, MPH\nGlenn Takata, MD\nGregg Lund, DO\nCouncil on Clinical Information Technology\nDenise Dougherty, PhD\nAgency for Healthcare Research and Quality\nEllen Schwalenstocker, MBA, PhD\nNational Association of Children's Hospitals and Related Institutions\nCaryn M. Davidson, MA\nAll policy statements from the American Academy of Pediatrics automatically expire 5 years after publication unless reaffirmed, revised, or retired at or before that time.\n- ↵Essaihi A, Michel G, Shiffman RN. Comprehensive categorization of guideline recommendations: creating an action palette for implementers. Presented at: the annual symposium of the American Medical Informatics Association; November 8–12,2003; Washington, DC\n- ↵Institute of Medicine, Committee on Clinical Practice Guidelines. Guidelines for Clinical Practice: From Development to Use. Washington, DC: National Academy Press;1992\n- ↵American Academy of Pediatrics, Steering Committee on Quality Improvement and Management. Classifying recommendations for clinical practice guidelines. Pediatrics.2004;114 (3):874– 877\n- ↵Moyer VA, Butler M. Gaps in the evidence for well-child care: a challenge to our profession. Pediatrics.2004;114 (6):1511– 1521\n- ↵Tierney WM, Overhage JM, Takesue BY, et al. Computerizing guidelines to improve care and patient outcomes: the example of heart failure. J Am Med Inform Assoc.1995;2 (5):316– 322\n- ↵Codish S, Shiffman RN. A model of ambiguity and vagueness in clinical practice guideline recommendations. Presented at: the annual symposium of the American Medical Informatics Association; October 22–26,2005; Washington, DC\n- Copyright © 2008 by the American Academy of Pediatrics","Citation: The PLoS Medicine Editors (2012) Does Conflict of Interest Disclosure Worsen Bias? PLoS Med 9(4): e1001210. https://doi.org/10.1371/journal.pmed.1001210\nPublished: April 24, 2012\nCopyright: © 2012 PLoS Medicine Editors. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nFunding: The authors are each paid a salary by the Public Library of Science, and they wrote this editorial during their salaried time.\nCompeting interests: The authors' individual competing interests are at http://www.plosmedicine.org/static/editorsInterests.action. PLoS is funded partly through manuscript publication charges, but the PLoS Medicine Editors are paid a fixed salary (their salary is not linked to the number of papers published in the journal).\nAbbreviations: APA, American Psychiatric Association; COI, conflict of interest; DSM, Diagnostic and Statistical Manual of Mental Disorders\nProvenance: Written by editorial staff; not externally peer reviewed.\nThe PLoS Medicine Editors are Virginia Barbour, Jocalyn Clark, Melissa Norton, Paul Simpson, and Emma Veitch.\nOn March 13, 2012 PLoS Medicine published an analysis by Lisa Cosgrove and Sheldon Krimsky  that examined the financial conflicts of interest of members of the American Psychiatric Association (APA) responsible for updating the Diagnostic and Statistical Manual of Mental Disorders (DSM), the so-called bible of psychiatry. Despite a new APA policy designed to address conflicts of interest (COIs), nearly 70% of current DSM-5 task force members have financial relationships with pharmaceutical companies, up from 57% for the manual's previous version. 83% of current contributors to the psychotic disorders section, and everyone responsible for the sleep disorder section, have links to the pharmaceutical industry. Wide media coverage and commentary about these findings – have raised concerns that so many of the experts charged with the responsibility of defining mental health conditions and treatments have financial ties to the very companies that sell drug treatments for mental health. It is widely established that financial conflicts of interest impair objectivity and integrity in medicine.\nConcerns about the conflicts of interests associated with the APA—undeniably the leading authority for psychiatry and mental health—are critical, not least because of the association's legacy of involvement with the pharmaceutical industry: the psychiatric profession receives more money than any other medical specialty  and has recently been scandalized by cases of ghostwriting and publication bias. Their judgments define mental illness, thus legitimizing some disorders and denying others, and determine what warrants treatment and how. The DSM is used by insurance companies, hospitals, courts, prisons, schools, researchers, regulators, and government agencies to define who is sick/abnormal and who is not. The expansion of diagnostic categories and new diagnoses (and thus markets) in every DSM is said to be a virtual “bonanza for the pharmaceutical industry” . And on the other side of the coin, the DSM is a boon for the APA, which sold over a million copies of the DSM-IV; 20% of APA funding is said to now come from pharmaceutical companies .\nCosgrove and Krimsky also identified several worrying gaps in the APA's new COI policy (previous DSMs in 1952, 1968, and 1980 were not subject to COI policies). While the policy limits the amount panel members can receive from drug companies annually to US$10,000 and of their company stock holdings to US$50,000, these are still considerable amounts. (Even small gifts invoke obligations to reciprocate ). Worse, the policy does not consider unrestricted research grants from pharmaceutical companies to be problematic and does not require they be disclosed. Participation in lucrative speakers' bureaus (networks of prominent physicians designed to influence communities of prescribers and usually forbidden in medical schools) is likewise permitted under the APA's policy, and the monies received for participation are required only to be reported as honoraria, thus concealing their true genesis. The APA has responded to the PLoS Medicine analysis by saying that the DSM-5 development process “is the most open and transparent of any previous edition of the DSM” .\nBut are disclosure mandates simply a band-aid on a unrelenting problem of bias?\nDisclosure is generally considered preferable to nondisclosure, because it makes explicit and transparent details that are important to the interpretation, credibility, and value of the information presented—vital in the context of clinical decision-making and patient care. But the overemphasis and reliance on disclosure policies is exactly what leaves the real problem of the conflict of interest unaddressed.\nDisclosure has severe limits as a strategy for mitigating bias. Cosgrove and Krimsky mention three reasons: that disclosure alone merely shifts “secret bias” to “open bias”; that it sometimes involves so much information about ties to the industry, for example, that the reader is blinded by the sheer “signal to noise ratio”; and that disclosure may be perceived as absolving a person from their responsibility for managing their conflict .\nEven more compelling is evidence emerging from the social sciences that suggests disclosure to be not only ineffective but also regressive. Decision scientist George Loewenstein and colleagues have argued that disclosure can actually lead doctors to give biased advice, either through strategic exaggeration (whereby more biased advice is provided to counteract anticipated discounting), or “moral licensing” such that advice is legitimized because advisees “have been warned” (that is, caveat emptor or “buyer beware”) . Their experiments have essentially shown that bias is considerably greater when conflicts of interest are disclosed. Worse, because Loewenstein and colleagues have demonstrated that advisees (i.e., patients) both think that their advisers (i.e., doctors) would never intentionally mislead them and tend not to discount advice in light of conflicts, disclosure policies will never be the solution and are very likely exacerbating the problem of bias in medicine .\nExtending this analysis to the APA's DSM, the result would be disastrous if the public or physicians were to disregard their concerns about financial conflicts of interest in deference to the authority of the APA. And if clinical experts were to believe that disclosure alone made them impervious to bias, their advice forming the DSM may be even more favorable toward the pharmacological products and markets their industry funders seek and uphold.\nIndeed, if disclosure worsens bias, then this is a game-changer for discussion and debate about managing conflicts of interest in medicine. Journals, professional associations, clinical guideline developers, and others need to worry not just that disclosure provides a band-aid to the real problem of the COI itself, but that any attempt to stem the trouble through disclosure policies may actually be worsening the problem.\nWrote the first draft of the manuscript: JC. Contributed to the writing of the manuscript: VB JC MN PS EV.\n- 1. Cosgrove L, Krimsky S (2012) A Comparison of DSM-IV and DSM-5 Panel Members' Financial Associations with Industry: A Pernicious Problem Persists. PLoS Med 9: e1001190.\n- 2. [No author listed] (16 March 2012) DSM-5 Criticized for Financial Conflicts of Interest. ABC News. Available: http://www.12newsnow.com/story/17154951/dsm-5-criticized-for-financial-conflicts-of-interest. Accessed 27 March 2012.\n- 3. Ledford H (16 March 2012) Industry ties remain rife on panels for psychiatry manual. Nature News. Available: http://www.nature.com/news/industry-ties-remain-rife-on-panels-for-psychiatry-manual-1.10206. Accessed 27 March 2012.\n- 4. Aldhous P (13 March 2012) Many authors of psychiatry bible have industry ties. New Scientist. http://www.newscientist.com/article/dn21580-many-authors-of-psychiatry-bible-have-industry-ties.html. Accessed 27 March 2012.\n- 5. Yeung Y (16 March 2012) Study finds ties to drugmakers on mental health panel. UT San Diego News. Available: http://www.utsandiego.com/news/2012/mar/16/study-finds-ties-to-drugmakers-on-mental-health-pa/. Accessed 27 March 2012.\n- 6. Angell M (14 July 2011) The Illusions of Psychiatry. The New York Review of Books. Available: http://www.nybooks.com/articles/archives/2011/jul/14/illusions-of-psychiatry/?pagination=false. Accessed 27 March 2012.\n- 7. Katz D, Caplan AL, Merz JF (2003) All gifts large and small: Toward an understanding of the ethics of pharmaceutical industry gift-giving. Am J Bioeth 3: 39–46.\n- 8. Loewenstein G, Sah S, Cain DM (2012) The unintended consequences of conflict of interest disclosure. JAMA 307: 669–670.\n- 9. Frances A (26 June 2009) A Warning Sign on the Road to DSM-V: Beware of Its Unintended Consequences. Psychiatric Times. Available: http://www.psychiatrictimes.com/print/article/10168/1425378. Accessed 27 March 2012."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:54f1c85b-1983-453c-ad26-d22b5b7f7c46>","<urn:uuid:eb4564c3-164c-4009-aeb4-91c95ce68f75>"],"error":null}
{"question":"I'm interested in wildlife viewing experiences - how do the game viewing opportunities at Hluhluwe Umfolozi Park compare with those at Leshiba Wilderness in terms of animal species diversity?","answer":"While both locations offer diverse wildlife, their species compositions differ. Hluhluwe Umfolozi Park is Big 5 territory, offering professional guided game drives to spot major African wildlife. In contrast, Leshiba Wilderness features leopards, giraffes, zebras, sable brown hyena and various antelope species, but lacks some of the Big 5. Uniquely, Leshiba allows unguided walks among animals like wildebeest, kudu, eland, and smaller species like red duiker, aardvark, pangolins, porcupines and civets. Both locations complement traditional game viewing with rich birdlife.","context":["16 days from £3130pp plus international flights.\nExplore the gorgeous scenery, the rich history and of course the prolific wildlife which goes hand in hand with a visit to KwaZulu Natal! You will get to experience the infamous Battlefields, the Drakensburg, Pongola Game Reserve, beautiful Kosi Bay and finish on a relaxing note by the beach in Umhlanga, Durban.\nHighlights of this trip include the captivating history of the Battlefields, you cannont help but be moved as you hear the stories of the epic battles. You will experience the dramatic scenery of the Drakensberg mountains, take game drives seeking out the Big 5 and visit the north coast of Kosi Bay before venturing south again and finishing off by the Indian Ocean. If you've decided to self-drive you'll be in the driving seat, and you'll have freedom and can set your own pace and stop for photo opportunities whenever you wish. However we can also plan this trip for you with your own driver who'll transfer you from place to place to make it more relaxing.\nThe accommodation we have chosen for you along the way will welcome you and provide the perfect home from home. All have creature comforts that you would expect but each has their own style, personality and ambience. They vary from guest houses to tented lodges to a boutique hotel. all with en-suite accommodation and with enough variety so that you get a taste for this wonderful province.\nIf you like to explore, meet people and experience a broad range of activities, then this could be the perfect holiday for you!\nOn arrival at Johannesburg airport you'll either be met and taken to the car hire office to collect your vehicle, or you'll meet your driver. From here it's a drive of some 4 hours to Three Tree Hill Lodge in the northern Drakensberg region. You'll have time to unpack and relax and settle in to your home for the next 3 nights. Three Trees is a really welcoming place, and your holiday starts here! Full board\nToday you will have a full day to explore the northern region of the Drakensberg and enjoy the stunning scenery of Cathedral Peak and the Champagne Valley. The Drakensberg Mountains are the highest mountains in southern Africa and curve around the border between KwaZulu Natal and Lesotho. The walking and hiking in this area are excellent with some good examples of rock art to be seen. Full board.\nToday we step back in time to what was the bloodiest single day of the entire South African War. Listen to a moving account of how the British attempted to capture Spioenkop on 23 January 1900. Spioenkop was the highest point on the Boer defensive line. Walk in the footsteps of Louis Botha, Winston Churchill and Mahatma Ghandi who all played a role in this battle. Later spend some free time back at this gorgeous lodge. Full board.\nToday you drive on to the Zulu Battlefields through scenic countryside. This area is where the British fought the Zulus in a series of well-documented bloody conflicts.\nYou are staying at a famous place called Fugitive's Drift. This comfortable, family run guest house is steeped in local history. Full board.\nToday you will discover more about the history of the area. From the lodge you can take guided tours of the famous battlefields of Isandlwana and Rorke's Drift. With the help of a knowledgeable guide who talks about each battle, discussing the strategies used by both sides, the numbers who perished and the medals won by the brave soldiers, history and the area come to life. There is time to reflect on the day back at the lodge. Full board\nToday you will drive on, through more South African scenery to the Pongola Game Reserve in northern Zululand, close to the Swaziland border. Dominated by the majestic Lebombo mountains and the shimmering waters of Lake Josini, this is an area of stunning natural beauty with wildlife to match. You will be able to explore this location over the next few days. Settle into your safari tent and have a drink at the bar as you look forward to the safari adventures to come. Full board.\nGet close to nature both on land and in the water with a variety of activities on offer. Take an early morning game drive and then in the afternoon experience the beautiful waters of Lake Jozini either by motor boat or by canoe. Track black rhino with a professional guide, or perhaps head out on foot to look for elephant and see what else can be discovered on the way. Pongola was one of the first proclaimed game reserves in South Africa and is proudly home to 4 of the Big 5 as well as 350 species of bird and numerous antelope. Full board.\nYou will drive to iSimangaliso Wetlands near the coast, your home for the next 2 nights. You will meet a representative from Kosi Forest Lodge at an agreed location, where you leave the car at a secure place and transfer to the lodge in open 4x4 vehicle. It takes around 30 minutes, so enjoy the ride. The lodge overlooks a lake, and has a tranquil and relaxing ambience. Delicious meals are served alfresco either on the deck or under the shade of large albizia trees. You can start to explore straight away if you wish, perhaps with a guided walk, or you may just decide to take things easy. Full board.\nToday you can enjoy the Kosi Bay Nature Reserve to the full. This network of lakes and channels covers a vast area. You can explore the waterways paddling peacefully by canoe with a professional guide to point out the local flora and fauna, or perhaps take a guided walk in the raffia palm forest and admire these enormous trees. Many species of birds make their home amongst these, the largest palms in the world, including the palm nut vulture. Alternatively (as optional extras), explore the area on a full day motor boat trip complete with picnic lunch and a refreshing dip in the sea, or head up to the Kosi Bay mouth for some great snorkelling and views. Full board.\nYour time at Kosi Bay has come to an end. You will be taken back to your vehicle via 4x4 and then it's onward to the beautiful Hluhluwe Umfolozi Park, the heart of Big 5 territory. You will arrive at your accommodation in the afternoon, giving you time to unpack, relax and take in some of the breath-taking scenery of the area. Full board.\nStart the day with early morning coffee, rusks & fruit on the deck then set off with a professional guide for an exciting game drive exploring the private concession and the greater Hluhluwe Umfolozi Park beyond. You will return to the lodge to enjoy a scrumptious breakfast, then take time to relax and perhaps take a well-earned siesta. After some mid-afternoon refreshments, you'll join your guide again for an afternoon game drive in a different area of the concession to see what other wildlife you can find. The guides are hugely knowledgeable and always keen to share their knowledge with you. You will be able to swap stories with other guests at dinner. Full board.\nThe adrenalin portion of your holiday is now over and it's time for some well earned relaxation at the beach, as you head south towards Umhlanga Rocks and your home for the last three nights of your South African adventure. The Oyster Bay is an upmarket hotel boasting its own lighthouse. It's right on the beach and you have the Indian Ocean stretching out before you. Grab a cocktail at sundown and admire the beautiful views. Breakfast.\nYou can opt to spend time at the pool or on the beach, perhaps taking a dip in the sea. You may want to partake in a little last minute shopping for souvenirs, the choice is yours. There are plenty of wonderful restaurants to choose from both in the hotel, nearby in Umhlanga, or in downtown Durban. All are easily accessible from your hotel. You could explore the vibrant market or simply opt for a pampering spa treatment at the hotel's in-house luxury spa. Choices, choices! Breakfast.\nSadly your holiday comes to an end today. It's time to pack your bags and drive to King Shaka International Airport in Durban where you will drop off your hire car. From here we can arrange for you to fly home or you can continue with onward arrangements.\nGuide price from (pp sharing): £3130 (ex. international flights)\nPrice notes: 2017 guide prices pp sharing: Self-drive in LOW season (May to July) £3130 to HIGH season (December) £4500. If you prefer to be use private transfers, the range of prices is from £4520 to £5715 pp sharing.\n15 nights accommodation on twin share basis; meals as specified (12 nights full board and 3 nights bed and breakfast); group C hire car from Johannesburg on day 1 to Durban on day 16 (or private transfers thoughout if you choose that option); 4x4 transfer to and from Kosi Forest Lodge; half day Spioenkop tour, half day Rorkes Drift tour, half day Isandlwana tour, full day Drakensberg tour, two daily activities at White Elephant Lodge, all activities at Kosi Bay and game drives at Rhino Ridge.\nInternational flights; departure taxes; visas; travel insurance; meals not specified; drinks at Fugitives' Drift Guest House; items of a personal nature; tips. Car hire rental deposit, E-tolling fees (Gauteng), traffic fine handling fee, claim handling fee, assessor’s fee, GPS unit, fuel and toll fees.\nGetting there: This holiday starts in Johannesburg and ends in Durban. Return flights from the UK start from around £850, depending on season and availability.\nDeparture dates: This is a tailor made holiday and can begin on the date of your choice, subject to availability.\nComfortable en-suite accommodation owned by the Rattray family, near Rorke's Drift in the Battlefields area.\nKosi Forest Lodge offers comfortable accommodation, good food and a perfect base from which to explore this fascinating area.\nSet on a ridge with far reaching views, this lodge offers elegant accommodation within the Big 5 Hluhluwe-Umfolozi Game Reserve.\nA fine, historic hotel on Umhlanga Beach beside the famous lighthouse, with an impressive array of facilities and stylish accommodation.\nA lovely little lodge in the Drakensberg mountains. A great base for exploring the battlefields and the great outdoors.\nComfy tented accommodation, beautiful views, game viewing by land and water all combine to make White Elephant Lodge a great choice in Pongola Game Reserve.\n5/5 from 61 reviews\nFind the best time to visit South Africa\nTime: GMT +2 hours\nFlight time: There are daily flights from Europe to Johannesburg (about 10 ½ hours direct) and Cape Town (about 11 hours). There is also a good network of domestic flights connecting all main towns.\nLanguage: English is South Africa's main administrative language and is widely spoken. There are also 11 official tribal languages.\nVisas: Visas are not required for British passport holders.\nHealth: No vaccinations are compulsory. Malaria is found in certain areas (mainly Kruger and the east coast towards Mozambique), but not throughout the country.","About Leshiba Wilderness Game Farm\nLeshiba Wilderness - Place of Dreams ...\nFor a magical African experience, visit our Eco-friendly Private Game and Nature Reserve, which stretches along the top of the western Soutpansberg Mountains in the Limpopo province of South Africa. Located in mountainous terrain, the farm is often lost in a morning mist; the high moisture has resulted in rich and varied vegetation, from open plains to natural bushveld and high canopy forest.\nThere are more than 335 species of trees and a wide variety of birds, including black eagles, purple crested and Knysna Turacos and the rare Narina Trogon. The larger game includes leopard, giraffe, zebra, sable brown hyena and numerous species of antelope. Leshiba is best explored on foot where we offer you the unique opportunity to walk unguided amongst our animals wildebeest and the elusive leopard plus kudu, eland and many of the smaller species; red duiker, aardvark, pangolins, porcupines and civets.\nLeshiba offers accommodation in The Venda Village Lodge, Hamasha Bush Camp and Luvhondo Camp - Indigenous Knowledge Centre with both full board and self-catering options.\nThe Venda Village Lodge:\nThe Lodge is an authentic traditional village re-created together with internationally renowned Venda artist, Noria Mabasa. It has 5 en-suite African style huts that open up to sculpted sun-baked courtyards. Facilities Include:\n• Lounge with an indoor fire place\n• Outdoor open fire place\n• Pool deck with a stunning view over the Duluni valley\n• Famous “most delicious bath in Africa”\nThe Lodge sleeps 9 persons sharing. Truly African, truly amazing!\nHamasha Bush Camp offers self-catered accommodation. Surrounded by trees and nestled within a spectacular amphitheatre of high cliffs and mountains, this eight-bedded Camp has a magnificent view through Hamasha gorge and is the perfect getaway for small groups of nature lovers, and the ideal base to discover the flora and fauna of Leshiba on foot along a well-marked network of trails. Camp facilities include:\n• Private small bush camp for maximum of eight persons\n• Fully-equipped kitchen and outdoor braai / barbecue area\n• Comfortable, cozy lounge with open fire place\n• Two units with two en-suite inter-leading bedrooms each\n• Outdoor showers\n• Ideal for small groups and families\n• Remotely situated and offering total privacy and exclusivity\n• No cell reception\nPlease note: 4 x 4 or high clearance vehicle required\nLuvhondo Camp – Indigenous Knowledge Centre:\nLuvhondo is Leshiba’s group accommodation venue. It is situated close to the cliffs with endless views and is ideal for workshops eg. yoga, photography, retreats; small conferences and interest groups eg. hiking, birding, dendrological. All the systems have been designed to be Eco-friendly and self–sustainable, facilities include:\n• Maximum of 28 persons sharing (bunk beds)\n• Excluding the bunks, maximum of 16 persons sharing (2 persons per room)\n• 3x Large rondavels, divided into two (each able to sleep 2-4 persons)\n• Two outside bathrooms with showers & toilets\n• A fourth rondavel divided into two has en-suite facilities, shower & toilet & sleeps 2persons in each room\n• A large open plan thatched communal room with kitchen, lounge and surrounding verandah\n• Outdoor boma\n• Solar powered & grey water is filtered into a dam, which attracts game to the immediate area\n• Wireless internet\nGame drives are available, but the best way to experience all the reserve has to offer, is on foot. There are 12 marked trails from 2 to 5 hours walking time, with paths meandering through lush bush, over plains and down spectacular gorges, where in summer you can swim in clear rock pools.\nFor an experience which reflects the soul of Africa walk along the marked trails, join a game drive or take a horse ride (experienced riders only), within our diverse environment. Visit ancient rock art sites and meander under dense canopies of indigenous forest or hike down rocky gorges."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:f53250a5-7ece-4915-9c8c-b91a40b90f18>","<urn:uuid:bf1406a0-149b-4a0e-9676-607a64fedae7>"],"error":null}
{"question":"As a naval historian, I'm curious about the recovery of historic cannons - can you compare what was found from HMS Victory (1744) versus the Sea Venture (1609)?","answer":"From HMS Victory, two bronze guns were recovered in 2008 - a 42-pounder and a 12-pounder. These were extremely rare finds, especially the 42-pounder which is the only one ever recovered from a shipwreck. Both guns were fully loaded with hemp rope wadding, gunpowder, and cannon balls when found. From the Sea Venture wreck, two cannon guns were recovered much earlier, in 1612, and were subsequently used in British military forts in Bermuda - one was placed at Castle Island Fort and the other at Governor's Island fort.","context":["Sunk in July 1609\nThis ship Sea Venture\nhad actually started the history of Bermuda\nand brought in the first settlers in the island. They were all originally headed for the English colony Jamestown but were forced to land in Bermuda due to a fierce storm. Here is the story of Sea Venture:\nIn July 1609, Sir George Somers had started with a fleet of 9 ships from Plymouth (England) towards the new English colony Jamestown in Virginia. He was carrying settlers and food supplies for the new colony. As a chief commander, he was aboard the lead ship (or the flagship) called the Sea Venture. There were 150 sailors and settlers aboard the vessels, and a dog.\nThe fleet was caught in a severe storm. The sailors were somehow able to spot the east-end reefs of Bermuda (near St. George) and were able to steer the ship towards the rocks. The ship however got wrecked at the reefs off Gate's Bay (i.e. the bay of St. Catherine's Beach\n). But luckily all men on board survived including the dog :-). All came ashore and but few were asked to stay back to retain British claim of the newly found island... they started the first human settlement in Bermuda.\nThe survivors later built two new ships - The Deliverance\nand The Patience\n. The Deliverance was constructed primarily out the materials stripped from the Sea Venture itself. Having built the two new vessels, most of them had set sail again for Jamestown except the few who stayed back.\nAn artist's depiction of Sea Venture caught in a storm\nPainting by Christopher Grimes, Bermuda\nFor a long time this 300 ton single timbered ship of London Company sat on top of the reefs and was largely stripped off its cargo and many other parts over time by the early settlers. However whatever remained finally sank and sat below the sea amongst underwater reefs. But the exact location of the wreck remained unknown for a long time.\nHowever two canon guns were recovered from the wreck in 1612 and used in the eastern-end military forts built by the British in early 17th century. One can be seen at Castle Island Fort\nand the other one at Governor's Island fort located opposite to Paget Fort.\nNearly 350 years after the Sea Venture got wrecked, Edmund Downing set out to find the Sea Ventures\nremains in 1958. He found a wreck in 30 feet of water. The Smithsonian's, Mendel Peterson and the legendary Bermuda diver Teddy Tucker\nwere called to verify the site. They recovered a stone jug, a clay pipe, and a vase and confirmed that they belonged to the period when the original Sea Venture ship had sunk. They also took timber measurements and concluded that this was indeed the vessel that had brought Bermuda's first colonists in the year 1609.\n2) There is a new monument built in St. George Bermuda in memory of the survivors of the Sea Venture. Read Monument for Sea Venture\nto know the details.\nMichael W McClure (September 2018)\nCan the wreckage of the original Sea Venture (1609) be reached by any diving opportunities in Bermuda?\nRaj (bermuda-attractions.com) September 2018\nOriginal Sea Venture wreck is a restricted site and recreational diving is not allowed there. What is available for dive is Sea Venture Ferry wreck at the Eastern Blue Cut (off the north-western coast of the island and quite far away from the original location)... this was a government ferry which was sunk in October 2007 and made into a dive site. It lies at a depth of about 50 ft. The wreck was named after the original Sea Venture but does not have an iota of resemblance with the original one. It's more like a modern ferry that sits upright.\nMonica Hake (February 2018)\nMy ancestors, Samuel Jordan and William Bass also survived the 1609 shipwreck of the Sea Venture and continued on to Jamestown.\nSusan Wellborn (November 2017)\nMy ancestors were among the shipwrecked..\"John Wellborn\" who arrived in Jamestown in 1609.","Alan M. Smith\nOn display in the National Museum of the Royal Navy (NMRN) in Portsmouth, UK, is a mighty 42-pounder bronze gun which was retrieved from the wreck of HMS Victory (1737) in 2008. Also recovered at the same time was a 12-pounder bronze gun which remains in the NMRN Conservation hall. These two guns form part of a full complement of 110 bronze guns which served on the flagship of Admiral Sir John Balchen. Captained by Samuel Faulknor, the Victory was the finest and mightiest First-rate warship in the Royal Navy of the time—and one of the last of the large ships to sail with a full complement of bronze guns.\nHMS Victory was lost with her 1,100 crew in a storm on the English Channel on the night of 3–4 October 1744. She was part of a large Anglo-Dutch fleet commanded by Balchen, returning to England having successfully relieved a vital victualling convoy which had been blockaded by a French fleet at the River Tagus, in Portugal, en route to the Mediterranean squadron. The wreck of the Victory was located in 2008 by Florida-based Odyssey Marine Exploration, lying 75 metres below the surface, around 100 km west of the Channel Islands and approximately 80 km south-east of Plymouth.\nWith permission from the UK Ministry of Defence (MoD), the two guns were brought up from the wreck site and handed over to the Receiver of Wreck in 2009 for the positive identification of the wreck. The NMRN then took custody of the guns and commissioned Mary Rose Archaeological Services to conserve them. The work was carried out at the Royal Armouries workshop at Fort Nelson in Fareham, Hampshire. Conservationist experts from West Dean College of Arts and Conservation, near Chichester, were called in to aid in the preparation of the guns for display after extensive chemical desalination treatment. Whilst undergoing conservation work, the excavation of the interior of the guns revealed that they were still fully loaded—complete with hemp rope wadding, gunpowder, and a cannon ball—and ready to fire. In September 2018, the 42-pounder gun went on public display in the ‘Sailing Navy’ gallery of the NMRN at Portsmouth Historic Dockyard, where it remains today.\nWeighing over three tonnes, with a length of 3.4 m (11.5 ft) and a muzzle diameter of 178 mm (7 in), the gun is a beautiful example of period craftsmanship. It includes elegantly ornate dolphin handles and an intricately depicted Royal Crest of King George I (1714–1727). The 12-pounder gun from the ship bears the Royal Arms of King George II (1727-1760). The reigns of these monarchs allow us to date the guns, but also provide relevant contextual information as regards their acquisition for use aboard Victory. The decision to rebuild HMS Victory to the specification of a 100-gun warship—upon the frame of the old ship of the same name—was made in 1726, and her keel was laid the next year in a Portsmouth dry-dock. However, the official warrant to rebuild her was not issued until September 1733 and she was not re-launched until 1737. As such, the manufacture of the larger gun must have preceded the ship’s construction by a number of years.\nVictory was the last major Royal Navy warship to carry a complete complement of guns cast in bronze, as thereafter cheaper iron gun production gradually superseded bronze manufacture. Since bronze guns from this period were typically melted down for re-use, the guns recovered from the Victory are quite rare. The 42-pounder example is extremely rare—indeed, the Victory is the only shipwreck from which a 42-pounder cannon has ever been recovered. Considered the most powerful and prestigious guns used in naval warfare, the Victory carried twenty-eight 42-pounder cannon along her lower decks. Charles Trollope notes that the Victory is “the only wreck site of a First Rate Royal Navy warship with an intact collection of cannon known in the world”. He claims it is possible that the addition of these long, heavy guns may have contributed to the ship’s sinking in rough seas.\nThe Victory’s guns have been described as “extremely rare examples of hybrid guns designed by Colonel John Armstrong based on the former Borgard system and a master template obtained from the French”. Albert Borgard was a Danish mercenary soldier who joined the English Army in 1692, and in 1716 oversaw the standardisation of artillery. Under his method of reform, guns were thereafter known by the weight of the shot being fired (i.e. 12-pounder, 24-pounder, 32-pounder, etc). Armstrong, as the new Surveyor General of Ordnance in 1722, then redesigned the Borgard system to incorporate further modifications.\nThe base ring of the 42-pounder gun displays the name of the maker, “SCHALCH”. Andrew Schalch was born in Schaffhausen, Switzerland in 1692 and was employed at the cannon foundry in Douai, in northern France, before moving to England. In 1716, he was engaged to build furnaces and other material for the new brass works being constructed at Woolwich. In May 1718, he was appointed as the first master founder to the Royal Brass Foundry in Woolwich (later Woolwich Arsenal) where he served until retiring in 1770. The name “SCHALCH”, together with the year “1723”, also appears on a 24-pounder bronze gun currently held in a museum storage facility in the Netherlands. The MoD maintain that the gun was illegally removed from the Victory wreck site by a Dutch salvage vessel using a hydraulic grab in July 2011 and taken back to Holland. Inter-governmental negotiations are currently underway to retrieve the gun.\nThe Victory shipwreck—described by the NMRN as “probably the best example of the early Georgian period that carried bronze cannons”—remains in situ, where it is monitored against any further excavation or interference following a 10-year legal wrangle over the future conservation of the wreck, involving the Maritime Heritage Foundation (MHF), Odyssey Marine Exploration, and the MOD, and subsequent Judicial Reviews sought by the Joint Nautical Archaeology Policy Committee chair and MHF. According to the findings of the most recent Judicial Review in the High Court (September 2019), the wreck contains “at least 41 bronze cannons, ship-borne artefacts, iron ballast, wooden fixtures and fittings, parts of two anchors and a rudder”. Although claims had been made that some guns have been disturbed, damaged, or scattered by trawling over the years, the latest court judgement ruled that all the artefacts should remain with the ship and that the wreck was at “minimal risk” and could be “appropriately monitored”.\nIn addition to the 42-pounder gun on public display at the NMRN in Portsmouth, there are three other related items held in the National Maritime Museum in Greenwich. The first is a magnificent contemporary full-hull, large scale (1:34.3) model of HMS Victory (1737) once used by the Royal Naval Academy at Portsmouth Dockyard to educate young men entering sea service. The second is a portrait of Admiral Sir John Balchen (circa. 1705) by Jean Baptiste de Medina. Finally, there is an oil painting (circa. 1745) entitled The loss of HMS Victory, 4 October 1744 by Peter Monamy, which depcits the ship in extreme difficulty amongst the mighty waves of the storm that took her. Also of related interest is a magnificent memorial to Sir John Balchen in Westminster Abbey, which was erected by his widow in 1746.\nBaugh, Daniel A. 2004. ‘Balchen, Sir John (1670-1744)’ in Oxford Dictionary of National Biography. Oxford: Oxford University Press.\nBBC (British Broadcasting Corporation). 2010. ‘HMS Victory cannon ‘rare example’, says expert’. Digital edition: 11 October. <https://www.bbc.com/news/world-europe-guernsey-11516163>.\nBBC (British Broadcasting Corporation). 2019. ‘HMS Victory: Ship artefacts to remain on wreck’. Digital edition: 27 September. <https://www.bbc.com/news/uk-england-devon-49854546>.\nCharnock, John. 1795. Biographia Navalis: or, impartial Memoirs of the Lives and Characters of Officers of the Navy of Great Britain from the Year 1660 to the present time, Vol. 1. London: R. Faulder. [Available via the National Museum of the Royal Navy archives, Portsmouth].\nHepper, David J. 1994. British warship losses in the age of sail: 1650-1859. Rotherfield: Jean Bardriot Publications.\nMaritime Heritage Foundation. n.d. ‘The Cannon’. HMS Victory – 1744. <http://www.victory1744.org/thecannon.html#>.\nNMRN (National Museum of the Royal Navy). 2018. ‘Incredibly Rare Cannon from Legendary HMS Victory 1744 Wreck Site is to be Displayed for Very First Time’. 8 September. <https://www.nmrn.org.uk/news-events/nmrn-blog/incredibly-rare-cannon-legendary-hms-victory-1744-wreck-site-be-displayed-very>.\nOsborne, John. 1994. ‘Cannon ball sizes: Borgards standardised ordnance’. NZAR A83. New Zealand Arms Register.\nReynolds, Rebecca. 2019. ‘Judicial review undertaken for HMS Victory salvage’. Institute of Art & Law. 10 April. <https://ial.uk.com/judicial-review-undertaken-for-hms-victory-salvage/>.\nWinfield, Rif. 2007. British Warships in the Age of Sail 1714–1792, Vol. II. Barnsely: Seaforth Publishing.\nWinfield, Rif. 2010. First Rate: The Greatest Warships of the Age of Sail. Barnsley: Seaforth Publishing."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:4ab6fa5f-6297-4075-8b31-713b8e23a1c2>","<urn:uuid:903a0196-39e9-46bd-bed7-1d7a77b92f83>"],"error":null}
{"question":"What are the nutritional benefits of B vitamins for children's development, and what are the health risks associated with B12 deficiency in early life?","answer":"B vitamins are essential for children's growth and development, creating and maintaining energy needed for long school days. They help form red blood cells that carry oxygen throughout the body and are crucial for healthy eyes, skin, hair, and liver. Being water-soluble, these vitamins must be regularly replenished through food. Regarding B12 deficiency risks in early life, infants can experience serious health issues including developmental delays, failure to thrive, megaloblastic anemia, and mobility problems. Early intervention is crucial since neurological symptoms of B12 deficiency can occur even without anemia, and untreated deficiency can lead to nerve damage and cognitive decline.","context":["The best thing you can do for your children's health is to serve well-balanced meals when possible and to encourage your kids to be active. While all vitamins and minerals are important, pay particular attention to foods that are rich in B vitamins because they are key players in a child's growth and development.\n1. Health Benefits\nThe B suite of vitamins are essential to creating and maintaining energy, which is vital for children whose brains and bodies need to go the distance during a long school day. In addition, the B group makes red blood cells, which carry oxygen throughout the body. Every organ and cell thrives on oxygen. B vitamins -- B1, B2, B6 and B12, as well as folic acid -- are essential for healthy eyes, skin, hair and liver. Because the vitamins are water-soluble, the body does not store them, meaning the body replenishes them through foods.\n2. Meats and Fish\nOrgan meats are rich in B1 vitamins, but kids might prefer pork, which is also high in B1. Make pulled pork sandwiches in a slow-cooker using boneless pork chops or a pork roast cut into smaller pieces. Make a perfect mixture of the tangy-sweet signature taste of barbecue using an apple-cider vinegar and brown sugar mixture that's combined with broth and other spices. Serve the pork with good rolls and cole slaw, potato salad or macaroni salad you pick up at the store. Salmon is also full of B vitamins, as are tuna, cod and snapper. Any of these fishes are delicious cooked on the grill or under the broiler and served with a lemon-dill sauce or just lemon wedges. Zucchini squash sautéed with sweet white onion and yellow bell peppers in a light olive oil are a fine accompaniment, along with your favorite salad and dressing. If your kids eat all their fish, reward them with a banana split with frozen yogurt and fresh berries or fruit and nuts, where the banana gives them another vitamin B boost.\n3. Dairy and Eggs\nServe kids flavored yogurt because it's a good source of B vitamins. Top it with granola and fresh fruit or drizzle it with honey for extra flavor and texture. Many yogurts freeze well, so put a few in the freezer for an icy sweet treat that seems as indulgent as ice cream. A glass of skim milk with breakfast or dinner delivers B vitamin-rich food to kids, as does a hard-boiled egg. Add a whole-grain English muffin to the meal to round out the folate, which is also part of the B group.\n4. Vegetables and Legumes\nMany vegetables are good sources of vitamins, including those in the B group. Make large baked potatoes for dinner. Scrub the skins well and soften them with low-fat butter before baking to entice your little ones to eat the skin, which holds even more B vitamins. Top the potatoes with the basics: low-fat butter, cheese and sour cream. Get more creative and use turkey chili, marinara or light ranch dressing. A cold spinach, pesto and green pea pasta salad delivers B vitamins found in the peas and spinach. A colorful three-bean salad or a confetti bean soup adds B vitamins to any meal. Yams -- not sweet potatoes -- are a sweet treat that can be mashed and drizzled with a little bit of low-fat butter and brown sugar for those picky eaters who demand something sweet for their palate.\n- Jupiterimages/Pixland/Getty Images","Vitamin B12, also called cobalamin, is a type of B vitamin that is essential in the human diet. Vitamin B12 plays a role in DNA production, the functioning of the nerves and blood cells, immune health, and cell metabolism. People with B12 deficiency experience a type of anemia called megaloblastic anemia that causes fatigue, weakness, and dizziness. Read on to learn more about vitamin B12, its benefits, and how you can meet your daily B12 needs.\nAbout Vitamin B12\nVitamin B12 is an essential nutrient for the nerves, blood cells, metabolism, DNA, immune system, cognitive function, and much more.\nMost people get enough B12 from their diet, but certain groups—including older adults and strict vegetarians—are at risk of deficiency. People who have low levels of stomach acid or digestive issues are also at risk, because stomach acid breaks down B12 and helps the body absorb it.\nThese groups may benefit from a B12 supplement. Supplements come in two main forms: methylcobalamin and cyanocobalamin. Although cyanocobalamin is the most common form of B12, methylcobalamin is rapidly gaining in popularity as the body may absorb it more easily.\nUntreated, vitamin B12 deficiency can cause anemia, weakness, fatigue, nerve damage, cognitive decline, and intestinal problems.\nVitamin B12 Benefits\nResearch into B12 is still ongoing, although experts know it affects many areas in the body. To date, research has explored the following health benefits of vitamin B12:\nVitamin B12, along with folic acid and vitamin B6, may help the heart. These vitamins reduce levels of homocysteine in the blood, a substance that increases the risk of heart disease, stroke, and other health issues.\nHowever, research is limited and vitamin B12 supplements have not been shown to reduce heart disease risk. However, eating a diet rich in B12 foods may still be beneficial for heart health and other areas of well-being.\nTake advantage of a professional-grade blood pressure monitor for comprehensive heart readings. ( See Product )\nOther ways to look after your heart include eating a balanced diet, exercising daily, and monitoring your vital signs with a blood pressure monitor cuff and a stethoscope.\nMonitor your heart health with a stethoscope made for professional and home use. ( See Product )\nLow levels of vitamin B12 in the body may increase the risk of cognitive issues, including dementia. That’s because people with dementia and related conditions often have high homocysteine levels.\nSupplementation with vitamin B12 (in the form of methylcobalamin) lowers homocysteine levels and could potentially prevent dementia. More research is needed in this area, but it can’t hurt people who have a low B12 status to increase their dietary intake of B12, or take supplements.\nEnergy and Athletic Performance\nHaving a B12 deficiency saps your energy and leaves you lethargic. Taking vitamin B12 supplements is a surefire way to increase your energy levels again, so you can get back to the activities you enjoy.\nThat said, there’s no evidence to suggest that people without a B12 deficiency can enjoy improved energy or athletic performance from taking supplements.\nVitamin B12 Deficiency\nPeople with a vitamin B12 deficiency develop a form of anemia called megaloblastic anemia. This blood disorder results in larger-than-normal red blood cells. Because they are bigger, there are not enough of them.\nWithout enough red blood cells, the body struggles to transport oxygen to its tissues and organs. Also, the cells may be too big to leave the bone marrow to make their way into the bloodstream and deliver oxygen around the body.\nSometimes, people are unaware they have vitamin B12 deficiency because they take in large amounts of folic acid through diet or supplements. High levels of folic acid masks B12 deficiency symptoms by correcting anemia, but not the neurological damage it causes. To avoid this situation, do not take in more than 1,000 micrograms (mcg) of folic acid daily from fortified foods and supplements.\nVitamin B12 deficiency is typically treated with vitamin B12 injections or high-dose oral supplements. If you have a deficiency, your doctor will decide if you can absorb enough B12 for oral supplements to be effective. If not, you may need vitamin B12 shots.\nVitamin B12 Deficiency Symptoms\nSigns and symptoms of megaloblastic anemia and a B12 deficiency include:\n- Balance issues\n- Difficulty maintaining balance\n- Loss of appetite\n- Numbness and tingling in the hands and feet\n- Poor memory\n- Soreness of the mouth or tongue\n- Weight loss\nIn infants, signs of a vitamin B12 deficiency include:\n- Delays in development\n- Failure to thrive\n- Megaloblastic anemia\n- Mobility and movement issues\nIf you are at risk of vitamin B12 deficiency, monitor your health carefully and look for signs and symptoms of low vitamin B12. For example, you may notice the pounds dropping off you thanks to your digital bathroom scale.\nDigital bathroom scales are a quick and easy to to stay on top of your physical fitness. ( See Product )\nOr, you might find yourself relying more and more on your folding cane to address your newly acquired balance issues.\nIf mobility is a challenge, try a folding cane to stay in motion. ( See Product )\nYou should be aware that the neurological symptoms of B12 deficiency can occur even in people who don’t have anemia. Early intervention is crucial to prevent long-term damage.\nCauses of Vitamin B12 Deficiency\nMost people get enough vitamin B12 from their diet, but some people have problems absorbing it. Other people eat diets that are low in, or devoid of, vitamin B12. It is estimated that up to fifteen percent of Americans have a B12 deficiency.\nThe following are risk factors for B12 deficiency:\nOlder adults are more likely than young people to become deficient in vitamin B12. That’s because stomach acid production declines with age, and stomach acid is necessary for B12 absorption. Experts recommend that people over the age of 50 get most of their vitamin B12 from fortified foods and supplements, as they are less able to absorb it from food sources. Some older adults need doses higher than the RDA to avoid deficiency.\nBeing Vegan or Vegetarian\nVitamin B12 only occurs naturally in animal products such as meat and eggs. Those who avoid animal products are therefore more at risk of B12 deficiency. The babies of pregnant or breastfeeding vegetarians or vegans are also at risk unless the mother supplements or eats fortified foods.\nHaving a Digestive Issue\nPeople with digestive problems such as celiac disease or Crohn’s disease may have vitamin B12 malabsorption issues. These conditions reduce the amount of B12 the body absorbs. Learn more about causes of poor digestion.\nHaving Pernicious Anemia\nThose with pernicious anemia have less intrinsic factor in their stomachs. Intrinsic factor is a substance that, along with stomach acid, is necessary for the breakdown of vitamin B12. People with pernicious anemia—approximately one to two percent of older adults—often require B12 shots or high-dose supplements.\nHistory of Stomach Surgery\nIf you have had weight loss surgery or another type of gastrointestinal surgery, you may struggle to absorb B12 as well as you once did. This is because you have fewer cells that secrete stomach acid and intrinsic factor. Your doctor should monitor your B12 status carefully following these types of surgery.\nSources of Vitamin B12\nSources of B12 include certain foods—namely animal products and fortified products, supplements, and B12 shots.\nFoods with vitamin B12 in them include:\n- Meat, including beef liver, beef, ham, and chicken\n- Seafood, such as clams, trout, salmon, tuna, and haddock\n- Fortified breakfast cereals\n- Fortified plant-based drinks such as soy, almond, and rice milks\n- Dairy products such as milk, yogurt, and cheese\n- Fortified vegan dairy alternatives\n- B12-fortified nutritional yeast\nTaking a vitamin B12 spray or capsule is an excellent way to meet your daily needs. You can also get vitamin B12 sublingual supplements, which you can dissolve under your tongue.\nIf you already take a good multivitamin with B12, you may be getting enough from that—check the label to confirm it contains 100 percent of your daily needs. Alternatively, you can take a single B12 supplement or a B-complex that also contains other B vitamins including B6 and folic acid. Make sure you take your supplements every day by keeping them in your pill organizer.\nKeep important medications organized and accounted for with a portable organizer. ( See Product )\nVitamin B12 Injections and Gels\nIf you are severely B12 deficient or have issues absorbing B12, your doctor may recommend a course of vitamin B12 shots. These injections are given into the muscle and help treat or prevent megaloblastic anemia and pernicious anemia.\nA vitamin B12 gel is also available as a prescription medication. This gel is applied in the nose and may be just as effective as shots at increasing levels of B12, although more research is necessary to say for sure. If you are B12 deficient and don’t like injections, ask your doctor if an intranasal gel is right for you.\nVitamin B12 Dosage\nThe amount of B12 a person need depends on their age. Adults and teenagers aged fourteen and over require 2.4 micrograms daily, while children require less. Pregnant and breastfeeding women need 2.6 mcg and 2.8 mcg, respectively, of B12 each day.\nYou can safely take higher doses than this. Being a water-soluble vitamin, the body absorbs what B12 it needs and excretes the rest through the urine. Therefore, it is extremely difficult to achieve a vitamin B12 overdose.\nVitamin B12 Side Effects\nResearch suggests that vitamin B12 does not cause any harm. After all, it is an essential nutrient that our body requires.\nHowever, very high doses of B12 may cause:\nAnother of vitamin B12’s effects is that it can interact with some medications. In some cases, medicines can interfere with how the body uses B12. Medications that interact with B12 in some way include:\n- Aminosalicylic acid, a drug used to treat digestive problems\n- Chloramphenicol, an antibiotic for bacterial infections such as conjunctivitis and meningitis\n- Colchicine, an anti-inflammatory drug used to treat gout\n- Histamine H2 receptor antagonists, such as cimetidine, famotidine, and ranitidine, for peptic ulcers\n- Metformin, a diabetes medication\n- Proton pump inhibitors, to reduce stomach acid (such as omeprazole and lansoprazole), which can reduce B12 absorption\nAlso, vitamin C (ascorbic acid) supplements can reduce the amount of B12 available. To avoid this, take vitamin C at least two hours after taking a vitamin B12 supplement or eating foods fortified with B12.\nIf you are taking other medications, ask your doctor or pharmacist if they interact with vitamin B12 or other supplements you may be taking. If they do, your doctor may recommend using an alternative medication or taking B12 supplements at a different time to your medication.\nEnsuring Optimal B12 Levels for Health and Wellbeing\nClearly, vitamin B12 is an incredible nutrient that does so much for the functioning of our bodies and our overall health. Reduce your risk of neurological issues by ensuring you get enough vitamin B12 every day. If you are vegetarian or are over 50 years of age, take a B12 supplement and eat plenty of B12-fortified foods. Speak to your doctor if have a health condition that may inhibit absorption, as they may recommend you get regular B12 shots or take high-dose oral supplements."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:14adb59e-59f6-47ad-8ee9-df265960d50c>","<urn:uuid:801e9b96-2d4d-4beb-b67f-beb101233182>"],"error":null}
{"question":"As someone new to farming, I'd like to know what's the main difference between production risk in tomato seed imports and production risk in general farming?","answer":"Production risk in tomato seed imports specifically focuses on pest and disease threats, with risks from specific pathogens like Pepino mosaic virus and pospiviroids being managed through testing and treatment protocols. In contrast, production risk in general farming is much broader, encompassing weather, climate change, pests, diseases, and other factors affecting both quantity and quality of products. General farming production risk can be managed through various approaches including controlling/minimizing risk, reducing output variability, or transferring risk through insurance.","context":["We have completed a pest risk analysis for Pepino mosaic virus and pospiviroids associated with tomato seed.\nWhen we do a risk analysis, we:\n- review the science on pests and diseases of concern\n- assess and analyse biosecurity risks\n- develop risk management measures, if required\n- consult the public on the draft report and then review comments\n- publish the final report\n- publish import conditions in our Biosecurity Import Conditions system (BICON).\nAbout the risk analysis\nThe importation of tomato seed is currently subject to emergency measures to manage the risks presented by Pepino mosaic virus and certain viroids (pospiviroids). Australia introduced the emergency measures in June 2008 and revised the measures in February, May and November 2012, and November 2013.\nWe initiated this risk analysis to assess the risks, evaluate the emergency measures, consider ongoing phytosanitary measures and ensure any ongoing measures are technically justified.\nSummary of the final report\nWe released the final report on 26 February 2021.\nThis pest risk analysis supports the risk management measures that are in place for Columnea latent viroid (CLVd), Pepper chat fruit viroid (PCFVd), Tomato apical stunt viroid (TASVd), Tomato chlorotic dwarf viroid (TCDVd) and Pepino mosaic virus (PepMV).\nEmergency measures for Potato spindle tuber viroid (PSTVd) will remain in place while the department continues to evaluate the regulatory status of this viroid.\nRisk management measures\nIn addition to the department’s standard seeds for sowing import conditions, a combination of risk management options is recommended for seeds of Solanum lycopersicum (tomato) and hybrids of this species that includes a test or a treatment for each identified pest:\n- Option 1. Polymerase chain reaction (PCR) test—an option that is applicable to all five quarantine pests.\n- Option 2. Enzyme-linked immunosorbent assay (ELISA) test—an option that is applicable only to PepMV.\n- Option 3. Heat treatment—an option that is applicable only to PepMV.\nYour feedback on the draft report\nBased on stakeholder comments and a review of scientific literature, we have made a number of changes to the risk analysis. These changes include:\n- removal of Tomato planta macho viroid from the risk analysis because there is insufficient evidence that this viroid is associated with the tomato seed for sowing pathway\n- retention of an ELISA test as an option to manage the risk of introducing PepMV.\nDownload submissions on the draft report\nAvailable until February 2022\n|Australian Processing Tomato Research Council PDF||5||213 KB|\n|Australian Seed Federation PDF||9||181 KB|\n|AUSVEG PDF||4||290 KB|\n|International Seed Federation PDF||2||89 KB|\n|Western Australia Department of Primary Industries and Regional Development PDF||5||535 KB|\n|Queensland Department of Agriculture and Fisheries PDF||2||714 KB|\nPublished submissions may not meet Australian Government accessibility requirements as they have not been prepared by us. If you have difficulty accessing these files, contact us for help.\nDownload final report\nDepartment of Agriculture, Water and the Environment, February 2021.\n|Final pest risk analysis for Pepino mosaic virus and pospiviroids associated with tomato seed PDF||102||3.5 MB|\n|Final pest risk analysis for Pepino mosaic virus and pospiviroids associated with tomato seed DOCX||102||1.3 MB|\nIf you have difficulty accessing this file, visit web accessibility for help.\nWe released the Draft pest risk analysis for Pepino mosaic virus and pospiviroids associated with tomato seed on 8 August 2018 for a 60-calendar day stakeholder consultation period, closing on 8 October 2018.\nDownload the draft report\nDepartment of Agriculture and Water Resources, August 2018.\nAvailable until February 2022\n|Draft pest risk analysis for Pepino mosaic virus and pospiviroids associated with tomato seed PDF||129||2.6 MB|\n|Draft pest risk analysis for Pepino mosaic virus and pospiviroids associated with tomato seed DOCX||129||1.7 MB|\nIf you have difficulty accessing these files, visit web accessibility for assistance.\nTomato industry in Australia\nAustralian producers rely on the overseas supply of seed for tomato crop production. In 2018-2019, Australia’s production of tomato vegetable was valued at $674.2 million.\nSource: Horticulture Innovation Australia—Australia Horticulture Statistics Handbook (2018-19)\nNotice of the implementation of the revised import conditions will be given on the Biosecurity Import Conditions system (BICON).\nRegister as a stakeholder\nSubscribe to the Biosecurity Risk Analysis Plant to receive notices about plant biosecurity policies.\nFor more information, email imports or phone 1800 900 090 (option 1, option 1).","By Erin Murphy, Statewide Education Coordinator\nFarming is risky business! Learn about the different ways farmers can manage risk to keep doing what they love and do best.\nWhat is risk?\nRisk is the chance of a loss, or other bad consequence, associated with any decision. Each decision we make has a risk-return tradeoff where the amount of risk is positively correlated to the amount of potential return. In other words, the riskier a decision, the larger the potential payout and vice versa. Uncertainty is defined as not knowing what will happen in the future and increases risk. Risk management involves finding the balance between the risk of a loss and the potential for profit.\nCalculating and avoiding risk\nTo calculate risk, ask yourself:\n- Is there a good chance of bad consequences? This will help you measure the likelihood of the risk. A “yes” is a red flag, and means that a bad outcome is likely.\n- Would the bad consequences drastically disrupt the farm business? This will help you measure the magnitude of the risk. Another “yes” would be another red flag, and means that the consequences would be severe.\nIf you answered “yes” to both questions, there is a good chance of a bad outcome that will drastically harm your farm. That’s a decision you probably don’t want to make!\nThe best way to avoid a bad outcome is to consider all alternatives when making a major decision. We are often quick to assume that there are only one or two solutions, but these solutions are typically the most extreme courses of action falling on either end of the risk-return tradeoff line. Push yourself to consider all alternatives — the goal is to find solutions that balance risk and return and fall somewhere in the middle of the risk-return tradeoff line. As a business owner, it’s essential that you know your personal risk tolerance to guide your decision-making and help you make a realistic risk management plan. Investment surveys can be used as a good proxy for whether you are risk averse or risk seeking.\nWhat is crop insurance?\nCrop insurance can provide protection from production loss, price decline or a combination of both. Crop insurance plans can be based on production, revenue or the farming area. Production and revenue plans are based on the farmer’s actual production history while area plans are based on averages in a specified farming area. Insurable causes of loss include adverse weather conditions (drought, flooding, frost), fire, insects, plant disease and wildfire.\nTwo programs most relevant to small and mid-sized diversified farms are the Whole-Farm Revenue Protection Program (WFRP) and the Noninsured Crop Disaster Assistance Program (NAP).\nWhole-Farm Revenue Protection Program\nThe Whole-Farm Revenue Protection (WFRP) program is the first crop insurance product available in every county nationwide and is available for any farm with up to $8.5 million in insured revenue. It insures all crops on the farm under one policy, meaning that individual crop losses are not considered. Instead, the overall farm revenue is used to determine losses. A farm or ranch may have up to $1 million in expected revenue from animals and animal products. A qualifying farm must provide five consecutive years of Schedule F or other farm tax forms. Qualifying beginning farmers or ranchers (“an individual or entity who has not operated a farm or ranch, or who has operated a farm or ranch for not more than 10 consecutive years”) may qualify with three consecutive years of Schedule F forms.\nCoverage levels range from 50-75% and are chosen by the farmer. The cost of the insurance program or premium varies depending on the county, coverage level and the diversity of crops, and is subsidized by the federal government. WFRP is designed to reward diversification so farms with a qualifying commodity count of three or more (meaning that you produce three or more individual crops that make up a substantial amount of your total farm revenue) can select coverage levels at 80-85% as well. The chart above shows the percentage of the total premium that is paid by the government based on the coverage level (selected by farmer) and the qualifying commodity count (determined by crop insurance agent). For example, at the 75% coverage level, a farm with a qualifying commodity count of two will have 80% of their premium paid for by the government. Qualifying beginning farmers and ranchers are eligible for an additional 10% discount on their premium.\n- Sales Closing, Cancellation and Termination Due Dates\n- Calendar and early fiscal year filers: January 31, February 28 or March 15 (by county)\n- Late fiscal year filers: November 20\n- Revised Farm Operation Report Due Date\n- All filers: July 15\n- Contract Change Date\n- August 31\nKey dates for WFRP are included in the table above and are based on your taxes. For the specific dates that apply to your county, talk to your local crop insurance agent. Premiums are not paid when you sign up for the plan, but rather in September of the year of coverage. The USDA’s Risk Management Agency (RMA) manages the Federal Crop Insurance Corporation (FCIC) which provides crop insurance products to farmers and ranchers. Approved Insurance Providers (AIP) sell federal crop insurance products through a public-private partnership with RMA.\nNoninsured Crop Disaster Assistance Program\nNoninsured Crop Disaster Assistance Program (NAP) is administered by the USDA’s Farm Service Agency and provides financial assistance to producers of non-insurable crops to protect against natural disasters that result in lower yields, crop loss or prevented plantings. Eligible crops are those not already covered by another federal crop insurance program. They include crops grown for food or fiber, floriculture, grain and forage crops for animal consumption, and yield-based and value-based crops. (For a comprehensive list, contact your local FSA office.) Eligible causes of loss are generally damaging weather or natural occurrences including drought, hail, excessive moisture, freeze, tornado, hurricane, excessive wind, earthquake, flood, volcano and insufficient chill hours.\nNAP offers two types of coverage: basic coverage and buy-up coverage. Basic coverage covers 50% of your crop and is covered at 55% of the average market value. There are no premiums for basic coverage, but the service fees are $325 per crop, not to exceed $825 per county and not to exceed $1,950 per producer. All service fees are waived for qualifying beginning, minority or limited resource farmers and ranchers. Buy-up coverage covers 50-65% of the crop at 100% of the average market value but has a premium in addition to the service fees. Premiums are due in early January the following year after signing up for coverage. Qualifying beginning, minority or limited resource farmers and ranchers are eligible for a 50% discount on premiums.\nApplication closing dates vary by crop so contact your local FSA office for specific questions. Coverage periods begin the earlier of 30 days after you file for coverage or the day you plant the crop and last through the day the crop is harvested, destroyed or abandoned. For perennial crops, the coverage period ends 10 months after the application closing date.\nHow can I manage other types of risk?\nRisk on the farm can generally be broken down into five types:\n- Production — refers to the variability in actual outcomes versus your expected outcome and includes uncertain production-related activities. Examples include weather, climate change, pests, diseases, and other factors that affect quantity and quality of your products. To manage production risk, you can control or minimize the risk, reduce the variability in your output, or transfer the risk.\n- Marketing — arises due to all the factors that affect your marketing channels such as consumer preferences, weather, government actions, price and currency variability, cost of inputs, limited market outlets, and global economic conditions. The marketing chain is the process of adding value to your product through processing, wholesaling, and retailing. Marketing risk can be mitigated through the development of a robust marketing plan.\n- Financial — refers to the cost and availability of loans, your ability to pay your bills, absorb short term financial shocks (like a wet spring), and maintain and grow your business. Rising interest rates and increasing costs in family living are other sources of financial risk. The use of financial statements and ratios can be helpful in monitoring your farm’s financial status to keep you on top of any potential risks that get thrown your way.\n- Legal — include governments changing laws, regulations, and policies, and any dispute or disagreement between individuals and/or groups. The legal system of laws, courts, and contracts is designed to address these risks. Everything from the way you decide to organize your business (e.g. sole proprietorship, LLC, etc.) to your farmland lease agreement can be potential sources of legal risk. Need legal advice for your farm? Farm Commons out of the Hudson Valley is a great resource to get you started.\n- Human — the uncertainty and imperfection of being human and having relationships that affect the farm or ranch’s success. Some examples include health and well-being, family and business relationships, employee management, and transition/succession planning. Human risk may be the least technical type of risk, but it can be the trickiest to navigate due to the unpredictable nature of human beings. Some ways to mitigate human risk are having employee handbooks and standard operating procedures that lay out expectations and promote health and safety, practicing work-life balance, and doing check-ins and regular meetings where all members of the team are able to share their successes and challenges.\nFor more information on the five types of risk, check out “Introduction to Risk Management: Understanding Agricultural Risks.”\nThis project is funded in partnership by USDA, Risk Management Agency, under award number RM16RMEPP522C020. This institution is an equal opportunity provider."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:ac7ad416-28dc-4502-b93f-827eff4813c5>","<urn:uuid:ca42f6f5-6cef-4118-9e19-66913065d81a>"],"error":null}
{"question":"What are the key technological features of modern laser displays, and what safety concerns need to be considered when operating them?","answer":"Modern laser displays use techniques like projecting light onto liquid mediums and creating microbubbles that act as voxels (3D pixels) to create three-dimensional images. One example is a system using a column of glycerin where laser-created bubbles stay in place to form static 3D images. Regarding safety, these systems pose several hazards that require careful attention. They can produce internal voltages exceeding 25,000 volts, may use compressed gases, and emit intense radio frequency energy that can cause severe burns. Additionally, there are risks from both diffuse and specular light reflections, with the latter being more dangerous as it can recreate nearly 100% of the original light. Eye and skin protection is crucial, as laser exposure can cause effects ranging from mild skin reddening to serious retinal damage.","context":["Researchers really want to make good three-dimensional displays. It is so much harder than you might think—at this point, we have lots, but most attempts are literally smoke (light projected and scattered on some medium) and mirrors (optical illusions). There aren’t many examples of voxels, 3D pixels, lined up to create an image that you can walk around and see clearly from all sides.\nOne Japanese team has released an interesting but rudimentary proof-of-concept for their own so-called volumetric display, using lasers projected in a liquid column. Ultimately, the team hope to create an updatable 3D projection visible from all angles to put on display in a museum or aquarium.\n“To develop a volumetric display of the kind we see in science fiction movies is a dream of we display researchers,” Kota Kumagai, researcher at the Center for Optical Research & Education (CORE) at Utsunomiya University in Japan, wrote in an email to Gizmodo. “I think it will take several years until we will be able to see as in sci-fi movie.”\nAnd Kumagai really wants to create a working 3D display. He and his team came out with one prototype—a laser scattering off of a stack of fluorescent screens—back in 2015, for example.\nHis newest idea came after trying to build a screen based on a fluorescent liquid that can light up. Kumagai and his collaborators realized increasing the laser’s energy created bubbles in the fluid. Their new screen uses a column of viscous liquid, in this case glycerin, so the bubbles will stay in place and become voxels. An illuminating light shined onto these voxels scatters to become the image. The researchers used this method to create images of a mermaid, dolphin and rabbit in the liquid, and published their results in the journal Optica today.\nWhen we say this display is a “proof of concept,” we mean it. The screen only creates small graphics, and though the team says they can do full-color images, everything presented in the paper was single-color. Plus, this display isn’t a video display—the produced image is stationary. An updatable version would require a stream of liquid to move or pop the bubbles. Essentially, it’d be like an updatable version of those 3D glass etchings you can buy in tourist gift shops, but with a vat of glycerin and lasers. There are a lot of better ways to make a stationary 3D image... like a sculpture.\nWhy go through the trouble of creating a display like this when we have so many other ideas for 3D-screens? Well, every screen seems to have some drawback that makes it not truly volumetric, or sacrifices detail. This makes a lot of sense. How can you make a screen opaque enough so you can’t see behind a voxel, but transparent enough so that you can see through the screen? How do you stop a beam of light in a precise location, but change that location over the course of lots of frames, to make a movie?\nResulting images often have obviously visible voxels, and look like an image made from lots of dots. Others are only three dimensional from a certain viewing spot, like 3D televisions. There are bulky ones requiring lots of projectors or strange-shaped screens. Some are dim and use stacks of screens with a projector, like the thousand dollar Volume screen, or the screen Kumagai made back in 2015. Kumagai’s current screen is sort of a combination, voxels made with a laser for light to scatter off of. We’ve reached out to several scientists to see if there are any other drawbacks we should be aware of.\nSo, will microbubbles be the future of volumetric displays? Maybe! Based on the method that the screen needs to be refreshed, it’ll probably only be used for projecting single images, rather than video. But if Kumagai’s team finds a way to turn these illuinated vats of glycerine into high-definition images that can be updated with a stream of fluid, they might make an appearance in public spaces in the future. It’s certainly a wild idea.","With the constant changes coming to the industrial and manufacturing sectors, and advanced equipment like laser marking systems becoming increasingly prevalent, it’s more important than ever to be aware of laser marking safety procedures.\nThese procedures range from ways to protect from bodily harm while utilizing laser marking systems to ways to avoid electrical dangers on the factory floor. It’s important to know what all of these dangers are and how to train yourself and your staff in essential laser safety.\nCommon Laser Hazards\nOne aspect to remember about laser marking safety is the variety of hazards associated with the systems which are also prevalent in many other forms of equipment. Basic safety training should mean workers are already familiar with some of these issues, but employers should be aware of:\n- High voltage\n- Compressed gasses\n- Intense radio frequency energy\nLet’s take a look at each of these individually:\n- For concerns regarding high voltage, it’s important to know that pulsed CO2 lasers can produce internal voltages exceeding 25,000 volts while containing large capacitors that can deliver in excess of 200 Joules of energy.\n- Some pulsed lasers also utilize a flowing gas design, which requires connection to a cylinder of compressed gas. Most of the gases used for laser systems are extremely safe, though since pressurized cylinders can pose a risk, their transportation and usage should occur only when the cylinders are properly restrained.\n- As for radio frequency energy, severe burns can result from the improper servicing of units which use RF generators, such as sealed CO2 lasers. Anyone servicing such units should be trained in the unit’s safety procedures, and the connections for RF energy should not be touched during operation.\nDue to the laser engraving safety checks in place for marking systems, units are designed in such a way to prevent operators from coming into direct contact with the laser beam.\nOne of the issues that can arise, however, involves unintentional reflected light, which falls into the following categories:\n- Diffuse Reflections – This type of reflection occurs when a reflective surface’s irregularities create a scattering of light in all directions. Of the types of reflections, this is the safer of the two since energy is being divided in many directions and weakened.\n- Specular Reflections – This type of reflection is produced in a way that is more mirror-like, recreating close to 100% of the original light compared to the scattered light of diffuse reflections. Though there is a higher danger associated with this form, it is much less common and laser marking systems are often designed to eliminate specular reflective surfaces that would come across the beam’s path.\nWith proper safety procedures in place, and by following guidelines which accompany your laser marking systems, these hazards should be greatly minimized.\nSkin and Eye Hazards\nDue to laser engraving safety checks in place, the effects of lasers on skin are usually considered of secondary importance. Compared to laser marking and engraving systems, high powered infrared lasers utilized in applications for welding and cutting pose a much greater risk of injury.\nSome potential effects that can occur if safety procedures are not followed include mild reddening, blistering, and charring, all of which are usually reversible or repairable. Other potential issues could include ulceration, scarring of the skin, depigmentation and damage to underlying organs.\nIn regards to eye safety, some forms of laser beams operate at a wavelength which if exposed to the eye can pass beyond to the retina. With no pain indicators present and the beam being invisible, someone exposed can be unaware at the time of occurrence.\nThis is just another reason why utilizing industry-standard laser safety glasses is so vital to maintaining a safe and healthy workspace.\nAn additional hazard that can be posed when using laser systems is that of electrical shocks. Some of the ways shocks can occur is through contact with:\n- Exposed utility power utilization\n- Device controls\n- Power supply conductors operating at potentials of 50+ volts\nOperators and service workers for laser marking systems must be especially careful to avoid these dangers by following standard safety protocols. These hazards could be experienced during:\n- Set-up and installation\n- Maintenance and service\n- Troubleshooting, if equipment protective covers are removed\nThe electrical injuries that can be sustained from improper and haphazard use of laser systems can include everything from a slight tingle to serious bodily injury to death. To avoid accidental contact with these energized conductors, laser marking units utilize a barrier system within the equipment.\nIn the United States, laser systems are overseen by the Center for Devices and Radiological Health (CDRH), which is a component of the FDA. Depending upon the risk of a specific type of laser, it is graded from Class I to Class IV. Laser marking lasers tend to be graded as Class IV lasers, making laser marking safety all the more important.\nTwo important additional classifications to keep in mind are the Maximum Permissible Exposure (MPE) and Nominal Hazard Zone (NHZ) designations.\n- Maximum Permissible Exposure – This is the highest level of laser radiation an individual can be exposed to without receiving harmful physical effects to the eye or skin. Control measures for laser marking systems are in place to ensure any laser radiation emitted falls below the MPE.\n- Nominal Hazard Zone – To create conditions for flexibility in laser marking applications, the laser beam of the unit is often not fully enclosed for practicality reasons. In these instances, it is necessary to define an area of potentially hazardous laser radiation, which is the NHZ. Control measures are then put into place for this area to ensure safety during use of the laser system.\nBeing fully aware of these classifications and their rationales are vital to ensuring proper workplace laser marking safety.\nLearn More About Laser Engraving Safety Today\nFor a more detailed look at laser marking safety information, be sure to visit our laser safety manual, which includes graphics, information about warning labels, and procedural controls to utilize.\nAnd if you have further questions for our team, or would like to know more about our premier line of laser marking systems, be sure to contact us today.\nLaser marking and engraving has a wide variety of applications for businesses and hobbyists, with everything from the medical field to the everyday carry subculture and more getting real benefits from laser marked parts.\nAt TYKMA Electrox, we design and manufacture industrial laser marking machines for applications across an array of industries. When it comes to deep laser engraving, our systems serve as cost-effective solutions for your marking needs. From standard products to custom-made machines, our team strives to make sure that you are receiving the best product available. […]\nLaser Marking in the Athletic Industry The market for the laser engraving industry has expanded significantly over the past few years. What used to be thought of as a process used only by industrial companies has broadened to reach businesses across many diverse fields. With countless applications and easy-to-use systems, laser marking has become a […]\nAt TYKMA Electrox, our laser systems are used to personalize a wide variety of everyday carry items. From knives to firearms, our lasers are built to intricately engrave your personalized designs or logos onto your gear. The systems we provide are even used for laser marking gun cases. If you’re interested in adding personalization to […]"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:fca458b1-eb7e-4270-b4ec-816e37a3b8f8>","<urn:uuid:3c01b6d6-eff9-454b-bb0e-5715e14d0b14>"],"error":null}
{"question":"As a geology student, I'm curious how Zealandia and Antarctica compare in terms of their size and geographical characteristics?","answer":"Zealandia has a surface area of about 2 million square miles (5 million square kilometers), which is approximately half the size of Australia. In contrast, Antarctica is significantly larger, being the fifth largest continent with a total surface area of about 14.2 million square kilometers in summer. Zealandia is mostly submerged, with 94% of its land mass underwater beneath the Pacific Ocean, while Antarctica is a vast ice-covered landmass. Antarctica's size actually varies seasonally, doubling in winter due to sea ice formation around its coasts, while Zealandia's boundaries remain constant beneath the ocean.","context":["Earth’s eighth continent is 94% underwater and 100% awesome.\nEarth’s mysterious eighth continent doesn’t appear on most conventional maps; that’s because almost 95% of its land mass is submerged thousands of feet beneath the Pacific Ocean.\nZealandia — or Te Riu-a-Māui, as it’s referred to in the indigenous Māori language — is a 2 million-square-mile (5 million square kilometers) continent east of Australia, beneath modern-day New Zealand. Scientists discovered the sprawling underwater mass in the 1990s, then gave it formal continent status in 2017. Still, the “lost continent” remains largely unknown and poorly studied due to its Atlantean geography.\nNow, GNS Science — a geohazards research and consultancy organization owned by the government of New Zealand — hopes to raise Zealandia (in public awareness, at least) with a suite of new maps and interactive tools that capture the lost continent in unprecedented detail.\n“We’ve made these maps to provide an accurate, complete and up-to-date picture of the geology of the New Zealand and southwest Pacific area — better than we have had before,” Nick Mortimer, a geologist and lead author of the maps, said in a statement. “Their value is that they provide a fresh context in which to explain and understand the setting of New Zealand’s volcanoes, plate boundary and sedimentary basins.”\nThe new maps reveal Zealandia’s bathymetry (the shape of the ocean floor) as well as its tectonic history, showing how volcanism and tectonic motion have shaped the continent over millions of years. Data for the bathymetric map was provided by the Seabed2030 project — a global effort to map the entire ocean floor by 2030. (The project is about 20% complete.)\nThe team also released interactive versions of both maps on a new Zealandia webpage. Spend a few minutes clicking around the hyper-detailed images — and, when someone asks what you’re doing, simply tell them you’re “discovering Earth’s lost continent.”\nAbout 3,500 feet (1,000 meters) below the Pacific Ocean, off the coast of Australia, there lies Zealandia, the lost eight continent that has long fascinated scientists since it was identified in 2017.\nNow, researchers from New Zealand were able to map the submerged landmass in impressive detail, offering a more accurate image of the submerged continent than we’ve ever seen.\nThe researchers from GNS Science mapped the shape and size of the continent in unprecedented detail, making the maps available to everyone on an interactive website. They mapped the bathymetry surrounding Zealandia, the shape and depth of the ocean floor, as well as its tectonic profile, illustrating where the continent falls across the limits of tectonic plates.\nThe maps revealed brand new information on how Zealandia formed before it became submerged underwater millions of years ago. The area is has a surface of some two million square miles (five million square kilometers), which is half the size of Australia. But only 6% of it is above sea level, which underpins the north of New Zealand, and the rest is underwater.\nMortimer and his team mapped both Zealandia and the submerged continent to raise awareness about it, but also to better understand the continent. The bathymetric map (below) shows how high the continent’s mountains and ridge rise toward the water’s surface. It also shows coastlines, territorial limits, and the names of major undersea features. The map is part of an initiative to map all the ocean floor of the planet by 2030.\nA second map done by the team (below) shows the types of crust that make up Zealandia, the age of the crust, and the major faults. The continental crust, which is Earth’s older and thickest crust, can be seen in the map in red, orange, yellow, and brown. Meanwhile, the oceanic crust can be seen in blue. The volcanoes are illustrated with red triangles.\nGeophysicist Bruce Luyendyk came up with the moniker Zealandia in 1995. Speaking to Business Insider, he said he didn’t intend to describe a new continent. Instead, the term originally referred to New Zealand and a group of submerged pieces of crust that separated from the ancient supercontinent Gondwana about 85 million years ago.\nGondwana was formed when Earth’s ancient supercontinent, Pangea, split into two fragments. Laurasia was transformed into North America, Asia, and Europe, while Gondwana became Africa, South America, Australia, and Antarctica. But land masses continued to be rearranged afterward, with Zealandia breaking off Gondwana.\nZealandia was classified as a “microcontinent,” as the island of Madagascar, until 2017. But according to Mortimer, it has all the requirements to be classified as a continent. It has defined boundaries, it occupies an area of over one million square kilometers and is elected above the ocean crust.\nThe new maps offer more evidence Zealandia should be considered the eighth continent, Mortimer added. “If we could pull the plug on the world’s oceans, it would be quite clear that Zealandia stands out,” he told Science News in 2017, adding, “If it wasn’t for the ocean level, long ago we’d have recognized Zealandia for what it was — a continent.”","Animals in the Antarctic Ice. 25 Facts About Antarctica That Are So Cool They're Freezing. KS3 Bitesize Geography - Antarctica : Revision, Page 7. Antarctica Facts: 38 Facts about Antarctica. Antarctica Fact File, What is it like in Antarctica, Antarctic environment 1. It's cold, but you guessed that already, it's also the highest and windiest continent.\nWhat are the details and other than this, how cold is it really and why is it the way it is? - page 2 Where is Antarctica? How big is it? Antarctica is the fifth largest of the seven continents. The nearest other land masses are South America 1000 km (600 mls) away across the roughest stretch of water in the world - the Drake passage, Australia is 2500 km (1550 mls) away, and South Africa 4000 km (2500 mls) away. The total surface area is about 14.2 million sq km (5.5 million sq mls) in summer, approximately twice the size of Australia, half as big again as the USA and fifty times the size of the UK. In the winter Antarctica doubles in size due to the sea ice that forms around the coasts.\nWhy is Antarctica so cold? This means that at the poles the available sunlight and heat is spread over a greater area. 2/ Temperature falls as altitude increases at the rate of about 1C per 100m. Lichen-covered rock Moss. Log in - Oddizzi. Fun Antarctica Facts for Kids - Interesting Information about Antarctica. What Is Antarctica? Antarctica is Earth's fifth largest continent.\nCredits: NASA Icebergs form on ice sheets in Antarctica. Penguins live in Antarctica. Scientists find a meteorite in Antarctica. This article is part of the NASA Knows! Antarctica is a continent. What Is Antarctica Like? Antarctica has two seasons: summer and winter. Antarctica is a desert. Antarctica has no trees or bushes. Who Lives in Antarctica? What Can NASA Learn About Earth From Studying Antarctica? One tool that NASA uses is ICESat. NASA instruments have also helped scientists create detailed maps of Antarctica. What Can NASA Learn About Space From Studying Antarctica? NASA sends teams to Antarctica to learn more about the planet Mars. NASA also goes to Antarctica to study astronaut nutrition. More About AntarcticaNASA LIMA: Faces of Antarctica ICESat Meteorites From Antarctica Read What Is Antarctica? Return to NASA Knows! Return to Students K-4. Antarctica: Facts About the Coldest Continent. The coldest, windiest, and driest continent, Antarctica contains 90 percent of all of the ice on the planet in an area just under one and a half times the size of the United States.\nLet's take a look at one of the world's most desolate regions. Antarctic climate Lying in the Antarctic Circle that rings the southern part of the globe, Antarctica is the fifth largest continent. Its size varies through the seasons, as expanding sea ice along the coast nearly doubles its size in the winter. Supporting New Zealand’s activities in Antarctica. Antarctic. Antarctica is a vast ice-covered landmass surrounded by sea (unlike the Arctic, which is an ice-covered ocean surrounded by land).\nIt’s bigger than Europe, and in summer, it's still 62 times the size of the UK! Antarctica is the world’s highest, driest, windiest and coldest continent. Its record low temperature is -94°C. But it doesn’t actually snow much – the Antarctic is so dry it’s classed as a polar desert. And it’s in darkness part of the year. Not surprisingly there are no people permanently living or native to Antarctica – although there can be up to 5,000 scientists and researchers based there (including teams supported by WWF studying Emperor and Adélie penguins). The Antarctic is one of the world’s least disturbed places, but it’s increasingly vulnerable, especially to global warming and climate change. Antarctica: Importance.\nPockets on this berg were made under the water and then it flipped over.\nVery old ice is a unique blue color. Fantastic icebergs change with each season. Erebus disaster - Erebus disaster. Impacts of climate change - Discovering Antarctica."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b1e7f64f-7ce9-4ffd-b966-3219899df6dd>","<urn:uuid:a06d6473-90cd-4b09-b89a-642c17878e45>"],"error":null}
{"question":"I'm interested in a career change - can you compare the educational requirements for becoming a genetic counselor versus working at a genetics service department in a hospital?","answer":"A genetic counselor requires specific educational credentials: a bachelor's degree (typically in biology, chemistry, psychology, or counseling) followed by a master's degree in genetic counseling from an ABGC-accredited program. They also need to obtain the Certified Genetic Counselor (CGC) designation. In contrast, the genetics service departments at hospitals employ various professionals including board-certified medical geneticists, genetic counselors, registered dietitians, social workers, and child life therapists. All genetic counselors must be certified by the American Board of Genetic Counseling and licensed in their state (such as New Jersey).","context":["Joseph M. Sanzari Children's Hospital\nGenetics and Genetic Counseling\nProviding You the Information You Need to Make the Best Possible Decisions\nThe Genetics Service is a leader in clinical genetics in the metropolitan area and has been in existence for more than 30 years, offering genetic services and testing for both children and adults. All patients are seen by board-certified medical geneticists and/or genetic counselors in the areas of preconception/prenatal, cancer, pediatric, and adult genetics. The Genetics Service is also a NJ Department of Health-designed newborn screening follow-up center. Patients can also benefit from the expertise of the physicians and clinical experts of Hackensack University Medical Center if any further treatment is required.\nPatients are evaluated for risk assessment, testing, and management, as well as supportive counseling. Our goal is to improve the lives of our patients by providing them with the important information they need to make the best possible decisions. More than one session may be required. The initial session includes a review of the patient’s medical history as well as three generational family history. Clinical impressions and testing recommendations will be discussed. Laboratory tests such as blood tests, urine tests, imaging studies (x rays, CT scans, MRI) or EKG may be necessary. Referrals may be made to other specialists. Follow up appointments are scheduled to review the results of the testing as needed. If a diagnosis is made, medical management recommendations are made.\nOur staff includes board certified physicians, board certified/eligible genetic counselors and a registered dietitian, Social workers and child life therapists are available as needed as well.\nGenetic Counselors have a master’s degree in genetic counseling and are certified by the American Board of Genetic Counseling and licensed in the state of New Jersey.\nTo schedule an appointment please call:\nMonday – Friday 9:00AM – 5:00PM\nPrenatal and Preconceptual\nHereditary Cancer Risk Assessment\nGeneral Office Number\nYou will be asked to send us a copy of the patient’s pertinent medical records before your visit. These would include notes from previous visits to other specialists, reports of any imaging studies, laboratory results including, genetic testing and pathology where applicable.\nYou May Be Interested In:\nThe Genetics Service has been a leader in clinical genetics in the metropolitan area for more than 30 years offering genetic services and testing for both children and adults.\nThe Genetics Service is also a NJ Department of Health-designated newborn screening follow-up center.\nWomen/couples are referred to Genetics for the following reasons:\n- Couples who are interested in genetic testing or screening.\n- Advanced maternal (35 years and older) or paternal age.\n- Couples who have experienced multiple pregnancy losses or babies who died in infancy.\n- Couples who are consanguineous.\n- Abnormal prenatal ultrasound findings or prenatal screening test results that indicate the pregnancy may be at an increased risk for certain complications or genetic abnormalities.\n- Couples where either partner has been identified to be a carrier of a genetic condition.\n- Couples with a previous child with a genetic condition or a family history of a genetic condition.\nHereditary Cancer Risk Assessment:\nThe goal of the hereditary cancer program is to provide individual risk assessment that can be incorporated into the patient’s ongoing medical care. The program evaluates families with multiple members with cancer (of the same or different type) for the purpose of assessing the likelihood of a hereditary cancer syndrome. Patients are usually referred by their physician based upon their personal medical and/or family histories.\n- Cancer diagnosed before age 50 (breast, ovarian, colon, endometrial, renal cancer).\n- Cancer in multiple siblings or sequential generations of the same or related cancer.\n- Individuals with a rare cancer (i.e. Male breast cancer).\n- Family member with a cancer related gene mutation (i.e. BRCA1, BRCA2)\n- Personal and/or family history of multiple cancers.\nPediatric/Adult Medical Genetics:\nIndividuals come for clinical genetics evaluation for a variety of reasons. The most common referral reasons are listed below, though we offer evaluation and counseling for any concerns related to health and family history.\n- Developmental delay/Intellectual disability.\n- Birth defects/Congenital anomalies.\n- Chromosomal disorders.\n- Growth concerns: short/tall stature.\n- Early onset hearing loss.\n- Cardiovascular conditions known to possibly be associated with genetic factors (cardiomyopathy, long QT, hyperlipidemia).\n- Metabolic/inborn errors of metabolism.\n- Lysosomal storage diseases.\n- Abnormal newborn screening result\n- Any known or suspected genetic diagnosis. Family history of a known or suspected genetic condition","How Do I Become a Genetic Counselor?\nFind out about the types of jobs you could pursue in genetic counseling. Read on to learn more about career options along with salary and certification information.\nWhat Is a Genetic Counselor?\nGenetic counselors research the human genome and help educate families about inherited genetic traits. They meet with clients to discuss their medical history and possible tests. Then, they evaluate the information, write reports and discuss medical options with the patients. While most professionals provide genetic counseling in the traditional fields of prenatal, cancer and pediatric care, more counselors than ever are choosing to specialize in growing areas of the field, such as genomics, neurogenetics and heart health. Jobs are available in a wide range of settings, including hospitals, clinics, biotech companies, diagnostic laboratories, and more. See the table below for more information.\n|Degree Required||Bachelor's degree (minimum requirement)|\nMaster's degree (usually required)\n|Education Field of Study||Biology|\n|Key Skills||Good interpersonal skills|\nAssess individual or family risk for inherited conditions\nProvide information and advice to healthcare providers\n|Certification||Certified Genetic Counselor (CGC) designation by ABGC|\n|Job Growth (2020-2030)||26%*|\n|Mean Salary (2020)||$89,710*|\nSource: *U.S. Bureau of Labor Statistics\nComplete Your Undergraduate Training\nTo become a genetic counselor, you must first earn a bachelor's degree. Aspiring counselors often pursue degrees in the physical sciences, such as biology or chemistry, or the social sciences, like psychology or counseling. Almost any major is acceptable; however, you'll need to make sure your curriculum includes coursework in biochemistry, genetics, statistics or human development. These types of classes are often prerequisites for admission to a genetic counseling master's program.\nGet Your Master's Degree\nA master's degree is usually required in order to work as a genetic counselor. Many schools that offer these 2-year programs look for applicants who have not only completed undergraduate course prerequisites, but also have volunteer or paid experience in a counseling or social work role. You should consider programs that are accredited by the American Board of Genetic Counseling (ABGC) - graduation from one of these programs is a requirement for voluntary board certification. Common master's-level coursework includes research methodology, public health, counseling theory and ethics. You may also complete clinical rotations, conduct independent research and prepare a thesis.\nConsider Becoming Certified\nOnce you receive your master's degree in genetic counseling, you can apply to earn the Certified Genetic Counselor (CGC) designation offered by the ABGC. Before you can sit for the certification exam, you must apply for Active Candidate Status (ACS), which proves that you completed an accredited graduate program and an adequate clinical experience. ACS is good for one examination cycle; if you don't successfully pass the exam while you have ACS, you'll need to reapply. After earning certification, you'll need to participate in continuing education or take a recertification exam every five years.\nFind Your Niche\nIn a clinical setting, you'll work directly with families in different sectors, such as prenatal, pediatric, cancer, adult, cardiovascular, hematology or neurovascular genetics. You might be interested in working with biotech companies that develop and perform tests, or working as a liaison between diagnostic laboratories and physicians. Becoming an educator, working with lawmakers to develop public policies and becoming a researcher are also possible career choices.\nWhat Are Some Related Alternative Careers?\nIf you are interested studying human disease, particularly as it relates to public health, you may want to consider a job as a research epidemiologist. These professionals conduct health-related research on a wide variety of topics, including some that are associated with genetics, such as chronic diseases and maternal/child health. Epidemiologists usually hold at least a master's degree. Alternatively, if you would rather focus your career on the practice of medicine, you could become physician assistant, where you would work in conjunction with physicians to diagnose and treat illnesses and injuries, including those associated with heritable conditions. To become a physician assistant, you must complete a master's degree program and pass a licensure exam."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:3d1ef094-ad27-4a9d-b4c2-b6b8a2a83cae>","<urn:uuid:1cedc8c3-025a-4d68-b158-10163f80b125>"],"error":null}
{"question":"What are the main factors that influence atomic stability, and how do surface diffusion mechanisms affect thin film growth?","answer":"Atomic stability cannot be explained by classical physics due to a paradox where electrons should fall into the nucleus. Instead, stability is explained through quantum mechanics, where electrons occupy atomic orbitals in quantized energy levels, interacting with the nucleus via electromagnetic force. The electrons' behavior is described by wave functions that calculate probability distributions rather than exact positions. In thin film growth, surface diffusion mechanisms are crucial - adatoms (atoms on surfaces) must have sufficient mobility to create smooth, uniform films. Without mobility, adatoms stay where they land, creating rough surfaces. The diffusion process involves atoms jumping between equilibrium sites and can include complex mechanisms like atomic exchange and subsurface diffusion. The competition between on-terrace and at-step nucleation, influenced by the Erlich Schwoebel barrier, determines the final film structure.","context":["IFIS - LITORAL   24734\nINSTITUTO DE FISICA DEL LITORAL\nUnidad Ejecutora - UE\ncongresos y reuniones científicas\nCharla invitada. Surface diffusion mechanisms and growth mode.\nConferencia; First Ibero-American Conference on Surface, Materials and Vacuum Applications (ICSMVA); 2014\nBrazilian Vacuum Association\nOur comprehension of the atomistic mechanisms related with the thin film growth has evolved in an impressive way in the last years. The way an adatom diffuses over a surface reaching either defects, steps or another diffusing adatom to nucleate, determines the basic paths of growing. These diffusing mechanisms can be either simple as one atom jumping from one equilibrium site to the nearest one, or they can also involve more complicated processes like atomistic exchange,, subsurface diffusion,, and long jumps, including even memory effects, for mention some of the last startling discoveries. The increasing needs of growing flat and structurally almost perfect layers, coming from the requirements involving the fabrication of elaborated artificial structures, such as magnetic superlattices, lead to the develop of new methods for obtaining flatter surfaces and sharper interfaces. However, the layer by layer growth (LbL), where no atomic level starts to growth before the preceding atomic monolayer (ML) is completely filled, is the exception rather than the rule. Intermixing, even between (in bulk) immiscible materials, and defect generation, for mention a couple of facts, may lead to roughness developing from the very beginning of the growth. The diffusion of an adatom on a flat surface is by far the most important kinetic process in film growth. Smooth, uniform films could not be formed without enough surface mobility. In the extreme case of zero mobility parallel to the surface, an adatom stays where it lands, and the resulting growth front is very rough. The island nucleation is, on the other hand, the only possible growing mechanism over a perfectly plane surface. However, a real surface is composed by flat terraces limited by steps. While ascending steps constitute nucleating sites, descending ones usually act as atom mirrors, since the Erlich Schwoebel barrier (the energy needed for the adatoms to overcome the steps, falling down to the lower terrace) is too high as compared to the energy needed to return to the inner terrace. Whereas decoration of steps at the upper side is clearly disfavored in metals and insulators due to this mechanism, it has been observed in few cases. However, these cases involve the modification of the nature of the step by the interaction with the adatoms11,. Thus, a competition between on terrace and at (ascending) steps nucleation is established. For large atomic/molecular diffusion lengths as compared to terrace size, adatoms have time enough to reach the steps nucleating there; otherwise they may nucleate over the terrace. The growth of insulator films is a physical phenomenon even more important than metallic growth due to technological requeriments and certainly more complicated from the basic physics point of view. In this work we analyze different systems from experimental and theoretical point of views, using different experimental and theoretical techniques. The analyzed systems include metal ? metal, homo (Cu on Cu (100) and Cu(111)) and heteroepitaxial (Co/Cu(111)) growth, and insulator-metal film growth (AlF3 on Cu(100) and Cu(111)). We also studied film growth using ion implantation techniques, as in NCu on Cu(100) and Graphene on Cu(111). Among the experimental tools, we used thermal energy atom spectroscopy (TEAS), scanning tunneling microscopy (STM), electron energy loss (ELS), Auger (AES), xray photoelectron (XPS) and ion scattering (ISS) spectroscopies. The analysis is complemented by Metropolis Montecarlo and Molecular Dynamics simulations, and Density Functional Theory calculations. All these systems present particular characteristics related to the surface diffusion mechanisms that largely influence the surface and interface develop. . G. Antczak and G. Erlich, Surface Diffusion. Metals, Metal Atoms , and Clusters (Cambridge University Press 2010) and references therein. . G. L. Kellogg and P.J. Feibelman, Phys. Rev. Lett. 64 (1990) 3143. . C. Chen and T.T Tsong, Phys. Rev. Lett. 64 (1990) 3147. . R. Tromp, Nature Materials 2 (2003) 212. . J. Camarero, J. Ferrón, V. Cros, L. Gómez, A.L. Vázquez de Parga, J.M. Gallego, J. E. Prieto, J.J. de Miguel and R. Miranda, Phys. Rev. Lett. 81 (1998) 850. . G. Antczak and G. Ehrlich, Phys. Rev. Lett. 92 (2004) 166105. . J. Ferrón, L. Gómez, J.J. de Miguel and R. Miranda, Phys. Rev. Lett. 93 (2004) 166107. . W.J. Egelhoff, P.J. Chen, C.J. Powell, M.D. Stiles, R.D. McMichael, C.L. Lin, J. M. Sivertsen, J. H. Judy, K. Takano, and A.E. Berkowitz, J. Appl. Phys. 80 (1996) 5183. . See F. Besenbacher, L. Pleth Nielsen and P. T. Sprunger, in Growth and Properties of Ultrathin Epitaxial Layers, edited by D. A. King and D. P. Woodruff, The Chemical Physics of Solid Surfaces Vol. 8 (Elsevier, Amsterdam, 1997), Chap. 6, and references therein. . G. Ehrlich and F.G. Hudda, J. Chem. Phys. 44 (1966) 1039; R.L. Schwoebel and E.J. Shipsey, J.Appl.Phys. 37 (1966) 3682. . L. Gómez, C. Slutzky, J. Ferrón, J. de la Figuera, J. Camarero, A.L. Vázquez de Parga, J.J. de Miguel and R. Miranda, Phys. Rev. Lett. 84 (2000) 4397. . F. Calleja, J.J Hinarejos, A.L. Vázquez de Parga, S.M. Suturin, N.S. Sokolov and R. Miranda, Surface Sci. 582 (2005) 14. D. Farías, K.F. Braun, S. Fölsch, G. Meyer, H.K. Rieder, Surface Sci. 470 (2000) L93.","Everything we see is made up of atoms, many atoms.\nThe word \"atom\" comes from the Greek word \"atomos\" (indivisible). Long ago already, in the 4th century BC, the Greek philosophers Leucippus and Democritus theorize that all matter is composed of tiny particles in constant motion, very solid and eternal. Today we have a somewhat more accurate because the atom is not indivisible. We know its approximate size since 1811, Amedeo Avogadro estimates the size of atoms, to 10-10 meters. In 1911, Ernest Rutherford specifies the structure of the atom and gives the atomic nucleus size of about 10−14 meters. Concerning the size of atoms, it is called atomic orbitals, i.e. the electron cloud surrounding the nucleus (see image opposite), this cloud has a theoretical diameter between 62 pm (picometers) for the Helium atom at 596 pm for the Cesium atom. But nothing is simple in the nature of matter and the tiny distance varies depending on the chemical nature of the surrounding atoms. While the nucleus concentrates essential of the mass of the atom (99.99%), we known as its mass for stable atoms, it is between 1.674 × 10-24 g for Hydrogen and 3.953 × 10-22 g for uranium. We also know its composition, inside we see a nucleus and electron cloud which occupies the spatial extent of the atom because it is more than 10,000 times larger than its nucleus. Even more amazing, we even know the number of atoms in the Universe, this number is extremely large, so we had to write should write a 1 followed by 72 zeros.\nBut what maintains the stability of atoms?\nThe stability of atom can not be explained by classical physics because in classical physics, the electron corpuscular negatively charged and the proton positively charged raise a paradox.\nIn classical physics, the matter should disappear, because an electron which radiates around nucleus loses energy (Maxwell's theory) and therefore should fall on the nucleus. This means that the stability of an atom is incomprehensible in the context of the classical theory. Scientific geniuses of the 20th century will solve this paradox by wave mechanics by Louis de Broglie in 1924 and generalized in 1926 by Erwin Schrödinger (Nobel Prize in Physics in 1933 with Paul Dirac, for the wave equation called the Schrödinger equation.\nIn quantum mechanics, it is not possible to know the exact value of a parameter without measure it.\nMathematical theory describes a state, not by a couple speed and position precisely, but by a wave function that calculates the probability of finding a particle at a point. Hence the probabilistic nature of quantum mechanics which predicts that particles are also waves and not only material points.\nThe electrons occupy atomic orbitals in interaction with the nucleus via the electromagnetic force, while the nucleons are held together in the nucleus by the nuclear binding, which is a manifestation of the strong nuclear interaction. The electron cloud is stratified into quantized energy levels around the nucleus defining layers and sub-layers electronics.\nNucleons are also divided into nuclear layers, although quite convenient model, popularized by the nuclear structure of the liquid drop model.\nMore atoms can establish chemical bonds between them through their electrons and generally, the chemical properties of atoms are determined by their electron configuration, which stems from the number of protons in their nucleus. This number, called the atomic number, defines a chemical element.\n| || |\nImage: Representation of the atomic structure of the atom of helium-4. For reasons of clarity, the above image is not to scale. The nucleus appears pink in the center, and all shades of gray around, the electron cloud or atomic orbital. The nucleus of helium-4 is a schematic enlarged, showing the two protons and two neutrons in red and purple, its size is 1 femtometer or 10-15 meter. In reality, the nucleus (and the wave function of each nucleon) is also spherical, as the electrons of the atom. Credit image: public domain.\nIn 1911, Ernest Rutherford specifies the structure of the atom by bombarding gold foil with particles from the radioactive decay of uranium. It gives a size at the atomic nucleus of the order of 10-14 meters. Concerning the size of atoms, it is called atomic orbitals, i.e. the electron cloud surrounding the nucleus, this cloud has a theoretical diameter between 62 pm (picometers) for the Helium atom at 596 pm for the Cesium atom. Ernest Rutherford would like to see well the atoms but the wavelengths of visible light (400 to 800 nanometers) are greater than the dimensions of the atoms.\nToday we can see atoms with the STM (Scanning Tunneling Microscope) invented in 1981, developed by IBM researchers, Gerd Binnig and Heinrich Rohrer (Nobel Prize in Physics for this invention in 1986).\nThe scanning tunneling microscope is a small microscope of a few centimeters, type scanning probe microscopes, with a tungsten tip (W) or platinum-iridium (Pt Ir) so fine the size of an atom, that it can scan in vacuum, the surface of a matter sample.\nA computer adjusts and records in real time with high precision, the height of the probe to maintain a constant current. Then the computer measures and amplifies the resultant current, by quantum tunneling, of the passage of electrons between the tip and the sample surface.\nThis movement reflects the topography of the surface and thus the atoms themselves which allows to reconstruct the detailed picture of the surface covered at the atomic scale.\nTo see the atoms, scientists use a metal conductor of electricity that does not oxidize, such as gold or platinum iridized, because most of the matter surfaces are covered with a layer hyperfine oxide that prevents the passage tunnel current. Quantum tunneling is one of properties of a quantum particle, this property allows it to cross a potential barrier even if its energy is less than the minimum energy required.\nnota: the spectrum of visible light is in the infrared to ultraviolet, it corresponds to wavelengths of 400 nanometers in the violet to 800 nanometers in the red, that is to say 4x10-7 to 8x10-7 meter. Between the wavelength (λ) and frequency (ν) is the following relationship: ν = c / λ where c is the speed of light is about 300,000 km/s.\n|Submultiples of the meter||Symbol ||Name|\n| || || |\n|10-6 meter||µm ||micrometer|\n|10-24 meter||ym||yoctometer|| |\nImage: This image, closer to matter, details the surface of a sheet of pure gold (Au (100), visualized by a scanning tunneling microscope. The gold atoms are visible in this image, they are evenly spaced them on the crystalline structure. This image atomic was made with STM Omicron low temperature by Erwin Rossen, Eindhoven University of Technology, 2006."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:856ccf59-81e0-4439-b3e3-896350a18dc3>","<urn:uuid:393a39ff-9148-44c4-bf00-2827bdee74f9>"],"error":null}
{"question":"As an environmental scientist, I'm interested in understanding both India's uranium expansion plans and the various mining methods. Could you provide a comprehensive overview of India's planned uranium production increase and the main extraction techniques used in uranium mining?","answer":"India plans a tenfold increase in uranium production by 2031-2032, to be implemented in three phases. The first phase aims to increase production 3.5 times by the 12th year, the second phase targets a sevenfold expansion, and the final phase will achieve the tenfold increase. As for mining methods, uranium can be extracted through several techniques: open pit mining, underground mining, and in-situ leach (ISL) mining, with ISL being the fastest-growing method. ISL mining involves drilling wells to the uranium deposits in aquifers and using a special leaching solution, while open pit mining is typically used for deposits less than 100 meters deep. The choice of method depends on the deposit characteristics and environmental considerations.","context":["The uranium mining projects in India have time and again failed to gain the “social consent” which leads in slowing down approvals and increasing the project costs.\nWith the high population growth rate, urbanisation and the growing middle-class, the population has led to a higher demand for power generation. This demand in power generation has led to the critical importance of improving new power generation capacity. According to the Central Electricity Authority (CEA) the requirement of energy generating capacity is to increase from the current 43 GW to 640 GW by 2026-27. There is a constant demand for cheaper renewable sources.\nThe World Nuclear Association has estimated a 25 pc contribution of nuclear power (against current 2 pc) in India’s energy basket by the year 2050. India’s Nationally Determined Contribution (NDC) to United Nations Framework Convention on Climate Change (UNFCCC) has outlined goals to reduce the carbon emissions intensity of its economy by 33-35 pc by 2030 as well as increase the clean energy electricity capacity to 40 pc of the total installed capacity in the same period. The ruling NDA government is intent on significantly scaling up installed nuclear capacity. India operates 22 nuclear reactors, eight of which are fuelled by indigenous uranium.\nThe expansion plan of uranium mining in India\nMinister of state Jitendra Singh told the parliament in March this year that India is planning a tenfold increase in uranium production over the next 15 years. State company Uranium Corporation of India Ltd (UCIL) has outlined the expansion plans to meet the Department of Atomic Energy’s (DAE) vision of achieving self-sufficiency in uranium production.\nJitendra Singh said that UCIL, which is a public service undertaking with the DAE, has outlined a plan for “massive expansion” leading to a tenfold rise in uranium production by 2031-2032. The plan includes maintenance of sustained supply from existing facilities, capacity expansion of some existing units and construction of new production centres (mines and plants) in different parts of the country, he said.\nThe expansion is planned in three phases, with the first expected to increase uranium production to 3.5 times existing levels by the “12th year”. Completion of projects in the second phase is expected to achieve a sevenfold expansion over current production, with the third phase of projects leading to a tenfold increase over current levels by 2031-32.\nThe problems with the expansion plan\nThe uranium mining projects in India have time and again failed to gain the “social consent” which leads in slowing down approvals and increasing the project costs. But the major problem related to the expansion plans are the people living in the mining areas. Mining projects mainly lie in the tribal areas of Odisha, Chhattisgarh and Jharkhand that fall under Schedule V of the Constitution of India giving the communities near the mining areas protection against alienation of their land and resources. People fear being alienated from their land, not gaining any economic benefit from such mining activities and mainly due to health hazards caused by the radiation.\nThere are also environmental aspects regarding the uranium mining in India. A new Duke University-led study has found widespread uranium contamination in groundwater from aquifers in 16 Indian states. “Nearly a third of all water wells we tested in one state, Rajasthan, contained uranium levels that exceed the World Health Organisation and U.S. Environmental Protection Agency’s safe drinking water standards,” said Avner Vengosh, a professor of geochemistry and water quality at Duke’s Nicholas School of the Environment.\n“By analysing previous water quality studies, we also identified aquifers contaminated with similarly high levels of uranium in 26 other districts in north-western India and nine districts in southern or south-eastern India,” he added. The new findings are the first to demonstrate the widespread prevalence of uranium in India’s groundwater.","to handle both the positive and negative impacts of mining and processing on .. In situ leach (ISL) mining, the fastest growing method of uranium mining in the\nUranium Mining in ia: Scientific, Technical, Environmental, Human Health and Safety, and Regulatory Aspects of Uranium Mining and Processing in\nRössing Uranium's operations are built on two distinct activities: mining uraniumbearing rock, and processing this ore into uranium oxide for the world's nuclear\nEnvironmental Impacts of Different Uranium. Mining Processes. Prepared by: SENES Consultants Limited. Ottawa, Ontario. For: Alberta Environment. May 2008\nWe have 55 years of experience in delivering uranium mines and processing facilities, covering all aspects of the industry. While uranium projects have not been\nThe need to find significant uranium deposits for mining has resulted in [2,4] Most of the chemistry of the uranium mining process occurs after\nPower Resources International in-situ leach uranium mine in central Wyoming. In situ recovery process. In all cases, ISL uranium mines must obtain -.\nThe Navajo Nation has banned further uranium mining on their lands. . ailings, or waste generated from the milling process, are stored in specially designed\nUranium Mill Chemistry. The crushed and ground ore, or the underground ore in the case of ISL mining,\nIn this article, we'll chart the uranium mining process, tracking the element as it transitions from ore to fuel pellets ready to be used by nuclear reactors. We'll also\nContamination of local water supplies around uranium mines and processing plants has been documented in Brazil, Colorado, Texas, Australia\nThis review focuses on underground mining of uranium as this method is currently experiencing renewed attention. Although open pit mining may also resume\nCommonwealth of Australia 2006, Uranium Mining, Processing and Nuclear Energy — Opportunities for Australia?, Report to the Prime Minister by the Uranium\nUranium miners say the ISR process is less environmentally damaging than open-pit mining. This is, in some ways, a reasonable claim.\nAt its most basic, in-situ mining is a process by which the miners drill a well straight down to where the uranium sits in the aquifers. A special leaching solution of\nUranium resources can be extracted from the ground in three ways: open pit, Only effective method to extract uranium from conventionally mined ores. Modern\nUranium Overview. Mining & Milling. Fuel Processing. Electricity Generation. Spent Fuel Management. Electricity generation from nuclear power creates virtually\nRabbit Lake is the longest uranium mine in operation in Saskatchewan. Mining in the 1980s and 1990s was primarily by the open-pit method as the deposits\nGeographical process, Case study: uranium mining in Kakadu, Issues in Australian environments, Geography, Year 9, NSW Introduction The case study chosen\nUranium is mined to provide uranium ore which is processed at a The CNSC's licensing process for uranium mines and mills follows the\nMost Canadian uranium is mined in the Athabasca Basin in Northern less than 100 metres deep, it is typically extracted by the open-pit mining method.\nMining-technology.com profiles the world's biggest uranium mines by acid heap leaching method for processing the low-grade uranium ore.\nUranium Resources' Mark Pelizza explains how uranium is mined--either through a conventional or in situ uranium mining process--to provide\nUranium mining in South Dakota, Wyoming, Montana, and North Dakota This exploratory process itself allows radioactive pollutants to cross contaminate.\nUranium Processing. Uranium occurs in a variety of ores around the world. After mining, uranium is concentrated as an oxide (U3O8) with a yellowish color,\nThe federal government banned uranium mining on more than 1 million acres of in 1973, which used uranium as fuel, hundreds were in the planning process.\nUranium mining is the process of retrieving uranium from deposits for use in nuclear reactors to generate electricity. Uranium is also used for the important task\nMost reserves have uranium with a concentration of between. 0.1 bis 0.2 %. There is one underground mining, or a chemical process of In-Situ-Leaching (ISL).\nMost uranium ore is mined in open pit or underground mines. Other waste piles consist of ore with too low a grade for processing.\nUranium mining is the process of extraction of uranium ore from the ground. The worldwide production of uranium in 2015 amounted to 60,496 tonnes."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:fdc95ad8-19ce-4be1-b741-02364a56132f>","<urn:uuid:3149b720-a377-4ea9-95ef-d152306afaf9>"],"error":null}
{"question":"I'm learning about web design and security. How can I ensure my website loads quickly for users while maintaining secure data transmission?","answer":"To ensure both speed and security, you should optimize images by using appropriate file formats (PNGs for buttons and screenshots, JPEGs for photos) and keep page content minimal to reduce loading times. For security, implement SSL protocol which provides privacy through symmetric cryptography and automatic message integrity checking. SSL can be used alongside TLS and SSH protocols for maximum security, especially crucial for eCommerce sites. You can verify security by checking if your web pages start with 'HTTPS' instead of 'HTTP'.","context":["This article is for anyone with even the slightest interest in web design. You are going to learn what it may take to get started if you want to get into webdesign, but you will also get some solid, general website design information as well. No matter what your level of experience, there is always more to learn.\nUsers can navigate easier when you use fixed-position navigation. This means locking down the site’s navigation panel into position when the visitor scrolls up and down the webpage. This is not only convenient for the visitor, but can also be useful for internet marketers, as this makes it easier for visitors to take some desired action (e.g. buy a product, sign up for a newsletter).\nChoose proper graphic for your web designs. Keep in mind that bitmap images are not usually the best type of images to use. Try PNGs instead. PNG should be used for buttons with text and screenshots that have 256 colors or more. Photos require JPEG to ensure high quality.\nPhotoshop is a great tool that novice designers should invest in to help make better looking web designs. Using a program like Photoshop can help amateur web designers create professional looking sites really fast. Although a copy of Photoshop represents a significant investment, the time it will save you and the improvement it will make in your results can easily justify the program’s cost.\nUsers can navigate your site easier when you have fixed-position navigation. This will make sure the navigation panel is locked in place as your readers move throughout the site. It benefits virtually everyone who will visit your site.\nWhen designing pages that have links, make certain your links have text content. Your visitors should know exactly what they’re heading for when they click on a link. Textless links can easily be clicked by mistake.\nIncorporate a search function that lets visitors find what they need. If people visit your site for something specific, they will be on the lookout for a search box. When you don’t have one, they are more likely to move on to another site that does. Most people look for search forms in the top right so that’s where you should put it.\nChoose fonts that are legible, and look professional. Look at the font of a site and you can distinguish whether or not it’s professional. Fonts that are overly artistic may seem like a good idea, but often aren’t accessible on all computers. Use a font that is part of the default font subsets on user computers. This can make your website look a lot worse.\nDesign web pages to not take up too much space. Not every person using Internet has a high-speed connection, so if a site takes longer to load, they won’t be as interested in it. If the wait is too long, your visitors may give up and leave.\nWhile creating your website, you should not feel forced to have your website occupy the entire available space. If you are using every single available pixel, the website may feel extremely cluttered. Dividing up your site’s elements with blank space, can make it easier for visitors to distinguish what elements perform what function. There are many situations in which empty space makes a major impact.\nDo not publish any site pages without verifying all the links are working. Visitors can get frustrated when they click a link and come across an error page. You can choose to use a link checking program or check your links by hand.\nProofread everything so it looks nice. This will make it easier for people to read. While you may not think errors are a big deal, many people will view it as unprofessional and will believe that you don’t care enough to get their orders or service correct either.\nKeep your topics separated. Separate topic of discussion throughout your site by putting them on separate pages. This ensures that no one ends up confused and leaves as a result. Search engines will also have an easier task of ranking specific pages.\nGather knowledge from online resources when making your first website. When you learn all you can about webdesign from experts, you will be able to quickly build a site of your own. Without that correct information, you could end up with a site that has a poor design that people want to avoid.\nMake use of free software in your site setup. Many people falsely believe that they need to purchase expensive software in order to create a good website, however, there are currently numerous excellent free tools on the market that help you to develop a very professional looking website. Instead, look around for a free product which has the tools you need.\nWhen you first begin designing a web page you should choose a design layout that’s as simple as possible. This allows you to build your skills slowly and avoid problems you might encounter with more complicated features. You can advance beyond the basics when you are more comfortable with the web design process.\nAs you design your site, avoid overuse of multiple fonts. You need to consider how large the font is since some may be too small for concentrated website reading or smaller screens. Lots of sites use typefaces like Verdana since it reads well in different sizes and colors.\nMake sure your slogan is on every page of your website, including those that are housed in subdomains. It needs to be the first thing the viewers sees when they follow links, so make sure the text is large and bold. The tagline informs of the goal of each page, and can retain or repel your readers.\nBe sure you take care of the people that use your site’s needs. A good web designer will always remained focused on the needs of the end user. Factors of note are usability, accessibility, user experience and interaction. These are important considerations to make. Try looking at the websites from your audience’s perspective when designing.\nRefrain from forcing readers to click on any specific links, but rather let them come to that decision on their own. Pop-up advertising, surveys and other offers are annoying and should never be used. When you narrow the viewer’s choices, they’re likely to back out to another website and badmouth yours.\nA captcha is a great way to improve the security of user registration pages, but there are hardly any other places where one should be used. Being forced to use a Captcha frustrates users, as they simply want to use the website. Captchas are especially frustrating for website users with visual and hearing difficulties. The only way they will stay on this page is if they’re already a member that’s devoted to the site.\nFile types can affect the time it take for a website to load. When it comes to graphics, it is normally wise to employ GIFs and JPEGS. PNG and BMP files take up a ton of bandwidth. Convert your images to file types that are smaller and easier to manage if you want a guaranteed great user experience.\nRemember literacy levels as you write your content. If you want everyone to be able to understand your website, use language they can understand. Your content is useless to your visitors if they can’t understand it.\nWhile development platforms aid you by creating code for you, they may not be as reliable as the original coding tool, the basic text editor. What a platform does is help you paste the code onto features that you have made. To minimize errors and work with the code in a more hands-on way, choose a standard text editor.\nPractice time management when building a site if you wish it to be completed any time soon. There are many small, tedious tasks that are easily left for another day if you are not diligent. Soon, you will realize that these small tasks have added up and you will become overwhelmed. Complete work as it occurs.\nYou really should have a dedicated space set aside, where you can design websites and manage your own site. Ideally, you should remove any distractions and strive to create an efficient and motivating workspace. Make your office supplies and tools easily accessible, and keep an optimized work space for your needs.\nDon’t assume that the design process has wrapped up just because the site is live. Be prepared to keep active on your website. While you don’t need to do something all the time, it will need to be updated on a regular basis. This is especially true if you’re hosting videos or working with current events. Luckily, keeping website current isn’t as overwhelming as tending to a blog. You will need to work.\nAsk your friends what they know about web design, in case there is something you’ve overlooked learning. Obviously, you want to ensure that all the new information you have just learned has been remembered. It would be quite a nuisance to be halfway through developing a new site and find you can’t recall the details you need.\nIt is very important to always consider what types of security the website has. For example, if people are purchasing products from you and entering their credit card numbers, you want to get an SSL certificate. You can also consult with the host about security features they may have for the package you choose.\nDon’t write above the reading level of most of your visitors. If you want everyone to be able to understand your website, use language they can understand. If you desire a greater amount of viewers to your site, create content for all different types of people.\nYou need to always make a visual sitemap in order to more accurately plan ahead. With the visual sitemap, you are going to see exactly how your site structure is coming along. This also helps you see areas that are lacking in content, or need other improvements at a glance. There is no better way to get an overall view of your website than to have a visual sitemap.\nNever stop learning if you want your website design to keep growing. As soon as you figure one aspect of site design out, you should attempt to tackle another. This can make your initial construction of the site take longer, but once you’re done with that first one, you’ll be ready to make dozens.\nYou want to glean information from others as you navigate your way through the world of site design and network with others. This will help you to become diverse, and it will give you lots of options when it comes to designing your site.\nBuilding a website does not dictate you need to purchase books. It might not be a bad investment. Many professional books and magazines exit that have a ton of great information in them. However, you can find the same information online free of charge. It can be a silly thought to believe a retail book has better information than most that are offered for free.\nUsing free stock images can help you save a bit of money when creating your website. These pictures are easily found online; plus, they are usually high-quality. Using stock graphics means that you can save money and spend what you have on other important aspects of your site.\nMake sure the content on your site is accessible. Try finding a person out of country to check the site for you. Computer hardware and display settings can vary between countries, so it pays to test your site in as many locations as possible.\nTypical websites have neutral-colored backgrounds. Avoid textured backgrounds because they can overwhelm the site and make it look amateur. Try neutral colors or white for the background. Neutral colors have been shown to be the easiest background to read on.\nDo regular checks to make sure none of the links on your webpage are out of date or broken. Good web design should always be easy to navigate, and free of error messages. To prevent these things from happening, create a time frame at regular intervals where you check all the links for proper functionality.\nYou should not only place your search box close to the top of the page, but should allow visitors to type 27 or more characters into the box. The button you use to start the search should also say only “search” to thwart any confusion. The term ‘search’ will help people notice the search box and encourage them to use it.\nAs a web designer, you have to set a series of realistic goals for yourself, especially regarding how long you should work on each task. If you set an unrealistic deadline for yourself, it will ultimately cause you to compromise the quality of your work. The better practice you should follow, is to leave yourself enough time to do good work the first time around.\nNow that you know more about being a web designer, you can feel better about your skills in the future. Keep on learning, and you soon will be as comfortable as a professional.\nWeb designers that are trying to decide whether or not it is prudent to obtain an SSL certificate should understand that it depends on whether their website will be storing private information, such as credit cards and personal details. If so, then purchasing an SSL would be a good idea for security reasons.","One of the most important aspects of web hosting security is the protocol that is used to transfer files from the web server to a visitor’s computer. This is especially true for web sites that frequently engage in eCommerce activity. Understanding the various types of web hosting security protocols can help you build discretion when selecting an FTP client or web hosting plan in the future. If you’re interested in learning more about the difference between SSL, TLS and SSH security protocols, then you may find the following information to be quite useful.\nSSL (Secure Sockets Layer) is a commonly used security protocol that provides supreme privacy when transmitting data over the internet. FTP clients and web shots utilise this protocol to ensure the utmost security during sensitive site browsing activities like eCommerce shopping and online banking. SSL protocol uses symmetric cryptography to perform state-of-the-art data encryption. SSL adds extra security by automatically checking the encrypted messages for integrity before delivering them to the recipient. SSL optimizes CPU resources to facilitate expedited communication, and can be used in conjunction with the following security protocols to increase effectiveness.\nTLS (Transport Layer Security) utilities two main components – TLS Handshake Protocol and TLS Record Protocol. TLS protocol also uses symmetric cryptography, and like SSL checks the messages before the are sent to ensure that they are thoroughly encrypted. TLS differs from SLL because the encrypted massage can only be delivered to a third-party user after two authorized users have received the data. This adds an extra layer of security to the encryption, when used in combination with SSL, this method of encryption provides the highest level of security available to the public.\nSSH (Secure Shell) provides encrypted channels through which encrypted data can travel. Thus, SSH acts more as a medium for the aforementioned encryption methods than as an actual encrypting mechanism, hence the term “shell.” SSH is commonly used to send commands to a computer from a remote location. Although SSH is not the strongest encryption method, it is viable method of secure communication, and is frequently used in the web hosting industry as a standalone security precaution with some applications. Standard FTP services do not utilize Ssh technology, however SFTP (Secure File Transfer Protocol) does utilize this technology along with the other two aforementioned security protocols.\nAlthough all three of these security protocols could be used by themselves with success, most web hosting providers now utilize all three of them simultaneously in one form or another. If you’re serious about keeping your web site data safe form intruders and hackers then you’ll need to pay close attention to the technology used by your web hosting provider, especially if you’re the owner of an eCommerce business. Make sure all of your checkout pages are encrypted and have an SSL certificate. The easiest way to check if a web page is secured is to look at the first letters of the web address. If the web address begins with “HTTPS” then it is secured, however if the web page begins with “HTTP” it is not secured. Never conduct eCommerce on an unsecured web page."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:c95cdca5-5c98-4118-b53b-b5e2ef802ef7>","<urn:uuid:ee1ef7a3-a083-4824-b19b-a64266b68f50>"],"error":null}
{"question":"What is the significance of gold coins in Estonia's monetary history, and how does Finland's approach to forest management demonstrate sustainability?","answer":"The Estonian 15.65 kroon gold coin, issued in 1999, marked two significant events: the introduction of the euro and the 80th anniversary of the Bank of Estonia. This commemorative piece was composed of .900 fine gold, had a mass of 1.73 grams, and featured the Estonian coat of arms and the Muhu mänd symbol. It was sold for 600 krooni until mid-2004. Regarding Finland's forest management, it demonstrates sustainability through several measures: the Forest Act ensures regeneration after logging, mandatory forest use declarations are required before felling, and over 90% of commercial forests are certified under PEFC and FSC schemes. Finnish forests are growing more than they are being harvested, with growth increasing significantly over the last 50 years, primarily due to good and long-term forest management practices.","context":["|Measurements and composition|\nCoat of arms of Estonia, state title, year\nMuhu mänd, stars, value\n|v · d · e|\nThe 15.65 kroon coin is a commemorative piece that was issued by the Republic of Estonia in 1999 to celebrate the introduction of the euro and the 80th anniversary of the Bank of Estonia, which was founded in 1919. It was distributed by the Bank of Estonia and sold via the bank's museum from May 3, 1999, to mid-2004 for a price of 600.00 krooni. Commissioned to strike the coin was the Austrian Mint in Vienna. The piece had a legal tender face value equivalent to 15.65 krooni prior to its demonetization. This unusual non-integer value was selected because at the time of the coin's introduction, it was the approximate amount of krooni equal to one euro.\nThe coin is composed of .900 fine gold and has a mass of 1.73 grams and a diameter of 13.92 millimeters. It has medallic alignment and is round in shape. Both sides of the piece were designed by Estonian sculptor Tiiu Kirsipuu (1957–), whose submissions to a closed 1998 coin design competition were accepted over those of five other individuals. Featured in the middle of the obverse is the coat of arms of Estonia – which consists of a central escutcheon bearing three lions passant guardant, above a wreath consisting of two oak branches intersecting at the ends. Inscribed in a clockwise direction along the upper rim of the piece, above the arms, is the Estonian state title of the Republic of Estonia – \"EESTI VABARIIK\" – and written counterclockwise along the bottom periphery, below the heraldic depiction in the coin's center, is the date \"1999\". Featured in the middle of the reverse is the Muhu mänd, a cross-like symbol used in Estonia since ancient times. It superimposes a spiral that symbolizes development and movement. Arched along the upper and right rims of the reverse are eleven five-pointed stars representing the states that were joining the third stage of the European Monetary Union: Austria, Belgium, Finland, France, Germany, Ireland, Italy, Luxembourg, the Netherlands, Portugal, and Spain. Between the end of the spiral and the eleventh star is the number \"15,65\", and curved around the bottom periphery in small print is the accompanying word \"KROONI\".\nAs many as 5,000 examples of the coin were produced, all with a proof finish.\n- 15,65-kroonine münt on the Estonian (Eesti) Wikipedia\n- Commemorative Coins – Bank of Estonia\n- Bank of Estonia – Chronicle of Monetary Policy of 1998 (November and December)\n- Bank of Estonia – Chronicle of Monetary Policy of 1999 (March and April)\n|Banknotes||1 kr • 2 kr • 5 kr • 10 kr • 20 kr • 25 kr • 50 kr • 100 kr • 500 kr|\n|Coins||1 s • 2 s • 5 s • 10 s • 20 s • 50 s • 1 kr • 2 kr • 5 kr • 10 kr • 15.65 kr • 25 kr • 50 kr • 100 kr • 500 kr|\n|Miscellaneous||Bank of Estonia • Estonian euro coins • Soviet ruble|","Sustainable forest management in Finland\nForests have played an important role in the Finnish history. Finnish forests have provided food, shelter, employment and income for the Finns through centuries. Most of the Finnish forests are in commercial use. Commercial forest are not only utilised but also regenerated. Finnish forests grow more than they are being harvested and the growth has increased significantly through the last 50 years. The growth and evolution of forests in Finland is mainly the result of good and long-term forest management.\nWhat guides forest management\nThe Forest Act provides a framework for forest management in Finland. The leading principle of the forest legislation has been the obligation to regenerate after logging. The inception of Forest Act in 1997 (revised again in 2014) raised multi-functionality to a key driver in forest management, emphasizing safeguarding of ecological sustainability as well as promotion of diverse use of forests, landscape management and water protection.\nThe Forest Act requires that the forest owner delivers a mandatory forest use declaration before felling. Forest use declarations are monitored by a competent authority. The forest use declaration is a way to make sure that environmentally important values are considered in the management of forests. The declaration is also the core of the control system in the EU Timber Regulation, the purpose of which is to prevent the entry of illegally produced timber on the EU market. The Forest Act also obligates that forests shall be managed and used in such a manner that the conditions for the preservation of habitats important for the biological diversity of forests are safeguarded.\nIn addition, Best Practice Guidelines for sustainable forestry that are based on national research on forests provide extra tools to sustainable forest management over requirements of the legislation, including biomass harvesting for bioenergy production. The guidelines have been successfully implemented to private forestry.\nHabitats protected under the Natura 2000 network form the basis for forest protection. The habitats are protected e.g. under Nature Conservation, Forest, and Wilderness Acts. There are several country-wide forest protection programs, such as programs for old forests, herb-rich forests and eskers. The network of protected areas consists of national parks, strict nature reserves, protection program areas etc.\nThe Forest Biodiversity Action Program METSO supplements legislation-based protection by providing landowners a voluntary tool for both permanent and temporary forest protection in Southern Finland. The goal is to protect permanently 96 000 ha forests via METSO by 2025. Almost 90% of the target is reached today. Helmi habitats program, which started 2019, complements METSO in increasing habitat restoration and conservation of other habitat types than forests.\nForest management in Finland today\nForest data has been gathered in Finland for 100 years. The National Forest Inventory (NFI) data is based on statistical sampling and a new inventory of forest resources is implemented every 5-10 years. Accurate forest data is the base for planning forest management activities.\nUntil 2014 commercial forests were managed with an even aged structure because of legislation. Sustainable forest management in Finland concentrates on finding a balance between economic, social and environmental aspects. These are also the basic pillars for the two forest certification schemes PEFC and FSC that are actively used in Finland. Over ninety percent of Finnish commercial forests are certified under these schemes.\nDeforestation in Finland has been rather small-scaled and caused by agriculture and infrastructure building. The successful forest industry has laid the foundation for forests to remain forests. Forests have been and will continue to be managed sustainably and actively. Furthermore, the number of natural disasters in forests has been small compared to several European countries as a result of climate conditions and good forest management."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:e4b35f44-1607-44cb-bc35-9f13f21fc12e>","<urn:uuid:6ad5d7e9-5f35-4cdc-af69-6ce0e61c671d>"],"error":null}
{"question":"What are the key differences between Brazilian and Ecuadorian coffee cultivation practices in terms of farm size and crop diversity?","answer":"Brazilian coffee is grown in large-scale production across states like Paraná, Espirito Santos, São Paulo, Minas Gerais, and Bahia, focusing primarily on varietals like Bourbon, Typica, Caturra, and Mundo Novo. In contrast, Ecuadorian coffee is mostly grown on small several-hectare fincas where coffee is co-planted with other crops like cacao, citrus fruits, bananas, and mangoes, which provides interesting flavored beans. This shows a contrast between Brazil's large-scale, coffee-focused production and Ecuador's smaller, diversified farming approach.","context":["Coffee was introduced in Brazil by Francisco de Mello Palheta in 1727 from Cayenne, French Guiana. Today, Brazil is the world’s largest coffee producer and is becoming a significant player in the specialty coffee industry. Bourbon, Typica, Caturra, and Mundo Novo coffee varietals are grown in the states of Paraná, Espirito Santos, São Paulo, Minas Gerais, and Bahia. Bourbon and Typica are the two most popular Arabica coffees grown worldwide, Typica being the base plant of many coffee varietals. The highest grade of well-known Brazilian beans, Bourbon Santos is a coffee to savor at any time of day. It is smooth and pleasant with fruity notes, medium body and mild flavor. Next time you buy coffee, check the origin of the beans. It’s quite possible they will be Brazilian.\nOne of the great things about Idaho Springs was its small town atmosphere of shops lining Main Street and people visiting on street corners, without some of the provincial thinking that could make a small town oppressive. Every community, be it big or small, had its characters, the people who inspired the “only in …” phrase. Idaho Springs embraced their citizens who teetered on the ragged edge of sanity, certain that without them, the town simply wouldn’t be right. There was the man whose name nobody was sure of, because he tended to claim to be someone different every week. This week he was Teddy Roosevelt, sporting a very dapper cowboy hat with the right brim tacked up, and a pair of rounded specs with no glass in them. He loved reminiscing about San Juan Hill as he sipped his morning latte` at Java Mountain Roasters. Asta always gave him free biscotti if he promised to stay at his own table and let the other patrons come to him rather than the other way around. When Ricky stepped into the coffee shop she was immediately enveloped in the aromas of fresh, deeply roasted coffee, and the smile of one Stanton Christophersen, who was currently rapt in conversation with Teddy. Ricky ordered a Pumpkin Spice Breve Latte with whipped cream and a salted, caramel biscotti and sat down next to Teddy, across from the adorable Stan. He was handsome this morning. She sipped her drink, happy to revel the charms of these two interesting men.\nKona coffee is Coffea arabica which is cultivated on the slopes of Hualalai and Mauna Loa in the North and South Kona Districts of the Big Island of Hawaii. It is one of the most expensive coffees in the world because only coffee from the Kona Districts can be described as “Kona”. The heritage trees in the Kona districts have been developed over the past 175 years on the west side of Hawaii. The mix of sunshine and rainfall, combined with porous, mineral rich volcanic soil creates favorable coffee growing conditions and produces a coffee that is typically mild and sweet with a hint of spice.\nThe coffee plant was brought to the Kona district in 1828 by Reverend Samuel Ruggles, from Brazilian cuttings. The coffee market crashed in 1899, and big coffee plantations became a thing of the past, replaced by small family farms. The tradition of running family farms has continued throughout Kona to this day. Since the first introduction of coffee here, Hawaii has been the only state in the US to commercially grow it. According to the Kona Coffee Council, the rocky location and the fact that the coffee does not ripen all at the same time means the Kona trees cannot be mechanically harvested. Since they must inspect each bean as they are picked, you are assured a perfect cup of coffee, which was picked when ripe, and not a combination of immature or overripe beans. This excellent quality has made Kona coffee one of the most highly valued coffees in the world.\nCoffee was first introduced to Colombia in 1723, presumably by Jesuit priests that brought the seeds from Venezuela. The country produces about 12% of the coffee in the world, second only to Brazil. Colombian coffee is often regarded as some of the highest quality coffee in the world. Colombia has traditionally grown arabica beans although today Bourbon, Typica, Caturra, and Maragogype coffee varietals are cultivated. Its unique geography makes Colombia perfectly suited for producing a delicious, high quality brew. Arabica beans come from a species of coffee originally indigenous to the mountains of Yemen in the Arabian Peninsula, hence its name. It is also known as the “coffee shrub of Arabia”, “mountain coffee” or “arabica coffea”. Coffea arabica is believed to be the first species of coffee to be cultivated, being grown in southwest Arabia for well over 1,000 years. Gourmet coffees, such as Colombian coffee, are almost exclusively high-quality mild varieties of arabica coffee. The climate in Colombia has traditionally been hot and dry enough to grow this variety very successfully, although climate change in the last twenty years has caused coffee production to drop off some in the region. These beans are so popular, the next time you’re in a restaurant and order a cup of Joe, chances are you’ll be drinking Colombian. Freshly roasted Colombian coffee beans are rich in flavor, heavy bodied, have a bright acidity, and are intensely aromatic. So drink up – and enjoy!\nCoffee from different regions of the globe have distinct flavors. This is because the plant and its fruit are affected by the soil and surrounding environment. Traditionally, it is thought that coffee originated in Africa. From there it spread into Arabia and eventually Italy. Coffee beans are still grown in African and beans from this region are very distinctly flavored. Coffee beans that are grown in Kenya have a bold flavor that has an overtone of black currant. This berry flavor is very unique and gains fans from across the globe.\nEast African coffee beans, in particular, are loved for the intense body they yield, as well as their spicy acidity. These include beans from Ethiopia and Tanzania. If all you drink is Columbian coffee, you can change your game by sampling the unique flavor of African coffee.\nCaffè breve is an American variation of a latte: a milk-based espresso drink using steamed half-and-half instead of milk. The snows bring with them a desire for warmth and comfort making this a perfect cold-weather drink – richer and slightly less sweet than a latte. For a special treat to welcome the autumn snow, try a Pumpkin Spice Breve from Java Mountain Roasters. It’s delectable ~ a bit like pumpkin pie in a cup. Do you love the winter weather? This will add to the joy of the season. Do you not? A breve is a truly wonderful way to console yourself. Drink up – you deserve it.","The coffee industry is worth over $ 100 billion world wide ( more than gold ) employing over 120 million people throughout the world. It is the most sought after commodity after oil. Coffee was introduced in Ecuador early in the 19th century, and remained one of Ecuador's top export crops into the 1970s. Today it has been replaced by oil, shrimp and bananas.\nQuality of coffee varies with genetic variety, altitude, weather, air, and soil - similar to wines and cacao. The varied ecosystems in Ecuador provide different coffee growing cultures, which creates complex and varied flavours.\nAdditionally growing, processing, roasting, and finally the method of extraction all impact the coffee's flavour. This is why Ecuadorean coffee beans make some of the most interesting and unique cups of joe on our planet.\nUntil recently our premium coffees, as with most agricultural products, are exported. Over 30 countries worldwide. Russia, Poland, Germany, Colombia, Italy, and the Netherlands buy over 80 percent of our total export volume. In 2015 Ecuador's total annual coffee production was about 42,600 tons ( 644,000 - 60 kg bags ) from 200,000 hectares under cultivation. World coffee production is 9.7 million tons annually - so Ecuador is a very small player producing less than .5 % of world production.\nHowever there is some recognition, and growing local demand for our home grown quality coffee. It is still fairly rare as Ecuadorians prefer, and most cafes and restaurants here use instant coffee. Actually 86 % of Ecuador's coffee production is high yield, low quality instant coffee. Most coffee is grown on small several hectare fincas, where it is co-planted with cacao, citrus fruits, bananas, and or mangoes to provide interesting flavoured beans.\nToday's educational coffee tour is to southern Ecuador's Loja, one of the most important coffee growing regions. About 5.5 hours south of Cuenca we arrive at a coffee farm in Vilcabamba.\nThe coffee plants seem randomly scattered, and grow on steep very irregular misty hillsides where the morning sun gains momentum in drying the overnight dew.\nThis high altitude is close to the sun for hotter days, colder nights, and less oxygen causing the plant more stress, which forces it to send most of its nutrients to its seed. From a North American agriculture perspective the organic farms are very natural, primitive, and poorly maintained, and as a result provide marginal yields. Protected by the haughty spirit of Saraguro's indigenous people, these 100 % arabica beans that are grown at an elevation of 1,500 meters are of high quality.\nArabica beans taste and smell better, however, they are much more susceptible to diseases and harsh weather. The primary growing regions in Ecuador for Arabica and Robusta coffee beans are:\nLoja is one of the most important regions for quality-growing altitude between 1,000 and 2,000 meters;\nPichincha a newer region-growing altitude between 1,000 and 1,800 meters;\nZamora - Chinchipe - growing altitude between 800 and 1,800 meters;\nCarchi-growing altitude between 1,200 and 1,800 meters;\nManabi oldest region with higher yields of poorer quality- growing altitude between 200 and 700 meters;\nGalapagos - growing altitude between 300 and 400 meters;\nEl Oro growing altitude between 500 and 1,300 meters;\nThe \" bean belt \" for growing coffee is between the latitudes of 25 degrees North and 30 degrees South. Coffee plants can live for up to 100 years, but risk of diseases such as leaf rust and roya increases with plant age. Prices fluctuate and when too low to justify harvesting the beans are left to rot on the bushes which causes problems with pests. On this finca the soil is rich and dark, the steep mountainside provides altitude, varying climate, and fresh air. Pigs and chickens are raised to \" weed and aerate the soil \" and contribute organic matter.\nPlants are usually about five years old when they first flower ( twice annually ) and set a green cherry. After about 8 months the cherry ripens and turns red. When the berries are red and ready for harvesting they are hand picked - a difficult and labour intensive process. Selectively harvesting only the ripe cherries a typical worker will collect 50 kilos daily. Red berries, with their higher aromatic oil and lower organic acid content, are more fragrant, smooth, and mellow. Coffee picking is one of the most important stages in coffee production.\nFarmers deliver their daily harvests to a local cooperativa for processing. Processing production encompasses the drying (wet milling) process, threshing / dry milling, classification, and roasting of the coffee beans. All green coffee is processed however the methods vary and have a significant effect on the flavor.\nFirst is the wet milling process of which there are two main ways to dry the coffee.\ni.) The wet process immediately depulps the bean. After the pulp has been removed the bean still has two additional layers a silver skin and the parchment. It is then left to sit overnight so that a sugar layer called the mucilage can break down and then the coffee is washed. Once all the sugars are washed off to eliminate the risk of fermentation, the coffee in parchment is placed on elevated beds to dry for about a week. This process is used for exported coffee.\nii.) The dry or natural process spreads the beans out to dry in the sunshine on cement patios. They are regularly raked to prevent mold and ensure uniform drying. When coffee is dried in the fruit, the sugars of the fruit ferment and seep into the bean, if done properly these can give exotic fruity undertones and even wine notes to the coffee, but if done wrong the coffee can have a strong taste of rotten fruit. It is much less labour intensive and no water is required. It is used primarily for domestic consumption known as bola.\nThe beans must be dried to a water content of about 10% before they are stable.\nNext in the threshing, or hulling process, a rustic machine removes the dry husk parchment from the green bean and sorts the beans according to size. The smallest beans are the sweetest.\nThese cleaned and sorted beans then fill sacks which are then sent to a cooperativa roaster for the final processes of tostión or roasting.\nThe beans are now officially considered coffee. The aroma experienced from grinding the beans was heavenly.\nCoffee is not a formula like Coca Cola or Sunkist orange juice. The coffee flavour changes depending on many different factors. There are four points in a coffee´s lifespan where its flavour can drastically be changed.\nThe first being its genetics: altitude, micro region and varietal. The coffee cherry's quality is at its peak when harvested deep red.\nThe second being how it is processed can showcase the inherent flavour attributes of the bean. The third being the roast level, whether light, medium or dark. And finally the fourth being how we extract the coffee, whether espresso, pour over, French press or simply cowboy style.\nIt is because of these nuances that people developed a system to classify coffee. In recent years experts have even begun to borrow tools from the wine industry, most importantly the wine flavor wheel.\nThe flavor wheel describes different aromas and flavors in your cup of Joe. The quality is determined by several characteristics: flavour, fragrance and aroma, residual taste, acidity, body, uniformity, and clarity.\nNow it's time to test your taste buds with a coffee challenge. You can identify fresh aromas ranging from ripe fruits and spices to chocolate and even a garlic/onion combination in different roasts of coffee beans before, and after grinding. Contrary to popular belief, the lighter the roast the higher the caffeine content. Also in lighter roasted coffees you can detect more unique aromas and flavors.\nFinally, hot water is added to the ground coffee to enable you to taste each variety. This tasting portion of the tour focuses on immersing the senses in the characteristics of the coffee. Guests learned to identify different coffees by aroma, flavor, body, and acidity. This allows them to differentiate between different types and qualities of coffee. It was an overwhelming sensory experience, though the professionals make it look easy.\nEcuadorian barista 2014 & 2015 champion Diego Felipe Mejia has been invited to compete in the Third Global Barista Championship, December 2, 3 and 4, in Harbin, China. He will use San Lorenzo brand coffee grown at Hacienda La Papaya in Loja Province.\nEcuadorian coffee has gained international recognition in recent years, Mejia says one of his missions is to promote Ecuadorian coffee around the world. “We have some of the best anywhere,” he says. “It has an excellent balance, is bright and rich, and has a smooth, clean taste.”\nThere is still some controversy about health benefits or side effects from caffeine. It is mood enhancing, increases alertness and performance, and has antibacterial properties for prevention of dental cavities. The negative side effects include insomnia, and diuretic effects - in excess nervousness, and accelerating heartbeat.\nThe Consejo Cafetalero Nacional is one of the institutions in Ecuador which promote the development of the industry. However, government COFENAC, CORPEI, are primarily interested in high-yield production and sales, not quality. Most government funds are going into Robusta plantations and low quality Arabica plantations and very few make it to high quality regions. In 2014, 86% of their coffee exports were instant coffee and only 14% green coffee. and ANCAFE (Asociación Nacional de Exportadores de Café) are amongst Ecuador's leading coffee companies.\nThe country lacks a solid coffee agency to promote good practices and give technical assistance to farmers, such as the Federation of Coffee Growers (FNC) in Colombia. This is due to lack of funding, and the fact that coffee is not considered a primary agricultural activity in Ecuador like it is in Colombia. Focus has been on production volumes and lower quality coffees.\nRemember fair trade coffee purchases allow you to make a direct impact on the lives of small-scale coffee producers and their families. Or even better plan your next vacation around a coffee country tour. We hope that you learned something new today and enjoyed"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:7711a11f-bdd7-4f40-a1b7-a536e9e97c3c>","<urn:uuid:7aff7798-d483-413a-bd4e-8bbc20d555e3>"],"error":null}
{"question":"How do the pastoral duties of a traditional pastor compare to those of a cantor in Jewish worship?","answer":"Traditional pastors and cantors have different roles in religious leadership. Pastors are expected to be fully devoted to caring for their flock, feeding them spiritually, protecting them from harm, and giving themselves up for their sheep's sake - they should not engage in other occupations. In contrast, cantors (chazans) have a more specialized role focused on preserving synagogue music and leading prayers, psalms, and blessings with their trained voices. While pastors are the primary spiritual leaders, cantors may act as spiritual leaders only if the community lacks a rabbi. Cantors create special atmospheric moments during festivals and ceremonies through their singing, whereas pastors focus on overall spiritual guidance and protection of their congregation.","context":["|« Prev||Of Pastors.||Next »|\nDemonstration X.—Of Pastors.\n1. Pastors are set over the flock, and give the sheep the food of life. Whosoever is watchful, and toils in behalf of his sheep, is careful for his flock, and is the disciple of our Good Shepherd, who gave Himself in behalf of His sheep.10171017 John x. 11, sq. And whosoever brings not back his flock carefully, is likened to the hireling who has no care for the sheep. Be ye like, O Pastors, to those righteous Pastors of old. Jacob fed the sheep of Laban, and guarded them and toiled and was watchful, and so received the reward. For Jacob said to Laban:—Lo! twenty years am I with thee. Thy sheep and thy flocks I have not robbed and the males of thy sheep I have not eaten. That which was broken I did not bring unto thee, but thou required it at my hands! In the daytime the heat devoured me and the cold by night.10181018 Gen. xxxi. 38, 40. My sleep departed from my eyes. Observe, ye Pastors, that Pastor, how he cared for his flock. He used to watch in the night-time to guard it and was vigilant; and he used to toil in the daytime to feed it. As Jacob was a pastor, so Joseph was a pastor and his brethren were pastors. Moses was a pastor, and David also was a pastor. So Amos was a pastor. These all were pastors who fed the sheep and led them well.\n2. Now, why, my beloved, did these pastors first feed the sheep, and were then chosen to be pastors of men? Clearly that they might learn how a pastor cares for his sheep, and is watchful and toils in behalf of his sheep. And when they had learned the manners of pastors, they were chosen for the pastoral office. Jacob fed the sheep of Laban and toiled and was vigilant and led them well; and then he tended and guided well his sons, and taught them the pattern of pastoral work. And Joseph used to tend the sheep along with his brethren; and in Egypt he became guide to a numerous people, and led them back, as a good pastor does his flock. Moses fed the sheep of Jethro his father-in-law, and he was chosen from (tending) the sheep to tend his people, and as a good pastor he guided them. Moses bore his staff upon his shoulder, and went in front of his people that he was leading, and tended them for forty years; and he was vigilant and toiled on behalf of his sheep, a diligent and good pastor. When his Lord wished to destroy them because of their sins, in that they worshipped the calf, Moses prayed and besought of his Lord and said:—Either pardon the people for their sins, or else blot me out from Thy book that Thou hast written.10191019 Ex. xxxii. 31, 32. That is a most diligent pastor, who delivered over himself on behalf of his sheep. That is an excellent leader, who gave himself in behalf of his sheep. And that is a merciful father who cherished his children and reared them up. Moses the great and wise shepherd, who knew how to lead back the flock, taught Joshua the son of Nun, a man full of the spirit, who (afterwards) led the flock, even all the host of Israel. He destroyed kings and subdued the land, and gave them the land as a place of pasturage, and divided the resting-places and the sheepfolds to his 384sheep. Furthermore, David fed his father’s sheep, and was taken from the sheep to tend his people. So he tended them in the integrity of his heart and by the skill of his hands he guided them.10201020 Ps. lxxviii. 72. And when David numbered the flock of his sheep, wrath came upon them, and they began to be destroyed. Then David delivered himself over on behalf of his sheep, when he prayed, saying:—O Lord God, I have sinned in that I have numbered Israel. Let Thy hand be on me and on my father’s house. These innocent sheep, in what have they sinned?10211021 2 Sam. xxiv. 17. So also all the diligent pastors used thus to give themselves on behalf of their sheep.\n3. But those pastors who did not care for the sheep, those were hirelings who used to feed themselves alone. On this account the Prophet10221022 Ezek. xxxiv. 2–4, 9, 10–12, 18, 19. addresses them, saying to them:—O ye pastors who destroy and scatter the sheep of my pasture, hear the word of the Lord. Thus saith the Lord: Lo! I will visit My sheep as the pastor visits his flock in the day of the whirlwind, and I will require My sheep at your hands. O foolish pastors, with the wool of the sheep do ye clothe yourselves and the flesh of the fatlings do eat, and the sheep ye do not feed. That which was sick ye did not heal, and that which was broken ye did not bind. The weak ye did not strengthen, and the lost and the scattered ye did not gather together. The strong ones and the fatlings ye did guard, but with harshness ye subdued them. The good pastures ye yourselves graze upon, and what remains ye trample with your feet. The pleasant waters do ye drink, and whatever remains ye defile with your feet. And My sheep have eaten the trampled (herbage) which your feet have trampled, and they have drunk the waters which your feet have defiled. These are the greedy and base pastors and hirelings, who did not feed the sheep, or guide them well, or deliver them from the wolves. But when the Great Pastor, the chief of pastors, shall come, He will call and visit His sheep and will take knowledge of His flock. And He will bring forward those pastors, and will exact an account from them, and will condemn them for their deeds. And those who fed the sheep well, them the Chief of Pastors will cause to rejoice and to inherit life and rest. O stupid and foolish pastor, to whose right hand and to whose right eye I committed my sheep. Because thou didst say concerning the sheep, let that which dieth, die, and let that which perisheth perish, and whatever is left, let them devour the flesh of one another; therefore, behold I will make blind thy right eye and I will wither up thy right arm. Thy eye which regarded a bribe shall be blinded, and thy hand which did not rule in righteousness shall waste away.10231023 Zech. xi. 9, 17. And as for you, my sheep, the sheep of my pasture, ye are men; but I am the Lord your God.10241024 Ezek. xxxiv. 31. Behold henceforth will feed you in a good and rich pasture.10251025 Ezek. xxxiv. 14.\n4. The good shepherd giveth himself for the sake of his sheep.10261026 John x. 11. And again He said:—I have other sheep and I must bring them also hither. And the whole flock shall be one, and one shepherd, and My Father because of this loveth Me; that I give Myself for the sake of the sheep.10271027 John x. 16, 17. And again He said;—I am the door of the sheep. Every one that entereth by Me shall live and shall go in and go out and find pasture.10281028 John x. 9. O ye pastors, be ye made like unto that diligent pastor, the chief of the whole flock, who cared so greatly for his flock. He brought nigh those that were afar off. He brought back the wanderers. He visited the sick. He strengthened the weak. He bound up the broken. He guarded the fatlings. He gave himself up for the sake of the sheep. He chose and instructed excellent leaders, and committed the sheep into their hands, and gave them authority over all his flock. For He said to Simon Cephas:—Feed My sheep and My lambs and My ewes.10291029 John xxi. 15–17. So Simon fed His sheep; and he fulfilled his time and handed over the flock to you, and departed. Do ye 385also feed and guide them well. For the pastor who cares for his sheep engages in no other pursuit along with that. He does not make a vineyard, nor plant gardens, nor does he fall into the troubles of this world. Never have we seen a pastor who left his sheep in the wilderness and became a merchant, or one who left his flock to wander and became a husbandman. But if he deserts his flock and does these things he thereby hands over his flock to the wolves.\n5. And remember, my beloved, that I wrote to thee concerning our fathers of old that they first learned the ways of tending sheep and in that received trial of carefulness, and then were chosen for the office of guides, that they might learn and observe how much the pastor cares for his flock, and as they used to guide the sheep carefully, so also might be perfected in this office of guidance. Thus Joseph was chosen from the sheep, to guide the Egyptians in the time of affliction. And Moses was chosen from the sheep, to guide his people and tend them. And David was taken from following the sheep, to become king over Israel. And the Lord took Amos from following the sheep, and made him a prophet over his people. Elisha likewise was taken from behind the yoke, to become a prophet in Israel. Moses did not return to his sheep, nor did he leave his flock that was committed to him. David did not return to his father’s sheep, but guided his people in the integrity of his heart.10301030 Ps. lxxviii. 72. Amos did not turn back to feed his sheep, or to gather (the fruit of) trees, but he guided them and performed his office of prophecy. Elisha did not turn back to his yoke, but served Elijah and filled his place. And he10311031 Sc. Gehazi. who was for him as a shepherd, because he loved fields and merchandise and vineyards and oliveyards and tillage, did not wish to become his disciple; and (therefore) he did not commit the flock into his hand.\n6. I beseech you, ye pastors, that ye set not over the flock, leaders who are foolish and stupid, covetous also and lovers of possessions. Every one who feeds the flock shall eat of their milk.10321032 1 Cor. ix. 7, sq. And every one who guides the yoke shall be ministered to from his labour. The priests have a right to partake of the altar, and the Levites shall receive their tithes. Whoever eats of the milk, let his heart be upon the flock; and let him that is ministered to from the labour of his yoke, take heed to his tillage. And let the priests who partake of the altar serve the altar with honour. And as for the Levites who receive the tithes, they have no portion in Israel. O pastors, disciples of our great Pastor, be ye not like hirelings; because the hireling cares not for the sheep. Be ye like our Sweet Pastor, Whose life was not dearer to Him than His sheep. Rear up the youths and bring up the maidens; and love the lambs and let them be reared in your bosoms; that when ye shall come to the Chief Pastor, ye may offer to Him all your sheep in completeness, and so He may give you what He has promised: Where I am, ye also shall be.10331033 John xii. 26. These things, brief as they are, will be sufficient for the good pastors and leaders.\n7. Above, my beloved, I have written to remind thee of the character that becomes the whole flock. And in this discourse I have written to thee about the pastors, the guides of the flock. These reminders I have written to thee, beloved, as thou didst ask of me in thy dear letter.\n8. The Steward brought me into the King’s treasury and showed me there many precious things; and when I saw them my mind was captivated with the great treasury. And as I looked upon it, it dazzled my eyes, and took captive my thoughts, and caused my reflections to wander in many ways. Whosoever receives thereof, is himself enriched, and enriches (others). It lies open and unguarded before all that seek it; and though many take from it there is no deficiency; and when they give of that which 386they have received, their own portion is greatly multiplied. They that receive freely let them give freely10341034 Matt. x. 8. as they have received. For (this treasure) cannot be sold for a price, because there is nothing equivalent to it. Moreover the treasure fails not; and they that receive it are not satiated. They drink, and are still eager; they eat, and are hungry. Whosoever is not thirsty, finds not ought to drink; whoever is not hungry, finds nothing to eat. The hunger for it satisfies many, and from the thirst for it flow forth water-springs. For the man who draws nigh to the fear of God is like the man who in his thirst draws near to the water-spring and drinks and is satisfied, and the fountain is not a whit diminished. And the land that needs to drink in water, drinks of the fountain, but its waters fail not. And when the land drinks, it needs again to drink, and the spring is not lessened by its flowing. So is the knowledge of God. Though all men should receive of it, yet there would come no lack in it, nor can it be limited by the sons of flesh. He that takes from it, cannot take away all; and when he gives, he lacks nothing. When thou takest fire with a candle from a flame, though thou kindle many candles at it, yet the flame does not diminish when thou takest from it, nor does the candle fail, when it kindles many. One man cannot receive all the King’s treasure, nor when a thirsty man drinks of the fountain, do its waters fill. When a man stands on a lofty mountain, his eye does not (equally) comprehend the near and the distant; nor, when he stands and counts the stars of heaven, can he set limits to the hosts of the heavens. So when he draws nigh unto the fear of God, he cannot attain to the whole of it; and when he receives much that is precious, it does not seem to be diminished; and when he gives of that which he has received, it is not exhausted, nor has it come to an end for him. And remember, my beloved, what I wrote to thee, in the first discourse, about faith, that whoever has freely received ought to give freely as he has received, as our Lord said:—Freely ye have received, freely give.10351035 Matt. x. 8. For whosoever keeps back part of anything he has received,10361036 Matt. xxv. 29. even that which he has obtained shall be taken away from him. Therefore, my beloved, as I have been able to obtain now from that treasure that fails not, I have sent unto thee from it. Yet though I have sent it to thee, it is all with me. For the treasure fails not, for it is the wisdom of God; and the steward is our Lord Jesus Christ, as He testified when He said:—All things have been committed to Me by My Father.10371037 Matt. xi. 27. And while He is the steward of the wisdom, again, as the Apostle said:—Christ is the power of God and His wisdom.10381038 1 Cor. i. 24. This wisdom is imparted to many, yet nothing is lacking, as I explained to thee above; the Prophets received of the spirit of Christ, yet Christ was not a whit diminished.\n9. Ten treatises have I written unto thee, my beloved. Whatsoever thou hast asked of me, I have explained to thee without (receiving) ought from thee. And that which thou enquiredst not of me, I have given unto thee. I have asked thy name and written unto thee. I have asked of myself thy question, and I have answered thee as I was able, for thy persuasion. Whatsoever I have written unto thee, meditate in these things at every time; and labour to read those books which are read in the church of God. These ten little books that I have written for thee, they borrow one from another, and depend one upon another. Separate them not one from another. From Olaph to Yud I have written for thee, each letter after its fellow. Read thou and learn thou and the brethren, the monks, and the faithful, they from whom mocking is far removed; as I wrote unto thee above. And remember that which I pointed out to thee, that I have not brought these matters to an end, but short of the end. Nor are these 387things sufficient; but hear thou these things from me without wrangling, and enquire concerning them with brethren who are apt for persuasion. Whatsoever thou hearest that assuredly edifies, receive; and whatever builds up strange doctrines, overthrow and utterly demolish. For wrangling cannot edify. But I, my beloved, as a stonecutter have brought stones for the building, and let wise architects carve them out and lay them in the building; and all the labourers that toil in the building shall receive reward from the Lord of the house.\n|« Prev||Of Pastors.||Next »|","Role of Rabbi\n- Means 'teacher', 'sir' or 'my master'\n- Not a priest, though may coincidentally be a 'cohen' (of priestly line)\n- Spiritual leader of a Jewish community\n- Title itself seems to originate around the time of the destruction of the Temple (70 CE)\n- His legal work is confined to religious matters and questions of conversion, marriage and divorce\n- His function is mainly the interpretation and teaching of Jewish tradition, the guiding of and preaching to the people and the pastoral role of caring for the community\n- Strives to be a dedicated 'scholar of the Torah'\n- Must be able to advice on how to apply Jewish law (Halachah) to everyday life\n- He also counsels, giving advice, preparing Bar Mitzvah candidates, couples for marriage, offering advice on all matters of family life and community living\n- Pastoral duties include visiting the sick, the bereaved, those in prison, caring for Jewish students attached to any nearby university or college\n- In traditional communities, Rabbis spend much of their day studying, teaching or deciding on matters of Jewish law. Some Jews bring disputes to the Rabbi for settlement according to Jewish law rather than civil courts\n1 of 5\n- The Rabbi presides over services and ensures that everything is conducted according to Jewish law\n- He ensures that the structure of services follows the SIDDUR (daily prayer book)\n- He reads or oversees the Torah and Haftorah readings or any special readings on festivals (e.g. the Scroll of Esther for Purim)\n- The correction portion (SIDRA) of the Torah must be read.\n- He prepares for and delivers the weekly sermon\n- Training - today, those preparing for the Rabbinate usually study in a rabbinical college (seminary) or a YESHIVA. Some have a degree is Jewish studies and other disciplines before embarking on the rabbinical course.\n- Rabbis HAVE NEVER BEEN SEEN AS INTERMEDIARIES BETWEEN GOD AND HUMANS. They are not thought to be imbued with special powers. Their status delivers largely from their scholarship.\n- A synagogue service need not be conducted by a rabbi. In orthodoxy, any able congregant may lead prayers. However, as the rabbi has come to be viewed more as a 'minister of religion', conducting services has come to be viewed as part of his role.\n2 of 5\nRole of Cantor (chazan)\n- Reuben Turner: 'The Chazan has the task of preserving synagogue music'\n- Many synagogues employ a cantor to lead prayers, Psalms and blessings.\n- He has a trained voice which often creates a special mood or atmosphere at key Jewish festivals (eg. Kol Nidrei at Yom Kippur) or Rites of Passage (the Chazan sings the betrothal and nuptial blessings at a wedding - Kiddushin)\n- Delivery is in the form of a TROP or CHANT and there are particular ones for key festivals\n- Usually not ordained, though he has had training in Torah and Siddur\n- May teach in the community\n- If the community lacks a rabbi, he may act as spiritual leader\n- Origins of this role may come from the LEVITES who sang in the Temple - song and prayer have always been a part of Jewish liturgy\n3 of 5\nRole of Men\n- Clear distinction made that men should take lead in public life (eg. synagogue)\n- Required to pray three times a day, at set times, (shacharit - morning, mincha - middle day and maariv - evening) This does not have to be in the synagogue. but if they are able to go there then they will\n- Can make up a minyan (required for congregational prayer)\n- Required to wear ritual dress (kippah/yarmulke, tallit, tefillin)\n- Only men can enter the rabbinate\n- Study of Talmud is a male domain, women should encourage their husbands to do so\n- Males responsible for opening the Ark and carrying the Torah scrolls to Bimah\n- Rabbi, Cantor and choir members are male\n- Only men can lead synagogue worship\n- Enter into a covenant with God through 'Brit Millah' - The covenant of cutting, Wouk 'A mark at the source of life', Mohel is also male\n- Presides over the Sabbath Rituals and Pesach rituals\n- Orthodox is very much based on Genesis 2 idea of creation, that Eve was made from Adam's rib, ie. their roles are complementary but distinctive\n4 of 5\n- No set roles men are required to fulfill, they may do whatever women do if they please eg. take lead role in family life\n- Emphasis is placed on Genesis 1 - man and woman were made in the image of God, 'imago dei' - EGALITARIAN\n- Therefore in reform tradition, it is very much down to the personal decision of the couple on which role is fulfilled by whom\n5 of 5"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:37d73ed4-f173-4a6e-8f7c-ae164e513ba8>","<urn:uuid:1be026e1-a4a2-4ac8-8ec0-15d5d2228407>"],"error":null}
{"question":"Does light have similar symbolic meanings in Art Deco interiors and Luminist paintings?","answer":"No, light serves different symbolic purposes in these styles. In Luminist paintings, light is used to create tranquility and contemplative perception of nature, often depicting soft, hazy skies and calm waters. In Art Deco interiors, light is used to create drama and luxury, with bright, intense lighting designed to reflect off shining metals and polished surfaces, celebrating the machine age and industrial progress rather than natural serenity.","context":["- Title: A View of a Lake in the Mountains\n- Artist: George Caleb Bingham\n- Medium: Oil on canvas\n- Date: between circa 1856 and circa 1859\n- Dimensions: Height: 53.9 cm (21.2 ″); Width: 76.5 cm (30.1 ″)\n- Current Location: Los Angeles County Museum of Art\nWhat I love about this picture:\nThis scene depicts an hour of utter serenity in the turbulent life of the artist. The late afternoon sunlight falls gently on the rocky path above the calm waters. Shadows fall in all the right places but don’t darken the moment. There is a dreamlike quality to the day, as if the artist painted his deepest wish. This is a pleasant, restful painting.\nAbout the Artist, via Wikipedia:\nGeorge Caleb Bingham (March 20, 1811 – July 7, 1879) was an American artist, soldier and politician known in his lifetime as “the Missouri Artist”. Initially a Whig, he was elected as a delegate to the Missouri legislature before the American Civil War where he fought the extension of slavery westward. During that war, although born in Virginia, Bingham was dedicated to the Union cause and became captain of a volunteer company which helped keep the state from joining the Confederacy, and then served four years as Missouri’s Treasurer. During his final years, Bingham held several offices in Kansas City, as well as became Missouri’s as Adjutant General. His paintings of American frontier life along the Missouri River exemplify the Luminist style.\nBingham ran for election as a Whig to the Missouri House of Representatives the following year. He appeared to have won in 1846 by 3 votes but lost in a recount. In a reprise of the election in 1848, Bingham won the seat by a decisive margin, becoming one of the few artists to serve in elected political office. He actively opposed the pro-slavery “Jackson resolutions” in 1849, although their proponent was also a resident of Saline County. He would also represent Missouri’s eighth district at the Whig National Convention in June 1852. Bingham’s political interests would be reflected in his vivid paintings of frontier political life.\nAbout the Luminist style, via Wikipedia:\nLuminism is an American landscape painting style of the 1850s to 1870s, characterized by effects of light in landscape, through the use of aerial perspective and the concealment of visible brushstrokes. Luminist landscapes emphasize tranquility, and often depict calm, reflective water and a soft, hazy sky.\nAs defined by art historian Barbara Novak, luminist artworks tend to stress the horizontal, and demonstrate the artist’s close control of structure, tone, and light. The light is generally cool, hard, and non-diffuse; “soft, atmospheric, painterly light is not luminist light”. Brushstrokes are concealed in such a way that the painter’s personality is minimized. Luminist paintings tend not to be large so as to maintain a sense of timeless intimacy. The picture surface or plane is emphasized in a manner sometimes seen in primitivism. These qualities are present in different amounts depending on the artist, and within a work.\nLuminism has also been considered to represent a contemplative perception of nature.\nThe artists who painted in this style did not refer to their own work as “luminism”, nor did they articulate any common aesthetic philosophy outside of the guiding principles of the Hudson River School.\nCredits and Attributions:\nA View of a Lake in the Mountains by George Caleb Bingham, via Wikimedia Commons. Los Angeles County Museum of Art [Public domain].\nWikipedia contributors, “George Caleb Bingham,” Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=George_Caleb_Bingham&oldid=900386053 (accessed June 6, 2019).\nWikipedia contributors, “Luminism (American art style),” Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Luminism_(American_art_style)&oldid=886912140 (accessed June 6, 2019).","The art deco style is specific in its design. It includes manmade materials in geometric shapes with rich and vibrant colors combined to have a luxurious appearance. The art deco style is reminiscent of the golden industrial age of the 1920’s and it endured through the great depression age of the 1930’s. This represents how the style is timeless in its beauty and class no matter which room or rooms you want to upgrade with art deco style.\nWhat Exactly is Art Deco?\nArt deco includes many different factors that can be combined to make an exquisite formal room. Geometrical shapes appeared when this style first evolved. The shapes were made of manmade items in both the architecture and decor. Metals of goldtone, silvers chromes and other reflective materials including mirrors were used extravagantly for a lot of shine and sophistication. Inlaid wood is a popular art deco style along with rich leathers, bright colors and many textures. The geometric shapes include stepped forms, sunburst, curves and chevron patterns throughout the room, which also are found in the window treatments and throw pillows to tie the entire look in with the room. Many of the motifs were adapted from Egyptian and Aztec motifs, which give a homeowner many different things to choose from to make an art deco room.\nWhat Types of Furniture are Considered Art Deco?\nArt deco furniture has a very sleek design that combines modern style and comfort all in one area. Wood frames are heavily lacquered most often in black and upholstery is generally done in either velour or leather. If you choose furniture with a bright color it is combined with gold, chrome and black in the accents and you can choose a light blue or gray to soften the design scale so it doesn’t’ look too harsh.\nWhat are Art Deco Styles of Materials?\nThe range of materials in the art deco style is tremendous. It can be quite expensive as in the case of sharkskin or zebra or ebony accents. Rare woods with inlaid intricate designs as well as marble are art deco materials. Flooring is often minimalist in nature and includes the sleekness of black or white in marble or a black and white checkerboard print in tile flooring. You can also find flooring that is one color in a wood finish as a naturally dark stained floor or white wooden flooring for a great contrast. Art deco can include area rugs if they have only one or two colors and especially if they contain a beautiful geometrical pattern.\nWhat Types of Lighting are in the Art Deco Style?\nThe lighting types in art deco are perhaps the most specific in their design to achieve the decorating style. Lights of all types are streamlined in design along with all of the other interior accessories. Geometric shapes are incorporated into each and every type of light along with the metals in bronze, iron and chrome as well as glass. The lighting is very bold and bright to reflect off the shining metals and brighten the room intensely.\nWhat Type of Artwork is in the Art Deco Style?\nThere is a huge array of artwork in the art deco decorating style. It includes exotic patterns and fabric prints including original artwork and ethnic decorating ideas. You will often see animal prints and artistic combinations of colors and textures to beautify a room with the art deco style.\nWhat Kind of Wallpaper and Drapes are Art Deco?\nEither the wallpaper or the drapes in an art deco room include patterns with zigzags, stripes and chevron patterns either in a strikingly bright color or in a black and white color. Other popular patterns include the ziggurat, the sunburst and the highly acclaimed lightning bolt pattern. Other popular prints include highly stylized patterns that are adapted from plants and animals along with the celebration of lines. Draperies are often pooling on the floor for a very sophisticated appearance in a room that is art deco. They are often a thicker fabric that speaks of elegance in a formal setting such as a dining room or living room.\nWhat Art Deco is Not\nSince art deco first evolved in the architectural age of manmade materials, it definitely does not include any type of raw materials but instead all hard surfaces are very bright and lacquered with several coats to have a shining effect when the lighting plays off them. Everything is smooth, polished and tooled to celebrate the machine age of its birth. Art deco doesn’t include any types of frills whatsoever such as fluffy items, soft florals, lace or plaid patterns in any area of the room.\nWhat are Some Great Additions for an Art Deco Room?\nYou can easily add art deco lighting to a room in many manners. Table lamps that are made with interesting bases are a must. They can have circles of hard and shiny materials or squares of solid wood stained in a dark color. Also included are table lamps with an intricate geometric pattern in the base in either a light blue, gray or dark color. Wall sconces in art deco style will have possibly bronze detailing with an X across the front of the globe or in polished chrome. Pendant lights are a great style to place over a dining room table. You can find them in circular designs of bright metals with either one large circle or a cascade of circles from top to bottom. Gold colored chandeliers are the ultimate in pendant lights for the art deco style.\nAny type of a bookcase, shelving unit, wardrobe or any other object made of wood that contains an inlaid pattern with a dark finish is a great addition for an art deco style.\nStained glass accents of brightly colored animals, plants or flowers even if you use them as a folding dressing screen are a great addition. One of the most popular designs is a large, brightly colored peacock to add pizzazz to a room in art deco style.\nArt deco ceiling fans are generally quite a bit different than a traditional ceiling fan. They may be made of any shiny metal material and have only three fan blades for an ultra architectural design element that looks much like an airplane propeller. Dark wooden fan blades are also art deco although the three fan blades with have curves in them for an interesting pattern.\nA huge dramatic art deco item is a ceiling to floor glass geometric shape that sparkles in the bright lighting of a room.\nThese ideas can get you started on creating an art deco room. It is a decorating style that is very old, has never gone out of style and is here to stay for eternity."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:2cf730d8-2730-40f8-baa4-9b31f5595cfd>","<urn:uuid:e8b20a0e-4a68-4a1b-b1f4-95e19f0075b6>"],"error":null}
{"question":"What are the technological advancements in live concert sound systems, and what security risks emerge when implementing new audio technologies?","answer":"L-Acoustics L-ISA Immersive Hyperreal Sound technology represents a major advancement in live concert sound, providing a 360-degree audio experience that brings audiences closer to the music. This system was successfully deployed at London's Royal Albert Hall, using multiple hangs of speakers including K2, Kara, and Syva systems to create an immersive experience. However, when implementing new technologies, security risks arise particularly from interconnected devices. As highlighted by cybersecurity experts, any new technology implementation expands an organization's digital footprint and perimeter, increasing vulnerability to cyber attacks. Without effective collaboration and proper security measures, risks can emerge in the technological gaps, potentially exposing systems to ransomware and other cyber threats.","context":["Having already made history as the first band to use L-Acoustics L-ISA Immersive Hyperreal Sound technology at their ground-breaking concert at Forest Hills Stadium in New York earlier this year, UK indie rockers alt-J recently clocked up another first. This time it was on home ground at London’s Royal Albert Hall (RAH). For the final two dates of their world tour in support of their 2017 album Relaxer and its more recent counterpart REDUXER, the UK’s first ever deployment of the fully immersive, revolutionary audio system brought more fans closer than ever to the band’s combination of ethereal harmonies and striking sound effects, which lend themselves perfectly to this 360-degree audio experience.\nalt-J’s front of house engineer Lance Reynolds appreciates that the band’s music includes “a lot of interesting sonic nuggets and sound effects,” and that L-ISA has allowed him to bring these to the fore in his 360° mix. This, in turn, allows the audience to truly appreciate the difference in comparison to conventional stereo live sound reinforcement and “feel completely immersed in alt-J’s music.”\nFor audience members surveyed after the show, it was mission accomplished. “Spectacular, perfect, mesmerizing, powerful, and involving,” were just some of the adjectives that came to mind amongst spectators to describe their experience. One concertgoer summed up his delight as, “By far the best sound of any gig I’ve ever been to. Unforgettable.” Another enthused, “It couldn’t have been better.”\nMusic critics in attendance were similarly enthralled. The Evening Standard, who gave the show an exceptional five-star rating, reported that L-ISA Immersive Hyperreal Sound “literally caused heads to turn when the synth pulse of ‘Hunger Of The Pine’ began to blip across the space.” The Telegraph further noted that the enveloping sounds of a children’s choir as the song ‘Pleader’ reached its crescendo were “astonishing” and concluded that “such ideas need to be embraced. This is progress.”\nFor the L-ISA system at the RAH, alt-J’s long-time production supplier Rat Sound deployed three central hangs of 12 K2 with three Kara down each, flanked by two hangs of 16 Kara either side of the K2 hangs to create the Scene system. Two hangs of four KS28 were flown behind the K2 hangs. Two hangs of 12 Kara formed an extension system and two hangs of 10 Kara provided sidefill coverage. Two ARCS II, left and right, sat on the stage lip, with two ARCS Wide centre stage for frontfill and three X8 per side for stalls fill. Four SB18 were positioned below the front of the stage for low end frontfill.\nThe surround system comprised ten Syva positioned around the top of the Gallery (Level 5), plus ten X12 evenly distributed around the bottom of the Rausing Circle acting as a delay ring for the main Syva surrounds.\nThe audience was on its feet for much of the show, confirming that the band’s adoption of L-ISA helped bring this world tour to a vibrant finale.\n“Touring is becoming a more and more important part of what you do as a band, so it’s exciting to think when you are writing a song how you can make it sound as amazing as possible live,” says alt-J’s Gus Unger-Hamilton (keyboards/vocals). “And now that we know what can be done [with L-ISA], it will be in our minds, for sure.”","Employees who work from their homes may be putting their companies’ systems at risk.\n“Many employees do company work from personally managed and owned systems and these machines are often the ‘Wild, Wild West’ in terms of how they are secured,” said Mike Gentile, the chief executive of San Clemente-based cybersecurity company Cisoshare.\n“The majority of complex attacks, such as ransomware, etc., right now are still often caused by a simple phishing attack or an employee mistake like clicking on a bad link.”\nCisoshare is one of several cybersecurity firms that are emerging in Orange County, which is carving a strong position in internet security due to the proliferation of hackers from Russia, China and North Korea who demand eye-popping sums in ransomware.\nCrowdStrike Holdings Inc. (Nasdaq: CRWD), a Sunnyvale-based firm that now has a $55 billion market cap, started in Orange County where it still has a large local presence. Irvine’s Cylance sold for about $1.4 billion to BlackBerry in 2019 and also counts a base of operations here.\nIn Newport Beach, the ioXt Alliance started by Mobilitie founder Gary Jabara, wants to make sure the interconnections among the various devices used each day—such as cellphones, smart home lighting controls and automotive technology—are also secure.\nUC Irvine’s Cybersecurity Policy & Research Institute studies ways to make the internet and networks safer, including running mock attack drills. Cybercriminals\nIrvine-based Netwrix Corp. is expanding so quickly that it’s made four acquisitions since January.\n“Most organizations did not have time to prepare a transition plan and provide security training to the employees” when they started working from home last year, said Ilia Sotnikov, security strategist and vice president at Netwrix.\n“Hence the increase in reported incidents that included data loss or oversharing.”\nAttackers know that ransomware is arguably the quickest way to get money from a company without breaking into its system, he said.\n“The cybercriminals took advantage of the global pandemic and highly divisive political scene in the U.S. last year,” Sotnikov said. “We’ve seen considerable changes in how the threat landscape evolved over the last couple years with ransomware as a service, more specialized groups.”\nThe Coronavirus Chaos\n“There was so much chaos during the first few months of the lockdown that every CISO will need to go back and review all of the access and changes that happened,” said Bil Harmer, who is the chief information security officer (CISO) and chief evangelist at computer identity security software maker SecureAuth.\n“When there is chaos and change, the threat actors will be there looking for ways in.”\nHe predicted that companies “will begin putting more and more focus on digital identities and a continuous authentication methodology that will allow them to adjust access on the fly as the landscape or the user behavior changes.”\nCisoshare, founded by Gentile, placed No. 21 on this year’s Business Journal list of Best Places to Work in Orange County and No. 2 on last year’s list of fastest-growing companies, both in the small-firm category.\nCompanies who let employees work from their homes face an increasing threat level similar to that of an apartment owner who adds more apartments to a portfolio: the more units there are, the greater the business risks, Gentile said.\n“When employees are working from home, it expands the digital footprint and perimeter of the organization,” Gentile said during a recent interview.\nProviding security also requires using precious resources and talent—both of which are in short supply at plenty of companies, Gentile said.\n“The majority of security risk lives in the cracks when people don’t effectively collaborate and ‘cover all the bases’ when building something,” he said.\nTraining on workstations from which a network is accessed can reduce the risks when employees work from home in a decentralized environment, Gentile said.\nCompanies that opt for a “hybrid” model combining both work-from-home and the office should be wary.\n“Hybrid can be risky due to any time rules change, there is a higher likelihood of mistakes,” Kevin McDonald, the chief operating officer and chief information security officer of Alvaka Networks in Irvine lists 16 points of vulnerability. They include use of bootlegged software, browsing illicit sites, opening infected files that would otherwise be blocked, communicating with unverified individuals and illegal sharing of various contraband such as movies, images, and games.\n“Gambling, pornography, sports, gaming sites, alternative bulletin boards, messengers, even terrorism and extremist sites lead to infections of the host that then connects to the company,” he said.\n“We all suffer from a bit of that-won’t-happen-to-me syndrome. We’re not a target, we don’t have anything they want, we’re not that rich of a company.”\nHe says ransomware attackers are well aware of the potential payoffs: “One hit and you can retire.”\nCompanies are starting to nudge employees into coming to their offices though the daily back-and-forth from the COVID-19 Delta variant makes it difficult to set firm guidelines.\nFor example, data analytics software maker Alteryx Inc. has “voluntarily opened a number of our offices, including our Irvine location for those who are comfortable coming in,” Chief Financial Officer Kevin Rubin said on Aug. 5. “There’s no mandate that they do.”\n“We will more officially begin asking associates to start coming back no sooner than January,” he added.\nSotnikov sees some bright spots.\n“I think many of the WFH (work-from-home) specific dangers were mitigated over the last 12 months, as organizations had a chance to catch their breath, get new budgets in 2021, catch up on trainings for both admins and employees,” he said.\nThe Senate included more than $1.9 billion in cybersecurity funds as part of the roughly $1 trillion bipartisan infrastructure package, The Hill website said on Aug. 10.\nThe funds will go toward securing critical infrastructure against attacks, helping vulnerable organizations defend themselves and providing funding for a key federal cyber office, among other initiatives.\nExperts point to the targeting of Colonial Pipeline and JBS meat packers earlier this year as examples of the dangers of ransomware demands.\nThe picture is acute on the international front, with both Sotnikov and McDonald noting President Joe Biden’s warning last month that a significant cyber-attack on the U.S. could lead to “a real shooting war” with a major power, highlighting the growing threats posed by Russia and China.\n“That is a very aggressive and provocative statement,” McDonald said. He points to China in particular as he surveys global cybersecurity threats to the U.S.\n“I should be worried about China finally deciding it’s time to become the sole world power and using its understanding of our weak infrastructure to show us how much we don’t really have control of the world anymore,” McDonald said.\nAnd the ultimate piece of bad news?\n“Replacements are made in China,” he said.\nDangers, Positive Signs: OC Cybersecurity Experts Look at Internet Risks\nOC Cybersecurity Experts Give the Business Journal These Tips:\nKEVIN MCDONALD, chief operating officer/chief information security officer, Alvaka Networks in Irvine\nSome dangers are “removed or reduced” if the equipment is owned by the employer.\n“Employee-owned computers are far less likely to be patched and kept up-to-date against vulnerability. This includes the operating systems, office applications, third party applications such as Adobe, Internet browsers, etc.\n“Having a system shared with non-employees (of unknown behavior tendency, character, education, intent) means that there is a high potential for risky behaviors that can result in a compromised local computer.\n“Big time execs and powerful people are targets and they’re the most reticent to participate in this whole process.\n“Cryptocurrency is the “primary reason” for the rise in ransomware in which hackers hijack a computer system and demand payment to release it.”\nMIKE GENTILE, founder/chief executive, Cisoshare in San Clemente\nThe biggest risk to work from home or hybrid for security “is that collaboration and effective small team dynamics are hindered when people can’t work together in person.”\n“The good news is that some of the strongest safeguards when a workforce is decentralized is a strong security training and awareness program, as well as a communication system so employees know how to get in touch with the security team and vice versa. Both of these items are highly effective, but also much more inexpensive than almost all technical safeguards.”\nBIL HARMER, chief information security officer/chief evangelist, SecureAuth in Irvine\n“The hybrid model will not go away, there is far too much upside for companies in it. From 48 extra minutes per day per employee in productivity to reduced footprints in the office (desks, power, coffee, etc), this is a model that will continue.\n“Companies will begin moving to Secure Identity as the first line of defense. They will begin putting more and more focus on digital identities and a continuous authentication methodology that will allow them to adjust access on the fly as the landscape or the user behavior changes.\n“This will allow the user to move around the physical world and have their authentication and authorization adjust as they do to keep them within the acceptable risk profile.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:10549f30-4812-4881-962c-e4123b54eef7>","<urn:uuid:ad45c4f6-8fbd-4d17-b17d-404d81708d47>"],"error":null}
{"question":"How do literary heroes like Beowulf compare to antihero characters like Forrest Gump in terms of archetypal patterns?","answer":"Beowulf represents the strong warrior archetype, embodying traditional heroic qualities, while Forrest Gump falls into the antihero category. These represent contrasting archetypal patterns in literature and culture. According to archetypal literary criticism, these character types are recurring patterns that shape a text's meaning through cultural and psychological myths. While the strong warrior archetype like Beowulf follows classical heroic patterns, the antihero like Forrest Gump represents a departure from traditional heroic characteristics while still serving as a protagonist.","context":["The word archetype, \"original pattern from which copies are made\", first entered into English usage in the 1540s and derives from the \"Latin \"noun archetypum, \"latinisation of the \"Greek noun ἀρχέτυπον (archetupon), whose \"adjective form is ἀρχέτυπος (archétupos), which means \"first-molded\", which is a compound of ἀρχή archḗ, \"beginning, origin\", and τύπος tupos, which can mean, amongst other things, \"pattern,\" \"model,\" or \"type.\"\nUsage of archetypes in specific pieces of writing is a holistic approach, which can help the writing win universal acceptance. This is because readers can relate to and identify with the characters and the situation, both socially and culturally. By deploying common archetypes contextually, a writer aims to impart realism to his work. According to many literary critics, archetypes have a standard and recurring depiction in a particular human culture and/or the whole human race that ultimately lays concrete pillars and can shape the whole structure in a literary work.\nThe origins of the archetypal hypothesis date back as far as \"Plato. Plato's ideas were pure mental forms that were imprinted in the soul before it was born into the world. They were collective in the sense that they embodied the fundamental characteristics of a thing rather than its specific peculiarities. In the seventeenth century, Sir \"Thomas Browne and \"Francis Bacon both employ the word 'archetype' in their writings; Browne in \"The Garden of Cyrus (1658) attempted to depict archetypes in his usage of symbolic proper-names.\nThe concept of psychological archetypes was advanced by the \"Swiss psychiatrist \"Carl Jung, c. 1919. In Jung's psychological framework, archetypes are innate, universal prototypes for ideas and may be used to interpret observations. A group of memories and interpretations associated with an archetype is a \"complex ( e.g. a mother complex associated with the mother archetype). Jung treated the archetypes as psychological organs, analogous to physical ones in that both are morphological constructs that arose through \"evolution. At the same time, it has also been observed that evolution can itself be considered an archetypal construct.\nJung states in part one of Man And His Symbols that:\nMy views about the 'archaic remnants', which I call 'archetypes' or 'primordial images,' have been constantly criticized by people who lack a sufficient knowledge of the psychology of dreams and of mythology. The term 'archetype' is often misunderstood as meaning certain definite mythological images or motifs, but these are nothing more than conscious representations. Such variable representations cannot be inherited. The archetype is a tendency to form such representations of a motif—representations that can vary a great deal in detail without losing their basic pattern.\nLater in the 1900's, a Viennese psychologist named Dr. \"Ernest Dichter took these psychological constructs and applied them to marketing. Around 1939, he moved to New York and sent every ad agency on Madison Avenue a famous letter boasting his new discovery. He found that applying these universal themes to products promoted easier discovery and stronger loyalty for brands. \nArchetypal literary criticism argues that archetypes determine the form and function of literary works and that a \"text's meaning is shaped by \"cultural and \"psychological myths. Cultural archetypes are the unknowable basic forms personified or made concrete by recurring \"images, \"symbols, or \"patterns (which may include motifs such as the \"\"quest\" or the \"\"heavenly ascent\"; recognizable character types such as the \"\"trickster\", \"\"saint\", \"\"martyr\" or the \"\"hero\"; symbols such as the apple or the snake; and imagery) and that have all been laden with meaning prior to their inclusion in any particular work.\nThe archetypes reveal shared roles among universal societies, such as the role of the mother in her natural relations with all members of the family. This archetype may create a shared imagery which is defined by many stereotypes that have not separated themselves from the traditional, biological, religious and mythical framework.\nIn 1939 he wrote to six big American companies, introducing himself as 'a young psychologist ...","Key concepts and definitions\nArchetype: the original pattern or model of which all things of the same type are representations or copies. From the Greek archetypos, formed from the verb “archein” (“to begin” or “to rule”) and the noun “typos” (“type”). The ancient Greek philosopher Plato, believed that all things have ideal forms of which real things are merely shadows or copies. And in the psychology of C. G. Jung, “archetype” refers to an inherited idea or mode of thought that is present in the unconscious of the individual. In everyday prose, however, “archetype” is most commonly used to mean “a perfect example of something.”\nIn literary criticism, a primordial image, character, or pattern of circumstances that recurs throughout literature and thought consistently enough to be considered a universal concept or situation. Term adopted from psychologist Carl Gustav Jung, further developed in a literary context by critic Northrop Frye (Encyclopaedia Britannica, 2016)\nStereotype and cliché: both words come from French and were originally printers’ terms, and both have come to take on somewhat negative meanings in modern use. Their original meanings are essentially synonymous, referring to printing blocks from which numerous prints could be made. Today cliché refers to something hackneyed, such as an overly familiar or commonplace phrase, theme, or expression. Stereotype refers to an often unfair and untrue belief that many people have about all people or things with a particular characteristic.\nExamples of character archetypes\nTaking as an example the book I’ve recently mapped to the Hero’s Journey, Matilda by Roald Dahl, Matilda, the Protagonist, has also combined features of several Jungian character archetypes: the Hero, the Rebel, the Magician and the Explorer. I think of Miss Trunchball, which in the book is the Antagonist, as an Ugly Witch and a Terrible Mother.\nIn the same novel I can see some other character archetypes. Miss Honey, the Mentor, can be considered according to C.G. Jung the Orphan, the Caregiver as well as the Sage, being a teacher.\nSome character archetypes with examples\nThe petty bourgeois hero: Giovanni Vivaldi, the main character played by Alberto Sordi in An Average Little Man (1977), film directed by Mario Monicelli\nThe strong warrior: Beowulf, Achilles\nThe tortured hero: Odysseus, the mathematician and Nobel prize Nash played by Russel Crowe in A Beautiful Mind (2001), film directed by Ron Howard\nThe simple young man: Renzo Tramaglino, the male hero in The Betrothed (Italian: I promessi sposi), an Italian historical novel by Alessandro Manzoni (1827)\nThe innocent young girl: Lucia Mondella, the female hero in The Betrothed (Italian: I promessi sposi), an Italian historical novel by Alessandro Manzoni (1827)\nThe unaware princess: Cinderella and Snow White\nThe antihero: Donald Duck, Forrest Gump\nThe self-destructive artist: Amedeo Modigliani, Kurt Cobain\nThe tragic lovers: Orpheus and Eurydice, Romeo and Juliet\nThe rebel: James Dean, Che Guevara\nThe evil stepmother: Miss Trunchball in Matilda by Roald Dahl, the Evil Queen in Snow White fairytale\nThe mermaid: Anita Ekberg in La dolce vita (1960), film directed by Federico Fellini\nThe faithful friend: doctor Watson for Sherlock Holmes\nThe interior enemy: Mr Hyde in the Strange Case of Dr Jekyll and Mr Hyde (1886) by Robert Louis Stevenson\nThe little naughty kid: Pinocchio, the protagonist of the children’s novel The Adventures of Pinocchio (1883) by Italian writer Carlo Collodi\nThe evil (negative) hero: Richard III\nThe wise old man: Merlin the wizard\nIt seems that there is virtually no end to archetype generation. Perhaps, like plot, through which we organize events in a meaningful connected way, this is our spontaneous (human) way to organize things, concepts, people in categories, and so, again, to put order to chaos. This is a concept that I would like to explore in more depth.\nMerrian-Webster. (2016) ‘Archetype’ definition [online] At: http://www.merriam-webster.com/dictionary/archetype (Accessed 17.11.16)\nMerrian-Webster. (2016) ‘Sterotype’ definition [online] At: http://www.merriam-webster.com/dictionary/stereotype (Accessed 17.11.16)\nEncyclopaedia Britannica (2016) ‘Archetype’ article [online] At: https://www.britannica.com/topic/archetype (Accessed 17.11.16)\nhttps://www.hccfl.edu/media/724354/archetypesforliteraryanalysis.pdf (Accessed 17.11.16)\nhttp://www.soulcraft.co/essays/the_12_common_archetypes.html (Accessed 17.11.16)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:75044fd4-7838-445d-ab8d-9ad91f4439e0>","<urn:uuid:3fa7ebc6-db5c-4ddd-a978-b80f1eef0a1e>"],"error":null}
{"question":"What are the key differences between air and airless spray application methods, and what safety precautions are needed for spray paints?","answer":"In air spray systems, atomization occurs through air flowing from a ring-shaped opening, with spray jet shape controlled by shaping air bores. Airless systems use high pressure (up to 5000 psi) to push material through a small nozzle opening, making them ideal for large surfaces and protective coatings. Regarding safety, spray paints are highly flammable due to containing propellants like butane and propane. They can be explosive when pierced or exposed to high temperatures. Proper safety measures include keeping spray paints away from heat and flames, ensuring proper ventilation, and correctly disposing of empty or damaged containers to prevent dangerous flashback explosions where escaping gases can ignite.","context":["More Details on Paint Applicators\nHand-Held Spray Guns:\nAutomated Spray Guns:\nPaint serves two purposes: it protects and decorates. The advantage of paint over other types of surface coating such as nickel or chrome is that paint can be applied to any kind of surface, including wood, metal, stone and synthetic materials. Paints are either solvent- or water-based and contain pigments, fillers and other additives.\nPaint and Painting\nTypes of Paint\n- By the binder (e.g. acrylic paint)\n- By the solvent or diluent (e.g. water-based paint)\n- By the composition (e.g. two-component paint)\n- By the glossiness (e.g. high gloss paint)\n- By the optical characteristics (e.g. special effect paint)\n- By the use (e.g. primer)\n- By the area of application (e.g. wood paint)\n- By the type of object being coated (e.g. car paint)\nHow to Paint\n- To achieve optimal long-lasting protection and a flawless paint coat, it is important to prepare the surface properly.\n- There are two basic methods of preparation: a mechanical and a chemical procedure. The purpose of the preparation is to clear the surface of dust, oil, fat and moisture. Metal surfaces should always be treated with rust protection before they are painted. So-called wash primers or anti-rust paint is ideal for this purpose.\n- One-component paints are in a liquid state during processing and harden when exposed to air. Two-component paints are also liquid, but in contrast to conventional paints they harden due to a chemical reaction with the hardener. The process by which the paint state changes is depicted in the illustration shown here. A paint contains several substances that are dissolved in a solvent or water. After drying, during which the solvent volatilizes, the once solid substances return to their former consistency.\n- Viscosity is a measure of a fluid's resistance to flow. Thick paints are called high viscosity paints while thin paints have a low viscosity. Viscosity is measured by recording the time it takes for a certain quantity of paint to flow through a viscosity cup. This procedure is not suitable for thixotropic materials, which are measured using rotational viscometers. The paint consistency should be matched to the type of application. Temperature also plays an important role.\n- Basically, paints are thinner when warmer. This can lead to considerable quality impairments if temperatures fluctuate. A rise or fall in the temperature can strongly influence viscosity. This in turn will increase or decrease spray gun performance, which will result in a film that is either too thick or too thin. The consequences are often poor coating results and long drying times.\n- Airless - atomization is accomplished by directing fluid under high pressure through a controlled orifice. Airless works best when used to quickly apply high volumes of coatings to large surfaces, such as bridges, storage tanks, or large sheets of metal. Airless is most often used to apply protective, rather than decorative coatings. Typical fluid pressures are up to 5000 psi at the gun.\n- Air-Assisted - atomization is a hybrid application method that combines the hydraulic atomization method of airless with the atomization of aircoat / air spray. The result is an atomization level between air spray and airless. This process is ideal for many finishing and coating applications that require high production levels and a relatively smooth finish, such as wood furniture topcoats, fabricated metal parts, and farm and construction equipment. Air-assisted atomization typically utilizes air pressures up to 35 psi and fluid pressures range from 250 to 4000 psi at the gun.\n- Air-Assisted Electrostatic - Electrostatic technology can be applied to air spray , air-assisted and airless spray methods. Electrostatic applicators greatly reduce over-spray while providing better film coverage than non-electrostatic applicators. Electrostatics are commonly used in automobile, aircraft, heavy equipment manufacturing and metal finishing. Kirkco offers both air spray and air-assisted/airless electrostatic spraying equipment.\n- Material: This is an airless atomization paint spraying process. An electric, pneumatic or gasoline engine powered pump feeds the spray medium from the intake to the nozzle. The pump puts the liquid medium under pressure and pushes a relatively large amount of material through a small nozzle opening. The dynamic pressure of the material can be regulated to up to 530 bar. The nozzles are of tungsten carbide and contain bores of 0.13 - 1.3 mm. An airless system consists of a pump, hose, filter, gun and nozzle. The material delivery rate and jet width are determined by the shape of the nozzle.\n- Air: In this process, the atomization air flows out of a ring-shaped opening formed by a bore in the atomizer head (air cap) and the paint nozzle positioned centrically within it. The spray jet shape is controlled by shaping air (horn air) bores. The air emerging from these bores converts a spray jet with a nearly circular cross section that is vertical to the jet axis into a jet with an elliptical cross section. The atomizer air pressure is generated by compressors.\n- Rotary: The paint to be atomized is directed into the rotating bell through small bores and valves. This material can be fed into the bell either centrically or acentrically. The bells are powered by pneumatic turbines. Via the interior surface of the bell, the paint film reaches the edge of the rotating bell and is mechanically atomized on the basis of aerodynamic and centrifugal forces. The paint droplets that fly off of the bell move radially away from the bell and therefore would never reach the workpiece located in front of the atomizer along the same axis. This is where shaping air comes in. It emerges from a ring behind the atomizer bell and guides the droplets toward the workpiece.","If you want to burn a painting with a butane torch or create artwork in your oven, you may wonder, “Is paint flammable?” The result has fantastic effects. However, you must take measures while storing and using the type of paints to avoid any paint become flammable or combustible.\nSome paints are flammable, while others are combustible. Your paint’s response is determined by its ingredients. Water-based paints, such as acrylic and latex, are not often flammable. When exposed to heat, oil-based paints and spray paints may catch fire.\nTable of Contents\nFlammable vs Combustible: What’s the Difference?\nCombustible is occasionally used interchangeably with flammable. However, there are some differences between them. A substance’s flammability and combustibility are determined by its properties and flashpoints.\nMaterial vapours may ignite or catch fire on the liquid surface at temperatures as low as flashpoints. Depending on its flashpoint, many materials may be divided into many categories.\nA flammable substance is defined as having a flashpoint temperature higher than 100 degrees Fahrenheit by the OSHA regulations the United States Department of Labor (F) set out. OSHA continues by saying:\nFlammable liquids can catch fire and burn rapidly at standard operating temperatures. A low-temperature flame is all that is required to light them.\nCombustible liquids may catch fire when temperatures rise over the normal working range. They often include paints and have a high flammability rating.\nVapour may ignite even when liquids don’t often do so. A liquid’s flashpoint determines the minimum temperature required for combustion in air. Therefore, flammable liquids are a significant source of fire hazards.\nFor example, when flammable liquid vapours burn quickly, vast amounts of heat and deadly black smoke result. Because they may burst at high temperatures, combustibles emit a vapour that can ignite the atmosphere and do much more damage.\nAs volatile and combustible as they may be, paint thinner, and other paints are among the most dangerous. Understanding the dangers and the proper way to operate with these substances is essential for your safety.\nIs Paint Flammable?\nLiquid paint is seldom flammable in practice. Paints and solvents may generate toxic or combustible vapours. Paint fumes may catch fire as the temperature rises or a neighbouring fire breaks out, which is very hazardous.\nCombustible paints include aerosols and oil-based paints. Flammable ingredients are often found in oil-based paints, varnishes, and stains.\nAcrylic, vinyl, and latex are water-based paints that will not burn. Painters often utilize water-based, non-flammable paints. Certain water-based acrylic paints come in flame-retardant patterns. The combustible nature of certain water-based paints is another matter.\nThe paint depends on its primary material if it is flammable or combustible. In terms of flammable paint, the most prevalent varieties are:\nAerosol paints need propellants like butane and propane.\nCombustible chemicals such as toluene, methanol, and ketones may be present in oil-based paints.\nPaints containing alcohol are called “alcohol-based.” Alcohol is a highly flammable and easily combustible liquid.\nTo get the greatest results, study the paint’s ingredients. Include a description of the product’s potential dangers, as well as an indication of whether it’s flammable or not.\nIs Paint Combustible?\nSome paints have the potential to catch fire. Spray paints and aerosols have a high flammability index. They catch fire as soon as they’re pierced or exposed to extreme temperatures. The danger of fire and explosion is increased with products that employ spray-on oil-based paint.\nTypes Of Flammable Paints\nAn oil-, alcohol-or solvent-based paint is likely to be combustible if the paint includes these ingredients. Combustible paint is most often seen in the following forms:\nAs a result of the gas fuels, it contains, spray paint is very flammable. This includes propane and butane, which may be explosive when mixed with paint. An explosion caused by vapours that have escaped from old, broken, or poorly sealed cans can be very hazardous. In addition, spray paints may occasionally flashback because of the high pressure in the container.\nHeat is drawn back into a limited place by escaping gases that have ignited (almost like an invisible fuse string). During a grenade-like explosion, the liquid within the canister might become shrapnel.\nBecause of this, spray paint is not combustible once applied and hardened. It is safe to touch after it has dried since gas fuels are no longer present.\nIf you have spray paints around the house or at work, keep them away from heat and flames and apart from any flammable substances. You should also properly dispose of them if they are empty, broken, or obsolete.\nSolvent Based Paint\nToxic fumes from solvent-based paint make it dangerous. Consequently, solvent-based paint should be kept in a cool, dry location away from other flammable things, and it should never be used in an area where heat or flames might be present. It is no longer flammable after drying since the solvents’ combustible qualities have been removed.\nOil paints are very combustible if you look at how oil is utilized in heaters and fires. As the solvents evaporate and the paint dries, oil paint becomes non-flammable.\nOn the other hand, oil paintings will be reduced to ash in the case of a home fire due to the combustibility of dry oil paint. Keep your oil paints away from heat and fire, and always shut the tubes after use if you like an oil painting.\nOil-based Enamel Paint\nThe solvents in the oil and the oil-based makeup of certain enamel paints make them combustible. On the other hand, water-based enamel paints may be applied, stored, and produced with the same gloss level as oil-based paints.\nExterior House Paint\nPaint for the outside of a home that is oil-based and so combustible is often used.\nYou should be cautious when storing large quantities of these paints at home or on a construction site since they might catch fire because they are oil-based.\nWhenever feasible, utilize an outside paint business and ensure it is far from your property. It’s also a good idea to keep minors out of the paint shop and keep it out of direct sunlight and heat.\nOil-Based Epoxy Paint\nThe volatile solvents in oil-based epoxy paint make it a combustible product. In contrast, water-based epoxy paint is non-flammable and hence safer to use.\nTake additional measures if you have a lot of oil-based paint in your house. For example, it often paints garage floors and other hard-to-paint surfaces. Curing epoxy paint reduces its flammability, although the resin itself remains flammable.\nTypes Of Non-Flammable Paint\nFor the most part, water-based paints don’t catch fire. To put it another way, water is a non-flammable liquid utilized to put out fires.\nIt is possible to utilize paint as a flame retardant if the water content is high enough. Certain water-based paintings may dry out and become combustible, as with any rule.\nYour home will be protected from the spread of a blaze if you apply flame-resistant paint.\nExamples of non-flammable paints include:\nIt is safe to use and store water-based acrylic paint in your house since it does not burn. Acrylic paint, however, turns into plastic polymer when it cures.\nWater colours are one of the most harmless types of paint, thanks to their high water content. They are equally safe in dried form and are ideal for children to ingest. They are undoubtedly allowed on aeroplanes since they are non-flammable and non-hazardous. difference between them and acrylics can be found on this post for better under standing to get desired painting results on different surfaces.\nEmulsion paint is now made with water-based latex. In other words, most emulsion paints are non-flammable and safe to use and store in your house.\nEmulsion paints with flame retardant properties are widely available, making them an excellent choice for interior designers.\nWater-based latex paints are non-combustible. As it dries, latex paint has a rubbery feel but is not flammable.\nWater-based glass paint is non-flammable. Even after drying, it is non-flammable and safe to use. know more about how to paint glass windows easily here for stunning results from the paint job.\nThis type of paint is non-flammable since it is water-based. Fire retardant paint is widely considered to stop and delay flame spread.\nFabric paint is non-flammable since it is water-based. It’s safe to have about the house because it’s non-toxic and odourless.\nWater-based latex paints are the house’s most common type of wall paint. Because they don’t include easily combustible solvents, water-based paints aren’t flammable.\nHazards of Flammable or Combustible Paints\nPaint thinners should be handled and stored safely to prevent fires and explosions. After usage, flammable and combustible paints pose a risk. If you don’t properly dispose of the paint, you might put yourself and others at risk.\nThe health risks of working with flammable or combustible compounds go beyond the damage caused by fire or explosions. For example, if you inhale the vapours, you may experience the following:\nDisease and Illness\nBefore using or storing paint in your house, follow all the instructions on the container, including the safety warnings. Poisoning, chemical burns, and flames can all be avoided this way.\nSafety Tips for Working with Paint and Paint Thinner\nIf you use a potentially toxic or flammable paint thinner, you should always take measures. The materials must be used, stored, and disposed of correctly. Precautions must be taken while using paint and paint thinner.\nUse only in mixture with paint.\nIf paint thinner is mixed with anything other than paint, a potentially lethal reaction might result. Adding a thickener to an oil-based paint is a very typical practice. Don’t combine colours that aren’t exactly alike. If you’re not sure, ask the paint maker.\nMake Use of Safety Equipment\nWhen working with paint and thinners, use safety equipment. Wear safety equipment like gloves, goggles, and a respirator or mask. Protect your work area with a tarp or newspaper.\nWhen dealing with significant volumes of dangerous vapours, always work in a well-ventilated area and wear a respirator. Avoid crowded places. Thinners’ harmful vapours can cause headaches, vertigo, nausea, and breathing difficulties.\nIf you must work indoors, open a window or door. A carport is an excellent way to shelter your project from the weather. If required, include a fan.\nNever place it near flammable items.\nAccidents occur. Keep any combustible materials in a safe place. If your storage is insufficient, you risk starting a fire and causing damage to the structure. Thinners and spray paints are incredibly combustible and might result in a catastrophic explosion.\nDon’t use it to clean.\nEven though certain thinners are meant for cleaning walls, counters, and floors, never use paints or thinners to clean your workstation. The combustible items might readily catch fire and explode.\nAvoid Consuming Neighboring Foods\nSome paints and thinners create fumes and vapour’s that can harm humans. Avoid dining near your job since the food may absorb these toxins and cause long-term damage.\nCleanse Thoroughly After Use\nWear gloves and wash your hands often, especially before eating. Clean thoroughly with soap and water after using the paints and thinners. Organize your work environment. Inspect the containers for spills as well.\nKeep paints in a safe place. Place flammable or combustible materials safely away from flames, sparks, or a fire source. Make sure that it is out of reach of minors.\nIf you are unsure whether your paint is flammable, store it carefully just in case. Some companies also provide safety storage cabinets for storing flammable or combustible goods in bulk.\nUnderstanding how to store, use, and dispose of paints is critical for safety reasons. Flammable and combustible things provide a fire risk and can result in severe physical damage. Always practice safety by inspecting the components of your paints and keeping them safely away from children.\nCan paint cause a fire?\nMost paints lose their combustibility after drying because the combustible gases reduce and dissipate. However, many paints can become flammable after drying and thus catch fire if exposed to high heat.\nIs it safe to leave paint in a hot garage?\nPaint should never be kept in a garage. When the paint is subjected to high temperatures, its consistency changes and loses value. Long-term paint storage will turn it into hazardous waste that must be disposed of properly. know more about proper disposal of acrylics in this guide for reduced environmental impact and harm to human life.\nIs it possible for the paint can explode?\nAny paint compound poses a significant risk of fire or explosion. Paint should always be stored in a well-ventilated, dry area away from heat sources and direct sunlight.\nIs it permissible to leave paint in the car?\nWhen the paint is cold, it thickens; when heated, it thins. Before utilizing paint stored in a hot or cold (seasonally) automotive trunk, the temperature must be returned to 70°F-77°F (20-25°C). Because of its thick application, challenging paint has poor flow qualities and tends to droop.\nBeing associated with art and craft field since decades as a hobbyist and life long learner has given me an opportunity to learn many new things related to art, craft, paints and pottery which i am trying to share with your guys on this website. I have expertise of being professional painter and potter for the last 20+ years\nI have learned mind blowing cool tips and insights which makes me a person with ability to improvise and come up with creative ideas and solutions to make stunning and impeccable art pieces of all types which are adored by people across the globe on this website and other platform."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:8af8880a-0991-4f20-8ba8-b26fae6694f6>","<urn:uuid:d6ae26e2-df88-439c-8d27-b495c5bc383a>"],"error":null}
{"question":"What is the process for accessing psychiatric services at UHS compared to Nebraska Medicine's University Health Center?","answer":"At UHS, students can directly access psychiatric consultation and medication management services. At Nebraska Medicine's University Health Center, new patients must first schedule a brief appointment with a Counseling and Psychological Services therapist, who will assess the situation and determine if a psychiatric evaluation is needed before referring them to the psychiatry team. For those who have previously seen a psychiatrist, they can schedule directly with the Health Center's psychiatry team by calling 402.472.5000.","context":["Students, faculty, staff, and teaching assistants are essential when it comes to preventing suicide and promoting help-seeking behaviors among their peers. UHS offers consultation with concerned third parties.\nUHS offers high-quality medical and mental health care, in addition to wellness services to all current UW-Madison students. Most sservices are available at no charge because students pay for UHS services with their tuition and fees. A fee is charged for some extra services and all medications. Most fee-based services at UHS are covered at no cost for members of the UW-Madison Student Health Insurance Program (SHIP).\nOur medical services include a primary care clinic and specialty clinics for immunizations, women's health, travel and sexual health. For students with complex or unstable conditions requiring other types of specialty care, UHS can refer to providers off campus utilizing personal insurance.\nMental health services include individual, couple/partner, and group counseling, crisis counseling, and psychiatric services. Students with mental health concerns that go beyond the scope of care available at UHS can work with Care Managers to obtain referrals to mental health providers off campus.\nMedical and Mental Health do not provide appointments during evening, night, or weekend hours; emergency room care; ambulance services; hospitalization; or home care. We have an after-hours nurse line (608-265-5600, option 1) and a 24-hour mental health crisis line (608-265-5600, option 9) available to students at no cost.\nCare for illnesses\n- 24-hour nurse advice line (608-265-5600, option 1)\n- Evaluation and treatment of acute and chronic illnesses\n- Referral for specialty care (with personal insurance)\n- Basic radiology tests (X-rays) when ordered by a UHS provider\n- Laboratory testing when ordered by a UHS provider\n- Monitoring and management, including diagnostic tests, for stable chronic health conditions such high blood pressure or diabetes.\nCare for injuries\n- Uncomplicated cuts needing stitches\n- X-rays for the evaluation of possible fractures\n- Sale of crutches, splints, or other orthopedic durable medical supplies\n- Physical therapy (fee charged) and athletic training services\nWomen’s health care\n- Problem-focused and preventative care supporting reproductive health and wellness\n- Screening tests, exams, and procedures that provide comprehensive contraceptive options, care for abnormal bleeding, pelvic pain, and pregnancy or STI concerns.\nImmunizations, allergies, & other specialty care\n- No-cost annual influenza vaccines\n- Immunizations including hepatitis A and B, HPV, Tdap, varicella, rabies, meningococcal, MMR (measles, mumps, rubella) and travel immunizations (fee charged)\n- International pre-travel consults regarding health requirements and recommendations for international travel\n- Allergy desensitization injections (the desensitizing solution must be prescribed and provided by an outside physician/allergist)\n- Physical examinations\n- STI screening, cholesterol screening, and flu shots\n- Physical exams required for employment or travel, or required by other third parties (fee charged)\n- Occupational health services such respiratory fit testing, TB screening, and occupational medicine consultation\nPsychological and psychiatric care\n- 24-hour crisis intervention available (608-265-5600, option 9)\n- Brief individual and couple/partner counseling\n- Group counseling\n- Assessment for substance abuse and disordered eating\n- Screenings for attention disorders\n- Gender identity consultations\n- Psychiatric consultation and medication management\n- Care management and referral\n- Self-help online mental health support\n- Stress management\n- Yoga and Yoga for Every Body (BMI>30)\n- Acupuncture (fee charged)\n- Sleep management\n- Smoking cessation\n- Massage therapy (fee charged)\nWho can use UHS?\nAny undergraduate, graduate, or professional student enrolled for the current semester may make appointments at UHS and use\nany service. Students entering UW–Madison in the fall semester will be eligible to receive care at UHS on August 15.\nHealth Insurance Information\nEnrolled UW-Madison students may use UHS regardless of their health insurance coverage. UHS does not bill insurance, and it is considered “out of network” for almost all plans.\nAccess to UHS is not a substitute for having comprehensive health insurance coverage. Students should review health insurance plans before classes begin to see if they are covered in Madison. Many plans cover emergency care but not routine, urgent, or specialty care when students are away from home. UHS sees students who have to interrupt their studies and travel home for care they could have received in Madison had their policy permitted.\nIf you don’t have health insurance, the Student Health Insurance Plan (SHIP) may be your best option. SHIP is administered by UHS and is designed specifically to meet the needs of students. In addition to the primary and preventive care at UHS, SHIP members are protected by a nationwide network of hospitals, clinics, and specialized medical services. Since SHIP is not motivated by profit, it also provides good value, with rich benefits and comparatively low member out-of-pocket expenses. Visit uhs.wisc.edu/ship or call 608-265-5232 for more information.\nImmunizations and Medical Records for Incoming students\nUHS strongly recommends that all students be up-to-date on immunizations before coming to campus. Complete your immunization and health history forms in MyUHS (do not mail us any documents; we only collect this information through MyUHS).\nMyUHS (myuhs.wisc.edu) is a secure online patient portal. Before the semester begins, use MyUHS to complete immunization and health history forms. After the semester begins, students will be eligible to make appointments online, view scheduled appointments, exchange secure messages with providers, view lab and radiology results, and request health records.\nUHS and UW Health\nUHS is not part of UW Health, nor does UHS have any special referral relationship with UW Hospital and Clinics. Students who are referred or transported to any hospital from our clinic, including UW Hospital, are responsible for any emergency room or hospitalization charges.\nWhat if I don’t want to go to UHS?\nThere is no requirement to use UHS. Students may seek health care from the provider who is best for them and their family. Every year, about 50 percent of students visit UHS at least once; nine out of 10 students come to us at some point during their college career, and all students participate in our online prevention programs.\nWhat to bring to campus\n- Health insurance card (and prescription medication card, if separate)\n- Names and phone numbers of home primary care and specialty care providers\n- Prescription medicines and refill information\n- Pain reliever (ibuprofen and/or naproxen)\n- Fever reducer (acetaminophen)\n- Throat lozenges\n- Alcohol-based hand sanitizer\n- Basic first-aid supplies: Band-Aids, antibacterial ointment, hydrocortisone cream","Nebraska Medicine – University Health Center offers psychiatric services. Our psychiatric team is comprised of medically trained providers that offer assessment, education and medication management for a wide array of conditions such as depression, anxiety, life stressors, eating disorders, ADD/ADHD, bipolar disorder, schizophrenia and more.\nFor those who are new to psychiatric services or are uncertain if this type of care is needed, we first have students schedule a brief appointment with a Counseling and Psychological Services therapist. The therapist will assess the situation and help determine if a psychiatric evaluation is appropriate or recommended. If so, they will assist the student in making an appointment with a member of the Health Center psychiatry team or a psychiatrist in the community.\nFor those who have already seen a psychiatrist and would like to schedule an appointment with a member of the Health Center’s psychiatry team, please call 402.472.5000.\nFrequently Asked Questions\nPsychiatric medication management visits are not covered under the University Program and Facilities Fee and will therefore incur a charge. For pricing information, call 402.472.5000 and select Billing and Insurance among the options. Please be advised, mental health appointments with a Health Center general medical provider are also not covered under the University Program and Facilities Fee, and will incur a charge.\nYour first psychiatric evaluation appointment is scheduled for 60 minutes. It starts very similar to a regular doctor’s appointment. After you’ve checked in at the front desk, a nurse will call you back from the waiting room and will gather your vital signs, review your allergies and medications, and go over your health history. When this is completed, you will then meet with a psychiatric team member. Plan on being asked a lot of questions about your physical health (past or current illnesses, injuries, or surgeries), social history (school, work, hobbies, relationships, gender identity, spiritual beliefs, drug/alcohol use, etc.) and mental health history (any treatment you may have received prior to this appointment). Of course, time will also be spent talking about the concerns that prompted you to schedule the appointment.\nAt the end of the evaluation, the psychiatric provider will talk with you about your symptoms and possible diagnosis, go over your treatment options (which may or may not include medication), answer any questions you may have and make a return appointment.\nAfter you’ve been established with one of our psychiatric providers, follow up appointments are scheduled to see if your symptoms are getting better, if any changes need to be made and to address any additional questions or concerns you may have. How often you will follow up with your psychiatric provider will be determined by you and your provider. Appointments are scheduled on average for 30 minutes but can vary from 20 to 40 minutes.\nThese decisions are based on several criteria such as your symptoms, if you have any other medical conditions, what medications/herbal supplements you are currently taking, what medications you’ve already tried, etc.\nRemember, responses to medication are highly individualized. A medicine that works well for one person (even if a family member) may work very differently for you.\nNo. The goal of treatment is to stabilize brain chemistry and relieve your symptoms so you can feel like yourself again. Think of it like having a broken leg. Putting a cast on your leg doesn’t change who you are. Instead, it stabilizes your leg so you’re more capable to be who you are.\nNo. Treating mental health issues with medication is no different than taking medication for medical conditions like allergies, asthma, infections or diabetes.\nNo. These medications are not addictive.\nHowever, there are some medications in other drug classes used by medical providers that can be habit forming. Your provider will discuss this with you should it be a concern. At any time, feel free to ask questions.\nNo need to worry. Our psychiatric team is more than willing to coordinate care with your psychiatric provider at home. Every effort is made to be sure the transitions between home and school are as seamless as possible.\nThe best option is to have your psychiatric provider at home write a prescription that you can have refilled at a local pharmacy when you’re at school. Unfortunately, members of our psychiatric team cannot refill medications without having an initial evaluation and routine follow up appointments."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:36a92241-f7bd-4128-863a-fbccf4189afa>","<urn:uuid:28b3ef12-aef5-4fb5-9c22-21309f40bb7a>"],"error":null}
{"question":"As a medical researcher studying vaccines, I'm curious how the effectiveness of mRNA-1273 in nonhuman primates compares to BNT162b2's performance in adolescents. Could you analyze their immune responses and protection levels?","answer":"Both vaccines demonstrated strong immune responses and protection, but in different study populations. The mRNA-1273 vaccine in nonhuman primates induced antibody levels exceeding those in human convalescent-phase serum, with live-virus reciprocal 50% inhibitory dilution geometric mean titers of 501 in the 10-μg dose group and 3481 in the 100-μg dose group. It provided rapid protection in upper and lower airways, with no viral replication detectable in BAL fluid by day 2 in most vaccinated animals. The BNT162b2 vaccine, tested in 12-to-15-year-olds, produced an even greater immune response than in young adults and showed 100% efficacy (95% CI, 75.3 to 100), with no COVID-19 cases occurring 7 or more days after dose 2 among vaccine recipients, while 16 cases occurred in placebo recipients.","context":["- Kizzmekia S Corbett, Barbara Flynn, Kathryn E Foulds, Joseph R Francica, Seyhan Boyoglu-Barnum, Anne P Werner, Britta Flach, Sarah O'Connell, Kevin W Bock, Mahnaz Minai, Bianca M Nagata, Hanne Andersen, David R Martinez, Amy T Noe, Naomi Douek, Mitzi M Donaldson, Nadesh N Nji, Gabriela S Alvarado, Darin K Edwards, Dillon R Flebbe, Evan Lamb, Nicole A Doria-Rose, Bob C Lin, Mark K Louder, Sijy O'Dell, Stephen D Schmidt, Emily Phung, Lauren A Chang, Christina Yap, John-Paul M Todd, Laurent Pessaint, Alex Van Ry, Shanai Browne, Jack Greenhouse, Tammy Putman-Taylor, Amanda Strasbaugh, Tracey-Ann Campbell, Anthony Cook, Alan Dodson, Katelyn Steingrebe, Wei Shi, Yi Zhang, Olubukola M Abiona, Lingshu Wang, Amarendra Pegu, Eun Sung Yang, Kwanyee Leung, Tongqing Zhou, I-Ting Teng, Alicia Widge, Ingelise Gordon, Laura Novik, Rebecca A Gillespie, Rebecca J Loomis, Juan I Moliva, Guillaume Stewart-Jones, Sunny Himansu, Wing-Pui Kong, Martha C Nason, Kaitlyn M Morabito, Tracy J Ruckwardt, Julie E Ledgerwood, Martin R Gaudinski, Peter D Kwong, John R Mascola, Andrea Carfi, Mark G Lewis, Ralph S Baric, Adrian McDermott, Ian N Moore, Nancy J Sullivan, Mario Roederer, Robert A Seder, and Barney S Graham.\n- From the Vaccine Research Center (K.S.C., B. Flynn, K.E.F., J.R.F., S.B.-B., A.P.W., B. Flach, S. O'Connell, A.T.N., N.D., M.M.D., N.N.N., G.S.A., D.R.F., E.L., N.A.D.-R., B.C.L., M.K.L., S. O'Dell, S.D.S., E.P., L.A.C., C.Y., J.-P.M.T., W.S., Y.Z., O.M.A., L.W., A.P., E.S.Y., K.L., T.Z., I.-T.T., A.W., I.G., L.N., R.A.G., R.J.L., J.I.M., W.-P.K., K.M.M., T.J.R., J.E.L., M.R.G., P.D.K., J.R.M., A.M., N.J.S., M.R., R.A.S., B.S.G.), the Infectious Disease Pathogenesis Section (K.W.B., M.M., B.M.N., M.G.L.), and the Biostatistics Research Branch, Division of Clinical Research (M.C.N.), National Institute of Allergy and Infectious Diseases, National Institutes of Health, Bethesda, and Bioqual (H.A., L.P., A.V.R., S.B., J.G., T.P.-T., A.S., T.-A.C., A. Cook, A.D., K.S., I.N.M.) and the Public Health Service Commissioned Corps (M.R.G.), Rockville - both in Maryland; the Department of Epidemiology, University of North Carolina at Chapel Hill, Chapel Hill (D.R.M., R.S.B.); Moderna, Cambridge, MA (D.K.E., G.S.-J., S.H., A. Carfi); and the Institute for Biomedical Sciences, George Washington University, Washington, DC (E.P.).\n- N. Engl. J. Med. 2020 Oct 15; 383 (16): 1544-1555.\nBackgroundVaccines to prevent coronavirus disease 2019 (Covid-19) are urgently needed. The effect of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) vaccines on viral replication in both upper and lower airways is important to evaluate in nonhuman primates.MethodsNonhuman primates received 10 or 100 μg of mRNA-1273, a vaccine encoding the prefusion-stabilized spike protein of SARS-CoV-2, or no vaccine. Antibody and T-cell responses were assessed before upper- and lower-airway challenge with SARS-CoV-2. Active viral replication and viral genomes in bronchoalveolar-lavage (BAL) fluid and nasal swab specimens were assessed by polymerase chain reaction, and histopathological analysis and viral quantification were performed on lung-tissue specimens.ResultsThe mRNA-1273 vaccine candidate induced antibody levels exceeding those in human convalescent-phase serum, with live-virus reciprocal 50% inhibitory dilution (ID50) geometric mean titers of 501 in the 10-μg dose group and 3481 in the 100-μg dose group. Vaccination induced type 1 helper T-cell (Th1)-biased CD4 T-cell responses and low or undetectable Th2 or CD8 T-cell responses. Viral replication was not detectable in BAL fluid by day 2 after challenge in seven of eight animals in both vaccinated groups. No viral replication was detectable in the nose of any of the eight animals in the 100-μg dose group by day 2 after challenge, and limited inflammation or detectable viral genome or antigen was noted in lungs of animals in either vaccine group.ConclusionsVaccination of nonhuman primates with mRNA-1273 induced robust SARS-CoV-2 neutralizing activity, rapid protection in the upper and lower airways, and no pathologic changes in the lung. (Funded by the National Institutes of Health and others.).Copyright © 2020 Massachusetts Medical Society.\nDo you have a pearl, summary or comment to save or share?\nYou can also include formatting, links, images and footnotes in your notes\n- Simple formatting can be added to notes, such as\n- Superscript can be denoted by\n- Numbered or bulleted lists can be created using either numbered lines\n1. 2. 3., hyphens\n- Links can be included with:\n[my link to pubmed](http://pubmed.com)\n- Images can be included with:\n![alt text](https://bestmedicaljournal.com/study_graph.jpg \"Image Title Text\")\n- For footnotes use\n[^1](This is a footnote.)inline.\n- Or use an inline reference\n[^1]to refer to a longer footnote elseweher in the document\n[^1]: This is a long footnote..","· SARS-CoV-2-Specific Antibodies in Breast Milk After COVID-19 Vaccination of Breastfeeding Women: A prospective cohort study conducted by researchers in Israel including 84 breastfeeding women who received 2 doses of the Pfizer-BioNTech vaccine between December 23, 2020 and January 15, 2021 found positive levels of anti-SARS-CoV-2-specific IgA antibodies (ABs) in breast milk in 61.8% of samples 2 weeks after the first dose, 86.1% after 4 weeks, and 65.7% after 6 weeks as well as anti-SARS-CoV-2-specific IgG ABs in 91.7% at week 5 and 97% in weeks 5-6. There were no serious adverse events reported during the study period but 47 (55.9%) and 52 (61.9%) reported vaccine-related adverse events after the first and second dose, respectively. These findings suggest the antibodies found in breast milk showed strong neutralizing effects and potentially provide a protective effect in infants as protection against SARS-CoV-2 infection.\n· Adverse pregnancy outcomes, maternal complications, and severe illness among U.S. delivery hospitalizations with and without a COVID-19 diagnosis: A multi-center retrospective cohort study conducted by researchers from the CDC COVID-19 Response Team and United States Public Health Service Corps investigated 489,471 pregnant patients who delivered at 703 hospitals between March and September 2020. 6,550 of those patients had confirmed COVID-19, which was associated with increased risk for severe adverse outcomes including ARDS, death, and sepsis, as well as preterm labor with preterm delivery, and longer hospitalization time. These findings signify the importance of implementation of mitigation strategies, counseling, and clinical attention to lessen the risk of SARS-CoV-2 infection for pregnant women given the increased risk for adverse events.\nTransmission & Prevention\n· Safety, Immunogenicity, and Efficacy of the BNT162b2 Covid-19 Vaccine in Adolescents: A research team from Cincinnati Children's Hospital (OH), Kaiser Permanente Vaccine Study Center (CA), and the California Research Foundation, San Diego (CA) funded by BioNTech and Pfizer used a multinational, placebo-controlled, observer-blinded trial of 30 μg of BNT162b2 delivered in two doses to determine immunogenicity in 12-to-15-year-old participants. 1131 received BNT162b2, and 1129 received placebo; there were no vaccine-related serious adverse events and few overall severe adverse events. Noninferiority of the immune response to BNT162b2 in 12-to-15-year-old participants as compared with that in 16-to-25-year-old participants was determined by measuring neutralizing titers after dose 2. The observed vaccine efficacy was 100% (95% CI, 75.3 to 100), and no COVID-19 cases with an onset of 7 or more days after dose 2 were noted among BNT162b2 recipients (who did not have previous infection), while 16 cases occurred in placebo recipients. Overall, BNT162b2 vaccine in 12-to-15-year-olds produced a greater immune response than in young adults and was shown to be both safe and highly effective.\nR&D: Diagnosis & Treatments\n· Corticosteroid therapy for COVID-19: A systematic review and meta-analysis of randomized controlled trials: A systematic review and meta-analysis conducted by researchers in United States and India included 8 randomized-controlled studies conducted through March 10, 2021 with 7,737 patients, 2795 (36.1%) who received corticosteroids along with the standard of care (SOC) for treatment of moderate and severe-critical COVID-19 infection. The odds of mortality were significantly lower in patients who received corticosteroids compared to SOC (OR 0.85), and odds for need of mechanical ventilation were reduced with corticosteroids compared to SOC (OR 0.7), suggesting beneficial effects of corticosteroid treatment in moderate to severe COVID-19 infection.\n· Interim Results of a Phase 1-2a Trial of Ad26.COV2.S Covid-19 Vaccine: An international team of researchers shared interim results from a Phase 1-2a clinical trial on the efficacy of the Ad26.COV2.S candidate vaccine, a replication-incompetent adenovirus vector encoding a SARS-CoV-2 spike (S) protein, in 403 individuals aged 15-55 (cohort 1) and 402 individuals aged >65 (cohort 3). Both cohorts received a high or low dose of the vaccine in a single-dose or two-dose schedule 56 days apart. The most common adverse reactions included fever, fatigue, headache, myalgia, and injection-site pain, which were more commonly reported in cohort 1 and in those who received the low dose. Neutralizing antibody titers against wild-type SARS-CoV-2 were present in more than 90% of participants on day 29 after the first dose, and a second dose increased titer by a factor of 2.6–2.9. These findings suggest that the Ad26.COV2.S vaccine is safe and effective in both younger and older adults with minimal side effects.\n· Methylprednisolone as Adjunctive Therapy for Patients Hospitalized With Coronavirus Disease 2019 (COVID-19; Metcovid): A Randomized, Double-blind, Phase IIb, Placebo-controlled Trial: Brazilian researchers conducted a randomized, double-blinded, placebo-controlled, Phase 2b trial between April 18 and June 16, 2020 to assess the efficacy of methylprednisolone (MP) in treating hospitalized COVID-19 patients. They performed modified intention to treat (mITT) analysis of 393 participants (194 in MP and 199 in placebo group) and found no significant difference in the primary outcome (28 day mortality) and secondary outcomes between the two groups. Post hoc analysis revealed lower 28 day mortality in ages over 60 in the MP group. Authors conclude that methylprednisolone did not lower mortality in hospitalized COVID-19 patients."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:87ce1c49-623e-45a9-84ff-8daa0193c38d>","<urn:uuid:40b11f65-00f6-4bc6-8aeb-30d797f2dd1a>"],"error":null}
{"question":"What are the design considerations for mobility aids when comparing canes for balance versus leg support crutches?","answer":"For canes used primarily for balance, a standard single-tip design is recommended, with the handle aligning with the wrist and elbow bent at 15 degrees. Leg support crutches, however, require more complex design considerations including proper strut placement, thigh and shin cradles, and mechanisms to transfer weight to the ischium bone while maintaining stability for ambulatory movement.","context":["How do you use a cane?\nLearning how do you use a cane reduces falls and eases stressCanes have the simple, often elegant design of a fashion accessory, but theyíre an essential aid for anyone who has trouble walking. But how do you use a cane when youíve walked without assistance for your entire life?\nWalking with a cane is not complicated, but itís important to distribute your weight correctly whether youíre walking, standing or going up stairs. With a little practice, your body will learn to move with the cane naturally.\nWhether youíre older, healing from an injury or dealing with balance problems, the right cane will prevent falls and help you get around. Ask your caregiver for advice when choosing a cane, and consider these tips from the Mayo Clinic before buying one online.\n- If the cane is primarily for balance, try a standard cane with one tip.\n- If the cane is needed to bear weight, use one with four tips at the bottom, known as a offset cane.\n- Check the bend of your elbow to determine the right fit. When standing with the cane in hand, your elbow should only have to bend at about 15 degrees, and the handle should align with your wrist.\nWalking with a cane\nWhen walking with your cane, hold it in the hand that is opposite the weaker leg. Keep your elbow close to the body for stability, and move the cane so itís in-step with the weaker leg. The rubber tip(s) should touch the ground about 4 inches in front of the stronger leg. Shift your weight to the stronger leg if youíre using the cane for support. If youíre using the cane for balance, hold it on the side that feels comfortable.\nSitting down with a cane\nStand with the back of your legs against the chair, and set the cane to the chairís side. With your weight shifted to the stronger leg, reach back and hold the chairís arms. Lower yourself down slowly and inch back into the seat.\nGetting up from a chair\nHold onto the caneís handle with your cane hand, and then place both hands on the chair arms. Lift yourself slowly and put your good foot slightly forward.\nUsing steps with a cane\nSteps require extra care when youíre using a cane. Hold on to the railing with one hand and your cane with the other, and step up with the strong leg first. Move the cane up one stair then step with the weaker leg. When moving down stairs with a cane, position the cane on the next step down, and then step with the weak leg.\n- Folding canes make traveling much easier.\n- Mind your tip(s). The rubber tip on the bottom end of the cane is what keeps it from slipping. Check the tip from time to time, and if it tears buy a replacement tip.\n- Use a backpack or bag to carry items, but donít try to carry anything too heavy when youíre still getting accustomed to the cane.\n- Wear shoes with rubber soles to reduce the chances of a fall.\nLearning how to use a cane will have a significant impact on your quality of life. Canes can help you recover from knee injuries, reduce stress and, most importantly, help you get on with your life.","|20060219280||Walker foot||October, 2006||Robinson et al.|\n|20020112752||Canopy frame with outdoor canopies for chair||August, 2002||Blakney|\n|20020121296||Integrated electric fan and patio umbrella||September, 2002||Copple|\n|20080142064||Aluminum Frame For the Construction of a Sunshade With Double Layer of Fabric and Adjustable to Any Kind of Sunshade||June, 2008||Maraki|\n|20090159106||MOBILITY DEVICES WITH INTERCHANGEABLE FEATURES||June, 2009||Schulz et al.|\n|20070209690||Collapsible umbrella with small closed volume||September, 2007||Ko|\n|20070039638||Backpack with deployable umbrella||February, 2007||Johnson et al.|\n|20080314427||Protective play enclosure||December, 2008||Lai|\n|20030089388||Frame for objects that open out, such as umbrellas||May, 2003||Cassagne|\n|20090139555||Dual Folded Umbrella||June, 2009||Kharag|\n|20070251558||Positioning apparatus of a collapsible umbrella||November, 2007||Ko|\n 1. Field of the Invention\n This invention relates generally to crutches and more specifically to leg support crutches designed to permit ambulatory movement by a patient recuperating from an injury or surgery to the foot, ankle, knee, or lower leg.\n 2. Description of the Prior Art\n As needed following an injury or surgery to a foot, ankle, knee, or lower leg, a patient commonly uses a pair of crutches to support himself or herself when recuperating. The crutches aid the patient when walking by supporting the weight otherwise carried by the injured limb.\n A typical approach is to use a pair of crutches. The injured member is held in a bent position so as not to come into contact with the floor and a crutch is placed to each side of the body to extend into the arm pit area with the weight otherwise supported by the injured member assumed by the hands of the person placed on a cross member of the crutch. Unfortunately, with this approach, the person usually has discomfort in the arm pit areas, shoulders, and in the hands. Also, holding the member in a bent position is trying and leads to discomfort. Another disadvantage is the unavailability of the hands to do their normal tasks. Transporting objects is a normal task that becomes complicated by the use of crutches.\n A number of efforts have been made to design a hands free crutch that straps in some manner to the injured member to transfer weight otherwise carried by it to the knee or thigh. Examples of these are shown in U.S. Pat. Nos. 5,941,263, 5,575,299, 5,300,595, and 5,178,595. For extended use, these examples have drawbacks. They fail to satisfactorily minimize stresses on the knee, distribute the forces that bear on the thigh, or optimize stability for ambulatory movement.\n It is the object of this invention to provide a means for hands free support of body weight, bypassing the entire leg. Another object is to provide a brace allowing hands free walking. The walkable brace is attached to the lower leg and thigh in such a manner as to transfer the weight to the ischium bone.\n These and other objects and advantages will be apparent to those skilled in the art in light of the following disclosure, claims and accompanying drawings.\n Referring to\n The thigh cradle\n The shin cradle\n The struts\n In substitution of caps\n Certain height variation can be accommodated by placement of bolt holes passing through struts\n When strapped into and secured within thigh cradle\n Although the invention has been shown and described with respect to preferred embodiments thereof, it should be understood by those skilled in the art that various changes and omissions in the form and detail thereof may be made therein without departing from the spirit and scope of the invention as defined in the appended claims."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:eca61507-20f7-4f8d-b277-dd0d9e4919e7>","<urn:uuid:6589437b-01e5-4a4c-b8ee-79386d64ce70>"],"error":null}
{"question":"What similarities and differences exist between the symptoms of depression described by historical accounts and those identified through modern brain imaging research?","answer":"Historical accounts describe depression through experiential symptoms like feeling everything to be heavy, inability to finish tasks, profound inability to will change, feeling disconnected from others, and experiencing recurring negative thoughts. Modern brain imaging research, while confirming these subjective experiences, reveals physical manifestations through DTI MRI scans, showing differences in brain white matter integrity between depressed and non-depressed individuals. This scientific approach demonstrates that depression involves alterations across multiple brain networks rather than being localized to a single area, providing biological evidence for what was historically understood primarily through personal accounts.","context":["Mental Health and the Brain: Depression, Anxiety, Schizophrenia\nMental Health and the Brain:\nOur eighth session and resulting on-line forum discussion completed our discussion of mental health therapies in general. This week, we will begin discussion of particular instances of mental health issues in the context of a variety of available therapeutic procedures and our earlier discussions of the brain.\nReadings for this week\nDepression (and anxiety?)\n- Exploring depression: drugs, psychotherapy, stories, conflicts ...\n- David Hume: a letter to a physician\n- Depression from the inside\n- Schizophrenia, \"Just the facts\". What we know in 2008. Epidemiology and etiology.\n- Connections in schizophrenia\nRelevant recent materials elsewhere\n- Science Times last Tuesday on genes, including \"Scientists and philosophers find that 'gene' has a multitude of meanings\"\n- In a novel theory of mental disorders, parents' genes are in competition (see comment for a perspective on this article different from that emphasized by the title)\n- A special issue of Science on \"Genes and Behavior,\" including a review on Genes and Social Behavior\nWhere we've been ...\nI can't help but wonder, if our brains can, so to speak, \"get us into these messes\" ... shouldn't our brains alone be able to fix it? ... How can it create something, without having a way to undo it? It seems that if therapy and meds were always needed to fix whatever the brain had caused, then, evolutionarily, we'd be at a huge disadvantage, because what if these things were not available? ... I just find it hard to believe that we don't have the ability to will ourselves to get better ... kgins\nBetter than 80% of us have some experience with one or another form of mental health procedure ... Paul Grobstein\nI'm glad we got to spend a little time talking about Freud and psychoanalysis on Monday ... the man and his thoughts are such a huge influence on pretty much everything we talk about in class every week ... As far as a comparison between CBT and psychoanalysis, I don't see much difference in the end goal - to develop a new story. It seems like CBT just gives you a more concrete method for doing this. More tools... It seems empowering to me ... ryan g\nI find psychoanalysis fascinating ... I agree ... that CBT seems more like pharmacotherapy in that it treats the symptoms but may not treat the underlying cause. In my opinion psychoanalysis is more about self-discovery ... Paige Safyer\nPharmacotherapy and talk therapy provide us with a tools that may be more useful for some individuals than others. For example, individuals with clearly defined problems that they wish to address can appreciate the brief, cheaper, and more targeted approach of CBT. However, others are incapable of articulating a specific problem. That doesn’t mean that they don’t deserve help, need help, or are ready for help. It is possible that they are seeking professional help because of an inability to resolve dissatisfaction with their mental states through informal routes. Their lack of communication can be considered a symptom of their mental illness ... jrlewis\nWho am I to say that any one therapy is ineffective or even inefficient? Who are we to say that one is better than any other? Do we even have a definition about what a \"better\" therapy would be? Is it cheaper? Quicker? Gets people off drugs faster? Do the patients \"feel\" better? I have no answers for these kinds of questions, and I haven't heard a cohesive argument from the class as of yet ... Ljones\nI want to second akerle's comments ... social structures - for good and for bad have been eroded by modernity and city life. We have significantly more freedom and that freedom - can be both very helpful for people (especially people who do not \"fit in\") but it can also prove to be very isolating to people ... adiflesher\nSince class, I have been trying to reconcile my emotions with my thoughts ... Emotional and mental pain seem mysterious, perhaps always will *be* mysterious, but if there were such things as antibiotics for depression, medications that consistently worked and lightened the brunt, would we advocate avoiding them? ... ysilverman\n\"the very moment I stopped thinking of my condition as “the enemy,” I made a turn and began to get better. I wasn’t cured, wasn’t forever well, but I was better. Metaphors matter.\" ... Siri Hustvedt\naccepting something doesn't mean a resignation to it--and often in that acceptance can come the time and space to learn the true nature of that which we initially felt compelled to fight ... all illness, mental, physical, is part of our bodies, our brains, or stories, and so an appropriate relationship to it is not one of pure other/enemy, but of trying to figure what it's doing, how it got there, and how now to proceed in light of its presence ... It's very interesting to me to consider what the brain is trying to do through conflict and illness, and what we as patients and physicians may be doing to help or impede those responses and processes ... mstokesthe ability to experience conflict and have it be generative necessitates a vision of something other than that which one is experiencing, another story that exists simultaneously with the current story. Without the possibility of a new story, one that involves less tumult, a sustainable vision of something less conflicted, embracing one’s “pain,” owning it and leading an “examined life” may lead to increasing distress, or a sense of paralysis ... The move towards an embrace of ever narrower definitions of “normal” and a diminished sense of interpersonal connections has led to more social isolation ... We have reached a place, collectively, where our bodies (yes, our minds, too) are seen in times of duress as the enemy ... The very language of \"illness\" and \"health\" seems to exist not along a continuum but as an all or nothing, defined set of milestones and endpoints that undermines individual, nuanced experiences and catapults illness into the realm of battle: doctor vs. unruly body ... I see therapy, whether psychodynamic, CBT or anything in between, as a way to further understand one's experience of herself in the world, her place in the world ... Sophie F\nTake off points - schizophrenia\ndescriptions of schizophrenia have been fairly consistent over the past two centuries and .... its occurrence has been relatively stable over the period\nworldwide prevalence with pockets of low and high prevalence\n- Heritability is high ...\n- There is no \"major\" gene locus ... and a large number of candidate ... genes may contribute\n- No gene appears to be either sufficient or necessary for the developent of schizophrenia\nAlthough it appears that our understanding of the causation of schizophrenia has substantially increased during the past two decades, what we can confidently assert is essentially the same - both genetic and environmental factors are important, but ... how they cause schizophrenia is still unknown .... A reconsideration of our basic strategies and fundamental assumptions may be in order.\nTo paraphrase Mark Twain in this regard, \"it ain't what people don't know that hurts them, it's what they know that ain't so\".\nWe need to consider the possibility that there is no \"one\" schizophrenia.\nWe have accumulated a significant amount of infomration pertaining to the causes of schizophrenia, but our comprehension of its etiology remains limited. It is vital that we examine the reasons for this continuing gap between \"findings\" and \"understanding.\"\n\"Only recently has much attention been given to white matter in schizophrenia. The primary research focus has been on gray matter, where the neuronal cell bodies, dendrites, and synapses are located, not on the white matter, which provides the wiring between cortical regions ... the identification of morphological and neurobiological abnormalities in white matter lends further support to the hypothesis that there is a problem in brain connectivity in schizophrenia.\"\n\"If we interpret the increased white matter metabolism as an indication that the white matter is stressed, it would be helpful to better understand the nature and causes of this stress.\"\n\"Future advances in schizophrenia research will depend on examinations of connections at and across multiple levels, including cellular pathways, neuronal circuittry, brain regions, and different modalities and disciplines.\"\nTake off points - depression (and anxiety?)\n\"These days, most people think of depression in terms of a contemporary pharmacology-based \"medical model\", the core of which is the idea that depression is an \"illness\" resulting from \"chemical imbalances\". From this perspective, the actual feelings and experiences that depressed individuals have are of relatively little interest, either therapeutically or in terms of trying to better understand depression, and the principle task is to find ways to \"correct\" the underlying disturbed pharmacological pattern ... My own guess has been and continues to be that while there are certainly pharmacological correlates to depression, the condition does not at all reduce to those, and that a fuller understanding of both depression and ways to treat it depends fundamentally on paying more attention to individual feelings and experiences, to observations made and reported \"from the inside\"\" ... Exploring Depression\nI have of late--but\nwherefore I know not--lost all my mirth, forgone all\ncustom of exercises; and indeed it goes so heavily\nwith my disposition that this goodly frame, the\nearth, seems to me a sterile promontory, this most\nexcellent canopy, the air, look you, this brave\no'erhanging firmament, this majestical roof fretted\nwith golden fire, why, it appears no other thing to\nme than a foul and pestilent congregation of vapours.\nWhat a piece of work is a man! how noble in reason!\nhow infinite in faculty! in form and moving how\nexpress and admirable! in action how like an angel!\nin apprehension how like a god! the beauty of the\nworld! the paragon of animals! And yet, to me,\nwhat is this quintessence of dust? man delights not\nme: no, nor woman neither, though by your smiling\nyou seem to say so.\n\"all my ardour seemed in a moment to be extinguished, and I could no longer raise my mind to that pitch, which formerly gave me such excessive pleasure ... I was continually fortifying myself with reflections against death, and poverty, and shame, and pain, and all the other calamities of life. These no doubt are exceeding useful, when joined with an active life, because the occasion being presented along with the reflection, works it into the soul, and makes it take a deep impression; but in solitude they serve to little other purpose, than to waste the spirits, the force of the mind meeting with no resistance, but wasting itself in the air, like our arm when it misses its aim ... Upon my mentioning it to my physician, he laughed at me and told me I was now a brother, for that I had fairly got the disease of the learned ... It is a weakness rather than a lowness of spirits which troubles me, and there seems to be as great a difference betwixt my distemper and common vapours, as betwixt vapours and madness. I have noticed in the writings of the French mystics, and in those of our fanatics here, that when they give a history of the situation of their souls, they mention a coldness and desertion of the spirit, which frequently returns and some of them, at the beginning, have been tormented with it many years. As this kind of devotion depends entirely on the force of passion, and consequently of the animal spirits, I have often thought that their case and mine were pretty parallel, and that their rapturous admirations might discompose the fabric of the nerves and brain, as much as profound reflections, and that warmth or enthusiasm which is inseparable from them\"\nFeeling everything to be heavy, things, even simple ones, are just too hard to do. Starting things, not being able to finish them, developing into a feeling that therefore nothing is worth starting.\nFeeling that nothing will ever change, that one's discomfort is what has always been, will always be.\nA profound inability to \"will\" change in either behaviors or mood\nOne is aware of one's own behavior/feelings, but can't do anything about them.\nfeeling very lonely and removed from everything. Being with people usually only intensifies the loneliness, making me more aware of how disconnected I feel.\nIt isn't that I don't deserve happiness, I just will never have it, and so it often seems best to give up and settle for something that isn't as wonderful as happiness but also isn't as terrible as depression.\nThere's always a voice that pops in and reminds me that anything I do become mildly interested is pointless because it will soon become unsatisfying, so no point in trying.\nI have been FRUSTRATED and ANGRY at everything, small things, big things, and at nothing, and at myself. I am pretty sure I'm feeling better than before, but then sometimes I don't know. I feel good. Then I feel bad. Then I feel good. Then I feel bad again. It makes me feel overwhelmed. I feel, for lack of a better word, CRAZY. Maybe fragmented is a good word.\nWithout the mask of depression, my anxiety has returned.\nI feel like I need there to be something wrong with me when nothing is. My story of myself, my identity, very much includes medications, depression, and anxiety. Without them, I don't know who I would be. They are my crutches. This is who I am. And sometimes I am unwilling for that to change.\n\"My GP once said that people often emerge from 'the dark place' that is depression with a great deal of emotional knowledge and empathy. She was right! Having a breakdown has changed my life for the better. I now look at the world with new eyes, I've become a much more compassionate and patient person and feel so much the better for it. Depression truly can help you clarify what is important.\" .... comment on Is Depression Good for You?","Summary: Researchers are utilizing neuroimaging data to identify brain patterns and predict depression with the help of machine learning.\nSource: University of Texas at Austin.\nUniversity of Texas researchers use Stampede supercomputer to identify patterns in neuroimaging data that are predictive for mental disorders.\nDepression affects more than 15 million American adults, or about 6.7 percent of the U.S. population, each year. It is the leading cause of disability for those between the ages of 15 and 44.\nIs it possible to detect who might be vulnerable to the illness before its onset using brain imaging?\nDavid Schnyer, a cognitive neuroscientist and professor of psychology at The University of Texas at Austin, believes it may be. But identifying its tell-tale signs is no simpler matter. He is using the Stampede supercomputer at the Texas Advanced Computing Center (TACC) to train a machine learning algorithm that can identify commonalities among hundreds of patients using Magnetic Resonance Imaging (MRI) brain scans, genomics data and other relevant factors, to provide accurate predictions of risk for those with depression and anxiety.\nResearchers have long studied mental disorders by examining the relationship between brain function and structure in neuroimaging data.\n“One difficulty with that work is that it’s primarily descriptive. The brain networks may appear to differ between two groups, but it doesn’t tell us about what patterns actually predict which group you will fall into,” Schnyer says. “We’re looking for diagnostic measures that are predictive for outcomes like vulnerability to depression or dementia.”\nIn 2017, Schnyer, working with Peter Clasen (University of Washington School of Medicine), Christopher Gonzalez (University of California, San Diego) and Christopher Beevers (UT Austin), completed their analysis of a proof-of-concept study that used a machine learning approach to classify individuals with major depressive disorder with roughly 75 percent accuracy.\nMachine learning is a subfield of computer science that involves the construction of algorithms that can “learn” by building a model from sample data inputs, and then make independent predictions on new data.\nThe type of machine learning that Schnyer and his team tested is called Support Vector Machine Learning. The researchers provided a set of training examples, each marked as belonging to either healthy individuals or those who have been diagnosed with depression. Schnyer and his team labelled features in their data that were meaningful, and these examples were used to train the system. A computer then scanned the data, found subtle connections between disparate parts, and built a model that assigns new examples to one category or the other.\nIn the study, Schnyer analyzed brain data from 52 treatment-seeking participants with depression, and 45 heathy control participants. To compare the groups, they matched a subset of depressed participants with healthy individuals based on age and gender, bringing the sample size to 50.\nParticipants received diffusion tensor imaging (DTI) MRI scans, which tag water molecules to determine the extent to which those molecules are microscopically diffused in the brain over time. By measuring this diffusion in multiple spatial directions, vectors are generated for each voxel (three-dimensional cubes that represent either structure or neural activity throughout the brain) to quantify the dominant fiber orientation. These measurements are then translated into metrics that indicate the integrity of white matter pathways within the cerebral cortex.\nOne common parameter used to characterize DTI is fractional anisotropy: the extent to which diffusion is highly directional (high fractional anisotropy) or unrestricted (low fractional anisotropy).\nThey compared these fractional anisotropy measurements between the two groups and found statistically significant differences. They then reduced the number of voxels involved to a subset that was most relevant for classification and carried out the classification and prediction using the machine learning approach.\n“We feed in whole brain data or a subset and predict disease classifications or any potential behavioral measure such as measures of negative information bias,” he says.\nThe study revealed that DTI-derived fractional anisotropy maps can accurately classify depressed or vulnerable individuals versus healthy controls. It also showed that predictive information is distributed across brain networks rather than being highly localized.\n“Not only are were learning that we can classify depressed versus non-depressed people using DTI data, we are also learning something about how depression is represented within the brain,” said Beevers, a professor of psychology and director of the Institute for Mental Health Research at UT Austin. “Rather than trying to find the area that is disrupted in depression, we are learning that alterations across a number of networks contribute to the classification of depression.”\nThe scale and complexity of the problem necessitates a machine learning approach. Each brain is represented by roughly 175,000 voxels and detecting complex relationship among such a large number of components by looking at the scans is practically impossible. For that reason, the team uses machine learning to automate the discovery process.\n“This is the wave of the future,” Schnyer says. “We’re seeing increasing numbers of articles and presentations at conference on the application of machine learning to solve difficult problems in neuroscience.”\nThe results are promising, but not yet clear-cut enough to be used as a clinical metric. However, Schnyer believes that by adding more data — related not only to MRI scans but also from genomics and other classifiers — the system can do much better.\n“One of the benefits of machine learning, compared to more traditional approaches, is that machine learning should increase the likelihood that what we observe in our study will apply to new and independent datasets. That is, it should generalize to new data,” Beevers said. “This is a critical question that we are really excited to test in future studies.”\nBeevers and Schnyer will expand their study to include data from several hundred volunteers from the Austin community who have been diagnosed with depression, anxiety or a related condition. Stampede 2 — TACC’s newest supercomputer which will come online later in 2017 and will be twice as powerful as the current system — will provide the increased computer processing power required to incorporate more data and achieve greater accuracy.\n“This approach, and also the movement towards open science and large databases like the human connectome project, mean that facilities like TACC are absolutely essential,” Schnyer says. “You just can’t do this work on desktops. It’s going to become more and more important to have an established relationship with an advanced computing center.”\nAbout this psychology research article\nFunding: National Institutes of Health, National Science Foundation funded this study.\nSource: Aaron Dubrow – University of Texas at Austin Image Source: NeuroscienceNews.com images are credited to David M Schnyer, Peter C. Clasen, Christopher Gonzalez and Christopher G. Beevers. Original Research:Abstract for “Evaluating the diagnostic utility of applying a machine learning algorithm to diffusion tensor MRI measures in individuals with major depressive disorder” by David M Schnyer, Peter C. Clasen, Christopher Gonzalez, and Christopher G. Beevers in Psychiatry Research: Neuroimaging. Published online March 22 2017 doi:10.1016/j.pscychresns.2017.03.003\nCite This NeuroscienceNews.com Article\n[cbtabs][cbtab title=”MLA”]University of Texas at Austin “Enlisting The Help of Machine Learning to Diagnose Depression.” NeuroscienceNews. NeuroscienceNews, 28 March 2017. <https://neurosciencenews.com/depression-machine-learning-6301/>.[/cbtab][cbtab title=”APA”]University of Texas at Austin (2017, March 28). Enlisting The Help of Machine Learning to Diagnose Depression. NeuroscienceNew. Retrieved March 28, 2017 from https://neurosciencenews.com/depression-machine-learning-6301/[/cbtab][cbtab title=”Chicago”]University of Texas at Austin “Enlisting The Help of Machine Learning to Diagnose Depression.” https://neurosciencenews.com/depression-machine-learning-6301/ (accessed March 28, 2017).[/cbtab][/cbtabs]\nEvaluating the diagnostic utility of applying a machine learning algorithm to diffusion tensor MRI measures in individuals with major depressive disorder\nHighlights •Examination of utility of applying support vector machine (SVM) learning to MRI measures of brain white matter in order to classify individuals with major depressive disorder (MDD). •High degree of accuracy when utilizing a feature selection approach. •Distributed information is more diagnostically informative than highly localized.\nAbstract Using MRI to diagnose mental disorders has been a long-term goal. Despite this, the vast majority of prior neuroimaging work has been descriptive rather than predictive. The current study applies support vector machine (SVM) learning to MRI measures of brain white matter to classify adults with Major Depressive Disorder (MDD) and healthy controls. In a precisely matched group of individuals with MDD (n = 25) and healthy controls (n = 25), SVM learning accurately (74%) classified patients and controls across a brain map of white matter fractional anisotropy values (FA). The study revealed three main findings: 1) SVM applied to DTI derived FA maps can accurately classify MDD vs. healthy controls; 2) prediction is strongest when only right hemisphere white matter is examined; and 3) removing FA values from a region identified by univariate contrast as significantly different between MDD and healthy controls does not change the SVM accuracy. These results indicate that SVM learning applied to neuroimaging data can classify the presence versus absence of MDD and that predictive information is distributed across brain networks rather than being highly localized. Finally, MDD group differences revealed through typical univariate contrasts do not necessarily reveal patterns that provide accurate predictive information.\n“Evaluating the diagnostic utility of applying a machine learning algorithm to diffusion tensor MRI measures in individuals with major depressive disorder” by David M Schnyer, Peter C. Clasen, Christopher Gonzalez, and Christopher G. Beevers in Psychiatry Research: Neuroimaging. Published online March 22 2017 doi:10.1016/j.pscychresns.2017.03.003"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:dc598a6c-f6c9-4b6a-8c49-b8e231c6d7ec>","<urn:uuid:d127661c-da97-4115-9abd-a098698f3a07>"],"error":null}
{"question":"How do standardized care processes in chronic disease management compare with hospital value-based purchasing program requirements?","answer":"Standardized care processes in chronic disease management focus on maintaining consistent workflows to engage patients while still individualizing therapy, using electronic health records to identify at-risk patients and implementing team-based approaches to improve adherence. In contrast, the Hospital Value-Based Purchasing Program implements standardization through specific quality metrics, requiring hospitals to eliminate adverse events, adopt evidence-based care protocols, improve patient care experiences, and increase transparency - all while maintaining cost efficiency. Both approaches aim to improve care quality, but value-based purchasing explicitly ties financial reimbursement to performance on these standardized measures, while chronic disease management standardization serves primarily to improve care delivery and patient outcomes.","context":["Education and team-based care: The two essentials to providing effective chronic care.\nToday, more Americans than ever are living and struggling with chronic health conditions. According to the CDC, roughly 70 percent of the population will die from a chronic disease and caring for those patients accounts for over 86 percent of the nation’s healthcare spending.\nA chronic condition, by its very definition, requires ongoing and attentive treatment by healthcare providers to attain optimal outcomes for their patients. This is no easy task, as healthcare resources are stretched to their breaking point and the prevalence of chronic diseases are rising at an alarming rate. The market is at the tip of the proverbial iceberg of what is undoubtedly an epidemic, as chronic diseases account for the majority of deaths in the United States and globally. However, there are best practices for managing the vast population of chronic diseases such as diabetes.\nThere are two approaches for caring for chronically ill patients: a team-based strategy involves a cross-departmental collection of providers caring for one patient while a patient-centered method brings the patient into the decision-making process of creating an individualized plan for care. These two strategies are not mutually exclusive, but they aren’t correlated either.\nCompliance vs. Adherence\nA common challenge for providers and educators is engaging patients to proactively participate in their care and adhere to the designated plan of action. It is reasonable to assume that most patients inherently want to be well, but they can become frustrated and exhausted in dealing with the daily grind that is diabetes care and often give up.\nProviders continue to struggle with the terms compliance vs. adherence when describing a patient’s ability to follow a plan of care. The distinguishing factor is that adherence allows the patient to participate in the self-management of their disease rather than yielding themselves to a proposed regimen from the healthcare provider. When a patient adheres to a treatment plan they are collaborating with their provider to develop and execute next steps together, increasing engagement and employing a team-based and patient-centered approach to care.\nIn order to enhance care quality, providers and educators need to address the challenge of improving adherence and therefore outcomes. It is not enough to continue approaching diabetes care, or any chronic illness, with traditional episodic one-on-one care as these types of diseases need the collaborative assistance of an interdisciplinary team, calling for a different strategy such as a team-based and/or patient-centered approach to care.\nDespite an expanding portfolio of diabetes medications available and strong efforts to expand diabetes education programs, the outcomes for these patients remain relatively poor. When caring for members of the chronic care population, there needs to be a shift in the care model to involve the patient, educators, and additional team members, which will impact treatment plans, patient engagement, and ultimately outcomes.\nContinuing with the example of diabetes, the disease is extremely personal beyond the differences between type 1 and type 2 diabetes, each person has different treatment goals, medication needs, and required insulin doses. As such, effective care depends upon a patient’s adherence to their customized and recommended plan of action. When patients are not adhering to their plan, it is imperative to identify the root cause and combat it to begin re-engaging with them.\n- Financial issues often force patients to choose between medical costs and other living expenses.\n- Adverse side effects can include hypoglycemia, swelling, nausea, etc. and may result in ED visits or hospitalization. These side effects can cause patients to discontinue treatment.\n- Knowledge deficits regarding the multiple aspects of their care plan, including medications, exercise, diet, hypoglycemia treatment, etc. also hinder one’s ability to adhere.\n- Psychosocial challenges such as depression, a common comorbid condition in people with diabetes, can deter patients from continuing with their plan.\nOnce the barrier is identified, team-based care and disease education are instrumental in engaging patients as well as improving adherence. Understanding new concepts, technology and benefits of medication regimens are essential to this strategy. Education is best delivered by a team of providers and Certified Diabetes Educators across multiple disciplines, but is often underutilized, as it can be costly and difficult to find local experts. Since people learn in a variety of ways: reading, writing, observing, listening, etc., providers need to utilize a form of education that best suits their patient’s learning style to increase their likelihood of self-management of diabetes. Embracing social media platforms (Twitter, Blogs, Facebook, etc.) is an effective manner for patients to communicate with their peers and share their struggles in diabetes management, fostering community. These touch points also serve as a launching point for long-term diabetes self-management support as patients connect with one another.\nStandardization of Chronic Care Processes\nClinicians and experts are often asked, “How can healthcare providers standardize the care of patients with chronic diseases?” Standardization of care processes for patients with chronic diseases can result in improved care quality, increased drug adherence, enhanced patient outcomes and reduced costs. It is important for providers to individualize therapy and maintain a patient-centered approach, as recommended by clinical guidelines published by numerous associations around the world to ensure optimal outcomes throughout standardization.\nAs value-based payment models begin to take effect, it is imperative that providers identify a consistent workflow to engage patients in their individualized care. The allocation of resources, both electronic and human, containing strategies to successfully manage a large population of patients is essential to the success of any chronic disease management program. In that same vein, overall positive population health management is a key to successful chronic disease management. Providers need to be able to identify, engage, and communicate successfully with patients who are struggling with their care – chronic or not. Going back to the diabetes example, providers must standardize a process to utilize electronic health records (EHRs) to mine the data and identify patient populations with diabetes and flag those who are at-risk by missing appointments, not achieving A1c, blood pressure, or lipid targets, frequently visiting the emergency room, requiring recurrent hospitalizations, not refilling their medications, and who are missing their specialty appointments.\nThere must be a team-based approach to care in place to engage these patients once they are identified by working with care coordinators, educators, providers, etc. These struggling patients may require frequent communications, at times daily, in order to keep these individuals out of the hospital and encourage their adherence to the personalized care plan. This requires both manpower and technology, but providers should be vigilant about engaging with patients through their preferred methods, such as text messages, e-mail, phone calls, telehealth visits or even “snail” mail. Successful chronic disease management includes standardizing a process to identify the at-risk patients, and then individualizing their engagement, education, and treatment through a team-based and patient-centered approach.","Value-based healthcare is a healthcare delivery model in which providers, including hospitals and physicians, are paid based on patient health outcomes. Under value-based care agreements, providers are rewarded for helping patients improve their health, reduce the effects and incidence of chronic disease, and live healthier lives in an evidence-based way.\nValue-based care differs from a fee-for-service or capitated approach, in which providers are paid based on the amount of healthcare services they deliver. The “value” in value-based healthcare is derived from measuring health outcomes against the cost of delivering the outcomes.\nWhat Are the Benefits of Value-Based Healthcare Delivery?\nThe benefits of a value-based healthcare system extend to patients, providers, payers, suppliers, and society as a whole.\nManaging a chronic disease or condition like cancer, diabetes, high blood pressure, COPD, or obesity can be costly and time-consuming for patients. Value-based care models focus on helping patients recover from illnesses and injuries more quickly and avoid chronic disease in the first place. As a result, patients face fewer doctor’s visits, medical tests, and procedures, and they spend less money on prescription medication as both near-term and long-term health improve.\nProviders achieve efficiencies and greater patient satisfaction.\nWhile providers may need to spend more time on new, prevention-based patient services, they will spend less time on chronic disease management. Quality and patient engagement measures increase when the focus is on value instead of volume. In addition, providers are not placed at the financial risk that comes with capitated payment systems. Even for-profit providers, who can generate higher value per episode of care, stand to be rewarded under a value-based care model.\nPayers control costs and reduce risk.\nRisk is reduced by spreading it across a larger patient population. A healthier population with fewer claims translates into less drain on payers’ premium pools and investments. Value-based payment also allows payers to increase efficiency by bundling payments that cover the patient’s full care cycle, or for chronic conditions, covering periods of a year or more.\nSuppliers align prices with patient outcomes.\nSuppliers benefit from being able to align their products and services with positive patient outcomes and reduced cost, an important selling proposition as national health expenditures on prescription drugs continue to rise. Many healthcare industry stakeholders are calling for manufacturers to tie the prices of drugs to their actual value to patients, a process that is likely to become easier with the growth of individualized therapies.\nSociety becomes healthier while reducing overall healthcare spending.\nLess money is spent helping people manage chronic diseases and costly hospitalizations and medical emergencies. In a country where healthcare expenditures account for nearly 18% of Gross Domestic Product (GDP), value-based care has the promise to significantly reduce overall costs spent on healthcare.\nHow Does Value-Based Healthcare Translate to New Delivery Models?\nThe proliferation of value-based healthcare is changing the way physicians and hospitals provide care. New healthcare delivery models stress a team-oriented approach to patient care and sharing of patient data so that care is coordinated and outcomes can be measured easily. Two examples are reviewed here.\nValue-Based Care Models: Medical Homes\nIn value-based healthcare models, medical care does not exist in silos. Instead, primary, specialty, and acute care are integrated, often in a delivery model called a patient-centered medical home (PCMH). A medical home isn’t a physical location. Instead, it’s a coordinated approach to patient care, led by a patient’s primary physician who directs a patient’s total clinical care team.\nPCMHs rely on the sharing of electronic medical records (EMRs) among all providers on the coordinated care team. The goal of EMRs is to put crucial patient information at each provider’s fingertips, allowing individual providers to see results of tests and procedures performed by other clinicians on the team. This data sharing has the potential to reduce redundant care and associated costs.\nValue-Based Care Models: Accountable Care Organizations\nAccountable care organizations (ACOs) were originally designed by the Centers for Medicare & Medicaid Services (CMS) to provide high-quality medical care to Medicare patients. In an ACO, doctors, hospitals, and other healthcare providers work as a networked team to deliver the best possible coordinated care at the lowest possible cost. Each member of the team shares both risk and reward, with incentives to improve access to care, quality of care, and patient health outcomes while reducing costs. This approach differs from fee-for-service healthcare, in which individual providers are incentivized to order more tests and procedures and manage more patients in order to get paid more, regardless of patient outcomes.\nLike PCMHs, ACOs are patient-centered organizations in which the patient and providers are true partners in care decisions. Also like PCMHs, ACOs stress coordination and data sharing among team members to help achieve these goals among their entire patient population. Clinical and claims data are also shared with payers to demonstrate improvements in outcomes such as hospital readmissions, adverse events, patient engagement, and population health.\nHospital Value-Based Purchasing\nUnder CMS’s Hospital Value-Based Purchasing Program (VBP), acute care hospitals receive adjusted payments based on the quality of care they deliver. According to the CMS website, the program encourages hospitals to improve the quality and safety of acute inpatient care for all patients by:\n- Eliminating or reducing adverse events (healthcare errors resulting in patient harm)\n- Adopting evidence-based care standards and protocols that make the best outcomes for the most patients\n- Changing hospital processes to create better patient care experiences\n- Increasing care transparency for consumers\n- Recognizing hospitals that give high-quality care at a lower cost to Medicare\nCMS is expected to continue to refine its VBP measurements, making it important for hospitals to continuously improve their clinical outcomes so they can simultaneously improve reimbursement and their reputation among healthcare consumers.\nWhat Is the Future of Value-Based Healthcare?\nMoving from a fee-for-service to a fee-for-value system will take time, and the transition has proved more difficult than expected. As the healthcare landscape continues to evolve and providers increase their adoption of value-based care models, they may see short-term financial hits before longer-term costs decline. However, the transition from fee-for-service to fee-for-value has been embraced as the best method for lowering healthcare costs while increasing quality care and helping people lead healthier lives."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:7bd78991-554d-4a4c-89e0-f2f273f5e389>","<urn:uuid:9d09430b-00ce-4b27-9d69-a6706b4f1818>"],"error":null}
{"question":"What are the main differences between Apollo and Bran mac Febail in terms of their divine or mythological roles?","answer":"Apollo and Bran mac Febail have distinct mythological roles. Apollo is a Greek god with multiple divine functions, including being the god of the sun, truth, prophecy, healing, plague, music, and poetry. He is the son of Zeus and Leto, and serves as the patron god of music and poetry as the leader of the Muses. In contrast, Bran mac Febail is a mortal figure from Irish narrative who encounters the supernatural when he is lured to the Otherworld by a beautiful divine woman. While Apollo holds permanent divine status with established godly powers, Bran is a mortal who experiences a temporary journey into the magical realm of Emain Ablach, where he spends what seems like a brief time but actually amounts to hundreds of years in mortal time.","context":["Irish title for the 7th- or 8th-century narrative known in English as The Voyage of Bran Son of Febal, or Bran's Voyage to the Land of Women; also Echtrae Brain Maic Fhebail. Several extant texts are found in Ireland, including the 11th-century Book of the Dun Cow, and the Book of Leinster, both thought to be derived from a lost manuscript compiled at the monastery of Druim Snechta (Co. Monaghan). One of the oldest Irish narratives, Imram Brain is quite long and contains many digressions; at its core is the familiar European story (international folk motifs: F111; F112) of the mortal lured to the Otherworld by a beautiful, divine woman.\nOne day Bran [raven] mac Febail is walking near his dún [stronghold] when he hears sweet music coming behind him that lulls him to sleep. When he awakes he has a silver branch in his hand covered with silver-white apple blossom, which he carries back with him to the stronghold. As he shows the wondrous branch to his people, a beautiful woman in strange clothing appears before him, singing of Emne (i.e. Emain Ablach), the island where there is no grieving, winter, or want, where the golden horses of Manannán mac Lir prance, and where games and sport continue without cease. (Emne/ Emain Ablach is known in English under different names, the Land of Promise and the Isle of Women.) She bids Bran seek out that island, and when the song is done she disappears, the apple branch falling into Bran's hands, which cannot hold it.\nThe next morning Bran sets out with a fleet of currachs, three foster-brothers, and twenty-seven warriors. They row far across the sea until they come upon a warrior driving his chariot as if he were on land. Greeting them, he identifies himself as Manannán and sings further of Emain Ablach, urging Bran and his men to visit it. Though Bran feels he is rowing over the sea, it is for Manannán the flowery land of Mag Mell [Delightful Plain], where leaping salmon are calves and lambs. Then, in a sudden Christian interpolation, the sin of Adam is recalled and the death of Christ is foretold. Returning to the pre-Christian milieu, Manannán tells that he will become the lover of Caíntigern who will bear him the son Mongán, how the son will live and how he will die. And he tells Bran that if he keeps rowing he will reach Emain Ablach before sunset. Bran and his men follow this advice, but before reaching their goal they pass the Island of Merriment or Delight, whose inhabitants are so given to giddy shouting and laughter they will not answer enquiries. When Bran puts one of his men ashore, he too joins in the hilarity.\nFinally arrived at Emain Ablach, Bran is greeted by the leader of the women but is reluctant to go ashore. She throws a ball of thread that entangles his hands and draws him towards the great hall on the many-coloured island. Each of the men is given a bed, a female companion, and an endless supply of food until they begin to lose any sense of time. Eventually one of the shipmates, Nechtan mac Collbrain, speaks of his home-sickness for Ireland, and urges Bran to leave with him. Bran's lover warns against this, telling him that only sorrow will come of it. When it is clear that he will leave with all his men, she counsels him to retrieve the man left on the Island of Merriment so that his company will be complete, and that when they all return to Ireland they should look at it and call out to friends but that no one should actually touch the land. The first point they see is called Srub Brain, usually identified with Stroove Point on Lough Foyle, Co. Donegal, although Kuno Meyer (1895–7) thought it should be in south-western Ireland. Bran calls out his name, ‘Bran son of Febal’, to people on the shore; they answer that they do not know him, only that the tale of his voyage is one of their ancient stories. Nechtan, who has longed to return to Ireland, then jumps from the currach and wades through the surf; but as soon as his foot touches land his entire body crumbles into dust as if he has been in his grave 500 years. Bran stays long enough to tell his countrymen of his adventures by writing them in ogham on wooden sticks and casting them, and then he sails away with his companions, never to be seen again.","Name that god, goddess, hero or monster - SharpSchool\nWho is that Greek God/Goddess? Part 3 Who Is This\nGod? Ares/Mars God of war Ares often represents the physical or violent aspect of war, in contrast to the\narmored Athena, whose functions as a goddess of intelligence include military strategy and generalship. Ares was a dangerous force, overwhelming, insatiable in battle, destructive, and manslaughtering. Fear and Terror were his battle chariot. Ares is well known as the\nlover of Aphrodite, the goddess of love who was married to Hephaestus, god of craftsmanship. Ares is often depicted wearing a helmet, and carrying a weapon and a shield.\nMore illustrations of Ares What are some of his traits? Ares What are his traits?\nWho Is This God? Apollo\nGod of the Sun Apollo has been variously recognized as a god of light and the sun, truth and prophecy, healing, plague, music, poetry, and more. Apollo is the son of Zeus and Leto. He has a twin sister\nArtemis. As the patron of Delphi, he was an oracular god - prophetic deity. Medicine and healing are associated with Apollo, yet Apollo was also seen as a god who could bring ill-health and deadly plague.\nAs the leader of the Muses, Apollo also functioned as the patron god of music and poetry. Hermes created the lyre for him, and the instrument became a common attribute of Apollo. Symbols: Lyre, Raven Laurel Branches\nMore illustrations of Apollo What are some of his traits? Apollo What are some of his traits?\nHelios Who Is This\nGoddess? Artemis/Diana The Virgin Huntress Artemis was one of the most widely venerated of the Ancient Greek deities.\nShe is the daughter of Zeus and Leto and has a twin brother, Apollo. She was the Hellenic goddess of the hunt, wild animals, wilderness, childbirth, virginity, and protector of young girls, bringing and relieving\ndisease in women; she often was depicted as a huntress carrying a bow and arrows. The deer and the cypress were sacred to her. In later Hellenistic times, she even assumed the ancient role aiding childbirth.\nMore illustrations of Artemis What are some of her traits? ArtemisWhat are some of her traits?\nWho Is This God? Hephaestus/Vulcan\nGod of the Blacksmith Hephaestus was the Greek god of technology, blacksmiths, craftsmen, artisans, sculptors, metals, fire and volcanoes. In Greek mythology, Hephaestus was the son of Zeus and\nHera , the King and Queen of the Gods. As a smithing god, Hephaestus made all the weapons of the gods in Olympus. He served as the blacksmith of the gods, and was worshipped in the professions of manufacturing .\nHephaestus's symbols are a smith's hammer, anvil, and a pair of tongs. More illustrations of Hephaestus What are some of his traits?\nHephaestusWhat are some of his traits? Who Is This Goddess?\nHestia/Vesta Goddess of Home and Hearth/Fireside As the virgin goddess of the hearth (fireside), architecture, and the right ordering of domesticity, the family and\nthe state Hestia received the first offering at every sacrifice in the household. She is a daughter of Cronus and Rhea. This peace loving goddess is usually depicted as cloaked in a head veil,\nsometimes shown with a staff in hand or guarding a fire. More illustrations of Hestia What are some of her traits?\nMore illustrations of Hestia What are some of her traits? Who Is This\nGoddess? Demeter/Ceres Mother of the Earth Demeter is the goddess of the harvest, who presided over grains and the fertility of the earth,\nand the cycle of life and death. Demeter's daughter Persephone was abducted to the underworld by Hades. With her loss and her grief, the seasons halted; living things ceased their growth, then began to die. Faced with the extinction of all life on earth, Zeus sent his\nmessenger Hermes to the underworld to bring Persephone back. Hades agreed to release her if she had eaten nothing while in his realm; but Persephone had eaten a small number of pomegranate seeds. This bound her to Hades and the underworld for certain months of every year. To\nthe Greek, this period of time is tied to the winter, when plants cease to grow due to Demeters pain of being apart from her daughter. Symbols: Cornucopia wheat bread honey, cats and dogs\nMore illustrations of Demeter What are some of her traits? More illustrations of Demeter What are some of her traits?\nWho Is This God? Hades/Pluto\nRuler of the Underworld Hades is the god of the underworld. Seated next to him is his wife. Persephone, daughter of Demeter. All mortals are judged after death and are either rewarded or cursed\nunder his watch. Very few mortals could leave his realm once they entered. Symbols associated with him are the Helm of Darkness,and the threeheaded dog, Cerberus,and Tartarus, a deep, gloomy river in the\nunderworld. More illustrations of Hades What are his traits? HadesWhat are some of his traits?\nCerberus and Charon Quiz Time! Name that Olympian Lets quickly review your notes for 5\nminutes Please use the back side of your worksheet for the quiz. You may NOT use your notes during the quiz.\nTSS concentrations from urban land uses lower than what is observed in urban streams, suggesting that other downstream sources are also responsible for the urban sediment budget. Research studies. Urban stream channel erosion is a major component of the urban...\nPrizes Win prizes like: a bird feeder garden tools books a video iPod Program Title: Pollinate Your Mind - Summer Reading Program CDs clay pots dinner at the local fancy restaurant hose guides seeds & planters Gift Certificates to Local...\nEgypt: Middle and New Kingdoms 9th Grade Integrated Honors Mr. Coia Middle Kingdom 2050 B.C. to 1650 B.C. Began when a family of ruling nobles in Thebes emerged victorious in its struggles with the rulers of other cities, seized control...\nTo foster the development of applications for societal benefit at the local level by the NMHS. NOAA 2011 Satellite Direct Readout Conference * Key points of the VLab strategy Partnership between space agencies and training centres Covering all WMO Regions...\nAtomic number = Z, Mass number = A, Neutron number = N. Identify number of protons, the mass number, and number of neutrons for the following elements. Isotopes. Atoms of the same element that have different atomic masses. Same number...\nChest X-ray of HAP. Dr. Mike Malinzak. Duke University. Dept. of Radiology. Location . of item (slide # 17): \"Diagram showing a build up of fluid in the lining of the lungs (pleural effusion) CRUK 054\" by Cancer Research UK...\nReady to download the document? Go ahead and hit continue!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:01a946c0-8e1f-46dc-a8bb-2582b870cb2f>","<urn:uuid:d8db4483-e281-4897-9b9a-0d0ed9cb5a70>"],"error":null}
{"question":"How does modern upscale barbacoa preparation in Mexico City compare to traditional Aztec cooking methods in terms of ingredients and preparation techniques?","answer":"Traditional Aztec cooking and modern barbacoa preparation show significant differences. Modern barbacoa primarily focuses on meat preparation in earth ovens (hornos de la tierra) with specific layered components including a prepared surface, fire, hot rocks, green plant material layers, and an earthen cap. This technique is used extensively for cooking lamb and other meats. In contrast, Aztec cuisine was more focused on readily available ingredients like insects and grubs, with specialties including chapulines (grasshoppers), escamoles (ant eggs), and magüey worms. While some upscale Mexico City restaurants now serve these traditional Aztec ingredients as a cultural revival, they weren't originally prepared using the elaborate earth oven methods that characterize modern barbacoa.","context":["We’ve started collecting as many barbacoa videos as possible at YouTube in a playlist. We’re only collecting videos of hornos de la tierra, ie, earth oven cookery, not home oven or steamer “barbacoa”. There is nowhere on the internet or in books or magazines with more original source material on barbacoa than YouTube. You can find regional variations from all over Mexico, from barbacoa de borrego found in Central Mexico in backyard dirt hornos or commercial brick pits. There are various versions from the North, even Texan versions that only use the head of a cow. There are version from the Mixteca in Oaxaca using shallow rectangular pits that include not only meat, but giant tamales. There is, of course, the Yucatan’s cochinita pibil. And there are the various versions from the West, such as birria de chivo. To see the amazing diversity of Mexico’s barbacoa, really the best place is YouTube.\nWe will keep adding videos as we find them, so keep checking back. If you find a good one that’s not on our list, let us know.\nThere are many wonderful places to eat barbacoa around Mexico, but a few places stand out as extraordinary, places that transcend the quality, as good as it may be, of the barbacoa. For El Pica, it’s the overwhelming spectacle of a town within a town, this barbacoa wonderland that serves far more people each day it’s open than the entire population of La Purificacion, the pueblo it inhabits.\nEntering El Pica through the tunnel of trees brings back memories of hopping on the boat for the Jungle Cruise at Disneyland. The whole complex is a maze of pathways through vines, vendors around every corner selling tlacoyos, menudo, pan dulce, salsas, avocados, tamales, chicharron, and, of course, barbacoa.\nThe Torres family started in 1967 with just one horno, but now serve as many as 48 lambs a day from 12 hornos. They cook mixiotes of rabbit and chicken, pancita, and consome in the hornos along with the barbacoa. Tortillas from nixtamal, fresh pulque, pork skins fried until bubbly and crisp before your eyes — the emphasis here is on quality.\nFamilies picnic in verdant dining rooms wallpapered with the leaves of the trees surrounding them. Mariachis move from family to family offering their serenades. Abuelitas walk the pathways carrying buckets of consome to consume then or later.\nPaco and I wandered El Pica just trying to absorb it all. Even after spending several hours there, we would discover something new and delightful. There was so much to see, we barely took time to eat. We tried a few things, but the sesos quesadilla, fried in the fat of the freshly-made chicharron and stuffed with barbacoa and salsa verde, offered to us by Saul, the son of the original barbacoyero, was spectacular.\nAnother place I can’t wait to return to.\nEl Pica, in La Purificacion, Texcoco, is a massive complex of Mexican food delights. You enter through a tunnel of trees and vines, like a ride at Disneyland. While the Torres family has been making barbacoa here since 1967, around 20 families from the town also sell various foods, such as tlacoyos, desayunos, pan, salsas, chicharron, tamales, and so on, to the thousands of people who come here each weekend.\nFor barbacoa de borrego aficionados, the reputation of the pit-cooked lamb in the state of Mexico is second only to that of Hidalgo. The state has dominated Mexican culture since at least the time of the Aztecs, who took an island in the middle of a lake no other tribe wanted and turned it into an empire. Texcoco was part of that prehispanic empire, one of the three city-states that founded the Triple Alliance. Today it’s more of a blue collar suburb of Mexico City, just 20 miles east of the megopolis, just past the marshy vestiges of the lake that bore its name and the massive new airport being built. The greater municipality remains rather rural. Driving through the curvy mountain roads dotted with small pueblos it’s hard to imagine what people might survive doing that they weren’t doing 100 or 200 years ago.\nAs is often the case, the places that maintain their traditions the strongest are rarely those places that are the most successful on the national and international stage. Financial success requires changing with the times (some might say “selling out”). Maintaining centuries-old traditions is about survival (often of a community and families). Texcoco lacks Mexico City’s financial success, but its barbacoa tradition is strong.\nWhile there are some very good spots for barbacoa in Mexico City, most are imports, families that brought their traditions with them and now serve Chilangos. I know of no market in Mexico City with a “zona de barbacoa”. But when you walk into Texcoco’s central market, the Mercado de San Antonio, just off the central plaza, you are greeted with a sign directing you to just that, a zona de barbacoa.\nI haven’t found a market with a larger concentration of barbacoa vendors yet, not even in the towns of Hidalgo. There are 20 or more stands, all claiming to make barbacoa in a traditional horno. Quality varies, of course, but there are some gems.\nOur favorite of the ones we tried was El Pichon. It had two things going for it: 1) the barbacoyero had the girth of a man who knows food, and 2) when we walked up, he was dipping spring onions in liquid lamb fat to grill them. He actually had a comal dedicated to sopes, quesadillas, and tlacoyos where everything was griddled in lamb fat. That’s my kind of guy. And his barbacoa was quite good. The espadilla was unctuous, lightly seasoned with salt, and redolent with maguey flavor.\nI haven’t had a chance to explore the offerings in the city of Texcoco entirely. We talked with a few barbacoyeros around town and planned to return Sunday, but the unique greatness of El Pica overwhelmed us and by the time we went back through the city, it was too late. There are spots that open only on weekends with multiple hornos doing things right that I would love to try. I will get back.\nOne of the most difficult parts so far of researching traditional barbacoa has been trying to nail down its history, both recent and pre-Conquest. There are plenty of myths out there — the things people say and repeat, often without any evidence other than that someone else said it. But I’ve been surprised at how thin the historical record is and even more surprised how thin the anthropological/archaeological record is. Even in books devoted to the history of food in mesoamerica, earth oven cookery is rarely mentioned.\nHowever, I was happy to find, recently, the work of Professor Stephen L Black, especially his article co-authored with Alston V Thoms entitled, “Hunter-Gatherer Earth Ovens in the Archaeological Record: Fundamental Concepts.” Here is the abstract:\nRemains of earth ovens with rock heating elements of various sizes and configurations are common at hunter-gatherer sites around the world. They span the last 30,000 years in the Old World and some 10,000 years in the New World. Although various foods were baked in these ovens, plants predominate. Earth ovens are ethnographically well documented as family-size and bulk cooking facilities, but related technology and its archaeological signatures remain poorly understood and understudied. These ubiquitous features are often mischaracterized as generic cooking facilities termed hearths. It is proposed that, in fact, most rock “hearths” are heating elements of earth ovens. Reliable identification and interpretation of earth ovens requires documentation of heating elements, pit structure, rock linings, and various remnants thereof. Fundamental technological concepts for investigating their archaeological signatures include thermodynamics, construction designs, and life cycles in systemic context, as informed by ethnographic, archaeological, and experimental data. Earth oven technology explains well the primary purpose of labor-intensive thermal storage for long-term cooking and conserving fuel. Information from the extensive archaeological record of earth ovens on the Edwards Plateau of south-central North America illustrates these points.\nIt’s really a primer on the archaeology of earth ovens. The bibliography will be an opportune rabbit hole that I look forward to getting lost in, but the article itself has a lot to offer, including the basic timeline of earth oven usage and its spread across the globe, how to spot the remains of earth ovens, the science behind the effectiveness of earth ovens, the nutritional advantage of earth ovens, and the basic technology of how earth ovens work.\nTake, for example, this outline of the basic structure of earth ovens, as illustrated by the picture at the top of this page:\nIn its moist-heat baking mode, a typical earth oven consists of seven layers (Figure 1), from bottom to top: (1) prepared surface (i.e., basin or deeper pit); (2) fire (reduced to glowing coals and ashes when oven is sealed); (3) layer of hot rocks; (4) lower layer of green plant material, which we call packing; (5) food being baked; (6) upper packing layer; and (7) earthen cap. The packing layers envelop the food, keeping it clean, supplying critical moisture, and adding flavor. Moist heat allows the food to undergo hydrolysis, during which complex molecules are broken into smaller, more easily digestible molecules (Wandsnider 1997). A moist-heat baking environment is also essential because, as long as adequate moisture from the packing material and food is retained in the oven (water is sometimes added), the temperature of the food remains below the phase change from water/liquid to steam/gas (ca. 100°C) and thus prevents burning/charring the food (Stark 1997).\nThis gives basic insights into the process for making barbacoa and what is necessary to the process and how it has changed or stayed the same throughout its history. I can’t help but notice, for example, that while we think of this earth oven technology as primarily a means to cook meat, the earth ovens were originally used more for vegetables, especially those least digestible. I have always thought of mezcal production as a post-Conquest technology. However, the cooking of the agave roots in large underground pits, while it may today be used almost exclusively as the first step towards turning starch to sugar to alcohol, it is clearly a process that pre-dated the Spanish, but turned the starch to sugar to food.\nIf you want to check out some of the field work Black and his students are doing on earth ovens, you can check out this blog post on earth ovens in Nevada, too.","Eating like Aztecs\nTravel journalist Dea Birkett tests her taste buds with a foray into Mexico City's latest food craze - the sometimes grisly cuisine of the ancient Aztecs.\nWhat was on Montezuma’s menu? That’s the latest concern of Mexico City’s chefs, as the craze in the capital is for food to reflect what the Aztecs ate over 500 years ago.\nChicken burritos are taboo at top tables. (The Aztecs didn’t eat chicken, preferring dog.) In Aztec times, you had to eat whatever you could scavenge – from the ground or on plants. And the most nutritious food the hunters found was bugs and grubs. But where once pre-Columbian cuisine was devoured out of necessity, now it’s uber-cool to declare a penchant for magüey (similar to cactus) worm and have a prediliction for snacking on ants’ eggs.\nIn the courtyard of the smart El Bar at the Four Seasons colonial-style hacienda hotel in the Paseo de la Reforma on the edge of Mexico City’s University area and Chapultepec Park, the local elite crunch onchapulines (grasshoppers) lightly fried with coriander and escamoles (the eggs of a giant black ant). An outdoor brazier keeps them warm in the chilly evenings.\nI scoop up my crispy dish of chapulines. It’s not so much the flavour that’s challenging; it’s that I can’t forget the salty barbequed matchstick I’m crunching is, in fact, an insect’s upper thighbone. Splinters get stuck between the teeth. Was this what they meant by Montezuma’s revenge? The escamoles taste rather like soggy popcorn. I smother them in salsa and wrap them up in a tortilla. My magüey worms have a strong resemblance to maggots. I feel as if I’m in Roald Dahl’s The Twits, tricked into eating a plate of Wormy Spaghetti. It’s a meal that needs to be washed down. I go for a bottle of Pacifico beer. More authentically, I should have joined the cashmere-clad couples and diluted my arthropods with a toloache drink, made from the roots of a giant flowering plant, which is said to make your companion fall in love with you.\nThe Four Seasons is dedicated to this 500-centuries-old cuisine’s revival. It even boasts a pre-Hispanic breakfast. The speciality omelette is made with zucchini flowers and huitlacoche - ‘aged corn’. ‘Aged’ is a polite euphemism to put on the a la carte. The corn is so old it’s gone rotten and black with mould. I found the texture a bit mushy, but when I closed my eyes it could be quite tasty, like marmite on soggy cotton wool. Connoisseurs of the New Aztec cuisine fondly call huitlacoche Mexican truffles. (In similar hope, they dubescamoles Mexican caviar.)\nDon’t imagine turning the clock back to pre-Columbian times means the prices will deflate accordingly. Eating like an Aztec can be expensive. But there are a few places where Aztec food is served up without an achingly cool coating. At the café-style Fonda Don Chon restaurant, a short walk from the historic centre, my fellow diners were root-conscious Mexico City bohemians and local families, rather than business people. A stuffed armadillo stands in the doorway, tatty from being stroked. (Armadillo in mango sauce – an Aztec feast – is one of the restaurant’s seasonal dishes.) There’s a curling poster of the emperor on the wall as if he were the local football hero.\nIf the clientele is less chic, the cuisine is more challenging. Best to leave the Spanish dictionary back in the hotel room and delete your iPhone’s translation app, as you don’t always want to know what you’re eating. I had the ant eggs sauted in butter as a starter. I discovered smothering them in raw onion and chilli sauce gave them a crunch and bite they otherwise lacked - unless they’d hatched.\nI settled back and watched the families on the other tables, with piles of grasshoppers, worms and grubs in shared bowls in front of them. I ordered wild boar for my main course, an Aztec emperor’s favourite. I realized that I was beginning to enjoy all these unfamiliar gustatory textures, as if my tongue had learnt an entirely new culinary language. There isn’t much difference, after all, between grasshoppers’ thighs and frogs’ legs. I toast the Aztec emperor’s exquisite palate with another Pacifico and call over the waiter. ‘Más escamoles, por favor.’"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:7b3ee192-ea32-4219-87ad-7a9e9455ac42>","<urn:uuid:039c1f2e-ab82-49f4-b7ca-43a1c0e35138>"],"error":null}
{"question":"As a new manager, I'm wondering what similarities exist between establishing an LPMO and implementing a PMO in terms of management support and success factors?","answer":"Both LPMO and PMO implementations share critical success factors related to management support. Both require strong executive backing - LPMOs need C-level executive agreement for the long term and an executive champion who communicates the mission and engages stakeholders, while PMOs need an executive steering committee for oversight. Both offices emphasize the importance of having qualified, experienced project managers - LPMOs require talent and ability to achieve results, while PMOs must be staffed with experienced managers who can lead and demand satisfactory performance from teams. Additionally, both stress the importance of clear processes and documentation - LPMOs need documented processes and common metrics, while PMOs require defined reporting requirements and communication plans.","context":["Define the Scope of the Legal Project Management Office (LPMO)\nWhat does the “P” stand for in this abbreviation for a Management Office? Does the LPMO control governance for projects, does it direct strategy under the guidance of the CIO for Legal and Compliance, it may oversee a program of work, or does it operate at an enterprise level? To define, ask:\n1. Do the key people in my organization agree that an LPMO is required?\na. Agree on authority of the LPMO.\nb. Articulate the benefits of introducing the LPMO.\n2. Are our processes mature enough for us to capture the value of an LPMO, and to make the long-term commitment required for success?\nEstablishing a Legal Project Management Office (LPMO)\nEstablishing an LPMO may start with introducing the concept to the CIO, and to key stakeholders in Legal and Compliance. A first step in evaluating the maturity of a legal organization in supporting an LPMO may be to conduct a survey or assessment of the current state. This will establish a baseline within the legal organization that the CIO and top legal executives may use to evaluate where and how best to mature specific legal processes where risk is greatest, or ROI may be realized.\nIn addition to talent and ability to achieve results identified by an LPMO, the organization needs to have the drive and commitment to introduce the LPMO and support its efforts to integrate across Legal, IT, Records, Compliance and other business units. If support is not there, then it is time to stope and focus on supporting and managing programs and projects.\nGenerally, organizational change transitions can either be quick and painful, or slow and relatively painless. Management buy-in is fundamental to success. Both approaches to the speed of change (fast or slow) have pros and cons; we have seen that taking an aggressive approach and ‘pushing hard’ can reap rewards, but requires a strong commitment to succeed.\nAfter completing your project charter, having peer-level reviews and gaining the necessary support and alliances to make it a success, step back and take a good look at the approach and the desired end result. Depending on your organization size ask yourself if the launch strategy is accepatable. Whatever your plan, be sure that the C-level executives agree to it for the long term. We suggest that you are conservative in your proposed results and set realistic goals, timelines, savings, productivity improvements, etc.\nLook for, achieve and communicate early, quick wins as the LPMO starts to make changes, even if they are minor. Any positive changes are building blocks which will fuel project momentum and sustain interest. Do not oversell.\nThere will be lulls in the project – stick with it. As with most projects and project teams, there may be initial enthusiasm but as the newness wears off and the work really begins there will be a drop in morale. As Team Lead, this is the time to step up your game, to drive work efforts and whip up morale and project support. Keep up the positive vibe. Setting up an LPMO is a big challenge; it takes time. The period most likely to require attention is during the lull period between the times during which you’ve completed, documented, and communicated your early wins or short-term goals, and the commencement of achieving the long-term objectives for your LPMO.\nIf the LPMO is up and working, it is time to declare success. You have documented processes, common metrics, basic standards, common tools, a central repository, training packages, and are seeing visible improvements in workflow, project estimates, information usage, greater compliance with prescribe procedures, etc. It is time to reflect – to review project schedules and budget outcomes, project phase as part of governance, lessons shared and learnt, and stakeholders are pleased with results.\nRecognize when to decide if the LPMO is “as good as it gets” and is part and parcel of the operation. If it is then you can declare success, and consider whether to take the next step of advancing the LPMO to the next level.\nImplementing a Successful Legal Project Management Office (LPMO)\nOne of the top differentiators of success is how well an LPMO is embedded within an organization. These four factors are key in determining the integration level:\n1. Collaboration: The LPMO should encourage collaboration between project professionals and functional departments.\n2. Recognition of Expertise: Do the project professionals working with the LPMO improve the level of respect project management achieves within the organization? This should also influence who works in the LPMO.\n3. The Mission is Well Understood: Do those outside the LPMO know its purpose?\n4. Support from Upper Management: Is there an executive champion who will not only communicate the mission, but will work to gain engagement from stakeholders?","ERP project management is key to a successful enterprise software implementation. Inadequate project management has played a major role in failed implementations. A qualified, dedicated project manager is rule number one for a successful ERP implementation.\nThe Project Management Office (PMO)\nUltra recommends the establishment of a project management office (PMO). The PMO should be staffed with experienced ERP project managers. The role requires full-time management. A PMO can include multiple team members as long as responsibilities are clearly defined.\nThe PMO is responsible for managing the resources and the plan. The PMO will report directly to an executive steering committee. The project team – both vendor and client – report to the PMO. The team members are responsible for completing their tasks according to the plan.\nThe following are characteristics of a good project manager:\n- Previous experience managing projects\n- Respect of organization\n- Ability to lead\n- Demands satisfactory performance from the team, both client and vendor\n- Understands the entire range of processes\nUltra can often supplement client resources with the execution of PMO duties.\nUltra’s ERP Project Management Resources\nAll consultants at Ultra are skilled, experienced ERP implementation project managers. We can assist our clients with any of the following capacities:\nOne of our esteemed consultants can fill the position of Project Manager. In this position, Ultra will manage your staff and the ERP software vendors to achieve an on-time, on-budget implementation. The Ultra resource assigned to your project will be on-site 100% of the time. Ultra’s clients in the past have requested this service once they find they do not have a manager within their company they can assign to this role.\nProject Manager Assistant\nAn Ultra consultant or partner can fill the position of Project Manager Assistant. In this role, the Ultra resource will assist the client project manager with the enterprise project management duties. The Ultra Consultant will be on site one to three days per week. Time requirements will vary based on how much time the client manager can devote to the project. Our clients request this service when they have an internal manager, but the individual cannot devote 100%, and therefore needs an experienced assistant.\nBusiness Team Leader\nAn Ultra consultant can fill the position of business team leader. Each ERP project requires several functional team leaders. One example is a financial team leader. This individual leads the client staff through the implementation process. The Ultra Consultant will be on site three to four days per week. Clients request this service when they do not have an individual that has the time or the ability to lead a functional team.\nSteering Committee Advisor\nAn Ultra consultant or partner can fill this position. In this role, Ultra provides advice and counsel to the project steering committee and project team throughout the project. The Ultra Consultant or Partner will be on site two to four days per month. Clients request this service when they have qualified a project manager, but want to maintain Ultra’s experience and knowledge into the implementation.\nUltra’s ERP Project Management Services\nThe following make up Ultra’s project management services:\n- Project planning\n- Organizing roles and responsibilities\n- Reporting requirements\n- ERP data conversion plans\n- Communication and reporting\n- Weekly meeting management\n- Issues log\n- Project budget\n- Project reports\n- Steering committee meetings\n- Project monitoring\n- Issue resolution\n- Vendor performance\n- Core team performance\nUltra’s ERP project management services do not displace the vendor project manager, nor does the vendor project manager displace the need for an Ultra or Client project manager. Ultra’s services are designed to support the client’s responsibilities throughout the implementation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:7912edf9-3b7b-4e9a-a422-90536b593c1a>","<urn:uuid:58c57242-7d41-485f-b5c7-8b53757506fd>"],"error":null}
{"question":"What are the safety advantages of water fed pole cleaning systems, and what maintenance issues can affect water softener performance in these systems?","answer":"Water fed pole cleaning systems improve safety by allowing window cleaning from ground level, eliminating the need for ladders, platforms, cradles, and scaffolding in high-rise buildings, thus reducing injury risks. Regarding water softener maintenance, several issues can affect performance: the safety float mechanism can malfunction and fail to shut off water flow, potentially causing flooding; dirt can collect on rotating valves in the controller making them inoperable if a sediment filter isn't installed before the softener; and proper salt levels must be maintained in the brine tank for effective operation.","context":["The traditional window cleaning method involves the use of water and detergent with an applicator, and a squeegee is used to remove the water, detergent, and dirt.\nAn absorbent cloth is then used to wipe off excess water from the surface of the glass to ensure the window is dry.\nIn contrast, modern window cleaning utilises the water fed pole, a system that uses pure water and a special brush head without detergents.\nAs the quality of running water varies from one area to another, it is essential that only purified water is utilised if water fed pole window washing is to be fully successful.\nParticulate and inorganic content such as dirt, silt, and other fine particles can smear the surface of the glass or leave marks and spots behind.\nThe water purification system utilises a variety of water system products designed to remove water hardness and produce contaminant free water.\nPurified water does a better job of removing dirt from windows and it does not leave any residue behind that can smear the glass surface.\nWater softeners remove hard water scale and scum minerals through an ion exchange process. When the hard water is passed over a resin material, the hardness minerals are exchanged for sodium to make the water soft.\nThe system is equipped with a brine tank which provides the salt for the softening process. Sodium, the by-product of the softening process and the other inorganic contaminant are removed by reverse osmosis, a membrane technology.\nDeionisation is another water treatment system which works similarly to water softeners. They are popular because of their low initial cost, but if large quantities of water are required on a daily basis, it may become expensive to use a deioniser as the sole water purifier.\nThe water fed pole cleaning system is also preferred to the old method of using a rag and a water filled bucket with detergent because window cleaning can be done at ground level without the need to work at a height.\nThis improves safety in window cleaning and reduces the incidence of injuries that can occur with the use of ladders, platforms, cradles, and scaffolding to access windows in high rise buildings.\nThe water fed pole is a telescopic pole fitted with a brush and a means of delivering purified water for cleaning the window. The water fed pole is connected to a water treatment system through a trailing hose.\nThe building should provide reasonable access so that the windows to be cleaned can be viewed without obstruction.\nThe water fed pole is raised vertically and if the windows to be cleaned are low lying, the pole can be operated by the movement of the arms alone. If this is not possible, the pole can be extended horizontally to the desired length on the ground and then raised vertically from this position.\nTwo people may be required to carry out this operation as one person will be required to stabilise and steady the base of the pole while the other person raises the pole vertically.\nCleaning of higher level windows requires some training to learn the techniques for working to maintain the natural balance of the pole so that less effort is expended when the operator goes through the cleaning task.\nIt is desirable for new operators to acquire some experience in the use of shorter poles before moving on to longer poles so that acceptable standards of cleaning can be delivered.","Jul 22, 2019 · This mechanism is designed to shut off the water flow into the brine tank if too much water is being added. The safety float will ensure that your water softener doesn't cause a flood. If the safety float isn't functioning properly, it will fail to shut off the water …Estimated Reading Time: 5 minssp.info Explore further15 Most Common Water Softener Problems And How to ...waterfiltercast.comWater Softener Repair: Do NOT Do Anything Before Reading Thiswww.mrwatergeek.comWhy is my water softener filling up with water?findanyanswer.comTroubleshooting Water Softener Problems | DoItYourself.comwww.doityourself.comWater Softener Regeneration Cycle Diagnosis & Repair FAQsinspectapedia.comsp.info Should I Install a Water Filter Before or After the …Jun 04, 2020 · A water softener is meant to filter out calcium and magnesium minerals, not particulates and dirt. If dirty water is allowed to flow into the water softener, dirt can collect on the rotating valves in the controller and become inoperable. This is another good reason to install a sediment filter before the softener. Water softeners will remove small amounts of iron, but are not specifically designed to treat …sp.info Best 5 Water Softener & Filter Combo Systems In 2021 …This home water filter and softener is also a salt-based unit that uses an ion exchange process to remove hard ions from water. Its system capacity is 26 000 grains, so it’s suitable for a household that sustains 1-2 people. The unit has a unique feature of twin tank water softener, which means you don’t have to replace the salt very often.\nEstimated Reading Time: 9 minsPublished: Apr 05, 2020 Water Softener Problems Water In Salt Tank. Perhaps the primary concern water softener users …Water Softener Problems Salt Bridges. Water softeners are designed to prevent minerals from …Water Softener Makes Water Brown. We’ve all seen disgusting brown water flowing out of the tap …Water Softener Doesn’t Use Salt. If you notice the salt level in the brine tank stays the same over …Brine Tank Water Too Low. We already described the main causes of a too-full brine – or salt – …Water Softener Problems Resin Beads. Salt-based water softeners use resin beads to enhance …Water Softener Not Softening. Perhaps the most annoying of all water softener problems is noticing …Water Softener Causing Low Water Pressure. Low water pressure is daunting for most …Water Softener Problems Salty Taste. Drinking salty water can turn into an unpleasant experience, …Water Softener Motor Failure. Motors can easily fail, and since water softeners run on electricity …See full list on waterfiltercast.comsp.info Water Softener Leaking: (Reasons And Solutions)Nov 10, 2020 · Water Softener Resin Tank Leaking from Bottom. If the resin tank is leaking from the bottom, you most likely have a cracked tank for one reason or another. The easy way to fix this is to turn off the water softener and clean the tank properly. Then you can mark and follow the crack to see how big it is and if it is repairable or not.Estimated Reading Time: 9 minssp.info How does a water softener system work in a water filter?Water softener systems use resin beads that attract and attach sodium ions in the softener tank. Pumps push hard water through the resin beads exchanging hardness ions with sodium ions, softening the water.See all results for this questionsp.info Where can I buy a water softener system?If you don’t like buying online, go to the local store with household appliances or specified store for the water softeners. In general, price depends on the material, water softener system type, type and longevity of filters, brand, additional features, etc.See all results for this question\nIt is located inside your brine tank inside the plastic cylinder that marks the salt level in your tank. This mechanism is designed to shut off the water flow into the brine tank if too much water is being added. The safety float will ensure that your water softener doesn't cause a flood.See all results for this questionsp.info How does a mineral tank for water softening work?The mineral tank is the tall narrow tank where the actual water softening occurs. It is filled with several cubic feet of porous plastic polystyrene resin beads. As water flows through this tank, the negatively charged beads attract and hold the positively charged calcium and magnesium particles in the water.See all results for this questionsp.info Perfect water softener filter For Pure Quality Water ...3.2G RO pressure water filter tank for home water filtration system Product Name 3.2G water pressure tank Model No. 2, Q:What’s your main products 9 A: Our products are PP/PP yarn cartridge,CTO,UDF,valves, housings,T33,RO water filter and other accessories. 3,Q:Can I get your price list 9 A: Sure, In order to send you our available offered price, please choose the products and let us ...sp.info Water Softener Full of Water - How to Drain — …Oct 17, 2019 · 4. Do a Manual Regeneration Cycle: If your water softener allows for a manual regeneration you can activate a manual regeneration cycle by pushing and holding the \"regenerate\" button to empty a water softener full of water.. During regeneration, your water softener automatically sucks all of the water out of the brine tank, so activating this cycle can remove the water for you.\nWater Filter Softener Filtering Machine Water Refilling Station Uv Filter Reverse Osmosis Water Filtration Plant Inversa Softener 500lph 1000lph. US $2250-$2790 / Set. ... 100L itre PE Sand Filter Water Softener Tank Exchange Resin Brine Tank Price. Up to 5 years warranty. US $18.50-$19.00 / Piece. 1 Piece (Min. Order) 4 YRS Wuxi Ewater Water ...sp.info Water Softener Tank Jacket (14x65, Blue) - - Amazon.comCompare with similar items. This item Water Softener Tank Jacket (14x65, Blue) 9\" x 48\" mineral resin tank for filter or softener. AFWFilters 5600SXT 48,000 Grain Water Softener Digital SXT Metered Whole House System. AFWFilters 5600sxt Metered On-demand 48,000 Grain Water Softener with brine tank, bypass and 1\" adapters.Reviews: 43sp.info Best Water Deionizer for a Spotless ... - Water Filter ZoneJul 19, 2021 · The most lightweight water deionizer on this list, the On the Go Spotless Mixed Bed Deionizer weighs only 18 lbs. It measures 22” height by 6.75” diameter. It has an easy-to-carry handle, and the head swivels 360 degrees for a connection from any direction for the inlet hose.sp.info In-tank Resin Water Filter | Barista SuppliesDescription. In-tank resin water filter also known as a water softener — suits most coffee machine tank inlet hoses that take this style of filter (e.g. Isomac, Expobar, etc…). The resin is designed to reduce Total Hardness to minimise the risk of scale build up in the machine.\nJun 18, 2021 · If it’s not easily breaking, find something sharper and heavier to use. Remove the loose salt pellets from the top by scooping them up with a plastic container. Vacuum the bottom of the softener with a wet/dry vacuum to remove the water from the tank. Add fresh salt to …sp.info All About Water Softeners and How They WorkApr 21, 2021 · If your home has a water softener, it is very likely this type. Salt-free: This device uses a mechanical filter to remove calcium, but it doesn't work very well on very hard water. It does not remove magnesium. Reverse osmosis: This device filters water through a semipermeable membrane that removes as much as 98% of water impurities.sp.info What’s Inside your Water Softener: A Closer Look at Resin ...May 17, 2020 · What’s Inside your Water Softener: A Closer Look at Resin. Water softening is a process in which water flows through a bed of resin to exchange the hardness ions, calcium and magnesium, for sodium ions. When the resin has reached its capacity for holding hardness ions, the water softener initiates a regeneration cycle.sp.info Whole House Water Filtration | Water Softener | Water ...Water softeners do not filter contaminants from the water; they simply exchange the calcium and magnesium with sodium, thus producing soft water. Salt-free systems act as a conditioner, taking the calcium and magnesium and turning them into microscopic nano crystals that become suspended in water so they cannot attach to appliances and pipes to create scale.Brand: FilterSmartPrice: $3398\nWhirlpool WHESFC Pro Series – Softener/Whole Home Filter Hybrid, Gray. 4.5 out of 5 stars. 746. $641.35. $641. . 35. Many wells are bored into rock, which means lots of minerals in the water. The Whirlpool water softener is great for well water because its hardness removal rating is a robust 120 grains per gallon.sp.info 7 Best Hard Water Softener for Home - Bathroom … WaterScience CLEO Shower & Tap Filter (SFU-717) If you are losing tons of hair, then the main …D’Cal Hard Water Scale Prevention System. A hard water conditioner that works by preventing the …PuriFit 10-Stage Shower and Tap Filter for Hard Water. This hard water filter for tap & shower …RiverSoft SF-15 PRO Shower and Tap Filter for Hard Water. RiverSoft SF-15 PRO shower filter will …KOHLER RainDuet Filter Shower. This product is the most unique of all the products listed here. …Dr. Recommends 15 Stage Shower & Tap Filter for Hard Water. Dr. Recommends 15-stage shower …Caresmith SIFT+ Shower & Tap Filter. We liked this shower & tap filter because, unlike other …See full list on bestrowaterpurifier.insp.info Lelit In-Tank Water Softener / Particle Filter – Clive CoffeeLelit In-Tank Water Softener / Particle Filter. By: Lelit. $14.95 - $24.95. Regular price. 2 Reviews. Protect your machine from the long term effects of hard water with this replacement water softening and particle filter designed by Lelit for Lelit espresso machines. We recommend testing the water from your machine quarterly to ensure the cartridge is active.sp.info Water softening for espresso machinesThe water tank will require a little modification to accept the water softener cartridge. Bezzera modification kit for softener. For machines that draw water from a silicone inlet hose like the Bezzera BZ07, BZ09, BZ10, BZ13, Crema, Expobar Pulser, Brewtus with Vibration pump, Leva EB61, etc.\nDiamond Crystal 40-lb Water Softener Salt Pellets with Iron Reduction. These 99.6% pure water softener salt pellets help prevent rust stains on laundry, fixtures, sinks and tubs. Virtually 100% water soluble, it also minimizes mushing, bridging and rust buildup in your water softener brine tank—helping to keep your water softener running smoothly.sp.info Is Your Water Softener Clogged? Here's What to Do ...Water softener drain clogged. If the water you receive at home has high iron levels, you will likely have a clogged control valve. To identify this issue, you will need to check your brine tank and see if there is an overflow. If that is the case, clean the control valve. Now your water softener should be working again.sp.info 12 Best Iron Filters for Well Water Reviewed and Rated in 2021 Durawater Air Injection Iron Eater Filter. If your running well water in your faucet and pipes, …Iron Pro 2 Combination of Water Softener and Filter. The cheapest way to remove iron from well …iSpring WGB32BM 3-Stage Whole House Water Filtration System. The iSpring whole house iron …Home Master Whole House Filter. Another item not to miss in the category is the Home Master …DuraWater Fleck 5600 SXT. The well water filtration system for iron and other contaminants in the …Express Water Heavy Metal Whole House Water Filter. The Express Water iron filter is another …American Water Solutions Air injection filter. This American Water Solutions unit is a top of the line …AFWFilters IRON Pro 2 Combination Water Softener Iron Filter. Do you want just one unit that …AFWFilters AIS10-25SXT AFW Air Injection Filter. The best water filter for well water with iron is …Well Water Whole House Filtration System. The whole house iron filter for well water is another …See full list on saveourh2o.orgsp.info Common water softener problems - no soft water | Symptom ...The venturi sends brine from the salt tank to the resin filter tank to regenerate the resin beads. A clogged or damaged venturi can cause hard water because the venturi can't move the brine water into the resin filter tank. Clean or replace the venturi if brine water level in …\nFor over 70 years we have provided the highest level of water softener installation and repair service in all of Southeastern Wisconsin. We work with only the highest quality components when manufacturing and installing residential and commercial water softeners, iron filters, and reverse osmosis drinking water systems. LEARN MORE. From cutting ...sp.info Water Softener: Not using salt - Replacement Water FiltersDec 31, 2014 · Water Softener: Not using salt. Salt pellets can cake up and form a salt bridge inside the salt tank, preventing the brine water from reaching and dissolving salt pellets once salt underneath the bridge is used up. Problems with the water softener timer, valve motor, rotor valve or venture can also keep the water softener from using salt.sp.info product - Henan Demalong Filtration Equipment Co., Ltdfilter cart pall filter cart vacuum filter cart coales cence separation filter cart high precision filter cart box-type filter cart portable filter cart high solid content filter cart high-viscosity filter cart filter cart hydraulic oil cleaning machine filter machine filter unit mobile filtersp.info Water Softeners at Lowes.comPelican water filters 48,000 grains water softener treats hard water by using salt and ion-exchange resins to remove calcium and magnesium hardness from the water. These resins are coated in a sodium solution and when hard water comes into contact with the resin beads, the calcium and magnesium ions migrate out of the solution to the active ...\nIt works with your water softening system as part of the filtering process. Over time the water softener tank fills up with hardness ions and can no longer soften the water. When you add salt, the salty liquid from the brine tank washes away the hardness ions replacing them with a fresh layer of sodium ions.sp.info Related searches for water softener filter water tank Machi…water in water softener tankwater filter and water softenerwater softener tank systemwater softener tanks for salewater softener replacement tankwater softener tank partswater softener filter systemswater softener tank coverSome results are removed in response to a notice of local law requirement. For more information, please see here.\nYou may also leave contact information, we will contact you as soon as possible!\nE-Mail: [email protected]\nAddress: Development Zone, Zhengzhou, China"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:ca1f9d9c-b3e0-4bf6-801d-8ec59e69d5f4>","<urn:uuid:2b09e797-df80-4edc-b1e3-20da1f5fee6d>"],"error":null}
{"question":"What specific academic indicators suggest a child might be experiencing cyberbullying?","answer":"The academic indicators include: getting into trouble at school, skipping school, losing interest in school, and experiencing a drop in grades.","context":["Cyber Bullying – How to spot it, and stop it.\nWith young people today doing almost everything online, it was only a matter of time before bullying also became virtual. Cyber Bullying is one of the most common forms of harassment today, with 52% of young people admitting to having been bullied online, with 1 in 3 having experienced it repeatedly.\nFiguring out how to spot the signs that it is happening to your child is the first step. The NCPC have drawn up the following list of tell-tale behaviour to look out for:\n– Becomes withdrawn or shy\n– Shows signs of depression\n– Is extremely moody or agitated\n– Is anxious or overly stressed out\n– Shows signs of aggressive behaviour\nSocial or Behavioural\n– Suddenly stops using the computer\n– Changes eating or sleeping habits (e.g., nightmares)\n– No longer wants to participate in activities they once enjoyed\n– Hurts self, attempts or threatens suicide\n– Suddenly changes friends\n– Doesn’t want to go to school\n– Gets into trouble at school\n– Skips school\n– Loses interest in school\n– Drops in grades\nIf you believe your child is being cyber bullied, you should approach them gently, and ask them what’s happening as calmly and open-heartedly as possible. Most children struggle to tell their parents if they’re being bullied, so be prepared for them to avoid talking to you about it, but simply by letting them know that you’re there for them – and that you want to help – can encourage them to open up.\nIf it turns out that your child is being bullied, there are lots of things that you, and they, can do.\nHelp them to understand that it’s not their fault. Children who are victimised can begin to believe that they deserve the treatment they receive. Work to show them how special they are, and that no one “deserves” poor treatment.\nNever respond or retaliate. This is aimed mainly at the child, as responding to the harassment is most likely to be the reaction the bully is after. By ignoring the ill-treatment, they can begin to regain some control of the situation. However, as a parent, it can be also be very difficult not to lash out when you discover that someone has been causing your child distress. The best course of action is to calmly bring this child’s behaviour to the attention of their parents. Hopefully they will be able to help you prevent it from happening again in the future.\nFinally, work to restore your child’s self-respect. It is this aspect of their personality that is likely to be most damaged, as bullying chips away at a person’s self esteem. How best to do this depends very much on the child, but you know them better than anyone – and you can help them get there.\nSadly, it’s also worth noting that there are signs to look out for that point to your child engaging in bullying behaviour online:\n– Stops using the computer or turns off the screen when someone comes near\n– Appears nervous or jumpy when using the computer or cell phone\n– Is secretive about what they are doing on the computer\n– Spends excessive amounts of time on the computer\n– Becomes upset or angry when computer or cell phone privileges are limited or taken away\nIf this is the case, there are also steps you can take to try and stop them from cyber bullying other people.\nExplain that this kind of behaviour is unacceptable. Stop any show of aggression you may see, and talk to your child about other ways they can deal with how they’re feeling. Try to get them to empathise with the person they have been unkind to, as most cyber bullies rarely look at the situation from the perspective of their victim.\nLimit, and monitor, their internet usage. Explain that until they prove that they understand how to behave appropriately online, they will have their access restricted to times when you can be there to see what they’re doing. This may mean confiscating their cell phone, tablet or computer – or changing the wifi password so they no longer have access.\nTry to find out what caused this behaviour to occur. Ask you child – did something happen to them to make them act in this way? Is there something going on at home, or school, that is causing them to lash out? Try to discover the root of the problem, and work to deal with it.\nExplain the severity of what they are doing. Most children who bully other children online are unaware that their actions could be considered illegal.\nAsk them to stop, now. Make it clear that you will not tolerate this type of behaviour, and explain the consequences your child will suffer if they continue. You should also encourage them to apologise to the victim.\nThere are lots of organisations out there to help children who are being bullied online, and their parents. For a full list, see Safe Network."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:ae52d167-fb43-47a6-9c2d-281d93d9e620>"],"error":null}
{"question":"What is a separation process and what role does solubility play in it?","answer":"A separation process is a method that converts a mixture of chemical substances into two or more distinct product mixtures. Solubility plays a key role in this process - it refers to how substances break down into smaller parts (molecules, ions, or atoms) and interact with solvents. For example, in chromatography, the separation works because different compounds have varying levels of solubility with the mobile phase (solvent) and different interactions with the stationary phase, causing them to separate as they move at different rates.","context":["Get ready for all the rainbow heart eyes in this easy and gorgeous introduction to chromatography. 🌈😍 Recommended age 3.5+\nThis activity focuses on the concept of solubility and how an appropriate solvent can carry molecules along a stationary phase. Read on for some basic science and how to make some jaw dropping art!\n- Paper Towels\n- Sharpies in several colors\n- Rubbing alcohol/ isopropanol\n- Eye dropper (can also use a spoon in a pinch)\n- Baking sheet or something to protect your table\n- Safety glasses (to protect eyes from the isopropanol)\n- Cut squares from a paper towel (four from a large sheet or two from a select-a-size sheet).\n- Find the center of sheet and using different color Sharpies, heavily color in dots around the center point. Make sure the dots have a lot of ink in them, but don’t puncture the paper towel.\n- Using the eye dropper, drop isopropanol onto the center of the paper towel and watch as the ink radiates out from the center. Keep slowly adding isopropanol to grow your chromatograph.\n- Use your science art to make new crafts or hang it up to display!\nExtra Experiments and Questions\n- Try doing this with water instead of isopropanol. Does it work? What’s happening?\n- Try doing this with washable markers (like Crayola). Which works better- water or isopropanol?\n- Do you think this would work with crayons?\n- How does this relate to stain removal? Why can’t you wash Sharpie out of your clothes with water?\n- Try putting less ink on the dots and see if you can separate some of the colors within the ink. The success of this will vary on the markers/colors you use, but its worth a shot!\nChromatography is used frequently in labs to separate compounds in a mixture. There are many types of chromatography but they are all based on a similar concept: a mobile phase carries your molecules of interest through a stationary phase, and based on the different interactions with the mobile and stationary phase, the different compounds can be separated. This experiment illustrates how a solvent (the isopropanol) can carry soluble molecules (the ink) through a stationary phase (the paper towel). After kids grasp this concept, you can move on to more delicate examples of chromatography like separating the components of fall leaves or a bouquet of flowers. See below for some key definitions to go over.\nChromatography: A way to separate parts of a mixture by moving the mixture and a solvent (mobile phase) along a surface (stationary phase). Because the different parts of the mixture will “prefer” to be on the stationary phase or mobile phase differently, they travel at differing rates, causing the parts to separate.\nSolubility: A demonstration really helps to explain this to kids. They first must know that everything is made up of smaller parts, like molecules, ions, or atoms. Mix sugar or salt into warm water and show them that it seemingly disappears into the water. Explain that the smaller parts are being broken off from the larger crystal and surrounded by water molecules, which keeps them suspended in the liquid. They are still there, we just can’t see them. Then try doing this with chalk or something else that is not soluble in water. They will be able to see the bulk either floating or sinking to the bottom. Explain that these things are insoluble. The sugar or salt have properties that make them want to associate with water, kind of like magnets sticking to each other, while the chalk molecules do not.\nIn this experiment, the ink from Sharpies is soluble in isopropanol but not in water. The isopropanol is called a solvent, and the ink molecules are called the solute.\nMobile Phase: In this experiment, the mobile phase is the isopropanol. It carries the ink molecules along the paper towel through capillary action.\nStationary Phase: In this experiment, the stationary phase is the paper towel. If solute molecules interact strongly with the stationary phase, they will stick to it earlier than molecules with less attraction to it.\nShare the art you create with this project on Instagram and join our community! Tag us and use the hashtag #IBravedTheElements for a chance to be featured!","Hyphenated separation techniques refers to a combination of two (or more) techniques to detect and separate chemicals from solutions most often the other technique is some. Journal of chromatography and separation techniques discusses the latest research innovations and important developments in this field. Such separation techniques include filtration or evaporation separation process, or a separation method, or simply a separation, is a methodology to attain any mass. Separation of mixtures - explained chem academy loading chrmatography, decantation and magnetic separation separation techniques - duration. Lab #2 physical separation techniques introduction when two or more substances, that do not react chemically, are blended together, the result is a mixture in which each component retains its individual identity and.\nSeparation techniques are an important part of chemistry however, their importance is not just limited to chemistry they are also used in our daily lives separation techniques are methods used to separate and/or purify mixtures. A separation process is a method to achieve any phenomenon that converts a mixture of chemical substance into two or more distinct product mixtures, which may be referred to as mixture, at least one of which is enriched in one or more of the mixture's constituents. In this worksheet, students study the techniques used to separate mixtures and solutions into their components. Separation techniques i have designed this lesson, it is a very fun and engaging lesson for year 7 or 8 carousel activity is fun for the students (differentiated) and a some pair assessment as well. 23:15:56 2 why we need separation techniques deal with the separation of mixtures to enhance purity of substances are important because.\n, euroscicon conference separation techniques 2018 will be conducted on theme: launching the innovative ideas and technologies of separation techniques. There are several types of separation techniques, including hand separation, filtration, distillation, chromatography and centrifugation other methods include absorption, crystallization, decantation, evaporation and extraction.\nSeparating funnel magnetic separation precipitation let’s discuss some of the separation techniques using a separating funnel: a separating funnel is used for the. We can separate an analyte and an interferent if there is a significant difference in at least one of their chemical or physical properties this section provides a partial list of separation. Separating mixtures: techniques and applications a mixture is a combination of two or more pure substances that are not chemically combinedmixtures come in.\nJoin taz tally for an in-depth discussion in this video separation techniques, part of photography: exploring composition. The food processing industry uses various techniques to transform food ingredients into different forms for consumers separation techniques. A key stage 3 revision and recap resource for science, covering chemical reactions, compounds and molecules it also covers mixtures and techniques for separating their ingredients, like distillation.\nChemistry - separation techniques separation techniques when we perform an experiment, we end up with a mixture of substances rather than only one we should know how to separate the mixtures a single substance that has no other substances mixing with it is known as pure substance.\nKey 1 separation techniques name - _____ 1) a red-brown solution of bromine in water (=1 01 ) is poured into a separatory funnel trichloroethane (=1 34. Separation techniques when students mixed two common substances there was a new precipitate formed in a solutionfiltration and evaporation filtration is a method for the separation of the parts of a heterogeneous mixture a heterogeneous mixture was formed. Separation methods ways to separate so processes bases on differences in physical properties are used to separate component numerous techniques have. Introduction to separation techniques identify and explain the principles behind a particular separation technique that is used in daily life and in industry identify an appropriate separation technique to separate a mixture based on the physical properties of the components of the mixture. To separate mixtures in a compound by using different techniques."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:4ef5c98c-4dbb-439a-947e-d1d8e964c73b>","<urn:uuid:4190c60c-c63c-4618-ab20-1fdf7ff0fa7d>"],"error":null}
{"question":"Which has higher water permeability - the Nolin soil or fine sand mixed with silt?","answer":"The Nolin soil has higher water permeability. According to the documents, the Nolin soil had a saturated hydraulic conductivity of 1 inch/hour. In contrast, fine sand mixed with silt had a permeability coefficient of only 10, which is considered low, as coefficients less than 100 are classified as low permeability.","context":["Vol. 18, No. 1, 1997\nHOW DO BACTERIA MOVE THROUGH SOIL?\nM.S. COYNE, J.M. HOWELL, and R.E. PHILLIPS\nThe contamination of water supplies by fecal bacteria is an important\nwater quality issue in Kentucky. Contamination may come from point\nsources, such as straight pipes depositing raw sewage into streams, or\nnonpoint sources, such as manure runoff from cropland. A direct cost\nof contaminating water supplies is the expense that homesteads or\nwater companies incur to chlorinate, filter, and otherwise treat water\nto make it potable. Indirect costs are the time lost to illness from\ndrinking inadequately treated water, slower weight gain in livestock\ndrinking contaminated water, and the degradation of aquatic habitats.\nMany Kentuckians obtain their potable water from ground water. We\ndepend on soil to filter and purify wastes and waste water that are\nland-applied before they affect ground water. However, in central\nKentucky, we have observed a direct relationship between grazing\ncattle in pasturefields and fecal bacteria in shallow wells and wet\nweather (breakout) springs. To understand how these water supplies\nbecome contaminated by fecal organisms, we decided to first study\nhow fecal bacteria moved through soil. With this understanding, we\ncould test various control methods. We specifically looked at fecal\ncoliforms in this report, since they are the bacteria used to indicate\npotential fecal contamination in water quality assessment.\nWe extracted intact soil blocks (13 x 13x 13 inches) from two soils: a\nNolin silt loam and a Lowell silt loam. They were placed on a\ncollection chamber that partitioned leachate into 100 individual\nsections (10 x 10). This let us map where water and bacteria exited the\nblocks. Although the blocks were too shallow to realistically assess the\ndepth of fecal bacteria movement in the field, they were large enough\nso that we could examine the pattern of bacteria movement in a\nrepresentative mass of undisturbed soil. Before each experiment, we\ncentered about one half pound of fresh dairy manure (about a tenth of\na pound dry weight) on top of a soil block and shaped it to resemble a\nvoided dung deposit that covered 38% of the soil block surface. Each\ndeposit had approximately 5 billion fecal coliforms. We used a\nlaboratory rainfall simulator to rain on the soil blocks at a rate of about\n1 inch per hour to cause leaching. Leachate was periodically collected\nfrom the 100 individual sections beneath the soil block, and analyzed\nfor total water flow and fecal coliform concentration (the background\ncontamination was not detectable).\nResults and Discussion\nThe saturated hydraulic conductivity of the Lowell soil was 0.3\ninch/hour. Leachate was collected in most sections beneath the block\n(Figure 1) but 20 sections in it accounted for about 60% of the leached\nwater. In contrast, the Nolin soil had a saturated hydraulic\nconductivity 3 times greater (1 inch/hour) and 20 sections accounted\nfor nearly 100% of the total leachate; these sections were widely\ndistributed (Figure 2).\nIn both soil blocks, the 20 most rapidly flowing sections accounted\nfor almost 100% of the fecal coliforms that leached. In the Lowell soil,\nthese sections were immediately below the manure deposit (Figure 1)\nand one section alone accounted for almost 40% ofthe fecal coliforms.\nIn the Nolin soil, fecal bacteria followed the same distribution pattern\nas leachate (Figure 2) and 55% of the coliforms that leached were in\njust one section.\nFar more fecal coliforms were trapped in the soil, usually within the\nfirst few inches, than were transported through it, however. The fecal\ncoliforms collected in leachate only accounted for 0.01% of the total\nfecal coliforms in the Lowell soil and 0.1% in the Nolin soil.\nIn both soils, fecal coliform concentrations steadily increased with\ntime even though the flow through each soil remained constant. After\njust 1.2 inches of rain was applied, fecal coliform concentrations in the\nLowell soil ranged between 2 and 34 fecal coliforms/100 ml. In the\nNolin soil, there were 15 to 680 fecal coliforms/100 ml depending on\nthe section we examined. For comparison, the potable water standard\nin Kentucky is <1 fecal coliform/100 ml and the primary water contact\nstandard (bathing and swimming water) is 200 fecal coliforms/100 ml.\nFecal coliform concentrations in excess of water quality standards\nrapidly leached through soil blocks with freshly applied dung deposits.\nThe bacteria moved through the most rapidly flowing pores of these\nsoils. The Lowell soil had the lowest saturated hydraulic conductivity,\nthe most evenly distributed water flow, and the lowest fecal coliform\nconcentrations in leachate. The Nolin soil, which had greater saturated\nhydraulic conductivity and several high-flowing pores, also had the\nhighest fecal coliform concentrations in leachate and transmitted more\nof the total fecal coliforms from the manure deposit.\nGiven the many fecal coliforms in leachate from a single manure\ndeposit, and the number of manure deposits typically found on grazed\nland, it is obvious why wet weather springs and shallow wells\nunderlying pasture lands frequently exceed water quality standards.\nWhile the greatest bacterial filtration occurs at or near the soil surface,\nbacteria moving past this zone can be transported to whatever depth\nthat pores are continuous. The potential for bacteria movement\ndepends on soil characteristics such as soil structure, and will affect\ngroundwater to different extents depending on rainfall intensity and\nduration, and the depth of soil to ground water. The risk it represents\nfor water supplies is presently unknown. Our current challenge is to\nassess that risk and develop management recommendations to reduce\nFigure 1. The distribution of leachate and fecal coliforms at the bottom\nof a 13 inch block of Lowell silt loam. The height of the bars is\nproportional to the % of total flow or fecal coliforms.\nFigure 2. The distribution of leachate and fecal coliforms at the bottom\nof a 13 inch block of Nolin silt loam. The height of the bars is\nproportional to the % of total flow or fecal coliforms.","Principles of OccurrenceThe fundamental principles of the occurrence and movement of ground water have been given by Meinzer (1923), and a general discussion of the occurrence of ground water with special reference to Kansas has been given by Moore (1940). The reader is referred to these publications for a more detailed discussion of the occurrence of ground water.\nThe rocks that make up the outer crust of the earth generally are not solid but have numerous openings, called voids or interstices. The number, size, and shape of these openings depend upon the character of the rocks; therefore, the occurrence of ground water in any region is determined by the geology of that region.\nThe interstices or voids in rocks range in size from microscopic openings in clay to huge caverns in limestones. The openings generally are connected so that water may move from one void to another, but in some rocks the voids are isolated so that there is little or no movement of the water. Several common types of interstices or voids, and the relation of texture to porosity, are shown in Figure 12.\nFigure 12--Diagram showing several types of rock interstices.\nBelow a certain level in the earth's crust the rocks generally are saturated with water and are said to be in the zone of saturation (Fig. 13). The upper surface of the zone of saturation is called the ground-water table or the water table. The rocks above the water table are in the zone of aeration. This zone generally consists of three parts: the belt of soil water at the top, the intermediate vadose zone, and the capillary fringe at the bottom.\nFigure 13--Diagram showing divisions of subsurface water.\nThe belt of soil water lies just below the land surface and normally contains water held by molecular attraction. During periods of ground-water recharge this zone contains water in excess of the amount that can be held by molecular attraction, and the excess percolates downward to the water table. The thickness of the belt of soil water depends upon the soil, the precipitation, and the vegetation.\nThe intermediate belt of vadose water lies between the soil belt and the capillary fringe. In this zone the interstices in the rocks contain water held by molecular attraction, and at times of ground-water replenishment they contain also water that is moving downward to the water table. The intermediate zone may be absent or may be several hundred feet thick, depending on the local geology, topography, and climate. In Reno County the intermediate zone is absent in some areas and is nowhere more than 60 feet thick.\nThe capillary fringe lies directly below the intermediate belt and over the water table and is formed of water held up from the zone of saturation by capillary force. The water in this zone is not available to wells, which must be deepened to the zone of saturation to obtain water. The capillary fringe may be absent or very thin in coarse-grained materials, but it may be several feet thick in fine-grained materials.\nThe porosity of a rock aggregate is its property of containing interstices. Porosity is expressed as the percentage of the total volume occupied by the interstices.\nThe moisture equivalent of a water-bearing material is expressed as a ratio of (1) the weight of water that the material, after saturation, will retain against a centrifugal force 1,000 times greater than the force of gravity, to (2) the weight of the dry material. To convert this figure to percentage of volume, the moisture equivalent is multiplied by the apparent specific gravity of the dry material.\nThe specific retention of a rock or soil, with respect to water, is the ratio of (1) the volume of water which, after being saturated, it will retain against the pull of gravity to (2) its own volume. It is stated as a percentage and may be expressed by the formula R = 100 (r/v), where R is the specific retention, r is the volume of water retained by the rock or soil against the pull of gravity, and v is the volume of the rock or soil.\nThe specific yield of a water-bearing formation is the ratio of the volume of water a saturated material will yield to gravity in proportion to its own volume (Meinzer, 1923, p. 28). The specific yield is equal to the porosity minus the specific retention. The specific yield of a formation is needed to estimate the quantity of water available to wells and to estimate the quantity of water represented by a rise or decline in the water table during periods of recharge or discharge.\nPhysical and Hydrologic Properties of Water-bearing MaterialsThe quantity of water an aquifer will yield to wells depends upon the physical and hydrologic properties of the materials composing the aquifer. Geologic descriptions of the materials penetrated by test holes and wells are useful in making estimates of the quantity of water an aquifer will yield. A more precise estimate of the amount of water that an aquifer will yield can be obtained from field or laboratory tests of the water-bearing materials.\nSamples of water-bearing materials were collected for analysis in the hydrologic laboratory of the Geological Survey in Lawrence. These studies included mechanical (particle-size) analyses and permeability determinations. Some of the samples were collected in the fall of 1945 during an investigation of the ground water in the Arkansas River valley in the vicinity of Hutchinson (Williams, 1946). In November 1949, samples were collected from six test holes that were drilled in the county, and mechanical analyses and permeability determinations were made on a part of these samples (Table 3).\nA mechanical, or particle-size, analysis of materials consists of separating into groups the grains of different size and determining the percentage by weight of each size group. Results of the analyses are shown in Table 3.\nLaboratory determinations of porosity and specific yield were not made on any of the samples from test holes in Reno County, but such determinations were made on some well cuttings from wells in the Wichita well field, which is a few miles east of Reno County. The porosity ranged from 24.1 to 60.2 percent. The specific yield averaged 26.8 percent (Williams and Lohman, 1949). The water-bearing materials near Hutchinson in Reno County in the Arkansas River valley are very similar to those in the Wichita well field and probably have about the same porosity and specific yield.\nThe permeability of water-bearing material generally is expressed as a coefficient of permeability. The coefficient of permeability is defined as the number of gallons of water a day at a temperature of 60°F that will be conducted through each mile of the water-bearing bed under investigation, measured at right angles to the direction of flow, for each foot of thickness of the bed and for each foot per mile of hydraulic gradient (Meinzer's coefficient, or meinzer).\nThe quantity of water that will percolate through a given cross section of water-bearing material under a known hydraulic gradient is directly proportional to the coefficient of permeability. Thus, to compute the quantity of water that will percolate into or out of a given area the permeability must be determined.\nCoefficients of permeability have a wide range in value. Clay and silt, which are fine grained, may have high porosity, but very low permeability; a coarse-grained sand may have a lower porosity, but a high permeability, owing to the greater ability of the coarse-grained material to transmit water. Coefficients of permeability of less than 100 are considered low, coefficients of 100 to 1,000 are medium, and those more than 1,000 are considered high.\nPermeability tests were made on samples of water-bearing materials collected near Hutchinson in Reno County. The permeability ranged from 10 for fine sand mixed with silt to 4,400 for coarse and medium gravel. The permeability of most samples ranged from 1,000 to 3,000, except in the sand-dune area (Table 8). Permeability of the silts and clays was not tested, but generally the permeability of these materials is very low.\nKansas Geological Survey, Reno County Geohydrology|\nComments to firstname.lastname@example.org\nWeb version Feb. 2001. Original publication date Aug. 1956."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:8a20cc0d-43ee-40d6-8664-5a4558590cc7>","<urn:uuid:2f7a4aa1-790d-466d-94e0-ddb71fd98d64>"],"error":null}
{"question":"How can VOCs impact indoor air quality in bedrooms, and what signal words on product labels indicate different levels of toxicity?","answer":"VOCs are emitted by various bedroom items including upholstery, paints, lacquers, adhesives, and solvents. Common VOCs like formaldehyde are found in particleboard, hardwood plywood paneling, and medium density fiberboard, contributing to indoor air pollution. Regarding product toxicity labels, 'POISON' indicates highly toxic or poisonous substances, 'DANGER' signals extremely flammable, corrosive, or highly toxic products, while 'WARNING' or 'CAUTION' indicate moderately or slightly toxic substances. For pesticides specifically, 'WARNING' means moderately toxic (one teaspoon to one ounce can be lethal), while 'CAUTION' indicates slightly toxic products (requiring over one ounce to be lethal).","context":["Bedrooms offer many opportunities for saving energy and creating a greener, healthier environment. In this section, we first discuss components of bedrooms that have environmental attributes, including electronics equipment, furnishings, and flooring. Then, we discuss changes to everyday activities that you can make in the bedroom, including easy changes to cleaning, purchases, and habits.\nBedroom Green Practices\nChoose energy efficient ENERGY STAR® labeled products when purchasing electronics for your bedroom, including your:\nENERGY STAR® products meet strict efficiency guidelines set by the U.S. Environmental Protection Agency and the U.S. Department of Energy.\nFor more information on ENERGY STAR labeled electronics, go to:\nWhen choosing furnishings for your bedroom, from the furniture to decorations, consider environmentally preferable products. Environmentally preferable products are those that are considered to be “greener” overall than their conventional counterparts.\nRecycled content is one factor in determining an environmentally preferable product. Consider furniture made from recycled content or reclaimed materials, and pieces made from sustainably harvested materials such as certified hardwood. Other environmental attributes to consider include: reduced energy use during the products production and use; conservation of resources; and reduced impacts to air, water, and land. Furthermore, environmentally preferable products contain fewer or no toxics or hazardous constituents, including those that can result in indoor air quality issues such as Volatile Organic Compounds (VOCs).\nVOCs are emitted by a wide array of products including upholstery, paints and lacquers, adhesives, and solvents. Some VOCs contribute to outdoor smog, as well as indoor air pollution. Formaldehyde is an example of a common VOC that is used in the manufacture of furniture and materials, including most types of particleboard (used as shelving, in cabinetry, and furniture); hardwood plywood paneling (decorative wall covering, cabinets, and furniture); and medium density fiberboard (used for drawer fronts, cabinets, and furniture tops). Other types of VOCs include benzene, xylene, toluene, to name just a few. Look for products that contain low or no–VOC finishes and adhesives.\nEPA launched the Environmentally Preferable Purchasing (EPP) program to help the federal government “buy green,” and to stimulate demand for green products and services. Environmentally preferable purchasing means including environmental considerations into buying decisions, along with traditional factors such as performance and price. EPA’s EPP Program has summarized information about popular environmentally preferable products and services, including environmental attributes to look for, procurement guidance, tools, case studies, and other useful resources. Although geared towards the federal government (and its own institutional, mainly non-residential, buildings), this program can also help consumers identify EPP products and places to buy them.\nFor a database of environmental information on EPP products, including furnishings, go to:\nPLEASE NOTE: Linking to this database does not constitute “endorsement” of these products or companies on the part of the EPA.\nOctober 2006 EPA530-F-06-013\nBe smart about using household products!\nBe smart when you use, store, and dispose of household products.\nDid you know that the products you use for cleaning, carpentry, auto repair and gardening can contain ingredients that can harm you, your family and your environment?\nThese products may harm your children and pets, cause physical injury to sanitation workers if put out for regular trash pick-up, and contaminate septic tanks or pollute the ground water if poured down drains and toilets.\nHere’s what you can do to safeguard your family, your home and your community…\nREAD the Label Before you buy, always check the product labels. Look for labeling that reads “DANGER,” “WARNING,” “CAUTION,” “TOXIC,” “CORROSIVE,” “FLAMMABLE,” or “POISON.” These warnings tell you if the product is harmful to you, your family and the environment, and how to use, store and dispose of it safely.\nIf you have area rugs at home, rug cleaning must surely be one of the things that are constantly on your mind. Keeping the rug nice and clean can be a tough job and something that a lot of homeowners are having trouble with.\nCleaning your rugs on your own can take up a lot of your time and energy, things which many rug owners really do not have the luxury of. Thankfully, there is a way that you can get your rugs cleaned without having to do everything yourself and that is by hiring a professional cleaner to do the job.\nHiring a professional rug cleaning company to do the tedious job of cleaning your rug is surely one of the best things that you can do. If you have not tried hiring a professional to clean your rugs, here are some of the biggest advantages of doing so.\n1. Ease and Comfort – If you have tried regularly cleaning and maintaining your rugs at home, you can surely attest that it is a difficult and boring job. It takes up a lot of your time and you still don’t have any guarantees that you can do the job properly. Hiring a professional rug cleaner though can really be a big help as you won’t need to do anything other than set up an appointment with them. You don’t need to labour for hours on end just to get your rugs clean. This ease and comfort can really be a big difference maker as not all homeowners can really afford to spend their precious energy on a task that can be done better by professionals.\n2. High Level of Cleaning – Cleaning the rugs on your own can be okay in trying to get your rugs cleaned but you are not assured that you will get the best results. This is the difference that hiring a professional cleaner can make as pros can offer your rugs the highest level of cleaning possible. They have the training, tools, and experience to carry out a very thorough cleaning and keep your rugs in great condition. This is why getting a rug cleaner to do the job can really pay dividends as your rug will be properly cleaned and maintained so that it will last for a long time.\nThe next time you decide to clean your rugs, try hiring a professional cleaner to do the job. It will be a decision which you will thank yourself for making.","Hazardous Products in Your Home\nBy Dr. Wilma Hammett\nof households products sold each year contain toxic ingredients.\nDrain cleaners, oven cleaners, pesticides, and furniture polish\nare a few examples. Use them improperly and, these products can\nendanger our health and the air quality in our homes. Dispose of\nthem improperly, and they can pollute our drinking water. What can\nyou do to reduce the amount of hazardous products in your home?\nUse multi-purpose cleaners.\nto what advertisers would have you believe, you do not need a different\nproduct to clean each surface in your home. There are many products\nthat will clean a variety of different surfaces. Multi-purpose cleaners\ncan reduce the number of cleaners you use, reduce the number of\nhazardous products in your home, and save you money, too! Read and\nfollow label directions carefully.\nBuy the least harmful product available.\nyou know the difference between a product that is labeled poison\nand one that is labeled danger? These signal words are regulated\nby the federal government. Any product which contains hazardous\nsubstances must be labeled as such. The front label must include\na warning and a description of the hazard.\n. . highly toxic or poisonous\nDANGER. . . extremely flammable, corrosive, or highly toxic\nWARNING or CAUTION. . .moderately or slightly toxic\nproduct must include a statement telling you how to avoid the hazard\nand how to use the product safely.\nreduce the danger in your home, buy cleaners labeled \"warning\"\nor \"caution\" and pesticides with \"caution\" on\nthe label. These products are less harmful.\nreading labels, do not be fooled by the words \"non-toxic.\"\nThis is an advertising term. It is not defined by the federal government,\nso it can be used on toxic products.\nis very important that you know as much as possible about products\nbefore you use them so that you can protect yourself and your family.\nIf a product label doesn't give a list of ingredients or adequate\ninstructions for its safe use, choose another product.\nRegulations concerning pesticides are different. On pesticides,\nthe word \"warning\" means that the product is moderately\ntoxic. This means that one teaspoon to one ounce can kill an average\nadult. The word \"caution\" means that the product is slightly\ntoxic. It would take over one ounce to kill an average person.\nmore information on levels of hazards, see Hazardous Household Products.\nUse preventative measures.\nan old saying that an ounce of prevention is worth a pound of cure.\nThat's true for cleaning and polishing. If soil is allowed to accumulate,\nremoving it becomes more difficult. Wiping up spills when they occur\ncan prevent stains and eliminate the need for tough specialty cleaners,\nwhich often are more toxic and more harmful to surfaces.\nexample, harsh abrasives gradually scratch and chlorine bleach can\ndull the shiny finishes of sinks, bathtubs, and appliances with\nporcelain enamel surfaces. Once the surface becomes dull and rough,\nit will get dirty faster and stain deeper. Then it becomes almost\nimpossible to keep clean.\naway grease and spills in the oven after each use, or put a liner\non the oven bottom to catch spills and you can reduce the need for\nan oven cleaner.\nsink and shower drains with a screen to keep out food scraps and\nhair. Don't pour grease down the drain. Collect it in an empty can\nand put it in the trash. These steps will reduce your need for a\nwindows to air out the house occasionally to avoid the use of chemical\nUse alternative or less toxic, homemade products.\nway to get a safer product is to make it yourself. For \"recipes\"\nfor homemade cleaning products, see Cleaning\nRecipes for a Healthy Home. Homemade products have definite\nadvantages, but they also have disadvantages. Be sure to consider\ndo you gain by making your own products?\n-- Many of the ingredients are inexpensive, so you may save money\nStorage space -- Many of the ingredients are common household products\nyou already have, and you can mix up small batches so that you don't\nhave to store many products.\nControl of the chemicals in your home -- Since you mix them, you\ndecide the amount and type of chemicals in the cleaning products\nSafety -- Homemade products generally have less toxic chemicals\nin them. They are safer for you, the air in your home stays cleaner,\nand disposal of these products is less dangerous.\nWhat are the problems related to homemade products?\nmay take longer to clean effectively. Since they may not be as strong,\nthey may take more time to work. You may need to let the product\n\"sit\" on the surface for longer than usual, or you may\nhave to go over a surface several times.\nMore elbow grease may be required. You may have to scrub harder.\nThey may not clean as well. If you have used harsh cleaners on surfaces\nover a long period of time, the surface may be scratched. Then you\nwill need strong chemicals to truly clean deep stains.\nIf you decide to make your own cleaners, you must use and store\nthem safely. While the ingredients in homemade cleaners are safer,\nthey are not all non-toxic. Keep these guidelines in mind:\nBe careful what chemicals you mix. Some chemicals, such as chlorine\nbleach and ammonia, produce a very toxic gas if they are mixed together.\n2. Do not mix more than a month's supply at a time. The chemicals\nmay lose their effectiveness.\n3. Mix solutions in a well-ventilated area.\n4. Store all cleaning solutions out of reach of children.\n5. Store solutions in unused, store-bought containers. Use permanent\nstorage containers which are kept in a permanent location. Never\nput them in old food containers. They may interact with residue\nfrom the original contents, or they may be mistaken for food or\n6. Label containers carefully. This is especially important if other\npeople in your home clean or have access to the cleaners.\nManaging Hazardous Cleaners\nIt may be impossible for you to eliminate hazardous cleaning products\nin your home, but you can still reduce the risks to your family\nand your environment by making wise buying decisions and by handling\nRead labels. Make sure the product will do what you want and that\nyou will feel safe using it. If ingredients aren't listed, choose\n2. Select the least hazardous product. Let the signal words -- poison,\ndanger, warning, or caution -- be your guide.\n3. Buy only as much as you need and use it up in a short period\n4. Avoid aerosol products. Choose the pump spray or another alternative.\nAerosols have toxic propellants which can explode. Also, the fine\nmist is more easily inhaled.\n5. Choose water-based paint, glue, shoe polish, and similar products\nrather than solvent-based products.\nRead the directions and follow them. Using more of a product doesn't\nmean you'll get better results.\n2. Wear protective equipment, such as rubber gloves, as recommended\nby the manufacturer.\n3. Handle products carefully to avoid spills. Keep the container\nclosed tightly when it's not being used to avoid fumes and spills.\n4. Use products in well-ventilated areas. When working indoors,\nopen windows and use a fan to circulate the air toward the outside.\nTake plenty of fresh-air breaks.\n5. Do not eat, drink, or smoke while using hazardous products. Traces\nof chemicals can be carried from hand to mouth.\n6. Do not mix products unless directions say that you can do so\nsafely. Even different brands of the same product may contain incompatible\n7. If you're pregnant, avoid exposure to toxic chemicals. Many toxic\nproducts have not been tested for their effect on an unborn infant.\n8. Don't wear soft contact lenses when working with solvents and\npesticides. They can absorb and hold the chemicals next to your\n9. Carefully and tightly seal products when you finish. Escaping\nfumes can be harmful and you will avoid spills.\n10. Use common sense.\nFollow label directions.\n2. Leave the product in its original container with the original\n3. Never store hazardous products in food or beverage containers.\n4. Make sure lids and caps are tightly sealed.\n5. Store hazardous products on high shelves or in locked cabinets\nout of the reach of children and animals.\n6. Store incompatible products separately. Keep flammable products\naway from corrosive products.\n7. Use volatile products -- those that warn of vapors and fumes\n-- in a well-ventilated area.\n8. Keep containers dry to prevent rusting.\n9. Store rags used with flammable products, such as furniture stripper\nand paint remover, in a sealed, marked container.\n10. Keep flammable products away from heat, sparks, or sources of\n11. Know where flammable materials are located in your home, and\nknow how to extinguish them. Keep a fire extinguisher or materials\nto control fires where you can get to them.\n12. Never store hazardous products in the same area as food.\nare several ways you can reduce the amount of hazardous products\nin your home and protect your air and water.\nand use multi-purpose cleaners on a variety of surfaces, rather\nthan buying a different product for each surface.\nBuy the least harmful product available. Read the label and buy\nproducts marked \"Warning\" or \"Caution\" rather\nthan \"Danger\" or \"Poison.\"\nWipe up spills when they happen to avoid the need for strong chemicals\nto remove stains later.\nMake your own cleaning products.\nReducing the number of hazardous products you buy reduces the sources\nof household hazardous waste later. Wise buying decisions and good\nmanagement practices can reduce the hazards in the home, in the\nair we breathe and in the water we drink.\nmore information on proper disposal, see Disposal\nof Hazardous Household Wastes. For more information on reducing\nwaste in general, see Packaging\nChoices that Reduce Waste.\nHomrich, Alicia M. Keep It Clean, Keep It Safe: Less Toxic Cleaning\nProducts for Your Home. Leader Training Materials, Orlando, Fla.\nHammer, Marie. Hazardous Household Substances: A Primer for Extension\nProfessionals. Gainsville, Fla.: Florida State University.\nHammer, Marie. Common Household Products/More Than One Use. Gainsville,\nFla.: Florida State University.\nThe World Is Full of Toxic Waste. Your Home Shouldn't Be. San Diego,\nCalif.: Environment Health Coalition.\nGuide to Hazardous Products Around the Home. Springfield, Mo.: Household\nHazardous Waste Project.\nConsumer Tips. Household Hazardous Waste Fact Sheet #1, Springfield,\nMo.: Household Hazardous Waste Project.\nHow to Reduce, Recycle and Safely Dispose of Household Hazardous\nWastes. Seattle, Wash.\nHousehold Waste: Issues and Opportunities. Washington, D.C.: Concern\nKnow Your Chemicals: Alternatives and Precautions. Vermont Agency\nof Environmental Conservation.\nHazardous Household Products: A Guide to Safer Use and Disposal.\nResearch Triangle Park, N.C.\nby Dr. Wilma Hammett, Housing Specialist, North Carolina Cooperative\nExtension Service, North Carolina State University, Raleigh, N.C.\nPublished by North Carolina Cooperative Extension Service\nNorth Carolina State University, Raleigh, N.C."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:a053b9d7-0240-4d09-82ae-babc06bf1483>","<urn:uuid:4e4a9529-1630-4179-8292-a2d9456bdb42>"],"error":null}
{"question":"What is the difference in funding approaches between the Christchurch and Los Angeles seismic monitoring initiatives?","answer":"Christchurch City Council has committed $765,000 over three years for their seismic sensor network, encouraging private sector participation to expand coverage. In contrast, the Los Angeles system is part of a larger US federal initiative that has received gradual increases in Congressional funding, from an initial $5 million in 2014 to $22.9 million last year, with total system completion estimated at $39.4 million and annual operating costs of $28.6 million.","context":["Christchurch City Council is partnering with a local company to install a dense urban network of seismic sensors that will help build the city’s earthquake resilience.\nAs part of its Smart Cities Programme it has committed to spending $765,000 over three years on installing and monitoring ground accelerometer sensors at 150 locations throughout Christchurch.\nEQRNet and its sensors, developed by Canterbury Seismic Instruments (CSI), measure ground shaking and will provide critical, real-time information for building owners, lease-holders, engineers, and Civil Defence in the event of another quake.\nThey will also provide an ongoing source of valuable data about ground movement across Christchurch which engineers, research organisations and authorities can use to understand the resilience of the city’s built environment.\n“We are breaking new ground in establishing such a dense network of seismic monitoring in an urban environment. It is going to change the Christchurch narrative from one of seismic risk to one of seismic resilience,’’ says Christchurch City Council Smart Cities Programme Manager Teresa McCallum.\n“What we learned from the Canterbury earthquakes is that there is huge variability in the ground shaking experienced. Proximity to the source of the quake, the direction of the ground shaking and local ground conditions causes significant variability in ground shaking across the city.\n“Our existing GeoNet stations provide a baseline of data in an earthquake, but we need more. The EQRNet initiative with CSI has created an affordable model that encourages uptake by public agencies and commercial users alike so that we can deploy a dense network of sensors across Christchurch and confidently produce a citywide shake map.\nMs McCallum says the sensors are being placed across the city at locations where there is critical infrastructure.\n“When we get another quake, big or small, they will provide invaluable information so we can tailor our response accordingly. Businesses and decision-makers will be able to get information immediately about the most affected areas and will be able to prioritise resources accordingly,’’ Ms McCallum says.\n“Our aim is to have all our sensors installed by the end of the year but we’re hoping the private sector will jump on board and add their own sensors to the network. The more sensors we have in the network, the stronger our resilience as a city.’’\nCSI General Manager Len Damiano says, “CSI is very pleased to be collaborating with Smart Cities to deliver a more seismically resilient future for our city. The support and encouragement from the Smart Cities team has been essential in enabling us to deliver our shared vision”.\nWhilst seismic instrumentation in itself is not a new idea, CSI’s lessons from the Canterbury, Seddon and Kaikoura earthquakes have been instrumental in the development of EQRNet.\n“We need immediate, useful data on the impact across the city; defensible decisions from reliable information. This benefits everyone from private individuals to business to critical infrastructure”, explains Dr Hamish Avery, CSI’s Chief Technical Officer.\n“EQRNet is a new and unique way of delivering this; a global first making Christchurch the most seismically prepared city in the world”.","Millions of people in Southern California can now receive alerts on their smartphone seconds before shaking from an earthquake is about to strike, marking the first time large numbers of Americans will have access to such early warning technology.\nLaunched this week, ShakeAlertLA will alert users when a magnitude 5.0 or greater earthquake has been detected in Los Angeles County.\n\"Earthquakes are a matter of when — not if,\" Los Angeles Mayor Eric Garcetti tweeted Wednesday, announcing the release of the mobile application, which was developed with AT&T as part of a pilot program with the US Geological Survey.\nIt's the first earthquake early warning application to be widely released to the public in the US after decades of funding challenges that put the country behind other nations, like Japan and Mexico, which have built their own early warning systems.\nGarcetti encouraged other cities to follow Los Angeles' lead.\n\"We weren't looking to be the first, we weren't looking to be the only — we want all of California, all of this country, wherever there’s earthquakes to use this,\" he said during a press conference Thursday. \"Somebody had to develop it. We stepped forward and we did.\"\nA separate mobile app, QuakeAlert, has been available to about 1,000 beta testers and will be released to 100,000 users across California later this year, Josh Bashioum, founder of Early Warning Labs, the Santa Monica–based company that developed the app, told BuzzFeed News.\nBoth applications rely on a USGS network of 800 ground sensors located up and down the West Coast that can detect shaking and trigger the alerts.\nThe early warnings are generated with data from the sensors by tracking an earthquake's \"P waves,\" which travel faster through the ground than the more violent \"S waves\" that cause the shaking. The amount of lead time people receive depends on how far they are from a quake's epicenter.\n\"There's a physical limitation if you're right on top,\" Robert de Groot, a staff scientist at USGS Earthquake Science Center, told BuzzFeed News. \"You will still get a shake alert, but it will likely arrive while the shaking is going on or after.\"\nGovernment agencies and private companies hope to continue expanding access to these cellphone-based alerts that could potentially save scores of lives in temblor-prone cities in California, Oregon, and Washington.\n\"The opportunity to have maybe a second or two or maybe a few seconds to do something about it is of great value,\" de Groot said.\nThe USGS is also developing an alert system using the same delivery method as Amber Alerts, which are transmitted through a federal system and don't require the download of a special app.\nOne challenge with the federal system, however, is that it may not be fast enough to send earthquake warnings in advance. De Groot said the Los Angeles application will help the agency test how delivery times change as the number of users increases.\n\"The idea is to get out shake alerts to everyone residing on the West Coast,\" de Groot said. \"That's going to take some time, but this is going to allow us to learn how to do this with very large amounts of people.\"\nEarly warning systems have been credited with saving lives in other countries. When a magnitude 7.2 earthquake struck Mexico's southern region last February, the app SkyNet sent notifications to residents' cell phones, in some cases, as much as 75 seconds before the shaking started.\nIn Japan, earthquake warnings appear on all TV broadcasts when a temblor is detected.\nA television broadcast shows Japan's early warning system in action during the magnitude 9.2 Tōhoku earthquake in 2011.\nThe estimated cost of completing the early warning system is $39.4 million, while the annual cost to operate and maintain the system is estimated $28.6 million, according to a recent USGS report.\nCongress initially allocated just $5 million for the system in 2014 and has since gradually increased funding. Last year, lawmakers allocated $22.9 million for the warning system.\nRep. Adam Schiff, a California Democrat who has long pushed for federal funding for the system, congratulated Los Angeles on the launch of the app Thursday, calling it \"another major milestone.\"\n\"By downloading this app on their phones, Angelenos will be able to receive a warning before the shaking starts, saving lives when the 'big one' hits,\" Schiff said in a statement. \"I’ve been proud to work with my congressional colleagues to secure over $45 million to date in federal funding to build the system, but the effort could not be successful without great local partners.\"\nDeGroot said at the end of 2018 about 50% of the network's planned 1,675 sensors had been installed so far.\n\"Completing the system is really dependent on getting the money that we need to do it,\" he said. \"If things proceed the way they should we're probably looking into the early 2020s for a complete system.\"\nGarcetti on Thursday called on lawmakers in Washington, DC, to continue funding the project, saying the technology is critical to saving lives.\n\"This works and we need it,\" Garcetti said."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b9fd8d96-d586-4202-8e54-80ef95654c3a>","<urn:uuid:f7dec5f4-61f5-4a74-b8e6-caa2d3889215>"],"error":null}
{"question":"As a female player interested in baseball, how do Troy Silva's hitting instructions compare to Andre Ethier's views on women in baseball?","answer":"Troy Silva focuses on universal hitting principles that apply to all players, emphasizing individual strengths and proper mechanical development regardless of gender. Andre Ethier specifically addresses women in baseball, expressing strong support for female players by stating that girls are just as capable as their male counterparts and suggesting that women might one day break barriers to become major league pitchers or position players.","context":["June 20, 2016\nWYC 083 – Youth Baseball – Troy Silva talks 9 Innings of Hitting\nTroy Silva is the author of the #1 bestselling book on Amazon & iTunes for coaching baseball – 9 Innings of Hitting. Troy spends his days coaching baseball at Rijo Athletics in the Seattle area. Troy has spent his life playing and coaching baseball, including being drafted by the Cleveland Indians in 1997 and playing 6 years of professional baseball. Troy is married and has 3 beautiful children.\nBook: 9 Innings of Hitting\nListen on iTunes: iTunes link\nListen on Stitcher: Stitcher link\nListen on Google Play Music: Google Play link\n‘Hitters have to be smart enough to have a good approach and dumb enough to get in there and hit.’\nPretty swings vs. developing consistent swings in players\n- Swing instruction is different than hitting instruction – you need someone to develop the concept of hitting. It starts with the swing, but that’s just the first step in the process of hitting.\nEach individual has different needs\n- Early on while Troy was coaching – he realized that kids weren’t necessarily getting better, they were just looking better. He was trying to teach them all one way to swing, instead of working with each individual’s strengths.\n- There is a huge difference between expecting to hit the ball vs. trying to hit the ball\n- Mental approach actually has a physical effect on batspeed\nMental approach – how do you not overthink when at the plate?\n- ‘Hitters have to be smart enough to have a good approach and dumb enough to get in there and hit.’\n- ‘Free hitters up to be athletic and just get in there and compete’\n- Quiet the mind – it’s ok to be thinking about 1 thing while at bat – just don’t start complicating it by thinking of 5 or 10 different things you need to do\n- A great start is just to have kids watch their favorite big-leaguer and copy what they are seeing.\n- Start with mechanics. Then it comes down to the individual and what they need to become a productive hitter. Great progression chart in the book 9 Innings of Hitting\nShould we teach the ‘oppo first’ approach when setting up our batting practices?\n- You should learn power first and how to swing hard BEFORE learning how to hit to the opposite field\nHIT – Honor, Integrity, Truth\n- As a Christian, Troy uses baseball to be a light in a dark world.\n- As a coach your job is to be a mentor and positive influence in these young peoples’ lives.\nThe One that got away\n- Being drafted as a pitcher was really tough for Troy\nBest borrowed/stolen idea\n- Ed Sheft – Mental toughness – You have to know you are better than your competitor\nFavorite coaching/leadership quote/book\n- Personal experience is the best teacher\nRijo Athletics and 9 Innings of Hitting\n- Online video hitting/fielding portal with over 300 videos: rijobaseball.tv\n- Book by Jose Rijo: Creating Winning Relationships through Sports","The Los Angeles Dodgers Foundation (LADF) continued their virtual Dodgers RBI season last night with its seventh coaches training featuring Dodger Alumni, Andre Ethier. Ethier joins the series not only as a two-time All-Star, Silver Slugger, and Gold Glove winner, but also as a parent and youth sports coach. Over 250 Dodgers RBI volunteer coaches and parents registered to hear his approach to youth development during an interview by LADF and Dodgers Training Academy staff.\nDuring the virtual coaches’ session, Ethier shared the power of parents as coaches. His mother was a volunteer coach and his very first T-ball coach at the age of four. His grandfather, also a professional baseball player in the 1940’s and 1950’s, was a major influence and reason for the baseball love running through his family.\nNow as a coach himself, his passion for youth sports and keeping kids engaged in baseball is evident. “I started coaching 8 to 10-year-olds and I really just wanted to create and instill a love for the game,” said Ethier. “It’s been a blast. I have a real joy doing it.”\nOn creating boundaries as both a parent and coach, Ethier admits it can be tough. “It’s not a serious game yet at this age. There is no benefit of winning other than the confidence of the kids,” said Ethier. “I try to be as diplomatic and as easy going as I can. I want the kids to have the attention and know what’s going on on the field and get the accolades as to why the team is successful.”\nEthier has had important conversations with his own kids about the current pandemic. “We’ll get back to doing this when we’re permitted and it’s the right opportunity and right parameters. There are other things we need to do civically and our human responsibility is to put everyone in a safe position. We are not going to go out there and force baseball games.”\nHe encouraged families to do what they can when they are home. He shared that when he was younger his family had to get inventive with his equipment. “We had a pecan tree and we would put the pecans in a bucket so we could still play all year round,” said Ethier. “Be creative and keep whatever aspect of baseball going that you can.”\nOn encouraging females in baseball, Ethier said, “One day we might see a woman in baseball being a pitcher or a position player. There are a lot of girls out there who may break the barrier and be a major leaguer. There are just as many girls in the youth game that are just as capable of playing baseball or softball as well as their male counterparts.”\nAlthough new to the Dodgers RBI coaches training series, Ethier is no stranger to the work of the Los Angeles Dodgers Foundation. During his Dodger days, Ethier was an avid supporter of Dodgers Dreamfields, LA Reads, Breakfast in the Classroom, and several of LADF’s grantees through his #DreDayz initiative which hosted youth for VIP experiences at Dodger Stadium. In 2014, LADF also partnered with Ethier and his wife, Maggie, to revitalize the learning center for the homeless men and women of Skid Row at Union Rescue Mission.\nTo ensure that coaches have the appropriate resources, LADF’s training series teaches baseball and softball fundamentals, trauma-informed approaches, concussion protocols, and positive youth development strategies from Dodger coaches, Positive Coaching Alliance, UCLA Steve Tisch BrainSPORT Program, Up2Us Sports and the Dodgers Training Academy. Virtual training sessions feature interactive polls, demonstration videos from presenters, Q&A sessions with panelists, and LADF community resources and programming."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:7ce7a6d3-a614-4f4e-8ace-90a9e4fe4c39>","<urn:uuid:964dfac1-26a9-474e-a775-1c02f38de5e1>"],"error":null}
{"question":"What are the key differences between El Niño and La Niña in terms of their effects on Pacific Ocean temperatures and wind patterns?","answer":"El Niño and La Niña represent opposite conditions in the Pacific Ocean. During El Niño, there is a warming of the Pacific Ocean between South America and the Date Line, with temperatures typically 1-2 degrees C above average, and up to 3-4 degrees C in strong events. Trade winds weaken or reverse during El Niño. In contrast, La Niña is characterized by cooler than usual ocean temperatures in the same region, accompanied by stronger than usual trade winds from the east. These stronger winds during La Niña push ocean water away from the equator in each hemisphere, causing cold water from below to rise and replace the warm surface water.","context":["Presentation on theme: \"El Nino is a shift in ocean temperatures and atmospheric conditions in the tropical Pacific that disrupts weather around the world. It is a poorly understood.\"— Presentation transcript:\nEl Nino is a shift in ocean temperatures and atmospheric conditions in the tropical Pacific that disrupts weather around the world. It is a poorly understood recurrent climatic phenomenon that primarily affects the Pacific coast of South America, but has dramatic impacts on weather patterns all over the world El Nino is a shift in ocean temperatures and atmospheric conditions in the tropical Pacific that disrupts weather around the world. It is a poorly understood recurrent climatic phenomenon that primarily affects the Pacific coast of South America, but has dramatic impacts on weather patterns all over the world\nWhat is El Niño - Southern Oscillation (ENSO)? El Niño is an excellent example of the interaction between the ocean and the atmosphere and there combined effect on climate. El Niño is a disruption of the ocean-atmosphere system in the tropical Pacific having important consequences for weather around the globe. This condition results in redistribution of rains with flooding and droughts. Along the equator, the western Pacific has some of the world's warmest ocean water, while in the eastern Pacific, cool water wells up, carrying nutrients that support large fish populations. Every two to seven years, strong westward-blowing trade winds subside, and warm water slowly moves back eastward across the Pacific, like water shifting in a giant bathtub. The warm water and shifting winds interrupt the upwelling of cool, nutrient-rich water. Fish die; climatic changes affect many parts of the world. Peruvians named this phenomenon El Niño, for the Christ child, because it first appears around Christmas.\nWhy El Niño occurs? Normal conditions El Niño conditions\nEl Nino is thought to occur due to changes in the normal patterns of trade wind circulation. Normally, these winds move westward, carrying warm surface water to Indonesia and Australia and allowing cooler water to upwell along the South American coast. For reasons not yet fully understood, these trade winds can sometimes be reduced, or even reversed. This moves warmer waters toward the coast of South America and raises water temperatures. Warmer water causes heat and moisture to rise from the ocean off Ecuador and Peru, resulting in more frequent storms and torrential rainfall over these normally arid countries.\nNormal Conditions El Niño Conditions Illustration of the changes in upper-level jet stream patterns over the Pacific, which typically occur during El Niño episodes. Normal flow toward the northwestern United States is deflected northward toward the Canadian and Alaskan coasts. At the same time, a secondary southern storm track appears that increases the probability of storm systems affecting the California coast, and increases the potential for storms to form in the Gulf of Mexico (Wallace and Vogel, 1994).\nThe animation follows the evolution of sea level (the undulating surface) and sea-surface temperature (color) for a Warm event followed by a Cold event as simulated by the numerical model of Battisti(1988).\nThe relatively small motions in sea level shown here ( centimeters) are indicative of much larger motions in the opposite direction in the depth of the thermocline below the surface. (Thermocline animation) Vertical displacements of the thermocline are particularly important along the equator in the eastern half of the Pacific basin, where they control the availability of cold water that can reach the surface through the process known as upwelling. For example, when the sea level is low, the thermocline tends to be shallow, indicating that unusually cold water is near the surface. Upwelling motions can then bring this cold water to the surface, resulting in cold conditions. When the sea level is high the situation is reversed, with cold water lying too deep to reach the surface. The result is a warm event.\nDuring El Niño conditions, high pressure conditions develop over the western Pacific Ocean and South East Asia and the high pressure area over the Easter Islands diminishes. As a result of these changes, the pressure difference across the Pacific Ocean becomes smaller. This results in a decrease in the force of the trade winds.\nAnticipating, Alerting, Protecting\nDry Warm Wet Hurricane El Niño around the world, Climatic effects of El Niño (Crédits D.Ducros)\nEl Nino Scenario\nGalapagos Islands (located in equatorial Pacific Ocean)\nThe El Niño phenomenon centers on the Galapagos Islands in the eastern tropical Pacific Ocean. It is a common phenomenon in this area of the world occurring unpredictably every three to ten years.\nMajor ocean currents on Earth circulate clockwise in the northern hemisphere and counterclockwise in the southern hemisphere.\nGalapagos is in the path of a cold current (the Peru or Humboldt Current), coming from Antarctica, up the coast of South America. It is also in an area that has significant seasonal upwelling. Upwelling brings deep, nutrient rich, bottom water to the surface, which enhances marine productivity. The tops of the Galapagos Islands show through the dense cloud cover during an El Niño.\nThe Galapagos Islands are usually very dry and desert-like (left, Halbach photo) but during El Niños the rainfall may increase so much that the islands can not absorb the water and it runs off the islands (right).","El Nino is a warming of the Pacific Ocean between South America and the Date Line, centered directly on the Equator, and typically extending several degrees of latitude to either side of the equator. Coastal waters near Peru also warm. The warming is expressed as a departure from long-term average ocean temperatures, which are generally cool in the region, due to upwelling. El Nino is thus associated with a slackening, or even cessation, of the cold upwelling conditions which typically prevail in that area.\nDuring a typical El Nino, the ocean warms a degree or two (C) above its climatological average. A strong El Nino can warm by 3-4 degrees C over large areas, and even 5 degrees C in smaller regions.\nTypically, El Nino is first noticed along the South American coast\naround Christmas (hence the origin from Peruvian fishermen of its\nSpanish name (\"the child\")). Farther west, in the open ocean, El Nino\ntypically begins to appear about a month later (near the Galapagos) to\nabout 4 months later (near the Date Line) than near the coast.\nWhat is La Niña?\nLa Nina is essentially the opposite of El Nino. La Nina exists when cooler than usual ocean temperatures occur on the equator between South America and the Date Line. The name La Nina (\"the girl child\") was coined to deliberately represent the opposite of El Nino (\"the boy child\"). The terms El Viejo and anti-El Nino are also sometimes used. La Nina occurs almost as often as El Nino, but has been lesser known. La Nina and El Nino are but two faces of the same larger phenomenon.\nStronger than usual trade winds accompany La Nina. These winds, from the east, push the ocean water away from the equator in each hemisphere. (This is caused by the rotation of the earth.) Cold water from below rises to replace the warm surface water which has moved away from the equator.\nThe cool water acts as an impediment to the formation of clouds\nand tropical thunderstorms in the overlying air. This suppression of\nrain-producing clouds leads to dry conditions on the equator in the\nPacific Ocean from the Date Line east to South America.\nWhat is ENSO?\n\"ENSO\" stands for \"El Nino / Southern Oscillation\". The acronym arose in the climate research community, and reflects an attention bias toward the warm phase of the entire cycle.\nEl Nino is just one phase of an irregular fluctuation between warmer than usual and colder than usual ocean temperatures in the region mentioned above. The cold phase has recently come to be known as \"La Nina\". The El Nino/La Nina \"cycle\" does not occur with strict periodicity. Historically, an El Nino usually recurs every 3-7 years, as does its (cold) La Nina counterpart.\nThe overlying atmosphere is tightly coupled to ocean temperatures and circulation patterns. An atmospheric pressure signal is seen throughout the tropics that is strongly linked to El Nino and La Nina. When barometric pressure is higher than usual in the western Pacific near Indonesia, pressure is lower than usual in the subtropical Pacific near Easter Island and Tahiti. This global-scale pressure signal, identified 70 years ago, is known as the \"Southern Oscillation\". Surface barometric pressure at Darwin, Australia and the island of Tahiti are strongly anti-correlated: when one is higher than usual, the other is lower than usual. The difference, Tahiti minus Darwin, suitably normalized, is referred to as the Southern Oscillation Index (SOI), and is frequently used as a convenient, simple and reasonably accurate tool to monitor the status of El Nino/La Nina.\nBecause more attention has been devoted to El Nino, and noting the association between the Southern Oscillation in the atmosphere and El Nino (and La Nina) in the ocean, the research community began to refer to the combination as ENSO (El Nino/Southern Oscillation). This moniker is somewhat asymmetric: El Nino pertains to just one of the two phases of the Southern Oscillation.\nIt would be perhaps more accurate to refer to El Nino as the warm\nphase of the Southern Oscillation, and to La Nina as the cold phase of\nthe Southern Oscillation. The term \"ENSO\" is, however, firmly\nHow does El Nino affect climate in the West?\nThe most unambiguous signal is seen in the winter half-year,\ntypically from October through March or April. A weaker signal may be\nseen in some parts of the West in summer or early autumn. Most of\nthis discussion concentrates on winter.\nEastern Pacific autumn tropical storms, west of Mexico, appear to\nbe less frequent in El Nino years, a tendency which is well\nestablished in the Atlantic. However, those tropical storms that do\noccur have a greater than usual tendency to recurve into Mexico or the\nsouthwest U.S. Higher than usual water temperatures off the Mexican\ncoast in El Nino years can help maintain their strength, or cause them\nto be stronger than they would otherwise be. Hurricanes need water\ntemperatures of about 27 C (81 F) or more to sustain themselves.\nDuring El Nino years, the storm track more frequently splits into\ntwo preferred branches. The Aleutian Low, in the Gulf of Alaska, is\ndeeper than usual, and one branch of the jet stream departing from its\nvicinity heads toward the Queen Charlotte Islands and the southern\ncoast of the main part of Alaska, bringing mildly increased storminess\nto those areas. A second branch of the jet stream is seen across the\nsouthern tier of the U.S. and northern Mexico, and with higher speeds\nthan usual. Storms approaching the Pacific Northwest, and southwest\nCanada, are often split and weakened as they approach the shore, as\ntheir energy is shunted toward the north and/or the south.\nWith El Nino, the period October through March tends to be wetter than usual in a swath extending from southern California eastward across Arizona, southern Nevada and Utah, New Mexico, and into Texas. There are more rainy days, and there is more rain per rainy day. El Nino winters can be two to three times wetter than La Nina winters in this region.\nIn the Pacific Northwest, El Nino tends to bring drier winters. The area affected in this manner includes Washington, Oregon, and the more mountainous portions of Idaho, western Montana and northwest Wyoming. This area of influence extends well up into Canada, and coincides very well with the Columbia River Basin on both sides of the U.S./Canada border.\nIn between these regions, including central and northern California, northern Nevada, southern Oregon, northern Utah, southern Wyoming, and much of Colorado, the effects of El Nino are ambiguous. No strong association in either direction (toward wet or dry) can be discerned.\nFarther north, from the Queen Charlotte Islands around toward Kodiak Island, the relationship again switches sign, and southern Alaska tends to have wetter winters with El Nino.\nIn Hawaii, El Nino tends to bring dry winters. Drought is more likely during El Nino years, during the October-March period. This association is well known in the Hawaiian Islands.\nIn general, in all these regions, La Nina climate effects are\napproximately, but not exactly, opposite to El Nino climate effects.\nWinter temperatures with El Nino conditions tend to be warmer than\nusual from Washington and northern Oregon across the northern tier to\nMontana, and also along the West Coast. Conversely, cooler than\nnormal temperatures are seen in the far southeastern portion of the\nWest, especially in southeastern New Mexico.\nWith El Nino conditions, precipitation and temperature effects\ncombine to accentuate the effect on snowfall. In the Southwest, there\nis a slight tendency toward cooler winters, and a strong tendency\ntoward wet winters, which makes higher elevation snowpack deeper. In\nthe Pacific Northwest, El Nino winters are warmer and drier than\nusual, so that at a given elevation 1) less precipitation occurs, and\n2) the freezing level is higher, so the type of precipitation is more\nlikely to be rain, and 3) the accumulation season is shorter. All\nthree conspire to produce a smaller snowpack accumulation by the end\nof winter in the Pacific Northwest.\nMost streamflow in the West is produced by melting snow in the\nspring (in general, about 75 percent). At lower elevations rain can\nbe an important component of streamflow. El Nino effects on\nstreamflow are magnified versions of the effects on the climate\nelements. Because of hydrologic lags (snow does not usually begin to\nmelt until spring), the effects of El Nino are typically delayed, in\nsome cases for several months. Thus, the effects of El Nino on\nstreamflow may not be manifest until late spring or summer. Usually,\nEl Nino results in less streamflow to the Pacific Northwest and\ngreater streamflow in the Southwest.\nIn southern California, Arizona, southern Nevada, New Mexico and\nsouthern Utah, almost of the major flood episodes on mainstem rivers\nhave occurred during El Nino winters. None have occurred during La\nNina winters. The likelihood of flooding is considerably increased,\nbut flooding is not a guaranteed outcome.\nHow does La Nina affect climate in the West?\nTo a first approximation, it appears that the consequences of La Nina are nearly the opposite of El Nino in much of the U.S., including the West. In the previous discussion of El Nino effects, simply substituting the opposite words yields an approximately correct description.\nExceptions to La Nina / El Nino Opposition\n1. The La Nina climate signal in the West seems more reliable than the El Nino signal. This is especially true in the Southwest. El Nino generally brings wet weather there in winter, but there are a number of exceptions. La Nina brings dry winters to the Southwest, and there are no exceptions, during the past 65 years. That is, La Nina brings much more consistent consequences in the Southwest.\nIn the Pacific Northwest, this appears to be not as true. La Nina generally brings cold, snowy, wet, active winters to the northern Cascades and the northern Rockies. There are a few exceptions to this picture among La Nina years. There appear, however, to be more such exceptions in El Nino years, to the dry, mild winter pattern these regions typically experience with El Nino.\n2. For both El Nino and La Nina, the north/south dividing line between the opposing effects in the Southwest and the Pacific Northwest extends as a zone from about San Francisco to Cheyenne, Wyoming. The effects of both phases of ENSO are equally nebulous in this region. The effects of larger El Ninos extend farther north along the West Coast, and the effects of La Nina can extend south along the West Coast from the Pacific Northwest. Thus, northern California can be the recipient of moisture from the northern end of El Nino's effects, and the southern end of La Nina's effects.\n3. In the central Sierra there are no large-scale winter floods associated with El Nino. All but one of the biggest floods have occurred in La Nina winters. However, not all La Nina winters have large floods, and many have small or average winter flood peaks. Thus, La Nina opens the door to, but does not guarantee, large scale rain-on-snow conditions associated with the biggest Sierra floods--more so than El Nino. The deep tap to abundant tropical moisture (the so-called \"pineapple connection\") associated with major Sierra floods has a higher likelihood of occurrence in La Nina years than in El Nino years, but in both cases is not common. These very large floods can be generated in just a few days, and the weather pattern during that time may poorly represent the overall character of the winter. Both 1996-97 and 1985-86 illustrate this point very well: without the short period of intense rains, these two years with the largest floods would likely have entered the record books as drought winters.\n4. The overall atmospheric flow patterns differ substantially between La Nina and El Nino winters. El Nino winters tend to feature strong and persistent flow from the Pacific into North America, blocking the movement of cold Canadian air toward the south. La Nina winters have much more north-south movement of air masses, and alternations of temperature, particularly in the northern half of the West.\n5. Although El Nino has received considerably more attention than\nLa Nina, evidence suggests that the types of weather associated with La\nNina winters have more deleterious effects to the national economy than\ndo those of El Nino.\nAre there combined effects that are more likely?\nIn some areas, combined events can interact to accentuate the\neffects of El Nino. For example, in the southern West, during the\nwinter months vigorous storms are more likely, precipitation amounts\nare heavier, the frequency of precipitation is higher, and events are\nmore likely to persist. Thus, ground saturation is likely to be\ngreater, and landslides are more frequent. With more frequent and\nvigorous storms, coupled with wet soils, trees are more likely to\ntopple. In the northern West, by contrast, such circumstances are\nproportionately less likely.\nAre big El Ninos different from average El Ninos?\nThis is one of the strongest El Ninos on record. There is some\nevidence that the effects of large El Ninos (which constitute a small\nsample) may be different from those of the typical El Nino. In\nparticular, heavier precipitation may occur farther north in Nevada\nand California, and especially along the coast. For example, the\nrecord El Nino of 1982-83 brought heavy precipitation as far north as\nOregon and Washington, the only major exception in the last 70 years\nto the typical dry winter response expected in the Pacific Northwest.\nHow much confidence can we place in the predictions?\nAll forecasts are fallible(!). In the Southwest, not all El Ninos\nbring wet winters, and in the Northwest, not all El Ninos bring dry\nwinters. The most appropriate way to use these forecasts is to \"hedge\none's bets\" in the indicated direction. In the Southwest, typically\nthe likelihood of a wet winter is increased from 50 percent (a coin\ntoss) to about 65-75 percent likely. In the Pacific Northwest and\nnorthern Rockies, for a typical El Nino the likelihood of a dry winter\nis increased to 65-75 percent. For the 1997-1998 El Nino, these\nlikelihoods are even higher than usual in the most affected areas, as\nmuch as 80 percent for a dry or wet winter, in the two respective\nIs there a tie between El Nino and global warming?\nThis is a matter of considerable speculation in the climate\nresearch community. It is plausible that a warmer earth would produce\nmore and stronger El Ninos. There is some evidence that the earth has\nwarmed over the past two decades, and there is no doubt that El Nino\nhas been much more frequent in that time. If the evidence of a\nwarming earth is taken at face value (not universally accepted), there\nstill remains a wide spectrum of opinions on whether we are seeing a\nmanifestation of human modification of global climate, or whether the\nnatural climate system would be exhibiting this behavior anyway.\nAre there long term trends associated with El Nino?\nYes. A number of climate indicators were noted to have changed in 1976, especially around the Pacific Basin. Prior to 1976, El Nino and La Nina occurred with about equal frequency, each at intervals of about 3-7 years. Since 1976, there have been 9 El Ninos (using a 6-month average of the Southern Oscillation Index of -0.50 as the criteria), or one every 2.2 years. There has been just one moderate La Nina in that interval (1988-89) and a rather weak La Nina (not even counted by some) in 1996-97. Longer perspectives, since 1860, indicate that the 1976-1997 period is quite unlike any other in the record. This is a source of considerable puzzlement at this time."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"temporal_focus","category_name":"historical"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:15604a2c-2a3a-4597-805c-b289fcc06576>","<urn:uuid:b74e8e00-1f13-4bcc-bd39-fb936206af4a>"],"error":null}
{"question":"How do load distribution requirements compare between the front and back of a trailer?","answer":"When loading a trailer, the heaviest objects should be placed in the center to ensure stability. If there's too much weight in the back, it will cause the rear to be lower, affecting weight distribution and potentially causing sway while also lifting the hitch up and reducing front tire traction. Conversely, if the front is overloaded, it will be lower and pull the hitch down, causing the back to lift and have less rear tire traction. The load must be distributed as low as possible to lower the center of gravity, and all items must be securely fixed with strong straps to prevent movement during transport.","context":["The trailer itself must also be an approved model and have license plates and stop lights, position, and turn signals, with the precise installation for connection to the tractor vehicle. When buying it, you should advise on the suitability of the trailer and your vehicle, because its power must be sufficient to drive without problems with the excess load involved in the trailer. In these cases, having a diesel car helps.\nOnce the trailer hooked, you should always perform an inspection of the hitch system, which must be perfectly secured and with the lighting connections well attached.\nAlthough it is not a motor vehicle, the tires must bear the correct pressures, not showing signs of wear or deformities.\nBefore starting up, check the correct functioning of the lights. Clean them frequently as they tend to get quite dirty with road splashes. Repeat this operation with the registration because we must not forget that they can sanction us if it is not very visible.\nInspect the structure\nLook closely at the trailer’s chassis, checking its elements and verifying that it has not suffered any damage after using it for the last time, or that it lacks any element. Please pay special attention to the third wheel, which helps you to handle the uncoupled trailer: check that it is well folded and secured before starting the ride.\nSort the load\nTrailer weight distribution is critical to ensure ride stability. Place the heaviest objects in the center of the trailer and ensure that the load distributed as low as possible to lower the center of gravity.\nFix the load\nLuggage must not move inside the trailer. Use strong and quality straps to fix the load and make sure it is well covered. There are trailers with a rigid cover, but if yours does not have it, use a well-tensioned tarp and secured to the trailer box.\nTo drive with a small trailer behind, something usual is necessary. With the passing of the km, one tends to forget that the dimensions and dynamics of the vehicle he drives have varied substantially, which means that when driving and maneuvering. Also, the side wind greatly influences the driving with a trailer, before which you must always reduce the speed.\nWhen carrying a road trailer, the total vehicle is longer and heavier. The braking distance will be much greater (directly proportional to the weight of the load), so you must increase the safety distance from the vehicle that precedes you.\nDo not forget that greater length and weight when overtaking, because you will not have the same acceleration and you will need more distance to join the right lane after overtaking, key in the case of secondary roads. Nor can you take the curves at the same speed as when you don’t carry the trailer. It is advisable to anticipate the movements before starting the turns.\nIf this is the first time you use a light trailer, it is best to train the maneuvers before making a trip. The first thing is to try to carry out stationary maneuvers to get used to the reverse trailer that turns the opposite of the vehicle, so you must move the steering wheel in the opposite direction to where you want to go.\nThen check on the road how the trailer with load affects the responses and reactions of the car. Little by little, one is getting used to having to use shorter developments and the new dynamic.","Making sure your trailer is attached to your towing vehicle is the most important thing you need to do before setting off. The forgotten toothbrush is nothing compared to a forgotten trailer 🙂\nBut you need to make sure that your trailer is hitched correctly before setting off, which leads us to our question, should a trailer be level when towing?\nIt’s really important that your trailer is level when towing. If it’s not level then it can sway, will have poor braking, and can even leave the ground, this makes for a dangerous driving experience. Make sure that the trailer is level so all tires are on the ground and you can maneuver and brake effectively.\nMaking sure your trailer is level means that the tires and brakes will wear evenly, thus saving you from expensive repairs. I will delve into why it is so important for your trailer to be level whilst towing.\nWhy Should A Trailer Be Level When Towing?\nThere are lots of reasons why your trailer should be level when towing, and most of them are safety related. An un-level trailer can run the risk of controlling the towing vehicle instead of the other way round. Let’s delve into the reasons:\nThis is a biggie, you want your trailer to have proper ground clearance so it doesn’t get damaged going over bumps in the road. Every trailer is designed to be able to withstand small bumps that occur in the road, but if it’s uneven then this could cause the trailer to leave the ground, or hit and scrape the tarmac.\nYou do not want your trailer to leave the ground. If it is unlevel and the front of the trailer is too high air will be pushed underneath and cause it to lift. The back of the trailer can also hit the ground and cause substantial damage by being dragged along.\nApart from you not being able to control the trailer, if this happens it will also use more fuel, as you’re flying a really heavy kite behind your truck. Not to mention the repair costs because the undercarriage of your trailer has been hitting the road every 5 seconds.\nTravel trailers are designed to be as aerodynamic as possible to reduce fuel consumption when towed. If the trailer is not level then the aerodynamics will be off.\nAn unlevel trailer will mean that the air is hitting the trailer at the wrong angle and not seamlessly flowing over it, but forcefully pushing it back. Therefore making it harder to pull and costing a lot more in fuel.\nTrailers are designed to be really stable, but only when they’re level. The weight distribution plays an important role in keeping the trailer stable.\nIf the trailer is not level, or has too much weight at front or back then this can lead to the trailer swaying.\nWhen the trailer starts to sway from side to side, also called fishtailing, it can move the vehicle with it, causing the vehicle to lose control.\nIf there is too much weight in back of the trailer, this will cause the back to be lower than the front. This affects the weight distribution and can cause sway. The trailer could be lifting the hitch up damaging it and will have less traction on its front tires.\nIf the front of the trailer is overloaded, then it will be lower at the front and will pull the hitch down. This will cause the back to lift and have less traction in the rear tires.\nTrailer sway can also happen if the trailer is level, but it is far more likely in an unlevel trailer.\nAn unloaded trailer will have an uneven weight distribution and will put more weight on one axle than the other. This will cause excessive wear to the tires on that axel as they have more pressure on them.\nThe increased pressure will also cause excessive heat which can increase the chances of a tire blowout. Nobody want a tire blow out, but make sure that you always have a spare and the equipment to fit it just incase.\nIf the front of the trailer is overloaded this will cause the front axle to be overloaded too, you may be able to tell just from looking at the trailer. The tires on the overloaded axel will look flatter than the other axel, if this occurs distribute the weight more evenly.\nThe braking performance of an unlevel trailer will be severely affected. Each tire will have a different pressure pressing down on the road.\nIf the trailer is ‘nose down’ (overloaded at the front) then the rear tires will not have has much pressure on the road. This can cause the rear tires to skid and this might not be enough braking power to effectively stop the trailer.\nHow different travel trailers are affected\nYou can categorize travel trailers into single axle or multi-axle as they react differently to not being level.\nSingle Axle Trailers\nAs they only have one axle (two wheels) single axle trailers are less sensitive to being level. You’re less likely to see problems like uneven tire wear and blow out with single axle trailers as there is only one set of wheels.\nAlso, most single axle trailers don’t have auxiliary brakes so the braking performance won’t be a factor.\nWith that being said, you should still try to get your single axle trailer as level as possible because ground clearance, stability and sway can still come into play.\nMulti Axle Trailers\nMulti axle trailers are more sensitive to being unlevel, because there are multiple axles that can all be affected differently.\nThey are susceptible to uneven tire wear and blow out, if one axle has more pressure on it. The braking performance will also be affected if one set of wheels has more pressure than the other.\nThe aerodynamics, ground clearance and stability will all pay a price if the trailer is uneven, even more so than a single axle trailer.\nYou will need to know your axle ratings, to ensure that you don’t overload your trailer and make it uneven. This should be detailed in the manufacturer’s manual, you can also work this out based on the diameter of the axle.\nAs a rule of thumb these are the axle capacities based on diameter:\n|Axle Diameter||Axle Capacity|\n|1 to 1 ½ inches||1,000 lbs|\n|1 ¾ to 2 inches||2,000 lbs|\n|2 to 2 ⅜ inches||3,500 lbs|\n|3 inches||6,000 to 7,200 lbs|\n|3 to 3 ½ inches||8,000 lbs|\n|4 inches||9,000 lbs|\n|5 inches||10,000 lbs|\nMake sure that the cargo inside the trailer is evenly stored, so the weight is evenly distributed throughout the trailer.\nHow to Level Your Trailer For Towing\nLeveling your trailer isn’t too big of a job, as always make sure that you have the right tools before you start. You will need:\n- Tape Measure\n- Flat Land\nTo level your trailer on your towing vehicle follow these simple steps.\n- Firstly you need to make sure that the ground that you are on is flat and level, if your driveway is not level then you can go to a parking lot to do this.\n- Load the trailer as if you do when you are going on a trip, this will ensure that it’s level when full as you would use it.\n- When you are on level ground take a look at the trailer. Is it level? Use the level to check, you may have to re-organise the cargo inside to make sure that the weight is distributed evenly.\n- If this doesn’t work then you find the right hitch rise or drop, use the tape measure to find the right height.\n- Measure your receiver height on your vehicle, this is the distance between the ground and the top of the receiver tube.\n- Level the trailer on a jack hand and then measure the coupler height, from the ground to the end of the coupler.\n- Subtract the coupler height from the receiver height. A negative number indicates the inches of drop needed, a positive value indicates the inches of raise needed.\n- If you don’t have a self leveling vehicle then you’ll also need to account for the amount of squat. This is how much the vehicle drops when the trailer is attached, measure the height of the receiver with the trailer attached and subtract that from the height without the trailer attached to give you the squat.\n- You may need to invest in an adjustable hitch like this Uriah Products Aluma-Tow 6″ Drop Ball Mount for 2″ Receiver. An adjustable hitch will allow you to find the right height for your trailer to make it level.\n- Once you’ve found the right height for the hitch double check the trailer is level with the level, you can adjust the hitch accordingly.\nWhat if You Can’t Level Your Trailer?\nSometimes you can try everything but the trailer is still slightly off. If you’ve done your measurements and tried the different ball mounts but the trailer is still not level, the safest option is for the trailer to be slightly nose down.\nTowing with the nose slightly down is the next safest thing to the trailer being level, as it helps to prevent sway.\nIf you have to tow your trailer in this position make sure that the hitch tongue weight can handle it as it will be adding more pressure to the tongue.\nFrequently Asked Questions\nHow should trailers sit when towing?\nYour trailer should be level when towing. A level trailer is the safest and most fuel-effective way to tow.\nSwitching ball mounts and using an adjustable hitch like the Uriah Products Aluma-Tow 6″ Drop Ball Mount for 2″ Receiver will make it as easy as possible to get your trailer level.\nHowever, if you’ve tried everything but still can’t get your trailer level, the next safest option is to have the trailer with the nose slightly down. If you have to do this make sure that the tongue capacity of the hitch and vehicle can withstand the extra weight.\nSo to sum it up, towing your trailer level is best, but if that’s not possible go with the nose slightly down but take extra care driving.\nHow high should my trailer hitch be off the ground?\nMake sure your trailer is loaded for these measurements. The minimum height your loaded trailer should be is 11 inches from the bottom of your loaded trailer hitch ball mount to the ground.\n11 inches will give you enough clearance to avoid scraping the bottom of your trailer if you come across bumps and dips in the road.\nHow do I keep my trailer from swaying?\nThere are a few measures that you can take to prevent your trailer from swaying:\n– Use the manufacturer’s recommended gear for towing\n– Ensure the trailer is level\n– Drive steady – no sudden turns or maneuvers\n– Drive slowly, especially if it’s windy as this can cause trailer sway\n– Check tire pressures before setting off\n– Don’t overload the trailer\n– Don’t overload the towing vehicle\nThere are also various hitch designs to reduce sway like this CURT 17063 Round Bar Weight Distribution Hitch with Integrated Lubrication and Sway Control. If you are worried about the trailer swaying then I’d highly recommend a sway control hitch.\nIt’s really important to make sure that your trailer is level before towing. If that’s not possible then have the nose slightly down to prevent sway.\nIf you are worried about the trailer swaying then I’d recommend getting a hitch with sway control like this one CURT 17063 Round Bar Weight Distribution Hitch with Integrated Lubrication and Sway Control."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:eef691f2-6732-4d58-9c2e-7f133cbc2252>","<urn:uuid:84de61f4-5f43-4874-985e-3b028933e19c>"],"error":null}
{"question":"How to properly install rebar in concrete construction?","answer":"To properly install rebar in concrete construction, follow these key steps: 1) Choose the right size and type of rebar based on your project's strength requirements. 2) Follow local building codes and regulations regarding size, spacing, and placement. 3) Use proper handling techniques and equipment like rebar benders and cutters. 4) Ensure the rebar is protected against corrosion by using zinc-coated or corrosion-resistant material. 5) Use high-quality concrete mix and proper curing techniques for a strong, long-lasting structure.","context":["Rebar (short for “reinforcing bar”) is a type of steel bar that is used to reinforce concrete structures. It is typically used in construction to provide additional strength and support to concrete foundations, walls, and other structural elements.\nRebar is made of high-strength steel and is typically ribbed or deformed in some way to improve its bonding with the concrete. It is available in various sizes and shapes to suit different applications, and it is commonly used in construction projects around the world.\nIt is not possible to use chain link fence as a substitute for rebar in concrete construction. Chain link fence is made of wire that is woven together to form a mesh pattern, and it is not designed to be used as a structural reinforcement material. It is much weaker and less durable than rebar, and it would not be able to provide the same level of support and strength to a concrete structure.\nThe Differences Between Chain Link Fencing and Rebar\nWhen you get right down to it, there’s a world of difference between chain link fencing and rebar:\n- Composition: Chain link fencing is made of wire that is woven together to form a mesh pattern, while rebar is made of high-strength steel that is typically ribbed or deformed in some way to improve its bonding with concrete.\n- Use: Chain link fencing is used to create enclosures or boundaries, typically for security or decorative purposes. Rebar is used to reinforce concrete structures, providing additional strength and support to foundations, walls, and other elements.\n- Strength: Rebar is much stronger and more durable than chain link fencing, as it is specifically designed to withstand the forces and stresses of reinforcing concrete structures. Chain link fencing is not strong enough to be used as a structural reinforcement material.\n- Appearance: Chain link fencing is typically silver or galvanized in color, while rebar is typically gray or black. Rebar is also typically ribbed or deformed in some way to improve its bonding with concrete, while chain link fencing has a smooth, uniform surface.\nReason to Use Rebar in Concrete Reinforcement\nRebar is necessary in concrete work because concrete is a brittle material that is prone to cracking and breaking under stress.\nWhile concrete is strong in compression, it is relatively weak in tension, which means that it can break or fail when subjected to forces that pull or stretch it.\nBy adding rebar to a concrete structure, the rebar helps to reinforce the concrete and distribute the forces of tension more evenly throughout the structure.\nThis helps to prevent the concrete from cracking or breaking under stress, and it allows the structure to be stronger and more durable.\nThis material is commonly used in a wide range of concrete construction projects, including foundations, walls, columns, beams, and other structural elements. It is an essential component of many concrete structures, as it helps to provide the necessary strength and support to ensure that the structure is stable and safe.\nThe bottom line is this – concrete projects would fail much sooner (and more catastrophically) without rebar than if they had it in the first place.\nIf you need to reinforce a concrete structure, it is important to use the appropriate materials, such as rebar, to ensure that the structure is strong and stable.\nUsing the wrong materials or attempting to use chain link fence as a substitute for rebar could result in a weak or unstable structure that is prone to failure.\nTo check the current price and availability of Rebar, click here to view the listing on Amazon.\nLimitations of Using Chain Link Fencing as Rebar\nThere are a ton of limitations that chain link has as a “rebar substitute”, not the least of which is the lack of strength it brings to the table.\nChain link fence is much thinner and made of a wire material. Rebar, on the other hand, is much thicker, much stronger, and designed to withstand all the stress and pressure that concrete work inevitably places upon it.\nChain link is also not protected against corrosion the way that rebar would be. You might not think that metal sunken into a concrete mixture and completely encased in the material would corrode, but all those chemicals in concrete would eat away at (and basically dissolve) chain link.\nAll in all, rebar is the way to go.\nTo check the current price and availability of Chain Link, click here to view the listing on Amazon.\nAlternative Materials for Concrete Reinforcement\nThere are no safe alternatives for rebar in concrete construction, as rebar is a critical component of many concrete structures and is essential for providing the necessary strength and support.\nRebar is made of high-strength steel and is specifically designed to withstand the forces and stresses of reinforcing concrete structures.\nOther materials, such as chain link fencing or wire mesh, are not strong enough or durable enough to provide the same level of support and reinforcement.\nAlways – ALWAYS – go with rebar.\nBest Practices for Using Rebar in Concrete Construction\nUsing rebar to reinforce concrete work is pretty simple and straightforward, though there are a couple of things you’ll need to think about before jumping right in:\n- Choose the right size and type of rebar: Rebar is available in various sizes and shapes to suit different applications. It is important to choose the right size and type of rebar for your project, based on the size and strength requirements of the structure you are building.\n- Follow local building codes and regulations: Rebar must be used in accordance with local building codes and regulations, which may specify the size, spacing, and placement of the rebar in a structure. It is important to research and follow these guidelines to ensure that the structure is safe and compliant.\n- Use proper handling and placement techniques: Rebar is heavy and can be difficult to handle, especially in large quantities. It is important to use proper handling techniques and equipment, such as rebar benders and cutters, to ensure that the rebar is placed correctly and securely in the structure.\n- Protect against corrosion: Rebar is prone to corrosion if it is not properly protected, which can weaken the structure over time. It is important to use rebar that has been coated with a layer of zinc or another corrosion-resistant material, and to take steps to protect the rebar from moisture and other corrosive elements during construction.\n- Use proper concrete mix and curing techniques: The strength and durability of a concrete structure is also influenced by the quality of the concrete mix and the curing techniques used. It is important to use a high-quality concrete mix and to follow proper curing techniques to ensure that the structure is strong and long-lasting.\nChain link fence is not, and should never be used as, a safe alternative to rebar in construction work.\nUsing this material in place of rebar is asking for trouble. Serious trouble. We’re not just talking about the failure of the concrete project you’re working on, either.\nWe’re talking about the potential for serious harm and even death to those involved in the inevitable collapse of that concrete work.\nSteer clear of using chain link as rebar. Use the real deal. You won’t regret it."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"temporal_focus","category_name":"current"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:a3885870-6247-43d1-9455-b0cb019f7de7>"],"error":null}
{"question":"As an archaeology student focusing on Iron Age settlements, what were the key architectural features found in both Hatzor and Rosh Zayit (possible Biblical Cabul)?","answer":"Both sites featured significant architectural elements from the Iron Age period. Hatzor had a six-chambered gate with two towers typical of Solomonic architecture, a casement wall system, and a large Canaanite palace with decorated basalt stones and cedar wood walls. The site also contained typical four-room Israelite houses with central courtyards. At Rosh Zayit (possible Biblical Cabul), excavations revealed a Phoenician citadel with storage rooms, defensive walls, and typical Israelite structures with four open spaces - three parallel and one perpendicular. Both sites also contained evidence of olive oil production, with Rosh Zayit having several oil press installations and Hatzor featuring a beautifully restored 8th century BCE oil press.","context":["The Canaanites who lived in Hatzor during the biblical era were pretty smug. And for good reason: Not only was Hatzor a metropolis comparable in size to the biggest cities in powerful Babylonia and Egypt, but it also towered above the Via Maris – the main trade route utilized in ancient times. They had other reasons for complacency as well, for their military capabilities were formidable and their fortifications daunting. It was obvious that soldiers daring to try an attack would shiver with fear as they anticipated the burning oil, boiling water, spears and arrows that the defenders would throw down from the walls.\nBut as King Jabin of Hatzor watched the Israelites conquer piece after piece of the Promised Land, he began to worry. To make certain that he and his people would never fall into Israelite hands, the king initiated a union that consisted of 10 kingdoms in northern Israel, who “made camp together at the Waters of Merom, to fight against Israel” [Joshua 11:5].\nDespite the consolidation of their forces, Joshua managed to carry out a vastly successful surprise assault on Hatzor. And when it was over, he commanded his soldiers to devastate the once proud city. “So Joshua and his whole army came against them suddenly at the Waters of Merom and attacked them, and the Lord gave them into the hand of Israel… Israel did not burn any of the cities built on their mounds — except Hatzor, which Joshua burned” [Joshua 11:7-13]. After putting this important city to the torch, Joshua could finally settle the land of Israel.\nIn 2005, UNESCO added Tel Hatzor to its list of World Heritage Sites of outstanding universal value, and over the last few years Hatzor has undergone an incredible facelift. Today the tel is a fascinating site with partially restored and reconstructed structures and excellent signs.\nTogether with Megiddo and Gezer, Hatzor was mentioned in Kings (9:15) as part of Solomon’s vast building, and fortification, program. The newly well-protected city now had everything it needed to survive as a settlement: fertile land, lush springs, a major thoroughfare, and hills so high that its soldiers could spot an attacking army long before it reached the city gates. But there was still one glitch: Hatzor’s water sources were located outside the city walls. Enemies who couldn’t charge up the heights, or make it through the massive gates, could simply lay siege to the city and wait until the inhabitants began dying of thirst.\nKing Ahab, ruler of the northern kingdom of Israel, ordered his engineers to find a solution. The result, executed with hammer and chisel in the 9th century B.C.E., was a monumental, sophisticated water system that kept the water supply safe inside in the city. During this period Hatzor doubled in size and became the greatest city in the land of Israel.\nFortifications, the water system, and the loftiness of Hatzor all proved useless when the Assyrians attacked in 732 B.C.E. After the battle, the people of Hatzor were led into exile. “In the time of Pekah King of Israel, Tiglath-Pileser king of Assyria came and took Ijon, Abel Beth Maacah, Janoah, Kedesh and Hatzor. He took Gilead and Galilee, including all the land of Naphtali, and deported the people to Assyria” [2 Kings 15:29].\nWhat remains from Hatzor, which never recovered even a shadow of its former glory, are ruins from the 21 cities that stood here one atop the other. In short – a veritable Disneyland for archeology buffs. Save this article – for if nature is your passion, you might want to wait to visit until next April. The gorgeous, delicate Lortet Iris, no longer visible from Highway 90, will be flowering in all its glory on the slopes across from the tel. Just now, there are large white wild carrots there, instead.\nTel Hatzor features a lookout over the Lower City, which extended all the way to the trees you see to your north. Settled during the Canaanite period, the lower city boasted about 15,000 inhabitants. Among the most important finds uncovered in the lower portion of Hatzor were remains from a Canaanite temple. Some experts believe that the Israelites, who lived in the wilderness for centuries after the Exodus, had few building skills and had to copy from what they saw around them. Thus Canaanite temples like this one, full of similarities with Solomon’s Temple, may have seem to have served as its prototype.\nVisitors enter the Upper City through a gate typical of those also built by King Solomon in Megiddo and Gezer. It had six chambers and two towers. Look for the casement wall (a double wall with rooms) to the left of the gate. It was probably just like the one in Jericho, where Rahab hid Israelite spies and later “let them down by a rope through the window, for the house she lived in was part of the city wall.” [Joshua 2:15].\nOther highlights at Tel Hatzor include the huge, Canaanite palace where two enormous column bases still stand at the entrance. The lower portions of the walls were built of heavy, decorated basalt stones that today lack their ornamentation but give you the basic idea. Above them the walls were made of mudbrick, interwoven with cedar wood: reconstruction illustrates a bit of its former splendor.\nDon’t miss the jewel on Hatzor’s crown: the monumental water system, consisting of a vertical shaft that reached through the earth for 46 meters, a 25-meter long sloping tunnel, and a small pool. Walk all the way down (200 steps) and you will understand the immensity of King Ahab’s project.\nWhen you emerge from the water system, walk over to the Israelite area, which was carefully moved from its original site to permit further excavations. Here you will find a beautifully restored 8th century B.C.E. oil press, one of about 20 similar ones that have been discovered so far. Explore a typical “four-room” Israelite house, which had a central courtyard and rooms on three sides, and a large open structure with two long rows of stone columns. This was a public storehouse, and the pillars held up the mid-portion of the roof.\nA metal “soldier”, visible from a distance, tops the Israelite Tower at the western edge of the tel. Constructed as the Assyrian threat became frightening reality, it was designed to protect this side of the city from invasion. Sadly, it was of no help to Hatzor, whose conquest signaled the beginning of the end of the independent northern kingdom of Israel.\nAviva Bar-Am is the author of seven English-language guides to Israel.\nShmuel Bar-Am is a licensed tour guide who provides private, customized tours in Israel for individuals, families and small groups.","Ruins of an Iron age site, in the hills above the plain of Cabul. An 11th-8th C BC Phoenician fortress, regional administrative center, military post and agriculture village. The site may be identified with the Biblical Cabul from the times of King Solomon.\n1 Kings 9:11,13: \"King Solomon gave Hiram twenty cities in the land of Galilee...And they were called the land of Cabul, unto this day\"\nRuins of an Iron age site, in the hills above the plain of Cabul. Excavations reconstructed an Israelite structure from the times of King Solomon, several oil presses, an impressive Phoenician citadel, and an agriculture site that lasted until the 8th C BC.\nThe findings indicate that the site was a central Phoenician administration center for the region, and may be identified with the Biblical Cabul from the times of King Solomon.\nThe site is located on the road (#805) that climbs up to Misgav, 1 KM before the junction of Yaad. It is situated on a hill (height 152M) on the north side of the road. You can point on the purple points to navigate to the selected site.\nThe ancient city was part of tribe of Asher (Joshua 19: 24-27): \"And the fifth lot came out for the tribe of the children of Asher according to their families. And their border was ... And it turned toward the sunrising to Beth-dagon, and reached to Zebulun and to the valley of Iphtahel northward at Beth-emek and Neiel; and it went out to Cabul on the left hand\".\nKing Solomon gives away Cabul\nKing Solomon imported wood in order to build the first temple in Jerusalem (2 Chronicles 2 1,8,16): \"And Solomon determined to build an house for the name of the LORD, and an house for his kingdom... And Solomon sent to Hiram the king of Tyre, saying,... Send me also cedar trees, fir trees, and algum trees, out of Lebanon... And we will cut wood out of Lebanon, as much as thou shalt need: and we will bring it to thee in floats by sea to Joppa; and thou shalt carry it up to Jerusalem\".\nCabul was given by king Solomon to Hiram King of Tyre as exchange for his assistance in building the temple (1 Kings 9:11-13):\n\"Now Hiram the king of Tyre had furnished Solomon with cedar-trees and cypress-trees, and with gold, according to all his desire--that then king Solomon gave Hiram twenty cities in the land of Galilee. And Hiram came out from Tyre to see the cities which Solomon had given him: and they pleased him not. And he said: 'What cities are these which thou hast given me, my brother?' And they were called the land of Cabul, unto this day\".\nThe implied reason was that King Solomon had to pay for his debts (to pay the cost of the timber, labor and other goods), and he paid with prime real estate (although Hiram was not pleased as per the Biblical text). Actually, this old treaty lasted for 3000 years until Israel was founded, since the area of west Galilee always remained under the Phoenician control.\nCutting Cedar of Lebanon for Solomon's temple\nDrawing by Gustav Dore (French artist, 1832-1883)\nWhere is Cabul? The Biblical site is not positively identified, and according to some scholars and recent excavations it may be located in Khirbet Rosh-Zayit. The site was partially excavated and several of its structures were beautifully reconstructed, including an impressive Phoenician citadel from the times of the Israeli Kings (11th C BC), with storage rooms and vessels. Additional residential buildings were also dated to the 11th C. These findings indicate that the site was a central administration and military post for the Cabul region. This is an important finding, which backs up the Biblical description of one of the oldest agreements between two nations - Phoenicia and Israel - from more than 3000 years ago.\nThe fortress was destroyed around the beginning of the 9th C BC, and then it was inhabited by a small agriculture village, which relied on the olive oil production as a source of income. Several oil presses were excavated and indicate that the name of the site (Hebrew for: Head of the Olive tree) was based on the abundance of olive trees and oil production.\nThis village was destroyed in the 8th C (734-732BC), after the intrusion Tiglath-Pileser III (Kings II 15: 29): \"In the days of Pekah king of Israel came Tiglath-pileser king of Assyria, and took Ijon, and Abel-beth-maacah, and Janoah, and Kedesh, and Hazor, and Gilead, and Galilee, all the land of Naphtali; and he carried them captive to Assyria.\".\nThis intrusion wiped out most of the Galilean sites, as written in the Bible, and it was never inhabited again. It was relocated during the Hellenistic period to another site closer to modern village of Kavul.\nOrthostat relief - depicting soldiers from different orders of the Assyrian Army, in procession; basalt; Hadatu;\nTiglath-Pileser III period (744-727BC)\n[Istanbul Archaeological museum]\nA Roman village was established 800 years later on the north-west side of the hill, 500M from Rosh-Zayit. The site is in ruins, and is called Khirbet Beza (in Arabic: Baz'awiya, Baz'ua). After initial excavations, the site was dated to the 1st and 2nd C AD. The first season, managed by Motti Aviam, unearthed an oil press and a private house. The site is well preserved, but requires more digging. One day it could be coupled with Rosh-Zayit to become an archaeological park, as planned by the regional council of Misgav.\nThe site has been excavated and partially restored by Zvi Gal after several seasons (1983-1992).\nIt is open to the public and can be accessed by walking up the hill from the main road to Segev.\nThis photo is an aerial view which was captured by a drone from the west side. The site is located on a hill, and is quite large for an Iron Age site. Only the ruins on its summit were excavated so far, and seen in the center. Road #805, here on the right, loops around its southern foothills.\nClick on the photos to view in higher resolution...\nAnother view, from the opposite (eastern) side of Kh. Rosh Zayit, is in the next photo. The modern Arab village of Kabul is seen on the upper left side of the view.\nFly over the site with the following Youtube video, which was captured by a drone:\nIn the site are traces of ruins of dozens of structures. One of the houses, dated to the 8th C BC, was reconstructed on its western side, as seen below. It included four open spaces - three parallel and the forth perpendicular, which is typical of the Israelite structures at the time of the Kings.\nIn its central yard was a storage place for olive oil. Around the structure were several oil press installations, each included a crushing basin, weights and collecting place for the extracted oil.\nAnother view of the reconstructed house is seen below.\nNearby, another reconstructed house was identified as a ritual structure.\nA view from the ground level to the entrance:\nA closer detail in the entrance to the structure is seen below. Among the findings was a small censer and two fragmentary figurines.\nThe photo below shows the road that cuts through the site. On the left side is a small forest with traces of ruins of structures. On the right is a steep hill leading to the Phoenician fortress.\nThe stones on this eastern side are part of its fallen walls, which surrounded and protected it. The fortress was probably a regional administration center, perhaps for the twenty cities that the Bible told that were given to Hiram as part of the land of Cabul. Hiram came to inspect the land that King Solomon gave him, and was disappointed... perhaps he stood up there?\nThe Phoenician citadel is located on the summit of the steep hill. The aerial photo shows the citadel from its west side:\nSome of the walls have been reconstructed during the excavations, as this section on the north-west side.\nIn the center of the fortress is a two story structure. The rooms on the lower floor contained storage rooms, where jars stored food and water. The excavations unearthed many Phoenician ceramics and iron tools. They dated the destruction of the fortress to the beginning of the 9th C BC.\nOn the top side of the fortress is a section of the wall which is paved with stones, which looks like part of a defense glacis.\nThe photo below shows a view from the top of the fortress to the north-east side. The modern settlement of Yaad is seen in the background.\nNearby is a large layer of fragments of ceramics, left over from the excavations. A large number of Phoenician jars, storage vessels and pottery were found in the excavated structures. The team left behind these fragments that could not be assembled together, or were missing the rims which are commonly used for dating the ceramics.\nIn the north side of the site are traces of more ruins which have not yet been excavated. The excavations covered so far only few of the central structures, and the rest of the 80% of the site is still waiting for future archaeological works.\nThese structures belonged to the agriculture village that existed after the destruction of the fortress, from the beginning of the 9th C until the end of the 8th C BC.\nAn aerial view of the eastern side is seen in the following photo.\nThe next photos is a ground view of the eastern side, with scattered ruins. The settlement of Yaad is on the left background, while Segev/Atzmon on the right background.\nAn oil press is located in one of the reconstructed houses.\nIs this the location Biblical Cabul? This site has an Iron age level and Phoenician structures, so it is very likely that indeed it is the Biblical Cabul The modern village of Kabul, which is located south to the site, preserved the ancient name of Kevul, and therefore there is a high probability that the Biblical site is its vicinity.\nThere are other possible candidates at the nearby sites which have a Bronze and Iron age level:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"atemporal"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:770a1926-92cd-40e0-a4b8-0f85a8db9a05>","<urn:uuid:db00f545-eef3-4b52-b10c-d3140c4fef1f>"],"error":null}
{"question":"For building evaluation in historic preservation, what is minimum age requirement such as my grandmother house?","answer":"50 years is the established age to begin evaluating if a building is eligible for listing on the National Register. This requires research with the county assessor to determine the building's age.","context":["The Office of Archaeology and Historic Preservation (OAHP) is a division of History Colorado (The Colorado Historical Society) and serves as the Colorado State Historic Preservation Office (SHPO).\nFederal agencies are required to engage in what is termed “consultation” with the SHPO to determine if any Federally funded or permitted activities may have an adverse effect on historic properties. Historic properties include not only those properties that are officially listed on the National Register of Historic Places, but also those that are determined to be eligible for the National Register. This requirement is set forth in Federal legislation known as the National Historic Preservation Act. The consultation process is described in Section 106 of that act.\nSometimes Federal agencies will delegate to its grantees or permitees the job of consulting with the SHPO in the Section 106 consultation process. This guide is designed to help those who are unfamiliar with the Section 106 consultation process both understand what the process is and how to provide to the SHPO information needed to respond to a request without our office having to request additional information from you.\nFor an overview of the Section 106 review process, you may wish to read A Citizens Guide to Section 106 Review, a short publication found on the ACHP web site.\nA list of properties currently on the National Register in Colorado can be found on our website. In addition, our office maintains a library of many thousands of inventory forms for properties already recorded. It may be possible to determine if the property or properties involved with your project has/have already received determinations regarding whether they are eligible to be listed on the National Register. This process is called a “File Search” for which there may be a small fee depending on how much research is required. Therefore your next step may be to contact our office to determine if this is the case.\nFor more information on file searches, please contact the SHPO at (303) 866-5216 or visit our file search page on this web site.\nThe following guidance is intended to be a guide to the information that the SHPO may need to respond to your Section 106 consultation request.\nIdentify the Federal agency involved, the agency program and type of Federal involvement\nExample 1: The Federal agency involved is the Department of Housing and Urban Development (HUD) the staff contact information for which is ___________; the Federal program is CDBG; and the type of Federal program is a low-interest loan to conduct rehabilitation work on downtown buildings.\nExample 2: The Federal agency involved is US Department of Agriculture Rural Development the staff contact information for which is ___________; the Federal program is the rural solar program; and the type of Federal program is a grant to fund a portion of the costs to place solar panels on a barn.\nExample 3: The Federal agency involved is the Department of Energy the staff contact information for which is ___________; the Federal program is rural electrification; and the type of Federal program is permitting transmission lines.\nWhat do you propose to do?\nExample 1: Remove metal siding on several buildings; repair façade materials in need of repair; and paint.\nExample 2: Install solar panels on the south side of a barn.\nExample 3: With heavy machinery construct towers and string lines that may be seen for hundreds of feet in any direction.\nProvide vicinity information\nAre there any buildings or structures 50 or more years of age on or adjacent to property site?\n(This is important because 50 years is the established age to begin to evaluate if a building is eligible for listing on the National Register and will require research with the county assessor.)\nWill any buildings 50 or more years of age be vacated elsewhere as a result of this project?\n(This is important because abandonment of a building eligible to be listed on the National Register may be considered an adverse effect to the building.)\nWill there be any ground disturbance?\n(This is important to determine if archaeological sites may be affected by the project.)\nHas the land been previously disturbed?\nWill access roads be constructed?\nWill the project require borrow areas?\nWill the project require staging or storage areas?\nWhat are the previous use(s) of site?\n(This is important to determine if previous uses may have damaged properties or obliterated archaeological sites.)\nEstablish the Area of Potential Effect (APE), which is defined as the geographic area or areas within which an undertaking or project may cause changes in the character or use of historic properties, if such properties exist. The APE should reflect the potential visual, auditory and physical effects to the setting of historic resources\nExample 1: The APE of a project to do interior work may be only the building involved. However the APE for exterior work may also include the neighborhood if the neighborhood may be eligible or listed as a historic district.\nExample 2: The APE for a tall structure (such as a barn) may include the area in which it is visible if there are eligible properties for which the viewshed is part of their significance.\nExample 3: The APE will include the areas of the ground that will be used for construction, staging, and building access roads. In addition, because the electrical towers will be seen for hundreds of feet in any direction, the APE may include all or part of the area in which they are visible (similar to Example 2).\nComplete Inventory form(s) for each building or structure (50+ years old) and archaeological site within the APE. The inventory form should include your opinion on the National Register-eligibility of any resource for which an inventory form is completed. The form that is used for a building is called the Architectural Inventory Form 1403 and is available on our website. There are other forms for archaeological sites and linear resources (such as railroads or ditches) for which you are best advised to hire a consultant to complete. Inventory forms may be found on our website.\nFederal regulations state that the federal agency should supply its opinion on the project’s potential effects to resources identified as eligible for the National Register within the APE. If the federal agency has delegated this to you and you do not have the expertise to do it, you will have to rely upon the SHPO’s determinations. The types of effects are:\nNo historic properties affected\nThis finding of effect is appropriate when there are no National Register-listed or eligible properties within the APE, or if there are such properties they will not be affected by the project.\nNo adverse effect\nThis finding of effect is appropriate when the project will affect a National Register-listed or eligible property but the effect will not diminish the characteristics rendering a property eligible for the National Register.\nThis finding of effect is appropriate when the project will diminish the characteristics rendering a property eligible for the National Register—either completely (for example, by demolishing it), or significantly (for example, by altering the façade of a building).\nConsult with the appropriate local government and other consulting parties regarding your determinations of eligibility and potential effects, if applicable. The local government in which your project is located is required to be given the opportunity to offer its opinions on the project. Therefore you should also forward your request to the local government as well as the SHPO. Many local governments have their own historic preservation ordinance. A list of these local governments is on our website.\nIn addition, Native American tribes are required to be notified depending on the nature of the project. This is particularly important in rural areas or for projects where ground disturbance will take place. The applicable federal agency remains responsible to conduct this consultation.\nFor more information on consulting party requirements, you should contact the SHPO.\nCorrespondence with our office should be addressed to:\nState Historic Preservation Officer\nDenver, CO 80203"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"temporal_focus","category_name":"future"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:cabe9362-dadc-4d1a-8ca4-f370108a33c8>"],"error":null}