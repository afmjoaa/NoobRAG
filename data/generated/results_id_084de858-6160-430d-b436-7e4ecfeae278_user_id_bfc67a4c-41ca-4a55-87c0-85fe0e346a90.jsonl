{"question":"How do SMS material construction methods differ from traditional materials, and what specific fire protection standards must protective clothing meet in different regions?","answer":"SMS material is made through a specialized three-step process: first spunbonding, where PP is processed to create a cotton candy-like texture, then meltblowing to decrease permeability, and finally spunbonding again to add elasticity. This creates a material that is far less permeable than regular PP, making it suitable for operating rooms and clean room environments. Regarding fire protection standards, in the UK, protective clothing must comply with EN ISO 11612 for protection against heat and flame. This standard specifically requires limiting fire spread so the material burns itself out. The clothing must be designed to protect against various hazards including flash fire, arc flash, and molten-metal splatter by offering ignition prevention or self-extinguishing properties.","context":["Disposable Coveralls, lab coats, gowns, aprons, even pants -we can do it! When it comes to barrier protection for your body, there are numerous options. Think of body protection like clothing. In clothing, there are three primary aspects:\n- Material of clothing\n- Type of garment\n- Style of garment\nFor instance, with with a pullover, it can be made of cotton, wool, polyester, nylon, another material or a combination of materials. For simplicity, let’s say the pullover you choose is a cotton pullover. The style of the pullover means that the pullover has two side pockets, no collar, and a zipper that zips halfway up. The style of a garment represents the majority of complexity of design and manufacturing because there are myriad variations and styles each garment can have (an essence of fashion). While most people don’t consider disposable body protection fashionable -it can be complex…but it doesn’t have to be.\nWhen it comes to the type of garments within body protection there are 4 primary types of garments:\n- Lab coats\nIn terms of usage, gowns and lab coats represent the largest market share of the body protection market because each of those items is heavily used within the medical market. Coveralls are also used by a significant number of industries, but primarily in the industrial market. Whereas, aprons are mostly used within the food industry and lastly, pants are used in niche situations.\nDisposable Gowns generally provide body coverage on the torso, arms. Some gowns provide back coverage -whereas others do not provide coverage on the back. Very few gowns provide barrier protection from the waist down or on the head. Most gowns are made of PP and SMS, although there are gowns made of PE material too.\nDisposable Labcoats all provide barrier protection on the front and back of the torso, unlike some gowns. Also, unlike most gowns, there are a wide variety of styles of lab coats (buttons, zippers, pockets, collars, elastic wrists, etc.). Most lab coats are made of PP or SMS material.\nDisposable Coveralls provide the most body protection of any garment. While there are many styles of coveralls, most lab coats cover your legs, torso, and head. Some coveralls cover your feet as well. Most common variations are related to hoods, booties, zippers, buttons, and tape on the seams. As for material, most coveralls are made of PP, but you can find many coveralls made of microporous and SMS.\nDisposable Aprons primarily cover the front of your body or your chest. Depending on the size and length of material, some coveralls cover a portion of your back or a portion of your arms, but most only cover your upper torso. These are made of PE, PP and PVC material.\nDisposable Pants are rarely used in the market, but when they are used it is made of PP, SMS or Microprous material in the medical or corrections environment. Pants are used as a scrubs replacement or coverall, gown, or lab coat is not sufficient for the specific task at hand. While this garment has the least demand, we are certainly open to discussing your pant need.\nAs for materials, there are 4 primary types of materials used in disposable body protection. Bear in mind, all disposable body protection is considered non-woven material. Meaning, it is not a woven textile product -it is a plastic based product. As such, all of the materials used to make disposable body protection are plastic products.\nPE stands for Polyethelyene material. Trashbags are made from this material. Polyethelyene is the most commonly manufactured plastic in the world. All of these plastics are water resistant, generally durable and provide good barrier protection depending on how they are used (e.g. certain materials perform better under certain conditions). PE material is generally considered the least expensive material within body protection.\nPP stands for Polypropylene. PP is basically like a plastic paper. While still water resistant, it is more breathable than PE material. While more expensive than PE, it is significantly less expensive than SMS and Microporous material.\nSMS is a type of PP material. It stands for Spunbond, Melt blown, Spunbond. What distinguishes SMS material is how the PP material is processed. When SMS is manufactured, PP is processed with a machine that separates the plastic compounds apart and turns the PP material into what looks and feels like cotton candy. This process is called “Spunboning”. This process increases the material’s surface area and changes the material dynamics of the PP material. Afterward, the material is “meltblown” meting the material decreases the permeability of the PP material. After it has been melt blown, it is again Spunbond to add material elasticity and improve the feel of the product. SMS is far more expensive than PP, but it is far less permeable garment. SMS is most appropriate for Operating Rooms, medical settings, or clean room environments where it is important to prevent any permeability of liquid to or from the garment. Hypothetically, a garment can be SMMS or SMMMS if a person is looking to minimize liquid permeability.\nMicroporous material is a flash spun PE material. This material is used in building insulation, for packages, and for personal protective equipment (PPE). Most are familiar with the company Dupont. Several decades ago, Dupont promoted its brand Tyvek selling patented versions of microporous material. While Tyvek is a great variation of this product, the underlying material is a microporous PE simply known as microporous. This is the least permeable material in body protection. The permeability is less than 2 nano meters. Unfortunately, it is the most expensive material and the least breathable material; however, for high risk situations, most Hazmat suites utilize are a variation of microporous material. Within microporous material there are a wide variety of specifications; however, most garments made using this material are coveralls.\nPVC stands for polyvinylchloride. Mostly disposable sleeves and gloves are made of our this material. Only some aprons utilize material. PVC has the best “hand” or overall feel of all the plastic materials in non-woven. However, it is the most dense, most expensive second to microporous material. While many people prefer to feel and comfort of the material, the cost is prohibitive for most as it relates to body protection.\nWhen it comes to body protection, we recognize no two people are the same as such no PPE provider should provide a one fit solution for all companies. We work you find the best product for the task at hand.","What is Fire Resistant Clothing? - iWantWorkwear-- Alternatively, inherent fire resistant clothing is a permanent safety solution because its FR properties are at a molecular level. The protection doesn’t wash or wear ：\nWhat Can Fire-Resistant Clothing Protect Against? - FROutlet-- They are the only non-fire-related danger fire-resistant clothing effectively protects against. Although single layers of fire-resistant clothing are usually strong\nThe Limits of Protection Provided by Fire-Resistant Clothing-- If you do, you need to use other pieces of FR clothing for your layers. If you don’t, you’ll get just as severely burned as you would with no FR clothing on at all.\nFire Resistant Clothing | Nomex - DuPont-- Beat the heat with fire resistant clothing. Fire is unpredictable. And deadly. That’s why workers who face on-the-job threats of fire and electric arc hazards rely on\nFlame-Resistant Clothing: Everything You Need -- . Just as hard hats, gloves, impact-resistant glasses, and ear protection are common safety gear for power plant operators, flame-resistant clothing is also important.\nUnderstanding Flame-Resistant Clothing and When to Use It.-- Firefighters enter hazardous situations on a regular basis and must be properly protected from the elements. Because of the nature of the profession, firefighters\nProtection from fire: Nonflammable clothing — A reviewAbstract. Technology has the capability of providing nonflammable protective clothing for fire fighting, industrial, and military applications. However, there are practical problems to be\nFlame Resistant Clothing vs Fire Retardant -- The unique thing about flame-resistant clothing is the material. The fibers and threads are made with materials that prevent flames. The clothing is sewn together in a way that prevents flames from burning if\nHome - Ultima WorkwearEstablished in , Yong Fah International is a Singapore-based manufacturer of the ULTIMA brand of Boilersuits / Coveralls known for their pre-shrunk technology, long\nQuick Guide to Flame Resistant Clothing - Fire Protection As a minimum, FR clothing in the UK should comply with EN ISO : – Protective Clothing to Protect against Heat and Flame. Standard EN ISO : goes further to specify the requirements of limiting fire spread so it burns itself out. Of course, there is a wide range of other standards you may require flame resistant clothing to\nFlame Resistant Clothing | PK Safety-- Flame resistant clothing (also called fire resistant or FR clothing) is safety workwear specially designed to protect the worker wearing it from the hazards related to fire and heat. FR clothing is made of flame-resistant fibers that allow it to resist ignition or quickly self-extinguish if it does catch fire.\nFire Resistant Clothing | Nomex - DuPont-- Nomex EMS wear provides protection from flash fire hazards, chemicals and bloodborne pathogens, enabling EMS professionals to keep saving lives. Residual oils & contamination on Nomex vs. FR cotton/nylon See how Nomex outperforms fire-resistant cotton/nylon blends in personal protective equipment. SafeSPEC ™\nFlame-Resistant Clothing: Everything You Need -- Please contact or call -- (M – Th am – : pm and F am – pm. ET) , to start a free trial, get pricing information, order a reprint, or post an\nProtective Clothing -Chemical Resistant -- Fire-resistant clothes. Proximity suits. Disposable protective clothing. Electrician uniform. Welding protective clothes. Chemical protective clothing (CPC) is used by the people who work with chemicals to protect\nFlame Resistant Workwear | Bisley Flame Retardant Tradie Our Bisley work gear complies with safety standards to keep you safe and reduce the risk of serious injury on the job. Sort by / products % $. $. Bisley Taped Two Tone Hi Vis FR Lightweight Long Sleeve Shirt (BST) Bisley Available in colours Save % $. $.\nFire Resistant Clothing – Forcefield Canada - Hi Vis Aprons & Bouffant Caps Fire Resistant Clothing Gloves Work Gloves Abrasion Resistant Gloves Anti-Vibration Gloves Arc Flash Gloves Chemical-Resistant Gloves\nFR Coveralls | Flame Retardant Coveralls | Shop FR CoverallsFrom flame retardant coveralls to Balaclava, FR jackets and more, we have a carefully curated selection of high-quality protection solutions right at your fingertips. Our protective apparel at PDS has been rigorously tested to ensure your safety. Browse and purchase FR coveralls and other personal protective clothing online today.\nThe Evolution of Flame-Resistant Clothing - Occupational -- Today's FR garments can protect against a variety of hazards — including flash fire, arc flash and molten-metal splatter — by offering ignition prevention or self-extinguishing properties that\nFire Protection Solutions Brandschutz und Feuerschutz-- im Brandschutz Willkommen bei Fire Protection Solutions – Brandschutz und Feuerschutz auf hchstem Niveau Ob Sprinkleranlage oder Sonderlsung – wir bieten Lschanlagen fr alle Bereiche. Unsere Unternehmen blicken auf langjhrige Erfahrungen im Brandschutz zurck – und geben Ihnen so Sicherheit. Auch wenn es brennt.\nFire Resistant Clothing - What You Need To There are rated levels of flame resistance and, put simply, the ARC rating determines the protective characteristics of protective fabric. - The higher the ARC rating value the greater the protection. Manufacturers of fire resistant\nFire Resistant Clothing - DuPont\nImportant Facts About Fire Resistant (FR) Clothing-- To reduce your PPE costs for fire resistant clothing, disposable FR garments are used to extend the life of primary FR outfits and reduce the financial load on PPE budgets. Here we will look at the important facts for choosing disposable FR clothing that performs well for the level of protection needed, and are a cost-effective solution for\nFlame Resistant Clothing - Fire Protection Online-- Flame Resistant ClothingWhether you’re looking to protect yourself or your colleagues, our range of flame resistant clothing is designed to protect you multiple hazards at the same time. Including jackets, trousers and coveralls, they’re comfortable to wear all day long and are suitable for even the most demanding environments.Flame resistant for when\nFlame Resistant Clothing vs Fire Retardant -- The fibers and threads are made with materials that prevent flames. The clothing is sewn together in a way that prevents flames from burning if exposed to fire. Flame-resistant clothing self-extinguishes quickly. Cost:\nFlame Resistant Workwear | Bisley Flame Retardant Tradie Flame Resistant. Our Bisley flame resistant clothing is designed to significantly reduce your risk of serious injury in the work place. If you’re performing dangerous jobs that requires fire safety gear, then we have everything you need to work smart and stay safe. Our Bisley work gear complies with safety standards to keep you safe and\nFire Protection Suits | Firefighting Clothes | Fire Resistant Fire Protection Suits. () Draeger CPS for excellent protection. The gas-tight suit Drger CPS provides excellent protection against industrial chemicals, biological agents, and other toxic substances. Its innovative material qualifies the CPS equally well for work in explosive areas and for handling cryogenic substances.\nProtection from fire: Nonflammable clothing — A reviewAbstract. Technology has the capability of providing nonflammable protective clothing for fire fighting, industrial, and military applications. However, there are practical problems to be overcome before everyone can enjoy the protection of\nArc Flash Resistant | Clothing Garments PPE Protection -- This is because arc flash-protective clothing is designed not only to protect operatives from fire, but from the thermal energy generated by an arc flash, which can also cause external and internal burns. While the threads used for the structural seams must be fire resistant, under IEC , arc flash-resistant clothing has various standards\nPrevious: arc flash gloves omanNext: Лондонгийн гал командын музей\nFire Resistant Solutions | GE InsulatedFire Resistant Solutions to Ensure Maximum Protection. We can offer a range of hygienic composite panel fire resistant solutions which provide a cost-effective, rapid build in comparison to traditional fire compartmentation methods and are suitable"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:eb89edaa-bc1d-4f26-b3c8-1cdb4081e4a2>","<urn:uuid:9a81d417-f7c7-4516-b38a-c148cede68db>"],"error":null}
{"question":"How do the protective mechanisms in Oropetium grass compare to those found in baker's yeast during drought conditions?","answer":"Both Oropetium grass and baker's yeast have evolved protective mechanisms for surviving drought, but they use different approaches. Yeast cells produce trehalose, a special sugar that acts as a referee to prevent irreversible chemical reactions between dried cell components. Meanwhile, Oropetium has developed mechanisms to protect its DNA and RNA during dry periods, which scientists describe as similar to putting antifreeze in a vehicle. Both organisms can survive in a dormant state and revive when water becomes available, though Oropetium's specific mechanisms are still being studied through genome sequencing to understand how this trait could be applied to improve crop resilience.","context":["There is a miracle in my refrigerator.\nI'm not referring to the leftovers from the delicious lamb curry that my wife, Linda, prepared last night for supper. I'm referring to something even more spectacular.\nCan you guess what it is?\nHere's a clue — bakers and pastry-makers routinely experience this miracle. Give up?\nIt's a packet of dry yeast.\nBefore you say, \"No way, Steve,\" consider the marvel in this package. Inside is a tan-colored, granular substance, which is comprised of yeast cells that have been dried and coated with an inert material.\nThe cells show no signs of life. No metabolism. No reproduction. No growth. For all practical purposes, the yeast is dead.\nBut the cells aren't really dead. They are waiting in a state of suspended animation for a little water to rouse them from their sleep, much like the prince's kiss did for Sleeping Beauty.\nAdd warm water and in about the time it takes to hard-boil an Easter egg, the yeast cells will be growing, metabolizing and dividing — oblivious to their deep slumber.\nIn short, the yeast cells have undergone the wonder of resurrection.\nThe yeast cells were originally grown in large vats and then dried under controlled conditions. The cells went from an active living state to a dry nonliving condition, and finally in our kitchens they are revived with water.\nBiologists call this phenomenon in which an organism can be dried and then rehydrated anhydrobiosis.\nAnhydrobiosis is an adaption that evolved in organisms that live in environments where moisture availability is sporadic. A variety of microscopic animals, such as rotifers, nematodes and tardigrades, as well as fungi such as yeast, and even a few plants such as rusty woodsia (Woodsia ilvensis) are capable of resurrection.\nIf you've hiked around Quarry Park & Nature Preserve in Waite Park, you may have seen rusty woodsia. This fern grows in the cracks and crevices on the rocky outcrops.\nWhen there is adequate rain, rusty woodsia looks like an ordinary fern with expanded green fronds. However, during dry periods, the fronds curl up and turn brown and the plant looks dead. A subsequent rain will kiss it back to life.\nYeast and rusty woodsia are able to resurrect because they evolved a clever mechanism to protect their cells as they dry. Unlike humans and most other organisms, the cells of anhydrobiotic organisms produce a special sugar called trehalose that acts a little like a referee to prevent irreversible chemical reactions between the dried cell components. Without trehalose, a cell can't be revived when it is rehydrated.\nThe practical implications of anhydrobiosis are enormous. For example, if we can figure out how to load trehalose into blood cells, the blood could be dried down and would have a long shelf life without the need for continuous donations.\nAll of this talk about yeast, bread and lamb curry is making me hungry. I think I'll go check out what new miracle Linda has waiting for me in the refrigerator.\nThis is the opinion of Stephen G. Saupe, a professor in the biology department of the College of St. Benedict and St. John's University and director of the CSB/SJU Bailey Herbarium and Melancon Greenhouse. He can be reached at firstname.lastname@example.org.","How Resurrection Plants Could Lead to More Drought-Tolerant Crops01/05/2016\nA Lazarus heart beats inside an obscure grass. Rip it from the soil; throw it on the counter; walk away; let it dry out and turn brittle. Looks dead and should be dead. But splash on a bit of water and life comes forth: plant resurrection.\nRecent genome sequencing of Oropetium grass has given researchers a blueprint in distinguishing genes related to phenomenal plant resilience. The bizarre self-preservation abilities of resurrection plants like Oropetium hold tremendous promise toward engineering stronger drought-tolerance in crops, and the effects soon could reach farmland.\nScientists at the Donald Danforth Plant Science Center submitted an Oropetium sequencing proposal to Pacific Biosciences’ “Most Interesting Genome in the World” grant program. The genome sequencing results are the first steps toward mapping the resurrection trait. “This discovery is related to all crops in agriculture,” says Robert VanBuren, NSF postdoctoral scientist, Danforth Center. Plants use similar pathways during seed drying. The capacity to perform resurrection may be a matter of changing around how the genes are regulated.”\nOropetium endures harsh, rocky terrain in Africa and India. When lacking water, it shows extreme tolerance. The hearty characteristic is similar to dessication when seeds dry out and remain dormant yet viable for years. The embryo stays alive and can germinate. Essentially, add water and the fuse is lit.\nIt’s a remarkable trait, but an elusive one lacking in crop grasses. During dry periods, Oropetium locks up and protects its DNA and RNA. “It’s sort of like putting antifreeze in your vehicle to protect it from extremes,” says VanBuren. “This is an understudied phenomena as a seriously promising path for crop improvement.”\nVanBuren isn’t seeking extreme dormancy in crops, but rather better endurance under stress. “We want to boost crops in periods where they would normally lose production. Essentially, we’re looking for better performance in crops, not resurrection.”\nThe genome sequencing effort was performed with PacBio’s SMRT (Single Molecule, Real-Time) technology. With SMRT Sequencing, genome sizes such as Oropetium can be sequenced in two days, and only a matter of weeks for larger plant genomes like corn.\nWith these falling costs and technological leaps, breeding programs have gained unprecedented access to genome sequencing. Industry-leading ag companies recognize the need to overcome genetic bottlenecks in major crops. Narrow gene pools leave crops highly exposed to pest pressure, disease, and weather extremes, according to Jonas Korlach, chief scientific officer, PacBio. “After centuries of breeding, genetic diversity has decreased, partly through the pursuit of yield. The agriculture industry wants to get it back. With this technology, we can think about sequencing multiple strains of a particular crop and wild varieties,”Korlach says. “The gene pool holds so many answers for the challenges faced in agriculture.”\nEvery genome tells a story. The genome and the genes within explain the adaptation of an organism, which are the nuts and bolts of survival and prosperity. Oropetium has amazing drought-tolerance and the genetic basis for its stamina is hiding inside the genome. “We will identify Oropetium genes that confer the ability to withstand drought,” says Todd Mockler, associate member, Danforth Center. “We’ll do it in a research context, but an agbiotech or seed company will deploy it in a commercial context.”\nWhether through GMO tech, computational breeding, advanced genome editing, RNAi or another technology, the door to transfer resurrection genes into plant crops is opening. Mockler’s goal is to make heartier crops which stand tough against drought. “We want to improve drought tolerance and water use efficiency. During extreme weather years, farmers would still get some yield.” In addition, Mockler says Oropetium genes can be manipulated to provide benefits during standard weather years. “In a normal year, plants still get all sorts of daily stress. We want discoveries from sequencing the Oropetium genome to boost yield stability in the field during those normal years.”\nOropetium’s Lazarus heart is enabling a marriage of resurrection plants and field crops. VanBuren and Mockler will next test genes related to resurrection properties by placing them in model plants. “When we see boosted drought-tolerance, we’ll be ready to insert the genes into crops,” says VanBuren.\nSource: Chris Bennett, Ag Professional"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:f1191bdc-4408-48ab-801d-176ccbd0314d>","<urn:uuid:70367f5a-01c9-4064-abfe-6f1c737a34fd>"],"error":null}
{"question":"What are the notable differences in rainfall patterns between Ernakulam district and Sindhudurg district, and how do they impact their coastal characteristics?","answer":"Ernakulam district receives an average annual rainfall of 3099.1 mm with 132 average annual rainy days, while Sindhudurg district has a slightly higher average rainfall of 3287 mm. Both are coastal regions - Ernakulam has a 46 km coastline along the Arabian Sea, while Sindhudurg has a longer 121 km coastline. The rainfall in both districts contributes to their tropical climate characteristics, with Ernakulam having a moderate climate due to its rivers, lakes and seas, and Sindhudurg experiencing a semi-tropical climate that remains warm and humid most of the year.","context":["Ernakulam at a Glance\nDistrict Name Ernakulam\nGoegraphical Area 2407 Sq.kms\nCoastal line 46 Kms\nWater bodied area 12,700 Hectares\nForest Land 8,123 Hectares\nLocation Latitude : 9o57' N; Longitude 76 o15' E\nBoundaries North â€“ Thrissur District, East â€“ Idukki District, West â€“ Arabian Sea, South â€“ Kottayam & Alappuzha Districts\nTime Indian Standard Time +5:30 hours GMT\nAltitude Sea Level\nCorporation 1 (Kochi)\nAssembly Segments 14\nParliament Constituencies 2 & Part of Mukundapuram\nLanguage Malayalam is the Official Language. English is popularly used in business circles.\nReligion Hindus accounts for the largest community followed by Christian and Muslims. A small population of Jains, Jews and Sikhs are also residing in Kochi.\nClimate Summers are hot, winters mild. The average annual rainfall is 3099.1 mm with 132 average annual rainy days.\nSeason Best time to visit is between October and March\nClothing Warm tropical climate recommends light cotton clothes. Waterproofing is required during Monsoons.\nErnakulam district formed on 1st April 1958 carving areas of erstwhile Travancore-Kochi-Malabar kingdoms. Major portion of the district is from the Kochi kingdom. The district consists of the mainland Ernakulam, the man made Willington Island, Mattanchery, Fort Kochi, world's most populous area of Vypeen Island, Bolghatty Palace etc. Kochi is the most modern city of Kerala where the best shopping, markets and bazars are located.\nFrom time immemorial Arabs, Chinese, Dutch, British and Portuguese seafarers followed the sea route to Kochi and left their impressions in the town. The Chinese fishing nets, believed to be erected in 1350 AD, swaying in the breeze over backwaters, the Jewish Synagogue, Dutch Palace, Portuguese Architecture, Bolghatty Palace etc. enrich the heritage of Kerala.\nThe word Ernakulam was drawn from a Tamil word Erayanarkulam means abode of Lord Shiva's.\nThe present Ernakulam District include Paravur, Aluva, Kochi, Kanayannoor, Muvattupuzha, Kunnathunadu, Kothamangalam Taluks which come under Fortkochi and Muvattupuzha Revenue Sub Division. Prior to the formation of Idukki District, Thodupuzha Taluk was also a part of Ernakulam District. Ernakulam District was formed on April 1st 1958. The District which has an area of 895.3Sq.K.M. can be divided geographically as Highland, Midland and Coastal area. The altitude of Highland is about 1000 feet.\nThe borders of the district are the Arabian Sea in the West, Thrissur District in the North, Idukki District in the East and Alappuzha and Kottayam District in the South. Periyar, Kerala's second largest river flows through all the Taluks except Muvattupuzha. Muvattupuzha river and a branch of Chalakkudy river all gifts of this District. Average rainfall yearly is 3431.8 mm and 139 rain days. As there are rivers, lakes and seas the region has a moderate climate. Temperature is between 31.3oCand 25.8oC . Many types of sands and soil and also rocks which are geological importance is abundant here.\nMajority of islands in Kerala are in Ernakulam district which was from different parts of old Cochin-Travancore-Malabar area. The main islands of this district all man-made Willington Island,World's most populated Vypeen Island, Cheriya Kadamakkudi, Valiya Kadamakkudi, Ramanthuruthu, Ponjikkara, Vallarpadam, Kumbalam, Panangad, Cheppanam, Nettoor, Pizhala, Kankattuthuruthu, Korampadam, Cheranelloor, Chathanadu, Chendamangalam. All these islands are populated areas. All these islands are believed to have formed from the soil accumulated due to soil erosion of highlands.\nROAD TRANSPORT SERVICES\nSl No. Name Telephone\n1 Aluva,K.S.R.T.C Station 2624242\n2 Angamaly,K.S.R.T.C Station 2453050\n3 Ernakulam,K.S.R.T.C Station 2372033\n4 Kothamangalam,K.S.R.T.C Station 2822202\n5 Muvattupuzha,K.S.R.T.C Station 2832321\n6 Perumbavoor,K.S.R.T.C Station 2523416\n7 Perumbavoor,K.S.R.T.C Station 2442373\n8 High Court Junction Bus Station -\n9 Railway Station Junction -\n10 Kaloor Bus Station -\n11 Tamil Nadu Road Transport Bus Station 2372616\n12 Indira Travels Inter State Private Bus Service,Jos Junction 2360693\n13 Yesbee Travels,Inter State Private Bus Service,Jos Junction 2375080\n14 Princy Tours,Inter State Private Bus Service,Opp.Sealord Hotel 2354712\n15 Sharma Transports,Grand Hotel 2350712\n16 Ashirvad Travels,Jos Junction 2367709\nSl No. Name Telephone\n1 Ernakulam Junction 131\n2 Ernakulam Town 2390920\n3 Aluva Railway Station -\n4 Angamaly Railway Station -\n5 Kochi Harbour Terminus 2666001\nSl No. Name Telephone\n1 Vypeen- Frequent Services(1/2 hour Journey) -\n2 Mattancherry-Daily 6 Ferries:At 9:10,9:40,11:00,14:00,15:20,17:40(1/2 hr) -\n3 Fort Kochi-Daily 30 Services:First Trip at 6:00 and last at 21:10(20 min) -\n4 Varapuzha-Daily 6 Boats:(2 hour journey) -\n5 Willingdon Island:Frequent Services(15 minutes journey) -\nAIRPORT & AIRLINE SERVICES\nSl No. Name Telephone\n1 Nedumbassery International Airport -\n2 Air India Office,MG Road 2380700/2351295\n3 Indian Airlines,DH Road 2370238/2353826\n4 Jet Airways Atlantis,MG Road 2369212\n5 Oman Air(RL Travels) Atlantis,MG Road 2357093\n6 Cathay Pacific,MG Road 2362064\n7 Saudi Arabian Airlines,MG Road 2352689\n8 Singapore Airlines,MG Road 2351829\n9 British Airways,MG Road 2364867\n10 Kuwait airways,MG Road 2360123\n11 Air Sahara 2365508\nBuilt in the Indo-European style way back in 1667 AD, Bastion Bungalow get its name from its location on the site of the Stromberg Bastion of the old, Dutch fort\nscenic dam site with boating facilities is situated in a vast virgin forest. It is a popular picnic sport with Salim Ali Bird Sanctuary near by. (50 km north east of Ernakulam town)\nBolghatty palace built by the Dutch in 1744. Later it became the seat of the British Resident of Cochin. Today it is a hotel run by K.T.D.C. The grounds have a small golf course & several vantage points for lovely views of the harbour & the sea. (3 kms from Ernakulam by Boat)\nThis land was once the abode of the Paliath Achans, the prime ministers of the Maharajas of Kochi. Their residence, the Paliam Palace represents the architectural splendour of Kerala. The Palace houses a collection of historic documents and relics. (about 42 km from Ernakulam)\nCherai Beach offers a unique combination of sea and backwaters rimmed by lush green coconut palms. (25 kms from Kochi)\nMother Goddes worshipped in three different forms - as Saraswati in the morning - draped in white, as Bhadrakali at noon draped in crimson, and as Durga in the evening decked in blue. (13 kms from Kochi)\nThe Dutch palace or Mattancherry palace was originally built by the Portuguese and presented to the Raja of Cochin, Veera Kerala Varma in 1555. It was later taken over by the Dutch who improved it through extensions and repairs in 1663. (Mattanchery)\nThe largest archeological museum of Kerala. Paintings, carvings, etchings and other trappings of royalty like majestic beds ,weapons and samples of epigraphy. (13 Kms from Cochin on the Ernakulam-Chottanikara route )\nImportant scenes of Kerala history are portrayed through through Sculptures. Greeting the visitor outside is a statue of Parasurama ,the mythological safe who is said to have created Kerala. (6 kms from Ernakulam at Edappally )\nIringole Forest Temple\nIringole Kavu, a temple in tropical rain forest in the middle of a town (8 kms from Kalady Town )\nThe Synagogue at Mattancherry built in 1568 is the oldest Synagogue in the Common Wealth Countries. It was partially destroyed in the war of 1662, but was rebuilt by Dutch. In the mid 18th century the clock tower was added. (Fort Kochi )\nKalady is the birth place of Adi Shankaracharya the great Indian Philosopher who lived in the 8th century. 2 shrines in memory of Sankaracharya - one for Dakshinamoorthy and the other for the Goddes Sharada. (Kalady, 45 Kms from Cochin )\nA 9th century Jain temple shaped out of from a huge rock on a small hill in a picturesque surrounding. The visitor has to climb 120 steps to reach this rare historic temple. (22 kms from Kalady )\nIt is believed that the mosque was erected over the mortal remains of a Muslim saint, Sheikh Parid. Another great Muslim saint, Baver is supposed to have prayed here and attained salvation. (Kanjiramattom, 30 km from Kochi )\nKodanad is one of the largest elephant capturing centres of South India. In 1977 the elephant capturing came to an end, but the elephant kraal and training centre are still there intact. (45 kms. to the north east of Cochin city and 12 kms. to the east of Perumbavoor town )\nKottayil Kovilakam at Chennamangalam, which was the seat of Kshatriya chieftains of Villarvattom is situated near the ancient Kunnathali temple. Chennamangalam is an important center of handloom weaving and coir manufacturing. (42 kms from ernakulam)\nA place of prayer and pilgrimage; made rich by the foot steps of St. Thomas the Apostle, who sowed the seeds of Christianity in this part of the world. (47 kms from Ernakulam )\nMarine Drive is considered to be one of the most beautiful part of Kochy city. The marine walk is the main hangout for the local populace as the view of the backwaters and the harbour from here is excellent. (Ernakulam)\nPallippuram fort was built by the Portuguese in 1503. It is one of the oldest existing European monuments in India. The Dutch captured the fort in 1661 and sold it to the State of Travancore in 1789 This fort is situated in the northern extremity of the Vypeen Island at Pallipuram\nParikshit Thampuran Museum\nThis museum features collections of the 19th century paintings , Pre-historic monuments , Old coins in a numismatic gallery,Scriptures in stone & Plaster of paris, Copies of mural paintings etc & the collection from the Cochin royal family. adjacent to the Shiva temple in Darbar Hall Road, Ernakulam\nSanta Cruz Bascillica\nThis Roman Catholic church is situated near the St.Francis church and is worth a visit.It is a specimen of portugese architecture built in 1503. 'Frescoes'and mural paintings decorate the ceiling and interior parts of the church.\nThe first European church in India. Originally built in wood and named Santo Antonio in the 16th Century. Vasco da Gama was buried here in 1524.\nThe 'Vamanamoorthy' temple has notable inscriptions dating back to the 10th-13th century. ( 8 Kms. from Kochi )\nA manmade island named after Lord Willingdon a former British Viceroy to india. Cochin Harbour, Southern Naval Command, Best hotels in the city, Port Trust head quarters, major trading centers etc. are situated at Willingdon Island. (Connected to the mainland by road-cum-railway Venduruthy Bridge )","|District of Maharashtra|\nLocation of Sindhudurg district in Maharashtra\n|Administrative division||Konkan Division|\n|Tehsils||1. Dodamarg, 2. Sawantwadi, 3. Vengurla, 4. Kudal, 5. Malvan, 6. Kankavli, 7. Devgad, 8. Vaibhavwadi|\n|• Lok Sabha constituencies||1. Ratnagiri-Sindhudurg (shared with Ratnagiri district) (Based on Election Commission website)|\n|• Assembly seats||4|\n|• Total||5,207 km2 (2,010 sq mi)|\n|• Density||170/km2 (430/sq mi)|\n|• Sex ratio||1079|\n|Average annual precipitation||3,287 mm|\nSindhudurg (Marathi: सिंधुदुर्ग जिल्हा) is an administrative district in the state of Maharashtra in India, which was carved out of the erstwhile Ratnagiri District. The district headquarters are located at Oros (ओरस). The district occupies an area of 5207 km² and has a population of 868,825 of which 9.47% were urban (as of 2001).\n- 1 Origin of name\n- 2 Statistical Details\n- 3 Transport & Communication\n- 4 Agriculture Details\n- 5 Irrigation\n- 6 Education Section\n- 7 Banking Sector\n- 8 Fisheries\n- 9 Location\n- 10 Climate\n- 11 People\n- 12 Cities & Towns\n- 13 Cuisine\n- 14 Places of attraction\n- 15 Demographics\n- 16 Divisions\n- 17 Transportation\n- 18 References\n- 19 External links\nOrigin of name\nThe district is named after the fort of Sindhudurg (which means \"fort in the sea\"), which lies on a rocky island just off the coast of Malvan. Sindhudurg fort, built in the 16th century by Shivaji is the only fort which has Shivaji temple inside the fort and palm imprint of King Shivaji. Sindhudurg district has 37 forts, the highest number of forts in Maharashtra as well as all types of forts (Jaldurg – Sea), (Bhuikot – fort on land) and (Giri- fort on hilltop).\nPopulation 868,825 Male 417,890 Female 450,935 Literacy 80.30% Male 90.30% Female 71.20% Density 167 Per km2. Sex Ratio 1079 ( For 1000 Male)\nThe area is largely rural populated with 91% of rural population of the total population of 868825.\n- Panchayat Samiti:\n- Nagar Palika:\n- Gram Panchayat: 433\n- Tot. Villages: 743\n- No. of Towns: 5\n- Police Stations: 9\n- Police outpost: 23\nTransport & Communication\nTotal Railway track 103 km Villages Connected by roads 743 Total Road Length 4640 km National Highway 108 km State Highway 668 km Dist. Roads 1473 km Village Roads 2391 Railway stations- (7) Vaibhavwadi, Nandagav, Kankavli, Sindhudurgnagari, Kudal, Sawantwadi, Madura\nMajor Crops Rice, Coconut, kokam, Mango, Cashew Annual Crop kokam, Mango, Cashew Irrigated 33,910 Hector Non-Irrigated. 104,390 Hector Forest 38,643 Hector 74% of total land holding in the district, are held by small and marginal farmers. The irrigated area is only 23.48% through well and small channels.\nMajor Projects 2 (Tilari & Talamba) Medium Projects 4 Small Projects State owned : 33, Z.P. owned : 460\nPrimary Schools Zilla Parishad – 1469, Private – 49 Secondary Schools Grantable : 184, Central Govt. : 1, Private : 22 Junior Colleges 43 Senior Colleges 7 D.Ed./ BEd Colleges 4 + 1 Medical Colleges 2 Engineering Colleges 1 Polytechnic Colleges 1 Industrial Training Institutes (ITI)- (7) 1.Sawntwadi 2.Malvan 3.Deogad 4.Sindhudurgnagari 5.Vengurla 6.Phondaghat 7.Vaibhavwadi\nNationalised Banks 66 Branches Cooperative Banks 106 Branches Rural Banks 15 Branches\nSea Coast Length 121 km Fishing Area 16000 km2. Main Fisheries Centers – (8) Vijaydurg, Devgad, Achara, Malvan, Sarjekot, Kochara, Vengurla, Shiroda Fisherman Population 25365 Total Fish Production 19273 M. Tons Fisheries Co.Op. Soc. 34 (Total Members 14216)\nSindhudurg is bordered on the north by Ratnagiri District, on the south by the state of Goa, on the west by the Arabian Sea, and to the east across the crest of the Western Ghats or Sahyadris is Kolhapur District. Sindhudurg is part of Konkan (coastal) region, a narrow coastal plain in western Maharashtra which lies between the Western Ghats and the Arabian Sea.\nSindhudurg has a semi-tropical climate and remains warm and humid in most of the year. It has three clear seasons : Rainy (June – October), winter (November-mid February) and Summer (mid February–May). Temperatures vary between Max. 32 °C and monsoon winds bring heavy rains (average rainfall 3240.10 mm).\nCities & Towns\nCities in the district include:\nSmaller towns include:\nThe cuisine of the district is popularly known as Malvani cuisine. Coconut, Rice and Fish assume prime significance in the Malavani cuisine. Seafood containing fish, especially Bangada (Soloman) Paplet (Pomfret), Prawns, Bombil (Bombay Duck) and Tisrya (Mussels) is very popular. \"Kombdi Vade\", a chicken savoury, is the most popular dish here. Others include Ukadya Tandulachi Pej (उकड्या तांदळाची पेज – a semi-fluid boiled preparation made of brown-red rice variety) and Sol Kadhi (सोल कढी – A preparation made of Sol (Kokum) सोल and coconut milk). Dry fish is also a local delicasy like \"Golma\" (dried prawns).\nMalvani cuisine is one of the unique cuisine than rest of the Maharashtra with very low oil and spices but very testy with use of locally available spices. Even TAJ group has also included Malvani cuisine in their menu. Below are some of the famous dishes/ sweets are Malvani\n- Kombadi vade (Puris made of rice atta)\n- Ghavane – Ras\n- Amboli – Usual\n- All types of fry fishes and fish curries in Malvani masala\n- Ukadiche Modak in Malvani style\n- Olya Kajuchi Usual\n- Pitla Bhat with Suko Bangdo\n- Ukdya Tandlachi Pej with Narlacha khobra\nMango is a major factor to the life of Sindhudurg. Varieties of Alphonso Mango (हापुस आंबा ) from Devgad are particularly popular. Other varieties of mango: Mankur (मानकुर), Pāyari (पायरी) and Karel (करेल – used for preparing Mango Pickle) are also popular for their distinct taste.\nThe Malvani cuisine also has many vegetarian dishes, including garyache sandan, pickle of karmal, bimble, amba halad, karadichi bhakri, kanyacha sanja, appe, ghavan, dalimichi usual, and kaju usual, Raiwal Ambyacha Rayta, Yelapp.\nPlaces of attraction\n- Tilari Dam (Dodamarg)\n- Vengurla Website\n- Redi Ganesh Vengurla\n- Navadurga Temple at Redi\n- Navdurga Redi\n- Amboli Hill Station Sawantwadi\n- Sindhudurg Fort Malvan\n- Vijaydurg Fort Devgad\n- Dev Kaleshwar Temple, Nerur\n- Kunkeshwar temple, Devgad\n- Lakshminarayan Temple, Walwal\n- Shri Bramhanand Swami Math, Ozar (Taluka Malvan)\n- Shri Sai Baba Temple (First and oldest temple of Saibaba in India), Kudal\n- Napapne Waterfall, Vaibhavwadi\n- Bharadi Devi temple, Aangnewadi\n- Achara Beach and Rameshwar Temple (16th Century)\n- Shree Dev Rameshwar Temple, Achara\n- Bhalchandra Maharaj Ashram, Kankavli\n- Mangeli waterfall (Dodamarg Taluka – near Goa)\n- Amboli hill station near Sawantwadi\n- Shri Dev Rameshwar temple (17th Century) in Aakeri, Sawantwadi\n- Shri Dev Rameshwar temple (15th Century) in Girye Rameshwar, Vijaydurg\n- Shri Dev Kaleshwar temple, Nerur (Kudal)\n- Shri Dev Kudaleshwar Temple, Kudal\n- Shri Dev Laxmi Narayan, Walawal (Kudal)\n- Shri Devi Mauli Temple, Walawal (Kudal)\n- Shri Devi Yakshini Temple, Mangaon (Kudal)\n- Shri Dev Rameshwar Temple, Humarmala- walawal (Kudal)\n- Shri Devi Sateri Shantadurga Temple, Mhapan (Vengurle)\n- Shri Dev Siddheshwar Temple, Mhapan (Vengurle)\n- Shri Dev Adnarayan Temple, Parule (Vengurle)\n- Shri Devi Chamundeshwari Temple, Aandurle (Kudal)\n- Shri Dev Vetoba temple, Parule (Vengurle)\n- Shri Dev Kshetrapal temple, Parule-chipi (Vengurle)\n- Shri Dev Maruti Temple, Kudal City\n- Shri Dev Vetal Temple, Pendur (Kudal)\n- Shri Ganesh Temple at Sawarwad\n- Rock Garden at Malvan\n- Tarkarli Beach\n- Dhamapur Lake\n- Snorkeling and cuba dyving at Tarkarli (Malvan)\n- Shri Lingeshwar-Pavanadevi Mandir,Janavali(Kanakavli)\n- Shri Maooli-Ravalnath-Vetal-Bagwe Maharaj Samadhi, Masure.\n- Nivati (Mhapan – Taluka Vengurle)\n- Khavne (Mhapan – Taluka Vengurle)\n- Kondura (Dabholi – Taluka Vengurle)\n- Devbaug (Malvan)\n- Wayaangani (Vengurle)\n- Sagareshwar (Vengurle)\n- Achara (Malvan)\n- Mochemad, Aravali(Vengurle)\n- Mithbaon (Devgad)\n- Chivla, Rajkot (Malvan)\n- Bhogve (Vengurle) – you can see this beach in famous marathi movie 'Shwaas'\nSindhudurg Tour Guide Pvt Ltd is an innovative initiative launched to boost tourism in Sindhudurg district. The organisation trains youth from local communities to take various self employment opportunities in tourism industry. These trainees are later motivated to run businesses like tourist guides, tour operators, travel agency, rental cars, home stays, agro tourism, water sports and village tourism. More details about this initiatives can be found on www.sindhudurgguide.com.\nAccording to the 2011 census Sindhudurg district has a population of 848,868, roughly equal to the nation of Qatar or the US state of South Dakota. This gives it a ranking of 474th in India (out of a total of 640). The district has a population density of 163 inhabitants per square kilometre (420 /sq mi) . Its population growth rate over the decade 2001–2011 was -2.3 %. Sindhudurg has a sex ratio of 1037 females for every 1000 males which is second highest in Maharashtra, and a literacy rate of 86.54%.\nThe Sindudurg district is connected to state capital Mumbai by road through National Highway 17 ( NH-17 ) which is now renumbered as NH-66. This highway also connects district to neighbouring state Goa and Karnataka also. There are regular MSRTC and private luxury buses connecting to adjoining cities like Kolhapur (110 km away from Kanakavli City), Belgaum (90 km away from Sawantwadi City), Panaji – Goa (55 km away from Sawantwadi & Vengurle). Towns and major villages has good connectivity with Mumbai as major migrated population of district is located in Mumbai area. According to one source, there are roughly more than 120 luxury buses running daily towards Mumbai and suburbs. District is also well connected by Konkan railway to Mumbai, Thane,Goa and other parts of the country like Mangalore, Karwar Ernakulam, Thiruvananthapuram, Coimbatore, Tirunelveli, Hapa, Veraval, New Delhi, Jodhpur, Porbundar by Konkan rail. The main railway station on this line are Kudal, Kankavli and Sawantwadi. There are daily train services to these places. The nearest airport is Dabolim Airport in Goa which is very close for cities like Sawantwadi, Kudal and Vengurle.\n- \"District Census 2011\". Census2011.co.in. 2011. Retrieved 30 September 2011.\n- \"Kunkeshwar Temple and Beach | Sindhudurg\". Konkanonline.com. Retrieved 21 October 2013.\n- US Directorate of Intelligence. \"Country Comparison:Population\". Retrieved 1 October 2011. \"Qatar 848,016 July 2011 est.\"\n- \"2010 Resident Population Data\". U. S. Census Bureau. Retrieved 30 September 2011. \"South Dakota 814,180\"\n- Election Commission, Maharashtra – No. of Voters 1.8.2006\n- \"NH in state renumbered\". www.thehindu.com. Retrieved 9 October 2012.\n- Sindhudurg Guide Website\n- sindhudurg tourism Website\n- sindhudurg tourism\n- Sindhudurg district official website\n- Vengurla Website\n|Arabian Sea||Kolhapur district|\n|North Goa district, Goa||Belgaum district, Karnataka|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:e7fe3e79-9e2f-4d82-a8e8-1f99b1522ab4>","<urn:uuid:ded0be3c-6980-424b-b786-7e8f7d5d2a21>"],"error":null}
{"question":"What was different between US military presence in Philippines in 1898 versus 1990s?","answer":"In 1898, the US military was engaged in hostile actions against Spanish forces in the Philippines, with notable battles like the Battle of Manila Bay where US ships USS Olympia, USS Baltimore, and USS Boston shelled Spanish positions equipped with Ordóñez guns at Sangley Point. In contrast, by the 1990s, the US-Philippines relationship was based on mutual defense agreements. The US maintained military bases in the Philippines through the Military Bases Agreement until 1991, when the Philippine Senate rejected extending the lease period. After a brief cooling of relations, the relationship was renewed through the 1999 Visiting Forces Agreement (VFA), which allowed US personnel to visit the Philippines for bilateral military exercises, representing a shift from permanent basing to temporary joint training operations.","context":["In the aftermath of the Second World War, two important agreements that initially defined the legal parameters of US-RP security relations were signed. These are the Philippine-American Military Bases Agreement (MBA) and the Philippine-American Military Assistance Agreement (MAA). In furtherance of these agreements, the two countries also signed a Mutual Defense Treaty (MDT) on August 30, 1951. The treaty stipulates that “an armed attack in the Pacific Area on either of the Parties would be dangerous to its own peace and declares that it would act to meet the common dangers in accordance with its own peace and safety… and in accordance with its constitutional processes” (Mutual Defense Treaty between the Republic of the Philippines and the United States of America).\nThrough the MBA, the US initially maintained 23 military installations in the country, including the Clark Air Force Base and the naval installation in Subic Bay, for an initial lease period of 99 years. The MBA, however, was amended in 1979 and updated in 1983 to decrease the lease period to 25 years. The decision to extend the lease period for another 10 years was rejected in a landmark decision by the Philippine Senate in 1991, and the last ship sailed out of Subic Bay in November 1991.\nThe decision not to extend the presence of US military bases in the country resulted in a lukewarm relationship between the two countries. The relationship further suffered when the Supreme Court of the Philippines decided in 1996 that any joint military exercise requires the ratification of the Philippine Senate, thus leading to the suspension of all large-scale military exercises between the two countries. This, however, proved to be a temporary hiatus in the longstanding Philippine-US relationship. In February 1998, the Visiting Forces Agreement (VFA) was signed and eventually ratified by the Philippine Senate in 1999. The VFA stipulates the terms and conditions covering US personnel visiting the Philippines for bilateral military exercises. Compared with the earlier MDT, the VFA was treated as an executive agreement that need not to be ratified by the US Senate.\nThe Balikatan 16 Exercises taking place in the Philippines for two weeks this April is actually one of the many, probably already numbering to hundreds, of exercises between the Philippines and the US military. This year, about 5,000 US; 3,000 Philippine; and about 80 Australian defense personnel, with observers from 12 countries, will be participating in several Balikatan sites within major Philippine military camps, such as Crow Valley, Fort Magsaysay, Clark Air Field, Subic Bay, Palawan, and Panay. As a matter of fact, Balikatan 16 is the 32nd iteration of Balikatan that started as single service bilateral exercises between the Philippine Navy and the US Navy in 1981. By 1983, the Balikatan had expanded to become a combined/joint exercise involving the different branches of Philippine and US armed forces. Aside from Balikatan, a number of joint military exercises involving Philippine and US forces have been regularly conducted in the country. As listed in the Visiting Forces Agreement Commission’s website, there are about 20 of these joint military exercises/activities yearly. Some of these include the CARAT or Cooperation Afloat Readiness and Training (CARAT); Amphibious Landing Exercise or PHIBLEX; and the Pandagat, Lupa, Himpapawid, PALAH.\nMany of these military exercises, including Balikatan, were geared towards improving the interoperability of Philippine and US forces against external aggression. The objective is to improve tactics, coordination, and maneuvers against a hypothetical external threat (Ramos 2005). Some of the stated objectives of the Balikatan include improvement of the combined planning and combat readiness; interoperability through enhanced security relations; and the demonstration of US resolve to support the Philippines against external aggression. For the earlier cited CARAT, it has a tri-fold mission of enhancing regional cooperation; building friendships between the United States and nations involved; and strengthening professional skills at every level (GlobalSecurity.Org). According to Padua (2010), Balikatan is representative of several exercises that the Philippines conducts jointly with the US forces. And on this account, the specific objectives of the Balikatan can actually be used as a broad representation of the objectives of some of the routine exercises conducted between the two countries.\nAside from regular military exercises, special military assistance was also provided by the US to the Philippines. From 2001 to 2014, as part of the US’ global war on terrorism, a US Special Operations Forces (SOF) contingent dubbed as the Operation Enduring Freedom (OEF) – Philippines was stationed in Mindanao. This led to the creation of a Joint Special Operations Task Force (JSTOF-Philippines) with the objective of helping the Philippines defeat terrorist groups in the country. Through the JSTOF-P, assistance was provided in terms of: 1) operational advice and direct support to operations of Philippine military against various threat groups; 2) training and equipment provision that helped improve the Philippine military’s capability; and, and in partnership with Philippine military, 3) the conduct of extensive civil–military operations (CMO) and information campaigns in support of combat operations (Rand 2016).\nA cursory review of available archival data would show that the Philippines and the US have long been involved in series of bilateral military exercises. A few years after the signing of the Mutual Defense Treaty, the two countries acted as hosts to the 1957 SEATO (Southeast Asia Treaty Organization) Exercise called the “PhibLink” wherein around 5,000 combined personnel of US Marines and Philippine Army participated. In 1958, another SEATO Exercise called “Strongback” required the participation of about 500 aircraft. In 1962, still another SEATO Exercise was conducted in various locations around the country that involved around 37,000 troops, 78 ships, and 400 aircraft. Similar to the Balikatan of today, these exercises were designed to provide opportunities for forces of participating countries to develop mutual confidence in one another, gain familiarity with the equipment and operational procedures of each country, and develop further good will and friendship among the exercise participants. In the 1970s, there were also the Bayanihan military exercises involving the military elements of both countries.\nWith this backdrop of regular military exercises subsumed under the agreed upon protocols of the Visiting Forces Agreement, an Enhanced Defense Cooperation Agreement (EDCA) was negotiated by the representatives of the two countries and was finally approved in time for the visit of President Barack Obama in the country in April 2014. The US government’s policy of “Rebalance to Asia and the Pacific” strategy is also generally perceived as material to the expeditious approval of the EDCA.\nOne of the highlights of the EDCA is the provision allowing US troops, ships, and planes access to the facilities of the Armed Forces of the Philippines (AFP) to undertake high-impact and high-value security cooperation exercises. Since the negotiating parties of both countries considered EDCA as an executive agreement and not a treaty, they do not see it as requiring the ratification of the Senate of both countries. Many analysts (e.g. Baviera 2014, Amador and Pabeliña 2014) perceive that the recent aggressiveness of China in staking its claims in the South China Sea has something to do with the relatively quick approval of EDCA. As stated in the signed agreement, the objectives of the agreement include the pursuit of maritime security and maritime domain awareness. More important however is the provision to address the Armed forces of the Philippines’ short-term capability gaps and long-term modernization. EDCA would allow the US to upgrade Philippine facilities and infrastructure for joint use of Philippine and US forces in yet to be determined locations within the Philippines.\nThere are a number of identifiable international “push factors” for this invigorated military relations between the Philippines and the US. Foremost is the aggressive posturing of China and its unabated construction in the land/rock formations that the country controls within the Spratlys Group of Islands. The US, on many occasions, pronounced that they would ignore the Chinese imposed Air Defense Identification Zone (ADIZ) and would insist on the freedom of flight and navigation in the disputed Spratlys Island Group. Without doubt, these developments weighed heavily in the decision of the US to implement the Rebalance to Asia and the Pacific Policy, and ultimately push for the approval of EDCA with the Philippines.\nAs a treaty ally of the US, the Philippines will have an important role in this policy, primarily as a host for the larger US presence in the region and the stronger military relations brought about by the decades-long bilateral military relations between the two countries. Thus, the concomitant push and the eventual approval of the EDCA did not come as a surprise. Although approved in 2014, the Supreme Court of the Philippines only decided with finality on the legality of the EDCA in January of this year. Since then, representatives of the two countries have identified the initial basing facilities in the following areas: 1) Antonio Bautista Air Base in Puerto Princesa, Palawan; 2) Basa Air Base in Pampanga; 3) Fort Magsaysay in Nueva Ecija; 4) Lumbia Air Base in Cagayan de Oro; and 5) Mactan-Benito Ebuen Air Base in Cebu.\nMeanwhile, on the domestic front, there are also several endogenous push factors that encouraged the Philippines to pursue an even “closer” bilateral relation with the US. For one, the country is still confronted with insurgency and the threat of terrorism on multiple fronts. The communist inspired New People’s Army, operating in many parts of the country, still presents a significant security threat. Down south, in Mindanao, risks are posed by remnants of the Moro National Liberation Front (MNLF) and the breakaway group, Moro Islamic Liberation Front (MILF). An MILF splinter group, the Bangsa Moro Islamic Freedom Fighters (BIFF) is another dangerous band to contend with in the region. Then there is the more radical terrorist group with known Al-Qaeda links, the Abu Sayyaf Group (ASG). The ASG is concentrated mainly in the islands of Basilan and Jolo and was responsible for numerous kidnap for ransom operations and other terrorist activities in the area.\nThese domestic security risks plus the maritime dispute issue with China have resulted in a renewed thrust for the AFP’s Modernization. For a country with limited economic resources and a host of socio-political challenges, modernizing the military was never a topmost priority. In recent times, there have been two attempts to modernize the country’s military. First was during the term of President Fidel Ramos by virtue of the 1995 AFP Modernization Act, and again, in 2013, when President Benigno S. Aquino II signed into law the Revised AFP Modernization Act. The first was considered a dismal failure, while the second, expected to be implemented in three waves, is already beset by problems.\nThe longstanding bilateral military relations between the Philippines and the US are characterized by hits and misses. For sure, there were perturbations (i.e. lukewarm relations in the aftermath of the 1991 cancellation of US bases) in the overall bilateral relations of the two countries. At the same time, such can also be characterized as having a semblance of continuity. With the Mutual Defense Treaty signed in 1951 still in effect, the Philippines remains one of the closest allies of the US within the ASEAN region. Proof of this is the $647 million military and defense assistance provided by the US to the Philippines between 2002 and 2014 with amount ranging from $37 M to $77M per year.\nWith the decades-long military relations between the Philippines and the US, the renewed interest to forge closer military ties between the two countries can be seen as a critical juncture for the militaries of the Philippines and the United States, an occasion to rethink what each can bring into the negotiation table that will define the future relations between the two countries. Following the historical-institutionalist perspective, the confluence of international (exogenous) and domestic (endogenous) socio-political developments opens a space for the militaries of both countries to redefine their long-standing relations, and to reexamine the strengths and weaknesses that each party can contribute to the military equation within the region.\nIt will be difficult to stop the momentum that defined the earlier character of Philippine – US relations. However, these bilateral relations, including military ones, do not necessarily have a locked-on mechanism that hinders any possibility of a redefinition. At a critical juncture, there is a certain amount of fluidity, a sort of relaxation in the rules of the game that can be utilized by the militaries of both countries to recalibrate the terms of their relations with one another. While both the Philippines and the US are preoccupied with similar global security issues and share a lot of common security interests, each country should be guided by their own unique national interests.\nThe Philippines, for one, should be able to translate the existing national security policy into a set of more specific national targets to effectively integrate the military and civilian development agenda that should be pursued in both the short and long-term periods. As pointed out by the Center for International Strategic Studies (2016), Philippine defense establishments are not known for sound long-term planning. There has to be a national security agenda to effectively plan and monitor the development targets of the country.\nThis national security plan should also be the basis for negotiating bilateral relations even with the country’s closest allies, such as the United States. These national objectives should help define the country’s terms of engagement with other nations. A clear plan with specific targets is the most efficient mechanism to evaluate whether or not targets have been reached, including those of the AFP’s modernization. And while “minimum credible defense” is a very attractive rallying point for the members of the military, there is a need to flesh out what such a phrase actually entails. There should be well defined parameters to measure the concept of “minimum credible defense” against whom it is directed and in terms of how it can be achieved.\nAn assessment made by the Center for International Strategic Studies (CSIS) in 2016 bluntly pointed out the AFP’s inability, for now and in the foreseeable future, to stand up against Chinese coercion in the West Philippine Sea. All points indicate that it would take decades of modernization before the AFP’s capability would be anywhere near that of China, or for that matter, the militaries of the country’s closest neighbors such as Indonesia, Malaysia, and Vietnam. CSIS suggested further that the AFP’s modernization would be difficult without any assistance from the US, an opportunity explicitly provided for in the EDCA.\nWhile the assessment is most realistic, the plan to modernize the Philippine military should be clearly anchored in the country’s unique national interests. The selection of equipment to be purchased, the physical infrastructure development to be pursued, and the type of training for the next cohort of marine and ground forces of the AFP should not merely be dictated by opportunities provided by the country’s bilateral partners. The selection and decision must be based on the unique long-term development goals of the AFP. While maritime disputes like the one unfolding in West Philippine Sea is critical, domestic security problems should not be relegated to the back burner. At the very least, equal importance should be given to both. This should include the search for long-term solutions that may or may not be military in nature.\nIn the end, global maritime disputes such as the one taking place in the West Philippine Sea will ultimately be played out by the big powers. Now is the age of small wars and asymmetric warfare. Global warfare is almost always carried out by multi-nation forces. Battling a multi-pronged insurgency for almost half a century now, the AFP had no choice but to develop expertise on counter insurgency warfare. This is a strength that the AFP can share with bilateral partners in the future. While military modernization is still a priority, AFP modernization should be defined by the conditions in the country and the aspirations of the Filipino people. In short, by the country’s unique national interests.\nFeatured image taken from this site.","Ordóñez guns are a type of coastal artillery that Salvador Diaz Ordóñez, an artillery officer in the Spanish Army, designed in the late 19th Century. Most of the models were guns, but some were howitzers. The guns ranged in caliber from 150mm (5.9\") to 305mm (12\"). They were made in Spain, at the Trubia Arms Factory (Fábrica de armas de Trubia), in Asturias, and the Spanish installed them in forts and batteries at home, for instance at Ceuta, and throughout their empire, in Puerto Rico, Cuba, and the Philippines. The Ordóñez guns appear to have been used for protecting Spain's colonies; reportedly the Spanish generally reserved the higher quality, and much more expensive, Hontoria guns for the defense of Spain.[Note 1]\nAlthough they have been obsolete for more than a century, a few Ordóñez guns have survived to the present as historical artifacts. There is one at Santa Clara Battery in Havana, a second, heavily damaged by the explosion of a shell, and brought from Subic Bay, at the Presidio of San Francisco, and a third at Castillo de San Cristóbal (Puerto Rico).\nThe guns were rifled breech-loading weapons with a cast iron body, hooped with wrought iron, and with a steel tube screwed in place that contained the breechblock and extended just forward of the trunnions. The Ordóñez guns captured at Havana were all 35 to 36 calibers in length.) The breechblocks were lever-actuated, and of the French or interrupted screw type, though the obturating ring followed the Krupp design. The guns appear to have been mounted en barbette, rather than on a disappearing carriage.\nA US Naval Intelligence report from 1892 described the Ordóñez guns as being less powerful than most other modern guns of equal calibers, but also much cheaper (because they were of iron rather than entirely steel). Comparison of the 305mm Ordóñez guns captured at Havana with the US 12\" all-steel naval gun found that the Ordóñez guns had a longer though narrower powder chamber that held less powder. As a result, the Ordóñez guns threw a lighter shell with less velocity to a shorter range than the US 12\" gun.\nThe Americans captured guns of 150mm (5.9\") at Havana, Manilla and Puerto Rico, 240mm (9.45\") guns at Havana and Manilla, and 305mm (12\") guns at Havana. At Havana the Americans also captured Ordóñez 210mm (8.27\") howitzers. There may also have been 140mm (5.5\") and 280mm (11\") Ordóñez guns, though none at any place that the Americans captured.\nOrdóñez also designed the 1891 240mm coastal artillery breech-loading howitzer, which was 14 calibers in length. It could fire a 140 kg projectile to 9,000 meters. Some of the howitzers served in Spain, including four at a battery at Fort La Mola in Menorca, and some at Montjuïc Castle, Barcelona.\nIn 1896 Ordóñez designed another 240mm howitzer, this one 16 calibers in length and consisting of a tube and two sleeves. The howitzer was made of forged and tempered steel, with a de Bange interrupted-screw breech-block with six screw sectors. The howitzer also had a hydraulic recoil mechanism. It could fire a 200 kg shell 11,320 metres. The artillery factory at Trubia produced the first exemplars in 1903, but the howitzer was not ready for adoption for active duty until 1916, by which time it was obsolescent. Still, it went into service and by 1936 M1916 240mm howitzers armed several batteries around Spain. Four were at Ferrol in the Fuente Seca Battery, four at Cartagena at the Loma Larga Battery, which were moved in 1940 to Ceuta, and four each were in the Regana and Refeubeitx batteries on Mallorca. Lastly, eight of the howitzers were held in reserve at an artillery park. In April 1937 the army moved four of these howitzers by rail to Águilas. The four remaining howitzers were sent to Madrid where three were emplaced and one was converted to a railway gun.\nOn 7 May 1898, the Spanish lured the USS Vicksburg and the US Coast Guard cutter Morrill into chasing a Spanish schooner under the guns of the Santa Clara Battery at Vedado, Havana, Cuba. The battery, which was armed with two Ordóñez guns, amongst others, fired too soon on the US vessels, which were able to escape without taking a hit.\nOn 10 May 1898, Captain Ángel Rivero Méndez ordered Castillo San Cristóbal's guns to fire on the USS Yale; the guns fired two poorly aimed shots, both of which fell far short. These shots marked Puerto Rico's entry into the war. On 12 May the US Navy warships conducted a day-long bombardment of San Juan. The U.S. Navy had more and larger guns than the Spanish. The battleships, cruisers and monitors carried four 13\", four 12\", eight 10\", twelve 8\", and four 6\" guns, in addition to many smaller pieces. Fort San Cristobal had two 150 mm (5.9\") Ordóñez guns and two 240 mm (9.45\") Ordóñez howitzers, Castillo San Felipe del Morro, which apparently fired the first shot, had five 150 mm Ordóñez guns and two 240 mm two Ordóñez howitzers, the San Antonio battery had four 150 mm Ordóñez guns, the San Fernando Battery had four muzzleloading 210 mm (8.3\") sunchado (or zunchado, meaning wrapped or banded) howitzers, the Santa Elena battery had three more, the San Agustin battery had three almost as obsolete 150 mm sunchado guns, and the Santa Teresa battery had three 150 mm Ordóñez guns. The Navy fired 1,362 shells whereas the Spanish fired only 441 rounds. Even so, military casualties were very light on both sides; civilian deaths exceeded combined military deaths by one.\nTwo 150 mm Ordóñez guns were in place in a battery at Sangley Point, which the USS Olympia, USS Baltimore, and USS Boston, shelled during the Battle of Manila Bay. Four more of these 150 mm guns were to be mounted at a battery at Subic Bay but had not yet been at the time of the battle. Filipino freedom fighters resisting the US colonization of the Philippines later moved one of these to a battery they constructed there.\nIn September 1899, US forces attacked the battery at Subic Bay, where shells from the USS Charleston knocked the Ordóñez gun in the battery out of action. William Randolph Hearst acquired this gun and presented it to the City of San Francisco where it was on display at Columbia Square Park until 1973, when it was moved to the Main Post of the Presidio of San Francisco.\nPossibly the last action for any Ordóñez piece occurred in 1937 when two of the M1916 howitzers at Madrid participated, on the Republican side, at the Battle of Brunete during the Spanish Civil War. The Nationalists did deploy an armored train with \"a huge artillery gun\", at the Battle of Teruel, but it is not clear whether the gun in question was the Ordóñez M1916.\nNotes, citations, and references\n- Journal of the United States Artillery, (1898), Vol. 10, p.143.\n- Shull (1901), p.130.\n- United States Office of Naval Intelligence, (1892), p.90.\n- United States Office of Naval Intelligence, (1899) pp.35-6.\n- Schull (1901), p.132-4.\n- United States, Office of Naval Intelligence (1892), p.90.\n- Schull (1901), pp.137-8.\n- Congress (1902), Vol 4285, pp.648-51.\n- New York Times, 8 May 1798\n- Anderson, (2009) pp.38–39.\n- Eby (2007), p.276.\n- Anderson, Gerald R., Subic Bay from Magellan to Pinatubo: The History of the U.s. Naval Station, Subic Bay. Gerald Anderson; 2009. ISBN 978-1-4414-4452-3.\n- Eby, Cecil D., (2007) Comrades And Commissars: The Lincoln Battalion in the Spanish Civil War. (Penn State Press). ISBN 9780271029108\n- Schull, Lieut. Herman W. (1901) \"Spanish Ordnance in the Defense of Havana\". Journal of the United States Artillery, Vol. 15, No. 2, Whole No. 48, pp. 129–146.\n- United States Congress (1902) Congressional edition, Volume 4285. (U.S. G.P.O.).\n- United States, Office of Naval Intelligence (1892) Information from abroad. (Govt. Print. Off.).\n- United States, Office of Naval Intelligence (1899) Information from abroad: War notes, Issues 1-8. United States. 56th Cong., 1st sess., 1899. (Govt. Print. Off.)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:be0a0e0b-67ce-49ae-9f33-45c1a9bcdbd1>","<urn:uuid:c256c892-9cfc-4623-a226-4d513d16b928>"],"error":null}
{"question":"How do the turkey cooking guidelines compare to general meat thermometer usage recommendations?","answer":"Both sources emphasize the importance of using a meat thermometer for safety, stating that visual indicators are unreliable. For turkey specifically, it should be cooked at 325 degrees F for 15-18 minutes per pound, with the thermometer placed in the thickest part of the thigh reaching 180 degrees F. The general guidance confirms that the thermometer should be placed in the thickest part of any meat, away from bone, fat, or gristle, as this is the only way to ensure safe cooking temperatures.","context":["Food Safety for the Holidays\nWhen we think of the holidays, we usually think of family and friends\ntogether having fun and lots of good food. We don't want to think about\npeople getting sick. Unfortunately, many people do get sick at this time\nof year from the food they eat.\nDuring the holiday season, there may be several times when people could\nget sick from the food they eat. The foods that we are most likely to\neat, at this time of year, are no more likely to make us sick than foods\nwe eat at other times. The problems Wine from the way food is prepared,\nserved,, and stored. Often we have more people, are fixing larger amounts\nof food and serving it over a longer period of time than we are used to.\nSometimes we do not have enough space to store all of the food that needs\nto be refrigerated. Sometimes we get busy visiting and forget to put the\nfood away right after the meal.\nNo one wants to serve food that might make their guests sick. As you\nplan and prepare for the holidays this year, plan carefully to make sure\nyour food will be safe.\nHere are some tips to help you as you plan for holiday parties:\n1. On the serving table, never put fresh food into the same bowl that\nhas already had food in it. Remove the old bowl and put a clean bowl of\nfresh food in its place. Also, use a clean serving spoon.\n2 Do not partly cook food ahead of time and finish cooking later. Cook\nfoods until they are done.\n3. If foods are fixed early, keep refrigerated until time to eat.\n4. After the meal, take any leftover meat from bones and refrigerate\nin small containers. Use refrigerated leftover turkey and stuffing within\nthree to four days and gravy within one to two days.\n5. Cooked foods such as meat, turkey, chicken, stuffing, and dishes containing\neggs or dairy products should be at room temperature no more than a total\nof two hours.\n6. Turkey should be cooked at 325 degrees F. for 15 to 18 minutes per\npound. A meat thermometer put in the thickest part of a turkey thigh should\nread 180 degrees F. This will ensure that the turkey is cooked.\n7. Do not serve foods that have raw eggs. In eggnog and other recipes\nthat use raw eggs, egg substitutes may be used or better yet, look for\nrecipes that cook the eggs.\n8. Always thaw a frozen turkey in the refrigerator or in cold water (change\nthe water every thirty (30) minutes).\n9. Always work with clean spoons and counter tops. Wash your hands often\nwhen fixing and serving food.\nKeep these tips in mind to ensure that your holiday parties will be fun\nPrepared by: Barbara Farner, Nutrition\nand Wellness Educator, Kankakee Extension Center, Manteno\nNew Features | Home\nCare | Wellness | Consumer\nEconomics | Foods & Nutrition","Food Safety Tips\nBig Green Egg is committed to culinary quality and food safety. We encourage proper use of a high quality food thermometer to ensure that meat, poultry, seafood and other cooked foods reach a safe minimum internal temperature. Remember, you can’t tell whether meat is safely cooked by looking at it!\nWashing hands with soap and warm water before and after handling raw food is the best way to reduce the spread of germs and prevent food poisoning.\nThoroughly wash utensils, cutting boards, and countertops with soap and hot water. Rinse. They may be sanitized by applying a solution of 1 tablespoon of unscented, liquid chlorine bleach per gallon of water. Air-dry.\nWash fruits and vegetables thoroughly under running water just before eating, cutting, or cooking. Washing fruits and vegetables with soap or detergent or using commercial produce washes is not recommended.\n1 in 6 Americans will get sick from food poisoning this year.\n3,000 Americans will die. Keep your family food safer.\nKeep raw meat, poultry, eggs, and seafood and their juices away from ready-to-eat food.\nSeparate raw meat, poultry and seafood from produce in your shopping cart. Place food in plastic bags to prevent their juices, which may contain harmful bacteria, from dripping onto other food.\nAt home, put raw meat, poultry and seafood in containers, on plates, or in sealed plastic bags in the refrigerator to prevent their juices from dripping onto other food.\nUse a separate cutting board for raw meat, poultry and seafood.\nSauce that is used to marinate raw meat, poultry or seafood should not be used on cooked food, unless the sauce is boiled first.\nNever place cooked food back on the same plate that previously held raw food unless the plate has first been washed in hot, soapy water.\nColor and texture are unreliable indicators of safety. Using a food thermometer is the only way to ensure the safety of meat, poultry, seafood, and egg products.\nThese foods must be cooked to a safe minimum internal temperature to destroy any harmful bacteria.\nThe food thermometer should be placed in the thickest part of the food, away from bone, fat or gristle.\nTo learn more about Big Green Egg Thermometers Click Here.\nDownload the PDF\nThe temperature in a refrigerator should be 40°F/4°C or below, and the freezer 0°F or below.\nPerishable food should be thawed in the refrigerator, in the microwave or\nin cold water. They should never be thawed on the counter or in hot water. Do not leave food at room temperature for more than two hours (one hour when the temperature is above 90°F/32°C).\nMeat and poultry defrosted in the refrigerator may be refrozen before or after cooking. If thawed in the microwave or cold water, cook before refreezing.\nDivide large pots of food, like soup or stew, into shallow containers. Cut cooked meat or poultry into smaller portions or slices. Place in shallow containers, cover, and refrigerate.\nOnly buy eggs from a refrigerator or refrigerated case. Store eggs in the refrigerator in their original carton and use within 3-5 weeks.\nWhen selecting pre-cut produce choose only those items that are refrigerated or surrounded by ice and keep refrigerated at home to maintain both quality and safety.\nRaw milk and products made from raw milk (including certain cheeses, ice cream, and yogurt) are foods that can pose severe health risks. Raw milk and products made from raw milk can carry harmful bacteria and other germs that can make you very sick or kill you. At the grocery store, look for milk and milk products that are labeled “pasteurized” (which means the milk has been heated briefly to kill disease-causing germs). If you do not see the word “pasteurized” on the product label, the product may contain raw milk. Pasteurized milk and milk products are safer than raw milk and products made from raw milk.\nGet more info from FoodSafety.gov"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:d64c1882-a319-440c-bbce-22a415aa1667>","<urn:uuid:4e8c0f3d-9239-46bd-a50d-b3af488f177a>"],"error":null}
{"question":"What's the difference between replacement cost coverage for homes versus coverage for stolen vehicles?","answer":"For homes, replacement cost coverage protects against inflation of materials and labor costs, providing between 125-200% of the dwelling value limit to rebuild the home. For stolen vehicles, if not recovered, insurance will pay the covered amount of the car's value, while if recovered with damage, they will either pay for repairs or write it off for the covered value. Both require having the appropriate coverage type - replacement cost provision for homes and comprehensive coverage for vehicles.","context":["Well, the fun has just begun. You’re going to need homeowner’s insurance coverage to protect your investment. The problem with insurance, in most cases, is you have to get it when you don’t need it because when you need it, you can’t get it.\nMake sure the homeowner’s insurance policy you buy includes these four items:\n1. Appropriate liability limit\nHave you ever been sued and had to pay for an attorney out of pocket? I’ll bet you weren’t happy with the cost of that experience.\nWithin every homeowner’s insurance policy is a liability limit that will protect you if you are sued. The trick is that you have to decide how much liability coverage is enough.\nAsk yourself what your stuff is worth and how much it would cost to replace everything you have at today’s prices. If your liability limit is less than the value of your house or your retirement account or both, then you’re probably underinsured.\nAre you comfortable with that? Sometimes homeowners don’t want to pay for the right amount of homeowner’s insurance coverage, but if something happens and you have to liquidate assets to replace your belongings, is it worth it? And what happens if you don’t have enough money to replace the things you need, such as furniture, clothing, and electronics? The out-of-pocket expense of replacing these items will easily be a lot higher than the cost of the homeowner’s insurance premium for the proper amount of coverage.\n2. Updated dwelling value\nSince the housing bubble burst, there’s been a lot of controversy about homeowner’s insurance coverage for homes that have fallen in value.\nIn many places around the country, houses are selling for less than what people owe. In short, these properties are “underwater,” which isn’t a comfortable place for homeowners to be.\nSo why have some insurance companies increased the dwelling value limit on homeowner’s insurance policies?\nSome carriers incorporate a 2 percent automatic inflation guard to protect the insured value. Depending on how long you’ve had the same policy or how long it’s been since you reviewed it, it’s possible the dwelling value limit may be overinflated.\nAsk your agent to conduct a replacement cost estimator based on your home’s specifications (including square footage, the year the property was built, the number of bedrooms and baths, etc.). This will help determine an adequate value for your home so it is sufficiently insured in today’s real estate market.\n3. Replacement cost\nIf your home is completely destroyed in a fire, you’ll want to have the replacement cost provision included in your policy to protect against inflation of materials and labor to rebuild your home.\nDepending on the insurance carrier, the replacement cost could be between 125 percent and 200 percent of the dwelling value limit. Keep in mind, however, that despite the 2 percent automatic inflation guard, if the value of the home was not sufficiently insured initially, it could pose a problem in the event of a partial loss (which is more likely than a total loss of the property).\nIf the insurance carrier determines that the home was underinsured to begin with, the insured (that’s you!) would be responsible for coinsuring the loss.\nI’ve made that call, and, let me tell you, it’s not a fun discussion to have with your clients.\n4. Loss assessment\nLoss assessment is a provision within a homeowner’s policy for condominium owners. It provides coverage for damage to the common area as a result of a covered claim. For example, let’s say the hallway outside your condominium catches fire and needs to be repaired.\nIf the association assesses each unit owner for a portion of the claim, the loss assessment part of your homeowner’s insurance coverage is tapped. Most carriers will automatically include this coverage in a typical homeowner’s insurance policy, but if you haven’t reviewed your insurance in a few years, you may need to update it to include this provision.\nLet me remind everyone that insurance coverage was developed to protect against catastrophes. The first insurance contracts were out of London, protecting ships and their cargo if the ship sank. Insurance is not a maintenance contract, which is why wear and tear is excluded from homeowner’s insurance policies.\nIf you need a maintenance contract, buy an appliance with a warranty. If you want to protect your property from a physical catastrophe, be smart and buy the proper amount of homeowner’s insurance coverage.\nHave you asked your agent to review your homeowner’s insurance policy recently? You might find you’re eligible for additional discounts or need to change the amount of coverage.\nLinda Rey is a licensed insurance agent at Rey Insurance with a broad spectrum of expertise in life, accident, health, property and casualty insurance as well as retirement planning and college funding strategies.\nFollow Linda on Twitter.","Are items stolen from car covered by insurance? Third party car insurance covers you for damage, legal costs and repairs to other (third party) vehicles or property in an accident. Unless otherwise stated, third party car insurance doesn’t cover you for the cost of repairs or replacement for your own car, or the cost of replacing your car if it gets stolen.\nWhat insurance covers theft from car? Comprehensive insurance usually helps cover theft of the car itself, stolen car parts or damage caused by a break-in (such as broken windows or damaged door locks). Comprehensive coverage is typically required by your lender if you’re leasing or financing your vehicle.\nDoes car insurance cover items stolen out of car? Car insurance can provide peace of mind and cover for your belongings in case they’re stolen or damaged in a break-in.\nAre the contents of my car covered by insurance? Comprehensive auto insurance will only cover the components and features that are permanent, pre-installed parts of the car. It will not cover your own personal belongings left inside, such as an iPod or wallet. However, these items would likely be covered by a homeowners or renters insurance policy.\nTable of Contents\nAre items stolen from car covered by insurance? – Related Questions\nHow does insurance cover stolen items?\nHomeowners insurance may help cover theft and break-ins. Personal property coverage helps pay to replace or repair your belongings if they are stolen or damaged by a covered loss (including theft). If an intruder steals items from your home, personal property coverage may help pay to replace them.\nHow does car theft affect insurance premiums?\nDo High Car Theft Rates Increase Insurance Rates? Car insurers base all their calculations on risk. So a higher risk of theft = higher insurance rates. If you live in an area where car theft is common, then that can bump up your insurance premium — even if your own neighborhood is considered pretty safe!\nWill a theft claim raise my insurance?\nIf you have coverage for theft or damage, then there’s a good chance your premiums will increase after a claim. This is because the insurance company has no choice but to pay for the claim. There is no one else who can bear the costs.\nWhat happens if your car gets stolen insurance?\nIf your stolen car is not recovered, your insurance company may pay you the amount for which it’s covered. If your car is recovered with damage as a result of the theft, your insurance company will either pay to have it repaired or write it off and pay you for the value for which it’s covered.\nDoes insurance cover theft if your car door is unlocked?\nWe’ve often been asked about insurance coverage in the event of a vehicle theft if in fact the doors were unlocked and if the keys were left somewhere in the vehicle. The short answer is yes as long as you have comprehensive insurance coverage .\nDoes Geico cover items stolen from car?\nComprehensive coverage by Geico will cover non-accident related damage caused to your car, like theft, vandalism, flooding, fire, and natural or man-made disasters.\nHow do I find out my deductible?\nA deductible can be either a specific dollar amount or a percentage of the total amount of insurance on a policy. The amount is established by the terms of your coverage and can be found on the declarations (or front) page of standard homeowners and auto insurance policies.\nWhat happens if you don’t have receipts for insurance claim?\nReview your policy carefully; nowhere does it say a claim can be denied if you do not have a receipt for your personal property. Failure to have a receipt is not grounds for an automatic denial, but it could trigger a further investigation, including an examination under oath.\nWhat is covered under theft insurance?\nTheft-insurance contracts cover losses from burglary, robbery, and other theft. Aviation insurance usually covers physical damage to the aircraft and legal liability arising out of its ownership and operation.\nDoes homeowners insurance cover stolen money?\nHomeowners insurance and stolen cash and jewelry\nA homeowners policy can reimburse you for missing money, but only up to a certain dollar limit. Lost or damaged jewelry is another high value item that is limited in how much will be covered.\nHow claims affect insurance premiums?\nThe cost and severity of a claim are key factors when it comes to whether your insurance premium may increase. Auto insurers typically consider your driving record when calculating the cost of your car insurance policy. However, filing a claim doesn’t mean your insurance premium will automatically increase.\nDo I have to pay a deductible if my car is stolen?\nIf your car is stolen, you pay your deductible before your auto insurance pays you the loss. If your car is worth $2,500 and you have a $500 deductible, the most you can collect is $2,000. On the other hand, the higher your deductible, the lower your monthly premiums.\nHow much does your insurance go up after a claim?\nHow much does insurance go up after a claim? A single claim can raise your rates an average of 28%, according to one major insurer, but different claims are weighted differently, so a minor fender bender may not increase your premium the way a major at-fault accident might.\nWill my premium go up if I am not at fault?\nOne fear many motorists have after an accident is that their car insurance premiums will rise. Short answer: Actually, you don’t—provided you were not primarily responsible for the accident. Under California law, an insurer cannot increase your premiums when you aren’t at fault.\nDo I need to notify the DMV if my car is stolen?\n4. File a stolen vehicle report with the DMV. Now, you’ll need to report the situation to your state’s Department of Motor Vehicles (DMV). The DMV maintains a database of stolen cars and can help the police recover the vehicle faster if somebody comes in to register the car under their name.\nDoes a stolen car lose value?\nBuying a stolen and recovered vehicle can often times help you buy a newer vehicle with more options… all for less money than its clean-titled counterpart! Additionally, stolen and recovered vehicles depreciate at a slower rate than traditional vehicles because their value has already dropped.\nAre you insured if your door is unlocked?\nYour personal belongings are covered under your homeowner’s insurance. It doesn’t matter where your personal property is at the time it is stolen, your homeowner’s insurance will still cover it. If your belongings were stolen from your unlocked car, your homeowner’s insurance coverage is the one that would pay for it.\nWho can I call to unlock my car door for free?\nDial 911. Safety comes first; so don’t hesitate to call 911 if you think you’re in danger. In many cases, the police can unlock the car’s door. If they can’t, they will probably call a tow truck, which will be on your tab, of course.\nWhat does liability insurance cover on a car?\nWhat is liability coverage? Liability coverage pays for property damage and/or injuries to another person caused by an accident in which you’re at fault. This coverage is required by most states to legally drive your vehicle. Liability coverage is broken down into 2 parts: property damage and bodily injury.\nWhat counts towards a deductible?\nA deductible is the amount you pay for most eligible medical services or medications before your health plan begins to share in the cost of covered services. Depending on how your plan works, what you pay in copays may count toward meeting your deductible.\nCan you make an insurance claim without receipts?\nThat said, you can usually claim without the original receipt, as long as you have some other proof of ownership, such as a bank statement recording the purchase. If your insurer insists on seeing a receipt, try asking the retailer you bought the item from if it can give you another copy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:3aa5a37d-c887-4cd4-a224-f4d92a3683d0>","<urn:uuid:25c385cb-0724-4ef2-b43f-f82ad12b4233>"],"error":null}
{"question":"How does the financial impact of Hurricane Katrina compare to the Great Storm of 1987 in terms of economic damage? Which natural disaster had more severe monetary consequences for its respective region?","answer":"Hurricane Katrina caused significantly more financial damage than the Great Storm of 1987. Katrina resulted in an estimated USD 108 billion in damages, making it the costliest natural disaster in U.S. history. In comparison, the Great Storm of 1987 caused £7.8bn worth of damage and led to £50bn of market value destruction in the financial markets when combined with Black Monday. The impact of Katrina was also broader, affecting multiple states including Florida, Louisiana, Mississippi, Alabama, and parts of Cuba, while the 1987 storm primarily affected southern England.","context":["Intranets and business continuity: lessons from 1987\nThis week marks 25 years since the Great Storm of 1987, widely (but erroneously) considered the only UK hurricane in living memory. 23 people died in the storm, which also caused £7.8bn worth of damage, cut power to thousands of homes and drove transport to a halt as fallen trees blocked roads and train lines.\nBut one of the less-well-known consequences was in the financial markets. Severe travel problems across the south of England meant few traders managed to struggle into work in London on Friday 16 October 1987. Unfortunately this coincided with the Hong Kong and Tokyo stock markets crashing and financial crisis spreading west through Europe.\nMany analysts believe part of the blame for the speed, timing and impact of Black Monday was due to the storm which left London’s markets unable to function on that crucial working day. By the time London re-opened on the following Monday (19 October) there was mass panic selling of stocks, with the FTSE 100 closing 10.8% down. At the time it was the biggest one-day fall ever and destroyed £50bn of market value.\nIn 1987 the web was still a twinkle in Tim Berners-Lee’s eye, with the first web browser not emerging until three years later. Surely the same thing wouldn’t happen today? Enterprise technology means knowledge workers can access work systems from multiple locations and devices, right?\nBut it’s not quite that simple. The challenge for digital workplace professionals is to ensure their tools support business continuity plans effectively and work in the event of a crisis. So how do you do that?\nIdentify the business impacts\nAs part of their planning process, most organisations will have carried out Business Impact Analysis, which identifies the most crucial functions and the length of time the organisation can survive without them. Part of this process is focused on gathering technical or logistical requirements for each of these critical functions to be restored.\nIntranet managers have a key role to plan in this process, liaising between IT, business stakeholders and end users so that the planned solution isn’t just feasible, but it something that people can use and rely on.\nKey questions to ask include:\n- What are your vital systems – the ones your organisation simply couldn’t work without?\n- Who needs them?\n- How can they access them?\n- What information do they need to use these? As well as access to business systems themselves, people may also need to access supporting information or documentation, particularly if they’re using these in usual or exceptional circumstances.\nKnow your threats\nWeatherman Michael Fish famously dismissed suggestions that a hurricane might be heading our way. Don’t be so dismissive, or you too might end up a laughing stock, to be ridiculed for decades to come. Alongside the impact analysis, your business continuity planners will also have undertaken some threat and risk analysis. This considers the likely threats, how they might be eliminated or minimised, and what specific steps might need to be considered to recover in the event of a particular event or set of circumstances.\nAs part of your intranet business continuity plan, you should consider possible threats and what might need to change on the intranet in order to recover from this. Remember, this might include your own non-availability; who will manage your intranet and other critical systems in the event of your absence?\nEstablish and implement a plan\nYou have identified the business impact and threats and now it is time to create your intranet business continuity plan. Be realistic, your plan may document specific scenarios, remember to establish an overall approach in order to help you, your team and your colleagues react to a number of situations.\nInvolve key stakeholders to test your plan. Walk them through the potential risks and impacts to the business and your approach to each one. The success of a business continuity plan relies heavily on your colleagues. Make sure they understand and agree with your plan.\nTest your plan\nNo one had considered the impact of a single day’s widespread absence following the 1987 storm, and how business-critical it was for the workforce to be there. A simulated exercise will help you test your business continuity plan, revealing weaknesses and the potential risks to your business. Running through a test exercise will help to uncover any holes or gaps, or where people do not have the information they need.\nTest your technology\nDon’t wait until a real incident to check your equipment works. Ensure your priority teams have a real drill in re-located or home working so that if disaster strikes they’re confident their technology works.\nRemember that in a real emergency everyone will be accessing remotely, so ensure your testing doesn’t just prove everyone can log on, but that they can all log on at the same time. When the UK Civil Service planned for the London Olympics this year, they did real, fortnight-long exercises involving the full workforce to ensure their systems were resilient enough to support large numbers of people working remotely for extended periods. In doing this, they could be confident their systems could support both short-term transport failures and any unexpected but more catastrophic incidents too.\nThink about what would be important in an emergency situation\nWhat are the few things that become vital if staff were forced to work from home or from an unfamiliar office? Which departments or teams might need to call in extra staff or support? How might your homepage change to support these things in a business continuity plan? Do people have permission to edit the page in emergencies?\nUse this to your advantage\nAll too often, the crucial role the intranet can play in business continuity only becomes clear when it is tested in a real crisis. In fact, the vital role the digital workplace plays in avoiding and recovering from catastrophe is one of the strongest reasons to invest in usable and resilient technology. Your intranet is what could stop your business going down the pan in the event of a crisis. This should definitely be in your next business case for intranet investment.\nMaintain, measure & evaluate\nNever ignore your plan until you need it. Always review your plan, have others review your plan and ensure it is up to date. It may be useful to hold biannual or annual review sessions with your team and key business stakeholders. These meetings should confirm the plan is still factually correct, that employees identified in the plan are trained and are aware of their role and that there has been a yearly test conducted to evaluate the plan.\nRemember, a successful intranet business continuity plan follows the highly effective ‘Plan-Do-Check-Act’ model. If you are struggling to create your plan, break it down into easy to manage pieces. You can always start small and grow from there. If you are still struggling, you can always buy the business continuity standard.","Natural disasters are unpredictable, deadly, and destructive. One cannot foretell the amount of loss that it may cause. ScienceStruck describes the worst natural disasters that have occurred in the United States.\nDid You Know?\nHurricane Katrina, with an estimated damage of USD 108 billion, was the costliest natural disaster to have occurred in the United States. It was also one of the deadliest disasters, having killed 1,833 people.\nOver the years, the United States has witnessed numerous disasters, both man-made and natural. Although the man-made disasters are equally destructive, it is the rage of Mother Nature that sends chills down our spine. These massive tragedies claim thousands of lives, wipe out entire cities, cost billions of dollars, and leave millions of families devastated.\nThe natural disasters strike us in the form of floods, tornadoes, hurricanes, earthquakes, wildfires, droughts, etc. Although it is very difficult to claim the deadliest ones, here are the worst of the lot based on their intensity, loss of lives, and cost of property damage caused due to these disasters.\nDEADLIEST DISASTERS OF UNITED STATES\nThe 1994 Northridge Earthquake\nDate: January 17, 1994\nDamage: USD 23 billion\nAreas affected: Greater Los Angeles\nThe Northridge earthquake first hit the San Fernando Valley of Los Fernando Valley region of Los Angeles, California on January 17, 1994 at 4.31 am. The magnitude of the earthquake was very strong (6.7 Mw), and was felt up to 220 miles of the epicenter. Along with San Fernando, the cities of Santa Clarita, Santa Monica, and Simi Valley were also largely affected.\nThe peak ground velocity of the quake was 4.09 mph, which was the fastest one ever recorded. The earthquake lasted about 10 – 20 seconds, in addition to which two aftershocks occurred. The massive earthquake caused a loss of 57 lives, and injured more than 8,700 people. The estimated damage was USD 23 billion.\nThe 1906 San Francisco Earthquake\nDate: April 18, 1906\nDamage: USD 400 million\nAreas affected: Greater Los Angeles\nThe 1906 San Francisco Earthquake hit the coast of Northern California and San Francisco, on April 18, 1906 at 5.12 am. The epicenter of the earthquake was 2 miles from San Francisco, near Mussel Rock. The magnitude of the earthquake was 7.8 Mw and its impact was felt from Oregon to Los Angeles. The main shock of the earthquake lasted for 42 seconds.\nIt destroyed 80% of San Francisco, most of the Bay Area, and the nearby cities of Santa Rosa and San Jose. After the quake, enormous fires broke out in San Francisco. Within three days of the quake, over 30 fires had broken out, destroying 25,000 buildings in the city. Since the water mains were also broken, there were few resources to extinguish the fire―the reason why it lasted several days. As many as 3,000 people were killed and 300,000 were left homeless as a result of the earthquake and the subsequent fires. The estimated damage caused at the time was USD 400 million.\nDate: August 29, 2005\nDamage: USD 108 billion\nAreas affected: South Florida, Louisiana, Mississippi, Bahamas, Alabama, Cuba, eastern North America\nHurricane Katrina was one of the most deadliest as well as costliest natural disasters to hit the US soil. Hurricane Katrina first formed over Bahamas on August 3, 2005 as a Category 1 hurricane, and strengthened to a Category 5 hurricane over the Gulf of Mexico.\nIt ultimately hit southeast Louisiana as a Category 3 hurricane on August 29, 2005. The highest wind velocity of the hurricane was 175 mph. Around 1,833 people died in the hurricane and the subsequent floods, making it one of the worst natural disasters of the US. The hurricane caused a lot of destruction to Florida, Cuba, Mississippi, Louisiana, southeast USA, as well as Canada. It caused an estimated damage of USD 108 billion, the costliest one on record.\nThe New England Hurricane of 1938\nDate: September 21, 1938\nDeaths: Around 800\nDamage: USD 306 million\nAreas affected: Bahamas, Long Island, Connecticut, Rhode Island, New Jersey, New York, Massachusetts, Vermont, New Hampshire, Maine, southwestern Quebec\nThe Great New England Hurricane formed near the coast of Africa, and made its landfall on Long Island on September 21 as a Category 3 hurricane. It was centered 100 miles east of Cape Hatteras, with a forward speed of 50 mph.\nThe highest recorded wind velocity of the hurricane was 160 mph. Around 800 people were killed due to the hurricane, and 708 were left injured. The hurricane also damaged a total of 4,500 cottages, 25,000 homes, 26,000 automobiles, and 20,000 electric poles. The hurricane caused a damage of around USD 306 million.\nThe Galveston Hurricane of 1900\nDate: September 8, 1900\nDeaths: 6,000 – 12,000\nDamage: USD 20 million\nAreas affected: Mississippi, Louisiana, Texas, Oklahoma, Kansas, Jamaica, Cuba, Turks and Caicos Islands, Bahamas, Lesser Antilles, Puerto Rico, Hispaniola, Florida, Nebraska, Iowa, Illinois, Wisconsin, Michigan, New York, Eastern Canada\nThe Hurricane of 1900 made its landfall on September 8, 1900 as a Category 4 hurricane with a wind velocity of 145 mph.\nThe hurricane was first detected on August 27, over the Atlantic Ocean, after which it reached Cuba as a tropical storm on September 3. It then moved into the Gulf of Mexico on September 5, before it finally made its landfall on September 8.\nThe hurricane caused major destruction to Midwest, New York City, and Galveston. At the time of the hurricane, the highest point in the city of Galveston was 2.7 m above sea level. The hurricane, which caused a storm surge above 4.6 m, washed out the entire island. Due to this, around 20% of the population of the island was killed and over 3,600 homes in the city were destroyed. The highest measured speed of the winds of the hurricane was 145 mph. The hurricane caused a damage of USD 20 million at that time.\nDate: October 25, 2012\nDamage: USD 68 billion\nAreas affected: Bahamas, Greater Antilles, Bermuda, eastern Canada, eastern United States\nHurricane Sandy, also known as the Superstorm Sandy, was a Category 3 hurricane that first hit Cuba on October 25. It had first formed over the western Caribbean Sea on October 22, and gradually strengthened as it moved toward Greater Antilles.\nOn October 24, it became a hurricane, and made its landfall in Jamaica, before it hit Cuba. It then weakened to a Category 1 and moved to Bahamas on October 26. After killing 285 people, and causing severe destruction to USA, Jamaica, Haiti, Dominican Republic, Puerto Rico, The Bahamas, and Canada, Hurricane Sandy finally dissipated on November 2, 2012. The highest wind velocity recorded during the hurricane was 115 mph. The hurricane destroyed thousands of homes, blew off various buildings, and left millions homeless. It caused a property damage of USD 68 billion, which is the second-costliest hurricane in the history of the US.\nThe Great Tri-State Tornado\nDate: March 18, 1925\nDamage: USD 1.4 billion\nAreas affected: Missouri, Illinois, and Indiana\nThe Great Tri-State Tornado was the deadliest one of the 12 tornadoes that hit the Midwestern and Southern U.S. on March 18, 1925. The tornado alone killed 695 people, making it worse than the second deadliest, the 1840 Great Natchez Tornado, in the history of the US.\nThe tornado rode for 3.5 hours on a 219-mile track, which was the longest single track to be recorded. It greatly damaged the areas of Missouri, Illinois, and Indiana, destroying numerous homes and buildings along its path. In all, 695 people were killed and 2,027 were injured due to the tornado. Approximately 15,000 homes were destroyed, and the tornado caused an estimated damage of USD 1.4 billion.\nThe 2011 Joplin Tornado\nDate: May 22, 2011\nDamage: USD 2.8 billion\nAreas affected: Joplin, Jasper County, and Newton County\nThe Joplin Tornado was a massive catastrophic EF5 multiple-vortex tornado that struck the US on May 22, 2011. The tornado first developed to the east of Kansas, at 5.34 pm at EF0 intensity. It strengthened into a EF1 intensity and hit the rural areas near Joplin, Missouri.\nIt finally hit the southwest corner of Joplin near the Twin Hills Country Club at the intensities of EF1 and EF2. It continued to strengthen, and destroyed several homes in the city, with the intensity increasing up to EF5. The total track length of the tornado was 22.1 miles. Overall, it killed 158 people and injured 1,150 others. The tornado destroyed 6,954 homes, and damaged 875 homes. The estimated damage caused by the tornado was USD 2.8 billion. This tornado was the deadliest one to hit the US after the 1947 Glazier-Higgins-Woodward tornadoes.\nThe 2011 Mississippi River Floods\nDate: May 04, 2011 – June 20, 2011\nDeaths: Around 400 (in the flood as well as preceding storms)\nDamage: USD 4 billion\nAreas affected: Mississippi, Tennessee, Arkansas, Illinois, Kentucky, Louisiana\nThe devastating floods that took place in the Mississippi River basin in 2011 caused the maximum damage ever recorded. It was a series of events in which severe storms affected the areas along with numerous tornadoes before it started flooding.\nIt was said to be one of the worst monsoon seasons faced by the United States. The first areas to be affected were Missouri and Illinois around May 3, and consequently the flood waters spread to Arkansas and Tennessee around May 10. When the flood waters reached Mississippi, maximum property damage had already been caused. The water flow measured was found out to be 2,310,000 cubic feet per second, which was greater than what was estimated during the 1927 Great Mississippi River Flood. This disaster had a large effect on the economy of the US and cost an estimated USD 4 billion.\nThe Johnstown Flood\nDate: May 31, 1889\nDamage: USD 17 million\nAreas affected: Johnstown, East Conemaugh, South Fork\nThe Johnstown Flood or The Great Flood of 1889, hit Johnstown, Pennsylvania on May 31, 1889. The disaster occurred as a result of heavy rains and a catastrophic failure of the South Fork Dam on the Little Conemaugh River, situated 14 miles away from the town.\nThe dam, which was built in 1881 by the South Fork Fishing and Hunting Club, was in desperate need for repair by 1889. However, this was neglected by the club officials, which caused the dam to break down, when heavy rainfalls hit the town on May 31. The dam completely washed down as 20 million tons of water from the Lake Conemaugh roared down the valley toward Johnstown.\nOn its 14-mile path to Johnstown, the deluge accumulated debris of houses, trees, cars, animals, and people. By the time it reached Johnstown, it was nothing but a 30-feet high rolling hill of debris, which had drowned 2,000 people with it. Around 80 survivors died when the debris piled 40 ft high and caught fire at the Old Stone Bridge. A total of 2,209 people were killed because of the disaster. The estimated damage caused by the flood was USD 17 million.\nThe 2009 Samoa Earthquake\nDate: September 29, 2009\nDamage: USD 17 million\nAreas affected: Samoa, American Samoa, Tonga, Fiji, Cook Islands, New Zealand, French Polynesia\nThe 8.1 Mw, 2009 Samoa earthquake, hit the Samoan Islands on September 29, 2011. As a result of the earthquake, a 15 ft tsunami was generated, which killed a total of 189 people in Samoa, American Samoa, and Tonga.\nA 3-inch rise in the sea levels was recorded by the Pacific Tsunami Warning Center near the epicenter. The earthquake occurred in the Kermadec-Tonga Subduction Zone, which is a part of the Pacific Ring of Fire where volcanic activities are common. Four 20 ft high tsunami waves hit American Samoa, thus causing immense damage to the natural reserves, wiping out a beach village, and leaving thousands injured and homeless. Samoa avoided numerous deaths by moving most of its island population to a higher ground. However, twenty villages on Upolu island were completely wiped out, leaving an estimated 3,000 people homeless. A 13 ft high wave which hit Tonga killed 10 people and left around 192 families homeless. The tsunami caused an estimated damage of USD 17 million.\nThe 1960 Great Chilean Earthquake\nDate: May 22, 1960\nDeaths: 61 (total 6,000 due to the quake)\nDamage: USD 17 million\nAreas affected: US, Argentina, Chile, Japan, Philippines\nThe Great Chilean Earthquake of 1960, with a magnitude of 9.5 Mw was the most powerful earthquake that was ever recorded. The tsunami that resulted due to the earthquake caused huge destruction to US, Argentina, Chile, Japan, Philippines, eastern New Zealand, southeast Australia, and the Aleutian Islands.\nThe tremors of the earthquake caused tsunamis with waves up to 82 ft. The impact of the quake was so high that waves as high as 35 ft were recorded in Japan and Philippines, 10,000 kilometers away from the epicenter. The main tsunami crossed the Pacific Ocean with an immense speed, thus devastating Hilo, Hawaii, and killing 61 people. Apart from the tsunami, the earthquake, also triggered the Riñihuazo flood, eruption of Cordón Caulle eruption, numerous landslides, as well as a seiche. In all, the quake caused up to 6,000 casualties and an estimated damage of up to USD 800 million.\nThe Dust Bowl\nDeaths: 7,000 approx.\nAreas affected: Great Plains, Nebraska\nThe Dust Bowl was a series of dust storms that struck the Great Plains in the 1930s. It was caused as a result of severe drought and failure to prevent wind erosion. The drought came in three waves – first in 1934, then 1936, and again between 1939 and 1940. The drought affected 100,000,000 acres of land in Texas, Oklahoma, New Mexico, Kansas, and Colorado.\nThe storms destroyed hundreds of houses. More than 500,000 Americans were left homeless. Numerous families were forced to leave their farms and migrate to other areas. Over 86,000 people moved out of the Great Plains states and moved to California in less than a year. It was the largest migration in American history in such a short span. Up to 500 residents of the Plains fell ill and died as a result of dust pneumonia and malnutrition. The disaster killed approximately 7,000 people.\nThe Year Without a Summer\nDate: April, 1815\nDeaths: 90,000 (total deaths due to the volcano)\nAreas affected: Northeastern US, eastern Canada\nSevere climate abnormalities caused the global temperatures in summer to decrease by 0.7 – 1.3°F in the year 1816, making it a year with no summer. The lack of sunlight became very severe, thus resulting into loss of crops and major food shortage in the Northern Hemisphere.\nIt caused persistent rainfall in Europe, endless snowstorms in England and Ohio, and a prolonged drought in the eastern US. The cause of this disaster was the massive eruption of Mount Tambora in Indonesia in April 1815. The eruption had built up an enormous amount of dust, due to which less sunlight passed through the stratosphere, resulting into a temperature drop worldwide. The famine largely affected various areas in North America, Europe, and Asia.\nThe loss of crops, the cool temperatures, the heavy rainfalls, and dramatic temperature swings caused starvation, malnutrition, and an increased fatality rate. The food prices shot up, and thousands of people were left begging for food. This gave rise to various arsons, riots, and loots in the affected areas. Due to the famine, a major typhus epidemic occurred in Ireland between 1816 and 1819 in Ireland, whereas India was affected by cholera due to late torrential rains. The volcano, due to its various aftereffects, killed 90,000 people in all.\nThe Great Blizzard of 1888\nDate: March, 1888\nDamage: USD 25 million\nAreas affected: New Jersey, New York, Massachusetts, Connecticut\nThe Great Blizzard of 1888, that hit the Mid-Atlantic coast in March 1888, was the most disastrous blizzard in the history of the US. The storm that had formed on March 11, continued to produce severe winds till March 14. The wind velocity of the blizzard was more than 45 miles per hour, keeping people confined to their houses for several days.\nAs much as 50 inches of snow was dumped in Connecticut and New Jersey, whereas New York and Vermont had a snow of up to 40 inches and 30 inches, respectively. The storm generated severe winds in New York which had a velocity of 80 mph. The storm completely froze the Atlantic provinces of Canada and the East Coast, from Chesapeake Bay to Maine. The telegraph infrastructure was completely disabled as a result of which the northeastern US cities from Washington, D.C. to Boston were in complete isolation. Neither rail nor road transport was available for several weeks. Almost 200 ships were wrecked due to the storm, resulting in the death of up to 100 seamen. Various fires broke out in New York, the loss from which was around USD 25 million. More than 400 people were killed due to the storm and the resulting cold.\nMost of the time, natural disasters are, in fact, man-made disasters, which occur as a result of exploitation of the environment. So, when it comes to preventing such incidents, taking suitable measures to protect our environment becomes the only and primary solution."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:82f3e509-cbf6-4ce5-ae11-b482efafa8f0>","<urn:uuid:517e5618-796c-4c75-b3c7-5c9e0eb046e1>"],"error":null}
{"question":"What happens to gut bacteria changes when people stop exercising?","answer":"The exercise-induced changes in gut bacteria are largely reversed once exercise training ceases. The beneficial effects only last as long as people continue to exercise.","context":["The Effect of Physical Exercise on Our Gut Microbiome\nRegular moderate exercise has a beneficial effect on gut health.\nPosted Feb 16, 2018\nA large body of scientific evidence supports the fact that physical exercise is good for cardiovascular and brain health. In particular, a previous study from Ireland comparing professional rugby players and sedentary healthy control subjects suggested that exercise increases short chain fatty acid production by gut microbes and thereby improves your gut health. However, as the Irish study did not control for the dietary differences between the two groups (professional athletes consuming more calories and a different diet), it was not clear if the observed microbiota effects were not simply diet-related.\nThree recent studies, one performed in mice and two in healthy human subjects, demonstrate that endurance exercise does indeed have an effect on the community structure and function of the gut microbiome, which is independent of exercise-related dietary changes.\nIn one study, the investigators wanted to find out if high-intensity endurance exercise altered the gut microbiota composition and metabolic activity, and if this effect was related to a change in intestinal permeability or the leakiness of the gut. 73 soldiers were provided three rations of food per day with or without protein- or carbohydrate-based supplements during a 4-day cross-country ski-march. Intestinal permeability, blood, and stool samples were measured before and after the 4-day strenuous exercise. The leakiness of the gut increased by 60 percent and was associated with the activation of the immune system, measurable in the circulation. The observed exercised induced changes in gut microbial composition (increase in the less common taxa and decrease in the more abundant ones) and microbial function (metabolites) were associated with the increased leakiness.\nIn the other human study, investigators explored the impact of six weeks of endurance exercise on the composition and function of the gut microbiota in lean and obese adults with multiple-day dietary controls. Eighteen lean and 14 obese subjects, previously sedentary, participated in six weeks of supervised, endurance-based exercise training (3 days per week) that progressed from 30 to 60 minutes per day and from moderate to vigorous intensity. Subsequently, participants returned to a sedentary lifestyle activity for a period of six weeks. Fecal samples were collected before and after the six weeks of exercise, as well as after the sedentary washout period. The investigators found that the exercise-induced alterations of the gut microbiota diversity were dependent on the participant’s obesity status. Exercise increased fecal concentrations of short chain fatty acids in lean, but not obese, participants. Exercise-induced shifts in metabolic output of the microbiota paralleled changes in bacterial genes and microbial taxa capable of short chain fatty acid production. Interestingly, exercise-induced changes in the microbiota were largely reversed once exercise training ceased. The authors concluded that exercise training induces compositional and functional changes in the human gut microbiota which are dependent on obesity status, independent of diet, and contingent on the sustenance of exercise.\nBut how do the microbes know that their host (us) is exercising? Physical exercise activates the autonomic nervous system which sends signals to the gut, which can change peristalsis, regional transit, and secretion of fluid and mucus. All these changes alter the environment the microbes live in, and the microbes likely adjust to these changes. During a high intensity endurance exercise, these autonomic nervous system signals can increase the leakiness, reduce blood flow to the gut, and even directly affect gut microbial behavior.\nWhat is the take home message from this growing evidence that physical exercise is associated with changes in the gut microbiome?\n• Regular moderate exercise has a beneficial effect on gut health (via increased production of short chain fatty acids), but unfortunately, this benefit is only seen in lean subjects, and the effect only lasted as long as people continued to exercise.\n• In contrast, too much strenuous exercise may not be good for your gut health, resulting in increased leakiness and immune system activation.\nBarton, W. et al. The microbiome of professional athletes differs from that of more sedentary subjects in composition and particularly at the functional metabolic level. Gut, doi:10.1136/gutjnl-2016-313627 (2017). Karl, J. P. et al. Changes in intestinal microbiota composition and metabolism coincide with increased intestinal permeability in young adults under prolonged physiological stress. Am J Physiol Gastrointest Liver Physiol 312, G559-G571, doi:10.1152/ajpgi.00066.2017 (2017). Allen, J. M. et al. Exercise Alters Gut Microbiota Composition and Function in Lean and Obese Humans. Med Sci Sports Exerc, doi:10.1249/MSS.0000000000001495 (2017)."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:6ca374b6-3051-437c-bba4-2695e6930f83>"],"error":null}
{"question":"As a genetics researcher, I'm curious: what is RNA interference's role in both genetic inheritance disruption and medical treatments?","answer":"RNA interference (RNAi) plays dual roles. In genetic inheritance, it's involved in meiotic drive systems like Segregation Distorter (SD) in D. melanogaster, where it may help kill sperm carrying certain alleles, resulting in biased transmission of genes. In medical applications, RNAi can be used therapeutically through small interfering RNAs (siRNAs) that are 19-23 nucleotides long and can block specific protein production by chopping up target messenger RNA. These siRNAs can be designed to target almost any gene, making them potentially useful for treating conditions like androgenetic alopecia by inhibiting proteins involved in hair loss.","context":["Genomes are frequently in conflict with selfish genes that bias their transmission to the next generation in a process called meiotic drive, often at the cost of the organism. Segregation Distorter (SD) is a well-studied meiotic drive system in D. melanogaster: in SD/SD+ heterozygous males, the SD chromosome is transmitted to 95% of the progeny by killing sperm carrying sensitive alleles of its target locus, Responder (Rsp-a satellite repeat near the centromere of chromosome 2). The mechanism behind SD is unknown but it may involve RNA interference (RNAi), similar to other meiotic driver systems. Many features of spermatogenesis in eukaryotes cannot be explained without invoking a species history of genetic conflict (e.g. RNAi, rapid gene evolution). Understanding how selfish genes, like Sd, exploit RNAi to kill sperm will offer unique insight into the role of small RNAs in spermatogenesis. This proposal aims first to determine the cause of SD+ spermatid dysfunction in SD/SD+ heterozygotes, and second, to describe the distribution of Rsp repeat-associated short interfering RNAs (rasiRNAs) in the testis and test if they are disrupted in SD/SD+ heterozygotes. Third, this proposal will test the hypothesis that SD kills SD+-bearing spermatids by interfering with postmeiotic rasiRNA production in the testis. In the presence of SD, SD+ spermatids with a Rsp locus sensitive to distortion (Rsps), fail to condense their chromatin at a time when spermatids swap their histones for sperm-specific protamines to aid in condensing the nucleus as it is re- shaped into the sperm head. To determine which chromatin components show aberrant localization in SD/SD+ testes, this proposal will immunolocalize core histones and analyze the expression of GFP-tagged transition proteins, protamines and a protamine-associated chromatin component also involved in nuclear re-shaping (Aim 1). Preliminary results described in this proposal show Rsp rasiRNA expression in the testis after meiosis. This proposal will test the hypothesis that SD interferes with Rsp rasiRNA localization or expression by looking for a disruption of Rsp rasiRNAs in SD/SD+ testes using Fluorescence In Situ Hybridization (Aim 2). This proposal will also test the hypothesis that Rsp rasiRNAs originate from a genomic location outside of the satellite repeat itself, as preliminary results suggest. Thi will be accomplished by generating deletions of genomic regions containing Rsp repeats found outside of the satellite locus. The deletions will be used to test for a disruption of Rsp rasiRNAs and consequently, distortion against the Rsps-bearing chromosome (Aim 2). Finally, this proposal will test the hypothesis that SD interferes with postmeiotic rasiRNA production in the testis. To do this, I will sequence and compare small RNAs from dissections of SD/SD, SD/SD+ and SD+/SD+ testes enriched for mitotic and postmeiotic cells, separately, with Illumina short read technology (Aim 3). If SD affects Rsp rasiRNA production, then SD genotype will correlate with Rsp rasiRNA abundance in postmeiotic testis dissections.\nOur genomes are frequently in conflict with selfish genes: genes that increase their transmission through spermatogenesis by killing sperm. In some cases, these selfish genes exploit the RNA interference machinery, which is meant to defend our genomes against invaders. Determining how these selfish genes kill sperm to increase their frequencies will provide a unique perspective on spermatogenesis and help us understand the role of RNA interference in the male germline.\n|Unckless, Robert L; Larracuente, Amanda M; Clark, Andrew G (2015) Sex-ratio meiotic drive and Y-linked resistance in Drosophila affinis. Genetics 199:831-40|\n|Larracuente, Amanda M; Ferree, Patrick M (2015) Simple method for fluorescence DNA in situ hybridization to squashed chromosomes. J Vis Exp :52288|\n|Larracuente, Amanda M (2014) The organization and evolution of the Responder satellite in species of the Drosophila melanogaster group: dynamic evolution of a target of meiotic drive. BMC Evol Biol 14:233|","Part 3 – RNA Interference\nAnother possible strategy for combating androgenetic alopecia in the future could involve harnessing a molecular phenomenon known as RNA interference (RNAi) to block the expression of the genes that cause hair loss in the first place.\nRibonucleic acid (RNA) is a biological polymer that is essential for all known forms of life. One type of RNA, known as messenger RNA (mRNA), carries genetic information derived from DNA (the master genetic \"blueprint\") out of the cell nucleus to the cytoplasm where it is translated into proteins (e.g., receptors or enzymes).\n(For more details on RNA check out the entry on Wikipedia.)\nThe Science Behind RNAi\nYears of research on gene expression in plants, worms, and eventually mammals, have led to the understanding that small fragments of nucleic acids like RNA can specifically block the production of any given protein in a cell.1\nSmall interfering RNAs (siRNAs) are short fragments of RNA, approximately 19-23 nucleotides in length, which recognize and bind to specific sequences in a target mRNA and recruit RNAi machinery (including an enzyme called \"Dicer\") that chop up the target mRNA. Once the mRNA is cleaved, it can no longer be translated into the corresponding protein it encodes. With the sequencing of the human genome completed, siRNA sequences can be designed to specifically target almost any gene.\nRNAi technology could be utilized in the context of treating androgenetic alopecia to inhibit the production of proteins that are involved in hair loss or that slow the growth of hair.\nGiven the widely recognized role of dihydrotestosterone in hair follicle miniaturization and pattern hair loss, two particularly attractive targets for RNAi therapy are the androgen receptor (AR) and the 5-alpha reductase enzymes.\n“Antiandrogen therapeutic oligonucleotides targeting the downregulation of the AR expression is advantageous because both will be possible to eliminate the only way for androgens to act and simultaneously this strategy allows the medication to be topically administrated. In fact, this could be very useful in a long-term treatment of, for instance, androgenetic alopecia...”\nA Possible Manufacturer?\nA company called Sirna Therapeutics described just such an approach in a patent application, published in 2005 as US 2005159376 A1. In the application, Sirna suggested using siRNA targeting either AR or 5-alpha reductase to treat alopecia:\n“Specifically, the invention relates to small nucleic acid molecules [...] capable of mediating RNA interference (RNAi) against 5-alpha reductase and/or androgen receptor. Such small nucleic acid molecules are useful, for example, in providing compositions for treatment of traits, diseases and conditions that can respond to modulation of 5-alpha reductase and/or androgen receptor expression in a subject, such as alopecia, acne, polycystic ovary disease, prostitic hypertrophy, and prostate cancer.”\nSince RNA does not cross the cell membrane or the skin barrier efficiently, one approach for delivering siRNA molecules is to encapsulate them in a sphere of \"phospholipids\" similar to those that make up the cell membrane. These spheres, known as liposomes, would cross through the skin and facilitate the entry of siRNA into the desired cells in the hair follicle.\n“The siNA molecules of the invention are added directly, or can be complexed with cationic lipids, packaged within liposomes, or otherwise delivered to target cells or tissues. The nucleic acid or nucleic acid complexes can be locally administered to relevant tissues ex vivo, or in vivo through direct dermal application, transdermal application, or injection, with or without their incorporation in biopolymers.”\nNot Quite There Yet\nWhile the approach outlined above will theoretically be effective as a treatment for androgenetic alopecia, some details still need to be worked out. Delivery with simple liposomes works very well in cell culture models, but may not be as effective in the more complex environment of the skin. According to a review article published this year:\n“More efficient drug delivery vehicles are therefore being sought. Among the newly emerging concepts, drug delivery systems based on nano- and microparticles, which efficiently penetrate via the follicular route, are highly promising approaches.”3\n“Nevertheless, this is still a very incipient area that promises to bring new and highly targeted strategies for skin and hair diseases.”\nAs with all the other treatments described in this series of posts, any therapeutic strategy would need to be tested in clinical trials to make sure it is safe and effective before approval by the FDA.\nThanks again for joining us this week as we look into The Future of Hair Regrowth. Don't forget to come back soon for the next installment in the series, which will cover what may be the most promising area in hair growth research today: stem cells.\n1. Zamore PD, Tuschl T, Sharp PA, Bartel DP. RNAi: double-stranded RNA directs the ATP-dependent cleavage of mRNA at 21 to 23 nucleotide intervals. Cell. 2000 Mar 31;101(1):25-33. Link to PubMed\n2. Dugour A, Hagelin K, Smus C, Balañá ME, Kerner N. Silencing the androgen receptor: new skills for antiandrogen oligonucleotide skin and hair therapy. J Dermatol Sci. 2009 May;54(2):123-5. Link to PubMed\n3. Araújo R, Fernandes M, Cavaco-Paulo A, Gomes A. Biology of human hair: know your hair to control it. Adv Biochem Eng Biotechnol. 2011;125:121-43. Link to PubMed\nLack of Evidence for Safety and Efficacy?\nSeveral online distributors now offer products that contain minoxidil in higher doses than have been approved by the U.S. Food and Drug Administration (FDA) or that contain minoxidil in combination with additional active pharmaceutical ingredients.\nAlthough the FDA has only approved minoxidil at concentrations of two percent (2%) for women or five percent (5%) for men in the treatment of hair loss, some products available over the internet contain minoxidil at concentrations as high as 15%. Many are formulated with additional ingredients such as azelaic acid, retinoic acid, caffeine, and even finasteride (the active ingredient in Propecia®), in combinations that have not been reviewed by the FDA.\nThe FDA evaluates two major concerns when considering new drug applications – safety and efficacy. Until these products are tested in clinical trials, there is no way to ensure that they are safe or to know whether high doses of minoxidil even provide any benefit over products that have been approved by the FDA.\nSome consumers may find the marketing of these products to be misleading because of references to the FDA or to claims based on FDA-approved products containing 2% or 5% minoxidil.\nThe website for MinoxidilMax claims to offer “effective hair regrowth products... for male pattern baldness (alopecia androgenetica)” with “unmatched effectiveness.” Their products are manufactured “by an FDA registered cGMP compliant facility,” contain “the only effective ingredient approved by FDA in topical hair regrowth solution,” and have “the maximum strength of FDA approved hair growth stimulator (15% minoxidil).”\nConsumers who read further on the company's website may be surprised to find that their products are not FDA-approved and that “the statement on this website has not been evaluated by the FDA. This product is not intended to diagnose, treat, cure or prevent any disease.”\nAnother manufacturer, Perfect Image Solutions, offers high dose minoxidil products “specifically formulated to treat conditions associated with male pattern baldness (Androgenetic Aloepicia).”\nThe FAQ section of the website relies on the long history of FDA-approved minoxidil products like Rogaine® to imply that high dose minoxidil products must also be safe:\n“How do I know using a high concentration of minoxidil isn’t an overdose?\nMinoxidil has been on the market for over 20 years with an extremely low incidence of side effects, especially from topical application.”\nAlthough claiming that the products are “clinically proven to yield unparalleled results in the field of hair loss,” manufactured in an “FDA registered cGMP compliant facility,” and that all ingredients, “including Minoxidil must meet all FDA guidelines,” the website does not provide any evidence of clinical trials demonstrating the safety or efficacy of its products.\nFDA Takes Action\nEarlier this year, Regrowth LLC, a well known distributor of high dose \"specialty\" minoxidil formulations (formerly sold under the name Xandrox), released the following statement on its website:\n“We're very sorry to inform you that Regrowth LLC has to suspend all operations at this time. In an ongoing audit, the U.S. FDA has deemed our medications to be 'unapproved illegal drugs'.”\nRegrowth LLC had been selling minoxidil formulations containing 15% minoxidil (three times the FDA-approved dose contained in products such as Rogaine® and Avacor Physicians Formulation®).\nOn May 25, 2011, the FDA initiated a recall of 57,999 bottles of Regrowth LLC's products, stating that “[t]hese products are unapproved drugs and may present potential health hazards.”1 Based on this recall, it would not be surprising if the FDA takes action to stop other distributors of high dose minoxidil formulations/combinations from selling unapproved drugs in the future.\nAt the time of posting this article, we are unaware of any clinical trials proving that products containing higher concentrations of minoxidil than 5% are safe or more effective than FDA-approved products containing 5% minoxidil.\nIn January, Dr. Glenn Charles, a member of the International Alliance of Hair Restoration Surgeons, commented:\n“I have not seen any studies comparing 5% Minoxidil with higher % Minoxidil concentrations. I would imagine that the incidence of side effects might be higher with greater % of Minoxidil. However, many of the reported side effects might actually be caused by the other ingredients in these hair loss products containing Minoxidil.”\nIf you are still interested in experimenting with formulations containing high dose minoxidil or combinations with other active ingredients, it may be a good idea to consult with a physician before you begin using any such products.\n1. Enforcement Report for May 25, 2011: RECALLS AND FIELD CORRECTIONS: DRUGS - CLASS II. FDA.gov (accessed September 8, 2011).\n2. Photo above from Occupycorporatism.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:8a327df3-ba34-44b6-86e6-6655b3982c4c>","<urn:uuid:bc9a168b-7b01-4fa5-b666-a427f83355f3>"],"error":null}
{"question":"What maintenance safety measures are essential in high-risk environments, and how does worker fatigue affect safety compliance?","answer":"In high-risk environments, essential maintenance safety measures include scheduling routine safety checks, establishing proper communication systems, monitoring work to ensure compliance with agreed safety systems, and implementing appropriate protection for maintenance workers and others present. Regular maintenance must be correctly planned and performed to maintain machine and workplace safety. However, worker fatigue can significantly compromise safety compliance. Fatigue affects workers' ability to follow safety protocols by reducing their capacity to communicate effectively with others, impairing their judgment, and decreasing their vigilance. This is particularly problematic because maintenance work often requires high levels of concentration and is frequently carried out under pressure, such as when equipment failures halt production processes.","context":["safety points for attention in maintenance work of\nTips on Safety Maintenance: Keeping Your Crew and Machinery Safe\nSep 26, 2019 · How to set up a system for maintenance safety using a CMMS. A CMMS can be used to help your maintenance team adopt needed safety protocols. When using your CMMS in this way, these steps might help: Step 1: Schedule routine safety checks. Start by setting up recurring work orders for safety checks on key pieces of equipment, machinery, and tools.Learn More\nFive basic rules for safe maintenance - JE Bearing\nSafe work procedures have to be communicated and understood by workers and supervisors, and applied correctly. The work should be monitored so that the agreed safe systems of work and jobsite rules are observed. Maintenance is often carried out under pressure – for example, when a fault has brought the production process to a standstill.Learn More\nHazards during maintenance - Safe maintenance\nUndertaking maintenance activities can potentially expose the workers involved (and others) to all sorts of hazards, but there are four issues that merit particular attention because of the severity of the harm that could be involved, and because they are commonly encountered during plant and building maintenance.Learn More\nIs Your Preventative Maintenance Work Actually Safe?\nBut first and foremost, during these times employees must demonstrate constant vigilance and attention to detail. Follows Rules: Maintenance efforts ensure that units meet safety standards and don’t break down, so following all required steps in these procedures, as well as the general safety rules for the particular work setting, are vital ...Learn More\nMaintenance | Safety & Work\nISSA-Instruction Maintenance in Plants with High Safety Requirements Maintenance is part of the usual business in process plants. As indicated in various legal and technical standards, the instruction of workers for safety topics is crucial. In practice, it is very difficult to ensure safety. The accident rate of maintenance work is still a ...Learn More\nMaintenance and Occupational Safety and Health: A statistical ...\nMaintenance influences the safety and health of workers in two ways. Regular maintenance that is correctly planned and carried out is essential to keep both machines and the work environment safe and reliable. Maintenance itself has to be performed in a safe way, with appropriate protection of maintenance workers and others present in the ...Learn More\nFoundation: Attention Maintenance - Child Development (CA ...\nNov 05, 2020 · Pay attention to the infant care teacher's voice without being distracted by other noises in the room. (9-11 mos.; Parks 2004; 12) Focus on one toy or activity for a while when really interested. (By 12 mos.; American Academy of Pediatrics 2004, 241) Behaviors leading up to the foundation (19 to 35 months) During this period, the child may:Learn More\n10 Daily Workplace Safety Tips in Manufacturing\nOct 31, 2019 · Being safety oriented can help improve your employees’ morale, productivity, and even make a good impression on visitors. If you are an employee, following safety protocols are in your and your coworkers’ best interests. We’ve got 10 good tips for keeping you and other people safe at work below.Learn More\nQuality Of Work Performance Review Phrases Examples\nQuality Of Work Performance Review Phrases Examples. Performance review phrases examples for quality of work to write a performance evaluation and complete your performance review form for free. Try to use these positive, negative and self evaluation quality of work phrases and examples to write a performance appraisal feedback.Learn More\n5 Manufacturing Safety Tips for a Safer Workplace ...\nMar 30, 2017 · 3. Eliminate Fire Hazards. Always make sure your work space complies with fire safety codes by removing any items blocking doorways and walkways. Furthermore, if you are using combustible materials, only keep the amount you need for the job, and store the flammable material in a safe storage when it is not being used.Learn More\nMaintenance in the workplace - A guide for health and safety ...\nJul 22, 2010 · A guide for health and safety representatives Health and safety August 2010 Download Maintenance in the workplace guide [PDF]. Maintenance work can be hazardous. Although it is estimated that 6% of the working population are involved in maintenance work (not always all the time), it is estimated that, throughout Europe, between 15 and 20% of injuries at work happen during maintenance work.Learn More\n[Attention, repetitive works, fatigue and stress]\nWork characterized by the maintenance of high levels of performance for a long time, produce cognitive effort with high level of vigilance, selective attention, decisional ability, automated control mechanisms, such as \"eye-hand\", and may contribute to the fatigue.Learn More\nMaintaining Worker Situational Awareness: Focus on Fatigue ...\nworkers often felt fatigued to the point that they had safety concerns when they worked 10 hours per day for 3 to 4 consecutive days ( 8 ).Currently, the U.S. Department of Labor's Occupational Safety and Health Administration (OSHA) does not have a specific standard for extended orLearn More\nSection 5. Day-to-Day Maintenance of an Organization\nAn invisible maintenance task is that of maintaining the passion for the work and the belief in the vision and mission that drive most grass-roots and community-based organizations. This is almost always the work of the leader, and, though not always conscious, is among the most important of her day-to-day tasks.Learn More\nMowing and Trimming Safety\n6 - Mowing and Trimming Safety Safety Messages and Signs Manufacturers put important safety messages on mowing equipment and in the operator’s manual. It is critical to read, understand and follow all safety messages. The triangle shape is the symbol for caution. The exclamation mark in the center means Pay Attention. In some instances, the ...Learn More\n10 Rules for Machine Safety - EHS Daily Advisor\nDec 10, 2012 · Do your employees understand the fundamental machine safety rules? Here are 10 rules supervisors can present at their next machine safety meeting. 1. Never remove or try to defeat machine safeguards. 2. Don’t create new hazards, such as allowing objects to fall into the moving parts or by creating a new pinch point. 3. Report […]Learn More\nSafetyWorks!: Managing Safety and Health\nA safety and health management system, or safety program, can help you focus your efforts at improving your work environment. Whatever you call it, your plan describes what the people in your organization do to prevent injuries and illnesses at your workplace.Learn More\nHow to Make Safety Training More Fun and Engaging: Tips from ...\nJun 16, 2015 · Below are some tips for making safety training more fun, engaging, and effective.. Games, Competition, and Rewards. A number of safety professionals said they tried to include some form of game, competition, and/or reward in their safety training.Learn More\nSafety Inspections Online Course - National Safety Council\nThis course is designed for those responsible for conducting or supervising safety inspections, as well as for those responsible for training industrial safety and/or health inspectors. Register today and get the confidence you need to manage an effective safety inspection program that identifies hazards before they cause problems.Learn More\nHomeowner's Maintenance Checklist | A Maintenance and Home ...\nNov 11, 2020 · Take the time to tap down any protruding nails and sand any rough areas to ensure safety throughout the seasons. Check siding. Warm weather is ideal for pressure washing vinyl or fiber cement siding. Pay close attention to each piece of siding as you clean it, looking for cracks, soft spots and any other signs of trouble.Learn More","How and Why Fatigue Increases Safety Risks\nFatigue decreases productivity and increases the risk of workplace accidents by affecting a worker’s ability to think clearly, to the point that they are unaware of their own impairment.\nMental or physical exhaustion also reduces the ability to recognise and assess risks and inhibits co-ordination, reaction times and normal function, according to WorkCover Queensland.\nThey say that staying awake for 17 hours is equivalent to having a blood alcohol level of 0.05%, while being awake for 21 hours has the same effect as a blood alcohol level of 0.1%.\nThis is particularly concerning when fatigued people are performing tasks where the consequence of an error would be very serious, such as operating machinery, working at heights, performing electrical work and working with flammable or dangerous substances.\nWhat work increases fatigue?\nAdequate sleep is the most important way to prevent and counter-act fatigue. Adults typically require seven to eight hours daily of deep, uninterrupted sleep.\nPoor quality sleep, being awake for long periods and body clock disruption all contribute to fatigue, as do health and emotional issues.\nAccording to Better Health Victoria, studies have found that 50 to 80 per cent of fatigue cases are predominantly due to psychological factors, such as stress.\nWork schedules and job demands can also have a significant effect on sleep, such as:\n- Shift work and overtime which doesn’t allow for adequate rests between shifts.\n- Night shifts which disrupt a worker’s body clock.\n- Work which is high pressure, requires concentrating for long periods, sustained physical effort or repetitive actions.\n- Harsh or uncomfortable work conditions causing workers to tire more quickly.\nShift workers, night workers, FIFO and DIDO workers, seasonal workers and on-call or call-back workers are at a greater risk of fatigue.\nHow to manage fatigue\nManagers should take into consideration fatigue when designing rosters or setting company policies regarding overtime and shift work.\nEnsuring adequate rest time for workers between shifts will reduce the risk of fatigue, while scheduling critical tasks for the daytime (but not the afternoon slump between 2-4pm) will have a positive impact on safety.\nJob-rotation to reduce mental and physical exhaustion from tasks and providing adequate break rooms with facilities and comfortable conditions can also reduce fatigue.\nWorkers can also be encouraged to adopt a variety of different strategies to fight fatigue by maintaining adequate hydration levels and improving their diet, sleeping habits, lifestyle and psychological wellbeing.\n- Drinking adequate healthy fluids – including eliminating energy drinks which have been banned on some construction sites, reducing or cutting out caffeine – particularly before sleeping, not skipping meals, maintaining a healthy diet and eating plenty of iron-rich foods.\n- Going to bed and getting up at the same time, avoiding daytime naps and having a warm bath and shower.\n- Quitting smoking as it leads to lower energy levels and fatigue for a number of reasons, including reduced oxygen levels in the blood.\n- Increasing physical activity which can improve sleep as well as having positive physical and mental effects.\n- Avoiding abuse of alcohol or recreational drugs which also contribute to fatigue and have been found to be a particular problem in the construction industry.\n- Reducing stress through relaxing activities, talking to a professional about ongoing problems or issues, and taking time out.\nCommon effects of fatigue:\n- Lack of concentration\n- Impaired recollection of timing and events\n- Poor judgement\n- Reduced capacity for communicating with others\n- Reduced hand-eye coordination\n- Reduced visual perception\n- Reduced vigilance\n- Reduced capacity to judge risk\n- Slower reaction times\n- Micro sleeps\n- Heart disease and high blood pressure\n- Lower fertility\n- Poor mental health\n- Stomach disorders\nMore information can be found in the Safe Work Australia Guide for Managing the Risk of Fatigue at Work."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:3daa5456-6e32-43bb-9c37-6cfeacf27948>","<urn:uuid:9de48fda-c521-452f-ae97-cce66923b5ad>"],"error":null}
{"question":"What parallels exist between cellular cooperation in snowflake yeast colonies and coral reef ecosystems, and what factors can disrupt both systems?","answer":"In snowflake yeast colonies, cells cooperate through division of labor, where multiple cells perform different functions for group benefit, and some cells even commit suicide for the cluster's advantage rather than going rogue. Similarly, coral reef ecosystems rely on cooperation and balance, but this can be disrupted by various threats. Both systems can be destabilized by environmental factors - in coral reefs, these include wastewater pollution affecting temperature, pH, and oxygen levels, while snowflake yeast clusters require specific environmental conditions to maintain their multicellular structure. Disease can affect both systems - coral reefs face bacterial diseases that can reduce colony density and cause community phase-shifts, while in snowflake yeast, cells that 'go rogue' (similar to cancer) can threaten the cluster's integrity.","context":["A team of researchers from the United States have coaxed single-celled, brewer’s yeast – Saccharomyces cerevisiae – into evolving as multicellular, snowflake-like, groups of yeast.\nThey’re called snowflake yeast and have even evolved to use apoptosis – programmed, cell suicide – to create new, multicellular, life.\n“We see some cells evolving to give up their life in a way that helps the cluster itself reproduce,” William Ratcliff, study co-author and postdoctoral researcher at the University of Minnesota told Science-Fare.com. “Some of the cells evolve to commit suicide, and those cells that die actually become the breakpoint, within the cluster, where a new propagule is formed.”\nThat propagule is a genetically identical, multicellular, daughter cell – but substantially smaller. The researchers say it grows like a child does and when it matures – at a size that’s fairly consistent within all snowflake yeast – it starts producing its own, genetically identical, daughter cells.\n“Division of labour is a huge advantage that multicellular organisms possess,” Ratcliff said. “We have multiple cells that can perform different functions, allowing multicellular organisms to have sophisticated higher level functionality and complexity.”\n“And, it has to involve more or less from scratch,” he added.\nThese cells aren’t just clumping either, the researchers say. They’ve shown the cells are forming by postdivision adhesion – they rejoin after dividing – leading to a cluster of cells that’s genetically identical. This minimizes the chances of a single cell ‘going rogue’ for its own benefit.\nThe researchers say it increases the overall productivity of the cells and creates an environment that actually encourages a cell to commit suicide for the benefit of the group instead of going rogue and reproducing itself – a hallmark of cancer.\n“The unifying characteristic of all multicellular organisms is that we see natural selection acting in between whole groups of cells,” Ratcliff said. “A consequence of this selection – between whole groups – is the evolution of group level traits.”\nAfter coaxing the yeast into acting as a multicellular group, the researchers wanted to see how they’d evolve next. Instead of increasing the amount of cells in the cluster, which was fairly constant and consistent, the cells grew in size, increasing the overall size of the snowflake yeast, without increasing the number of cells in each ‘snowflake’.\n“We’re seeing the evolution of increased cell size as a way of getting larger clusters, without requiring the yeast to essentially wait for mutations that increase the number of cells within the cluster,” Ratcliff said.\nThe shape of the cells also started changing – they started getting less circular and more oblong. The researchers say this is what allows them to grow in size while they evolve to grow more cells per cluster.\n“We’re looking at the mode and tempo of how these cluster level traits evolve through time,” Ratcliff said. “There’s several ways in which snowflake yeast could evolve to settle faster.”\nHow organisms made the transition from unicellular to multicellular life isn’t well understood – it didn’t even happen just once, researchers say. To make it more challenging, they evolved to be multicellular at very different points of history, giving each multicellular organism hundreds of millions of years to evolve and refine those group level traits.\n“We’re most familiar with are the animals, plants and fungi,” Ratcliff said. “But, we know that multicellularity has evolved at least 25 times in different biological groups.”\nThese experiments challenge the belief that evolving to be multicellular is an intensive, complex process, but even the researchers say they had to fail before they could succeed.\n“In nature we don’t know what the ecological conditions were under which multicellularity first evolved, Ratcliff said. But we’re confident it had something to do with an environment in which forming a cluster of cells was beneficial.”\nAfter trying to select for cluster formation in yeast, using a detergent – and killing them all, instead of some – the researchers shifted to a method that was as forceful in favouring cluster formation, but not as destructive. They selected yeast by choosing those that settled the fastest.\nThe researchers essentially mixed a solution of yeast mixed and liquid that encourages growth – like shaking a snow globe – and removed those that settled first into new fresh liquid, allowing them to live.\n“It’s a way in which we can select for clusters of cells – clusters of cells settle more quickly through liquid media than single cells,” Ratcliff said. “As a result we provide an advantage to forming clusters, but it’s not so sensitive that it’s easy to kill everything.”\n“Even if there are no cluster forming yeast in our tubes we still transfer yeast every round of settling.” He added.\nThe researchers have also started exploring possible applications for studying cancer and have even looked at possible biotechnology applications – beer and winemakers are always looking to improve the quality of their yeast.\nCheck out some cool video and more pictures of snowflake yeast here\nThe research was published in the journal, Proceedings of the National Academy of Sciences","Beyond threats associated with climate and ocean change, coral reefs are also affected by various local and regional threats. These threats may occur alone or synergistically with climate change adding to the risks to coral reef systems.\nOverfishing and Destructive Fishing\nUnsustainable fishing has been identified as the most pervasive of all local threats to coral reefs. ref Over 55% of the world’s reefs are threatened by overfishing and/or destructive fishing. Overfishing (i.e., catching more fish than the system can support) leads to declines in fish populations, ecosystem-wide impacts, and impacts on dependent human communities. Destructive fishing is associated with some types of fishing methods including dynamite, gill nets, and beach seines. These harm coral reefs not just through physical impacts but also through by-catch and mortality of non-target species including juveniles. Read more about threats and management strategies in the Reef Fisheries Toolkit.\nTraditionally, impacts from wastewater pollution have been associated with human health, but the detrimental effects of wastewater pollution on marine life – and the indirect impacts they have on people – cannot be overlooked. Wastewater transports pathogens, nutrients, contaminants, and solids into the ocean that can cause coral bleaching and disease and mortality for coral, fish, and shellfish. Wastewater pollution can also alter ocean temperature, pH, salinity, and oxygen levels disrupting biological processes and physical environments essential to marine life.\nOther sources of pollution to coral reef waters include land-based pollution associated with human activities such as agriculture, mining and coastal development leading to the discharge or leaching of harmful sediments, pollutants, and nutrients. Marine-based pollution associated with commercial, recreational, and passenger vessels can also threaten reefs by discharging contaminated bilge water, fuel, raw sewage, and solid waste, and by spreading invasive species. Learn more in the Wastewater Pollution Toolkit or in the Wastewater Pollution Online Course.\nMore than 2.5 billion people (40% of the world’s population) live within 100 km of the coast, ref adding increased pressure to coastal ecosystems. Coastal development linked to human settlements, industry, aquaculture, and infrastructure can cause severe impacts on nearshore ecosystems, particularly coral reefs. Coastal development impacts may be direct (e.g., land filling, dredging, and coral and sand mining for construction) or indirect (e.g., increased runoff of sediment, sewage, and pollutants).\nTourism and Recreational Impacts\nRecreational activities can harm coral reefs through:\n- Breakage of coral colonies and tissue damage with direct contact such as walking, touching, kicking, standing, or gear contact that often happen with SCUBA, snorkelling, and trampling\n- Breakage or overturning of coral colonies and tissue damage from negligent boat anchoring\n- Changes in marine life behavior from feeding or harassment by humans\n- Water pollution by tour boats through the discharge of fuel, human waste, and grey water\n- Invasive species which can be spread through transportation of ballast water, hull fouling of cruise ships, and fouling from recreational boating\n- Trash and debris deposited in the marine environment\nCoral disease is a naturally occurring process on reefs, but certain factors can exacerbate disease and cause outbreaks. Coral disease outbreaks can lead to an overall reduction in live coral cover and reduced colony density. In extreme cases, disease outbreaks can initiate community phase-shifts from coral- to algal-dominated communities. Coral diseases can also result in a restructuring of coral populations.\nDisease involves an interaction between the coral host, a pathogen, and the reef environment. Scientists are learning more about the causes of coral disease, especially in terms of identifying the pathogens involved. To date, the most infectious coral diseases are caused by bacteria. Transmission of coral diseases can be facilitated in areas of high coral cover ref as well as through coral predation, as predators can act as vectors by oral or fecal transmission of pathogens. ref\nThe causes of coral disease outbreaks are complex and not well understood, although research suggests that important drivers of coral disease include climate warming, land-based pollution, sedimentation, overfishing, and physical damage from recreational activities. ref\nOn coral reefs, marine invasive species include some algae, invertebrates, and fishes. Invasive species are species that are not native to a region. However, not all non-native species are invasive. Species become invasive if they cause ecological and/or economic harm by colonizing and becoming dominant in an ecosystem, due to the loss of natural controls on their populations (e.g., predators).\nPathways of introduction of marine invasive species include:\n- Ship traffic, such as ballast water and hull fouling\n- Aquaculture operations (shellfish aquaculture is responsible for the spread of marine invasive species through global transport of oyster shells or other shellfish for consumption)\n- Fishing gear and SCUBA gear (through transport when moving from place to place)\n- Accidental discharge from aquaria through pipes or intentional release\nSargassum are a type of brown, fleshy macroalgae that can have detrimental ecological and economic impacts on coral reefs when overabundant.\nIn the Indo-Pacific, high percent cover of Sargassum is common on degraded coral reefs and often represents a phase-shift from a coral to algae-dominated reef system. ref Their reproductive biology and morphology make them excellent colonizers of free space and particularly resilient to disturbances such as tropical storms. ref When overabundant, they can negatively impact the reef by shading, limiting space available for coral larvae to recruit, and transmitting pathogens. ref\nIn the Atlantic, two species of floating sargassum, S. natans and S. fluitans, are responsible for causing large mats of algae blooms which are particularly harmful and prevalent on the Caribbean and West African coastlines. ref Floating algae mats are naturally prevalent in the Northern Atlantic and provide many ecological benefits such as habitat, food, and nursery grounds to many species of fish, crustaceans and even sea turtles. ref However, in the last ten years, a shift in oceanic currents has led to an algae invasion in coral reef areas, causing reduced sunlight required by corals and anoxic and hypoxic conditions on reefs, as well as poor conditions on beaches that are detrimental to the tourism industry. ref\nCoral predators (or 'corallivores') are naturally occurring organisms that feed on corals for their polyps, tissue, mucus, or a combination of the above. Such predators typically include echinoderms (starfish, sea urchins), mollusks (snails), and some fish.\nCorallivory is a common process that, under normal conditions, allows for natural turnover in the ecosystem. However, when these predators are overly abundant (e.g., outbreak conditions), they can cause significant declines in coral cover.\nCommon coral predators include:\n- Crown-of-Thorns starfish (COTS), which are found throughout the Indo-Pacific region, occurring from the Red Sea and coast of East Africa, across the Pacific and Indian Oceans, to the west coast of Central America. COTS can be a major driver of coral loss in the Indo-Pacific, particularly under outbreak conditions.\n- Drupella snails, which are commonly found living on corals in reefs throughout the Indo-Pacific and Western Indian Ocean.\n- Coralliophila snails, which are often more problematic for Caribbean reefs, although some species are prevalent in the Pacific."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:f46b30d3-0d8e-4e2f-9a94-2d19fd3f0b5a>","<urn:uuid:6dd87468-19aa-450b-b9cc-46d264a1ee33>"],"error":null}
{"question":"What are the neural mechanisms of propofol-induced loss of consciousness, and what are the clinical manifestations of awareness during anesthesia?","answer":"Propofol-induced loss of consciousness involves specific neural mechanisms, including a decrease in backward corticocortical connectivity from frontal to parietal cortices, while thalamocortical connectivity remains unchanged. During initial propofol infusion, there is an increase in fast rhythms followed by slow activity during loss of consciousness. As for clinical manifestations, anesthesia awareness occurs in 1-2 per 1,000 procedures, where patients may have brief, vague recollections, remember specific moments of surgery, or recall a feeling of pressure. While this awareness can be disturbing, patients typically do not experience pain during these episodes.","context":["Connectivity changes underlying spectral EEG changes during propofol-induced loss of consciousness.\nBoly, Mélanie ; ; et al\nin The Journal of neuroscience : the official journal of the Society for Neuroscience (2012), 32(20), 7082-90\nThe mechanisms underlying anesthesia-induced loss of consciousness remain a matter of debate. Recent electrophysiological reports suggest that while initial propofol infusion provokes an increase in fast ... [more ▼]\nThe mechanisms underlying anesthesia-induced loss of consciousness remain a matter of debate. Recent electrophysiological reports suggest that while initial propofol infusion provokes an increase in fast rhythms (from beta to gamma range), slow activity (from delta to alpha range) rises selectively during loss of consciousness. Dynamic causal modeling was used to investigate the neural mechanisms mediating these changes in spectral power in humans. We analyzed source-reconstructed data from frontal and parietal cortices during normal wakefulness, propofol-induced mild sedation, and loss of consciousness. Bayesian model selection revealed that the best model for explaining spectral changes across the three states involved changes in corticothalamic interactions. Compared with wakefulness, mild sedation was accounted for by an increase in thalamic excitability, which did not further increase during loss of consciousness. In contrast, loss of consciousness per se was accompanied by a decrease in backward corticocortical connectivity from frontal to parietal cortices, while thalamocortical connectivity remained unchanged. These results emphasize the importance of recurrent corticocortical communication in the maintenance of consciousness and suggest a direct effect of propofol on cortical dynamics. [less ▲]Detailed reference viewed: 28 (4 ULg)\nGranger causality analysis of steady-state electroencephalographic signals during propofol-induced anaesthesia.\n; ; Bruno, Marie-Aurélie et al\nin PLoS ONE (2012), 7(1), 29072\nChanges in conscious level have been associated with changes in dynamical integration and segregation among distributed brain regions. Recent theoretical developments emphasize changes in directed ... [more ▼]\nChanges in conscious level have been associated with changes in dynamical integration and segregation among distributed brain regions. Recent theoretical developments emphasize changes in directed functional (i.e., causal) connectivity as reflected in quantities such as 'integrated information' and 'causal density'. Here we develop and illustrate a rigorous methodology for assessing causal connectivity from electroencephalographic (EEG) signals using Granger causality (GC). Our method addresses the challenges of non-stationarity and bias by dividing data into short segments and applying permutation analysis. We apply the method to EEG data obtained from subjects undergoing propofol-induced anaesthesia, with signals source-localized to the anterior and posterior cingulate cortices. We found significant increases in bidirectional GC in most subjects during loss-of-consciousness, especially in the beta and gamma frequency ranges. Corroborating a previous analysis we also found increases in synchrony in these ranges; importantly, the Granger causality analysis showed higher inter-subject consistency than the synchrony analysis. Finally, we validate our method using simulated data generated from a model for which GC values can be analytically derived. In summary, our findings advance the methodology of Granger causality analysis of EEG data and carry implications for integrated information and causal density theories of consciousness. [less ▲]Detailed reference viewed: 123 (5 ULg)\nPropofol anesthesia and sleep: a high-density EEG study.\n; Bruno, Marie-Aurélie ; et al\nin Sleep (2011), 34(3), 283-91\nSTUDY OBJECTIVES: The electrophysiological correlates of anesthetic sedation remain poorly understood. We used high-density electroencephalography (hd-EEG) and source modeling to investigate the cortical ... [more ▼]\nSTUDY OBJECTIVES: The electrophysiological correlates of anesthetic sedation remain poorly understood. We used high-density electroencephalography (hd-EEG) and source modeling to investigate the cortical processes underlying propofol anesthesia and compare them to sleep. DESIGN: 256-channel EEG recordings in humans during propofol anesthesia. SETTING: Hospital operating room. PATIENTS OR PARTICIPANTS: 8 healthy subjects (4 males) INTERVENTIONS: N/A MEASUREMENTS AND RESULTS: Initially, propofol induced increases in EEG power from 12-25 Hz. Loss of consciousness (LOC) was accompanied by the appearance of EEG slow waves that resembled the slow waves of NREM sleep. We compared slow waves in propofol to slow waves recorded during natural sleep and found that both populations of waves share similar cortical origins and preferentially propagate along the mesial components of the default network. However, propofol slow waves were spatially blurred compared to sleep slow waves and failed to effectively entrain spindle activity. Propofol also caused an increase in gamma (25-40 Hz) power that persisted throughout LOC. Source modeling analysis showed that this increase in gamma power originated from the anterior and posterior cingulate cortices. During LOC, we found increased gamma functional connectivity between these regions compared to the wakefulness. CONCLUSIONS: Propofol anesthesia is a sleep-like state and slow waves are associated with diminished consciousness even in the presence of high gamma activity. CITATION: Murphy M; Bruno MA; Riedner BA; Boveroux P; Noirhomme Q; Landsness EC; Brichant JF; Phillips C; Massimini M; Laureys S; Tononi G; Boly M. Propofol anesthesia and sleep: a high-density EEG study. SLEEP 2011;34(3):283-291. [less ▲]Detailed reference viewed: 36 (6 ULg)","If you’re having a major surgery, you most likely will receive general anesthesia and be unconscious during the procedure. This means you will have no awareness of the procedure once the anesthesia takes effect, and you won’t remember it afterward.\nVery rarely — in only one or two of every 1,000 medical procedures involving general anesthesia — a patient may become aware or conscious. The condition, called anesthesia awareness (waking up) during surgery, means the patient can recall their surroundings, or an event related to the surgery, while under general anesthesia. Although it can be upsetting, patients usually do not feel pain when experiencing anesthesia awareness.\nAlthough it can be upsetting, patients usually do not feel pain when experiencing anesthesia awareness.\nAnesthesia awareness is not the same as remembering some activities surrounding your procedure, such as something that happened just before the anesthesia started working or when its effects began to wear off after surgery. This is normal. You might even dream during surgery and only think you experienced awareness.\nAnesthesia awareness during surgery can happen for a few different reasons. It can be more common in patients with multiple medical conditions, and certain surgeries or circumstances increase the risk of awareness because the usual dose of required anesthesia cannot be used safely. These surgeries are often emergencies, such as emergency C-sections, certain types of heart surgery and surgery that’s needed after a traumatic injury.\nPeople who have experienced awareness under anesthesia report different levels of awareness. Some people have brief, vague recollections. Others remember a specific moment of surgery or their surroundings. In some cases, people recall a feeling of pressure.\nPatients also are more likely to experience awareness with procedures that do not involve general anesthesia. For example, you may recall all or part of your procedure if you have one of the following types of anesthesia:\n- Intravenous, or “twilight” sedation, which is often used for minor procedures such as a colonoscopy, certain types of biopsies, or a dental procedure.\n- Local or regional anesthesia, such as an epidural, spinal block, or nerve block, which temporarily numbs the area being treated during the surgery.\nDepending on the person and the event, anesthesia awareness can be disturbing and even traumatic. If it should happen to you, be sure to describe your experience to your anesthesiologist after your surgery. Some patients benefit from counseling after surgery to help cope with feelings of confusion and stress.\nBefore your surgery, your anesthesiologist will meet with you to learn about any health conditions you may have as well as your previous experiences with anesthesia. To reduce your risk of experiencing awareness during general anesthesia, it is important to tell your anesthesiologist as much information about your health as possible, including the following:\n- Previous problems with anesthesia, including a history of being aware during surgery\n- All medications you are taking — prescription, over-the-counter, and herbal supplements\n- Concerns you may have about surgery, including fear of awareness during surgery\n- History of drug or alcohol use, which can increase the risk of anesthesia awareness\nAnesthesiologists are the most highly skilled medical experts in anesthesia care, pain management, and critical care medicine with the education and training that can mean the difference between life and death."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:331abdbe-53f5-4b4b-b4ec-3a8c4bc228a5>","<urn:uuid:13b0dcc7-8e42-4ca4-95c8-7f00859d1aec>"],"error":null}
{"question":"How do the forex trading volumes compare between peak hours versus regular trading sessions?","answer":"During the 24-hour forex market, trading volumes vary significantly between peak and regular hours. The highest volume and pip movement occurs when market sessions overlap, particularly during the London/New York sessions for pairs like EUR/USD, USD/CHF, and GBP/USD, while the Tokyo/Sydney sessions are most active for pairs like EUR/JPY, AUD/USD, USD/JPY, and AUD/JPY. The overall forex market is incredibly liquid, with average daily trading volume exceeding $5 trillion, making it the largest and most liquid market in the world.","context":["In normal market conditions and size in the most liquid currency pairs you should see no slippage on your trades or orders. Foreign Exchange trading commonly referred to as Forex or FX is the exchange of one currency for another currency at an agreed price. Forex is short for \"foreign exchange\" and is the largest, most liquid market in the world with an average daily trading volume exceeding $5 trillion. Like any other market, currency prices are set by the supply and demand of sellers and buyers.\nThis means that the rate of return, the profit or loss from the initial capital outlay, is significantly higher than in traditional cash trading. The forex market can DotBig account be broken up into four major trading sessions. The Sydney session, the Tokyo session, the London session, and Trump’s favorite time to tweet, the New York session.\nHow To Start Trading Forex\nThe tourist has to exchange the euros for the local currency, in this case the Egyptian pound, at the current exchange rate. Because of the worldwide reach of trade, commerce, and finance, forex markets https://www.usbank.com/index.html tend to be the largest and most liquid asset markets in the world. There are some major differences between the way the forex operates and other markets such as the U.S. stock market operate.\n- Low Transaction Cost Due to the high liquidity and 24 hours market the spread in currency pairs is small meaning the cost of trading is low.\n- Like any investment, there is a possibility that you could sustain losses of some or all of your investment whilst trading.\n- In addition, the pip fundamentally enhances the precision of orders and specificity of spreads.\n- One would presume that a country’s economic parameters should be the most important criterion to determine its price.\n- Your financial situation is unique and the products and services we review may not be right for your circumstances.\nMarket sentiment, which is often in reaction to the news, can also play a major role in driving currency prices. If traders believe that a currency is headed in a certain direction, they will trade accordingly and may convince others to follow suit, increasing or decreasing demand.\nHow Do I Get Started With Forex Trading?\nWhen trading with leverage, you don’t need to pay the full value of your trade upfront. When you close a leveraged position, your profit or loss is based on the full size of the trade.\nDespite the enormous size of the forex market, there is very little regulation since there is no governing body to police it 24/7. Like most financial markets, forex is primarily driven by the forces of supply and demand, and it is important to gain an understanding of the influences that drive these factors. A key advantage of spot forex is the ability to open a position on leverage.\nMajor re-occurring news includes central bank benchmark interest rate decisions, employment data, inflation reports and gross domestic product numbers. Following this data for any of the currencies you plan on trading will help you avoid market shocks and understand the bigger economic picture. Forex margin is a good-faith deposit made by the trader to the broker. It is the portion of the trading account allocated to servicing open positions in one or more currencies. Margin is a vital component to forex trading as it gives participants an ability to control positions much larger than their capital reserves. Flexibility and diversity are perhaps the two biggest advantages to trading forex. The ability to open either a long or short position in the world’s leading major, minor or exotic currencies affords traders countless strategic options.\nLearn about the benefits of forex trading and see how you get started with IG. CFDs are leveraged products, which enable you to open a position for a just a fraction of the full value of the trade. Unlike non-leveraged products, you don’t take ownership of the asset, but take a position on whether you think the market will rise or fall in value. This often comes into particular focus when credit ratings are upgraded and downgraded. A country with an upgraded credit rating can see its currency increase in price, and vice versa. All services and products accessible through the site /markets are provided by FXCM Markets Limited with registered address Clarendon House, 2 Church Street, Hamilton, HM 11, Bermuda. FXCM Markets Limited (\"FXCM Markets\") is incorporated in Bermuda as an operating subsidiary within the FXCM group of companies (collectively, the \"FXCM Group\" or \"FXCM\").\nHow Does Forex Trading Work?\nWhether you are a seasoned market veteran or brand-new to currency trading, being prepared is critical to producing consistent profits. If you’re new to forex trading, then it’s best to start small. Trading lower leverage ensures that you have enough capital to become experienced in the market. There’s plenty of time to implement higher degrees https://pick-kart.com/review-of-forex-broker-dotbig-ltd-advantages-and-disadvantages-of-a-broker-features-of-deposit-withdrawal-of-funds/ of leverage once you gain competency and security in the marketplace. The forex trading platform is the trader’s window to the world’s currency marketplace. To be effective, it’s imperative that your trading platform is up to the many challenges of the live market. Like all markets, forex features a unique collection of pros and cons.\nIt can be a fee for every transaction or a monthly fee to use their trading product. Liquidity The FX market is the most liquid market in the world, making the cost of trading lower than other asset classes. Additionally, slippage is far less likely to occur than in other markets due to the depth of the market.\nCurrency traders buy currencies hoping that they will be able to sell them at a higher price in the future. Before you fly back home, you stop by the currency exchange booth to exchange the yen that you miraculously have remaining (Tokyo is expensive!) and notice the exchange rates have changed. You go up to the counter and notice a screen displaying different exchange rates for different currencies.\nWhat Can You Trade In The Forex Market?\nAfter the Bretton Woodsaccord began to collapse in 1971, more currencies were allowed to float freely against one another. The values of individual currencies vary based on demand and circulation and are monitored by foreign exchange trading services. Note that you’ll often see the terms FX, forex, foreign exchange market, and currency market. These terms are synonymous and all refer to the forex market. It is also a good idea to find out what kind of account protections are available in case of a market crisis, or if a dealer becomes insolvent.\nBenzingas Best Forex Trading Platforms\n24/7 Market – One distinctive feature of the forex market is the fact that it is always open. This allows you to always make money no matter the hour or your location.\nLeave A Comment","Forex trading hours est\nthank you Japan for keeping it simple. During the 24 hours period currency pairs in Forex market experience several hours, when the volume of trades is the highest and so is the pip movement. Local Time, eDT, bST (GMT1 sydney Open 7:00 AM, sydney Close 4:00 PM 5:00 PM 2:00 AM 10:00 PM 7:00. You can make money trading when the market moves up, and you can even make money when the market moves down. London 8:00 am 5:00 pm, frankfurt 7:00 am 4:00 pm, america, new York 1:00 pm 10:00 pm, chicago 2:00 pm 11:00. Sundays (opening) and Mondays are days when traders are mostly watching and analyzing the market and predict further price moves. Open and close times will also vary during the months of October/November and March/April as some countries (like the United States, England and Australia) shift to/from daylight savings time (DST). Below are Forex market sessions and examples of the most active currency pairs: London/ New York sessions: EUR/USD, uSD/CHF, gBP/USD, tokyo/Sydney sessions: EUR/JPY, aUD/USD, uSD/JPY, aUD/JPY.\nFormation trading marrakech, La plateforme de trading forex mac, Cfd broker forex,\nThe day of the month that a country shifts to/from DST also varies, confusing us even more. . Forex Market Center, time Zone. You should consider whether you understand how CFDs work and whether you can afford to take the high risk of losing your money. Lets take a more in-depth look at each of the sessions, as well as those periods when the sessions overlap. Most successful day traders understand that more trades are successful if conducted when market activity is high and that it is best to avoid times when trading is light. Wellington 10:00 pm 6:00 am, trading sessions according to EST (Eastern Standard Time Region, city, open (EST close (EST). BUT you will have a very difficult time trying to make money when the market doesnt move at all. Restricting cookies will prevent you benefiting from some of the functionality of our website. If you need the precise time, see http www. Sydney session: AUD/USD, eUR/USD, during the week the most active Forex trading days are: Tuesday, Wednesday and Thursday. Please send questions, comments, or suggestions.\nYoud think that Sydneys Open would only move one hour when the.S. Before looking at the best times to trade, we must look at what a 24-hour day in the forex world looks like. Some of the most active market times will occur when two or more Market Centers are open at the same time. 77 of retail investor accounts lose money when trading CFDs with this provider. The forex market can be broken up into four major trading sessions: the.\nJe ne vais pas vous faire un cours la dessus, yen a beaucoup sur le web et lexcellent site que je vous propose vous forme si vous le souhaitez. Attention certainsRead more\nIn this article, I will unveil an extremely useful tool, widely used by banks and financial institutions, that goes by the name of volume profile. This is a tool that canRead more"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:3beb247e-6fdd-4923-8d8f-b2d4c30be741>","<urn:uuid:7ba4b4e9-24b3-4573-a486-8f57058ec303>"],"error":null}
{"question":"What are the differences between Alibaba Cloud's tag-based security approach and Azure's Conditional Access policies for controlling resource access?","answer":"Alibaba Cloud's tag-based security approach and Azure's Conditional Access policies differ in their access control mechanisms. Alibaba Cloud uses tags for resource-based or role-based access control, where access is determined by the tags attached to resources. Azure's Conditional Access policies, however, use a more dynamic approach based on multiple signals, including user roles, IP addresses, geographic locations, and applications being accessed. Azure's system can automatically adjust security by restricting access based on location, forcing multi-factor authentication for external IP addresses, and requiring compliant devices. While both systems provide access control, Azure's Conditional Access offers more context-aware and dynamic security controls compared to Alibaba's tag-based approach.","context":["As the number of cloud resources increases, the difficulty in managing the resources also increases. For example, you may be unable to batch collect cost statistics about and perform O&M and monitoring on multiple cloud resources that serve the same purpose or belong to the same application category or organization to ensure that the resources are properly used and efficiently managed. To efficiently manage your resources, you can use tags to classify the resources. Tags are an important grouping tool that helps you in the horizontal management of personnel, finances, and materials and the fine-grained management of resources. Tags are suitable for various cloud resources and can meet your business requirements.\nTags are suitable for scenarios that involve resource management, access control, automated O&M, and cost allocation.\nFor information about other scenarios for which tags are suitable, see Overview of tags.\nManagement of application publishing procedures\nResource tracking and tag-based group search and management\nTag-based and group-based automated O&M by using Alibaba Cloud services such as CloudOps Orchestration Service, Resource Orchestration Service (ROS), Auto Scaling, and Cloud Assistant\nTag-based cost management and cost allocation\nResource-based or role-based access control\nBest practices for tag design\nYou can implement the best practices for tag design based on the following principles:\nTo implement mutual exclusivity, we recommend that you use only one tag key for an attribute. For example, if you use a tag key of\nownerto represent the owner attribute, you cannot use other tag keys, such as own and belonger, to represent the attribute.\nCollective exhaustion means that when you plan resources, you must plan tags at the same time and prioritize tag keys. All resources must have tags that consist of the planned tag keys and the corresponding tag values.\nEach tag key-value pair must be named in a standard format.\nCollective exhaustion is a prerequisite for future tag-based access control, cost tracking, automated O&M, and group-based search.\nLimited values mean that excess tag values must be removed and only core tag values are retained.\nYou can simplify the procedures for resource management, access control, automated O&M, and cost allocation by following this principle. You can also use tags and automation tools based on this principle to manage resources. Elastic Compute Service (ECS) allows you to control tags by calling API operations to facilitate the automated management, search, and filtering of resources.\nConsidering ramifications of future changes\nWhen you plan tags based on the principle of limited values, you must consider the impacts of adding or removing tag values to improve the flexibility of modifying tags.\nIf you modify tags, tag-based access control, automated O&M, or related billing reports may change. For corporate or personal business, the best practice is to create business-related tag groups to manage resources in the technical, business, and security dimensions. When you use automated O&M tools to manage resources and services, you can add automation-specific tags to facilitate automated O&M.\nSimplified design means that when you plan tags, you must create tag keys that have fixed dimensions to simplify the use the keys. By following this principle, you can reduce operation errors that are caused by redundant tag keys.\nYou can create business-related tag groups to manage resources in the technical, business, and security dimensions.\nWhen you use automated O&M tools to manage resources and services, you can add automation-specific tags to the resources and services.\nExamples of designing tags\nThe following table provides examples on how to name tags in common dimensions. We recommend that you use lowercase letters to name tags.\nBusiness department (to implement cost allocation and business tracking)\nOwner from the finance dimension (to identify the resource owner)\nNames or emails\nCustomer from the finance dimension (to identify the customers whom a specific resource group serves)\nCustom or actual values\nProject from the finance dimension (to identify the projects that are supported by specific resources)\nOrder from the finance dimension\nOrder category IDs\nBest practices for using tags\nYou can manage resources in a fine-grained manner based on tags. You can use tags for the following purposes or to perform the following operations.\nBefore you manage cloud resources based on tags, you must create tags and add the tags to the resources. For more information, see Create or add a tag or Use OOS to add tags to multiple ECS resources at a time.\nYou can add tags to your cloud resources based on your business requirements, modify tags, or delete the tags that you no longer need. For more information, see Use OOS to modify a tag value of multiple resources or Delete or remove a tag.\nSearch for or export resources.\nPerform access control.\nResource Access Management (RAM) users can search for authorized resources by global tag. For more information, see Best practices for global tags.\nAttach policies to RAM users to allow the RAM users to manage access to ECS instances by using tags. For more information, see Implement fine-grained access control by using tags and Control access to resources by using tags.\nYou can add the same tag to ECS instances and use the smart tag synchronization feature of CloudMonitor to assign the instances to the same application group for group-based automatic monitoring. The monitoring items include the health, CPU utilization, and memory usage of ECS instances. For more information, see Implement automatic resource monitoring by group based on tags.\nPerform automated O&M.\nYou can use Cloud Assistant to run commands on or send files to ECS instances that have specific tags. For more information, see Control the executions of Cloud Assistant commands based on tags.\nYou can use OOS to implement automated O&M for resources based on tags. For example, you can batch start ECS instances that have specific tags. For more information, see Overview.","How to secure your Azure cloud\nIn our previous post we recalled and emphasized a number of important cloud security challenges, reported by the Cloud Security Alliance (CSA) in 2019. Reported security threats are related to, for example; Identity and Access Management, data identification and classification, threat and vulnerability management…\nWhen adopting a cloud solution, you should understand and leverage the numerous tools that are available to secure your cloud services.\nFor example, the Microsoft Azure Cloud Platform offers multiple capabilities in order to manage cloud security risks and to secure your Azure cloud environment.\nIn this article we will discuss a number of solutions provided by Microsoft Azure that can help administrators to perform their security tasks in an effective and efficient way.\nAzure Cloud Solutions\nMost importantly, the Azure cloud solutions enable you to apply a more streamlined approach to manage security tasks automatically and centralized.\nBelow, we will discuss following solutions in more detail:\n- Azure Active Directory (AD), MFA and SSO\n- Azure Role-Based Access Control (RBAC)\n- Azure Vault\n- Azure Conditional Access\n- Azure Information Protection (AIP)\n- Azure Data Discovery and Classification\n- AIP policy\n- Azure Privileged Identity Management (PIM)\n- Azure Security Center\nMicrosoft Azure provides several licensing possibilities, each of them with different features. Therefore, the Azure solutions discussed below might not be included in your specific license. Details can be verified in the following location: https://azure.microsoft.com/nl-nl/pricing/#product-pricing\n1. Identity, Credentials and Access Management\n1.1. Azure Active Directory and SSO\nAzure Active Directory (Azure AD) is Microsoft’s cloud-based identity and access management service. Note: Azure AD is not the same as Active Directory Domain Services. If you would like to know more: Microsoft published an article explaining the differences, see Compare Active Directory to Azure Active Directory.\nAzure AD enables users to sign in and access both external resources (e.g. Office 365, The Azure portal, and numerous SaaS applications) and internal resources (e.g. self-developed cloud apps or apps on your corporate network).\nWhen using Azure AD with single sign on (SSO), a user must no longer remember application-specific passwords to sign in to each application. SSO enables users to sign in only once with their account to access SaaS applications, web applications, company resources etc. Moreover, IT staff can centralize user account management, and automatically add or remove user access based on group membership. Beware that your applications are as safe as your weakest protected device; an unprotected laptop gives readily access to all your resources.\n1.2. Multi Factor Authentication (MFA)\nApplying MFA is critical to mitigate the risk of user accounts becoming compromised. MFA is an authentication method which relies on the user’s normal credentials and at least one other item of information. For example: something the user knows (password) combined with something the user has (mobile phone). If the username and password are stolen, hackers cannot use the credentials without also stealing the second authentication.\nBy combining MFA and SSO, perimeter security increases while the authentication process is simplified (i.e. by enabling a single, centrally managed IAM solution).\n2. Azure Role-Based Access Control (RBAC)\nBasically, role-based access control (RBAC) restricts access based on a person's role within an organization. The roles refer to the levels of access that employees have. “Much as with a traditional Active Directory, user account permissions should be configured using such a role-based approach in order to provide users the least amount of privileges required to perform their job tasks” .\nAzure RBAC helps you to manage who has access to your Azure cloud resources, what they can do with those resources, and what areas they have access to. This is done by creating role assignments. A role assignment consists of three elements: security principal, role definition, and scope.\n- Security Principal: “A security principal is an object that represents a user, group, service principal, or managed identity that is requesting access to Azure resources” \n- Role definition: “A role definition is a collection of permissions. A role definition lists the operations that can be performed, such as read, write, and delete. Roles can be high-level, like owner, or specific, like virtual machine reader” \n- Scope: “Scope is the set of resources that the access applies to. When you assign a role, you can further limit the actions allowed by defining a scope. In Azure, you can specify a scope at multiple levels; management group, subscription, resource group, or resource. Scopes are structured in a parent-child relationship” \nMore details can be consulted via: https://docs.microsoft.com/en-us/azure/role-based-access-control/overview.\nIn addition, Microsoft has published several blueprints that can help you configuring user account permissions. https://docs.microsoft.com/en-us/azure/governance/blueprints/samples/\n3. Azure Conditional Access\nThe change to a more cloud-centric model results in a security perimeter that extends beyond an organization's network to include user and device identity. As a consequence, controlling the access to your corporate resources is more challenging.\nConditional Access Policies can be applied as: IF a user wants to access a resource, THEN a certain condition should be met first. When making a policy decision, conditional access can take into account a number of signals, such as;\n- The user’s roles and the groups he or she belongs to\n- The IP address of the user or the geographic location of the user attempting to log in\n- The application\n- Users attempting to access specific applications can trigger different Conditional Access policies\nUsing these attributes, administrators can use Conditional Access to adjust security by  :\n- Restricting users from logging in from foreign countries or restricting access to specific sensitive information when logging in from locations other than their offices\n- Forcing users to use MFA when logging in on IP addresses external to the organization\n- Requiring users to use compliant devices when attempting to access organizational resources\n4. Azure Information Protection (AIP)\nOrganizations might be challenged with detecting their data and classifying this data according to their policies and procedures. Some useful tools to implement data classification are :\n4.1. Azure Data Discovery and Classification\n“This tool can help you to discover, classify, label and report sensitive data stored within Azure’s SQL databases” . Moreover, Azure SQL auditing enables the possibility to generate dashboarding and reports to have a view on what data is stored within the databases and who is accessing this data.\n4.2. AIP policy\nAzure Information Protection Policies can be developed to assist your organization in protecting documents and emails based on specific conditions. Documents and emails can be labeled (labels apply a certain classification value) based on triggers, for example words or phrases that are used when writing a document or email.\n5. Key Vault\nAzure Key Vault is a tool to securely store and manage your encrypted keys, passwords or certificates. This way, access to your key vaults is only allowed by authorized applications and users in order to protect your data.\nIn summary, Azure Key Vault helps solve the following problems :\n- Secrets Management – “Securely store and control access to tokens, passwords, certificates, API keys, and other secrets”\n- Key Management – “Azure Key Vault facilitates the creation and controlling of the encryption keys used to encrypt your data”.\n- Certificate Management – “Azure Key Vault is also a service that lets you easily provision, manage, and deploy public and private Transport Layer Security/Secure Sockets Layer (TLS/SSL) certificates for use with Azure and your internal connected resources”.\n- Store secrets backed by Hardware Security Modules – “The secrets and keys can be protected either by software or FIPS 140-2 Level 2 validated HSMs”\n6. Azure Privileged Identity Management (PIM)\nPrivileged Identity Management enables you to manage the assigned privileged roles throughout your organization and to gain insights about the activities of these privileged roles. “Users assigned to a PIM protected role, must elevate to use the granted privileged access rights, for example; perform MFA, obtain approval or provide a reason of activation. In addition, the tool enables to track these elevations via notifications and the audit event logs” .\n7. Azure Security Center\nAzure Security Center unifies security management and enables advanced threat protection for workloads in the cloud and on-premises. “Not only does the Security Center offer recommendations throughout the Azure portal in order to further secure the cloud, but individual resources can be integrated with the Security Center to allow for health checks, patch management, and security alerts”  .\nSecurity Center provides you with the tools to :\n- Strengthen your security posture: “Security Center will assess your environment and enables you to understand the status of your resources, and whether they are secure”.\n- Protect against threats: “Security Center assesses your workloads and raises threat prevention recommendations and security alerts”.\n- Get secure faster: “Deployment of Security Center is easy, providing you with auto-provisioning and protection with Azure services”.\nWhen security policies are being broken, the Security Center will notify administrators and provide recommended solutions to resolve the incident. Moreover, administrators are able to streamline the incident response process by developing rules to automatically apply efforts to resolve known or common issues.\nThe Security Center provides you with a dashboard of organization databases. Identified critical vulnerabilities will be notified with recommendations to resolve the incident.\nMany more Microsoft security tools are and might become available for cloud administrators. Organizations are moving more and more of their activities and data to a cloud environment, therefore it is important to get to know the tools that are available to mitigate the security risks to secure your cloud services.\nWhile these tooling’s can increase an organization’s security posture, it is still critical that management continue to perform traditional responsibilities, such as user access reviews and periodic review of role matrices, to verify that excessive access is not provisioned to users.\n Microsoft Azure, https://docs.microsoft.com/en-us/azure/?product=featured\n Crowe LLP, Zak Thoreson, 2020; https://www.crowe.com/cybersecurity-watch/securing-the-azure-cloud-dgs?utm_source=linkedin&utm_medium=social&utm_campaign=mo2114-002a\nFor more information about securing your cloud environment, please contact:\nMicrosoft, Windows, and Azure are either registered trademarks or trademarks of Microsoft Corp."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:747588b1-4de3-42a3-bd0f-e7af248bef7c>","<urn:uuid:cb8bce81-713b-4c1b-9ab7-06b039ec87f2>"],"error":null}
{"question":"How do the earliest Egyptian hieroglyphic inscriptions compare chronologically to the oldest known cave paintings in Europe?","answer":"The earliest Egyptian hieroglyphic inscriptions, known as the Narmer inscriptions, date to around 3,200 BC, while the oldest known cave paintings in Europe are significantly older, particularly those in the Chauvet cave in France, which have been dated to 32,900±490 years ago based on radiocarbon dating. The hieroglyphic system was complex and known only to a select few like scribes and religious orders, while cave paintings demonstrate sophisticated artistic techniques including perspective and detailed observation of animal behavior, though their exact purpose remains unknown.","context":["What civilization was responsible for the development of writing?\nThe beginning of writing has its inception in \"hash marks\" used for counting by people living 50,000 years Before Christ (B.C.). These marks slowly took on more complexity. By 4,000 B.C. the marks had gotten very complex and were used to convey detailed bits of information. In the Indus Valley, some peoples were using such marks to decorate pottery but the marks didn't convey enough cohesive information to be qualified as writing. Writing is defined as an organized and consistent system of graphemes that conveys both concrete and abstract information [grapheme: the minimal unit of a writing system (a, b, c, etc)]. The first to develop such a system were the Sumerians who developed an organized system of graphemes to use in taxation transactions to keep information organized in such a way that all participants in a taxation transaction knew the same things and could agree to the same actions and outcomes. From this beginning, the whole Sumerian writing system developed that later met the definition of an organized system of graphemes for conveying both concrete and abstract information.\n[For more detail, see the article \"The Invention of Writing\" at Erasmatazz.com from which this answer is drawn.]\nI agree with all of the previous answers that state the oldest form of writng (cuniform) is attributed to the ancient Sumerians of Mesopotamia 3200 B.C.E. However, recent archaeological discoveries in Egypt by Gunter Dreyer, German Archaeological Institute in Cario might tell a different story. The small tags he found in the tomb of King Scorpion I date from 3400-3200 B.C.E. suggesting that there might be a two hundred year window that could possibly change the answer to your question, which is why your question is so important to human history. It is vital that those who study our human origins through historical method continue to measure their evidence with that of the archaeological evidence. The quest to understand and correctly document the human experience is ever changing, therefore new information/evidence that has academic merit should always be acknowledged.\nWe do not know this for sure, because some culture might have been writing on wood or papyrus or something and its writings could have been lost because they rotted. However, the first writing we know of was invented by the Sumerian culture of Mesopotamia. This writing was done with a stylus on wet clay. It is known as cuneiform. The word \"cuneiform\" means \"wedge-shaped.\" This term is used because that is what shape the characters generally were.\nThis type of writing was being used as long as five thousand years before the present time.\nWriting has developed with as a series of innovations and and improvement, that took place over a period of around 20,000 years, in ways of representing information in form of pictures and symbols made on plain surfaces. Perhaps, thousands of people from different parts of the world have contributed such development, in prehistoric and ancient era. There are no historical evidence available to to identify any one person or even a small group of person as inventors of writing.\nThe origin of writing can be seen in drawings made on walls of caves, more than 20.000 years back to describe some events. Writing of this type conveyed meaning without depending on knowledge of any particular language. Many scholars believe that writings that depended use of symbols defined by a system of language were first developed by Sumerians around 3500 B.C. This system of writing called cuneiform was very complex with use many symbols that represented words and syllables. Earliest system of writing using a system of alphabets is believed to have been developed by Semites around 1500 B.C.\nIn many ancient communities, writing had an important role, especially since the writing system was known only to certain people. They were considered privileged and keepers of the gift of the gods. Egyptians, Mayans, Chinese, everyone saw writing as a gift from God. Thus, the invention of writing was attributed, by the ancient Egyptian,to the god named Thoth, who had given them the arts and sciences,for the Sumerians, writing was a gift given by the god Enlil, while for the Mayans, shaman and Itzamna wizard were creators of the world and writing.So who knew the secret of the gods was privileged, and sometimes, in this respect, was superior even to leaders.\nThe modern world has decided: writing weren't given to us by the gods, but it was invented by humans, and its invention was made independently, in places like China, Mesopotamia and Mesoamerica. Recently Europe entered in books, struggling for recognition as the place where writing was invented long ago.\nAt first, people scratched on the walls of caves, but in time turned to other media that can be transported. Thus, occured the notes on clay tablets, on the bone or tortoiseshell.\nThe ancient civilizations of Mesopotamia,are fighting for a podium place on the older writing systems. The oldest examples of writing from Mesopotamia is dating from IV millennium BC, but apparently cuneata records on various media have been made much earlier: the adhesive tablets were used by communities in Mesopotamia since 8000 BC.\nThe tablets used were simple (revealed in today's Turkey, Syria, Israel and Jordan), but others, were more complicated, where writing was decorated with all kinds of signs.These small plates, in their evolved from, occurred only in IV millennium BC. AD and it seems that they were used to record trade. Tablets were stored either in \"envelopes\" made from clay, or they were connected with thick twine.\nAnd the Chinese have proved to be inventive, but one thing should be noted: the Chinese system of writing was registering little changes in 3500 years of evolution.The oldest forms of Chinese writing were found on the frog shell and animal bones, being dated between 1500-1000 BC. Their features? Pictographic symbols, that are stylized representations of the objects that they represent. Later, the Chinese have placed their notes on bronze vessels and some of the oldest forms of writing are used today when it comes to art of calligraphy.\nFor the V-VI centuries BC, the Chinese have started to harmonize the system of writing and to remove variations in different regions, so outlined then writing has been preserved until today , only in the twentieth century it was less simplified.\nComplex proved to be the system of Egyptian writing,so hieroglyphics scratched were known only to few people: the scribes or members of religious orders.Therefore, often, even all Pharaohs were not aware of the writing system. So, in addition to hieroglyphics, the Egyptians developed simpler writing systems : hieratic, used by priests, and Demotic, used by ordinary people.Among the oldest inscriptions in hieroglyphs are those known as Narmer and dated to around 3200 BC.\nThe Latin alphabet is the most widely used system of writing and is dated around 700 BC.","The Upper Palaeolithic (Upper Paleolithic or Late Stone Age) is the third and last part of the Palaeolithic period. It lasted from about 40,000 to 10,000 years ago. Humans used tools for hunting and fishing. They also developed cave paintings. In this period, the Neanderthal man completely disappeared, leaving Homo sapiens as the only surviving species in the human genus.\nThe first modern humans found in Western Europe date back to about 36,000 years ago. Those fossils were found in the south-west of Romania. The founds were made in a stone cave called Peștera cu Oase.\nEvidence for belief in the afterlife in the Upper Palaeolithic: appearance of burial rituals and ancestor worship.\nCulture[change | change source]\nVenus figurines[change | change source]\nPossibly among the earliest traces of art are Venus figurines. These are figurines (very small statues) of women, mostly pregnant with visible breasts. The figurines were found in areas of Western Europe to Siberia. Most are between 20,000 and 30,000 years old. Two figurines have been found that are much older: the Venus of Tan-Tan, dated to 300,000 to 500,000 years ago was found in Morocco. The Venus of Berekhat Ram was found on the Golan Heights. It has been dated to 200,000 to 300,000 years ago. It may be the one of the earliest things that show the human form.\nToday it is not known what the figurines meant to the people who made them. There are two basic theories:\n- They may be representations of human fertility, or they may have been made to help it.\n- They may represent (fertility) goddesses.\nScientists have excluded that these figurines were linked to the fertility of fields, because agriculture had not been discovered at the time the figurines were made.\nThe two figurines that are older may have mostly formed by natural processes. The Venus of Tan-Tan was covered with a substance that could have been some kind of paint. The substance contained traces of iron and manganese. The figurine of Berekhat Ram shows traces that someone worked on it with a tool. A study done in 1997 states that these traces could not have been left by nature alone.\nCave paintings[change | change source]\nCave paintings are paintings that were made on the walls or roofs of caves. Many cave paintings belong to the Palaeolothic Age, and date from about 15,000 to 30,000 years ago. Among the most famous are those in the caves of Altamira in Spain and Lascaux in France.p545 There are about 350 caves in Europe where cave paintings have been found. Usually, animals have been painted, like aurochs, bisons or horses. Why these paintings were done is not known. They are not simply decorations of places where people lived. The caves they were found in usually do not show signs that someone lived in them.\nOne of the oldest caves is that of Chauvet in France. Paintings in the cave fall into two groups. One has been dated to around 30,000 to 33,000 years ago, the other to 26,000 or 27,000 years ago.p546 The oldest known cave paintings, based on radiocarbon dating of \"black from drawings, from torch marks and from the floors\". As of 1999, the dates of 31 samples from the cave have been reported. The oldest paintings have been dated from 32,900±490 years ago.\nSome archaeologists have questioned the dating. Züchner believe the two groups date from 23,000–24,000, and 10,000–18,000 years ago. Pettitt and Bahn believe the dating is inconsistent. They say the people at that periods of time painted things differently. They also do not know where the charcoal used to paint some things is from, and how big the painted area is.\nPeople from the Palaeolithic era drew well. They knew about perspective, and they knew of different ways to draw things. They also were able to observe the behaviour of animals they painted. Some of the paintings show how the painted animals behaved. The paintings may have been important for rituals.\nRelated pages[change | change source]\nReferences[change | change source]\n- \"Upper Paleolithic from Academic Press Dictionary of Science and Technology\". credoreference.com. 2011 [last update]. http://www.credoreference.com/entry/apdst/upper_paleolithic. Retrieved July 21, 2011.\n- \"Paleolithic Period (anthropology) -- Britannica Online Encyclopedia\". britannica.com. 2011 [last update]. http://www.britannica.com/EBchecked/topic/439507/Paleolithic-Period. Retrieved July 21, 2011.\n- \"The Stone Age\". history-world.org. 2011 [last update]. http://history-world.org/stone_age.htm. Retrieved July 21, 2011.\n- Trinkaus, Erik (2003). Early modern human cranial remains from the Pestera cu Oase, Romania. (45 ed.). Journal of Human Evolution. pp. 255–259.\n- \"'Oldest sculpture' found in Morocco\". BBC News online. 23 May 2003. http://news.bbc.co.uk/1/hi/sci/tech/3047383.stm.\n- Alexander Marshack (1997). \"The Berekhat Ram figurine: a late Acheulian carving from the Middle East\" (pdf). http://www.utexas.edu/courses/classicalarch/readings/Berekhat_Ram.pdf.\n- Klein, Richard G. 2009. The human career: human biological and cultural origins. 3rd ed, Chicago.\n- Quotes from Clottes 2003b p214.\n- Archaeologists sometimes use the phrase \"B.P.\" (before the present day) to mean \"years ago\"\n- Clottes 2003b p33. The oldest is sample Gifa 99776 from \"zone 10\". See also Chauvet (1996 p131, for a chronology of dates from various caves. Bahn's foreword and Clottes' epilogue to Chauvet 1996 discuss dating.\n- Züchner, Christian (September 1998). \"Grotte Chauvet Archaeologically Dated\". Communication at the International Rock Art Congress IRAC ´98. http://www.rupestre.net/tracce/12/chauv.html. Retrieved 2007-12-23.\nClottes (2003b), pp. 213-214, has a response by Clottes.\n- Pettitt, Paul; Paul Bahn (March 2003). \"Current problems in dating Palaeolithic cave art: Candamo and Chauvet\". Antiquity 77 (295): 134–141. http://www.antiquity.ac.uk/ant/077/Ant0770134.htm."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:922f9ad6-a9dd-43f7-9e21-4b138a43bfb9>","<urn:uuid:c36dd9f4-9e71-4d92-b916-49935ced55ed>"],"error":null}
{"question":"I study war memorials and their historical significance. How does the USS Arizona serve as both a historical artifact and a memorial site, and what management challenges does this dual role present?","answer":"The USS Arizona functions as both a crucial historical artifact from the Pearl Harbor attack and a National Historic Landmark under the National Park Service's care. As a historical artifact, it provides tangible evidence of the December 7, 1941 attack, with its physical remains being actively documented and studied using various technologies. From a management perspective, the site presents unique challenges that require balancing preservation with public access. Park managers must develop strategies for monitoring and caring for the submerged vessel while simultaneously fulfilling their role in interpreting and presenting this national heritage site to visitors. The site requires careful stewardship to protect its historical significance while serving as a memorial that helps the public understand its importance in American culture.","context":["On 7 Dec 1941, the US Naval base, at Pearl Harbor, Hawai’i, was attacked from the air. Among the many vessels that were lost, the USS Arizona and USS Utah are now National Historic Landmarks, under the care of the National Park Service. Multibeam Sonar, LIDAR, and photogrammetry were used to survey the ships, monuments, and other historical structures. These data will be tied together to create a 3D visualization of the underwater and above-water structures; providing a valuable tool to allow the US Park Service to monitor degradation of the ships over time. The data will also be used to give park visitors a unique view of the ships.\nThe Sonic 2024 was used to survey most of the Arizona using a small vessel of opportunity. Due to very shallow water and the Memorial itself, a Sonic 2020 was mounted on an Unmanned Surface Vessel (USV) to survey the areas where the manned vessel could not survey. The USS Utah was surveyed solely with the Sonic 2024. R2Sonic’s I2NS provided inertial navigation. QPS QINSy software was used for the multibeam data collection. Multibeam data were post processed in QINSy, Fledermaus, and HyPack. Applanix POSPac processing was also employed for PPK positioning. LIDAR was used to scan the above-water structures. Dive teams performed visual inspection and photographed the hull structures.\nA big mahalo goes out to the following:\nNational Park Service for coordinating the activities;\nAutodesk’s Stategic Projects division for project management, tying all the data together, and underwater photogrammetry;\nR2Sonic for the bathymetric survey and post processing the 2024 data;\nEtrac for supplying bathymetric survey talent and post processing the 2020 data;\nDeep Ocean Engineering for providing the USV for the 2020 survey;\nGilbane Building for providing land surveying and LIDAR;\nU.S. Navy dive team and the U.S. Navy Public Affairs.\nThe discovery of HMS Erebus, one of Sir John Franklin’s two lost ships, is a significant occasion for Fisheries and Oceans Canada as it showcases the important work of the Canadian Hydrographic Service (CHS) and the Canadian Coast Guard (CCG). Along with public and private partners, the Canadian Hydrographic Service and the Canadian Coast Guard conducted the 2014 Victoria Strait Expedition with a number of common objectives such as mapping Canada’s Arctic seabed, promoting arctic research and searching for the lost ships of Sir John Franklin’s 1845 voyage to find the Northwest Passage. The data collected from the Canadian Hydrographic Service’s R2Sonic 2022 multibeam sonars was used to produce a three-dimensional model of HMS Erebus that was used by marine archeologists from Parks Canada to confirm the identification of the wreck. The multibeam data will continue to be used by archaeologists and researchers as they conduct further analyses of HMS Erebus. YouTube video. Wikipedia article.\nCSL Kinglett and CSL Gannet – two 7-metre hydrographic launches each equipped with R2Sonic 2022 multibeam sonar, Applanix POSMV V5 position and orientation system, AML Oceanographic sound velocity sensors and utilizing Hypack/Hysweep for data collection. The survey was conducted using post processed kinematic positioning derived from Applanix POSPac MMS. Data processing and image rendering was completed using CARIS HIPS.\n2014 marks the 70th Anniversary of the Allied landings on the beaches of Normandy – better known as D-Day. The vast naval operation resulted in many vessels being sunk by mines, enemy fire or submarines. A large scale hydrographic survey was organised to map and document the state of those wrecks. R2Sonic’s U.S. agent, Measutronics (Lakeland, FL), provided much of the equipment required for the survey, including their Sonic 2024. R2Sonic provided an UHR (700 kHz) upgrade to Measutronics’ Sonic 2024. R2Sonic also provided survey personnel support to assist in setting up and getting the multibeam survey operations off to a good start. Hypack was used for data collection and presentation in the television documentaries.\nThe results of the survey are documented in the following television programmes and will be presented as a movie to the veterans and dignitaries at the D-Day Anniversary ceremony in France (06 June 2014):\nUS: NOVA (PBS) “D-Day’s Sunken Secrets” 2 hours\nUK: Channel 5 “D-Day’s Sunken Secrets” 1 hour and very different from the NOVA programme of the same name\nFrance: “Opération D-Day La cartographie des épaves du débarquement”\nData collected using the Sonic 2024 sonar.\n37 metres max depth, shallowest part of hull 17 m depth. Hull length approximately 85m.\nLocation: 37° 52.1617′, -122° 25.2500′. Marked on nautical charts as a hazard, wreck name and type is unknown. 3D anaglyphs were produced using in-house experimental software.\nData collected using the Sonic 2024 sonar at 700 kHz (UHR) and 400 kHz, using QINSy, Qloud, and rendering in Feldermaus.\nData is a composite of 3 lines. 24 metres depth. Survey by Swathe Services.\nAdditional reference: DiveSiteDirectory\nData collected using the Sonic 2024 sonar at 400 kHz using QINSy. 21 metres depth.\nAdditional reference: Wikipedia\nData collected using the Sonic 2024 sonar at 400 kHz using Hypack, edited on Fledermaus. 8 to 20 metres depth.\nSurvey by Aspect Land & Hydrographic Surveys.\nData collected using the Sonic 2024 sonar at 400 kHz using QINSy data collection software.\nQuay in Valparaiso showing marine growth and displaced blocks. Vessel is Bentos survey boat.\nData collected using the Sonic 2024 sonar at 400 kHz using QINSy data collection software and Fledermaus visualization software.\nTest mapping area and objects around the Breakwater Fort in 11 metres water depth.\nData collected using the Sonic 2024 sonar at 400 kHz using QINSy and QLoud data collection software.\nCurtin Artificial Reef, Moreton Bay, Australia. Numerous barges, ships, concrete pipes, bouys, and tires make up this reef.\nWater depth is approximately 25 metres.\nData collected using the Sonic 2022 sonar at 400 kHz using QINSy and QLoud data collection software.\nData collected using the Sonic 2024 sonar at 400 kHz using PDS 2000 data collection software..\nData collected using the Sonic 2024 sonar at 400 kHz using HYPACK data collection software.\nSurvey data near the edge of the Tokyo Canyon.\nData collected using the Sonic 2024 sonar at 400 kHz, 20 us pulse width and PDS 2000 data collection software.\nData collected during demo using the Sonic 2024 sonar at 400 kHz, 20 us pulse width and QINSy data collection software.\nData collected by eTrac using the Sonic 2024 sonar and QINSy data collection software.","USS ARIZONA MEMORIAL\nSubmerged Cultural Resources Study:\nUSS Arizona and Pearl Harbor National Historic Landmark\nChapter I: Introduction\nThis monograph is one in a series of reports that emanate from the offices of the Submerged Cultural Resources Unit in Santa Fe, New Mexico. Intended to fulfill several functions, it is primarily a source document for managers and researchers who will be involved in future stewardship of the USS ARIZONA and other period resources in Pearl Harbor. This document discusses the road thus far traveled and suggests future directions. It becomes a milestone of sorts in the administrative history of the USS Arizona Memorial and serves to define explicitly the values believed worthy of protecting and interpreting in a national context.\nThis report also fulfills professional researchers' obligations to be explicit about what they did and why they did it. Their data permits a realistic evaluation of the research process by other professionals. The report also ensures the survival of the knowledge gained in a way that allows cumulative understanding, which is the ultimate pursuit of all scientific inquiry. For the general public, the following is a guide so readers may select chapters of greatest interest to them.\nChapter II discusses the history of the Pearl Harbor attack and complements the discussions of the archeological record presented in Chapter III. It is written by Daniel Martinez, who has served as an interpreter and historian at the Arizona Memorial for many years. Daniel emphasized the aspects of the attack that enriched our understanding of the remaining archeological vestiges in the harbor. Consequently, heavy emphasis is given to the USS ARIZONA and USS UTAH because they are the only two vessels still remaining, although we acknowledge that many others played important roles in that historic event. Historic photos credited to \"NPS: USAR\" in this section are from the photo collection maintained at the Memorial by the National Park Service. This chapter should interest a general reading audience.\nChapter III discusses the archeological method, activities and results. It emphasizes analysis and description of the remaining fabric of the ARIZONA and the UTAH from the perspective of an imaginary swim through the sites. It is written by the volume editor, Dan Lenihan, who was principal investigator for all phases of the field studies, and by Larry Murphy, a SCRU Archeologist who was intimately involved with the design and implementation of the project from the beginning. This chapter may be of general interest but tends to be more technical than Chapter I and II and is directed to a professional and managerial audience.\nChapter IV, a highly technical chapter that covers the special study of biofouling and corrosion conducted in 1986, was written by Scott Henderson of the Naval Ocean Systems Center in Hawaii. The chapter is aimed at scientists and park managers who face similar management issues.\nLike most products of the Submerged Cultural Resources Unit, this document reflects a blending of the objectives of park managers with those of cultural-resource specialists (archeologists, historians and anthropologists). Chapter V focuses on the unique perspectives of the managers ultimately responsible for the site's disposition. Gary Cummins and Bill Dickinson served as consecutive superintendents of the USS Arizona Memorial from the beginning of the National Park Service stewardship to 1988. In this chapter, they describe the process of learning the nature of the resource they were managing and developing strategies for monitoring and caring for it. This chapter, although aimed at fellow managers, should prove interesting to most readers.\nChapter VI presents a framework for understanding the significance of various submerged remains of the Pearl Harbor attack as part of our national heritage. This chapter is written by National Park Service Maritime Historian James P. Delgado, who nominated all the sunken vessels thus far designated as National Historic Landmarks, including the USS ARIZONA and USS UTAH. This section of the report will particularly interest World War II historians and managers of war memorials who deal on a daily basis with public perceptions regarding worth and importance. It will also be informative to the anthropologist studying symbols and icons in American culture.\nLast Updated: 27-Apr-2001"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:72a2365f-8721-4c63-8a23-e59111081981>","<urn:uuid:dee4bf31-cb36-41a9-b1f2-fd246ce3757e>"],"error":null}
{"question":"How are email open rates technically measured?","answer":"Email open rates are measured by automatically adding a piece of code that requests a tiny, invisible image from web servers. When a reader opens the email, the image is downloaded and recorded as an open. However, this measurement isn't 100% accurate since it only works if the reader's email client can display HTML with images enabled. Text-only emails can't record opens except when links are clicked, and preview panes might download images automatically without actual reading.","context":["Our customers often ask us what â€˜open rateâ€™ means, and whether the open rate they are getting is any good or not. Weâ€™ve put together the following guide to help you understand what an open actually is, how they are measured and what typical rates are.\nWhat is an open rate?\nOpen rate is a measure of how many people on an email list open (or view) a particular email campaign. The open rate is normally expressed as a percentage, and at Fantastic Edge we calculate it as follows:\nSo a 20% open rate would mean that of every 10 emails delivered to the inbox, 2 were actually opened.\nHow do you measure an open?\nWhen each email is sent out, we automatically add a piece of code that requests a tiny, invisible image from our web servers. So when a reader opens the email, the image is downloaded, and we can record that download as an open for that specific email.\nIt is important to understand that the open rate is not a 100% accurate measure. Recording an â€˜openâ€™ can only happen if the readers email client is capable of displaying html with images, and that option is turned on. So if you are sending text-only emails, there is no way to record open rates (the exception is if they actually click a link). Similarly, people reading your html email without images showing will not be recorded as opens.\nAnother issue is that your readers may have a preview pane in their email client. That preview pane might be displaying your email automatically (and therefore downloading the images) without the reader ever having to click on it or read it.\nSo you should never take your open rate as a hard and fast number, because you can never know the true figure. It is much better used as general guide, and as a way of measuring the trends on your email campaigns.\nWhat is a typical open rate?\nReally, there is no typical open rate. The rate obtained for any list, or group of lists will depend on how it was measured, when it was sent, the size of the list and a zillion other potential variables. There is no shortage of benchmark numbers out there, but even between benchmark figures you will find big variation in the reported open rates.\nSo instead of giving a specific percentage, weâ€™ve come up with the following chart.\nThere are certainly some broad trends in open rates.\n- As list size goes up, the open rate tends to fall; possibly because smaller companies are more likely to have personal relationships with their list subscribers.\n- Companies and organizations that are focusing on enthusiasts and supporters, like churches, sport teams and non profits see higher open rates\n- More specific niche topics, like some manufacturing areas also typically have higher open rates than emails on broader topics\nWhy donâ€™t you just give me a number!\nSo what if youÂ just have no idea of what is a reasonable open rate? Based on everything we have seen here at Fantastic Edge, and on the other research out there, the bottom line is this:\nIf you are getting an open rate between 20% and 40%, you are probably somewhere around average.\nVery few lists of reasonable size are getting much above 50% open rates from normal campaigns. Your list may have some specific factors that give you higher rates; if so, well done.\nHowever, donâ€™t expect to be getting 80% open rates. People are too busy, inboxes are too full and the measurements are technically limited. If, after all that, you are still interested in seeing specific figures, see the footer for some references you can browse through.\nHow can I increase my open rate?\nThere are a ton of elements you can vary to try to entice more of your subscribers to open up your emails. Here are just a few things you could try:\n- Experiment with your subject lines: Try including details about the content of the email right in the subject line, instead of using your standard subject.\n- Send on a different day: Are your subscribers too busy on a Wednesday morning to read your email, leaving it languishing down the inbox? Maybe a Friday afternoon email would be welcomed.\n- Get the important content up the top: Remember that many people will see a preview of your email before deciding to open it or ignore it. Make sure your email is recognizable, and that your key points are in the top third."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:c8a8e7a3-4284-4bc7-803c-01209765c0fa>"],"error":null}
{"question":"What is the comparative mortality impact of heart disease versus breast cancer in American women, and how do these conditions specifically affect minority women?","answer":"Heart disease has a larger mortality impact on American women than breast cancer. Heart disease and stroke together cause one-third of all women's deaths, which exceeds all cancer deaths combined. For breast cancer specifically, while it affects 1 in 8 women, African American women have the highest death rate from breast cancer, with many of these deaths being preventable. Both conditions can be managed better with early detection - heart disease through risk factor control and breast cancer through screening, where 9 out of 10 women can be cured if found and treated early.","context":["Heart Disease FAQs\nReviewed by William C. Shiel Jr., MD, FACP, FACR\nTest your Knowledge!\n- In the U.S., 1 in every 4 deaths is caused by heart disease. True or false?\n- What is meant by the term myocardial infarction?\n- What does sudden cardiac arrest mean for the heart?\n- What are symptoms of a heart attack?\n- Heart disease is the leading cause of death of American women. True or false?\n- What are risk factors for heart disease?\n- In the heart, a clogged artery causes a heart attack. What does a clogged brain artery cause?\n- When heart trouble is sensed, who waits at least two hours before help is called: Men or women?\n- The term \"heart failure\" means the heart has stopped working. True or false?\n- People can be born with heart disease. True or false?\n- Has heart disease ever been responsible for half of the deaths in America?\n- What is the most common type of heart disease in the U.S.?\n- What is the medical term for chest pain?\n- Improve your Health I.Q. on Heart Disease\n- Heart Disease Related Slideshows\n- Heart Disease Related Image Collections\nQ:In the U.S., 1 in every 4 deaths is caused by heart disease. True or false?\nHeart disease causes the deaths of over half a million people in the U.S. annually, which totals a quarter of all deaths in men and women each year. Approximately three-quarters of a million U.S. heart deaths are from heart attack. Most of these are first-time heart attacks.\nQ:What is meant by the term myocardial infarction?\nA:The medical term for heart attack is myocardial infarction.\nQ:What does sudden cardiac arrest mean for the heart?\nA:When the heart suddenly stops beating, it is referred to as a sudden cardiac arrest.\nSudden cardiac arrest stops the blood from flowing to vital organs, including the brain.\nQ:What are symptoms of a heart attack?\nA:The many symptoms of a heart attack include fatigue, weakness, shortness of breath, dizziness, palpitations, chest discomfort and/or pain, chest heaviness, nausea, vomiting, and pain that radiates to the jaw, arm, or throat.\nSometimes heart disease causes no symptoms at all.\nPeople with the symptoms above persisting for longer than five minutes should seek emergency medical care after calling 9-1-1. Your doctor should be notified for symptoms lasting less than five minutes. Urgent evaluation and care is key in preventing injury and death.\nQ:Heart disease is the leading cause of death of American women. True or false?\nHeart disease and stroke are the leading killers of women, leading to a third of all deaths, which is more than all cancer deaths combined! Older women are more likely to have heart disease, however all women should be screened for prevention and early treatment.\nQ:What are risk factors for heart disease?\nA:There are many risks for heart disease.\nRisk factors for heart disease include smoking, high blood pressure, diabetes, high cholesterol, being overweight, family history of heart disease, lack of exercise, and age (over 55 years for women).\nQ:In the heart, a clogged artery causes a heart attack. What does a clogged brain artery cause?\nWhen blood flow is significantly impaired to the brain a stroke occurs. A stroke that results from inadequate blood supply to the brain is referred to as an ischemic stroke. When a blood vessel to the brain breaks open and bleeds, it causes a hemorrhagic stroke.\nQ:When heart trouble is sensed, who waits at least two hours before help is called: Men or women?\nA:Both men and women.\nUnfortunately, Americans typically wait over two hours before calling for help when they have symptoms of heart disease. This can be a dangerous mistake.\nQ:The term \"heart failure\" means the heart has stopped working. True or false?\nWhen the heart does not pump strongly enough to circulate blood throughout the body, the condition is referred to as heart failure. This can occur because of heart muscle weakness or inadequate filling of blood into the heart chambers.\nQ:People can be born with heart disease. True or false?\nCongenital heart disease and congenital heart defects are heart problems that are present at birth. There are many forms of heart defects including abnormal design of the heart muscle, heart valves, and blood vessels into and out of the heart. Heart defects are common birth abnormalities, present in over 30,000 infants born annually.\nQ:Has heart disease ever been responsible for half of the deaths in America?\nHeart disease and stroke were the causes of 50% of deaths in the U.S. by 1950. At that time, it was not fully understood what led to cardiovascular disease and heart deaths were common in people 50 years of age.\nQ:What is the most common type of heart disease in the U.S.?\nA:Coronary artery disease.\nThe most common form of heart disease in the U.S. is coronary artery disease (CAD). CAD causes heart attacks because the supply of blood and oxygen to the heart muscle is inadequate. Lifestyle changes and medications can reduce the risk of CAD.\nQ:What is the medical term for chest pain?\nSource quiz on MedicineNet\nImprove your Health I.Q. on Heart Diseaseback to top ↑\nHeart Disease Related Slideshows\nGet the latest treatment options.","We’re fighting breast cancer in minorities with the American Cancer Society. Find out how we’re helping to fill the information gap.\nAlthough October is National Breast Cancer Awareness Month, women of color should know about breast cancer every month. But awareness alone, is not enough: Knowledge + Action = Power! ®.\nAlthough breast cancer occurs in 1 out of 8 women, it can often be cured without breast removal if found and treated early. African American women have the highest death rate from breast cancer, and many of these deaths are preventable. Too many women of color who develop breast cancer don’t see a doctor at once because of fear of losing a breast, or looking different. With early diagnosis and treatment, physical appearance is often not a problem, and many complications can be avoided.\n- If breast cancer is found and treated early, 9 out of 10 women can be cured. Men get breast cancer, too, but it’s less than 1%.\n- Although there are no simple ways to prevent breast cancer, women can do a lot to increase early diagnosis and treatment. Remember, although prevention is always better than a cure, early disease detection is the next best thing.\nKey Risks Women Can Avoid or Control\n– First pregnancy after age 30\n– Having used oral birth control pills (for some)\n– Hormone treatment after menopause\n– Drinking alcohol heavily\n– Being overweight or obese\n– Not having enough physical activity\nBy the way, physical activity is also good for preventing and treating diabetes, obesity, high blood pressure (hypertension), and heart disease.\nHow to Detect Breast Cancer Early\nWomen should know key signs and symptoms of possible breast cancer and if they observe any of them, see their doctor at once. If they don’t have a regular doctor, they should go to a community health center or hospital. Here are the key signals:\n– A breast lump or thickening that feels different from the surrounding tissue, however, a lump often doesn’t mean cancer.\n– Bloody discharge from the nipple\n– Change in the size or shape of a breast\n– Changes in the skin over the breast, like redness or dimpling\n– Peeling, scaling, redness or flaking of the nipple, or skin\n– Inverted nipple.\nAlthough most breast changes don’t turn out to be cancer, women should still see a doctor for any of the signals above.\nRecommended Screening Tests for Early Breast Cancer Detection\nScreening tests before women develop signs of breast cancer often result in earlier diagnosis and treatment, and thus a much greater chance of cure.\nThe three most important screening tests are:\n(1) breast self – examination at the same time every month,\n(2) a mammogram (x-ray of the breast) and\n(3) clinical breast examination – by a physician or other skilled health professional .\nThe American Cancer Society, and we at Health Power both recommend that women 20 to 39 have a mammogram every 3 years, and starting at age 40, every year. In addition, starting at age 40, women should have a clinical breast examination every year.\nDon’t Let Access to Care Issues Stand in the Way\nUnfortunately, many minority women don’t have health insurance and therefore, don’t get proper care. Efforts to repeal the federal health reform act that passed last year would make matters a lot worse., which is why they should be actively opposed. Even many minority women who have health insurance, including Medicaid and Medicare, don’t get the preventive care, or treatment, they need. Don’t be caught short with unnecessary illness and early death. You may also want to learn about the benefits of 3D mammography.\nAlthough access to care is clearly a challenge for some, it should not stand in the way of breast cancer screening and early detection. Think of it this way: Respect Yourself and Protect Yourself. And, if you don’t want to do it for yourself, do it for those who love you!\nRemember the Health Power motto: Knowledge + Action = Power!®"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:34db122c-386d-49f5-a1ef-2569ebc21356>","<urn:uuid:ffc14fb7-1872-4638-b2d6-badfb0b9f685>"],"error":null}
{"question":"What were the different soil types examined in these two agricultural research studies?","answer":"The manure fertilizer study used agricultural fields in two locations in South Dakota - Beresford and Brookings sites, where corn and soybeans were grown. The polymer study examined two different soil types: a local Iowa topsoil (Nicollet, fine-loamy, mixed, mesic Aquic Hapludoll) with 4.0% organic matter, and a commercial baseball infield clay called QuickDry®.","context":["By Penelope Hillemann\nIn a 2018 study published in the Soil Science Society of America Journal , researchers compared how fertilising with cattle manure, an organic fertiliser, affects soil quality compared with inorganic fertiliser.\nEkrem Ozlu of the University of Wisconsin-Madison, USA, and his team studied two continuous cropping fields in South Dakota. From 2003 to 2015, the research team applied either manure or inorganic fertiliser to field plots growing corn and soybeans. They used low, medium, and high manure levels, and medium and high inorganic fertiliser levels. They also had a control treatment of no soil additives to provide a comparison.\nThere were two sources of manure one beef feedlot manure applied on the Beresford site, and dairy feedlot manure applied on the Brookings site. The had different nutrient concentrations, table 1.\nTable 1: The nutrient concentrations in dairy manure and beef manure were different.\nIn the summer of 2015, they collected soil samples at a variety of depths using a push probe auger. Then they analyzed the samples and found:\n• Manure helped keep soil pH—a measure of acidity or alkalinity—in a healthy range for crops. Inorganic fertiliser made the soil more acidic.\n• Manure increased soil organic carbon for all the measured soil depths compared to inorganic fertiliser and control treatments. More carbon means better soil structure, figure 1.\n• Manure significantly increased total nitrogen compared to fertiliser treatments. Nitrogen is key to plant growth.\n• Manure increased water-stable aggregates. These are groups of soil particles that stick to each other. Increased water-stable aggregates help soil resist water erosion. Inorganic fertiliser application decreased these aggregates.\n• Manure increased soil electrical conductivity at all soil depths in comparison to inorganic fertiliser and control treatments. Higher soil electrical conductivity means higher salt levels in the soil.\nOzlu and his team concluded that long-term annual application of manure improved most soil quality properties compared to inorganic fertiliser.\n“Increased electrical conductivity is one of the few negative impacts of manure,” Ozlu said.\nThe team also measured the effects of larger and smaller doses of each treatment at different soil depths. This will provide useful guidance to growers.\nSo, what could a farmer learn from this study? Ozlu said, “I recommend using composted manure, especially in solid form, because manure is the fertiliser that supports better soil quality by improving almost all soil properties. Inorganic fertiliser is better in terms of electrical conductivity, but it does not improve other soil properties and crop yields better than manure.”\nOzlu concluded, “If you think of soil as a heart, manure is the lifeblood going through it.”\nThis is a poetic view of manure, to be sure. But perhaps this humble yet enormously useful substance deserves a little poetry.\nThe research is published in Soil Science Society of America Journal, October 2018. This research work was partially supported by the Agricultural Experiment Station (AES) of South Dakota State University (SDSU), and the General Directorate of Agricultural Research and Policies, Ministry of Food, Agriculture, and Livestock, Republic of Turkey.\nFigure 1: After 12 years of fertiliser additions, the cattle manures increased soil carbon levels higher than the inorganic fertilisers.","A soil aggregate is defined as many soil particles held in a single mass or cluster, such as a clod, crumb, block, or prism (Brady and Weil, 2002). Pore space created by binding these particles together improves retention and exchange of air and water. Stability of soil aggregate refers to the ability of soil aggregates to resist disruption when outside forces are applied. Products that increase soil aggregation would benefit turfgrass growth on compacted soils with poor soil aeration. This study was initiated to determine if a liquid organic polymer mixture has any influence on turfgrass quality or soil aggregation.\nMaterials and Methods:\nThe study was conducted from December 22, 2003 to March 16, 2004 (76 day growing period) in the research greenhouse at the Iowa State University Horticulture Department, Ames, IA. SoilTech was applied to two soils. Local Iowa topsoil (Nicollet, fine-loamy, mixed, mesic Aquic Hapludoll) with 4.0% organic matter was screened and dried. Commercial baseball infield clay, QuickDry®, was used as the second soil. Material for both soils was processed through a hammer mill and soil that passed a 149 micron sieve was used in the green house study. Soil was placed in 2.5 by 2.5 inch plastic pots and treated with SoilTech liquid organic polymer solution.\nThe Iowa-Soil required 100g of soil treated with 89 ml of SoilTech® and QuickDry® required 70g of porous clay material treated with 76.5 ml of SoilTech solution to fill each pot (Fig 1). Two conditions, with grass and without grass, were made to measure stability of soil aggregate. An additional set of treatment pots with grass were used for destructive sampling during root weight measurement. Pots were seeded with ‘Catalina’ Perennial ryegrass (Lolium perenne L.) at 7 lbs/1000 sqft on December 22, 2003. Fertilizer was applied 30 days after planting to supply 1.0 lb of N, P, and K/1000 sqft. One inch of water per week was applied to promote growth during the study. On 16 Mar 2004 at the end of the study period, aggregate stability was measure according to a modified method by Cambardella and Elliott (1993).\nThe experimental design was a randomized complete block with three replications and 12 treatments (Table 1). There were 3 rates of SoilTech liquid organic polymer (0, 2, and 4%), 2 soil sources (Iowa-soil and QuickDry® soil) and 2 grass conditions conditions (with and without grass) for a total of 12 treatments. The data were analyzed using PROC ANOVA of the SAS software, Version 8 of the SAS System for Windows (SAS Institute, 1999). Means were separated (α = 0.05) by Fischer’s protected LSD\nThe greenhouse study was initiated as a preliminary study to determine if there was any beneficial response from SoilTech treatment. Results of the preliminary findings were to serve as the basis for further study.\nThe aggregate particle size distribution and the aggregate mean weight diameter (MWD) are presented in Table 2. Mean weight diameter of soil aggregates is an indication of the stable fraction of the aggregates in the soil system. A higher mean weight diameter value indicates more stable aggregates. Treatment effects were significant for mean weight diameter. The Iowa-soil with grass had more stable aggregates when treated with 2% and 4% SoilTech (MWD 1.09 and 0.93, respectively) compared with the untreated control (MWD 0.62). Increasing SoilTech rate from 2% to 4% did not influence aggregate MWD.\nTable 1:Treatments showing 3 levels of SoilTech, 2 soil types, and 2 levels of grass cover.\nTable 2: Summary ANOVA, aggregate size distribution, and aggregate mean weight diameter for SoilTech (S.T.) treatments evaluated in a green house study conducted 22 Dec 2003 to 16 Mar 2004.\n* Significant at 0.05 probability level\n** Significant at 0.01 probability level\n† Mean weight diameter\nBrady, N.C, and R.R. Weil. 2002. The nature and properties of soils. 13th ed. Pearson Education, INC., NJ.\nCambardella, C. A. and Elliott, E. T. 1993. Carbon and nitrogen distribution in aggregates from cultivated and native grassland soils.\nSoil Sci. Soc. Am. J. 57:1071-1076.\nSAS Institute. 1999. The SAS system for windows, Version 8. SAS Institute Inc., Cary, NC."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:5aab9cef-c96e-45d1-894d-55e8d8f79b8f>","<urn:uuid:f3ed6cc7-c8dc-484b-a57f-34f4ca1a1a34>"],"error":null}
{"question":"How do persistent pollutants like DDT bio-accumulate through the food chain and what are their ecological impacts on predator species?","answer":"Persistent pollutants like DDT, PCBs, and heavy metals bio-accumulate by being stored in fat and become concentrated through the food chain. In the 1950s, DDT had severe impacts on raptors - it was absorbed into soils, persisted for long periods, and accumulated in fish. When raptors consumed contaminated fish, the DDT caused their eggshells to thin by inhibiting calcium production, which reduced their reproduction rates. While many pollutants are now banned in the U.S., trace amounts can still be found in raptors and large fish. The full long-term effects of bio-accumulation in predator species remain unknown, but it is associated with reduced reproduction or death in many species.","context":["The blue whale is the largest animal ever known to live on Earth. They are up to 100 feet long and can weigh 200 tons.\nThreats to Wildlife\nOur wildlife face a variety of threats that test their ability to survive and reproduce. Wildlife are forced to either adapt to life with humans or face extinction.\nWhen we expand our territories, we invade wildlife’s territories. When forests are cleared and fields subdivided, wildlife is affected. Some species that can not evade bulldozers, like salamanders and turtles, may die outright. Others, like birds and some mammals, are forced into adjacent patches of habitat. That habitat may not be suitable for them to survive. Then we force our wildlife to navigate onto roads, across power lines, and around wind turbines.\n- Habitat Loss\n- Climate Change\n- Invasive and Exotic Species\n- Illegal Trapping and Poaching\n- Accidental Deaths\nThe survival of every species of wildlife is critical to preserving our state’s rich biodiversity and unique natural history. Over time, we have altered our landscape to mostly benefit people and our civilization. We cut down forests for agriculture and housing developments, we introduce exotic species, and we change natural processes by releasing pollutants and greenhouse gases. The major threat to wildlife in New Jersey is habitat loss. However, wildlife face a variety of threats that include climate change, invasive/exotic species, pollution, illegal trapping, and accidental deaths.\nHabitat loss is the destruction of habitat. Habitat fragmentation is the degradation, destruction, or alteration of once continuous habitat when we alter and “chop up” the environment. Humans are the main cause for the loss of habitat. Wildlife that used to live there are usually displaced or killed. It is the leading cause for the loss of species from extinction.\nThe effects can be devastating. Gaps and breaks in large patches of habitat create suitable habitat for less desirable species and unsuitable habitat for native species. For example, in North America before European settlement the Brown-headed cowbird only occupied the Great Plains region. Today, its range has dramatically increased. Over time land use-land change practices fragmented forested habitat and created optimal cowbird habitat. Habitat fragmentation has allowed the cowbirds to expand their range to most of the continent south of the Arctic.\nHabitat loss is the leading cause for the loss of species from extinction.\nMany migratory songbird populations have declined because they are very susceptible to cowbird parasitism. Songbirds that nest in forested areas near cowbird habitat (open areas) are the most vulnerable. The cowbird is a generalist parasite (the female relies on other birds to raise its young). It lays its eggs in other smaller bird’s nests. Most birds can spot the different egg, but most do not notice and keep incubating. Generally, the cowbird’s egg hatches first (warblers’ eggs hatch in 12-14 days; cowbirds’ eggs usually hatch in 10-13). This slight advantage gives the cowbird nestling a head start. The cowbird nestling almost always outcompetes the other nestlings for food.\nClimate Change or global warming is the overall increase in average temperatures on Earth. The rate of warming has increased dramatically due to the increased outputs of greenhouse gases (particularly carbon dioxide) since the industrial revolution. Its effects on wildlife are dramatic. Entire populations will be effected. Many species are already in jeopardy of becoming extinct, like the Polar bear. Other species will have to adapt to a warmer planet. It is believed that many ecosystems will shift north. Our climate in New Jersey will be like the climate in South Carolina if nothing is done to reduce our impact on our planet. Climate change may also significantly alter the chemical balance of the seas, off-shore currents, and plankton distribution and abundance, thereby affecting migration routes of marine species and impacting the entire food web.\nInvasive and exotic species are species that were introduced to North America that reduce biodiversity of native species. Many exotics were accidentally introduced during the colonial times when many plants (from other continents) were used as packing materials on ships. Animals from other continents may have also hitched a ride in those same ships or they were intentionally brought to North America. Invasive plants choke out natives and do not provide the same functions in the ecosystem. Exotic species can wreak havoc on native populations of wildlife by displacing them or altering their habitat.\nMost exotics are able to quickly adapt to our environment. For example, house cats are not native and can have devastating effects on bird populations. Surveys have shown that only 35% of the known 77 million pet cats are kept indoors (American Bird Conservancy). Feral cats compete with native predators, reproduce quickly, and transmit disease. It is estimated that hundreds of millions of birds and billions of small mammals are killed by feral cats each year (American Bird Conservancy). Cats are opportunistic and kill a variety of birds. Common birds like Cardinals and endangered birds like the Piping plover all can fall prey to feral cats.\nPollution is man-made waste or by-products that are released into the environment. Pollutants can change ecosystems and can have severe effects on people, wildlife and the natural environment. Many organisms ingest or absorb harmful toxins that ultimately get passed along through the food chain. Persistent pollutants, like DDT, PCBs, and heavy metals bio-accumulate (are stored in fat) in predators. All of the effects, especially over the long-term, are unknown. In many species bio-accumulation can be associated with reduced reproduction or death.\nFor example, the widespread and heavy use of persistent pesticides (DDT and DDE) in the 1950’s caused severe declines in many species of raptors. At the time, the effects of DDT on wildlife and the environment were unknown. It was quickly absorbed into soils and the environment and it was found to persist for a very long time. It caused egg shells to thin by inhibiting the production of calcium, which reduced their rate of reproduction. DDT is highly toxic to fish. It accumulates and gets concentrated through the food chain. When they were consumed by fish-eating raptors the DDT contributed to their decline. Since then, many pollutants have been banned in the U.S.; however, trace amounts are still found in many raptors and large fish.\nNon-point source pollution can also have major impacts, but it is hard to measure because it comes from all across our landscape. When the rain falls and washes our roads clean, for example, the runoff carries car oil, metals, sediment, trace toxins, and litter (and even heat) down the storm drains and into nearby streams and rivers. It washes fertilizers, pesticides, pet waste and grass clippings from our lawns; farm fields give up soil, nutrients, and chemicals too. Fish spawning habitat may become smothered with silt. The extra nutrients and higher temperatures can cause oxygen-stealing blooms of algae and eutrophication in water bodies. Even the road salt that helps keep us safe in winter will accumulate in wetlands, causing problems for amphibians, aquatic animals, and plants. On the plus side, non-point source pollution is one threat we can all do something about, with a little care and education.\nPersistent marine debris, such as plastic bags and bottle caps, balloons, discarded fishing line, and medical waste threatens many forms of marine life. Such items may entangle and suffocate, drown, or otherwise restrict the movement of marine life or may be ingested and ultimately lead to the death of that animal. Such debris may take several hundred years to break down, thereby threatening wildlife for many years.\nAnother form of pollution is noise pollution. Whales’ primary means of communication, navigation, locating food, locating mates, and avoiding predators and other threats is through their sense of hearing, which is much more highly developed than that of humans. Noise pollution created by ship traffic or offshore construction may negatively impact whales by disrupting their normal behavior. Active sonar, such as that used by the Navy, also threatens marine mammals by disrupting navigation, foraging and communication abilities. There have been instances of whale stranding and death caused by acoustic trauma. This may be due to a fatal injury within the structure of the ear, or may result from the distressed animal surfacing too rapidly and developing nitrogen bubbles within their blood (decompression sickness).\nWildlife trafficking is thought to be one of the most profitable illegal trades in the world.\nIllegal trapping, poaching, and other demands for wildlife are a huge problem throughout the world. Many species are sought for their use as valuable products. Snakes are sought for their skins, elephants for their ivory tusks, and birds for their feathers. Wildlife are also trapped or taken from wild populations to be sold or bred in the pet trade. The worldwide demand for pets and medicinal products drives the illegal trade of wildlife, especially rare species. Sadly enough, wildlife trafficking is thought to be one of the most profitable illegal trades in the world.\nIn many parts of the world, people kill wildlife for food or to protect their food source. Northern South America is where Ospreys winter. They catch and eat fish, sometimes from fish farms. People will shoot ospreys to protect fish farming operations in these areas where they are not protected. You can help protect Ospreys by not supporting fish that are farmed in these regions.\nAccidental deaths and collisions pose considerable threats to vulnerable species. An unknown number of deaths are caused by this worldwide. Accidental entanglement in fishing nets and collisions with ships pose major threats to marine mammals, especially whales. Vehicles strike birds and other wildlife when driving along roads. Large buildings, towers, and wind turbines also injure or kill many different species of wildlife.\nWildlife Viewing Map\nLearn the best sites to view wildlife in New Jersey. Created by biologists who work to protect the species you wish to view!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:1240b5f7-ae3f-460c-add0-421a7cb809ac>"],"error":null}
{"question":"How does the size classification of graphite particles potentially mislead about their true characteristics?","answer":"Size classification of graphite can be misleading because a size statement only indicates particle size and nothing about individual crystals. For example, a product that is 100% less than 200 micrometers with an average size of 150 micrometers could consist of only 180 micrometer particles mixed with 20 micrometer particles, or all 150 micrometer particles. Additionally, larger particles of 300 micrometers that have been bent and folded can pass through a 200 micrometer screen.","context":["Dr. Flint answers the question: What is Graphite?\nImportant work is currently being done at the National University of Singapore (NUS) to standardize graphite characterization. The reason is that graphite consumers have difficulty getting reproducible results from different graphite sources. The result is that they must formulate their products using the product from a specific supplier. The reason for the lack of reproducibility is that graphite with the same grade and particle size often behaves very differently because many other characteristics have not been described. This short description tries to explain why. Currently graphite is categorized by grade and a particle size; stated differently, by the mineralogical composition and morphology of the crystal.\nThickness: Graphene is a single layer of honeycomb carbon one atom thick; essentially a plane of infinite linked poly aromatic hydrocarbons. A graphite crystal is a book of graphene; a crystal that may be composed of hundreds to millions of layers of graphene.\nAggregates: Graphite as it is produced can be that book or an entire library as graphite crystals often grow intimately together and are not processed into individual flakes. Thus, an individual particle of graphite may be one crystal or an entire population of crystals.\nStacking: How the graphene is layered, also, makes a difference in the characteristics of the graphite. The layers maybe closer or further apart, broken within the crystal, or be displaced.\nBending: a crystal of graphite bends very easily and processing often bends, folds and balls the graphite.\nSize: A size statement only gives information on the particle size and nothing about individual crystals that may aggregates or bent. In addition, the size is often quoted as being a certain percentage smaller than a particular size. This, again, says very little about the size distribution. As an example, a graphite product assured to be 100% greater than 200 micrometers could be composed of individual flakes or aggregates or any combination. A product of 100% less than 200 micrometers with an average size of 150 micrometers may be made up only of 180 micrometers and have 20 micrometers particles, or all 150 micrometers. Or, the particle may be made of up 300 micrometer particles that have been bent and folded until they fit through the 200 micrometer screen.\nGet our daily investorintel update\nIntergrowth: Often, graphite from the higher metamorphosed rocks will contain contaminates that have grown in and around or through a graphite crystal resulting in processed graphite that is not a continuous but with holes, gaps or shapes other than the theoretical hexagonal crystal.\nEncapsulation: when graphite is not processed correctly the graphite can bend and fold around contaminate particles affectively preventing removal of this contamination by physical or chemical means.\nThe quality of the graphite depends on the grade of the graphite but also on the amounts of other materials present. Some contaminates are important for some applications and not others. In addition, how the contamination is present can be important. Typical contaminates are silica, pyrite or other sulfides, host rock, sulfur, mica, kaolin or other clays, carbonates: malachite, calcite, dolomite and others, garnets, tourmaline, topaz, corundum, and many other minerals. These may occur as individual particles contained separated within the population of graphite particles, locked to the graphite, or contained between the graphene layers. Thus, the graphite grade often tells only part of the story.\nAssay reports often come with a statement of organic and graphitic carbon. These numbers can be confusing because it is actually a statement of how much carbon burns at a low temperature under the assumption that it is organic carbon. Disordered, functionalized and graphite edges can also burn at these low temperatures albeit at a slower rate. A graphite from a geothermal vein or metamorphosed rocks are highly unlikely to contain organic carbon. Such a classification should be reconsidered and labelled differently than graphite sourced from high ranking coals or artificial graphite.\nDr. Flint has been active in the graphite/graphene industry for over 25 years with experience ranging from engineering review, test work, pilot plants, process design, ... <Read more about Dr. Ian Flint>"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:e85cb6d9-8451-430a-ae07-157e3541eb33>"],"error":null}
{"question":"How do human activities threaten both rhinos and monk seals?","answer":"Both species have been severely impacted by human activities. Rhinos have been decimated by poaching, with African rhino populations dropping from 500,000 to 27,000 in the twentieth century, driven by demand for keratin. Monk seals were hunted extensively for their pelts and oil, and later persecuted as perceived threats to fish stocks. Both species also face habitat destruction - rhinos can barely survive outside protected areas, while monk seals suffered from coastal development and pollution. Today, surviving monk seal species face additional threats like death as bycatch in fishing nets and disturbance from military activities.","context":["Sudan was the last male northern white rhino left on earth, who sadly passed away from natural causes in 2018. This was a devastating loss, not only for the people who had devoted their lives to caring for him but for his entire species.\nBefore Sudan’s passing, our team was able to meet him and the incredible team at Ol Pejeta Conservancy who cared for and protected him.\nBy the time of his death, Sudan had become a globally beloved ambassador for his species. His death resonated with so many and left the world devastated. To us, his death was a stark reminder that extinction is a very real threat, one we are experiencing first-hand.\nSudan spent every minute of his life under full-time protection. His carers at Ol Pejeta were wholly devoted to his care.\nWitnessing the relationship between the people who were risking their lives daily to protect these rhinos and these incredible species, is something we will never forget. Their bond is indescribable and so closely resembles that of a parent and child.\nThese prehistoric species whose ancestors can be traced back millions of years once roamed the planet freely. This is no longer the case. Rhinos have been on frontlines of the poaching crisis around the world for the last few decades, which has severely impacted their population sizes and distribution.\nIn Africa alone, rhino populations plummeted from around 500 000 in the twentieth century to a mere 27 000 today. The heartbreaking reality is that conservancies like Ol Pejeta have become critical to the survival of rhino and other endangered species. So few rhinos can survive outside of protected areas, and as poaching syndicates have become more sophisticated, even rhinos under constant care are at risk.\nAs a team, we have documented the poaching crisis extensively. We have seen first-hand the impact poaching has on rhino populations, the orphaned rhino calves left behind and the individuals who despite the challenges remain committed to protecting these animals.\nWe often find ourselves questioning how we reached this point. How have we allowed human greed and the demand for keratin, the very same substance that composes our nails and hair, to all but eradicate an entire species?\nHowever, what keeps us going and gives us a constant source of hope are the countless organisations and individuals who keep fighting to protect these iconic animals. They are risking their lives, working under challenging circumstances and face constant obstacles like lack of funding or resources, and yet they remain devoted to ensuring that rhinos are around for generations to come.\nThis is what we are choosing to focus on today, on World Rhino Day – the hope that it is not too late to save rhinos and safeguard them for years to come, thanks to the unwavering efforts of conservationists around the world.\nOne of the most recent achievements in rhino conservation is the use of in-vitro fertilisation processes to create viable embryos. After veterinarians at Ol Pejeta realised that Fatu and Najin would be unable to reproduce, an incredible plan was put in place to ensure that northern white rhinos would have a chance at restoring their populations.\nThis complex process is the result of a remarkable collaboration between veterinarians, scientists and conservationists around the world, and forms part of Ol Pejeta’s BioRescue research project.\nThe IVF procedure involves harvesting a viable egg from a female northern white rhino and artificially inseminating it, to create a viable embryo, which will be carried to term by a surrogate female southern white rhino. Recently, scientists were able to successfully create four viable southern white rhino embryos from one donor and one procedure. This means that there is hope that the procedure could be successful on northern white rhinos too.\nThere is still a long and complicated journey ahead, but this is a victory worth celebrating. We are so grateful for initiatives like this and for organisations like Ol Pejeta and their partners who are working tirelessly to find innovative ways to safeguard rhino populations.\nIn honour of World Rhino Day, we implore you to take action for rhinos and join the fight against extinction. Here are four ways you can help protect and conserve rhinos:\nLearn more about these iconic species and their habitats. Help spread the word about the threats they are facing and how we can help save them.\nThere are a lot of different ways to do this from volunteering your time, supporting sustainable tourism, donating if you are able or even ‘adopting’ a rhino to help financially care for them. There are so many remarkable organisations that are in dire need of support such as Ol Pejeta, The Malilangwe Trust, Save The Rhino, Care for Wild, Rhino Rescue Project, and Grumeti Fund to name a few. Check them out and see how you can get involved to support rhino conservation.\nPoaching and the illegal wildlife trade are by far the most significant threat rhinos face. In recent years the poaching trade has become significantly more sophisticated, which has meant that anti-poaching teams have had to adopt improved strategies and technology to keep wildlife safe. By supporting rangers and anti-poaching units, you can help keep rhinos safe and play a role in the fight against poaching.\nWhile there is a lot of remarkable work being done on the ground to safeguard rhino populations, the situation has reached crisis level and requires further intervention. Ground-breaking research and scientific advancements like the BioRescue project being carried out by Ol Pejeta are essential in the fight to save our rhino populations. Learn more about the BioRescue project and how to support this innovative work here.","Video of the Day\nThe Caribbean monk seal (Monachus tropicalis) is another example of a marine mammal that is no longer with us. The species was exterminated in the first half of the 20th century, with the last confirmed sighting being in 1952. Habitat destruction might have played a part in their demise, although the main reason appears to have been uncontrolled hunting.\nThe Caribbean monk seal used to be plentiful in the warm waters of the Gulf of Mexico, the western Atlantic and, as the name suggests, the Caribbean Sea. Their range extended from the coasts of Georgia and South Carolina right down to the north east side of South America. The events leading to their eventual extinction began with the arrival of the first Europeans in the area at the end of the 15th century.\nCaribbean monk seals inhabited temperate to tropical waters. Although they were not creatures of the open ocean, they didn’t often venture onto the mainland, preferring isolated islands when they needed to come out of the water. This might have been an adaptation to avoid large land predators but it didn’t help the species avoid humans.\nHuman hunters killed the seals in huge numbers for their pelts and oil. Later, they were also persecuted as a perceived threat to fish stocks. A significant number of seals were killed and stuffed for exhibition in museums or caught alive for zoos. At the same time as the seals were being slaughtered in their thousands, their habitat was being degraded and polluted by the developments springing up along the coast. According to Animal Diversity Web, loss of suitable habitat was the last straw.\nThe last sightings occurred in the first years of the 1950s. By the 1960s, it was suspected the species was extinct. In 2008, it was officially confirmed with the removal of Caribbean monk seals from the US Endangered Species Act. Occasional reports of sightings still occur, but these are almost certainly misidentification of other seal species, such as the hooded seal. Exhaustive scientific surveys have found no trace of Caribbean monk seals in their former habitat.\nMonk Seal Distribution Today\nThe Caribbean monk seal has gone but two relatives remain. Both are critically endangered and may end up going the same way if current population trends continue. The survivors of the genus Monachus are the Mediterranean monk seal and the Hawaiian monk seal. Their range is now restricted to small areas of, respectively, the Mediterranean (and North Atlantic) and around the islands of Hawaii. The major threats to both species are overfishing, habitat destruction and disturbance, death as bycatch in fishing nets, pollution and deliberate persecution. The International Union for the Conservation of Natures notes that Hawaiian monk seals are also threatened by military activities in the area and may be vulnerable to diseases carried by domestic pets.\n- NOAA Fisheries -- Office of Protected Resources: Caribbean Monk Seal\n- International Union for the Conservation of Nature: Monachus tropicalis\n- Animal Diversity Web: Monachus tropicalis\n- International Union for the Conservation of Nature: Monachus monachus\n- International Union for the Conservation of Nature: Monachus schauinslandi\n- NA/AbleStock.com/Getty Images"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:4e72defb-7f46-429d-b86a-402fb546b668>","<urn:uuid:1358b1f2-0ede-4429-9833-ceafc78f9e5c>"],"error":null}
{"question":"How do the Rule of Thirds and ISO settings work together to create effective photographs in different lighting conditions?","answer":"The Rule of Thirds is a compositional guideline where the subject is placed off-center, aligning with imaginary grid lines that divide the frame into thirds both horizontally and vertically. This creates more visually engaging images than center-weighted compositions. When it comes to ISO settings, they need to be adjusted based on lighting conditions: 100-200 ISO is used for bright sunlight, 400 for shade or overcast conditions, and 800-1400 for stormy days or low light indoor situations. Higher ISO settings can help capture images in limited light when flash isn't desirable, though they may result in grainy images or 'noise'. The combination of proper composition using the Rule of Thirds and appropriate ISO selection for the lighting conditions is crucial for creating successful photographs.","context":["Photography, like most forms of art and science has rules. The rules are meant to be your guideposts to creating an aesthetically pleasing image. The rules of composition have been around since long before photography was invented; their origin goes way back to the old masters who first put brush to canvas. We have become “genetically engineered” to expect these rules in our art. However, art being what it always has been, sometimes it’s exciting and daring to break the rules.\nComposition: Science vs. Art\nAs for the science piece of photography, they are somewhat less forgiving. The rules of exposure, focus and shutter speed are pretty steadfast. If you do not have the correct exposure, the shot will be under or over exposed. Period. If the camera is out of focus, kiss that shot good bye. The shutter speed may be too slow, then the shot will have blurred motion. If that’s the effect you were going for –great. If not; too bad. Some things can be fixed in post-production, but it’s always best to get them right in the camera.\nThis post we will be discussing the rules of composition. I’ve already covered the others.\nRule of Thirds\nThe Rule of Thirds is not that difficult to understand: Have your subject off center –not smack dab dead center on the shot. Most people just starting out do not realize this. Most cameras are designed to have the subject centered; they focus that way, the exposure meter reads in a center weighted manner. Why? Because they know that most people do not get the Rule of Thirds. Don’t be most people.\nPicture the view finder with a tic tack toe grid overlaid on it. Some cameras even have a feature you can turn on to do this for you. If your camera does, use it at first, but don’t get used to relying on it; turn it on for a while to get yourself in the habit of using the rule of thirds –then shut it off.\nOn this grid, simply line up your subject to intersect with one of the lines. Why? Because since man began painting on something other than cave walls it was determined that it just looked better that way. We’ve become “hard-wired” to believe that. It’s in our DNA. When photography came along a couple hundred years ago they just applied the same rules.\nNot all subjects or scenes lend themselves to this rule, so don’t be afraid to “break” the rule if it just feel right to you. But all things considered, if you can apply the Rule of Thirds –just do it!\nPlace the horizon at the top third or the bottom third. This will make for a more pleasing composition. This is another piece of the Rule of Thirds. Don’t place it right down the center line of your shot. Think about what you are taking the picture of. For example, if you are at the beach and you want to get a nice shot of the water and sky, look at which is more interesting to look at: Is it the surf crashing on the rocks under a bright clear blue sky –or is it the dark foreboding clouds in the sky over the calm water? The part of the shot that’s more interesting is the part you want to show more of. If they are both interesting, pick one; take a couple shots both ways and decide later which you like best.\nLeading lines is a great way to draw your viewers into the shot. Use leading lines to take the eye toward the subject, or wherever you want your viewer’s eyes to go. Using the concept of leading lines along with the Rule of Thirds can make for a very powerful shot.\nFocus on the Eyes\nLike the Rule of Thirds, we humans have become hard-wired to look for eye contact. If your subject has eyes, focus on them! A great shot with out of focus eyes will leave the viewer feeling like the shot just didn’t make it. They may not even realize why. Just not being able to see the eyes clearly will detract from your image. It doesn’t matter if the subject is a person, a dog or a bug, if it has eyes focus on them!\nRules are Meant to be Broken\nThese rules are meant to help you better compose your photographs. You should practice them until they become second nature. Once you do them without even thinking about them, then you can start to think about breaking them. When composing your shot, compose it to the rules; then think about whether the shot lends itself to breaking those rules. Don’t just break them for the sake of breaking them; break them with purpose!","Exposure This is by far the most technical lesson of this session. However it is essential to begin to get a grasp of the exposure triangle concept. You must refer to your personal camera manual in order to completely understand how to use the creative modes. Be sure to take notes when necessary.\nExposure Exposure is the total amount of light that you allow into the camera\nExposure Triangle Changing any of the three elements will effect the exposure Aperture Shutter Speed ISO Aperture and shutter speed are inversely proportional to each other.\nPosing for 19 th century photographs Usually took as much a 5 minutes of Sitting perfectly still. Luckily, today we can control Shutter speeds. Opelika, AL\nShutter Speed Shutter Speed—the amount of time the shutter is open— which determines how much light is captured in the recording process – Measured in seconds: super fast 1/2000 second to 30 seconds – The slower the speed, the longer light can enter the camera. Appropriate for shooting pictures in darker situations; also great for freezing action and movement 13 Shutter speeds and aperture (f-stops) are inversely proportional.\nShutter Speed Shutter speed is measure in fractions of a second. A shutter speed greater than 1/60 th second requires the use of a stabilizing device like a tripod. Without stabilization, the photograph will not be crisp and clear. 15\nLow Light requires longer shutter speeds and a tripod Full 1 second shutter speed Langdale Mill\nLow Light requires longer shutter speeds and a tripod\nShutter Speed Must use A tripod Best for Sports shots\nISO is the measurement of how sensitive the image sensor in the camera is to light. – Measured in numbers 100, 200, 400, 800, etc. – Use a lower number when smooth crisp images are need and you have plenty of light. – Higher numbers are used when light is limited, you do not want to use a flash, or the subject is moving; may result in grainy images 20\nThe Exposure Triangle ISO – International Organization of Standardization ISO—the measurement of the sensitivity of the camera image sensor to light. Measured in numbers 100, 200, 400, 800, etc. – Lower numbers used when smooth crisp images are desired and you have plenty of light. – Higher numbers are used when light is limited, you do not want to use a flash, or the subject is moving; may result in grainy images 21\nISO ISO is the measurement of the sensitivity of the camera image sensor to light. An ISO of around 200 is average. 22\nISO An ISO of around 200 is average. Lower numbers are used when smooth crisp images are needed and you have plenty of light. Below 200 23\nISO An ISO of around 200 is average. Lower numbers are used when smooth crisp images are needed and you have plenty of light. Below 200 24\nISO Higher numbers are used in limited light conditions, Flash is not desirable subject is moving May result in grainy images 25 400 and above\nChoosing ISO Setting 100 to 200 -------Outside Bright Sun 400----Shade or overcast conditions 400----Bright light indoors 800-1400----Stormy day 800-1400 – Low light indoors\nHigher ISO 400 ISO 800 ISO Light is limited and flash was not desirable in either shot.\nISO Too little light can leave parts of your image too dark to make out details. If the ISO had been higher this photograph would have been brighter. 28 Trevi Fountain, Rome ISO 200\nHigh ISO This photo taken with ISO 800. Notice the grainy appearance. This is called noise. 29 Rome by night\nNotice the camera exposes to the light not the bride. ISO 160 30 More advanced cameras have a mechanism that will allow you to expose to individual areas with in the shot.\nAperture f-stops Aperture is the camera feature that regulates the amount of light that passes through the lens by controlling the size of the opening in the lens. It is measured in f/stops.\nAperture f-stops Stopping down Aperture is the camera feature that regulates the amount of light that passes through the lens by controlling the size of the opening in the lens. Described as the f/stop the smaller the number the wider the lens will open\nAperture f-stops The common range for f-stops goes from f/2, f/2.8, f/4, f/5.6, f/8, f/11, f/16, f/22 f/8 is a good average aperture where Most everything will be in focus. Narrow depth of field Infinite depth of field\nF/stops and depth of field Depth of Field decreases Depth of Field increases Lower f stops need less light to expose Higher f stops need more light to expose F /22 f/2 f/8\nAperture f-stops Full f-stops go from f/1.4, f/2, f/2.8, f/4, f/5.6, f/8, f/11, f/16, f/22, f/32 Each full stop value increase, decreases the light entering the camera by half. Fast lensSlow lens Shorter shutter speed Longer shutter speed Narrow depth of field Large depth of field\nAperture 37 More exposed Longer Shutter Speeds Greater Depth of field Narrower Depth of Field\nShutter Aperture Relationship The smaller the aperture, the longer the shutter speed The larger the aperture, the shorter the shutter speed.\nSunny Sixteen Rule The \"rule of sunny-16\" is simply a handy trick to remember away to set proper exposure when the sun is high in the sky And casts strong shadow on a bright day. The camera aperture is set to f/16 (hence the -16 in sunny-16). dark shadows. The sunny sixteen rule was NOT used in this photoghraph.\nSunny Sixteen Rule Aperturestopsconditions f/22-1 stop snow or beach f/16sunny-16 bright daylight dark shadows f/11+1 stop weak or hazy sun sun low in sky f/8+2 stops cloudy bright f/5.6+3 stops darker clouds subject in shadow f/4+4 stops sunset Aperture Stops Conditions\nIssues with each exposure point Changing any of the three elements will effect the exposure Aperture Shutter Speed ISO Aperture and shutter speed are inversely proportional to each other. Motion Blur Depth of Field Noise\nCreative Camera Modes Point and Shoot Modes Creative modes\nProgram Mode Shutterspeed (exposure) and Aperture are set in unison automatically. When you change the shutter speed, the aperture changes automatically When you change the aperture, the shutter speed changes automatically P\nAperture Priority Mode Av In AV mode, the aperture remains constant. By turning the main dial on your camera, you can adjust the shutter speed. The aperture remains the same. Use Av when : Want everything In focus (use high f/stop) OR Want the background to be out of Focus (use low f/stop)\nShutter Priority Mode Tv Action shot mode In Tv mode, the shutter speed remains constant. By turning the main dial on your camera, you can adjust the aperture. The shutter speed remains the same.\nManual exposure mode In M mode, the aperture and the shutter speed can be altered. By turning the main dial on your camera, you can adjust the shutter speed. The aperture remains the same. Read your camera manual to determin how to set boththe Av and the Tv In the Canon Rebel Set the Tv by turning the dial Set the Av by holding down the Av button and turning the dial.\nManual exposure mode In the Canon Rebel Set the Tv by turning the dial Set the Av by holding down the Av button while turning the dial.\nManual exposure mode In the Canon Rebel Set the Tv by turning the dial Set the Av by holding down the Av button while turning the dial. The standard exposure level is achieved when the slide bar is located in the center. This meter may be viewed on the LCD screen or through the view finder\nAE Lock Consult your camera manual for AE lock steps Exposed to the light (normal ) Exposed to a chosen darker subject in the photo ( AE lock)\nAutomatic Depth of Field Canon's auto depth of field (A-DEP) feature works by (1) finding the nearest and furthest of all the autofocus points, (2) setting the camera's focusing distance to optimally position the depth of field between these nearest and furthest points, and (3) setting the aperture so that the edges of the depth of field extend far enough to contain these nearest and furthest points."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:0b4eab89-0234-46a6-b348-d3c97edae5a9>","<urn:uuid:eb638d9c-94d9-40d2-8d1e-d019ddae2c76>"],"error":null}
{"question":"How do Carlo Martelli's Symphony No. 2 and Tchaikovsky's Symphony No. 4 compare in terms of their personal significance to the composers' lives?","answer":"Both symphonies were composed at pivotal moments in the composers' lives and reflect deep personal meaning. Martelli's Second Symphony, completed when he was only twenty years old, demonstrated remarkable technical skill and melodic invention, combining gripping intensity with graceful lyricism. It represented a significant achievement in his early career. Meanwhile, Tchaikovsky's Fourth Symphony was written in 1877, during a particularly crucial point in his life - the year of both his disastrous marriage and the beginning of his fifteen-year correspondence with his patroness Nadezhda von Meck. The Fourth Symphony reflects Tchaikovsky's sense of 'fate' which he believed controlled his destiny, describing it as 'the fateful force which prevents the impulse to happiness from achieving its goal.'","context":["Music Webmaster Len Mullenger\nCARLO MARTELLI- A GIFTED MUSICIAN by Paul Conway\nCarlo Martelli was born in 1935 to an Italian father and an English mother. He showed a keen interest in music at a very early age: as a five year old schoolboy, he heard a teacher in assembly playing Schubert's \"Marche Militaire\" and became obsessed with the piece, wanting to hear it again and again. By the age of ten he became acquainted with Italian operas and soon after this discovered his unending love of the Orchestra by means of listening to Beethoven Symphonies (to this day, his favourite symphony is Schubert's \"Great\" C Major).\nHe had violin lessons and developed his orchestral score reading whilst beginning to compose pieces of his own. He first went to the Royal College of Music on September 21st 1949 at the age of 13 as a Junior Exhibitioner where he learned elementary harmony and studied composition under William Lloyd Webber. At this time he changed his instrument from violin to viola and has remained an enthusiastic and accomplished viola player to this day. Upon leaving school at 16, he became a full-time student at the RCM, where his composition teacher was Bernard Stevens. Whilst he was studying, he went along to the Proms every summer, trying not to miss a concert and being bowled over by the unique atmosphere he found there: so, he resolved to write a grand and opulent orchestral piece which he felt would be appreciated by the Prommers, tailor-made to their taste. Thus he came to write some orchestral works for massive forces, containing every grand gesture known to him at the time: works in this category include the Festival Overture (inspired by the Festival of Britain in 1951) and his First Symphony. Tragically, both these works are now lost and so the fruits of an important stage in the composer's development are therefore unavailable to us. Mrs Swann, a friend from the local music society (and stepmother of Donald Swann) sent his score of the Festival Overture to Edmund Rubbra who was so impressed with the work that he submitted it to the reading panel of the Society for the Promotion of New Music. His First Symphony was shown to George Dyson, a move which cemented rather than won his place as a full-time student at the RCM.\nParadoxically, it is not another orchestral Titan but a work in the rarefied field of chamber music that Carlo Martelli considers to be his \"Opus 1\": a String Quartet (1953), an amazingly accomplished work for a seventeen-year-old composer. Throughout the 1950s, he composed a string of impressive, well-crafted, original and lively works including the second String Quartet (1954), the Serenade for Strings (1955), the Second Symphony (1955-56), a Terzetto for two violins and viola (1956), Shredni Vashtar, a setting for narrator, boy soprano and orchestra of a story by Saki (1956), a Quartet for Flute, Oboe, Viola and Bassoon (1958) and a Fiesta Overture for orchestra (1959). All these early works bear the hallmarks of Carlo Martelli's distinctive style which marries an accomplished technique with an instantly appealing and communicative style. Of these many pieces from the 1950s, three stand out as being exceptionally important works, remarkably mature and deeply felt: the Second String Quartet, the Serenade for Strings and the Second Symphony - their subsequent neglect and resulting descent into obscurity sadly forms but one of the many examples of injustice in the recent history of British music.\nThe String Quartet no2 (op4) is a tightly-constructed, powerful piece and represents the composer in his most serious vein. In four movements and of about thirty minutes' duration, the Second Quartet manages to cover a wide range of emotions and effects within its compact frame. The first movement, a thrillingly scored Allegro non troppo begins in a halting, fragmentary way, gradually picking up the pieces and developing a driving momentum. Every line has something of interest, woven with great contrapuntal skill into the whole texture. There is tenderness here as well as vehemence, yet the overriding impression is one of intellectual strength, a bold and vigorous opening statement. The relationship between the following Vivace Scherzo and Trio and the slow movement, a Lento entitled \"Lament\" is a subtle and complex one. The pounding rhythms of the Scherzo fade to the sparer writing of the Trio whilst at the end of the Scherzo's Da Capo, the Lento begins and towards the conclusion of this impassioned and restless movement, the Trio returns followed by the return of the pounding opening of the Scherzo returns to create a \"movement within a movement\" effect, an ear-catching touch which produces a profoundly disturbing effect in the listener, having experienced the depth of emotion generated by the \"Lament\". (Incidentally, the second subject of the Scherzo contains a series of vehement repeated chords in the manner of guitar or mandolin strumming, one of the very few places in Martelli's music where one is reminded of his Italian ancestry. The Finale is a series of finely-wrought variations, the initial theme presented Andante sostenuto but gathering pace and dynamism almost at once in the Allegro non troppo second variation. The movement continues to press forward inexorably until the last, coda-like variation marked Presto possibile, spins the piece towards a gravity-defying conclusion. The Quartet is not only exceptionally idiomatically written, with something of interest for each instrument, but its musicianship and mastery of the form shines out to the listener - this work demands to be heard.\nNo less consummately constructed and inventively scored, though altogether more genial and relaxed in mood is the Serenade for Strings (op5). The airy, wide-open spaces of the initial theme of the opening Allegro moderato sound Coplandesque but the creator of the Second String Quartet is clearly in evidence in the scoring and imaginatively deployed forces of this attractive and rewarding piece. Alternation of arco and pizzicato playing, harmonics, swiftly changing dynamics, trills, glissandi, divisi and solo writing serves notice of the rich orchestral palette created by the composer (all this by page three of the score!) Yet the movement is concise and pithy with no lingering over its more sumptuously scored passages. The solemn, ritualistic slow movement, marked Quasi una preghiera (like a prayer) is an Andante lamentoso with an important solo role allocated to the viola (it is clear that this is Martelli's instrument, so often does it shine out in his compositions, not least in the Quartet for Flute, Oboe, Viola and Bassoon!). The vital rhythms of a Tarantella inform the Scherzo-like third movement; the 'celli assume the role of soloists in parts of the more relaxed Trio. As in the Quartet no2, Martelli employs variation form in the Finale of the Serenade. Unlike the Finale of the earlier work, which gathers momentum like a wound-up machine let loose, the Allegretto marking of the Serenade's Finale holds good throughout the entire movement and the work ends in beautifully judged fading lights, the texture having been paired down to solo lines supported by gossamer muted sustained chords, the piece topped off by a pizzicato pluck from the lower strings. This fine work is at least as attractive as examples in the genre by Elgar and Berkeley and its continuing absence from the concert halls is depriving the concert-going public of a appealing and satisfying work which presents the opposite end of the spectrum from the Boulez and Stockhausen soundworld of the 1950s.\nThe Second Symphony (op6) was completed when the composer was only twenty years old: a remarkable achievement given the technical skill, superb manipulation of large orchestral forces and melodic invention on display. Originally planned as a one-movement work, when Martelli showed the Symphony to Malcolm Arnold, the older composer advised him to add more movements to it: this Martelli did, supplying a terse, austere first movement, virtually monothematic along the lines of Beethoven's Fifth. The lighter-scored second movement eases the tension before the gripping nineteen-minute Finale with its sinister insistent timpani rhythm. Perhaps the work does provide evidence of Martelli's interest in Nielsen's symphonies and in particular Shostakovich's then recently-completed Tenth, but the voice is a distinctive and original one and it combines the gripping intensity and rigour of the Quartet no2 with the graceful lyricism of the Serenade. Again, this piece cries out for performance: I can think of no more suitable venue than the Royal Albert Hall where its almost tangible power, intellectual rigour and Romantic sensibility should guarantee the ovation it richly deserves - thus, despite the tragic loss of the First Symphony, I do feel the composer has provided an archetypal \"Prom\" work after all!\nUpon leaving the RCM, Martelli became a professional viola player: he played in the RPO under Beecham and also performed with the LSO, including amongst the concerts he appeared in the premiere of Vaughan Williams' Ninth Symphony. When the LSO was rehearsing the RVW London Symphony, the aged composer complimented Martelli on his playing of the solo viola part in the second movement. In fact, RVW took a keen interest in the compositions of the young Martelli and attended the premieres of the two Quartets. RVW was also reported to have been most impressed with the Martelli Second Symphony when he heard one of the broadcasts of the work on the radio.\nCarlo had been interested in film music from an early age and developed this side of his talent by his friendship with the composer Gerard Schurmann: throughout the 1960s his main output was in this field. He wrote film scores for \"Catacombs\"(1964) , \"Witchcraft\" (1964), \"Do You Know This Voice?\" (1964), \"The Curse of the Mummy's Tomb\" (1964), \"The Murder Game\" (1966), \"\"Who Killed the Cat?\" (1967), \"Them\" (1967), \"Slave Girls\" (1967) and \"It\" (1967). Both \"Slave Girls\" and the Mummy film were made by the Hammer film company and it seems somehow appropriate that Martelli should have written for this particular company since his surname means \"hammers\" in Italian and the Music Supervisor at Hammer at this time was a man called Phil Martell. Martelli's first Hammer score, \"The Curse of the Mummy's Tomb\" (a video of which is still available coupled with another Hammer film \"The Revenge of Frankenstein\" - Columbia Tristar home Video VHS CVRP 178) is a splendid example of pseudo-Egyptian mood painting; the powerful music to the opening and closing credits sequence stands up perfectly well on its own and is in fact available on a CD compilation from Silva records imaginatively entitled \"Horror!\" in a performance by the Westminster Philharmonic under Kenneth Alwyn - it is somewhat sad that at present this is the only piece of music by Carlo Martelli available on CD! The composer himself played the tambourine in the recording sessions of the soundtrack to the belly-dancing sequence in the film, a reminder that he taught himself percussion at the RCM (the percussion also plays an important part in the Finale of the Second Symphony). Martelli's second Hammer film, \"Slave Girls\" was less successful at the time but has since acquired something like cult status in the thirty years since its initial release. Most of the other films Carlo Martelli wrote for were produced by an American company anxious to cash in on the success of the British horror film at the time in the wake of Hammer and the subsequent rise of other companies such as Amicus and Tigon. His final film, \"It\", starred Roddy Macdowell and was a mildly-effective remake of the classic German silent film \"Der Golem\", despite the presence of a monster which resembled an ambulating tree trunk!\nIf Martelli wrote a series of impressive masterpieces in the 1950s and the 1960s were characterised by a string of effective and lucrative film scores, the 1970s saw a period of silence for Carlo Martelli. Discouraged by the \"Glock years\" at the BBC during which a number of his scores which had previously been taken up with enthusiasm were swiftly returned to the composer on account of their lack of serialism and \"progressive\" techniques, Martelli gave up composing altogether - a tragic situation but not a unique one, there being plenty of examples of other talented composers who subsequently gave up composition when they realised their accessible style did not fit the inflexible policies of the Musical Establishment. At this dark time, Martelli spoke of himself as someone who \"used to be a composer\", a phrase which puzzled his friends but sounds chilling to anyone who has heard the fine works of this natural, highly talented artist. He took to occasional ghost-writing and orchestration for other composers, something which brought him money and for which he wrote a substantial amount of original material, his aptitude for producing rich and exciting scores standing him in good stead. Fortunately, he found himself drawn back into composition under his own name when he was asked to play in a chamber group at one of the Pizza Express chains. He began to arrange pieces for the string quartet, found his innate gifts as an arranger (already obvious from the rich scoring of his own concert works and film scores) undimmed and began to write his own quite lengthy introductions to the arrangements, producing original material of great charm and distinction.\nBy the early 1980s, Martelli was writing original music for orchestra again, of a \"light\" nature, his inherent ability as a lyricist and melodist (perhaps owing something to his Italian blood or his trips to the Italian operas as a boy!) shinning through such works as \"Persiflage\" (1983) and \"Aubade\" (1984). He sent his new scores to the post-Glock BBC who accepted his scores immediately and thus such works as those mentioned above were performed and broadcast by the BBC Concert Orchestra. All these \"light\" pieces are quite delightful and constitute tuneful compositions of a very high order. The title for \"Persiflage\" (meaning \"little piece of fluff\") came from a TV programme on William Walton where the aged composer described his \"Facade\" as \"a little bit of Persiflage\")! The second subject of this heart-warmingly upbeat and cheerful work resembles nothing less than the theme tune of \"Tom and Jerry\"! Expertly scored and beguiling the ear, both this piece and \"Aubade\" deserve wider dissemination through live performances and recordings.\nHis arrangements for string quartet, which he started about 1980, have been immensely successful and rightly so, for they bespeak a musician with an innate understanding of the nature and possibilities of string writing and a unique ability to create entertaining and ear-catching sounds. Works which Martelli has arranged for string quartet include much Broadway and Hollywood material from the 1920s to the 1950s, British and American light orchestral (genre) pieces, many \"pop\" songs and works of all kinds by composers such as Tchaikovsky, Dvorak, Chopin, Chabrier, Strauss, Sullivan and Rossini; he has even arranged some Beethoven Symphonies for string quartet! All his skilful arrangements zip along on a current of compositional skill and technical virtuosity. There are over 250 titles in this series and many are available from the publishers Broadbent and Dunn Ltd.\nWithin the last ten years, Carlo Martelli has completed two operas. He finished \"A Monkey's Paw\" in March 1990 but began the work in the 1950s and worked on it intermittently throughout the intervening three decades - thus, it has a fair claim to being his most personal score. Based on a short story by W. W. Jacobs, the opera was written out of the composer's enthusiasm for the tale and not to a commission with the inevitable consequence that it has never been performed (in its original version, it was one of the scores the BBC rejected in the early 1960s, along with the Fiesta Overture). In 1992, Shropshire County Council commissioned a children's opera from him, which he called \"The Curse of Christopher Columbus\", an appropriate subject matter in the 500th anniversary year of the discovery of America.\nMost recently he has written a Prelude and Fugue for 18 Violas (1993) for the National Youth Orchestra, another demonstration of his talents as an arranger and tone painter within restricted resources (a gift which came in useful when producing film scores to very limited budgets!). The Prelude and Fugue shows his love for and skill in writing for his own instrument remains undiminished.\nFortunately, there is some good news to end this summary of a career of varied fortunes but unswerving quality. There is a projected CD release of the Second Symphony coupled with the Serenade for Strings - the conductor is Jose Serebrier and the recording company is Dinamek. The Serenade has already been recorded (with the Philharmonia) to the composer's complete satisfaction and the Symphony sessions are set for the beginning of August, the Royal Scottish National Orchestra doing the honours. As for the other works, some tapes exist in the possession of the composer and any interest in performances of his works will be duly welcomed by him - his publishers are Lengnick and they possess scores from most periods of his compositional life. Should you wish to meet him, he plays with his own highly professional chamber group at the Soho restaurant Kettner's most Sunday evenings so if you get the chance, do go and listen to a great Symphonist, Opera and Quartet writer serenading you as you tuck in to your meal!!\n© Paul Conway 7/98\nReturn to: Classical Music on the Web\nSince April 1998 you are visitor number\nThese pages are maintained by Dr Len","Notes and Editorial Reviews\nThese are splendid Tchaikovsky performances.\n“I think Tchaikovsky was always ready for immortality ... and with his final three symphonies he secured his place in the pantheon of Great Composers.” Valery Gergiev.\nThe EMI Classics label has raided their extensive back catalogue for this three disc compilation.\nSymphony No.4 in F minor, Op. 36 The Fourth Symphony was written at a particularly crucial point in Tchaikovsky’s life. 1877 was not only the year of his disastrous marriage but also the year in which he began his fifteen-year correspondence with his patroness Nadezhda von Meck. The F minor Symphony has always been a popular work with its muscular and melodic writing. Infused throughout\nthe score is the sense of ‘fate’ which Tchaikovsky believed controlled his destiny as he described in a letter to Madame von Meck, “the fateful force which prevents the impulse to happiness from achieving its goal … which hangs above your head like the sword of Damocles.”\nIn the opening movement Andante sostenuto - Moderato con anima the performance from the Philadelphia Orchestra under Muti is as exciting as one is likely to hear. Muti blends passion and power to perfection and the conclusion was awe-inspiring. The moving and robust Andantino in modo di canzona was so convincing that I was left with a compelling sense of Tchaikovsky’s despair and fatigue. In the pizzicato section of the Scherzo: Pizzicato ostinato (Allegro) the Philadelphia strings are on their finest form and the woodwind deserve praise for their assured contribution at 1:44-3:04. There’s tremendous energy and drama in the Finale. Allegro con fuoco. It is hard to imagine better playing and I found myself on the edge of my seat. The sound quality throughout is to demonstration standard.\nThis performance of the Fourth Symphony is in the premier league of the alternative recordings and I believe it is probably the best of all the versions. Other favourite accounts from my collection are those from Jansons with the Oslo Philharmonic Orchestra on Chandos CHAN 8361 (c/w Romeo and Juliet Overture); Mengelberg with the Concertgebouw on Music & Arts mono CD809 (c/w Symphonies 5 and 6); Rozhdestvensky and the LSO on Regis RRC 1212 (c/w Marche Slave and 1812 Overture); Mravinsky with the Leningrad PO on DG 419 745-2GH2 (c/w Symphonies 5 and 6); Karajan with the VPO on Decca Penguin 460 655-2 (c/w Romeo and Juliet Overture) and Gergiev and the VPO from Vienna in 2002 on Philips 475 6315 0 PX3 (c/w Symphonies 5 and 6).\nFantasy Overture, Romeo and Juliet Composed in 1869, revised in 1870 and again in 1880 the Fantasy Overture, Romeo and Juliet is Tchaikovsky’s musical interpretation of Shakespeare’s greatest tragedy and one of the most enduring works in popularity. The tone poem contains musical themes that principally represent: Friar Laurence, the Montague and the Capulets feud, and the love music of Romeo and Juliet.\nMysterious, powerful, sensuous and passionate, Muti and his Philadelphia players take the listeners through a broad spectrum of colour and emotions. The recording is decent enough but a touch close for my taste.\nI remain a great admirer of the account of the Romeo and Juliet Overture from Pletnev and the Russian NO on DG 471 742-2 (c/w Pathétique) and also the performance from Karajan with the VPO on Decca ‘Penguin series’ 460 655-2 (c/w Fourth Symphony).\nSymphony No.5 in E minor, Op. 64 Composed in 1888 the Fifth Symphony is generally considered to be the most attractive of Tchaikovsky’s major works. When he first began writing the symphony he was suffering from a deep depression. However, he moved to the countryside and his state of mind became much more relaxed, enjoying the peace and quiet, gaining a new-found pleasure from his garden. This E minor Symphony reflects all the violent and conflicting emotions that he was experiencing at the time of its composition.\nMuti and the Philadelphians provide a grey and sombre opening Andante - Allegro con anima that gives way to increased weight and power. One, however, wonders if Muti is keeping something in reserve. The principal elements of melancholy and beauty are blended to considerable effect in the Andante cantabile, con alcuna licenza, although the speeds feel rather too measured. Here Muti builds up great tension in a moving interpretation. In the Valse - Allegro moderato the infectious playing is light with a convincing lilt. With highly authoritative playing the Philadelphia convey a triumphant mood in the Finale: Andante maestoso - Allegro vivace movement. I found the recording clear, bright and fairly close.\nFrom my collection I highly rate the accounts of the Fifth Symphony from Jansons and the Oslo PO on Chandos CHAN 8351; Rozhdestvensky and the LSO on Regis RRC 1213 (c/w Capriccio Italien); Mravinsky with the Leningrad PO on DG 419 745-2GH2 (c/w Symphonies 4 and 6); Ormandy and the Philadelphia Orchestra on Sony SBK 46538 (c/w Serenade for Strings) and Gergiev and the VPO from Salzburg in 1998 on Philips 475 6315 0 PX3 (c/w Symphonies 4 and 6).\nFrancesca da Rimini: Symphonic Fantasy after Dante, Op. 32 Tchaikovsky composed Francesca da Rimini in less than three weeks during his visit to Bayreuth in 1876. The premiere performance was given in Moscow in 1877 and proved so popular that the work was repeated twice a couple of months later. This is programme music of Dante’s Francesca da Rimini with the first section depicting the gateway to the Inferno and the agonies of the condemned. The middle section represents the tragic love of Paolo and Francesca, and the third part returns to the Inferno followed by a concluding section.\nIn Francesca da Rimini Muti and the Philadelphians revel in the fierce and stormy passages but provide contrast in music of contemplation with an air of mystery. The clarinet playing from Anthony Gigliotti especially at 8:28-9:25 is impeccable.\nMy preferred version of Francesca da Rimini is from the New Philharmonia Orchestra under Igor Markevitch on Philips Classics Duo 446 148-2 (c/w Symphonies 1-3).\nSymphony No.6 in B minor, Op. 74 ‘Pathétique’ Tchaikovsky’s Sixth Symphony, universally known as the Pathétique, is among the most deeply moving and profound of all works. An enduring masterwork which Tchaikovsky considered to be his greatest composition. Once again the struggle against ‘fate’ is central to this symphony which was to be the last Tchaikovsky wrote. The première took place in October 1893 at St. Petersburg and just eight days later the composer was dead. Few farewells in music are more poignant.\nMuti and the Philadelphia in the opening movement Adagio - Allegro non troppo impart a sinister air of shadowy foreboding with vigour and passion. The main theme is performed with just the right level of strength and poignancy. The Allegro con grazia is smooth and good humoured and I was impressed with the assured control and potency that Muti conveys in the Allegro molto vivace. Tchaikovsky’s mood of intense desperation and torment is impressively communicated in the Finale: Adagio lamentoso - Andante as they bring the score to a harrowing conclusion. A special mention goes to the woodwind section for their splendid playing throughout. Decent sound quality, reasonably clear and well balanced.\nI have several favourite versions of the Pathétique Symphony in my collection that I find deeply satisfying. The account from Pletnev and the Russian NO on DG 471 742-2 (c/w Romeo and Juliet Overture); Jansons with the Oslo PO on Chandos CHAN 8446; Rozhdestvensky and the LSO on Regis RRC 1214 (c/w The Storm Overture); Mravinsky with the Leningrad PO on DG 419 745-2GH2 (c/w Symphonies 4 and 5) and Gergiev and the VPO from Vienna in 2004 on Philips 475 6315 0 PX3 (c/w Symphonies 5 and 6). I still admire and regularly play my first recording of the work, which is on a vinyl LP, conducted by Fritz Reiner and the Chicago Symphony Orchestra on Camden Classics CCV 5024.\n1812 Overture, Op. 49 The Festival Overture ‘The Year 1812’ was composed in 1880 as a commission for the Moscow Exhibition of 1882; principally for the consecration of the Temple of Christ the Redeemer constructed to commemorate the Russian victory over Napoleon. The highly popular 1812 Overture is noted for its use of Russian themes, cannon shots and church bells in the coda.\nMuti and the Philadelphians provide a vigorous and characterful performance and is my premier recommendation of the score. I found the recording most acceptable although two friends thought the forte passages a touch fierce. This interpretation is also on the outstanding Muti/Tchaikovsky set from Brilliant Classics 99792 (see below).\nThose looking for a complete surveys of Tchaikovsky’s six symphonies and the Manfred Symphony may wish to turn to Muti’s superb earlier set with the Philharmonia from London in 1975-81. The set is now available at super-budget price on Brilliant Classics 99792. Also included are Muti’s recordings of the Philadelphia Orchestra during 1981-91 of the Francesca da Rimini, 1812 Overture, Swan Lake Suite and Serenade for Strings and a version of the Romeo and Juliet Overture from the Philharmonia in 1977 in London.\nAnother splendid alternative is from Mariss Jansons and the Oslo Philharmonic Orchestra. The set was recorded in the Philharmonic Hall, Oslo in 1984-86 on Chandos CHAN 86728 and reissued in 2006 on Chandos CHAN 10392 (c/w Capriccio Italien).\nReturning to the present set: these are splendid performances from Riccardo Muti and the Philadelphia Orchestra on EMI Classics. However, the competition in these scores is extremely fierce and Muti’s earlier set on Brilliant Classics is generously filled and makes a tempting first choice.\n-- Michael Cookson, MusicWeb International\nWorks on This Recording\nRomeo and Juliet Overture by Peter Ilyich Tchaikovsky\nWritten: 1869/1880; Russia\nLength: 20 Minutes 10 Secs.\nFrancesca da Rimini, Op. 32 by Peter Ilyich Tchaikovsky\nAnthony Gigliotti (Clarinet)\nWritten: 1876; Russia\nLength: 23 Minutes 12 Secs.\n1812 Overture, Op. 49 by Peter Ilyich Tchaikovsky\nWritten: 1880; Russia\nLength: 15 Minutes 26 Secs.\nSymphony no 4 in F minor, Op. 36 by Peter Ilyich Tchaikovsky\nWritten: 1877-1878; Russia\nSymphony no 5 in E minor, Op. 64 by Peter Ilyich Tchaikovsky\nWritten: 1888; Russia\nSymphony No. 4 in F minor Op. 36: I. Andante sostenuto_Moderato con anima\nSymphony No. 4 in F Minor, Op.36: II. Andantino in modo di canzone\nSymphony No. 4 in F Minor, Op.36: III. Scherzo: Pizzicato ostinato, allegro\nSymphony No. 4 in F minor Op. 36: IV. Allegro con fuoco\nFantasy Overture - Romeo and Juliet\nSymphony No. 5 in E Minor, Op.64: I. Andante - Allegro con anima\nSymphony No. 5 in E Minor, Op.64: II. Andante cantabile, con alcuna licenza\nSymphony No. 5 in E Minor, Op.64: III. Valse: Allegro moderato\nSymphony No. 5 in E Minor, Op.64: IV. Andante maestoso - Allegro vivace\nFrancesca da Rimini Op. 32\nBe the first to review this title"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:54e5bbe0-d47d-4e92-be89-793d1e0d3a9e>","<urn:uuid:1634176c-6877-448b-a210-f5185234546c>"],"error":null}
{"question":"How do the musical beginnings of Muddy Waters and Elvis Presley compare in terms of their early religious and cultural influences?","answer":"Muddy Waters and Elvis Presley had different early musical influences shaped by their respective environments. Waters became entranced with blues through phonograph records of artists like Blind Lemon Jefferson, Lonnie Johnson, and Tampa Red in the rural South of the 1920s and 1930s. He started performing locally at parties and fish fries by age 13, initially playing harmonica before switching to guitar at 17. In contrast, Elvis Presley's first musical influence came from church, where he gained inspiration from gospel music that would influence him throughout his life. After receiving a guitar at age 10, he began playing music regularly on his own, and later developed his style by studying music by ear, frequenting record stores with jukeboxes and listening booths in Memphis.","context":["» Edit Band Information\n» Edit Albums\n» Add a Review\n» Add an Album\n» Add News\nMuddy Waters was the single most important artist to emerge in post-war American blues. A peerless singer, agiftedsongwriter, an able guitarist, and leader of one of the strongest bands in the genre (which became a proving ground foranumber of musicians who would become legends in their own right), Waters absorbed the influences of rural blues fromtheDeep South and moved them uptown, injecting his music with a fierce, electric energy and helping pioneer the ChicagoBluesstyle that would come to dominate the music through the 1950s, ‘60s, and '70s. The depth of Waters' influence on rockaswell ...read more\nMuddy Waters was the single most important artist to emerge in post-war American blues. A peerless singer, agiftedsongwriter, an able guitarist, and leader of one of the strongest bands in the genre (which became a proving ground foranumber of musicians who would become legends in their own right), Waters absorbed the influences of rural blues fromtheDeep South and moved them uptown, injecting his music with a fierce, electric energy and helping pioneer the ChicagoBluesstyle that would come to dominate the music through the 1950s, ‘60s, and '70s. The depth of Waters' influence on rockaswell as blues is almost incalculable, and remarkably, he made some of his strongest and most vital recordings in the lastfiveyears of his life.Waters was born McKinley Morganfield, and historians argue about some details of his early life; while heoften told reportershe was born in Rolling Fork, Mississippi on April 4, 1915, researchers have uncovered census records andpersonal documentsthat would pin the year of his birth at 1913 or 1914, and others have cited the place of his birth as Jug'sCorner, a town inMississippi's Issaquena County. What is certain is that Morganfield's mother died when he just three yearsold, and from thenon he was raised on the Stovall Plantation in Clarksdale, Mississippi by his grandmother, Della Grant. Grantis said to havegiven young Morganfield the nickname \"Muddy\" because he liked to play in the mud as a boy, and the namestuck, with\"Water\" and \"Waters\" being tacked on a few years later. The rural South was a hotbed for the blues in the '20sand ‘30s, andyoung Muddy became entranced with the music when he discovered a neighbor had a phonograph and recordsby the likes ofBlind Lemon Jefferson, Lonnie Johnson, and Tampa Red.\nAs Muddy became more deeply immersed in the blues, he took up the harmonica; he was performing locally at parties andfishfries by the age of 13, sometimes with guitarist Scott Bohanner, who lived and worked in Stovall. In his early teens,Muddywas introduced to the sound of contemporary Delta blues artists, such as Son House, Robert Johnson, and CharleyPatton;their music inspired Waters to switch instruments, and he bought a guitar when he was 17, learning to play in thebottleneckstyle. Within a few years, he was performing on his own and with a local string band, the Son Simms Four; he alsoopened ajuke joint on the Stovall grounds, where fellow sharecroppers could listen to music, enjoy a drink or a snack, andgamble.Waters became a fixture in Mississippi, performing with the likes of Big Joe Williams and Robert Nighthawk, and in thelatesummer of 1941, musical archivists Alan Lomax and John Work III arrived in Mississippi with a portable recording rig, eagertodocument local blues talent for the Library of Congress (it's said they were hoping to locate Robert Johnson, only to learnhehad died three years earlier). Lomax and Work were strongly impressed with Waters, and recorded several sides ofhimperforming in his juke joint; two of the songs were released as a 78, and when Waters received two copies of the singleand$20 from Lomax, it encouraged him to seriously consider a professional career. In July 1943, Lomax returned to recordmorematerial with Waters; these early sessions with Lomax were collected on the album Down On Stovall's Plantation in 1966,anda 1994 reissue of the material, The Complete Plantation Recordings, won a Grammy award.In 1943, Waters decided to pullup stakes and relocate to Chicago, Illinois in hopes of making a living off his music. (He movedto St. Louis for a spell in 1940,but didn't care for it.) Waters drove a truck and worked at a paper plant by day, and at nightstruggled to make a name forhimself, playing house parties and any bar that would have him. Big Bill Broonzy reached out toWaters and helped him landbetter gigs; Muddy had recently switched to electric guitar to be better heard in noisy clubs,which added a new power to hiscutting slide work. By 1946, Waters had come to the attention of Okeh Records, who tookhim into the studio to record butchose not to release the results. A session that same year for 20th Century Recordsresulted in just one tune being issued asthe B-side of a James \"Sweet Lucy\" Carter release, but Waters fared better withAristocrat Records, a Chicago-based labelfounded by brothers Leonard and Phil Chess. The Chess Brothers began recordingWaters in 1947, and while a few early sideswith Sunnyland Slim failed to make an impression, his second single for Aristocratas a headliner, \"I Can't Be Satisfied\" b/w \"(IFeel Like) Goin' Home,\" became a significant hit and launched Waters as a star onthe Chicago blues scene.Initially, the ChessBrothers recorded Waters with trusted local musicians (including Earnest \"Big\" Crawford and Alex Atkins),but for his live work,Waters had recruited a band which included Little Walter on harmonica, Jimmy Rogers on guitar, andBaby Face Leroy Fosteron drums (later replaced by Elgin Evans), and in person, Waters and his group earned their reputationas the most powerfulblues band in town, with Waters' passionate vocals and guitar matched by the force of his combo. Bythe early '50s, theChess Brothers (who had changed the name of their label from Aristocrat to Chess Records in 1950) beganusing Waters'stage band in the studio, and Little Walter in particular became a favorite with blues fans and a superb foil forWaters. OtisSpann joined Waters' group on piano in 1953, and he would become the anchor for the band well into the '60s,after LittleWalter and Jimmy Rogers had left to pursue solo careers. In the '50s, Waters released some of the most powerfulandinfluential music in the history of electric blues, scoring hits with numbers like \"Rollin' and Tumblin,'\" \"I'm Ready,\" \"I'mYourHoochie Coochie Man,\" \"Mannish Boy,\" \"Trouble No More,\" \"Got My Mojo Working,\" and \"I Just Want to Make Love toYou\"which made him a frequent presence on the R&B charts.\nBy the end of the '50s, while Waters was still making fine music, his career was going into a slump. The rise of rock & rollhadtaken the spotlight away from more traditional blues acts in favor of younger and rowdier acts (ironically, Watershadheadlined some of Alan Freed's early \"Moondog\" package shows), and Waters' first tour of England in 1958 waspoorlyreceived by many U.K. blues fans, who were expecting an acoustic set and were startled by the ferocity of Waters'electricguitar. Waters began playing more acoustic music informed by his Mississippi Delta heritage in the years that followed,evenissuing an album titled Muddy Waters: Folk Singer in 1964. However, the jolly irony was that British blues fans wouldsoonrekindle interest in Waters and electric Chicago blues; as the rise of the British Invasion made the world aware of theU.K.rock scene, the nascent British blues scene soon followed, and a number of Waters' U.K. acolytes became internationalstars,such as Eric Clapton, John Mayall, Alexis Korner, and a modestly successful London act who named themselves afterMuddy's1950 hit \"Rollin' Stone.\" While Waters was still leading a fine band that delivered live (and included the likes of PinetopPerkinson piano and James Cotton on harmonica), Chess Records was moving more toward the rock, soul, and R&Bmarketplace, andseemed eager to market him to white rock fans, a notion that reached its nadir in 1968 with Electric Mud, inwhich Waterswas paired up with a psychedelic rock band (featuring guitarists Pete Cosey and Phil Upchurch) for rambling andaimless jamson Waters' blues classics. 1969's Fathers and Sons was a more inspired variation on this theme, with Watersplaying alongsidereverential white blues rockers such as Mike Bloomfield and Paul Butterfield; 1971's The London MuddyWaters Sessions wasless impressive, featuring fine guitar work from Rory Gallagher but uninspired contributions from SteveWinwood, Rick Grech,and Georgie Fame.\nCuriously, while Chess Records helped Waters make some of the finest blues records of the '50s and ‘60s, it was thelabel'sdemise that led to his creative rebirth. In 1969, the Chess Brothers sold the label to General Recorded Tape, and thelabelwent through a long, slow commercial decline, finally folding in 1975. (Waters would become one of several Chess artistswhosued the label for unpaid royalties in its later years.) Johnny Winter, a longtime Waters fan, heard the blues legendwaswithout a record deal, and was instrumental in getting Waters signed to Blue Sky Records, a CBS-distributed label thathadbecome his recording home. Winter produced the sessions for Waters' first Blue Sky release, and sat in with a bandcomprisedof members of Waters' road band (including Bob Margolin and Willie \"Big Eyes\" Smith) along with James Cotton onharp andPinetop Perkins on piano. 1977's Hard Again was a triumph, sounding as raw and forceful as Waters' classic Chesssides, witha couple extra decades of experience informing his performances, and it was rightly hailed as one of the finestalbums Watersever made while sparking new interest in his music. (It also earned him a Grammy award for Best Traditional orEthnic FolkRecording.) Waters also dazzled music fans when he appeared at the Band's celebrated farewell concert onThanksgiving1976 at the invitation of Levon Helm, who had helped produce one of his last Chess releases, The Muddy WatersWoodstockAlbum. Muddy delivered a stunning performance of \"Mannish Boy\" that became one of the highlights of MartinScorsese's 1978concert film The Last Waltz. Between Hard Again and The Last Waltz, Waters enjoyed a major career boost,and he foundhimself touring again for large and enthusiastic crowds, sharing stages with the likes of Eric Clapton and theRolling Stones,and cutting two more well-received albums with Winter as producer, 1978's I'm Ready and 1981's King Bee, aswell as a solid1979 concert set, Muddy \"Mississippi\" Waters Live. Waters' health began to fail him in 1982, and his final liveappearancecame in the fall of that year, when he sang a few songs at an Eric Clapton show in Florida. Waters died quietly ofheartfailure at his home in Westmont, Illinois on April 30, 1983. Since then, both Chicago and Westmont have named streetsinMuddy's honor, he's appeared on a postage stamp, a marker commemorates the site of his childhood home in Clarksdale,andhe appeared as a character in the 2008 film Cadillac Records, played by Jeffrey Wright. « hide\nSimilar Bands: B.B. King, Otis Rush, Johnny Winter, Jimmy Reed, Howlin Wolf\nContributors: Kaiiser, DikkoZinner, rockandmetaljunkie, manosg, Ehar, ZedO, morrissey, John Paul Harrison, Bron-Yr-Aur, Muddy Hendrix, riffariffic7, Mad., TwigTW, rockandmetaljunkie, tylerdurdenpt,","On January 8, 1935, the “King of Rock ‘n’ Roll,” Elvis Presley, was born in Tupelo, Mississippi. Elvis is known as one of the most significant American pop culture icons of the 20th century. Popularizing the musical styling known as rockabilly, which is an uptempo, backbeat-driven fusion of country music and rhythm and blues, he ranks as one of the most successful and well-known musicians of all time with hits like “Heartbreak Hotel,” “Don’t Be Cruel,” “Love Me Tender,” and many more.\nElvis was born to parents Gladys and Vernon in a shotgun house built by his father in Mississippi. Elvis was actually supposed to have an identical twin brother, but his brother, Jesse, was born a stillborn 35 minutes before Elvis. As an only child, Elvis had a very close relationship with his parents, and his mother raised him to have a very strong faith in God. It was in church that he gained his first musical influence, and he drew inspiration from the gospel music he heard there for the rest of his life.\nAfter receiving a guitar as a gift at the age of 10, Elvis began playing music regularly on his own. In 1948, Elvis moved with his family to Memphis, Tennessee. He got his first taste of live performance at his high school where he played in a talent show and won. His classmates then began to take notice of him because of his previously unknown musical abilities and his new appearance. He grew out his sideburns, styled his hair with rose oil and Vaseline, and began wearing flashy clothing he found on Beale Street, the blues hub of Memphis which Elvis frequented. He never received any formal musical training, instead studying and playing music by ear. He frequented record stores with jukeboxes and listening booths to supplement his love for music and gain inspiration.\nIn 1953, Elvis began visiting Sun Studios and recording songs. Before long, record label owner, Sam Phillips, recognized Elvis’s talents and decided to help the young musician develop his sound. Just a few days after recording his first single, “That’s All Right,” a popular local radio DJ played Elvis’s new single, and listeners bombarded the phone lines, curious who this new singer was. Elvis’s popularity soon began to swell, with fans attracted to his dashing good looks, unique musical style, and signature gyrating hips.\nThe following years saw huge turning points in the singer’s career. His then manager and Phillips, Sun Studios owner, struck up a $40,000 deal for RCA to acquire Elvis’s Sun contract. In 1956, he recorded his first No. 1 single, “Heartbreak Hotel,” and his first album, Elvis Presley, which skyrocketed to No. 1 as well. He became a favored guest on many televised variety shows, even though his provocative dance moves caused controversy. That same year, he signed a movie contract with Paramount Pictures and starred in his first box office hit, Love Me Tender.\nAfter receiving a draft notice from the United States Army in 1957, Elvis did not ignore his duties. He began training in Fort Hood, Texas, and was granted leave at one point to return to Memphis for the funeral of his mother. Though he was devastated by her death, he returned to service in Germany where his sadness was quelled when he met Priscilla Beaulieu. They eventually married after a seven and a half year courtship. Elvis was afraid that serving in the army would ruin his career, but his producers were prepared for his absence with several unreleased recordings. The musician had ten Top 40 hits during his time in the army.\nWhen he returned from his army hiatus, he filmed the movie GI Blues and also recorded the soundtrack for it, once again sending him to the top of the charts. During the following years, he starred in several more films and recorded music along with them. The movies were hit and miss with audiences, but the soundtracks still became widely popular.\nHis popularity began to fade in the late 1960s, but he proved himself to still be the “King of Rock ‘n’ Roll” with his “’68 Comeback” TV special where he reminded audiences of his incredible singing abilities and guitar prowess. During the late ’60s, he also married his longtime girlfriend, Priscilla, and soon they had their first and only child, Lisa Marie. Their marriage did not last long, however, and they were divorced by 1973, with Priscilla gaining custody of Lisa Marie.\nElvis’s health slowly began to decline as he suffered from weight problems and prescription drug abuse. Although his personal life was not doing well, Elvis still drew large crowds of fans at his Las Vegas performances and on his last tour. His last performance was in Indianapolis, Indiana in June 1977.\nOn August 16, 1977, Elvis was found dead in his home, Graceland, at the age of 42. He suffered from heart failure. He was eventually buried with his family on the Graceland property.\nThe legacy left by Elvis is huge. He’s credited with bringing rock ‘n’ roll music to the forefront of popular music in the United States. After his death, President Jimmy Carter issued a statement saying Elvis, “permanently changed the face of American popular culture.” During his life, he won 3 Grammy Awards and was nominated several more times. He had 18 No. 1 hits, and several of his albums were certified gold or platinum. He was one of the first artists to be inducted into the Rock and Roll Hall of Fame and was later also inducted into the Country Music Hall of Fame and the Gospel Music Hall of Fame. Since his death, many documentaries have been made about him, and there are countless Elvis impersonators around the world who constantly try to channel “The King.”\nIf you’re around the Memphis area this time of year, you can celebrate Elvis’s birthday by attending the multi-day birthday celebration held at his home, Graceland."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:d53eac7e-8874-437d-981d-9207a19bd1dc>","<urn:uuid:7bdc87b8-9635-4b3a-afb4-944a105923b7>"],"error":null}
{"question":"I'm researching forced relocations in Eastern Europe - what similarities exist between Bulgaria and Karaganda oblast regarding deportations during the Soviet era?","answer":"Both regions experienced significant forced relocations during the Soviet period. In Bulgaria, the communist Zhivkov regime's 1984 assimilation campaign forced Muslims to adopt Bulgarian names and renounce Muslim customs, leading to 310,000 Turks fleeing Bulgaria in 1989. Similarly, Karaganda oblast was an area where peoples repressed by Stalin were deported in the 1930s-40s, including Russians, Ukrainians, Germans, Tatars and Belorussians who were deported from their European homelands. The region also housed KARLAG and the Spassk concentration camp for German, Japanese, Italian and Romanian captives after World War II.","context":["Islam in Bulgaria\nIslam is the largest minority religion in Bulgaria. According to the 2011 Census, the total number of Muslims in the country stood at 577,139, corresponding to 7.8% of the population. The Muslim population of Bulgaria, which is made up of Turks, Bulgarians and Gypsies, lives mainly in parts of northeastern Bulgaria (mainly in Razgrad, Targovishte, Shumen and Silistra Provinces) and in the Rhodope Mountains (mainly in Kardzhali Province).\nMuslims in Bulgaria\nAccording to the census results for ethnicity, Muslims in Bulgaria were divided into the following groups in 2011:\n- Turks - 444,434, of which 420,816- Sunni and 21,610- Shi'a\n- Bulgarians - 67,350\n- Roma (Gypsies) - 42,201\n- Оverall - 577,139\nMost of the Muslims in Bulgaria, a total of 546,004 people, are Sunni Muslims, the form of Islam which was espoused during the reign of the Ottoman Empire. Shi'a sects such as the Alians, Kizilbashi and the Bektashi also are present, however. About 27,407 Shi'a Muslims live mainly in the Razgrad, Sliven and Silistra provinces. I It should be noted that 31.1% of the overall Bulgarian population did not state a religion during the 2011 census and respectively 10% did not wish to answer the question on ethnic group. In the voluntary census section of self-determination 14,698 persons of the Turkish ethnic group have pointed the answer “no religion”, and 39,529 have not stated their religion.\nHistory of Islam in Bulgaria\n|At the 2011 census answering the question for religion was not obligatory\nThe first documented Muslim contacts with Bulgaria are dated to the mid-ninth century when there were Islamic missionaries in Bulgaria, evidenced by a letter from Pope Nicholas to Boris of Bulgaria that the Saracens must be extirpated. During the time of Tsar Simeon insignificant Islamic influences on Bulgarian art began to appear, though it is believed that these can be traced to Byzantine influence. Later during the 11th and 12th centuries, nomadic Turkic tribes such as the Cumans and the Pechenegs entered Bulgaria and engaged the Byzantine Empire. According to scholars, some of these were Muslim.\nMigration of Muslim Seljuq Turks to Dobruja during the 13th century is also mentioned. In 1362, Ottoman Empirate captured the city of Adrianople (present-day Edirne) and within the next two years they had advanced as far as Plovdiv. The city of Sofia fell in 1385. Islam established in the conquered Bulgarian lands in the late 14th century at the Ottoman rule of the Balkans, its spreading grew until the Liberation of Bulgaria in the late 19th century after the Russo-Turkish War. According to the office of the Grand Mufti in Sofia during the Turkish Ottoman rule in Bulgaria there were 2,356 mosques, 174 tekke, 142 madrasah and 400 waqf. After the Russo-Turkish War, many Islamic properties were either destroyed or confiscated for civilian use. Currently there are 1458 mosques in Bulgaria.\nLike the practitioners of other beliefs including Orthodox Christians, Muslims suffered under the restriction of religious freedom by the Marxist-Leninist Zhivkov regime which instituted state atheism and suppressed religious communities. The Bulgarian communist regimes declared traditional Muslim beliefs to be diametrically opposed to secular communist ideology. In 1989, 310,000 Turks fled Bulgaria as a result of the communist Zhivkov regime's assimilation campaign. That program, which began in 1984, forced all Turks and other Muslims in Bulgaria to adopt Bulgarian names and renounce all Muslim customs. The motivation of the 1984 assimilation campaign was unclear; however, many experts believed that the disproportion between the birth rates of the Turks and the Bulgarians was a major factor. After the breakdown of communism, Muslims in Bulgaria again enjoyed greater religious freedom. Some villages organized Qur'an study courses for young people (study of the Qur'an had been completely forbidden under Zhivkov). Muslims also began publishing their own newspaper, Musulmani, in both Bulgarian and Turkish.\nIn 2011, the first study in 25 years found that 48.6 percent of its 850 Muslim respondents described themselves as devoutly religious, 28.5 percent of which very. Some 41 percent never went to the mosque and 59.3 percent did not pray at home. 0.5 percent believed that disputes should be resolved using Islamic Sharia law and 79.6 percent said that wearing a veil in school was \"unacceptable.\" 88 percent of respondents said they circumcised their boys and 96 percent observed Muslim burial practices for their relatives. More than half of the respondents said cohabitation without marriage was \"acceptable\", 39.8 percent ate pork and 43.3 percent drank alcohol.\nOttoman era textile canopy from Bulgaria.\n- Ragaru, Nadege (June 2001). \"Islam in post-communist Bulgaria: An aborted \"clash of civilizations\"?\". Nationalities Papers 29 (2): 293–324. doi:10.1080/00905990120053755.\n- Kristen R. Ghodsee, Muslim Lives in Eastern Europe: Gender, Ethnicity and the Transformation of Islam in Postsocialist Bulgaria, Princeton University Press, 2009. isbn = 978-0-691-13955-5\n- 2011 Bulgarian census (in Bulgarian)\n- Bulgaria. The World Factbook. CIA\n- \"Ahmadis barred \"because it is against the religions that people follow here\"\". thePersecution. Retrieved 28 October 2010.\n- H. T. Norris: \"Islam in the Balkans: religion and society between Europe and the Arab world\" 1993 pp.21-27\n- H. T. Norris: \"Islam in the Balkans: religion and society between Europe and the Arab world\" 1993 pp.21-22\n- Ali Eminov: \"Turkish and other Muslim minorities in Bulgaria\" 1997 pp.25\n- H. T. Norris: \"Islam in the Balkans: religion and society between Europe and the Arab world\" 1993 pp.26\n- H. T. Norris: \"Islam in the Balkans: religion and society between Europe and the Arab world\" 1993\n- R. J. Crampton: \"A short history of modern Bulgaria\" 1987\n- Office of the Grand Mufti - Sofia:Müslümanlar Publication 5/2009\n- Office of the Grand Mufti - Sofia:Müslümanlar Publication 11/2009\n- See in Bulgarian: Stoyanov, V., Turskoto naselenie v Balgariya mezhdu polyusite na etnicheskata politika [The Turkish population between the poles of ethnic politics], (Sofia: LIK, 1998); Gruev, M., Mezhdu petolachkata i polumesetsa: Balgarite myusyulmani i politicheskiya rezhim (1944-1959) [Between the Five-pointed Star and the Crescent: The Bulgarians-Muslims and the Political Regime (1944-1959],(Sofia: IK “KOTA”, 2003); Kalkandjieva, D., The Bulgarian Communist Party’s Policies towards the Non-Orthodox Religious Communities (1944-1953),” Trudove na katedrite po istoria i bogoslovies [Historical and Theological Studies Department of Shumen University], v. 8 (2005): 252-264; Gruev M. and A. Kalyonski, Vazroditelniyat protses: Myusyulmanskite obshtnosti i komunisticheskiya rezhim [The “Revival Process”. Muslim Communities and the Communist Regime: Policies, Reactions and Consequences] (Sofia: CIELA, 2008).\n- Glenn E. Curtis, ed. Bulgaria: A Country Study. Washington: GPO for the Library of Congress, 1992\n- Agence France-Presse (2011-12-09). \"Bulgaria's Muslims not deeply religious: study\". Hürriyet Daily News. Retrieved 10 December 2011.\n- This article incorporates public domain material from websites or documents of the Library of Congress Country Studies.","Karaganda oblast overview\nKaraganda or Karagandy (Kazakh people spelling) oblast (region) of Kazakhstan lies mostly in Kazakhstan Uplands in dry steppe zone, rising gradually in elevation eastward to a maximum in the Karkaraly Mountains of 5,115 feet (1,559 m). From Kazakh people language “karaganda” is a place rich in acacia. The capital city of the region is Karaganda city.\nAt present Karaganda oblast is the largest oblast in its territory and industrial potential, rich in minerals and raw. This is a powerful industrial center. The region is the leading one in Kazakhstan in its economic, cultural, scientific potential and developed infrastructure.\nKaraganda oblast has the population of about 1,344,000 (2009) on the territory of 428,000 sq. km.\nKaraganda oblast facts\nKaraganda oblast was formed on March 10th, 1932. First Petropavlovsk city was the center of the oblast. Since August 3rd, 1936, the center is located in Karaganda. Within its current boundaries the oblast was formed in May 1997, Zhezkazganskaya oblast was joined to it.\nThe major cities of Karaganda oblast, after Karaganda, the capital, are Shakhtinsk, Saran, Satpaev, Balkhash, Temirtau, Abay and Karkaralinsk.\nThe population of Karaganda region is about 85 percent urban, with the Kazakhs predominating. Russians, Ukrainians, Germans, Tatars and Belorussians also living there. A lot of people of the last four groups were deported from their European homelands by Soviet authorities.\nKaraganda oblast nature views\nKaraganda oblast nature view\nKaraganda oblast, Kazakhstan nature\nKaraganda oblast features\nThe climate of Karaganda oblast is of sharp continental type: hot, dry summers, severe winters with little snow but with winds and snowstorms. There are over 200 large and small rivers in the oblast. The main waterway in the oblast - the river Nura crosses the oblast from east to west and flows into one of the largest lakes in Central Kazakhstan - Tengiz. In the south the largest lake in Kazakhstan - Balkhash - is located. It is unique as water in its eastern part is salty, in western part - it’s fresh.\nThe oblast is linked with cities in CIS, Germany, China through railway. The international airport “Sary-Arka” carries out direct flights to Russia, Germany, Arab Emirates and other countries. Karaganda oblast is crossed by the highway linking Almaty with Astana - the capital of Kazakhstan.\nKaragandinskaya oblast is famous first of all by the fact that its territory was crossed by the northern branch of ancient Silk Way. Historically the region was a trade way, an area of economic and cultural exchange between Kazakhstan and other countries.\nIn 30-40s of 20th century Karaganda oblast was an area where peoples repressed by Stalin were deported. It is the place where KARLAG was located. After World War II in Spassk there was a concentration camp for German, Japanese, Italian and Romanian captives. At present there is the Memorial to Victims of Stalin’s totalitarianism there.\nKaraganda oblast pictures\nKaraganda oblast, Kazakhstan picture\nKaraganda oblast, Kazakhstan view\nKaraganda oblast economics\nKaraganda oblast takes an important place in mineral and raw complex of Kazakhstan. The principal economic resource of the region is Karaganda coal basin, also manganese, tungsten, molybdenum, lead and zinc are mined. Other mineral resources of Karaganda region include barite, nickel, iron ore and copper. Karaganda coalfield is the main coking coal supplier to the plants of metallurgical industry of Kazakhstan Republic.\nIn 2006 Karaganda oblast produced 100% of Kazakhstan manganese ore; 99,5% - of copper ore, 99% - of coke, 95% - of copper, 93% - of steel, 79% - of silver, 53% - of limestone, 29% - of coal, 16% - of iron ore.\nKaraganda oblast is one of the leading industrial regions in Kazakhstan. The industry of the region specializes in power- and material-consuming industries: ferrous and non-ferrous metallurgy, coal mining as well as a number of natural deposit ores. There are fairly well developed plants of food industry, machine-building and metal-working, chemical industry, rubber and plastic goods production and so on.\nKaraganda oblast produces about 24% of Kazakhstan processing industry output, which is due to prevailing share of Karaganda oblast in metallurgical industry production (52,8% of the republican output), among them in ferrous industry production - 49,7%, in non-ferrous metallurgical production - 56,6%. The oblast takes an important place in Kazakhstan Republic machine-building and metal-working.\nSpring wheat and fodder grasses are grown, sheep, horses and camels are bred in more arid regions of the western part of Karaganda oblast.\nDairying, truck gardening, and the cultivation of millet are carried on around Karaganda region but must be supported by irrigation. The water being brought by Irtysh-Karaganda Canal, which also supplies the industry of Karaganda city.\nAlthough large-scale industrialization began only about 1930, partly through the use of forced labor, Karaganda is now a major industrial center of Kazakhstan."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:cac362af-1840-4790-9e5a-99514d8b5ef4>","<urn:uuid:7ed32555-918c-4828-a392-27a6865b675d>"],"error":null}
{"question":"How do environmental stressors affect fish development in the Antarctic versus the Arctic regions?","answer":"In the Antarctic, dragonfish embryos are particularly vulnerable to environmental stressors, showing synergistic negative effects when both temperature and CO2 levels increase. Their development is accelerated in warmer, more acidic waters, which could be detrimental as it may lead to hatching during periods of limited food availability. Conversely, in the Arctic, warming temperatures actually present an opportunity for lake trout, as their phenotypic plasticity allows them to adapt to changing conditions. The relaxation of environmental constraints on growth in the far north is expected to enhance their development and allow them to colonize new water bodies successfully.","context":["Scientists at University of California Davis and San Francisco State University have discovered that the combination of elevated levels of carbon dioxide and an increase in ocean water temperature has a significant impact on survival and development of the Antarctic dragonfish (Gymnodraco acuticeps). The research article was published today in the journal Conservation Physiology.\nThe study, which was the first to investigate the response to warming and increased pCO2 (partial pressure of carbon dioxide) in a developing Antarctic fish, assessed the effects of near-future ocean warming and acidification on early embryos of the naked dragonfish, a shallow benthic spawner exclusive to the circumpolar Antarctic. As the formation of their embryos takes longer than many species (up to ten months), this makes them particularly vulnerable to a change in chemical and physical conditions.\nThe survival and metabolism of the dragonfish embryo was measured over time in two different temperatures and three pCO2 levels over a three-week period, which allowed the researchers to assess potential vulnerability of developing dragonfish to future ocean scenarios. The results showed that a near-future increase in ocean temperature as well as acidification have the potential to significantly alter the physiology and development of Antarctic fish. One of the article's authors, Assistant Professor Anne Todgham, explained that \"temperature will probably be the main driver of change, but increases in pCO2 will also alter embryonic physiology, with responses dependent on water temperature.\"\nProfessor Todgham went on to say: \"Dragonfish embryos exhibited a synergistic increase in mortality when elevated temperature was coupled with increased pCO2 over the course of the three week experiment. While we predictably found that temperature increased embryonic development, altered development due to increased pCO2 was unexpected.\" These unique findings show that single stressors alone may not be sufficient to predict the effects on early development of fish, as the negative effects of increased pCO2 may only manifest at increased temperatures. They also show that fish may differ from other marine invertebrate embryos in how they respond to pCO2.\nThe faster development of the embryos in warmer and more acidic waters could be bad news for the dragonfish. Hatching earlier, at the start of the dark winter months when limited food resources are available, has the potential to limit growth during critical periods of development. Furthermore, impacts to survival would reduce numbers of embryos that hatch and could impact dragonfish abundance.\nChloe Foster | EurekAlert!\nSingle-stranded DNA and RNA origami go live\n15.12.2017 | Wyss Institute for Biologically Inspired Engineering at Harvard\nNew antbird species discovered in Peru by LSU ornithologists\n15.12.2017 | Louisiana State University\nDNA molecules that follow specific instructions could offer more precise molecular control of synthetic chemical systems, a discovery that opens the door for engineers to create molecular machines with new and complex behaviors.\nResearchers have created chemical amplifiers and a chemical oscillator using a systematic method that has the potential to embed sophisticated circuit...\nMPQ scientists achieve long storage times for photonic quantum bits which break the lower bound for direct teleportation in a global quantum network.\nConcerning the development of quantum memories for the realization of global quantum networks, scientists of the Quantum Dynamics Division led by Professor...\nResearchers have developed a water cloaking concept based on electromagnetic forces that could eliminate an object's wake, greatly reducing its drag while...\nTiny pores at a cell's entryway act as miniature bouncers, letting in some electrically charged atoms--ions--but blocking others. Operating as exquisitely sensitive filters, these \"ion channels\" play a critical role in biological functions such as muscle contraction and the firing of brain cells.\nTo rapidly transport the right ions through the cell membrane, the tiny channels rely on a complex interplay between the ions and surrounding molecules,...\nThe miniaturization of the current technology of storage media is hindered by fundamental limits of quantum mechanics. A new approach consists in using so-called spin-crossover molecules as the smallest possible storage unit. Similar to normal hard drives, these special molecules can save information via their magnetic state. A research team from Kiel University has now managed to successfully place a new class of spin-crossover molecules onto a surface and to improve the molecule’s storage capacity. The storage density of conventional hard drives could therefore theoretically be increased by more than one hundred fold. The study has been published in the scientific journal Nano Letters.\nOver the past few years, the building blocks of storage media have gotten ever smaller. But further miniaturization of the current technology is hindered by...\n11.12.2017 | Event News\n08.12.2017 | Event News\n07.12.2017 | Event News\n15.12.2017 | Power and Electrical Engineering\n15.12.2017 | Materials Sciences\n15.12.2017 | Life Sciences","Climate warming at high latitudes has long been expected to exceed that predicted for tropical and temperate climes, but recent warming in the Arctic has exceeded even those expectations1. The geophysical consequences of this warming are reasonably well established2, but the impacts on freshwater fauna are poorly understood. Here we use a large-scale geospatial analysis of the population dynamics of one of the most abundant north temperate freshwater fish species to forecast increased demographic rates, productivity and colonization range in response to IPCC climate warming scenarios. Geospatial lake morphometry data were used to characterize 481,784 lakes in the Canadian Arctic capable of supporting lake trout (Salvelinus namaycush) populations. Lake trout productivity in existing habitat is projected to increase by 20% by 2050 due to climate change, but an expanded habitable zone may result in a 29% increase in harvestable biomass. Although many ecosystems are likely to be negatively impacted by climate warming, the phenotypic plasticity of fish will allow a rapid relaxation of the current environmental constraints on growth in the far north, as well as enhanced colonization of bodies of water in which there are few potential competitors.\nThis is a preview of subscription content\nSubscribe to Nature+\nGet immediate online access to the entire Nature family of 50+ journals\nSubscribe to Journal\nGet full journal access for 1 year\nonly $8.25 per issue\nAll prices are NET prices.\nVAT will be added later in the checkout.\nTax calculation will be finalised during checkout.\nGet time limited or full article access on ReadCube.\nAll prices are NET prices.\nThe geospatial data (Canadian Digital Surface Model and Canadian Digital Elevation Model) are available from http://maps.canada.ca/czs/index-en.html. Air temperature data from 881 weather stations across Canada are available from http://climate.weather.gc.ca/climate_normals/results_1981_2010_e.html.\nHuang, J. et al. Recently amplified arctic warming has contributed to a continual global warming trend. Nat. Clim. Change 7, 875–879 (2017).\nMeredith, M. et al. in IPCC Special Report on the Ocean and Cryosphere in a Changing Climate (eds H.-O. Pörtner et al) Ch. 3 (IPCC, Cambridge Univ. Press, 2019).\nThuiller, W. et al. Consequences of climate change on the tree of life in Europe. Nature 470, 531–534 (2011).\nBeaugrand, G. A. et al. Prediction of unprecedented biological shifts in the global ocean. Nat. Clim. Change 9, 237–243 (2019).\nForcada, J., Trathan, P. N. & Murphy, E. J. Life history buffering in Antarctic mammals and birds against changing patterns of climate and environmental variation. Glob. Change Biol. 14, 2473–2488 (2008).\nPacifici, M. et al. Assessing species vulnerability to climate change. Nat. Clim. Change 5, 215–225 (2015).\nFrainer, A. et al. Climate-driven changes in functional biogeography of Arctic marine fish communities. Proc. Natl Acad. Sci. USA 114, 12202–12207 (2017).\nWessely, J. et al. Habitat-based conservation strategies cannot compensate for climate-change-induced range loss. Nat. Clim. Change 7, 823–827 (2017).\nRyder, R. A. The Morphoedaphic Index—use, abuse and fundamental concepts. Trans. Am. Fish. Soc. 111, 154–164 (1982).\nMessager, M. L., Lehner, B., Grill, G., Nedeva, I. & Schmitt, O. Estimating the volume and age of water stored in global lakes using a geo-statistical approach. Nat. Comm. 7, 13603 (2016).\nCampana, S. E., Casselman, J. M. & Jones, C. M. Bomb radiocarbon chronologies in the Arctic, with implications for the age validation of lake trout (Salvelinus namaycush) and other Arctic species. Can. J. Fish. Aquat. Sci. 65, 733–743 (2008).\nShuter, B. J., Jones, M. L., Korver, R. M. & Lester, N. P. A general, life history based model for regional management of fish stocks: the inland lake trout (Salvelinus namaycush) fisheries of Ontario. Can. J. Fish. Aquat. Sci. 55, 2161–2177 (1998).\nCampana, S. E. Accuracy, precision and quality control in age determination, including a review of the use and abuse of age validation methods. J. Fish. Biol. 59, 197–242 (2001).\nCasselman, J. M., Jones, C. M. & Campana, S. E. Bomb radiocarbon age validation for the long-lived, unexploited Arctic fish species Coregonus clupeaformis. Mar. Freshwat. Res. 70, 1–8 (2019).\nLester, N. P., Shuter, B. J. & Abrams, P. A. Interpreting the von Bertalanffy model of somatic growth in fish: the cost of reproduction. Proc. R. Soc. Ser. B 271, 1625–1631 (2004).\nMinte-Vera, C. V., Maunder, M. N., Casselman, J. M. & Campana, S. E. Growth functions that incorporate the cost of reproduction. Fish. Res. 180, 31–44 (2016).\nCollins, M. et al. in Climate Change 2013: The Physical Science Basis (eds Stocker, T. F. et al.) 1029–1136 (IPCC, Cambridge Univ. Press, 2013)\nClimate Change 2014: Synthesis Report (eds Pachauri, R. K. and Meyer, L. A.) (IPCC, Cambridge Univ. Press, 2014).\nIslam, D. & Berkes, F. Indigenous peoples' fisheries and food security: a case from northern Canada. Food Secur. 8, 815–826 (2016).\nMusick, J. A. Ecology and conservation of long-lived marine animals. Am. Fish. Soc. Symp. 23, 1–10 (1999).\nSchloss, C. A., Nunez, T. A. & Lawler, J. J. Dispersal will limit ability of mammals to track climate change in the Western Hemisphere. Proc. Natl Acad. Sci. USA 109, 8606–8611 (2012).\nHirsch, P. E., N’Guyen, A., Muller, R., Adrian‐Kalchhauser, I. & Burkhardt‐Holm, P. Colonizing Islands of water on dry land—on the passive dispersal of fish eggs by birds. Fish. Fish. 19, 502–510 (2018).\nSpens, J., Englund, G. & Lundqvist, H. Network connectivity and dispersal barriers: using geographical information system (GIS) tools to predict landscape scale distribution of a key predator (Esox lucius) among lakes. J. Appl. Ecol. 44, 1127–1137 (2007).\nSwanson, H. K. et al. Anadromy in Arctic populations of lake trout (Salvelinus namaycush): otolith microchemistry, stable isotopes, and comparisons with Arctic char (Salvelinus alpinus). Can. J. Fish. Aquat. Sci. 67, 842–853 (2010).\nOckendon, N. et al. Mechanisms underpinning climatic impacts on natural populations: altered species interactions are more important than direct effects. Glob. Change Biol. 20, 2221–2229 (2014).\nWilson, K. L., De Gisi, J., Cahill, C. L., Barker, O. E. & Post, J. R. Life‐history variation along environmental and harvest clines of a northern freshwater fish: plasticity and adaptation. J. Anim. Ecol. 88, 717–733 (2019).\nGauthier, G. et al. Long-term monitoring at multiple trophic levels suggests heterogeneity in responses to climate change in the Canadian Arctic tundra. Philos. Trans. R. Soc. B 368, 20120482 (2013).\nThomas, C. D. Climate, climate change and range boundaries. Diversity Distrib. 16, 488–495 (2010).\nHealey, M. C. The dynamics of exploited lake trout populations and implications for management. J. Wildl. Manag. 42, 307–328 (1978).\nBurr, J. M. Growth, density and biomass of lake trout in Arctic and Subarctic Alaska. Am. Fish. Soc. Symp. 19, 109–118 (1997).\nMills, K. H., Dyck, M. & Harwood, L. A. Proceedings of the second lake trout symposium 2005, Yellowknife, Northwest territories. Can. Tech. Rep. Fish. Aquat. Sci. 2778, 247 (2008).\nHollister, J. W., Milstead, W. B. & Urrutia, M. A. Predicting maximum lake depth from surrounding topography. PLoS ONE 6, e25764 (2011).\nLivingstone, D. M., Lotter, A. F. & Walker, I. R. The decrease in summer surface water temperature with altitude in Swiss alpine lakes: a comparison with air temperature lapse rates. Arct. Antarct. Alp. Res. 31, 341–352 (1999).\nShuter, B. J., Schlesinger, D. A. & Zimmerman, A. P. Empirical predictors of annual surface water temperature cycles in North American lakes. Can. J. Fish. Aquat. Sci. 40, 1838–1845 (1983).\nDa Fang, X. & Stefan, H. G. Long-term lake water temperature and ice cover simulations/measurements. Cold Reg. Sci. Technol. 24, 289–304 (1996).\nCampana, S. E. Physical Characteristics of 55 Canadian Arctic Lake Trout Lakes (Knowledge Network for Biocomplexity archive, 2020); https://doi.org/10.5063/F1ZP44F1\nCampana, S. E. Lake Trout Population Characteristics in 55 Canadian Arctic Reference Lakes (Knowledge Network for Biocomplexity, 2020); https://doi.org/10.5063/F1TX3CPV.\nSamarasin, P., Minns, C. K., Shuter, B. J., Tonn, W. M. & Rennie, M. D. Fish diversity and biomass in northern Canadian lakes: northern lakes are more diverse and have greater biomass than expected based on species–energy theory. Can. J. Fish. Aquat. Sci. 72, 226–237 (2015).\nCampana, S. E., Valentin, A. E., MacLellan, S. E. & Groot, J. B. Image-enhanced burnt otoliths, bomb radiocarbon and the growth dynamics of redfish (Sebastes mentella and S. fasciatus) off the eastern coast of Canada. Mar. Freshw. Res. 67, 925–936 (2016).\nFrancis, R. I. C. C. Growth in age-structured stock assessment models. Fish. Res. 180, 113–118 (2015).\nSmith, M. W. et al. Recommendations for catch-curve analysis. North Am. J. Fish. Managem. 32, 956–967 (2012).\nRicker, W. E. Computation and Interpretation of Biological Statistics of Fish Populations (Bulletin of the Fisheries Research Board of Canada, 1975).\nDeriso, R. B. Optimal F 0.1 criteria and their relationship to maximum sustainable yield. Can. J. Fish. Aquat. Sci. 44, 339–348 (1987).\nThis work was supported by the Nunavut Wildlife Management Board, Fisheries and Oceans Canada, US National Science Foundation grant OCE-9985884 and the University of Iceland. We thank S. Armsworthy, P. Bentzen, J. Brazner, C. Campana, P. Campana, S. Campana, S. Casselman, M. Fowler, D. Houlihan, W. Joyce, P. Leblanc, A. MacDonnell and M. Showell for their exceptional assistance in the field and laboratory. B. Shuter and J. Morrongiello provided valuable comments on the MS.\nThe authors declare no competing interests.\nPeer review information Nature Climate Change thanks Arild Folkvord and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nAbout this article\nCite this article\nCampana, S.E., Casselman, J.M., Jones, C.M. et al. Arctic freshwater fish productivity and colonization increase with climate warming. Nat. Clim. Chang. 10, 428–433 (2020). https://doi.org/10.1038/s41558-020-0744-x\nPolar Biology (2022)\nNature Climate Change (2021)\nScientific Reports (2020)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:cebb7679-8815-4c2b-91e3-d7f2de1ef1c4>","<urn:uuid:50be4d14-7b3a-469d-8c9a-24571c88c15c>"],"error":null}
{"question":"How does aluminum foam improve safety barriers on highways? Can you explain specific applications?","answer":"Aluminum foam damping elements improve guard rails by being installed between the guardrail and fastening posts. The material's main benefit is its ability to absorb crash energy, which leads to reduced vehicle damage and lower repair costs for the guardrails. Additionally, aluminum foam products can be used for fire and explosion protection, quick protection for safety-relevant applications, screening in case of accidents, and noise protection at mobile construction sites.","context":["with this newsletter at the end of the year, the Havel metal foam would like to inform you again about the latest developments and thank you for the good cooperation. We hope you enjoy reading.\nBest regards Your Havel metal foam - Team\nAward Winner Brandenburg Innovation Award Metal 2018\nAs part of the Cluster Conference on November 14, 2018, the three companies Havel metal foam GmbH, Sinfosy GmbH and the tom company were logically awarded the \"Brandenburg Innovation Award\" in the Metal Cluster. The three companies share the prize money of 10,000 euros in equal parts. For the fifth time, the Brandenburg Ministry of Economic Affairs organized the competition. The agency Medienlabor was responsible for the competition and the award ceremony for the second time.\n\"The metal industry is a figurehead of the Brandenburg economy - and one of its strongest pillars. 17 percent of all employees in the manufacturing industry in Brandenburg work in the metal industry. That's more than 38,000 people in 2,600 companies, ranging from metal production and technological processing to the finished product. The competition for the Innovation Award has once again demonstrated impressively how creative and innovative this industry is. Congratulations to the three companies that have made it to the podium, \"said Hendrik Fischer, State Secretary in the Ministry of Economic Affairs and Energy, in the run-up to the conference. [Nbsp]\nWe are very proud of the price! Many thanks to the jury. :)\nTake a look at our little movie: http://download.agentur-medienlabor.de/Innovationspreis_Metall_2018_Havel-metal-foam.mp4\nPhoto: Agentur Medienlabor / Benjamin Maltry Video: Agency Media Lab\nNoble & strong\nStainless steel does not just look good. Compared to others, this material offers some advantages. In addition to the high corrosion resistance, stainless steel is characterized by a high fire resistance. The outstanding properties of aluminum foam in metallic bond with stainless steel as cover layer allow the application in all industries. For example, pipes and profiles made of stainless steel can be foamed with aluminum foam. The thoroughly positive properties of the material are used by the Havel metal foam to develop the right solutions to meet your requirements.\nAluminum foam products as protection on motorways & construction sites\nGuard rails are a good thing in themselves. In combination with the innovative aluminum foam damping elements of the Havel metal foam they are even more attractive. Between the guardrail and the fastening posts, the outstanding advantages of aluminum foam are really important. The inclusion of the crash energy leads to damage minimization on vehicles and saves repair costs on the guardrails.\nThe aluminum foam products of the Havel metal foam can be used as:\nFire and explosion protection\nThe following applications are possible:\nQuick protection for safety-relevant applications\nScreen in case of accidents\nNoise protection at mobile construction sites\nFire and explosion protection on construction sites\nIn the future, the rear underrun protection, which is legally required for trucks and their trailers, should become even more secure. In order to better protect car occupants in rear-end collisions with trucks, Europe is defining new safety requirements.\nThe responsible body of the United Nations Economic Commission for Europe (UNECE) has been working for several years on new safety standards for the rear underrun protection of trucks. As a result, this body finally adopted the 03 series of amendments to UNECE Regulation No 58 for underrun guards.\nAn underrun protection for trucks now not only has to have different geometrical properties than before, but also withstand almost twice as strong testing forces in a strength test. By September 2021 at the latest, all vehicles placed on the market shall comply with the requirements of the 03 series of amendments to UNECE Regulation No 58.\nIn some cases, considerable modifications to the design of the vehicle manufacturers are possible. While there are hardly any changes for vehicles of categories M, N1 and O1 and O2 and furthermore only geometrical requirements have to be met, the requirements for heavy commercial vehicles (N2, N3, O3, O4) have been significantly tightened.\nThis development is very accommodated by the design-related advantages of our products. The foaming of corresponding profiles with metallurgical connection leads to an enormous increase of rigidity and strength as well as to higher energy absorption in the event of an impact. In existing underrun protection profiles our 3D foam parts can be inserted. As a result, they also meet the new requirements.\nWe are happy to discuss with you the possibilities for your requirements.\nMerry Christmas and a happy new Year...\n\"Basically, it's always the connections with people who give life its value.\" (Wilhelm von Humboldt)\nThe year is almost over. We want to take this as an opportunity to thank you for the confidence you have shown in us and the pleasant cooperation. We wish you merry Christmas holidays and a healthy and successful New Year.\nYour team of Havel metal foam\nHavel metal foam GmbH | Am Gleisdreieck 10 | 14774 Brandenburg an der Havel | Germany P. +49 (3381) 80 43 88 20 | Fax +49 (3381) 80 43 88 40 firstname.lastname@example.org | en.havel-mf.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:5c6610bc-a677-415c-a3e1-bcef17507ed7>"],"error":null}
{"question":"What are the physical characteristics of Saturn's ring system, and how have the Cassini observations revealed the nature of Enceladus's plumes at different viewing angles?","answer":"Saturn's ring system consists of five major components (G, F, A, B, and C rings) made of tiny particles of water ice mixed with dust and rocky matter, surrounded by an atmosphere-like environment of molecular oxygen. While the F and G rings are thin and hard to see, the A, B, and C rings are broad and visible, separated by the Cassini division. As for Enceladus's plumes, Cassini's observations have shown that they are best viewed at high solar phase angles, particularly when backlit by the Sun. The plumes appear brightest at phase angles around 175 degrees, as seen in eclipse images, while the smallest phase angle for successful plume imaging is about 140 degrees. Attempts to view the plumes at smaller angles have been challenging due to internal reflections from Enceladus's highly reflective surface.","context":["Saturn’s most distinctive feature is the thousands of rings that orbit the planet. Despite the fact that the rings look like continuous hoops of matter encircling the giant planet, each ring is actually made of tiny individual particles. Saturn’s rings consist largely of water ice mixed with smaller amounts of dust and rocky matter. Data from the Cassini spacecraft indicate that the environment around the rings is like an atmosphere, composed principally of molecular oxygen.\nThe ring system is divided into 5 major components: the G, F, A, B, and C rings, listed from outside to inside (but in reality, these major divisions are subdivided into thousands of individual ringlets). The F and G rings are thin and difficult to see, while the A, B, and C rings are broad and easily visible. The large gap between the A ring and and the B ring is called the Cassini division. One of Saturn’s moons, namely; Enceladus is the source of Saturn’s E-ring. The moon’s geyser-like jets create a gigantic halo of ice, dust, and gas that helps feed Saturn’s E ring.\nEnceladus has a profound effect on Saturn and its environment. It’s the only moon in our solar system known to substantially influence the chemical composition of its parent planet. The whole magnetic environment of Saturn is weighed down by the material spewing from Enceladus, which becomes plasma — a gas of electrically charged particles. This plasma, which creates a donut-shaped cloud around Saturn, is then snatched by Saturn’s A-ring, which acts like a giant sponge where the plasma is absorbed.\nCredit: Bjorn Jonsson, NASA/JPL/SSI\nSeriously thinking of making some new characters based of these African Gods\nsomebody should make a sick movie with these guy\nI love this so deep and sexy at the same dam time\nLet’s face it - English is a crazy language. There is no egg in eggplant nor ham in hamburger; neither apple nor pine in pineapple. English muffins weren’t invented in England or French fries in France. Sweetmeats are candies while sweetbreads, which aren’t sweet, are meat. We take English for granted. But if we explore its paradoxes, we find that quicksand can work slowly, boxing rings are square and a guinea pig is neither from Guinea nor is it a pig.\nAnd why is it that writers write but fingers don’t fing, grocers don’t groce and hammers don’t ham? If the plural of tooth is teeth, why isn’t the plural of booth beeth? One goose, 2 geese. So one moose, 2 meese? One index, 2 indices? Doesn’t it seem crazy that you can make amends but not one amend? If you have a bunch of odds and ends and get rid of all but one of them, what do you call it?\nIf teachers taught, why didn’t preachers praught? If a vegetarian eats vegetables, what does a humanitarian eat? In what language do people recite at a play and play at a recital? Ship by truck and send cargo by ship? Have noses that run and feet that smell? How can a slim chance and a fat chance be the same, while a wise man and a wise guy are opposites?\nYou have to marvel at the unique lunacy of a language in which your house can burn up as it burns down, in which you fill in a form by filling it out and in which an alarm goes off by going on. English was invented by people, not computers, and it reflects the creativity of the human race (which, of course, isn’t a race at all). That is why, when the stars are out, they are visible, but when the lights are out, they are invisible.","Central to the Enigma of Enceladus are the fantastic plumes of water vapor and ice that emanate from the \"tiger stripe\" fractures, or sulci, at the moon's south pole.\nInterestingly, the plumes have yet to be seen by Cassini's Imaging Science Subsystem (ISS) cameras under anything but a fairly limited viewing geometry, specifically large angles between the spacecraft and Sun seen from Enceladus.\nObservations at multiple viewing geometries allow planetary scientists to determine physical characteristics of plume particles.\nThe existence of the plumes was confirmed by Cassini's ISS in images obtained at high solar phase angles, or as they were viewed nearly backlit by the Sun. (The solar phase angle is the angle formed between Enceladus, the Sun, and Cassini.) Usually, Cassini cannot aim its cameras within 15 degrees of the Sun, which limits the phase angle at which the plumes (or anything else) can be viewed to 165 degrees or less. If the spacecraft is safely in eclipse behind Saturn, however, then higher phase angles are attainable. Cassini was in fact in eclipse behind Saturn, when it captured the spectacular 'Ghostly Fingers of Enceladus' image below at a phase angle of about 175 degrees.\nNASA / JPL / SSI\nGhostly Fingers of Encleadus\nNotice how much brighter the plume is, below the black disk that is Enceladus, than the vast E ring, the eventual repository for some fraction of plume particles. Both appear bright because they are efficient forward scatterers. Every eastbound morning (and westbound evening) commuter with a dirty windshield knows all too well how tiny dust particles scatter sunlight in the forward direction... right into their squinting eyes.\nBut what do the plumes look like at smaller phase angles? The smallest phase angle at which any plume images have been successfully acquired is about 140 degrees, as shown in the figure below.\nPlume from Enceladus\nets emanate from Enceladus' south pole as the surface is illuminated by Saturnshine.\nCassini ISS has attempted to obtain plume images at smaller phase angles, but aiming its camera near a bright source (Enceladus is, afterall, the most reflective body in the Solar System!) produces numerous internal reflections rather than useful data. One such image appears below.\nnternal reflections hide Enceladus' plumes from view at low phase angles.\nSo, how to observe Enceladus' plumes at phase angles smaller than 140 degrees? Could the current equinox season at Saturn provide any assistance? With equinox season, comes mutual event season, and the satellites will be eclipsing each other as equinox approaches in August. If the bright surface of Enceladus were to be in eclipse, darkened by the shadow of another saturnian moon, Cassini might have shot at getting a look at the plumes free from those frustrating internal reflections at a phase angle lower than 140 degrees. Of course, viewed from Enceladus, such an event would be a total solar eclipse!\nGetting Some Help from Rhea\ntotal solar eclipse by Rhea, viewed from Enceladus.\nSuch an eclipse doesn't last very long, however, so exposures would have to be well timed to capture the plumes in sunlight while Enceladus is in momentary darkness. With luck, it just might work.\nAnne Verbiscer is a Research Associate Professor at the University of Virginia. She studies the surfaces of icy bodies in the outer Solar System and has been involved with the Cassini mission to Saturn since 2007. Currently a visitor at Southwest Research Institute, she has been enjoying the past year living in the mountains above Boulder, Colorado."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:2574aaf7-0f90-4cf2-9356-965de3f72ef6>","<urn:uuid:3f0cad99-e716-4826-9b1d-ffcde04cb32f>"],"error":null}
{"question":"How do they extract magnesium from seawater? I'm curious about the process of getting metals from the ocean.","answer":"Magnesium is extracted from seawater through an electrolytic process. It is the only metal currently extracted directly from seawater, where it exists at a concentration of about 1,000 parts per million. In the United States, approximately 60% of magnesium metal and many magnesium salts are produced through this electrolytic extraction from seawater. The remaining portion comes from mining ancient ocean deposits containing minerals like magnesite (MgCO3) and dolomite (CaMg[CO3]2).","context":["Mineral Resources from the Ocean\nOceans cover 70 percent of Earth's surface, host a vast variety of geological processes responsible for the formation and concentration of mineral resources, and are the ultimate repository of many materials eroded or dissolved from the land surface. Hence, oceans contain vast quantities of materials that presently serve as major resources for humans. Today, direct extraction of resources is limited to salt; magnesium; placer gold, tin, titanium, and diamonds; and fresh water.\nAncient ocean deposits of sediments and evaporites now located on land were originally deposited under marine conditions. These deposits are being exploited on a very large scale and in preference to modern marine resources because of the easier accessibility and lower cost of terrestrial\nPrincipal Mineral Resources\nResources presently extracted from the sea or areas that were formerly in the sea range from common construction materials to high-tech metals to water itself. Chemical analyses have demonstrated that sea water contains about 3.5 percent dissolved solids, with more than sixty chemical elements identified. The limitations on extraction of the dissolved elements as well as the extraction of solid mineral resources are nearly always economic, but may also be affected by geographic location (ownership and transport distance) and hampered by technological constraints (depth of ocean basins).\nThe principal mineral resources presently being extracted and likely to be extracted in the near future are briefly considered here.\nSalt, or sodium chloride, occurs in sea water at a concentration of about 3 percent and hence constitutes more than 80 percent of the dissolved chemical elements in sea water. The quantity available in all the oceans is so enormous that it could supply all human needs for hundreds, perhaps thousands, of years. Although salt is extracted directly from the oceans in many countries by evaporating the water and leaving the residual salts, most of the nearly 200 million metric tons of salt produced annually is mined from large beds of salt. These beds, now deeply buried, were left when waters from ancient oceans evaporated in shallow seas or marginal basins, leaving residual thick beds of salt; the beds were subsequently covered and protected from solution and destruction.\nLike the sodium and chlorine of salt, potassium occurs in vast quantities in sea water, but its average concentration of about 1,300 parts per million (or 0.13 percent) is generally too low to permit direct economic extraction. Potassium salts, however, occur in many thick evaporite sequences along with common salt and is mined from these beds at rates of tens of millions of metric tons per year. The potassium salts were deposited when sea water had been evaporated down to about one-twentieth of its original volume.\nMagnesium, dissolved in sea water at a concentration of about 1,000 parts per million, is the only metal directly extracted from sea water. Presently, approximately 60 percent of the magnesium metal and many of the magnesium salts produced in the United States are extracted from sea water electrolytically. The remaining portion of the magnesium metal and salts is extracted from ancient ocean deposits where the salts precipitated during evaporation or formed during diagenesis . The principal minerals mined for this purpose are magnesite (MgCO 3 ) and dolomite (CaMg[CO 3 ] 2 ).\nSand and Gravel.\nThe ocean basins constitute the ultimate depositional site of sediments eroded from the land, and beaches represent the largest residual deposits of sand. Although beaches and near-shore sediments are locally extracted for use in construction, they are generally considered too valuable as recreational areas to permit removal for construction purposes. Nevertheless, older beach sand deposits are abundant on the continents, especially the coastal plains, where they are extensively mined for construction materials, glass manufacture, and preparation of silicon metal. Gravel deposits generally are more heterogeneous but occur in the same manner, and are processed extensively for building materials.\nLimestone and Gypsum.\nLimestones (rocks composed of calcium carbonate) are forming extensively in the tropical to semitropical oceans of the world today as the result of precipitation by biological organisms ranging from mollusks to corals and plants. There is little exploitation of the modern limestones as they are forming in the oceans. However, the continents and tropical islands contain vast sequences of limestones that are extensively mined; these limestones commonly are interspersed with dolomites that formed through diagenetic alteration of limestone. Much of the limestone is used directly in cut or crushed form, but much is also calcined (cooked) to be converted into cement used for construction purposes. Gypsum (calcium sulfate hydrate) forms during evaporation of sea water and thus may occur with evaporite salts and/or with limestones. The gypsum deposits are mined and generally converted into plaster of paris and used for construction.\nThe deep ocean floor contains extremely large quantities of nodules ranging from centimeters to decimeters in diameter (that is, from less than an inch to several inches). Although commonly called manganese nodules, they generally contain more iron than manganese, but do constitute the largest known resource of manganese.\nDespite the abundance and the wealth of metals contained in manganese nodules (iron, manganese, copper, cobalt, and nickel), no economic way has yet been developed to harvest these resources from the deep ocean floor. Consequently, these rich deposits remain as potential resources for the future. Terrestrial deposits of manganese are still relied on to meet human needs.\nComplex organic and inorganic processes constantly precipitate phosphate-rich crusts and granules in shallow marine environments. These are the analogs (comparative equivalents) of the onshore deposits being mined in several parts of the world, and represent future potential reserves if land-based deposits become exhausted.\nMetal Deposits Associated with Volcanism and Seafloor Vents.\nSubmarine investigations of oceanic rift zones have revealed that rich deposits of zinc and copper, with associated lead, silver, and gold, are forming at the sites of hot hydrothermal emanations commonly called black smokers. These metal-rich deposits, ranging from chimneyto pancake-like, form where deeply circulating sea water has dissolved metals from the underlying rocks and issue out onto the cold seafloor along major fractures. The deposits forming today are not being mined because of their remote locations, but many analogous ancient deposits are being mined throughout the world.\nPlacer Gold, Tin, Titanium, and Diamonds.\nPlacer deposits are accumulations of resistant and insoluble minerals that have been eroded from their original locations of formation and deposited along river courses or at the ocean margins. The most important of these deposits contain gold, tin, titanium, and diamonds.\nToday, much of the world's tin and many of the gem diamonds are recovered by dredging near-shore ocean sediments for minerals that were carried into the sea by rivers. Gold has been recovered in the past from such deposits, most notably in Nome, Alaska. Large quantities of placer titanium minerals occur in beach and near-shore sediments, but mining today is confined generally to the beaches or onshore deposits because of the higher costs and environmental constraints of marine mining.\nThe world's oceans, with a total volume of more than 500 million cubic kilometers, hold more than 97 percent of all the water on Earth. However, the 3.5-percent salt content of this water makes it unusable for most human needs.\nThe extraction of fresh water from ocean water has been carried out for many years, but provides only a very small portion of the water used, and remains quite expensive relative to land-based water resources. Technological advances, especially in reverse osmosis , continue to increase the efficiency of fresh-water extraction. However, geographic limitations and dependency on world energy costs pose major barriers to large-scale extraction.\nSEE ALSO ; M INERAL R ESOURCES FROM F RESH W ATER ; .\nJames R. Craig\nCraig, James R., David J. Vaughtan, and Brian J. Skinner. Resources of the Earth: Origin, Use, Environmental Impact, 3rd ed. Upper Saddle River, NJ: Prentice Hall, 2001.\nLahman, H. S., and J. B. Lassiter III. The Evolution and Utilization of Marine Mineral Resources. Books for Business, 2002.\nUSGS Minerals Information: Mineral Commodity Summaries. U.S. Geological Survey. <http://minerals.usgs.gov/minerals/pubs/mcs/> .\nUSGS Minerals Information: Minerals Yearbook. U.S. Geological Survey. <http://minerals.usgs.gov/minerals/pubs/myb.html/> ."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:6c7beb72-7a7b-4bd9-ab7e-c5c996fdfaa0>"],"error":null}
{"question":"How do workplace flu prevention strategies compare between on-site vaccination programs and company cleanliness policies in terms of their approach to reducing employee sickness?","answer":"On-site vaccination programs and company cleanliness policies represent different approaches to flu prevention in the workplace. On-site vaccination programs offer a direct medical intervention, with companies like ARCpoint providing flu shots to employees at their workplace, helping reduce the $26.7 billion annual cost of influenza in medical expenses and lost earnings. In contrast, workplace cleanliness policies focus on preventing transmission through environmental measures, such as providing anti-bacterial hand gel and encouraging employees to keep workstations clean, since most infectious diseases spread through touch. Both approaches are part of a comprehensive strategy to reduce winter sickness absence in the workplace.","context":["As the weather turns colder and days grow shorter, many companies struggle to deal with increases in sickness absence in the workplace. Colds and flu start to do the rounds, while Seasonal Affective Disorder may cause mental health issues requiring time off.\nStudies have shown that employees take 53% more sick days in January, compared to other months of the year, making it the toughest month for businesses to help ensure their staff remain healthy and motivated.\nCompanies should also be aware of employees who continue to work despite suffering from a minor illness, known as presenteeism, but who are afraid to take time off due to work pressures and the potential impact on their own performance management reviews.\n6 top tips to help you combat an increase in winter sickness absence\n1. Encourage sickness absence when necessary\nHaving a policy of ‘stay at home when you’re ill’ and a culture which embodies this, while putting the employees’ health at the forefront, will help to ensure that staff maintain a healthier lifestyle and reduce the spread of illness in the workplace.\n2. Promote a culture of exercise\nRegular exercise is the most effective way to build up your defences against winter viruses, so promoting a company culture which actively encourages employees to work out is a surefire method of building a healthy workforce resilient to winter sicknesses.\n3. Encourage a healthy diet\nWith staff spending a significant part of their day in the workplace, employers have an opportunity to encourage healthy eating and positively influence their eating habits. Replacing sugary snacks with bowls of fruit is one way to help. Another idea is to create a centralised place for staff to share healthy recipes (for instance on your company’s intranet). This can further promote a culture of healthy eating and reduce the risk of major illnesses.\n4. Prevent rather than cure\nThe vast majority of infectious diseases are transmitted by touch. So it’s worth instilling a regime of cleanliness in your company’s culture. Making available anti-bacterial hand gel would be a good start as well as encouraging your employees to keep their workstations clean. Some companies also offer flu jabs for their employees, helping reduce the spread of the virus and therefore reducing the number of absences.\n5. Discuss regular sickness absence with your employees\nIf an employee is taking recurrent short term absences you should discuss the issue with them and provide support.\nEnsure confidentiality and if necessary keep a record of the conversation on your performance management system.\n6. Identify and address the symptoms of Seasonal Affective Disorder\nHarder to spot but equally detrimental to any business’s winter absence rates is Seasonal Affective Disorder, a depressive disorder which commonly affects people during the winter months.\nManagers who recognise the symptoms of SAD in one of their employees should consider reducing their workload or adjusting their schedule.\nFind out more about our top tips here. [insert link when blogs are live]\nThe Bradford Factor: A scientific approach to absence management\nSickness absence leads to mounting costs for employers. This is why savvy companies are utilising more sophisticated methods of monitoring employee absence.\nTracking and monitoring employee absence in a performance management and HR suite is crucial if companies want to keep a lid on the potentially exorbitant costs of sickness absence.\nThis is where the Bradford Factor comes in. Representing the number of unplanned absences an employee has in a year, the Bradford Factor allows managers to assess the potential impact absence is having on the overall running of the business.\nOriginally developed at the Bradford University School of Management in the early 1980’s, this system allows managers to run insightful reports from their HR tools, better identifying employees who have a high volume of absences and giving them the information they need to deal with the situation.\nThe more absences an employee clocks up, the higher the Bradford Factor score. Essentially, the score given provides managers with potential courses of action, depending on how extreme an employee’s score is.\n|50-124||Consider a verbal warning|\n|25-399||Consider a first written warning|\n|400-649||Consider a final written warning|\nConsiderations when using the Bradford Factor\nIt is inadvisable to use the Bradford Factor in isolation, but combined with return-to-work interviews and regular one-to-one conversations with employees, as part of your company’s performance and HR tools software it offers managers a useful early warning tool.\nUltimately, it is up to managers to be in personal contact with their employees, especially when pre-existing health problems are present and the employee is likely to require more time off than average.\nReturn to work interviews and how to structure them\nOne of the most commonly used methods for combating absence rates is the return to work interview. Effective for dealing with both short- and long-term absences, the return to work interview has been proven to cut sickness absence rates.\nA return to work interview is a common HR process, which involves checking in on a returning employee, before they get stuck back into their work. While conducting such an interview is not a legal requirement, it is becoming standard practice for larger companies in the UK.\nResearch shows that carrying out a return to work interview is one of the most effective ways to manage attendance and reduce absence, according to Fit For Work, a Government-funded initiative offering free expert advice about work-related health.\nThere are many benefits to conducting return to work interviews. These include:\n- Making sure employees really are well enough to get back to work\n- Updating employees on what’s happened in their absence\n- Identifying whether any workplace adjustments are needed\nPreparing for the return to work interview\nManagers should be fully prepared before conducting a return to work interview, reviewing attendance records and any notes relating to previous absences.\nStructuring the return to work interview\nIt is important to ensure an employee feels welcome when returning to work after an absence. This is one of the key purposes of a return to work interview. If you get this wrong, you risk damaging that employee’s confidence which may in turn lead to more absence.\nAsk if they are currently fit to work and if there is any doubt consider the possibility they require more time off. Do not necessarily rely on the employee’s account of their fitness for work. The employer also has a duty of care to assess an employee’s fitness for work.\nCheck whether they have visited their doctor, keep a record of sick notes and any other information you have from your review of their absence records which might be relevant to ongoing health issues.\nOnce you have established that an employee is fit for their return to work, there are various actions that should be implemented to ensure a smooth transition.\nIf the most recent absence is part of a string of connected absences, it is important to establish whether there is an ongoing health issue that needs to be discussed further. For ongoing health issues it may be necessary to make workplace adjustments to help people perform more productively.\nFor longer term, more persistent absences, it may be necessary to schedule a review, and any new information should be logged in your HR tools.\nAfter the meeting\nIt is important to remember that both employer and employee must sign the return to work form. Once this has happened be sure to log the results, flagging any work or disability-related issues while ensuring that any other cause for concern has been identified.\nKey Performance Indicators (KPIs)\nOnce employees have returned to work after time off for ill health or any other reason, it’s time to establish key performance indicators (KPIs) to enable a comprehensive system of performance measurement and target-setting.\nKPIs should be aligned to the top-level goals of your company, be measurable and capable of being continuously monitored.\nEffective KPIs will keep your staff and company on track to achieve objectives, spot problems and opportunities and keep your projects in line with its core goals.\nChoose measures which best drive success and improve performance. Bear in mind that targets should be achievable to build confidence and improve morale.\nThis is especially important for ensuring that people returning after an absence feel they are a productive part of their team.","Onsite Flu Vaccination Program\nEvery year influenza, or “flu,” affects employers and businesses. Influenza costs an estimated $10.4 billion a year in direct medical expenses and an additional $16.3 billion in lost earnings annually.* ARCpoint of Nashville South offers a proactive and innovative approach that saves time and money to you and your employees. ARCpoint of Nashville South Labs can come on-site to your organization and provide flu shots to your employees.\n*as reported by CDC Foundation, citing a 2007 study: Molinari NA, Ortega-Sanchez IR, Messonnier ML, et al. The annual impact of seasonal influenza in the US: measuring disease burden and costs. Vaccine. 2007; 25(27):508\nCredentialing Services & More\nARCpoint Labs of Nashville South provides titer testing, vaccinations, and credentialing services to individuals, organizations, schools and businesses, including:\n- Tech Schools, Colleges, Universities with Allied Health Programs:\n- Medical Assisting, Nursing, Dental, Radiology, Physical Therapy, Occupational Therapy, Certified Nursing Assistants,\n- Phlebotomy, Emergency Medical Technician/EMT, Veterinary\n- Nursing Homes, Assisted Living Facilities, Senior/Retirement Communities\n- Home Care Agencies\n- Home Health Agencies\n- Physician Practices\n- Dental Practices\n- Orthodontist Practices\n- Endodontics/Oral Surgery Practices\n- Surgery Centers\n- Physical Therapy Centers\n- Childcare Centers, Preschools\n- Employers with International Travelers\n- Correctional institutions (staff and inmates)\n- School Staff\nARCpoint Labs of Nashville South can provide vaccinations to your employees who may be exposed to a variety of viruses or illnesses and need immunization and protection in the workforce. Contact us to learn how ARCpoint Labs of Nashville South can support your organization and employees with vaccinations and credentialing. We offer the following Immunizations:\n- Hepatitis A\n- Hepatitis B\n- Hepatitis C\n- Influenza/Flu Vaccine\n- Measles, Mumps, and Rubella (MMR)\n- Tdap and DTaP (Tetanus, Diphtheria, and Pertussis)\n- Varicella (Chickenpox)\nTiter Testing (Proof of Immunization)\nA titer is a laboratory blood test that measures the presence of antibodies in the blood and provides degree of immunity to a disease; it is used to check immune status to either a previous vaccination or infection of a disease. If a test is positive or above a certain value, then the individual has immunity. If a test is negative or below a certain value, then there is no immunity. Titer tests allow a clinician to determine which vaccinations are necessary or if a booster vaccine is needed. A Qualitative Titer will indicate immune vs. non-immune status without any numerical value. A Quantitative Titer will provide a specific numerical value with a range for immunity. Some employers and schools require quantitative serum titers only.\nTypes of Titers\nMMR Titer (Measles, Mumps, Rubella):\n- MMR Titer Test is used to check immunity for Measles, Mumps, and Rubella due to previous infection or previous MMR vaccination.\nVaricella Titer Test (Chickenpox Test):\n- The varicella-zoster virus causes chickenpox which is highly contagious and can be spread through the air or direct contact with an infected person. Generally, once a person has recovered from a chickenpox infection they generally develop a lifelong immunity. The varicella vaccination of two doses will provide ongoing immunity.\n- The Varicella Zoster Virus Antibodies IgG Test is used to look for the antibodies that indicate immunity as a result of past episode of chickenpox or vaccination.\n- The Varicella Zoster Virus Antibodies IgM Test can identify a current chickenpox infection.\nTdap Titer Test:\n- A panel that checks your immunity to Tetanus, Diphtheria, and Pertussis. These are the components of the Tdap and DTaP vaccines.\n- The Tetanus & Diphtheria Titer is useful for determining whether you have protective antibodies to tetanus and diphtheria. These antibodies are usually generated from prior vaccination. The Pertussis Titer checks for IgG antibodies to Bordetella pertussis.\nHepatitis A Titer:\n- Hepatitis A IgM Antibody Test is used to determine if a person has recently acquired the Hepatitis A infection.\n- Hepatitis A Total Antibody Test is used to determine if immunity is present due to previous vaccination or infection.\n- Hepatitis A is a contagious liver disease that results from infection with the Hepatitis A virus.\nHepatitis B Titer:\n- Hepatitis B Surface Antigen (HBsAg) is used to screen for recent or long-standing Hepatitis B infection.\n- Hepatitis B Surface Antibody (Anti-HBs) is used to detect previous exposure to HBV; it can also develop from successful vaccination so it is used to determine the need for vaccination (if anti-HBs is absent) or to determine if a person has recovered from an infection and is immune (cannot get the infection again).\n- Hepatitis B virus is an infection spread through contaminated blood and bodily fluids and can cause acute and chronic hepatitis and cirrhosis of the liver.\nRabies Antibody Titer test (Rapid Fluorescent Focus Inhibition Test (RFFIT):\n- Rabies Titer is used to evaluate the immune response to rabies vaccine and monitor the rabies antibody levels in a person.\n- Antibody levels are highest 2 to 3 weeks after a primary rabies vaccine series. The titer levels drop gradually over time so a second infection with the rabies virus may require a Rabies Antibody Titer Test to determine whether a booster rabies vaccine or another series is recommended.\n- A cut-off titer for adequate immunization is between 6-12 IU/mL in healthy individuals completing the vaccination series. High results indicate that the person has adequate immunization against the rabies virus. Low results may indicate that the person is not adequately immunized against the rabies virus.\n- Rabies is a progressive fatal disease caused by rabies virus spread through a rabid animal.\n- Employees in veterinary medicine or animal shelters are at an increased occupational risk.\nLearn More About Our Testing Services\nAccurate. Reliable. Confidential Testing."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:3303e510-59dc-48f1-9b1c-0dc70cee24d2>","<urn:uuid:72244289-8bef-4379-900c-7b25da668781>"],"error":null}
{"question":"How does the remaining forest percentage in the Atlantic Rainforest compare to the impact of mountaintop removal mining in Appalachia in terms of affected land area?","answer":"While the Atlantic Rainforest has only 7% of its original forest remaining due to expansion of plantations and urbanization, mountaintop removal mining has affected approximately 2,200 square miles of Appalachian peaks and valleys (an area almost the size of Delaware). Both represent significant environmental impacts, though through different mechanisms - the Atlantic Rainforest's degradation is primarily due to agricultural expansion and urban development, while Appalachia's landscape transformation is specifically due to coal mining operations that blow the tops off mountains and dump debris in valleys.","context":["Since the 1990s, small bands of Appalachian residents, regional environmental groups, and more recently the EPA have fought what often seemed like a futile battle against mountaintop-removal mining, the radical practice of blowing the tops off mountains to get at the coal seams underneath. The coal companies, backed by local political establishments and conservative jurists skeptical of possible regulatory overreach, have fended off multiple attempts to shut down mountaintop operations. As a result, an ever-widening swath of Appalachian peaks and valleys has been obliterated: approximately 2,200 square miles, according to the EPA, in what is likely a conservative estimate because the footprint often extends beyond the permit zones. That’s an area almost the size of Delaware.\nThat expanse kept growing as the battles mostly went in coal’s favor. Until this month, that is, when environmental groups won a decisive legal victory over a coal company. It may prove to be turning point in the war over the mountaintops, and for the future of coal.\nOn Nov. 15, St. Louis-based Patriot Coal agreed to phase out its mountaintop excavations and redirect its efforts back to underground mining. Adding a symbolic punch, Patriot agreed to decommission its two draglines—enormous boom excavators that do the actual mountaintop demolitions—and can sell them only on the condition that they’re never used in the Appalachian coalfields again. Coal executives usually shrug off complaints about mountaintop-removal impacts as the grumbling of dilettantes and naysayers who don’t understand the need for mining jobs. Yet here was the practically unheard-of spectacle of Patriot’s CEO, Ben Hatfield, acknowledging that mountaintop removal affected both people and ecology: “Patriot Coal recognizes that our mining operations impact the communities in which we operate in significant ways, and we are committed to maximizing the benefits of this agreement for our stakeholders, including our employees and neighbors,\" Hatfield said in court. \"We believe the proposed settlement will result in a reduction of our environmental footprint.\"\nTrue, a specific chain of misfortunes brought Patriot to the table. It’s looking to exit from bankruptcy, the result of falling coal prices, bad business decisions, and huge pension obligations. It was in this context that Appalachian environmental groups forced their issue. Patriot was also on the hook for hundreds of millions of dollars to clean up discharges of the mining pollutant selenium from its operations. Pulling the plug on mountaintop removal was part of the cleanup settlement. Other companies aren’t planning to follow suit: “No, the decision and agreement is theirs and it does not affect our mining plans,” a spokesman for Alpha Natural Resources, the leading surface mining company in West Virginia, told the Charleston Gazette.\nBut Patriot’s woes aren’t unique. They grew out of endemic problems with mountaintop removal that have been getting worse, putting mounting legal, regulatory and economic pressure on the companies that do it.\nAppalachian forests are an ancient and unique ecosystem, one of the oldest in the world, with some of the highest levels of biodiversity to be found in a temperate zone. Destroying them is almost by definition ecological disaster. But because mountaintop removal in Appalachia is a relatively new phenomenon, it’s only in the past few years that scientists have begun to fully document not just the extent, but the nature of the damage.\nMountaintop removal is a three-step process. In the first, peaks are dismantled, descending layer by layer, using explosives and draglines, and the coal harvested. I’ve been to Patriot’s Hobet mine in Boone County, W.Va. Its sprawling operation—the largest single mountaintop-removal mine, covering more than 25 square miles—evokes the smoky, cratered, lifeless aftermath of a World War I battlefield. In the second phase, the vast piles of rock and soil debris this creates—larger in volume than mountaintops themselves, because of the extra space between all those boulders—are dumped in neighboring valleys, creating so-called “valley fills.” Finally, the lopped-off mountaintop is usually bulldozed and planted with something green, creating artificial meadows where none existed before.\nThe worst problem isn’t actually the missing peaks. Forests can be replanted, and sometimes the more virtuous companies actually do this. It’s what the mines do to the delicate mountain waterways. Dumping a mountaintop’s worth of rock into a valley doesn’t just obliterate all life there. It stops up headwater streams, literally the wellspring for the forest ecosystem and the point of origin for entire watersheds. So far more than 1,200 miles of stream beds have been buried. These cannot be restored. And rainwater that once would have fed those nascent brooks passes through a filter of mashed-up mountain instead.\n“There are coal residues mixed with shale rock that’s been buried for millennia, trace metals, pyrite, all these salts weathering out of the rock. Then it’s sitting right in the stream network,” says Emily Bernhardt, a Duke University biologist who has been studying the impacts of mining. “As it rains, it’s basically a chemical battery, if you will, that is constantly releasing its contents into that stream network.”\nThis “battery effect” generates a range of problems for miles downstream. It injects high levels of mineral ions into the water, where high electrical conductivity harms many kinds of aquatic life. Hard-hit species include mayflies, which happen to sit at the bottom of the forest food chain and are a broad indicator of ecosystem health. An EPA study that first drew wide attention to this problem in 2008 said: “Our results indicate that [mountaintop mining] is strongly related to downstream biological impairment.”\nIt’s these downstream impacts that came back to haunt Patriot. Selenium is a micronutrient that bioaccumulates up the food chain and has been linked to a variety of problems, including deformed fish. Selenium contamination downstream from Hobet and other Patriot mines is chronically high. Patriot had thus violated the terms of the Clean Water Act it promised in its permits to observe. Environmental groups took to the courts to get those terms enforced. Cleaning up selenium contamination is complex and expensive; when Patriot agreed to a major cleanup last January, it took on a crushing obligation estimated at $400 million.\nAnd that was the whole idea, the environmentalists say. Until now, companies have not paid a significant price for the collateral environmental damage they cause. “When you look at a company like Patriot which has scores of outfalls across dozens of permits, you’re taking some serious money to come into compliance,” says Derek Teaney, an attorney for Appalachian Mountain Advocates, one of the groups that brought the litigation. “It just revealed, in stark terms, exactly what expenses there are when a company is forced to internalize these costs rather than just having the environment bear them.”\nMountaintop removal’s impacts on human health are fuzzier, but here too new findings are painting an alarming picture. Since 2009, Michael Hendryx, a professor of public health at West Virginia University, has produced a series of peer-reviewed studies with various colleagues that show an association between proximity to surface mines and various health problems: cardiovascular illness, cancers, and birth defects.\nHendryx has begun investigating whether there is a causal connection for any of these. Finding a systematic link between local environmental contamination and such a wide range of health problems would be quite unusual. But in a study published in September, he and several colleagues collected particulate matter from around a mountaintop site in southern West Virginia, consisting mostly of sulfur and silica: that is, dust from the rocks pulverized by demolition explosions and other mining activities. They exposed rats to it, and their lung tissue showed telltale signs of stress that’s a common precursor to cardiovascular problems. Next up, Hendryx says he is planning to collect more samples from air, water, and soil around mountaintop sites.\nAs with the environmental evidence, such public health findings are likely to cause more legal and regulatory problems for coal companies. Of course, the companies have been aggressive, and mostly successful, at defending their interests in the courts. After years of indifference to the issues under Bush, the Obama EPA has tried to assert more authority over mountaintop permits. The EPA is supposed to enforce the Clean Water Act, and the more aggressive posture is part of the “War on Coal” that energy companies furiously denounce. But the agency’s efforts have so far been rebuffed by the courts as an overreach: Under the weird legal regime that governs mining, it’s the Army Corps of Engineers, not the EPA, that has the ultimate say-so over those permits. This is why the Patriot settlement is a breakthrough: It is a crack in the legal firewall that has protected mountaintop removal for 20 years. If more selenium suits follow, it may crack further.\nLong-term, such big cleanup costs could be a big drag on surface mining, which is already facing serious economic headwinds. The Appalachians have been actively mined for more than a century, a longer time than any other region of the United States. The easy-to-get stuff is long gone. Mountaintop removal is a product of this trend. Blowing the tops off mountains offers an economies-of-scale approach to coal mining: bigger mines, fewer employees, lower operating costs per unit. Until recently, aggressively expanding the practice helped make up for plunging productivity. (In central Appalachia, coal mine productivity fell 45 percent, an average of 5.9 percent per year, between 2000 and 2010.) But overall production has continued to fall. Add to that increasing competition from natural gas. Over the past year, low gas prices have driven down the domestic price of coal, and the Appalachian coal industry has been particularly hard-hit. Some producers are shifting their focus to metallurgical coal, a higher-quality form that is mined underground, rather than the thinner seams exposed by mountaintop mines.\nSomeday, some combination of economic forces and regulatory control will make mountaintop removal untenable. Even when that happens, though, there’s no way to rebuild a mountaintop or unfill a valley. Which means those blots on the landscape, and the environment, will linger for decades—or centuries.","but they contain more than 80% of terrestrial biodiversity, and they provide many natural resources to both humans and other creatures. However, our global forests are rapidly degrading and disappearing due to a number of man-made factors¹.\nThe following is a list of five of the most endangered forests on Earth today.\nAtlantic Rainforest, South America\nThe Atlantic Rainforest is located along the Coast of Brazil and extends to parts of Paraguay, Argentina, and Uruguay. The Forest also includes islands off the coasts of Brazil and the offshore archipelago of Fernando de Noronha. The Atlantic Rainforest habitat is composed of both tropical and subtropical moist broadleaf forests, consists of 20,000 plant species, and over 24 Critically Endangered vertebrate species such as Lion Tamarins. The primary threats include the expansion of sugarcane and coffee plantations, and increased population and urbanization of the areas immediately surrounding Rio de Janeiro and Sao Paulo. A mere 7% of the original Atlantic Rainforest remains today²,³,⁴.\nThe tropical and subtropical moist broadleaf forests in this region contain over 8,000 plant species, and many unique fauna species such as the Philippine Eagle (the second-largest eagle in the world), many other bird species, and the Panther Flying Frog that can glide through the air using webbed fingers and toes. Primary threats include deforestation for farming, a rising human population, and logging. Only 7% of the original habitat remains⁵,⁶,⁷,⁸.\nThis forested region is located in Indonesia, and consists of 17,000 equatorial islands, including Borneo and Sumatra. The special fauna that lives there includes the Sumatran Orangutan and the Bornean Orangutan, and two Southeast Asian rhino species. Threats to the flora and fauna include international animal trade, unsustainable legal and illegal logging, and commercial rubber, oil palm, and pulp production. Only about 7% of the original forest habitat remains⁹,¹⁰.\nNew Caledonia, Asia Pacific\nThis region is located in a group of islands in the South Pacific in the Melanesian region, and consists of tropical and subtropical moist broadleaf forests. More than five endemic plant families exist here, including a parasitic conifer and a large share of the world’s Araucaria trees. The fauna in New Caledonia includes the endangered Kagu, a bluish-grey bird that is endemic to the region’s forests. The primary threat to this forest region include nickel mining, forest destruction, and invasive species. Only 5% of the forest’s original range exists today¹¹,¹²,¹³,¹⁴,¹⁵.\nIndo-Burma, Asia Pacific\nThis region contains both tropical and subtropical moist broadleaf forests. Fauna here include birds, freshwater turtles, and some of the world’s largest freshwater fish, including giant catfish and Julien’s Golden Carp. Threats to this region include draining for rice cultivation, the establishment of shrimp aquaculture ponds, overfishing, and the building of dams, which are destroying large amounts of habitat. Only 5% of the original Indo-Burma forests remain today¹⁶,¹⁷.\nIt is important for humanity as a whole to come to terms with the continued destruction of the world’s forests. If we do not stop the exploitation of our global forests, we will lose a large proportion of the world’s many plant and animal species and their associated values such as medicine and beauty. Because of the importance of global forest ecosystems in carbon sequestration and the regulation of the world’s climate, we will be increasing our vulnerability to climate change dramatically if we lose these precious forests.\nWith so many resources at our disposal today, we can absolutely solve the problems of poverty and the ability of the human race to live sustainably on our planet. We just need the political will to do the right things for both present and future generations. Together, we can solve these problems and create a better world, leaving it better than we inherited it."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:36532fb1-ee8a-4dfe-8bc7-e52faf98869b>","<urn:uuid:5fd7c43a-44fb-407a-a792-c6932fc5ddc5>"],"error":null}
{"question":"How do dietary requirements compare between service monkeys and individuals with Type 2 diabetes, and what are the potential health risks in both cases?","answer":"Service monkeys require strict dietary guidelines consisting of commercial monkey chow three times daily, supplemented with whole oats, vitamins, vegetables, water, and small amounts of apple and nuts. They eat approximately 6 times per day due to high metabolism, and deviation from these guidelines can lead to Type II diabetes. For humans with Type 2 diabetes, dietary management focuses on reducing saturated and trans fats, increasing fiber intake to 30g per day, incorporating low glycemic index carbohydrates at each meal, and including healthy fats from sources like olive oil and nuts. In both cases, proper diet adherence is crucial to prevent serious health complications - diabetes in monkeys and, for human diabetics, complications such as damage to eyes, kidneys, and nerves.","context":["Please take a moment to read the most popular questions about our application process and life with a monkey helper. If you cannot find your answer here, please check our Organizational FAQs or feel free to contact us directly with your question. You can also download a document of all the Life with a Monkey Helper FAQs and Application Process FAQs.\nApply for a Monkey\nOur service monkeys are provided at no cost to our recipients.\nLife with a Monkey Helper\nNo, monkeys do not assist with tasks in public. Monkeys are trained to perform tasks within the home environment only. We believe it’s important to respect our monkey’s hierarchy and acknowledge that service in public would be stressful on both monkeys and their recipients alike.\nIn a placement home, once a bond has been formed and a recipient is given approval from their staff advisors, monkeys enjoy outdoor time with their recipients when weather allows. Many of our monkey-recipient pairs relish their playtime together in the backyard or on the deck or patio. During these times, monkeys are leashed to keep them safe.\nNo – having a service monkey is more like having a small child. Monkeys require a lot of time, patience, and problem solving to develop a solid working relationship. This relationship you develop with your monkey is based on trust and respect (much like a child). You need to earn the trust and respect of a monkey before they will start performing tasks. Developing this relationship can take 6-12 months and the Helping Hands staff advisors are there every step of the way to guide you, your family, and your caretakers.\nIn general terms, hierarchy reflects how each monkey ranks individuals in terms of what role or purpose they have in his/her daily life. A monkey will typically place the recipient (you) then a primary caregiver at the top of the hierarchy, and assign an appropriate rank to other family members, caregivers, friends, visitors, and even household pets. The monkey will also perceive himself/herself as having a particular rank within the hierarchy, with some people above him/her and others below.\nBy understanding your monkey’s natural behaviors, respecting this point of view, and acting responsibly, you will be able to create a positive living environment for your service monkey. It’s important to understand that hierarchy is natural and not something that can be eliminated through training. We, as human caretakers, must respect the monkey’s hierarchy and change our own actions and interpretations accordingly. The concept of hierarchy can initially be tricky to understand but if you are accepted into the program, this will be explained in great detail to you and your family/caretakers by Helping Hands’ staff advisors.\nPlease watch the Monkeys in a Minute Episode, What is Hierarchy for more information.\nMonkeys make a variety of noises that vary greatly in pitch and volume. You will quickly pick up on monkey sounds when a monkey is placed in your house. Monkeys make a variety of grunting/squeaking sounds that represent different emotions/feelings. Excitement, happiness, anticipation, alarm, and fear are just some of the emotions monkeys express verbally.\nMost monkeys that graduate from Helping Hands are “potty trained.” (By potty trained, we mean that they return to their cage to go to the bathroom.) The floor of the cage is wire mesh and all waste falls through to a pan with papers below. Then the cage papers can be changed once or twice daily. Monkeys that are not fully potty trained may wear diapers in a home. A care attendant will most likely have to diaper the monkey if the recipient is unable. It’s important to remember though that any monkey may still have accidents if they are nervous or if something scares them.\nA monkey must place his/her recipient as “alpha” or the top of their hierarchy in order for the bond to be made and a working relationship to be formed. Unfortunately, a monkey will never choose a child as alpha, especially in the presence of other adults (Mom/Dad/caregivers). Therefore, the recipient-service monkey relationship between a monkey and a child would never be successful. There are no exceptions to this reality.\nIn our 35+ years of experience, we have learned that homes with small children are not the best fit for a monkey because kids are often unpredictable and cannot fully grasp the importance of a monkey’s hierarchical structure and the need for a quiet, stable environment.\nIt’s unfair to expect a small child who is pre-conditioned to thinking of monkeys as zoo animals or funny characters they see in movies and cartoons to respect and understand the complexities of actual monkey behaviors. Kids cannot fully comprehend that a monkey is there for service and not as a playmate. For this reason, we require that all children who are living in the household to be 12 or over prior to an adult family member’s application for our program. Please understand that we cannot make exceptions to this policy.\nWe place our monkeys with recipients free of charge. All of the costs associated with training and placing the monkeys (about $40,000 per animal) are funded by individual donors and foundation grants. There are however some long term financial commitments such as food, vitamins, shampoo, and a once a year veterinary wellness visit that we hope our recipients can contribute to but it’s not a limiting factor for acceptance if you need some help with these costs. Helping Hands will cover all placement costs and any major medical care if the need arises.\nOften times, the initial few months of a placement may be more difficult for caregivers and family members rather than the recipient. Often times the caregiver or family members end up doing the everyday monkey maintenance, but do not get the same affection from the monkey that a recipient receives. (In fact, we typically instruct family members and caregivers to “ignore” the monkey so that the bond with the new recipient is built.)\nInitially a monkey placement may be seen as an extra burden for caregivers as they now must take care of the monkey in addition to prior commitments. As a recipient, you may also feel guilty assigning your caregivers additional chores. It is important to identify one primary caregiver who you feel will be responsible for most of the monkey care prior to the placement week. In fact, you should thoroughly discuss the ongoing commitment and responsibility that this family member or caregiver will have prior to applying for a monkey helper. Some chores your caregiver may have to assist with are: monkey bathing (at least once a week), nail filing (once every few weeks or as needed), changing cage papers (daily), changing blankets (daily), and feeding (daily).\nMonkeys eat several small meals throughout the day. The primary staple of their diet is commercial monkey chow. They eat chow three times a day and it is supplemented with whole oats, vitamins, vegetables, water, and a small amount of apple and nuts. Because of their high metabolisms, between chow meals and snacks, monkeys typically eat about 6 times a day.\nIt is extraordinarily important to follow our diet guidelines exactly, as monkeys are at risk for developing Type II diabetes if they are fed inappropriately. Monkey chow is purchased through Helping Hands and costs roughly $30 per bag. One bag of chow will last 2-3 months, depending on the monkey.\nFor more information, please watch the Monkeys in a Minute Episode, Monkey Helper Diet.\nNo – Helping Hands monkeys are New World monkeys, native to Central and South America. New World monkeys do not carry the zoonotic diseases often associated with Old World monkeys (from Africa) such as Herpes B, Monkey Pox, or Simian Immunodeficiency Virus (SIV). Additionally, our monkeys are raised in a closed colony and receive periodic veterinary exams keeping their overall health status high. In fact, our monkeys are more likely to catch the common cold from human germs in your own home.\nThe application process for becoming a monkey helper recipient is an extensive multi-step process. Our goal is to assure that each placement represents the best environment for the monkey helper and the best match for his/her recipient. After meeting all of our eligibility requirements, the process begins by completing a 10 page paper application, providing medical and personal references, and making a home video.\nHelping Hands will typically make a home visit prior to placement. During this visit we continue to assess your readiness for a monkey helper, and talk in person about the placement week, expectations, home modifications, and the roles and responsibilities that you and your family and caretakers will need to perform if you are approved as a recipient.\nThe amount of time the application process takes depends on how quickly you get all application materials returned to Helping Hands. Once ALL application materials are received, the Placement Team reviews all your information and discusses if we have an appropriate monkey that will meet your needs in training at that time.\nThe pool of potential monkey helper recipients fluctuates throughout any given time period. As mentioned above, we place an average of 6-8 monkeys/year. However, our waiting list is not, “first come, first served.” Because our monkey matching process is so unique we are really looking for the “right fit” when it comes to matching our monkeys with recipients. An individual may hear immediately after we review their application that they are accepted, while others can wait up to a year for an appropriate monkey to finish training. That being said, we don’t want our applicants to feel discouraged—we want you to assume you may be a perfect fit for a monkey we have in training right now.\nWe base this decision on several different factors. Our monkeys, much like humans, have a variety of different personalities. Some of our monkeys are very outgoing, some are shy, some like to explore, some like to cuddle, some are reserved while others are confident. Some monkeys prefer men over woman or vice versa, some need a dominant personality in their life while others would prefer someone passive.\nWe base our “monkey matching” on personality profiles of our candidates and the monkeys that are currently in training. We often say that applying for a monkey is, “half adoption process, half dating-service.” Other factors that we consider are: other individuals in your life, home environment, a candidate’s task needs, and general personality traits required to develop a positive relationship of trust and understanding with a monkey.\nOnce a candidate is accepted, we will call them and let them know. At that time we will further discuss what it is like to have a helper monkey and details of the placement week. We ask that you further discuss this commitment with your caregivers/family members and often arrange for a call with them to answer their questions. Once we have scheduled the placement week, we begin shipping supplies to your home that we’ll use to outfit you with everything you need to begin life with your new service monkey.\nThe placement week is when the Helping Hands staff comes to your home to bring the service monkey and to train you and your family and caretakers. This training usually lasts 4-6 days and you can expect to work between 6-8 hours/day with our staff and your new monkey. We try to plan our schedule as much as possible around your daily routine but we do require that recipients clear their schedules that week to give us the maximum amount of time together.\nNo, during the placement week our staff stays at a hotel nearby. During our pre-placement planning, it is helpful if you can direct us to a hotel near a shopping plaza (Home Depot, Walmart, etc.) so we can purchase supplies as needed.\nDuring the placement week, the Helping Hands staff works on setting up a successful relationship between you and your monkey. This is largely a week where we teach you monkey behaviors, hierarchy, relationship development, etc. During this week, we will set up and adapt equipment you will use in the future to help your monkey learn tasks in your home. We will show you some of the tasks that your monkey will eventually do for you. However, it is important that you understand that you need to build a relationship with your monkey first before you will be able to complete tasks independently without the assistance of our staff trainers. Working successfully with tasks usually begins after a few months, once you develop a strong and trusting relationship with your monkey. Again, your staff advisor will instruct you every step of the way.\nBefore the Helping Hands staff leaves your home, you will be assigned to a staff member who will provide phone support or internet video using SKYPE. You will continue to have daily support for the next few weeks and months, depending on the ongoing level of support that is needed. Once you develop a strong relationship with your monkey, task training will begin. This is usually the period of time many recipients find most stressful. Your monkey is still getting used to his/her new home and once our staff leaves, the monkey has to restructure his/her hierarchy in the new home.\nMoving slowly is very important in the first few months of any new placement. This allows for a successful transition for both the recipient and the monkey. It is extraordinarily important to follow all of the instructions of your staff advisor as our experience helps prevent you from making mistakes during this crucial relationship building period.","GOOD NUTRITION FOR TYPE 2 DIABETES\nApproximately 1.5 million Australians1\nhave been diagnosed with Type 2 Diabetes, and it is believed that a similar number\nare yet to be diagnosed. It is the fastest growing chronic disease in Australia,\nyet up to 60% of cases can be prevented. Many more of our population have\nglucose intolerance and insulin resistance, which are pre-diabetic states that can\nlead to Type 2 Diabetes. The incidence of this lifestyle related disease is\nincreasing rapidly in our community as a direct result of people being overweight\nand obese, and reduced physical activity.\nDiabetes can have serious implications for health.\nThe aim of diabetes management is to prevent complications, including permanent\ndamage to the eyes, kidneys and nerves through peripheral vascular disease, heart\nattack and stroke. Damage to the body occurs when blood glucose levels are\ncontinuously elevated. Proper management of diabetes, including diet and\nexercise, results in a more “normal” fasting blood glucose level between\n4.0-7.9mmol/L. This can prevent lifelong damage and disease complications.\nDiet and Lifestyle\nDiet and lifestyle are the two factors that we can\ncontrol. Weight management, a healthy diet with plenty of physical activity,\navoidance of smoking and moderation of alcohol consumption are essential in the\nmanagement of diabetes. Exercise and physical activity can assist greatly in\nreducing blood glucose levels as the working muscles use the blood’s glucose\nfor energy. Having a higher muscle mass also assists in the maintenance of\nnormal blood glucose levels, as skeletal muscles readily take up and store\nLifestyle intervention studies in individuals with\ndiabetes and pre-diabetes show a major impact in preventing and slowing the\ndevelopment of diabetes. These studies incorporate physical activity such as\nbrisk walking, cycling or jogging (for a minimum of 30 minutes on most,\nif not all days of the week), reducing body weight if overweight, increasing\ndietary fibre and reducing saturated fat intake. Lifestyle intervention studies\nshow a significantly greater benefit in the reduction of diabetes progression\nthan taking metformin, a commonly prescribed diabetes medication. Discuss\nlifestyle and medication interventions with your GP, dietitian and pharmacist\nto find an option that is best for you.\nResearch suggests that smoking is associated with\nan increased risk of developing Type 2 Diabetes, as well as increasing the\ncomplications of diabetes, such as cardiovascular disease, leg ulcers and\namputations. Your pharmacist can provide\nadvice on smoking cessation treatments that are most effective.\nType 2 diabetes most commonly occurs as a result of\nbeing overweight or obese. In particular, abdominal adiposity is the major\nfactor in insulin resistance and drives the development of diabetes. A waist\ncircumference of >80cm for females or >90cm (Asian) / >94cm\n(Caucasian) for males is associated with an increased risk of Type 2 diabetes.\nA reduction in body weight 5-10% makes a significant difference, even if that\ndoes not achieve a body weight within the healthy weight range.\nWeight loss, if overweight or obese, should be slow\nand gradual and include sustainable changes in eating habits. It is essential\nto exercise during weight loss to maintain muscle mass and prevent the slowing\nof metabolic rate. Strategies for weight loss in diabetics include significant reduction\nin the intake of fats (particularly saturated fats), sugars and processed\nfoods. Increasing in the proportion of high fibre foods, such as vegetables and\nwholegrain products is important in sustainable weight loss. Discuss diet\nmodification with a dietitian before making major changes to your diet.\nA portion of low glycaemic index (GI) carbohydrates\nshould be consumed at each main meal and snack to manage and maintain your\nblood glucose level. GI ranks foods according to their ability to raise blood\nglucose levels; a high GI meal produces a rapid rise in glucose levels. It is\nalso important to consider glycaemic load (GL). GL takes into account both the\namount of carbohydrate in the food as well as its GI. The nutritional quality\nof the food is also a factor, just because a food is low GI does not mean it is\na good food choice. The GI and GL of commonly eaten foods, as well as useful\nadvice on their relationship to blood glucose levels, can be found at www.glycemicindex.com.\nThe dietary intake of saturated and trans fats\nshould be minimised. These fats are mainly found in animal products and\nprocessed or deep-fried foods. Replace these fats with healthier monounsaturated\nand polyunsaturated fats; e.g. olive oil and almonds. A major dietary\nintervention study found that a diet with a higher proportion of\nmonounsaturated fats reduced fasting blood glucose levels, whilst the\nlow fat diet and ‘usual’ diet lead to increased blood glucose levels and\nprogression of diabetes. High levels of monounsaturated fats are found in\nolive, canola and peanut oils, while whole grains, avocados, nuts, legumes and\nseafood also contain healthy fats.\nA fibre intake of 14g per 4200KJ (or 30g per day)\ncan assist in the stabilising of blood glucose levels. Insoluble fibre from wheat\nbran and wholegrain breads helps to keep bowel motions regular. Soluble fibre\nfrom vegetables, fruits, and legumes helps us to feel full and also slows down\nthe absorption of blood glucose in the small intestine. Soluble fibre can\nassist in regulating blood glucose levels and also lower blood cholesterol\nAlcohol has many risks for those with diabetes,\nsuch as increasing blood pressure, affecting blood sugar levels and reducing\nthe ability to achieve or maintain a healthy weight. Consumption of alcohol should be moderate,\nwhich means no more than 2 standard drinks per day for males and females. Try\nto have at least two alcohol free days per week. When consuming alcohol, you\nshould eat a carbohydrate containing meal or snack to ensure blood glucose\nlevels do not drop too low and result in a ‘hypo’.\nHaving diabetes doesn’t have to prevent you from\nenjoying life and eating tasty, nutritious food. You may find that making diet\nand lifestyle changes to manage your diabetes also results in positive mental\nhealth benefits. Enjoy life to the fullest whilst knowing that you are taking\ncare of yourself as best you can. Remember; it is the person with diabetes who\nis ultimately responsible for managing their diabetes and maintaining positive changes.\nSeek support from your GP, pharmacist, dietitian and physiotherapist to assist\nyou in making lifelong positive changes to your diet and lifestyle.\n1. Diabetes association of Australia.\n2. Ausdiab: www.diabetes.com.au/research.\n3. Lachin et al 2002. New Engl Journ\n4. Tuomilehto et al 2001. New Eng J\n5. ADA Diab Care 2008;31(suppl 1):s61-s78.\n6. Willi C et al 2007 Active smoking and the\nrisk of T2D. JAMA; 298(22):2654-2664.\n8. Due A et al. Comparison of the effects on\ninsulin resistance and glucose tolerance of 6mo high monounsaturated fat, low-fat\nand control diets. Am J Clin Nutr. 2008;87:855-62.\nClinical Practice Guidelines: Management of overweight and obesity in adults"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:c9a5477d-0b08-47f9-9f45-ce776fbeb323>","<urn:uuid:a871d237-34ca-4ef2-81c3-5afac7bdd901>"],"error":null}
{"question":"What is the SCAMPER technique, and how can it help manage resistance to organizational change?","answer":"SCAMPER is a creativity technique developed by Bob Eberle in the early 70s that helps overcome challenges through a checklist of idea-spurring questions. The acronym stands for Substitute, Combine, Adapt, Magnify, Put to Other Uses, Eliminate, and Rearrange. When implementing organizational change, SCAMPER can be valuable for addressing resistance, as it allows managers to explore different approaches through systematic questioning and creative problem-solving. This is particularly important since resistance to change often stems from communication gaps, inadequate information, and insecurity. By using SCAMPER's structured approach, managers can develop strategies to involve employees in the change process, communicate effectively, and address concerns systematically.","context":["SCAMPER is a technique you can use to spark your creativity and help you overcome any challenge you may be facing. In essence, SCAMPER is a general-purpose checklist with idea-spurring questions — which is both easy to use and surprisingly powerful. It was created by Bob Eberle in the early 70s, and it definitely stood the test of time.\nIn this posting, I present a complete SCAMPER primer, along with two free creativity-boosting resources: a downloadable reference mind map and an online tool that generates random questions to get you out of a rut whenever you need.\nSCAMPER is based on the notion that everything new is a modification of something that already exists. Each letter in the acronym represents a different way you can play with the characteristics of what is challenging you to trigger new ideas:\n- S = Substitute\n- C = Combine\n- A = Adapt\n- M = Magnify\n- P = Put to Other Uses\n- E = Eliminate (or Minify)\n- R = Rearrange (or Reverse)\nTo use the SCAMPER technique, first state the problem you’d like to solve or the idea you’d like to develop. It can be anything: a challenge in your personal life or business; or maybe a product, service or process you want to improve. After pinpointing the challenge, it’s then a matter of asking questions about it using the SCAMPER checklist to guide you.\nConsider, for instance, the problem \"How can I increase sales in my business?\"\nFollowing the SCAMPER recipe, here are a few questions you could ask:\n- S (Substitute): \"What can I substitute in my selling process?\"\n- C (Combine): \"How can I combine selling with other activities?\"\n- A (Adapt): \"What can I adapt or copy from someone else’s selling process?\"\n- M (Magnify): \"What can I magnify or put more emphasis on when selling?\"\n- P (Put to Other Uses): \"How can I put my selling to other uses?\"\n- E (Eliminate): \"What can I eliminate or simplify in my selling process?\"\n- R (Rearrange): \"How can I change, reorder or reverse the way I sell?\"\nThese questions force you to think differently about your problem and eventually come up with innovative solutions.\nA classic example is MacDonald’s founder Ray Kroc. In hindsight, it’s easy to identify many of the ideas he used through the SCAMPER lens: selling restaurants and real estate instead of simply hamburgers [P = Put to other uses]; having customers pay before they eat [R=Rearrange]; letting customers serve themselves, avoiding the use of waiters [E=Eliminate] — just to mention a few.\nYou will find below a comprehensive help guide to using SCAMPER. There are more than 60 questions that can be asked, along with almost 200 words and expressions you can create associations with.\nThink about replacing part of the problem, product or process with something else. By looking for replacements you can often come up with new ideas. You can change things, places, procedures, people, ideas, and even emotions.\n- Can I replace or change any parts?\n- Can I replace someone involved?\n- Can the rules be changed?\n- Can I use other ingredients or materials?\n- Can I use other processes or procedures?\n- Can I change its shape?\n- Can I change its color, roughness, sound or smell?\n- What if I change its name?\n- Can I substitute one part for another?\n- Can I use this idea in a different place?\n- Can I change my feelings or attitude towards it?\nalternate, colorize, exchange, fill in for, locum, proxy, relieve, rename, repackage, replace, reposition, reserve, shape, stand in for, surrogate, swap, switch, take the place of\nThink about combining two or more parts of your problem to create a different product or process or to enhance their synergy. A great deal of creative thinking involves combining previously unrelated ideas, goods, or services to create something new.\n- What ideas or parts can be combined?\n- Can I combine or recombine its parts’ purposes?\n- Can I combine or merge it with other objects?\n- What can be combined to maximize the number of uses?\n- What materials could be combined?\n- Can I combine different talents to improve it?\namalgamate, become one, blend, bring together, coalesce, come together, commingle, conjoin, fuse, intermix, join, link, merge, mingle, mix, package, relate, unite\nThink about adapting an existing idea to solve your problem. The solution of your problem is probably out there already. Bear in mind that all new ideas or inventions are borrowed to some degree.\n- What else is like it?\n- Is there something similar to it, but in a different context?\n- Does the past offer any lessons with similar ideas?\n- What other ideas does it suggest?\n- What could I copy, borrow or steal?\n- Whom could I emulate?\n- What ideas could I incorporate?\n- What processes can be adapted?\n- What different contexts can I put my concept in?\n- What ideas outside my field can I incorporate?\nacclimatize, adapt oneself, adapt, adjust, alter, amend, become accustomed, bend, change, conform, contextualize, copy, emulate, familiarize, find your feet, fit, get a feel for, get used to, incorporate, make suitable, match, modify, readjust, refashion, revise, rework, settle in, transform, vary\nThink about ways to magnify or exaggerate your idea. Magnifying your idea or parts of it may increase its perceived value or give you new insights about what components are most important.\n- What can be magnified or made larger?\n- What can be exaggerated or overstated?\n- What can be made higher, bigger or stronger?\n- Can I increase its frequency?\n- What can be duplicated? Can I make multiple copies?\n- Can I add extra features or somehow add extra value?\namplify, augment, boost, enlarge, expand, extend, grow, heighten, increase, intensify, lengthen, make seem more important, multiply, overemphasize, overstress, raise, strenghten, stretch out\nPut to Other Uses\nThink of how you might be able to put your current idea to other uses, or think of what you could reuse from somewhere else in order to solve your own problem. Many times, an idea only becomes great when applied differently than first imagined.\n- What else can it be used for?\n- Can it be used by people other than those it was originally intended for?\n- How would a child use it? An older person?\n- How would people with different disabilities use it?\n- Are there new ways to use it in its current shape or form?\n- Are there other possible uses if it’s modified?\n- If I knew nothing about it, would I figure out the purpose of this idea?\n- Can I use this idea in other markets or industries?\nabuse, apply, avail yourself of, behave, benefit, bring into play, contextualize, deplete, draw on consume, employ, enjoy, exercise, exhaust, expend, exploit, get through, handle, luxuriate, make use of, manage, manipulate, mistreat, operate, reposition, source, spend, take advantage of, take pleasure in, tap, treat, use up, utilize, waste, wear out, work\nEliminate (or Minify)\nThink of what might happen if you eliminated or minimized parts of your idea. Simplify, reduce or eliminate components. Through repeated trimming of ideas, objects, and processes, you can gradually narrow your challenge down to that part or function that is most important.\n- How can I simplify it?\n- What parts can be removed without altering its function?\n- What’s non-essential or unnecessary?\n- Can the rules be eliminated?\n- What if I made it smaller?\n- What feature can I understate or omit?\n- Should I split it into different parts?\n- Can I compact or make it smaller?\nabolish, control, curb, destroy, disregard, do away with, eradicate, exclude, excrete, expel, exterminate, get rid of, jettison, kill, lessen, limit, liquidate, lower, moderate, modulate, pass, play down, purge, reduce, reject, remove, restraint, restrict, shorten, simplify, temper, throw out, tone down, underemphasize, waste, wipe out\nRearrange (or Reverse)\nThink of what you would do if part of your problem, product or process worked in reverse or were done in a different order.\n- What other arrangement might be better?\n- Can I interchange components?\n- Are there other patterns, layouts or sequences I can use?\n- Can I transpose cause and effect?\n- Can I change pace or change the schedule of delivery?\n- Can I transpose positives and negatives?\n- Should I turn it around? Up instead of down? Down instead of up?\n- What if I consider it backwards?\n- What if I try doing the exact opposite of what I originally intended?\nadjourn, annul, back up, change the date, change, delay, drive backward, go backward, invalidate, invert, move backward, move, overturn, postpone, put off, quash, readjust, rearrange, relocate, render null and void, reorder, reorganize, repeal, reposition, reschedule, reshuffle, retreat, swap, switch, transpose, turn around, undo, withdraw\n(icons by Everaldo Coelho)\nThere are many ways to use SCAMPER. For example, you can sequentially go through all the questions in the previous section as fast as you can; or you can stay on each question until you think you exhausted all possibilities.\nHowever, when it comes to creativity, getting random — and unexpected — input can really help your mind find a solution for that ‘impossible’ problem. With that in mind, as a companion to this article, I created the SCAMPER Random Question Tool: it shows you an unexpected question drawn from all the SCAMPER questions in the previous section. Think about a problem that has been nagging you then give the tool a try to see how many options you can generate.\nI’ve put together all the SCAMPER questions from the previous sections in a mind map, formatted for a single printed page. Think of it as a handy one-page reference you can use whenever you are stuck or just need a kick start to get your creative juices flowing.\n- SCAMPER Reference Mind Map [.pdf, 646 KB]\n3. Thinkertoys Book\nThe best resource I know about SCAMPER is Michael Michalko’s wonderful book Thinkertoys: it has more than 40 pages dedicated to SCAMPER alone. Michael’s book is the most comprehensive creativity reference I have put my hands on: there are more than 40 creativity techniques that should suit every taste — from the most logic to the most intuitive types. Highly recommended!","How organizations manage resistance to change\nSevere competitive and economic pressures that organizations face today were unthinkable a few decades ago. In order to shed excess costs and to respond more nimbly to customers and competitors, they are being urged to adopt new organizational forms, tightened inter organizational linkages and improved management practices (cf. Miles and Snow 1980, Johnston and Lawrence, 1988). Any change in organization is followed by a kind of resistance from its employees. In this assignment a few methods that can be used to overcome change in the organization are described.\nTechnology developments, social and demographic shifts, competition of changing market and economic issues, tend an organization to implement change in it as well. The rapid and dynamic change in market has increased consumerism. Whether it is an automobile industry or cosmetic industry or IT industry, consumer today has lots of choices these days that they need not have to wait for longer for any product. This changing market scenario imparts a message to managing bodies that the way of work should also change with the changing market. From managerial point of view a change is referred to as change in work pattern, work routine and work culture inside the working atmosphere. Change is normally a reaction to changing commercial, technological, economical, structural and strategic environment in which the company operates (Barbara Senior, Organizational Change). For example; departmentalization, job redesign, implementation of an international division are the examples of structural changes whereas work processes, methods and equipments are technological changes.\nChange should be welcomed as it can produce positive benefits for the individuals, bring opportunities for personal change and development, reduces boredom of work, provides new challenges and an opportunity to participate and shape the outcome. But unfortunately as change is accompanied by resistance, it is very important that the Change Manager anticipate and plan strategies for dealing with resistance not only at the introduction of change but also for monitoring the change over long term (Ronald, G and Smith, J 1995). It is helpful to understand why people resist change, because understanding this allows us to plan strategies to reduce resistance from the beginning. Kotter and Schlesinger identified the basic reasons of resistance to change are communication gap and inadequate information that creates misunderstanding, sense of insecurity, different assessment of situation and disagreement over advantages and disadvantages. Moreover, individuals are more concerned with the implications for themselves (Management by Robbins and Coulter).\nOrganizations do not change, individuals do. No matter how large is the project you are taking on, the success of project ultimately lies with each employee doing their work differently multiplied across all of employees impacted by the change (Web 1). Individual barriers to change include- tradition and set ways; loyalty to existing relationships; failure to accept the need for change; insecurity; preference for the existing arrangements; break up of work groups; different person ambitions; fear of power; skills and income; inability to perform as well in the new situation as for example, when quality control methods based on statistical models were introduced into manufacturing units, the quality control department have to learn the new methods. Some may fear that they will be unable to do so and may develop negative attitude towards the change or perform poorly if required to use the new methods. Sometimes change is resisted because of failures in the way it is introduced to the employees and the management fails to explain the need for change and its future benefits. Poor employer relations, lack of involvement in process and failure to offer support and training for the introduced change are the other reasons for change resistance (Web 2).\nResisting change takes many forms (Web 3) and the more obvious form is of active resistance, objection and refusal to cooperate with the change occurs. Sometimes, resistance appears to be individual and sometimes it is clearly situational. It may be passive in which colleagues agree to a change but are unwilling or unable to implement something new. This subtle form of resistance is dealt with more difficulty. For example, at a staff meeting everyone agrees to follow a new procedure, but after several weeks it is being discovered that the procedure has not been implemented yet. Another example of this kind is the introduction of new computers at the new place but virtually no one is using them for the purpose for which they are intended, since the staff had their own machines. The employee consents to change by agreeing to it but later he only changes to appear cooperative, but in fact he is doing most things the way he was before the change.\nAt the moment the change program is announced, many employees will employ tactics to protect themselves, their turf, and ultimately their place in the organization.Ã‚Â Some will aggressively challenge the necessity for change. This is a time waster and thus prevents critical objectives from being met. Every person who facilitates the change process must work diligently to build consensus. The employee must be assured that every idea is worth considering. If anyone argues, he or she can be asked to explain why he or she feels the way they do and ask for three or four suggestions for making the process work.Ã‚Â Some managers and members of the leadership team will avoid change by passively refusing the commitment to the process. Often these leaders will resist the change effort by being unavailable for meetings, denying resources, or withholding feedback. “The leadership” is a particularly difficult foe, because change efforts often require the use of resources managed by the leadership, such as time and money. Without these resources change efforts are likely to fail. Accountability with consequences is the primary means for assuring leadership participation. Many employees and organizational leaders search for personal or professional diversions during the change process that will ultimately hinder the effort. A distracted individual can undermine the change effort by not being present physically or mentally when his or her critical input is needed. Not being mindful of change creates an unnecessarily difficult experience for every member of the team. Such carelessness calls to mind the wasted energy expended when one runs against the wind. Change efforts provide an opportunity for every one affected to secure a new place in the organization or make a decision to seek a better fit elsewhere.\nKen Hultman argues that while no-one is a perfect change agent, managers have to be impeccable role models for bringing up a successful change. The essential attributes of such a person include the ability to be a clear thinker who is able to get a view about organizational situation and reach at logical conclusions. Hultman suggests few things in creating the right environment for change to occur. Firstly we must do things to establish a positive climate (p172) and secondly we must attempt to create environmental conditions that encourage an interest in improvement. Managers must demonstrate that how changes will improve employees circumstances and that there are opportunities in the change such as enabling colleagues to increase their knowledge and skills leading to genuine achievements and progress They must cultivate a value for collaborative working among staff and colleagues need each other to complete their tasks, it is easier to develop values of co-operation and mutuality. Whatever are the circumstances management must stay calm. At the heart of HultmanÃ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s analysis is a set of humanistic values along with an assumption that one cannot even hope to influence another colleague without firstly demonstrating that they will have their needs met in some way. It is likely to be counterproductive by getting impatient, exasperated and angry.\nBeing a change manager it is his/her duty to reduce the resistance towards change and towards change and to increase the enthusiasm and level of commitment for the change. While likely to encounter the people who resist change, people who welcome change will also be encountered and by knowing the reasons for their acceptance to change, the communication plan will be better formulated. People will accept change when they see possibility that they will gain something from the change. The gain may be either personal like, money; increased job security; status; self satisfaction; less effort and time and gain in better personal contact or other like it provides new challenges, likeness of the source, reduction in boredom etc.\nIn order to reduce resistance to change, the manager should involve people affected by change, actively seeking their thoughts and reactions to proposed changes. They must develop a proper attitude towards resistance to change and realize that it is neither good nor bad. The best way to minimize resistance to change is to involve those responsible for implementing it and those affected by it. People are more motivated towards successful completion when they feel that they are the valued participants in planning and implementing the change. Also ensure that people from all the levels of organization are involved in planning the change process and they should be listened carefully. In the early stages, manager should not launch into lengthy diatribes justifying the change as people are not interested in that. They want to be heard and have their concerns attended to. They must recognize that it takes time to work through reactions to change. Then people should be engaged in dialogue about the change. They should do this only after understanding the specific concerns of others completely. Change must be realistic, achievable and measurable.\nCommunication and education is helpful method to sort out the things when resistance is due to lack of information or inappropriate information and analysis. Though time consuming, this method provides great employee support if persuaded. When cause of resistance is difficulty in adjustment to changes, management support and facilitation do work at times. This is expensive and still unreliable way to overcome the change. Manipulation of some information is necessary some times in order to avoid negative reactions by the employee. The people that easily accept changes and get adapted to changing atmosphere can set an example for others and hence they follow the suit. Therefore, they should be the first target of change program.\nThree basic steps- planning, implementation, and evaluation of outcomes of both the plan and implementation are involved in the change process. Resistance to change should be dealt ideally with planning and early stages of implementation. For proper planning for change, a manager must consider about how and when the change is needed and the way it should be communicated to the employees for their better support. Managers should pay attention to the focus of change, the amount of change, and the rate of change in order to implement change. Evaluation of outcomes of change is also very important as all the change efforts are result oriented. If change is not monitored, its effectiveness cannot be measured. This can be done by collecting data and comparing the results against original goals.\nTo wind up at the end of an interesting discussion we can conclude that a degree of resistance is normal since change is disruptive and stressful but in general, most people have mixed reactions towards purposed change, so the change agents can be helpful in highlighting the positive aspects in realistic manner. Although most people feel comfortable with minor changes, no one can live and work by yesterdayÃ¢Ã¢â€šÂ¬Ã¢â€žÂ¢s reality. Managers must reduce change in very effective, meaningful and healthy way without hurting the sentiments of the employees. By providing resources to support the changes, allowing enough time and flexibility and with the widespread commitment of people throughout the organization, change efforts will succeed.\n(2) Hultman, K. (1998), Making Change Irresistible: Overcoming resistance to change in your organisation, Davies-Black Publishing, Palo AltoOrder Now"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:2c27fa4a-ebaf-4ecc-921b-dfcae2ab8d12>","<urn:uuid:1a1104dc-49cd-429f-a39e-8094db2177d4>"],"error":null}
{"question":"How to balance cost and risk in engineering design process?","answer":"The balance between cost and risk in engineering design can be achieved through a risk management approach rather than a transactional approach. While transactional approaches might reduce immediate costs through standardized calculations and low-fee services, a risk management approach adds value by balancing system performance with the client's risk profile. Although this requires higher initial professional fees due to the involvement of experienced professionals, these fees are usually a small proportion of construction costs and minimal compared to life-cycle costs. The investment typically yields returns through modest savings in construction or in-service phases and reduced risk, without relying on excessive conservatism. This approach optimizes the risk-reward balance and increases the likelihood of project success.","context":["by Richard J. Driscoll, P.E.\nThe practice of engineering is often defined in terms of the application of mathematics and science, along with experience and judgment, in the investigation, evaluation, planning, design, construction or operations of systems, in which the health, safety and welfare of the public must be protected. Another definition could be extrapolated from a popular definition of structural engineering: engineering is the art of modeling systems that we cannot precisely analyze, which consists of elements and interfaces we do not fully understand, to withstand loads we cannot properly assess, in such a way that the public at large has no reason to suspect the extent of our ignorance (Schmidt 2009).\nThe first definition emphasizes a rational approach and the consequences of failure; we use mathematics, science, experience and judgment to protect the public. The second definition emphasizes an environment of uncertainty; engineering is an art and we use inherently imprecise models to make predictions. Neither definition emphasizes the value to the public of the systems engineers create and maintain. This value supposedly justifies the acceptance of some level of risk to the public due to the inherent hazards associated with engineered systems and the inherent uncertainty in our methods to mitigate the effects of these hazards.\nConsidering the uncertainties involved in engineered systems and the need to proactively protect the public from hazards, a proper definition of engineering might incorporate the definition of risk management. If we recognize that risks can have both positive and negative outcomes, risk management can be defined as the art and science of recognizing, assessing and acting upon uncertain events and their consequences. Like risk management, engineering is both an art and a science, functioning in an environment of uncertainty. Through engineering, we allow the public to enjoy the benefits of engineered systems, while we seek to protect the public from the hazards associated with those same systems. This requires recognizing, assessing and acting upon risks in the design process. Thus risk management underlies the practice of engineering whether or not engineers are conscious of it.\nThe risk management component of engineering may not be self-evident by watching engineers at work. A lot of engineering practice consists of routine calculations, document preparation and field observation, often performed according to highly structured procedures. Engineers and client alike may come to the impression that engineering is primarily transactional, requiring little more than basic technical knowledge and adherence to procedures to complete projects. A transaction approach leads to an inappropriate emphasis on generating engineering documents as efficiently as possible, and deprecation of the art, science, experience and judgment embedded in engineering documents.\nIn the transactional approach, risks are managed in a highly simplistic way, through the use of codes, standards and procedures, which provide some means of accounting for uncertainty and common, well-understood hazards. However, this can result in some risk being managed through arbitrary and overly conservative means and other risks being neglected. An alternative approach would be to balance the performance goals for the engineered system with the client’s budget and risk tolerance, thus providing a value proposition to the client by increasing the likelihood of project success, without excess conservatism. Since this approach requires a conscientious management of project risk, we can call it the risk management approach.\nThe Risk Management Process\nQualitatively, risk can be defined as the deviation of project outcomes from the nominal or expected outcomes (Wang & Roush 2000). In civil engineering, these outcomes are often thought of as the negative effects of hazards. However, the more general definition allows consideration of positive deviations. Quantitatively, a risk is a loss or a gain associated with deviation from the expected outcome, which can be expressed in terms of cost, schedule or some other applicable metric and used for decision making.\nRisk Management Process\nWhile there is no one process for managing risk, most references describe a few steps in the process, including identification, assessment and disposition.\nFirst, reasonably foreseeable risks must be identified based on available information. Depending on what is known about the project at the time risks are identified, this may rely primarily on experience and judgment, based on what has gone right and wrong for similar projects or projects with similar conditions. In some cases, a particular hazard might be identified and the effect has to be assumed, while in other cases, a risk may be identified without causation being understood. Risk identification requires the honesty and humility to acknowledge that no project is perfect. However, it is also useful to identify outcomes that might be better than expected.\nOnce risks are identified, they can be assessed. Risk assessment is a field unto itself, but generally, it involves determining the likelihood and consequence of each potential hazard, which together comprise the risk associated with that hazard. Since risk is defined in terms of the expected performance of an element or system, it may be necessary to determine the expected performance at this stage as well. Note that this analysis of nominal system performance is part of the typical design process. Depending on the available information and the level of detail for decision making, the risk assessment might be qualitative or quantitative. In practice, a semi-quantitative approach is often adequate, in which relative probabilities and impact severity measures are used to compare and prioritize risks. It may also be useful to classify risk according to types and the extent to which they can be controlled by various parties.\nThe heart of risk management is the risk disposition step, in which a strategy for addressing each identified risk is determined, based on the risk assessment. Generally, the available means of risk disposition are avoidance, reduction, transfer or retention.\nAvoidance: Avoidance requires eliminating the activity from which they risk arises. For example, the risk associated with pile driving vibrations can be avoided by substituting a mat foundation system.\nReduction: Risks can be reduced, but perhaps not eliminated, through prevention or mitigation. Prevention measures reduce the probability or magnitude of a hazard, which will typically reduce the expected impact and thus the risk associated with the hazard. Grouting a soil mass in advance of tunnelling will reduce the variability of its properties and thus the risk associated with those properties. Mitigation is used to reduce the consequence of a hazard when the hazard cannot be controlled. Providing freeboard between the base flood elevation and the first floor of a building in a flood zone mitigates the risk of flood damage to the contents and finishes of the building.\nTransfer: Risks can be transferred from one party to another by contract in exchange for a premium. This can lead to efficiencies if another party can manage a risk better than the party exposed to it. For example, an insurance carrier can manage severe, but rare risks better than their policyholders by pooling the premiums and risks from a large number of policies.\nRetention: Any risk that is not avoided, reduced or transferred is retained by the party exposed to it. Risk retention is often employed with risk reduction, since prevention and mitigation measures leave residual risks. Retained risks are often subjected to control and monitoring. Control consists of operating procedures and contingency plans that allow for risks to occur, but provides for corrective measures that can be taken before consequences become too severe. Since control requires early detection of deviation from expected performance, monitoring is necessary, which consists of various means of observation and measurement. Control is conceptually similar to prevention and mitigation, but consists more of processes and other “soft” systems than more tangible prevention and mitigation measures. Peck’s “Observation Method” is a classic example of risk control in geotechnical engineering (van Staveren 2006).\nRisk Management in the Engineering Process\nIt should be apparent that, like the engineering design process, risk management is an iterative, nonlinear, and perhaps recursive process. The available information changes throughout the duration of the risk management process. For example, exploration is often used to reduce the uncertainty associated with existing conditions. The additional information from the exploration might allow previously unforeseen risks to be identified. Thus the risk management process begins for these newly discovered risks. Similarly, with each phase of a project, as more information is available, new risks will be recognized and new opportunities to optimize how risks are managed will arise. In addition, risk management measures may carry their own risks, which have to be assessed and managed prior to implementation. This requires the involvement of the entire design team for the duration of the project. Early involvement of construction managers and key contractors is also useful in managing project risk.\nThe risk management process provides a framework for understanding the uncertainty associated with a project and taking measures to control the outcomes. This framework is embedded in the process of solving an engineering problem, but if used consciously, it provides opportunities to add value to the problem’s solution. If the hazards and uncertainties specific to the problem can be accounted for as part of the solution to the problem, it becomes unnecessary to control risks through arbitrary measures or excessive conservatism. The result is a solution to the problem that is adequately reliable, considering the risk profile of the project stakeholders, while using resources effectively. Thus, ideally, effective risk management optimizes the risk-reward balance.\nA few, hopefully, familiar examples may help illustrate the value of a conscious approach to risk management in engineering practice compared with the transactional approach.\nLow-Fee Geotechnical Services\nMany geotechnical investigations are performed by testing agencies on a low-bid basis. Since the low fee provides little budget for project-specific engineering analysis, the reports for these investigations typically include limited data presentation and analysis and are prepared with little input from senior professionals. Some risks are not identified or assessed, while others are managed through generic or excessively conservative recommendations. While higher in cost, a more thorough geotechnical scope can provide a better basis for constructor bids, as well as more appropriate foundation recommendations and construction guidance, resulting in lower construction cost and risk.\nDesign Calculation Standardization\nSome design firms make extensive use of standard calculation templates, software and sometimes design assumptions to standardize quality from one project to the next, to maximize productivity and profitability of junior staff and to reduce the amount of time need for senior staff to oversee the production of design calculations. For these organizations, calculation preparation is transactional. The reduced involvement of senior staff can result in junior staff performing inappropriate calculations for a particular problem because a template is available. In addition, non-computational aspects of design, such as constructability can be neglected resulting in increased cost and risk.\nFirms that routinely prepare construction documents usually make use of typical details and standard specification, which are not always coordinated with the particular aspects of the project for which they are used. This can increase the likelihood of errors and omissions in the construction documents, requests for information from the contractor and claims.\nIn each of these examples, a transactional approach is used to reduce the cost of the professional services to be provided and the deliverables associated with those services. The client likely misconstrues the benefit of this approach, understanding the economy, but failing to understand the risk to which they might be exposed by their choices regarding the scope and budget of the services provided to them. Given this attitude, it is hardly surprising that a lot of engineering services have been commoditized.\nManaging uncertainty and risk is fundamental to engineering practice and risk management is embedded in the engineering design process. Consequently, some level of risk management is inherent in engineered systems. However, the unconscious risk management embedded in transactional engineering practice is unlikely to yield an optimal risk-reward balance. A risk management approach to engineering practice adds value at every stage of a project, by balancing performance of the engineered system with the client’s risk profile and increases the likelihood of project success, without relying on excess conservatism.\nConscious risk management through the engineering design process requires a higher level of effort and a higher level of service, with the involvement of well educated and experienced professionals. The resulting professional fees will necessarily be higher than document preparation under a transactional approach. However, design fees are usually a small proportion of construction costs and are minuscule compared to life-cycle costs for a typical construction project. Modest savings in cost or a commensurate reduction in risk during the construction or in-service phases can offset these costs, often with a highly favorable return on investment. Therefore, the risk management approach has the potential to be a great bargain on a project.\nRoss W. Hayes, John G. Perry, Peter A. Thompson and Gillian Willmer. Risk Management in Engineering Construction. London: Thomas Telford Ltd.: 1987.\nWilliam G. Ramroth, Jr., AIA. Risk Management for Design Professionals. New York: Kaplan Publishing: 2007.\nJon A. Schmidt. “InFocus: The Definition of Structural Engineering.” Structure Jan09\nMartin van Staveren. Uncertainty and Ground Conditions, A Risk Management Approach. Oxford: Butterworth-Heinemann: 2006.\nJohn X. Wang and Marvin L. Roush. What Every Engineer Should Know About Risk Engineering and Management. New York: Marcel Dekker, Inc.: 2000."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:88f9c568-f6c1-4d01-878e-d97dfcba0a96>"],"error":null}
{"question":"What are the key differences between sports-related sprains and strains in terms of their anatomical involvement and symptoms?","answer":"Sprains and strains affect different anatomical structures and present distinct symptoms. A sprain involves the stretching or tearing of ligaments, which are connective tissues joining bones together. Symptoms of sprains include tenderness, bruising, inflammation, swelling, and joint instability. Common sprain locations include ankles, knees, and wrists. In contrast, a strain is an injury to muscles or tendons (which connect muscle to bone), resulting from overstretching or overcontraction. Strain symptoms include pain, muscle spasm, and loss of strength.","context":["Downloadable Welfare Booklet\nDownload the Player_Welfare_Booklet. This document has information on a wide range of topics including injury prevention, nutrition, hydration and more.\nDownload a Fitness_Testing_Guide\nThis guide is presented as a service to athletes, coaches, parents and exercise physiologists who wish to evaluate their own or someone else’s fitness level, or to gain a greater understanding of tests that they have performed.\nThere are probably hundreds of standard fitness tests used, and hundreds more variations of these. They can range from elaborate and expensive laboratory tests to simple and inexpensive field tests. Each test also has many advantages and disadvantages that can ultimately determine which is the most appropriate test to perform. If you are designing your own fitness testing regime, with the information about the relative merits and requirements of each test that is contained in this guide, you can make an informed choice on the most appropriate test or tests to use.\nMaintaining good nutrition is essential for everyone, but for serious sportspeople, it can be the difference between winning and losing. When you consider that all teams contain talented players and conditioning levels are high, nutrition can be the most important factor in improving performance.\nPlease click on the links below for a document with more information on each area:\nContains information on:\nNutrition for the Immune System\nLosing Weight for GAA FootballersAdditional resources and information below:\nContains information on:\n* A sample High Carbohydrate, Low Fat Diet & Advice on Reading a Food or Drinks Label\n* Irish Nutrition and Dietetic Institute resource booklet\nWhat Are Sports Injuries?\nThe term sports injury, in the broadest sense, refers to the kinds of injuries that most commonly occur during sports or exercise. Some sports injuries result from accidents; others are due to poor training practices, improper equipment, lack of conditioning, or insufficient warm-up and stretching.\nAlthough virtually any part of your body can be injured during sports or exercise, the term is usually reserved for injuries that involve the musculoskeletal system, which includes the muscles, bones, and associated tissues like cartilage.\nCommon Types of Sports Injuries\n* Muscle sprains and strains\n* Tears of the ligaments that hold joints together\n* Tears of the tendons that support joints and allow them to move\n* Dislocated joints\n* Fractured bones, including vertebrae\nSprains and Strains\nA sprain is a stretch or tear of a ligament, the band of connective tissues that joins the end of one bone with another. Sprains are caused by trauma such as a fall or blow to the body that knocks a joint out of position and, in the worst case, ruptures the supporting ligaments. Sprains can range from first degree (minimally stretched ligament) to third degree (a complete tear). Areas of the body most vulnerable to sprains are ankles, knees and wrists. Signs of a sprain include varying degrees of tenderness or pain, bruising; inflammation; swelling, inability to move a limb or joint; or joint looseness, laxity, or instability.\nA strain is a twist, pull or tear of a muscle or tendon, a cord of tissue connecting muscle to bone. It is an acute, noncontact injury that results from overstretching or overcontraction. Symptoms of a strain include pain, muscle spasm and loss of strength. While it’s hard to tell the difference between mild and moderate strains, severe strains not treated professionally can cause damage and loss of function.","Whether you’re a high-performance athlete, an occasional exerciser or the busy parent of an active toddler, you want to feel your best and attain your goals. If injury or pain has taken you out of action, a sports medicine provider can help you return to the sports and activities you love.\nWhat it is\nSports medicine employs non-surgical means to prevent and treat pain and injury to the musculoskeletal system—your bones, muscles, joints, cartilage, tendons, ligaments and other connective tissue.\nInjuries to the musculoskeletal system are commonly categorized as either acute or chronic. An acute injury occurs suddenly, and usually during an activity—for example, an awkward landing or an unexpected collision. Chronic injuries build over time, by repeated use or misuse of a joint or muscle group.\nAlthough the specialty is called “sports medicine,” you don’t have to be an athlete or have been injured playing a sport to benefit from seeing a sports medicine provider. Allina Health sports medicine providers can help just about anyone, no matter how your injury happened.\nGood for treating\nPlaying sports and being active are generally beneficial to your health, but occasionally injuries happen. Some of the conditions we treat are:\n- sprains and strains, two of the most common sports injuries. A sprain happens when a ligament—a band of connective tissue that connects one bone to another—tears or overstretches. A strain is a pulled muscle, which occurs when muscle or tendon fibers stretch too far.\n- “overuse” injuries involving tendons, muscles or joints\n- runner’s knee, jumpers knee, patellofemoral syndrome, initial evaluation and management of ligament and meniscus injuries\n- shoulder dislocation, shoulder impingement syndrome, AC joint separation\n- tennis elbow and golfer’s elbow\n- broken bones and stress fractures\n- injuries to the hand and wrist, trigger finger, carpal tunnel and gamekeepers thumb\n- hip bursitis and tendonitis\n- ankle and foot sprains and strains, Achilles tendinitis, plantar fasciitis.\nWhat to expect\nYour visit with a sports medicine provider is a collaboration. We know it’s important to understand how your condition happened and how it affects the quality of your life, so we take the time to really listen.\nAfter evaluating your injury, your sports medicine provider will talk with you about your treatment options. Options are many, and can range from new and innovative healing therapies and techniques such as regenerative medicine, to traditional treatments, including:\n- physical therapy\n- prescription medications\n- ultrasound-guided injections\n- sports nutrition and supplementation.\nOur sports medicine and non-surgical orthopedic physicians work closely with our specialized orthopedic surgeons. If your condition indicates the possibility of surgery, your sports medicine team can directly facilitate a consultation with a surgeon who has expertise managing your specific condition.\nGood to know\nIf you’re an athlete, we get you. Allina Health physicians, physical therapists and certified athletic trainers spend thousands of hours each year providing medical sports coverage and athletic training services at school and community events. Many are athletes themselves. All have extensive training in the musculoskeletal system, and all are committed to delivering the highest level of sports medicine expertise to each athlete.\nAlong with providing expertise at scores of community events, schools and athletic clubs, Allina Health is the Official Orthopedic Partner of Minnesota United FC. Whether it’s mending a sprain, fixing a fracture or treating chronic joint pain, athletes of all abilities can access these same top-notch orthopedic specialists.\nIf you don’t think of yourself as an athlete, you still deserve to feel and function your best. Together, we’ll find the solution that best fits you.\nGood for preventing\nIt’s best, of course, to avoid injury altogether. Talk with a sports medicine provider if:\n- you’ve been injured in the past and want to know if there is something you can do to avoid getting injured again\n- you’re recovering from injury and want to know if you are cleared to resume activity\n- you haven’t been injured, but have concerns about playing a sport or participating in a physical activity.\nReviewed by: Robby Bershow, MD; G. Budd Renier, MD; Brent Millikin, ATC, MEd\nFirst published: 5/4/2018\nLast reviewed: 5/2/2018"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:ce0214f5-1062-447f-9f5f-131438c60294>","<urn:uuid:11941828-7ae9-4d42-8747-2b72fb1bfb71>"],"error":null}
{"question":"I'm a lawyer comparing traffic violation penalties: do dangerous driving and operating a vehicle without insurance resulting in bodily injury carry the same maximum jail sentence in Canada and Illinois?","answer":"No, they have different maximum jail sentences. In Canada, dangerous driving can result in up to 10 years of imprisonment for more severe outcomes. In Illinois, operating a vehicle without insurance that results in bodily injury to another person carries a maximum sentence of up to one year in jail.","context":["Traffic law violations and driving offences are not uncommon in Canada. In most cases, you pay the fine, and that’s the end of the process. However, this makes people assume that they are not a big deal and often dismiss them or not take them seriously until they or someone they know gets charged with dangerous driving causing property damage, injury, or, God forbid, someone’s death. In these cases, not being aware of the violations and their consequences can get you into some serious trouble.\nFor this reason, it is essential to understand the various types of traffic offences and what steps you should take to ensure you receive the best possible deal in the courtroom. If you don’t educate yourself on the various traffic violations and their penalties, you could end up losing your license, and in worst-case scenarios, even your personal freedom.\nBelow we discuss some of the most common traffic violations and why you should take them seriously.\nTypes Of Traffic Violations\nIn Canada, there are many different types of traffic violations. The consequences of these violations and infractions can vary from simple tickets to the possibility of severe criminal code convictions that can adversely affect your driving record. However, an experienced criminal defence lawyer in Edmonton, such as Slaferek Callihoo Lawyers, can help prevent this from happening.\nSome of the most common traffic law offences include:\n- Improper braking or non-working brakes\n- Traffic sign or stop sign infraction\n- Not wearing a seatbelt\n- Not carrying an insurance card\n- Taking unsafe turns and more\n- Careless driving\n- Highway offences\n- Not following speed limits at construction or school zones\n- Leaving the scene of an accident\n- Stunt driving\n- Failing to produce valid car registration documents or license\n- Operating a vehicle without insurance\nSerious Criminal Violations\n- Dangerous driving\n- Driving under the influence or impaired driving\n- Failing to cooperate with law enforcement officers\n- Fleeing the scene of an accident with the intent of avoiding criminal charges\n- Driving with a Blood Alcohol Concentration or BAC of 0.08 or over\n- Criminal negligence or causing an accident that results in injury or death\n- Refusing a Breathalyzer Test and more\nAlthough the violations mentioned above are the most common ones, the list of traffic crimes and driving offences is extensive. Let’s discuss some of these common offences and their penalties in detail below:\nThe careless driving offence is committed when a driver drives without due care or attention to the other drivers and traffic laws and causes bodily harm or death to any person. Its penalties include:\n- A fine of not less than $2000 and more than $50,000\n- A possible two-year jail term\n- Up to five years of license suspension\n- Six demerit points\nDangerous driving is a hybrid offence and is a criminal offence under the Criminal Code of Canada. If convicted with less serious outcomes, it can result in a maximum jail time of two years. If convicted with more severe outcomes, the driver could face imprisonment for up to 10 years. In addition to other penalties, the court may also order a licence suspension.\nPenalties for dangerous driving increase if the violation committed resulted in someone’s injury or death.\nCriminal negligence is also a criminal offence under the criminal code of Canada. According to this, a person is criminally negligent if he shows reckless disregard for other people’s lives and safety—for example, criminally negligent behaviour causing harm or death to someone due to street racing, excessive speed, or impaired driving.\nThis driving offence is punishable by a jail term of up to 10 years. Criminal negligence causing death is punishable by life imprisonment.\nPenalties For Traffic Violations\nMinor traffic and driving violations can result in traffic tickets. However, if you let these tickets add up over time, they can lead to serious consequences. For example, in Alberta, you might receive demerit points with a traffic-related conviction. If you receive at least 15 points within 2 years, your license may be revoked or suspended altogether.\nMoreover, for minor convictions, you may receive small or moderate fines. However, major or serious criminal offences involve hefty fines, court costs, jail time, or life imprisonment, depending on the subsiding facts.","Illinois laws impose strict penalties on drivers who operate a motor vehicle without insurance: they face a minimum of a $500 fine if they are caught. Any motorist who breaks these Illinois traffic laws -- not just Illinois residents -- is subject to the same monetary fines, but non-residents may face different non-monetary penalties.\nRequired Insurance Coverage\nIllinois has a mandatory requirement for drivers to carry auto insurance. The minimum requirements for residents include $20,000 for the injury or death of one person in an accident, $40,000 for the injury or death of multiple people in an accident and $15,000 for the property damage that the other driver sustains. Illinois laws require that a person carry an insurance card in his vehicle at all times while driving. A nonresident must show proof of insurance and his insurance must conform to the minimum requirements of his state. A questionnaire may also be sent to random registered drivers in Illinois to ensure compliance with Illinois mandatory insurance laws.\nFines for the First Offense\nA first offense for driving without insurance results in a minimum fine of $500. The maximum fine for a first offense of driving without insurance is $1,000. If you are convicted of driving without insurance and you are an Illinois resident, your license plates will be suspended until you show proof of insurance and pay a $100 reinstatement fee.\nFines for Multiple Offenses\nA person who is convicted of three or more offenses is required to pay a $1,000 fine if the infraction did not result in injury. If you cause bodily harm to another person in an automotive accident and you have been convicted twice or more for driving without insurance, you will be required to pay a $2,500 fine. Repeat offenses also result in a four-month suspension of your license plates and a $100 reinstatement fee.\nNon-monetary Penalties for Residents\nIf you are charged with operating a motor vehicle without insurance, you are required to appear in court. Your driver's license will be suspended for three months and you will have to pay $100 to reinstate your license if it is your first offense. If you violate your driving suspension, your license will be suspended for six additional months. If you acquire insurance in the future, your rates may be especially high if you receive a single or multiple violations for driving without insurance. If the accident caused bodily injury to another person, you can be sentenced up to one year in jail.\nPenalties for Non-residents\nAll states except for Alaska, California, Michigan, Montana, Oregon and Wisconsin have entered into an agreement called the Non-Resident Violator Compact. This agreement provides a way for states to uniformly process traffic violations that non-resident drivers incur. A person who is a resident of a state that is a member of the Non-Resident Violator Compact can stay in Illinois and argue the case in court or he can sign a promise to pay the traffic ticket. If the ticketed driver fails to comply with his promise to appear in court or pay the fine, his state's motor vehicle department will suspend his license until he pays the fine. If the driver is from a state that is not a member of the Non-Resident Violator Compact, he is required to post bail before he can proceed with his trip through Illinois.\n- CyberDriverIllinois.com: Mandatory Insurance\n- CyberDriverIllinois.com: 2012 Offense Code Book\n- Oncle.com: Illinois Compiled Statutes 625 ILCS 5 Illinois Vehicle Code. Section 3-707\n- Illinois Criminal Defense Lawyer: Illinois Compiled Statutes 625 ILCS 5 Illinois Vehicle Code. Section 3-707\n- Brady & Jensen, LLP: Stricter Penalties for Driving Without Insurance in Illinois\n- National Motorists Association: Illinois Motorist Information\n- Missouri Department of Revenue: Insurance Information\n- Illinois General Assembly: Illinois Compiled Statutes\n- CyberDriverIllinois.com: Chapter 8: Driver's License Revocation, Suspension, Denial, Cancellation\n- Colt Law Firm: What is the Non-Resident Violators Compact (NRVC)?\n- Hemera Technologies/AbleStock.com/Getty Images"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:e6823822-943f-4563-bcf5-5b083aaaf3da>","<urn:uuid:0236f566-9ea6-434b-8475-e1b37657fd78>"],"error":null}
{"question":"I'm studying the history of justice symbols - what does Lady Justice's imagery represent in the legal system?","answer":"Lady Justice, derived from the Greek goddess Themis and Roman goddess Justice, is typically depicted with three symbolic elements: A scale representing balance and the careful weighing of arguments by judges; a sword symbolizing the power of decision and verdict-making; and sometimes a blindfold, which has varying interpretations. A full blindfold represents impartiality and judgment without regard to persons, while the absence of a blindfold suggests the need to consider personal circumstances of the accused when determining punishment.","context":["What is the Public Prosecution Service?\nThe Public Prosecution Service is an organization that is tasked with the enforcement of criminal law, on behalf of a country. The Public Prosecution Service executes its duties on the premises of the consensus Kingdom Act Public Prosecution Service.\nThe Public Prosecutor examines the grounds to bring a suspect before the Judge in the Court of First Instance to face criminal charges. A suspect or the Public Prosecution Service can lodge an appeal if the suspect or the Public Prosecution Service disagrees with the judge’s verdict in the Court of First Instance.\nIn the court of appeals the Public Prosecution Service is represented by the Attorney General or, on his/her behalf, the Solicitor General. The Attorney General and the Solicitor General both work at the Public Prosecutor’s Office.\nThe core tasks of the Public Prosecution Service can be divided into three categories:\n- Investigate criminal acts\n- Prosecute criminal acts\n- Oversee implementation and execution of criminal sentences\nHowever the Public Prosecution Service also:\n- Keeps the public informed about ongoing cases or issues\n- Offers advice to the Minister of Justice in terms of law and order\n- Support and integral approach\nWhat are the tasks of a Public Prosecutor?\nA Public Prosecutor is a law officer (Public Defender) who conducts criminal proceedings on behalf of the government or in the public interest. The police are responsible for investigations when a criminal act is committed, while the ultimate responsibility for the investigation lies with the Public Prosecution Service.\nThe Public Prosecutor monitors the investigation to ensure that it is conducted thoroughly and in accordance with the law. When the investigation is completed, the Public Prosecutor can decide to present the case to the court. The Public Prosecutor ensures that the defendant is summoned. A summons is a letter that contains the accusations against the suspect.\nDuring court proceedings the Public Prosecutor elaborates on the accusations against the suspect. Thereafter the judge interrogates the suspect about the case. The Prosecutor is also allowed to post questions, as well as the lawyer of the defendant. In concluding the Prosecutor presents his/her argumentation in support of the penalty demanded.\nThe ancient Greeks had a goddess of justice, Themis. The Romans called that goddess Justice. That is also where the word justice comes from. In legal buildings you often see a statue of Lady Justice or a painting. So Lady Justice is the goddess of justice.\nJustice is always depicted with a scale and a sword, sometimes also with a blindfold. These objects are symbolic of legal tasks.\n- The scale: stands for balance. A judge must carefully weigh the arguments of both parties.\n- The sword: is the symbol of the power of decision, the judge has to make the decision. He has to issue a verdict.\n- The blindfold: There are different opinions about the blindfold. Sometimes Lady Justice is blindfolded, sometimes she is not blindfolded, and there are also images of Lady Justice where she is only blindfolded on one eye. A fully blindfolded Lady stands for impartiality. Judging without regard to persons. However, there are also good arguments to the effect that Lady Justice should not be blindfolded. When determining a punishment, for example, the judge must also look at the personal circumstances of the accused. For that very reason, Lady Justice must not be blindfolded.\nJustice is the symbol of the judiciary. Every Public Prosecutor and every judge must be impartial, capable of properly weighing the odds of the situation and able to demand or give the right punishment. (Source: rechtersenadvocaten.nl)\nThe Trias Politica, the separation of powers is an important principle of modern democracy. It goes back to the ancient Greek concept of social organization, and especially to the distribution of the exercise of power. This means a division of power into legislative, executive and judicial power, which may never rest with one and the same person or body.\n- Legislative powers write the law: States and Parliament.\n- Executive powers act upon and propose laws : Ministers, Civil Servants, Police.\n- Judicial powers implement law: Independent Court and the Public Prosecutor’s Office.\nIn the purest form of distribution of powers, the parliament has the power to pass laws, but their implementation and enforcement (e.g. by the police) are left to the government. The parliament acts as controller of the executive branch.\nIn addition, there are independent judges, who, on the basis of the law (adopted by the Parliament), administer justice and have the power to punish crimes and to judge disputes between citizens. That’s what we call the judiciary. Parliament and the government have no say in the judge’s judgment.\nThere is also a separation between the standing judiciary (the Public Prosecution Service) and the incumbent judiciary (the judge). The Public Prosecutor’s Office is again subject to the Minister of Justice, but in principle it is independent. (Source: montesquieu-institute.eu)"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:d713cf65-9555-4886-b0fa-ed94954d5c01>"],"error":null}
{"question":"How does DevOps solve the traditional waterfall development problems?","answer":"DevOps solves traditional waterfall development problems by introducing the 'Shifting to the Left' approach, which combines Operations and Development teams into one vertical structure. Unlike waterfall methodology where steps must be completed sequentially and modifications require climbing back up several steps, DevOps creates a continuous delivery chain. It automates the software development framework and configuration management, eliminating the need for manual provisioning of network or hardware changes. This reduces the lead time - the total time from code commitment to successful production deployment - from several weeks or months to a much shorter period, while also reducing deployment errors and technical debt.","context":["DevOps helps to solve that one problem every business is concerned about – that is, the ability to tackle business requirements swiftly and deliver the best solution in a short period.\nMany look at DevOps as a holistic approach to being agile. However, for those trying to understand what DevOps is all about or why you must care, there is a better way to look at it.\nDevOps is all about being responsive in an Agile Environment\nResponsive begins with the flexibility of responding to user behavior in a particular environment. From a good SEO perspective, this could be a responsive screen size, platform or the orientation. The meaning does not vary much when we talk about being responsive in software development.\nCompanies share certain common principles like optimal resource utilization, quality software, and timely delivery that help a business to build better customer relationship and brand image. However, aligning all these objectives on one plane is a struggle.\nWe call this the struggle of software delivery triangle\nIf we consider the Waterfall methodology, a developer has to undergo several steps to deliver the final output. Such as specification, analysis, design, coding, testing, deployment, maintenance. Each step begins only after the preceding step is complete.\nIn that case, programmers might have to work almost 18 hours a day until the project reaches the shipping date when rapid testing and last minute quality assurance become afterthoughts. Besides, one has to climb back up the steps to implement modifications or change requests.\nThis is just the Waterfall method and there are other complex methods of development to think of as well. Introducing Agile methodology might have improved the way we develop software today. However, the struggle remains even after applications have moved to the cloud, especially when it comes to overcoming the problem of maintaining the three joints of software delivery triangle.\nSoftware companies continue to face immense pressure when trying to deliver fixes faster. In one typical software development scenario, we saw the devs submitting code at the last hour and the ops working overnight to make up for the wasted hours before the deployment deadline.\nThe entire process gives rise to technical debt, hopelessness and despair down the development chain and the momentum heightens when an issue is raised from the customer end. Pin pointing the exact place of an issue gets incredibly painful not to forget the lack of traceability between an issue, the code file and the requirement.\nAll these lead to the ever-evolving wide gap stretching far and wide between business expectations and delivered output.\nA major reason for this could be the disconnection between the development, QA, and the operations teams. Different teams use different tools to develop the same software. That means they work from tool environments different from one another. So no matter what kind of software development methodology you use, there will always be a disconnection in the way information flows between teams.\nThe Result – Increased Confusion and Slow Delivery of Quality Software\nWith agile, software development might have improved, but DevOps actually adds to the movement. Basically, it enhances the Agile method by building a continuous delivery chain.\nDevOps introduces the ‘Shifting to the Left’ approach, which shifts the Operations and the Development teams in to one vertical structure. Hence, collaboration between both now revolve in an infinite chain without hurting the overall stability of a software.\nSo what drives DevOps?\nIt takes care of the most important metrics – the Lead Time, i.e. the total time taken for a code to get committed and running successfully into production.\nA standard development model may take several weeks to several months to deliver the final output, which in the end could be highly prone to catastrophic deployment and errors. By combining Agile methodology with DevOps, organizations cut down the time of delivery as well as catastrophic disaster.\nFirst, it begins with an agile programming approach that revolves around the four basic principles –\n- Working software over comprehensive documentation\n- Interaction over tools and processes\n- Collaborative teamwork over contractual negotiation\n- Responding to change over pre-defined plan\nNext, it accelerates the feedback loop by aligning product features with market demand. This makes the product always responsive to change. DevOps puts the product at the epicenter of the software delivery triangle ruled by the three principles that contribute to continuous delivery. As a result, implementing new change becomes quicker and the time taken to retrace back and start over again is eliminated.\nThe approach is enhanced using popular methodologies like Scrum and Extreme Programming, and tools such as Chef, Ansible, Puppet, SaltStack for provisioning the server environment.\nTwo important aspects that make DevOps possible\n- Automation of Software Development Framework\nAutomation of software development framework eliminates the need of manually provisioning any network or hardware changes in a data center. You also get to cut down the overall cost and the time taken to deploy an application.\nDevOps predicates the idea that all elements of technology framework can be controlled through code or just one click. With the rise of cloud, you can now also do it in real time through web service.\n- Automation of Configuration Management\nDevOps solves the problem of manual installation and configuration of packages. So with configuration automation, a server is deployed in exactly the same manner every time.\nYou do not have to run across hundreds of servers if you need to make any change. With DevOps, you can do it all from one ESB based Omnibus platform.\nDissolving the Dev-Ops gap\nThe quality of a software depends much on the changing mindset of an organization’s teamwork.\nWhile devs are traditionally concerned with updates and changes, the ops try to ensure that the quality and stability of a software remain unaffected irrespective of any update or change.\nThe problem arises when it leads the Ops team to believe that reducing the number of changes implemented in a software will automatically reduce the frequency of software failures.\nThis tendency of opposing priorities inevitably leads to animosity and cultural clash where both sides of the workforce try implementing their mindset over one another. Besides, it also hinders the overall development process between the two teams.\nDevOps puts a stop to this cultural clash by creating a united loop where all teams come together to plan, code, build, test, release, deploy, operate, and monitor.\nThe first step for implementing this change begins with introducing a software that will reduce the clash and focus more on the collaboration part. This is where we will try to demonstrate the function of DevOps backend and frontend –\nTo do this, we have used the above two figures – (1) a DevOps workflow to explain how DevOps work in the backend, and (2) Traceability View of an ALM instance to show how things look from the frontend.\n**Traceability View is an important feature through which Managers can directly check the code files on which the developer had worked on and the change-set linked to the registered defect.\nIn the above demo flow image, we have used the names of only a few tools. But in real world, teams use more tools for code analysis, security and production monitoring.\n- An error is identified and raised via an Incident Management tool (example – Service Now).\n- The registered defect is reflected inside the Traceability View and the IDE tool (example – Eclipse) that the developer is working in.\n- The developer views the details and then works on the related code files to fix the issue.\n- He or she then checks in the fixed code files to a Source Code Management repository (example – GitHub) and central repository.\n- All details related to the fixed code files are reflected in the Traceability View.\n- The Build execution pipeline is triggered followed by deployment.\n- The codes are validated using automated scripts, which if successful, proceeds to the Staging Server and finally to the Production Server.\n- Committing the fixed code files to the central repository leads to the generation of Change Sets, which is directly visible from the Traceability View.\nAnd that’s how Dev-Ops fulfills the promise of software delivery triangle\nIn short, DevOps delivers the promise of improved business services by using IT operations efficiently. It aligns the work of everyone involved in software production with the way a software responds to customer interaction when out in the market.\nIt also breaks down the strict control that governs who has accessibility to the individual production environment, hardware, and data centers thereby empowering the team to achieve better efficiency together.\nMaintaining software quality is a challenge. As new defects arise, it will bring new challenges and complexities that might be hard to solve. Nevertheless, with DevOps in scene, the framework of your software delivery triangle will remain unchanged. That is why you should care about DevOps!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:ccb87999-6d07-425e-9781-cbf943278364>"],"error":null}
{"question":"How do documentary television programs such as Travel Wild contribute to educating viewers about sustainable tourism practices?","answer":"The Discovery Channel's Travel Wild program, featuring EarthCheck members, showcases sustainable tourism to more than a quarter of a billion people across 34 countries in 15 languages. The program demonstrates concrete sustainability actions by tourism operators while taking viewers to diverse destinations like Mexico, New Zealand, Papua New Guinea, Antarctica, and Australia. This educational approach aligns with current market demands, as recent data shows that nearly 70% of travelers are actively seeking sustainable travel options, though many struggle to identify genuinely sustainable operators among those simply engaging in greenwashing.","context":["Green and sustainable tourism is a badge that most tourism operators wear with pride, but there is a big difference between those who say they do and those who actually do.Over upcoming months, the Discovery Channel is showcasing the sustainability achievements of a number of EarthCheck members around the world to an audience of more than a quarter of a billion people.EarthCheck is the world's leading certification and benchmarking program for sustainable tourism and has 1300 members in 70 countries. The program was established in Australia by sustainable tourism experts EC3 and is heralded as one of the most transparent 'green' reporting programs in the tourism industry due to its scientific approach to data collection and its third party auditing of green initiatives.EarthCheck members including Tourism Queensland, Orion Expedition Cruises, Kaikoura District Council, Auckland Airport, Awaroa Lodge, Dart Jet Safaris, United Campervans, Oamaru Penguin Colony, The Langham Auckland, Xel-Ha, and Grupo Vidanta Mexico, are in the spotlight for the contribution they are making to protecting the environment in their parts of the globe.The coverage kicks off on Discovery World HD Asia this month. The programme, called Travel Wild, will air in 15 languages in 34 countries and will feature tourism operations in Australia, New Zealand, Papua New Guinea, Antarctica, and Mexico.The ten-episode series will then air in Europe and USA on the Discovery Network later in the year.The EarthCheck supported program showcases the changing face of travel, taking viewers to visually spectacular, biologically diverse destinations. More than just a travel programme, it demonstrates what tourism operators are doing to protect these global assets for travellers and the local communities who call these destinations home.Episode descriptions and supporting environmental case studies:Mexico RivieraThe Riviera Maya and Cancun are tourism meccas attracting people from all around the world. But behind the busy tourism strip is a stunning environment that hides the ancient history of when a meteor crashed to earth. From five star resorts, an ancient Mayan culture and a jungle paradise this place has it all. The episode explores fresh water senotes (underwater caves) that stretch for hundreds of kilometres, theme parks that educate guests about Mexican culture and the local environment and features at mega resorts, like Grand Palladium Riviera Resort & Spa and Grupo Vidanta, who are their part toward sustainable tourism.Learn more about the sustainability achievements of Grand Palladium Riviera Resort & Spa here.Learn more about the sustainability achievements of Grupo Vidanta here.New Zealand 1 - Land of the long white cloudNew Zealand has built its reputation on its clean green image and spirit of adventure so this episodes explores tourism businesses who prioritise environmental sustainability in their day to day practices. From the comfort of a camper van, the program explores the South Island and visits the fairy penguins at Oamaru and ups the ante over the Dart River with a jet boat ride to the place where parts of Lord of the Rings were filmed before heading through New Zealand's most travelled national park and the Awaroa guest lodge.Learn more about the environmental sustainability achievements of Dart River Jet Safaris here.Learn more about the green initiatives at United Campervans here.Palau - The Making of a SanctuaryPalau's healthy marine biodiversity has given the island republic a reputation as a haven for divers and adventurers. Home to the world's first shark sanctuary, this small country has become an icon in sustainability with tourism operators leading the way and demonstrating how environmental sustainability contributes to the nation's overall economic health.Papua New Guinea - The Land of CulturePapua New Guinea is one of the last places in the world where tribal culture still resinates strongly through its foundations. This episode explores how local village communities are working together with tourism operators to improve the local economy without compromising the traditional way of life. Viewers will see active volcanoes, cruise the untouched Sepik River and visit pristine islands in a show embracing the traditions and culture of PNG.Learn more about the sustainability achievements of Orion Expeditions here.Huatulco - Sustainability in actionHuatulco is fast becoming another sought after Mexican holiday location but the locals are developing their tourism products with sustainability as a priority. From the tourism minister of Mexico setting laws to local school kids cleaning beaches - this community is committed to conservation. This episodes discovers the lush jungles of Huatulco, cascading waterfalls, and the adventure of abseiling, kayaking rivers, snorkelling and surfing. This is a beautiful episode of adventure fun but it also uncovers the passion the locals have to help preserve their destination.Learn more about the achievements of the EarthCheck Gold certified sustainable community in Huatulco here.Antarctica - The last true frontier (2 episodes)Antarctica offers an adventure to a place few people ever visit and that is as beautiful as it is wild. These episodes cover 5000 miles and explore the strict measures taken by tourism operators to ensure that the islands and Antarctic continent remain as it was when it was first discovered by early explorers.Learn more about the sustainability achievements of Orion Expeditions here.The Great Barrier ReefThe Great Barrier Reef is the world's most valuable reef and an irreplaceable asset for our planet. It is the world's biggest natural-tourism drawcard so this episode questions the importance of reefs around the world for tourism and what it would mean to lose them. It explores measures the tourism industry is taking to protect the eco-system and the management processes in place to deal with changing conditions.New Zealand 2 - Kaikoura and AucklandThe small coastal township of Kaikoura was once best known for its high unemployment, but today, it's the whale watching capital of the world. Back in the 1980s, the local Iwi (Maori tribe) mortgaged their homes and purchased the first of what would become a fleet of whale watching vessels; transforming the community's fortunes in the process. According to Kati Kuri legend, Paikea travelled to New Zealand on the back of a whale. So, when looking for a means to create local employment, it seemed only appropriate for his descendants to look to the whale for a better way of life. Whale Watch Kaikoura helped rally the local community around a single focus in purpose and today, this small coastal town is bursting at the seams with trendy eco-accommodation, cafes and galleries that attract nature lovers from across the globe. The program also covers initiatives at the other of the spectrum including the significant achievements made by the Auckland airport and the sustainable tourism leaders at the five-star Langham Hotel in Auckland.Learn more about the environmental achievements of the EarthCheck Gold Certified Langham Hotel here.Learn more about the environmental sustainability programs at Auckland Airport here.Learn more about the sustainable community of Kaikoura here.The Yucatan - Tourism Reviving the Mayan CultureThe Yucatan is a place where dense jungles hide the ruins of a once powerful Mayan civilisation. This episode encounters\"Sobadores\"; traditional bone healers, who are using skills passed down through the generations which have found an unexpected modern-day niche in Mexico's flourishing luxury Spa industry.Viewers will see how how the Mayan people are reclaiming their heritage, and that of others, at a restored Hacienda owned by a community collective. Everything about the hotel reflects the region's rich tapestry of history, making staying there more akin to a living history lesson in luxury, than an eco-friendly place to rest the head.Learn more about the sustainable community initiatives at the Haciendas here.\nEarthCheck members featured on Discovery Channel\n6 min reading time\nPublished on 13/07/12 - Updated on 17/03/22\nEvery week, the HON team brings you an expert look at the world of hospitality. By becoming a member, you will have access to a complete ecosystem: exclusive content, jobs, etc.","Folks mentioned the pandemic made them wish to journey extra responsibly sooner or later.\nNow new information signifies they’re really doing it.\nBased on a report revealed in January by the World Journey & Tourism Council and Journey.com Group:\n- Practically 60% of vacationers have chosen extra sustainable journey choices within the final couple of years.\n- Practically 70% are actively searching for sustainable journey choices.\nHowever discovering corporations which might be severe about sustainability is not straightforward, mentioned James Thornton, CEO of tour firm Intrepid Journey.\n“You see accommodations saying they’re sustainable, and then you definitely’re utilizing these little journey bottles for shampoos and bathe gels,” he mentioned.\nIt is all simply “greenwashing,” he mentioned, referencing the time period that describes corporations’ efforts to look extra environmentally sound than they’re.\nFor a corporation to say they’re “100% sustainable” or they’re “eco-conscious” … doesn’t suggest something.\nCEO, Intrepid Journey\nThe time period has risen in reputation alongside the rise in demand for sustainable services.\nThe result’s a mixture of those that are really devoted to the trigger — and people who sprinkle eco-buzzwords and images of seedlings, forests and different “inexperienced” imagery of their advertising and marketing supplies, with no actual motion to again up their claims.\nDiscovering corporations which might be sustainable\nBe cautious of those techniques, mentioned Thornton.\n“For a corporation to say they’re ‘100% sustainable’ or they’re ‘eco-conscious’ … doesn’t suggest something,” he mentioned. “I might urge vacationers to be very cautious after they’re seeing these phrases, and to essentially dig in and look in a bit extra element.”\nShopper curiosity in sustainable journey has modified significantly previously twenty years, mentioned Thornton. He mentioned when he joined Intrepid journey 18 years in the past, “folks would take a look at us like we’re a bit loopy” when the corporate talked about sustainability.\nNow, many corporations are doing it, whether or not they’re severe, or not.\nThornton mentioned he believes the journey business is at present divided into three classes. One third have “extremely good intentions, and [are] working very actively on addressing the local weather disaster … and so they’re making good progress.”\nOne other third have “good intentions however [aren’t] really taking motion but. And infrequently … they are not fairly positive easy methods to take motion.”\nThe ultimate third “is simply completely burying its head within the sand and hoping that this factor goes to go away, and the reality of the matter is — it is not.”\nTo determine corporations within the first class, Thornton recommends vacationers search for three crucial issues.\n1. A historical past of sustainability\nTo determine whether or not an organization could also be leaping on the eco-bandwagon, look at its historical past, mentioned Thornton.\nHe advises in search of “a protracted historical past of affiliation with problems with sustainability, or is that this one thing that solely simply appeared?”\nIntrepid Journey CEO James Thornton.\nSupply: Intrepid Journey\nIf the messaging is new for the corporate, that is not a deal breaker, he mentioned.\n“However that will then encourage the client to most likely wish to look in a bit extra element to see if what an organization really does has rigor behind it,” he mentioned, “Or whether or not it is one thing that is simply being achieved for advertising and marketing sake — and due to this fact greenwashing.”\n2. Verify for measurements\nSubsequent, vacationers ought to see if the corporate measures its greenhouse fuel emissions, mentioned Thornton.\n“The sincere fact is that each journey firm is finally contributing in direction of the local weather disaster,” he mentioned. “So the most effective factor any journey firm can begin to do is measure the greenhouse fuel emissions it creates.”\nTo do that, Thornton suggested vacationers to examine the Glasgow Declaration on Local weather Motion in Tourism.\n“The Glasgow Declaration web site lists the organizations which have agreed to actively scale back their emissions … and really have a local weather plan that reveals how they’re doing that,” he mentioned.\nSignatories should publish their local weather plan, which is monitored by the United Nations World Tourism Group, he mentioned.\n“Customers can use this as a approach to examine if the corporate they’re reserving with is severe about decarbonization,” he mentioned, including that greater than 700 organizations are on the checklist.\nThornton mentioned vacationers may also examine the Science Primarily based Targets Initiative, which is a partnership between CDP, the United Nations International Compact, World Assets Institute and the World Large Fund for Nature.\nIts web site has a dashboard that particulars emission-reducing commitments made by greater than 4,500 corporations worldwide, together with American Specific International Enterprise Journey, the UK’s Reed & Mackay Journey and Australia’s Flight Centre Journey Group.\n3. Search for certifications\nLastly, vacationers can examine for impartial assessments, mentioned Thornton.\nOne of the rigorous and spectacular is the B Corp Certification, he mentioned.\n“It took Intrepid three years to turn into a B Corp,” he mentioned.\nDifferent corporations with B Corp standing embrace Seventh Technology, Ben & Jerry’s, Aesop — and Patagonia, which Thornton known as “arguably probably the most well-known B Corp on this planet.”\nTo get it, corporations are reviewed by the non-profit B Lab and a certification lasts for 3 years, mentioned Thornton.\nKristen Graff, director of gross sales and advertising and marketing at Indonesia’s Bawah Reserve resort, agreed that B Corp is the “most generally revered” certification.\nGraff additionally recommends the International Sustainable Tourism Council, saying that it and B Corp are “really … legit.” The GSTC doesn’t certify journey corporations, however slightly accredits third get together certification our bodies that use its requirements.\nBawah Reserve, a resort in Indonesia’s Anambas Islands, is making use of for B Corp certification. The resort makes use of solar energy and desalinates ingesting water on the island.\nSupply: Bawah Reserve\nDifferent journey eco-certifications are much less exacting, mentioned Graff.\n“A lot of them are only a racket to generate profits,” she mentioned.\nBawah Reserve began the method to turn into B Corp licensed in November of 2022, mentioned Graff. “We anticipate it can take a couple of yr to finish,” she mentioned.\nB Corp makes use of a sliding scale for its certifications charges, which begin at $1,000 for corporations with lower than $1 million in annual income.\n“The fee is pretty minimal,” mentioned Thornton, particularly “for those who’re severe about sustainability.”\nHe mentioned Intrepid pays about $25,000 a yr for the certification.\nThornton additionally suggested vacationers to ask questions like:\n- Are you utilizing renewable vitality sources?\n- Is the meals domestically sourced?\n- Are staff from native communities?\n- Who owns the lodge?\nHe mentioned there are locations which might be perceived to be sustainable however which might be “really owned by a on line casino.”\nLastly, Thornton recommends vacationers look to on-line opinions.\n“Usually slightly little bit of analysis on Google … can provide you a very good indication round whether or not a lodge or a journey expertise is doing what it says it is doing — or whether or not they’re really greenwashing.”\nClarification: This text has been clarified to replicate that the International Sustainable Tourism Council accredits third get together certification our bodies that use its requirements."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:90c7d4de-589c-4cc4-9ea1-7b3afe805802>","<urn:uuid:79dcda8d-542c-4229-9f7e-64d2eb6667c9>"],"error":null}
{"question":"What is the fundamental difference between how academic writing is addressed in the Higher Education Systems book's country case studies versus the Academic Encounters textbook's methodology?","answer":"In the Higher Education Systems book, academic writing appears as part of broader analytical case studies examining national education systems and their societal impacts, while Academic Encounters takes a direct instructional approach to academic writing through specific guided assignments, study skills development, and task-based activities focused on building practical writing capabilities for academic success in English-speaking environments.","context":["Dominik Antonowicz and Marek Kwiek authored and co-authored three chapters in a book from Oxford University Press and edited by Brendan Cantwell, Simon Marginson, and Anna Smolentseva: High Participation Systems of Higher Education. Oxford (2018). Link here.\nThe three chapters result from a research project coordinated by Simon Marginson (University College London Institute of Education), started in 2014, with colleagues from seven institutions.\nThe three chapters are:\nMarek Kwiek, “Building a new society and economy: high participation higher education in Poland” (334-357). Here\nBrendan Cantwell, Romulo Pinheiro, and Marek Kwiek, “Governance” (68-93).\nDominik Antonowicz, Brendan Cantwell, Isak Froumin, Glen A. Jones, Simon Marginson and Rómulo Pinheiro, “Horizontal diversity” (94-124).\nTable of Contents:\n1: High participation systems (HPS) of higher education, Simon Marginson\n2: Comparative data on high participation systems, Brendan Cantwell, Patrick Clancy, and Simon Marginson\n3: Governance, Brendan Cantwell, Rómulo Pinheiro, and Marek Kwiek\n4: Horizontal diversity, Dominik Antonowicz, Brendan Cantwell, Isak Froumin, Glen A. Jones, Simon Marginson, and Rómulo Pinheiro\n5: Vertical stratification, Brendan Cantwell and Simon Marginson\n6: Equity, Simon Marginson\n7: High participation society, Anna Smolentseva\n8: Decentralization, provincial systems, and the challenge of equity: High participation higher education in Canada, Glen A. Jones\n9: Broad access and steep stratification in the first mass system: High participation higher education in the United States of America, Brendan Cantwell\n10: Regulated isomorphic competition and the middle layer of institutions: High participation higher education in Australia, Simon Marginson\n11: Stratification by the state and the market: High participation higher education in Russia, Anna Smolentseva, Isak Froumin, David L. Konstantinovskiy, and Mikhail Lisyutkin\n12: Building a new society and economy: High participation higher education in Poland, Marek Kwiek\n13: Reproducing social equality across the generations: The Nordic model of high participation higher education in Finland, Jussi Välimaa and Reetta Muhonen\n14: Balancing efficiency and equity in a welfare state setting: High participation higher education in Norway, Rómulo Pinheiro and Bjørn Stensaker\n15: Towards universal access amid demographic decline: High participation higher education in Japan, Akiyoshi Yonezawa and Futao Huang\n16: Conclusions: High participation higher education in the post-Trow era, Brendan Cantwell, Simon Marginson, and Anna Smolentseva\nHigher Education has become a central institution of society, building individual knowledge, skills, agency, and relational social networks at unprecedented depth and scale. Within a generation there has been an extraordinary global expansion of Higher Education, in every region in all but the poorest countries, outstripping economic growth and deriving primarily from familial aspirations for betterment. By focusing on the systems and countries that have already achieved near universal participation, High Participation Systems of Higher Education explores this remarkable transformation.\nThe world enrolment ratio, now rising by 10 per cent every decade, is approaching 40 per cent, mostly in degree-granting institutions, including three quarters of young people in North America and Europe. Higher Education systems in the one in three countries that enrol more than 50 per cent are here classified as ‘high participation systems’.\nPart I of the book measures, maps, and explains the growth of participation, and the implications for society and Higher Education itself. Drawing on a wide range of literature and data, the chapters theorize the changes in governance, institutional diversity, and stratification in Higher Education systems, and the subsequent effects in educational and social equity. The theoretical propositions regarding high-participation Higher Education developed in these chapters are then tested in the country case studies in Part II, presenting a comprehensive enquiry into the nature of the emerging ‘high participation society’.","Academic Encounters: The Natural World, develops students' listening, note-taking, and discussion skills using authentic interviews and lectures and a variety of pre- and post-listening activities. The Natural World engages students with academic readings, photographs, illustrations, and graphics on stimulating topics within the fields of Earth Science and Biology. Tasks to build academic language and writing skills occur throughout the book. The companion book, Academic Listening Encounters: Each chapter ends with a guided writing assignment. Students also learn study skills such as highlighting, note taking, and preparing for a quiz. The Natural World uses a sustained content approach to help students develop the reading, writing, and study skills they need to meet the demands of high school or college academic courses in an English-speaking environment. Academic Encounters: Tasks that accompany readings develop important reading skills such as reading for detail, skimming, reading critically, and applying what you have read.\nStudent`s Book Academic Encounters. The Natural World.\n|Формат:|| Страниц 240|\nCambridge University Press\nЖестокое обращение с детьми\nPsychological aspec s of socie y. L.: Academic Press, 1978. Book 4. he s ruc ured crowd Данные приведены по России, однако многие зарубежных авторы отмечают подобные тенденции и развитых капиталистических странах ссылка приведена по более поздней монографии см. 5, с.149 ссылается на работы Л.Берковица, см.: сборник монографий: Берковиц Л. Агрессия: причины, следствия контроль – СПб.: 2001\n|Academic Encounters: Life in Society Student's Book Academic Encounters Cambridge University Press Hood |\n|Academic Encounters: The Natural World 2-Book Set (+ Audio CD) Academic Encounters Cambridge University Press Jennifer W. |\nThe companion book, Academic Listening Encounters:\n|Life in Society. Academic Encounters 3. Listening and Speaking: Student' s Book (+ DVD) Academic Encounters Cambridge University Press Sanabria K. |\nThere are two books for each content area.\n|Teacher' s Manual Listening and Speaking Academic Encounters 4: Academic Encounters Cambridge University Press Espeseth M. |\nThe Academic Encounters Second edition series uses a sustained content approach to teach skills necessary for taking academic courses in English.\n|Academic Encounters 2. Student's Book. Reading and Writing Academic Encounters Cambridge University Press Williams J. |\nAcademic Encounters Level 2 Student' s Book Reading and Writing: American Studies engages students through academic readings, photos, and charts on stimulating topics from U. S. History and Culture. Topics include the foundations of government, equal rights, and the American Dream. Students develop important skills such as skimming, reading for the main idea, reading for speed, understanding vocabulary in context, summarizing, and note-taking. By completing writing assignments, students build academic writing skills and incorporate what they have learned. The topics correspond with those in Academic Encounters Level 2 Listening and Speaking: American Studies. The books may be used independently or together. The Academic Encounters Second edition series uses a sustained content approach to teach skills necessary for taking academic courses in English. There are two books for each content area.\n|Audio CD. Academic Encounters 1. Listening and Speaking: The Natural World: (количество CD дисков: 3) |\nThe Natural World contains the listening materials for Academic Encounters Level 1 Student' s Book Listening and Speaking: The Natural World. The materials include warm-up listening activities, informal interviews, and formal academic lectures. Videos of the lectures are also available on DVD, which comes with the Student' s Book. Academic Encounters Level 1 Class Audio CDs (3) Listening and Speaking:\n|Human Behaviour Student' s Book (+ Audio CD) Academic Listening Encounters: Academic listening Encounters Cambridge University Press Espeseth |\n|Academic Listening Encounters: The Natural World, Low Intermediate Student's Book with Audio CD: Listening, Note Taking, and Discussion (+ Audio CD) Academic listening Encounters Cambridge University Press Yoneko K. |\nAcademic Listening Encounters:\n|Listening, Note Taking, and Discussion (количество CD дисков: 3) Audio CD. Academic Listening Encounters: American Studies: |\nThe Class Audio CDs contain the listening material for the Student' s Book. The material includes warm-up listening exercises, informal interviews, and authentic academic lectures. The Academic Encounters series uses a sustained content approach to teach skills necessary for taking academic courses in English. There are two books for each content area.\n|Study Skills in English. A course in reading skills for academic purposes Study Cambridge University Press Michael J. W. |\nTo facilitate the use of the text for self-study purposes materials from the Tutor' s Book (previously published separately) have been incorporated to form one combined student-friendly text. Study Skills in English has a full answer key and can be used for self-study. It is accompanied by an audio CD or audio cassette. Study Skills in English is a complete course for students who are currently attending a university or college or who hope to begin university or college studies soon. The emphasis throughout the course is on student activity and realistic practical work. The course covers: • reading academic texts efficiently and effectively\n• taking notes from lectures and books\n• doing basic research\n• using library or computer-based resources\n• writing academic papers\n• taking part in discussions\n• presenting papers\n• managing study time\n• preparing for examinations\nThe Second Edition has been comprehensively revised and updated, and features developments in the use of computers for academic study and the internet as a research tool.\n|Academic Writing Student's Book Cambridge University Press Leki |\nIt teaches attention to form, format and accuracy.\n|Academic Writing Course Academic Writing Course Pearson R. J. |\nSpecially designed for students starting a tertiary level course in higher education, this book addresses all aspects of academic writing, from the details of accurate grammatical construction, pronunciation and spelling to appropriate style choices.\n|English for Academic Study Series Listening Teacher's Book Pearson |\n|IELTS Teacher' s Guide. Academic Module High Impact: Pearson Education (Longman) Bourne P. |\nFrom an overall guide to the IELTS exam right up to suggestions for preparation the night before, High Impact IELTS covers all the details essential for a good result. - Every unit has a specific IELTS theme (e. g. the environment, social issues). - Each lesson in the unit has a specific aim (e. g. paragraphing, multiple choice). - Important or useful hints for the course are highlighted in a Point of Impact.\n149 руб -30% 104 руб\n|Write Ahead 1: Skills for Academic Success Pearson Education (Longman) Linda R.F. |\n|Check Your Vocab for Academic English Macmillan Publishers David P. |\n|An Academic Sketch (1892) Книга по Требованию William E.G. |\nЭта книга будет изготовлена в соответствии с Вашим заказом по технологии Print-on-Demand.\n|Focus on. Academic Skills for IELTS (+ Audio CD) Pearson Education (Longman) Terry M. |\n|Academic Connections 2 with MyAcademicConnectionsLab Academic Connections Pearson Education (Longman) David H. |\n|Capitalizing on the Extra Curriculum. Participation, Peer Influence, and Academic Achievement Книга по Требованию Alexander B. |\nThe physical plant of each school houses a living, breathing social system with all of the requisite parts and functions."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:514bbf2b-601b-4b13-9f86-648b07fd4e0a>","<urn:uuid:d29e5c83-a9cc-43a4-a1a1-762aba31f168>"],"error":null}
{"question":"What are the fundamental cooking techniques for preserving nutrients in vegetables, and what safety practices should be implemented when using kitchen knives?","answer":"For preserving nutrients in vegetables, proper steaming is essential - using a stainless steel or bamboo steamer, cutting vegetables into equal sizes, and cooking over medium heat until just fork tender and bright in color. It's better to slightly under-steam as residual heat will continue cooking. For kitchen knife safety, key practices include keeping fingertips curled under while cutting, maintaining sharp blades to prevent dangerous dragging, proper storage in knife racks, and immediate cleaning and storage after use. Knives should never be left in sinks, and when moving with knives, the point should always face the floor. Constant supervision and proper technique are crucial for safety.","context":["You can’t explore culinary nutrition without getting your hands a little dirty in the kitchen. With the explosion of cooking shows and celebrity chefs, home cooking has become intimidating for some of us. However, you don’t need to agonize over time-consuming cooking techniques and recipes – all you need are some culinary nutrition basics to get started, which allow you to eat well and support your health at the same time. These healthy cooking techniques and recipes are part of the foundation of the Culinary Nutrition Expert Program and they are very simple to master!\nESSENTIAL Cooking Techniques and Basic Cooking SKILLS\n- Basic Knife Skills\n- Meal Prep\n- Batch Cooking\n- Cast Iron Cooking\n- Slow Cooking\n- Steaming Vegetables\n- Using Veggies Root to Stem\nBasic Knife Skills\nYou don’t need to be lightning fast, but if you want to cook you should get comfortable with using a knife, one of the most handy and inexpensive cooking tools. Take your time practicing small, medium-sized and large cuts, which don’t need to be perfect. The more you practice, the easier mincing, dicing and chopping will be.\nMeal prep is one of the first skills we teach our students in the Culinary Nutrition Expert Program. Preparing ingredients, snacks, condiments and entire meals ahead of time makes it so much easier to stick to your healthy eating goals. With a few hours of prep, you don’t need to worry about what you’ll be eating all week long, saving you time as well as money in the process.\nMeal prepping and cooking is an investment in time, money and effort. Why not cook once, and then enjoy the spoils multiple times? Batch cooking, whether in large amounts to eat throughout the week or to freeze for later (or a mix of both of these things) is a lifesaver for busy days and tiresome days, when unexpected guests arrive, the holiday season, or when recovering from surgery, illness or childbirth.\nThis basic cooking technique involves frying ingredients over medium to high heat (preferably in a healthy cooking oil) to cook them quickly and achieve browning and flavour. Sautéing is the foundation for many meals, including soups, stews, one-pot meals and Instant Pot recipes.\nCast Iron Cooking\nCast iron is one of our favourite pans for cooking because it’s durable, retains heat well, is free of non-stick coatings, and is affordable. There is, however, an adjustment period to cooking with cast iron if you’re not used to it.\nGet Started Cooking With Cast Iron\nWhat could be easier than letting an appliance do all the work? Stick ingredients into a slow cooker and come home later to a warm, hearty meal. Slow cooking is almost foolproof and is totally delicious, especially on a blustery fall or winter day.\nSlow Cooker Inspiration\nFermentation is a traditional preserving technique that is great for digestive health, immunity and nutrient absorption. We teach a few fermentation basics in the Culinary Nutrition Expert Program, and for a more in-depth practice, check out our self-paced Fundamentals of Fermentation Course.\nSome of our favourite fermented foods that you can easily make at home at a fraction of the cost of store-bought options:\nRich in vitamins, minerals, enzymes and antioxidants, sprouts are an inexpensive food you can grow indoors. And they are so easy!\nOversteaming or overboiling vegetables saps them of nutrients, colour and flavour. Learn to create the perfect tender-crisp steamed veggies by:\n- using a stainless steel or bamboo steamer\n- cutting vegetables into equal sizes so they’ll cook evenly\n- steaming over medium heat until the veggies are just fork tender and bright in colour\nIt’s better to go by texture rather than time, as thicker vegetables like carrots or sweet potatoes will take much longer to steam than chard. Better to slightly under-steam them, as the residual heat of the steamer will help to finish things off if needed.\nUsing Veggies Root to Stem\nWe have a habit in North America of throwing out parts of vegetables that we could use for another purpose. Explore the root to stem approach to using vegetables, which includes leaving the skins on for certain veggies, saving scraps for broth, using the stems of dark leafy greens, like kale, for juicing or stir-fries, using a whole pumpkin, or freezing herbs in olive oil. Check out more zero waste cooking tips that will help you explore root to stem cooking here.\nInterested in learning more about essential cooking techniques and kitchen skills, and how you can take your passion for health to the next level? Check out our 14-week certification program that is 100% online. Click here to learn more.\nbaSic recipes to master\nVeggie Stock or Bone Broth\nVegetable stocks and bone broths are reliable, full of flavour, and can be used in many different recipes. We like making huge batches of stock and then freezing it so we always have some on hand. Once you master a basic broth, power up the culinary nutrition power of your broth with culinary adaptogens, herbs, spices and immune-supportive foods.\n- Recipe: How to Make Stocks and Broths\nHomemade Nut or Seed Milk\nThis staple liquid is perfect for smoothies, gluten-free baking, dairy-free elixirs, homemade chocolate desserts, soups and more. Homemade nut milk skips the refined sugars, stabilizers and preservatives, and is so customizable (chocolate milk anyone?).\nCooking Gluten-Free Grains\nThere’s more to gluten-free grains than just rice (though it’s good to know how to cook that well, too!). Explore using quinoa, buckwheat, millet, oats, sorghum, wild rice, amaranth and teff to add depth, flavour, nutrition and texture to your dishes.\nWhile most grains use a 2:1 ratio (2 cups water, 1 cup rice), some grains such as sorghum and wild rice usually require a 3:1 ratio. You’ll also need to use more water if you want a porridge-y, soupy texture.\nBlending a Smoothie\nTake your health to the next level with a dairy-free smoothie. You can pack way more nutrition into a blended beverage than you could likely eat in one serving if you ate everything whole. Check out our Best Smoothie Formula for smoothie building and blending tips, and try one of these 20 Best Dairy-Free Smoothie Recipes. Once you’re a skilled blenderist, try moving on to smoothie bowls for extra creativity and pizzazz.\nMastering Salad Dressing\nA good salad dressing takes your salad – or roasted veggies – from meh to wowza. Oils, acidity and herbs are the basis of a solid dressing, but there are also many flourishes you can use to add creaminess, texture or sweetness.\n- Learn: How to Make Salad Dressing\nCrafting Dairy-Free Elixirs\nSmoothies and juices aren’t the only healthful beverages – hot drinks like dairy-free elixirs can serve as a light meal or snack, while also supporting our health in a targeted way by incorporating healing herbs.\n- Learn: DIY Guide to Dairy-Free Elixirs\n- Recipe: 20 Best Dairy-Free Elixir Recipes\n- Recipe: Dairy-Free Gingerbread Latte\n- Recipe: Our Favourite Hot Chocolate Combinations\nBaking a Good Loaf of Bread\nA solid loaf of gluten-free bread is fantastic for avocado toast, plunging into dips and spreads, sandwiches, or tearing into salad croutons. Baking bread with gluten-free or grain-free flours can be tricky at first. We love this power-packed almond bread recipe (it’s a student fave too!). If baking bread seems intimidating, try an easy flatbread recipe instead.\nOnce you get the hang of some of these healthy cooking techniques, basic cooking skills and recipes, you’ll be cooking from scratch on the regular with confidence and pleasure.\nHeader Image: iStock/Magone","parents become extremely fearful imagining their child using a 10-inch\nchef's knife! Children under 8 years' motor skills are not reliable enough\nto safely use such tools. But little ones can make quick work of soft\nvegetables and even a whole head of broccoli using a mere table knife.\nWhat's important to understand is that kids love to chop; it's repetitive\nand a pile of chopped food gives them a great sense of accomplishment.\nChildren 8 years and older have more developed motor skills and can begin\nto learn how to properly use a chef's knife. Professional knife techniques\ntake much of the fatigue out of these assignments and make quick work\nof them as well. They also significantly increase the safety factor. Following\nare my \"Family Knife Safety Commandments\" and some basic knife techniques\nthat are embellished to include parental participation as well as supervision.\nAdditionally, I've provided some recipes from my book Cooking\nTime Is Family Time.\nI chose recipes that require a lot of chopping and knife wielding to offer\nplenty of opportunities to practice.\nFamily Knife Safety Commandments\n1) The first and most important thing to remember in knife safety is not\nwhat you do with the knife, but what you do with your other hand. That's\nthe one in danger of being cut. Always cut, chop, slice, dice with the\nfingertips of your other hand curled down and around so only the flat\npart of your knuckle is facing the blade. This then becomes a guide for\nthe top of the blade to work against. In this way, fingertips are safely\naway from the blade and in no danger of being cut.\n2) The second most important knife safety tip to remember is to keep your\nknives sharp. When a knife is kept sharp evenly along the length of the\nblade, it will cut quickly, efficiently and cleanly. If it is not sharp\nenough, the blade will drag and that is how accidents can happen; the\nknife can drag and slip and consequently cut fingers that have already\ngone on to the next step because the knife was not supposed to be lagging\n3) The third knife safety commandment involves the movement and storage\nof knives in the kitchen. Sharp knives should be stored in a stand-up\ntype knife rack with the blades concealed. This is by far the safest type\nof storage. When moving around the kitchen holding a knife or when handing\nit to someone, the knifepoint should always be pointed towards the floor,\nwith the hand gripping the handle. This way, the blade and point are never\nwaved around in danger of injuring someone. Knives should never be left\nin the sink. Their wooden handles will get wet and ruined and some small\nhand may reach in, not see the knife and get hurt. Knives should be used,\nand immediately washed, dried and then put away.\n4) Always stay in the room and close by your child while they are working\nwith a knife. Even once you've all become accustomed to working together\nand your older child has become quite competent with a chef's knife, you'll\nwant to keep your eye on their technique. Kids and adults alike tend to\nforget and get lazy. That's when they forget to curl fingers or start\nexerting more force out of rushing and over confidence and this can lead\nto accidents. Keep an eye on the kids with knives, gently and supportively\nadmonishing their lack of 'form' and remind them that proper technique\nis the safest way and you love them, so you want to make sure they don't\nThe slicing technique involves a gentle rocking motion. Practice with\na potato or apple, peeling it first and then cut it in half lengthwise.\nRight-handed family members should hold the knife in their right hand.\nWith your left, hold down the item to be sliced as your curl your fingertips\nunder. Make sure kids really curl their fingertips under; this is very\nimportant for safety. Next, hold the knife at a 45-degree angle and come\ndown swiftly through the potato on its extreme right end. Before coming\nup again, be sure you continue to have a proper grip and slide the knife\ndown to the end of the blade near the handle, then gently rock back along\nthe blade to the tip of the knife. At this point, slowly adjust the knife\nto come down again -- this is all one smooth movement -- slicing through\nthe potato once more at the desired width. (If you or the child is left-handed,\nreverse the above instructions.)\nAll along, fingertips should continue to be tucked under and are firmly\nholding the potato in place. As you make progress slicing, you must be\nsure to continue to move you left fingertips away from the blade and at\nthe same time, inch the food towards the knife. The knife motion should\nfeel like one continuous movement and the blade never lifts off the cutting\nboard. Have the children practice the rocking motion with the knife on\nthe cutting board not cutting anything. If they are doing it correctly,\nit should feel easy and relaxed. The knife should not be held too hard\nor strenuously; the controlled rocking motion of the sharp blade should\nbe doing the real work -- not your muscles!\nWhen you have your apple or potato sliced, you can now make uniform dices\nvery easily. Stack 2 slices of potato horizontally in front of you, now\nslice through them to make 1/4\" slices, keeping the pieces snugly together\nwith your left hand as well as you can. When it is completely sliced,\nuse your left hand to pivot the slices so they are now vertical, slice\nagain in 1/4\" slices creating a checkerboard dice of 1/4\" size. Continue\nin this fashion until all the potato is diced. Place your diced potato\nin a bowl and move on to your next\nCheck out other\nrecipes that bring the whole family together:\nCooking Time is Family Time: Cooking Together,\nEating Together, and Spending Time Together\nby Lynn Fredericks, William Morrow & Co., August, 1999\nCOOKING TIME IS FAMILY TIME, Lynn Fredericks shows people how they\ncan improve the time they spend with kids by inviting them into\nthe kitchen to help prepare meals.\nIncluded are 125 recipes emphasizing a variety of fresh, healthful\ningredients and strategies to get kids to gobble them down. Each\nrecipe offers directions that specify which steps are right for\nyounger kids and which are more challenging for their older siblings."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:95cdba16-970d-425c-b00b-3e6a3f4db657>","<urn:uuid:c94e3949-2092-48b4-8118-767dd45834df>"],"error":null}
{"question":"As a business consultant, I've observed that both macro and micro environments affect companies differently. How do these two types of environments compare in terms of controllability for businesses?","answer":"The key difference in controllability is that micro environment factors are generally manageable by businesses while macro environment factors are not controllable. The micro environment includes elements like customers, suppliers, labor, and competitors that companies can influence through their actions and strategies. In contrast, macro environment factors like economic conditions, political situations, socio-cultural changes, and technological developments are external forces that businesses must adapt to but cannot control. For instance, while a company can adjust its supplier relationships or customer service (micro factors), it cannot control government policies or global economic trends (macro factors).","context":["Components of International Business Environment\nThe business environment can be divided into two ways.\n(A) Internal Environment\n(B) External Environment\n(I) Micro Environment\n(II) Macro Environment\nInternal Environment: It includes all those factors which are with in the business itself and influence business. These are usually under the control of business. For example, objectives, policies, organisation structure, management, production method, etc.\nExternal Environment: It includes all those factors which are outside the business and business has no control over these factors. It is divided into two parts:\n(I) The Micro Environment of Business\nThe micro environment consists of the forces in the company’s immediate environment that affects the performance of the company. The micro factors may affect different firms in a particular industry in different ways.\nIt consists of following elements:\n(i) Customers: Customers have direct impact on the micro environment of business. The desires, preferences, attitudes and expectations of customers keep on changing and it impose a constant challenge to business.\n(ii) Suppliers: Suppliers are the most important force in the task environment of a business. For the smooth functioning of the business it is very important to have a reliable source of supply. Multiple source of supply helps to reduce the risk of unavailability or uncertainty of supply of raw material.\n(iii) Labour: In big organisations, where hundreds of workers are employed, the labour force is organized in the form of trade unions. The trade unions pressurise the management for the fulfillment of their demands like higher wages, better working conditions and bonus etc.\n(iv) Competitors: Competitors play a vital role in running the business enterprise. Business has to adjust its business activities according to the behaviour of the competitors. It is very necessary to know about the competitor’s strategies, policies and product features for other companies to secure its market share.\n(v) Regulating Agencies: The regulators include government departments and other organisations which monitor the activities of business. There are certain departments like income tax department, quality control department and other revenue departments and professional bodies like ICAI which prescribes certain standards and practices for the business in their respective areas.\n(II) The Macro Environment of Business\nThe macro environment of business includes activities which are uncontrollable and need proper attention on the part of a business enterprise. It refers to the general and overall environment within which an environment entity operates.\nIt consists of following elements:\n(i) Economic Environment: It refers to all those economic factors which have a bearing on the functioning of a business unit. The major macro-economic factors which have considerable influence on business are:\n· Economic Systems\n· Economic Planning\n· Economic Policies\n(b) Industrial Regulations\n(c) Business Laws\n(d) Import and Export Regulations, etc.\n· Economic Growth\n· Interest Rates\n· Economic Reforms\n· Currency Exchange Rates\n(ii) Political and Government Environment: Political environment constitutes all the factors related to government affairs such as the type of government in power, the ideology of ruling party, attitude of government towards different groups of societies. The businessman has to make changes in his organisation according to the changing factor of political environment. For example, in 1977 when Janata Party came in power they made the policy of sending back all the foreign companies. As a result, the Coca Cola and IBM companies had to close their businesses and leave the country.\n(iii) Socio-Cultural Environment: Socio-Cultural environment include all the social factors like people’s attitude, education system, their beliefs and values, culture, religion, ethical issues and social responsibility of business etc. All these factors have a great impact on activities of business enterprise. For instance, the chocolate boy ad of AXE Effect was banned by Information and Broadcasting Ministry on grounds of being offensive and vulgar.\n(iv) Technological Environment: It consists of new products, new techniques and new approaches to production, new methods and new equipments. In order to survive in today’s competitive world, a business has to adopt technological changes from time to time. If they will not do so, they will be out of market. For instance, in late 1990’s Pagers were very popular among the people, but then came the mobile phone revolution. The companies those were manufacturing pagers at that time, they have to shift towards mobile phones.\n(v) Demographic Environment: It includes:\n(a) Size, growth rate, age composition, sex etc. of population\n(b) Family size\n(c) Educational level\n(d) Economic stratification of population, etc.\nAll these demographic factors are relevant to business. These factors affect the demand for goods and services. For example, increase in the demand of baby products shows the increase in the birth rate in certain area.\n(vi) International Environment: Due to liberalisation, globalisation, now the Indian companies are competing with the foreign companies. It has been observed that major international developments have their impact on domestic market. Recent example is increase in fuel prices in Indian market because of rise in prices of crude oil at international level. In the same way, because of US recession, Indian stock market also faced downfall.","External business environment comprises of both micro and macro environments. Microenvironment refers to factors that are closely related to the business, and influence the business operations. Macro-environment, in its turn, consists of uncontrollable forces that affect business in general. The fact that the macro environment is external and rarely controlled makes it the basis for an external environment. The following paper examines both micro and macro environments of Merlin entertainment company.\n1. MACRO ENVIRONMENT\nThe adaptability of a business depends on the growth and survival against external forces. These factors may either present opportunities or threats to business. Hence, forces of macro environment are the following: economic, political, social-cultural, legal and technological environment (Jain, Trehan & Trehan, 2010, p.10).\nEconomic policies, conditions and systems form the basis of economic factors affecting business. Economic conditions affect the phases of an entity and the purchasing power of the customers. In the case of depression, the purchasing power is low, and it negatively affects the business. Government policies and legislation may either harm or assist the business depending on how they are imposed to multinationals such as Merlin Entertainment. A capitalist economy is a private sector oriented and thus attracts more investors unlike the socialist and mixed modes of economy (Jain, Trehan & Trehan, 2010, p.11).\nThe outcome of various ideologies determines the political temperatures and in return influence operation of a business. In the case the country’s politics and attitude towards Merlin Entertainment business are poor; the company may not thrive as expected. Therefore, ideologies that influence good partnership between the business world and the government results in favorable market share. On the other hand, negative attitude against Merlin Company from political affiliations suppress the business world and affect a country‘s relation to the world (Singa, 2010, p.43).\nSocial –Cultural Environment\nPeople’s preferences and tastes change in regard to the trend of the Merlin Entertainment Place are diverse. The general belief among customers is that the quality is related to advertisement. Hence, the more adverts a product receives, the better its quality and vice versa. This notion drilled through media leads to a shift in demand for commodities depending on their availability. In addition, the composition of the population determines the market size of certain goods offered by Merlin Entertainment as opposed to others (Ghuman, 2010, p.45)\nGet 19% OFF\nwith discount code:\nEvery citizen in a given country is governed by laws stipulated under the constitution. Multinational companies have to take the hosting country’s law into consideration to avoid hiccups. Prior to investing in the country, Merlin Entertainment ought to identify an issue of such as security, cost of doing business and legal framework of a given country. The strength of security and legal provisions influences the global market to invest more and thus creates competitiveness in terms of commodities.\nAdvanced technology has impacted the business culture and their mentality towards commodities. The rapid growth in levels of information has resulted in a change of consumption patterns and tastes. However, the incorporation of technology into Merlin Entertainment may not always result in positive feedback from consumers. Hence, there is a need for preparedness in case of any setbacks that may arise in the future. The company ought to have a framework to deal with impacts of technologies on commodities to foreign countries (Young &Pagoso, 2008, p. 56).\n2. MICRO ENVIRONMENT\nInternal environment commonly known as micro-environment forms the basis of Merlin Entertainment Company. The company has to put up marketing strategies to ensure high productivity. Since the company has the ability to control such activities through management of strengths and weaknesses. Merlin Entertainment micro-environment includes value chain, ccompetition, customer service, cost accounting, and technology as discussed further.\nThe subdivision of a Merlin Entertainment allows a mode of organization that helps monitor day-to-day activities. Value chain revolves around from the concept of accounting in analyzing the cost incurred throughout the entertainment. Activities on value addition include primary and support activities. Primary function involves sales, logistics and marketing among others. The transformation from inputs to output is a primary activity brought closer to customers. The support activities form the basis for the primary functions and are the backbone of Merlin Company.\nAnalyzing the cost in terms of infrastructure and procurement allows the management to come up with activities that add value to the final commodity. In return, it creates competition by similar institutions and improves on products. In the case of Merlin Entertainment, employees should lead the mission of the company towards achieving the set goals. The training of employees equips them with skills and knowledge to effectively deliver on the services. Technology dependency adds value to a commodity through the emerging designs.\n5% OFFfor 30 pages\n10% OFFfor 50 pages\n15% OFFfor 100 pages\nAutomation of tasks is a technology dependent function that ensures satisfaction of customers in offering better products (Mukerjee, 2007, p.70). Forecasting demand entails understanding the market and developing entertainment products that suit these needs. Adopting a unique strategy does not always work, but optimization of Merlin Entertainment is the best practice. Managing customer relation not only entails human resource, but also the technical approach through putting up infrastructure to facilitate service delivery.\nThe practice of having a central location for the customer service of Merlin Entertainment Company is vital, in order to receive a response on standardization of products (Mukerjee, 2007, p.69). Centralization of data enables the company to give feedback with a uniform appeal across borders. The incorporation of dealers into the improving the face of the company has resulted in quality feedback. In addition, managers need to monitor the turnover as well as stock to enable them gauge business performance.\n- FREE plagiarism report (on request)\n- FREE revision (within 2 days)\n- FREE title page\n- FREE bibliography\n- FREE outline (on request)\n- FREE e-mail delivery\n- FREE formatting\n- Quality research and writing\n- 24/7/365 Live support\n- MA, BA, and PhD degree writers\n- 100% Confidentiality\n- No hidden charges\n- Never resold works\n- 100% Authenticity\n- 12 pt. Times New Roman\n- Double-spaced/Single-spaced papers\n- 1 inch margins\n- Any citation style\n- Up-to-date sources only\n- Fully referenced papers"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:cef7f424-66da-49a2-97d2-527c495ff046>","<urn:uuid:bcead9f6-05a4-41f9-9b62-cd59251bb1f2>"],"error":null}
{"question":"What are the nutritional benefits of nuts, and what potential health risks should individuals be aware of regarding nut consumption?","answer":"Nuts are nutritional powerhouses offering multiple health benefits. They contain protein (with almonds and pistachios providing 6 grams per ounce), good fats (both monounsaturated and polyunsaturated) that help reduce bad cholesterol and triglyceride levels, and fiber (almonds having the highest at 3.5 grams per ounce). They're also rich in minerals like magnesium, potassium, calcium, and contain antioxidants, folate, and vitamin E. However, nuts can pose serious health risks for some individuals, as nut allergies are among the leading causes of fatal and near-fatal reactions to foods. An estimated 1.8 million Americans have tree nut allergies, with reactions ranging from mild symptoms like mouth tingling and facial swelling to severe anaphylaxis, which can be life-threatening and require immediate medical attention. Walnuts and cashews cause the most allergic reactions within the nut family, and approximately 90% of people diagnosed with tree nut allergies will have them for life.","context":["By: Rebecca Mason, RDN\nNuts are awesomely versatile. They’re an easy portable snack, delicious on salads and have endless uses in a variety of cuisines. They’re also nutritional superstars, packed with protein and healthy fats. In honor of National Nut Day, nurture your nutrition knowledge with these nine nutty facts!\n1. Ever wonder which nuts have the most protein per serving?\nWonder no more—almonds and pistachios are both the protein powerhouses of the nut world. Both contain 6 grams of protein per ounce. On the flip-side, the nut with the lowest protein content is the pine nut, which contains only 1 gram of protein per serving.\n2. Nuts contain good fat.\nNuts have both monounsaturated and polyunsaturated fat, which can help reduce bad cholesterol and triglyceride levels. This, in turn, reduces risk of developing heart disease and Type 2 diabetes.\n3. Not all nuts are created equal.\nWhile all nuts are known to contain fat, walnuts have the greatest amount of heart-healthy alpha-linolenic acid—a plant-based omega-3 fatty acid—per serving.\n4. Brazil nuts are the highest food source of selenium.\nJust two Brazil nuts provide the total recommended daily intake! Consuming adequate (not excessive) amounts of selenium may help protect against prostate cancer.\n5. Peanuts are technically a legume, not a nut.\nWhat’s the difference? Legumes (like peas, beans and peanuts) have multiple seeds in a pod, but nuts have a single seed within a hard shell.\n6. Fiber is something many of us need more of in our diet.\nIt’s recommended women consume 25 grams of fiber daily, and 35 grams for men. While all nuts contain fiber, almonds have the highest amount, offering up 3.5 grams per ounce.\n7. Cashews are high in carbs.\nCashews have the highest carbohydrate content of all the nuts. A one ounce serving contains 9 grams of carbohydrate.\n8. Nut butters can be a great way to consume a serving of nuts.\nBut you should always check the ingredient list—your best bet will be brands where nuts are the only ingredient. Be mindful of nut spreads, like chocolate-hazelnut, where the first two ingredients are sugar and palm oil. These make for a nice treat, but they won’t offer the same nutritional benefits as actual nuts.\n9. Pistachios get their green color from plant-pigments.\nThey’re called carotenoids act as antioxidants in the body. Interestingly, pistachios also contain the antioxidants resveratrol and anthocyanin, which are responsible for their red and purple hues.\nWhether you’re spreading them on toast, roasting them or just eating them straight out of the shell, nuts are a tasty and healthy addition to your diet. This National Nut Day, bust out your nutcracker and spread your newfound nut-knowledge. And now that you know these nutty nutrition facts, you can get cracking all year long.\nAbout the Author: Rebecca Mason, RDN, is a registered dietitian/nutritionist. She is passionate about helping families and individuals improve their health through nutrition education and nutritious food access. Rebecca is certified in adult weight management, and has a background in both clinical nutrition and wellness programming.","- Food and Cooking»\n- Food Safety»\n- Food Allergies\nNut Allergy - What is Nut Allergy and What Causes Nut Allergy\nNuts are classified as fruits, in their appearance they are hard shelled and contain seeds, even though the seeds are not exposed as the nuts are opened or cracked. Biologically they are described as indehiscent fruit of plants. Botany describes nuts as dry fruits which contain one and sometimes two, seeds. When a nut becomes mature, its outermost area is said to be stony or hard. These fruits are high in oil content which makes them a great source of energy, they are said to be a nutrition powerhouse.\n“They are low in sodium and rich in fiber; contain a significant amount of mono- and polyunsaturated fatty acids and provide the minerals magnesium, potassium and calcium; and contain antioxidants, foliate, vitamin E and plant sterols.” (Nuts: Anti-atherogenic Food? By Alexiadou Kleopatra)\nSome common types of nuts are:\n6. Pine nuts\n9. Macadamia nuts\n10. Brazil nuts\n12. Gingko nuts\n13. Hickory nuts\nWhat is a Nut allergy?\nNut allergy or tree nut allergy is a common type of food allergy in which there is an overreaction of the immune system due to the consumption of the nuts. The body’s immune system usually fights infection, but its overreaction to a substance called allergen, causes this problem. This form of allergy is most common and affects millions of people worldwide. The most allergic reaction is caused by Walnuts and cashews in the nut family. Statistics say that at least 90 percent of children (or people) diagnosed with tree nut allergies will have them for life, and this will require them to zero the consumption of those nuts in their lifetime.\nAccording to American Public Health Association (APHA), “An estimated 1.8 million Americans have an allergy to tree nuts. Allergic reactions to tree nuts are among the leading causes of fatal and near-fatal reactions to foods.”\nCauses of Nut allergy\nThe main cause of nut allergy is simply the exposure of the dry fruit to a person who has the allergy to begin with. Some people feel a reaction coming on when they have been exposed to just one nut, and sometimes people do not feel the reaction coming on until they have exposed to two or more nuts. Whereas some people have an reaction just by standing next to a tree nut which is being consumed by another person. As we will discuss below (see Nut allergy reaction), the severity of the reaction depends upon an individual.\nTechnically speaking, when we consume a tree nut (or nut), there are proteins that are present in nuts that trigger a release of histamine. Histamine is the chemical (neuron-transmitter) your body produces when you're having an allergic reaction. So, we can say Histamine is released to counteract the effect. Even though the protein that is being consumed is not necessarily bad for us, our body perceives it to be bad and so allergic reaction to nuts is what we are actually having as a reaction to the histamine that is being released.\nSymptoms of Nut allergy\nSymptoms of the nut allergy can be divided in to symptoms of mild reaction and symptoms of severe reaction.\nOf mild reaction:\n· Mouth and lips begin to tingle.\n· Swelling of the face\n· Red spots\n· Stomachache or diarrhea\n· Urticaria (nettle rash or hives).\n· A feeling of tightness around your throat. (like throat closing up)\n· Itchy, watery, red or swollen eyes.\nOf severe reaction\nMild reaction symptoms prevail but they are also accompanied by the following:\n· Difficulty breathing like an asthma-like attack would feel like, or swelling around your throat, trouble breathing\n· “A sense of impending doom.”\nMedically there is a dilation (opening up) of the blood vessels in the body, which can cause:\n· Redness of your skin\n· A fast heart rate\n· Low blood pressure.\nWhich is why it is important to immediately to go the emergency room as soon as these symptoms begin to show.\nNut allergy reaction\nA severe reaction to nuts is called anaphylaxis, and it can be life threatening. Though the reaction to the nut allergy varies from person to person, this reaction is very dangerous and if the person experiencing it does not get medical attention right away, they can become faint or unconscious or may get even worse, some people even die. It is also important to remember that nut allergic reaction may come back a second time after few hours, so doctors recommend for patients to remain in hospital after the first reaction.\nAnaphylaxis can start off as completely mild symptoms but quickly progress in to symptoms like; having trouble breathing or feeling lightheaded like you are going to pass out or swelling all over the body. There are also gastrointestinal symptoms (i.e., vomiting, diarrhea, or cramping)\nHow to prevent nut allergy?\nNut allergy can be prevented by taking an active role in determining the ingredients of the type of food one eats. For example, by checking the labels of different foods before buying or consuming is a good idea.\nEducating oneself about the different types of nuts and foods containing them is always handy; make sure one is aware of the hidden ingredients in different cuisines. Family and friends always play a big role in one’s life, make sure they are aware of the allergies and know what to do in case of a reaction, like carry injectable epinephrine at all times.\nIs coconut a nut?\nSince October 2006, the FDA identifies coconut as a tree nut (for labeling purposes), even though there are very little incidents of coconut allergic reactions every year in people with allergies to tree nuts. Coconut contains the seed of a drupaceous fruit and doctors do not restrict their intake for tree nuts allergies.\n“Coconut allergies are exceedingly rare, with fewer than 10 reported cases” (Food Allergy and Anaphylaxis Network)\nThe Food Allergy & Anaphylaxis Network (FAAN) does not recognize coconut as a nut. There is controversy to whether coconut is a fruit, seed or a nut. Botanists, doctors and people with tree nut allergies each have their own opinions whether it is safe to consume coconut for a person with tree nut allergies.\nAccording to (FAAN),”Coconut is difficult to classify, and even botanists frequently disagree on its classification. The most accepted theory is that coconuts are classified as dry drupes. While it is possible to be allergic to coconut, the cross reactivity for those with tree nut allergies is very rare.”\nSome insect bites can be mistaken as nut allergy. You should be well aware of your local habitat and carry out precautionary measures accordingly. Nut allergies can be life threatening so make sure that you are fully aware of people around you that are sensitive to any kind of nuts."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:a5a436ac-d243-44ec-bcb0-a8d090dab789>","<urn:uuid:4125e889-f9ce-4af0-9216-d248cc9bdc69>"],"error":null}
{"question":"What material properties are important for aircraft construction, and how is the missile heating challenge addressed in supersonic flight?","answer":"Several material properties are crucial for aircraft construction, including high strength-to-weight ratio, resistance to corrosion, and good joining properties for fabrication. Materials must also have low thermal conductivity to protect from excessive heating and good fatigue strength to resist cyclical stress. Regarding the missile heating challenge in supersonic flight, the wall temperature can exceed 200°C during low-altitude supersonic flight, making traditional aluminum alloys unfeasible. This requires special consideration for thermal protection systems and material selection, with both 'hot' and 'cold' structure approaches being considered to address the aerodynamic heating challenge.","context":["|M.Sc Student||Peker Avi|\n|Subject||Feasibility Study of a Medium/Long Range Ramjet Cruise|\n|Department||Department of Aerospace Engineering||Supervisors||Professor Benveniste Natan|\n|Professor Rimon Arieli|\n|Full Thesis text - in Hebrew|\nMost cruise missiles operate at cruising altitudes that are two orders of magnitude lower than those of long-range ballistic missiles. This feature is one of the main reasons for the high interest in developing new systems that travel at high speeds and low altitudes, thus increasing detection and interception times for most common interception systems, improving their survivability.\nThe ramjet engine is an air-breathing engine that operates without any moving parts at high Mach numbers. Furthermore, the ramjet engine is known for its high specific impulse, a performance parameter that represents the capable operational range.\nThis research conducts a feasibility study of a cruise missile powered by a ramjet engine. The mission profile chosen is a surface-to-surface mission with a 500 kg payload and a 1500 km operation range.\nThe feasibility question is answered in two parts that are inherently coupled. They combine the external ballistics and flight performance and the internal ballistics (propulsion unit).\nThe first part involves the missile structure and aerodynamic properties, aerodynamic heating and its effect on the design parameters, and a 3DOF trajectory simulation of the missile. The internal ballistics study includes the solid booster and the ramjet engine performance analysis, where thrust, specific impulse and flow mass rates are derived as a function of altitude, flight velocity and time.\nAerodynamic optimization was conducted to satisfy the design criterion that was developed for maximum range. The aerodynamic coefficients were calculated using a component build-up method, MissileDatcom07. Based on the aerodynamic characteristics, estimations of the flight performance were made, such as required hinge moments and missile maneuver accelerations capability.\nThe sizing process was based on empirical correlations derived for similar missiles, in addition to the constraints defined by the specific mission, such as total weight and missile length.\nAn energetic performance analysis was conducted for both the solid rocket booster and the ramjet engine in order to meet the mission requirements. Thermochemical calculations, determined the propellant composition for highest performance. A rod-and-tube propellant grain design was chosen and validated with consideration to erosive burning. The ramjet engine cycle was calculated and a suitable engine configuration was obtained.\nThe aerodynamic heating was taken into account by solving the heat transfer equation for areas on the missile far away from stagnation conditions using lumped analysis assumption and Reynolds analogy. The heat flux and wall temperature of the missile were calculated along the trajectory, in order to determine the thermal protection system or other materials selection. Since the wall temperature exceeds 200 0C, the use of traditional aluminum alloys is not feasible for supersonic flight at low altitudes. Different design approaches within the \"hot\" and \"cold\" structures were reviewed to address the aerodynamic heating challenge.\nFinally, a 3DOF trajectory simulation was carried out and presents the various performance parameters along the missile flight. Moreover, in the trajectory simulation, an applicable \"bending\" law of the trajectory that utilizes thrust vector control and control surfaces steering was tested and implemented successfully, ensuring that all pre-defined mission requirements were kept.","This set of Aerospace Materials and Processes Multiple Choice Questions & Answers (MCQs) focuses on “Materials Selection – Properties of Flight Vehicle Materials”.\n1. Materials with ______________ are typically used in aircraft construction.\na) no strain\nb) lower strength/weight ratio\nc) average strength/weight ratio\nd) higher strength/weight ratio\nExplanation: The strength/weight ratio is an indication of the material’s ability to sustain the load. Materials with high weight are not preferred. Material with high strength and low weight have good strength/weight ratio, hence they are used.\n2. Which of the following is not a property of aluminium that makes it ideal to use as an aircraft material?\na) Resistance to corrosion\nb) Light in weight\nc) High fuel consumption\nd) High strength alloy\nExplanation: High fuel consumption is not an advantage. Moreover, aluminium has a low fuel consumption. It is light in weight compared to other metals like steel. It is also a high strength alloy. It is resistant to corrosion.\n3. Using materials with good joining properties is an advantage.\nExplanation: Materials with a good joining property are an advantage. The joining property of a material is the ability to manufacture a structural joint by mechanical methods or welding, soldering, etc. It is useful in fabrication.\n4. Which of the following methods would be helpful if a material that is not resistant to corrosion is used?\na) Painting a protective layer\nb) Heating to 10°F\nc) Increase the quantity of material\nExplanation: Materials that are not resistant to corrosion can be painted with a protective layer of another metal. A few examples of metals are chromium, nickel or zinc. Increasing the quantity of material or heating will not help.\n5. Why is low thermal conductivity important for aircraft systems?\na) To increase the strength of the material\nb) To protect from excessive heating\nc) To keep aircraft warm for passengers\nd) For oxygen\nExplanation: Thermal conductivity of a material is the rate at which it conducts heat. Low thermal conductivity is important because it prevents the systems from excessive heating hence preventing failure. It does not affect the oxygen levels or increase the strength.\n6. What is fatigue strength?\na) Strength/weight ratio\nc) Resistance to cyclical stress\nd) Strain energy\nExplanation: The highest point up to which the material can resist cycles of stress. Aircraft are affected by several stresses and loads. It is important to consider their resistance to these loads and stresses to avoid failure.\n7. Landing gear wheels are made of rubber to absorb shock.\nExplanation: Landing gear wheels are made of thick rubber. This is to absorb the shock waves and vibration caused when landing an aircraft. It also prevents the loss of fluids. Rubber is a good nonmetallic component of aircraft.\n8. What does a good working property of material mean?\na) To have high strength\nb) To be able to bend or machine the material in the desired shape\nc) To have a corrosive nature\nd) To ability to join the material in the desired shape\nExplanation: The working property of a material is the ability to change its shape in the required form. The ability to join the material is called joining property. They are important in the fabrication of aircraft parts.\n9. Which of the following is a mechanical property of materials?\nExplanation: Elasticity is a mechanical property. It is the ability of a material to return to its original shape after the material undergoes deformation. Appearance and colour are physical properties of a material.\n10. An aircraft material should be resistant to ______________\nc) shock loads\nExplanation: An aircraft should be resistant to shock loads. Joining property and working property are important for the design and fabrication of components. Strength is an advantageous property for a material.\nSanfoundry Global Education & Learning Series – Aerospace Materials and Processes.\nTo practice all areas of Aerospace Materials and Processes, here is complete set of 1000+ Multiple Choice Questions and Answers.\nParticipate in the Sanfoundry Certification contest to get free Certificate of Merit. Join our social networks below and stay updated with latest contests, videos, internships and jobs!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:8f29a902-09b0-4514-b9b8-d4a1adec7e49>","<urn:uuid:12ad3952-23b9-4131-a000-5e49e7e7efbb>"],"error":null}
{"question":"How do strength training recommendations compare between mothers looking to lose weight and basketball players?","answer":"For mothers trying to lose weight, strength training is recommended at least twice per week by the CDC to maintain and increase muscle tissue, boosting metabolism - with one hour of weightlifting burning about 365 calories for a 160-pound person. For basketball players, while strength is recognized as an important component, especially for high school athletes who may play multiple sports, the focus is on preserving fast-twitch muscle fibers and explosive power rather than doing extensive aerobic or endurance training that could compromise strength development.","context":["Losing sight of your weight is easy when you're tasked with the daily demands of motherhood. Lack of exercise and unhealthy eating habits can quickly take their toll. Experts recommend losing weight at a rate of no more than two pounds per week for lasting results. Since one pound of fat equals 3,500 calories, a 500-calorie daily deficit is required to lose one pound per week. Although this might sound like a lot, various lifestyle changes can help you get the job done safely and effectively.\n1. Food Choices\nIf finishing your child's leftovers and resorting to fast-food dinners has become a habit, making health-conscious choices can reduce your caloric intake. Swap high-calorie foods for low-calorie foods. For instance, instead of having ice cream for dessert, have strawberries to satisfy your sweet tooth; drink fat-free milk instead of whole milk and instead of chips, snack on air-popped popcorn without butter. Eating smaller portions can also help you reduce calories. Read and compare food labels to keep track of your caloric intake per serving size.\nThe Centers for Disease Control and Prevention suggest doing 30 to 60 minutes of moderate cardiovascular exercise five days a week. However, your motherly duties might not always allow you to work this into your schedule. In this case, spread your workout over the day into three sessions that last 10 or 20 minutes. Walking at a rate of 2 miles per hour for 30 minutes can burn about 100 calories for a person who weighs 160 pounds, while jumping rope can burn more than 400 calories in the same amount of time. Get your kids involved if you like -- walk with a stroller or join your kids in a rope jumping game.\n3. Strength Training\nAlthough you might associate weightlifting with grunting, sweaty and muscular men, don't pass on it. Targeting your body's major muscle groups can help maintain and increase muscle tissue to boost your metabolism. One hour of weightlifting can burn about 365 calories in a 160-pound person. If you're intimidated by the gym's weight room, try an exercise DVD or hire a certified, personal trainer who can safely introduce you to different exercises. Lifting weights at least twice a week is recommended by the CDC.\nBefore changing your diet or starting an exercise routine, consult a doctor, especially if you've been inactive or have a medical condition or injury. Finding the right balance is essential to weight loss. If one day you don't feel like exercising or if you had one too many cookies, don't beat yourself up over it. Make a concentrated effort to get back on track. Ask a supportive loved one to check in with you periodically to see how you are progressing. Remember that daily activities also burn calories and contribute to your weight loss.\n- Centers for Disease Control and Prevention: Losing Weight\n- MayoClinic.com: Counting Calories: Get Back to Weight-Loss Basics\n- Centers for Disease Control and Prevention: How Much Physical Activity Do Adults Need?\n- MayoClinic.com: Exercise for Weight Loss: Calories Burned in 1 Hour\n- MayoClinic.com: Metabolism and Weight Loss: How You Burn Calories\n- Jupiterimages/Pixland/Getty Images","|a youth basketball coaching and athletic resource PowerBasketball.com | Site Map | About Us | Contact Us | Advertise|\n|Coach's Clinic||Coaching Tips||Fundamentals||Books||Videos||Resources|\nDo Basketball Players Need Aerobics?\nby Charles Poliquin www.CharlesPoliquin.com\nRecently a father asked me about the testing protocols used by his sonís high school basketball coach. The coach had been using a mile run to test for aerobic endurance but then switched to a half-mile run. The father wanted to know if this was a wise decision, but he also pointed out that his team had won the state championship the previous year. OK, we have some issues here.\nFirst, to fulfill the definition of sports specificity, an acceptable time for a high school basketball player to be able to run a half mile would be 1 Ĺ to 2 hours. Sound crazy? Letís do some math.\nAl Vermeil won several world championship rings when he was a strength coach for the Chicago Bulls. Vermeil says that during a basketball game, video analysis shows itís rare that a single player will run as much as one mile by the time the final buzzer sounds. There are 48 minutes of playing time in an NBA game, but add to that all the timeouts, fouls, out-of-bounds and halftime Ė as a result, a typical NBA game will last about 2 Ĺ hours. It doesnít take a highly developed aerobic system to cover a mile in 2 Ĺ hours. At the high school level, total playing time is only 32 minutes, so even if a player were on the court the entire game, it is doubtful he or she would run a half mile. These numbers should dispel any idea that an aerobic base is necessary for basketball.\nAs for high school student whose team won the state championship Ė congratulations! But consider that with about six high school sport classifications in most of our 50 states, every year there are approximately 300 state championship basketball teams. And in high school, itís possible that one dominant player can make a significant difference in how well that team performs.\nIn his freshman year in high school, for example, LeBron James averaged 21 points and six rebounds a game, and his team finished with a 23-1 record and a state championship. Guess how the team did during the rest of Jamesí high school career? My point is that at the high school level, there are many factors that determine success in basketball, and when you do the math, having extraordinary aerobic endurance is probably not one of those factors.\nFurther, consider that if strength is an important component in basketball (and consider that high school athletes often play other sports such as football that do require considerable strength), aerobic training may compromise this physical quality. Letís explore.\nFor starters, it has been shown that athletes who have focused on aerobic training have fast-twitch fibers that behave like slow-twitch fibers. In other words, their fast-twitch fibers have greater endurance and smaller diameters, have a slower time to peak force, and are weaker. The fast-twitch fibers normally have large diameters, have a shorter time to peak force, and are stronger; but because in this case they were exposed to high volumes of aerobic work, the fibers adapted themselves to the training response.\nNext, when an athlete performs slow cyclical-type activities, the brain tends to organize muscle contractions in that manner. In other words, it is harder for the brain to organize ballistic, high-force muscle contractions. A Japanese study done a few years ago showed that the more you increase your V02 max, the more your vertical jump actually decreases Ė not a desirable result for a basketball player. And a Finnish study showed that doing aerobic work for the upper body makes your legs slower, showing that the negative power adaptation did not come from the muscle itself but from the nervous system.\nThe bottom line here is that itís important for coaches to conduct physical fitness tests on their athletes, but there are far better tests of endurance for a basketball player than a half-mile run.\nCharles Poliquin is one of the most accomplished strength coaches in the world. He has designed workouts for Olympic medalists in 17 different sports, world record holders in 10 different sports, and professional athletes in the NBA, NFL, NHL, MLB, and UK Premier League. He has lectured or consulted for a variety of high-profile organizations such as the US Secret Service, Walt Disney Corporation and the World Swimming Congress.\nPoliquin has written 600-plus articles and 10 books. His works have been translated into 12 different languages: English, French, Chinese, Finnish, German, Italian, Czech, Slovak, Spanish, Japanese, Dutch and Swedish. His innovative work in strength training is frequently cited in peer-reviewed literature.\nIn January of 2009 Poliquin opened the Poliquin Strength Institute in East Greenwich, Rhode Island. The institute contains both a 5,200-square-foot teaching gym with the best equipment in the world and a 2,000-square-foot multimedia classroom. Coach Poliquin has certified coaches in 56 countries through his Poliquin International Certification Program (PICP), and many of his former students are continuing his legacy with their success in training Olympians and professional athletes. He is the inventor of the BioSignature Modulation Method which has ascertained the relationship between body fat stores and hormonal profiles, and the methodology to improve site specific body composition.\nYou can read more about Poliquin at his website, www.CharlesPoliquin.com .\nNOTICE:All material on this web site is copyrighted. No article may be reproduced or redistributed in any form or manner without the expressed written consent of the respective author. Commercial reproduction is not permitted without the written permission of the Coaching Staff at PowerBasketball.\nhelping individuals and teams realize their fullest potential\nFundamentals and Training DVD's for the Player and Coach\n|Open since October 21, 1998. Copyright © 1998- PowerBasketball. All rights reserved.\nNo part of PowerBasketball, either text or image may be used for any purpose other than personal use.\nThis includes framing of web content, modification, reproduction, storage in a retrieval system or retransmission, in"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:748763eb-153f-4792-bdf3-d03dcda3fec5>","<urn:uuid:c7cfdcda-fe39-4c04-8d61-d794d213e217>"],"error":null}
{"question":"As an environmental researcher, I'm curious about how solar panel production impacts both metal resources and mining sustainability. What are the critical metals needed for solar panels, and what environmental damage does their mining cause?","answer":"Solar panels require numerous metals including arsenic, aluminum, boron, cadmium, copper, gallium, indium, iron, molybdenum, phosphorous, selenium, silica, silver, tellurium, and titanium. Some of these, like tellurium, are extremely scarce, making up just 0.0000001% of Earth's crust. The mining of these metals causes severe environmental damage, including erosion, loss of biodiversity, contamination of soil and water, deforestation for debris storage, and formation of sinkholes. The mining can leave behind huge holes, destroy natural landscapes, and create lifeless wastelands that threaten plant and wildlife species. Additionally, the extraction process can contaminate both surface and groundwater with harmful chemicals.","context":["As the demand for solar energy has increased so has the demand for the metals needed in the production of solar or photovoltaic panels and the batteries. However, a number of renewable energy forms (wind turbines, for example) and technology items (such as mobile phones) that we use daily have added to that growing demand as they also require metals for their production. As a result it can be hard to quantify what is required by solar specifically.\nMetals used in solar panels and batteries\nThere is a very wide range of metals used in solar panels and many are used in minute quantities. These are, in alphabetical order: Arsenic (used in semi-conductor chips), Aluminum, Boron minerals (used in semi-conductor chips), Cadmium (used in certain types of cells), copper (used in wiring and certain types of cells), Gallium, Indium (used in cells), Iron ore (steel), Molybdenum (used in photovoltaic cells), Phosphorous, Selenium, Silica, Silver, Tellurium, and Titanium.\nSome of these metals, such as Iron and Copper, are plentiful and mined in numerous locations. Others, including the so-called “rare earth” minerals are expensive and or scarce and or only available in one or few countries.\nThe batteries are far less metal-intensive as one is essentially dealing with either Lead or Lithium.\nMetal scarcity and its impact\nNicola Jones wrote an article titled A Scarcity of Rare Metals is Hindering Green Technologies published by Yale Environment 360. She discusses the situation in vivid and worrying terms. For example: “The move toward new and better technologies… means an ever-increasing demand for exotic metals that are scarce thanks to both geology and politics. Thin, cheap solar panels need tellurium, which makes up a scant 0.0000001% of the earth’s crust, making it three times rarer than gold.”\nJones goes on to say that the unprecedented demand for these metals created by renewable energy caused their average price to increase by a staggering 750% in 2011 and this crisis only ended when new trade deals were brokered.\nBecause the dangers of shortages, embargoes, and having to close aging mines, etc. remain, attention has been turned to potential solutions such as reusing, recycling, and finding alternative sources, opening new mines, and improving mining methods to increase efficiency and decrease costs.\nSolar power and silver\nSolar production and further development may at some point be affected to various degrees as trade agreements and metal availability changes. This is no truer than in terms of the precious metal that solar power has affected the most: Silver. In The Impact of Solar Power Projects on Silver Consumption published in 2014 by Kitco Metal Inc., the author states that, “the average solar panel contains between 15 and 20g of silver” and that “solar could equal or exceed the silver volumes previously used in the photographic film industry.”\nSilver went through a slump but the situation in terms of demand has dramatically changed and the primary reason is solar power. Tim Maverick of Wall Street Daily reported that, “there’s another new demand source and, surprisingly, it’s coming from the industrial sector. Last year , this sector accounted for 56% of overall demand for the precious metal … silver is an energy metal – it’s a necessary component in photovoltaic solar cells in the form of paste used as a conductor… The solar industry alone is forecast to use at least 70 million ounces of silver this year.”\nProtecting the future\nThere are websites and groups of individuals and organizations such as Solar Action Alliance that keep readers up to date with new developments in terms of solar power including in the aspects that relate to growing and further this source of renewable, sustainable energy.\nRecycling and reusing metals\nTons of what is now referred to as e-waste is thrown away each year. In theory, all the rare and other metals used in solar panels and batteries can be removed and recycled. What still needs to be established is whether it is viable long-term and what the economies of scale are. There are now companies, notably in Japan and Belgium, that process this type of highly specialized but increasingly important waste.\nOnce the metals needed in the production of solar power panels or cells have been recovered they can be reused. This will help to cushion solar energy and the developments in the field from any shortages or supply problems caused by failing resources or political issues.\nNew technology is requiring less expensive and rare metals. A final solution to protecting the future of both solar power and the metals it requires is to find alternative materials. Some new photovoltaic technology uses less-expensive and abundant metals such as zinc and copper in place of “rare earth” metals like Gallium and Indium.","Mining has several bad effects. It leaves behind a huge hole after mining is done. Secondly it damages natural beauty. A beautiful landscape which once existed is now a huge piece of dug up earth.\nEnvironmental Effects. Environmental issues can include erosion, formation of sinkholes, loss of biodiversity, and contamination of soil, groundwater and surface water by chemicals from mining processes. In some cases, additional forest logging is done in the vicinity of mines to create space for the storage of the created debris and soil.\nThe effects of mining in Africa have left large-scale devastation when companies do not honour their responsibility. Because mining areas are left in an unsustainable condition, plant species and wildlife are threatened and these areas are at risk of becoming lifeless wastelands.\nThe Impact and Effect of Illegal Mining (galamsey) towards the Socio-economic Development of Mining Communities: A Case Study of Kenyasi in the Brong Ahafo Region Adjei Samuel1, N.K.Oladejo1, I.A. Adetunde2, * 1University for Development Studies, Department of Mathematics, Navrongo. Ghana.\nSome of the major effects of mining on the environment are as follows: Minerals are the natural resources which play an important role in the economic development of the country. But the extraction and mining of these natural resources leads to some adverse effect on our environment as well.\nMar 09, 2017· The mining industry has the potential to disrupt ecosystems and wipe out wildlife populations in several different ways. Here's how mining affects the environment and wildlife. Habitat Loss; Mining can lead to the destruction of habitats in surrounding areas. The …\nModern mining is an industry that involves the exploration for and removal of minerals from the earth, economically and with minimum damage to the environment. Mining is important because minerals are major sources of energy as well as materials such as fertilizers and steel.\nApr 25, 2017· Mining is the extraction of minerals and other geological materials of economic value from deposits on the earth. Mining has the potential to have severely adverse effects on the environment including loss of biodiversity, erosion, contamination of surface water, ground water, and soil.\nSome gold can be found by panning in rivers; heavy gold will remain in the pan, whereas lighter rocks and minerals float out. This small-scale form of gold mining has little effect on the body of water, but the large-scale practice of mining gold from ore can have tremendous negative effects on water quality.\nMining can effect the earth because first, deforestation, and because mining requires large portions of land to be removed before they can start mining, lots of trees and plants are removed.\n1.1 PHASES OF A MINING PROJECT There are different phases of a mining project, beginning with mineral ore exploration and ending with the post-closure period. What follows are the typical phases of a proposed mining project. Each phase of mining is associated with different sets of environmental impacts. 1.1.1 Exploration\nFeb 07, 2018· The effects in such cases can be devastating for the environment. Be it due to ignorance of the regulations or just a freak accident, incidents like the Guyana spill of 1995 may occur again. This highlights the fact that issues like mining's effect on the environment are worth some serious deliberation.\nAug 26, 2010· Dust, radon and mercury impact miners' health. Dust, radon and mercury impact miners' health. ... Miners Face Health Risks, Even on Good Days ... mining …\nThe effects of mining coal on the environment. There are 2 ways to mine coal – Strip Mining and Underground Mining – both ways have their own impact to the environment and health. We know it but coal is such a cheap energy source that we don't want to let go of it. The negative effects of coal mining cannot be disputed:\nApr 21, 2019· The human health effects due to cyanide leach gold mining are not well documented, and this is no exception in Montana. The State of Montana has done no formal studies to specifically study mine-related health effects. Pegasus, the last mining company at Zortman-Landusky, started to fund a health study with the $1.7 million supplemental money from the 1996 settlement, but because …\nADVERTISEMENTS: Some of the major environmental effects of mining and processing of mineral resources are as follows: 1. Pollution 2. Destruction of Land 3. Subsidence 4. Noise 5. Energy 6. Impact on the Biological Environment 7. Long-term Supplies of Mineral Resources. Mining and processing of mineral resources normally have a considerable impact on land, water, […]\npositive and negative effects of mining on the environment. Mankind has been mining for precious metals since 42000 years ago and that's a staggeringly long time ago and that's exactly how long our species has been digging into the ground, to harvest its precious metals.\nDownload Coal Mining sounds ... 76 stock sound clips starting at $2. Download and buy high quality Coal Mining sound effects. BROWSE NOW >>>\nMining affects the environment by exposing radioactive elements, removing topsoil, increasing the risk of contamination of nearby ground and surface water sources, and acidification of …\nApr 20, 2015· Effects of Mining. Coal mining, the first step in the dirty lifecycle of coal, causes deforestation and releases toxic amounts of minerals and heavy metals into the soil and water. The effects of mining coal persists for years after coal is removed.\nJul 25, 2018· Environmental impacts from fossil fuel pollution are rapidly increasing in regions that have the highest concentrations of fuels. There are multiple effects of mining fossil fuels. Drilling and mining practices take a substantial toll on local water sources, biologic life and natural resources.\nPublished by the American Geosciences Institute Environmental Awareness Series. ... How can metal mining impact the environment? PDF version. Material adapted from: Hudson, T.L, Fox, F.D., and Plumlee, G.S. 1999. Metal Mining and the Environment, p. 7,20-27,31-35,38-39. Published by the American Geosciences Institute Environmental Awareness Series.\nMining operations usually create a negative environmental impact, both during the mining activity and after the mine has closed. Hence, most of the world's nations have passed regulations to decrease the impact. Work safety has long been a concern as well, and …\nEffects of mining on aquatic resources are both physical and chemical in nature. Most of earthmoving activities of mining occurred well before the enactment of laws designed to protect aquatic resources - particularly the 1977 Federal Water Pollution Control Act.\nThe former is known as underground mining, the latter as strip mining or mountaintop removal. Either process contributes a high level of damage to the environment: #12 Noise pollution. One of the most obvious (albeit perhaps least harmful) environmental effects of coal mining is noise pollution.\nMining has an adverse effect on soil quality. Soil degradation is the prime impact. Another impact is deforestation and loss of fauna and flora.\nThe impact of mining on the environment and the effects of mining techniques need to be more advanced with the utilization of modern equipment to be unintrusive to the environment. Economic growth is high on the agenda of leading countries, sustaining …\nMining is an inherently invasive process that can cause damage to a landscape in an area much larger than the mining site itself. The effects of this damage can continue years after a mine has shut down, including the addition to greenhouse gasses, death of flora and fauna, and erosion of land and habitat.\nNov 14, 2016· After mining is over, the land is left as barren land. The effects of mining sometimes vary depending on what is mined out, but these are some of the general effects you will see in all mine-areas. I'm not an expert when it comes to health impact on miners, but here are some of the things I know will affect them-\nJul 08, 2017· In coal mining, the extraction, crushing, and transport of coal can generate significant amounts of airborne respirable (extremely fine) coal dust. Dust less than 10 microns in size (cannot be seen with the eye). In non-coal mining, stone, and san...\nEnvironmental impacts of mining can occur at local, regional, and global scales through direct and indirect mining practices. Impacts can result in erosion, sinkholes, loss of biodiversity, or the contamination of soil, groundwater, and surface water by the chemicals emitted from mining processes. These processes also have an impact on the atmosphere from the emissions of carbon which have ...\nApr 04, 2017· The Dangerous Effects of Illegal Mining. April 4, 2017 Environmental Issues Written by Greentumble. Illegal mining has been ravaging our planet for. decades. Not only is illegal mining riskier from a safety perspective for those who choose to participate, but it encourages reckless behavior and leads to outcomes that have negative long-term ..."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:6e0dfe8d-5edc-48f6-9fc5-94fcfb1a9f79>","<urn:uuid:11ce18f5-51f7-4dfa-a26b-c4a5796e1622>"],"error":null}
{"question":"How do the environmental challenges of London's transport system compare to China's industrial pollution in terms of their impact on international relations?","answer":"While London's transport-related pollution remains largely a local issue being addressed through initiatives like Smarter London Together and green transport alternatives, China's air pollution has significant international implications. China's pollution affects neighboring countries like South Korea, Japan, and the Philippines, and studies have found that Chinese pollutants can reach as far as California, where a quarter of one type of pollutant was attributed to China. This international impact has created diplomatic challenges but also opportunities for China to engage in global environmental initiatives, such as its commitment to cap carbon dioxide emissions by 2016 in response to US climate actions.","context":["Creating the smart cities of the future is high on the agenda for national governments, local councils, and innovative tech companies alike.\nTake London as an example. In the summer of 2018, Mayor Sadiq Khan launched Smarter London Together, a plan to make London “the smartest city in the world.” Smarter travel is set to be a huge part of this – and when you consider how much data Transport for London (TfL), mytaxi, Uber, and others hold about both their own networks and the movements of almost 2 billion annual passengers, it’s clear there are some exciting opportunities to be had.\nSome strides towards smarter travel have already been made. The TfL route planner, for example, allows users to optimize their journey with features like accessibility requirements or walking preferences in mind. Others, such as CityPlanner or Transit, do something similar, often via TfL’s APIs. But this could be pushed a lot further, and in ways that benefit not just travelers, but businesses, the environment, and transport providers’ profitability, too.\nComplicating matters further, a recent King’s College study showed Oxford Street to be the most polluted street in Europe, and a DEFRA report demonstrated that much of the pollution in central London is due to buses. And as for last-leg transportation (for those in a hurry or with luggage, or for the elderly and children), it isn’t always delivered by TfL; it’s often filled by Uber, taxis, and so on.\nSo, with all this in mind, what does a smarter future for London’s transport really look like – and what tools are needed to make this vision a reality?\nGreater choice and better flow for travelers\nThe tools users currently have to navigate London’s transport system are functional enough, but far from optimal. End users seem to be concerned with the obvious: price, accessibility, speed. But they are also interested in things like avoiding busy areas or traffic, not contributing to pollution, enjoying the weather, and aligning travel with their fitness goals.\nA holistic transport provider could crunch data on a cloud platform and develop truly intelligent route-mapping applications – and give users choice. The application could extend the purview of that journey to include the micro-journeys that can arise: a taxi back from the supermarket or a bus to take you directly to a large office. A new view of the journey starts to arise: a multi-modal one, where the passenger or traveler isn’t tied up with complexities about finding the nearest taxi stand or ensuring they have mobile signal to command an Uber.\nThere are multiple ways to answer: “What’s the best route for my journey?” Shortest in time, for sure. Most reliable or least traffic could be more useful for important journeys like hospital trips. An insurer might want to incentivize you to consider the least risky route, while your conscience might prefer the one that causes the least pollution. In all cases, it’d be nice to know what these answers are if we want to make informed decisions about our own travel.\nThis level of personalization and journey aggregation wouldn’t only create a better experience of moving through London, it would also improve traveler flow, facilitating smarter journeys with recommended stops along the way. This would benefit everyone from the tourist who wants the most scenic route back to their hotel to the office worker who needs to swing by a flower shop on their way home.\nCreating a greener, smarter city\nIn 2018, London hit its legal air pollution limit just one month into the new year. This figure speaks for itself, but as anybody who’s ever witnessed a horde of near-empty buses creep their way down Oxford Street will know, the city’s transport system is far from optimized. Having a robust platform in place to better analyze the flow of travel could play an invaluable part in minimizing unnecessary transport pollution and helping planners figure out exactly where and when the majority of people need to travel.\nAlthough the mayor has already introduced measures like charges for dirty vehicles entering the city, a smart-city platform could contribute further – both by optimizing public transport schedules to reduce needless journeys and by directing travelers to green transport alternatives, like Santander Cycles.\nAn opportunity for profit\nConsider the number of people traveling in and out of London each day – by plane, train, or tube. Providing seamless, end-to-end journeys on trips that start outside London isn’t easy, and given that travelers don’t have a great deal of choice, there’s relatively little impetus for providers to invest in a platform that can assist with this. But as the city’s population and numbers of visitors grow, access to this platform’s insights could be a hugely valuable resource for various private businesses and individuals. Which means a smart travel platform could actually generate profit for an organization like TfL.\nAirlines are a prime example of this. Passengers arriving into Heathrow may have planned to get the tube into the city. But a business professional who needs to make calls and get ready for a meeting during their journey into London may be left scrambling to find another route. Through integration with a smart platform, the airlines could offer push notifications through their apps with the opportunity to book a taxi or access shared minibus services – impressing their customers, improving their service, and smoothing the onward journey in one fell swoop. For the passenger, it’d be great to be able to miss out the taxi queue at T5 and jump straight into their pre-booked black cab or Uber car. No standing around in the cold. No hour spent underground without mobile signal. No issues trying to find a cashpoint.\nEqually, there’s sometimes little support for Londoners’ use of their own city. One of the more frenetic taxi stands in the city is at the Sainsbury’s in Angel, with people pilling into cabs, their arms bulging with purchases, as there is little or no other alternative for the many elderly in Islington who want to do their own shopping and take it home. It seems a pity that this kind of journey can’t be paid for by Oyster cards.\nAccess to the platform could even be bought by supermarkets, food chains, and restaurants around key transport hubs, allowing them to predict footfall and adjust their strategies accordingly. This kind of deal would be a significant commercial opportunity for ticketing providers such as TfL, so it’s certainly worth exploring. There’s also the possibility for providing subsidized local transport to job seekers and the homeless.\nEqually, such a strategy might help London re-evaluate the potential of river craft (especially for Canary Wharf / City journeys), incentivize cycle usage; and enable better timings and placings of services within and around stations.\nBuilding London’s smart future\nLondon’s smart-city future is a compelling prospect, and more intelligent travel should play a significant role in this. Because whether you’re a tourist, a commuter or an infrequent visitor to London, everybody ultimately wants the same thing: the most seamless end-to-end journey. Increasingly, as journeys are streamlined, it will be necessary for this platform to become more dynamic and responsive to real-time situations. “Take me home without a traffic jam” might become an interesting option on Waze or Google Maps.\nBut to deliver this, transport-providing organizations need to do more to invest in travel and transportation technologies that can help them build the relevant applications, whether that’s intelligent route mapping or a platform to provide data access for partner businesses. Without this innovation, London’s plans to become the world’s smartest city may well falter before they truly begin.\nFor more on smart-city technology, see How Future Cities Can Engage Citizens.","How serious is air pollution in China and how is the country addressing it at a policy level? Also, what kind of business opportunities is this creating? A comprehensive look.\nIf you live in China, chances are the one thing you’d be most concerned about is the level of PM2.5 in the air. PM2.5, or particulate matter of a diameter of 2.5 micrometres or less, reduces life expectancy drastically because of its ability to penetrate deep into the lungs and cause health problems. The World Health Organization’s recommended standard for PM 2.5 is 25 micrograms per cubic meter within a 24-hour period of time, and 10 micrograms per cubic meter aunually. Yet China’s cities routinely cross the recommended levels. Beijing, for instance, hit a PM2.5 level of 600 micrograms per cubic meter and “may even have hit 900” in January 2013. In 2013, the PM2.5 reading averaged 89.5 micrograms per cubic meter.\nGiven this backdrop, it is not surprising to see that out of 20 cities with the worst air quality globally, 16 are in China. Every year, 1.2 million people in China die of air pollution-related diseases. China’s pollution problems routinely makes headlines in newspapers around the world.\nYet what people may not know is that the air quality is actually getting better. In Beijing, the level of the worst pollutants was cut by one-third during the last decade and that of sulphur dioxide alone was cut by nearly 70%, says Anthony Liu, Visiting Assistant Professor of Economics at CKGSB. In this interview, Liu, who researches environmental economics, talks about the impact of air pollution in China and how the country is addressing it at a policy level.\nQ. Who is tackling these pollution issues and how are they doing so?\nA. To look at solutions, you have to understand the sources. The three top contributors are cars, coal, and industry. Heavy industry, which is extremely polluting, can be a big contributor to air pollution, and there are smaller ones as well. Construction, trash burning, or even barbecue can be a big contributor to air pollution.\nIn each of these areas the Chinese government has moved substantially in trying to cut them. In cars for example, a number of cities in China have recently implemented car restrictions, by which before people could purchase a car, they have to first win the right to do so. They either have to enter an auction, where they purchase a license plate, or they have to win a lottery. My own research suggests that in Beijing, the car lottery will cut the number of automobiles by 10% by 2020. So, this will cause substantial decreases in the number of cars over the next decades. Also, the Chinese government has worked to remove five million of the worst polluting vehicles from roadways–330,000 in Beijing alone. Because it is removing many of the most polluting [vehicles] we should see improvements in pollution, at least the forms of pollution emitted from cars.\nThe second form of pollution is coal. The Chinese government recently mandated flue gas desulphurization, a technology which removes the worst of sulphur dioxide from coal plant emissions. It allows the release from generating electricity to be much cleaner. We’ve seen sharp improvements around the country in sulphur dioxide, which leads to decreased acid rain, decreased damage to people’s health.\nThe third area is heavy industry. Here I think the best solution has just been to remove heavy industry from the city, from areas where people live. In [a way] this makes perfect [sense]. Beijing recently ordered 53 heavily polluting companies to move out of the city.\nQ. What do you see as a consequence that the public isn’t aware of about this PM2.5 issue?\nA. One of the most exciting and important developments in the past few years has been the increased amount of information that has been available to people. The Chinese government has exponentially increased the number of air monitors and the frequency with which these air monitors release data. As a result, the average Chinese citizen can now, for the first time, see the air quality in their area in real time. The increased amount of information has really improved the level of engagement of common citizens because people can adjust their plans in accordance with whether the air quality will be good or bad. That has really improved people’s quality of life with regard to air pollution.\nQ. How does PM2.5 affect international relations between China and the regional countries?\nA. Air pollution in China has a heavy toll on its neighbors. Countries like South Korea, Japan, and the Philippines, all receive heavy doses of China’s air pollution. One study looked at pollution in California found that a quarter of one type of pollutant could be attributed to a kind of pollutant that only occurred in China. So, even across the Pacific Ocean, air pollution could reach all the way to the US.\nAs a result, China’s relationships with its neighbors are certainly negatively affected. That’s not to say there aren’t opportunities. China’s environmental problems have created major opportunities for it to engage [in] and gain international prestige.\nFor example, China is currently the number one emitter of greenhouse gas. Obama recently initiated a 30% cut in the emissions from power plants in the US, and China followed the next day with its own announcement that by 2016 it would begin to cap its own emissions of carbon dioxide. Because China’s emissions are growing the fastest, this was regarded as a major change in the climate change negotiations. So there are major opportunities just by the virtue of China’s status as number one emitter of greenhouse gas, the number one polluter.\nQ. Is there a business opportunity for foreign corporations from PM2.5 emissions and problems?\nA. When I think about business opportunities in the area of environmental quality I think of two major issues: willingness to pay for a product or service that can improve environmental quality and who is best positioned to capture this value. For both of these questions, the answer is very clearly yes.\nFor air quality, people have demonstrated a willingness to shell [out] large quantities of money for personal protection, for air masks, for in-room air purifiers, for devices that can shelter them from air pollution problems. This ties in well with the second issue, which is which companies are best positioned to benefit. International companies are currently better positioned to capture the value from environmental goods, because the number one issue in capturing environmental value is trust. International brands have an advantage because they are frankly regarded with some more trust than many domestic brands.\nSince there is higher trust with international brands, companies like Johnson & Johnson, GE and Siemens are potentially very well positioned to capture portions of the market for environmental services. If they move into markets that improve people’s health and offer a benefit that people can’t find from domestic markets, there will be huge business opportunities for those companies.\nQ. What are some of the unexpected businesses which are going to thrive under this PM2.5 situation?\nA. There are some companies that are directly positioned to benefit and some indirectly positioned. For air purifiers I’m thinking of the segment more broadly, from small air purifiers that can clean out a room to large ones in an apartment building or an office space. There is very clear space for those types of firms, but there are many kinds of companies that are going to indirectly benefit from air quality problems, [such as] natural gas companies. Even though China has massive natural resources, it cannot burn coal recklessly. It is going to move away from coal towards natural gas, the other major cost effective source for electricity generation. So I expect companies in natural gas to do very well and companies in nuclear power and in alternative energy as well.\nOn the consumer side, I think that vacation planning is going to be a big industry. As people consider vacation destinations, they think about going to islands, to areas with cleaner air. Many of these already advertise how clean they [are] compared to many major cities, so I think this will be an increasing attraction for people who want to get away from the city.\nAnother area that I think will be significantly affected by air quality is real estate. Real estate and property in cities that are cleaner are going to do better as people take in the environmental amenities of those areas."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:18215f2f-a11d-40c6-8fff-643bacbd6ba9>","<urn:uuid:bc5b8e77-7702-45c3-b5fb-1ac1e97d831f>"],"error":null}
{"question":"Who are the percussionists that performed at the front of the stage during Threshold's premiere?","answer":"Tim Palmer and Jason Huxtable (Maraca2) were the percussionists who worked with instruments placed at the front of the stage.","context":["Glockenspiel, octobons, spiral cymbals, temple bowls, vibraphone … that’s a partial list of percussion instruments on display at Walt Disney Concert Hall January 25-28 for the Los Angeles Philharmonic concert “Dudamel Conducts Brahms.”\nThe instruments were present for the world premiere of the concerto Threshold by Los Angeles Philharmonic principal timpanist Joseph Pereira. The stage was so crowded with percussion that principals entering and exiting had to execute a cautious traffic pattern to accommodate the array. The mood among the packed house with rapt. Whatever sound was planned had to be extraordinary.\nA timely concerto\nPereira was at the mastermind center of the 25-minute work. Six timpani, played in otherworldly ways, were set before him. In composing the piece, Pereira’s main influence was the world’s deepening gravitas (or lack thereof). “Life has become saturated with tension and anxiety – much like the feeling of this piece,” he writes.\nThreshold was dense with a taut restlessness, except for a blessed, several-beat rest about one-third of the way through – all the contrast it needed. There were mechanized sub notes: a scrub brush rasped over a bass drum, and stones moved across a formation of ceramic tiles.\nPercussionists Tim Palmer and Jason Huxtable (Maraca2) worked instruments placed at the front. Three additional percussionists were at the rear, sandwiching the orchestra with novelty. A mostly improvised tom-tom / roto-tom solo was striking. Arriving after a build-up of layered sounds, it provoked the question:\nWhere is all of this heading?\nAn apt question, given current events mirrored by Threshold. In truth, Pereira’s work didn’t need to head anywhere (although it did), but it was central that the question arose. Even in Walt Disney Hall Concert Hall one does not escape the headlines, or at least their penetrating effect. Threshold traveled to a conclusion with glissandos and temple bowls that evoked ritual, and at least a whisper of promise. The ear was left with air – ineffable sound that replenished the space.\nBrahms’ alternate reality\nThe evening began with a 5-minute sparkly appetizer: Igor Stravinsky’s Fireworks. The final piece: Johannes Brahms’ Symphony No. 1.\nIn itself, Brahms’ masterwork does nothing less than confirm trending theories of multi-dimensional realities. Multiverses, I believe that they’re called – the simultaneous, interconnected existence of any possible number of worlds.\nBrahms’ polyphonous symphony happens all at once, beginning with its ominous launch – an edgy dark blast that locks you in for the ride. The work is all protean intrigue from there, even in the hazier andante. No. 1 delivers an interactive web of sound.\nDudamel’s off-the-book direction took the symphony one step further: it placed the audience inside the alternate reality that Brahms deftly created over 14 years labor, beginning in 1854. Dudamel inhabited the music in ways that few accomplish – and merging with the Los Angeles Philharmonic in a kind of otherworldly mind-meld, the conductor and orchestra delivered astonishment.\nThe coda brought Brahms’ heroic thunder, spun into an expanding shockwave by Dudamel’s hand. By that time, I was wholly inside one of those alternate worlds.\nPhoto courtesy Los Angeles Philharmonic"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:1b1e00c9-e3cf-4f5d-b311-8ecca9ff0cfb>"],"error":null}
{"question":"For future agricultural monitoring systems, which monitoring methodology - remote optical/long-path monitoring or IR spectroscopy-based predictions - shows more promise for providing comprehensive multi-element analysis of environmental conditions?","answer":"Based on the documents, IR spectroscopy-based predictions show more promise for future comprehensive multi-element analysis. While remote optical/long-path monitoring does offer multi-component measurements and is useful near sources, the IR spectroscopy approach demonstrated through the AfSIS database has already shown success in simultaneously analyzing multiple elements (12 elemental concentrations) with good prediction accuracy for several elements like Ca, Mg, Al, and B. The spectroscopic method also has the advantage of being cheaper and more accessible, with potential for improvement through advanced algorithms and additional data collection. In contrast, remote optical monitoring is relatively expensive, requires trained operators, and its data is not readily comparable with point measurements, making it less practical for widespread agricultural implementation.","context":["For growers it is generally very important to know the composition of their soil. A soil that lacks certain nutrients will commonly need some form of amendment while a soil that contains very high concentrations of certain elements – like aluminium for example – might not be suitable for growing certain crops. However measuring the concentration of metal ions in soil is not trivial – since direct lab measurements of ion concentrations are expensive – so a cheap way to do this using a “field friendly” method would be invaluable. Can we create accurate models to predict metal concentrations based on a cheap measurement? We’ll see!\nThis is where the AfSIS database comes into play. This set of data – created by the Africa Soil Information Service project, contains the output of a cheap spectroscopic technique with the actual lab analysis results of hundreds of different samples, distributed along the African continent. With this spectral and analysis data we can then construct statistical models to attempt to predict actual ion concentrations from these simple and cheap IR spectroscopy measurements. Better yet, the AfSIS database is freely available and hosted by Amazon AWS. The database files even contain some simple examples of how to carry out basic machine learning and statistical analysis using the project files.\nSeveral years ago, a kaggle competition was actually done for this and the winner – not surprisingly – used a complex ensemble of models using support vector machines and neural networks with different levels of complexity (view the winning solution here). The scores of the final solution were also not very high and can now be surpassed with new boosting algorithms developed in recent years (like LGBM) plus the use of additional data that has been collected by the project since that time. The kaggle challenge also addressed 5 measurements Ca, P, pH, SOC and Sand, while I find it way more interesting – from a practical standpoint – to look at all 12 elemental concentrations that have been measured.\nUsing a simple boosting regression approach (LGBM) with no optimization, using the gradient of the spectral information and the Log(M+1) of the ion concentration as the target, I was able to obtain the results shown in this post. The first image shows the results of 5 testing sets obtained with random shuffling of the data for each ion while the second image shows the mean square Pearson correlation coefficient of these graphs. We can see that pretty good predictions can be achieved for Ca, Mg, Al and B, while the problem becomes way harder for elements like P and Zn.\nNotice that at this point I have carried no extensive effort to improve the model, since at this point I’m just interested in understanding the problem. What’s hard to predict, what’s easier to predict, so that I can better understand where to focus. I want to know where the spectral measurements made hold the most value and where they are weak.\nThe questions we need to ask now are, what makes some elements easier to predict than others? Why are Zn and P so hard to predict from this data? Can we use the relatively accurate predictions we have for Mg and Ca to enhance our predictions for other ions? Is there a more intelligent way to preprocess this data to extract information? Stay tuned for part two in this series, where I will try to answer some of these questions.","Air monitoring methodologies can be divided into five main types, covering a wide range of costs and performance levels. The methods and their relative merits are shown in the table below and discussed in the following section. The use of a particular type of monitoring equipment may need to be justified in review and assessment reports and therefore should be chosen appropriately.\nIt is also important to choose the most appropriate monitoring location for investigating a specific air pollution source or problem.\nAdvantages and Disadvantages of Monitoring Methods\nPlease click the method links to view detailed description of the method.\n|Passive sampling||Low cost - simple. Useful for screening and base-line studies and in support of automatic monitoring for Detailed Assessments.||Unproven for some pollutants. Laboratory analysis required. In general, only provide weekly or longer averages.|\n|Photochemical and optical sensor systems||Can be used portable.||Low sensitivity may only provide spot measurements.|\n|Active (semi-automatic) sampling||Low cost - easy to operate - reliable. Historical data sets available from UK networks.||Provide daily averages. Some methods are labour intensive. Laboratory analysis required.|\n|Automatic point monitoring||Provide high resolution data. On-line data collection possible. Provide path or range-resolved data.||Relatively expensive. Trained operator required. Regular service and maintenance costs.|\n|Remote optical/long-path monitoring||Useful near sources. Multi-component measurements possible.||Relatively expensive. Trained operator required. Data not readily comparable with point measurements.|\nSince monitoring instrumentation covers a wide range in capital and running costs, it is usually advisable to choose the simplest method available to meet the specified monitoring objectives. Many baseline monitoring, spatial screening and indicative surveys can be served perfectly well by inexpensive active or passive sampling methods. Only proven and generally accepted measurement methods should be considered.\nMonitoring site locations\nMonitoring sites can be classified according to the type of environment in which they are located, in order to permit more meaningful evaluation of data. The site description will generally reflect the influence of a particular pollutant source or of overall land use. Typical monitoring location types, as used in national automatic monitoring networks, are described in the table below.\n|Urban||Vehicle, commercial, space heating.||Identification of long-term urban trends|\n|A site sampling within 1m of the kerbside of a busy road.||Local traffic.||Identifying vehicle pollution blackspots. Assessing worst case scenarios. Evaluating impacts of vehicle emission control technologies. Determining impacts of traffic planning/calming schemes.|\n|A site sampling between 1m of the kerbside of a busy road and the back of the pavement. Typically this will be within 5m of the road, but could be up to 15m.||Local traffic.||Assessing worst case population exposure.Evaluating impacts of vehicle emission controls.Determining impacts of traffic planning/calming schemes.|\n|A location type situated in a residential area on the outskirts of a town or city.||Traffic, commercial, space heating, regionaltransport, urban plume downwind of a city.||Traffic and land-use planning.Investigating urban plumes.|\n|An urban location distanced from sources and therefore broadly representative of city-wide background conditions e.g. urban residential areas.||Vehicle, commercial, space heating.||Trend analysis.Urban planning.Traffic and land-use planning.|\n|An urban location representative of typical population exposure in towns or city centres e.g. pedestrian precincts and shopping areas.||Vehicle, commercial, space heating.||Identification of long-term urban trends.|\n|An area where industrial sources make an important contribution to the total pollution burden. Intermediate. 20-30m from the kerb of a busy road.||Industrial, motor vehicles.||Assessing local impacts on health and amenity. Process optimization. Source attribution/identification. Providing model input data. Model development/validation. Local planning and plant authorization.|\n|20-30m from the kerb of a busy road.||Vehicle, commercial, space heating.||Identification of long-term urban trends.|\n|Monitoring within the boundary of an airport perimeter.||Aircraft, vehicle, commercial, space heating.||Determine air quality impact of airport.|\n|Any special source-orientated or location category covering monitoring undertaken in relation to specific emission sources such as power stations, car parks or tunnels.||As specified.||As specified.|\n|An open countryside location, in an area of low population density distanced as far as possible from roads, populated and industrial areas.||Regional long-range transport, urban plume.||Ecosystem impact studies.Assessing compliance with critical loads and levels for crops and vegetation.Investigating regional and long-range transport.Identification of ozone hot spots.|\n|A site in open country, located in an isolated rural area, experiencing regional background pollutant concentrations for much of the time.||Regional/hemispheric background.||Assessing unpolluted global or hemispheric background conditions.Long-range transport studies. Long-term baseline trend analysis.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:978001f6-2ca3-4477-a862-d517170bf1aa>","<urn:uuid:5b783d23-1aef-4ac3-b0cd-1861bca174c0>"],"error":null}