{"question":"How do I calculate if mortgage points are worth it for my home loan?","answer":"To calculate if mortgage points are worth it, divide the dollar amount of the points by your monthly interest savings. The result shows how many months you need to stay in the home to break even. For example, if you get 95 months, you'd need to stay in the home for almost 8 years (7.91 years) before the points become worthwhile. You should also compare three scenarios: savings if you sell/refinance in a few years, savings if you keep the mortgage for its full term, and savings based on your realistic expected stay in the home. Additionally, consider that points are tax deductible if you itemize, so factor in whether the tax break is more beneficial in the purchase year or as larger annual deductions over time.","context":["Mortgage points are money you pay up front at the time of closing in exchange for a lower interest rate on your home loan. They’re also referred to as “discount points,\" and the process has been called “buying down the rate.” Buying mortgage points at the time of closing can save some significant money on monthly mortgage payments over the years. Plus, you may find the points tax deductible.\nHow Do Mortgage Points Work?\nMortgage discount points are something like prepaid interest. Each point you purchase as an upfront cost when you close on a mortgage to buy down the interest rate on your loan by a certain percentage. Your home mortgage interest rate will be lower if you buy one point instead of no points, and even lower if you buy two points rather than one. You can even buy just a fraction of a point. They’re a one-time cash-out-of-pocket expense made to cover the cost of the points, in exchange for reduced mortgage payments for as long as you carry the loan.\nMortgage payments can be comprised of numerous factors: principal toward paying off your loan, homeowners insurance, property taxes and interest on the amount you borrowed. The percentage of interest each point will save you depends on your lender, and these percentages can differ between types of mortgage loans as well. The market at the time you take out your mortgage can have an effect, too.\nRead More: How to Calculate Mortgage Interest\nHow Are Points Calculated?\nThe cost of points is universal among lenders. Each mortgage point equals 1 percent of the amount you’re borrowing. For example, a single point would cost you $1,750 if you’re taking out a $175,000 mortgage. That $1,750 would typically reduce your interest rate by about 0.25 percent, so you'd pay only 4.25 percent over the life of your loan if you were initially quoted a rate of 4.5 percent.\nYou might pay $1,750 now, but that will slash each and every one of your mortgage payments by $35 or so. Now assume that yours is a 30-year fixed-rate mortgage. That works out to a savings of $12,600 over the term of your loan: $35 times 360 months (12 months a year for 30 years).\nWhen Do You Pay Points?\nAny points you buy are payable at closing, and the money goes to your lender. It’s added to your closing costs, so you have to add them to the check you’re writing on that day. But you’re not prohibited from negotiating a deal with the home seller to pay some or all of your points. This might be an option for you as a homebuyer in a strong buyer’s market with a seller who’s desperate to get their home sold.\nThe points you and your lender have agreed on, as well as the corresponding interest rate reduction, should appear on the loan estimate you receive at the time you apply for the mortgage, as well as on the closing disclosure, on page 2 in Section A. The closing disclosure should be provided to you a few days before settlement.\nComparing Mortgage Loans\nThe rate-reducing power of a point is at a lender’s discretion. One lender might cut your interest rate by 0.22 percent for each point, while another will give you 0.26 percent off. Points aren’t regulated by federal law, except to the extent that you must receive some discount on your interest rate in exchange. It’s also possible that a lender will offer a certain percentage to one borrower, and a different percentage to another, or that two separate lenders will offer different percentages to two individuals with the exact same loan qualifications and credit scores.\nThat said, points are definitely something to inquire about when you’re comparing loans. You also want to make sure that you’re not being offered a really nice interest discount in exchange for higher other fees. Make sure the equation works out in your favor.\nWhen Points Aren’t Really Points\nNot all points are points in the true sense of the word. Other loan terms can include the word “points” when, in fact, they’re something else entirely.\nOrigination points are a prime example. These points are fees you pay to your lender in exchange for processing your home loan. Origination fees are payable at settlement as part of your closing costs, too, and they’re also generally equal to 1 percent of your mortgage amount, but they’re not going to have any effect on your interest rate.\nThe Consumer Financial Protection Bureau (CFPB) indicates that some lenders have been known to tag any upfront fee that’s based on a percentage of your loan amount as \"points.\" Again, check your loan estimate and closing disclosure so you’re sure you understand what you’re paying, even if you have to take the paperwork to an attorney for further review.\nAre Points Right for You?\nYou might be thinking that paying to get a low rate with tax benefits all sounds like a pretty good deal, particularly if you have the cash on hand to pay a little more at closing. It can be, but as with any transaction, there are pros and cons.\nComing up with that extra cash out of pocket at settlement is probably a solid choice if you’re reasonably sure that you’re going to remain in that home – with that mortgage – for decades. You’ll obviously save much less if you move out or refinance in five years rather than run out the entire 30-year mortgage term. Otherwise, you might want to go ahead and pay a higher interest rate in exchange for some other lender concession or credit at closing.\nThe CFPB suggests comparing three scenarios: how much you’ll save if you were to sell or refinance in a few years, your savings if you were to remain in place with the same mortgage and monthly payment for the life of the loan, then again based on how long you honestly expect to live there.\nAnother option is to divide the dollar amount of the points you’re considering paying by the monthly savings in interest. The answer will tell you how long you must stay in the home, with that mortgage, to reach the breakeven point where paying for points begins to make sense. It’s expressed in months, so you’d have to remain in the home without refinancing for almost eight years if the number you arrive at is 95. It works out to 7.91 when divided by 12 months in a year.\nAnd here’s something else to consider: Points are tax deductible if you itemize, and mortgage interest is as well. So when is this tax break going to do you the most good? Will it be more helpful in the year you purchase, or taking a larger deduction annually over a period of time? You’ll want to factor this into your math calculations as well. Keep in mind that a good many rules apply to claiming this tax deduction. For example, the home must be your primary residence, and the points must be clearly noted on your closing disclosure and any disclaimer statements. There are limits to the amount of mortgage debt that qualifies as well.\nRead More: Can You Claim Mortgage Interest on Taxes?\nThe ultimate goal is making your money work for you to the greatest extent possible by negotiating the best mortgage for your circumstances. You can often reduce your interest rate by making a larger down payment, too, and that money buys you more home equity. It doesn’t go into your lender’s pocket.\n- Consumer Financial Protection Bureau: What Are (Discount) Points and Lender Credits and How Do They Work?\n- IRS: Topic No. 504 Home Mortgage Points\n- Navy Federal Credit Union: Understanding Mortgage Points\n- Bankrate: Mortgage Points and How They Can Cut Your Interest Costs\n- Bank of America Better Money Habits: What Are Mortgage Points?\nBeverly Bird has been writing professionally for over 30 years. She is also a paralegal, specializing in areas of personal finance, bankruptcy and estate law. She writes as the tax expert for The Balance."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:a8da4592-6c60-4060-9cae-8907b428f229>"],"error":null}
{"question":"What is the relationship between inbound/outbound access list processing and cloud migration strategies?","answer":"In traditional networks, inbound access lists process packets before routing to outbound interfaces, while outbound access lists process packets after routing but before queuing. This processing distinction relates to cloud migration strategies through Gartner's 5R's framework. When rehosting ('lift and shift'), existing access list configurations must be carefully considered as applications are redeployed on cloud servers. More complex strategies like refactoring or rebuilding require optimization of these access controls to fully leverage cloud services. The choice between migration strategies must account for how access list processing will work in the new environment, particularly in hybrid cloud deployments that combine private networks with cloud-hosted services.","context":["In this article we will look at very basic security feature of routers and L3 switches – Access Lists\nAt the most basic level, an access list is a list of conditions that categorize packets and because of this, they can be really helpful when you need to exercise control over network traffic. One of the most common and easiest-to-understand uses of access lists is filtering unwanted packets when implementing security policies. There are a few important rules that a packet follows when it’s being compared with an access list:\n- It’s always compared with each line of the ACL in sequential order and progress in that way, beginning with the first line of the ACL, moving to line 2, then line 3, and so on.\n- Packets are compared with lines of the ACL only until a match is made. Once the packet matches the specified condition delimited on a line of the ACL, the packet is acted upon and no further comparisons take place.\n- There is an implicit “deny” at the end of each ACL, which means that if a packet doesn’t match the condition on any of the lines in it, the packet will be discarded.\nStandard Access List\nThese use only the source IP address in an IP packet as the condition test. All decisions are made based on the source IP address. This means that standard access lists basically permit or deny an entire suite of protocols. They don’t distinguish between any\nof the many types of IP traffic such as Web, Telnet, UDP, and so on. Standard ACLs are old and not used any longer in production networks.\nExtended Access List\nExtended access lists can evaluate many of the other fields in the layer 3 and layer 4 headers of an IP packet. They can evaluate source and destination IP addresses, the Protocol field in the Network layer header, and the port number at the Transport layer header. This gives extended access lists the ability to make much more granular decisions when con- trolling traffic.\nExample Named Access List\nnexus7k(config)# ip access-list Deny_FTP\nnexus7k(config-acl)# deny tcp any host 10.1.1.1 eq ftp\nnexus7k(config-acl)# permit ip any any\nnexus7k(config-acl)# int e3/2\nnexus7k(config-if)# ip access-group Deny_FTP out\nTo use an access list as a packet filter, you must apply it to an interface on the router where you want the traffic filtered. And you’ve got to specify which direction of traffic you want the access list applied to.\nInbound access lists\nWhen an ACL is applied to inbound packets on an interface, those pack-ets are processed through it before being routed to the outbound interface. Any packets that are denied won’t be routed because they’re discarded before the routing process is invoked.\nOutbound access lists\nWhen an ACL is applied to outbound packets on an interface, packets are routed to the outbound interface and then processed through the access list before being queued.\nWhen configuring ACLs from the Internet\n- Deny any source addresses from your internal networks.\n- Deny any local host addresses (127.0.0.0/8).\n- Deny any reserved private addresses (RFC 1918).\n- Deny any addresses in the IP multicast address range (18.104.22.168/4).\nWildcards are used with access lists to specify an individual host, a network, or a certain range of a network or networks. to specify a host, the address would look like this:\nto specify a subnet\nStandard Accecss List 1-99\nCorp(config)#access-list 10 deny 172.16.0.0 0.0.255.255\nExample Extended Access-list 100-199\nCorp(config)#access-list 110 deny tcp any host 172.16.3.2 eq 23 log\nExample of how to deny access to Telnet and FTP from host 172.16.50.5\nLab_A(config)#access-list 110 deny tcp any host 172.16.5.5 eq 21\nLab_A(config)#access-list 110 deny tcp any host 172.16.5.5 eq 23\nLab_A(config)#access-list 110 permit ip any any\nLab_A(config-if)#ip access-group 110 out","More companies are migrating apps and services to the cloud every day, but it can be challenging. Start with the basics – what is cloud migration and what businesses stand to gain – and then get strategies and a checklist for making the change with confidence.\nCloud migration is the process of moving applications, data, and other components hosted on servers inside an organization to a cloud-based infrastructure.\nSome of the leading cloud providers are Amazon AWS, Microsoft Azure, and Google Cloud Platform. These not only provide the hardware but also offer a variety of rich apps and services for continuous integration, data analytics, artificial intelligence, and more. At Sumo Logic, our cloud-neutral products can easily integrate with most leading cloud-based solutions.\nWhy Migrate to the Cloud?\nOrganizations have traditionally been held back by the challenge of growing their information infrastructure. However, moving to the cloud adds tangible value to their outlook. Here are a few benefits:\nAgility and speed\nWith the cloud, procurement of new inventory and storage space is reduced to a matter of days or even hours, giving businesses the agility to respond to a rapidly changing technological environment.\nThe simplicity in cloud solutions makes teams more productive. In distributed teams, the cloud removes region-specific dependencies, creating a collaborative team setting.\nMost popular cloud solutions have robust built-in security programs. At Sumo Logic, we help optimize cloud security, providing security analytics and visibility across the entire cloud environment.\nCloud providers package several useful features such as disaster recovery, automatic logging, monitoring, continuous deployment, and others as part of their solution.\nHigher resource availability\nThe cloud environment comes with a no-downtime promise that increases the availability of resources, in turn leading to better asset utilization and customer satisfaction.\nAt large volumes, the unit price of servers comes down noticeably in comparison with native data centers. The pay-as-you-use model provides the flexibility that companies seek to counter seasonal demand and scale up or down as required by the business.\nHow to Securely Migrate to the Cloud\nCloud Migration Strategies\nGartner’s 5R’s – Rehost, Refactor, Revise, Rebuild, and Replace – is a great starting point for deciding on a cloud migration strategy. Here is a quick synopsis:\nAlso called ‘lift and shift,’ rehosting is the use of infrastructure-as-a-service (IaaS). It’s about simply taking the existing data applications and redeploying them on the cloud servers. This works great for beginners, who are not yet accustomed to a cloud environment or for systems where code modifications are extremely difficult.\nAlso called ‘lift, tinker, and shift,’ refactoring involves making some optimizations and changes for the cloud and employing a platform-as-a-service (PaaS) model. Applications keep their core architecture unchanged, but use cloud-based frameworks and tools that allow developers to take advantage of the cloud’s potential.\nAdding another layer atop the previous two, this approach involves making architectural and code changes before migrating to the cloud. The objective is to optimize the application to take complete advantage of the cloud services, introducing major changes to the code. Advanced knowledge is required to implement this strategy.\nSimilar to revise in its big-bang approach, rebuild discards the existing code base in favor of a new one. For example, moving from Java to .NET. This is a time-consuming process and is only used when there is consensus that the existing solution does not suit the changing business needs and needs a revamp.\nMigrating to a third-party, vendor-based application from an existing native application is what this strategy is all about. The existing application data needs to be migrated to the new system, however, everything else will be new.\nWhile discussing and debating the 5 R’s, it’s also useful to ask – should a common strategy be used for all applications or different ones for different applications? The answer could lead down the path of a hybrid solution combining both cloud and on-premise.\n2018 Global Security Trends in the Cloud\nWhich Apps to Migrate and Which to Leave Native?\nThe first step is to determine which applications (if any) make more sense in-house. Circumstances will vary, but these apps may include certain databases, applications for managing internal processes, or other applications that have special sensitivity to your organization.\nThe opposite of leaving apps in-house is making them portable, or ready to be ‘dragged and dropped’ into a cloud architecture and set straight to work. Portable apps present many administrative advantages such as easier disaster recovery, scaled capabilities for geographic cloud regions, faster turnaround for bringing versions to market, and cost-leveraging new and existing providers.\nHowever, packaging older apps for portability involves a major reworking of the operating code, performed by highly-skilled software engineers who understand the legion of technical interactions taking place in a cloud environment.\nThe most common environment today is the hybrid cloud, which is a combination of native and portable apps working together on a platform that blends private internal networks with cloud-hosted services like Amazon’s AWS. These environments represent traditional and new networking challenges but also leverage the power of the cloud and its services to expand the reach and interaction capabilities of existing infrastructures.\nAnalyze carefully before undertaking the tricky path to cloud migration. This will save you endless frustration and costs down the line and build a strong cloud foundation from which to grow.\nCloud Migration Checklist\nBelow is a checklist of items that have extremely important ramifications for your cloud infrastructure today and on its future growth.\n1. Choose the Right Cloud Platform\nHow much raw storage will you need to host and properly backup your main databases? How much overhead will you need for hypervisors like those used in VMWare and Microsoft’s Azure environment? Build out the appropriate virtual workspace in advance and be selective when pricing cloud storage and hosting options.\n2. Check for Hardware Obsolescence\nMany existing network devices employ hardware acceleration to power through heavy traffic. Not all simulated devices—like virtual routers, switches, and load balancers—currently support hardware acceleration in the cloud. Audit your traffic and processor demands and trends before assuming virtual replacement devices will perform all the duties of existing native hardware.\n3. Research Licensing Issues\nNative networking environments of yesterday usually licensed software by the user, the device, or by enterprise. But the cloud changes these variables, hosting apps on machines with adjustable virtual processor counts and adding the ability to scale application services and servers. Factoring this impact on your licensing model could save huge sums of money.\n4. Mind Your SSLs and Certificates\nSecure socket layer certificates operate with precision to verify data routes and security. Changing a host location can throw your SSLs off track. It’s best to review where and how you use SSLs and prepare to renew or replace them before going live from a new hybrid cloud environment. Companies like DigiCert offer insight tools to assess your SSL chain.\n5. Audit IP Addresses\nIP addresses, usually statically or dynamically assigned and then forgotten about, take on another layer of challenge in the cloud. Old DHCP scopes and static addresses will likely change when moving to a cloud infrastructure, creating the need for a thorough IP address relationship audit prior to the move so that dependent sockets won’t be broken on migration.\n6. Evaluate Access Control List Dependencies\nAn access control list (ACL) regulates user and service activity across your network. As with IP addresses, migration will impact ACL dependencies. Customer traffic and system activities like backups, hypervisors, and monitoring will need to be reviewed for a shift to a full or hybrid cloud infrastructure to make sure everyone and everything can reach what it has to in the new design.\nChallenges to Traditional Cloud Computing\nPrepare for Audit\nAdvance planning and prep will make complying with audits much simpler.\nConsider all the interactions that must be logged, analyzed, and reacted to every time traffic hits your site. IP traffic history, user tracking data, threat detection, security intrusion attempts, anomalies, and more collide in terabytes of data. Any one string in that data could have deep implications on your business.\nUnderstandably, correctly analyzing all this raw data can be intimidating. Log analysis of that much data requires assistance and software solutions that assist with or even guarantee compliance are often smart investments.\nLog Analysis on the Microsoft Cloud\nKeep Compliance in Mind\nNot if, but when an event strikes your cloud environment, it is critical to run root-cause analysis and trace logs for culprit activity. It’s is an ever-evolving challenge, but a good compliance strategy addresses these requirements:\nAll data interaction between all machines, virtual and physical, must be recorded and compiled into one or more highly secure locations.\nOnce collected, requirements dictate that data must be demonstrated to be immutable. Safety requirements include heavy encryption and a hard audit trail for the ‘black box’ in which centralized logging data is locked.\nDaily infrastructure reviews\nA smart cloud model includes daily probes for threats and vulnerabilities. By simulating attacks, outages, and other crises, good teams can respond with agility or even prevent service interruption.\nClear and rigid data retention policies\nIndustry standards for keeping a full history of cloud activity vary from three months to 12 or more. It’s important to first define your policies, and then implement. Inconsistencies between policy and procedure can lead to big compliance trouble fast, so data retention is a critical focus area for audit preparation.\nCloud Migration Made Easy with Sumo Logic\nSumo Logic helps enterprises accelerate their cloud strategy by offering an automated, easy, and rapid development and deployment process for cloud-based applications. We have developed the first truly next-generation machine data analytics platform, delivered as a cloud-based service.\nSome of the features that our platform offers are:\nContinuous intelligence capabilities\nwith built-in advanced analytics that help uncover patterns and anomalies.\nA unified platform\nfor logs and metrics with the ability to analyze data, perform root-cause analysis, and monitor apps and infrastructure in real-time.\nto support rapid growth and cloud migration thanks to a multi-tenant architecture.\nand identification of any risks and threats within the cloud environment.\nWith the Sumo Logic platform, your company can truly innovate and grow.\nSumo Logic Log Reduce Demo\nCheck out the next generation of log analytics from Sumo Logic and learn how our technology can simplify your compliance as you migrate to the cloud."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:66bccea9-f41f-40fc-a7af-367562096562>","<urn:uuid:ab37899d-c15a-4e7d-abb2-ad555f6914de>"],"error":null}
{"question":"What is the relationship between concentration and enlightenment in Buddhist and yogic meditation practices?","answer":"In Buddhist practice, concentration (through Jhanas) is a means to enlightenment but is not enlightenment itself - it is part of Right Concentration on the Eightfold Path that leads to Nirvana. The practice involves progressively abandoning mental hindrances and cultivating specific mental states through four stages of Jhana. In yogic tradition, concentrated meditation in the form of Samadhi is itself considered the state of enlightenment - it represents the final limb of yoga where complete transcendence occurs. It is described as a state where consciousness becomes detached from the body and one experiences pure unadulterated bliss and wholeness.","context":["Jhana, The Rapturous State of Stillness in Buddhist Meditation Practice\nSummary: This is a comprehensive essay on Buddhist Jhanas, the meditative states experienced by the Buddhist monks during their practice of Right Concentration on the Eightfold Path\nBuddhism is a practical religion. Its philosophy and practice are rooted in the mundane lives of common people who are subject to transience and suffering. The Buddha presented his teachings in clear terms, and set achievable and verifiable goals so that there was little scope for ambiguity or speculation. Therefore, Buddhism sets itself apart from the speculative philosophies and complex theologies of other religions.\nFor the same reason, although you may find identical words in Hinduism and Buddhism, they may not carry the same meaning, intent or purpose. Hinduism is primarily focused upon the eternal Self (Atman) and Buddhism upon the objective reality or the not-Self (Anatma). Hence, they fundamentally differ in their interpretation of various spiritual concepts and experiences.\nFor example, liberation in Buddhism does not mean immortality. It only means liberation from the cycle of births and deaths. Buddhism does not believe in the eternality of anything, nor does it view the world as unreal or illusory. The illusion arises in the mind due to unwholesome thoughts, desires and expectations. Buddhism also acknowledges the existence of devas (gods) in heavenly realm, but does not consider them to be immortal or immutable.\nAnother important concept where Buddhism differs from others is Samadhi. Samadhi in Buddhism does not mean absorption in the eternal self, but absorption within the mind or entering the unified states of awareness in which neither any thought nor emotion nor any other modification of the mind remains active.\nTo achieve this state, Buddhism prescribes the attainment of Jhanas through the transformative practices such as right actions, right thinking, right awareness, right concentration, etc., on the Eightfold Path. In the following discussion we focus upon the importance of Jhanas in Buddhist meditative practices and their distinguishing features.\nJhana is the Pali equivalent of the Sanskrit word Dhayna. In Hinduism Dhyana refers to the practice of meditation or contemplation, which can be done with or without concentration. However, in Buddhism Jhana refers to both concentrated meditation and any meditative state which arises from it. In Buddhism, the practice of Jhana forms part of the Right Concentration on the Eightfold Path. By practising it one overcomes the hindrances of unwholesome and evil states of the mind and attains one pointedness, peace and happiness.\nThe practice of Jhana has a great significance in Buddhism. It is central to many Buddhist Practices. The Buddha constantly advised his followers to practice Jhanas to attain Nirvana. Jhana is also frequently mentioned in many Buddhist texts. They attest to its popularity and importance in Buddhist spiritual practices.\nThe Buddha identified four progressive states of Jhanas, which arise from the practice of Right Concentration and which lead to meditative absorption (Samadhi). He also explained their characteristic features and how to identify them and distinguish them. From the Buddhist perspective, Jhana (Concentrated meditation) is neither an abstract concept nor an aesthetic, mental exercise. It is a transformative practice, which is difficult but practicable. Regular practice of jhana leads to verifiable and distinguishable mental states, which can be discerned to know the progress one has made.\nThe Jhanas are attained by a two-pronged approach. One is by eliminating the unwholesome factors, which need to be removed to purify the mind such as negative thoughts and emotions, and the other is by cultivating factors that need to be acquired such as positive thoughts and states of mind. However, as we see later, when a monk advances into the higher Jhanas, he has to abandon even the positive aspects of the mind to end up with just one unified mental state in which one-pointedness, equanimity, discernment, sameness, etc., work in tandem.\nTo enter the first state of Jhana, the initiate has to begin the journey by abandoning the five hindrances (pañcanivarana) so that he can experience five positive states or modifications. These too he has to abandon as he advances into higher states, so that in the end he remains with nothing but sheer discernment (buddhi). The five hindrances are sensual desire (kama), ill will (vyapada), sloth and torpor ((thina and middha), restlessness and worry (uddhaca and kukkucca), and doubt (vicikiccha). Detailed description of the Jhanas is found many Buddhist texts. The Buddha himself described them to the monks for guidance. A brief description of the same is provided in the following section.\nThe first Jhana\nThe Buddha: “There is the case where a monk — quite withdrawn from sensuality, withdrawn from unskillful qualities — enters and remains in the first jhana: rapture and pleasure born from withdrawal, accompanied by directed thought and evaluation.”\nExplanation: The practitioner enters the first jhana when he wholly gives up the aforementioned five hindrances or unwholesome states of mind and cultivates the five “factors of possession” or the mental features that need to be cultivated. They are the directed thought (vitarka), evaluative thought (vichara), one pointedness (ekagrata), rapture (priti) and happiness (sukham). To attain this the practitioner has to follow a strict monastic discipline and live in a suitable dwelling place amidst likeminded people. It may be noted that vitarka and vicara are differently defined by different scholars. As their names suggest vitarka is associated with controlled thought, and vicara with analysis and evaluation. Together, they represent the whole thought process.\nThe second Jhana\nThe Buddha: \"Furthermore, with the stilling of directed thought and evaluation, he enters and remains in the second jhana: rapture and pleasure born of composure, unification of awareness free from directed thought and evaluation — internal assurance.”\nExplanation: When he is fully stabilized in the first Jhana and perfected the five factors of possession, with mastery in strengthening, attaining and sustaining those factors (which may involve considerable time), the practitioner qualifies to enter the second Jhana. In this stage, he has to abandon both types of thought so as to silence the mind and free it from all thoughts, and remain stabilized in the remaining three factors of possession namely rapture, pleasure and one pointedness. With the mind freed from thoughts, with unified and one-pointed awareness, and with the feelings of assuredness, composure, confidence and tranquility which arises from it, he experiences rapture and pleasure as they permeate and pervade his whole body. Thus, in this Jhana out of the five factors of possession, only three are retained.\nThe third Jhana\nThe Buddha: “And furthermore, with the fading of rapture, he remains in equanimity, mindful and alert, and physically sensitive to pleasure. He enters and remains in the third jhana, of which the Noble Ones declare, 'Equanimous and mindful, he has a pleasurable abiding.'”\nExplanation: After mastering the second Jhana in different ways and having succeeded in attaining, strengthening, sustaining the three factors of it, the practitioner is now ready to practice and enter the third Jhana. In this state, he comes to realize that rapture is also a form of unwholesome disturbance only and needs to be abandoned or eliminated to advance further into the deeper states of stability and tranquility. Thereby, he cultivates indifference towards it. To achieve mastery in its practice, he resorts to mindfulness (sati), sameness or equanimity (upeksha) and discernment (samprajnata). Thus in this state, out of the five factors of possession, only two are retained and the rest are abandoned as unwholesome.\nThe Fourth Jhana\nThe Buddha: \"And furthermore, with the abandoning of pleasure and stress — as with the earlier disappearance of elation and distress — he enters and remains in the fourth jhana: purity of equanimity and mindfulness, neither-pleasure-nor-pain.”\nExplanation: The Fourth Jhana begins with the realization that one cannot completely abandon rapture without abandoning happiness. Since both are interlinked and have close affinity, the initiate realizes that since it is difficult not to experience rapture when one is happy, he is vulnerable to the risk of falling back to the previous Jhana and remaining stuck there. Further, he also realizes that abiding in prolonged happiness may lead to habitual clinging, which in itself is an unwholesome hindrance. Therefore, contemplating upon the state of equanimity or sameness and abandoning happiness along with its accompanying physical and mental feelings, he enters the four Jhana. With the neutral feeling of sameness firmly established in his mind in the place of happiness, with peace and stability reigning his mind, he abides in unified awareness more than ever. Having abandoned happiness, pleasure and pain, he practices unwavering concentration. Thus, in this Jhana, he retains only one pointedness, with equanimity and pure mindfulness (parisuddha sati), and abandons the remaining factors of possession as gross and unwholesome.\nThe subtle Jhanas\nThe Jhanas are usually four, since it is not practically possible to abandon one pointedness also and practice discernment. Once the four Jhanas are attained, there is nothing else to be abandoned or attained. The practitioner’s unified mental state becomes stabilized, without the risk of reversal or falling back. He cannot easily be disturbed or tempted with unwholesome thoughts and distractions.\nHowever, it is not the end of the road. The Buddha described the fifth state of concentration as the practice of one pointed concentration which is refined, improved and perfected with discernment. It arises from the attainment of the four Jhanas. The Buddha also spoke about the supernatural powers that arise from the perfection one achieves in the fivefold practice of Right Concentration.\nApart from the four Jhanas, the Buddhist texts also refer to four subtle Jhanas or non-material Jhanas, which are named after their respective objects of concentration. They are, mindfulness of space (akasa chetana), mindfulness of consciousness (Vijnana chetana), mindfulness of emptiness (akincana chetana), and mindfulness of neither perception nor non-perception (nevasaññana saññayatana). Some describe them as variations or modes of the fourth Jhana.\nFrom the names ascribed to them, it becomes clear that in these Jhanas the objects of the concentration are fixed, unlike in the previous Jhanas where one may choose different objects to practice concentration. The factor of possession, one-pointedness, also remains constant since it cannot be abandoned. The four objects of concentration are also progressively subtler. For example, consciousness is subtler than space, emptiness subtler than consciousness, and that which is neither perception nor non-perception is even more subtler than emptiness. By practising concentration on the subtle objects, one reaches the end of the objective world, the not-Self and attains the indescribable state of Nirvana.\nSuggestions for Further Reading\n- Right Concentration on The Eightfold Path of Buddhism\n- What is Dhyana? Definition and Significance\n- Right Mindfulness on The Eightfold Path of Buddhism\n- The Meaning and Practice of Mindfulness\n- Buddhism - Objects of Meditation and Subjects for Meditation\n- Buddhism - Right Concentration\n- Right Mindedness Or Right Resolve\n- Meditation on Anicca or Impermanence in Buddhism\n- Awakening and Enlightenment in Buddhism on the Path to Nirvana\n- Is Buddhism a Spiritual Religion?\n- Concentration and Mindfulness Meditation\n- Eight Realizations of the Great Beings\n- An Analysis of Hindu Buddhist Meditation Techniques\n- Buddhism - Right Concentration\n- The Buddhist Meditation\n- Buddhist Meditation and Depth Psychology\n- Buddhism: A Method of Mind Training\n- A Modern Treatise on Buddhist Satipatthana Meditation\nTranslate the Page","Samadhi is the 8th and final limb of yoga. Samadhi is a state of concentrated meditation that transcends the intellect, mind, and body and complete detachment from the physical world (meaning consciousness becomes detached from the body). This final stage of yoga is also known as enlightenment and can be achieved in Corpse Pose, after meditation involving Dharana and Dhyana. In this state, the yogi can suspend consciousness away from the body, being at one with the environment and surroundings while not being limited to physical restraints of the body. Samadhi represents a state of enlightenment and over time the yogi obtains a ceaseless state of transcendent bliss.\nIn Buddhism, Samadhi is known as the 8th wheel of the eightfold path referring to right concentration. Buddhists believe that this right concentration leads to extraordinary intelligence and even superpowers. But these are simply distractions for the practitioner from the goal of Moksha, or liberation. Samadhi leads to a pleasantness in your current life, knowledge of the divine third eye by concentration on light, clear comprehension of the fluctuations of feelings, perceptions, and thoughts through mindfulness, and the elimination of the 5 Skandha’s (attachments to matter, sensation, perception, mental habits, and discernment). In Buddhism, Samadhi does not refer to enlightenment, rather a state of concentrated meditation that leads to enlightenment. Nirvana is enlightened freedom from attachment and Samsara through Moksha.\nSamadhi is a state of supreme detachment, where consciousness is free to leave the body and can expand beyond the borders of the physical corpse of the consciousness. It is a supreme state of bliss that is experienced in Savasana, or in meditation after a yoga practice is completed. This is why you don’t skip Savasana! Meditate after your yoga practice, it is far more powerful after the body has been tempered. The sensations and insights that flow during these meditation can alter your perspective and even mental processes that can change. It is integral to the yoga practice to rest in Savasana and meditate; they are the most important things you can do to amplify the healing and regenerative qualities of yoga.\nSamadhi is intricately related to consciousness. It can be described as full awareness, perfect concentration, or an altered state of consciousness characterized by ananda and sukha (bliss and joy). Vyasa, one of the authors of the Mahabharata, said ‘yoga is Samadhi’. It is ultimately complete control over the fluctuations of consciousness including distractions and normal functionality of the nervous system and conscious experience.\nPatanjali said that Samadhi has three different aspects: Savikalpa, Asamprajnata, and Nirvikalpa. In Savikalpa the mind is still conscious and the imagination is active and the state can be described as holding onto the imagination with effort. Asamprajnata is a step forward from Savikalpa and is not quite gross awareness, but is a heightened state of conscious awareness. Nirvikalpa is the highest transcendent state of consciousness, the highest of the heights of yoga. It is an engrossing awareness where all things are one and pure unadulterated bliss, wholeness, and perfection are experienced. It is pure joy, freedom, and steady bliss in the knowledge of awareness.\nSamadhi is like balancing blocks on top of one another, where it takes years to learn all of the nuances of each block and how they work together. Simply allowing the body to meditate is not enough; full concentration and focus is required to obtain the state of pure freedom.\nThe final liberation of the yogi comes at the time of death, known as mahasamadhi and is a controlled exit of the consciousness from the body to merge consciousness with the divine. Maha means great.\nI would like to dedicate this post to BKS Iyengar, who died this morning, one of the greatest (yoga) teachers the world has ever known. My hope is that he found mahasamadhi in his last hours and that he has found the freedom and peace beyond. He brought yoga into the west and gave everyone seemingly limitless knowledge on even the most minuscule and minute details. He gave us in the west the opportunity to scale the heights of Raja yoga and changed the world for the better. Thank you."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:98346f1f-031c-4cda-bc0b-ef67e58bf114>","<urn:uuid:c198a1dd-6ba4-49c6-b875-26176848e6be>"],"error":null}
{"question":"What kind of fungal expertise do Dr. Samir Agrawal and Dr. Vugia have in common?","answer":"While both doctors work with fungal infections, they have different areas of expertise. Dr. Agrawal specializes in invasive fungal infections in haemato-oncology, focusing on developing diagnostics, rapid fungal biomarkers, and imaging probes for pathogen identification. Dr. Vugia, as chief of the Infectious Diseases Branch at the California Department of Public Health, focuses on Valley Fever (coccidioidomycosis), a specific fungal infection caused by breathing in Coccidioides spores from soil in endemic areas. Both are involved in public health aspects of fungal infections, but with different specializations and geographical focuses.","context":["Haemato-oncology Bacterial Infections\nThank you for taking the time to view this webinar, we hope you find it informative.\nRCPath CPD accredited\nCo-presenters: Dr Louise Sweeney, Dr Jonathan Lambourne, Dr David Partridge and Dr Samir Agrawal\n– – – – –\nIf you require a certificate of attendance, details of how to obtain will appear at the end of the webinar.\nPlease note that once you have completed your details on the form, the certificate will be generated so please be ready to download your certificate immediately. We would like to inform delegates that each time a certificate is generated, we will be notified with details.\nIf you have any problems creating or downloading your certificate please contact Esme Carruthers at email@example.com\nThis webinar was supported by an unrestricted educational grant from Shionogi.\nSpeakers have not been paid for their participation.\nHaemato-oncology Bacterial Infections\nThis course is designed to provide an overview of some of the commonly identified infection issues that occur in haemato-oncology. Experts will discuss how best to manage these issues patients using examples and case studies.\nTo access the course please click on the button below.\nThis will open the log in page in your default browser, all you will need to do is input an email and make a password, then you will be automatically redirected to the course.\nIf you have any issues with accessing the course (or any queries at all during the feedback process) please contact firstname.lastname@example.org Course\nDr Louise Sweeney, Consultant Medical Microbiologist, Liverpool\nSpecialist interests include infection in Haematology and Transplant patients, antimicrobial susceptibility testing and the approach to clinical management of multi-drug resistant infections, particularly those caused by Gram negative bacteria. Non-clinical interests include training and education of clinicians and allied professionals within and without the Infection Specialties.\nDr Jonathan Lambourne, Consultant in Infectious Diseases & Microbiology, Barts Health NHS Trust\nJonathan Lambourne is consultant in Infectious Diseases & Microbiology at Barts Health NHS, London UK. He undertook medical training at Cambridge and University College London, and his specialist training in Dublin and London. He completed a PhD at St George’s University of London, investigating immune determinants of susceptibility to invasive Aspergillosis and Staphylococcus aureus infection. His main clinical interest is infections in patients with primary and secondary immunodeficiency and the application of diagnostic techniques in this challenging patient population.\nDr David Partridge, Consultant Microbiologist, Sheffield Teaching Hospitals\nDavid Partridge is a Consultant Microbiologist and Infection Control Doctor at Sheffield Teaching Hospitals and Honorary Senior Lecturer at the Florey Institute, University of Sheffield. He has an interest in infections of the immunocompromised host and device related infection. During the COVID-19 pandemic, he has been working on clinical utility of sequencing of SARS-CoV-2 and also on management of secondary infections.\nDr Samir Agrawal, Senior Lecturer & Honorary Consultant, Barts Health NHS Trust & Blizard Institute, Queen Mary University of London\nResearch Interest: Infection management in Haemato-Oncology with. focus on invasive fungal infections; management pathways; diagnostics – development and clinical implementation of rapid fungal biomarkers; online database for (national) audit of infection, specifically fungal infection in hospital settings; developing probes for imaging for pathogen identification using CT-PET)\nGroup Expertise: Clinical fungal management, biomarkers for invasive aspergillosis, gallium (68Ga) labelling of fungal-specific probes for imaging; Fungal Audit Tool (for stewardship) F.A.T.S. – pilot project across England.\nESPAUR antifungal committee;\nNHS England antifungal stewardship programme;\nCI of grant submission for a national RCT of AFS vs empirical fungal management","Dr. Duc Vugia, chief of the Infectious Diseases Branch in the California Department of Public Health, discusses Valley Fever. Created: 2/4/2014 by National Center for Emerging and Zoonotic Infectious Diseases (NCEZID).\nDate Released: 2/5/2014. Series Name: Emerging Infectious Diseases.\n[Announcer] This program is presented by the Centers for Disease Control and Prevention.\n[Stephen Monroe] Hello, I’m Dr. Stephen Monroe, and today I’m talking with Dr. Duc Vugia, chief of the Infectious Diseases Branch at the California Department of Public Health. Our conversation is based on his study about Valley Fever, which appears in CDC's journal, Emerging Infectious Diseases. Welcome, Dr. Vugia.\n[Duc Vugia] Thank you, Dr. Monroe.\n[Stephen Monroe] EID has done some other articles about coccidioidomycosis but I think it’s still an infection most people don’t know anything about. Tell us what it is and how you get it.\n[Duc Vugia] Coccidioidomycosis, also known as Valley Fever, is an infectious disease caused by a fungus called Coccidioides. This fungus lives naturally in the soil in parts of the southwestern United States, Mexico, and Central and South America. People get infected from breathing in the fungal spores, which are seed forms too small to be seen, probably when they’re outdoors in an endemic area, meaning an area where this fungus is often found. Most of the time, this illness is mild and gets better on its own. On rare occasion, however, it can be severe and even fatal.\n[Stephen Monroe] How is Valley fever different from other fungal infections, such as the one people got from the cortisone spine injections and from yeast infections?\nBroader control of fungal exposures in the community can also be improved by awareness, especially education regarding high-risk practices and activities. Outbreaks of histoplasmosis linked to construction and cleaning activities in places contaminated with bird or bat guano have led to production of educational materials describing how risk can be mitigated. Furthermore, recent advances in whole-genome sequencing are being exploited to suggest novel vaccine and diagnostic targets for the agent of Valley fever.\n[Duc Vugia] Well, like bacteria and viruses, there are many different types of fungi that are in the environment and even some that we carry on our bodies. Yeast infections are usually caused by Candida albicans, a fungus that many of us carry harmlessly on our skin and mucous membranes. If a person’s immune system changes or is weakened, this yeast can overgrow and cause a yeast infection. And when a fungus gets into our body through a wound or through an injection, such as with the recent unfortunate outbreak from injecting contaminated steroids, then a person can become very sick with a fungal infection. Valley Fever, however, is different from those two settings in that the spores of this fungus sometimes get into the air when the soil in an endemic area is disturbed by wind or activities like digging and then people nearby get infected from breathing in the dust and these spores.\n[Stephen Monroe] People seem to be aware that mold is bad for them but think fungus is something like mushrooms.\n[Duc Vugia] Molds, yeasts, and mushrooms are all types of fungi. They may differ in size, shape, and other ways, but there are some molds, yeasts, and mushrooms that can make people sick.\n[Stephen Monroe] Dr. Vugia, tell us how Valley Fever affects a person.\n[Duc Vugia] About 60 percent of people infected with Valley Fever have little or no noticeable symptoms. The people who do get sick usually develop a flu-like illness 1 to 3 weeks after exposure to the fungus. Common symptoms include cough, fever, chest pain, headache, muscle aches, fatigue, and a rash. What is notable is that these symptoms can last a month or more; the fatigue can last several months. Then most sickened people usually recover and are protected from being infected again by developing immunity to the fungus.\nIn some people, the infection becomes more severe, and can spread outside the lungs to the brain, joints, bone, skin, or other organs. When the infection spreads outside the lungs, this is called disseminated Valley Fever. This form of the disease is rare but can be very serious and could be fatal.\n[Stephen Monroe] In what regions of the country are people most likely to get Valley Fever?\n[Duc Vugia] In the U. S., the southwestern region, including the states of Arizona, California, Nevada, New Mexico, and Utah have reported increased cases of infection with Valley Fever since 1998. Of the total number of cases reported to the Centers for Disease Control and Prevention during this time, 66 percent were from Arizona and 31 percent were from California. In California, most of the endemic counties are in the California Central Valley, and by the way, that’s where Valley Fever got its name.\n[Stephen Monroe] Is there treatment for Valley Fever? I know it can be hard to treat.\n[Duc Vugia] Yes, there are antifungal medications to treat Valley Fever, but some people with mild infections may get better without medication. The severe or disseminated infections can be difficult to treat but antifungal medications can be effective. For some people with Valley Fever meningitis or brain infection, they may need to take these medications for the rest of their life. It’s important for patients to follow up and work with their doctor to make sure that their Valley Fever infection is treated appropriately.\n[Stephen Monroe] Are some people more likely to get Valley Fever?\n[Duc Vugia] Anyone can get Valley Fever after breathing in this fungus. For people living, working, or traveling in areas where the fungus is in the soil the risk of infection is present. In these areas, activities that disturb the soil and bring up dust would put people nearby at higher risk of infection.\nSome people are at higher risk for severe or disseminated disease after they get infected. These “at risk” groups include people of certain racial or ethnic backgrounds, including African Americans, Filipinos, or Hispanics; older adults, aged 60 and older; pregnant women; people with diabetes; and people with weakened immune system, such as those with cancer or HIV AIDS.\n[Stephen Monroe] Your study looked at Valley Fever-associated hospitalizations. What did you find? How many of these people died?\n[Duc Vugia] We looked at California hospitalization data from 2000 through 2011. We found over 15,000 patients have been hospitalized with a diagnosis of Valley Fever during this time. Some were hospitalized more than once for a total of over 25,000 hospitalizations. The rates of patient hospitalization per year increased approximately two-fold between 2000 and 2011. And the total charges for hospitalizations during this time were over 2 billion dollars. Eight percent of the patients died during their hospitalizations.\n[Stephen Monroe] Why did the number of hospitalizations increase since 2000? Is the problem of Valley Fever getting worse?\n[Duc Vugia] We don’t know why the numbers of reported infections and hospitalizations with Valley Fever have increased since 2000. We have seen an epidemic increase before in the California Central Valley between 1991 and 1994 and we didn’t know why at that time either. But the number of infections came down to a baseline by 1995, and we’re hoping that the recently increased number of Valley Fever cases will also decrease.\nThere are some factors that might have contributed to the increase, including climate or rainfall factors, increasing activities disturbing the soil in the endemic areas, increasing number of people moving into these areas, and increasing number of people at risk for severe disease. It’s just not yet clear why the numbers of infections and hospitalizations with Valley Fever have increased in this recent time.\n[Stephen Monroe] Are people immune after infection? Can people get Valley Fever again and again?\n[Duc Vugia] After a person is infected with the Valley Fever fungus, whether with or without noticeable symptoms, that person develops some immunity to Valley Fever. This immunity is usually lifelong, meaning that people who have been infected with Valley Fever don’t usually get it again a second time. In rare instances where a person’s immunity is weakened, such as in some older adults or with some underlying medical conditions, a person can have Valley Fever again. Overall however, people are protected from Valley Fever once they have been infected and recovered.\n[Stephen Monroe] Is there a way people can tell if they’ve had it or not and if they’re immune?\n[Duc Vugia] There is a skin test that can show if a person has had Valley Fever in the past. This skin test may be commercially available soon.\n[Stephen Monroe] Dr. Vugia, according to your paper, options for prevention are limited. Is a vaccine one of the few options?\n[Duc Vugia] Options for preventing Valley Fever are indeed limited because the fungus exists widely in the soil in large areas and it is not possible to get rid of it in the soil. And there is no vaccine available yet. Some scientists have been working toward developing a Valley Fever vaccine. When a safe and effective vaccine is available, it would be a good option for preventing Valley Fever.\n[Stephen Monroe] So without a vaccine are there any other strategies for prevention?\n[Duc Vugia] Yes, the main precaution is to try to avoid breathing in dusty air in areas where Valley Fever is endemic. People living, working, or traveling in endemic areas can take some common sense precautions to decrease their risk of infection. For example, when it is windy outside and the air is dusty, stay indoors and keep windows and doors closed, or if driving, keep the car windows shut and use “recirculating” air conditioning. Before digging up dirt, wet the soil first to keep dust down.\nIn dusty situations, one can also consider wearing an N95 mask, that’s the letter N and the number 95. The N95 mask is also known as an N95 respirator. These masks can be bought in most hardware stores. However, they must be worn correctly to keep out dust; information on how to correctly use an N95 mask is available online.\n[Stephen Monroe] Is there anything else that can be done to address the Valley Fever problem?\n[Duc Vugia] Yes there is. The public should be more aware of Valley Fever, especially if they live, work, or travel in an endemic area. For people who think that they may have Valley Fever symptoms after being in an endemic area, they should see their doctors.\nDoctors in and outside the endemic areas should also be more aware of this fungal disease and consider it in patients with compatible symptoms. Early diagnosis and appropriate clinical management are important for patients with Valley Fever. There are still many features about this infection that we don’t fully understand. We would like to know more so that we can address the Valley Fever problem better, so more research is needed.\n[Stephen Monroe] Thank you, Dr. Vugia. I’ve been talking with Dr. Duc Vugia about his study, Coccidioidomycosis-associated Hospitalizations, California, USA, 2000-2011, which appears in the October 2013 issue of CDC’s journal, Emerging Infectious Diseases. The article is available at www.cdc.gov/eid.\nIf you’d like to comment on this podcast, send an email to firstname.lastname@example.org. I’m Dr. Stephen Monroe, for Emerging Infectious Diseases.\n[Announcer] For the most accurate health information, visit www.cdc.gov or call 1-800-CDC-INFO."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:15590c4d-d339-4544-b4c2-4937feffe929>","<urn:uuid:51e807ab-905a-40a0-bcce-1ae4c057ec37>"],"error":null}
{"question":"How do the oversight mechanisms compare between codified law and ethical guidelines in business contexts?","answer":"Codified law and ethical guidelines have distinct oversight mechanisms in business contexts. Codified law involves values and standards written into the legal system that are enforceable in courts, such as requirements for car licenses or corporate taxes. In contrast, ethical guidelines, while lacking legal enforcement, are governed through organizational mechanisms like codes of ethics, corporate culture, and oversight bodies such as boards of directors. For example, the IDEA Foundation's board holds responsibility for implementing and monitoring ethical standards, while following principles like UN-PRI and the UN Global Compact, creating a system of self-regulation rather than legal enforcement.","context":["Ethics – the code of moral principles and values that govern the behaviors of a person or group with respect to what is right or wrong.\nEthics is difficult to define in a precise way. In a general sense, ethics is the code of moral principles and values that governs the behavior of a person or group with respect to what is right or wrong. Ethics set standards as to what is good or bad in conduct and decision making . Ethics deal with internal values that are a part of corporate culture and shapes decisions concerning social responsibility with respect to the external environment. An ethical issue is present in a situation when the actions of a person or an organization may harm or benefit others.\nEthics can be more understood when compared with behaviors governed by laws and by free choice . Exhibit illustrates that human behavior falls into three categories . The first is codified law, in which values and standards are written into the legal system and enforceable in the courts. In this area, lawmakers have ruled that people and corporations must behave in a certain way, such as obtaining license for cars or paying corporate taxes. The courts alleged that Enron executives broke the law for examples by manipulating financial results, such as using of balance sheet partnerships so improperly to create income and hide debt. The domain of free choice is at the opposite end of the scale and pertains to behavior abut which the law has no say and for which an individual or organization enjoys complete freedom. A manager’s choice of where to eat lunch or a music company’s choice of the number of CDs to release are examples of free choice.\nBetween these domain lies the area of ethics. This domain has no specific laws, ye it does have standards of conduct based on shared principles and values about moral conduct that guide an individual or company. Executives at Enron, for example did not break any specific laws by encouraging employees to buy more shares of stock even when they believed the company was in financial trouble and the price of the shares was likely to decline. However, this behavior was a clear violation of the executives’ ethical responsibilities to employees . These managers were acting based on their own interests rather than their duties to employees and other stakeholders. In the domain of free choice obedience is strictly to oneself. In the domain of codified law, obedience is to laws prescribed by the legal system. In the domain of ethical behavior, obedience is to unenforceable norms and standards about which the individual or company is aware. An ethically acceptable decision is both legally and morally acceptable to the larger community.\nMany companies and individuals get into trouble with the simplified view that choices are governed by either law or free choice. It leads people to mistakenly assume that if it’s not illegal, it must be ethical as if there were no third domain. A better option is to recognize the domain of ethics and accept moral values as a powerful force for good that can regulate behaviors both inside and outside corporations. As principles of ethics and social responsibility are more widely recognized companies can use codes of ethics and their corporate cultures to govern behavior, thereby eliminating the need for additional laws and avoiding the problems of unfettered choice,\nA situation that arises when all alternative choices or behaviors have been deemed undesirable because of potentially negative consequences making it difficult to distinguish right from wrong.\nSource: New Era Management","Ethical guidelines for IDEA Foundation\nEthical guidelines and values - IDEA FoundationBackground and definitions\nA code of ethics is a set of principles of conduct within an organization that guide decision making and behavior. The purpose of the code is to provide members and other interested persons with guidelines for making ethical choices in the conduct of their work. Professional integrity is the cornerstone of credibility in a working community. Member of an organization adopt a code of ethics to share a dedication to ethical behavior and adopt this code to declare the organization's principles and standards of practice.\nViews on ethical considerations have changed greatly in recent years and are still evolving. Idea Foundation will work hard to meet our own and our stakeholders expectations to be at the forefront of this development. Establishing and implementing ethical standards and values for Idea will therefor be an ongoing and dynamic process, constantly under revision and improvement. The clear responsibility for this process lies with the board of directors. The board also holds the ultimate responsibility for implementation and monitoring of activities conducted by the Foundation it self, its partners, stakeholders and projects, supported by the Foundation.\nIdea Foundation is a non-profit organization and should not be compared with a traditional investor, maximizing his returns. Our ambition is never the less to influence and contribute to development of sustainable growth and prosperity through out all the projects and geographical areas where we are involved. To be able to successfully reach our long term ambitions The board of directors with Idea Foundation have set ethical standards, that as a minimum must meet the principles of ethical investment management, including endorsement of the UN principles for responsible investment (UN-PRI).\nThe UN Global Compact asks companies to embrace, support and enact, within their sphere of influence, a set of core values in the areas of human rights, labor standards, the environment, and anti-corruption:\nPrinciple 1: Businesses should support and respect the protection of internationally proclaimed human rights; and\nPrinciple 2: make sure that they are not complicit in human rights abuses.\nPrinciple 3: Businesses should uphold the freedom of association and the effective recognition of the right to collective bargaining;\nPrinciple 4: the elimination of all forms of forced and compulsory labor;\nPrinciple 5: the effective abolition of child labor; and\nPrinciple 6: the elimination of discrimination in respect of employment and occupation.\nPrinciple 7: Businesses should support a precautionary approach to environmental challenges;\nPrinciple 8: undertake initiatives to promote greater environmental responsibility; and\nPrinciple 9: encourage the development and diffusion of environmentally friendly technologies.\nPrinciple 10: Businesses should work against corruption in all its forms, including extortion and bribery.\nIn addition to the The Ten Principles of the UN Global Compact, the Idea Foundation has established a set of core values to be followed in all aspects of our behavior and in our relationships internally and externally. Idea Foundation has a clear ambition to take an active stand in implementing the same core values in all our projects and businesses established and developed on the basis of support from Idea Foundation.\nWe want to be a leader in every aspect of our involvement and our business. In the development of our team leadership skills at every level; in our management performance; in the way we design, build, and support our products and services; and in our financial reporting.\nWe are committed to practice the highest ethical standards, and by honoring our commitments. We will take personal responsibility for our actions and treat everyone fairly, with trust and respect.\nWe will strive for continuous quality improvement in all that we do, so that we will rank among the best in customer, employee, and community satisfaction.\nPeople working together\nOur strength and our competitive advantage is and always will be – people. We will continually learn, and share, ideas and knowledge. We will encourage cooperative efforts at every level and across all activities in our company.\nGood corporate citizenship\nWe will provide a safe workplace and protect the environment. We will promote the health and well-being of people and their families. We will work with local communities to support education and other worthy causes.\nFundamental equal rights and respect for all individuals Equal rights offering same opportunities to all, despite, race, religion, sex.\nThe Foundation will support business ideas and projects that can document a long term sustainable development.\nCorporate Social Responsibility\nNo matter how small a business is it can have a strong belief in social responsibility.\nThe ethical guidelines for the Idea Foundation are promoted through the following four measures:\nExercise of power to support proposed and established projects\nExclusion of companies and projects\nObservation of companies and projects\nThe role of the board and the investment committee is to provide evaluation on whether or not investment and support in specified companies and projects is inconsistent with the established ethical guidelines and The UN Global Compact. This work shall be based on the following principles:\nHonesty and Integrity: We act with honesty and integrity.\nProfessional Behavior: We operate within the letter and the spirit of applicable laws.\nCompetence: We apply appropriate skills and capabilities to every\ndecision we make\nObjectivity: We are objective in forming our professional opinion\nConfidentiality: We respect the confidentiality of information.\nFair Business Practices: We are committed to fair business practices.\nResponsibility to Society: We recognize and respect the impact we have on the world\nRespect and Fair Treatment:We treat all our colleagues with respect, courtesy and\nAccountability and Decision-Making: We lead by example, using our Core Values as"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:5adbcc58-a9c9-4dc5-bd47-668d5fb4738a>","<urn:uuid:10159761-6257-4e01-b061-4205ae03e2e1>"],"error":null}
{"question":"I'm researching Anglo-Saxon swords - were they really two-handed weapons as some sources claim?","answer":"No, Anglo-Saxon swords were actually single-handed weapons. Archaeological evidence shows that these swords had surprisingly small grips, often barely large enough for one hand. Moreover, these swords were typically used with a shield, which would have made two-handed use impossible. Physical experimentation with replica swords based on archaeological specifications demonstrates that only a child would need two hands to swing an Anglo-Saxon sword or similar Migration period Germanic sword based on the Roman spatha. The swords were effectively usable with just one hand, even for powerful strikes.","context":["by Antti Ijäs\nThe study of historical combat arts is comprised of archaeology, philology and practical experimentation. All of these domains have their own methodology and they provide answers for markedly different questions. On one hand, archaeology tells us what kinds of weapons existed at a given time period; on the other hand, philology gives us an idea of how they were used based on textual evidence of historical or instructional nature. Naturally, damage to weapons, armour and bones uncovered by archaeologists will also provide important data on how different armaments were used.\nExperimentation and actual physical exercise, which in this context could also be termed “experimental archaeology”, should be used to verify the plausibility of using the weapons and armour in the assumed way. Sometimes insufficient knowledge and lack of practical experience can result in rather strange conclusions, which could in the worst case be perpetuated in subsequent literature, thus contributing to the ever-growing corpus of common misconceptions. In this essay, I will give an example of the importance of synthesizing knowledge by pointing out an error in the article “Archaeology and Beowulf” by Leslie Webster originally published in an edition of Beowulf (Mitchell & Robinson 1998: 183-194) and later reprinted with the Present-Day English verse translation by Seamus Heaney (Donoghue 2002: 212-236). This essay is loosely based on my earlier off-the-cuff blog post “Anglosaksien miekoista, lähdeviittaamisesta ja relatiivipronominista” (Ijäs 2014), in which I also discuss the only vaguely related topic of Old English relative pronoun.\nIn the natural sciences, experiments must be documented so that they may be replicated by others. Similarly, even in the most humanistic kinds of historical studies, one should indicate the data used so that others may review it and follow the reasoning used to draw the conclusions presented. There are essentially two distinct kinds of referencing in scientific writing. The first kind entails giving a source for the data used for drawing specific conclusions. To give an example from linguistics, this means that instead of claiming that one expression is used more often than the other based on personal assumption (or “reflection”), one would refer to a specific text corpus where the instances of one expression outnumber those of the other. The validity of such a statement is then subordinate to the quality (or, better said, quantity) of the corpus used. Citing a secondary source also falls under this category, since eventually the trail will lead to the primary source. The second kind of referencing is purely ethical and entails giving credit to previous research for having made certain conclusions. Failing to do this can be construed as plagiarism and is punished by the most severe methods available to the academic community. From a methodological point of view, this kind of referencing is, strictly speaking, irrelevant, for a logical conclusion is no less valid no matter who presents it or takes credit for it. Giving credit where credit is due is, however, of utmost importance for the social dynamics of the academia.\nIn the worst case scenario, an innocuous offhand reference to an authority generally considered to be credible might lead to the accumulation of misinformation mentioned earlier. This is especially true of monographs and articles that are likely to be read by a wide audience of students or researchers. However, if referencing to primary sources is done properly, or if at least the trail of references does not end before primary sources, there exists at least the possibility for checking the facts.\nThe purpose of this essay is, ultimately, to provide the reader with an example of an (arguably minor) scholarly lapse and the method of critical reading necessary to correct it. Below is the relevant quotation from Webster’s article (1998: 191):\nThey [the swords] are archetypally the ealde lāfe, powerful heirlooms to be prized for their power and might; some are the work of marvellous smiths (l. 1681), or eotenisc (l. 1558), the work of giants long ago; they may have names, or carry owner inscriptions and images which tell of the mythic past (ll. 1688-98). They have richly decorated hilts (l. 1698) and bear twisting and branching patterns (wyrmfāh l. 1698, ātertānum fāh l. 1459); their iron blades are fearsome double-edged weapons, which need two hands to swing them (l. 1461). Supporting archaeological evidence for much of this has long been recognized.\nLine numbers refer to the primary source, i.e. Beowulf, or, more precisely, Mitchell’s and Robinson edition (1998) of Beowulf the article was published with. Interestingly, a footnote for line 1461 in the text proper refers to the passage quoted above, effectively lending Webster’s claim the authority of the said editors. As for “supporting archaeological evidence”, a passim reference is given to a secondary source, in this case H. R. Ellis Davidson’s book The Sword in Anglo-Saxon England: Its Archaeology and Literature (1962).\nFor anybody familiar with the Anglo-Saxon or similar Germanic swords (though attested in a single late 10th to early 11th century Old English manuscript, the story of Beowulf is set in Scandinavia, not England), the idea that their use would require two hands should strike as rather peculiar. According to the article, however, this proposition seems to be supported by both textual evidence and archaeological findings. To be fair, Webster does not specify which details are supported by archaeological evidence, but weight and handedness are certainly such basic and concrete qualities of a sword that they would be among the first characteristics inferred from surviving specimens.\nI will start my analysis from the reference to the primary source. Below are lines 1459 to 1463 describing Unferth’s sword Hrunting with a literal translation:\necg wæs īren, ātertānum fāh\nāhyrded heaþoswāte; nǣfre hit æt hilde ne swāc\nmanna ǣngum þāra þe hit mid mundum bewand\nse ðe gryresīðas gegān dorste,\n‘the edge was iron, with poison-twigs decorated,\nhardened with battle-sweat; never had it in battle betrayed\nany man who grasped it with hands,\nwho dared to go on perilous expeditions\nto the enemies’ meeting place.’\nWebster seems to assume that the plural mid mundum ‘with hands’ refers to using the sword exclusively with both hands. It should be noted that even though the antecedent manna ǣngum ‘any man’ (lit. ‘of-the-men any’) could arguably be either singular or plural, the verb bewand is singular, so it is indeed obvious that two hands of a single man are meant. But nevertheless, since we are dealing with poetry, two things should be considered. First, it could very well be that the poet has chosen words based on acoustic qualities such as rhythm, in this case bewand instead of the trisyllabic plural bewundon. Secondly, the verb bewindan is by no means the regular word for using, wielding or swinging a sword: the literal meaning is ‘to wind or bind around or about’ (in Gothic biwindan is used in the sense ‘to swaddle’). Its usage here seems rather poetic and could refer to grasping the handle with one hand and the sheath with the other when starting to use the sword. There is, of course, a very remote possibility that we are dealing here with a technical term (or a usage inspired by a technical term) related to the Middle High German winden ‘to turn (the sword)’ familiar to practitioners of historical European martial arts.\nNext I will consider the “supporting archaeological evidence”. Because no specific page number is provided, I had little choice but to look up the relevant passages myself. The one quoted below concerns the size of the hilt (Davidson 1962: 61):\nThe grips of Anglo-Saxon and Viking swords often seem surprisingly small. One sword found at Reading (now lost) is said to have had a grip too small for a grown man’s hand, and it may be noted that the other swords in this museum, including the one from Shifford, have small grips. […] Some of these swords may have been made for boys or slender men, but another possible explanation was given by R. E. Oakeshott, who suggested that grip and pommel might be grasped together, and confirmed this by illustrations from early manuscripts; the brazilnut pommel in particular fitted easily into the hand in this way.\nIn this case, archaeological evidence would definitely suggest single-handedness. In another passage Davidson says that “a good swordsman might be capable of using both hands as the occasion demanded” (ibid. 201), referring to the passage from the Droplaugarsona saga (written probably in the 13th century) quoted below:\nGrim could fight equally well with both hands; he brandished a sword in his left hand, but with the right he struck at Gauss and cut off his leg above the knee.\nAgain, there is no indication whatsoever about the sword requiring two hands to swing it in an effective manner. The Old Norse text (Jónsson 1878) makes it even more obvious that Grímr has indeed two swords, one in each hand:\nGrímr hafði tvau sverð, því at Gauss kunni at deyfja eggjar. Grímr vá jafnt báðum höndum. Hann brá upp sverði með vinstri hendi, enn hjó með hinni hœgri til Gauss ok af fótinn fyrir ofan kné. Nú fell Gauss, ok í því veifði hann sverðinu at Grími ok kom á fótinn, ok varð þat svörðusár. Nú flýði víkingrunn á brott, enn Grímr tók silfrit, ok fekk góðan orðstír af verki þessu.\n‘Grímr had two swords, because Gauss could make edges blunt. Grímr was equally good with both hands. He brought up his sword with his left hand, and with the right he struck at Gauss and cut off his leg above the knee. Then Gauss fell and waved his sword at Grímr and hit his foot causing a surface wound. Now the viking fled away, but Grímr took the silver and got a good reputation of this work.’\n(My emphasis and translation.)\nWith the sword in his left hand Grímr lures his opponent into lifting his defences and immediately uses the opening thus created to strike with the other sword in his right hand at his opponent’s leg, chopping it off. It should be borne in mind that sagas in general do not qualify as a reliable source and the descriptions contained in them should not be taken at face value. In any case, Grímr’s apparent ambidexterity needs not be considered a super-human feat conjured up by the imagination of the anonymous author. On a related side note, in Speculum regale (Konungs skuggsjá), a Norwegian educational textfrom the late 13th century, it is stated that an expert in the use of weapons should be trained to use both hands alike, though swords are not explicitly mentioned in this context (chapter 37).\nThe typical size of the grip and the fact that the sword was used together with a shield constitute conclusive and, as far as I am aware, widely undisputed evidence that the Germanic swords of the period discussed were single-handed. Whereas an interpretation of the Beowulf verses quoted earlier might tentatively suggest otherwise, a simple experiment with any replica sword made according to specifications provided by archaeological specimens will reveal that only a child would actually require two hands to swing the Anglo-Saxon sword or any other similar Migration period Germanic sword based on the Roman spatha. Further experimentation will also confirm that it is indeed within human capabilities to chop off a limb using only one hand, even though such a technique might not have played a decisive part in the ancient battlefields.\nBut despite the overwhelming facts regarding swords enumerated above, it is nevertheless worth discussing whether dismissing the plural mid mundum ‘with hands’ as a product of poetic licenceis a valid approach.Davidson points out that oral poetry can be expected to yield more accurate details than historical works, since the audience would have been intimately familiar with the objects described by the performer (1962: 3). There actually is reason to question the identification of Hrunting as a sword in the modern sense: inaddition to sweord, mēce ‘sword’ (passim)and waegesweord ‘wavesword’ (1489), it is once referred to as a hæftmēce (1457), glossed in Bosworth-Toller as ‘a hilted sword’ (also in Mitchell & Robinson 1998, but more interpretatively ‘sword with a long hilt’ in Wrenn 1958). This rather trivial extension (all swords have hilts) could be taken as a poetic byform of mēce, but in the supplement volume a comparison is drawn to the Old Norse hefti-sax mentioned in Grettis saga. In the saga, the weapon in question is wielded by a giant and is referred to as a fleinn ‘pike’ with a tréskapt ‘wooden shaft’ (66.4):\nEn er Grettir kom at honum, hljóp jǫtuninn upp ok greip flein einn ok hjó til þess, er kominn var, þvíat bæði mátti hǫggva ok leggja með því. Tréskapt var í; þat kǫlluðu menn þá heptisax, er þannveg var gǫrt.\n‘But when Grettir came near him, the giant leapt up, grabbed a pike and struck at him, who had just come, for with [the pike] he could both cut and thrust. A wooden shaft it had, and [a weapon] that was made in such a way men would back then call a haft-seax.’\n(My emphasis and translation.)\nIn a footnote to his critical edition, Boer notes that the word heptisax is not attested outside Grettis saga, where is appears twice, but the striking similarity with hæftmēce would suggest a connection with Beowulf (1900: 231). Davidson, who identifies the weapon as a ‘knife fastened to a wooden shaft’ points out that the weapon would seem to have been unfamiliar to the intended audience of the saga (1962: 134).\nInterestingly, a similar Old English word, stæfsweord ‘staff-sword’ appears in Ælfric’s glossary as a translation (possibly coined by the author) of Latin dolones, plural of dolo ‘iron-pointed staff, pike’ (see Wright 1857: 35ff. for this and other Latin and Old English weapons terms). An exact cognate of the compound stæfsweord is also attested in Old High German, where stabaswert ‘staff-sword’ is used to gloss Latin framea, which according to Tacitus is the Germanic word for hasta ’spear’ (Germania 6.1), but the word was used by later Christian writers to signify ‘sword’ (Green 1998: 185). Accordingly, Ælfric’s glossary gives sweord as the Old English equivalent of Latin framea.\nFollowing the lexical trail further is beyond the scope of this article. In any case, this etymological excursion would seem to indicate that if the weapon referred to in Beowulf was wielded with both hands, as mid mundum would suggest, and if indeed the name Hrunting is derived from hrung ‘rung, staff’, it would seem that the weapon – at least in the archetypal story – was something akin to a pike or a glaive, not the typical Anglo-Saxon or Viking sword or any of their known predecessor.\nThe linguistic surface usually passed through as quickly as possible by non-philologists offers a rich and, admittedly, confusing domain of data for studying not only language but the physical world language has been used to describe. In conclusion, I hope to have demonstrated the importance of not only checking second-hand references and studying the primary sources whenever possible, but also of inter-disciplinary approach combined with practical experimentation. This is especially true when it comes to weapons, since, after all, they are quite concrete tools and should not be treated and studied as philosophical abstractions.\nBoer, R.C. (ed.). 1900. Grettis saga Ásmundarsonar. Halle A.S.: Max Niemeyer.\nBosworth, Joseph & T. Northcote Toller (eds.). 1989 (supplement 1921). An Anglo-Saxon Dictionary.\nMitchell, Bruce & Fred C. Robinson (eds.). 1998. Beowulf: An Edition with Relevant Shorter Texts, including ’Archaeology and Beowulf’ by Leslie Webster. Oxford: Blackwell.\nDavidson, H. R. Ellis. 1962. The Sword in Anglo-Saxon England: Its Archaeology and Literature.Oxford: Blackwell.\nDonoghue, David (ed.). 2002. Beowulf: A Verse Translation. Translated by Seamus Heaney. New York: W.W. Norton & Co.\nGreen, D. H. 1998. Language and History in the Early Germanic World. Cambridge: Cambridge UP.\nIjäs, Antti. 2014. “Anglosaksien miekoista, lähdeviittaamisesta ja relatiivipronominista.” Filologogrammata. <http://filologogrammata.blogspot.fi/2014/07/anglosaksien-miekoista.html>\nJónsson, Þorleigr (ed.). 1878. Droplaugarsona saga. Reykjavík: Forlag Kristjáns ó. Þorgrímssonar.\nPetersen, Jan. 1919. De Norske Vikingesverd: En Typologisk-kronologisk Studie over Vikingetidens Vaaben. Kristiania: Jacob Dybwad.\nWebster, Leslie. 1998. “Archaeology and Beowulf.” In Michell & Robinson 1998; 183–194, reprinted in Donoghue 2002: 212–236.\nWrenn, C.L. (ed.) 1958. Beowulf with the Finnesburg Fragment. Revised and enlarged edition. London: George G. Harrap.\nWright, Thomas (ed.). 1857. A Volume of Vocabularies. Privately printed.\nTo learn more about Combat Archaeology Click Here.\nAntti holds a MA degree in English Philology from the University of Helsinki, with a minor in pedagogy and an advanced minor in Latin language and Roman literature. Having rounded up his classical education with advanced studies of Greek language and literature, he specializes in historical linguistics and philology with a particular interest in the Germanic language family. He served his military service in the Guard Jaeger Regiment and holds the rank of 1st lieutenant (infantry) in reserve. Currently he is working on his doctoral dissertation on Royal Armouries MS I.33 at the University of Helsinki. Antti is the chief instructor of Wing Tsun Finland, an instructor in Weapons Combat Systems and the Regional Head of KDF Finland."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:1d386500-b4e8-4957-8e48-1a77523fb0aa>"],"error":null}
{"question":"What are the key differences between motherboard component compatibility and PCIe backward compatibility for computer builders?","answer":"Motherboard component compatibility requires careful matching of specific parts - the processor must match the correct socket type (e.g., LGA 1366 for i7 processors), memory modules need appropriate slots, and video cards must be compatible with CrossFire/SLI chipsets. In contrast, PCIe offers more flexible backward compatibility across generations - PCIe 4.0 cards can work in PCIe 3.0 slots, and PCIe 3.0 cards function in PCIe 4.0 slots, though they will operate at the lower generation's specifications. This means that while motherboard components must be precisely matched for optimal performance, PCIe devices can be mixed across generations without compatibility issues, though they will be limited to the slower generation's speed capabilities.","context":["Configuring your own gaming computer is not always easy and is often a challenging task for many of us. After all; different components like memory modules, efficient processors and fast video cards need to be looked into and carefully chosen. But it is not just these components that have to be looked into detail, but also your motherboard that is responsible for ensuring that all these components perform at their best and as per the specifications to which they are built. It is not important to have one of these components working at its optimum level but all of them have to be in-sync with each other for the effective working of your computer.\nGiven below are few specifications that can be looked into while you are selecting a motherboard for your purpose.\nChoosing a suitable processor is where it all has to start. After all; motherboard is the place where processors are stationed. The reason why this specification is given due importance can be related to fact that CPU socket is located on motherboard and it therefore presents a logical way of selecting the best board for your purpose. For example, if you are using Inter Core i7 processor with a motherboard that is fitted with a 775 socket it will make little sense. This is because the socket so chosen is suitable only for Quad Core or Core 2 Duo processors. Contrary, the i7 processor has to make use of a motherboard that is fitted with LGA 1366 socket or LGA 1156.\nIt is also worth understanding that the processor you choose will have an impact on the motherboard you purchase. Depending upon the processor you choose, motherboard has to be chosen accordingly. Memory modules too need to be looked into while you are selecting a motherboard for your computer. If you are using the computer solely for gaming purposes you will need to ensure that it’s RAM is large enough to process data quickly and efficiently. An assessment for this can be made by checking out the number of slots that are available on the gaming motherboard.\nVideo cards themselves may not present a challenge while selecting motherboards, unless of course, multiple video cards are seriously considered for your purpose. In such a case, the compatibility of the video card has to be looked into. Also, they need to be checked if they are ready for CrossFire and SLI chip sets. A proper space has to be allocated for storing games in your computer. This space has to be large enough in your computer to make the games more accessible, especially in fast paced environments. Most of the gaming computers today are equipped with fast hard drives solely for these purposes. These hard drives, in turn, disk controllers that are placed on motherboards for their normal functioning.\nWhile deciding on integrated components for your computer, it is better not to spend too much amount of time and money on sound controllers and on-board videos as they can be purchased separately and can give you more enjoyment and clarity while you are watching different scenes throughout your game.","If you were wondering what PCIe Gen 4 is, as well as where you can find industrial grade computers that come equipped with PCIe 4.0, you've come to the right place. We will explain what PCI Express Gen 4 is in much detail below.\nWhat is PCIe 4.0?\nPCIe 4.0 is the 4th Generation of the Peripheral Component Interconnect Express (PCIe), which is an interface standard that connects high-end components to your PC. The first generation of PCIe dates back to 2003, which was PCIe Gen 1. Over time, there have been four new versions of PCI Express. Today's focus will be the PCI Express 4th Generation because it's the latest PCIe generation to hit the market.\nPCIe 4 doubles the data transfer speed of the previous generation (PCIe 3.0) from 1GB/s per lane to 2GB/s per lane, providing users with a total of 32GB/s in a 16 lane configuration. Furthermore, PCIe provides up to 16GT/s per lane when compared to the previous generation's 8GT/s. Each new generation of PCIe doubles the data transfer rate and total bandwidth per lane of the prior generation, paving the way for new, faster PCIe devices.\n(Photo credit: Microsemi)\nEvery PC, including commercial and industrial computing solutions, comes equipped with at least one PCIe slot, with some computers having multiple PCI Express slots. PCIe slots are typically used to add GPUs, Solid-state drives (SSD) add on cards, Wi-Fi cards, and Raid Cards.\nPCIe 4.0 expansion slots on a typical PC motherboard come in four different types: PCIe x1, PCIe x2, PCIe x4, PCIe x8, and PCIe x16. These numbers signify how many lanes each PCIe slot has. The more lanes that a PCIe slot has, the more data that can be transferred to/from the add on card.\n(Photo credit: HowToGeek)\nEach PCIe Lane is made from two pairs of wires, one for sending data and one for receiving data. The bandwidth scales linearly with PCIe, so an 8-lane connection will have twice the bandwidth of a 4 lane configuration.\nThe great thing about PCI express is that it maintains both forward and backward compatibility, supporting new and legacy technologies. PCIe is backward compatible with PCIe Gen 1, PCIe Gen 2, and PCIe Gen 3. PCIe Gen 4 will be compatible with PCIe Gen 5. However, a PCIe Gen 4 device will be limited to the 4th generation specifications when inserted into a Gen 5 slot.\nSaid differently, PCIe 4.0 cards will work on motherboards with PCIe 3.0 slots, and PCIe 3.0 cards will work on motherboards with PCIe 4.0 slots. That said, although PCIe 4 cards will work on PCIe 3.0 motherboards, their speed will be limited according to PCIe 3.0 standard.\nThe benefits of PCIe 4.0 will be most realized in data centers, which can utilize the new performance offered by PCIe 4. This is so because NVMe storage technologies are saturating the existing PCIe 3.0 standard; PCIe 4 will allow them to achieve optimal performance and higher data transfer speeds because of the increased bandwidth offered by PCIe 4.0.\nThe increased bandwidth offered by PCIe 4.0 improves the efficiency of performing workloads, such as machine learning (ML) and cloud computing, reducing the compute time required to complete any given task. Additionally, it reduces lane congestion and reduces the amount of power consumption used by the device. To sum things up, PCIe offers more performance while reducing the amount of energy required for a given workload.\nPCIe 4.0 vs. PCIe 3.0 (What are the Differences Between PCI Express Gen 4 vs Gen 3?)\nThe major difference between PCI Express 4.0 and PCI Express 3.0 is that it doubles the speed of PCIe 3.0, boosting performance from one gigabyte per lane to two gigabytes per lane while providing options for 1x, 2x, 4x, 8x, and 16x slot configurations, increasing the maximum potential bandwidth of a PCI express slot to 64 gigabytes per second.\n(Photo credit: Microsemi)\nThat said, current graphics cards are unable to take full advantage of PCIe 4 because they are not even coming close to saturating the bandwidth provided by the PCIe 3 slot. However, where PCIe 4 does shine is when it comes to storage add-in cards.\nFor example, PCIe 4.0 NVMe drives will benefit from the added bandwidth because NVMe drives operating in a Raid 0 configuration are able to perform sequential read/write speeds of up to 15 GB/s. This is crucial for persons and/or organizations performing AI computations using large data sets for training and testing machine learning algorithms because huge data sets need to be stored and fetched repeatedly.\nOverall, PCI Express 4 significantly improves the read/write speeds of SSDs, offering an excellent boost in performance for such complex workloads. As AI algorithms get more and more complex, there will always be a need for faster data transfer speeds. By increasing the available bandwidth, PCIe 4.0 significantly improves the speed at which data can be accessed, providing for smoother real-time data analysis.\nAdditionally, PCI Express has the added benefit of reducing the lane requirement for add-in cards. This is so because each of the lanes is becoming faster, reducing the number of lanes required for certain devices.\nFor example, an NVMe SSD card can operate at PCIe 4 x8 speeds while enjoying the same bandwidth as a PCIe 3 x16 slots while using half of the lanes. The added lanes can be leveraged for adding additional devices, permitting devices to run using smaller PCIe slots, allowing system builders to build smaller, more compact systems.\nCan You Insert a PCIe Gen 3 Card in a PCIe Gen 4 Slot?\nYes, you can insert a PCI Express Gen 3 Card in a PCI Express Gen 4 slot because PCI Express 4 is backward compatible with PCI Express 3, meaning older PCI Express devices will properly work if inserted into a PCI Express Gen 4 slot. That said, although older PCI e peripherals will work on a PCIe 4 slot, their speed will be limited to their generation's specifications.\nFor example, if a PCIe 3.0 graphics card is inserted into a PCIe 4.0 slot, the graphics card would function as if it were inserted into a PCIe 3.0 slot, meaning it would not benefit from being added to PCI Express 4.0 or even a PCIe 5.0 slot. This is so because the specifications of your GPU did not change; it's merely compatible with the new PCIe standard.\nAnother example would be connecting a PCIe Gen 3 SSD to a PCIe Gen 4 slot. Even though the PCIe 4 slot is backward compatible with the Gen 3 SSD, the fact that you're connecting it to a Gen 4 slot will not increase data transfer speeds because the SSD's PCIe controller is a Gen 3 controller. So, while compatible, you will not see an increase in performance.\nThat said, if you were to connect a Gen 4 PCIe SSD into a Gen 4 slot, you would notice that the PCIe SSD is twice as fast because the system and the SSD support it. You will see an improvement because the data transfer speeds are faster, and the latency to read/write data is also improved.\nWhy Do We Need PCIe 4.0?\nWe need PCIe 4.0 because as CPUs, GPUs, and data storage devices, such as NVMe SSDs continue to advance, the highways for the data passing among them must improve as well to avoid bottlenecking the system. PCIe Gen 4 improves the amount of bandwidth that's provided for these devices, keeping them satiated with data.\nTake, for example, a PCIe M.2 NVMe SSDs. Most M.2 NVMe SSDs utilize the x4 connection, which has a bandwidth of 4GB/s, which bottlenecks the SSD. However, PCIe Gen 4 increases the bandwidth to approximately 8GB/s, allowing the system to utilize the full potential of the M.2 NVMe drive. As such, it's easy to see that we need PCIe Gen4 to keep up with the advances in computing hardware.\nAdditionally, the growth of the internet, the ever-increasing number of IoT devices, and the increasing complexity of AI workload necessitate the need for faster and more efficient interfaces (pipes) through which to transfer data. PCIe Gen 4 satisfies the need by doubling the width of the pipeline. In the future, the need for additional bandwidth will continue to grow to handle the growth in the volume and velocity of data.\nTechnological innovations often move faster than market demand, especially as it relates to real-world deployments. A major factor that determines how quickly technology is adopted is the overall total cost of ownership. Even though newer technologies have benefits over pre-existing technology, real-world deployment may not make economic sense because of the cost of deploying new technology and using existing technology.\nThe sweet spot for newer technology adoption in many b2b applications is both the economies of scale and a reasonable cost delta between replacing legacy or older technologies that continue to function properly.\nA great example of this case is the introduction of high-performance NVMe storage solutions in many industry 4.0 or intelligent embedded computing applications. Traditionally many embedded applications in the past relied on the 6Gb/s SATA protocol for its cost but also its performance benchmark of 500 ~ 550 MB/s in transfer speeds. Even today, many embedded applications are still using the SATA protocol for data storage and transfer of data for local computer storage. Only recently did NVMe architecture (PCIe Gen 3.0) and storage solutions make a crossover into more embedded computing applications, thanks to the demand for faster processing, storage, and connectivity stemming from edge computing.\nThe demand for faster access to high-speed NVMe storage can be seen only recently in Intel’s 8th / 9th gen processors and Q370 chipset architecture and solutions for the embedded market. These generations of industrial computing solutions specifically dedicate PCIe Gen 3.0 lanes directly for an onboard m.2 NVMe storage solution for access to high-speed storage. Although PCIe Gen 4.0 is available on the market and has theoretically proven benchmarks in performance, many real-world embedded computing applications may not need access to these speeds until a later time in technology adoption. On the other hand, PCIe Gen 4.0 solutions are proving to be highly beneficial in the high-performance data center environment.\nPCIe Gen 4.0 NVMe SSDs & Data Centers\nWe will now explore the benefits that PCIe Gen 4.0 has to offer data centers.\n1. Boost in Data Transfer Speeds\nAlthough the average PC user will not significantly benefit from PCIe 4.0, data centers will benefit tremendously by switching from PCIe 3.0 to PCIe 4.0. This is so because PCI Express 4.0 makes it possible to move data twice as fast, allowing GPUs and PCIe SSDs to deliver faster I/O, offering a significant performance improvement when performing data-intensive and computational applications such as data analysis, machine learning, and artificial intelligence.\nPCIe Version 4 will result in a performance improvement when performing these tasks because it is able to transfer data at approximately 2GB/s per lane vs. the 1GB/s per lane of PCIe Version 3, thus doubling the performance of the prior generation while providing a 4-lane performance of 8GB/s, which is the standard most commonly used by PCIe SSDs.\nWhen tested, PCIe 4.0 SSDs were able to achieve sequential read speeds of up to 6,900 MB/s and write speeds of up to 4,200 MB/s. On the other hand, PCIe 3.0 SSDs were able to achieve sequential read speeds of up to 3,350 MB/s and sequential write speeds of up t0 3040 MB/s. This shows that PCI Express 4.0 provides a significant performance boost over PCI Express 3.0.\n2. Improvement in Energy Consumption\nThe second reason that data centers will benefit from deploying PCIe 4.0 is the added energy efficiency that PCIe 4.0 NVMe SSDs bring to data centers, thus reducing the TCO (total cost of ownership). To measure the energy efficiency of NVMe SSDs, manufacturers measure the rate for each watt of power an SSD consumes.\nOne PCI Express Gen 4 NVMe SSD manufacturers demonstrated that it took one watt of electricity to read 276 MB/s. At the same time, PCI Express Gen 3 SSD consumed one watt of electricity to read 134 MB/s of data, providing a 105% increase in energy efficiency over the previous generation.\nThe same SSD was tested for power efficiency when writing data. The results showed that it took one watt to write 168 MB/s using a PCIe Gen 4 NVMe SSD, whereas it took a PCIe Gen 3 NVMe SSD one watt to read 121 MB/s, showing that PCIe 4 NVMe SSDs provide a 38% increase in energy efficiency over the previous generation.\nSo, not only does deploying NVMe SSDs in data center increase performance, but it also results in better energy efficiency. Although the energy saved by deploying a single PCIe 4.0 NVMe is negligible, when deploying thousands or tens of thousands of these SSDs, you will notice a significant reduction in energy consumption, which translates into a lower cost of operation.\nIndustrial grade PCIe 4.0 NVMe SSDs are designed and manufactured to run 24/7 in data centers and industrial computers to deliver the maximum amount of performance. So, if you have an application that requires low-latency and blazing fast read/write speeds, PCI Express Gen 4 NVMe SSDs are the best option that is currently available.\nFrequently Asked Questions (FAQs)\n1. Is PCIe Gen 4 Worth It?\nAlthough PCIe Gen 4 provides little in advantages in regular applications and use cases, it does offer a significant and noteworthy improvement when it comes to high-performance PCIe NVMe SSDs (solid-state drives) as it allows them to read/write large volumes of data extremely quickly. The benefits of PCIe 4 can best be seen in artificial intelligence and machine learning, where large data sets must be stored and accessed extremely quickly.\n2. How Fast is PCIe Gen 4.0?\nPCIe has a maximum bandwidth of 64 GB/s as well as 16 GT/s. This translates into 2 GB/s per lane or 64 GB/s in total.\n3. Does Intel Support PCIe 4.0?\nIntel currently only supports PCIe 3.0; however, Intel has confirmed that it will be adding support for PCIe 4.0 starting in Q1 2021 with the launch of Intel's 11th generation Core processors codenamed Rocket Lake.\n4. When was PCIe 4 Released?\nPCI Express Gen 4 was released in 2017. However, hardware manufacturers have only recently begun implementing the technology into their hardware.\n5. What is PCI Express Gen 4 Used For?\nPCIe, including PCIe 4.0, is used to connect high-end hardware to computer systems. Hardware that is often plugged into PCIe slots includes GPUs, high-speed NVMe storage, raid cards, capture cards, and memory."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:f9a04c85-f2fc-4f6e-b228-35866b03fa4f>","<urn:uuid:263423f5-fbfd-4416-9f03-8afdb05f9ec7>"],"error":null}
{"question":"What are the key differences between architectural and engineering scales in technical drawings, and how do manual drafting techniques affect their usage?","answer":"Architectural and engineering scales differ in how their numbers run: architect scales can be read both left-to-right and right-to-left, while engineer scales only run left-to-right. Engineering scales are divided into decimal units with ratios like 1:10, 1:20, 1:30, 1:40, 1:50, and 1:60. For manual drafting, these scales are used with specialized tools including scale rules and set squares, requiring consistent pressure application and proper line weights. The scales must be used precisely with straight edges, typically employing a parallel motion or T-square for horizontal lines and a set square for vertical lines to maintain accuracy in technical drawings.","context":["Architect scales have numbers that run incrementally both from left to right and from right to left. A whole number or fraction to the left or right of the number line indicates the scale those numbers represent. Engineer scales have numbers that run incrementally from left to right.\nWhat does the title block generally contain?\nThe title block of a drawing, usually located on the bottom or lower right hand corner, contains all the information necessary to identify the drawing and to verify its validity.\nWhat are engineered plans for motors?\nengineered plans for motors, pumps, piping systems, and piping equipment. This means that the drawing gives approximate positions and sizes. These are schematic drawings which show all the equipment, piplines, valves, instruments, and controls needed to operate a piping system.\nWhat is one difference between an engineer’s scale and an architectural scale quizlet?\nTerms in this set (8)\nEngineer scales are only read from left to right where architect scales can be read from left to right and right to left. Whereas an architect’s scale uses fractional scaling.\nWhat is civil engineering scale?\nCivil engineer scales are used to design large projects such as roads, bridges and water mains. Depending on the project, 1 inch on the scale can represent 100 feet in real life. The civil engineer’s scale divides 1 inch into equal decimal units of 10, 20, 30, 40, 50, 60 and 80.\nHow are architectural and engineering scales divided?\nThe scale is divided into decimalized fractions of an inch. The six faces of the prism have the following dimensional ratios 1:10, 1:20, 1:30, 1:40, 1:50 and 1:60 unless otherwise stated. The first graduation on an engineer scale is the zero mark.\nWhat is a title block in engineering?\nTitle block has the highest concentration of information in a sheet of engineering drawing. It is the door to understanding the drawing and the company, and includes all the information which enables the drawing to be intepreted, identified and archived.\nWhat is the size of title block?\nThe size of the title block is 185 mm x 65 mm which is recommended by BIS (Bureau of Indian Standards).\nHow do you make a title box in engineering drawing?\nHow to make a title box In engineering drawing, Title box in …\nWhich of the following is a typical scale used for floor plans?\nFloor plans are most commonly (though not always) drawn on a 1/4” scale, which means a quarter-inch on the plan equals 1 foot of actual length on the completed structure. Some scales use metric instead of imperial measurements.\nWhat defines a key plan?\nA small-scale plan of a building or building group which indicates the placement of the principal elements of the scheme.\nWhat is a civil plan?\nCivil Plans (also known as “Construction Plans”) are submitted following approval of a Preliminary Plat and prior to or concurrently with submitting a Final Plat (which includes replats, amending plats, and minor plats).\nWhat are engineered plans for motors pumps pumping systems and piping equipment called?\nCore Module 5: Construction Drawings Vocabulary\n|mechanical plan||plans for motors, pumps, piping systems and piping equipment|\n|civil plan||used for work that has to do with construction in or on the earth|\n|structural plan||support the architectural plan\ngive details on the materials to be used\nWhen the metric system is used dimensions are written in?\nWhen the metric system is used, dimensions are written in: Meters, centimeters, and millimeters.\nWhat are engineered plans for motors pumps piping systems and equipment are called?\nmechanical plans. are engineered plans for motors, pumps, piping systems, and piping equipment.\nWhat scales are on an engineering scale?\nReferred to as 1:10, 1:20, 1:30,1:40, 1:50 or 1:60 scale. Typically in civil engineering applications, 1:10 (1″=10′) is used exclusively for detail drawings. 1:20 and 1:40 scales are used for working plans. 1:60 is normally used only to show large areas of a project.\nWhat is a scale in engineering drawing?\nDefinition. A scale is defined as the ratio of the linear dimensions of the object as represented in a drawing to the actual dimensions of the same.\nIs scale a ratio?\nA scale is a specific type of ratio. It compares a unit on a map or plan or on a scaled model to an actual distance on a map or the size of a real object.\nWhy are scale ratios used in engineering drawing?\nThere is a need to reduce or enlarge while drawing the objects on paper. Some objects can be drawn to their actual size. The proportion by which the drawing of aan object is enlarged or reduced is called the scale of the drawing.\nWhat is a scale in architecture?\nScale in architecture is measuring or ratio based on the relation of architectural elements with each other and their surroundings along with being in harmony with human body geometry.\nWhat are the scales on an architectural scale?\nCommon scales for a triangular architect’s scale are: 16 (full), 3, 1-1/2, 1, 3/4, 1/2, 3/8, 1/4, 1/16, 1/8, 3/32. Major divisions of each scale represent feet which are further subdivided into 12ths or 16ths.\nWhat is the size of engineering drawing sheet?\nDrawing Size Reference Table, Architectural and Engineering Drawing Sizes\n|Standard US Engineering Drawing Sizes|\n|Drawing Size||Dimensions (millimeters)||Dimensions (inches)|\n|ANSI A||215.9 x 279.4||8.5 x 11|\n|ANSI B||279.4 x 431.8||11 x 17|\n|ANSI C||431.8 x 558.8||17 x 22|\nWhat is reducing scale in engineering drawing?\nSo when a drawing is prepared smaller than the actual size of the object, the scale used is termed as reducing scale and the drawing is known as a reduced-sized drawing.\nWhat is layout of drawing sheet?\nLAYOUT OF DRAWING SHEET:\nAfter selecting the size of the drawing sheet and to divide the proper space for margin, title block, parts list, revision panel, folding marks and selecting a suitable scale on the drawing sheet is known as layout of drawing sheet.\nWhich of the following is reducing scale Mcq?\nWhich of the following scales is a reducing scale? Explanation: The representative fraction 1:3 indicates that the dimension of the drawing is one-third of the actual object. Since the drawing is smaller than the actual object, this type of scale is called a reducing scale.\nWhat is technical drawing?\nA technical drawing, also known as an engineering drawing, is a detailed, precise diagram or plan that conveys information about how an object functions or is constructed. Engineers, electricians, and contractors all use these drawings as guides when constructing or repairing objects and buildings.\nWhat is title block format?\nYou can customize title block formats in the standard drawing templates or you can create your own title block formats. A title block includes information about the drawing, the sheet, and the design properties\nit updates to display current information.\nHow many types of lines are there in engineering drawing?\nFor general engineering drawings, the types of lines recommended by the Bureau of Indian Standards shown in table 2 must be used. The thickness of the lines must be chosen according to the type and size of the drawing from any of the six groups given in Table 1.\nWhat is an assembly drawing?\nA drawing which displays the parts of a machine or a machine unit assembled in their relative working positions is known as assembly drawing.\nWhat is lettering in technical drawing?\nTechnical lettering is the process of forming letters, numerals, and other characters in technical drawing. It is used to describe, or provide detailed specifications for, an object.","Manual drafting techniques\nManual drafting is the practice of creating drawings by hand. Manual drafting techniques have traditionally enabled the planning and communication of design ideas and construction information. As there is a very diverse range of information that may need to be communicated, there are a similarly wide range of drawing types. See Types of drawing for more information.\nThe evolution of manual drafting techniques has created a discipline around which other forms of drafting, such as computer-aided design (CAD) have subsequently developed.\nThe advantages of manual drafting include; the low cost of equipment compared to CAD hardware and software, the clarity that can be achieved by being able to see all shapes, sizes and angles on one sheet, the ability to bring creative style and expression to drawings, and a degree of depth and weight that can be easier to convey with 'analogue' rather than 'digital' drawing techniques.\nHowever, manual drafting is now becoming something of a lost art, as; it requires a large amount of space, both for the drawing process, storage and viewing; drawings cannot be linked to digital information, they can take much more time to prepare in comparison with CAD drawings, they are more difficult to correct, text and colour can be more difficult to apply, it is more complicated to create three-dimensional representations and so on.\nManual drafting requires a flat drafting table, drafting or drawing board, typically with a parallel motion, that allows consistent drawing of parallel lines. Drawing boards can often be adjusted in height and angle to suit the user.\nVellum paper was used traditionally for drafting and came supplied in rolls. More recently, drafting paper, standard paper sheets, or tracing paper have become more common as they are less expensive.\nManual drafting tools include:\n- Specialised pencils with varying lead strengths.\n- Technical drawing pens of varying sizes (see technical drawing pen sizes for more information).\n- Colour application pens.\n- Scale rules.\n- Set squares.\n- T squares.\n- Steel rules that are not damaged by being used repeately to draw lines.\n- Drafting tape or clips to secure the corners of the sheet to the board.\n- Pencil sharpeners.\n- Pen cleaners.\n- Scalpel blades for scraping off ink or removing sections of drawings.\n- Clear tape for repairing drawings (this does not show on reproductions)\n- Mechanical lead holders for using various size leads for different parts of the draft.\n- Templates or stencils allowing easy drawing of certain objects, symbols, common shapes, text and so on.\n- Text writing machines.\n- Letraset lettering sheets that can be applied to drawings as a form of transfer. Other sheets include people, trees and other common objects and symbols.\n- Pantone colour and greyscale sheets than can be cut and applied to drawings to bock colour or shade areas.\n- Tipex (which does not show on reproductions).\n- Drawing board attachments that can be fixed to the parallel motion to create specific angles, reproduce lines or change scales.\nLines in manual drafts should be clean and sharp. A straight edge should always be used when manually drafting. A parallel motion, or T-square can be used to draw horizontal lines, and a T-square or set square to draw vertical lines or other common angles. Typically, the bottom of the set square is placed on the top of the parallel motion, and the vertical portion used to draw straight lines. To draw a vertical line using a T-square, the T portion should be placed flat against the top of the drawing board.\nEqual pressure should be applied on the pen or pencil when drawing lines, slightly rotating the point, and not removing it from the paper until the line is completed.\nSprinkling setting powder, or using fixing sprays over pencil lines can help maintain the integrity of the lines when the sheets are folded or rolled, or otherwise brushed against.\n Line weights\nManual drafting techniques typically involve drawing different line weights to represent different items. Different line weights can be created by using a different size mechanical pencil lead, or a different size of technical drawing pen.\nExterior walls are typically drawn on floor plans and sections with heavy, solid lines. Windows may be drawn lighter, or with the same heaviness but in a dash-dot-dash format. Interior walls have slightly lighter line weights. Elements such as fittings should be drawn with the lightest line weight, and sometimes dashed lines.\nIt is important to maintain correct lettering sizes in manual draft, for example:\n- Notes: 3/32 inch letters.\n- Special notes: 1/8 inch letters.\n- Titles: 1/4 inch letters.\nF pencils or technical drawing pens are typically used for lettering, with a small right triangle to draw straight portions of the letter. Curved portions may be drawn freehand. Thicker strokes are used for horizontal lines, thinner strokes for vertical lines. It can be useful to draw light guidelines first to ensure the correct and uniform text height.\nAlternatively, stencils, transfers or writing machines may be used.\nThe use of different scales enables objects and spaces to be depicted at a specific ratio to their actual size, helping to maintain consistency. Suitable scales will vary depending on the size of object, size of paper and level of information that needs to be conveyed, however, floor plans are commonly drawn with a 1/4 inch scale, kitchens and bathrooms with a 1/2 inch scale, wall sections with a 3/4 inch scale and so on.\nThere are a number of techniques of projection that can be used to represent three-dimensional objects in two-dimensions by 'projecting' their image onto a planar surface.\nSee Drawing projections for more information.\nStandard notation conventions should be followed so that there is clear communication between different people and mistakes are avoided.\nSee Notation and units for more information.\nA range of standard symbols and hatching techniques can be used to convey recognised meaning without the need for explanation.\nSee Symbols on architectural drawings for more informaiton.\nSince the 1980's, computer aided design (or computer aided drafting), and more recently, building information modelling have transformed drafting techniques, in particular allowing alterations to be made with relative ease.\nParametric modelling allows the 'automatic' creation of aspects of drawings based on a series of pre-programmed rules. So for example, a rule might be created to ensure that walls must start at floor level and reach the underside of the ceiling. Then if the floor to ceiling height is changed, the walls will automatically adjust to suit. Other examples might include the colour of similar elements, the height of window cills above floor level, the relationship between walls and a pitched roof, and so on.\n Related articles on Designing Buildings Wiki\nFeatured articles and news\nIn the week of the momentous Heathrow decision, we look back at the development and design of T5.\nBSRIA’s flagship event will address performance and wellbeing beyond compliance.\nYoung Architects and Developers Alliance launched to build the relationship between the two disciplines.\nBS 8536-2:2016 Design and construction: Code of practice for asset management (Linear and geographical infrastructure).\nPaying for off-site goods or materials can be useful, but it puts the client at risk.\nPeople power can be transformative if properly informed and inspired.\nZHA win competition to build an Urban Heritage Administration Centre in Saudi Arabia.\nLeaps, not steps, are needed to avoid a ticking time bomb, say BRE in response to Farmer Review.\nA multi-purpose hall in France covered in a translucent orange membrane.\nWinning designs revealed for a rock formation-influenced residential complex in Rennes.\nAn article explaining the techniques, regulations and environmental impacts of carbon capture and storage.\nWatch one of the first documentaries by the acclaimed Adam Curtis, examining the substandard system building of the 1960s."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:3970e46f-ad7e-48ab-b7bd-416c41419b41>","<urn:uuid:7c6f4793-8dca-469c-8b1a-437f690e9cb2>"],"error":null}
{"question":"how does leadership stress affect mental health, and what can managers do to prevent harmful workplace behaviors?","answer":"Leadership stress significantly impacts mental health when leaders try to control everything, which is one of the biggest sources of leadership stress. The Stoic mindset can help by teaching acceptance of uncontrollable external factors while focusing on controllable internal thoughts and feelings. Regarding workplace behaviors, leaders must actively prevent harmful practices like ostracism, which research shows has stronger negative effects on psychological wellbeing than other types of misconduct. To address this, leaders should increase relationship quality with direct reports, avoid creating subgroups, foster high-quality interactions between all employees, and regularly reach out to team members, especially during remote work. These practices help prevent both leadership stress and toxic workplace dynamics that could damage mental health.","context":["Today I escaped anxiety. Or no, I discarded it, because it was within me, in my own perceptions, not outside. ~ Marcus Aurelius\nDating back to ancient Greece, Stoicism was most famously practiced by Emperor Marcus Aurelius, who led Rome for two tumultuous decades and whose observations as a Stoic leader were published under the title Meditations. In recent years, this philosophy has been making a comeback, especially in the business world. Books like Ryan Holiday’s The Obstacle is the Way have popularised Stoic concepts across the globe. So, this week, my message focuses on how you can draw on time-tested Stoic principles in order to become a more resilient and effective leader.\nCentred around humility, awareness and control of your emotions, the Stoic mindset is tailor-made for leadership. It creates mental toughness, equipping you to stay calm and get through crises, and encourages you to transform unexpected obstacles into opportunities. As a philosophy, it’s also refreshingly straightforward and fuss-free. In Elevate Your Leadership with Stoicism, Blake Hammerton says:\nEssentially, as a Stoic leader, you control things you can control and that influence your life, and you do not let other variables out of your control impact you. It doesn’t mean you’re emotionless, but it does mean you’re level-headed, logical, and practice great mindfulness.\nThe Stoic mindset is relevant for anyone who has to deal with uncertainty. Since leaders are especially vulnerable to change, it can play a crucial part in their effectiveness as well as mental health.\nIn 1965, American aviator James Stockdale was shot down and imprisoned in Vietnam. For the next seven years, he underwent unimaginable hardships – from shattered bones, to solitary confinement. In the Forbes article, Want an Unconquerable Mind? Try Stoic Philosophy, Carrie Sheffield explains how Stockdale’s Stoic discipline helped him to not just survive but also lead and inspire:\nThough his body lay captive in Hanoi prison cells, Stockdale later recounted that his mind was free and his spirit unbroken. Through clandestine channels, Stockdale, a high-ranking officer, maintained chain of command among his fellow captured pilots—75 initially, growing to more than 460—issuing orders and boosting morale. Released at war’s end, Stockdale later won the Medal of Honor, the military’s highest award, and served as president of the Naval War College.\nIn essence, Stoicism tells us that before we can hope to influence events and lead others, we first need to work on ourselves. So, how can you harness Stoic principles to become a better leader? Here are seven recommendations:\n1. Accept what’s out of your control\nOne of the biggest sources of leadership stress is trying to control every single thing. Stoicism emphasises the importance of recognising that there are many things that are simply out of your control – from weather and market trends, to how other people behave. Accepting these “externals” doesn’t mean being submissive or giving up; it means doing your best and then accepting that the outcome could go either way, with equanimity. In the article mentioned above, Sheffield highlights an analogy given by Cato the Younger, a Stoic Roman statesman:\nA stoic is like an archer who diligently and confidently notches his arrow and draws his bow but must accept that once his arrow has flown it could be blown off course or its target could move.\n2. Take charge of your inner life\nBy accepting what’s out of your control, you can shift your focus to genuinely controllable things. As Sheffield says:\nA stoic leader realizes that only his thoughts and intentions are truly within his sphere of control; everything else is ultimately uncontrollable.\nStoics believe that your thoughts and feelings (“internals”) are a source of power that can change the way you experience life. Even in stressful circumstances where everything else is outside your control, you still have the ability to govern your own emotions. This approach helps you break out of the “victim” mindset and take charge of your own frame of mind. Meditation and mindfulness are powerful ways to build mental fortitude. Epictetus, a Stoic philosopher who was born into slavery and crippled as a child, famously wrote:\nWhere is the good? In the will…If anyone is unhappy, let him remember that he is unhappy by reason of himself alone.\n3. Prepare for the worst\nMarcus Aurelius offers leaders an unexpected piece of advice:\nBegin each day by telling yourself: Today I shall be meeting with interference, ingratitude, insolence, disloyalty, ill-will, and selfishness.\nReminding yourself of unpleasant possibilities enables you to prepare for them – be it during a tough conversation, a negotiation or just a regular day at the office. This way, you won’t freeze or be caught off guard when things go wrong. No, you don’t need to discard positive thinking altogether; simply make some space in your day for “negative thinking” as well. In Why Today’s Best Business Leaders Look to Stoicism, Aytekin Tank describes a Stoic method called “premeditatio malorum” (premeditation of evils) to help you manage fear:\nImagine your most dreaded outcome in any situation. Worst case scenario: You’re psychologically prepared for whatever the future holds. Best possible scenario: You realize that even the worst outcome won’t kill you, and maybe that original fear loses its teeth.\n4. Fail on purpose\nOne way to develop character is to take the hard route. Ancient Stoics would intentionally practice discomfort and put themselves in situations where failure was likely. This practice builds resilience, enabling you to bounce back from sudden disasters at work (and in life!). It also takes away irrational fear of failure, making you more willing to take risks. In 4 Reasons Why Stoics Make Great Leaders, LaRae Quy suggests the following:\nIf you want to increase your performance, set high goals where you have a 50-70% chance of success. According to psychologist and Harvard researcher David McClelland, that’s the sweet spot for high achievers. When you fail half the time, it motivates you to figure out what you should do differently and try again.\n5. Stay objective in success\nWhile we talk a lot about dealing with hardship, we don’t really talk about dealing with success – we think of it as merely to be enjoyed. In Stoic thinking, however, it’s important to stay reasonable and balanced even in times of great happiness. In an interview with Knowledge@Wharton, psychotherapist and practicing Stoic Donald Robertson elaborates:\nThe Stoics were quite cautious about the danger implicit in certain pleasant feelings or positive feelings, or feelings of happiness… If we get too carried away enjoying certain things, sometimes we make bad decisions, so we need to retain our senses. We need not to lose it when we’re having too much of a good time. Sometimes we can act irrationally when we’re happy.\n6. Turn problems around\nResist the temptation to always think of events in terms of “good” or “bad”. Stoics see every experience as an opportunity to practice and build virtues. For example, if a project is taking longer than anticipated to get off the ground due to factors outside your control, use it as an opportunity to do extra due diligence and to cultivate patience.\n7. Seize the day\nAn important pillar of Stoicism is to make the most of each day and not waste the limited time we have. The Stoic philosopher Seneca would ask himself three questions every day:\n- How am I better today?\n- What did I do with my time?\n- What were my outcomes?\nIncorporate this simple practice into your daily routine to hold yourself accountable for getting things done and staying on track.","What can a leader do about Ostracism in the workplace?\n“It doesn't matter what they say\nIn the jealous games people play\nOur lips are sealed”\nOur Lips Are Sealed by the Go Go’s (click here for the song)\nEarlier in the year I wrote about my concerns regarding incivility in remote work (click here for the blog). This week I was reminded of one of the more complex yet extremely damaging types of inappropriate interpersonal behaviour: Ostracism. Researchers have found that ostracism has stronger detrimental effects on employees’ psychological wellbeing than other types of misconduct, such as sexual harassment and aggression. Ostracism is subtle and quite ambiguous:\nit can be caused by both action (excluding) as well as inaction (omission of inclusion),\nit can be defined by perceptions, or behaviours.\noccasionally, behaviour may be perceived as ostracism when in effect it is not.\nOstracism presents a lot of confusion to the victim, as they may not know why it is happening, or if it is indeed happening.\nWhat are the personal impacts of ostracism?\nDrop in self-esteem\nFeelings of rejection\nIncreased stress levels especially at work,\nEmotional exhaustion, and\nSadly, ostracism is corrosive to psychological wellbeing whether or not it is intentional, face to face, or online.\n“But you're the gossip girl I thought you were real to me The gossip girl But you're just plastic, girl Shiny plastic hard shell”\nGossip Girl by Grace Vanderwaal (click here for the song, the first few seconds are brilliant)\nWhat are the organisational impacts of ostracism?\nHighly Corrosive to people and groups\nDecreases job performance\nDecreases discretionary effort\nDecreases organisational citizenship behaviour, (ie the extra things employees do above and beyond their role)\nIncreases counter productive behaviour – (ie staff act out in negative ways)\nDecreases job satisfaction\nCreates a negative perception of the organisation\nIronically, manipulative bosses may use ostracism:\nas a punishment for violating the rules,\nas a technique for bringing people into line, or\nto make the workplace so intolerable that specific employees people quit\n\"It’s a type of violence against the individual... It’s insidious.. it takes therapy a while to cure.. just don’t do it \"\nBen Baran, and Chris Everett\nWhat can a leader do to reduce the likelihood of ostracism in their workplace?\nIncrease the quality of your relationships with direct reports\nAvoid fault lines between subgroups\nBe mindful to not create unintentional in and out groups\nBe aware to remove non-intentional ostracism from the workplace\nStick to your morals and set the norms for the group\nLearn and use evidence based leadership practices\nRecruit according to values and inclusive behaviour\nHelp your employees find social support at work (click here for my blog on workplace support)\nBe mindful of your behaviour especially when you are very busy or stressed\nRemember you won’t do any harm by reaching out to everyone on a regular basis\nBe cognisant of who is in your team to ensure that there is never ostracism because “out of sight out of mind”\nFoster high-quality interactions connections between all employees, as relationships between remote co-workers are as rewarding as relationships between onsite co-workers and remote working has a propensity to task rather than interpersonal interactions.\nPlease remember, that:\nOstracism needs to be addressed because it’s inappropriate. Reach out to an HR, or an employment lawyer if the inappropriate behaviour has crossed the line.\nWilliams, K.D., Cheung, C.K.T. and Choi, W. (2000). Cyberostracism: Effects of being ignored over the Internet. Journal of Personality and Social Psychology, 79(5), pp.748–762.\nNews | Hofstra University, New York. (2020). Workplace Ostracism: People’s Psychological Attributions and Coping Strategies. [online] Available at: https://news.hofstra.edu/2020/05/21/workplace-ostracism-peoples-psychological-attributions-coping-strategies/ [Accessed 17 Nov. 2020].\nLiu, H. and Xia, H. (2016). Workplace Ostracism: A Review and Directions for Future Research. Journal of Human Resource and Sustainability Studies, 04(03), pp.197–201.\nHoward, M.C., Cogswell, J.E. and Smith, M.B. (2019). The antecedents and outcomes of workplace ostracism: A meta-analysis. Journal of Applied Psychology.\nQuade, M.J., Greenbaum, R.L. and Petrenko, O.V. (2016). “I don’t want to be near you, unless…”: The interactive effect of unethical behavior and performance onto relationship conflict and workplace ostracism. Personnel Psychology, 70(3), pp.675–709.\nSteinbauer, R., Renn, R.W., Chen, H.S. and Rhew, N. (2018). Workplace ostracism, self-regulation, and job performance: Moderating role of intrinsic work motivation. The Journal of Social Psychology, 158(6), pp.767–783."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:dfa4c856-1dce-4197-a25c-5f6ee63470a2>","<urn:uuid:3451e3f9-bc88-41cd-8a37-a2be301b99bd>"],"error":null}
{"question":"What are the common side effects of Xeomin when used for blepharospasm treatment?","answer":"When used to treat blepharospasm, the most common side effects of Xeomin were eyelid sagging, dry eye, dry mouth, diarrhea, headache, visual impairment, shortness of breath (dyspnea), and upper respiratory infections.","context":["Xeomin (incobotulinumtoxinA), made by Merz Pharmaceuticals, is approved by the US FDA for the treatment of adults with cervical dystonia or blepharospasm and Moderate to severe frown lines between the eyebrows. IncobotulinumtoxinA is made from the bacteria that causes botulism. Botulinum toxin blocks nerve activity in the muscles, causing a temporary reduction in muscle activity.\nFDA approval of XEOMIN, was based on the results of two U.S. clinical trials. A study cited in the Journal of Neurological Sciences on the efficacy and safety of XEOMIN found it showed “non-inferiority” to Botox when used in the same doses to treat cervical dystonia. It concluded that XEOMIN is a safe and effective treatment for the disorder.\nXeomin has now been approved for some cosmetic indications – Moderate to severe frown lines between the eyebrows. Similiar to other drugs in this category, including Botox and Dysport, that also have FDA approval for the treatment of facial wrinkles. Now that Xeomin is available and FDA approved, doctors can use it both on-label and off-label to treat the same facial lines treated by Botox and Dysport, including frown lines, crow’s feet and forehead wrinkles.\nWorldwide, more than 84,000 people have been treated with Xeomin injections. The U.S. is actually the 20th country to approve this new drug. It is available in 50-unit and 100-unit vials.\nBetter or worse then Botox?\nXeomin is the first one that does not need to be refrigerated before use, which may simplify distribution. Xeomin is also has no additives, just botulinum toxin type A. This may mean that there is less risk of developing antibodies against Xeomin than other available neurotoxins. The body develops antibodies in response to a foreign invader and attacks. In theory, antibodies could prevent a neurotoxin from having its desired effects. The effects of Xeomin occur within one week, and the results last from three to six months, making it comparable to Botox in terms of both onset and duration of action. Xeomin should not be used interchangeably with other botulinum products.\nWhat are the risks?\nThere is a risk that all botulinum toxin products may spread from the area where they were injected to other parts of the body, causing potentially life-threatening swallowing and breathing problems. This was predominantly seen in children treated with Botox off-label for cerebral palsy. These issues have not been reported among people who received botulinum products for cosmetic uses or to treat blepharospasm.\nOther risks may include bleeding and bruising at the injection site and allergic reactions such as itching, swelling or shortness of breath. Your doctor should discuss all the potential risks of this procedure with you during your consultation.\nWhen Xeomin is used to treat cervical dystonia, side effects include neck pain, muscle weakness, injection site pain and musculoskeletal pain. When used to treat blepharospasm, the most common side effects of Xeomin were eyelid sagging, dry eye, dry mouth, diarrhea, headache, visual impairment, shortness of breath (dyspnea), and upper respiratory infections.\nHow much does Xeomin cost?\nXeomin costs will likely be comparable to Botox if not slightly lower. Exact price points are not yet available since Xeomin was so recently approved for use in the U.S. If your doctor suggests Xeomin, make sure to get accurate pricing information before booking your appointment."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:5b866b29-74c1-4eec-a683-58c30484e97b>"],"error":null}
{"question":"How did the historical practice of wooden crafts evolve during hard times, and what modern storage considerations should collectors keep in mind?","answer":"Wooden crafts like tramp art flourished during challenging periods, particularly through the Depression when many people were out of work. Originally a pastime where men would carve cigar boxes during long winter evenings, the craft continued until cigarette smoking's rise made cigar boxes scarcer. For modern collectors storing such wooden pieces, it's crucial to keep them in areas with minimal temperature and humidity fluctuations. Large pieces should be stored upright and off the floor, while smaller pieces can be stored back to back and face to face on separated shelves. Acid-free materials and proper physical support are essential for storage, and items should be kept in archival boxes specifically designed for the type and size of the collectible.","context":["Whittling was for men what quilting was for women during long winter evenings inY the country back at the turn of the century. While the women painstakingly pieced together little fabric remnants to make comfy quilts, their husbands would take apart and carve cigar boxes and fruit crates with penknives, creating one-of-a-kind, chipped or carved wood novelties. You name it, the men created it--picture frames, clock cases, birdhouses, furniture.\nThis craft became known as \"tramp art\" because early collectors mistakenly thought vagabonds or tramps had made the quirky wood objects during their travels. But nothing could be further from the truth, according to author Clifford Wallach, co-author of \"Tramp Art: One Notch at a Time,\" which profiles individual whittlers.\nSome hobos did whittle wood objects as they sat around the campfire, passing time before turning in for the night. But most items of what now is called tramp art took years to construct, and often they were too large for hobos to carry, Wallach says. He cited a 4 1/2-foot-high model church made of cigar boxes that took one man 30 years to finish.\nCigar boxes were readily available for carving because cigar smoking flourished at the turn of the century and it was illegal to reuse the wooden boxes.\nTramp art-makers also salvaged beads, tacks, marbles, bits of leather, porcelain, glass shards and mirrors for their chipped-carved wonders. While most tramp art is varnished, some pieces are painted or gilded.\nThe craft continued to be popular with thrifty Americans and Europeans through the Depression when many people were out of work. But as the popularity of cigarette smoking increased, cigar boxes became harder to find. This and other lifestyle changes led tramp art to virtually vanish.\nCollectors rediscovered it as part of general renewed interest in American folk art. The craft's appeal is in the details. Collectors especially prize the craft's uniqueness. No two pieces are alike.\n\"It's very personal,\" said Deborah Colman, co-owner of Pavilion, a Bucktown boutique that sells vintage tramp art occasionally. The makers usually made these objects for themselves or members of their families, she said.\nWhile making tramp art has seen a revival of sorts in recent years, it's nothing like the craft's heyday during the late 19th and early 20th Century.\nThe current revival centers around a group of formerly homeless recovering alcoholics who have organized the Hermitage des Artistes, an artist's cooperative and residence in Troy, N.Y. The group produces some 3,500 pieces a year to sell.\nThis tramp art revival gladdens Chicago collector and fragrance designer Jane Gittings, who owns some 200 tramp art end tables, picture frames and boxes.\n\"It's beautiful, new or old,\" she said. \"I'm glad someone's doing it.\"\nWhat to look for\nCollectors should seek tramp art they find appealing and in good condition. They should check for tramp art's hallmarks: extensive notch-carving, extravagant wood layers and inlaid or applied wood decorations such as geometric shapes, hearts, stars, flowers and animals. Authentic pieces are likely to be lined with old newspaper, said Colman.\nWhat to avoid\nCollectors should avoid badly damaged tramp art. \"One of the problems with tramp art is that it wasn't revered, so a lot of it is damaged,\" noted Chicago collector Myra Gotoff.\nDealers and collectors agree, as with all original art, tramp art will increase in value. Tramp art that is signed and dated is particularly valuable, said Wallach. Authentic tramp art objects cost $200 to $1,000, depending on condition and rarity.\nHandle with care\nThe nooks and crannies of most tramp art don't lend themselves to easy cleaning, but collectors have found ways. A few like to spray collectibles with canned air, available at camera supply shops, to get rid of dust. For everyday cleaning, Wallach recommends a feather duster. For more serious cleaning, he uses a toothbrush and a little bit of water.\nWhen it's time for heavy-duty cleaning, Gotoff likes to use a mixture of equal amounts of linseed oil and turpentine.\nCollectors and dealers disagree on whether to repair a broken tramp art piece. \"For a lot of people, (fixing) ruins the value,\" noted Colman. But Wallach believes that minimal repairs can enhance tramp art's value.\nYou can find tramp art at flea markets and antiques stores. Renewed interest has led to inflated prices, so fewer high-quality pieces are on the market, collectors said.\nExamples occasionally are available at Pavilion, 2055 N. Damen Ave., 773-645-0924, and Harvey's Antiques, 1231 Chicago Ave., Evanston, 847-866-6766. Examples from Hermitage des Artistes are available at The Paper Source, 232 W. Chicago Ave., 312-337-0798.\nTo learn more\nHelpful books include \"Tramp Art: One Notch At A Time\" by Clifford Wallach and Michael Cornish (Send $65, plus $5 postage and handling to Wallach, 277 W. 10th St., New York, NY 10014) and \"Hobo & Tramp Art Carving: An Authentic American Folk Tradition\" by Adolph Vandertie and Patrick Spielman (Sterling Publishing Co., $16.95).","Preservation tips for paintings and prints\nArt & Antiques by Dr. Lori\nPeople love their collections. No matter the type of object — cookie jars, military memorabilia, fine art posters — collectors want to add to an existing collection, display their assembled objects, and learn more about their cherished treasures. One of the most important and interesting aspects of collecting is preserving art, antiques, and collectibles for the long term. Many collections include family heirlooms or assembled collectibles that will be handed down to younger generations so preserving a collection is very important. Here are some key points about how to protect, preserve, and enjoy your collections.\nLight is the real problem when it comes to the preservation of paintings and works on paper. UV protection using UV-filtered or opaque materials helps prevent fading and light damage. One of the best ways to preserve fine art is investing in quality framing. For an oil on canvas painting, a frame will protect both the stretcher and the canvas as well as give a finished look to the painting once it is on the wall. Paintings exist best when kept out of direct sunlight and hung away from elements that may spark temperature and humidity changes like heaters, radiators, and air conditioners.\nPrints require a different type of protection when it comes to framing and display. Prints and other works on paper like antique maps, historic documents, and the like should be matted and framed under glass using materials that are free of acid. Acid free materials like mats and storage boxes should have a pH level of 7.0 or greater and the adhesives used in the framing of a fine art print should be pH neutral to protect fragile works on paper. Some acid free materials are made free of lignin, which can produce acid and darken paper, this process is known as acid burning or tanning. Avoid acid burning or tanning whenever possible.\nSome of the most critical damage that happens to art and antiques happens when objects are stored. Although it is little known, significant damage can occur during storage. When you first put an antique object away in storage, everything is fine but over time, changes in temperature and humidity can occur. When no one is looking, other affects may take place which will impact the condition and value of an antique or collection. It is important to store objects in archival boxes intended for a certain type and size of collectible. Physical support is necessary for fragile objects and storage containers like archival boxes need to be constructed to stand the test of time. What you put into a storage box like acid free tissue paper along with an antique is as important as the storage container. One size does not fit all when it comes to archival storage.\nLarge paintings should be stored off the floor, preferably hanging up even in storage locations. If there is no room for a hanging storage solution, then store large paintings standing upright in a closet or storage area. While it may seem like a convenient place to store paintings, never lay paintings flat, face up under a bed. This will put stress on the stretchers and the canvas itself. Smaller paintings may be stored upright back to back and face to face on separated shelves. Use acid free foam core dividers to prevent the wire from the back of one painting from scratching the frame or canvas on the front of another painting positioned next to it. There are specific techniques to protecting art, a good rule of thumb is to handle with care, display works of art away from direct sunlight and store works in areas where temperature and humidity fluctuations are minimal.\nDr. Lori Verderame is the award-winning Ph.D. antiques appraiser on History channel’s #1 hit show about the world’s oldest treasure hunt, The Curse of Oak Island. For more information, visit www.DrLoriV.com and www.YouTube.com/DrLoriV."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:35d73342-fcc5-43b0-9cdc-ef3736af29c0>","<urn:uuid:1892276b-6ff4-4ac9-b00c-da15321a2cb7>"],"error":null}
{"question":"What are the main health benefits of chia seeds for the digestive system?","answer":"Chia seeds are beneficial for digestion because they contain over 10 grams of soluble fiber per two tablespoons. They can absorb up to fifteen times their weight in water, expanding in the stomach to become gel-like. This expansion helps people feel fuller, slows absorption, and can aid with weight loss.","context":["In at least one Mayan language, ‘chia’ means ‘strength’, and seems reasonable to conclude that this probably derives from the huge amounts of energy stored within the tiny seeds body. In the past, the chia seed held a supernatural aura. Today the acclaim and prestige they carry acts as a kind of twenty-first-century version of this.April 30, 2019 2:33 pm\nFirst recorded by the Aztecs around the middle of the third millennia BC, chia has been a staple of the people occupying the territory we today call Mexico. Between the years 1500 – 900 BC, chia was cultivated intensely and on a mass scale in Teotihuacan and by the people of Toltec culture. It would be consumed in either its full form, ground into a flour and used in baking, or used as a medicinal rub. Some historians believe that chia also played a role in human sacrifice. However, the idea of widespread human sacrifice is today adamantly contested by numerous archaeologists leading the Mesoamerican field (see the work of Elizabeth Graham).\nThe Tarahumara runner\nLiving in the rugged and hot Sierra Madre’s copper canyon, Mexico’s Tarahumara tribe is world renowned for their long-distance runners. They incorporate chia into the diet through a drink made up of squeezed lemon, water and chia which they call Iskiate. Legend has it that only after drinking Iskiate are the Tarahumara able to run for such great lengths.\nThe longest record the tribe holds for running without pause is a staggering 435 miles, equivalent to 16 back to back marathons. And, what’s even more impressive is this jaw-dropping feat took place in just over 48 hours!\nThe Tarahumara have lived in this hardy territory for around 500 years and their name comes from their ability to run further than almost any other population on earth, without shoes. Those opting to don footwear choose a minimal sandal made of either deerskin or discarded tire rubber. This result of this – runners of the Tarahumara have little to no protection around their feet.\nCould diet be the answer?\nMuch research has gone into the physical and cultural elements which distinguish these people from their non-traditional Mexican neighbours. It is believed that because running is such an integral part of inter-village communication and the only cultural form of athletic expression available to them, the Tarahumara have undergone physiological adaptations to cater for this. Other research looking at their diet forwards an idea more closely resembling that of their ancient ancestors. That it is, in fact, chia which accounts for the superhuman endurance they enjoy.\nAnother side of the debate grows out of the question of whether this endurance is based in our evolutionary history as hunters? Hundreds of thousands of years ago early hunter-gatherers pursued pray over extremely long distances. Chasing animals until they died of heat exhaustion is a very specific form of hunting termed persistence hunting. The Tarahumara still practice it today. By chasing animals in a way that makes them gallop you force their body temperature to spike. The way mammals cool down is by sweating combined with panting and when quadrupeds gallop they are unable to pant which leads to their overheating.\nIt may be just one or it may be a combination of all three of these reason, but it remains true that the Tarahumara possess extraordinary endurance. Whether this is down to chia is yet to be fully confirmed, but this magical little seed still holds an important role in the lives of these world-class athletes.\n\"The longest record the tribe holds for running without pause is a staggering 435 miles, equivalent to 16 back to back marathons.\"\nWhat is chia?\nChia seeds come from the plant called Salvia hispanica, which originates in Central America, or Salvia columbariae that grows in Mexico and south-western US. Chia are a tiny oval seed with an impressive nutritional profile. The plethora of health benefits may be new to you, but this ingredient has built a name for itself since the times of the Aztecs. In the plant world, chia seeds are quite a fierce competitor.\nChia health benefits\nIt is no wonder that chia seeds have long been a food staple. They pack a large amount of nutrients into their small size. Most notably, chia seeds are a great source of protein, fibre and omega-3 fatty acids. For instance, they have been studied for their benefits on heart health and digestive system, as well as their ability to help build stronger bones and muscles. They may also promote skin health and reduce signs of ageing.\nChia seeds are a great source of soluble fibre.\nLess than two tablespoons (about one ounce) of chia boast over 10 grams of fibre (out of 12g of carbs).(1) If you found yourself alarmed by the amount of carbs, this just goes to show that not all carbs are created equal. Fibre is a type of non-digestible carbohydrate that promotes gut health. Moreover, the high fibre content makes chia, along with flax and hemp, a natural blood balancer.\nChia seeds are unique in that they absorb up to fifteen times their weight in water. In our stomach, these seeds expand and become a gel-like substance. This expansion can make us feel fuller, slow absorption and aid with weight loss.(2)\nChia are one of the best plant-based sources of omega-3s.\nThe same two tablespoon serving of chia contains 9 grams of fat, 5 of those coming from omega-3 unsatturated fatty acids. If we are simply looking gram for gram, chia appears to be a better source of omega-3s than salmon. However, it is important to distinguish the two as animal versus plant omega-3s are different. With chia seeds, we are getting alpha linolenic acid (ALA), which our body must convert into active forms to be useful.(8)\nActive omega-3 fatty acids are eicosapentaenoic acid (EPA) and docosahexaenoic acid (DHA).(9) For instance, salmon contains DHA, which is a long-chain omega-3 fatty acid. DHA is a component of every cell in your body and a vital structural component of your skin, eyes and brain. ALA can be converted into DHA but bear in mind that the process is not very efficient.(10) So, while chia seeds do provide a lot of ALA, it is also important to consume other sources of omega-3 fatty acids.\nChia seeds are rich in minerals.\nChia seeds boast an impressive amount of minerals, which are essential for your wellbeing. A two tablespoon serving of chia seeds provides nearly a third of the recommended daily amount (RDA) of manganese and magnesium, more than a quarter RDA phosphorus and nearly a fifth RDA calcium.(1)\nChia is a good source of protein.\nA two tablespoon serving of chia provides 4.4 grams of protein.(1) With all nine essential amino acids, chia is considered a complete plant-based source of protein. Nevertheless, overall chia seeds rank relatively low in protein compared to other plants. Also, chia seeds only contain traces of lysine, an essential amino acid for the production of carnitine.(3) Carnitine is a compound responsible for converting fatty acids into energy and helping your body to lower cholesterol.(4)\nIn general, a good rule of thumb when partaking in a plant-based diet is to combine complementary sources of protein. A lot of times these are incomplete on their own, but when combined they can provide all the amino acids. Eating rice and beans in combination is a common example of complementary protein sources.(5) Amaranth is also an excellent source of protein. Learn more about amaranth here.\nKey chia seed benefits\n- Promotes digestion\n- Helps boost energy\n- Promotes healthy brain activity\n- Promotes healthy heart function\n- Supports healthy hair and skin\nTry these wholesome foods made with chia seeds\nThis granola is 100% organic, raw, gluten-free and vegan. Alongside chia seeds, this wholesome breakfast snack is made with other impressive ingredients, such as tigernut tubers, nopales, and sprouted buckwheat, all excellent for gut health. Sweetened with apples and raisins, this granola does not contain any processed sugar.\nCold-pressed from organically grown chia seeds, this oil is an excellent source of omega-3 fatty acids and vitamin E. Vitamin E is a powerful antioxidant helps to protect your body cells from the damaging effects of the free radicals. Chia seed oil is a perfect addition to soups and pastas.\nPulverised from organic chia seeds, raw chia powder is easier digested by the stomach that whole chia seeds. Chia powder is ideal to add to smoothies, porridge and baked goods.\nRecipes with chia you can’t resist\nStart your morning with this wholesome Porridge with chia and sea buckthorn oil recipe. It will will keep you full for longer and fuel your body with nutrients and energy.\nIf you love baking, make this delicious Gluten-free nut bread with hemp and chia recipe. The combination of hazelnuts, pumpkin and sunflower seeds, as well as raw hemp and chia powders creates a beautiful earthy flavour that goes perfectly with this Velvet beetroot and aronia soup or Spiced carrot and ginger bisque. This is an excellent option for someone with gluten intolerance. Moreover, this bread also aids digestion and nourishes your body with valuable minerals and plant-based protein.\n(1) “Seeds, chia seeds, dried”. Self Nutrition Data, https://bit.ly/2PN24Kc.\n(3) “Thermal and Physicochemical Properties and Nutritional Value of the Protein Fraction of Mexican Chia Seed (Salvia Hispanica L.)”. Food Science and Technology International, https://bit.ly/2GNZAYE.\n(4) “L-Lysine”. Amino Acid Studies, aminoacidstudies.org/l-lysine/.\n9 May 2019\nAna is sharing her favourite cookbooks\nInspired by her grandmother’s cooking, Ana Rusu likes to experiment within the traditional cuisine and make plant-based recipes that are out of this… Continue\n23 Apr 2019\nCee Fee on feeding her toddler\nCee Fee Dunn is a certified personal trainer, a coach and a mother to a toddler. She is sharing with us how she… Continue"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:31989e3d-4d09-4a32-b7db-317b47ef5aa3>"],"error":null}
{"question":"Which faced compulsory military service earlier - New Zealand or Canada? I'm studying military history and want to understand how these countries approached conscription.","answer":"New Zealand implemented compulsory military training earlier than Canada. New Zealand introduced compulsory military training (CMT) in 1911, which formed the basis of their Territorial Force. In contrast, Canada did not have compulsory military service until later, and notably ended conscription through a Cabinet order in 1946 which granted amnesty to those who went AWOL from the armed services.","context":["Story: Armed forces\nPage 1 – Historical overview\nNew Zealand has had its own armed forces since the early days of European colonisation. The first unofficial military organisation was the Kororareka Association (1838–40). Local volunteer forces were established in the early 1840s but were disbanded when the Militia Ordinance 1845, which authorised the raising of compulsory militias to supplement imperial (British) troops, was issued. Imperial troops were stationed in New Zealand from 1840 to 1870.\nUnder the Militia Ordinance 1845 all able-bodied European men aged between 18 and 60 could be called out for compulsory training or service within 25 miles (40 kilometres) of their town. Militia service was unpopular because the pay was low, and communities and families suffered economically when their men had to leave paid work to attend. Militia forces saw active service during the New Zealand wars, and the last force – the Taranaki Militia – was released from service in 1872. Militias were never used again but provisions for their use remained in legislation until 1950.\nVolunteer units were established during the New Zealand wars. The first permanent military force was the Colonial Defence Force, which was active from 1862. This was replaced by the Armed Constabulary, which performed both military and policing roles, in 1867. After being renamed the New Zealand Constabulary Force, it was divided into separate military and police forces in 1886. The military force was called the Permanent Militia and later renamed the Permanent Force.\nIn 1910 the Territorial Force was established, replacing Volunteers. This reserve force formed the foundation of the army. By then, the Permanent Force had evolved into the Royal New Zealand Artillery. This, along with the Royal New Zealand Engineers and the New Zealand Staff Corps (who were in charge of the Territorial Force) became the army’s professional, permanent centre.\nNew Zealand naval volunteers were first formed in the 1860s. The New Zealand Naval Forces and a New Zealand branch of the Royal Naval Reserve were established by the Naval Defence Act 1913. The first warship (HMS Philomel) was purchased in 1914. This was followed by the formation of the New Zealand Division of the Royal Navy in 1921 and the Royal New Zealand Navy in 1941.\nNew Zealand’s air force has its origins in the gift of an aircraft to New Zealand by the United Kingdom-based Imperial Air Fleet Committee in 1913. The New Zealand Permanent Air Force was created within the army in 1923, and the name was changed to the Royal New Zealand Air Force in 1934. It separated from the army in 1937.\nCompulsory military training and conscription\nCompulsory military training (CMT) began in 1911 and formed the basis of the Territorial Force. During the First World War the compulsory scheme was, in effect, replaced by conscription for overseas service from 1916. CMT continued after the war, but budget restrictions limited its scope and it was suspended in 1930.\nConscription was re-introduced in the Second World War. During both wars, conscription was used because the number who volunteered did not match the needs of the war effort.\nCMT was introduced again in 1949 but could not meet the technical needs of the navy and air force, and produced more servicemen than were required for the army. The scheme ended in 1959 and the strength of the Territorial Force declined almost immediately. By this time the balance had shifted from amateurs to professionals. Large-scale warfare was now unlikely, and highly trained permanent or regular forces were needed mostly for more localised conflicts overseas.\nA more limited form of national service (selective conscription by ballot) was introduced in 1961. This ended in 1972. The Territorial Force became a voluntary reserve service which supplemented the regular, professional force.\nDuring the First and Second world wars, the armed forces expanded to meet the needs of global warfare. In the Second World War 194,000 men (67% of those between 18 and 45) and 10,000 women served.\nThe armed forces diversified over time. Compulsory training initially did not apply to Māori, but some had volunteered for the Volunteers and Territorial Force. A small contingent of Māori fought in the First World War, but it was not until the Second World War that Māori participation became more prominent.\nAs Māori moved into the cities after the war, their proportion in the armed forces increased and they became moderately overrepresented compared to their proportion of the general population. In 2011 Māori comprised 17% of the regular force (compared to 14.6% of the general population in the 2006 census).\nWomen served as nurses in the First World War. In the Second World War women’s roles diversified, though no women actually fought. After the war women’s armed services were retained but participation was low – around 4–5% of the forces in the 1950s and 1960s.\nOver time more branches of the armed forces were opened to women and separate women’s services ended in 1977. However, human-rights laws reserved the right of the armed forces to give preferential treatment on the basis of gender in relation to combat roles. The Human Rights Commission Act 1977 and the Human Rights Act 1993 both contained particular provisions which allowed this discrimination. These provisions were repealed by the Human Rights (Women in Armed Forces) Amendment Act 2007. By this stage, restrictions on women’s service had already been lifted by the New Zealand Defence Force – the change in law reflected an existing change in practice.\nBy 2000 all restrictions on women engaging in combat had been lifted. In 2011, 16% of the regular forces were women.\nHomosexual acts between men were illegal in New Zealand until 1986. During the first and second world wars, members of the armed forces found to have engaged in homosexual acts were imprisoned for disgraceful conduct and ignominiously discharged. From 1950 most were discharged from the forces without prosecution.\nThe Homosexual Law Reform Act 1986 decriminalised homosexuality, but the armed forces were exempted from its provisions. Discrimination based on sexuality in the armed forces became unlawful after the passing of the Human Rights Act 1993. The chief of defence force approved the establishment of an LGBT (lesbian, gay, bisexual and transgender) support group in 2011.","26 December 1791 - British Prime Minister William Pitt passes the Constitutional Act, dividing Quebec along the Ottawa River into Upper and Lower Canada, each with a Lieutenant Governor and a Legislature.\n28 November 1812 - The Canadian and British militias defeat the second American invasion of the War of 1812 at the Niagara River crossing near Fort Erie\n26 September 1826 - The town of Bytown is incorporated. In 1955, Bytown will become the city of Ottawa and in 1867 it will officially become the capital of Canada\n14 November 1849 - Toronto becomes the capital of Canada following the burning of the Montreal parliament building by a Conservative mob; it would remain the capital until 1867\n12 May 1870 - The Manitoba Act is given Royal Assent, creating Canada's fifth province\n27 November 1885 - Wandering Spirit was hung alongside seven other Cree persons for the murders at Frog Lake on 1 April; these were the last public executions in Canada\n12 December 1885 - The first Canadian Pacific Railway freight train begins travelling to Montreal carrying Manitoban wheat\n18 September 1895 - Future Prime Minister John G. Diefenbaker is born in Neustadt, Ontario.\n7 November 1907 - The test tokens for the first issue of Canadian coins are struck at the Ottawa branch of the Royal Mint\n19 December 1917 - The first two games are played in the National Hockey League\n31 October 1918 - The Alberta government prohibits all gatherings of seven or more people in response to the outbreak of Spanish flu and churches, schools and theatres are all forced to close\n3 October 1927 - Canadian Prime Minister William Lyon Mackenzie King makes the first transatlantic telephone call to British Prime Minister Stanley Baldwin\n13 March 1928 - Eileen Vollick (1899-1972) becomes the first Canadian woman to obtain a pilot's license\n18 October 1929 - Canadian women are finally declared “persons” under the law in culmination of the famous “Persons Case.”\n6 June 1944 - Canadian troops land on Juno Beach as part of the allied invasion of Normandy on D-Day\n5 September 1945 - Canada’s first nuclear reactor becomes operational in Chalk River, Ontario\n8 August 1946 – A Cabinet order ends conscription for Canadian military service and grants amnesty to those who went AWOL from the armed services since the previous New Year’s Day\n1 January 1947 - The Canadian Citizenship Act comes into effect in Ottawa, Ontario.\n1 August 1959 - Georges-Philias Vanier becomes Canada’s first French Canadian Governor General\n10 August 1960 - The Canadian Bill of Rights was enacted by Prime Minister John G. Diefenbaker.\n17 April 1962 - Queen Elizabeth II signs the Royal Proclamation of the Canadian Constitution.\n15 February 1965 - Canada's new Maple Leaf flag is flown on Parliament Hill.\n10 October 1970 - The October Crisis comes to a head when FLQ terrorists kidnap Minister Pierre Laporte in response to the rejection of their demands by the Quebec government\n5 December 1976 - The first 4 Canadian female Rhode Scholars are chosen\n22 August 1979 - Former Prime Minister John G. Diefenbaker is buried on the University of Saskatchewan Campus\n19 September 1980 - Terry Fox is invested as a Companion of the Order of Canada in recognition of his marathon across Canada which raised millions of dollars for cancer research\n1 September 1982 - The Canadian Charter of Rights and Freedoms is adopted as part of the Constitution under Prime Minister Pierre Trudeau.\n25 July 1990 - Lucien Bouchard announced the formation of the Bloc Quebecois\n24 October 1995 - The 50 year anniversary of the United Nations brings together the largest gathering of world leaders in history, including Prime Minister Jean Chrétien"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:34ec2ec4-5362-49bb-a5a0-23a55b57e57f>","<urn:uuid:149592cd-a5bd-486e-ad75-28aaaf559c00>"],"error":null}
{"question":"What were the significant post-WWII military missions and roles of the PB4Y-2 Privateer aircraft?","answer":"After WWII, the PB4Y-2 Privateer served in multiple military roles. It was used as a hurricane hunter and played a key role in Reserve squadrons for training Naval Reservists. During the Korean War, mothballed Privateers were reactivated to detect North Korean infiltrators along the coasts using air-to-surface radar. They also flew nighttime Firefly missions, dropping flares to enable round-the-clock air support. Some Privateers were modified to deliver second-generation atomic bombs, while others were optimized for Ferret missions to gather electronic intelligence. The French colonial forces used 22 Privateers in Vietnam as bombers until after Dien Bien Phu. The last military Privateers served as radio control target drones in the early 1960s, with the final one, nicknamed 'Lucky Pierre,' being shot down by a Bullpup missile with an experimental proximity fuse.","context":["| Consolidated PB4Y-2 Privateer\nby Dennis Baer\nHaving belatedly, come to the conclusion that sea plane hulls were not compatible with efficient airframe aerodynamics, the U.S. Navy added land based patrol aircraft to their air arm in early WWII. Before the war, the Army Air Corp had set Navy brassís teeth on edge by using their early Boeing B-17 bombers to demonstrate long range interception of inbound surface ships. The Navy had countered this threat to their monopoly on defending America's coasts by getting the War Department to ban any maritime patrols by Army bombers that were more than 100 miles from shore. This bit of foolishness was instantly forgotten when Nazi submarines began sinking Allied trans-Atlantic shipping in record numbers.\nBeing late to the party, the Navy had to accept Consolidated B-24 Liberators as patrol planes. The sleeker, easier to fly B-17s were all reserved for Army Air Corp use. Eager to reduce crew fatigue on long patrols, Consolidated was instructed to allocate three B-24s for conversion to Navy requirements. To ease control problems, Consolidated had already attempted to change the B-24's twin rudders to a single vertical fin. The first conversion, the XB-24K, done at the Consolidated San Diego plant used the tail from a Douglas B-23 Dragon. The results were unsatisfactory.\n|The Ford Motor Company, which operated a huge B-24 license-production plant called Willow Run in Michigan, then transplanted a vertical fin from a C-54 transport onto one of their B-24s, creating the XB-24N. They also added a new custom built horizontal fin and replacing the nose turret with an ERCO 250 SH ball turret, which was more streamlined than the standard nose turret of production B-24s. It was superior enough to the standard B-24J that the Army ordered seven more pre-production YB-24Ns and inked an order for 5,168 B-24N bombers. Horrified that a mere car manufacturer might have upstaged them, Consolidated pulled strings to have this order canceled. Equally horrified that they might be forced to accept some of these planes instead of getting their own design, the Navy helped to kill the N-model.|\nChanges from the B-24 included a single vertical fin and replacing the nose turret which was more streamlined than standard models. The forward fuselage was lengthened and there were changes in armament. The Privateer used the same wing and landing gear as the B-24.\nMeanwhile, new fuselages were constructed for the Navy PB4Y-2s in San Diego. For the second time, the basic Liberator fuselage was lengthened (legendary founder of Consolidated, Ruben Fleet personally ordered the first B-24C and all subsequent Liberators to have the nose section lengthened 2 feet, 7 inches at company expense \"to improve the appearance.\" He could have saved the effort; it was still a homely plane). The new fuselage was now a full 74 ft, 7 inches, 10 feet 5 inches longer than the first B-24A. The space was devoted to new electronics and a flight engineerís station. The first three Privateers (the initial name \"Sea-Liberator\" had been quickly dropped) flew with standard twin tails. When the new single tail was ready, it caused the Privateer to tower a hanger-roof-scraping 29 feet 1 5/8 inches off the floor.\nA second dorsal turret was fitted with twin .50 caliber machine guns, as well as a pair of ERCO 250 THE turrets which replaced the two flexible, hand held guns of the Liberator's waist gunners. The Navy, in it's infinite wisdom, noted that the teardrop shaped waist turrets could be depressed downward to such a degree that the fields of fire from these guns converged at a point 30 feet below the belly of the plane. For this reason, no belly turret was fitted. How the waist gunners were suppose to stand in their turret blister and see to aim their guns under their feet without falling over was not explained.\nThe trademark of the Liberator family, the 110 foot long, twin spar Davis High Lift wing with it's massive Fowler flaps was unchanged. This was the secret to the long range and relatively high speed of the Liberator/Privateer family, and the reason the Privateer stayed on in squadron service long after the rival B-17 had been retired.\nFour Pratt & Whitney R-1830-94 engines were retained although they were fitted with only a single stage supercharger. As low altitude patrol planes, the two stage supercharger was considered unnecessary. The oil cooler air inlets were moved to above and below the engine, eliminating the characteristic horizontal oval appearance of the engine cowl. Military and T.O. power was rated at 1,350 hp (1,000 kW) each. Later, when some survivors of Navy and Coast Guard service were modified for use as fire bombers, they were fitted with 1,700 hp (1,270 kW) Wright R-2600 engines.\nThe two distinguishing features between the B-24 and the Privateer were the tail configurations and the engine cowlings. Cowling configurations for the oil coolers differed depending on the engine installation.\nTwelve .50 caliber Colt-Browning M2 machine guns were carried, all in powered turrets. This was sometimes augmented with one or two 20 mm nose cannons for strafing. Normal bomb load for a 1,300 mile radius patrol was 4,000 pounds of bombs, depth charges or mines. Some planes were also armed with one or two ASM-N-2 Bat anti-shipping glide bombs which was first used successfully during a raid against Japanese shipping in Valikpapan Harbor, Borneo in April of 1945.\nOperationally, the classic use of patrol bombers is to hunt down and sink enemy ships. The Privateer stepped easily into this role, the way having been paved by years of anti-submarine and anti-ship operations in Navy PB4Y-1 Liberators and USAAF B-24s equipped with a series of radar sets collectively known as \"Low Altitude Bombing\" sets. By WWII standards, the Privateer was lavishly equipped with an electronic suite that could be customized on a mix and match basis so that Privateers could be airborne communication platforms, radar and radio station hunter/killers, anti shipping search and destroy units, weather reconnaissance planes or search and rescue units to locate downed airmen with their radio direction finders. If the situation demanded, they could even act as their own standoff anti radar jamming unit.\nPatrol craft are not glamorous, like fighter planes, or vital to the troops on the ground like bombers, close support attack planes, or the cargo planes that keep them supplied. What the Privateer lacked in pizzazz, it more than made up for in versatility and practicality. The Navy wanted the seas swept clear of enemy transport, enemy radars, enemy radio navigation aids, and enemy scouting vessels. It wanted mines planted, submarines harassed or destroyed, communications augmented and weather information for 1,300 miles around the Privateer's base. No other aircraft was as capable of this as the Privateer .\nThe last Privateer was delivered in October of 1945. Of 739 airframes built, there may be two in flying condition today.\nAfter WWII, Privateers were used as hurricane hunters and played a large role in Reserve squadrons, helping to keep up training for thousands of Naval Reservists. In 1950, numerous mothballed Privateers were recalled for service in Korea, where their air-to-surface radar was used to hunt down and destroy North Korean infiltrators along the coasts. They also flew dangerous nighttime Firefly missions to drop flares over embattled United Nations troops so that air support could continue around the clock.\nThe US Coast Guard removed the side and nose turrets and rebuilt the nose of the plane with a huge glazed observation dome.\nOther Cold War missions included the theoretical deliver of nuclear weapons by Naval aircraft and an unspecified number of PB4Y-2s were modified to deliver second generation atomic bombs. Other airframes were optimized for Ferret missions to gather electronic intelligence. These are high value/high risk missions, often employing a long series of uncomfortable, tension filled runs aimed at approaching or penetrating enemy territory to eavesdrop on enemy radar signals or radio traffic. Most valuable for intelligence analysts is the air-to-ground and air-to-air communication between interceptor controllers and interceptor pilots. The easiest way to get this chatter is to provoke the enemy air defenses into launching an interception of your own aircraft. The hard part is getting away alive after they do. On April 8th, 1950 a Privateer from VP-26 was caught by Soviet MiGs over the Baltic Sea and destroyed with all crewmembers either killed in the crash or strafed to death. Apparently a number of the 38 Privateers seconded to the Nationalist Chinese Air Force suffered similar fates at the hands of the Red Chinese People's Army Air Force.\nOther post war service included 22 Privateers provided to Aeronautique Navale for service with the French colonial forces in Vietnam. They were used as bombers until after Dien Bien Phu, with four lost in combat. Six were returned to U.S. service, the remaining twelve were flown to North Africa where they fought in Algeria and later during the Suez Incident. In 1961, the survivors were scrapped in favor of new Lockheed P2V Neptunes, a fate shared by most other Privateers.\nThe longest serving Navy Privateers were expended as a radio control target drones off Point Magu, California in the early 1960's. The last, flying under the call sign Opposite 31 and carrying the ironic nickname Lucky Pierre was shot down by an Bullpup missile with an experimental proximity fuse that turned the Bullpup into an air-to-air weapon. The relatively large warhead detonated over the back of the Privateer's wing, sparking a spectacular fireball of burning fuel and marking the end of the Privateer's military career.\nA small number of ex-US Coast Guard search and rescue Privateers survived to become anti-forest fire water bombers. The Coasties had removed the side and nose turrets and rebuilt the nose of the plane with a huge glazed observation dome. After being sold off as surplus, civilian operators gave them new Wright R-2600 engines along with the installation of their borate/water slurry tanks. These were Super Privateers\" and these planes soldiered on into the 1990's.\nA modified Consolidated PB4Y was donated to the Yankee Air Force Museum in Ypsilanti, Michigan. The starboard turret is missing but Yankee Air Force Museum hopes to replicate the port side. |\n|One of these planes (call sign \"Tanker 125\") was donated to the Yankee Air Force Museum in Ypsilanti, Michigan. You will note from the rear view that only one ERCO 250 THE turret is installed. Unfortunately, only this portside turret was available, the starboard one having been scrapped. Eventually, the museum hopes to build a \"mirror image\" starboard turret and fit it to this static display plane.|\n|Consolidated PB4Y-2 Privateer|\n|Wing span:||110 ft 0 in (33.53 m)|\n|Length:||74 ft 7 in (22.73 m)|\n|Height:||30 ft 1 in (9.17 m)|\n|Empty:||37,485 lb (17,003 kg)|\n|Max Gross:||65,000 lb (29,484 kg)|\n|Maximum Speed:||237 mph (381 km/h) at 13,750 ft (4,190 m)|\n|Cruise:||140 mph (225 km/h)|\n|Service Ceiling:||20,700 ft (6,310 m)|\n|Range:||2,800 miles (4,506 km)|\nFour, Pratt & Whitney R-1830-94 Twin Wasp |\n1,350 hp (1,007 kW), 14-cylinder radial, air cooled engines.\nTwelve 0.50 machine guns (nose, turrets, waist and tail positions)|\nBomb load 12,800 lb (5,806 kg).\nReturn To Aircraft Index.\n©Dennis Baer The Aviation History Online Museum.\nAll rights reserved.\nCreated March 3, 2008. Updated January 14, 2014."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:d932787e-5a89-4e6a-a086-d0b242100061>"],"error":null}
{"question":"Looking back at these climate agreements - how did they handle the issue of compensating poorer countries for climate damage? Pretty crucial stuff!","answer":"The treatment of climate damage compensation evolved significantly. The Paris Agreement established a formal mechanism to address 'loss and damage' caused by climate impacts, covering issues like climate-related insurance and displacement of people. This was an improvement over previous arrangements, as including it in the Paris Agreement made it more permanent compared to earlier COP decisions that could be overturned. However, there was disappointment in the actual implementation - while wealthy nations promised to provide $100 billion per year for climate-related projects, the commitments for loss and damage compensation were not as firm or substantial as vulnerable countries wanted. As Tenzin Wangmo from Bhutan noted, the outcome on loss and damage fell short of expectations.","context":["World leaders commit to ambitious goals at U.N. climate summit\nMICHEL MARTIN, HOST:\nThere was an important decision on climate change today at the U.N. meeting in Glasgow, Scotland. Almost 200 nations agreed to set more ambitious targets for cutting greenhouse emissions. They also called for more climate-related aid to developing countries. It's being called a landmark agreement. And NPR's Dan Charles is here to tell us more about it. Dan, thank you so much for joining us.\nDAN CHARLES, BYLINE: Hi, Michel.\nMARTIN: So please give us the high points of the agreement.\nCHARLES: It is a significant step forward when it comes to targets for greenhouse gas emissions. It reaffirms an agreement from six years ago, which said nations will cut their greenhouse gas emissions enough to keep the warming of the planet to what they said well below 2 degrees, preferably 1.5 degrees. Most countries are nowhere close to reaching that goal as yet. But this deal puts the focus on that lower 1.5 degree target. And it says to get on that path, countries have to act right away. Next year, they are now supposed to deliver plans to cut their emissions by 45% within 10 years. And for the first time in one of these meetings, it specifically talks about getting rid of burning fossil fuels.\nMARTIN: You know, you have countries that depend heavily on exports of coal and petroleum. So did many countries object to that?\nCHARLES: There were objections. It was difficult at times. This is a consensus process, no voting. At the very last minute today, India and some other countries weakened one phrase from phasing out coal to just phasing it down. But you also had some powerful voices from countries whose survival is at stake from rising seas, such as Aminath Shauna from the Maldives.\n(SOUNDBITE OF ARCHIVED RECORDING)\nAMINATH SHAUNA: The difference between 1.5 and 2 degrees is a death sentence for us.\nCHARLES: And in the end, they did get this call for a massive shift away from fossil fuels.\nMARTIN: So could you tell us more about this idea of providing more aid to less wealthy countries to help them deal with climate change? We've been talking about this for a long time.\nCHARLES: Right. This was a major demand from the African group, the less-developed country group. They said, this is a huge fairness issue. We are suffering from the effects of climate change, but we didn't cause it. And we can't build this new clean energy future on our own.\nThe final deal did include some attempts to address this. Wealthier countries say they will finally deliver on a promise they made a decade ago to provide a hundred billion per year for climate-related projects. And there's this new section promising more money, eventually, for what's called loss and damage, compensating countries for the harm that warming temperatures are now causing. But that section was not nearly as much or as firmly committed as the vulnerable countries wanted. Tenzin Wangmo from Bhutan pointed that out today.\n(SOUNDBITE OF ARCHIVED RECORDING)\nTENZIN WANGMO: We would like to register that the outcome on loss and damage was not what we expected. But at this hour, we do understand this is not the time for us to fall back into our differences.\nCHARLES: So lots of disappointment there but not enough to sink the deal.\nMARTIN: Now I want to remind everybody that this just happened. But what reaction are you hearing so far?\nCHARLES: It's a mix. Environmental - environmentalists are glad that the goal of dramatic, immediate cuts in greenhouse gas emissions stayed in this document. But among some groups, there is a lot of anger about the lack of committed aid to developing countries.\nMARTIN: Before we let you go, Dan, I want to ask about what role the U.S. played. I mean, the Trump administration rejected these climate talks, and the Biden administration is making climate a big focus. How did that go?\nCHARLES: Former Secretary of State John Kerry was here the whole time, very visible, talking about the need for aggressive action to cut emissions. But when it came to the climate finance side, he did more talking than acting. The less-developed countries really wanted to set up a new organization to handle loss and damage. The U.S. helped block that idea.\nMARTIN: Well, that is NPR's Dan Charles in Glasgow. Dan, thank you so much for bringing us up to date.\nCHARLES: Thank you.\n(SOUNDBITE OF KIANA LEDE SONG, \"SEPARATION\") Transcript provided by NPR, Copyright NPR.","Understanding the Paris Agreement, where to from here, with S.P.R.E.P\nThe adoption of the Paris Agreement at the recent 21st Conference of the Parties (COP) to the United Nations Framework Convention on Climate Change (UNFCCC) marks a potentially revolutionary moment in the history of the battle against climate change.\nFor the first time we now have a global agreement that obliges all countries who are ‘Parties’ to the agreement to take meaningful action on climate change.\nThe main difference with the Kyoto Protocol, which had obligations for developed country Parties only – meaning that some larger developing countries, such as India, China, and Brazil, for example, had no obligation to reduce their emissions. Under this new Agreement, all countries are now required to reduce their greenhouse gas emissions and take some form of action to address climate change.\nWhat does the Paris Agreement mean for the Pacific? Agreement to limit global temperature change\nAs mentioned above, the Paris Agreement called for all parties to limit global temperature change to well below 2 degrees Celsius above pre-industrial temperature levels. To put this into perspective, scientists have noted that the global temperature has already increased by 0.7 degrees from pre-industrial levels. The Intended Nationally Determined Contributions (I.N.D.Cs) collected before COP 21 in Paris will put the world in a 2.7 degrees pathway.\nUnder the Paris Agreement, countries must take actions to ensure that the temperature increase does not exceed 2 degrees above pre industrial levels. Recognising that this level of warming would be too high for many vulnerable nations, the Paris Agreement also calls for countries to make further progress to limit warming to no more than 1.5 degrees Celsius above pre-industrial levels. This was a key ask from political leaders of the Pacific.\nEfforts aimed at the 1.5 degree target will be supported by the obligation of all countries to look at the long term picture through their low carbon emission development strategies.\nUnder the previous arrangements, the majority of climate change funds focussed on greenhouse gas emissions known as ‘mitigation’ actions, with less emphasis on adaptation funding which is an important source of funding for the Pacific, to help address the impacts of climate change. Under the new Agreement, equal weight will be placed on mitigation and adaptation funding, which will bring about more adaptation funding opportunities to the Pacific.\nSupport for Adaptation\nWhile the Kyoto Protocol focused almost purely on mitigation, the Paris Agreement adopted a global goal for adaptation – to enhance the capacity to adopt, strengthen resilience against impacts and reduce vulnerability to climate change. This has elevated the status of adaptation giving it equal importance as an issue. This is particularly beneficial to countries in the Pacific, who are amongst the most vulnerable to climate change.\nA mechanism to address Loss and Damage caused by Climate Change\nThe Paris Agreement also provides for a mechanism to address loss and damage, caused by the impacts of climate change. While previous COP’s had agreed to such a mechanism known as the Warsaw International Mechanism on Loss and Damage, the danger was that COP decisions could be over turned and may not be permanent. This new mechanism in the Paris Agreement spans issues such as insurance for climate change related events, and how to deal with the displacement of people due to climate change.\nWhat are the obligations of countries under this Agreement?\nUnder the Paris Agreement there are different sets of obligations - some for all countries, some for developed countries only, and some for developing countries such as those in the Pacific island region.\nAll countries will now have to formulate national strategies to reduce their emissions of climate changing gasses - mitigation against climate change. These will be in the form of ‘Nationally Determined Contributions’ where governments decide at the national level how they might best do this, for example, through their energy or transportation sectors, or in comes cases, through the protection of national forests which take carbon out of the atmosphere and store it.\nAll countries will be required to report against the actions that they take to both mitigate against climate change, and also what they are doing to adapt to climate change.\nThis information will be used in a Global Stocktake exercise held every five years. This ‘Global Stocktake’ will be used to measure progress against limiting the global temperature, and also to inform future discussions on whether or not more action is needed. Furthermore, all countries will have to formulate longer term strategies aimed at low carbon emission development.\nDeveloped countries will have much more stringent reporting requirements, and are requested to provide additional resources such as financial, technology and capacity building, to assist developing countries to meet their climate change obligations.\nDeveloping countries, like the Pacific islands will be provided with the assistance to carry out their obligations and to develop and implement strategies to cope with the impacts of climate change.\nThis demonstrates the willingness from the international community to tackle the global issue of climate change. The challenge now is for the region to be proactive and take the necessary steps for ratification and implementation of the Paris Agreement.\nThe Secretariat of the Pacific Regional Environment Programme (SPREP) will continue to work closely with its member countries, regional organisations and development partners to provide direct support to the implementation of this agreement as follows:\n• Assist Pacific Island Countries to sign on to ratify the Paris Agreement\n• Develop national strategies to reduce the emissions of greenhouse gases which are referred to as Nationally Determined Contributions\n• Provide assistance for countries to report against progress of meeting the goals of these strategies, every five years\n• Work with members to develop long term low carbon development strategies\n• Provide on-going support to adaptation and resilience building programmes\n• Work with countries to complete their Adaptation Communications which all countries are required to produce\n• Assist countries to access the financial opportunities provided by the Agreement, including mitigation and adaptation funding under the Global Environment Facility, Adaptation Fund and Green Climate Fund\nTo learn more about the Paris Agreement please access the below. You can also contact Dr Netatua Pelesikoti the Director of the Climate Change Division of SPREP at [email protected]\n• You can read the Paris Agreement here or visit http://unfccc.int/files/home/application/pdf/paris_agreement.pdf\n• To read the Decision adopting the Paris Agreement please click here or visit http://unfccc.int/files/home/application/pdf/decision1cp21.pdf\n• To learn more about The Next Steps for the Paris Agreement please click here or visit http://unfccc.int/files/meetings/paris_nov_2015/application/pdf/paris_agreement_next_steps_post_adoption.pdf\n• For a copy of All COP 21 Outcomes please click here or visit http://unfccc.int/meetings/paris_nov_2015/session/9057.php"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:d7d14ee8-f7ee-499f-9cf9-7470eb9eb526>","<urn:uuid:2a00a85d-3d08-43e4-a00b-750bc8865644>"],"error":null}
{"question":"Does fluorocarbon or monofilament fishing line offer better visibility underwater?","answer":"Fluorocarbon offers better invisibility underwater compared to monofilament. This is because fluorocarbon's light refractive index is very similar to that of fresh water, making it less visible than monofilament when submerged. This can be demonstrated by dipping strands of equal diameter of both materials in a glass of water, where the difference in transparency becomes apparent.","context":["Choosing the right fishing line\nFew aspects of bass fishing have changed more rapidly, or more dramatically, in recent years than the development of new fishing lines. Now, anglers are faced not only with choosing which strength of line the need but also which type of line.\n“Each type of line we use today, braid, fluorocarbon or monofilament, has both advantages and disadvantages,” explained veteran tournament pro Terry Scroggins, who’s been climbing the line-learning curve since he began fishing braids more than a decade ago. “For example, fluorocarbon is nearly invisible underwater and has less stretch than monofilament, and braid has a very thin diameter and extra strength, but virtually no stretch.\n“I believe every bass fisherman has to look at his own style of fishing and the conditions he’s actually facing, and then choose a line that best meets those conditions. The chances are, if you fish very often with several different techniques, you’re going to use all three types of lines.”\nScroggins uses all three, on occasion he combines braid with either a monofilament or fluorocarbon leader to suit the technique he’s using. Here’s how he rigs his rods for a variety of different lures and presentations:\nTopwaters – “Being from Florida, I have always fished a lot of prop baits like a Devils Horse and Boy Howdy, and for these types of lures I prefer 40 pound braid with a four to five foot leader of 15 pound monofilament.\n“Monofilament floats, and with prop baits we often use a very slow presentation during which the lure may sit motionless on the surface for 15 or 20 seconds, so a floating line gives the lures better action. The stretch in the mono also acts as a type of shock absorber for the non-stretching braid.”\nLipless Crankbaits – “I prefer 40 pound braid with a four- to five-foot leader of 15 to 17-pound fluorocarbon. “That’s because I frequently use these lures over hydrilla or milfoil, and many strikes come when you rip the bait out of that vegetation. The fluorocarbon has very little stretch and recovers quickly, while the braid helps cut through the grass.\n“I use the same combination when I’m yo-yoing these lures. Strikes come as the bait is falling, and the braid gives you instant hook-setting; you almost never lose a bass this way.”\nDeep Crankbaiting – “Here I use 10-pound monofilament. It’s strong enough to handle big bass, and the small line diameter allows the lure to reach maximum depth. It has just enough stretch to let the fish engulf the lure for a good hook-set, too.\n“Remember, we used monofilament for years before braids and fluorocarbons were developed, and we caught a lot of bass with it. There are still applications where I think mono out-performs these other lines. Some pros will use fluorocarbon line for deep cranking, but I like mono.”\nFlipping – “When I’m flipping, I use the ‘¾-ounce rule’. If I’m using a ¾-ounce or lighter sinker, it generally means the cover I’m fishing is not very thick nor is it very deep, so I’ll normally use 20-pound or heavier fluorocarbon for its strength and lack of stretch.\n“If I’m fishing slightly deeper and heavier cover, I’ll use 40-pound braid without a leader. Even in the clear water of a lake like Amistad, I’ll use braid because the fish are deeper in submerged timber and I need the strength of braid to get them out.”\nSpinnerbaits – “Depending on the type of cover, I’ll use 14- to 20-pound fluorocarbon. Because I’m in tight contact with the lure, I don’t think there’s a need for braid, although a number of pros do choose it when spinnerbaiting over vegetation. I prefer fluorocarbon because it’s more sensitive and I can feel the blades better.\n“We used to think monofilament was perfect for spinnerbaits, but once you get accustomed to fluorocarbon, using these lures with mono feels like fishing with a big rubber band.”\nShaky Head – “I use six-pound fluorocarbon exclusively with this technique, because I’m not trying for a big bass, but rather, for numbers of fish that are usually in deeper water. I don’t like mono because it has too much stretch.”\nShallow Crankbaits – “When I’m target fishing crankbaits in water six feet or less, I prefer 12- to 15-pound monofilament. It has better castability because it is usually more limp than fluorocarbon, and the visibility of monofilament is not really an issue because I’m usually fishing stained water.”\nSpoons – “My normal choice here is 40-pound braid with a four- to six-foot leader of 14- to 16-pound fluorocarbon. This allows me to make long casts, and because bass nearly always hit this lure while it falls, the braid/fluorocarbon combination gives good hook-sets because of the lack of stretch.”\nCasting Jigs – “With a football-style jig I work on the bottom, I prefer 12- or 14-pound fluorocarbon because of the lack of stretch and visibility, and if I’m working particularly heavy cover, I’ll use 20-pound fluorocarbon. If I’m swimming a jig, however, I’ll use braid without a leader because I’m usually working the lure shallower and want direct contact with it.”\nCarolina Rigs – “I nearly always choose 40-pound braid with a 14- to 17-pound monofilament leader. Mono floats better than fluorocarbon, and in this presentation, it should help my lure as it swims near the bottom.\n“You don’t really lose any sensitivity with mono here, because all your feel comes from your sinker. Most bites on a Carolina rig are nothing more than a ‘heavy’ feeling, and when you learn that bite, it doesn’t mater whether you’re using a monofilament or fluorocarbon leader.\n“The only real advantage I see with a fluorocarbon leader in this case is that it’s invisible. You need a shock absorber in your line system, and because fluorocarbon has such little stretch, you may realize you have to change to a lighter rod for Carolina rigging. To me, that’s a disadvantage, so my choice is monofilament.”\n“When I’m fishing my 10-inch Big Show worms, I use 16-pound fluorocarbon. Again, the advantages are low stretch and near invisibility underwater.”\nWhile most pros have chosen 65-pound braid as their standard, Scroggins feels 40 pound braid is more suitable for his fishing style. The advantage of the smaller size is that it casts easier since he can spool more of it on his reels. Even with 50-pound braid, Scroggins can practically empty a spool on a long cast, which translates into far less cranking power when a bass hits far away\n“I believe the real keys to choosing lines are analyzing both the situations you’re fishing, and then trying different lines and line sizes. Eventually, you’ll settle on some that work best for you, and they may not necessarily be the same choices I use. Each line has distinct characteristics that may or may not be an advantage in your own personal fishing style\n“The best part is that today we all have far more options to choose from than we did just a few years ago.”\n©2015 Bass Edge, Inc. All Rights Reserved.","While fluorocarbon leader and tippet materials are no longer a new discovery in the fishing scene, lots of us are still uncertain of the difference between newer fluorocarbon and traditional nylon monofilament. If you are in this category, before stocking up on tippet spools for the coming season, keep reading and we’ll try to clear up the differences between the two.\nFirst off, don’t be fooled by the substantially higher price tag on fluorocarbon materials when compared to monofilament. Many anglers reach for fluorocarbon under the assumption that because it is more expensive, it is a superior choice of line. While there are many superior qualities of fluorocarbon, both fluorocarbon and monofilament have their place depending on the situation. The higher price of fluorocarbon is as much a result of the manufacturing process as it is the “fishing value.” When comparing leader and tippet materials, there are a few qualities that are of utmost importance – read on.\nThe visibility, or better put, the “invisibility” of fluorocarbon line is most likely the best selling point of fluorocarbon when compared to standard nylon monofilament line. The light refractive index of fluorocarbon is very similar to that of fresh water (much more so than monofilament). In other words, when placed in water, it is less visible than monofilament.\nNot convinced? You can see for yourself. Take strands of equal diameter of both fluorocarbon and monofilament and dip them in a glass of water. Notice the difference in transparency of the materials in water.\nWhen talking about strength, there are several dimensions to consider. In the short term, fluorocarbon is a much harder material than monofilament. This results in higher abrasion resistance that is useful in situations such as nymphing or fishing around heavy structure. Also, most fluorocarbon line is thinner in diameter than monofilament line of the same breaking strength. However, this is not always the case from company to company.\nFluorocarbon is also non-permeable to water and therefore does not absorb water throughout the fishing day. This may not seem like a big deal but most do not realize how much water nylon monofilament actually absorbs throughout the day. Over time, this causes monofilament to weaken.\nOver the long term, fluorocarbon is extremely resistant to the elements as well, unlike monofilament. Overtime, U.V. rays, rain and humidity, and extreme temperatures (both hot and cold) can cause monofilament to break down and lose strength. Fluorocarbon is much more resistant to these conditions over the long term. For most of us, these conditions are the norm during a fishing day. This is worth considering before pulling out that dusty tippet spool you bought on sale two years ago.\nOn that note: due to the fact that fluorocarbon does not break down very readily, please take care when disposing of it. Any pieces clipped off and thrown into the river will be there for a very, very long time.\nFor you trout fisherman out there, the density of your leader material is actually very important. Fluorocarbon is actually denser than water. In other words, it sinks. This is great when dredging the bottom with nymphs or stripping streamers. However, if dead drifting or skating flies on the surface, this is the last thing you want. Nylon monofilament on the other hand actually suspends in water. If fishing dries, especially in very small sizes, monofilament is a clear winner here.\nMost anglers are aware that monofilament is a relatively “stretchy” material. Just grab your leader from both ends and pull; you will see it stretch. While a certain degree of stretch is advantageous to help absorb the shock while fighting a fish, less stretch results in higher sensitivity for detecting those subtle takes. Fluorocarbon is said to have less stretch than most nylon monofilaments, however there has been some debate among differing manufacturers.\nKnotability is often overlooked by anglers when selecting a leader or tippet material, but it is very important. The knot is always the weakest link in your setup and therefore it is important to choose a material that knots well.\nNylon monofilament is far superior here as it is suppler than fluorocarbon. For this reason, nylon monofilament is often the choice when tying big game leaders that require extremely large diameter lines. Due to the stiffness of fluorocarbon, knots do not always seat as easy and must be coaxed into lying just right. Take your time when tying knots into fluorocarbon materials and ensure the knot seats correctly to avoid knot slippage or breakage.\nWhile fluorocarbon seems to have a great deal of advantages over tradition monofilament, there are certain situations where the extra cost is not necessary. Evaluate what situations best fit you and buy accordingly. Also, it is important to mention that not all materials are created equal. Fluorocarbon or monofilament is often times very different between competing manufacturers."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:b11cd001-9a17-477f-b5cb-20cae24a4aef>","<urn:uuid:5499e474-c54c-4f5b-94f5-84f511b757e2>"],"error":null}
{"question":"How did the use of beads in traditional African jewelry vary across different regions and tribes?","answer":"The use of beads varied significantly across African regions and tribes. In South and East Africa, beads were used prolifically in ornamental items. The Yoruba tribes restricted beaded jewelry only to their rulers, while in Cameroon beaded items represented honor. In Kenya, beaded jewelry was mandatory for bridal wear. The earliest known large red Maasai and Samburu bead items date back to 1850, and later, small colorful glass beads from Italy were used to develop block patterns in neckbands.","context":["Traditional African Jewelry\nTraditional African jewelry is phenomenally interesting with numerous domains branching out and each having substantial depth. It has been and is a researcher’s paradise. Such jewelry items of Africa adorned both men and women. These comprised of necklaces, earrings, bracelets, nose rings, anklets, head bands, hair clasps, pins that bedecked the necks, ears, arms, legs, toes, hair and waist. At times, the pieces were pierced, sewn or strapped to body parts.\nTraditional African ornaments come in diverse forms and, besides bodily adornment, they have other functions. Religion, ceremonies and rituals play a dominant role, since the customary jewelry items have explicit symbolic meaning for the wearer. Basically, the jewelry is a decorative item, but it can also represent power and status in the society or it can be used as a storage cavity for storing precious stones. Alternatively, the artifacts were a crucial element in the barter system of earlier years, and were traded for food, clothing and other necessities.\nThe oldest forms of African vintage jewelry, pierced mollusk shell beads, probably forming a necklace or bracelet, were discovered in the southern tip of South Africa in the Blombos cave around 75000 years ago. Most of the native ornamental pieces were created from organic materials like ostrich bones, cowry shells, sea and land shells, egg shells, wood, hides, porcupine quills, animal teeth, husks, seeds, animal hair, nuts, carved stone and ivory.\nHistorical records indicate how, through generations, the Turkana community in Kenya was famed for manufacturing big-sized faceted iron beads that created exclusive jewelry items. Other materials, that had widespread usage in jewelry-making from the 1400 until the 1800, were molded glass, ceramic trade beads, coins and brass bells. These materials were beauty-enhancement elements of the ornament and were often used as embellishments. Likewise, copper, iron alloys, ivory and amber had similar utilities and dominated the native jewelry scenario.\nInterestingly, an astounding feature prevails during the coronation of kings and leaders of Ghana. The event is celebrated with an abundance of gold ornamental pieces that created a spectacular atmosphere. The creativity of the craftsmen found an outlet, while the designed motif and style indicated a symbolic meaning. Additionally, these native African jewelry items also expressed tribal association along with the wearer’s age, marital status and authority.\nSimilarly, the beaded traditional African jewelry pays tribute to the ingenuity of the artisans and signifies the use and trade of beads in the African history and culture. The role of San bushmen in piercing ostrich shell beads and master African blacksmiths in making cast bronze beads, hand-carved timber beads, blended with coral and amber bead stones, provide majestic additions in the creations that are both rare and beautiful. These unique bold and chunky items, using ethnic metal beads, ebony discs and ostrich-shell beads, are still raved about and their contribution cannot be under-estimated.\nThe earliest known large red Maasai and Samburu bead items date back to1850. However, the introduction of scintillating, small and colorful glass beads from Italy was used in the development of block pattern in neckbands that dominated the ethnic jewelry style of African men and women. Though south and east Africa saw prolific use of beads in their ornamental items, some inhabitants like the Yoruba tribes confined beaded jewelry adornment to their rulers while in Cameroon, the beaded items represented honor. In Kenya, beaded jewelry was compulsory for bridal wear.\nSurprisingly, traditional gemstone jewelry in Africa was not as popular as beaded items. Since the continent is the natural reservoir of precious and semi-precious gemstones having decorative as well as spiritual properties, the African artisans have used turquoise, sapphires, rose quartz and other gemstones in designing original artistic jewelry pieces. Incidentally, the indigenous natives in Africa were never favorably inclined towards the sparkling diamonds. This fact was prominent displayed as most of the ethnic ornaments did not showcase this gem stone.\nElephant hair bracelets adorn traditional South Africans but have become extremely rare. These mystical pieces are beautiful and are designed from original African elephant hairs. Such items are worn as a sign of respect to the earthly Gods and are indicative of the African elephants’ high esteem.\nTraditional African jewelry is an art form that is very much in vogue. The jewelry items embellish and enhance the beauty of the wearer and, at the same time, bestow status and provide pleasure to the person. The ethnic pieces showcase the creative skill-sets of the artisans and portray the rich cultural heritage of Africa.\nMore interesting stuff:"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:42453938-1f47-443a-a8b4-2a1eebaebf9b>"],"error":null}
{"question":"Can both Photo 51 and cross-fertilization experiments be used to study genetic inheritance, and if so, how do their approaches differ?","answer":"While both techniques study genetic inheritance, they use vastly different approaches. Cross-fertilization experiments, as used by Mendel, involve directly observing inherited traits across generations of organisms (like pea plants) by controlling their breeding and tracking visible characteristics like flower color, demonstrating patterns of inheritance such as the 3:1 ratio in the F2 generation. In contrast, Photo 51, created by Rosalind Franklin through x-ray diffraction, revealed the physical structure of DNA itself - showing its double helix shape and ladder-like structure. This crystallography technique operated at the molecular level, helping scientists understand how genetic information is physically stored and transmitted, rather than observing the outward expression of genetic traits.","context":["Gregor Mendel, 1822 - 1884, is known as 'the Father of Modern Genetics'. He published his work, Versuche über Pflanzenhybriden [Experiments on Plant Hybridization], on hereditary traits of peas, in 1866. This great 'gamechanger' came just 7 years after Charles Darwin's revolutionary book Origin of Species. Darwin had demonstrated how species evolve through adaptation. Mendel worked out, through a careful study of plant characteristics, how this adaptation worked: parents pass their traits to children in distinct packets (which have since been named genes).\nThe insight of his work became apparent later when the science of genetics confirmed his findings, and experiments on insects and animals demonstrated that exactly the same rules apply to all life!\nYou might well ask why Mendel chose to study peas, and not animals, or people?\nHe chose to work with peas because he needed a species which had many easily visible varieties, and plants are much easier to control than insects or animals. He needed to keep careful records of the hierarchies of the plants, which mated with which, and their offspring, and their offspring, for many generations. Peas are small plants, which he could grow quickly and keep in distinct places.\nIn fact, peas have seven distinct traits: seed shape, seed colour, seed coat colour, pod shape, pod colour, flower position, plant height.\nA basic technique Mendel used is called cross-fertilzation. Plants can fertilize themselves, because they have both stamens and carpel, which are the male and female parts. The stamens are removed from flower A, so it can only be fertilized by pollen from another plant, B. After pollination, the carpel of plant A develops into the peapod, which holds the seeds which can be planted and grown into offspring plants.\nA character is a heritable feature, such as flower colour, and a trait is a variation of a character: white, yellow or red flowers.\nHybridization is the process of mating two, contrasting, true-breeding varieties. The parents in this operation are referred to as the P generation.\nThe result of mating the P generation are hybrid offspring: the $F_1$ generation. When $F_1$ individuals self-pollinate, they produce the $F_2$ generation.\nUsing this technique, Mendel cross-fertilized (crossed) white and purple flowered pea plants. All of the offspring ($F_1$) were purple.\nThen Mendel crossed the $F_1$ plants: to his surprise he saw that $3/4$ of the plants had purple flowers, and $1/4$ had white flowers! What was going on?\nTry an experiment with a friend: you take two red disks, and your friend one red and one white. The white represent a 'recessive gene', which means if it is paired with a dominant red gene, the red colour will be the one that counts.\nNow we swap a disk. How many combinations of two are possible?: RR, RW, RR, RW.\nNow, you take two of these new combinations ($F_1$ generation) each, and swap a disk again. What are the possible combinations this time?: RR, RW, RR, WW.\nYou can see that one in four of the $F_2$ generation will have no dominant red gene, and two whites. The trait in 25% of the cases will be white, and 75% red. Exactly what Mendel found!\nGenes are located at specific positions on chromosomes. Mendel's genetic studies resulted in two requirements for inheritance: 1. Every organism must inherit a single copy of every gene from each parent, 2. During gamete production, the two sets of genes inherited from the parents must be separated so that each gamete contains just one set of genes.\nChromosomes achieve this separation requirement.\nHomologous: each chromosome from one parent has a corresponding chromosome from the other parent.\nDiploid: a cell that contains both sets of homologous chromosomes. Humans have 46 chromosomes, so have a diploid number of 2N = 46: i.e. 23 sets of chromosomes and 23 sets of genes from each parent.\nHaploid: gametes of sexually reproducing organisms which contain only one set of chromosomes.\nMeiosis: reduction division in which the number of chromosomes in a cell is halved through the separation of homologous chromosomes in a diploid cell.\nDNA determines how an organism grows and develops. DNA is also passed to offspring in genes. Genes must be passed from cell to cell to ensure growth and reproduction. The study of genes also provides an understanding of the mechanisms behind evolution.\nDNA stands for deoxyribose nucleic acid. It has a helix shape, and is the molecule which passes on the genetic information which make individuals unique.\nBy the 1940s, a molecule in the nucleus had been identified, but was thought to be of no value to the organism. In fact, researchers nicknamed it 'the stupid molecule'. How wrong they were.\nLed by the American Linus Pauling, who won two Nobel Prizes, a race developed to decipher the structure of the DNA molecule. If it was a race, it was won in 1953 by a collaboration of Francis Crick and James Watson, who used the crystallography photograph of Rosalind Franklin, which revealed the helix shape, to finally work out how the DNA was composed of two strands held together by the pairing of 4 types of bases.\nA gene is a fragment of DNA, and genes contain all the information which characterises an organism, such as its development pattern, appearance and functions. These are achieved by the genes providing instructions for the manufacture of proteins. Genes are also the units of heredity, passing on the characteristics of the parents to the offspring during reproduction.\nAll of the genes of an organism are known collectively as the genome of that organism. Every cell of an organism contains the same genetic information, however since most cells have a specialist function, different parts of the genome are expressed in different cells.\nIn the 1990s, it became technically possible to sequence entire genomes. The simplest genomes, such as yeast, can have as few as 6000 genes, composed of around 12 million DNA base pairs, while homo sapiens have of the order of 70,000 genes, composed of 3 billion base pairs.\nGenes are carried on chromosomes in the cell nucleus. A chromosome consists of a single DNA molecule and some protein (protein complexes called nucleosomes), and are between 0.2-20 µm long. When a cell nucleus divides, the chromosomes take on the appearance of rods. In prokaryotic organisms, (single-celled) there is no nucleus, so the DNA is attached directly to the cell membrane, and there is no protein.\nIn eukaryotic cells, chromosomes double up to form homologous pairs.\nWalter Sutton, Theodor Boveri, Paul Nurse, Francis Crick, Rosalind Franklin, Tomas Lindahl, Paul Modrich, Aziz Sancar, Maurice Wilkins, Sydney Brenner, Linus Pauling, Frederick Sanger, James Watson, Martha Chase, Erwin Chargaff.\nContent © Andrew Bone. All rights reserved. Created : August 21, 2015 Last updated :February 15, 2016\nThe most recent article is:\nView this item in the topic:\nand many more articles in the subject:\nPhysics is the science of the very small and the very large. Learn about Isaac Newton, who gave us the laws of motion and optics, and Albert Einstein, who explained the relativity of all things, as well as catch up on all the latest news about Physics, on ScienceLibrary.info.\nSir Paul Nurse is a British biologist and geneticist, who won a Nobel Prize for the discovery of controlling proteins in cell cycles. He is the current President of the Royal Society and Chief Executive and Director of the Francis Crick Institute.\nI have been told that Wagner's music is better than it sounds.\nWebsite © contentwizard.ch | Designed by Andrew Bone","Celebrating DNA and the History of the Double Helix\nApril 25 is National DNA Day, a day that commemorates the 60th anniversary of DNA's double helix discovery in 1953 and the completion of the human genome project in 2003. We all boil down, genetically, to chains of DNA—each of us with an individual DNA sequence. Take time this week to talk with your students and kids about DNA, its history, the scientists who helped crack the code, and ways that students at all levels can get hands-on with DNA-related science.\nWhen it comes to advancing understanding of genetics and genomics, the discovery of DNA's structure stands as one of the most important turning points in science history. DNA is the blueprint for all organisms, from tiny bacteria to huge whales and long-extinct T. rex dinosaurs.\nDNA, and the information it encodes, not only makes each individual organism unique, but also is responsible for certain similarities and traits in groups of organisms. A rose smells the way it does because of DNA. The color of your eyes has something to do with DNA. Whether or not you are at a higher risk of certain health problems may boil down to certain genetic markers you have or do not have. Although genomes (the sum total of DNA needed to encode an organism) are usually copied and acted on in predictable ways, occasionally these mechanisms go awry. Individual stretches of DNA can change or be altered. Transformations, mutations, and other errors in a person's DNA may result in differences in how people respond to a medicine, for example, or may, over long evolutionary time, result in entirely new species.\nEncoding the template for a whole organism sounds like a lot of responsibility for a single kind of molecule, but deoxyribonucleic acid (DNA) does just that!\nFinding the Double Helix\nThere were many scientists involved in identifying and isolating DNA and tying it to understanding of heredity and chromosomes. The history of DNA-related discoveries and breakthroughs dates back to the first isolation of DNA by Friedrich Miescher in 1869. Miescher extracted a DNA sample from cast-off, pus-covered bandages. It sounds kind of gross, but Miescher's discovery fueled further research and inquiry. Until the structure of the DNA molecule was established and modeled, however, scientists were unable to fully explain and further explore the role of DNA.\nThat all changed in 1953.\nThe publication of both Photo 51, an x-ray diffraction photo (taken in 1952) showing the crystalline structure of DNA, and of a series of papers describing the structure of DNA in Nature in 1953 was a pivotal moment in science. Photo 51 was taken by Rosalind Franklin, a scientist working to create a crystal of the DNA molecule that would enable x-ray diffraction studies and, she hoped, enable her to deduce the structure of DNA. The x-ray pattern captured by Photo 51 revealed, for the first time, the ladder-like structure and winding helix shape we now associate with DNA.\nThe findings published in the same 1953 issue of Nature as Franklin's photo were from James Watson and Francis Crick. After seeing Franklin's photo, Watson and Crick were able to make a model of DNA that showed the molecule's structure. In 1962, Watson, Crick, and Maurice Wilkins shared the Nobel Prize in Physiology or Medicine for their research on DNA. (Franklin, whose photo may have cracked the code, died in 1958, her contributions then largely unacknowledged.)\nApril 25 is National DNA Day, a day that commemorates the 60th anniversary of the double helix discovery in 1953 and the completion of the human genome project by the National Human Genome Research Institute (NHGRI) in 2003.\nBreaking It Down\nYou may have played with a model of the DNA structure, or maybe you wear a visual representation on a t-shirt or have a poster or model hanging on your bedroom wall. A helix is defined as \"an object having a three-dimensional shape like that of a wire wound uniformly around a cylinder or cone.\" In most DNA, two helical strands are wound together creating a double helix. Individual DNA molecules (or strands) are each constructed of two long polymers, chains of repeating units made up of pairs of four nucleotides that appear in various repeating combinations. These nucleotides are adenine (\"A\"), thymine (\"T\"), guanine (\"G\"), and cytosine (\"C\"). These letters give scientists the ABCs (or ATGCs) of DNA—it's a four-letter alphabet which underwrites all known life on Earth!\nArmed with knowledge of the structure, composition, and pattern of DNA strands, scientists are able to tackle questions both about history and about the future. Students can, too!\nStudents \"Do\" DNA\nFrom fun home activities that let students and parents explore (and show off!) their own DNA to sophisticated projects for advanced student exploration, Science Buddies has a range of Project Ideas that enable students to better understand the role of DNA and encourage them to explore questions related to genetics, genomics, biotechnology, and bioinformatics. You might be surprised at what your fruits and vegetables drawer will yield in terms of visible DNA discovery, but that's just the tip of the genome!\nSome DNA-related Science Buddies Project Ideas to explore:\n- \"Discovering DNA: Do Your Cheek Cells & a Strawberry Both Have DNA?\": use a home-friendly Bio-Rad kit to extract and compare DNA from your cheek and from a strawberry.\n- \"Extracting Onion DNA\": extract and spool DNA from an onion using household dishwashing detergent or shampoo.\n- \"Do-It-Yourself DNA\": another procedure to explore DNA extraction with strawberries.\n- \"Forensic Science: Building Your Own Tool for Identifying DNA\": build a homemade electrophoresis gel chamber and experiment with an important biotechnology procedure often used to separate DNA from other substances.\n- \"Who Done It? DNA Fingerprinting and Forensics\": explore the banding patterns produced in DNA fingerprinting to match a fictitious subject's DNA to a crime scene.\n- \"What Makes a DNA Fingerprint Unique?\": use online tools to compare DNA fingerprints from randomly generated DNA. With only four nucleotides in each DNA, how is everyone's DNA different?\n- \"Use DNA Sequencing to Trace the Blue Whale's Evolutionary Tree\": trace the blue whale's family tree using genomic sequences in the GenBank database and the BLAST search tool.\n- \"How Much DNA Can You Pack into a Cell?\": use online databases to investigate to see if there is a correlation between genome size and cell nucleus size.\n- \"A Magnetic Primer Designer\": experiment with the process scientists use to copy or clone a DNA sample for further study.\n- \"From Genes to Genetic Diseases: What Kinds of Mutations Matter?\": investigate why some gene mutations cause genetic diseases.\n- \"Drugs & Genetics: Why Do Some People Respond to Drugs Differently than Others?\": use an online pharmacogenomics database to research a drug of interest and investigate why a genetic mutation might account for differences in how an individual responds to the drug.\n- \"The Cancer Genome Anatomy Project\": use an online bioinformatics tools to perform \"virtual experiments\" on real data sets of gene expression data.\n- \"Trace Your Ancient Ancestry Through DNA\": an advanced independent project that lets students explore their DNA history.\nExtend the Conversation\nThere are still questions for scientists to ask and answers to be unlocked through further study of DNA. Just last year, samples of DNA with four strands were discovered. See this article in Nature to shake up your understanding of DNA just a bit. What happens when you square or quad your genetic code?\n- For more in-depth information about DNA, see \"DNA Is a Structure That Encodes Biological Information\".\n- For more information about the science history timeline that goes along with DNA, see \"Unraveling the Human Genome: 6 Molecular Milestones\".\n- For new findings about other molecules that can store and pass on genetic information, see \"Move over DNA: Six new molecules can carry genes\" (New Scientist)\nSee also Science Buddies Project Ideas in genetics and genomics.\nYou Might Also Enjoy These Related Posts:\n- Science is for Everyone: Geerat Vermeij, a Paleoecologist who is Blind\n- Student Discovers Green Thumb Growing Plants without Water\n- Elementary School Student Finds Science Fair Success\n- 10 Projects to Get Started Building Circuits on a Breadboard!\n- Students Race to the Finish with Solar Sprint Cars\n- Explore Crater Science to Celebrate Moon Landing Anniversary\n- STEM is for Everyone: Temple Grandin, Animal Scientist with Autism\n- 5 STEM Activities with Marshmallow Peeps\nExplore Our Science Videos\nFire Snake Experiment\nBuild an Infinity Mirror\nGel Electrophoresis and Forensic Science: Biotechnology Science Fair Project"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:01f97382-6b0f-4bd8-a3b6-f508ed59c4a0>","<urn:uuid:f881bad4-899e-433f-ac72-6d7cc610d483>"],"error":null}
{"question":"Hi, I learning about jazz history. Could you tell me which one had more success in USA - Kirk Whalum gospel jazz series or Stan Getz brazilian jazz albums?","answer":"Stan Getz's Brazilian jazz albums had significantly more mainstream success in the USA. His 1964 album 'Getz/Gilberto' was a massive success, winning the Grammy for Record of the Year and topping the Billboard charts. While Kirk Whalum also achieved Grammy success with his Gospel According to Jazz series, which started in 1998, the documents don't indicate it reached the same level of commercial success or chart performance as Getz's Brazilian jazz recordings.","context":["Kirk Whalum is a Grammy-winning jazz artist who recently released The Gospel According to Jazz IV, the fourth in a documentary and music series that dates back to 1998.\nPrimarily a saxophonist, Whalum has had a long and distinguished career in the jazz world, including touring for seven years with Whitney Houston. He contributed the saxophone to Houston’s monster hit “I Will Always Love You”.\nWhalum’s newest documentary features a number of original compositions performed by his band that included many of his family members. His band consists of people who have toured with everyone from Stevie Wonder to Bruno Mars to Billy Currington. Whalum and the band also play a few covers including a stirring version of the Foo Fighter’s “My Hero” sung by Sheléa (currently on tour with Wonder).\nRecorded at the Christian Cultural Center in Brooklyn, the GATJ IV is a two disc, 19-song CD, and a DVD which intersperses the music with interviews with Whalum. We talked to Whalum about his career, his inspiration for the GATJ series, and the differences between being the frontman and playing in a band.\n– 1 –\nSB: So what was the inspiration for the Gospel According to Jazz series?\nKW: This series is something that started out of a big disappointment. I was signed to Columbia Records for 12 years which was a big deal. I realized how much it meant to me when I got dropped from the label. I was about to have my pity party when my wife said to me, “What can you do today that you couldn’t do yesterday when you were a Columbia Recording artist?”\nIt was a complete switch of my paradigm. And right away I knew what I wanted to do. I wanted to do a series that features musicians that are not known as Gospel musicians, whether they were R&B or Jazz or whatever, playing Gospel music. Fast-forward many years and I won a Grammy for this series, and that’s very serendipitous because it came out of such a big disappointment.\n– 2 –\nSB: How much work goes into making a documentary like this?\nKW: It is a year’s worth of work. For me it’s conceptualizing the music that takes a long time. I write and arrange most of the music, and even when we cover songs I spend a lot of time reimagining those songs for this concept. As far as the directing, it’s a lot of dialogue about what this is about. It kind of falls in between the cracks. Is it Gospel? Is it Jazz? Is it both? If you listen you can see that it’s kind of an artistic statement with a spiritual core.\n– 3 –\nSB: So what is the central message of the Gospel According to Jazz?\nKW: The central message of part IV is God’s radical hospitality. God has welcomed all of His creation through this sacrificial act that the Savior Jesus has done. The door is now open, it’s no longer a matter of whether you work hard enough or if you’re good enough — that’s the Good News.\nThe message of the whole series has to do with instrumental music and how powerful it is. It can help make visible the invisible, to give a sonic imagery to He who is indescribable. That’s where the whole series is coming from, to introduce people to the idea that God is speaking through the language of music.\n– 4 –\nSB: You’ve played in a band with some very famous people, most notably Whitney Houston, but you’ve also been the frontman of your own band. Which do you prefer and how do you approach your job differently depending on your role?\nKW: I appreciate being able to work for people, to be able to add a spice to the gumbo. For Whitney I was the guy that was able to bring a certain spirit and soul and gospel kind of feel to what she was doing. That is a very fun thing to do. It’s important not to overdo things or make it about you because it’s not about you.\nThere are times for me to be out front. Frankly, half of the time I want to tell others to take the lead. It’s nice to have a band that I can pass the ball to and get out of the way. Having great special guests is beautiful.\n– 5 –\nSB: As someone who is playing an instrument and only communicating with the audience through that medium, how do you make sure you are keeping the audience engaged?\nKW: I’m constantly amazed that someone would get in their car and come hear me play. There’s a part of me that always pinches myself. This communication we do with these instruments is not just making noise, it’s a dialogue. My advice is to be authentic to who you are. Whatever influences that make you who you are and where you came from are important. It’s tempting to want to tailor the music to a given audience. You have to do some of that, but you should try your best to play it like you hear it.","- World excluding USA & Canada\n- Latin Soul\n- Catalogue Id:\n- CDBGPM 239\nTHE MUSIC OF Brazil came to the USA with a bang in 1962 with the release of Stan Getz and Charlie Byrd’s “Jazz Samba” LP, which topped the Billboard album charts. 1964’s “Getz/Gilberto”, the saxophonist’s collaboration with Bahia-born guitarist-composer Joao Gilberto, was when Brazilian musicians were taken to America’s heart. “Getz/Gilberto”, which largely comprised the songs of Antonio Carlos Jobim, had a secret weapon in the voice of Gilberto’s wife, Astrud, who sang the English language segment of ‘Girl From Ipanema’, the album’s hit single. With her chic jet-set look she seemed to epitomise a cool Rio sexiness. The LP was a massive success, winning the Grammy for Record of the Year and opening the doors for scores of Brazilian musicians to make the trip north and try their luck in America.\nThe height of this traffic may have been in the period as the 60s turned into the 70s. Airto Moreira and his wife Flora Purim had moved to the US in 1968 from their homeland. Moreira became the first-call Brazilian percussionist for sessions, turning up on recording dates for Cannonball Adderley, Donald Byrd and Duke Pearson, among others. He was a member of Miles Davis’ group in the wake of the recording of “Bitches Brew”, appearing on the “Live Evil” album and some of the “Jack Johnson” sessions. He became a founder-member of both Weather Report and Return To Forever. Airto’s support of other Brazilian musicians was important, encouraging them to settle in the United States and hooking them up with prospective employers. Trombonist Raul De Souza was one of these migrants. Moreira not only arranged De Souza’s record deal, but also produced his debut album.\nJoão José Pereira de Souza was born in 1934 and grew up in the Bangu suburb of Rio De Janeiro, where his father was a pastor. He took up the trombone in his teens, encouraged and taught by a trombonist in his local church band called Geraldo. Raul was a natural and by the age of 16 he was playing in the band formed by the workers from the biggest factory in Bangu. A hard slog followed, including a stint in the army where he impressed with his musical ability. Back in Rio in 1963 he formed Bossa Rio with Sergio Mendes, the first instrumental-only bossa nova group. The album they recorded “Você Ainda Não Ouviu Nada” (You Ain’t Heard Nothing Yet) is a classic. He then founded his first group as a leader, which included amongst its members Airto Moreira. They recorded an album for RCA, “Ā Vontade Mesmo”, prior to Raul moving to Monte Carlo to play a four-month stint at a casino.\nOn his return he formed the more progressive group Impacto 8 who recorded the album “International Hot” for the Equipe label. (On its reissue it was described as a “psychedelic funk bossa work-out”). It failed to be a commercial success and Raul moved to Mexico, where he played bossa in the clubs of Mexico City and Acapulco for four years, 1969-1972. In 1972 he moved to Los Angeles where he hooked up with Airto and Flora Purim to become a member of their touring band. He appeared on albums by the pair and on recordings by Cal Tjader and Sonny Rollins. His activity alerted Fantasy Records to his talents and the recording of his album “Colors” to be released on their Milestone label began.\nIn an interview with Brazilian reissue specialists whatmusic, Raul remembered the planning for the sessions. “In 1974 Airto produced “Colors”, my first…recording under my own name – Raul de Souza rather than Raulzinho – for Milestone Records. There I had the immense pleasure and honour to get to know J.J. Johnson, who agreed to do the arrangements for my compositions. I invited Jack DeJohnette, Richard Davis, Cannonball Adderley, Ted Lo and all those guys to play on the record and it was really a great experience for me.” It is a fine example of the fusion between Brazilian music and jazz. The first side opens with a fine arrangement of Moacir Santos’ ‘Nana’ with the melody played by Raul over a tight funk groove, and his trombone being joined by massed horns courtesy of a five-piece horn section of Snooky Young, Oscar Brashear, George Bohanon, Don Waldrop and Sahib Shihab. Their presence adds a richness to the record wherever they are present. Cannonball Adderley, in what was to be one of his final appearances as a side-man, appeared on a fantastic swing jazz version of ‘Canto De Ossanha’, the second track.\n‘Water Buffalo’ is a beautifully-judged number that demonstrates in its first half how well Raul could play a ballad on his instrument, before the pace gets picked up, driven by Jack DeJohnette’s pulsating drumming. The side ends off with a smooth glide through Joe Zawinul’s ‘Dr Honoris Causa’ where the horn section gives the track a lift.\n‘Festival’ is a fast and furious latin groove, and has been the standout title sought after by UK collectors. It is the sound of a Brazilian carnival on record, riding along on the piano groove provided by Ted Lo, before it breaks down into a percussion work out from Kenneth Nash (when producing, Airto doesn’t play). Chick Corea’s ‘Crystal Silence’ is another opportunity for the players to show their chops; it never loses interest throughout its 11 minutes. This invigorating album then ends with the fairly fast fusion workout of ‘Chants To Burn’.\nIt was to be Raul De Souza’s only LP for Milestone. In 1976 the trombonist signed to Capitol Records where his albums “Sweet Lucy” and “Don’t Ask My Neighbors” were produced by George Duke, who was just beginning to flex his muscles behind the mixing desk. They were successful jazz funk recordings that sold well. His final album for Capitol “‘Til Tomorrow Comes” was produced by West Coast veteran Arthur Wright. Though there was an attempt at being more commercial, it failed to sell. It was his last attempt to break the charts.\nDe Souza remained in the United States until the early 90s, when he returned to Rio. In 1999 he moved to Paris. He moves between the two, recording and playing, releasing albums such as the wonderful “Bossa Eterna” from 2008, released to celebrate 50 years of the bossa nova.\nDEAN RUDLAND / 2011"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:0982b346-1bc6-4e38-922b-cb28c6d9880c>","<urn:uuid:870ef9ee-2b53-4304-9413-ebebb7abd456>"],"error":null}
{"question":"How does vitamin D metabolism relate to kidney function, and what are the key risk factors for developing chronic kidney disease?","answer":"The kidneys play a crucial role in producing an active form of vitamin D, which is why kidney disease can impact vitamin D metabolism. Research has shown that vitamin D analogues like paricalcitol can help manage certain aspects of kidney disease, particularly proteinuria. Regarding risk factors, chronic kidney disease is more common in women than men and has higher prevalence among African Americans, Hispanics, Pacific Islanders, Asians, and Native Americans. Additionally, seniors over 60 are at increased risk due to natural loss of kidney function and higher likelihood of developing diabetes and hypertension. Approximately 1 in 3 adults with diabetes and 1 in 5 adults with high blood pressure may develop chronic kidney disease.","context":["Track topics on Twitter Track topics that are important to you\nRecently it has been documented that vitamin D has important functions in the human body that are unrelated to its primary effects in calcium homeostasis and bone mineralization. In clinical studies, paricalcitol - a low-calcemic vitamin D analogue - has been shown to decrease proteinuria, a marker of disease progression and cardiovascular risk in patients with chronic kidney disease (CKD). The purpose of this study is to investigate the effect of a paricalcitol on renal and cardiovascular variables in patients with moderate to severe CKD.\nAllocation: Randomized, Control: Placebo Control, Endpoint Classification: Efficacy Study, Intervention Model: Crossover Assignment, Masking: Double Blind (Subject, Caregiver, Investigator, Outcomes Assessor), Primary Purpose: Treatment\nChronic Kidney Disease\nDepartment of Medical Research\nPublished on BioPortfolio: 2014-08-27T03:13:14-0400\nThe purpose of this study is to obtain data on the safety and effectiveness of Zemplar® injection and Zemplar® capsules in real-life clinical practice. In this study, Zemplar® will be p...\nThe purpose of this study is to evaluate the effects of Zemplar® Injection and Hectorol® Injection on intestinal calcium absorption in Chronic Kidney Disease Stage 5 subjects on hemodial...\nThis is the post-marketing study conducted in two countries: Croatia and Serbia. In both countries Zemplar is the first injectable form of any Vitamin D Receptor (VDR) activator available ...\nThis study is being done to find out whether patients who receive a kidney transplant can benefit from taking the medication paricalcitol (trade name Zemplar®) compared to kidney transpla...\nThe study is about possible protective effects of paricalcitol (Zemplar) upon inflammation, blood pressure and kidney function. Kidney Inflammation occurs when white blood cells become abn...\nManagement of patients with chronic kidney disease has evolved since the last Kidney Disease Improving Global Outcomes clinical practice guideline was published in 2012. This article reviews the most ...\nDietary potassium (K+) has beneficial effects on blood pressure and cardiovascular (CV) outcomes. Recently, several epidemiological studies have revealed an association between urinary K+ excretion (a...\nProton pump inhibitor use is associated with incident chronic kidney disease, chronic kidney disease progression and end-stage renal disease. However, the extent of proton pump inhibitor prescriptions...\nChronic kidney disease is a global health problem that affects over 10% of adults worldwide. All doctors should have a basic knowledge of chronic kidney disease because it may complicate the managemen...\nChronic kidney disease (CKD) is recognised as a global public health problem, more prevalent in older persons and associated with multiple co-morbidities. Diabetes mellitus and hypertension are common...\nConditions in which the KIDNEYS perform below the normal level for more than three months. Chronic kidney insufficiency is classified by five stages according to the decline in GLOMERULAR FILTRATION RATE and the degree of kidney damage (as measured by the level of PROTEINURIA). The most severe form is the end-stage renal disease (CHRONIC KIDNEY FAILURE). (Kidney Foundation: Kidney Disease Outcome Quality Initiative, 2002)\nThe end-stage of CHRONIC RENAL INSUFFICIENCY. It is characterized by the severe irreversible kidney damage (as measured by the level of PROTEINURIA) and the reduction in GLOMERULAR FILTRATION RATE to less than 15 ml per min (Kidney Foundation: Kidney Disease Outcome Quality Initiative, 2002). These patients generally require HEMODIALYSIS or KIDNEY TRANSPLANTATION.\nDecalcification of bone or abnormal bone development due to chronic KIDNEY DISEASES, in which 1,25-DIHYDROXYVITAMIN D3 synthesis by the kidneys is impaired, leading to reduced negative feedback on PARATHYROID HORMONE. The resulting SECONDARY HYPERPARATHYROIDISM eventually leads to bone disorders.\nAbnormal enlargement or swelling of a KIDNEY due to dilation of the KIDNEY CALICES and the KIDNEY PELVIS. It is often associated with obstruction of the URETER or chronic kidney diseases that prevents normal drainage of urine into the URINARY BLADDER.\nA severe irreversible decline in the ability of kidneys to remove wastes, concentrate URINE, and maintain ELECTROLYTE BALANCE; BLOOD PRESSURE; and CALCIUM metabolism. Renal failure, either acute (KIDNEY FAILURE, ACUTE) or chronic (KIDNEY FAILURE, CHRONIC), requires HEMODIALYSIS.\nWithin medicine, nutrition (the study of food and the effect of its components on the body) has many different roles. Appropriate nutrition can help prevent certain diseases, or treat others. In critically ill patients, artificial feeding by tubes need t...\nOsteoporosis is a disease in which the bones become extremely porous, are subject to fracture, and heal slowly, occurring especially in women following menopause and often leading to curvature of the spine from vertebral collapse. Follow and track&n...\nCardiovascular disease (CVD)\nAcute Coronary Syndromes (ACS) Blood Cardiovascular Dialysis Hypertension Stent Stroke Vascular Cardiovascular disease (CVD) includes all the diseases of the heart and circulation including coronary heart disease (angina...","Symptoms, Consequences, and How to Reduce Your Risk\nA pair of fist-sized, bean-shaped organs, your kidneys sit on either side of your spine, just below the rib cage. Their main function is to filter your blood by removing waste and excess water to make urine. Dr. Christopher Poole, a partner at Nephrology Associates of Chattanooga, shares, “Our kidneys receive and process our entire blood volume 20 to 25 times each day.”\nIn addition to filtering blood, your kidneys also stabilize electrolytes like potassium, sodium, and phosphorus, and release hormones that help regulate blood pressure, create red blood cells, and produce an active form of vitamin D. In short, kidneys are vital organs that perform life-sustaining functions that the body requires. In the event they have some type of abnormality or become damaged, they lose the ability to perform their crucial functions, which results in kidney disease. “Kidney disease is defined as any damage to the kidneys that is found by laboratory testing,” explains Dr. Poole.\n“When function in the kidneys become irreversibly damaged, we call that chronic kidney disease, or CKD,” shares Dr. Rafael Duchesne, a nephrologist with Nephrology & Hypertension Specialists. With CKD, the kidneys cannot filter blood, which means excess fluid and waste build up in the body.\nThirty million people– that’s 15% of U.S. adults– likely have CKD, but many are entirely unaware, since the disease often doesn’t produce symptoms until it’s more advanced. In fact, nearly half of those who have severely reduced kidney function are unaware they have the disease, while 96% of people with some level of kidney damage or mildly reduced kidney function have no idea their kidneys aren’t healthy.\n“CKD is a progressive disease, and unfortunately, CKD symptoms are nonspecific and can go unnoticed until the disease has progressed to the later stages,” explains Dr. Duchesne.\nThe signs that might arise in later stages include fatigue, trouble concentrating, poor appetite, nausea and vomiting, dry and itchy skin, bloody urine, increased urination (particularly at night), painful urination, and swelling in the lower extremities.\nWho’s Most at Risk?\nApproximately 1 in 3 adults with diabetes and 1 in 5 adults with high blood pressure may have chronic kidney disease. CKD is more common in women than men and more prevalent in African Americans, Hispanics, Pacific Islanders, Asians, and Native Americans. Seniors over 60 are more likely to develop CKD due to the natural loss of kidney function, combined with an increased risk of diabetes and hypertension.\nCKD occurs when a disease or condition impairs kidney function, causing abnormalities or permanent damage. The leading cause of CKD is diabetes, which prevents your body from producing enough insulin or using it normally, resulting in a high blood sugar level. Over time, the high levels of glucose in the blood can damage the kidneys.\nHigh blood pressure, also known as hypertension, is the second leading cause of CKD. Uncontrolled hypertension can cause the arteries around the kidneys to narrow, weaken, or harden. This means that over time, they become unable to deliver enough blood to kidney tissues. “Diabetes and hypertension are responsible for two thirds of all CKD cases,” says Dr. Duchesne. “Anyone with either disorder should have a blood and urine test at least yearly.”\nThe third leading cause of kidney disease is glomerulonephritis, an inflammation of the tiny filters in your kidneys. This condition may arise on its own, or as part of another disease like diabetes or lupus.\nOther conditions that can affect your kidneys include kidney stones and infections, diseases that affect the body’s immune system, acute kidney injury (which can results from injuries, major blood loss, or reactions to some medicines), or genetic diseases such as polycystic kidney disease, which causes large cysts to form on the kidneys and damage the surrounding tissue.\nWhile the damage done to your kidneys cannot be reversed, there are treatments that can help your kidneys perform their functions.\nDialysis: Typically, when kidney function is reduced to 10-15%, dialysis becomes a viable treatment option. There are two versions, depending on severity and other factors.\nHemodialysis: With this form of dialysis, a patient is connected to a dialysis machine and an artificial kidney, called a dialyzer, which pumps blood, filters it, and returns it to the body. This method, which is usually done three times a week for approximately four hours, is most often completed at a hospital or dialysis center.\nPeritoneal dialysis: With this form of dialysis, a catheter is surgically implanted in the abdomen, and a sterile cleansing fluid, called dialysate, is sent through to filter and wash. Treatments can be done at home, at work, or while traveling. However, peritoneal dialysis isn’t possible for some patients. Those who are obese or who have had multiple prior abdominal surgeries may not be ideal candidates.\nKidney Transplant: For people whose CKD has advanced to the point of complete kidney failure, a kidney transplant may be the best option. The kidney can be donated from a living relative or friend, or from a stranger who wished to donate their organs upon death. Studies show that people with kidney transplants live longer than those who remain on dialysis. However, Dr. Duchesne explains, “It is important to note that transplantation is not a cure, but another treatment.” Those considered for a transplant must be healthy enough for operation and cancer- and infection-free.\nEarly detection is crucial to slow or prevent the progression of kidney disease. If you are 60 or older, have a family history of kidney disease, or have any high-risk conditions that are likely to cause CKD like diabetes or hypertension, it’s important to discuss prevention methods with your doctor.\nMost likely, you’ll need to undergo two simple tests as a part of your annual physical. Dr. Poole explains, “The lack of any outward symptoms is why seeing your primary care provider regularly to have blood work and urine studies done is vital to diagnosing and managing chronic kidney disease.”\nA urine test can identify the amount of the protein called albumin in your urine; too much of this type of protein indicates damage. Dr. Poole explains, “The filters in the kidneys are set up to retain almost all of the protein they are exposed to. If the filters are damaged, protein will leak out and will appear in the urine.”\nThe second test, a blood test, can measure kidney function by checking your creatinine (a waste product that is removed by the kidneys) levels to determine how well the kidneys are removing waste.\nIt is also important to manage blood pressure and blood sugar levels. Even pre-hypertension and pre-diabetes (slightly elevated blood pressure or blood sugar levels) can cause damage to your kidneys. Quitting smoking will help keep levels in check.\nMaintaining a healthy weight, incorporating regular exercise, and making good food choices reduce your risk of developing not only kidney disease but many other health ailments that are closely linked to CKD.\nExercise caution when taking both over-the-counter and prescription pain medications. Part of the kidneys’ job is to filter these types of medications from your body, meaning they have to work especially hard when pain medication is ingested on a regular basis. Take care to avoid excessive use of non-steroidal anti-inflammatory drugs such as ibuprofen and naproxen, as NSAIDs can lower kidney function over time.\nSince kidney disease is often asymptomatic, it’s vital to be vigilant and visit your doctor regularly. Don’t be afraid to ask questions if you fall into a risk category, and don’t ignore any signs that don’t sit well with you. With early diagnosis and proper treatment, you can continue a happy, healthy life for years to come."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:0bfa3bd2-fc9a-4934-869e-6368c95e73f3>","<urn:uuid:7f233ce4-9a2c-41a2-a6ee-ed6c4aa33c70>"],"error":null}
{"question":"How do predator and prey animals' visual fields differ? I'm studying animal adaptations.","answer":"Predator and prey animals have evolved distinctly different visual field arrangements. Most open-plains herbivores (prey animals) have eyes on the sides of their head, providing a panoramic view of nearly 360 degrees to detect approaching predators from almost any direction. For example, horses have a panoramic field of approximately 350 degrees. In contrast, most predators have forward-facing eyes that provide superior binocular vision and depth perception, which helps them judge distances when attacking prey. This difference is evident in the vision systems' evolution, where prey animals prioritized a wider field of view for survival, while predators developed better depth perception for hunting.","context":["Perspective, relative size, occlusion and texture gradients all contribute to the three-dimensional appearance of this photo.\nDepth perception is the visual ability to perceive the world in three dimensions (3D) and the distance of an object. Depth sensation is the corresponding term for animals, since although it is known that animals can sense the distance of an object (because of their ability to move accurately, or to respond consistently, according to that distance), it is not known whether they \"perceive\" it in the same subjective way that humans do.\nDepth perception arises from a variety of depth cues. These are typically classified into binocular cues that are based on the receipt of sensory information in three dimensions from both eyes and monocular cues that can be represented in just two dimensions and observed with just one eye. Binocular cues include stereopsis, eye convergence, disparity, and yielding depth from binocular vision through exploitation of parallax. Monocular cues include size: distant objects subtend smaller visual angles than near objects, grain, size, and motion parallax.\nWhen an observer moves, the apparent relative motion of several stationary objects against a background gives hints about their relative distance. If information about the direction and velocity of movement is known, motion parallax can provide absolute depth information. This effect can be seen clearly when driving in a car. Nearby things pass quickly, while far off objects appear stationary. Some animals that lack binocular vision due to their eyes having little common field-of-view employ motion parallax more explicitly than humans for depth cueing (e.g., some types of birds, which bob their heads to achieve motion parallax, and squirrels, which move in lines orthogonal to an object of interest to do the same).[note 1]\nDepth from motion\nWhen an object moves toward the observer, the retinal projection of an object expands over a period of time, which leads to the perception of movement in a line toward the observer. Another name for this phenomenon is depth from optical expansion. The dynamic stimulus change enables the observer not only to see the object as moving, but to perceive the distance of the moving object. Thus, in this context, the changing size serves as a distance cue. A related phenomenon is the visual system’s capacity to calculate time-to-contact (TTC) of an approaching object from the rate of optical expansion – an ability that is useful in contexts ranging from driving a car to playing baseball. However, calculation of TTC is, strictly speaking, perception of velocity rather than depth.\nIf a stationary rigid figure (for example, a wire cube) is placed in front of a point source of light so that its shadow falls on a translucent screen, an observer on the other side of the screen will see a two-dimensional pattern of lines. But if the cube rotates, the visual system will extract the necessary information for perception of the third dimension from the movements of the lines, and a cube is seen. This is an example of the kinetic depth effect. The effect also occurs when the rotating object is solid (rather than an outline figure), provided that the projected shadow consists of lines which have definite corners or end points, and that these lines change in both length and orientation during the rotation.\nThe property of parallel lines converging in the distance, at infinity, allows us to reconstruct the relative distance of two parts of an object, or of landscape features. An example would be standing on a straight road, looking down the road, and noticing the road narrows as it goes off in the distance.\nIf two objects are known to be the same size (e.g., two trees) but their absolute size is unknown, relative size cues can provide information about the relative depth of the two objects. If one subtends a larger visual angle on the retina than the other, the object which subtends the larger visual angle appears closer.\nSince the visual angle of an object projected onto the retina decreases with distance, this information can be combined with previous knowledge of the object's size to determine the absolute depth of the object. For example, people are generally familiar with the size of an average automobile. This prior knowledge can be combined with information about the angle it subtends on the retina to determine the absolute depth of an automobile in a scene.\nDue to light scattering by the atmosphere, objects that are a great distance away have lower luminance contrast and lower color saturation. Due to this, images seem hazy the farther they are away from a person's point of view. In computer graphics, this is often called \"distance fog.\" The foreground has high contrast; the background has low contrast. Objects differing only in their contrast with a background appear to be at different depths. The color of distant objects are also shifted toward the blue end of the spectrum (e.g., distant mountains). Some painters, notably Cézanne, employ \"warm\" pigments (red, yellow and orange) to bring features forward towards the viewer, and \"cool\" ones (blue, violet, and blue-green) to indicate the part of a form that curves away from the picture plane.\nThis is an oculomotor cue for depth perception. When we try to focus on far away objects, the ciliary muscles stretch the eye lens, making it thinner, and hence changing the focal length. The kinesthetic sensations of the contracting and relaxing ciliary muscles (intraocular muscles) is sent to the visual cortex where it is used for interpreting distance/depth. Accommodation is only effective for distances less than 2 meters.\nOcclusion (also referred to as interposition) happens when near surfaces overlap far surfaces. If one object partially blocks the view of another object, humans perceive it as closer. However, this information only allows the observer to create a \"ranking\" of relative nearness. The presence of monocular occlusions consist of the object's texture and geometry. Monocular occlusions are able to reduce the depth perception latency both in natural and artificial stimuli.\nAt the outer extremes of the visual field, parallel lines become curved, as in a photo taken through a fisheye lens. This effect, although it is usually eliminated from both art and photos by the cropping or framing of a picture, greatly enhances the viewer's sense of being positioned within a real, three-dimensional space. (Classical perspective has no use for this so-called \"distortion,\" although in fact the \"distortions\" strictly obey optical laws and provide perfectly valid visual information, just as classical perspective does for the part of the field of vision that falls within its frame.)\nFine details on nearby objects can be seen clearly, whereas such details are not visible on faraway objects. Texture gradients are grains of an item. For example, on a long gravel road, the gravel near the observer can be clearly seen of shape, size and colour. In the distance, the road's texture cannot be clearly differentiated.\nThe way that light falls on an object and reflects off its surfaces, and the shadows that are cast by objects provide an effective cue for the brain to determine the shape of objects and their position in space.\nSelective image blurring is very commonly used in photographic and video for establishing the impression of depth. This can act as a monocular cue even when all other cues are removed. It may contribute to the depth perception in natural retinal images, because the depth of focus of the human eye is limited. In addition, there are several depth estimation algorithms based on defocus and blurring. Some jumping spiders are known to use image defocus to judge depth.\nWhen an object is visible relative to the horizon, we tend to perceive objects which are closer to the horizon as being farther away from us, and objects which are farther from the horizon as being closer to us.\nBinocular cues provide depth information when viewing a scene with both eyes.\nStereopsis, or retinal (binocular) disparity, or binocular parallax\nAnimals that have their eyes placed frontally can also use information derived from the different projection of objects onto each retina to judge depth. By using two images of the same scene obtained from slightly different angles, it is possible to triangulate the distance to an object with a high degree of accuracy. Each eye views a slightly different angle of an object seen by the left and right eyes. This happens because of the horizontal separation parallax of the eyes. If an object is far away, the disparity of that image falling on both retinas will be small. If the object is close or near, the disparity will be large. It is stereopsis that tricks people into thinking they perceive depth when viewing Magic Eyes, Autostereograms, 3-D movies, and stereoscopic photos.\nThis is a binocular oculomotor cue for distance/depth perception. Because of stereopsis the two eyeballs focus on the same object. In doing so they converge. The convergence will stretch the extraocular muscles. As happens with the monocular accommodation cue, kinesthetic sensations from these extraocular muscles also help in depth/distance perception. The angle of convergence is smaller when the eye is fixating on far away objects. Convergence is effective for distances less than 10 meters.\nA. Medina Puerta demonstrated that retinal images with no parallax disparity but with different shadows are fused stereoscopically, imparting depth perception to the imaged scene. He named the phenomenon \"shadow stereopsis\". Shadows are therefore an important, stereoscopic cue for depth perception.\nOf these various cues, only convergence, accommodation and familiar size provide absolute distance information. All other cues are relative (i.e., they can only be used to tell which objects are closer relative to others). Stereopsis is merely relative because a greater or lesser disparity for nearby objects could either mean that those objects differ more or less substantially in relative depth or that the foveated object is nearer or further away (the further away a scene is, the smaller is the retinal disparity indicating the same depth difference.)\nMost open-plains herbivores, especially hoofed grazers, lack binocular vision because they have their eyes on the sides of the head, providing a panoramic, almost 360°, view of the horizon - enabling them to notice the approach of predators from almost any direction. However, most predators have both eyes looking forwards, allowing binocular depth perception and helping them to judge distances when they pounce or swoop down onto their prey. Animals that spend a lot of time in trees take advantage of binocular vision in order to accurately judge distances when rapidly moving from branch to branch.\nMatt Cartmill, a physical anthropologist & anatomist at Boston University, has criticized this theory, citing other arboreal species which lack binocular vision, such as squirrels and certain birds. Instead, he proposes a \"Visual Predation Hypothesis,\" which argues that ancestral primates were insectivorous predators resembling tarsiers, subject to the same selection pressure for frontal vision as other predatory species. He also uses this hypothesis to account for the specialization of primate hands, which he suggests became adapted for grasping prey, somewhat like the way raptors employ their talons.\nPhotographs capturing perspective are two-dimensional images that often illustrate the illusion of depth. (This differs from a painting, which may use the physical matter of the paint to create a real presence of convex forms and spatial depth.) Stereoscopes and Viewmasters, as well as 3D films, employ binocular vision by forcing the viewer to see two images created from slightly different positions (points of view). Charles Wheatstone was the first to discuss depth perception being a cue of binocular disparity. He invented the stereoscope, which is an instrument with two eyepieces that displays two photographs of the same location/scene taken at relatively different angles. When observed, separately by each eye, the pairs of images induced a clear sense of depth. By contrast, a telephoto lens—used in televised sports, for example, to zero in on members of a stadium audience—has the opposite effect. The viewer sees the size and detail of the scene as if it were close enough to touch, but the camera's perspective is still derived from its actual position a hundred meters away, so background faces and objects appear about the same size as those in the foreground.\nTrained artists are keenly aware of the various methods for indicating spatial depth (color shading, distance fog, perspective and relative size), and take advantage of them to make their works appear \"real\". The viewer feels it would be possible to reach in and grab the nose of a Rembrandt portrait or an apple in a Cézanne still life—or step inside a landscape and walk around among its trees and rocks.\nCubism was based on the idea of incorporating multiple points of view in a painted image, as if to simulate the visual experience of being physically in the presence of the subject, and seeing it from different angles. The radical \"High Cubist\" experiments of Braque and Picasso circa 1909 are interesting but more bizarre than convincing in visual terms. Slightly later paintings by their followers, such as Robert Delaunay's views of the Eiffel Tower, or John Marin's Manhattan cityscapes, borrow the explosive angularity of Cubism to exaggerate the traditional illusion of three-dimensional space. A century after the Cubist adventure, the verdict of art history is that the most subtle and successful use of multiple points of view can be found in the pioneering late work of Cézanne, which both anticipated and inspired the first actual Cubists. Cézanne's landscapes and still lifes powerfully suggest the artist's own highly developed depth perception. At the same time, like the other Post-Impressionists, Cézanne had learned from Japanese art the significance of respecting the flat (two-dimensional) rectangle of the picture itself; Hokusai and Hiroshige ignored or even reversed linear perspective and thereby remind the viewer that a picture can only be \"true\" when it acknowledges the truth of its own flat surface. By contrast, European \"academic\" painting was devoted to a sort of Big Lie that the surface of the canvas is only an enchanted doorway to a \"real\" scene unfolding beyond, and that the artist's main task is to distract the viewer from any disenchanting awareness of the presence of the painted canvas. Cubism, and indeed most of modern art is a struggle to confront, if not resolve, the paradox of suggesting spatial depth on a flat surface, and explore that inherent contradiction through innovative ways of seeing, as well as new methods of drawing and painting.","How Do Horses See The World?\nLearn about equine behavior by understanding how their vision works.\nAugust 7, 2008\nHow do horses see the world around them? Learning about how horses view their surroundings helps us better understand equine behavior and in turn makes us better horse people. We asked Paul E. Miller, DVM, Diplomate ACVO, Clinical Professor of Comparative Ophthalmology, Department of Surgical Sciences, School of Veterinary Medicine at the University of Wisconsin-Madison, to answer the most common questions about equine vision. Here’s what he had to say:\n1. Horses can be flighty animals prone to spooking. Does the way they \"see” the world have anything to do with their proclivity toward spooking?\nNo. Being \"flighty” has to do with being a prey species and not their sight. The average horse has a visual acuity of about 20/30 to 20/40, which is much better than the average dog (20/75) or cat (20/100), but slightly worse than the average human (20/20). In humans, 20/20 means that a person can identify the details of an object from 20 feet away. That standard is applied in animals too, so in horses 20/30 vision means that the horse needs to be 20 feet away from the object to see the same details that the normal person can discern from 30 feet.\n2. Generally speaking, horses seem to spook more when they see something unfamiliar in the distance. Does this behavior imply that they can’t see distances very well?\nNot really. Horses see at distance relatively well, but not quite as well as humans. The horse’s somewhat lower acuity means that images in the distance are little grainier for him, but not blurry. And unless the distant image is very small, a horse can see it. The horse’s eye is designed to scan the horizon (note the pupil is a horizontal rectangle) and as such potential predators at distance are of particular interest and understandably elicit a heightened state of awareness. It appears that the horse’s overall approach to unidentified objects is to treat them as potentially harmful until proven otherwise.\n3. Can horses see close up?\nAbsolutely. The horse is believed to be able to focus on images that are 50 centimeters (about 10 inches) or farther away. Images closer than this tend to be out of focus, just as images that are very close to our eyes are out of focus (especially as we get older and need reading glasses!). From about 6 feet away the average horse can tell a difference in depth of about 9 centimeters (about 3 1/2 inches) whereas the average person can detect a few millimeters (a couple sheets of paper) difference in depth from the same distance. The ability of the horse to see up close and to discern depth, however, is more than adequate for its lifestyle and its grazing habits.\n4. Does a horse’s vision change throughout his lifetime, similar to that of humans?\nI assume this is asking about normal horses and not horses that developed cataracts, et cetera. The effect of aging on the horse’s eye has not been very well investigated. The fact that horses do not read or do needlepoint means that even if they did lose some ability to focus on very near objects (as do older humans) it’s not likely to have much of an impact on their lives. Some age-associated human disorders such as small cataracts also appear in horses with age, but since most horses do not perform activities that require precise vision these disorders have relatively little effect on their lifestyle.\n5. Humans can’t see directly behind them. Do horses have blind spots too?\nHorses have a very large panoramic field of view. With one eye they can see approximately 190-195 degrees horizontally (e.g. from side to side--the horizon) and about 178 degrees vertically (e.g. from top to bottom or \"grass to sky”). Using both eyes horses see virtually 350 degrees and the horse almost has a complete \"sphere of vision” around its body with only a few small \"blind spots.” These blind spots are: 1) located between the two eyes in the region of the forehead (much like the area that would be occupied by a unicorn’s \"horn”); 2) directly below its nose; and 3) the width of the animal’s head directly behind it. Clearly, the horse’s large field of view makes it difficult for any potential predator to \"sneak up” on it. The horse also has an area where the field of view of the two eyes overlaps, allowing both eyes to see the same object at the same time (binocular vision). This area, which is about 55 to 80 degrees wide, is directly in front of the horse and significantly improves the horse’s ability to detect depth in this area. It should be noted that the area of overlapping vision between the two eyes is much larger than a dog’s or wolf’s (30 to 60 degrees) but less than that of a cat (around 140 degrees) or human (around 170 degrees).\n6. Do we know whether horses see color?\nYes. Unlike humans, which have three different types of retinal cells (blue, green and red cone photoreceptors) to detect color, horses have only two (a blue cone photoreceptor and a second photoreceptor that is most sensitive to light and is roughly between that of the human red and green cone). The horse also has fewer numbers of color detecting photoreceptors than do humans, and they are arranged differently in the retina than are human cone photoreceptors. Humans with normal color vision typically see four basic unique hues--blue, green, yellow and red--and about 100 intermediate colors that can be thought of as varying blends of pairs of the four unique colors (for example yellow-green, reddish-yellow/orange, reddish-blue/violet) and so forth). The most dramatic impact of having two cone types instead of three is that horses (and some \"color-blind” humans) have only two unique hues, believed to be something similar to blue and yellow, and there are no intermediate hues. When colors at the far ends of the spectrum of visible light are mixed, the result is either a white/gray (this occurs in the blue-green range of the color spectrum), or a desaturated version of one of the two basic hues (e.g. a pastel yellow or a pastel blue). In a sense, horses are orange-blue \"color-blind” in that although they can see objects with these colors, they cannot differentiate between orange and blue solely on the basis of color since they both appear to be gray-white to the horse. The fact that horses also have many fewer cones in the central retina than humans do also suggests that their perception of color may not be as vivid as that of humans and that colors appear as washed-out pastels or sepia. Nevertheless, this serves the horse’s purpose.\n7. Can horses see at night?\nYes, they have a number of adaptations that allow them to visually outperform humans in dim light. For example, they have a large pupil that is arranged as a rectangular slit, they have a reflective tapetum that further improves vision in dim light by acting as an internal light reflector (causing their eyes to \"glow” at night if the light is just right), and they have more rod photoreceptors that work in dim light. Humans have developed the ability to see color at the expense of their abilities to see well in dim light. Instead of developing to identify ripe, yellow fruit hanging from a green tree (like humans have), horses have evolved to function under a much greater range of lighting conditions (perhaps five times larger than humans) and to afford a more panoramic view so as to avoid predators. Because the horse’s food is essentially immobile and not particularly challenging to capture, much of its visual abilities are dedicated to ensure the animal’s survival by being \"defensive” in nature. Since many predators have evolved coat colors that closely match the background in terms of color, and in some cases texture, color may be a relatively poor way for the horse to \"break the camouflage” of the wolf and it can better see the wolf by having better visual acuity, depth perception, ability to function in dim light, and a wider field of view.\nLiked this article? Here are others you'll enjoy:\nIs it Safe to Ride a Blind Horse\nWatch for Eye Problems in Senior Horses\nCaring for a Blind Horse\nGive us your opinion on\nHow Do Horses See The World?"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:e98d327a-9ff6-476c-bc03-bf0e07b240ae>","<urn:uuid:d8c3a9ed-bf11-44f0-b353-b46f0f067e86>"],"error":null}
{"question":"How do archaeologists combine analysis of lithic flakes with broader site surveying techniques?","answer":"Archaeologists integrate detailed artifact analysis with comprehensive site surveying methods. When examining lithic flakes, they look for specific characteristics like bulbs of applied force, which indicate how the flakes were detached from their parent cores and what materials were used as fabricators. This detailed analysis is complemented by broader surveying techniques including remote sensing methods like aerial photography, ground penetrating radar, and LIDAR scanning, which help identify and map archaeological sites before excavation begins.","context":["- Bulb of applied force\nlithic analysis, a subdivision of archaeology, a bulb of applied force (also known as a bulb of percussion or simply bulb of force) is a defining characteristic of a lithic flake. When a flake is detached from its parent core, a portion of the Hertzian coneof force caused by the detachment blow is detached with it, leaving a distinctive bulb on the flake and a corresponding flake scar on the core. Bulbs of applied force may be distinctive, moderate, or diffuse, depending upon the force of the blow used to detach the flake, and upon the type of material used as a fabricator. Generally, the harder the material used as a fabricator, the more distinctive the bulb of applied force.\nWikimedia Foundation. 2010.\nLook at other dictionaries:\nLorentz force — This article is about the equation governing the electromagnetic force. For a qualitative overview of the electromagnetic force, see Electromagnetism. For magnetic force of one magnet on another, see force between magnets. Electromagnetism … Wikipedia\nLithic flake — Flakes may be produced by a variety of means. Force may be introduced by direct percussion (striking the core with a percussor such as a rock or antler), indirect percussion (striking the core with an object, sometimes referred to as a punch,… … Wikipedia\nPrismatic blade — In archaeology, a prismatic blade is a long, narrow, specialized lithic flake with parallel margins. Prismatic blades are removed from polyhedral blade cores through pressure reduction. This process results in a very standardized finished tool… … Wikipedia\nLithic reduction — The Levallois technique of flint knapping Lithic reduction involves the use of a hard hammer precursor, such as a hammerstone, a soft hammer fabricator (made of wood, bone or antler), or a wood or antler punch to detach lithic flakes from a lump… … Wikipedia\nLandslide mitigation — Landslides can be triggered by many often concomitant causes. In addition to shallow erosion or reduction of shear strength caused by seasonal rainfall, causes triggered by anthropic activities such as adding excessive weight above the slope,… … Wikipedia\nHeat equation — The heat equation is an important partial differential equation which describes the distribution of heat (or variation in temperature) in a given region over time. For a function of three spatial variables ( x , y , z ) and one time variable t ,… … Wikipedia\nAmerican football protective equipment — ( football gear ) consists of equipments worn by football players for the protection of the body during the course of football games. Basic equipments worn by almost all football players include a helmet, shoulder pads, a jockstrap (aka athletic… … Wikipedia\nanalysis — /euh nal euh sis/, n., pl. analyses / seez /. 1. the separating of any material or abstract entity into its constituent elements (opposed to synthesis). 2. this process as a method of studying the nature of something or of determining its… … Universalium\nMathematics and Physical Sciences — ▪ 2003 Introduction Mathematics Mathematics in 2002 was marked by two discoveries in number theory. The first may have practical implications; the second satisfied a 150 year old curiosity. Computer scientist Manindra Agrawal of the… … Universalium\nenvironment — environmental, adj. environmentally, adv. /en vuy reuhn meuhnt, vuy euhrn /, n. 1. the aggregate of surrounding things, conditions, or influences; surroundings; milieu. 2. Ecol. the air, water, minerals, organisms, and all other external factors… … Universalium","Engaged Archaeology. The first step in an archaeological excavation is surveying the area. This can be done either with remote sensing or direct visual observation. Archaeologists conducting a survey. Archaeologists also use non-invasive techniques to survey sites known as remote sensing. There are many methods including aerial photography which is simply taking pictures from an airplane, hot air balloon or even a remote controlled drone; ground penetrating radar which is used to locate artifacts hidden below ground, and LIDAR, which uses lasers to scan the surface from the air through vegetation.\nWhat Is Chronometric Dating?\nArchaeology is a branch of Anthropology. Archaeologists accomplish heir task mainly through excavation. Excavation is the process of finding sites that may contain artifacts. Artifacts are relics of the past. They are anything created or influenced by man.\nWhen used as a prefix or suffix of a date, it indicates the number of As various types of soil form under different conditions, soil analysis at an.\nThis page has been archived and is no longer updated. Despite seeming like a relatively stable place, the Earth’s surface has changed dramatically over the past 4. Mountains have been built and eroded, continents and oceans have moved great distances, and the Earth has fluctuated from being extremely cold and almost completely covered with ice to being very warm and ice-free. These changes typically occur so slowly that they are barely detectable over the span of a human life, yet even at this instant, the Earth’s surface is moving and changing.\nAs these changes have occurred, organisms have evolved, and remnants of some have been preserved as fossils. A fossil can be studied to determine what kind of organism it represents, how the organism lived, and how it was preserved. However, by itself a fossil has little meaning unless it is placed within some context. The age of the fossil must be determined so it can be compared to other fossil species from the same time period.\nUnderstanding the ages of related fossil species helps scientists piece together the evolutionary history of a group of organisms. For example, based on the primate fossil record, scientists know that living primates evolved from fossil primates and that this evolutionary history took tens of millions of years. By comparing fossils of different primate species, scientists can examine how features changed and how primates evolved through time. However, the age of each fossil primate needs to be determined so that fossils of the same age found in different parts of the world and fossils of different ages can be compared.\nThere are three general approaches that allow scientists to date geological materials and answer the question: “How old is this fossil?\nIn academic, historical, and archaeological circles, A. Dates are determined by a variety of processes, including chemical analyses as in radiocarbon dating and thermoluminescence , data correlation as in dendrochronology , and a variety of other tests. See Relative Dating. Acheulean — A stone tool industry, in use from about 1. It was characterized by large bifaces, particularly hand axes.\n10, B.C. to A.D. Historic artifacts and features in Indiana date after this time and refer to peoples of many ethnic and cultural backgrounds. These include.\nHaving an accurate time scale is a crucial aspect of reconstructing how anatomical and behavioral characteristics of early hominids evolved. Relative dating methods allow one to determine if an object is earlier than, later than, or contemporary with some other object. It does not, however, allow one to independently assign an accurate estimation of the age of an object as expressed in years. The most common relative dating method is stratigraphy. Other methods include fluorine dating, nitrogen dating, association with bones of extinct fauna, association with certain pollen profiles, association with geological features such as beaches, terraces and river meanders, and the establishment of cultural seriations.\nCultural seriations are based on typologies, in which artifacts that are numerous across a wide variety of sites and over time, like pottery or stone tools. If archaeologists know how pottery styles, glazes, and techniques have changed over time they can date sites based on the ratio of different kinds of pottery. This also works with stone tools which are found abundantly at different sites and across long periods of time.\nCity of Alexandria, Virginia\nWithout the ability to date archaeological sites and specific contexts within them, archaeologists would be unable to study cultural change and continuity over time. No wonder, then, that so much effort has been devoted to developing increasingly sophisticated and precise methods for determining when events happened in the past. Chronometric dating techniques produce a specific chronological date or date range for some event in the past.\nFor example, the results of dendrochronology tree-ring analysis may tell us that a particular roof beam was from a tree chopped down in A. Relative dating techniques , on the other hand, provide only the relative order in which events took place.\nThe following is a list of some archaeological terms that are used on our website. Please note that this is not an exhaustive list. For further terminology, please see the publication of the Historic Resources Branch of the Department of Culture, Heritage and Tourism of the Government of Manitoba, now hosted by the University of Manitoba.\nAbbreviation of the Latin anno Domini , meaning “in the year of our Lord. The study of humankind in all times and in all places. It takes a comprehensive approach to the study of the origin, behaviour and evolution of humans, looking at their biological, linguistic, cultural, social and economic characteristics and at their variability.\nDating Techniques In Archaeology\nThe Archiving the Archaeologists series is an oral history project of video interviews of archaeologists near retirement or already retired. Listen to real archaeologists reflect on their careers, how and why they became archaeologists, and their contributions to the discipline on the SAA YouTube channel.\nThe methods used by archaeologists to gather data can apply to any time period, including the recent past. One archaeologist in the U. This “garbology” project proved that even recent artifacts can reveal a lot about the people who used and discarded them.\nWhile historians and archaeologists both use written documents to learn about the past, The methods used to find sites will depend on the kind of research questions that the In the American Southwest, tree ring dating goes back to 59 BC.\nDating techniques are procedures used by scientists to determine the age of rocks, fossils, or artifacts. Relative dating methods tell only if one sample is older or younger than another; absolute dating methods provide an approximate date in years. The latter have generally been available only since Many absolute dating techniques take advantage of radioactive decay , whereby a radioactive form of an element decays into a non-radioactive product at a regular rate. Others, such as amino acid racimization and cation-ratio dating, are based on chemical changes in the organic or inorganic composition of a sample.\nIn recent years, a few of these methods have come under close scrutiny as scientists strive to develop the most accurate dating techniques possible. Relative dating methods determine whether one sample is older or younger than another.\nDating Rocks and Fossils Using Geologic Methods\nWhen the last excavated trench is backfilled with dirt and when survey is completed for another season, one is left with only the records, drawings, photographs, and cultural material to make sense of what everything means. The processing and interpretation of those material remains, in conjunction with the records, is the essential final step in completing the picture of past human activities occurring in an area over time. Artifacts, ecofacts, and features say little themselves, but researchers can make meaningful inferences about these when they are studied closely and in detail.\nAnalysis is the examination, description, classification, and identification of that material, as well as consideration of its broader meaning.\nDating in archaeology is the process of assigning a chronological value to an event in the past. Determining the hydration rate has been done in two ways.\nAll rights reserved. Relative techniques were developed earlier in the history of archaeology as a profession and are considered less trustworthy than absolute ones. There are several different methods. In stratigraphy , archaeologists assume that sites undergo stratification over time, leaving older layers beneath newer ones.\nArchaeologists use that assumption, called the law of superposition, to help determine a relative chronology for the site itself. Then, they use contextual clues and absolute dating techniques to help point to the age of the artifacts found in each layer. Learn how archaeologists dated the earliest metal body part in Europe.\nDating in Archaeology\nHe is affiliated with Cornell University. Columbus famously reached the Americas in Other Europeans had made the journey before , but the century from then until marks the creation of the modern globalized world.\nIn Alexandria, archaeological sites are discovered in a variety of ways. City Archaeologists consult maps, deeds, census, tax and other records. Historic and Native American sites can also be located through field surveys walking across the ground looking for artifacts. Sometimes sites are discovered by chance by home-owners who find artifacts, building foundations, abandoned wells or privies in basements and backyards. Such discoveries are visited by the City Archaeologists and are recorded with notes and photographs.\nInformation is then added to Alexandria Archaeology’s register of sites in the City. The City of Alexandria has laws that protect archaeological sites so that information may be recovered before they are destroyed by development. Planning dockets and construction applications are reviewed to see if proposed commercial projects could disturb sites.\nARCHAEOLOGY, TOOLS, METHODS AND ANALYSIS\nTwo systems of archaeological dating are used: absolute and relative chronology. Absolute chronology dates events in terms of the generally accepted calendar; relative chronology determines only the sequence of events. Relative dates are established by stratigraphy and by the typological method. The stratigraphic method, which observes the sequence of earth strata containing artifacts, makes it possible to attribute each stratum to a definite epoch.\nUsually, the deeper the stratum, the older it is.\nUsing relative and radiometric dating methods, geologists are able to answer the By comparing fossils of different primate species, scientists can examine how To establish the age of a rock or a fossil, researchers use some type of clock to P.R., Swisher, C.C. 40Ar/39Ar dating in paleoanthropology and archaeology.\nOpening King Tut’s tomb Archaeology is the study of historic or prehistoric people and their culture through the study of their artifacts, monuments and other items they left behind. Many archaeological sites are discovered accidently, often during construction projects. How they have new, almost forensic-like science to collect pollen and understand the vegetation. They do things that are unprecedented, in a way, and it’s very beautiful to see that.\nI’m really intrigued by modern-day archaeology. For example, a square foot in one of the caves in the filmit took five months to remove half a centimeter of sediment. Every single grain of sand was picked up with a pair of pincers and documented with laser measurements."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:cd86bdc2-1563-457f-9a14-8c63f03018d0>","<urn:uuid:192d7cf2-cd61-4c6c-838c-86fb64e41eb7>"],"error":null}
{"question":"How do the implications of New Prime v. Oliveira differ between transportation workers and non-transportation workers in terms of arbitration enforcement?","answer":"For transportation workers, New Prime v. Oliveira significantly expanded their ability to avoid arbitration by extending FAA Section 1 exemptions to both employees and independent contractors in the interstate transportation industry, allowing them to bring claims directly to court instead of being bound by arbitration agreements. However, for non-transportation workers, the case has minimal impact since it only affects industries involved in interstate transportation, not all industries involved in interstate commerce. The case confirms that workers still need to be 'transportation workers' engaged in interstate commerce to be exempted from arbitration requirements under the FAA. In New Jersey, as demonstrated in the Arafa and Colon cases, even when transportation workers are exempt from FAA arbitration, they may still be subject to state arbitration laws like the NJAA.","context":["From JDSupra, John Lewis discusses a recent case in New Jersey in which the court said that transportations were not subject to the Federal Arbitration Act (FAA) but were subject to the New Jersey Arbitration Act (NJAA). John writes:\nAs we noted in our June 10, 2019 blog post, many have questioned whether state arbitration laws could be applied when some transportation workers are held to be exempt from the Federal Arbitration Act (FAA) based on Section 1 of that act. This quandary was fueled by New Prime, Inc. v. Oliveira, 139 S. Ct. 532 (2019), and lower court opinions trying to harmonize Circuit City Stores, Inc. v. Adams, 532 U.S. 105, 113, 119 (2001), with the diverse group of workers involved in and around the movement of goods and passengers. See our April 29, 2019; April 3, 2020; and June 1, 2020 blogs detailing the potential tests to determine who might qualify as a transportation worker in interstate commerce. In other words, must they cross state lines, be directly involved in the process, carry goods rather than passengers or be in the actual stream of interstate rather than local commerce?\nNow, at least some arbitral certainty has been added by the Supreme Court of New Jersey in Arafa v. Health Express Corp. and Colon v. Strategic Delivery Solutions, LLC, Nos. 083174 and 083154 (July 14, 2020). The New Jersey Supreme Court opinion resulted from two cases where respective appellate divisions reached conflicting conclusions. The common question in both cases was “whether the disputed arbitration agreements would be enforceable under the New Jersey Arbitration Act (NJAA) . . . if they were exempt from the FAA.” The exemption clause declares that the FAA shall not “apply to contracts of employment of seamen, railroad employees, or any other class of workers engaged in foreign or interstate commerce.” The ultimate answer is “yes” the NJAA can apply.\nBoth cases stemmed from class action overtime claims brought by delivery drivers in New Jersey and the surrounding areas. In the Colon case, the court of appeals found that the NJAA could apply to an arbitration agreement exempt from the FAA and that the claims could be compelled to arbitration. See Colon v. Strategic Delivery Sols., LLC, 459 N.J. 349, 360 (App. Div. 2019).\nIn Arafa, decided the next day, a different court of appeals held that the driver was exempt from the FAA. This second court relied on the U.S. Supreme Court’s decision in New Prime Inc. v. Oliveira and found that the FAA exemption essentially undercut the entire basis for the agreement.\nThe New Jersey Supreme Court Analysis\nIn addressing the legal question found in both appeals – whether the agreements are exempt from the FAA premised on 9 U.S.C. § 1 they can be enforced under the NJAA – the Supreme Court considered three related issues. First, the Court found that the NJAA needed no “express invocation” to apply in the agreement. Instead, the law applies unless preempted by the FAA.\nSecond, the FAA has no specific preemption provision and the use of the NJAA would not conflict with any congressional purpose in the FAA. Indeed, Congress’s intent behind the exemption was later to pass legislation specifically covering certain transportation workers, not to exclude those workers from arbitration in any venue. Slip Op. at 24. Hence, “Congress did not intend to exclude transportation workers from arbitration altogether, but rather to subject their agreements to other statutes that may or may not require arbitration.” Id.\nFinally, the Supreme Court rejected the argument that the inapplicability of the FAA would negate the entire agreement to arbitrate. Because both agreements had severance clauses, they reflected the parties’ intent that the agreement was to survive the elimination of any unenforceable provision.\nThe Court concluded that the NJAA could apply to arbitration agreements even if employees are exempt from the FAA.\nThe parties in both Colon and Arafa were not exempt from arbitration and their agreements enforceable to require individual arbitration. As to Colon, the agreements are enforceable under the FAA or NJAA, to be ultimately resolved on remand when the trial court will determine if the employees were transportation workers engaged in interstate commerce.\nWhen dealing with workers who have a nexus with transportation, state arbitral law and procedures should be considered in order to limit uncertainty. But not all states permit enforcement of arbitration agreements with class or collective waivers if the FAA does not apply.Provisions in the agreements should also address severability, applicable law and situations in which the class or collective action waivers are not enforceable under state law. Facing the potential for aggregate arbitration, many companies would rather defend the claims in court.\nState law regarding arbitration agreements still may require that claims be arbitrated regardless of whether the parties are exempt under Section 1 of the FAA.","New Prime v. Oliveira: What Does It Mean For Arbitration Agreements?\nOn January 15, 2019 the United States Supreme Court released its decision in New Prime v. Oliveira, and for what was likely the first time in almost two decades, the Court did not enforce an arbitration agreement. In its unanimous decision, the Supreme Court expanded the reach of the Federal Arbitration Act’s section 1 exceptions, allowing a bigger group of individuals to circumvent their arbitration agreements and bring their claims directly to court.\nSo, what does this case mean for business owners and employees? Arbitration around Phoenix is a popular legal topic, especially when it comes to employment agreements and disputes. Does this new case render arbitration agreements worthless? Do employees suddenly have the upper hand when it comes to their arbitration agreements? This article will fill you in on everything you need to know about this decision, whether you are an employer or an employee.\nBackground: the FAA and New Prime\nThe New Prime case has to do with the Federal Arbitration Act (“FAA”), which enforces contractual arbitration provisions. Most commonly, and relevant here, this includes arbitration clauses found in employment agreements, which are widespread today. In New Prime, Mr. Oliveira was hired by New Prime as a truck driver. Initially, Mr. Oliveira was hired as an independent contractor, and signed an employment agreement with New Prime that stated that in the event of a dispute between them, the matter would go to arbitration instead of going to court. Additionally, the agreement held that an arbitrator, not a court, would determine whether potential disputes could be arbitrated. During his employment with New Prime, Mr. Oliveira was regularly paid in such a way that he was not receiving the minimum wage required by law, and so he brought suit against New Prime in a class action lawsuit. New Prime brought up the arbitration provision in the employment agreement, saying the case needed to be taken out of court and arbitrated. Mr. Oliveira pointed to section 1 of the FAA, which exempts “transportation employees” involved in interstate commerce. New Prime countered that Mr. Oliveira was an independent contractor, not an employee. However, New Prime argued that pursuant to their agreement, an arbitrator should decide whether the issue was subject to arbitration, and whether section 1 of the FAA applied. These two issues made their way to the Supreme Court, where the Court held that the language of the FAA indicated that independent contractors as well as employees could be considered “transportation workers,” and therefore exempt from the arbitration requirements of the FAA. The Court also held that a court, not an arbitrator, should decide whether section 1 of the FAA applied, regardless of any arbitration agreement, because if section 1 applied in the first place, those workers were exempt from arbitration, even to determine whether arbitration was appropriate.\nWhen the decision was handed down by the Court, some commentators discussed the possibility that this case would be a basis for allowing massive amounts of employees to be exempted from compulsory arbitration under FAA section 1. They pointed to Justice Ginsberg, who had agreed with the decision but also filed a concurring opinion in which she stated that the policy goals of the FAA were to exclude a much wider group of employees from compulsory arbitration agreements, even if the text of the statute was less clear. This was not the first time that Justice Ginsberg had voiced this opinion, and legal commentators have commented that New Prime might be the case to begin exempting more and more employees from arbitration agreements.\nWhat New Prime does and what it does not do\nNew Prime does expand the section 1 exceptions of the FAA. It does not, however, expand them as far as many might have feared. New Prime in no way changed or overruled the class of workers, “transportation workers,” that are excluded under FAA section 1. Workers need to be “transportation workers” engaged in interstate commerce to be exempted from arbitration requirements under the FAA.\nNew Prime does, however, expand FAA section 1 exceptions to independent contractors who are transportation workers engaged in interstate commerce. Under the case, the modern technical differences between an employee and an independent contractor do not matter when it comes to determining whether a worker is exempt under FAA section 1. This means that many more people will be exempt from FAA arbitration requirements. However, this will be limited to industries involved in interstate transportation only and will not affect all industries involved in interstate commerce, which would be essentially every modern American business.\nWhat does it mean for you or your business?\nThe reality is, unless you are working in the interstate transportation industry, New Prime probably is not a huge deal for you. The decision is an important one from the Supreme Court and could be an indication of future changes in arbitration jurisprudence, but all of that is speculation. Concretely, New Prime will impact only businesses and workers involved in interstate transportation.\nIf you do happen to be a business owner or employer working in the interstate transportation industry, you do need to be aware of this case and its implications. Prior to New Prime, workers could be employed as independent contractors to keep them out of the FAA section 1 exception. Now, those workers’ status as independent contractors will be considered equivalent to an employees’ exempt status. Therefore, all of your workers, regardless of technical status as independent contractor or employee, will have the option to avoid any potential arbitration agreements and bring their claims directly to court. This is something you need to be aware of as a business owner, especially if you have relied heavily on arbitration in the past. Although arbitration may still be an option if workers are willing to arbitrate, following New Prime those workers have the right to bring their claims to court and avoid arbitration if they so choose.\nIf, on the other hand, you are employed in the interstate transportation industry as an independent contractor or other worker, New Prime means that you may no longer be bound by any arbitration agreements you may have entered into with your employer. If you have claims you may want to bring to court, New Prime now allows you to do so and avoid arbitration. Now that the FAA section 1 exceptions extend to you, you cannot be bound to compulsory arbitration and may bring claims against your employer directly in court, including through a class action suit.\nNew Prime is undoubtedly a big Supreme Court case, and has a very real and significant impact, but only for a limited range of businesses and individuals in America. For those businesses and individuals working in interstate transportation, this case will certainly cause significant changes in the way disputes between employers and workers are resolved. By extending the FAA’s section 1 exceptions to a wider class of transportation workers, arbitration will no longer monopolize dispute resolution, and workers will be better able to bring claims against their employers in court.\nSince New Prime is such a new case, however, it is difficult to know exactly the impact it will have on businesses and workers. Skilled business attorneys with experience dealing with arbitration matters can assist you in navigating this new arbitration framework, whether you are a business owner or an employee. The attorneys at Denton Peterson Dunn, PLLC have been handling business and arbitration matters for decades, and have the experience and skill needed to make sure you are protected under the law."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:d69f81de-8420-4b8f-ba3e-f0f48aea0f7c>","<urn:uuid:2c3c7c8a-0c40-42b8-b2b6-65c704ef8625>"],"error":null}
{"question":"Looking back at supply chain transparency evolution - what major changes happened in 2012 vs 2021 regarding corporate disclosure requirements?","answer":"In 2012, the California Transparency in Supply Chains Act went into effect, requiring manufacturers and retailers to disclose their efforts to address slavery and human trafficking in supply chains. In 2021, there was a significant development when Nike and Adidas released their Annual Factory Lists in machine-readable format for the first time, though their supply chain reporting still lacked GHG emissions data and renewable energy targets for external vendors.","context":["California Supply Chain Act\nCalifornia Transparency in Supply Chain Act Disclosure\nThe Home Depot, its affiliates, divisions, and subsidiaries strive to conduct business in a responsible manner. As we expand our business activities and work with suppliers domestically and globally to meet customers’ needs, it is important to preserve our collective commitment to human rights and safety in the workplace.\nThe Home Depot expects that all suppliers will abide by all applicable international and local laws, rules and regulations in the manufacture and distribution of merchandise or services provided to The Home Depot. All suppliers are strongly encouraged to exceed The Home Depot’s guidelines and promote continuous improvement throughout their operations.\nOn January 1, 2012, the California Transparency in Supply Chains Act of 2010 (SB 657) went into effect in the State of California. This law is designed to increase the amount of information made available by manufacturers and retailers regarding their efforts (if any) to address the issue of slavery and human trafficking, thereby allowing consumers to make better, more informed choices regarding the products they buy and the companies they choose to support.\nSince 2005, The Home Depot has had a set of Social and Environmental Responsibility (SER) Standards which all suppliers are obligated to comply with by contract. Specifically our standards state the following for \"Forced Labor\":\n\"Suppliers will not use any form of involuntary labor including forced, prison, indentured, bonded, slave, or human trafficked labor.\"\nWe also actively monitor child labor in our supply chain which can be associated with slavery and human trafficking:\n\"Suppliers must not employ workers younger than the greater of 15 years of age -- or 14 where the local law allows such an exception consistent with International Labor Organization guidelines -- or the age for completing compulsory education or the minimum age established by law in the country of manufacture. In addition, Suppliers must comply with all local legal requirements for the work of authorized young workers, particularly those pertaining to hours of work, wages, and working conditions.\"\nThe Home Depot utilizes internal staff as well as a number of third-party social responsibility firms to monitor and prevent human rights abuse in our supply chain.\nOur efforts include:\nSupply Chain Verification\nIn addition to the contractual obligations outlined above, The Home Depot performs factory assessments (Audits) of suppliers of private brand and direct import products on a periodic basis. Every factory receives an Audit at least every two (2) years. The majority of our factories receive a new Audit every six (6) months to one (1) year, while those achieving a 95% score or better receive a new audit less frequently. These assessments are conducted by trained individuals from our internal SER team as well as our third-party social responsibility firm partners. Notably, almost a quarter of the questions used in these Audits focuses on the risks associated with the various types of forced and child labor and other national laws associated with them.\nThe Home Depot evaluates and verifies suppliers’ compliance with our SER Standards via on-site auditing. As described above, we audit all factories manufacturing private brand and direct import products while contractually holding our national brand partners to the same level of due diligence. Announced audits are conducted prior to placing any purchase orders with a private brand or direct import supplier. We also utilize pre-source assessments, specific response Audits when we are directly made aware of an issue by any source, and a comprehensive risk based re-audit program, which include both announced and unannounced audits, to ensure these factories are monitored on a continuous basis. At the conclusion of every Audit where an issue of noncompliance is found, a Corrective and Preventive Action (CAPA) plan is created with firm timelines based on the level of risk (High 30 days, Medium 90 days). The Home Depot has a policy of verifying the CAPA implementation to confirm that the factories are meeting the commitments set forth in the CAPA plan. Evidence of forced or child labor during any of the aforementioned discovery methods is deemed a \"Critical Deficiency\" and results in a multi-step engagement between The Home Depot and the supplier. This engagement consists of three (3) visits to the factory over a six (6) month period. At the completion of this engagement, a new Audit is required and the factory must demonstrate systemic improvements or risk termination by The Home Depot.\nThe Home Depot has a Supplier Buying Agreement in place with our suppliers requiring them to comply with international standards and applicable laws and regulations, including those related to forced labor and child labor as specified in The Home Depot’s SER Standards. In the event of a non-compliant supplier or factory, we work with our suppliers on remediation efforts. Notwithstanding the above, we have a termination mechanism in place which defines the circumstances under which a factory or supplier will be excluded from conducting future business with The Home Depot.\nWe believe in Respect for All People and Doing the Right Thing, which are emphasized to all associates as part of our values wheel. To help enforce our values, The Home Depot has an anonymous hotline available to our associates for the purposes of reporting violations of company policy or legal requirements. This resource is available 24 hours a day, 365 days a year and each report is investigated by our corporate compliance team.\nThe Home Depot trains internal employees responsible for sourcing products on how to identify and respond to social and environmental responsibility issues, such as forced labor or child labor.","Introducing Supply Chain Knowledge Graph for Climate Action\nWhile ESG disclosures are still voluntary and unstandardized, companies will soon be obliged to tell investors about their “carbon footprint”.\nSupply chains are the crux of the issue…\n90% of companies’ carbon footprint is located in the supply chain, often generated by third-party vendors.\nAs of 2021, there has been more than $130 trillion in public and private investment pledged to help curb climate change, but investors and climate action stakeholders need supply chain data to make better-informed decisions.\nNike, for example, aims to use 100% renewable energy by 2025 but its pledge applies only to its own offices, stores and factories… while 80% of the brand’s carbon footprint comes from the *sourcing* of its materials and manufacturing.\nOver 100,000,000,000 metric tons of emissions (equal to emissions of Brazil and Mexico combined) could be saved if suppliers to just 125 multinationals increased their renewable energy by 20%. (According to CDP.)\nWith the Supply Chain Knowledge Graph for Climate Action stakeholders can now contribute information about supply chains of leading companies that are not covered by their environmental pledges but play a significant role in their carbon footprint.\nThis data is a missing link in analyzing the environmental risks related to companies and countries participating in the manufacturing processes.\nSupply Chain Knowledge Graph for Climate Action is an open dataset with a mission to improve country and company-level collaboration on renewable energy adoption and local manufacturing sustainability reporting.\nAccording to CDP, suppliers’ climate change risks could cost investors and companies over US$906 billion in the next 5 years.\nBuilding a No-Code Supply Chain Knowledge Graphs\nMany companies are already releasing annual sustainability reports, but due to a lack of standardized reporting, analyzing their impact on the environment continues to be a challenge.\nWith no-code knowledge graph tools like Kgbase, climate action stakeholders can now collaborate on supply chain data.\nRemember Nike? In 2021, Nike and Adidas released their Annual Factory Lists in a machine readable format for the first time.\nThanks to Kgbase’s unique set of tools (collaboration, knowledge graph visualization, data aggregation, mapping and analysis), I was able to unify the company’s data and develop a Supply Chain Knowledge Graph for Climate Action.\nHere is how I created this data initiative with Kgbase — and how you can create a project about any topic you care about.\nCreate A Unified Template\nEven though the Adidas and Nike factory lists include very similar data inputs, their reporting format varies and requires significant cleanup and organization prior to analysis and mapping.\nBoth companies have released carbon neutrality pledges to the public, but their supply chain reporting does not contain GHG emissions data or renewable energy targets for external vendors, or gender data on their vendors’ leadership.\nWith Kgbase, you can create public and private templates for such reports and define the data inputs for collaboration, review and approval.\nImport & Map Relevant Data\nOnce you have your template ready, it’s time to import the data into the project.\nKgbase offers numerous integrations, file format options and databases to simplify this process for you and other contributors.\nCreate Relationships Between Entities\nThe unique properties of a knowledge graph enable an analysis of the relationships between entities.\nFor this project, I was able to create relationships between countries, brands and factories to identify which nations’ actions toward renewable energy solutions can bring the most impact in eliminating environmental risks.\nBuild Complex Queries to Analyze the Graph\nBy building complex queries with Kgbase you can answer this and many other questions about supply chain data.\nGenerate Universal Business Identifiers to Track Vendors across Supply Chains\nWhen analyzing the supply chains of Adidas, Nike and Under Armour, I found that the 3 companies, regardless of different policies and levels of engagement into sustainability efforts and ESG scores, share the same external vendors.\nFor example: Anhui Sporting Goods Limited\nSmall private companies like this one are often overlooked in the mapping process.\nWith Kgbase you can generate Universal Business IDs to track factories across different companies’ supply chains and enrich the data.\nUBIDs have many applications across finance, data science and business and can be generated and added to a database of companies here.\nAbout Knowledge Graphs\nKnowledge Graphs help build semantic networks that are useful in knowledge representation which helps in establishing relationships between different concepts and how they are visually interconnected.\nThis interconnectivity helps in gaining new insights from the available data which can lead to new discoveries, different perspectives in analytics tasks and to provide valuable information for predictive pipelines.\nKgBase, or Knowledge Graph Base, is a collaborative, robust database with versioning, analytics & visualizations.."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:4931daba-264a-4b02-8ab4-6d6bc24317a6>","<urn:uuid:f7cf7aa4-f6d8-46de-9303-bfde1eca4b7a>"],"error":null}
{"question":"¿Cuáles son las diferentes formas de expresar resultados y conclusiones en inglés académico?","answer":"There are several ways to express results and conclusions in English: 1) For expressing results, you can use phrases like 'for this reason,' 'as a result,' 'as a consequence,' 'because of this,' and 'therefore.' 2) For summarizing and concluding, you can use expressions such as 'in summary,' 'to summarize,' 'in conclusion,' 'in short,' 'to sum it all up,' 'in the final analysis,' and 'at the end of the day.' These transitional words help maintain clarity in communication and should typically be followed by a comma.","context":["ENGLISH TRANSITIONAL WORDS\nTransitional words and phrases are plentiful in English. Some modern English writing experts have advocated reducing the use of connectors and linkers. That is to say that the overuse or misuse of English transitional words should be avoided. For English learners, these words serve a useful purpose or multiple purposes. These words help keep one’s thoughts, words and writing clear; and using them helps the reader or listener to stay on the same page with the speaker or writer. Transitional words can be used as signposts. They enrich speech and written communication and makes the speaker or writer seem more intelligent or more fluent especially when the words are used correctly and judiciously. (One rule of thumb to remember is that you must almost always follow a transitional connector by a comma.)\nTransitional words like “additionally,” “therefore,” and “moreover” are used to:\n- Show contrast e.g. “By contrast”…\n- Add information e.g. “Additionally”…\n- Generalize e.g. “Ordinarily”…\n- Express results e.g. “As a result”…\n- Illustrate or emphasize a point e.g. “Accordingly”…\n- Summarize a series of points e.g. “In conclusion”…\n- Emphasize, Clarify, Repeat or Reiterate a point e.g. “Clearly”…\n- Keep a sequential order to your words or writing and show a time Relationship e.g. “Firstly”…\n- Express a comparison, similarities and differences in words or text e.g. “Similarly”…\n- Give examples and/or alternatives e.g. “Alternatively”…\n- Concede a point e.g. Concededly, “Admittedly”….\nBelow are a list of transitional words plus an audio and quiz. Free with a monthly pass or premium pass.\nLISTEN TO THE AUDIO\n[section title=”#1 Showing Contrast”]\n- In contrast\n- By contrast\n- Even though\n1) In contrast with normal procedure, my boss asked us to ignore the fire alarm.\n2) In contrast to normal procedure, my boss asked us to ignore the fire alarm.\n3) The Picasso was expensive; by contrast, the Renoir was not.\n4) Learning English is easy; conversely, Learning Sanskrit is hard.\n5) The woman is beautiful; by contrast her daughter is totally unattractive.\n6) The idea was to have a small wedding; however my mother had other ideas.\n7) I dont think I can make it tonight; rather, I would prefer to go home and rest and see you tomorrow.\n8) The attorney was insulted by the judge; yet she maintained her composure.\n9) This situation is not ideal but I do not have a choice but to go forward with it.\n10) The plaintiff did not want to settle the lawsuit; instead, she instructed her lawyer to prepare for trial.\n11) Although this multinational has polluted the global environment, no international court has been able to find them liable for gross negligence.\n12) Even though I am upset, I will still go through with the plan.\n13) Nevertheless, we parted as friends.\n14) Despite the urgency of the situation, the paramedics took their time to respond to the 911 call.\n[section title=”#2 Adding information”]\n- In addition\n- Added to that\n- As well as\n- By the way\n- As if that was not enough\n- On top of that\n1) Additionally, I am fluent in three languages.\n2) In addition, I am fluent in three languages.\n3) Added to that, I am fluent in three languages.\n4) Moreover, this was not one of the terms agreed upon.\n5) Furthermore, this was not one of the terms agreed upon.\n6) Likewise, this situation is unacceptable.\n7) Also, I would like to reiterate my interest in the position.\n8) Chinese nationals, as well as Malaysians, were on the doomed flight.\n9) Incidentally, nobody informed me that there would be a meeting this morning!\n10) By the way, are you going to John’s bachelor’s beer night?\n11) Notably, these results are inconclusive.\n12) And, it was a grand disaster, if I may say so myself.\n13) As if that were not enough, she then proceeded to where red stilettos.\n14) On top of that, I was hungry and tired.\n15) Plus, she paid for Google advertising.\n[section title=”#3 Generalizing”]\n- In general\n- As usual\n- As always\n- As a general rule\n- More often than not\n- In most cases\n- For the most part\n1) Generally, I don’t like blond perms.\n2) In general, PayPal is reliable.\n3) Normally, cats and dogs do not get along.\n4) As usual, she showed up late.\n5) As always, she showed up late.\n6) typically, men prefer to ask a woman to marry them; not the other way around.\n7) Ordinarily, he is a lot more careful than that.\n8) As a general rule, this company likes to avoid litigation.\n9) More often than not, I am late for work.\n10) In most cases, there appears to be no correlation.\n11) For the most part, I am content with my life.\n[section title=”#4 Expressing results”]\n- For this reason\n- As a result\n- As a consequence\n- Because of this\n- In the final analysis\n- In the end\n- This is why\n1) For this reason, I do not recommend this course of action.\n2) As a result, the hypothesis is flawed.\n3) As a consequence, I cannot make this recommendation.\n4) Finally, the other side acquiesced.\n5) Because of this, I think we should avoid this course of action.\n6) So, it is not a good idea to proceed.\n7) Thus, I can only conclude that this is not in our best interest.\n8) In the final analysis, it is you who has to make the decision.\n9) In the end, correlation appears to equal causation.\n10) Accordingly, I propose that you suspend action till further notice.\n11) Therefore, the marketing department should change its tactics.\n12) This is why the results are the same.\n[section title=”#5 Illustrating or emphasizing a point”]\n- That is\n- That means\n- That is to say\n- In particular\n- Most of all\n- Above all\n- In Fact\n- The fact is\n- For example\n1) This is, the financial data is inaccurate.\n2) That means, language Policy in the EU should remain an issue for each member state to handle.\n3) That is to say, the foregoing was in error.\n4) Incidentally, I never said that.\n5) In particular, what happened in the meeting should never happen again.\n* 6) Especially the first one.\n7) Most of all, the tribesmen would like assistance with brokering a peace deal.\n8) Above all, the primary consideration has to be the kids.\n9) In fact, it was mentioned in two clauses in the contract.\n10) Actually, this was not addressed in the boilerplate contract.\n11) The fact is, you are wrong.\n12) Namely, both groups contributed to the negligence.\n13) For example, the blue and red colors have been known to oxidize.\n[section title=”#6 Summarizing”]\n- In summary\n- To summarize\n- In conclusion\n- In short\n- To sum it all up\n- In the end\n- In the final analysis\n- On balance\n- To sum\n- At the end of the day\n1) Thus, my conclusion is that my hypothesis is correct.\n2) In summary, this sort of thing should never happen again.\n3) To summarize, let me begin by reiterating that the results are inconclusive.\n4) In conclusion, environmental protection agencies need to get more aggressive with protecting the global environment.\n5) Therefore, your position is unsupported by the facts.\n6) In short, you cannot do that.\n7) To sum it all up, 2014 was a banner year for us.\n8) In the end, I must concede that I was wrong.\n9) In the final analysis, there is no workable solution for this problem.\n10) Overall, the team did okay.\n11) Globally, the output was satisfactory but there were internal departments that need to show improvement.\n12) On balance, this could have been a much worse scenario.\n13) To sum it up, the red team wins.\n14) At the end of the day, he gained more than he bargained for.\n[section title=”#7 Repetition and emphasis”]\n- In other words\n- To put it mildly\n- That means that\n- That is to say that\n- Briefly stated\n- Put another way\n- Please take note that\n1) In other words, this proposal lacks persuasion.\n2) To put it mildly, I was shocked by their response.\n3) Clearly, this can’t happen again.\n4) That means that, barring any unforseen developments, I should complete my doctorate in 3 years.\n5) That is to say that what you did here was not only unethical, it was also illegal.\n6) Briefly stated, she felt threatened by her subordinate’s Superior intelligence.\n7) Reiterated, you must have made a mistake.\n8) Again, your leadership skills need some fine-tuning.\n9) Put another way, I was grossly disappointed.\n10) Notably, this center has always been at the center of Parisian culture.\n11) Please take note that, effective immediately, I no longer want to a member of this institution.\n[section title=”#8 Sequence, Order, Timing, Time Relations”]\n- Before that\n- In conclusion\n- At the same time\n- Just then\n1) First, I have worked in Madrid.\n2) Second, I speak Five languages which include Russian, Swahili, Chinese, French and English.\n3) Next, he took his shirt off.\n4) After, the boss yelled that she was an incompetent nincompoop.\n5) Then, ,her mother started to cry.\n6) Last, her husband asked for a divorce.\n7) Lastly, I would just like to close by saying that I had a marvelous time and would love to see you again.\n8) Finally, please give my regards to your mom.\n9) Before that, I worked at Ernst and Young.\n10) In conclusion, I think what you did amounts to malfeasance.\n11) The cars sped, contemporaneously, in opposite directions.\n12) At the same time, I did not think she would have sunk so low as to spy on me from a key hole.\n12) Just then, the doorbell rang.\n13) Earlier, I plugged the keyhole with toilet tissue.\n14) Later, the company hosted a cocktail soiree.\n15) Midway through the conference, the speaker collapsed.\n16) Simultaneously, the alarms sounded.\n[section title=”#9 Comparisons, similarities and distinctions”]\n- In the same way\n- By contrast\n- On the other hand\n- In a like manner\n- On the flip side\n1) Similarly, our firm should aim to be one of the top ten in the world.\n2) Likewise, we should want to be the best.\n3) In the same way, we should aim to be the best.\n4) By contrast, the level of motivation needed to become the best is lacking in our firm.\n5) Comparatively speaking, these two languages could not be more different in terms of the amount of time it takes to achieve proficiency.\n6) Correspondingly, the data for 2007 also shows a link between a change in administrative personnel and a drop in sales.\n7) On the other hand, there is another way to interpret this data.\n8) Conversely, when the same rules were applied in this situation, the outcome was not the same.\n9) In a like manner, you should not do and say to others what you would not have them do and say to you.\n10) On the flip side, social media advertising is very expensive.\n[section title=”#10 Giving examples”]\n- For example\n- Such as\n- For instance\n- To illustrate\n1) For example, this clause does not make sense.\n2) Example: Mr Garrette was late 25 times this month.\n3) I had numerous duties such as, responding to inquiries; doing research and assisting the director.\n4) For instance, she lied Under oath.\n5) To illustrate, lets suppose the two cars collided.\n6) Namely: the high bulbs with high resolution tend to last longer than the others.\n7) Particularly curious was the fact that he went surfing in weather such as that.\n8) Including: shirts, pants and blouses.\n9) Notably, this only happens on Wednesdays."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:fb1de5bf-1f76-4410-840d-f452b5d5bd9b>"],"error":null}
{"question":"Could you compare the planting depth requirements for annual flowers versus trees in garden soil?","answer":"Trees and annual flowers have significantly different planting depth requirements. Trees require soil preparation and loosening to a depth of up to 2 feet, while annual flowers like impatiens and begonias need much shallower soil preparation. For annuals, you only need to prepare holes about 1.5 times the size of their plant pots and ensure the top 12 inches of soil is properly turned over and amended with soil conditioner or compost. Both types of plants benefit from organic amendments, but the depth of incorporation differs based on their root systems.","context":["Poor soil grows poor plants. Unless you drew the lucky card and have a garden full of black gold, you’ll need to know how to improve soil. Improving garden soil is an ongoing process as the plants leach nutrients, leaving the soil inadequate for their needs. Whether your soil is nutrient deficient, compacted, heavy clay or any other issue, here’s a little soil amendment info to get you started.\nSoil Amendment Info\nSoil amendment can be as simple as mixing in leaf litter or it can be as complicated as running drainage pipes. The condition of your soil needs to be adequate for sustaining plant needs. Compact or hard soils are actually great for starting a lawn, as long as you add a little sandy topsoil if starting from seed. Plants like fruits and vegetables, however, need loose, nutrient rich soil with plenty of organic amendments added every year. There is no rule on the best soil for gardens, but there are some basic guidelines and some easy fixes.\nHow to Improve Soil\nIn most cases, the need for amending soil arises from having poor, compacted soil or nutrient deficient soil. Here are some general tips on improving your soil:\nPoor, Compacted Soil\nDense, hard soil may be the result of construction or simply little ones running across it constantly in play. The depth of compaction is important to know how to deal with it. If you have very deep, hard areas, you may have to rent equipment to dig it up and loosen it.\nLoosen the soil to a depth of at least 12 inches for most plants and up to 2 feet for trees and larger specimens. Garden soil preparation by manually shoveling is usually sufficient in most cases. Once the soil is loose, you may need to add several inches of compost or fine bark to keep it loose and workable.\nNutrient Deficient Soil\nImproving garden soil is imperative for a bountiful garden. Organic matter is the best soil amendment because it breaks down naturally to release nutrients for plant uptake. Some of the best items to use are:\n- Leaf litter\n- Clean hay or straw\n- Seedless weeds\n- Crop residue\n- Sphagnum moss\n- Peat moss\n- Pine needles\n- Grass clippings\n- Wood shavings\n- Dust and aged manures\nGarden soil preparation with these items works best if they are dug into the soil to a depth of 6 to 12 inches. You can even save your kitchen scraps to work into soil, but avoid meat, bones and fat. Cover crops provide “green manure” to work into soil in spring for an extra shot of nitrogen and increased soil percolation.\nMixing the Best Soil for Gardens\nThere isn’t an actual recipe for soil; however, it needs a good balance of macro-nutrients and micro-nutrients, should drain freely and have a balance of carbon to offset the nitrogen.\nAcid and alkaline soils can be amended with lime to sweeten the soil and sulfur to increase the acidity. Wood ash and oyster shells also naturally make acidic soil more neutral. Test kits are available at most garden centers to see if your soil is high or low in pH.","How to Plant Shade Gardens\nimage by S.F. Heron - All images\nHow to Plant Shade Gardens\nEvaluate which type of shade exists in your proposed garden space. Shade comes in a few varieties and deciding which type will help your plants thrive. Dappled shades feature sunlight that peeks through tree branches and leaves. Partial shade receives roughly 2 to 6 hours of sunlight each day. Full shade allows no direct sunlight.\nChoose plants to suit the location. Visit the local garden center and check out the plants located inside the greenhouse. Plants inside greenhouses tend to require less light than those situated in outdoor areas of the nursery. Examine the tags to determine which plants will suit your type of shade. Hardy perennial choices include astilbe (flowering variety), hosta and ferns for beautiful foliage. Annuals such as caladium and coleus offer beautiful intricate leaves to add color to the shade garden. Flowering annuals that thrive in partial shade include impatiens and begonias.\nPrepare the garden bed by turning over the top 12 inches of soil with a shovel or rototiller.\nAdd soil conditioner, peat or compost to the top layer of soil and turn the garden over again. You'll be adding important nutrients into the soil before planting any shade-loving plants. You can also remove this loosened top layer of dirt, place a 2- to 3-inch layer of compost or peat and return the dirt to the garden. Mix in additional soil conditioner, peat moss or compost as you replace it in the garden.\nSpace your plants with plenty of room to allow air to flow freely around them. If you're planting perennials, plan for these plants to increase in size roughly by half each year. To avoid having to split and transplant, allow growing space around each plant. Place annuals together in groups of three for splashes of color in your shade garden.\nDig holes roughly 1 1/2 times the size of the plant pot. Add soil conditioner or compost to the hole to encourage healthy root growth.\nHold the plastic temporary plant container in your hand for smaller plants and squeeze the plastic container. Place your hand across the top and flip the container upside down to free the plant. For larger gallon-size plant pots, tip the plastic container carefully onto the side and press down on the planter. Rotate the plant and continue pressing until you've done the entire plant pot. This loosens any rootbound soil inside the plant. You should be able to wiggle the plant right out of the pot. Use clippers to cut the plastic starter pot for plants with particularly stubborn roots.\nFree the roots by aiming a light spray of water from the hose on the exposed portion of the plant roots. You can also press your fingers into the soil surrounding the roots of a plant to loosen the roots. This encourages the roots to grow outward after being confined in a starter pot.\nPlace the plants into their appropriately sized holes and fill in around each plant. Press the soil firmly and continue adding soil around the base of the plant until level with the rest of the garden.\nApply a 3- to 4-inch layer of mulch to the garden surface and water thoroughly. Water the garden regularly for the first months to help the plants become established."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:dd4df76c-f5b4-45df-92df-ff492fd6780e>","<urn:uuid:30119811-7c44-4677-a2f9-ec8b8b3b86ef>"],"error":null}
{"question":"What are the historical burial restrictions and modern maintenance practices in pioneer cemeteries?","answer":"In the past, burial restrictions varied significantly. For example, the IOOF Cemetery initially only allowed members of the Odd Fellows Lodge or their immediate families to be buried there, though it later opened to other fraternity members and eventually the general public. In pioneer Southern cemeteries, maintenance involved specific traditions like the graveyard workday, where communities would gather annually to scrape grass, mound graves, place markers, and make repairs. This was also a social event that included meals, sermons, or singing. However, these communal maintenance practices have declined over time, particularly in the transitional phase of Southern folk cemeteries.","context":["Lake City Cemeteries\nThe City Cemetery, sometimes known as the Lower or Old Cemetery, came into existence in 1876. It is located on Cemetery Hill to the north of Lake City and just east of Colorado Highway 149 as it enters the town.\nIn February 1876, it was decided that a committee be appointed to select a site for a city cemetery.The Silver World newspaper observed: “ [The City Fathers] hope there will be no undue rivalry as to who shall be the first occupant, but if a certain party does not refrain from standing and reading copy on the case, this office will enjoy the honor of furnishing the first denizen of the ‘City of Dead.’”\nThe City Cemetery was started on several acres of hilly, pine covered ground north of Lake City which had originally been patented for a ranch. The land was never publicly owned and passed through several private ownerships, most recently the Carol White estate, before Hinsdale County acquired it in 1985.\nThere were apparently never any formal organization or records for the City Cemetery and burials took place on a haphazard basis and to this day, the majority of graves are unmarked.\nThe City Cemetery, as a common burial ground, reflects the widest spectrum of life in the late 19th century. All classes and professions of the people ended up on this quite hillside; the butcher, the baker, the Bluff Street prostitute, laundrymen and teachers, attorneys, leaders of the church and society, actors, gamblers and musicians, and of course, the ever present miners and prospectors. Immigrant burials make up a large portion of the City Cemetery, and there are identifiable sections for Catholics, Italians and paupers. “Paupers Row”, a stark row of side by side unmarked graves, runs the length of the entrance gate. There is also an area which was apparently reserved for gamblers and prostitutes. The marker of Benjamin House, a Bluff Street faro dealer, who died in October 1876, is the oldest tombstone in the City Cemetery.\nInternational Order of Odd Fellows Cemetery\nSilver Star Lodge, No. 27, International Order of Odd Fellows (IOOF), decided to create their own private burial ground on the northern outskirts of Lake City in 1877. The first burial to take place in this new cemetery, which is located on Cemetery Jill approximately a quarter mile west of Colorado Highway 149, was Andrew T. Hopkins, who died in April 1877.\nJohn F. Dodds completed the first formal survey of the IOOF Cemetery on November 21, 1877. The original cemetery consisted of 3.82 acres on the Peter A. Simmons ranch. Burial in the IOOF Cemetery was intended to be very restrictive. In the early years of its existence, only members of the Odd Fellows Lodge or their immediate families were permitted to purchase burial plots. Restrictions had loosened somewhat by 1900, however, and members of other fraternities and sororities were allowed to be buried in the cemetery. The cemetery was later opened to the general public and finally taken over by Hinsdale County after the Odd Fellows Lodge was dissolved. The IOOF Cemetery remains the principal burial location for the area at this time.\nThere are subtle differences between the City and IOOF Cemeteries. Due to its burial restrictions, the IOOF Cemetery contains fewer burials and it reflects a greater percentage of Lake City’s business and professional people. The memorials and fences of the IOOF Cemetery are in general more elaborate and costly than those in the City Cemetery. Where unmarked graves and wooden markers predominate in the City burial ground, the IOOF Cemetery features more stylish and permanent markers. Immigrant burials and a majority of the miners and prospectors are noticeably absent from the IOOF Cemetery.","Article from 2006 originally published on Southern Graves site. Some newer images added.\nSouth is the Upland South folk cemetery.\" - D. Gregory Jeane\nSome Southern cemetery traditions include wife-to-the-left burials, bordered family plots, and feet-to-the-east interments. These practices can be fairly common in today's Southern cemeteries, but there are other traditions that may not be.\nThe southern folk cemetery is characterized by hilltop location, scraped ground, mounded graves, east-west grave orientation, creative grave markers and decorations using materials readily available (not commercially produced), certain species of vegetation, the use of grave shelters, and the obvious devotion to God and/or parents and family with the graveyard workdays and monument dedications. Some of these characteristics can certainly be found in other parts of the United States, even the world. It is the compilation of most or all of these traits within one cemetery that allows it to be called folk. Since the regional distribution of this type of cemetery coincides with the southern culture, this cemetery is known as Southern Folk.\nThe folk cemetery was introduced in the late eighteenth or early nineteenth century, and though this was a relatively short time ago, there are three distinct phases or models of the Southern Folk Cemetery that have progressed over time: pioneer, transitional, and modern. The diverse, yet true to tradition aspects of each of these provides a glimpse into the changing attitudes toward death.\nThe pioneer model is the folk cemetery's most historic, likely to be found in remote rural burial grounds. Though thought to be around as early as the late 1700s, the pioneer southern folk cemetery was certainly well established by the 1830s. Southern folk cemeteries in the eastern Georgia Piedmont were established 1810 - 1820.\nAn essential requirement of the new frontier communities was to designate space for formal burial of their dead. This sometimes even came before the building of a church. However, the British tradition of a churchyard cemetery did cross the pond with the settlers of the southern United States, so both examples of the pioneer type folk cemetery can be found.\nThe pioneer folk cemetery was small, though it was not necessarily an immediate family burial ground as linked to the Southern plantation. Extended family ties that emerged when pioneer families intermarried were just as important. The most distinctive trait of the pioneer folk cemetery of the south was the ground scraped clean of grass. The graves were laid out in an east-west direction, neatly aligned and mounded with dirt. This cleared patch of land, free of grass and weeds, was often found on a hilltop. It would have been scraped a couple times of year, possibly resulting in a hardened surface. The clean cemetery showed honor and respect for the ancestors buried there.\n>>> Here is an old (somewhat shaky) video I took back in 2008 of a row of burials in a middle Georgia cemetery. The surrounding ground is not perfectly scraped, but you can get the idea. <<<\nTerry Jordan describes a southern cemetery with a scraped ground in his book, Texas Graveyards, A Cultural Legacy: \"The first glimpse of such a cemetery truly startles the unsuspecting visitor. Throughout the burial ground, the natural grasses and weeds have been laboriously chopped or \"scraped\" away, revealing an expanse of red-orange East Texas soil or somber black prairie earth, sometimes decorated with raked patterns. At each grave, this dirt is heaped in an elongated mound, oriented on an east-west and anchored by a head and foot stone.\"\nGrave mounds served several purposes, such as marking the grave and compensating for the settling of the grave.\nIf a grave marker is found in a pioneer model southern folk cemetery, this is where the art of \"making do\" is seen. Sometimes, a wooden stake is all that is found at either end of a grave mound. The field research of D. Gregory Jeane in west Louisiana and east Texas resulted in findings of small, slightly curved, clay turpentine cups stacked to form a marker. In other cemeteries, these same cups were used to border an individual grave or family plot. I have seen the same instances with rocks. What is missing most often from a pioneer southern folk cemetery is commercially produced gravestones or granite or marble.\nA common decorating practice in southern folk cemeteries still seen today is the use of shells. Conch shells, among others, are frequently seen. The shells are used to varying degrees, from a single one at the head of the grave to a line of them down the center of the grave or as a border. Sometimes the entire grave will be covered with shells.\nThe dominant species of vegetation seen in the pioneer southern folk cemetery is the eastern red cedar tree. It is found in connection with the southern burial grounds so often that it is known as the \"cemetery tree.\" Species of pine were also used, probably because of the evergreen characteristic.\nA grave shelter, or grave house, may also be found in a pioneer model southern folk cemetery. D. Gregory Jeane describes it as \"small, rectangular, gable-ended structure placed over the grave...of simple construction -- four corner posts, often surrounded by picket fencing, supporting a shallow, gable-ended roof.\"\nHonoring the family and ancestors buried in the cemetery was a common practice of the community keeping up these pioneer burial grounds. The graveyard workday was an annual event, in late summer of early fall, where all members of the community gathered to pay respect to the memory of the deceased. It was often an all day affair that went a long way for the maintenance of the cemetery. Grass was scraped, graves were mounded, grave markers were placed or replaced if desired, decorations were placed, repairs were made, and trash was removed. Furthermore, this was a social event. There might've been a noonday meal, a sermon, or singing.\nThe transitional phase of the folk cemetery may date from the mid to late nineteenth century, and it is seen across the South even today. It is characterized by the designation of family plots, a mixture of scraped plots and grassy areas, the lack of creative markings or decorations, the lack of grave shelters, and most sadly, the eventual decline or total lack of the gathering of the community to collectively honor and respect those interred.\nThe transitional phase also marked the decline of mounded graves and the increase of commercially produced grave markers, or tombstones. \"Making do\" was still seen with the bordering of family plots. A variety of materials were used: rocks, concrete, shells, wood, or whatever was available.\nThe transitional model southern folk cemetery also introduced different varieties of vegetation such as magnolia trees and crape myrtle. The cedar and evergreen still remained. Flowering shrubs such as roses, azaleas, and forsythia were also added.\nThough shells continued to be used, decorations did somewhat change with the transitional southern folk cemetery. By the 1950s, the artificial flower became dominant. With the increase of commercially produced gravestones came the use of portraits.\nThe South remained largely rural until after World War II. The urban areas and cemeteries that were around became influenced by the Victorian school of thought about death at an early date, and this began to seep into the rural graveyards. The influence showed itself with the designation of family plots and the increase of elaborate grave markers. This time and influence gave way to the modern model southern folk cemetery.\nAfter World War II, the South began to experience a cultural change, albeit at a leisurely \"southern\" pace. It was mainly urbanization, and it was called \"progress.\" Southern burial customs did not escape the pressure of urban cemetery organizations to modernize, nor the pressure to be fashionable. This was even felt in the still rural areas. Therefore, urban memorial gardens and perpetual care mortuary complexes entered the region. Amazingly, this pressure and \"progress\" has not yet taken over the South, so the cultural shift is still today an ongoing process.\nSome memorial gardens still show aspects of the traditional southern folk cemetery with bordered, and maybe scraped, family burial plots. Largely, though, more graves have standardized, commercially produced markers, and epitaphs are disappearing. Plastic flowers abound and are seasonally predictable.\nThere is much debate over the origin of the traits associated with the southern folk cemetery. Some say African American and Native American influences shaped the cemeteries. However, I agree with D. Gregory Jeane when he writes, \"It would appear more logical that the pioneers arriving on the Southern frontier already had a system acceptable to the group for choosing and properly maintaining the group's sacred ground. The survival of the pioneer trait complex into the twentieth [and twenty-first] century would suggest something about its antiquity as well as its resistance to wholesale alteration...Thus, it is to Europe that one must look for the basic cemetery traditions that diffused with the settlers who fanned out across the Southern frontier.\"\nThe research of Terry Jordan indicates a European origin for the folk cemetery traits of the mounding of graves, the use of shells, the preference for flowers, the use of gravestones, and the use of cedars and evergreens.\nThe basis of feet-to-the-east burials can be linked to sun worship cults found in Europe at the advent of Christianity. Evidence of this might be the Christian Sabbath day of Sunday. I've always been told it is so the interred can \"see the sun rise.\" It could be as simple as that.\nHusband-to-the-right (or wife-to-the-left) is a traditional position of married couples in the southern folk cemetery. This apparently derives from a British Christian belief that Eve was created from the left side of Adam. Of course, this is not a strict rule. About 25% of the time, I see the opposite in an otherwise traditional-looking southern cemetery.\nThe trait of bordered, or somehow defined family plots might trace back to the days when many cemeteries in the South were private graveyards on family land.\nAnother tradition associated with the southern folk cemetery not yet mentioned is a fence enclosure, though I have seen many, many without one. More often, I have seen certain family plots within a cemetery have the enclosure.\nA ceremonial entranceway known as a lichgate, or corpse gate, can be found in many southern cemeteries, as well. Even those that do not have entire fence enclosures may have an arched gateway. A lichgate is defined as a \"roofed gateway to a churchyard used originally as a resting place for a bier (a stand on which a corpse or coffin is placed) before burial. This may symbolize the crossing from life to death. In the Georgia coastal region, a funeral procession may stop outside the gate while the leader asks the dead for permission to enter.\nAll three models of the southern folk cemetery can be found scattered about today. In my experience and regions of study, the transitional phase is dominant. Though I must admit, one can more and more often see the modern model taking root in our southern cities.\n(1) Personal knowledge of Stephanie Lincecum.\n(2) The Upland South Folk Cemetery Complex: Some Suggestions of Origin, D. Gregory Jeane. (Part of) Cemeteries & Gravemarkers, Voices of American Culture, Richard E. Meyer, editor, 1989.\n(3) Texas Graveyards, A Cultural Legacy, Terry G. Jordan, 1982.\n(4) \"Elements Found in Texas Folk Cemeteries,\" Website [as of December 2016, link no longer available].\nDisclosure: The products above are affiliate links, which means I may receive a very small commission if you click a link and buy something. This helps to support my research projects and blogging activities, and also makes my two dogs' tails wag. Hopefully, the purchase benefits you, too! The price you pay will be no different than if you arrived at the same destination through any other link. My opinions are my own, to be sure. If I link to a product and say I like it -- I truly like it! Thanks for reading, following, and supporting this Southern Graves blog."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:77dcdb3b-c6ec-48b5-9661-5156c6086222>","<urn:uuid:c0065e5f-0ec8-4f55-8137-de2c12963295>"],"error":null}
{"question":"How do water supply forecasts in California account for both natural and operational factors, and what role does hydrogen storage play in managing renewable energy fluctuations?","answer":"Water supply forecasts in California rely on multiple natural and operational data sources, including river indices, snowpack measurements, groundwater data, and reservoir storage summaries. While these forecasts are well-informed, they must also consider man-made factors such as water infrastructure, regulations, and operations. However, these forecasts often don't account for potential operational adjustments or provide supply enhancement scenarios that could affect water availability. Similarly, in the renewable energy sector, hydrogen storage has emerged as a solution for managing energy fluctuations. Through Power-to-Gas technology, surplus renewable energy can be converted into hydrogen via electrolysis and stored in massive amounts over long periods. This stored hydrogen can then be injected into gas networks or used for fuel cell vehicles, providing a more comprehensive solution than traditional battery storage for managing variable renewable energy supply.","context":["Every year at this time as the new water year unfolds, predictions and warnings about California’s upcoming water supplies begin emerging each one trying to project the likely status of water availability in the upcoming months. To facilitate such predictions a rich bevy of resource data are available. These databases are shared among the various federal, State, and local agencies as well as research and academic institutions, all of which collectively help to refine and tailor these projections. Despite their coverage and seeming quality, we should not lose sight of the fact that these forecasts still remain only best guesses. Well informed guesses to be sure but guesses all the same.\nIf we step back and ask ourselves what is it that we are actually trying to predict? Several answers become apparent; a real-time State water balance, deliverable raw water; or finished, treated water. Alternatively, are we more interested in simply assigning a water year classification (e.g., critically dry, below normal, drought, wet, etc.) in deference to how our regulations are controlled? I believe that we are seeking a combination of all four. Ultimately, we want (and need) to know how much potential water will be available for beneficial use by the State’s water users.\nTo get that answer, California water planers are fortunate in that there is no shortage of readily available data. These are wide-ranging and include mainstem basin river indices, sub-basin station indices (e.g., 8-station, 5-station, Tulare Precipitation Index), groundwater data (e.g., CASGEM), reservoir storage summaries, river temperatures, snowpack SWE (e.g., California Cooperative Snow Surveys), etc. These data are dutifully collected and enjoy good spatial and temporal acuity, enough to provide regulators and water managers with a depiction of current hydrologic conditions and forecasted changes as determined by the California Nevada River Forecast Center and National Weather Service.\nThe California-Nevada drought outlook is based largely on the U.S. Drought Monitor, relying on precipitation and temperature (e.g., Palmer Drought Severity Index, the Standardized Precipitation Index, and other climatological inputs); the Keech-Byram Drought Index for fire, satellite-based assessments of vegetation health, crop bulletins, various indicators of soil moisture, El Niño status, hydrologic data such as the Surface Water Supply Index, and not to be overlooked, the expertise of many specialists like the State Climatologist or the Lead Forecaster for the NWS.\nSince informed projections are best made when ALL factors are considered, various man-made factors must also be considered. These, after all, have as much an effect on whether deliverable raw water becomes available or not. Such man-made factors typically include water infrastructure, regulations, and operations. Here is where operational facets such as reservoir capacity, inflows and releases, federal/State exports, pumping and releases between the DMC and California Aqueduct, Delta outflow indices, water accounting, etc. would come into play. The State’s water resource status at any given time depends on how operations, regulations, and water accounting is being managed.\nIf having access to, and consideration of ALL influential factors is essential in making legitimate claims of water status, we must ask ourselves whether we do this? I conclude that despite having appropriate information available, the data are not consistently used in the decision process thus, water shortage or drought declarations are being made on incomplete information.\nWe do not, for example, provide a suite of supply enhancement scenarios based on operational adjustments. We do not say, “Precipitation to date is currently generating X reservoir inflow, however, despite existing flood control requirements and the obligation to spill, we will be encroaching into the flood pool by Y amount based on our assessment of water storage need and low precipitation forecasts...” Going further, we do not admit, “…without refill based on historic averages, continued depletion will result in water surface levels moving into the conservation pool zone thus likely resulting in a drought declaration by the end of April…”. More importantly, we do not offer any potential alternatives. We don’t propose for example, “… if we vary from the rule curve requirement and encroach by X amount, the percent likelihood of avoiding a 25% curtailment to all north of Delta M&I contractors would increase by Y…”. In short, we do not offer the public any “if-then” options. As a result, the public has no idea as to how flexible our operational choices really are.\nDo we have flexibility in our various flood control rule curves? Of course, we do, particularly since almost all of the existing rule curves are outdated and few, if any have been revised to take into account a shifting winter/spring hydrologic regime due to a warming climate. Reservoir operators, if they chose, could operate around existing rule curves and potentially avoid the loss of significant quantities of water. In fact, to prove this point, we documented “spills” of over 600,000 AF from Folsom Reservoir during a recent January through May period only to have American River deliveries cutback later that same summer. Clearly, this was not a good example of effective reservoir management. But it happens year after year.\nComing back full circle, are our water projections reliable? I think most would agree that they are dependable, but the story does not end there. The public should know that, but for a few operational adjustments, the water status in any given year could be very different if reservoir/water operators were allowed to pursue other options after considering all risks. After all, despite what many think, droughts are not all about Mother Nature, there’s much more to the story…\nPlease note: The views in this commentary are those of the author’s and should not be attributed to Maven’s Notebook.\nAbout the Author\nRobert Shibatani, a physical hydrologist with over 35-years combined academic, legal, consulting and water advisory counsel, is an international expert witness on reservoir-operations and flood damage in the EU, UK, Australia and U.S., specifically as they pertain to major hurricanes. He is the Managing Partner for The SHIBATANI GROUP International and resides in Sacramento, California. email@example.com","On a breezy, sunny day in May 2016, something unusual happened in Germany. So much renewable energy was generated that electricity prices went negative, meaning that customers were effectively paid to use it. Unusual but not unique. For a few hours in March 2017, solar generation in California accounted for almost 40% of net grid power produced, reflecting a 50% growth in the state’s utility-scale solar photovoltaic installed capacity in 2016. The result of which, as occurred in Germany, was that power prices were driven into the negative, an event that continues to occur periodically in the state.\nNegative electricity prices sound like a nice problem to have. This might be true if they were only indicative of the impressive improvement in the development, cost reduction and efficiency increase in renewable energy production. However, this phenomenon rather illustrates the intermittency and somewhat unpredictable nature of wind and solar power: supply fluctuates, surfeits cannot currently be stored to any great extent, and conventional grid balancing methods are not designed for power generation fluctuations caused by clouds or changes in wind speed.\nThe growth in demand for electricity, the need to shift to low carbon energy sources, the geographically widespread nature of renewable energies, along with the development of increasingly more efficient technologies, are all leading to an important increase in the use of renewable energy.\nThis increase is happening worldwide at a pace that far exceeds many projections. According to the International Energy Agency, in 2012 around 13.2% of the world’s total primary energy supply was derived from renewable sources. While in terms of electricity, in 2013 renewables provided almost 22% of global production and the Agency estimates that this share will reach at least 26% in 2020. Moreover, certain countries are making particularly huge strides forward: according to World Bank data the use of renewable energies in 2014 as a percentage of total final energy consumption was 57% in Norway, 50% in Sweden, 36% in India, 31% in New Zealand… to mention just a few of the high achievers.\nThis trend must continue if the world hopes to mitigate climate change and reduce pollution. In fact, many experts consider tha\nt we have already moved beyond a turning point. The transition from fossil fuels to renewables is now irreversible. However, renewable energy risks becoming a victim of its own success: without economically viable storage options, renewable energy integration has been supported by fast-reacting fossil-based technologies, which act as back-up capacity to compensate for supply variability. But as the push for clean energy continues and renewable resources account for an increasing proportion of generated energy, there is simultaneously more variability and less fossil fuel buffering to absorb supply chain shocks.\nThe integration of renewable energy presents a challenge to one of the fundamental imperatives of the power industry: the need to match electricity supply and demand. Why so?\nGrid frequency fluctuates continuously, determined by the real time balance of demand and generation.\nElectricity networks have a utility frequency of 50 Hz or 60 Hz, because this corresponds to an efficient rotation speed for the steam turbine engines that power most generators (3,000 RPM for 50 Hz, 3,600 RPM for 60 Hz). This frequency must be tightly controlled to ensure the safe operation of the grid and also because appliances are specifically designed to operate at one of these frequencies.\nHowever, grid frequency fluctuates continuously, determined by the real time balance of demand and generation. If demand is greater than generation, the frequency falls, if generation is greater than demand, the frequency rises. The network operator has to compensate for such changes by requesting more or less generation in the hour leading up to real time, to keep the frequency in the acceptable range.\nTraditionally, power systems have been based on fossil or nuclear-powered plants, which have rotating generators that can rotate slightly slower or faster to compensate for the immediate imbalance between power supply and demand, allowing the required frequency to be maintained.\nAnd that’s where the integration of renewables raises issues. Renewable energy sources, notably wind turbines and photovoltaic units, do not have the rotational inertia of conventional generators. This has implications on grid stability in terms of frequency control and ensuring the stable operation of the power grid, because renewables are displacing conventional generators with their rotating machinery.\nBut also, solar and wind generation experience intermittency in terms of non-controllable variability and the partial unpredictability of the weather. This fluctuation in power output makes it more difficult to balance supply and demand on an instantaneous basis.\nMany solutions to the variability of renewable energy and its successful integration into the grid are already in use to varying extents around the world. A first step is minimizing challenges and costs from the outset by deploying a wide range of renewable energies where possible. Some sources — such as hydroelectric and concentrated solar power — offer greater control of output than others.\nThis solution can provide a higher degree of control and thus eases the challenge of balancing. It is not, however, always practical, as many renewables are geographically specific. Although this can be addressed by long-distance transmission and larger and more flexible grids with connections that cross international boundaries. The principle is that the larger area a network covers, the more chance there is that renewably generated power that is superfluous in one location will be able to supply a demand that is present elsewhere.\nAnother option is storage. The storage of mass-produced electricity has been around for many years in the form of pumped storage hydropower, which relies on gravity to capture off-peak power, releasing it at periods of high demand. This accounts for 99% of bulk power storage capacity worldwide, however its application is limited to very specific locations.\n“If you want days, months, or even years of energy storage, and in massive volumes, then you can’t do it with batteries.”\nChemical storage in the form of batteries also has potential to support the energy transition. Large batteries are already in use in industry but their implementation in managing variation in grid systems is problematic.\nDr Graham Cooley, CEO at ITM Power, a manufacturer of integrated hydrogen energy systems commented: “When most people think of energy storage, they think of batteries. And if you want one or two hours of energy storage, batteries are good. But if you want days, months, or even years of energy storage, and in massive volumes, then you can’t do it with batteries.”\nFactors such as lifespan, cycle efficiency, power leakage and production costs all need to be addressed before batteries can be considered as a viable option for grid balancing. All this isn’t sounding too promising so far. However, there is a technology that is already being implemented and has proven itself ideally suited to empowering the energy transition because it offers much more than just storage. And this solution is hydrogen.\nHydrogen can store excess renewably generated power, and do so reliably, in massive amounts and over long periods of time. It can also be used to power fuel cell vehicles which produce zero local emissions and provide energy for the industry. It can be transported in high energy density forms either in a gas or liquid, and injected into gas networks to supplement or replace natural gas. So how does it work?\n“An electrolyser can be turned off and on very rapidly”\nThe process, known as Power-to-Gas (P2G), involves converting surplus energy into hydrogen gas by rapid response electrolysis and its subsequent injection into the gas distribution network. Graham Cooley explained: “P2G is about making hydrogen using renewable power. You use a device called an electrolyser that splits water into hydrogen and oxygen when an electric current is passed through. You use the hydrogen either in the gas grid or for refueling vehicles. And because an electrolyser can be turned off and on very rapidly, it provides very efficient grid balancing.”\n“An electrolyser receives signals from the grid based on frequency. When the network is overloaded and the frequency starts to fall, the electrolyser is switched off. When the network is under loaded, or has an excess of renewable power, it receives a signal to turn on. Our electrolysis equipment can be turned on and off in less than a second and can be bid into primary grid balancing (Enhanced Frequency Response or Frequency Control Demand Management).\n“When switched on, the hydrogen produced goes straight into the gas grid. In this way, when we are balancing against rising frequency, an electrolyser can be used to absorb an excess of renewable power. The gas grid in the UK for example is three times the size of the power grid: there’s about 350 terawatt hours of energy flowing through the electricity grid every year, and about 1,000 terawatt hours in the gas grid. The primary difference between the two energy networks is that electricity has no energy storage, whereas there’s a huge amount of energy storage in the gas grid.”\nIt has been shown that hydrogen-enriched gas, containing up to 20% hydrogen, has no adverse impact on gas appliances. Surplus renewable power to produce hydrogen by electrolysis can also be combined with CO2, for example from biogas production, to form synthetic natural gas — which is the exact same molecule as the fossil natural gas that is used today — that can also be injected into gas networks.\nThen there’s the role of renewably-produced hydrogen in carbon-free transport. “The same principle of using electrolysers for grid balancing can be used to generate very low cost hydrogen for fuel cell electric vehicles,” said Graham Cooley. “Electrolysers can be installed directly at refueling stations so that hydrogen is made on site, meaning there’s zero carbon footprint involved in transport. The electrolyser is turned on and off according to the grid company’s balancing requirements, but here the hydrogen is stored in tanks and then deployed to vehicles. This means the hydrogen has the lowest possible carbon footprint and is absolutely clean: the water has never seen any carbon molecules and does not need purifying for use in vehicles.”\n“Electrolysers can be installed directly at refueling stations.”\nWhile this sounds promising, a number of critics have claimed that hydrogen fuel cell vehicles are a dead end technology. Prof. Dr. Christian Mohrdieck, Daimler’s Fuel Cell Director, addressed this point: “A very popular argument in the public discussion is that battery electric vehicles are more efficient than hydrogen. This is true because the conversion of hydrogen is an extra step that is not involved in battery electric vehicles. However, this is not taking the whole picture into account.\n“If we want to move to renewable energy then we have to consider all aspects”\n“We want to move away from fossil fuels towards renewable energy, and this means we also need to store electricity. So the big challenge is how can we store huge amounts of electricity over a long time, even from one season to the next. Very few technologies can do this, but hydrogen is one of them. Batteries cannot.” Indeed, batteries alone would not be able to do the work because the amount of energy to be stored for seasonal storage is too high. The amount of batteries needed would be significant — material, size and cost-wise.\n“Once you factor in the need for hydrogen in the storage of renewable electricity, then hydrogen and battery electric vehicles become very close in terms of efficiency. This is a complex picture, but if we want to move to renewable energy then we have to consider all aspects”, Prof. Dr. Christian Mohrdieck adds.\nIn the past, the use of clean, electrolysis-formed hydrogen has been hampered by relatively high production costs. The capital price and running costs of electrolysers prevented this solution from being sustainable. This major issue is on the verge of being solved, as production prices have decreased steadily year after year. Today, hydrogen production cost depends mainly on electricity cost, operational hours and CAPEX (capital expenditure). Thanks to increasingly lower renewable costs and economies of scale, clean hydrogen will soon be price-competitive. In regions that combine medium electricity prices (between 30 and 60 dollars per MWh) and a medium utilization factor, the cost of producing hydrogen can be around 4–5 dollars per kilogram.\nHuman societies now depend on a 24/7 energy supply. But neither the sun shines nor the wind blows all day, every day. And our electricity system is designed to function through the precise matching of supply and demand.\nHydrogen is one of the solutions to this intermittency problem. It can be produced from renewable energy through electrolysis and used for grid balancing, energy storage, heating, for refueling fuel cell vehicles and as a source of hydrogen for industry. If the world truly intends to avoid climatic catastrophe, then expanding the use of renewable energy is essential, and hydrogen could well be the key to unlock its full potential."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:a0327262-1651-46f5-836e-9a49509918ff>","<urn:uuid:9a216169-7000-4b68-8864-7e5eea49e9bb>"],"error":null}
{"question":"How do the negotiation capabilities of Facebook's chatbots compare to Pluribus's poker playing abilities in terms of developing their own unexpected strategies?","answer":"Both AI systems developed unexpected strategies, but in different ways. Facebook's chatbots invented their own shorthand language to negotiate more efficiently, which led to the experiment being shut down. Pluribus, on the other hand, successfully developed unconventional poker strategies like 'donk betting' - a traditionally weak move that the AI found ways to use effectively - and making unusually large bets that human players typically avoid. While Facebook's chatbots' creative behavior was seen as a problem, Pluribus's unexpected strategies contributed to its success, allowing it to consistently win against professional players at a rate of $1000 per hour.","context":["In an effort to increase skills in its artificial intelligence capabilities, Facebook developed a project to teach their chatbots to negotiate. It didn’t go as planned. The task required two chatbots to negotiate a division of items between the two of them. The items were assigned varying values for each bot, representing how much they cared about each thing, but the only way for them to figure out what the other bot valued was through language, as humans do: If you ask about an item, you must care about it. While it was impossible for both bots to end up with the best deal, they were rewarded for completing the negotiation; unlike between humans, they couldn’t just walk away.\nAlthough these robo-negotiators learned to pretend interest in a valueless item in order to later concede it – something humans do all the time, and an important negotiating tactic – Facebook shut the experiment down when the bots invented their own shorthand language to haggle with each other, and not straightforward English. AI experiments routinely run into language problems.\n“Human-compatible AI” by Stuart Russell (Data Driven Paris)Firstmark\nNot that humans don’t glitch too. When it comes to data analysis and pattern recognition, for instance in contracts, AI is already taking on the grunt work for negotiators of all stripe. JP Morgan replaced 360,000 legal hours with software that analyzes loan documents. This kind of processing is a robot’s field of dreams. But contingency thinking is a different game altogether.\nDespite sensational victories over chess and Go masters, hardware can’t yet replace brainware when it comes to responding quickly and adjusting to surprises outside the parameters of its programming. As Alibaba founder Jack Ma says, humans are better than AI when it comes to doing things that take heart.\nAnd negotiations take heart. Emotions are an important factor in negotiating. Because AI can easily miss microexpressions that convey crucial nuance, it will be a while before robots have a seat at the table. Negotiation takes psychological skill and emotional intelligence. Empathy helps all parties get to “yes.” According to hostage negotiator Chris Voss, demonstrating that you understand the other party’s perspective and showing respect are the most important aspects of getting what you want from a negotiation. Ultimately, negotiations hinge not just on what you say, but how you say it. And, as Facebook found in its experiment (and continues to find out), AI is still working out its language issues.\nNegotiations aren’t just about price, they’re about values. A good haggler is flexible, and able to throw in curveballs to shake up talks, making moves often based on instinct. The basics of negotiation are still very much in the human domain.\nThe Art of NegotiationSimon & Schuster\nAI is just not there yet, although that might change in the future. Researchers Noam Brown and Tuomas Sandholm designed a program that learns from human mistakes, so it can get better at predicting them. Their program, Pluribus, plays Texas Hold ‘Em poker with no limits against five other players. Like the Facebook project, the point is to better understand the dynamics of multi-party negotiations.\nDesigners taught Pluribus the strategies of top experts, but the program invented some tricks of its own, including unexpectedly large bets early in games. In other words, it learned to bluff. AI is continuously improving, but for the foreseeable future, the best deals are made through relationship-building, diplomacy and being able to read the room.","Computer scientists have developed a card-playing bot, called Pluribus, capable of defeating some of the world’s best players at six-person no-limit Texas hold’em poker, in what’s considered an important breakthrough in artificial intelligence.\nTwo years ago, a research team from Carnegie Mellon University developed a similar poker-playing system, called Libratus, which consistently defeated the world’s best players at one-on-one Heads-Up, No-Limit Texas Hold’em poker. The creators of Libratus, Tuomas Sandholm and Noam Brown, have now upped the stakes, unveiling a new system capable of playing six-player no-limit Texas hold’em poker, a wildly popular version of the game.\nIn a series of contests, Pluribus handedly defeated its professional human opponents, at a level the researchers described as “superhuman.” When pitted against professional human opponents with real money involved, Pluribus managed to collect winnings at an astounding rate of $US1000 ($1433) per hour. Details of this achievement were published today in Science.\nOver the past several decades, AI researchers have had a lot of success developing machines capable of playing perfect-information, two-player, zero-sum games. That is, games involving head-to-head matches in which both players have complete knowledge of what’s happening in the game (e.g. chess players can see all the pieces on the board), and in which one player wins and the other loses.\nBy contrast, poker is an information-incomplete game, in which the players can’t be certain of which cards their opponents are holding and which ones are still in the deck. Other elements, like betting and bluffing, add to the game’s complexity and unpredictability. Add multiple players to the mix, and the complexity rises further still.\nFor AI researchers, poker presents a better model of the real world. Rarely in life do situations involve just one winner and one loser, or scenarios in which information is fully available. By improving an AI’s ability to deal with hidden information in multi-participant scenarios, computer scientists are dramatically expanding the domains in which AI can be used.\n“While I am not focused on any particular application, I do think this research can be applied to a wide variety of settings such as cybersecurity, fraud detection, combating adversarial behaviour, and even having a self-driving car navigate traffic,” Brown told Gizmodo.\nFor the new study, Brown and Sandholm subjected Pluribus to two challenging tests. They first pitted Pluribus against 13 different professional players, all of whom have earned more than $US1 million ($1.3 million) in poker winnings, in the six-player version of the game. The second test involved matches featuring two poker legends, Darren Elia and Chris “Jesus” Ferguson, each of whom was pitted against five identical copies of Pluribus.\nThe matches with five humans and Pluribus involved 10,000 hands played over 12 days. To incentivise the human players, a total of $US50,000 ($71,662) was distributed among the participants, Pluribus included. The games were blind in that none of the human players were told who they were playing, though each player had a consistent alias used throughout the competition.\nFor the tests involving a lone human and five Pluribuses, each player was given $US2000 ($2866) for participating and a bonus $US2000 ($2866) for playing better than their human cohort. Elia and Ferguson both played 5000 separate hands against their machine opponents.\nIn all scenarios, Pluribus registered wins with “statistical significance,” and to a degree the researchers referred to as “superhuman.”\n“We mean superhuman in the sense that it performs better than the best humans,” said Brown, who is completing his Ph.D. as a research scientist at Facebook AI. “The bot won by about five big blinds per hundred hands of poker (bb/100) when playing against five elite human professionals, which professionals consider to be a very high win rate. To beat elite professionals by that margin is considered a decisive win. It’s a bit tough to qualify this in a [simple] way... but one way to understand it is that if the bot were playing for real money, it would have won about $US1000 ($1433) per hour.”\nAnd that’s against some of the world’s best poker players. Adorning Pluribus with superhuman status certainly seems justified, and Roman Yampolskiy, a computer scientist at the University of Louisville who wasn’t involved with the new work, agrees.\n“The machine showed superhuman performance by defeating the best players in the world,” Yampolskiy told Gizmodo. “It obviously could defeat weaker players, meaning that it is superior to all humans making its performance unquestionably superhuman in this domain.”\nFor Yampolskiy, the achievement was significant because, “unlike chess or Go, the game of Poker has hidden information and the element of luck, meaning you can’t just outcompute humans, you have to outplay them,” he said. “Poker in particular has been an early sandbox for AI, and to show such a level of dominance in an unrestricted version of poker with many players has been a holy grail of research since the early days of AI.”\nTo create a system capable of proficiently playing six-player no-limit Texas hold’em poker, Brown and Sandholm employed a grab bag of strategies, including new algorithms the duo developed themselves.\nBefore the competition started, Pluribus developed its own “blueprint” strategy, which it did by playing poker with itself for eight straight days.\n“Pluribus does not use any human gameplay data to form its strategy,” explained Brown. “Instead, Pluribus first uses self-play, in which it plays against itself over trillions of hands to formulate a basic strategy. It starts by playing completely randomly. As it plays more and more hands against itself, its strategy gradually improves as it learns which actions lead to winning more money. This is all done offline before ever playing against humans.”\nArmed with its blueprint strategy, the competitions could begin. After the first bets were placed, Pluribus calculated several possible next moves for each opponent, in a manner similar to how machines play chess and Go. The difference here, however, is that Pluribus was not tasked to calculate the entire game, as that would be “computationally prohibitive,” as noted by the researchers.\n“In Pluribus, we used a new way of doing search that doesn’t have to search all the way to the end of the game,” said Brown. “Instead, it can stop after a few moves. This makes the search algorithm much more scalable. In particular, it allows us to reach superhuman performance while only training for the equivalent of less than $US150 ($215) on a cloud computing service, and playing in real time on just two CPUs.”\nEven with a limited look-ahead strategy, Pluribus was still able to dominate its human opponents.\nImportantly, Pluribus was also programmed to be unpredictable - a fundamental aspect of good poker gamesmanship. If Pluribus consistently bet tons of money when it figured it had the best hand, for example, its opponents would eventually catch on. To remedy this, the system was programmed to play in a “balanced” manner, employing a set of strategies, like bluffing, that prevented Pluribus’ opponents from picking up on its tendencies and habits.\nSome of the strategies used by Pluribus came as a surprise to the researchers, including an unorthodox strategy known as “donk betting,” which happens when a player matches the bet, but then starts the next round with a bet. Poker players consider donking a weak move with little strategic sense.\n“The conventional wisdom is that if you are going to call [match the bet] and then bet [during the next round], then you might as well raise instead because it gives you more opportunities to get more money into the pot,” explained Brown. “Donk betting is something that weak players tend to do, though elite professionals acknowledge that it could, in theory, be a good action if done correctly in the right situations.\nHowever, doing it correctly without opening up exploitable weaknesses is typically too complicated for humans, even elite human professionals, so most only rarely if ever do it. Pluribus has found ways to donk bet much more effectively in a way that cannot easily be exploited.”\nAlso, Pluribus often made much larger bets that human players typically avoid. Brown said this put Pluribus’ opponent into very difficult situations, which allowed the machine to make much more money with good hands than humans could.\nChris Ferguson, WSOP champion: Pluribus is a very hard opponent to play against. It’s really hard to pin him down on any kind of hand. He’s also very good at making thin value bets on the river. He’s very good at extracting value out of his good hands. So it’s been very hard playing against him. He’s really a very strong opponent.\nDarren Elias: Its major strength is its ability to use mixed strategies.That’s the same thing that humans try to do. It’s a matter of execution for humans - to do this in a perfectly random way and to do so consistently. Most people just can’t. The bot wasn’t just playing against some middle of the road pros. It was playing some of the best players in the world.”\nJason Les: I probably have more experience battling against best-in-class poker AI systems than any other poker professional in the world. I know all the spots to look for weaknesses, all the tricks to try to take advantage of a computer’s shortcomings. In this competition, the AI played a sound, game-theory optimal strategy that you really only see from top human professionals and, despite my best efforts, I was not successful in finding a way to exploit it. I would not want to play in a game of poker where this AI poker bot was at the table\nJimmy Chou: Whenever playing the bot, I feel like I pick up something new to incorporate into my game. As humans I think we tend to oversimplify the game for ourselves, making strategies easier to adopt and remember. The bot doesn’t take any of these short cuts and has an immensely complicated/balanced game tree for every decision.\nSean Ruane: In a game that will, more often than not, reward you when you exhibit mental discipline, focus, and consistency, and certainly punish you when you lack any of the three, competing for hours on end against an AI bot that obviously doesn’t have to worry about these shortcomings is a gruelling task. The technicalities and deep intricacies of the AI bot’s poker ability was remarkable, but what I underestimated was its most transparent strength - its relentless consistency.\n“Once again, AI managed to outperform humans without relying on any data from human play,” Yampolskiy told Gizmodo. “This means that machines can teach themselves to solve complex problems independently of human supervision.”\nYampolskiy wasn’t surprised by how well Pluribus performed, though he would have liked to have seen Pluribus play standard 10-player games, and without having to abide by betting restrictions (unlike its human opponents, Pluribus was not allowed to make bets above $US10,000 ($14,332)).\nWhat does surprise Yampolskiy, however, is that there are still some games in which computers are not superhuman in terms of their performance. As to where this type of AI could be applied in the future, Yampolskiy said similar techniques could be used “to outperform humans in negotiations, trading, and game-like competitions such as war strategy.”\nTo which he added, perhaps ominously: “Essentially, any skill which could be represented as a game-like situation can be dominated by superhuman AI.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:2af97f9d-9556-4827-a201-34179668fef1>","<urn:uuid:f20a6d5e-1220-4815-9858-982446cd95bd>"],"error":null}
{"question":"How do the timing considerations in corporate identity projects of the 1960s compare to those in modern ship energy efficiency design?","answer":"Both fields emphasize the crucial importance of timing, particularly in early phases, but approach it differently. In corporate identity projects of the 1960s, designers had to face new challenges without prior training, requiring time to develop new definitions, vocabularies, techniques, and systems, while explaining complex matters to clients and preparing detailed manuals for long-term continuity. In modern ship design, Deltamarin specifically emphasizes that the concept design phase is critical as the most important choices regarding ship energy efficiency are made during this early phase, though they face challenges of limited time and rough information. However, they've developed energy flow modeling and analysis tools to devise quick solutions that are both energy and cost-efficient.","context":["The young profession of graphic design reached maturity during the 20th century. Within that new professional world, the sphere of work called ‘corporate identity’ became a true metropolis. There are a few classic examples of this development, and all historical surveys mention the visionary approach of AEG, the German pioneers in the realm of electricity, founded back in 1886.\nAEG Berlin, conceptual pioneers\nThe concept of corporate identity can be claimed by Peter Behrens (1868–1940). Trained as a painter, he soon developed his skills as an architect and product designer. While director of the Düsseldorf School for Applied Arts, he was also active as a freelance packaging and publicity designer for AEG. In 1907 he joined the company as their aesthetic and architectural advisor. He built their turbine factory and designed numerous products at a new, high level of functionality and beauty. He designed a new, strong AEG logo and employed young talents who would soon become design stars: Walter Gropius, Ludwig Mies van der Rohe and Le Corbusier. The whole quality cluster of AEG architecture, packaging, publicity and product design supports the recognition of Behrens as the Godfather of Corporate Identity. His motivation was purely idealistic: he was truly in search of ‘Making Things Better’.\nOlivetti: visionary entrepreneurs\nIn 1855, Giuseppe Ravizza from Turin invented a ‘writing piano’: the first typewriter. Twenty years later he himself said that it was sheer madness to expect that people would ever write or sew with a machine... But in the US, inventors like Remington had followed Ravizza’s footsteps and ideas, and by the start of the 20th century there were factories named Remington, Royal, Underwood and many others producing typewriters by the thousands. Ravizza’s countryman Camillo Olivetti saw clearly the potential of this market, and invested three years in developing his first typewriter, assisted by 20 collaborators. He also sent his son Adriano and his chief engineer Burzio on a study trip to the States to spy on American production methods and theories of technology, economic efficiency and organization. But Adriano also discovered that focusing on profit maximization was not his sole personal ideal: the quality of life of his employees was as least as important! With Adriano in charge (1933–60) Olivetti embarked on a new ethical and social course. As production in the US and Germany, Italy’s major importing countries, was reduced dramatically and the lire devalued to such an extend that foreign companies stood no chance of competing, Adriano’s Olivetti flourished with an increased market share. He then concentrated on working conditions in new factories. In Ivrea, Olivetti built modern apartments, sports grounds and a hospital. He worked with architects that did not follow the fascist style, but instead created landmarks of modernism. The typewriter became a consumer product when portables came on the market. For product design and company communications, Adriano invited ‘artists’ of repute, such as the AGI members Walter Ballmer, Franco Bassi, Silvio Coppola, Bob Noorda, Albe Steiner and Milton Glaser. The Olivetti visual identity had its roots in idealism and led to a corporate culture, intriguing communications and advanced product design. Who could beat Olivetti?\nCCA, Chicago: Responsibility as a guideline\nMany important aspects of the Olivetti story that began in the early thirties, were continued and extended after WW2. Before that war, corporate identity as we know it now was still a new concept that had to be ‘invented’. Gradually, organizations started to develop the concept and produced their own interpretations. Another striking example of the late thirties was set by the Chicago-based Container Corporation of America. Founder P. Paepcke started an advertising campaign in 1937 that was much more institutional than promotional. CCA’s philosophy, put into simple words, was: ‘Design is a vital obligation of the management.’ In media such as Fortune, CCA explained its responsibility to good design and culture. Paepcke employed designers like Cassandre and the German refugees Toni Zepf and Herbert Bayer. CCA was in fact Bayer’s first major project in the USA. Their ads had great impact, especially in wartime, when recycling became an important issue. At the same time the ads and visual identity built up a positive image of the CCA. In 1950 the company started a series called ‘Great Ideas of Western Man’, with designers like Will Burtin and Gene Federico (AGI members, like Bayer and Cassandre). Herbert Bayer was also commissioned by CCA to design a World Geo-Graphics Atlas (1953), a very early environmental exercise.\nPost-war developments change the profession entirely\nA different, post-war economic structure began to take shape in the industrialized world of the late fifties. It was a smaller world, due to a transportation and communication explosion, but with bigger business owing to new markets and large-scale mergers. With this, new concepts in corporate communications took shape. FHK Henrion and Alan Parkin published their book Design Coordination and Corporate Image in 1967, an early, clear and comprehensive publication on the subject. In the introduction they wrote: ‘A corporation controls a wide range of things whose appearance is, or could be, affected by design. Industrial design in the ordinary sense can ensure that each of these in isolation has a good appearance. But further effort, of a different kind, is needed to coordinate the many separate items all belonging to one corporation, to achieve coherent and controlled results over a long period. This is what we understand by design coordination.’\nThe guru Wally Olins wrote his book Corporate Identity: Making Business Strategy Visible Through Design in 1989. In the meantime, the design world itself had gone through enormous changes since the days of Henrion’s book. Corporate identity had become a kind of core business for many designers and communication specialists. To quote Olins, roughly translated, on the search for an identity:‘We look at four regions in which the enterprise is active:- Products/services – what they make or sell;- Environment – where they produce or sell, the venues of physical contact;- Information – how the company describes and publishes what it does;- Behaviour – how people within the organization go about with each other and how they relate with people outside of their organization.’Theory and practice of the subject had evolved over the years, meaning that designers often had to find support from business-oriented disciplines, whether they liked it or not, and they were confronted with a different kind of response at the clients’ end.From the sixties onwards, corporate identity was a new task for many graphic designers and they had to face this challenge in their own way because their training had not prepared them for such a task. Corporate identity required new definitions, a new vocabulary, new techniques, new systems and specifications. Designers had to explain in detail complex matters to their clients, and prepare presentations for boardrooms, middle management or even a whole company. They also had to produce manuals in which the rules of the ‘game’ were clearly laid down, to ensure long-term continuity.\nGraphic design redefined\nThese commissions for corporate identity programmes changed the structure of design practices dramatically. It was a revolution that can only be compared with the effects of the computer as a dominating new tool. Very soon these challenges were leading to the formation of design groups in which colleagues – often with different skills – could work as a team to cope with these broad demands from the market. From the outset, commissions for corporate identities came mostly from larger organizations. These design services were now required not only by commercial clients, but also by cultural institutions and governmental bodies. Seminars and publications in the trade press helped a rapidly growing legion of designers to get accustomed to the grammar and the idiom of this new playground. Soon, keen managers of smaller firms discovered the value of identity programmes.These could often be served by solo designers or smaller design units, with Chermayeff & Geismar (1957) in New York among the first generation of design groups specializing in these matters. Among their evergreens are Mobil Corporation, the Chase Manhattan Bank and NBC. But the full list of their ‘identity clients’ is almost endless.British Rail became a giant project for DRU (Design Research Unit: Milner Gray and Associates). Nine years elapsed between the first recommendations and their implementation by the railway system.FHK Henrion had founded his first group, Henrion Design Associates, as early as 1951 and turned international in 1972. His KLM Royal Dutch Airlines scheme survived the decades. It embraced house colours, company title, typography, livery for aircraft and ground vehicles, architecture, signs, catering, uniforms, stationery, publications, advertising, maps and more. Over the years many of these aspects were revised and updated, without denying the character of the original designs.Other strong players in this field were Fletcher/Forbes/Gill, who joined forces in 1962 and grew on the waves of their worldwide successes to become Pentagram in 1972, now operating offices in the UK, the US and Germany.\nThe German designer Otl Aicher was responsible for the graphic aspects of the identity of Braun Electronics, as well as the design programme for the Munich Olympics in 1972.In the Netherlands, Total Design was founded in 1963, and the group acquired a strong international reputation in this particular field. So did ‘Dutch Design’ as a whole: the Dutch visual environment, including many governmental bodies, the railways, the Post Office, police and fire brigade soon came to constitute a ‘thoroughly designed country’. Studio Dumbar also played an important role in this.In the USA, other AGI designers like Saul Bass (with projects like United Airlines, Bell Telephone, Warner, Exxon) and Paul Rand (IBM, UPS, Westinghouse) were great masters in the field. But the designers I mentioned and their strong, longlasting projects are just the very tip of one of the few icebergs still growing on our planet.\nBen Bos, Amsterdam, 2005\nEssay taken from 'AGI: Graphic Design Since 1950' by Ben & Elly Bos","Reducing operating costs is one of the main concerns of shipping industry today. To help ship owners and operators boost the energy efficiency of their vessels, Deltamarin has since day one strongly focused on Energy and Environmental Efficiency (EEE). By applying Deltamarin’s new smart simulation tool, several items with a significant fuel saving potential (3-6% of total fuel costs) can be found.\nThe “EEE” ship design process includes several dimensions, such as practical design tools for modelling the energy consumption, a consistent follow-up method of the performance indicators and a bunch of useful project practices that enable taking the design ideas and results and implementing them in real life. Energy flow modelling is at the heart of the EEE work, and Deltamarin has recently developed a new-generation design tool for this purpose.\nThe tool itself is a highly flexible, dynamic simulation platform at system level that can accept data in almost any format and at various levels of precision. Such flexibility is required, since the energy models are configured for various purposes. Examples of these are simple machinery or system studies for concept projects or feasibility evaluations of a new product or process for retrofit studies. It is also possible to establish follow-up of the key performance indicators of a ship from the feasibility stage to a detailed level of design within the same model, continuing with operational follow up.\nPlease watch a video of the new design tool and analysis methods here.\nThe roots of developing energy flow modelling are in the energy balance calculation that began as early as the 1990s at Deltamarin, but the current simulation tool development was started a couple of decades later. Even though the current tool is a result of Deltamarin’s in-house development, the basis for the methods and the platform for the simulation tool were created in two development projects between 2009-2016 in a consortium that included several research institutes and companies within the maritime industry.\nDeltamarin is constantly developing its design tools and following state- of-the-art technologies that can be utilised for boosting ship energy efficiency. Even though passenger vessels are currently a strong focus group, the simulation platform has been utilised for various ship types and numerous applications, as the figures here illustrate.\nThe latest development efforts at Deltamarin with the simulation models are strongly focused on developing the waste heat recovery system. The solutions in this field are not only limited to the processes utilising waste heat, such as Organic Rankine Cycles, various turbines for exhaust gases or steam or absorption chillers; the heat recovery and production equipment must also be considered, such as boilers, heat exchangers in the diesel engine cooling water systems and even heat pumps. The improvement potential in ship fuel consumption with the waste heat recovery technologies available today could reach 4-6%, even with the primary machinery consisting of the most efficient (dual-fuel) diesel engines on the market. Reaching this goal or an even higher saving potential requires, however, taking a holistic view of all heat processes in the ship.\nImprovement suggestions for an existing system require a fundamental understanding of the waste heat recovery and energy “potential” in the various ship heat flows. Therefore, the latest version of the simulation tool includes a new feature: efficiency analysis of heat utilisation. The analysis method, unlike the traditional energy balance, is based on the second law of thermodynamics. This analysis method helps in identifying the improvement potential in the heat systems, and ultimately helps in identifying the best sources of converting waste heat into electricity.\nFor a successful project, it is important to do the right things at the right time. The concept design phase is particularly crucial, since the most important choices regarding ship energy efficiency are made during this early phase. The challenge in this case is often the limited time and rough level of information to base the decisions on. The developed energy flow modelling and analysis tools enable the devising of solutions in a short time that are not only energy- and environmentally-efficient but also cost-efficient.\nFor more information please contact\nR&D & Consulting Services\nMobile +358 50 3788 333"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:22fdf689-9d71-46a3-b9fd-50eac0a1ce4c>","<urn:uuid:c241ae23-de1e-48cb-a836-47e12f2c2599>"],"error":null}
{"question":"As a historian studying trade routes, I'm curious how the influence of Buddhism under Kublai Khan's rule compared to the earlier Tang Dynasty's relationship with Buddhism?","answer":"During the Tang Dynasty (618-907), Buddhism had a strong presence with temples established across China and notable structures like the Giant Wild Goose Pagoda built in 652 CE. There was significant contact between India and China through Buddhist monks, leading to the creation of Buddhist shrines in various locations including Bamiyan, Mount Wutai, Angkor Watt, and Borobudur. In contrast, under Kublai Khan's rule, while he was known for his acceptance of different religions, his administration focused more on political and economic aspects. He organized a divide-and-rule system with separate laws for Mongols and Chinese, and while he supported trade, science, and the arts, there isn't specific mention of him promoting Buddhism in the same way as the Tang Dynasty did.","context":["Carried across land and sea from Asia the spices of the Far East arrived in the West via India. India was a half-way house between East and West and, along with the Axum Empire of northern Ethiopia and Eritraea fl. 100– CE which bounded the Red Sea, controlled the sea lanes into the Red Sea at this time although Roman merchants had learned how to take advantage of the seasonal monsoon winds. The Grand Trunk Road running across northern India was especially active in the Hellenistic period but probably dates back several millennia.\nIn the Indian segment of the Silk Road there are the c. 30 rock-cut Buddhist Ajanta Caves dating from the 2nd century BCE to about 480 CE with paintings and rock-cut sculptures that are outstanding examples of ancient Indian art and now a UNESCO World Heritage Site. They are, in effect, ancient monasteries and probably a monsoon retreat for monks, as well as a stopping point for travelling merchants and pilgrims.\nThe Ajanta Caves They are recorded by medieval Chinese Buddhist travellers and the Mugha official of the Akbar era in the early 17th century. Enveloped in jungle they were rediscovered and recorded for the West in 1819 by British colonial officer Captain John Smith when hunting tigers.\nBy 500 CE the Indian centre of trade had moved to Sri Lanka which was on the preferred trade route of merchants from Burma, Java, Sumatra and Malaya. Indian traders working in south-east Asia brought with them the Hindu and Buddhist faiths. Sanskrit inscriptions dating to the 4th century have been found in Indonesia perhaps the most obvious evidence of this Indian influence today being the 8th century Borobodur temple in Java and the 12th century temples around Angkor Wat in Cambodia.\nThe overland route from India to Europe was by camel and packhorse and it passed from India to Pakistan, Afghanistan, Iran, Iraq, Syria, Turkey, the Balkans and, finally, to Venice. The loyalty of the Veneti tribe to Rome had been rewarded with a gift of marshy lands where the tribe had prospered such that, by the 800s, a small township called Venice had emerged. By the early 1500s Venice had the largest merchant navy in Europe an it used a sophisticated banking system with branches in Europe’s main cities. Eastern influence was evident in the architecture, like the Doges palace. In the Middle Ages meat was salted and pepper was regarded as a culinary necessity for the well-to-do. Spices generated much of the wealth on display in Venice, driving the spice race that was launched from the Iberian Peninsula. Pepper served as a currency for the storage and exchange of wealth and it was a useful source of government tax revenue.\nArabic numerals though originating in India, had arrived in Europe with Arabic merchants.\nIndian connections with SE Asia and the Far East were invaluable to Arab and Persian traders of the 7th to 8th centuries.\nIslam, from its beginnings, encouraged trade. There was a natural transition from the star-guided navigation of camel caravans across the waves of the desert to navigation on the seas. From local trade in frankincense and myrrh to international trade in spices. Prophet Muhammad (c. 570-632 CE) was born into a family of spice traders and his revelations comitted to the Qur’an rapidly united the Arab empire from Baghdad to Alexandria under the religion of Islam and Muslim control of the spice trade throughout Arabia, initially under the control of the Umayyad Dynasty in Damascus. With the passing of the caliphate to the Abbasid Dynasty in 750 the centre of the Muslim world passed from Damascus to Baghdad, much closer to the Persian Gulf and trade along the Tigris and Euphrates. (B. p. 29) At the mouth of the Red Sea the port of Aden was used by shipping from India carrying ambergris, camphor, musk, and sandalwood. Captains would pay a tribute to the Sultan of Yemen to use the Red Sea and this led to the fabulous riches that are recounted in the Tales of Sinbad the Sailor. As Islam spread across North Africa c. 650 CE both overland European caravans and Red Sea shipping trade was closed off except via Arab merchants from Arabia and Persia.\nBy the mid 7th century CE there was a take-over of sea routes and this continued into the Middle Ages. Tribes of south and west Arabia also gained control of land trade between South Arabia and the Mediterranean. Nabateans took control of the route crossing the Negev from Petra to Gaza. Petra, the Nabatean city carved out of rock, provided an oasis for caravans on a route that passed by sea from Arabian Jeddah (adjacent to Mecca) on the east coast of Red Sea north to Medina, inland to Jordanian Petra and then on to Gaza on Israel’s southern coast. Trade in the Red Sea passed from Bab-el-Mandeb at the southern mouth of the Red Sea to Berenike, built by Ptolemy II and from the 1st to 2nd century CE the trading hub for Arabia, Egypt, India and the Malabar coast.\nWhen Alexandria was taken by Muslim forces in 641 CE direct trade with the Mediterranean ceased. As early as 671 Arab and Persian traders were operating in China and by the 8th century were trading with Canton (today’s Guangzhou). Here goods from the Moluccas would be loaded on the ships for the return trip – silk, camphor, porcelain, and spices, this trade coming to a halt when Guangzhou was destroyed by rebel forces in 878. At about the same time Buddhist traders had established a strong trading centre at Srivjaya (near Palembang) in South Sumatra where nutmeg and cloves from the Moluccas were passed on to merchants from China, Arabia, and India.\nIn the 13th century Arab vessels sailed from Basra at the head of the Persian Gulf trading their cargo on the way and using the monsoon winds to arrive in Canton (Guangzhou) six months later. Through the 15th century there was increasing European concern about Muslim control of trade to the East, about 80% of this being in Arab hands in 1400. The merchants of Venice were dealing with about 500 tons of spice a year, mostly pepper.\nMuslim control of trade continued in the high and late Medieval period through the Ottaman Turks. In the Middle Ages European trade was largely controlled by Venetian and Genoan merchants who dealt with the merchants of the Byzantine Empire. When the Ottoman Turks took Constantinople in 1453 the overland trade between Venice and the Arabs was finally closed off. There was just one possible way forward for Portugal and Spain, the maritime powers of the day … a sea route to the Indies. Ottoman Muslim rivals, the Mamluks, followed by increasing the duties on spices passing through Alexandria.\nMuslim trade domination was a major incentive for European sea powers to find alternative sea routes for their business and as navigational skills and shipbuilding improved the Age of Discovery opened up sea lanes via the Cape of Good Hope and world trade was steadily taken over by European colonial powers.\nThe first records of cloves in China date to the third century BCE.(B. p. 22)\nBy the 16th and 17th centuries technological advance in Europe – the improved ship building and sophisticated navigation aids so important to the Age of Discovery, the increasingly effective weaponry – presented European powers with the opportunity to loosen the Muslim grip on trade in the East while, at the same time, introducing the orient to the ‘one true religion’.\nEuropean merchants were also able to cash in on the fortunes to be made from spices, serving as middle-men in Silk Road trade that passed through Byzantium. Although trade was delivered to many ports it was, between about 700 and 1500 that the maritime republics of Italy, most notably Venice and Genoa, commandeered the trade passing to the Middle East – not only the lucrative spices and incense but also opium and other goods. Italy thus held a virtual monopoly on this trade until the rise of the Ottoman Empire and the fall of Constantinople that closed to Europeans the land-sea route to the east, the merchants of Venice amassing huge wealth together with considerable resentment.\nAt the Far Eastern nd of the trade route were the Austronesian sailors who were ancestors of Polynesians, Micronesians, Maoris, Malayans and Madagascans. They used outrigger canoes that carried cargoes of rice, taro, sugarcane, yams, breadfruit, and coconuts. Pliny noted how in his day these remarkable sailors traded cinnamon, the journey from their homeland to the coast of Africa and back taking about five years. However, their major trade occurred in India and included cloves and nutmeg as well as cinnamon also venturing along routes to the Mediterranean that included the Red Sea, Persian Gulf, as well as the Nile, Tigris, and Euphrates Rivers, in the classical era c. 50 BCE to 96 CE usually distributed round the Mediterranean following accumulation on the docks of Alexandria. Pepper was a major commodity traded with India at this time.\nWhile Rome was marching east, Chinese of the Han Dynasty (206 BCE-220 CE) were pushing west through the Gansu corridor to today’s northwestern Xinjiang Province from which it was about 1000 km to the crossroad city of Dunhuange on the fringes of the Taklamakan Desert. progress was hampered by the raids from the northern Yuezhi and Xiongnu nomadic tribesmen who, like the Scythians, were skilled horsemen. They were also a source of prize horses. The Kushan Empire was a syncretic Empire formed by the Yuezhi a people occupying the arid grasslands of today’s Chinese provinces of Xinjiang and Gansu. The Yuezhi siezed Greco-Bactrian territories in the early 1st century and at its height King Kanishka the Great (127–163 CE) ruled over an area that included much of Afghanistan and today’s Peshawar and northern India – from Turfan in the Tarim Basin to Pataliputra on the Gangetic plain. Turpan was a trading hub with numerous inns and brothels, a slave trade but mostly supplying vegetables, cotton and grapes, as China’s largest raisin-producing region. For many years China was prepared to pay for peace in goods but eventually lost patience and after about ten years intermittent struggle managed, in 119 BCE, to occupy the Gansu corridor marking what is possibly the time of east-west integration on the Silk Road.\nGeneral Ban Chao with tens of thousands of troops penetrated from the Great Wall as far as the Caspian Sea, even sending an envoy to Rome.\nSilk is a textile made from the extremely fine thread produced by silk-worms, the caterpillers of the insect Bombyx mori as they spin cocoons while feeding off the leaves of Chinese Mulberry, Morus alba. In a China lacking coinage silk was a valuable commodity of exchange and this use expanded into the international markets.\nIn the 7th century especially there was contact between India and China through the medium of Buddhist monks leading to later Buddhist shrines in Bamiyan (Afghanistan), Mount Wutai (China), Angkor Watt (Cambodia) and Borobudur (Indonesia).\nDuring the Tang dynasty (618-907) the capital city of Chang An in about 700 CE under female Wu Zetien had a population of about 1 million citizens and it was linked to the Silk Road as a city of great wealth and luxury based on its trade to the Mediterranean and Japan. Wu was a concubine who rose to become political leader during the brief Zhou dynasty interlude of 684–705 CE. Here there was the largest ever known palace complex (178 acres) that included areas for archery and polo. At this time rice was stored in vast granaries and Buddhism and its temples were established across the land. The famous five-storey Buddhist Giant Wild Goose Pagoda built in 652 CE in Xi’an was destroyed by an earthquake and was rebuilt in 701-704. This was an era in which women gained political power only to be resisted by the subsequent Confucian hierarchy.\nFrom 1206 to about 1360 the Mongol power of Ghengiz Khan and family extended beyond the Central Asian steppes into northern China forcing the Song Dynasty in 1227 to move to the southern trading port of Huangzhou and a new maritime trade and when this also fell in 1276 Kublai Khan ruled a trading route that extended 6000 km from China to the Black Sea. Strung along the route were trading hubs that included, from West to East, Erzurum (Anatolia), Kazan (C Russia), Solkhat (Crimea), Astrakhan (Lower Volta), Tabriz (N Iran, Samarkand (Transoxiana), Karakprum (C Mongolia) Beijing (N China). This ended the Islamic hold on world trade.\nFor the West this period is familiar through the chronicles of Marco Polo who was respectfully entertained by Kublai Khan who was keenly interested in life, the Roman Catholic Church, and other aspects of life in the the West. In 1271 the three Polo brothers Nicolo, Mafeo and the 15-year-old Marco had travelled with the Venetian merchant fleet noting that for the merchants of Venice, Pisa, and Genoa it was Antioch (Ayas) that was the main Mediterranean point of access to inland routes.\nAt the time of the Tang Dynasty China opened its doors, accepting foreigners and their trade while themselves sailing beyond India to become regular merchants along the Red Sea and Persian Gulf. During the Han Dynasty there was a long period of peace, a Pax Sinica, at the same time as the Pax Romana. This was later followed by a second Pax Sinica lasting from 589 to 907, mostly during the Tang Dynasty (618–907).\nMorco Polo describes Chinese smerchnat ships of the late 13th century carrying up to 300 people and cargos of 120 tons. Built with four masts, double hulls, airtight compartments (bulkheads) allowing the ship to remain afloat of the hull was damages – ships of this quality were not built by Europeans for another 500-600 years.By the 14th century Western visitors were also amazed by Chinese skills with porecelain and the use of paper money. By 1402 Emperor Yongle of the Ming Dynasty decided to open a political dialogue with the world’s great nations hoping that in return for gifts and protection they would pay tribute to the Emperor. Nanjing shipyards on the Yangtze River produced hundreds of giant junks – military, trade, and provision. This Treasure Fleet was placed under the command of Admiral Zheng He who, rom 1405-1433 led seven expeditions, the sixth rounding the Cape of Good Hope into the Atlantic. To mobilise such extraordinary campaigns, some with fleets of several hundred ships, required logistical skills and technology far in advance of any other country in the world. Though such forces were capable of subduing other countries these voyages were peaceable and on the death of Emperor Yongle in 1424 was succeeded by his Confucian son Zhu Gaotzi China once again looked inwards, leaving the seas open to the European ships and ambitions that would change the world. This theme is taken up in the next article.","Kublai Khan Biography\nDied: February 1294\nTa-tu (Peking), China\nKublai Khan was the greatest of the Mongol emperors after Genghis Khan and founder of the Yüan Dynasty in China. He was a wise ruler and was able to lead a vast empire of nations by adapting different traditions to his own government.\nKublai Khan was the fourth son of Tulë and the grandson of Genghis Khan (c. 1165–1227), the founder of the Mongol Empire. Strong, brave, and intelligent, Kublai was Genghis's favorite grandson; he had accompanied his father, Tulë, in battles as a child. By the age of twelve he was a skilled horseman, and his reputation as a warrior grew as he became older. Kublai was seventeen when his father died.\nIn 1251 Kublai was given control over Chinese territories in the eastern part of the empire after his brother, Möngkë, became Great Khan of the Mongol Empire. Kublai organized a group of Chinese advisers to introduce reforms in his territories. Kublai was also put in charge of expeditions with the goal of unifying China under the Mongol emperor. In 1257, unhappy with the progress of the war against the Chinese Sung Dynasty, Möngkë led an expedition into western China but was killed by the Chinese defense in August 1259. In 1260, supported by pro-Chinese groups, Kublai was elected as Möngkë 's successor, but his younger brother, Ariq Böge, disputed the election and proclaimed himself khan at Karakorum, Mongolia. In the following years Kublai fought his brother, defeating him in 1264.\nKublai Khan's administration\nUnder Kublai, the Mongols adopted divide-and-rule tactics. The Mongols and central Asians remained separate from Chinese life; in many ways life for the Chinese was left basically unchanged. Kublai was also well known for his acceptance of different religions. The rule of the Mongol minority was assured by dividing the population of China into four social classes: the Mongols; the central Asians; the northern Chinese and Koreans; and the southern Chinese. The first two classes enjoyed extensive privileges; the third class held an intermediate position; and the southern Chinese, the most numerous of all, were practically barred from state offices. Separate systems of law were maintained for Chinese and for Mongols. Kublai also reorganized the government, establishing three separate branches to deal with civilian (nonmilitary) affairs, to supervise the military, and to keep an eye on major officials.\nFollowing this reorganization, a new capital city was constructed at present-day Peking, China, in 1267. First called Chungtu, the city was renamed Ta-tu (or Daidu, \"great capital\") in 1272. In the eyes of Kublai, leaving some Chinese institutions and customs in place was a political decision. Outside the administration, much of the Mongol way of life still prevailed. The Mongols, especially the military, preserved their tradition as nomads (wanderers). Even within the administration, Chinese influence was controlled by the large numbers of Mongols and central Asians. Kublai Khan named his rule the Yüan Dynasty in 1271. By February 1278 he had destroyed the Sung dynasty and was the unquestioned leader of an empire that stretched across two continents.\nKublai was a great supporter of trade, science, and the arts. He introduced the use of paper money for the entire empire and ordered the creation of a new alphabet for the Mongol language that closely resembled Chinese writing. Kublai also established a system of sea transport and developed inland river and canal routes to move grain from the fertile rice-growing Yangtze River basin to provide food for the growing population. The Grand Canal system was finally extended north to Peking from the Yellow River.\nAs emperor of China, Kublai demanded loyalty and gifts from other states within the empire. Some of these, such as Annam and Korea, cooperated. To others, Kublai sent messengers asking for payment and attacked if his demands were ignored. Many of these expeditions, however, ended in failure. Twice between 1274 and 1281 Kublai's armies against Japan were either destroyed by storm or crushed by the Japanese because of the Mongols' inability to fight sea battles and the poor quality of their naval forces. Kublai suffered a setback when he failed to conquer the Malay kingdom of Champa in Indochina after a long war (1283–87). Three expeditions to conquer Burma in 1277, 1283, and 1287 also failed. In 1293 near the end of his reign, Kublai launched a naval expedition against the Javanese kingdom of Majapahit, but the Mongol forces had to withdraw after considerable losses.\nContact with the West\nUnder Kublai, the opening of direct contact between China and the West was made possible by Mongol control of central Asian trade routes and aided by the presence of efficient postal services. In the early thirteenth century, large numbers of Europeans and central Asians made their way to China. The presence of the Mongol power also enabled many Chinese to travel freely within the Mongol Empire, all the way to Russia, Persia, and Mesopotamia.\nMarco Polo (c. 1254–1324), who accompanied his father on this trip, was probably the best-known foreign visitor ever to set foot in China. It is said that he spent the next seventeen years under Kublai Khan, including official service in the administration and trips through the provinces of Yunnan and Fukien. The accuracy of his descriptions of China was questioned, but the popularity of his journal generated great interest among Europeans for going east. Rabban Sauma, a monk born in Peking, crossed central Asia to the Il-Khan's court in Mesopotamia in 1278 and was one of those whom the Mongols sent to Europe to seek Christian help against Islam. Under Kublai, the first direct contact and cultural exchange between China and the West had occurred.\nKublai Khan's legacy\nAfter a glorious reign of thirty-four years, Kublai Khan died in Ta-tu in February 1294. He is regarded as one of the great rulers in history. He was a shrewd and thoughtful ruler of a huge state. He was popular among the Chinese, and his achievements ranked him second to Genghis among the Mongol rulers. He showed great intelligence in using partial adoption of Chinese political traditions and divide-and-rule tactics to help in the administration of a large empire. The main problem with his reign was that as he and his successors became more involved in Chinese traditions, there was a growing conflict between the Mongol rulers of China and those of the other khanates within the Mongol confederacy. They preferred to maintain their own character instead of looking toward China for leadership.\nFor More Information\nDramer, Kim. Kublai Khan. New York:Chelsea House, 1990.\nRachewiltz, Igor de. Papal Envoys to the Great Khans. Stanford, CA: Stanford University Press, 1970.\nRossabi, Morris. Khubilai Khan: His Life and Times. Berkeley: University of California Press, 1988.\nSilverberg, Robert. Kublai Khan, Lord of Xanadu. Indianapolis: Bobbs-Merrill, 1966."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:17d1264e-4a2a-46b0-8100-cd7a09ad64e1>","<urn:uuid:749260b9-0d7e-4168-95c4-6b40907c9760>"],"error":null}
{"question":"What are the key differences between HyCoSy and hysterosalpingography as diagnostic procedures for checking fallopian tubes?","answer":"HyCoSy (Hysterosalpingo Contrast Sonography) and hysterosalpingography are different procedures for examining fallopian tubes. HyCoSy is a non-invasive ultrasound procedure that takes approximately 10 minutes and involves passing a thin catheter through the cervix into the womb. In contrast, hysterosalpingography is an x-ray procedure that takes 15-30 minutes and involves injecting a water-based or oil-based dye through a catheter into the uterus. While both procedures check for tubal abnormalities, hysterosalpingography can also detect tumors, scar tissue, or tears in the uterine lining, and the dye used can sometimes open up small blockages in the fallopian tubes.","context":["If you need infertility treatment, or are researching infertility online, you will often come across many forums and sites which seem to be packed full of fertility jargon – Clomid, AMH levels, hCG…it can confusing if you don’t yet understand what these terms mean.\nSo here’s an easy-to-understand explanation of some common terms and what they mean:\nClomid:Clomid is a tablet we use to gently simulate your ovaries.\nGonadotrophins:Gonadotrophins is medication used to stimulate egg growth during IUI or IVF.\nFSH: FSH means Follicle Stimulating Hormone. FSH affects your ovaries to increase the number of follicles and stimulates their development.\nLH: LH means Luteinising Hormone, it’s one of the necessary hormones needed to stimulate ovulation in your body.\nBuserelin: Buserelin is medication (taken nasally or as an injection) used to boost egg production, but if taken for an extended period of time will switch off your egg growth. We call this process down regulation.\nDown regulation:When your natural ovarian function is ‘switched off’ during your treatment cycle through the use of drugs such as Buserelin.\nhCG:Human Chorionic Gonadotrophin is an injectible hormone that helps your eggs to go through the final stage of maturation so they are ready for collection. We will know when your eggs are ready through regular scanning. You will receive an hCG injection before egg retrieval.\nHyCoSy:Hysterosalpingo Contrast Sonography is a non-invasive ultrasound procedure which checks for abnormalities of your womb and fallopian tubes. It’s usually performed before day 12 of your cycle. It takes approximately 10 minutes and involves passing a thin catheter through the cervix and into the womb.\nAzoospermia: This is when a man has no sperm present in their ejaculate. It is caused by a number of reasons: a blockage in the tubes that transport the sperm, vasectomy, failed vasectomy reversal or as a consequence of low or no sperm production in the testicle.\nBlastocyst: A blastocyst is a stage of embryo which has been allowed to develop for 5 days after fertilisation.\nFostimon:Fostimon is a brand name of a gonadotrophin which contains FSH.\nCyclogest:Cyclogest is a pessary we will give you, it contains progesterone, which is a natural hormone and is used to stabilise your own hormone levels. We will give you Cyclogest to use after your egg recovery, which you take up until you know the results of your treatment.\nEvorel:Evorel are patches which contain Estradiol/Oestradiol – a naturally occurring form of oestrogen. These patches stimulate the growth of the endometrial lining and typically they are used in women who are using a donor egg, to prepare the womb for transfer.\nAMH:Anti-Mullerian Hormone (AMH) is a substance made by the ovarian follicle which contains the egg. Your AMH level can help us identify the best course of treatment for you. It can tell us how well you will respond to ovarian stimulation and give us an indication of how many eggs you have in your ovaries – your ovarian reserve.\nAt Manchester Fertility we will always explain to you very clearly what your treatment involves and what any tests results mean, in a way that you can easily understand.\nAlthough we are fertility consultants and experts, we realise that what are common terms to us may be completely alien to you – and so we will always ensure that you’re completely comfortable and clear at all times, as any confusion only adds to the stress of your infertility journey.\nLast updated: 20th January 2020","hysterosalpingography(redirected from uterohysterography)\nAlso found in: Dictionary, Encyclopedia.\nHysterosalpingography is a procedure where x rays are taken of a woman's reproductive tract after a dye is injected. Hystero means uterus and salpingo means tubes, so hysterosalpingography literally means to take pictures of the uterus and fallopian tubes. This procedure may also be called hysterography (or HSG).\nHysterosalpingography is used to determine if the fallopian tubes are open, or if there are any apparent abnormalities or defects in the uterus. It can be used to detect tumors, scar tissue, or tears in the lining of the uterus. This procedure is often used to help diagnose infertility in women. The fallopian tubes are the location where an egg from the ovary joins with sperm to produce a fertilized ovum. If the fallopian tubes are blocked or deformed, the egg may not be able to descend or the sperm may be blocked from moving up to meet the egg. Up to 30% of all cases of infertility are due to damaged or blocked fallopian tubes.\nThis procedure should not be done on women who suspect they might be pregnant or who may have a pelvic infection. Women who have had an allergic reaction to dye used in previous x-ray procedures should inform their doctor.\nAs with other types of pelvic examinations, the woman will lie on her back on an examination table with her legs sometimes raised in stirrups. The x-ray equipment is placed above the abdomen.\nA speculum is inserted into the vagina and a catheter (a thin tube) is inserted into the uterus through the cervix (the opening to the uterus). A small balloon in the catheter is inflated to hold it in place. A liquid water-based or oil-based dye is then injected through the catheter into the uterus. This process can cause cramping, pain, and uterine spasms.\nAs the dye spreads through the reproductive tract, the doctor may watch for blockages or abnormalities on an x-ray monitor. Several x rays will also be taken. The procedure takes approximately 15-30 minutes. The x rays will be developed while the patient waits, but the final reading and interpretation of the x rays by a radiologist (a doctor who specializes in x rays) may not be available for a few days.\nInterestingly, sometimes the hysterosalpingography procedure itself can be considered a treatment. The dye used can sometimes open up small blockages in the fallopian tubes. The need for additional test procedures or surgical treatments to deal with infertility should be discussed with the doctor.\nThis procedure is generally done in the x-ray department of a hospital or large clinic. General anesthesia is not needed. A pain reliever may be taken prior to the procedure to lessen the severity of cramping.\nWhile no special aftercare is required after a hysterosalpingography, the woman may be observed for some period after the procedure to ensure that she does not have any allergic reactions to the dye. A sanitary napkin may be worn after the procedure to absorb dye that will flow out through the vaginal opening. If a blockage is seen in a tube, the patient may be given an antibiotic. A woman should notify her doctor if she experiences excessive bleeding, extensive pelvic pain, fever, or an unpleasant vaginal odor after the procedure. These symptoms may indicate a pelvic infection. Counseling may be necessary to interpret the results of the x rays, and to discuss any additional procedures to treat tubal blockages or uterine abnormalities found.\nCramps during the procedure are common. Complications associated with hysterosalpingography include abdominal pain, pelvic infection, and allergic reactions. It is also possible that abnormalities of the fallopian tubes and uterus will not be detected by this procedure.\nA normal hysterosalpingography will show a healthy, normally shaped uterus and unblocked fallopian tubes.\nBlockage of one or both of the fallopian tubes or abnormalities of the uterus may be detected.\nAmerican Society for Reproductive Medicine. 1209 Montgomery Highway, Birmingham, AL 35216-2809. (205) 978-5000. http://www.asrm.com.\nCatheter — A thin tube, usually made of plastic, that is inserted into the body to allow the passage of fluid into or out of a site.\nFallopian tubes — The narrow ducts leading from a woman's ovaries to the uterus. After an egg is released from the ovary during ovulation, fertilization (the union of sperm and egg) normally occurs in the fallopian tubes.\nHysterography — Another term for the x-ray procedure of the uterus and fallopian tubes.\nHysterosalpingogram — The term for the x ray taken during a hysterosalpingography procedure.\nSpeculum — A plastic or stainless steel instrument that is inserted into the opening of the vagina so the cervix (the opening of the uterus) and interior of the vagina can be examined.\nGale Encyclopedia of Medicine. Copyright 2008 The Gale Group, Inc. All rights reserved.\nradiography of the uterus and fallopian tubes.\nMiller-Keane Encyclopedia and Dictionary of Medicine, Nursing, and Allied Health, Seventh Edition. © 2003 by Saunders, an imprint of Elsevier, Inc. All rights reserved.\nRadiography of the uterus and fallopian tubes after the injection of radiopaque material.\n[hystero- + G. salpinx, a trumpet, + graphō, to write]\nFarlex Partner Medical Dictionary © Farlex 2012\nhysterosalpingographyA method for evaluating uterine tubal pathology, in which radiocontrast is instilled transcervically into the endometrial cavity and fallopian tubes, followed by fluoroscopy or taking plain films, as a way of defining the uterine cavity and tube.\nEvaluation of infertility (e.g., to determine fallopian tube patency), uterine-cavity (e.g., submucosal) leiomyomas, endocervical and endometrial polyps, pelvic tuberculosis, intrauterine adhesion-synequiae, endometrial cancer, congenital malformations (e.g., müllerian duct abnormalities), bicornuate uterus, septate uterus, uterus didelphys, uterine hypoplasia, and DES-induced defects.\nHysterosalpingography has waned in popularity, due to high false-positivity and false-negativity.\nSegen's Medical Dictionary. © 2012 Farlex, Inc. All rights reserved.\nhysterosalpingographyUterohysterography Reproductive medicine A nonsurgical method for evaluating uterotubal pathology, in which radiocontrast is instilled transcervically in the uterine cavity and fallopian tubes followed by fluoroscopic examination or obtention of a plain film, as a means of defining the shape and size of the uterine cavity and tubal patency. Cf Electronic fluorography, Salpingoscopy.\nMcGraw-Hill Concise Dictionary of Modern Medicine. © 2002 by The McGraw-Hill Companies, Inc."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:40bd4da1-6360-4155-bc6e-ae49a623a727>","<urn:uuid:f5a390f0-45b4-4744-92ff-8bf63abaa7d5>"],"error":null}
{"question":"Did the cinematographers of 'Carol' and 'Cafe Society' take different approaches to achieving their desired visual aesthetics?","answer":"Yes, they took opposite technical approaches. For 'Carol,' cinematographer Ed Lachman specifically chose to shoot on Super 16mm film to achieve a muted, grainy look that matched 1950s Kodak Ektachrome film aesthetics. In contrast, for 'Cafe Society,' Vittorio Storaro opted for the Sony F65 digital camera because it could record in 4K with 16-bit color depth, though he maintained that proper lighting, not just recording available light, was essential for true cinematography.","context":["I went to CineGear this year. It was great, was able to catch up and reconnect with some old friends and make new ones.\nI got a picture of the new Leonard Nimoy street sign on the Paramount lot where the Expo was held.\nHe was also a photographer as well as a director and did many projects at Disney including Body Wars.\nZeiss was there with a cut away of one of their lenses.\nThe Citizen Kane stage.\nPanavision showed the new DXL 8K camera. The footage shown was very nice!\nThe best thing was seeing Vittorio Storaro ASC.\nHe talked about working with Woody Allen on his new film for Amazon Studios, Cafe Society.\nThis is Woody’s first digital feature and Vittorio used the Sony F65;\n“I had seen that the Sony F65 was capable of recording beautiful images in 4K and 16 bit-colour depth in 1:2, which is my favorite composition,” Storaro said. “So when Woody called me this year asking me to be the cinematographer of his new film with the working title ‘WASP 2015,’ my decision was already made. I convinced him to record the film in digital, so we can begin our journey together in the digital world. It’s time now for the Sony F65!”\nHe spoke of the Technicolor IB process, light, shadows and color and said that digital makes it too easy.\nHe stated that a trend that has emerged with the use of digital cameras is that “people want to work faster or show that they can use less light, but they don’t look for the proper light the scenes needs. That isn’t cinematography, that’s recording an image. … I was never happy in any set to just see available light,” said Storaro, who has won Oscars for Apocalypse Now, Reds and The Last Emperor. “Even in very important films that take Academy Awards, you can record an image without location lighting. But that’s not necessarily the right light for the character. We have to always move a story forward, not step back.”\nHe elaborated on his work with Coppola and that he hasn’t used anamorphic lenses for many years. Sorry Mr. Tarantino.\nThe best and most important part though, was when he got even more philosophical. He mentioned Mozart, the Lumiere brothers, Newton, Caravaggio, architecture, and Plato and the Cave. From his website:\nEver since Plato’s “Myth of the Cave” we are used to seeing Images in a specific space. In Plato’s myth, prisoners are kept in a cave facing an interior wall, while behind them, at the entrance to the cave, there is a lighted fire, with some people with statues and flags passing in front of the fire. At the same time, their shadows are projected onto the interior wall of the cave by fire’s light. The prisoners are looking at the moving shadows in that specific area of the wall. They are watching images as a simulation, a “simulacre” of reality, not reality itself. The myth of Plato is a metaphor for the Cinema.\nHe believes that film is a collaboration as opposed to the auteur theory and emphasized the importance of story.\n“You need to find the balance of technology and art,” continued Storaro, who was inspiring and thought-provoking in his speech, also raising an argument against the use of the term ‘director of photography’ to define the role of the cinematographer. “That’s a major mistake. There cannot be two directors. … Let’s respect the director,” he asserted, saying that ‘cinematographer’ is the appropriate word, and adding that it’s not interchangeable with photographer. “Cinematography is motion, we need a journey and to arrive at another point. We don’t create a beautiful frame, but a beautiful film. That’s why I say ‘writing with light.'”","The end of celluloid film has been happening steadily over the last few years, with each new blow seemingly the knock-out punch. But its devotees refuse to give up the fight. “If I can’t shoot on film I’ll stop making movies,” said Quentin Tarantino in 2014. He was talking on radio station KCRW’s “The Treatment” in L.A, “The fight is lost if all we have is digital DCP presentations. To me, that’s just television in public.” he added. The DCP he refers to is Digital Cinema Package, the computer hard drive that contains a movie’s audio and video. It is sent to a movie theater where it is ingested into the projector for digital projection. Like reels of film before it, the DCP can be sent on to another theater, only its a lot cheaper for the studio to produce. This is not the latest technology, as many theaters can get their films directly by satellite transmission.\nThe use of 70 mm film stock in movie-making had already died by the end of the 1960s.That’s when studios used it to pry people away from their TV sets and TV dinners and into theaters to watch movies like South Pacific, Cleopatra, Lawrence of Arabia, and It’s a Mad Mad, Mad Mad World. But like many old movies, 70 mm was ripe for a sequel. Tarantino’s last movie, The Hateful Eight, recently, was shot on 70mm (actually 65mm) film stock. Director Christopher Nolan stated , “I don’t want anyone telling any filmmaker they can’t shoot on film any more than telling David Finch and Steven Soderbergh that they can’t shoot digital. It’s the director’s right. It’s their choice.” Christopher Nolan shot Intersteller in 2014 on 70mm film stock (65mm). Warner Brothers released the movie two days early to those theaters that still had film projectors. The iconic TCL Chinese Theater in Hollywood acquired film projectors for the occasion. Several smaller theater chains that had already converted to digital projection squwaked. “I can’t afford to get the projectors out of the warehouse,” said Joe Paletta of the Spotlight Theaters in Georgia, “and I don’t have anyone to operate them [the film projectors].” For Nolan, it was an opportunity to incentivize and reward the theaters that had kept projecting film.\nIn 2002 Star Wars Episode: II Attack of the Clones became the first movie to be shot entirely in digital. But now, the latest Star Wars Episode VII: The Force Awakens was shot on 65mm film stock by film proponent J.J. Abrams, with the encouragement of George Lucas. Part of the idea was to recapture the look of the original trilogy. This was important to Lucas because, as Abrams added “… the movie, in a way, goes backwards to go forwards.” George Lucas wants the future episodes to be shot on film also.\nYet the world of film still teeters on the abyss. The new Star Wars was only shown on 70mm film at IMAX theaters . Nearly everywhere else it was projected on a digital transfer. But film’s qualities come in different shapes. For the movie Carol, director Todd Haynes wanted a muted look in the cinematography. This was in keeping with the bleak times in Cold-War New York during 1952 when the story took place. The story was based on the novel by Patricia Highsmith. Haynes and his team including cinematographer Ed Lachman referenced the work of women photographers of the time: Ruth Orkin; Helen Leavitt; and Vivian Maier. They were especially intrigued by the photographers’ use of Kodak Ektachrome film and its muted colors, and wanted to achieve that look. This meant shooting on film rather than digitally. And more unusually, Ed Lachman used Super 16mm. His reasoning was that if he used 35mm, by the time it was converted for digital projection there would be no difference in the finished look – the “grain” would have all disappeared. “The feeling and texture of the grain reinforced the emotionality of the story,” said Lachman.\nBut the world of film still teeters on the abyss. In New York, Technicolor and Deluxe, long-time processors of color film for the movie industry, had been amalgamated as the Film Lab New York. After processing Carol the Lab went belly-up. After Ed Lachman found out that the lab equipment was going to be tossed, he made arrangements to salvage it all. Technicolor had already closed a plant in Glendale California.\nYet digital cinema, the current master of moviedom, is showing signs of the panic that befell the Hollywood studios in the late 1950s and 1960s. Recently we were given 3-D movies, a trend already showing signs of fatigue. Then we were offered dining and wine with our cinema, and reserved seating. Reclining seats are the next trend, with news of the introduction of sensory enhancements like vibrating seats and the diffusion of smells (didn’t Smell-O-Vision come out in the 1950s too?). Perhaps the thinking is that bombarding the senses with smells, booming sounds, vibrating seats, and explosions on the screen will get us all into the theaters.\nIt’s a rarely disseminated fact that movie attendance has been nose-diving steadily and surly since 1930. Back then, 80 million people, or 65% of the U.S. population, went to the movies a week. In 2015, that number has dropped to 25.7 million people a week, less than 10% of the U.S. population. A yearly chart, covering 1930-2000 can be found here. The attendance numbers have continued to drop since 2000. Back in the 1930s, to lure all those movie-goers, each of the seven big studios released 30 to 40 films a year. But we know that The U.S Courts broke up the studio-system in its Anti-Trust ruling in 1948, forcing them to sell of their movie theaters. How laughable with the multi-media, multi-national, corporate conglomerates we now have running movie studios.\nIn 2015, in a busy year for the busiest studio Warner Brothers, it released 21 movies. Movie revenues are freely quoted, which seem very impressive, and is helped by the always escalating prices for theater tickets. While this has nothing to do with whether or not a movie is film-based or digital, it is connected to movie industry economics – which brought us digital based film projection. And of course, as Kenneth Turan the film critic of the LA Times said in writing about Hollywood, “The one big thing it knows how to make is sequels and superhero movies and sequels to superhero movies.”\nBut which medium is best for the preservation of the content, one might wonder? It seems intuitive that digital is best as a preservation medium. And certainly that’s what we have been hearing for the last several years. But Martin Scorsese doesn’t think so. “Film is still the only preservation medium that we know to be durable.,” he said. “We just don’t know about digital storage systems. They haven’t been around long enough, and more importantly, they’re always changing. I think it’s important t to preserve our pictures on film, no matter how we shot them or finished them. That means negatives, and it means prints.” Mr. Scorsese is the founder of the Film Foundation, whose mission it is to preserve movies. Of course film has had a very rocky history. Approximately 90% of American silent films are considered lost, as well as 50% of sound films made before 1950. The combustible nitrate-based film of the silent era is partly responsible, leading to major fires at studio vaults. M-G-M had an electrical fire in 1967, destroying most of the studio’s cartoons, silent films, and films from the earlier Metro, Goldwyn, and Louis B. Mayer Pictures. Similarly, a fire at Fox Pictures destroyed its pre-1935 film negatives. Huge efforts have been made to preserve older films and newer ones since, but the job is colossal.\nThis blog post is disseminated digitally. Part I appeared in February 2014."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:148f535e-8e70-4c2d-b071-a01ce5b0f1fb>","<urn:uuid:bd07302b-a780-47bf-9dbb-9d2fe3e9d311>"],"error":null}
{"question":"Could you tell me what factors most significantly affect the intensity of a sonic boom? I'm studying atmospheric physics.","answer":"Of all factors influencing sonic booms, altitude is the most effective method of reducing sonic boom intensity. As the shock cone gets wider while moving outward and downward, its strength is reduced. Additionally, the size and weight of the aircraft affect boom intensity - larger and heavier aircraft create stronger and louder sonic booms than smaller, lighter aircraft because they must displace more air and create more lift to sustain flight.","context":["Space Shuttles and Sonic Booms\nOne of the most distinctive features associated with the return of Space Shuttles from orbital missions are the twin sonic booms that herald their supersonic arrival back into the atmosphere.\nLike all other vehicles traveling faster than the speed of sound, the orbiters produce shock waves as they pass through the atmosphere. On the ground, these shock waves are heard much like sharp thunderclaps.\nMost people expected to hear a sonic boom when the orbiter Columbia soared back into the atmosphere for the first Space Shuttle landing Apr. 14, l981, at NASA's Dryden Flight Research Center, Edwards, CA. Single booms were heard when the Mercury, Gemini, and Apollo spacecraft reentered the atmosphere. The double sonic booms are now distinctive signatures of each Space Shuttle landing.\nSonic booms are created by air pressure. Much like a boat pushes up a bow wave as it travels through the water, a vehicle pushes air molecules aside in such a way they are compressed to the point where shock waves are formed.\nThese shock waves form two cones, at the nose as well as at the tail of the vehicle. The shock waves move outward and rearward in all directions and usually extend to the ground. As the shock cones spread across the landscape along the flightpath, they create a continuous sonic boom along the full width of the cone's base. The sharp release of pressure, after the buildup by the shock wave, is heard as the sonic boom.\nThe nose and tail shock waves are usually of similar strength. The time interval between the nose and tail shock waves is primarily dependent on the size of the aircraft and its altitude. Most people on the ground cannot distinguish between the two and they are usually heard as a single sonic boom. As the time interval increases, two booms are heard. A small fighter-type aircraft about 50 ft long will generate nose and tail shock waves of less than a tenth of a second (0.1 sec). The ear usually detects these as a single sonic boom.\nThe interval between nose and tail shock waves on the Space Shuttles, which are 122 ft long, is about one-half of a second (0.50 sec), making the double boom very distinguishable.\nGeneral Factors Associated With Sonic Booms\nThere are several factors that can influence sonic booms -- weight, size, and shape of the aircraft or vehicle, plus its altitude, attitude and flight path, and weather or atmospheric conditions.\nA larger and heavier aircraft must displace more air and create more lift to sustain flight, compared with small, light aircraft. Therefore, they will create stronger and louder sonic booms than smaller, lighter aircraft. The larger and heavier the aircraft, the stronger the shock waves will be.\nAltitude determines the distance shock waves travel before reaching the ground, and this has the most significant effect on intensity. As the shock cone gets wider, as it moves outward and downward, its strength is reduced. Generally, the higher the aircraft, the greater the distance the shock wave must travel, reducing the intensity of the sonic boom. Of all the factors influencing sonic booms, increasing altitude is the most effective method of reducing sonic boom intensity.\nThe width of the boom \"carpet\" beneath the aircraft is about one mile for each 1000 ft of altitude. An aircraft, for example, flying supersonic at 50,000 ft will produce a sonic boom cone about 50 miles wide. The sonic boom, however, will not be uniform. Maximum intensity is directly beneath the aircraft, and decreases as the lateral distance from the flightpath increases until it ceases to exist because the shock waves refract away from the ground. The lateral spreading of the sonic boom depends only upon altitude, speed, and the atmosphere -- and is independent of the vehicle's shape, size, and weight.\nThe ratio of aircraft length to maximum cross sectional area also influences the intensity of the sonic boom. The longer and more slender the aircraft, the weaker the shock waves. The fatter and more blunt the vehicle, the stronger the shock wave can be.\nIncreasing speeds above Mach 1.3 results in only small changes in shock wave strength.\nThe direction of travel and strength of shock waves are influenced by wind, speed, and direction, and by air temperature and pressure. At speeds slightly greater than Mach 1, their effect can be significant, but their influence is small at speeds greater than Mach 1.3. Distortions in the shape of the sonic boom signatures can also be influenced by local air turbulence near the ground. This, too, will cause variations in the overpressure levels.\nAircraft maneuvering can cause distortions in shock wave patterns. Some maneuvers -- pushovers, acceleration, and \"S\" turns -- can amplify the intensity of the shock wave. Hills, valleys, and other terrain features can create multiple reflections of the shock waves and affect intensity.\nSonic booms are measured in pounds per square foot of overpressure. This is the amount of the increase over the normal atmospheric pressure which surrounds us (2,116 psf/14.7 psi).\nAt 1 lb overpressure, no damage to structures would be expected.\nOverpressures of 1 to 2 lb are produced by supersonic aircraft flying at normal operating altitudes. Some public reaction could be expected between 1.5 and 2 lb.\nRare minor damage may occur with 2 to 5 lb overpressure.\nAs overpressure increases, the likelihood of structural damage and stronger public reaction also increases. Tests, however, have shown that structures in good condition have been undamaged by overpressures of up to 11 lb.\nSonic booms produced by aircraft flying supersonic at altitudes of less than 100 ft, creating between 20 and 144 lb overpressure, have been experienced by humans without injury.\nDamage to eardrums can be expected when overpressures reach 720 lb. Overpressures of 2160 lb would have to be generated to produce lung damage.\nSonic Boom Footprints\nOverpressures recorded on the ground during the landing of the orbiter Discovery on mission STS-26 Oct. 3, 1988 revealed that the intensity was 1.06 lb in the Santa Barbara area as Discovery crossed the coastline at a speed of Mach 4.37 at an altitude of 115,400 ft. Intensity rose to 1.15 lb in the Santa Clarita Valley area, 45 miles southwest of Edwards, as the vehicle's speed and altitude reduced. At Edwards, when Discovery was about 60,000 ft overhead just moments before the landing the overpressure was 1.25 lb. The highest reading during the landing approach over Southern California was 1.75 lb in the areas of Palmdale and Lancaster 20 to 30 miles southwest of Edwards.\nTypical overpressure of other aircraft types are:\nSR-71: 0.9 lb, speed of Mach 3, 80,000 ft\nConcorde SST: 1.94 lb, speed of Mach 2, 52,000 ft\nF-104: 0.8 lb, speed of Mach 1.93, 48,000 ft"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:cb5a44c8-14f4-47d1-be06-86ce74f8df65>"],"error":null}
{"question":"Which is more rare: successful insanity defenses in murder trials or finding cognitive functions isolated to single brain lobes?","answer":"Both are rare occurrences, but in different contexts. The insanity defense is used in only about 1% of felony cases and rarely succeeds in murder trials. Similarly, recent molecular and cellular brain research has shown that most cognitive functions are not isolated to single brain lobes, but rather are distributed across multiple brain regions and connected through neuronal circuits. While some functions were traditionally thought to be localized (like speech production in the left frontal lobe), scientists now understand that many functions are actually more widespread across multiple brain areas.","context":["Interposed between criminal law and psychiatry are forensic psychiatrists, doctors who assess whether defendants are currently mentally competent to stand trial. Some of their work involves consulting on legal insanity cases, in which the forensic psychiatrist performs a retrospective evaluation to decide, based on long-established state and federal laws, whether the defendant was so impaired by mental illness at the time of the act that the jury or judge should find them not criminally responsible.\nPsychiatric evaluations can sway the outcome of a case — and the life of the person on trial. “You want the person to have a clinically accurate and just outcome,” says David Rosmarin ’74, the director of the Forensic Psychiatry Service at McLean Hospital in Boston.\nRosmarin has spent his career evaluating the mental states of those charged with crimes, including murder. A fraction of murder defendants use the insanity defense, and these cases have high stakes. It’s Rosmarin’s job to discern the true basis of the killing through videotaped, face-to-face examinations and convey it to the jury. It’s similar to those scenes in Law and Order, but there’s a key difference in how he conducts himself in these situations: impartiality. In forensic psychiatry, the doctor performs an unbiased evaluation for legal purposes, not treatment.\n“The guiding principle in forensic psychiatry is truth telling,” Rosmarin says. “You try to understand what happened and tell the truth about it, irrespective of whether you’re working for the prosecution or the defense.”\nThe insanity defense is used in about 1% of felony cases and rarely succeeds in murder trials. Still, some alleged murderers feign insanity to stave off harsh charges, while others with true mental illness kill for nonpsychotic reasons. Having conducted more than 150 psychiatric evaluations of murderers to determine if they are legally insane, Rosmarin says it’s rare for someone with no mental illness to malinger successfully, but it’s more challenging to assess whether those with true serious mental illness are exaggerating.\nThe work can be grueling, with people confessing how they carried out heinous crimes — and their thoughts before, during, and after — to Rosmarin. “It’s a sad business,” he says. But that sadness is softened, in part, by his role teaching Harvard Medical School students and residents. In a specialized field like Rosmarin’s, educating the next generation on a nuanced discipline like psychiatry is paramount. He cherishes the moments when he’s teaching a difficult concept, like an interviewing technique, and it clicks for his students. “I enjoy their energy,” he says.\nWhen the neuroscience major was a student himself, he leaned on the teachings of Colgate luminary Jerry Balmuth to learn about philosophy, opening his mind to concepts like ethics that aid in his work today. Upon leaving the Hill, Rosmarin earned his medical degree from Boston University, spending his chief residency year at Harvard Law School.\nBefore taking his post at McLean — and after 10 years at Massachusetts General Hospital — Rosmarin spent a decade consulting on psychiatric assessments of a very different subset: world leaders. Most of the information about his work with the CIA is classified, but he says he primarily assessed how various world leaders would likely react in a range of circumstances. However, Rosmarin emphasizes that he was never involved with assessing detainees. “Such CIA ethical breaches were performed by psychologists and physical medicine MDs, not psychiatrists,” he says.\nRosmarin’s findings aided United States officials to make decisions based on psychiatric assessment, not just their gut instinct. “It’s difficult to piece together an assessment of somebody whom you’re not examining directly,” he says. “You try to be very careful to give an appropriate level of confidence to your assessments. Decision makers don’t care about psychiatric peccadilloes, they care about what will affect the U.S.”\nAt the root of it, Rosmarin works in psychiatry for the reason many people enter the medical field — it’s compelling. His work in high-stakes criminal cases is no different. “One of the reasons I do this is because, in genuine cases, it’s a way of protecting people with serious mental illness from what society has said since the 1700s in England: We don’t punish people who we don’t think are morally culpable and, therefore, aren’t legally culpable.”","The Planetary Society, dedicated to exploring the solar system and seeking life beyond Earth, asked Michael S. Gazzaniga what a human community will look like on Mars.\nTalking with the world’s leading expert on cognitive neuroscience, one of the field’s founders, and one of the greatest minds of our time, the Society wanted to know Gazzaniga’s opinions in relation to the human brain. In other words, will the brain change? Look different structurally or perform functionally differently? In other words, will the brain enable the mind on Mars differently than on Earth?\nGazzaniga’s response, as reported on the Society’s website, tells us something about Gazziniga’s 45 years of research studying the most important organ in the human body:\n“New environments are fascinating and challenging. We combine elements of the new place with different outcomes. Yet, the piece of biologic tissue that does these wonderful things is the same for all humans and hasn't changed in thousands of years. On Sunday afternoon on Mars, citizens of the New City will still love a cold beer and a good NFL game.”\nWhat makes each of us uniquely human fascinates cognitive neuroscientists such as Gazzaniga. But before they dissect the cognitive processes that contribute to our uniqueness, they study the basics of brain structure. In other words, the journey of unraveling the mind-brain connection begins with brain anatomy.\nThe following description provides a general overview of the brain’s structure.\nDifferent Regions of the Brain\nThe Cerebral Cortex\nThis is the outer covering of the brain, the one with all the bumps (gryi or the singular gyrus) and folds (sulci or the singular sulcus). The cortex has a large number of connections – 75% in total. The other 25% of connections link to other brain regions, such as the nervous system.\nSix layers of nerve cells and the pathways connecting these layers comprise the cerebral cortex.\nScientists subdivide the cerebral cortex into four lobes:\n- Frontal lobe\n- Parietal lobe\n- Temporal lobe\n- Occipital lobe\nThere are two brain hemispheres, the left and right brain hemispheres. Hence, scientists say that there are actually eight lobes, referring, for example, to the lobes as plural (two frontal lobes, two parietal lobes, etc.)\nScientists localize some functions to particular lobes. For instance, speech production or generation is generally thought to occur in the left frontal lobe. However, molecular and cellular brain research from the last decade has shown that many particular “functions” are actually more widespread in one or more brain lobes or regions, and that neuronal circuits connect these distributed functional areas.\nThe Frontal Lobes\nThe frontal lobes are more highly developed in humans than other animals and are the centers for many higher functioning aspects of behavior, such as language, planning, generating ideas, problem solving, working memory, and personality. The frontal lobes are the last part of the brain to grow to full size, with full maturity occurring late into adolescence.\nThe primary motor cortex called the M1, an area associated with planning, executing, and refining motor movements, is located in the posterior area of the frontal lobes.\nThe prefrontal cortex\nThe prefrontal cortex is the anterior area (foremost area) of the frontal lobe. It differs from other parts of the frontal cortex by having an additional layer of neurons. This area is thought to control complex cognitive behaviors, such as modeling correct social behavior, determining consequences of certain behaviors, working toward goals, and predicting outcomes.\nThe Parietal Lobes\nA strip or groove of cortex running across the top of the brain called the central sulcus separates the frontal lobes from the parietal lobes, which are located posterior to (behind) the frontal lobes.\nThe somatosensation strip or the S1 is the first area or strip of the parietal lobe, a strip of cortex that contains a map of the entire body for processing touch sensations. After the S1 strip, areas of the parietal lobes mediate the integration of sensory information, connecting touch with visual data or memory.\nVisuospatial processing, and some aspects of language and attention are also conducted in the parietal lobes. An area of the left parietal lobe also manages motor control.\nThe Temporal Lobes\nThe anterior or front part of the temporal lobes are bounded by the frontal lobe to one side, and the brain stem to the other. The upper part of the brain stem called the pons actually connects and extends from the temporal lobes. To the rear of the temporal lobes are the parietal and occipital lobes.\nThe temporal lobes are further segmented into three gyri: the superior or upper; the medial or mid; and the inferior or lower.\nAn area of the superior gyrus is the primary auditory control center, also known as Heschl’s gyrus, that receives auditory inputs from the ears. On the left side of the primary auditory center, recognition of language sounds takes place. On the right side, recognition of non-verbal sounds such as tone, rhythm, and emotion takes place.\nParts of the inferior temporal lobes contribute to the recognition of specific visual stimuli, such as faces, objects, and animals. Scientists think that the inferior temporal lobes store memories in a highly organized way.\nThe Occipital Lobes\nLying below the parietal and temporal lobes are the occipital lobes - the smallest of the four lobes. They are located at the back or rearmost part of the brain. These lobes contain the roadmap for visual processing.\nAn area called Brodmann area 17 or V1 is the primary visual cortex, residing in the medial (mid) part of he occipital lobe. This area extends to the posterior pole of the lobe. The V1 is also referred to as the striate cortex because it contains a large stripe of myelin. Areas located around the striate are called extrastriate cortex, and they mediate the perception of forms, movement, movement direction, location, and color discrimination.\nWider Areas of Brain Structure\nThe regions described above provide an overview of the brain as if visually examined from the outside. But there are also many areas inside the brain responsible for memory, perception, learning, emotions, and other cognitive systems. Most of the deep brain structures connect with multiple brain areas and regions.\nScientists use another categorization system to name large sections of the brain, calling these sections the forebrain, midbrain, and hindbrain.\nThe forebrain contains the cerebral cortex, and the basal ganglia. The basal ganglia are clusters of nuclei consisting of several interrelated structures that scientists believe are involved with motor control and movement. The amygdala is one component of the basal ganglia.\nThe forebrain also contains the limbic system, often referred to as the brain’s emotional center, controlling emotions such as fear, anger, and pleasure. The limbic system includes the hippocampus, which plays an important role in long-term memory and spatial memory. It also includes the mammillary bodies, anterior thalamic nucleus, cingulate cortex, the formix, and the entorhinal cortex.\nThe limbic works with both higher cortical brain structures as well as those of the brain stem.\nThe thalamus, responsible for relaying sensation, spatial sense and motor signals, is centrally located on top of the brainstem. It helps regulate consciousness, sleep, and alertness. Some textbooks on the brain place the thalamus in the forebrain, others place it in the midbrain, still others place it between the forebrain and midbrain.\nThe midbrain’s structures are involved with the processing of auditory and visual reflexes, such as turning the head toward a sound or moving the eyes toward a visual stimulus. These structures are called the tectum or colliculi.\nThis area of the brain contains structures outside of the cerebral cortex: the pons, medulla oblongata, and cerebellum. The spinal cord becomes the medulla oblongata as it makes its way into the cranium. This main nervous system connector carries messages between the spinal cord and brain. It also controls basic bodily functions such as respiration, heart rate, and digestive activities.\nThe pons lies right above the medulla, relaying signals from the medulla to the brain. It also links the cerebellum to the rest of the brain.\nThe cerebellum lies on the dorsal (back) part of the brainstem at about ear level. This structure controls balance, contributes to the learning of coordinated movements, and processes sensory information used by motor systems.\nThose who make a living studying the brain describe it as a passion – not a job. For those interested in probing how the brain enables the mind, or in other words, how the mind-brain connection makes humans unique among all living creatures, several career paths are possible.\nAn bachelor's or graduate degree in psychology provides an excellent background for those interested in the brain. Most research positions require a Ph.D. Contact psychology schools for more information.\nCareers that Require a Strong Background in Psychology:\n- Cognitive Neuropsychologist\n- Cognitive Neuroscientist\n- Development Cognitive Neuropsychologist\n- University teaching and research\n- School Neuropsychology Consultation\n- Neuropsychology researcher for public and private organizations\n- Neurogenetics Psychologist\n- Neuropathology Specialist\n- Behavioral Neuropsychologist\n- Behavioral Neuroscientist\n- Forensic Neuropsychologist\n- Rehabilitation Neuropsychologist\n- Dementia Neuropsychologist"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:fb22e2d9-8772-4158-8b99-1eba94606017>","<urn:uuid:e8b9f7ab-6d55-418a-b1cb-cad036060495>"],"error":null}
{"question":"¿Cuáles son las estrategias actuales de CCUS para reducir emisiones y su desarrollo histórico?","answer":"The government has prepared CCUS (Carbon Capture, Usage, and Storage) as one of four key strategies to reduce carbon emissions. Three fields are conducting CCUS trials: Gundih (targeting 3 million tons carbon prevention by 2024/2025), Sukowati (aiming for 15 million tons in 25 years), and Tangguh (targeting 30 million tons in 10 years). Historically, CCUS technology evolved from early 1990s research at University of Calgary using ammonia-based solutions, with the first large-scale project launched in 1996 at a Saskatchewan coal-fired power plant. The technology has continued developing through various capture methods including solvents, adsorption, and membrane separation.","context":["The government has prepared four strategies to reduce carbon emissions: the application of Carbon Capture, Usage, and Storage (CCUS) technology, routine flare restrictions, optimization of natural gas utilization for households and transportation, and reduction of methane emissions, it said.\n\"The government plans to implement CCUS (Carbon Capture, Utilization, and Storage) for increasing oil and gas production and store potential emissions of about 48 million carbon dioxide (equivalent),\" ESDM ministry's director of oil and gas program development, Dwi Anggoro Ismukurnianto, informed in a statement released here on Friday.\nThere are three fields that have conducted trials on the implementation of CCUS: Gundih, Sukowati, and Tangguh, he said.\nThe government is targeting to start CCUS at Gundih in 2024 or 2025 to prevent the release of an estimated three million tons of stored carbon in 10 years, he said.\nMeanwhile, Sukowati, as the prototype, will be started next year and completed in 2025, he said. It is targeted to reach its full scale in 2030, with the aim of preventing the release of an estimated 15 million tons of carbon dioxide in 25 years, he added.\n\"Tangguh field will apply CCUS in 2026, with an estimated 30 million tons of carbon dioxide in 10 years,\" he explained.\nThe second strategy is restricting the use of flares, as stipulated in the ESDM Ministerial Regulation No.17 of 2021 concerning the management of flare gas in the oil and gas industry, Ismukurnianto informed.\nIn the regulation, it is stated that the daily limit of flare used for oil fields for six months is a maximum of two MMSCFD, he said.\nRelated news: Government should design framework for biofuel to lower emissions: NGO\nFor gas fields, the daily cap on flare used for a six-month period is 2 percent of gas feeds, he noted. Meanwhile, oil and gas processing activities are not allowed to utilize flares, he said.\nAs per the rule, there is an obligation to make a plan for the utilization of flare gas in new refineries, cooperation in flare gas management, comprehensive reporting concepts, and the implementation of sanctions and awards, he said.\nThe third strategy is to optimize the utilization of natural gas for households and transportation, he said.\n\"Since 2009, the government has been building a natural gas network for households. The goal is to reduce 654 thousand tons of carbon dioxide by 2024,\" he said.\nThe conversion of kerosene to LPG is estimated to reduce 15.39 million tons of carbon dioxide by 2024, he said.\nThe use of natural gas as a public transport fuel is estimated to have prevented the release of 178 thousand tons of carbon dioxide in 2019, he said.\nThe fourth strategy is reducing methane emissions, he said. \"Currently, Indonesia has priority to build a database of greenhouse gases, including reliable methane. It is important for the government to make valid identification in methane mitigation,\" he explained.\nThe government is also open to opportunities for collaboration for reducing methane emissions globally, he added.\nRelated news: Energy Ministry, IRENA to cooperate on reducing emissions","The coal industry has been under fire in recent years due to climate change and air pollution concerns. However, there’s still a strong demand for coal as a source of energy, which means companies are searching for ways to create cleaner coal. Here are five next-generation clean coal technologies to watch.\n1. Carbon Capture and Sequestration (CCS)\nCarbon capture and sequestration (CCS) is a technology that captures carbon dioxide emissions from coal-fired power plants and sequesters them in underground rock formations, where they can’t enter the atmosphere. CCS technology is still in the early stages of development, but it has the potential to greatly reduce greenhouse gas emissions from coal-fired power plants.\nWhen exploring the history of Carbon Capture and Sequestration, it is clear that the technology did not just suddenly appear out of nowhere. Rather, it was the result of decades of research and development in the field of carbon capture. One of the earliest documented examples of CCS came from the early 1990s when a team of researchers at the University of Calgary developed a process for capturing CO2 from flue gas using an ammonia-based solution. Since then, numerous other technologies have been developed for capturing carbon dioxide from power plant emissions, including solvents, adsorption, and membrane separation. While the capture technologies have continued to evolve, the basic concept of sequestering carbon dioxide underground has remained largely unchanged. The first large-scale CCS project was launched in 1996 when a coal-fired power plant in Saskatchewan began injecting CO2 into a deep saline aquifer. Since then, dozens of other CCS projects have been implemented worldwide, with more currently in development. It is evident that CCS is a technology that has been slowly but surely evolving over time, and it is likely that it will continue to do so in the future as we look for new ways to combat climate change.\n2. Coal-to-Liquids (CTL)\nCoal-to-liquids (CTL) technology converts coal into a liquid fuel that can be used in place of gasoline or diesel. CTL fuels produce fewer emissions than traditional fossil fuels and can be used in existing infrastructure without modifications.\nCoal-to-liquids (CTL) is a process of converting coal into synthetic fuels. The process was first developed in Germany in the 1920s and was used extensively during World War II to produce fuel for the German war effort. After the war, the CTL process fell out of favor due to its high cost and environmental impact. However, with the advent of new technologies, CTL has seen a resurgence in recent years as a potential solution to the global energy crisis. When combined with carbon capture and storage (CCS) technology, CTL can help to significantly reduce greenhouse gas emissions while providing a secure and affordable energy source. As the world looks for ways to move away from fossil fuels, CTL may play an important role in meeting that goal.\n3. IGCC Power Plants\nIntegrated gasification combined cycle (IGCC) power plants gasify coal to produce synthesis gas, which is then used to generate electricity in a combined cycle plant. IGCC power plants are more efficient than traditional coal-fired ones and produce fewer emissions.\nFor centuries, people have been harnessing the power of coal to generate electricity. But as we become more aware of the damaging environmental effects of coal-burning power plants, we have been looking for cleaner, more sustainable ways to generate power. Enter the IGCC power plant. IGCC stands for Integrated Gasification Combined Cycle, a power plant that uses coal more efficiently and environmentally friendly way. First, the Coal is gasified, meaning that it is turned into a gas. This gas is then used to power a turbine, which generates electricity. The leftover heat from this process is then used to generate steam, which powers a second turbine. This two-step process makes IGCC power plants much more efficient than traditional coal-burning plants and produces fewer emissions. As we look for ways to reduce our reliance on fossil fuels and protect our environment, IGCC power plants are an important part of the solution.\n4. Dry Sorbent Injection (DSI)\nDry sorbent injection (DSI) is a technology that reduces sulfur dioxide emissions from coal-fired power plants by injecting dry sorbents, such as activated carbon or calcium hydroxide, into the flue gas stream. DSI technology effectively reduces sulfur dioxide emissions while being cost-competitive with other emission-reduction technologies.\nIn the early 1990s, the U.S. Environmental Protection Agency (EPA) began looking for ways to reduce the emissions of sulfur dioxide (SO2) and other acid gases from coal-fired power plants. One option that was considered was dry sorbent injection (DSI). DSI involves injecting a dry powder into the exhaust stream of a power plant, where it reacts with the acid gases to form a solid compound that can be removed from the flue gas.\nThe EPA conducted a number of studies on DSI, and in 1995, it issued a report that found that DSI could be an effective way to reduce SO2 emissions. The agency began working with the electric power industry to develop and test DSI systems. In 2000, the EPA issued rules requiring coal-fired power plants to install DSI systems if they could not meet certain emission limits.\nSince then, DSI has been successfully used by coal-fired power plants to reduce their SO2 emissions. In some cases, DSI has also been used to reduce emissions of other acid gases, such as hydrogen chloride (HCl) and hydrofluoric acid (HF).\n5. Oxyfuel Combustion\nOxyfuel combustion is a technology that burns coal in an atmosphere of pure oxygen instead of air. This produces a concentrated stream of carbon dioxide that can be captured and sequestered more easily than traditional combustion methods. Oxyfuel combustion technology is still in the early stages of development but has great potential for use in new coal-fired power plants.\nOxyfuel combustion is a complete process in which pure oxygen is used instead of air. The most common method of producing oxygen for oxyfuel combustion is electrolysis, which involves using electricity to split water molecules into hydrogen and oxygen. The resulting oxygen can then be used for various applications, including oxyfuel combustion. Oxyfuel combustion has several advantages over traditional combustion processes. First, it results in a complete combustion reaction, yielding a higher percentage of CO2 and water vapor. This can benefit certain industrial applications where CO2 emissions need to be minimized.\nAdditionally, oxyfuel combustion can be used to generate a pure stream of CO2, which can be captured and used for other purposes, such as enhanced oil recovery or algae production. Finally, oxyfuel combustion can be used with other technologies to create a carbon-neutral power plant. While oxyfuel combustion is not yet widely used, it has the potential to become an important tool in the fight against climate change.\nThese five next-generation clean coal technologies show promise for helping to mitigate some of the environmental impacts of this important energy source. While more research and development are needed for many of these technologies, they offer hope for a cleaner future for coal-fired power plants worldwide."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:0fe0739f-9fb8-4930-9fb4-b71c455e7ddb>","<urn:uuid:5df75f75-a628-4989-8815-57b645907000>"],"error":null}
{"question":"As someone working on migration policy, I need to understand how urbanization and migration relate - what are the key patterns in rural-to-urban movement versus international migration to cities?","answer":"Both internal and international migration heavily concentrate in urban areas. For internal migration, there is significant rural-to-urban movement as people relocate to cities primarily for economic opportunities. For international migration, cities serve as 'global gateways' - in 2015, there were 22 metropolitan areas with over 1 million foreign-born residents, including 9 in US/Canada and 5 in the Middle East. Overall urban living is becoming the norm globally, with 55% of world population in urban areas in 2017, projected to reach 60% by 2030. While most migration is internal, international migration has also increased significantly from 173 million in 2000 to 244 million in 2015.","context":["Upgrade to remove ads\nAP Human Geography Unit 2 Section 3\nTerms in this set (42)\nA permanent move to a new location\nMigration from a location\nMigration to a new location\nThe difference between the number of immigrants and the number of emigrants\nA factor that induces people to move out of their present location\nA factor that induces people to move into a new location\nA person who has been forced to migrate to another country to avoid the effects of armed conflict, situations of generalized violence, violations of human rights, or other disasters and cannot return for fear of persecution because of race, religion, nationality, membership in a social group, or political opinion\nInternally displaced person\nA person who has been forced to migrate for similar reasons as a refugee but has not migrated across an international border\nA person who has migrated to another country in the hope of being recognized as a refugee\nAn environmental or physical feature that hinders migration\nInternal migration and give an example\nA permanent move within the same country. Moving from California to Nebraska in the United States\nInternational migration and give an example\nA permanent move from one country to another. Moving from New York City to Mexico City (the United States to Mexico). Also, Asia to Europe, Asia to North America, and Latin America to North America\nInterregional migration and give an example\nA movement from one region of a country to another. Moving from the Midwest to the South in the United States\nIntraregional migration and give an example\nA movement within one region, like moving from a rural area to an urban one, or an urban one to a suburban one.\nVoluntary migration and give an example\nThe migrant has chosen to move, especially for economic improvement. A family moving to a country for better, higher paying jobs\nForced migration and give an example\nThe migrant has been compelled to move, especially by political or environmental factors. Slaves were forced to move from Africa to the Americas\nChain migration and give an example\nThe migration of people to a specific location because relatives or members of the same nationality previously migrated there. Someone's daughter moves from the United States to Vietnam so the rest of their family moves there too\nStep migration and give an example\nMigration to a distant destination that occurs in stages. People first migrating from Guatemala to Mexico to eventually end up in the United States\nRural to urban migration and give an example\nThe movement of people from the countryside to the city. Moving from Troy, Ohio to downtown Cincinnati, Ohio\nCenter of population gravity\nThe place where the average person would live based on population density across an area. For example, in the United States it is somewhere in Missouri, but as population grows in the West and South it will move in those directions.\nA symbolically relocated capital usually because of economic reasons, to move more people to the center of the land mass, on purpose, etc. An example is Brasilia\nThe growth of cities and the migration of people into them, motivated primarily by economic factors\nThe movement of people to suburban areas, motivated primarily by a desire for safer communities, better schools, and more room.\nNet migration from urban to rural areas, made possible by new advancements in technology that allow you to stay connected even in remote areas.\nThe people who can not legally migrate to the United States but choose to move there anyway\nMoney that is sent home from people who are migrants\nGuest worker program\nA program for immigrants from poorer countries to immigrate temporarily to obtain jobs\nA large-scale emigration by talented people, usually from less developed countries to more developed countries\nWhat are the five categories of push and pull factors? Give examples of each\nDemographic, economic, environmental, political, and cultural. Demographic- There are many people there for many jobs and a high economy. There aren't enough people for a good economy. Economic- They move for jobs and more money with a better economy. There aren't jobs or there aren't enough jobs to make enough money for families. Environmental- There is an ocean or it is by the lakeside. A hurricane destroyed most things on the land. Political- Certain leaders are in some areas and policies allow religious freedom. There is religious persecution with the laws of the people with power. Cultural- People are able to practice the ways of different cultures, including their own language. People are forced to learn the new language in a country\nWhich countries typically are the sources of refugees and which host refugees?\nSyria, Afghanistan, and Iraq are sources while Pakistan, Iran, and Turkey host refugees. In general countries that have major political conflicts, civil wars, or lots of ethnic/religious tension create more refugees, while countries that are geographically close to them tend to absorb most refugees. More developed countries also tend to host more refugees.\nAccording to Ravenstein, what are the characteristics of migrants?\nMost migrants relocate a short distance and remain in the same country, long-distance migrants tend to head to the centers of economic activity (big cities), and international migrants are more likely to be young adults rather than elderly people\nHow can guest worker programs promote immigration?\nGuest worker programs provide temporary jobs for poorer people to have when they move to a new country. This means more people will be able to migrate without needing a certain amount of money\nWhat are the political, economic, cultural, and environmental consequences of migration?\nPolitical- People who move into the country will be more likely to vote for candidates in the country that have the best policies involving immigration and they allow a lot of immigrants. Economic- With more immigrants and more people to work, the economy will boost because the demand goes up so the prices go up and also the amount of money everyone makes will increase. Cultural- Cultural diversity occurs which makes our country unique but also, cultural blending occurs because people will eventually have to learn English for their jobs and they will take up American culture instead of their own languages and cultures. Environmental- With more work and factories, there will be more pollution to harm the environment and more resources are needed such as trees\nWhich countries have net-in migrations?\nThe United States of America, Germany, Russia, Australia, Canada, etc\nWhich countries have net-out migrations?\nBrazil, Mexico, China, India, most countries in Africa, etc\nWhy are the characteristics of unauthorized immigrants?\nSource country: 60 percent come from Mexico, children: 1 million children came with the 11.2 million immigrants, 4.5 million kids of immigrants were born in the United States as citizens, years in the United States: 35 percent had lived here for 15 years or more, labor force: about 8 million employed immigrants, and distribution: California and Texas have the largest numbers of unauthorized immigrants\nHow do countries create laws to try and limit and restrict migration?\nThe U.S. Quota Laws first said that 2 percent of people from each country that already had some immigrants in the United States could migrate there annually in 1924. Next, 170,000 could come from the Eastern Hemisphere and 120,000 could come from the Western Hemisphere. Then, 290,000 people could come total with no more than 20,000 from each other. Now, 700,000 is the total amount of immigrants that can come annually. The National Origins Act of 1924 involved the start of the Quota acts. Building a wall is an idea of Donald Trump to keep Mexicans from entering our border.\nWhat are the intraregional migration, including urbanization, suburbanization, and counterurbanization patterns? Where are these trends the strongest?\nUrbanization is moving from rural to urban areas, suburbanization is moving from urban to suburban areas, and counterurbanization is moving from urban to rural areas. Urbanization occurs mostly in less developed countries. Suburbanization occurs mostly in developed countries. Counterurbanization happens only in extreme circumstances.\nWhat are the political challenges presented with unauthorized immigrants?\nFalling tax revenue, social services that get provided to people who don't pay federal taxes, issues with knowing who does and does not live in an area (particularly as it relates to issues with crime)\nWhat are the concerns that countries tend to have with large immigrant populations?\nAmericans and Europeans have divided attitudes toward the larger population of immigrants, especially those arriving without proper documentation. Europeans fear that cultural diversity will occur and their culture will be threatened. Americans have different views for whether it hurts or helps the economy. More people means more jobs but some people we let in could commit crimes or kill our people\nWhat are the interregional migration patterns of the United States, China, Russia, Canada, and Brazil?\nThe United States: The center of population gravity has shifted primarily westward, but south a little as well.\nRussia: the western part in Europe is highly clustered and the east of the Ural Mountains and other parts are sparsely inhabited. The Soviet Union wanted to develop the Far North but this could not happen because not many people wanted to move out there, even when getting shorter work hours, more holiday time, more money, etc.\nChina: About 100 million people have emigrated from rural areas in the interior of the country. From the center to the east coast where it is densely populated with more jobs.\nCanada: It has shifted from east to west recently. Brazil: Most people live in a string of large cities near the Atlantic Coast. Moving the capital in 1960 from Rio to Brasilia moved the patterns from the east coast to the interior\nWhat are the major patterns of U.S. immigration, including knowing the three main periods?\nSeventeenth and eighteenth centuries: Europe and Sub-Saharan Africa. Mid-nineteenth to early twentieth century: 1840s and 1850s- Ireland and Germany, 1870s- Ireland and Germany, and 1880s- Scandinavia. Late twentieth to early twenty-first century: Asia and Latin America\nYOU MIGHT ALSO LIKE...\nAP Human Geography Chapter 3 Key Issue 1 & 2\nAP Human Geo Chapter 3 Key Issue 1 & 2\nChapter 3 AP Human Geography\nOTHER SETS BY THIS CREATOR\nAP Human Geography Review\nAP Human Geography Unit 2 Section 2\nOTHER QUIZLET SETS\nUnit 2 CH 2/8 Review\nChapter 8 - Title and Title Protection\nJ201 Midterm 1","Global gateways and local governments – how cities and migration interlink\nCities are centres of demographic, social, economic and political change, and magnets for migrants from near and far. The interplay between migration and urbanization poses both challenges and opportunities for the migrants, communities, cities and governments concerned. Chief of the Demographic Analysis Branch in UN DESA’s Population Division, Jorge Bravo, explains why.\nWhat is the link between cities and migration? What are the trends and future estimates for these processes?\n“Much of the internal population movements are from rural to urban areas, or between cities. Also, cities are in many cases the leading destination of international migrants, owing to the urban concentration of economic opportunity and availability of services. A number of cities serve as “global gateways,” hosting a large proportion of international migrants, partly because of the economic attractiveness of the cities, and also because of the migration networks that are located there.\nUrban living is, increasingly, the global norm: about 55 per cent of the world population in 2017 was living in urban areas, and this figure is projected to reach 60 per cent in 2030. Most of the world’s migration is internal – within national boundaries. But the number of international migrants, or persons living in a country other than where they were born, is also significant and has increased globally from 173 million in 2000 to 244 million in 2015.”\nDo we have any specific data on migrants living in cities?\n“There are several studies on the issue. Professor Marie Price of George Washington University, for example, researched a global sample of 200 cities. She found that in 2015 there were 22 metropolitan areas with over 1 million foreign-born residents. Nine of these were in US and Canada, five in the Middle East, four in Europe, two in Australia, and one each in Africa and East Asia. Professor Price found that nearly one in every five foreign-born people inhabited one of these 22 major cities in 2015.”\nHow can we use this “gravitational pull” of cities to help migrants and spur development at the same time?\n“Cities have more infrastructure and services to receive a larger number of migrants. But the degree of integration and the extent of migrants’ contributions depend critically on government policies, also at the local level. For example, research on the residential concentration of international migrants suggests that in both the U.S. and Western Europe, the areas with high immigrant concentration tend to be politically more favourable to diversity, while the opposite is true in areas with low numbers of immigrants. On a regional level, we have examples from South America. In countries like Brazil, Argentina and Chile, national and city governments have implemented flexible measures to provide documentation, residence and work permits to migrants.”\nRecently, experts gathered at the UN to discuss sustainable cities, human mobility and international migration. What were some of their main conclusions and recommendations moving forward?\n“The experts agreed that cities are the space where internal and international migration meet. They called for more and better data to effectively manage both processes, starting with comparable definitions of what is a city and who is a migrant. The existing information shows that refugees and migrants are mostly becoming city dwellers in their host countries. At the same time, there is no evidence to indicate that the arrival of migrants increases urban poverty. The experts also discussed empowering local governments, who have no governance over migration, yet are the ones who are most exposed to its consequences. We have also heard calls for including gender and environmental considerations into the debate on urbanization and migration.”\nFor more information:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:a71a8287-ee5d-40f4-87a2-9a79db0fe4ef>","<urn:uuid:bc997b14-fdcf-4384-8a9b-8ac5afcc6653>"],"error":null}
{"question":"What are the surgical treatment options for pubic rami fractures, and what specific risks can occur during hip fracture surgical procedures?","answer":"Pubic rami fractures can be surgically treated through several fixation techniques: internal fixation with an intramedullary screw, internal fixation with a plate, or indirect stabilization with external fixation. Intramedullary screws can be placed either retrograde or antegrade through percutaneous stab wounds. As for surgical risks, hip fracture procedures can lead to several complications including blood clots, damage to nerves and blood vessels, anesthetic complications, hardware irritation, fat embolism, nonhealing of the fracture, and broken hardware.","context":["1 Introduction top\nOperative treatment of pubic rami fractures\nFractures of the pubic rami are nearly always associated with further pelvic ring injuries.\nThe treatment of these fractures is guided by the displacement and other associated pelvic ring injuries.\nIn many cases, a strong periosteum, the inguinal ligament and the pectineal ligament will provide adequate stability and no additional treatment is required.\nWith more complex, unstable pelvic ring injuries, ramus fractures may require fixation to restore stability and promote healing.\nThe decision for operative vs. nonoperative is best made by evaluating the entire pelvic ring and its stability instead of focussing only on the ramus fracture.\nFor more complex injuries, posterior reduction and fixation must precede fixation of the ramus fractures. In these cases, the anterior fixation supplements and stabilizes the posterior fixation.\nOptions for fixation\nStabilization of ramus fractures can be performed by a variety of techniques:\n- Internal fixation with an intramedullary screw\n- Internal fixation with a plate\n- Indirect stabilization with external fixation\nIntramedullary screw fixation of ramus fractures is a form of anterior fixation that provides stability and can be inserted using either open or percutaneous techniques.\nThe decision to use this technique is based on the pelvic injury pattern, soft tissue, and the surgeons comfort with the anatomy and imaging.\nPercutaneous screws insertion sites\nIntramedullary pubic ramus screws can be placed retrograde or antegrade through percutaneous stab wounds.\nRetrograde screws, from the pubis laterally, are inserted through an incision made over the contralateral pubic tubercle. C-arm fluoroscopy is essential to define the starting point and passage of drill bit and screw within the superior pubic ramus, across the fracture, and cephalad to the acetabulum. The inlet and obturator outlet views are essential for confirming proper screw placement.\nFor more lateral fractures, an antegrade screw may be inserted from a stab wound cephalad to the hip joint along the same trajectory as the retrograde screw, but in the opposite direction.\n2 Reduction top\nReduction of the pelvic ring must be achieved prior to insertion of the intramedullary screw. It is important to understand that the screw functions simply to hold the reduction but in most cases does not achieve the reduction.\nReduction can be achieved by either closed or open methods.\nClosed reduction of ramus fractures typically requires reduction of the overall pelvic ring. In cases with displaced posterior ring injuries, reduction of the posterior ring may help to indirectly reduce the ramus fractures.\nOther methods of closed reduction include percutaneous manipulation of the pelvic ring using Schanz screws. After reduction is achieved (verified by X-ray), the reduction may be held by an adjacent K-wire or external fixator during screw insertion.\nA fracture gap in the ramus may be reduced by inserting a partially threaded lag screw. As the screw is tightened, the gap is closed.\nSmall amounts of displacement may be able to be reduced using cannulated screws. In these cases, a gently curved guide wire is used to traverse the mildly displaced fracture. Once the screw is inserted over the guide wire the fracture is indirectly reduced by screw insertion.\nThe K-wire should be removed once the screw is sufficiently past the fracture line.\nThe medial aspects of the superior pubic rami are exposed using an extended Pfannenstiel-approach.\nAfter exposure, the fracture may be directly manipulated with bone holding forceps.\nBe alert for rotational malalignment of the fracture. This can be corrected by derotation by bone holding forceps.\n3 Screw insertion topenlarge\nDetermine the type of screw\nThe type of screw that is chosen for fixation depends on the fracture pattern, location, patient's size and anatomy, and surgeon's preference.\nScrews that are commonly used are:\n- 3.5 mm cortical screws\n- 4.5 mm cortical screws\n- 6.5 or 7.3 mm cancellous screws\nCannulated screws as described above are also advantageous.\nScrew entry point\nIntramedullary screws for fixation of ramus fractures may be inserted in an antegrade or retrograde fashion.\nThe choice of the direction of insertion depends on the location of the fracture and surgeon's preference. Ramus fractures that are located closer to the pubic symphysis are often easier to reduce and control using a retrograde screw. In contrast more proximal ramus fractures are better stabilized with an antegrade screw.\nChoose the entry point caudal to the pubic tubercle as lateral as possible to optimize the screw direction in respect to the fracture line.\nThe screw is directed proximal and superior to avoid the acetabulum.\nCheck the orientation\nThe direction of the screw is determined using X-ray guidance. If an open technique is used for a retrograde screw, then check the orientation with the index finger along the inner side of the pubic ramus, as illustrated.\nProximity to the hip joint\nWhether inserting a retrograde or an antegrade screw, great care must be taken to prevent the screws from penetrating the hip joint.\nThe cross section of the anatomic model shows the correct screw position in the anterior column across the fracture side.\nIt displays the close relationship of the implant to the joint.\nDrilling and length measurement\nThe type of drill used will depend on the diameter of screw chosen. The drill is observed using X-ray guidance as it is carefully advanced. Unless the bone is very hard, only the near cortex needs to be drilled, as the screw will advance through the cancellous bone by itself.\nMultiple X-ray views are obtained intraoperatively to ensure that the screw does not penetrate the acetabulum and remains safely within the bone (inlet view, obturator oblique view).\nThe inlet view is used to ensure that the drill and later the screw do not penetrate the inner cortex of the ramus.\nThe obturator oblique view is used to ensure that the drill and later the screw do not penetrate the acetabulum.\nA depth gauge could be used to measure the length of the screw.\nWhen cannulated screws are used, the guide wire is used to determine the length and the position of the screw.","Hip Fracture ORIF\nWhat is a Hip Fracture?\nA hip fracture is a break that occurs near the hip in the upper part of the femur or thighbone. The thighbone has two bony processes on the upper part - the greater and lesser trochanters. The lesser trochanter projects from the base of the femoral neck on the back of the thighbone. Hip fractures can occur either due to a break in the femoral neck, in the area between the greater and lesser trochanter or below the lesser trochanter.\nWhat does ORIF mean?\nOpen reduction and internal fixation (ORIF) is a surgical technique employed for the treatment of a fracture to restore normal anatomy and improve range of motion and function.\nThe hip joint is a “ball and socket” joint. The “ball” is the head of the femur or thigh bone, and the “socket” is the cup-shaped acetabulum. It enables the upper leg to bend and rotate at the pelvis. The joint surface is covered by a smooth articular surface that allows pain-free movement in the joint.\nCauses of Hip Fractures\nHip fractures are most frequently caused after minor trauma in elderly patients with weak bones and by high-energy trauma or serious injuries in the young. Long term use of certain medicines, such as bisphosphonates to treat osteoporosis (a disease causing weak bones) and other bone diseases, increases the risk of hip fractures.\nSigns and Symptoms of Hip Fractures\nThe signs and symptoms of hip fractures include:\n- Pain in the groin or outer upper thigh\n- Swelling and tenderness\n- Discomfort while rotating the hip\n- Shortening of the injured leg\n- Outward or inward turning of the foot and knee of the injured leg\nMost often your physician can determine that you have sustained a hip fracture based on the abnormal position of your hip and leg and your symptoms. Your physician normally will order an X-ray to confirm the fracture and its position. If the X-ray fails to reveal the fracture, then an MRI or bone scan may be ordered to confirm the presence of a hairline fracture.\nPreparation for ORIF Surgery\nSince ORIF is often employed to treat severe fractures, it typically takes place as an emergency procedure. Prior to surgery, you may have:\n- A physical exam to inspect blood circulation and nerves affected by the fracture\n- X-ray, CT scan, or MRI scan to assess surrounding structures and broken bone\n- Blood tests\n- Depending on the type of fracture you have sustained, you may be given a tetanus shot if you are not up-to-date with your immunization\n- A discussion with an anesthesiologist to determine the type of anesthesia you may undergo\n- A discussion with your doctor about the medications and supplements you are taking and the need to stop any\n- A discussion about the need to avoid food and drink past midnight the night prior to your surgery\nTreatment for Hip Fractures\nOpen reduction and internal fixation is a procedure most commonly used to treat severe hip fractures.\nThe surgery is performed under sterile conditions in the operating room under general anesthesia.\n- After sterilizing the affected area, your surgeon will make a cut through the skin and muscle of the thigh.\n- Your surgeon will locate the fracture by carefully sliding in between the muscles of the thigh.\n- Your surgeon will put the fragments of your femur back into position (reduction).\n- Your surgeon will secure the fragments of the femur to each other (fixation) by using metal plates, screws, wires, or pins.\n- For a fracture in the middle of your femur, your surgeon may utilize a specially designed metal rod that passes through the middle of the bone and screws into both ends of the bone.\n- Your surgeon may also carry out any other repairs if required.\n- After securing the bone, your surgeon will close the incisions by suturing or staples and cover with sterile dressings.\nPost procedure, you may notice significant pain and pain medication will be prescribed by your physician to keep you comfortable. You may need to take a blood thinner to prevent blood clots. To ensure that the surgery was successful, you will probably have X-rays done.\nYou will be given instructions on weight-bearing activities and posture management. You will be instructed about dressing and incision care. You will also be advised on diet and supplements high in vitamin D and calcium to promote bone healing. Physical therapy and an exercise regimen are suggested to restore muscle strength, flexibility, and range of motion.\nDepending on your health condition and the extent of the injury, you may be able to go home the same day with scheduled follow-up appointments for monitoring progress and stitches or staples removal if required. Most people return to their normal activities within 4 to 6 months of the surgery.\nRisks and Complications of Hip Fracture ORIF\nAs with any surgery, some of the potential risks and complications of open reduction and internal fixation of a hip fracture include:\n- Blood clots\n- Damage to nerves and blood vessels\n- Anesthetic complications\n- Hardware irritation\n- Fat embolism\n- Nonhealing of the fracture\n- Broken hardware"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:1534526f-0b8d-4726-9d4a-409c907c9a73>","<urn:uuid:df3fd091-4fa9-4e4f-8141-ff5dafa45e55>"],"error":null}
{"question":"What's the difference between compatibility testing in software evaluation and pressure testing in materials?","answer":"Software compatibility testing checks if programs work across different operating systems, hardware devices, and other programs, while pressure testing specifically validates if pipes, tubes, fittings, or components can withstand designed internal pressure. Software compatibility focuses on cross-platform functionality, whereas pressure testing examines physical durability through non-destructive charging at specified pressures while observing for leaks or failures.","context":["Software evaluation is the process of determining if a piece of software meets the needs of its users and whether or not it has any potential for improvements. It’s important to note that this evaluation should be done before purchasing software, as there are many things you can’t tell until you test it out.\nTable of Contents\nBenefits of Software Evaluation to a Business\nSoftware evaluations can help companies make more informed decisions about their buying habits and which applications they should invest in.\nFor example, if a company wants to improve customer service, but doesn’t know anything about what type of software would best fit those needs, then they could run an application through an evaluation process to find out how well it might work for them.\nSoftware evaluations also provide insights into problems with certain programs which may need updating or fixing before release. This allows companies to avoid slowdowns in their operations when they are using faulty software.\nBenefits of Software Evaluation to an Individual\nFor individuals, evaluations are important because they help people decide which programs are the best for their needs without having to spend a ton of money or waste time by using applications that contain bugs or aren’t designed well.\nIf an individual is looking for a specific type of program but doesn’t know where to find it, then he or she can read reviews to see if it’s something that’s worth buying, or just head over to an evaluation site and have all of his or her questions answered before making a purchase.\nWays to Evaluate Software\nThere are many ways in which software can be evaluated, including hands-on testing, studying reports from other users, checking compatibility across different programs, and reading reviews written by others.\nHands-on testing means that a product is actually used to see how well it works.\nThe idea behind this type of evaluation is that no matter how good reports, compatibility checks, and reviews sound, the only way to know if an application works well or not is to try it out yourself.\nThis can be done through trial versions, which allow for a very abbreviated version of the program to be tested out. Other ways of doing hands-on testing include purchasing a full license key and downloading the program directly from its website.\nStudying reports from other users involves looking through customer feedback and seeing what problems people report and whether or not they have suggestions on how the product could be improved upon in the future.\nCompatibility checks are an important part of this type of evaluation because they show whether or not the software is compatible with different operating systems, hardware devices, and other programs that are currently being used.\nFor example, if a company is working on iPads their employees use, but wants to switch to iPhones instead, compatibility checks will help them make sure the new software works well across both types of devices before making any changes.\nReading reviews written by others can be helpful because people share what specific things they do and don’t like about certain applications. This allows for companies to get a better idea of how useful certain programs maybe without having to spend money first just to see if it’s worth buying.\nReviews can also highlight problems that might arise when using certain types of software, which may need fixing before its full release.\nThere are many ways in which applications can be evaluated. Those who are curious about what types of evaluations are used to determine if a piece of software is worth buying should check out some online reviews or read through customer feedback.","The two categories of bend-testing determine the flexibility or strength of a material.\nThis type of bend-testing determines the smallest radius that a specimen can bend without cracks forming on the outer surface. This bend test is often used to test the flexibility of welds.\nBending strength tests determine the modulus of elasticity and the strength of flat metallic samples in the form of strip, sheet, or plate.\nCharpy Impact Testing for Toughness\nCharpy Impact Testing is the application of a sudden applied load confined to a localized area of a material to determine its notch toughness or impact strength.\nThese tests determine a material’s toughness at subzero temperatures. A charpy impact test sample has a V-notch, keyhole, or U-notch machined on one side. It is supported at both ends and struck from behind the notch by a swinging pendulum of fixed mass. The energy that is absorbed by producing the fracture is the material’s impact energy. The more impact energy that a material absorbs, the more robust it is.\nThis test is often performed to determine the toughness of roll-over protection systems for farm equipment and automobiles. It is also critical in determining the ductile to brittle transition temperature of materials—essential in the failure analysis of structures at subzero temperatures. This material property is crucial for structural members such as bridge girders.\nTension testing is a routinely used method of determining material strength.\nThe test determines a material’s yield strength, ultimate tensile strength, and percent elongation (ductility). Tension testing assists in establishing if the alloy has been heat-treated to increase strength.\nTestlabs International machines tensile test specimens and is capable of performing tension tests at subzero (as low as -40 C), room, or elevated temperatures.\nA material’s hardness is a measure of the resistance of a material to surface indentation or abrasion. Hardness is a routinely measured and this parameter can indicate the strength and heat treatment of the material.\nA hardness test forces a ball, diamond cone, or pyramid into the material. The total force to the area or depth of indentation provides the measure of hardness.\nHardness testing includes:\nRockwell A, B and C Scale\nRockwell hardness testing is the most widely used method for determining hardness. It can determine the hardness of most metals and alloys, ranging from the softest materials to the hardest steels.\nVickers hardness testing is a form of micro-hardness testing, where a Vickers indenter (square-based pyramidal diamond) is forced into the test surface of the material using loads ranging from 1 to 1000g.\nMacro-hardness testing utilizes an applied force much higher than a micro-hardness test. Similar to Micro Vickers hardness testing, a pyramidal diamond indenter is forced into the test surface using loads ranging from 1 to 100 kilograms.\nOn-site Hardness Testing (Portable Krautramer)\nOn-site hardness testing is performed on components and structures that cannot be removed from service for laboratory testing. Hardness measurements are calculated based on the rebound of the indenter from the test surface. For this reason, this test method has limitations due to the mass and thickness of the test sample.\nCyclic and Constant Pressure Hydraulic\nPressure testing is used to validate that a pipe, tube, fitting, or component can withstand a designed internal pressure with a margin of safety. Pressure testing consists of non-destructively charging the test sample to a specified, constant pressure, and held for a predetermined duration. At this pressure, the test sample is observed for leaks or other indications of failure.\nPressure testing can also consist of cyclic fatigue testing to determine if the sample can withstand repeated pressure variations without fatigue and failure. This test is valuable as an accelerated evaluation to simulate the life cycle of the test component."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:9bd4ce04-c56d-43d0-beff-ddac4d5d64a7>","<urn:uuid:ade0270e-2622-4576-b571-1a5066b149b9>"],"error":null}
{"question":"How has the manufacturing process of Linear Alkyl Benzene evolved in terms of catalyst usage?","answer":"Historically, hydrogen fluoride catalyst was extensively used in the production of Linear Alkyl Benzene, but it has now been replaced with the DETAL process which uses a solid-state catalyst. This replacement was due to disposal problems and environmental issues associated with the hydrogen fluoride catalyst. The DETAL process allows for more efficient results.","context":["The global Linear Alkyl Benzene market is forecasted to grow at a rate of 4.67% from USD 8.82 Billion in 2019 to USD 12.69 Billion in 2027. Linear Alkyl Benzene (LAB) is produced by treating kerosene, which produces ethylene, which is then partially polymerized to give linear alkanes. On reaction with benzene in the presence of catalysts such as hydrogen fluoride and aluminum, chloride gives off linear Alkyl Benzene. Historically hydrogen fluoride catalyst was extensively used in production, which has now been replaced with the DETAL process.\nThe replacement is due to the disposal problems associated with hydrogen fluoride catalyst, and the environmental issues it has. In the DENTAL process, a solid-state catalyst is used; the process allows for more efficient results.\nLinear Alkyl Benzene is primarily used as a raw material in the production of linear Alkyl Benzene sulfonate (LAS), which is a surfactant employed in detergents. It is among the most extensively used ingredient in liquid and powder detergents. The increasing demand for industrial cleaners and household application of cleaners will help themarket garner growth over the forecast period. The LAS, which is among the main application products of the linear Alkyl Benzene market, is entirely biodegradable and does not leave a trace in the environment, which will help the linear Alkyl Benzene market maintain sustainable growth over the forecasted time frame. The rising awareness among the ordinary people about the positive effects of cleanliness will help the linear Alkyl Benzene market penetrate the household cleaning sector. Almost entire Linear Alkyl Benzene, which is produced, is sulfonated and used in the production of LAS. The raw materials for LAB, kerosene, and benzene, are supplied by significant petrochemical players such as ExxonMobil, Shell, and Sasol. Sasol and Chevron cut the supplier\\'s cost by producing linear Alkyl Benzene. The growing value of personal care and hygiene in the Asia Pacific region will help the linear Alkyl Benzene market gain massive demand due to the high population. The rising number of viral and bacterial diseases will increase the demand for Linear Alkyl Benzene in the household cleaning sector. The lack of personal or domestic hygiene results in diseases such as food poisoning, gastroenteritis, diarrhea, pneumonia, trachoma, and skin infections. The concerns regarding personal hygiene and cleanliness being a standard norm in restaurants will help the growing demand for detergents and cleaners. The demand for linear Alkyl Benzene is a direct influence of the demand in the fast-moving consumer goods industry.\nThe prices of raw materials are a significant influence on the trends in the LAB industry. The raw materials of linear Alkyl Benzene are petrochemically derived benzene and hydrogenated kerosene. The fluctuating price of crude oil mostly impacts the linear Alkyl Benzene market, and it acts as a restraining factor to the industry\\'s growth. The increasing shift toward bio-based alternatives as raw material will reduce the dependence on petrochemicals-based linear Alkyl Benzene, which will become a positive for the linear Alkyl Benzene market.\nThe increasing shift from the conventional soaps and detergents to liquid soaps and detergents will be a positive step for the linear Alkyl Benzene market. The liquid detergents have more surfactants per wash load than powder detergents. The higher cost of these liquid soaps and detergents will present as an opportunity for the linear Alkyl Benzene market.\nBased on the application, the linear Alkyl Benzene market has been segmented into Linear Alkyl Benzene Sulfonate (LAS), and Non-Surfactant. Linear Alkyl Benzene is used in the manufacture of Linear Alkyl Benzene sulfonate, which is extensively used in the detergents and cleaners and is biodegradable. LAS can be produced in the powder also and does not affect enzyme stability. However, LAS is a prevalent water pollutant and is observed to have been found in the skin of fish fingerlings. The increasing focus of R&D on biodegradation methods will help in the sustainability of the segment. The LAS segment will grow at a CAGR of 4.77%.\nThe non-surfactant applications are used as a liquid scintillator, as it offers good opacity, less radioactive impurities, and excellent light yield.\nEnd-Use Industry Outlook:\nLinear Alkyl Benzene enjoys high demand from the Heavy-duty laundry liquids industry, which results in the industry holding a significant share in the linear Alkyl Benzene market. The Linear Alkyl Benzene is used in the detergents on account of cost/performance ratio and versatile nature. LAB is regarded among the safest anionic surfactants and poses no threat to human health and the environment. The increased preference for liquid detergents will help the Heavy-duty laundry liquids industry have a stronghold.\nLaundry powders will also grow at a lucrative rate due to their low cost compared to liquid detergents. The segment will grow at a CAGR of 4.68%.\nThe growing use of light-duty dish-washing liquids in handwashing of dishes, laundering of delicate fabrics, and many automotive applications, will help the linear Alkyl Benzene market gain strength over the forecast period. The increasing use of automatic dishwashers will promote the demand for linear Alkyl Benzene in the light-duty dish-washing liquid segment.\nAlthough household and industrial cleaners segment saw little application, with growing concerns regarding domestic hygiene with new pathogenic and viral diseases will help the use for Linear Alkyl Benzene sulfone detergents in the cleaning industry, and it will grow at a CAGR of 4.70%.\nThe emerging economies present growth opportunities in the forecast period. Asia Pacific is expected to hold a significant share and will be the fastest-growing region. Due to their high population, India and China have large amounts of heavy-duty laundering activities; this is expected to propel the linear Alkyl Benzene market in the region. The growing personal care industry in China, Japan, and India will result in an extensive application of linear Alkyl Benzene. Soap, washing powder, and detergent market in China have grown by 10% in the past six years; the growth will add to the demand for linear Alkyl Benzene. The beauty sector has evolved in the region due to the skincare adoption by men, representing growth in the personal care sector.\nNorth America is another large market with significant demand for personal care, and household detergents will result in steady growth over the forecast period. The detergent industry has 630 establishments and has an annual revenue of USD 25 billion. The heavy-duty laundry segment has a stronghold in the region.\nThe European market will garner steady growth over the forecast period—Germany\\'s soap and detergent industry. The increased consumption of cleaning products will result in the demand of Linear Alkyl Benzene in the region.\n- Asia pacific present growth opportunities for the Linear Alkyl Benzene market, Indian Swachh Bharat Abhiyan will allow for the flourishing cleaning industry. India is a major producers of the world, with a consumption of 800 grams per capita. The large population in rural areas will help the Asia Pacific market. Asia Pacific is expected to grow at a CAGR of 5.11%.\n- The increasing focus on cleanliness owing to the rising effects of Covid-19 is helping the demand of detergents and soaps. As \\\"washing hands\\\" has become one of the only ways to fight the pandemic. However, covid-19 has affected global supply chains and has influenced the consumption of consumer goods. The stay at home rules will increase the demand for household cleaning products and positively impact the linear Alkyl Benzene market. The economic slowdown as an aftereffect of the pandemic will slow down the consumption of these products and will have an adverse effect on the linear Alkyl Benzene market. The economic downturn will affect emerging economies like India the most. It is imperative that strict economic reforms are put in place by the government, and prioritize the recovery of the economy.\nCompanies considered and profiled in the market study\nChevron Phillips Chemical Company, Honeywell International Inc, Cepsa, Deten Quimica S.A., Huntsman International LLC., Reliance Industries Limited, Jingtung Petrochemical Corp., SASOL, Qatar Petroleum, and JXTG Nippon Oil & Energy Corporation, have been profiled in the report. They are the major manufacturers of the product.\nThe players operating in the linear Alkyl Benzene market have undertaken several strategies related to the market in the period. The key highlight is:\n- In April 2017, Thai oil built an Alkyl Benzene plant to boost the demand for Alkyl Benzene in the region.\nMerger and acquisition is an integral part of the Linear Alkyl Benzene industry. Along with the companies operating in the linear Alkyl Benzene market, other key market players have also been entering the industry.\nReports and Data forecast promising developments for increased adoption of Linear Alkyl Benzene technology. According to industrial experts, applications relating to the market product will usher in the market and soon drive it. Moreover, new manufacturers are expected to enter the market who is likely to provide platform-centric Linear Alkyl Benzene solutions for production applications. New companies entering the industry would focus on innovating and providing through advanced technologies, thereby reducing complexity and lowering the overall total cost of operation. Experts also believe that increase in the adoption of the environment-friendly product in emerging nations would be a key development area for the industry.\nSegments Covered in the report:\nThis report forecasts revenue growth at a global, regional & country level, and provides an analysis of the market trends in each of the sub-segments from 2017 to 2027. The scope of the report can be defined as:\nFor this report, Reports and Data have segmented into the global Linear Alkyl Benzene market based on application, end-use industry, and region:\nApplication Outlook (Revenue, USD Million; Volume, Kilo Tons; 2017-2027)\n- Linear Alkyl Benzene Sulfonate (LAS)\nEnd-Use Industry Outlook (Revenue, USD Million; Volume, Kilo Tons; 2017-2027)\n- Heavy-duty laundry liquid\n- Laundry powder\n- Light-duty dish-washing liquid\n- Industrial cleaners\n- Household cleaners\nRegional Outlook (Revenue, USD Million; Volume, Kilo Tons; 2017-2027)\n- North America\n- Asia Pacific\n- Middle East & Africa\n- Latin America\nKey Question the Report Will Answer\n- At what rate will the Linear Alkyl Benzene Market grow? What will be the value of the market in 2027?\n- What are the key technologies driving Linear Alkyl Benzene?\n- What would be the impact of Linear Alkyl Benzene across different regions?\n- What are the strategies adopted by players to enter the Asia Pacific region?\n- What is the key application of the Market in the transportation sectors?\n- What are future growth strategies in the Market?"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:82b6c8e2-0d6e-42f4-bc10-133d4cf5d503>"],"error":null}
{"question":"What specialized clothing is required for high-altitude climbing, and what immediate actions should be taken if altitude sickness occurs?","answer":"Required specialized clothing includes Merino thermal layers, polar fleece pullovers, Gore-Tex waterproof jackets, down jackets with hood, mittens with Gore-tex over mitts, warm hats, glacier sunglasses, and thermal bottoms. Additionally, climbers need proper footwear including overboots with liners and sturdy hiking boots. If altitude sickness occurs, immediate actions should include stopping ascent, resting to allow the body to acclimatize, and monitoring if symptoms worsen. If symptoms persist or worsen, the best treatment is immediate descent to a lower altitude. Medical attention should be sought if severe symptoms of HACE (confusion, severe headache) or HAPE (wet cough with blood, severe fatigue) develop.","context":["Nepal Guide Treks and Expedition has short out the list of essential climbing gears and equipment required for peak climbing and mountaineering high Himalayas of Nepal. The lists of equipment required as per the climbing grade (easy to Complex) of the climbing trip are underneath:\nWe require a few sorts of gear to have each pinnacle climbing knowledge in Nepal. Here we have grouped the rundown or supplies, some you have to bring yourself and some we give. You can likewise purchase and lease your own types of gear in Kathmandu.\nList of Similar Information:\nEssential Climbing Gears and Equipment, We Provide\nEquipment and Tools you should have during your Peak Climbing in Nepal:\nDetailed Information about Essential Climbing Gears and Equipment:\n- Alpine Climbing Harness; It should be simply designed, light and easy to carry with positively foolproof features.\n- Crampons; It should be fitted with boots perfectly, still crampons with anti-balling and safely into ice.\n- Ice Axe; used to cut off the ice during climbing should be versatile light.\n- Ascender; also known as Jamar, used for ascending on a rope, should be flexible to use with gloves or mittens.\n- Head Lamp; multi led headlamp with spare batteries is essential.\n- Karabiners; requires minimum 2 locking carabineers (1 large & 1 small) and 4 regulars.\n- Rappel Device; includes Figure eight, ATC device or similar.\n- Ski Poles; Very handy for the approach; adjustable types are the best and are recommended type.\n- Slings; requires one 3m/10ft and three 2m/6ft\n- Masks, Hoses and Regulators; recommended good quality for your safety.\n- Altimeter Watch;\n- Climbing Helmets: It is an essential gear to be safe from possible icefalls and rock falls, should be light and comfortable.\nYou can also Search: Gears and Equipment List for Mera Peak Climbing\n- One T-shirt Icebreaker Merino 150 or lightweight 200.\n- Two long Icebreaker Merino 150 or lightweight 200 shirts.\n- One polar fleece pullovers, medium weight.\n- One polar fleece jacket.\n- One Gore-Tex waterproof and breathable jacket with a large hood to accommodate the climbing helmet.\n- Lightweight down jacket for chilly days in base camp or warm layer when stopping for short breaks.\n- One very warm goose-down (duvet) jacket with hood or a down/duvet suit if you prefer, for high altitude use.\n- One pair of lightweight poly-liner gloves. These will be worn when tying knots, but not inside your mitts\n- One pair mittens consist of 1 Goretex over mitt matched with the very warm polar fleece mitt liner\n- Warm hat wool or synthetic that covers your ears\n- Scarf or neck sleeve\n- Face mask\n- Ballcap or brimmed sun cap\n- Glacier Sunglass with side shields\n- One pair ski goggles (optional with light and dark lens)\n- Bandana or headscarf, useful for dusty conditions\n- Icebreaker Merino 150 underwear briefs\n- One pair of walking shorts\n- One pair of walking trousers for trekking and around camp\n- Two pair Icebreaker Merino 150 or lightweight 200 thermal bottoms\n- One pair Icebreaker Merino 200 weight thermal bottoms\n- One pair of polar fleece trousers\n- One pair of Gore-Tex trousers or bibs. Waterproof/breathable with full side zips\n- One pair of Goose-down (duvet) trousers or bibs. You may prefer a down (duvet)\n- One pair One-Sport Millet Everest Overboots or equivalent (with Aveolite liners; good quality plastic shells with inner boots; avoid tight fit with heavy socks.)\n- One pair sturdy leather or synthetic (Gortex) hiking boots with good ankle support for the walk to advanced base camp\n- One pair cross-trainers, running shoes and/or sandals for Kathmandu and in camp\n- One pair of booties (optional)\n- Two pair med-heavy poly or wool socks\n- Two Pair of liner socks. Polypropylene or wool\n- Vapour barrier liner socks or plastic bread-bags\n- Two pair lightweight trekking socks, poly or wool\n- Light Icebreaker Merino wool or cotton socks for in town.\n- Travel and Sleeping Gear\n- Rucksacks and Travel Bags:\n- One medium rucksack (50-70 litters / 3000-4500 cubic inches, can be used for aeroplane carry).\n- Two large (120 L / 7500 cubic inches) duffle kit bags for clothing and equipment. Must be durable for use on pack animals.\n- Small padlocks for duffel kit bags.\n- For high altitude, one down (duvet) sleeping bag (rated to –35 C (-30 F). In the high camp, you can sleep in your down (duvet) clothing inside your sleeping bag;\n- For base camp, one additional sleeping bag (good to -20 C (-5 F).\n- At least 3 closed cell foam mats for use in base camp and high altitude, which can be purchased in Kathmandu inexpensively; we do not recommend inflatable mats due to a high probability of accidental puncture.","High Altitude Climbing and What is Altitude Sickness?\nHigh Altitude climbing may bring on altitude sickness, the reaction of the body adjusting to decreasing amounts of oxygen. Normally, the higher the altitude, the less oxygen available for the body to carry on normal functions. Altitude sickness most commonly occurs from above 2,800 metres (9,200 ft) but this is different for everyone.\nThere is simply no way of knowing your own susceptibility prior to being at altitude thus it is vital you monitor your own health. Symptoms may be mild and subside/go away after a day’s rest, or if it is ignored it could lead to death.\nSymptoms of altitude sickness\nSymptoms can appear within 1-2 hours although most often appear 6-10 hours after ascent and generally subside in 1-2 days as the body adjusts to altitude. They may reappear as you continue to go higher. Symptoms usually occur gradually and can be one or a combination of the following:\n- Loss of appetite\n- Disturbed sleep or drowsiness\n- Swelling of hands, feet & face\nIf the body is unable to adjust to altitude these symptoms will persist and, if they are left untreated, altitude sickness may progress to High Altitude Cerebral Edema (HACE) or High Altitude Pulmonary Edema (HAPE). Both can be fatal if ignored.\nSymptoms of HAPE (fluid on the lungs):\n- A dry cough, developing to a wet one with blood-tinged discharge or saliva.\n- Tightness in the chest & blueness/darkness of face, lips & tongue\n- Low fever up to 38°C/100°F\n- Severe fatigue, progressing to coma\nSymptoms of HACE (fluid on the brain):\n- Severe headache symptoms not relieved by painkillers or lying down\n- Confusion, disorientation & drowsiness\n- Loss of balance or coordination\n- Blurred or double vision/retinal hemorrhage\nHow to avoid Altitude Sickness (AMS) Certain medical conditions (such as respiratory disease) or medications (such as sleeping pills) can increase the risk of altitude sickness – it is important that you inform your leader of any medical conditions or medications before ascending to altitude. You can help your body to acclimatize and avoid altitude sickness by:\n- Avoiding alcohol, tobacco and substances that can interfere with good delivery of oxygen to the body or cause dehydration.\n- Eating small, frequent meals high in carbohydrates.\n- Drinking plenty of water – at least 4 litres per day\n- Taking it easy or have a rest. Walk at a slower pace than you would at sea level and avoid overexertion.\n- Check with your health care provider to see if Diamox would be appropriate for you to take while climbing. See below.\n- If possible, don’t fly or drive to high altitude. Start below 10,000 feet (3,048 meters) and walk up. If you do fly or drive, do not over-exert yourself or move higher for the first 24 hours.\n- Hike high and sleep low. You can climb more than 1,640 feet (500 meters) in a day as long as you come back down and sleep at a lower altitude.\nTreatment Most travelers are able to successfully acclimatize by following the previously mentioned guidelines. However, there are instances where medical treatment is required. Ultimately, the best treatment for acute altitude sickness is to descend to a lower altitude. There may be times when your leader makes the decision that you or a member of your group is at risk of serious altitude sickness and for safety insists that you cannot ascend further – please respect that they are within their rights to do so and are making that decision in the best interests of your health and well-being. If you are experiencing any altitude sickness symptoms, we encourage you to discuss them with your leader straight away so you both can follow your acclimatization progress or seek the advice of a trained medical professional if necessary. Everyone will have a different perception of the severity of their symptoms, the key is to personally assess whether your symptoms are improving or worsening. If in doubt, go down!\nDiamox (Acetazolamide) allows you to breathe faster so that you metabolize more oxygen, while high altitude climbing, thereby minimizing the symptoms caused by poor oxygenation. This is especially helpful at night when respiratory drive is decreased. Since it takes a while for Diamox to have an effect, it is advisable to start taking it 24 hours before you go to altitude and continue for at least five days at higher altitude. The recommendation of the Himalayan Rescue Association Medical Clinic is 125 mg. twice a day (morning and night). (The standard dose was 250 mg., but their research showed no difference for most people with the lower dose, although some individuals may need 250 mg.) Possible side effects include tingling of the lips and finger tips, blurring of vision, and alteration of taste. These side effects may be reduced with the 125 mg. dose. Side effects subside when the drug is stopped. Contact your physician for a prescription. Since Diamox is a sulfonamide drug, people who are allergic to sulfa drugs should not take Diamox. Diamox has also been known to cause severe allergic reactions to people with no previous history of Diamox or sulfa allergies. Frank Hubbell of SOLO recommends a trial course of the drug before going to a remote location where a severe allergic reaction could prove difficult to treat. It is very important that you make yourself aware of the cause and effects of traveling at altitude, monitor your health and seek assistance accordingly.\nThe above is for information purposes only and is in no way intended to replace the advice of a trained medical professional. As such, BenefacTours Inc. is unable to accept responsibility for any loss, injury or inconvenience sustained by any person, caused by errors and omissions, or as a result of the advice and information given here."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:c850f1a1-c08b-4514-b932-f2b7a054290a>","<urn:uuid:ff43dea7-c47c-4133-bcb7-48f28c25697a>"],"error":null}
{"question":"How do pH levels compare between rainwater in the northeastern US and seawater used in desalination?","answer":"Rainwater in the northeastern United States tends to be acidic with low pH values due to the large number of cities, dense population, and concentration of power and industrial plants in the region. In contrast, seawater typically has a pH of 8, which is slightly alkaline. This difference in pH affects how substances like boron behave - in seawater at pH 8, boron exists primarily as boric acid rather than borate ions since this pH is below the pKa of 9.2 for the boric acid/borate equilibrium.","context":["Desalination Post-treatment: Boron Removal Process\nWhy should Boron be removed from drinking water?\nThe human body contains approximately 0.7 ppm of boron, an element that is not considered as a dietary requirement. Still, we absorb this element from food , because it is a dietary requirement for plants. Daily intake is approximately 2 mg. The amount of boron present in fruits and vegetables is below the toxicity boundary.\nA possible correlation exists between the amount of boron in soils and drinking water, and the occurrence of arthritis among people.\nWhy should Boron be removed from irrigation water?\nBoron can be toxic at very low concentration levels. Boron concentration lower than 1mg/L is essential for plant development, but higher levels can cause problems in sensitive plants. Most plants exhibit toxicity problems when the concentration of boron exceeds 2mg/L (see table below).\nSource: Extracted from the Australian Water Quality Guidelines for Fresh & Marine Waters (ANZECC)\nBoron occurrence in seawater varies from 4 to 5.5 mg/L, proportionally to seawater salinity. It mainly comes from waste water treatment plants discharge, begin used in soap and detergents, as well as agricultural fertilizers.\nBoron is present in water as Boric Acid H3BO3 and borate H3BO2-. The dominant form of boron species depends on the pH of the water. The pKa of H3BO3/H3BO2- is 9.2, therefore the equilibrium H3BO3 H3BO2-+ H+ is typically towards the left at standard seawater pH 8.\nReverse Osmosis membranes are very efficient at removing charged species like the borate ion rather than neutral molecules like boric acid.\nTypical Boron removal rates at pH 8 are between 73 and 90% for standard High Rejection Seawater Reverse Osmosis membranes, depending on the water temperature. Special High Boron Removal membrane can achieve a 95% removal.\nMost of the time, high salinity seawater have high Boron content and are located in very hot climate area like the Persian Golf, the Red, the Eastern Mediterranean sea or the Caribbean Sea. At 30oC, Boron removal drops at about 78%, leaving 1.15 mg/L in the Pass-1 permeate stream. Therefore, a specific Boron Removal Process is required to achieve the 0.5 mg/L required by the WHO.\nHow can Boron removed be from desalinated water?\nDepending on the water salinity, boron concentration and temperature, two main processes are used to produce drinking water below 0.5 mg/L of Boron:\n2nd-Pass RO with caustic soda addition to raise pH around 9.5. Some of the pass-1 permeate can be by-passed in order to keep some minerals in the water. The 2nd pass RO can be made of Seawater Low Energy membranes if temperature and salinity are high or Brackish Water High rejection membrane in case of milder conditions.\nSelective Boron Ion Exchange Resin with or without by-pass, depending on the residual boron concentration needed. The selective resin must be on-site regenerated with caustic soda and hydrochloric acid. A double column system is often required to ensure a continuous production.\nLENNTECH engineers design and size the most cost-efficient Boron Removal Process that matches your water requirement.","It's Raining, It's Pouring: Chemical Analysis of Rainwater\n|Time Required||Very Long (1+ months)|\n|Prerequisites||Must be familiar with titration or be able to learn how titration is used in this experiment|\n|Material Availability||Readily available|\n|Cost||Low ($20 - $50)|\nAbstractHere is an interesting project that could be approached from several different scientific angles: Environmental Science, Weather & Atmosphere, Chemistry, or Plant Biology. You can probably think of your own variations to emphasize the scientific area that most interests you.\nThe goal of this project is to assess the water quality of rainwater collected from different geographical areas. The water quality measures used in this project are hardness, pH, and plant growth. Additional measures could be chosen to expand this project.\nAndrew Olson, Ph.D., Science Buddies\nThis project was based on:\n- Allison, J.R., 2003. \"It's Raining, It's Pouring, the Radishes Are Growing: Chemical Analysis of Rainwater for the Nation's Food Production,\" California State Science Fair Project Abstract [accessed May 9, 2006] http://www.usc.edu/CSSF/History/2003/Projects/J0902.pdf.\nCite This PageGeneral citation information is provided here. Be sure to check the formatting, including capitalization, for the method you are using and update your citation, as needed.\nLast edit date: 2018-06-14\nIs the chemistry of rainwater from different geographical regions similar or different? How does rainwater chemistry relate to that of local surface water? How is rainwater chemistry affected by large-scale weather patterns? Does rainwater chemistry affect the growth of plants? These are some of the many questions you could choose to pursue with this project.\nThis project is based on Jonathan Allison's 2003 California State Science Fair entry. Here is how Jonathan summarized his experimental procedure: \"I contacted friends and family from 11 different cities in the United States and asked them if they could help me by collecting rainwater from their city. After they collected it, they shipped it back to me. Then I tested the rainwater for hardness, using the chemical process of titration. Next I tested the rainwater for pH levels. Then I planted radish seeds in potting soil and watered each set of seedlings with rainwater from a different city. I observed, measured and recorded any growth or changes daily for seven days.\" (Allison, 2003)\nWater hardness is a measure of dissolved compounds (e.g., magnesium carbonate, calcium carbonate) in the water. These compounds can precipitate out in boilers and water heaters (scaling). Hard water makes less suds with soap and detergent, so you need to use more soap and detergent to get clothes and dishes clean with hard water. General guidelines for classification of waters are: 0 to 60 mg/L (milligrams per liter) as calcium carbonate is classified as soft; 61 to 120 mg/L as moderately hard; 121 to 180 mg/L as hard; and more than 180 mg/L as very hard (USGS, date unknown).\nFigures 1 and 2 show USGS water hardness data for the continental United States. Figure 1 is a histogram showing the mean hardness data for each of the 344 stations sampled. Figure 2 is a map of the U.S., showing the regional patterns of groundwater hardness. In both cases, the data is from 1975, but the patterns shown have proven to be stable over time.\nFigure 1. Histogram of U.S. groundwater hardness from 344 collection stations (USGS, 1975 data).\nFigure 2. Map of U.S. groundwater hardness from 344 collection stations (USGS, 1975 data).\nAcidity and alkalinity are measured with a logarithmic scale called pH. pH is the negative logarithm of the hydrogen ion concentration:\npH = −log [H+] .\nWhat this equation means is for each 1-unit change in pH, the hydrogen ion concentration changes ten-fold. Pure water has a neutral pH of 7. pH values lower than 7 are acidic, and pH values higher than 7 are alkaline (basic). Table 1 has examples of substances with different pH values (Decelles, 2002; Environment Canada, 2002; EPA, date unknown).\n|pH Value||H+ Concentration\nRelative to Pure Water\n|0||10 000 000||battery acid|\n|1||1 000 000||sulfuric acid|\n|2||100 000||lemon juice, vinegar|\n|3||10 000||orange juice, soda|\n|4||1 000||tomato juice, acid rain|\n|5||100||black coffee, bananas|\n|8||0.1||sea water, eggs|\n|10||0.001||Great Salt Lake, milk of magnesia|\n|11||0.000 1||ammonia solution|\n|12||0.000 01||soapy water|\n|13||0.000 001||bleach, oven cleaner|\n|14||0.000 000 1||liquid drain cleaner|\nFigure 3 shows a map of the average pH of precipitation in the continental U.S. for the year 1992. \"The areas of greatest acidity (lowest pH values) are located in the Northeastern United States. This pattern of high acidity is caused by the large number of cities, the dense population, and the concentration of power and industrial plants in the Northeast. In addition, the prevailing wind direction brings storms and pollution to the Northeast from the Midwest, and dust from the soil and rocks in the Northeastern United States is less likely to neutralize acidity in the rain.\" (USGS, 1997)\nFigure 3. Map of U.S. annual average precipitation pH for 1992. (USGS, 1997).\nMost plants prefer soil that is near neutral pH. There are particular varieties (strawberries, azaleas and rhododendrons, for example) that prefer acidic soil. Soil pH also influences how readily available many soil nutrients are to plants.\nTerms and Concepts\nTo do this project, you should do research that enables you to understand the following terms and concepts:\n- Water hardness\nMore advanced students will also want to understand the following terms and concepts:\n- Wikipedia contributors, 2006. \"Titration,\" Wikipedia, The Free Encyclopedia. Retrieved May 9, 2006, from http://en.wikipedia.org/w/index.php?title=Titration&oldid=51987922.\n- These USGS webpages have information on patterns of water hardness of rivers and acidity in rainwater across the United States.\n- These sites explain the pH scale:\n- Environment Canada, 2002. \"Kids' Corner pH Scale,\" The Green Lane: Acid Rain, Environment Canada website. Retrieved March 14, 2006, from http://www.ec.gc.ca/acidrain/kids.html.\n- Decelles, P., 2002. \"The pH Scale,\" Virtually Biology Course, Basic Chemistry Concepts, Johnson County Community College. Retrieved March 14, 2006, from http://staff.jccc.net/pdecell/chemistry/phscale.html.\n- Review pages on moles:\nPark, J.L., 2004. \"The Mole Table of Contents,\" The ChemTeam, A Tutorial for High School Chemistry. Retrieved May 9, 2006, from http://encyclopedia.kids.net.au/page/mo/Molarity.\n- Allison, J.R., 2003. \"It's Raining, It's Pouring, the Radishes Are Growing: Chemical Analysis of Rainwater for the Nation's Food Production,\" California State Science Fair Project Abstract. Retrieved May 9, 2006, from http://www.usc.edu/CSSF/History/2003/Projects/J0902.pdf.\nNews Feed on This Topic\nMaterials and Equipment\n- This project requires planning ahead. Remember that it will take some time for your volunteers to collect samples and send them to you. You also need to allow time (at least one week) for the plant growth experiment once you have received all of the samples. Start early and make sure your volunteers send their samples in a timely manner!\n- Where to get samples? You will need to obtain rainwater samples from a wide geographical area. Consult the maps in the Introduction for historical patterns of variation. Ask friends and relatives to collect samples for you.\n- How much water do I need? Check your test kit instructions to see how much water is required for each test (usually about 5 ml). You will want to repeat your tests for each sample at least 3 times to assure that your results are consistent. So you'll need a minimum of 30 ml just for testing (best to plan on more). You will also need water for the plant growth experiment. Calculate how much water you will need for plant growth, and add 50 ml for testing purposes. This is how much rainwater each of your volunteers will have to send to you.\n- How should my volunteers collect rainwater samples? Simply putting a jar out on the lawn during a rainstorm is not going to be very efficient. In order to get enough water, your volunteers need a large catchment area. Probably the most straightforward solution is to collect water from the roof, by placing a collection jar underneath a downspout.\n- Make sure your volunteers label the water sample with the date and location from which it was collected.\n- For performing the water quality tests, the simplest method is to use a pre-packaged kit designed for testing aquarium water. There are several different brands available. You should be able to find a choice at a local pet store that sells fish. The kit will say how many water samples it will test. You should be able to find kits to test 50 samples for about $10. The kits you need for this project are:\n- For the plant growth experiment, you will need:\n- Radish seeds, (or other suitable, fast-growing seeds)\n- Small containers (peat pots or seedling trays)\n- Potting soil\n- A measuring device for dispensing water, such as a 10 mL graduated cylinder, which is available from online suppliers such as Carolina Biological, catalog #721610.\nDisclaimer: Science Buddies participates in affiliate programs with Home Science Tools, Amazon.com, Carolina Biological, and Jameco Electronics. Proceeds from the affiliate programs help support Science Buddies, a 501(c)(3) public charity, and keep our resources free for everyone. Our top priority is student learning. If you have any comments (positive or negative) related to purchases you've made for science projects from recommendations on our site, please let us know. Write to us at firstname.lastname@example.org.\n- For the water hardness and pH tests, follow the instructions that come with the water test kit. When titrating samples, it is important to mix the solution well after each drop of test solution is added.\n- For the plant growth portion of the experiment, it is important to keep all of the other growth conditions (sun exposure, soil, temperature, etc.) constant, and to vary only the source of water used for the plants. Be sure to use the same amount of water. Consult the Science Buddies resource, Measuring Plant Growth for methods you can use to quantify differences in growth.\nIf you like this project, you might enjoy exploring these related careers:\n- Does rainwater chemistry in your area vary with weather patterns? Collect samples over several weeks or months, and test the water quality. Keep track of the weather systems that produced the precipitation. Were there variations in the ultimate source of the moisture? Can you correlate these variations with changes in rainwater chemistry?\n- If you live in an urban area, is rainwater chemistry affected by smog? Check the air quality reported in the newspaper for the days that samples were collected. Do you see differences in rainwater chemistry after days with high smog compared to days with cleaner air?\n- For the samples in your study, how does rainwater hardness compare with groundwater hardness? (See Figure 2 in the Introduction, above.) How does the acidity compare to the 1992 U.S. data? (See Figure 3 in the Introduction, above.)\n- Here is a related Science Buddies project you might want to check out:\nAsk an ExpertThe Ask an Expert Forum is intended to be a place where students can go to find answers to science questions that they have been unable to find using other resources. If you have specific questions about your science fair project or science fair, our team of volunteer scientists can help. Our Experts won't do the work for you, but they will make suggestions, offer guidance, and help you troubleshoot.\nAsk an Expert\nNews Feed on This Topic\nLooking for more science fun?\nTry one of our science activities for quick, anytime science explorations. The perfect thing to liven up a rainy day, school vacation, or moment of boredom.Find an Activity\nExplore Our Science Videos\nBuild an Infinity Mirror\nHow to Build a Brushbot\nColorful Melting Ice Ball Patterns - STEM Activity"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:d2aa1816-0f07-441b-9d7b-8eb0f6307be2>","<urn:uuid:1de76532-160c-492d-8acd-38387fa3fc13>"],"error":null}
{"question":"How does the height of Stalin's planned eighth skyscraper near Red Square compare to the Deutsche Bank Building that was damaged on 9/11?","answer":"The planned eighth Stalinist skyscraper, which was never completed, was designed to be 275 meters tall, significantly higher than the Deutsche Bank Building which stood at 157.6 meters (517 feet). However, instead of this planned skyscraper, the Rossiya Hotel was built on its foundation. The Deutsche Bank Building, originally opened as Bankers Trust Plaza in 1974, was eventually demolished after suffering extensive damage in the September 11 attacks.","context":["Brainchild of Stalin, Gothic buildings looms high\nFrom time to time, the Soviet man, raised in the spirit of atheism, needed messages from on high. Fortunately, he had the inspirational Pioneer camp Artek in the Crimea, the Exhibition of Economic Achievements in Moscow, and the famous Moscow Metro — heralds of the communist paradise on earth, which would arrive one fine day, despite all the burdens of grey socialist work days. Soon after the end of World War II, in defiance of all doubters, a grandiose new architectural project was launched. Joseph Stalin, “the father and friend of all Soviet architects” as he was called at their All-Union Congress in 1946, embarked on a colossal construction project designed to convince the Soviet people and the whole world of the victorious Soviet power’s increased self-awareness.\nIn early 1947, the Council of Ministers adopted a resolution for the construction of seven “skyscrapers”. On September 7, that same year, during the celebration of Moscow’s 800th anniversary, at exactly 1:00 pm, the first stone was laid in a special ceremony.\nThe first Moscow skyscraper was completed in 1949, followed every three-five years by another (all but one of the remaining six). Later known as the “seven sisters”, these buildings became famous as monuments of “Stalinist-Gothic” and the defining symbols of Moscow: Moscow State University (240 m.) on Sparrow Hills remained the tallest building in Europe until completion of the Exhibition Center at Frankfurt-on-Main in 1990; the Foreign Ministry; the Ministry of Transportation; two residential buildings; the Hotel Ukraine; and the youngest of the sisters, the Hotel Leningrad (135 m.). In the post-war period, these symbols of a new era, glowing with magnificence, seemed like fantastic emblems of triumph and beacons of a resurgent country proclaiming its greatness to the world. “We can!” That was their architectural slogan, trumpeted from a country that lay in ruins, a country faint from hunger, a country most of whose residents still had to live in cramped communal apartments.\nThe supervision of the skyscrapers’ construction was initially entrusted to the notorious KGB chief Lavrenty Beria (who would be executed in 1953). Beria was also in-charge of creating a Soviet atomic bomb. Thousands of prisoners from the Gulag and German prisoners of war helped in constructing these towering high-rises. A different technique was used for every building to stabilise the ground under the foundation. The costs didn’t matter. Around 2.6 billion Soviet rubles were spent on Moscow State University alone. At current exchange rates, it would amount to around $650 million, more than two billion rubles set aside to rebuild the war-ravaged Stalingrad from 1946 to 1950. Roughly the same amount was spent on the construction of the other six “sisters”.\n|206 meters is the height of the Ukraina hotel, including the 73-meter-long spire. The total area building is more than 88 thousand square meters.|\nThese monumental skyscrapers built in the style of neoclassicism surround the center of the city like a fortress wall. All were built according to one stylistic conception: a dominating central tower which, like an Aztec pyramid, narrows in stair-like stages to the top, and is flanked, in a more or less strict order, by wings. The plan of the buildings may vary, as may the lavish decorations of the towers, the statues and bas-reliefs. Each skyscraper represents a set of architectural quotations and borrowings from various styles and tendencies, from the Renaissance and baroque to Russian church architecture. Clearly, Stalin’s tastes, known for his preference for Gothic, loomed large in the minds of architects. They didn’t always manage to please him: the Foreign Ministry was not originally designed with a spire, but the dictator insisted. So as not to violate the building’s statics, a special light-weight construction had to be perched on top with supports descending five floors made the same color as the skyscraper. After Stalin’s death in 1953, the architects asked the new General Secretary, Nikita Khrushchev, for permission to undo this Stalinist act of despotism. But Khrushchev refused. He wanted the spire on the Foreign Ministry to remain as a “monument to Stalin’s stupidity”.\n|1200 paintings hung on the walls of the corridors, halls and rooms. The most famous are by Vasily Polenov and Alexan- der Deineka.|\nContemporaries were immediately struck by the resemblance of these Stalinist-Gothic monuments to certain American precursors, such as the Manhattan Municipal Building completed in 1914. Given this resemblance, it was hard to make the necessary ideological case. A dialectical explanation was required here: capitalist temples of trade were studied in detail and used as a foundation, the object being to give them a completely new meaning. The shape of the Municipal Building in New York City was determined by the high price of land; this in turn deprived many of the apartments of natural daylight since they face a dark inner courtyard. Needless to say, the Soviet man, standing at the head of all city-planning projects, deserved better.\nStalin’s death marked an end to the principle that “the eye should delight” as quickly as the “father and friend of all architects” had disposed of the Soviet avant-garde in the 1930s. Khrushchev declared war on Stalinist extremes in city planning. Now everything would be sacrificed to functional understatement. Rapid construction of mass housing had begun, leading to many city dwellers getting separate apartments for the first time in their lives. In the Russian mind, however, Stalinist skyscrapers still equal quality, while Khrushchev’s matchbox-size apartments equal quantity.\nThe construction of the Stalinist-Gothic skyscrapers was completed under Khrushchev. But the most impressive skyscraper of all (number eight), which was supposed to be located next to Red Square and soar up to 275 meters, was buried. Instead, on the foundation of this would-be administrative building, the Rossiya was built, the largest hotel in Europe. Fate was not kind either to the classical model of “Stalinist Empire” architecture: the Palace of the Soviets. The construction of this 420-m tower to be crowned with a 100-m statue of Lenin was suspended during the war. When Khrushchev learned how much it would cost to complete the construction, he supposedly said: “Better build chemical combines instead.” In place of the Palace, an enormous outdoor heated swimming pool was built. In the 1990s, the pool was replaced by the Cathedral of Christ the Savior. (The original cathedral had been pulled down by Stalin in 1931.)\nToday, the “seven sisters” have somewhat lost their old lustre. In the two residential buildings the elevator breaks from time to time while the residents, members of the intelligentsia in the first case, pilots and cosmonauts in the second, must make their peace with the fact that their new neighbors are hardly high society. The central building at Moscow State University with its marble staircases and rich interior furnishings and special aura remains very grand, but many of the rooms in the dormitory badly need repair.\nAll rights reserved by Rossiyskaya Gazeta.","Deutsche Bank Building\n|Deutsche Bank Building|\nView from the Southeast in 1997.\n|Location||130 Liberty Street, Manhattan, New York City, New York|\n|Cost||$120 million (1973 USD)|\n|Roof||157.6 m (517 ft)|\n|Design and construction|\n|Architect||Shreve, Lamb and Harmon|\n|Structural engineer||The Office of James Ruderman|\n|Main contractor||Turner Construction Company|\nThe Deutsche Bank Building was a skyscraper at 130 Liberty Street in New York City, United States, adjacent to the World Trade Center (WTC). Opened in 1974 as Bankers Trust Plaza, the building was acquired by Deutsche Bank when it acquired Bankers Trust in 1998. It was part of the skyline of Lower Manhattan. The Deutsche Bank Building was heavily damaged in the September 11 attacks in 2001 after being blasted by the avalanche of debris, ash, dust, and asbestos that spread from the collapse of the South Tower. World Trade Center Tower 5 will eventually replace the building, expanding the ground space on which the World Trade Center stands, as this land was not part of the original World Trade Center.\nThe structure has since been completely dismantled, clearing the way for the construction of 5 World Trade Center and a vehicle security center.\nThe collapse of 2 World Trade Center during the September 11 attacks tore a 24-story gash into the facade of the Deutsche Bank Building. Steel and concrete were sticking out of the building for months afterward. This was eventually cleaned up, but due to extensive contamination it was decided that the 39 story ruin was to be taken down. After the 9/11 attacks, netting was placed around the remains of the building. The bank maintained that the building could not be restored to habitable condition, while its insurers sought to treat the incident as recoverable damage rather than a total loss. Work on the building was deferred for over two years during which the condition of the building deteriorated.\nIn September 2005 human remains were found on the roof. In March 2006, construction workers who were removing toxic waste from the building before deconstruction found more bone fragments and remains. This prompted calls from victims' family members for another search of the building by forensic experts. In 2006, between April 7 to April 14, more than 700 human bone fragments were discovered in the ballast gravel on the roof. Workers sifted through the gravel to find more remains.\nThe cost of this deconstruction had steadily increased to $75 million by the Bovis Lend Lease construction company as large amounts of toxic dust associated with the collapse of the World Trade Center, asbestos, dioxin, lead, silica, quartz, polycyclic aromatic hydrocarbons, chromium and manganese had been found in the building.\nIn 2004, an agreement was announced to settle the disposition of the building and insurance claims. Later that year as part of this agreement, the Lower Manhattan Development Corporation acquired the land and commenced its deconstruction.\nAn Associated Press December 7, 2006 report indicated that the building would be dismantled. The report indicated that area residents were fearful of possibly toxic dust associated with the two towers' collapses within the building.\nDeconstruction began in March 2007. However, due to a fire on August 18, 2007, that claimed the lives of 2 FDNY firefighters, work was suspended. A city stop-work order was lifted in April 2008 and decontamination work began again in May of the same year.\nDeconstruction was originally scheduled to be completed by the end of 2008, and later by the end of 2010. In October 2009, it was announced that deconstruction of the building would finally resume.\nDelays in taking down the Deutsche Bank at Ground Zero have forced the Port Authority to roll back the expected completion date of a crucial vehicle-security center by a year, to 2013. Repeated delays at Deutsche Bank had added roughly $100 million to the cost of rebuilding the World Trade Center.\nDeconstruction of the building finished, for the most part, on January 20, 2011. The basement was deconstructed in February 2011. It was the last part of the building to be taken apart.\nMay 2007 incident\nOn May 17, 2007, work was halted after a 22-foot section of pipe being cut by workers fell 35 stories and through the roof of \"The Ten House\", home to Engine 10 and Ladder 10 of the Fire Department of New York. Two firefighters were injured by the original falling debris, although they were not struck by the pipe itself.\nAugust 2007 fire\nOn August 18, 2007 at approximately 3:40pm, a seven-alarm fire broke out on the 17th floor of the building, caused by workers' careless smoking, in violation of the building's safety rules. By this time, the skyscraper, once 41 stories, had been reduced to 26, with crews removing a floor a week. At the time of the fire, crews were removing asbestos. The fire spread in both directions, affecting a total of 10 floors. The floors were filled with a maze of protective polyethylene sheets which were designed to prevent the spread of asbestos, but which also trapped smoke making fighting the fire extremely difficult. Firefighting was additionally hampered as the building did not have a functioning standpipe, forcing firefighters to raise hoses up from the street to combat the flames. The building had not been inspected since March, when it should have been inspected every 15 days. The fire burned into the night before being extinguished. 45 Engine Companies, 30 Ladder Companies, 3 Rescue Companies, 19 Battalion Chief Units, 4 Division Chief's Units, and numerous other special and support units responded from the New York City Fire Department to combat the fire. The fire killed two FDNY firefighters, Joseph Graffagnino, 33, of Brooklyn, and Robert Beddia, 53, of Staten Island, who succumbed on the 14th floor to smoke inhalation and carbon monoxide poisoning, and injured 115 firefighters, 46 seriously enough to require medical leave. Plans to deconstruct the building continued as quickly as possible. In 2008, the Manhattan District Attorney indicted three construction supervisors and the demolition subcontractor, the John Galt Corporation.\nDemolition of the former Deutsche Bank Building finished on January 20, 2011 with the removal of the crane. Deconstruction of the first floor and the foundation was finished on February 28, 2011. The Port Authority has taken over the site, which will eventually be used as a staging area for vehicle security sweeps, and to finally build Five World Trade Center if the proposed building finds approval.\nNegotiations over the World Trade Center site concluded in April 2006 with private developer Larry Silverstein yielding his right to develop on the site designated for Tower Five to the Port Authority of New York and New Jersey in exchange for assistance in financing Towers Two, Three, and Four. Mayor Michael Bloomberg later claimed there wasn't enough demand for office space to fill the five towers included in the World Trade Center's master plan and called for a major revision of the plan to include housing and hotels. But the demand for Manhattan office space, including downtown, has boomed since 2006, and January 16, 2007, financial giant JPMorgan Chase was in talks with the Port Authority about developing the new 5 World Trade Center skyscraper into a corporate tower, which would prevent Mayor Bloomberg's push to use the site for housing.\nSeveral sources familiar with JPMorgan Chase's talks with state and Port Authority officials said the firm's bid to develop the proposed 57-story tower for major corporate tenants is being taken seriously by the bistate agency. In fact, sources said, JPMorgan Chase is not the only corporate bidder for the site.\nTo make the commercial deal work, the Port Authority would have to agree to expand the size of Tower 5's base in order to accommodate the large, lower-level floors needed for trading rooms, a requirement for most large financial-services firms.\nOn June 14, 2007, Bloomberg and then-Governor Eliot Spitzer announced that JPMorgan Chase had won the bid to buy and build the new tower at 130 Liberty Street to replace the Deutsche Bank Building.\nHowever, after the acquisition of Bear Stearns by JPMorgan Chase in March 2008, the future of 130 Liberty Street has been put into question as JPMorgan Chase has announced that it intends to move into Bear Stearns' old headquarters at 383 Madison Avenue. If JPMorgan Chase does not renew their bid, the site would likely be used for a residential tower, as per Bloomberg's plan prior to JPMorgan Chase's bid. Recently community and civic leaders met to discuss the site's future with community leaders favoring a hotel or residential development and outgoing deputy mayor Robert Lieber favoring an office tower.\nOn October 14, 2011, the Port Authority of New York and New Jersey, which controls rebuilding at Ground Zero, and St. Nicholas Greek Orthodox Church announced an agreement that allows the church to build a 4,100-square-foot church and interfaith bereavement center at 130 Liberty Street.  The original church, which was located at 155 Cedar Street and founded by Greek immigrants in 1916, was the only religious building to be completely destroyed during the 9/11 attacks.\n|Wikimedia Commons has media related to Deutsche Bank Building.|\n|Wikimedia Commons has media related to 2007 Deutsche Bank Building fire.|\n- Collapse of the World Trade Center\n- Health effects arising from the September 11, 2001 attacks\n- Verizon Building\n- 90 West Street\n- Varchaver, Nicholas (March 20, 2008). \"The tombstone at Ground Zero\". CNN. Retrieved 2011-04-11.\n- Lower Manhattan Development Corporation. \"Frequently Asked Questions - 130 Liberty Street and Findings of Potential Human Remains\". Retrieved 2006-09-12.\n- Dunlap, David W. (2005-09-12). \"Bovis is Awarded Deal to Demolish a Tainted Tower at Ground Zero\". The New York Times. Retrieved 2011-01-09.\n- Governor's Press Releases (2004-02-27). \"GOVERNOR PATAKI: DEUTSCHE BANK SETTLEMENT REACHED\". New York State. Retrieved 2006-09-12.[dead link]\n- \"New York's Toxic Tower\". The New York Times. August 25, 2007. Retrieved May 1, 2010.\n- \"World Trade Center Reconstruction Delayed to 2013\". Fox News. April 3, 2010.\n- http://www.renewnyc.com/plan_des_dev/130Liberty/progress_reports.asp February 2011 Deconstruction Progress Report\n- Greg B. Smith (May 17, 2007). \"Debris from WTC skyscraper hits firehouse\". The New York Daily News. Retrieved 2007-05-17.\n- MICHAEL FRAZIER, Tim Farnam and The Associated Press (August 20, 2007). \"Fire was 'a horror show'\". Newsday. Retrieved 2007-08-20.[dead link]\n- Ann Givings (August 27, 2007). \"Lack of plan cited in Ground Zero fire\". Newsday. Retrieved 2007-08-28.\n- \"Deutsche Investigation Finds Many More to Blame\", Downtown Express, June 26-July 2, 2009, p. 3\n- \"10 Years After 9/11, Deutsche Bank Tower Vanishes\". NYTimes. 12 January 2011. Retrieved 11 September 2012.\n- \"Daily Activities\". Lower Manhattan. March 4, 2011. Retrieved 2011-03-23.\n- Dunning, Matt Dep. Mayor: 130 Liberty St. Should be Reserved for Office Tower\n- Kevin Eckstrom (October 15, 2011). \"St. Nicholas Greek Orthodox Church To Be Rebuilt At Ground Zero\". The Huffington Post.\n- Official announcement\n- Official website\n- Animation of its deconstruction\n- Wired NY\n- WCBS TV Ch 2\n- in-Arch.net description\n- JPMorgan Chase wins bid for 130 Liberty Street tower\n- CBS News - Fire breaks out at abandoned Deutsche Bank Building at Ground Zero (Video)\n- Smoke Over Lower Manhattan\n- Obscure Company Is Behind 9/11 Demolition Work\n- Bloomberg's Biggest Scandal—The Deutsche Bank Fire—Should Be His Downfall. Why Isn't It? by Wayne Barrett for the Village Voice July 21, 2009"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:5a096087-2fdf-4f27-badc-7c3131ff0379>","<urn:uuid:6272b0cb-304c-4e5b-89ed-092cb6e60318>"],"error":null}
{"question":"Could you compare the security features of hybrid encryption versus pure asymmetric encryption?","answer":"Hybrid encryption combines the benefits of both public-key and private-key schemes, where public-key encryption is used to encrypt a symmetric key, which then encrypts the actual message. This approach is more efficient than pure asymmetric encryption since public key operations are computationally expensive (like modular exponentiation), while symmetric key operations are usually more efficient. Pure asymmetric encryption, when used alone, is slower and requires longer key sizes (2048-bits compared to 128/256-bits for symmetric keys) to achieve equivalent security. Additionally, hybrid encryption maintains security - if properly implemented with a secure public-key scheme and secure private-key scheme, it inherits the security properties of both while improving performance.","context":["Scribed by Steve Hanna\nToday we discuss the three ways in which definitions of security given in class differ from the way they are given in the Katz-Lindell textbook,.\nThen we study the security of hybrid encryption schemes, in which a public-key scheme is used to encode the key for a private-key scheme, and the private-key scheme is used to encode the plaintext.\nWe also define RSA and note that in order to turn RSA into an encryption scheme we need a mechanism to introduce randomness.\n1. Definitions of Security\nThere are three ways in which the definitions of security given in class differ from the way they are given in the textbook. The first one applies to all definitions, the second to definitions of encryption, and the third to CPA and CCA notions of security for encryption:\n- Our definitions usually refer to schemes of fixed key length and involve parameters , while the textbook definitions are asymptotic and parameter-free.\nGenerally, one obtains the textbook definition by considering a family of constructions with arbitrary key length (or, more abstractly “security parameter”) , and allowing to grow like any polynomial in and requiring to be negligible. (Recall that a non-negative function is negligible if for every polynomial we have .)\nThe advantage of the asymptotic definitions is that they are more compact and make it easier to state the result of a security analysis. (Compare “if one-way permutations exist, then length-increasing pseudorandom generators exist” with “if is a one-way permutation computable in time , then there is a generator computable in time that is pseudorandom”)\nThe advantage of the parametric definitions is that they make sense for fixed-key constructions, and that they make security proofs a bit shorter.\n(Every asymptotic security proof starts from “There is a polynomial time adversary , an infinite set of input lengths, and a polynomial , such that for every .)\n- Definitions of security for an encryption algorithm , after the proper quantifications, involve an adversary (who possibly has oracles, etc.) and messages , ; we require\nwhile the textbook usually has a condition of the form\nThe two conditions are equivalent since\nand the absolute value in (1) may be removed without loss of generality (at the cost of increasing the complextiy parameter by one).\n- Definitions of CPA and CCA security, as well as all definitions in the public-key setting, have a different structure in the book. The difference is best explained by an example. Suppose we have a public-key scheme such that, for every valid public key , has some distinctive pattern that makes it easy to distinguish it from other ciphertexts. This could be considered a security weakness because an eavesdropper is able to see if a party is sending a message that concerns the public key.\nThis would not be a concern if the encryption mechanism were separate from the transport mechanism. For instance, if the application of this scheme occurred in such a way they two parties are securely communicating over an instant messaging client which exists in the application layer and encryption were occurring layers below in the transport layer. This abstraction of layers and separation of the encryption mechanism from the application abstracts away the notion that the public key could be encrypted with the public key. The messaging client is aware of the interface, but it never exposes the actual public or private key to the user, which prevents incorrectly using the cryptographic primitives.\nYou can show as an exercise that if a secure public-key encryption scheme exists, then there is a public-key encryption scheme that is secure according to our definition from last lecture but that has a fault of the above kind.\nThe textbook adopts a two-phase definition of security, in which the adversary is allowed to choose the two messages that it is going to try and distinguish, and the choice is done after having seen the public key. A random bit is chosen and then the ciphertext of is computed and given to the adversary. The adversary continues to have access to the Encryption function with the given public key. When the adversary is done, it outputs a guess called The output of this procedure is 1 when . A cryptosystem in which can be distinguished from other ciphertexts violates this definition of security.\n2. Hybrid Encryption\nLet be a public-key encryption scheme and a private-key encryption scheme. Consider the following hybrid scheme :\n- : same as\n- : pick a random key for , output\n- : output\nA hybrid approach to public key cryptography is often desired due to the fact that public key operations are computationally expensive (i.e modular exponentiation), while symmetric key cryptosystem are usually more efficient. The basic idea behind the hybrid approach that if we encrypt the symmetric private key with the public key and encrypt the message with the symmetric private key, only the small symmetric private key needs to be encrypted with the public key and symmetric key encryption/decryption can take place on the actual message. This allows for efficient computation of the message encryption and decryption while only using asymmetric key cryptography for transmitting the symmetric shared secret. This construction makes encryption and decryption much more efficient while still ensuring the construction has message indistinguishability and CPA security.\nTheorem 1 Suppose is -secure for one encryption and is -secure for one encryption. Suppse also that have running time . Then is -secure for one encryption.\nWe begin by assuming the conclusion of the theorem is false, that is is not -secure. Suppose there is an adversary A, that runs in time and there are two messages and such that:\nThen the definition of is applied, so\nWe then apply a hybrid argument in which the hybrid distributions have instead of . ( denotes a string of zeroes; any other fixed string could be used in the proof.) Producing:\nThis means that at least one of the following cases must happen:\nIf or are true, then it means that there is a message such that:\nThen there must exist one fixed such that\nand then we define an algorithm of complexity at most such that:\nwhich contradicts the security of .\nIf is true, then we define an algorithm of complexity at most such that:\nwhich contradicts the security of .\nThe RSA function has the same “syntax” of a public-key encryption scheme:\n- Key generation: Pick two distinct prime numbers , , compute , and find integers such that\nSet the public key to and the private key to\n- “Encryption:” given and public key , output\n- “Decryption:” given and secret key , output\nIt is a standard calculation using the Chinese remainder theorem and Fermat’s little theorem that and are permutations over , and they are one the inverse of the other.\nThis is, however, not a secure encryption scheme because it is deterministic, and it suffers from several weaknesses that can exploited in practice.\nA conjectural way to turn RSA into a CPA-secure encryption scheme is to employ it to encrypt plaintexts whose length is only about , and then pad the plaintext with random bits before applying . (Other choices for the length of the plaintext and the amount of randomness are possible, half-half is just an example.)\nThe assumption that this padded-RSA is CPA secure is a very strong one. In the next lecture we will see how to turn RSA into a CPA secure encryption scheme under the minimal assumption that is hard to invert on a random input for an adversary that knows the public key but not the secret key.","Asymmetric cryptography, also known as public key cryptography, uses public and private keys to encrypt and decrypt data. The keys are simply large numbers that have been paired together but are not identical (asymmetric). One key in the pair can be shared with everyone; it is called the public key. The other key in the pair is kept secret; it is called the private key. Either of the keys can be used to encrypt a message; the opposite key from the one used to encrypt the message is used for decryption.\nMany protocols like SSH, OpenPGP, S/MIME, and SSL/TLS rely on asymmetric cryptography for encryption and digital signature functions. It is also used in software programs, such as browsers, which need to establish a secure connection over an insecure network like the Internet or need to validate a digital signature. Encryption strength is directly tied to key size and doubling the key length delivers an exponential increase in strength, although it does impair performance. As computing power increases and more efficient factoring algorithms are discovered, the ability to factor larger and larger numbers also increases.\nFor asymmetric encryption to deliver confidentiality, integrity, authenticity and non-repudiability, users and systems need to be certain that a public key is authentic, that it belongs to the person or entity claimed and that it has not been tampered with nor replaced by a malicious third party. There is no perfect solution to this public key authentication problem. A public key infrastructure (PKI) -- where trusted certificate authorities certify ownership of key pairs and certificates -- is the most common approach, but encryption products based on the Pretty Good Privacy (PGP) model -- including OpenPGP -- rely on a decentralized authentication model called a web of trust, which relies on individual endorsements of the link between user and public key.\nHow asymmetric encryption works\nAsymmetric encryption algorithms use a mathematically-related key pair for encryption and decryption; one is the public key and the other is the private key. If the public key is used for encryption, the related private key is used for decryption and if the private key is used for encryption, the related public key is used for decryption.\nOnly the user or computer that generates the key pair has the private key. The public key can be distributed to anyone who wants to send encrypted data to the holder of the private key. It's impossible to determine the private key with the public one.\nThe two participants in the asymmetric encryption workflow are the sender and the receiver. First, the sender obtains the receiver's public key. Then the plaintext is encrypted with the asymmetric encryption algorithm using the recipient's public key, creating the ciphertext. The ciphertext is then sent to the receiver, who decrypts the ciphertext with his private key so he can access the sender's plaintext.\nBecause of the one-way nature of the encryption function, one sender is unable to read the messages of another sender, even though each has the public key of the receiver.\nExamples of asymmetric cryptography\nRSA (Rivest-Shamir-Adleman) -- the most widely used asymmetric algorithm -- is embedded in the SSL/TSL protocols which is used to provide communications security over a computer network. RSA derives its security from the computational difficulty of factoring large integers that are the product of two large prime numbers.\nMultiplying two large primes is easy, but the difficulty of determining the original numbers from the product -- factoring -- forms the basis of public key cryptography security. The time it takes to factor the product of two sufficiently large primes is considered to be beyond the capabilities of most attackers, excluding nation-state actors who may have access to sufficient computing power. RSA keys are typically 1024- or 2048-bits long, but experts believe that 1024-bit keys could be broken in the near future, which is why government and industry are moving to a minimum key length of 2048-bits.\nElliptic Curve Cryptography (ECC) is gaining favor with many security experts as an alternative to RSA for implementing public key cryptography. ECC is a public key encryption technique based on elliptic curve theory that can create faster, smaller, and more efficient cryptographic keys. ECC generates keys through the properties of the elliptic curve equation.\nTo break ECC, one must compute an elliptic curve discrete logarithm, and it turns out that this is a significantly more difficult problem than factoring. As a result, ECC key sizes can be significantly smaller than those required by RSA yet deliver equivalent security with lower computing power and battery resource usage making it more suitable for mobile applications than RSA.\nUses of asymmetric cryptography\nThe typical application for asymmetric cryptography is authenticating data through the use of digital signatures. Based on asymmetric cryptography, digital signatures can provide assurances of evidence to the origin, identity and status of an electronic document, transaction or message, as well as acknowledging informed consent by the signer.\nTo create a digital signature, signing software -- such as an email program -- creates a one-way hash of the electronic data to be signed. The user's private key is then used to encrypt the hash, returning a value that is unique to the hashed data. The encrypted hash, along with other information such as the hashing algorithm, forms the digital signature. Any change in the data, even to a single bit, results in a different hash value.\nThis attribute enables others to validate the integrity of the data by using the signer's public key to decrypt the hash. If the decrypted hash matches a second computed hash of the same data, it proves that the data hasn't changed since it was signed. If the two hashes don't match, the data has either been tampered with in some way -- indicating a failure of integrity -- or the signature was created with a private key that doesn't correspond to the public key presented by the signer -- indicating a failure of authentication.\nA digital signature also makes it difficult for the signing party to deny having signed something -- the property of non-repudiation. If a signing party denies a valid digital signature, their private key has either been compromised or they are being untruthful. In many countries, including the United States, digital signatures have the same legal weight as more traditional forms of signatures.\nAsymmetric cryptography can be applied to systems in which many users may need to encrypt and decrypt messages, such as encrypted email, in which a public key can be used to encrypt a message, and a private key can be used to decrypt it.\nThe SSL/TSL cryptographic protocols for establishing encrypted links between websites and browsers also make use of asymmetric encryption.\nAdditionally, Bitcoin and other cryptocurrencies rely on asymmetric cryptography as users have public keys that everyone can see and private keys that are kept secret. Bitcoin uses a cryptographic algorithm to ensure that only the legitimate owners can spend the funds.\nIn the case of the Bitcoin ledger, each unspent transaction output (UTXO) is typically associated with a public key. So if user X, who has an UTXO associated with his public key, wants to send the money to user Y, user X uses his private key to sign a transaction that spends the UTXO and creates a new UTXO that's associated with user Y's public key.\nAsymmetric vs. symmetric cryptography\nThe main difference between these two methods of encryption is that asymmetric encryption algorithms makes use of two different but related keys -- one key to encrypt the data and another key to decrypt it -- while symmetric encryption uses the same key to perform both the encryption and decryption functions.\nAnother difference between asymmetric and symmetric encryption is the length of the keys. In symmetric cryptography, the length of the keys -- which is randomly selected -- are typically set at 128-bits or 256-bits, depending on the level of security that's needed.\nHowever, in asymmetric encryption, there has to be a mathematical relationship between the public and private keys. Because hackers can potentially exploit this pattern to crack the encryption, asymmetric keys need to be much longer to offer the same level of security. The difference in the length of the keys is so pronounced that a 2048-bit asymmetric key and a 128-bit symmetric key provide just about an equivalent level of security.\nAdditionally, asymmetric encryption is slower than symmetric encryption, which has a faster execution speed.\nHistory of asymmetric cryptography\nWhitfield Diffie and Martin Hellman, researchers at Stanford University, first publicly proposed asymmetric encryption in their 1977 paper, \"New Directions in Cryptography.\" The concept had been independently and covertly proposed by James Ellis several years earlier, while he was working for the Government Communications Headquarters (GCHQ), the British intelligence and security organization. The asymmetric algorithm as outlined in the Diffie-Hellman paper uses numbers raised to specific powers to produce decryption keys. Diffie and Hellman had initially teamed up in 1974 to work on solving the problem of key distribution problem.\nThe RSA algorithm, which was based on the work of Diffie, was named after its three inventors -- Ronald Rivest, Adi Shamir and Leonard Adleman. They invented the RSA algorithm in 1977, and published it in Communications of the ACM in 1978.\nToday, RSA is the standard asymmetric encryption algorithm and it's used in many areas, including TLS/SSL, SSH, digital signatures and PGP.\nBenefits and disadvantages of asymmetric cryptography\nThe benefits of asymmetric cryptography include:\n- the key distribution problem is eliminated because there's no need for exchanging keys.\n- security is increased as the private keys don't ever have to be transmitted or revealed to anyone.\n- the use of digital signatures is enabled so that a recipient can verify that a message comes from a particular sender.\n- it allows for non-repudiation so the sender can't deny sending a message.\n- it's a slow process compared to symmetric crytography, so it's not appropriate for decrypting bulk messages.\n- if an individual loses his private key, he can't decrypt the messages he receives.\n- since the public keys aren't authenticated, no one really knows if a public key belongs to the person specified. Consequently, users have to verify that their public keys belong to them.\n- if a hacker identifies a person's private key, the attacker can read all of that individual's messages."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:a3ab9b24-354f-473b-87ee-c6489502822b>","<urn:uuid:d2b20f21-e9e6-4c7d-aeeb-b6a930f885e8>"],"error":null}
{"question":"How can I reduce the carbon emissions from employee travel in my company? Looking for some practical tips.","answer":"There are several effective ways to reduce carbon emissions from employee travel: 1) Implement video conferencing and web meeting strategies instead of in-person meetings, 2) Schedule individual trips together to reduce the number of departures and return trips, 3) Create an internal minimum MPG standard for vehicles rented by staff during business travel. Additionally, you can encourage alternative travel modes for employees through incentives and education programs.","context":["Tracking and reducing your organization’s carbon footprint may be voluntary today, but it’s an exceptionally good idea. The process will help you become more aware of your impacts and illuminate some less-obvious means of lowering emissions, as well as to prepare you in the event that emerging regulation mandates reporting.\nThe percentage of fossil fuels that powers your facility depends on the regional mix of sources your electricity provider uses to generate power. While your facility may be at the mercy of the grid mix, you don’t have to be a victim. Here are some techniques to help you curb facility’s reliance on fossil fuels.\nTrack – and even automate – building energy use. Much of the gains in efficiency can be made in capital improvements such as lighting upgrades or thermal system improvements. However, there are also low-cost options that focus on getting building occupants to think about the energy they are consuming – simply getting occupants to turn off lights when a space is not in use or making sure to shut down their workstations at the end of the day can add up to significant energy savings.\nImprove your energy mix by adding renewable sources. For renewable energy, you could consider your own solar, wind or other renewable energy generation source on site. Investigate local, state, and federal incentives to help defray the costs. DSIRE is an excellent place to get initial information: www.dsireusa.org. You can also work with a Power Purchase Agreement (PPA) provider that will put a low-, or no-cost hosted solar system at your site and sell the energy to you at a firm rate under contract. Over time, not only will renewable generation reduce your operational energy costs, it will protect you from a volatile commodity marketplace. Energy prices frequently spike and having access to your own resource makes your future energy costs more reliable.\nIf creating your own renewable energy isn’t right for your organization, support its larger-scale production by purchasing utility green power or renewable energy certificates (RECs). This solution goes beyond the borders of your building to shift the marketplace and ultimately provides more reliable costs and less carbon emissions on the entire grid. Your utility may offer green power or you might consider buying RECs from a Green-e certified provider. RECs specifically target the purchase of renewable energy power, such as wind, solar, hydro, geothermal, and biomass to get these sources into the electricity grids and enhance market demand.\nYour Footprint Beyond Energy\nAlthough few companies delve into the level of detail associated with less apparent carbon emissions, this is greatly underestimated territory and some progressive companies do examine these impacts. For example, Portland, OR-based Green Building Services (GBS) discovered its biggest impact was the greenhouse gas emissions from employee travel – specifically air travel. GBS now tracks all employee air travel as part of its carbon footprint – seeing the impacts of air travel has moved the organization to employ more alternate technologies such as video conferencing, Web meeting strategies, and scheduling individual trips together to reduce the number of departures and return trips, all as a way to reduce its travel footprint. Creating an internal minimum MPG standard or requirement for any vehicles rented by staff while on business travel can also help improve your organization’s emissions totals.\nThe Rose Garden Arena, the major league sports facility for the Portland Trail Blazers, also implements incentives and education surrounding alternative travel modes for employees and event attendees to reduce carbon impacts. In addition, the organization reduces, reuses, and recycles materials to lower carbon emissions associated with the embodied energy in source extraction, production, and transportation of waste materials.\nBecause facility managers know their occupants’ behavior, they are in an excellent position to lead the charge for change. Simply providing reusable mugs, cups, and clean water stations can significantly reduce a building’s waste stream and related emissions. According to the U.S. Government Accountability Office’s 2009 report on bottled water, 827,000 to 1.3 million tons of plastic PET water bottles were produced in the U.S. in 2006, requiring the energy equivalent of 50 million barrels of oil and 76% of these bottles ended up in landfills.\nBolster efforts to make double-sided copies throughout your building and reuse sheets to help lessen the 4,656 tons of office copy paper that gets transported to U.S landfills annually. Encourage people to take their bikes to work with a creative incentive.\nDifferent things appeal to different people so it’s critical to involve occupants in the creation of a variety of options so they can choose how they’d like to participate. When efforts are in place, track how they are affecting the building’s carbon emissions. All the small actions you champion for your organization will really add up. Report the findings to management and building occupants and jointly celebrate your successes.\nPatrick Nye has approximately 10 years’ experience in renewable energy and carbon. He leads business development for the new Building Insights LLC at Green Building Services. Patrick can be reached at 866.755.6565 or email@example.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:6a856f41-de0f-4530-8248-e36480bb615e>"],"error":null}
{"question":"What is personal data protection and what cybersecurity measures are needed to prevent unauthorized access to sensitive information?","answer":"Personal data protection involves safeguarding information like names, addresses, health details, and employment information that is considered sensitive personal data. It requires legal frameworks and procedural safeguards to ensure data is treated fairly and securely when stored, processed, or disclosed. Cybersecurity measures to prevent unauthorized access include implementing cyber defense systems, firewalls, intrusion detection/prevention systems, and countermeasures against malware. These protections help prevent data breaches, identity theft, and other cyber crimes that could result in significant financial losses for both businesses and individuals.","context":["Why Data Protection Matters\nIn an era of increased globalization and electronic advances, information has never been more readily available and transmittable. Businesses and international banking and financial services organizations, are increasingly exchanging personal data electronically and across borders.\nPersonal Data includes any information relating to an individual’s name, age, home address, race, sexual orientation, income, health, blood type, marital status, education, and employment information is internationally considered as Sensitive Personal Data. The result of the processing and mishandling –voluntary or involuntary- of personal data can have significant consequences, including credit card and identity theft. It is crucial that individuals’ right to privacy is protected by establishing effective data protection laws and enforcing legal safeguards to secure and protect personal data and its processing. Today governments and regulators world-wide, with the EU countries in the fore-front, are increasingly calling for measures to protect privacy and the adoption of data protection regimes to enforce such safeguards. The Office of the Data Protection Commissioner was established under the Data Protection Law of 2007 as amended by Data Protection Law Amendment Law, DIFC Law No. 5 of 2012 (Data Protection Law) as a neutral and objective body to ensure the protection of all personal information in the DIFC. The Data Protection Law and Data Protection Regulations Consolidated Version No. 2, in force on 23.12.2012 (Data Protection Regulations) (collectively the “legislation”) create a legal and procedural framework which ensure that all personal data in the DIFC is treated fairly, lawfully and securely when it is stored, processed, used, disseminated or disclosed.\nThe Data Protection Law encompasses and applies to regulated entities, including all banks and financial institutions, as well as non-regulated organizations that may process personal data to carry out their business activities.\nBenefits Of Registration For Companies\n- Registration will ensure that personal data are processed in accordance with the fundamental respect for the right to privacy and confidentiality without hindering the flow of data.\n- Requires companies (i.e. data controllers) to be transparent and open about their data processing activities as well as take necessary measures to correct the gaps in their security structures, from a business, technical and a legal standpoint.\n- One of the critical areas where data protection is relevant is business processing operations and outsourcing. Activities such as insurance claims management and payment processing activities typically handle and process large amounts of personal data. More recently offshore outsourcing has been under much scrutiny for failing to address data protection issues. With established safeguards in place, this will help in facilitating activities related to back office functions that are essential for the operation of banks and financial institutions.\nBenefits Of Registration For Individuals\n- Protects and raises awareness of individuals’ rights to privacy and confidentiality.\n- Promote openness, transparency and disclosure, in the use of personal information and helps individuals understand how their personal information is being processed by data controllers.\n- Allows individuals to request the correction of individual and/or personal information being processed.\nMission Statement – Office Of The Commissioner Of Data Protection\nThe Office of the Commissioner of Data Protection is the independent regulator set up to uphold information rights in the public interest and data privacy for individuals in or from the Dubai International Financial Centre (“DIFC”). Its mission is to uphold and protect information rights in the public interest and promote data privacy in the DIFC. In discharging its regulatory mandate, the Office of the Commissioner of Data Protection will demonstrate professionalism, independence, efficiency and leadership upholding the DIFC guiding principles of integrity, transparency and efficiency.","Cyber attacks can include attempts to access data or systems, destruction of data or systems, or interference with normal operations. Cyber defense strategies may include detection and response to attacks, prevention of attacks, and damage control after attacks.\nCyber attacks can have a wide range of consequences, from minor inconvenience to major business losses. Cybersecurity is a critical part of protecting computer networks and systems from cyber attacks. Cybersecurity includes preparing for and responding to cyber attacks, protecting data and systems, and enforcing security measures.\nA cyber defense system is a set of measures taken to protect computer networks and their data from attack.\nCyber Defense is the process of protecting information and systems from cyber attacks. Cyber Security is the practice of protecting information and systems from unauthorized access, use, disclosure, or destruction.\nCyber Security Defense is the practice of protecting information networks and systems from unauthorized access, use, disclosure, or destruction. Cyber Security Defense includes protecting data and systems from malicious actors, protecting against natural disasters and cyber attacks, and mitigating the impact of incidents.\nCyber Defense is the practice of protecting networks and systems from cyber attacks. Cyber Defense skills include the ability to identify and prevent cyber attacks, as well as the ability to respond to and recover from them.\nCybersecurity is one of the most important aspects of business today. The global economy is increasingly dependent on the internet and cyber crime has become a major threat to companies and the public. Cybersecurity is essential to protect our information and our systems from being compromised.\nCyber security is a critical part of business today. Many companies have been victims of cyber crime, and cyber crime costs businesses billions of dollars each year. Cybersecurity is not only important for companies that are targets of cyber attacks, but it is also important for companies that are simply vulnerable to cyber attacks. Cybersecurity helps protect data, protects against online fraud, and helps protect against other types of attacks.\nCybersecurity is an important part of any organization. It helps to protect against cyber attacks, which can damage or destroy data, disrupt operations, and expose confidential information. Cybersecurity also helps to protect employees and customers from identity theft and other online scams. By taking steps to protect your organization’s data and operations, you can ensure that your business remains safe and competitive.\nCybersecurity is an important issue for students, as cyber crime is on the rise. According to the National Cyber Security Alliance, cyber crime costs businesses and individuals more than $400 billion each year. Cybersecurity is not just about protecting yourself online; it’s also about protecting your data and networks. If you’re a student, make sure you’re up to date on your cyber security skills.\nCyber security professionals work to protect organizations’ information systems from cyber attacks. They use a variety of methods to detect and prevent cyber attacks, including monitoring networks, conducting research, and developing defensive strategies. Cyber security professionals must have a strong knowledge of computer security concepts and techniques, as well as the ability to analyze data and assess risks. They must also be able to communicate effectively with other members of an organization’s security team and with customers.\nCybersecurity is important in the digital world because it helps protect businesses and individuals from cyber attacks. Cyber attacks can include attacks on computer systems, data breaches, and unauthorized access to personal information. Cybersecurity can help protect businesses from financial losses caused by cyber attacks, and it can protect individuals from identity theft and other types of cyber crime.\nCyber Defense is the process of protecting computer networks and systems from unauthorized access, use, or disclosure. Cyber Defense may involve the use of defensive measures such as firewalls, intrusion detection/prevention systems, and countermeasures against viruses and other malware. Cyber Defense also may involve the detection, investigation, and prosecution of cyber crime."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:d24437ac-cba2-49d8-be40-85705188f984>","<urn:uuid:065ceb4f-e0fa-4fab-995a-43dd55e1c9a7>"],"error":null}
{"question":"Hey! Curious about industry experience - what's the difference between Ed Johnson's cabinet making training hours vs Justin Varnes' early musical training?","answer":"Ed Johnson spent around 200,000 hours training and practicing cabinet making, starting from when he first picked up his grandfather's chisels at age 18. In contrast, Justin Varnes began his musical training much earlier, at age 10, starting with the trumpet before transitioning to drums and pursuing formal education through university studies and private lessons. While both are experienced in their respective fields, Johnson specifically quantifies his extensive training hours while Varnes' journey is described through his educational progression and musical development.","context":["Design and Technology\nA Conversation with Superyacht Furniture Designer Edward Johnson\nOcean Independence is delighted to have the opportunity to sit down with UK-based superyacht furniture designer Edward Johnson to talk about the past, present and future of his work.\nA skilled artisan with over a decade of experience in his trade, luxury furniture designer Ed Johnson is no stranger to the yachting industry – today his pieces can be found on some of the world’s most well-known superyachts.\nOff the water, Ed has exhibited his bespoke furniture widely and has been recognised within the industry for achieving excellence within design, innovation, and craftsmanship. Previous accolades include the Wood Awards (bespoke furniture category) and three prestigious Bespoke Guild Marks, awarded by The Furniture Maker’s Company.\nEd has also been selected by Winch Design founder Andrew Winch as part of the ‘Under Winch’s Wing’ initiative, created to support outstanding small craftsmanship business throughout the Covid-19 pandemic.\nOI: How has the past year been for you?\nEJ: I’m looking out of the window of my design studio in the sailing village of Bosham and feeling a real sense of optimism. We are all hoping for a brighter tomorrow as we inch closer to a summer of all those experiences that have been sorely missed. It has been a cold winter in the workshop – I’ll be pleased to see the back of it! It has also been a decidedly solitary year here, as I suspect it has been for most artisans across the world. I feel so lucky to have a workshop with a view of the West Sussex countryside, where I have lived for more than 20 years, and as life begins to open up again, we can start to plan. We’ve booked our exhibition season, with local and international shows pencilled in the diary. I’m very much looking forward to talking to clients and interested folk about our latest designs and techniques.\nOI: What would you say are the key skills needed for producing a high-end cabinet?\nEJ: Malcolm Gladwell’s aphorism that it takes 10,000 hours to become an expert is a woeful underestimation when it comes to cabinet making. From the moment I picked up my grandad’s chisels at the age of 18, until the day I started to fully understand my medium and my own ability and dexterity, it was closer to 200,000 hours of training and professional practice. There’s a lot more to being a cabinet maker and craftsman than the romantic idea most people have - I say that from the mindset of being a perfectionist and my own worst critic. There have been an awful lot of expletives uttered through that time. It’s not as if you can just press the backspace button. Some mistakes set you back weeks; some you just have to live with.\nOI: Which projects bring you most pleasure?\nEJ: Of course, there are moments and projects that bring you great joy, and perhaps none more so than working on super high-end creations for yachts and luxury interiors. A standard piece takes a couple of hundred hours to make, while an amazing piece that makes the hairs on the back of your neck stand up takes a thousand hours. Fortunately, we work at the finer end of the industry and we are increasingly being commissioned to work on some extraordinary projects. These are projects that have allowed me and the team the time to focus our craftsmanship skills and cherish our position as innovators in the industry. From the family-tree boxes that take over 500 hours to finesse, to some extraordinary chairs we recently made, each one taking three months - yes, three months for each chair – we’ve been able to indulge and progress our ability as craftsmen and further explore the boundaries of furniture making. When you get to work at this level, it isn’t just a job, it’s a passion and a craft that we’re grateful to be part of. In a world full of mass-produced objects, it’s nice to be part of a traditional skill that will continue to stand the test of time because there are people out there with patronage that are willing to support such craftsmanship.\nOI: What are the biggest challenges you face?\nEJ: When I think of those 200,000 hours, in truth it’s a whole lifetime of adopting a mindset of challenging oneself and exploring beyond one’s comfort zone. My love of making is very much in my blood. Much to the frustration of my parents and teachers, I was always trying to test the boundaries of what I could do and constantly asking ‘why’ when given an instruction. My wife would say that I’m unemployable because of the number of questions I ask! A questioning nature has constantly motivated me to understand the basic principles of cabinet making in great depth. If you can deconstruct the building blocks of how timber works, moves, bends, joins etc, you can start to reconstruct it into new forms that haven’t been tried before.\nOI: Are there any innovations that you are working on at the moment?\nEJ: We’ve got an area in the workshop dedicated to failed prototypes! Currently, it hosts a lot of failed steam-bent components. Typically, I’ve favoured laminating as a process of bending timber as it is more accurate, and I find it to be more consistent. However, this doesn’t lend itself to batch-produced designs as it is very time consuming. Over the past couple of years, we’ve been mastering the technique of steam bending and very recently we’ve been playing around with the idea of double steam bending, and even steam bending and laminating within the same component. Given the number of split components upstairs, I’d say we’re very much still learning. As a seasoned woodworker, I wholeheartedly agree that there is something quintessentially romantic about steam bending; seeing a piece of 60mm x 60mm timber bend around a former like a piece of dough fills you with joy every time you see it. It is always a bit of a shame that you have to do it at such speed and under such pressure – you almost want to watch it in slow-mo. It’s like restraining a wild animal – you have to work quickly, accurately, and without panic, and then once it’s all strapped into position there is a sense of achievement and calm.\nOI: Has the pandemic led to any changes in your business?\nEJ: As the UK came to a grinding halt during the first lockdown, we did have a brief period of reflection in the business, and I tried to make sure that the team used their time wisely until we felt it acceptable to return to the workshop. We developed an Eco range that utilises all our beautiful hardwood offcuts, which has in turn created a position for a young apprentice to join the team – all round, a very satisfying result. After the initial jolt, we quickly found that the workshop was busier than ever, with our loyal patrons going out of their way to keep our team together and doing what we do best – crafting fine furniture. With our exhibition stream of work closed, private and commercial clients have been amazing in supporting and promoting us. The reputable Winch Design Studio has been championing our work alongside other artisans through its ‘Under Winch’s Wing’ initiative, which we joined last year. The more experienced I get, the more processes evolve, enabling me to deliver a complex and fantastic solution. This is a high-pressure job, but Winch gives you great clarity and the time to produce.\nOI: Would you say creativity and collaboration go together?\nEJ: I love collaborations that really do push boundaries. Much of the work we do for Winch has stretched me creatively, pushing me to create things I did not really know were possible. But through careful planning and ingenuity, we have delivered. Keeping to budgets when working on superyacht projects is crucial, but occasionally there is a project that really raises the bar and makes the struggle of creativity all the more worthwhile. For those jobs for yachts that look impossible at first, we make prototypes, which themselves can take weeks to produce. It’s a process of thinking and developing, then rethinking and developing again, so we can be sure of delivering the ultimate in woodworking. As with any customised project, we start with a blank piece of paper and find inspiration from initial discussions with the client. A table, say, needs to stand up to the rigours of use – red wine, crayons, and a dropped fork – but also meet the specific aesthetic requirements of the client.\nOI: What are you looking forward to for the rest of the year?\nEJ: There is a lot of undiscovered talent out there, and my task very soon will be gently to expand my business with additional engaged people. Artisans and woodworkers are so important to our future. I am also currently talking with artisans from all over the UK to discuss the possibilities of collective marketing and working together more closely. There is an undiscovered matrix of small, independent and talented artisans across the UK, and we all need the support of clients to develop and maintain traditional crafts for the enjoyment of all. For those that have supported us, we owe a huge thank you, and for those that are considering commissioning an artisan, please do.","A sustainable jazz club is a true gem in any community, and it requires a village of support and advice. We are fortunate to receive both from our stunning board. In their service as advisers, I ask that each of these individuals teach me at least one thing each year. We are proud to share the backgrounds of these outstanding men and women, and the areas in which they concentrate their invaluable input.\n— Tamara Fuller, Owner, The Velvet Note\nCONCENTRATION: PERFORMANCE TECHNOLOGIES\nBob Bakert’s earliest remembrances of his father were of him singing in a beautiful baritone in the style of Bing Crosby… to this day every time Bob hears Bing Crosby he can see his dad. As an adult, Bob continued his music appreciations studies as a performance major in the jazz department of Georgia State University, all the while taking private lessons in voice and guitar. Bob spent 30 years as the leader of high-end professional sound electronics enterprise, and is responsible for the extraordinary sound system that supports each artist who performs on The Velvet Note Stage.\nCONCENTRATION: NATIONAL ARTIST RELATIONS\nScotty Barnhart is a two-time GRAMMY- Award winner, acclaimed jazz trumpeter, leader, composer, arranger, educator, author and the Director of the legendary Count Basie Orchestra. Scotty has over ten recordings with the likes of Tony Bennett, Diana Krall, Ray Charles, and Tito Puente. Scotty released his debut solo CD Say It Plain on the Unity Music label. He invited Clark Terry, Ellis and Wynton Marsalis, Marcus Roberts, Jamie Davis and Etienne Charles to share his compositions and love of standards on “Con Alma”, “Giant Steps” and Barnhart’s own, “Dedicated To You”. Say It Plain achieved No. 3 in the Jazz Charts and has received celebrated reviews. When Scotty Barnhart is not touring with The Count Basie Orchestra or teaching at Florida State University, he performs with his own quintet-sextet, lectures and demonstrates at schools, colleges, universities, conferences around the world, spreading jazz music to ensure its future for others to explore and enjoy.\nLARRY AND AMANDA FICHTER\nCONCENTRATION: GUEST EXPERIENCE AND SPONSORSHIPS\nLarry and Amanda Fichter are husband-and-wife owners of Fichter Financial Group, LLC, established in 1998. Their company handles both institutional investments and personal financial advisory services. Amanda holds a certified financial planner (CFP) designation and Larry has served on the Estate Planning Council of Los Angeles.\nCONCENTRATION: MARKETING STRATEGIES\nDr. Dana Hall is an Associate Professor in the McCamish School of Business at Reinhardt University and serves as coordinator for the Reinhardt MBA Program and Faculty Senate Chair. She teaches a variety of courses that draw on both her academic credentials and corporate experience, including graduate-level courses in organizational communication, global management, research methods, and quantitative decision-making, as well as undergraduate courses in business statistics, marketing principles, digital marketing, and consumer behavior. She was honored to be awarded the 2017 Vulcan Materials Award for Teaching Excellence, chosen by her peers. In addition to a Ph.D. in Communication Studies, an M.B.A., and a B.A. in Art, she brings 25+ years of corporate experience to her teaching. She spent most of her career working in the advertising industry and was a Senior Vice President and Management Representative with McCann-Erickson Worldwide in the New York office and managed global advertising accounts that included L’Oréal and Lufthansa. While at Bernstein-Rein Advertising, she led the advertising launch team for Bayer Animal Health’s Advantage Flea Control. Her areas of research interest are focused on organizational identification and organizational commitment.\nCONCENTRATION: GRASS ROOTS MARKETING\nJohn Harris earned his Bachelor’s degree in Applied Mathematics from the University of Missouri – St. Louis and a dual Master’s degree in Management and Computer Science from Webster University in Missouri. His reputation for creative thinking combined with his technical background served him well during an extensive, 35-year career with United Parcel Service; with the majority of those years in their Engineering departments at the director level, completing his last assignment as the Corporate U.S. Hub Operations Engineering director in 2017.\nAs a fan of all types of music, John has a true appreciation for musical artists and live-music venues.\nJohn is currently working with Trivnow, a technology start-up company, as director of strategic marketing and sales.\nCONCENTRATION: NEW MARKET OPPORTUNITIES\nBill Killen is a businessman, strategist, entrepreneur, innovator, and a music lover.\nHe made his mark in the business world by combining a strategic vision of what is possible with the financial and business plans needed to implement that vision and make those dreams come true. He is currently the Principal of Intelligent Growth Group in Atlanta, Georgia.\nBill started down this path with an Electrical Engineering degree from Union College in Schenectady, New York and an MBA in Finance from Wharton School of Business.\nUpon graduation he began his career in New York City at RCA Corp as an internal auditor and business planner, and then moved to its subsidiary NBC Television. While at NBC his assignments included a stint as a Production Unit Manager on Saturday Night Live (1979), and on various Sports remotes and features. He also helped prepare bids for the rights to televise live sporting events.\nIn 1980, Bill decided to move to Atlanta to join Cox Enterprises Inc. where he spent twenty years investing in, acquiring, and creating new businesses for Cox in the fields of media, telecom, and internet. He was instrumental in positioning Cox to thrive in emerging opportunities such as telephone over cable-tv, cellular and pcs wireless phone service, internet access over cable-tv, and numerous internet web site businesses. He championed and started Cox’s Auto Trader.com which became the largest used car website on the Internet.\nWhile at Cox, Bill also invested in digital special effects pioneer Digital Domain of Venice, California. He served on the Board alongside its Founders, Hollywood luminaries Jim Cameron, Stan Winston, and Scott Ross. Digital Domain won numerous Academy Award nominations for movies such as Apollo 13, and won the Oscar for best visual effects for the movie Titanic.\nTo scratch his entrepreneurial itch, Bill left Cox and co-founded and built Blue Ridge Mountain Water Inc. from a start-up to become Atlanta’s second largest provider of bottled water service. BRMW delivered 5 gallon bottles of spring water, “Bottled at the Source” at the Company’s spring in the mountains of North Georgia. The Company grew to serve a subscriber base of over 14,000 homes and offices from a fleet of 20 delivery trucks and 65 employees. “Premium mountain spring water delivered fresh from the mountains to You!” BRMW was sold in 2008.\nSince then Bill has helped various small growing businesses, including several juice store chain startups in Charleston SC and Atlanta, an organic produce home delivery subscription service in Atlanta, and a sod farm in Ocala, Florida. He is also the Business Manager for a Contemporary Jazz Guitar recording artist and performer, Eric Essix, of Birmingham, Alabama. In his spare time, Bill noodles on his Hammond B-3 organ, and he rides his bicycle in locales all across the Southeast while attending various music concerts.\nSTEPHANE LE GOUILLE\nCONCENTRATION: GUEST SERVICE\nStephane Le Gouille hails from Brittany, France, where he attended pastry school and then went on to receive his degree in restaurant management. For the past 20 years, he has served as the Dining Captain/Maitre d’ of the acclaimed Patroon restaurant in New York City. Each year, Stephane and his restaurant team serve corporate executives, politicians, and an array of noted celebrities.\n“Advertising, location and word-of-mouth will drive guests into your living room,” he says, “but it is meticulous focus on each and every table that will keep them coming back. It is fun for me to provide guests with outstanding service and I feel grateful to be able to do this for a living, but you cannot fake your way through it. It is the real knowledge of wines, handling problems, maintenance, décor, and a service personality that mean the difference between being good and truly being viewed as world class.”\nCONCENTRATION: BRAND MANAGEMENT\nRakesh is a native of India and came to the US to pursue a master’s degree from the Wharton School of the University of Pennsylvania. After receiving an MBA, Rakesh has had an extensive career in marketing and brand management starting with Standard Brands Foods in New York. He then worked at the Hershey Chocolate Company, where he was responsible for introducing Reese’s Pieces. Next, he worked at Diageo where he managed various brands of alcoholic beverages such as The Famous Grouse Scotch Whiskey, Bisquit Cognac and Pernod.\nMost recently, Rakesh was a Director of Marketing at UPS. During his 27 year career at UPS, Rakesh held a variety of positions in US Domestic, International and Customer Technology Marketing. During this time, Rakesh also managed UPS.com, directing several redesigns to establish it as an award winning website.\nRakesh is passionate about providing customers an exceptional experience by optimizing their experiences at every point of customer interaction, whether in-person, on the phone, web or on mobile devices. He believes that businesses succeed by building relationships with their customers and delivering on their brand promise.\nRakesh’s love for jazz began at 16 while attending an engineering school in India where he listened to the Voice of America on the radio almost every night. This led to Rakesh playing alto and tenor saxophones in the college band. He also listens to Indian classical music which emphasizes improvisations and is thus similar to jazz in some respects.\nCurrently, Rakesh continues to pursue his passions in technology, entertainment and design by advising technology startups and small businesses.\nCONCENTRATION: INDUSTRY RESEARCH AND ANALYSIS\nDr. Lindsey Thornhill is a Vice President and Division Manager for Integrated Solutions for Systems, a small business engaged in a variety of engineering R&D projects. Prior to joining IS4S, Lindsey was a Senior Director for Program Management and Division Manager in the Global Markets and Missions Sector of Science Applications International Corporation. Lindsey manages a diverse staff of scientists and engineers engaged in a broad spectrum of defense-related research and development projects. He earned B.S., M.S., and Ph.D. degrees from the George W. Woodruff School of Mechanical Engineering at Georgia Tech, where he currently chairs the Woodruff School External Advisory Board and serves on the College of Engineering Advisory Board. Lindsey also serves on the the Parent Leadership Councils at both Belmont University and the University of Georgia. Lindsey has authored more than 50 publications, served as a reviewer for peer-reviewed journals, and delivered invited talks at international conferences and before the JASON Panel of the Department of Defense. He has served on dissertation reading committees, he co-directed student paper competitions at Georgia Tech and at Clark Atlanta University, and he has mentored students from a local magnet high school for the sciences.\nCONCENTRATION: MUSIC DIRECTION AND SPECIAL PROGRAMS\nJustin Varnes has served as The Velvet Note’s Music Director since the club’s doors first opened, and is one of the founders of the guiding concept “by musicians, for musicians”. Born in Jacksonville, Florida and residing in Atlanta, GA, Justin Varnes began studying music at 10 – on the trumpet. His father – an avid jazz listener – exposed him to Stan Kenton, Nat “King” Cole and Erroll Garner. After attending Douglas Anderson School of the Arts on trumpet, Justin studied drum set with Ray Charles alumnus Rick Kirkland. Then he studied music at the University of North Florida under Charlotte Mabrey, Bill Prince and Bunky Green while continuing to take privately from Rick Kirkland. After graduating with degrees in jazz studies and percussion, Justin relocated to New York City where he continued to study at the New School and study privately with Jojo Mayer. In New York, Justin began touring with Phoebe Snow, with whom he performed on the Roseanne Barr Show, as well as National Public Radio’s “World Cafe” (Justin would later perform on “World Cafe” with “Five for Fighting”).\nWhile in New York, Justin also studied with Vernel Fournier, Reggie Workman, and Joe Chambers. Outside of private lessons, he received a healthy education on Tuesday and Wednesday nights at Small’s listening to Kurt Rosenwinkle (with Jeff Ballard) and the Sam Yahel Trio (with Brian Blade). He also developed musically through regular rehearsing and performing in groups led by Coby Brown, Oscar Perez, Bob Reynolds, Nate Shaw and Doug Wamble.\nJustin is the founder and bandleader for the highly-successful Legends Of Jazz Series presented in its fourth season at the Velvet Note, and being presented throughout the Fulton County Library System. He is a member of the music school faculty of The Lovett School in Atlanta.\nCONCENTRATION: CONTRACTS AND HUMAN RESOURCES\nBrianna Williams is an attorney practicing in the areas of entertainment, employment, diversity management, and OFCCP compliance.\nShe is the founder of Mélange, a full service legal and consulting firm specializing in helping individuals and organizations visualize their dreams, implement tools to reach them, and protect them from harm. Through Mélange, Brianna has helped several organizations manage their differences, similarities, and tensions in meaningful ways. She has presented at conferences across the country on the topics of diversity management, OFCCP compliance, and strategic visioning, and her work has been published nationally.\nIn addition to a rich employment/diversity practice, Mélange’s entertainment practice is a full-service operation geared toward artists in the music industry. Among the clients represented by the firm are Jacquees (Cash Money Records), Rich Homie Quan, Trinidad James, and Reese Johnson (producer for Alicia Keys, Aaron Hall, and Changing Faces).\nBrianna is also an active member of the community. She has served as the Director of Communications for the National Bar Association, is a voting member of The Recording Academy; is an active volunteer for the Georgia Lawyers for the Arts; has served as a guest lecturer in Music Business at Loyola University, and is a board member of Music in the Park ATL. In 2010, Brianna was recognized as one of the Nation’s Best Advocates: Top 40 Under 40 by The National Bar Association and IMPACT.\nBrianna is admitted to practice in both Georgia and Louisiana."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:f718a6da-4997-4bc4-b96b-54de187d9ae4>","<urn:uuid:258db01d-8c63-4365-80e9-3cb353ec5023>"],"error":null}
{"question":"What were the leadership changes experienced by the Fifth Texas Cavalry and the 61st New York Infantry during their service periods?","answer":"The Fifth Texas Cavalry was initially commanded by Thomas Green, who was later promoted to brigadier general and then major general before being killed at Blair's Landing. Henry C. McNeill succeeded Green as colonel, and the regiment went through several other commanders including Hugh A. McPhaill as lieutenant colonel and John Samuel Shropshire as major. The 61st New York Infantry was led by multiple colonels including Francis C. Barlow, Nelson A. Miles, Spencer W. Cone, and George Washington Scott, with Oscar Broady serving as Lieutenant Colonel.","context":["FIFTH TEXAS CAVALRY\nFIFTH TEXAS CAVALRY. The Fifth Texas Cavalry was also known as the Fifth Texas Mounted Rifles and the Fifth Texas Mounted Volunteers. On August 12, 1861, Confederate Brig. Gen. Henry H. Sibley arrived in San Antonio to organize a brigade for a campaign in New Mexico and Arizona. His ultimate goal was to capture the gold and silver mines of Colorado and California and to secure a Confederate pathway to the Pacific. Three regiments of cavalry or mounted riflemen, each with an attached battery of howitzers, were quickly formed for service in what would come to be known as the Sibley's Brigade: the Fourth Texas under Col. James Reily, the Seventh Texas under Col. William Steele, and the Fifth Texas Mounted Volunteers. The Fifth was recruited, for the most part, in Waco, San Antonio, Bonham, Weatherford, and Austin and was organized and mustered into Confederate service at San Antonio with 926 officers and men. The volunteers supplied their horses and their own weapons, the quality of which varied widely. The regiment was to be commanded by the famed Texas Ranger, Thomas Green, who accepted his commission as colonel on August 20, 1861. Henry C. McNeill was elected as the regiment's lieutenant colonel and Samuel A. Lockridge as its major.\nThe Fifth Texas Mounted Rifles—numbering 835 effectives by the time it left San Antonio—marched for Fort Bliss in October. There Sibley took command of what would be designated as the Army of New Mexico. From Fort Bliss the all-Texas brigade began its march up the Rio Grande toward Santa Fe. After occupying Fort Thorn, New Mexico Territory, the brigade attempted the capture of Fort Craig. Although the fort remained in Union hands, the Texans defeated a sortie attempted by the garrison on February 20–21 at the battle of Valverde. The Fifth Texas Cavalry played a conspicuous role in the Confederate victory. With General Sibley indisposed, Colonel Green commanded the troops on the field. Capt. Willis L. Lang, the commander of Company B, led what was perhaps the only charge of lancers in the Civil War. As might have been expected, the company was cut to pieces, and its captain received an apparently mortal wound. (Despondent and in great pain, Lang later committed suicide.) The company's first lieutenant, Demetrius M. Bass, died of seven wounds received in the charge. Major Lockridge was also killed in action at Valverde, just at the point of overrunning a battery of four Union guns, and the regiment's adjutant, First Lt. Joseph D. Sayers, who Green compared to \"a hero of the days of chivalry,\" was promoted to captain and transferred to command of the captured guns, afterward known as the Val Verde Battery.\nThe regiment also played a conspicuous part in subsequent engagements of the New Mexico campaign, notably Apache Canyon (March 26), Glorieta (March 28), and Peralta (April 14). After enduring a hideous march back to Texas when Sibley's campaign ended in failure after the destruction of the brigade's supplies at Glorieta, the Fifth Texas returned to San Antonio with only 454 officers and men fit for duty. During the campaign, 45 of the regiment's soldiers had been killed in action or mortally wounded, 31 died of disease, 81 had been wounded, and 209 had been taken prisoner. Tom Green, who had been the brigade's de facto commander throughout the New Mexico campaign, was promoted to brigadier general and given command of what had been the Sibley's Brigade, and Henry C. McNeill became the regiment's colonel. Hugh A. McPhaill, formerly the captain of Company E, who had replaced Lockridge as major, was promoted to lieutenant colonel, and John Samuel \"Shrop\" Shropshire, formerly the captain of Company A, became the regiment's new major. When Shropshire was killed in action at Glorieta, Denman W. Shannon, formerly the commander of Company C, was elected to replace him.\nAfter a brief period in which the men rested and re-equipped themselves, the regiment was sent to the Texas Gulf Coast to take part in Maj. Gen. John B. Magruder's recapture of Galveston on January 1, 1863 (see GALVESTON, BATTLE OF). Confederate authorities at Houston refitted two steamboats lying in Buffalo Bayou into gunboats, armored with cotton bales. These were manned by volunteers from Green's command, humorously dubbed \"horse marines.\"\nFrom Galveston, the Fifth Texas Cavalry, as a part of Green's brigade, was transferred to the Bayou Teche region of southern Louisiana to resist Maj. Gen. Nathaniel P. Banks's attempted overland drive on Texas from the Mississippi River. The regiment fought with distinction through the spring of 1863 at the battles of Fort Bisland (April 12–13), Brashear City (June 23), and Donaldsonville, (June 28) where Major Shannon was wounded and captured and Capt. Daniel H. Ragsdale of Company D and lieutenants James A. Darby of Company I and James F. Cole of Company K of the Fifth were killed in action. (Following his exchange, Shannon returned to the regiment as its lieutenant colonel.) After his attempt to storm Donaldsonville was repulsed, General Green crossed the Atchafalaya River and on September 29 attacked the Union garrison at Fordoche. In this fight, also known as Stirling's Plantation, Green's command captured 462 prisoners and a battery of artillery. In the final battle of the campaign, Green won a significant victory over Maj. Gen. William B. Franklin's rear guard at Bayou Bourbeau on November 3, 1863.\nDuring this campaign, Tom Green was promoted to major general and given command of the cavalry division of the Trans-Mississippi Department. His former brigade was assigned to Brig. Gen. Arthur Pendleton Bagby, who commanded it through the Red River campaign. Green's cavalry regiments, including the Fifth, returned to Texas during the winter of 1863–64, but with the opening of the Red River campaign in the spring of 1864 they returned to Louisiana to reinforce the Confederate Army of Maj. Gen. Richard Taylor.\nThe Fifth Texas Cavalry was heavily engaged throughout the entire campaign, especially at the battles of Mansfield (April 8, 1864) and Pleasant Hill (April 9, 1864). It was instrumental in harassing Maj. Gen. Nathaniel P. Banks's defeated Union Army on its retreat to Alexandria but received a severe check at the skirmish at Blair's Landing, Louisiana, where General Green was killed on April 12, 1864.\nFollowing Banks's return to New Orleans, the Fifth Texas Cavalry was redeployed, first to the Teche region, then briefly to Valdalia, Louisiana, on the Mississippi River, then to Arkansas, and finally to Texas as part of Maj. Gen. John A. Wharton's division. In the final months of the war the brigade was commanded by acting Brig. Gen. Xavier B. Debray and then by Brig. Gen. William P. (Gotch) Hardeman. The Fifth Texas Cavalry spontaneously disbanded at Huntsville before Lt. Gen. Edmund Kirby Smith surrendered the Confederate forces in the Trans-Mississippi Department in June 1865.\nDonald S. Frazier, Blood & Treasure: Confederate Empire in the Southwest (College Station: Texas A&M University Press, 1996). Donald S. Frazier, Fire in the Cane Field: The Federal Invasion of Louisiana and Texas, January 1861–January 1863 (Austin: State House Press, 2009). Martin Hardwick Hall, The Confederate Army of New Mexico (Austin: Presidial Press, 1978). Richard G. Lowe, The Texas Overland Expedition of 1863 (Abilene: McWhiney Foundation Press, 1998).","61st New York Volunteer Infantry Regiment\n|61st Regiment New York Volunteer Infantry|\n|Active||October 25, 1861 – July 14, 1865|\n|Size||1526 men (total)|\n|Disbanded||July 14, 1865|\n|Col. Francis C. Barlow\nCol. Nelson A. Miles\n|1st Division, II Corps|\nThe regiment was organized in New York City on October 25, 1861 and was mustered in for a three-year enlistment in October, November, and December, 1861; the regiment was formed by consolidation of the Astor Rifles (or Regiment) with the Clinton Guards. At the expiration of its term of service, the men entitled thereto were discharged and the regiment retained in service, but at the same time consolidating Companies G and K into the other companies; on December 20, 1864, the men of the 54th New York Volunteer Infantry not mustered out with their regiment were transferred, forming the new Companies G and K.\nThe regiment left the State November 9, 1861; served at Washington, D. C., from November 10, 1861; in Howard's Brigade, Sumner's Division, Army of the Potomac, from November 27, 1861; in Howard's, 1st, Brigade, Richardson's, 1st, Division, 2d Corps, Army of the Potomac, from March 13, 1862-; in 3d Brigade, 1st Division, 2d Corps, from July, 1862- in 1st Brigade 1st Division, 2d Corps, from September, 1862; in 2d Brigade, 1st Division 2d Corps, from October, 1862; in 1st Brigade, 1st Division, 2d Corps, from November, 1862; and it was mustered out and honorably discharged July 14, 1865, at Alexandria, Virginia.\nThe 1908 book The Union army: a history of military affairs in the loyal states, 1861-65 -- records of the regiments in the Union army -- cyclopedia of battles -- memoirs of commanders and soldiers gives the following description of the regiment's service:\nThis regiment, was stationed for a short time at Washington, but moved on to Manassas on Nov. 28. It moved on to the Peninsula early in the spring, took part in the operations of the siege of Yorktown and was first closely engaged in the battle of Fair Oaks (Also called Seven Pines), in which 106 were killed or wounded and 4 reported missing, out of 432 who went into action. There Lieut. Col. Massett and many other gallant men lost their lives. The loss in the Seven Days' battles was still greater, and the ranks that gathered at Harrison's landing after the battle of Malvern hill were sadly depleted. At Antietam the regiment was in the thick of the fight - led by Colonel John Barlow, the 61st NY, with the 64th NY, flanked the 'sunken road' which was holding up the advance, and delivered enfilade fire on the confederates in the road, and at Fredericksburg it served in Hancock's division in the charge on Marye's heights and lost 36 in killed, wounded and missing. At Chancellorsville in May, 1863,. the troops under Col. Miles made a gallant defense of the retreating union army which won them high praise, and at Gettysburg the loss was once more severe, participating on the II Corps' advance in the 'Wheatfield', where the regiment was exposed to heavy enemy fire, losing more than half its strength. There was little rest for the worn regiment during the autumn. At Auburn, Bristoe Station (where the II Corps shattered an attack by Confederate general AP Hill), Rappahannock Station and in the Mine Run campaign, it was active, and it was mid-winter when it finally established permanent quarters near Brandy Station. In December and January a large number of men reenlisted and received veteran furlough. The regiment was reunited in the spring of 1864 and served with honor through the severe fighting which led up to Cold Harbor and Petersburg, suffering most severely in the bloody angle at Spotsylvania. It joined in the first assault on Petersburg, June 15; was engaged at Deep Bottom, Strawberry Plains, Reams' station and Hatcher's Run. It was present at the Fall of Petersburg, joined in the pursuit to Appomattox, and was engaged at Sailor's creek and Farmville. Its record is a long and glorious one and it bravely earned its right to rank among the most gallant organizations of the Union army.\nTotal strength and casualties\nThe total enrollment of the command was 1,526 members; during its service the regiment lost by death, killed in action, 11 officers, 113 enlisted men of wounds received in action, 5 officers, 67 enlisted men; of disease and other causes, 2 officers 134 enlisted men; total, 18 officers, 314 enlisted men; aggregate, 332; of whom 46 enlisted men died in the hands of the enemy.\n- Colonel Spencer W. Cone\n- Colonel Francis Channing Barlow\n- Colonel Nelson Appleton Miles\n- Lieutenant Colonel Oscar Broady\n- Colonel George Washington Scott\nThis article incorporates text from a work in the public domain: The Union army: a history of military affairs in the loyal states, 1861-65 -- records of the regiments in the Union army -- cyclopedia of battles -- memoirs of commanders and soldiers. Madison, WI: Federal Pub. Co., 1908. volume II\n- New York State Military Museum Unit History Project New York State Military Museum and Veterans Research Center - Civil War - 61st Infantry Regiment History, photographs, table of battles and casualties, and historical sketch for the 61st New York Infantry Regiment.\n- National Park Service UNION NEW YORK VOLUNTEERS 61st Regiment, New York Infantry\n- Antietam on the Web Federal Regiment61st New York Infantry"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:c615a720-dee4-4129-8431-d8776970166c>","<urn:uuid:b52bbf7e-f2ed-4921-80d5-564844ae76dd>"],"error":null}
{"question":"Hey there! I'm trying to understand how rice blast symptoms show up in different parts of the plant compared to sheath blight - could anyone explain the key differences? 🌾","answer":"Rice blast and sheath blight show distinct symptoms: For blast, leaf lesions are elliptical (diamond/football-shaped), 0.25-1.5 inches long with white/tan centers and dark reddish-brown margins. The most severe blast damage occurs at nodes below the head, causing stem breakage ('rotten neck') and preventing nutrient flow to kernels. For sheath blight, symptoms begin as oblong water-soaked lesions near the waterline that turn light grayish-white with reddish-brown margins after 2-3 days. Sheath blight spreads up the plant and between plants as the canopy closes, primarily affecting the upper canopy leaves that produce nutrients for developing kernels.","context":["Rice producers should start scouting fields for sheath blight and blast once joints begin to move. These two diseases pose the most threat for economic losses to Arkansas growers.\nMost rice in Mississippi County, Ark., does not have severe pressure from sheath blight which is seen more frequently on prairie soils west of Crowley's Ridge. However, isolated fields may have high enough levels to require a fungicide application.\nAccording to Rick Cartwright, Arkansas Extension plant pathologist, sheath blight is the most common and economically devastating rice disease in Arkansas. Fields with severe pressure typically suffer loss in yield of 10 to 25 percent and reduced milling quality. Weakened stems may also result in more lodging, causing additional yield loss and harvest difficulties.\nProperly timed fungicide applications are effective in reducing sheath blight. Cartwright said thresholds should be used to determine the need for a fungicide application.\nSheath blight is more likely to be a problem if a field has a history of the disease or is in a continuous rice rotation or if high nitrogen rates are used to promote lush growth. Rhizoctonia solani is the pathogen responsible for the disease. This fungus overwinters in the soil and crop residue.\nOnce rice is flooded, sclerotia float to the water surface and begin infecting plants at and just above the waterline. Infection occurs more rapidly under moist, humid conditions, with daytime temperatures in the 80-to 92-degree range and generally develops more rapidly on a producers' best fields which have dense canopies. In those fields, the morning dew persists longer, allowing more time for the disease to develop.\nThe first symptoms of sheath blight are oblong water soaked lesions near the waterline. In two to three days, the lesions turn light grayish-white, surrounded by reddish-brown margins. As the rice canopy closes and plants come in closer contact, the fungus can spread from plant to plant and up each plant.\nA producer's objective is to protect leaves in the upper canopy from infection because they manufacture a large majority of the nutrients that go into the developing kernels.\nScouting should start at internode movement and continue through heading. When scouting, several random points should be checked in a field. Three row feet should be observed at each stop. The stop is considered positive if any signs of the disease are present. One stop per acre or 50 stops per field are recommended.\nThresholds for treating sheath blight vary by variety. For semidwarf varieties like Cocodrie, which is rated very susceptible to the disease, the threshold is when 35 percent of the stops scouted in a field are positive for the disease from seven to 14 days after half-inch internode elongation. For taller varieties like Wells, which is rated only moderately susceptible, the threshold is when 50 percent of your stops are positive.\nThe threshold levels indicate the need for fungicide applications only if the disease is approaching the upper canopy. If lesions are found low, near the waterline, hold off fungicide application, and continue to monitor the field twice a week.\nCartwright said research shows Quadris 2.08 SC at 12.8 ounces per acre to be the most effective fungicide for control of sheath blight on fields with severe infections. However, for the majority of fields, which have only moderate levels of infection, the 8.5-ounce rate of Quadris has been adequate.\nIn addition, for moderate infections, other products labeled for sheath blight control may provide adequate control with less expense. These include Moncut, Tilt, Propimax, Stratego and GEM.\nIf a field with sheath blight is planted next to Cocodrie and the producer is planning a fungicide application to control kernel smut (recommended timing is late boot), he should consider using Tilt, Propimax or Stratego. All three products contain propiconazole, the active ingredient which has performed well against kernel smut.\nMake sure to not get sheath blight mixed up with a couple of other diseases — such as stem rot and black sheath rot — that have similar symptoms. They are not as widespread as sheath blight and do not pose nearly the same threat for reducing yields. In addition, there is very limited research on whether an economic response is possible using fungicides on these diseases.\nBlast is the other major disease for the producer to keep an eye out for in his rice. Blast is a windborne disease just like rust in wheat. Cartwright advises that many rice fields in the southern part of Arkansas are showing leaf blast, which has been favored by the wet weather the last few weeks.\nCartwright stresses the need to watch fields closely. Most rice grown in the state is planted to varieties that are susceptible (Wells, Francis) to moderately susceptible (Cocodrie) to blast. If favorable weather persists for disease development, head protection with fungicides may be warranted.\nBlast leaf lesions should be checked for from joint movement through heading. Leaf lesions from blast are generally diamond or linearly shaped and from 0.25 inch to 1.5 inches long. The lesions have white/tan centers surrounded by dark reddish-brown margins.\nIf a producer finds suspect lesions, he should take samples to the local county Extension office. The samples can be examined under the microscope to confirm the presence or absence of blast spores.\nBlast is more likely to be a problem on fields in which the flood is not properly maintained. Leaf blast lesions are often first found on drought-stressed plants (out of the flood) on the southwest (major prevailing wind) side of the field. Research has shown one way a producer can help reduce losses associated with blast is to maintain a slightly deeper flood (3 to 6 inches).\nIf a farmer has seen leaf blast lesions in a field and there is a damp, cool forecast, he should consider the use of fungicides to protect the head. Quadris and GEM are the two most effective products labeled for control in Arkansas. Research has shown both to be comparatively effective against blast.\nGenerally two fungicide applications will be needed for effective blast control. The first shot should be put out at 10 percent headed, followed by the second application when 70 to 90 percent of the heads have about emerged.\nDave Freeze is Extension agent for Mississippi County, Ark.","Rice Blast Control\nDivision of Plant Sciences\nDelta Center, Portageville\nDivision of Plant Sciences\nBlast, also called rotten neck, is one of the most destructive diseases of Missouri rice. Losses due to this disease have been on the increase since 2000. Blast does not develop every year but is very destructive when it occurs.\nRice blast can be controlled by a combination of preventive measures and foliar fungicides applied when rice is in the late boot stage and again when it is 80 to 90 percent headed. Whenever possible, consult an expert in plant disease diagnosis; check with the Delta Center, 573-379-5431, a regional extension agronomist in a county extension center, or the MU Plant Diagnostic Clinic, http://plantclinic.missouri.edu/ in Columbia, 573-882-3019.\nBlast symptoms can occur on leaves, leaf collars, nodes and panicles. Leaf spots are typically elliptical (football shaped), with gray-white centers and brown to red-brown margins (Figure 1). Fully developed leaf lesions are approximately 0.4 to 0.7 inch long and 0.1 to 0.2 inch wide. Both the shape and the color vary depending on the environment, age of the lesion and rice variety. Lesions on leaf sheaths, which rarely develop, resemble those on leaves.\nThe most serious damage occurs when the fungus attacks nodes just below the head (Figure 2). The stems often break at the diseased node. This stage of the disease is referred to as “rotten neck.” Disease in the node prevents the flow of water and nutrients to the kernels and they will stop developing. Heads of plants damaged in this way may be completely blank (Figure 3) to nearly normal, depending on the stage of head development when infection occurs. The poorly developed grain usually breaks up badly in milling, reducing quality.\nBlast symptoms appear on leaves as elliptical spots with light-colored centers and reddish edges.\nThe most serious damage from rice blast occurs when the disease attacks the nodes just below the head, often causing the stem to break.\nBlast damage at the nodes disrupts the flow of water and nutrients to the kernels, halting their development.\nThis disease is caused by a fungus named Pyricularia grisea, which overwinters in rice seeds and infected rice stubble. The fungus reproductive structures, spores, can spread from these two sources to rice plants during the next growing season and initiate new infections. Spores from these new infections can spread by wind to other rice plants over great distances.\nThere are several races of Pyricularia grisea. Race 1B49 has been found in Missouri, but it is not yet known if other races are present.\nUse preventive measures\n- Incorporate or roll the rice stubble soon after harvest to promote early decomposition.\n- Plant the least-susceptible varieties and use a broad-spectrum seed treatment.\n- Grow rice in open fields free of tree lines particularly on east and south sides\n- Grow rice in fields where flood levels are easily maintained. Damage from blast can be reduced by keeping soil flooded 2 to 4 inches deep from the time rice plants are 6 to 8 inches tall until draining for harvest. Draining for straighthead is incompatible with the flooding required for blast control, so avoid fields with a history of straighthead and varieties susceptible to straighthead, or plant blast-resistant varieties in these fields.\n- Seed over a range of time to spread the heading dates. However, avoid planting late because blast will be more severe.\n- Seed to a stand of 15 to 20 plants per square foot.\n- Avoid excessive nitrogen application rates and apply no more than 30 pounds per acre of nitrogen per application at midseason. In fields with a history of blast, always split applications.\nScout fields for blast symptoms from the seedling through heading stages (see Scouting). If symptoms are found, prepare to use fungicides at the late boot stage and again when 80 to 90 percent of plants are headed.\nApply fungicides during the time frame predicted by the DD50 program, which is about 5 to 7 days before heading (late boot stage). Fungicides are especially needed if blast symptoms have been observed in the field and the variety is very susceptible. Fungicides should be applied a second time about two days after 50 percent heading (90 percent head exsertion). In uniform stands, 90 percent heading will occur in 4 to 5 days after the first heads are visible.\nThe decision to treat is more easily made when one or more of the following factors exist:\n- A susceptible variety is grown in the field.\n- The crop has excessive growth and a dense canopy.\n- Leaf symptoms have been found in the field.\n- Disease is present in southern parts of the field.\n- Cool, rainy, or cloudy weather with high humidity and heavy dews is predicted during heading.\nThe development of this disease is difficult to predict, and fungicide treatments are expensive. Therefore, you should treat on the basis of the above factors or automatically treat the field with a fungicide if you are unwilling to risk disease damage.\nRice fields should be scouted for leaf symptoms of blast beginning at the seedling stage and continuing until early heading. Leaf symptoms will appear most readily on plants at the edges of fields, on levees, in areas of the fields that are shaded in the morning, or in areas that received excessive nitrogen. Symptoms usually are worse on drought-stressed rice.\nIf you are uncertain about diagnosing blast symptoms, send a sample to your local MU Extension center for identification. A preboot fungicide application may be needed when foliage damage is severe and the stand is threatened. Proper fertilization and continuous flooding should minimize seedling damage.\nYou should continue to scout for blast near the heading stage and watch carefully for flagleaf collar symptoms on early-planted susceptible varieties. Also, devote time to determine the stage of rice development to see if the DD50 predicted time frame for fungicide treatment for blast is correct. Symptoms appear 4 to 6 days after infection, so rice heads may be infected without symptoms appearing.\nDo blast spores overwinter in Missouri?\nWe don’t know, but spores were found in Arkansas during the winter of 1987–88 in standing stubble. Preliminary research indicates other inoculum sources exist, including seed.\nWill a cold winter destroy the overwintering blast organism?\nCan my rice field get blast from infected seed?\nThe fungus has been isolated at low levels from seed. Seed may carry blast spores that could cause early seedling infection and symptoms.\nWill fungicide seed treatment prevent blast?\nSeed treatment in general is good insurance against seedling diseases and is likely to stop any seed transmission of blast. It will not prevent infection from wind-borne spores. Not all seed treatment fungicides are effective. Use a broad-spectrum material.\nWill burning the straw from an infected field help?\nBurning of stubble may reduce trash-borne spores, but the disease can still spread to your field by wind-borne spores.\nWill rolling the straw soon after harvest be as effective as burning?\nIt could be more effective because of the difficulty in getting a thorough burn, and the crop residue can have a beneficial effect on the rotated crop. It may be more advantageous to burn when rice must follow rice.\nShould I use a fungicide automatically for blast?\nFungicides are expensive and should be used when conditions warrant.\nWhen is it too late to apply a fungicide?\nFungicides should not be used after grain is in the milk stage and panicles are turning over, or approximately 10 days after full heading.\nShould I treat with a fungicide if severe blast is discovered on tillering rice?\nRather than treat tillering rice, you should fertilize with nitrogen as needed, maintain a uniform flood of 2 to 4 inches depth and scout fields closely to determine disease progress. Treatment with a fungicide may reduce the inoculum but is probably not cost effective. Head protection is likely to be more economical.\nWhat should be done to a field that has both sheath blight and blast?\nFungicides should be applied to control the disease to which the variety is the most susceptible. A fungicide that is effective on both diseases is best. Use the most effective fungicide for sheath blight control if sheath blight is the only disease present. However, if blast is present or anticipated, use the most effective fungicide for blast control at the second application."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:9f3fb746-2554-4b3a-907b-5b9d6d256664>","<urn:uuid:a338a186-c079-479e-8bbf-44c93e9a0479>"],"error":null}
{"question":"What historic and modern threats face calcium carbonate structures in our oceans, and how do ancient theological approaches help us understand methods of studying these challenges?","answer":"Calcium carbonate structures in oceans face both historic and modern threats. Historically, there were major extinction events like the Permian extinction 250 million years ago that severely impacted coral reefs. Today, ocean acidification poses a serious threat, making seawater more corrosive to calcium carbonate structures and reducing carbonate ions available for shell/skeleton building. To study these challenges, we can apply methods similar to medieval theological approaches: just as monastic theology combined careful textual analysis with systematic interpretation, modern marine science uses both direct measurements (research cruises, automated moorings) and proxy analysis (ice cores, fossil records) to understand ocean chemistry changes. This systematic approach allows scientists to reconstruct ocean pH histories and predict future impacts on marine calcifiers.","context":["Monastic and Scholastic Theology\nPope Benedict XVI\nA natural friendship between faith and reason\nOn Wednesday, 28 October , at the General Audience in St Peter's Square, the Holy Father reflected on the flourishing of Latin theology in the 12th century. The following is a translation of the Pope's Catechesis, which was given in Italian.\nDear Brothers and Sisters,\nToday I am reflecting on an interesting page of history that concerns the flourishing of Latin theology in the 12th century which occurred through a series of providential coincidences. A relative peace prevailed in the countries of Western Europe at that time which guaranteed economic development and the consolidation of political structures in society, encouraging lively cultural activity also through its contacts with the East. The benefits of the vast action known as the \"Gregorian reform\" were already being felt within the Church. Vigorously promoted in the previous century, they had brought greater evangelical purity to the life of the ecclesial community, especially to the clergy, and had restored to the Church and to the Papacy authentic freedom of action.\nFurthermore, a wide-scale spiritual renewal supported by the vigorous development of consecrated life was spreading; new religious orders were coming into being and expanding, while those already in existence were experiencing a promising spiritual revival.\nTheology also flourished anew, acquiring a greater awareness of its own nature: it refined its method; it tackled the new problems; advanced in the contemplation of God's mysteries; produced fundamental works; inspired important initiatives of culture, from art to literature; and prepared the masterpieces of the century to come, the century of Thomas Aquinas and Bonaventure of Bagnoregio.\nThis intense theological activity took place in two milieus: the monasteries and the urban Schools, the scholae, some of which were the forerunners of universities, one of the characteristic \"inventions\" of the Christian Middle Ages.\nIt is on the basis of these two milieus, monasteries and scholae, that it is possible to speak of the two different theological models: \"monastic theology\" and \"scholastic theology\". The representatives of monastic theology were monks, usually abbots, endowed with wisdom and evangelical zeal, dedicated essentially to inspiring and nourishing God's loving design. The representatives of Scholastic theology were cultured men, passionate about research; they were magistri anxious to show the reasonableness and soundness of the Mysteries of God and of man, believed with faith, of course, but also understood by reason. Their different finalities explain the differences in their method and in their way of doing theology.\nIn 12th-century monasteries the theological method mainly entailed the explanation of Sacred Scripture, the sacra pagina to borrow the words of the authors of that period; biblical theology in particular was practised. The monks, in other words, were devout listeners to and readers of the Sacred Scriptures and one of their chief occupations consisted in lectio divina, that is, the prayed reading of the Bible. For them the mere reading of the Sacred Text did not suffice to perceive its profound meaning, its inner unity and transcendent message. It was therefore necessary to practise a biblical theology, in docility to the Holy Spirit. Thus, at the school of the Fathers, the Bible was interpreted allegorically in order to discover on every page of both the Old and New Testaments what it says about Christ and his work of salvation.\nLast year, The Synod of Bishops on the \"Word of God in the life and mission of the Church\" reminded us of the importance of the spiritual approach to the Sacred Scriptures. It is useful for this purpose to take into account monastic theology, an uninterrupted biblical exegesis, as well as the works written by its exponents, precious ascetic commentaries on the Books of the Bible.\nThus monastic theology incorporated the spiritual aspect into literary formation. It was aware, in other words that a purely theoretical and unversed interpretation is not enough: to enter into the heart of Sacred Scripture it must be read in the spirit in which it was written and created. Literary knowledge was necessary in order to understand the exact meaning of the words and to grasp the meaning of the text, refining the grammatical and philological sensibility. Thus Jean Leclercq, a Benedictine scholar in the past century, entitled the essay in which he presents the characteristics of monastic theology: L'amour des lettres et le désir de Dieu (Love of words and the desire for God). In fact, the desire to know and to love God which comes to meet us through his words to be received, meditated upon and put into practice, leads us to seek to deepen our knowledge of the biblical texts in all their dimensions. Then there is another attitude on which those who practise monastic theology insist: namely an intimate, prayerful disposition that must precede, accompany and complete the study of Sacred Scripture. Since, ultimately, monastic theology is listening to God's word, it is impossible not to purify the heart in order to receive it and, especially, it is impossible not to enkindle in it a longing to encounter the Lord.\nTheology thus becomes meditation, prayer, a song of praise and impels us to sincere conversion. On this path, many exponents of monastic theology attained the highest goals of mystic experience and extend an invitation to us too to nourish our lives with the word of God, for example, through listening more attentively to the Readings and the Gospel, especially during Sunday Mass. It is also important to set aside a certain period each day for meditation on the Bible, so that the word of God may be a light that illumines our daily pilgrimage on earth.\nScholastic theology, on the other hand — as I was saying — was practised at the scholae which came into being beside the great cathedrals of that time for the formation of the clergy, or around a teacher of theology and his disciples, to train professionals of culture in a period in which the appreciation of knowledge was constantly growing.\nCentral to the method of the Scholastics was the quaestio, that is, the problem the reader faces in approaching the words of Scripture and of Tradition. In the face of the problem that these authoritative texts pose, questions arise and the debate between teacher and student comes into being. In this discussion, on the one hand the arguments of the authority appear and on the other those of reason, and the ensuing discussion seeks to come to a synthesis between authority and reason in order to reach a deeper understanding of the word of God. In this regard St Bonaventure said that theology is \"per additionem\" (cf. Commentaria in quatuor libros sententiarum, I, proem., q. 1, concl.), that is, theology adds the dimension of reason to the word of God and thus creates a faith that is deeper, more personal, hence also more concrete in the person's life. In this regard various solutions were found and conclusions reached which began to build a system of theology.\nThe organization of the quaestiones led to the compilation of ever more extensive syntheses, that is, the different quaestiones were composed with the answers elicited, thereby creating a synthesis, the summae that were in reality extensive theological and dogmatic treatises born from the confrontation of human reason with the word of God. Scholastic theology aimed to present the unity and harmony of the Christian Revelation with a method, called, precisely \"Scholastic\" — of the school — which places trust in human reason. Grammar and philology are at the service of theological knowledge, but logic even more so, namely the discipline that studies the \"functioning\" of human reasoning, in such a way that the truth of a proposal appears obvious. Still today, in reading the Scholastic summae one is struck by the order, clarity and logical continuity of the arguments and by the depth of certain insights. With technical language a precise meaning is attributed to every word and, between believing and understanding, a reciprocal movement of clarification is established.\nDear brothers and sisters, in echoing the invitation of the First Letter of Peter, Scholastic theology stimulates us to be ever ready to account for the hope that is in us (cf. 3:15), hearing the questions as our own and thus also being capable of giving an answer. It reminds us that a natural friendship exists between faith and reason, founded in the order of Creation itself. In the incipit of the Encyclical Fides et Ratio, the Servant of God John Paul II wrote: \"Faith and reason are like two wings on which the human spirit rises to the contemplation of truth\". Faith is open to the effort of understanding by reason; reason, in turn, recognizes that faith does not mortify her but on the contrary impels her towards vaster and loftier horizons.\nThe eternal lesson of monastic theology fits in here. Faith and reason, in reciprocal dialogue, are vibrant with joy when they are both inspired by the search for intimate union with God. When love enlivens the prayerful dimension of theology, knowledge, acquired by reason, is broadened. Truth is sought with humility, received with wonder and gratitude: in a word, knowledge only grows if one loves truth. Love becomes intelligence and authentic theology wisdom of the heart, which directs and sustains the faith and life of believers. Let us therefore pray that the journey of knowledge and of the deepening of God's Mysteries may always be illumined by divine love.\nWeekly Edition in English\n4 Novembeer 2009, page 12\nL'Osservatore Romano is the newspaper of the Holy See.\nThe Weekly Edition in English is published for the US by:\nThe Cathedral Foundation\nL'Osservatore Romano English Edition\n320 Cathedral St.\nBaltimore, MD 21201\nSubscriptions: (410) 547-5315\nFax: (410) 332-1069","What is Ocean Acidification?\nSince the beginning of the Industrial Revolution, when humans began burning coal in large quantities, the world’s ocean water has gradually become more acidic. Like global warming, this phenomenon, which is known as ocean acidification, is a direct consequence of increasing levels of carbon dioxide (CO2) in Earth’s atmosphere.\nPrior to industrialization, the concentration of carbon dioxide in the atmosphere was 280 parts per million (ppm). With increased use of fossil fuels, that number is now approaching 400 ppm and the growth rate is accelerating. Scientists calculate that the ocean is currently absorbing about one quarter of the carbon dioxide that humans are emitting. When carbon dioxide combines with seawater, chemical reactions occur that reduce the seawater pH, hence the term ocean acidification.\nCurrently, about half of the anthropogenic (human-caused) carbon dioxide in the ocean is found in the upper 400 meters (1,200 feet) of the water column, while the other half has penetrated into the lower thermocline and deep ocean. Density- and wind-driven circulation help mix the surface and deep waters in some high latitude and coastal regions, but for much of the open ocean, deep pH changes are expected to lag surface pH changes by a few centuries.\nOcean acidification and global warming are different problems, but are closely linked because they share the same root cause—human emissions of carbon dioxide. The atmospheric concentration of carbon dioxide is now higher than it has been for the last 800,000 years and possibly higher than any time in the last 20 million years. Humans have thus far benefited from the ocean’s capacity to hold enormous amounts of carbon, including a large portion of this excess carbon dioxide. Had the ocean not absorbed such vast quantities of carbon dioxide, the atmospheric concentration would be even higher, and the environmental consequences of global warming (sea level rise, shifting weather patterns, more extreme weather events, etc.) and their associated socioeconomic impacts would likely be even more pronounced. However, the oceans cannot continue to absorb carbon dioxide at the current rate without undergoing significant changes in chemistry, biology, and ecosystem structure.\nMeasuring ocean acidification: Past and present\nScientists know that the oceans are absorbing carbon dioxide and subsequently becoming more acidic from measurements made on seawater collected during research cruises, which provide wide spatial coverage over a short time period, and from automated ocean carbon measurements on stationary moorings, which provide long-term, high-resolution data from a single location.\nThese records can be extended back through time using what are known as chemical proxies to provide an indirect measurement of seawater carbonate chemistry. A proxy is a measurement from a natural archive (ice cores, corals, tree rings, marine sediments, etc.) that is used to infer past environmental conditions. For example, by analyzing the chemical composition of tiny fossil shells found in deep ocean sediments, scientists have developed ocean pH records from ancient times when there were no pH meters. Furthermore, because the ocean surface water is in approximate chemical balance, or equilibrium, with the atmosphere above it, a record of historical ocean pH can be inferred from atmospheric carbon dioxide records derived from Greenland and Antarctic ice cores, which contain air bubbles from the ancient atmosphere. Such evidence indicates that current atmospheric carbon dioxideconcentrations and ocean pH levels are at unprecedented for at least the last 800,000 years.\nGoing back deeper in Earth history to the Paleocene-Eocene boundary about 55 million years ago, scientists have found geochemical evidence of a massive release of carbon dioxide accompanied by substantial warming and dissolution of shallow carbonate sediments in the ocean. Although somewhat analogous to what we are observing today, this carbon dioxide release occurred over several thousand years, much more slowly than what we are witnessing today, thus providing time for the oceans partially to buffer the change. In the geologic record, during periods of rapid environmental change, species have acclimated, adapted or gone extinct. Corals have undergone large extinction events in the past (such the Permian extinction 250 million years ago), and new coral species evolved to take their place, but it took millions of years to recover previous levels of biodiversity.\nHow is ocean acidification affecting ocean chemistry?\nSeawater has a pH of 8.2 on average because it contains naturally occurring alkaline ions that come primarily from weathering of continental rocks. When seawater absorbs carbon dioxide from the atmosphere, carbonic acid is produced (see Box 1), reducing the water’s pH. Since the dawn of industrialization, average surface ocean pH has decreased to about 8.1.\nBecause the pH scale is logarithmic (a change of 1 pH unit represents a tenfold change in acidity), this change represents a 26 percent increase in acidity over roughly 250 years, a rate that is 100 times faster than anything the ocean and its inhabitants have experienced in tens of millions of years.\nAcidification can affect many marine organisms, but especially those that build their shells and skeletons from calcium carbonate, such as corals, oysters, clams, mussels, snails, and phytoplankton and zooplankton, the tiny plants and animals that form the base of the marine food web.\nThese “marine calcifiers” face two potential threats associated with ocean acidification: 1) Their shells and skeletons may dissolve more readily as ocean pH decreases and seawater becomes more corrosive; and 2) When CO2 dissolves in seawater, the water chemistry changes such that fewer carbonate ions, the primary building blocks for shells and skeletons, are available for uptake by marine organisms. Marine organisms that build shells or skeletons usually do so through an internal chemical process that converts bicarbonate to carbonate in order to form calcium carbonate.\nExactly how ocean acidification slows calcification rates, or shell formation, is not yet fully understood, but several mechanisms are being studied. Most hypotheses focus on the additional energy an organism must expend to build and maintain its calcium carbonate shells and skeletons in an increasingly corrosive environment. In the face of this extra energy expenditure, exposure to additional environmental stressors (increasing ocean temperatures, decreasing oxygen availability, disease, loss of habitat, etc.) will likely compound the problem.\nThese effects are already being documented in many marine organisms, particularly in tropical and deep-sea corals, which exhibit slower calcification rates under more acidic conditions. The impact on corals is of great concern because they produce massive calcium carbonate structures called reefs that provide habitat for many marine animals, including commercially important fish and shellfish species that use the reefs as nursery grounds. Coral reefs are vital to humans as sources of food and medicine, protection from storms, and the focus of eco-tourism. In addition to corals, studies have shown that acidification impairs the ability of some calcifying plankton, tiny floating plants and animals at the base of the food web, to build and maintain their shells. Scientists have also observed increased larval mortality rates of several commercially important fish and shellfish.\nWhat can we expect in the future?\nOcean acidification is occurring at a rate 30 to100 times faster than at any time during the last several million years driven by the rapid growth rate atmospheric CO2 that is almost unprecedented over geologic history. According to the Intergovernmental Panel on Climate Change (IPCC), economic and population scenarios predict that atmospheric CO2 levels could reach 500 ppm by 2050 and 800 ppm or more by the end of the century. This will not only lead to significant temperature increases in the atmosphere and ocean, but will further acidify ocean water, reducing the pH an estimated 0.3 to 0.4 units by 2100, a 150 percent increase in acidity over preindustrial times. Assuming a “business-as-usual” IPCC CO2 emission scenario, predictive models of ocean biogeochemistry project that surface waters of the Arctic and Southern Oceans will become undersaturated with aragonite (a more soluble form of calcium carbonate) within a few decades, meaning that these waters will become highly corrosive to the shells and skeletons of aragonite-producing marine calcifiers like planktonic marine snails known as pteropods.\nAlthough ocean acidification has only recently emerged as a scientific issue, it has quickly raised serious concerns about the short-term impacts on marine organisms and the long-term health of the ocean. Scientists estimate that over the next few thousand years, 90 percent of anthropogenic CO2 emissions will be absorbed by the ocean. This may potentially affect biological and geochemical processes such as photosynthesis and nutrient cycling that are vital to marine ecosystems on which human society and many natural systems rely. At the same time, marine organisms will face the enormous challenge of adapting to ocean acidification, warming water, and declining subsurface-ocean oxygen concentrations.\nNews & Insights\nWHOI working to address ocean acidification; protect region’s vital shellfish industry\nA new report addresses the impacts of ocean acidification in Massachusetts and New England coastal waters on the region’s vital seafood industry.\nOcean acidification gets a watchful eye in New England aquaculture ‘hot spot’\nShellfish aquaculture is thriving in New England, but future growth in the industry could be stunted as coastal waters in the region become more acidic. Researchers at WHOI have developed…\nOcean acidification causing coral ‘osteoporosis’ on iconic reefs\nScientists Pinpoint How Ocean Acidification Weakens Coral Skeletons\nClimate Change Will Irreversibly Force Key Ocean Bacteria into Overdrive\n[ ALL ]\nWHOI in the News\nThe Top Eight Ocean Stories of 2022\nThe $500 Billion Question: What’s the Value of Studying the Ocean’s Biological Carbon Pump?\nEcology Research: Ocean acidification causing coral ‘osteoporosis’ on iconic reefs\nDisentangling influences on coral health\n[ ALL ]\nFrom Oceanus Magazine\nOcean acidification is no big deal, right?\nWHOI’s Jennie Rheuban discusses the very real phenomenon of an increasingly acidic ocean and the toll it’s taking on marine life.\nTo Tag a Squid\nHow do you design a tag that can attach to a soft-bodied swimming animal and track its movements? Very thoughtfully.\nHow Do Corals Build Their Skeletons?\nWHOI scientists discovered precisely how ocean acidification affects coral skeletons’ a factor that will help scientists predict how corals throughout the world will fare as the oceans become more acidic.\nSearching for ‘Super Reefs’\nSome corals are less vulnerable to ocean acidification. Can the offspring from these more resilient corals travel to other reefs to help sustain more vulnerable coral populations there?\nGraduate student Hannah Barkley is on a mission to investigate how warming ocean temperatures, ocean acidification, and other impacts of climate change are affecting corals in an effort to find…"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:d4214e2a-808f-4286-b60a-15220e49151e>","<urn:uuid:51ca6f21-f0f4-4343-8b14-a7f059f0fe69>"],"error":null}