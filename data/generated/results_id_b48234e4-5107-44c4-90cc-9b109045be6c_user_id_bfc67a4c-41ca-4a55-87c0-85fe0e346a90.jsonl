{"question":"How do Unrequited Leisure gallery and the Patsy Cline Museum differ in their approach to preserving and displaying musical heritage?","answer":"The Patsy Cline Museum focuses specifically on preserving physical artifacts and personal belongings of the legendary country singer, featuring hundreds of never-before-seen items. In contrast, Unrequited Leisure takes a more conceptual approach, displaying contemporary artistic responses to creative prompts, with works arranged in gridded groupings using office papers and letterpress printing, focusing on the labor and process of creative acts rather than historical artifacts.","context":["By Patrick Vincent\nILSSA Frameworks Gallery Entry Title, Unrequited Leisure, 2019, varied media. Photo courtesy of Patrick Vincent.\nILSSA (Impractical Labor in Service of the Speculative Arts) was established in 2008 as a form of artist-organization-as-union by co-operators Bridget Elmer and Emily Larned. ILSSA projects enlist the skills and engagement of its members to question hierarchy and labor in art making. In ILLSA’s latest call-and-response project beginning May 2019, members, identified as impractical laborers, were asked “What frameworks are essential supporting structures for your practice?” via the ILSSA listserv. Members’ responses were edited, compiled and published as a zine, ILSSA Frameworks: Notes toward a theory of practice. Additionally, eleven themes/prompts were identified out of this process: Questions, Attention, Alternatives, Risk, Pace, Embodiment, Handwork, Routine, Care, Community and Environment. ILSSA members were given a copy of the zine in July 2019 along with the resulting prompts. They spent a month responding to these prompts; the resulting work was mailed directly to Unrequited Leisure and is currently on display through September 27, 2019.\nILSSA Frameworks exhibition, detail, 2019, varied media. Photo courtesy of Patrick Vincent.\nUnrequited Leisure’s ILSSA: Frameworks features ILSSA-affiliated artists from all over the United States with some international participants, all responding to the same prompts. Multicolored office papers are letterpress printed with each prompt as a header, inviting the participating artists to treat the paper in response to the word. Treatments include writing/journaling, writing/poetry, list making, collage, calligraphy, suminagashi/marbling, as well direct tearing and cutting of the paper. The gallery is arranged in gridded groupings of each prompt; the different colors of paper create sections of thought and approaches. The paper is a surface for reflection, yet each paper reminds us that it is the physical evidence of labor. The zine as well as the structure of the exchange is both public and intimate, calling impractical laborers to visualize the underpinning aspects of their creative practice.\nFrameworks focuses on the labor of the creative act as well as meditation and experimentation that is often unseen in finished art works. One Pace submission is comprised of an even grid of holes punched in the paper. Another Risk has a crumpled and distorted, cut rectangle within the paper—it is barely attached at the top left corner. This activates the physical surface of the paper; the acts of distorting and puncturing the paper reify Risk and Pace. Others engage the paper as an invitation for writing. A response to Embodiment questions writing as an action executed by a body in thought, beginning with the line “A person in a body” and ending with “Maybe making a poem.” Each artist is anonymous but some continuity in approach identifies artists between the different prompts.\nILSSA envelopes, 2019, letterpress marker pen on envelopes. Photo courtesy of Patrick Vincent.\nThere is a dual suggestion of everyday activity and fine-craft value to each prompt. The letterpressed header connotes a sacred space of activity, but the office paper suggests daily, ephemeral action. Each paper/prompt is displayed unframed and allowed to exist as the naked reality of each participant’s engagement. The envelopes are displayed as well, reminding the gallery visitors that this is the work of community, connected by intention over great distances. The envelopes themselves are also letterpress printed in a metallic bronze ink, but the scars and blemishes of going through the mail are evident, as well as any circling, drawings or writings by the participating member.\nUnrequited Leisure is a curatorial space focused on interdisciplinary approaches to critical and topical investigations run by Chalet Comellas and Clinton Sleeper. The curatorial mission allows for exhibitions such as Frameworks that question the practice of art, making, and value.\n????, anonymous, 2019, 8.5″x 11″, pen marker letterpress on paper. Photo courtesy of Patrick Vincent.\nILSSA: Frameworks is at Unrequited Leisure, 625 7th Avenue South, Apartment A, Nashville, Tennessee, from Sept 7—Sept 27. Gallery hours are Fridays and Saturdays from 1 pm to 6 pm.\nPatrick Vincent is an artist and Associate Professor of Printmaking at Austin Peay State University.","Music Museums in Tennessee\nThe Country Music Hall of Fame® and Museum in Nashville, Tennessee\nRediscover the music you love, again and again, at the Country Music Hall of Fame and Museum. This scrapbook of county music’s roots comes to life with legendary instruments, unique lyric sheets, memorabilia of classic and contemporary artists and so much more. Now open, don’t miss Family Tradition: The Williams Family Legacy, a major exhibition exploring the legacy of Hank Williams and his kin – one of country music’s most iconic families. Complete your experience by shopping at the Museum Store and ordering a delicious southern meal from SoBro Grill.\nVisit countrymusichalloffame.org for further information.\nThe Musicians Hall of Fame and Museum in Nashville, Tennessee\nThe Musicians Hall of Fame and Museum focuses on and showcases the actual musicians and session players that have played with some of the major music artists in history. The Musicians Hall of Fame and Museum covers all genres of music including country, rhythm and blues, soul, funk, jazz, rock and pop. Whether it's Bob Dylan and The Beatles, the major artists from Motown, The Mamas and the Papas and The Beach Boys, or Elvis Presley and Johnny Cash, we have something to offer everyone! The musicians and instruments on display at the Musicians Hall of Fame and Museum have made their mark on music history and we would love to share some of their stories with you! Visit musicianshalloffame.com for further information.\nRCA Studio B, in Nashville, Tennessee\nAs Music City’s only historic studio tour, historic RCA Studio B provides a glimpse into the heart of Nashville music-making during one of its most exciting and classic periods. In its heyday, Studio B saw the creation of numerous chartbusters, such as Dolly Parton’s “I Will Always Love You,” Elvis Presley’s “Are You Lonesome Tonight?” and Charley Pride’s “Kiss an Angel Good Morning.” Tours of Historic RCA Studio B depart daily from the Country Music Hall of Fame® and Museum and are available only in conjunction with museum admission.\nRyman Auditorium in Nashville, Tennessee\nBuilt in 1892, Ryman Auditorium is a National Historic Landmark and a must-see for anyone visiting Nashville. Most famous as the home of the Grand Ole Opry from 1943-1974, the Ryman is the very epicenter of Music City. Three new exhibits are now open: “Grand Ole Opry 1940s-1970s,” “Ryman Stage to Screen” and “Proud to Be Here! The Legacy of Minnie Pearl.”See genuine artifacts from those early Opry years like the dress Dolly Parton wore the night she was inducted into the Grand Ole Opry. See a costume Sissy Spacek wore when she played Loretta Lynn in Coal Miners Daughter filmed at the Ryman. Watch exclusive Minnie Pearl footage and see the National Medal of Arts she received from president George H.W. Bush. Visit ryman.com for further information.\nPatsy Cline Museum in Nashville, Tennessee\nShe may have left the world so tragically at the age of 30 in 1963, but Patsy Cline left her own indelible mark on Country Music and remains one of the biggest names in Country history. The Patsy Cline Museum in Nashville celebrates the life of the legendary songstress and features hundreds of never before seen artefacts, personal belongings, and much more. Check out the Patsy Cline Museum for yourself, located on the second level of the Johnny Cash Museum building.\nGraceland in Memphis, Tennessee\nExperience the birthplace of rock ‘n’ roll and discover first-hand how Elvis Presley became the most celebrated entertainer in the world. Graceland takes you on a one of a kind journey through Elvis’s humble beginnings and rise to superstardom, with up close and personal looks at the influence that forever changed our music, and our lives. Enjoy an audio-guided tour of Graceland Mansion. Then, visit the Elvis Auto Museum, Sincerely Elvis featuring an Elvis Jumpsuits All Access exhibit, Elvis's Custom Jets and Elvis After Dark. Stay across the street at Elvis Presley's Heartbreak Hotel. For further information visit elvis.com.\nIn 1957, just before a two-year stint in the U.S. Army, Elvis took the proceeds from his first hit – Heartbreak Hotel – and purchased Graceland, a Southern-style Colonial mansion that sits on 13.8 acres. Opened on 7th June 1982, Graceland is one of the most famous rock 'n' roll residence in the world and gives visitors inside access to see where Elvis lived, relaxed and spent time with his family and friends. Graceland was designated a National Historic Landmark in March of 2006 and today, Graceland welcomes over 600,000 visitors a year.\nThe Rock ‘n’ Soul Museum in Memphis, Tennessee\nThe Memphis Rock ‘n’ Soul Museum’s exhibition about the birth of rock and soul music, created by the Smithsonian Institution, tells the story of musical pioneers who, for the love of music, overcame racial and socio-economic barriers to create the music that shook the entire world. The museum is the gateway to the history of the Memphis musical heritage and the cultural impact influencing society. Visit memphisrocknsoul.org for further information.\nSun Studio in Memphis, Tennessee\nTake a guided tour of the most famous recording studio in the world, \"The Legendary\" Sun Studio \"Birthplace of Rock 'n' Roll\", where the blending of Blues and Country Music exploded in the big bang of Rock 'n' Roll heard around the world. Visit sunstudio.com for further information.\nStax Museum of American Soul Music in Memphis, Tennessee\nStax Museum of American Soul Music is a 17,000-square-foot museum with more than 2,000 exhibits, artefacts and memorabilia celebrating the legacy of American soul music. Along with the legendary Stax Sound, the museum spotlights the music of Muscle Shoals, Hi and Atlantic Records. For further information visit staxmuseum.com\nTina Turner Museum at West Tennessee Delta Heritage Museum in Brownsville, Tennessee\nThe Queen of Rock and Roll, continues to be recognised as one of the world’s most popular entertainers having been inducted in to the ‘Rock and Roll Hall of Fame’ in 2015, she has won an incredible eight Grammy Awards and holds the record for selling more concert tickets than any other solo artist in history! Make sure you explore the Tina Turner Museum in Brownsville. Read more...\nThe International Rock-a-Billy Hall of Fame Inc and Museum in Jackson, Tennessee\nThe International Rock-A-Billy Hall of Fame and Museum is in Jackson since the city is the hometown of rockabilly star Carl Perkins and is located between Memphis, home of rock & roll and blues, and Nashville, home of country and hillbilly music. The museum traces the history and highlights of rockabilly music and features rare memorabilia and photographs. Exhibits, audio tapes tell the \"tales\" and \"stories\" of Elvis Presley, Carl Perkins, Roy Robison, Johnny Cash, Jerry Lee Lewis, Sonny Burgess and a host of musical artists who went from \"cotton fields\" to \"Cadillacs\" in a very short period of time. Visit rockabillyhall.org for further information.\nThe Birthplace of Country Music Museum in Bristol, Tennessee\nThe Birthplace of Country Music museum, based in Bristol Tennessee, is dedicated to calling attention to the crucial role played by artists from East Tennessee, Southwest Virginia and the southern Appalachian region in country, bluegrass, and other musical venues which have been nurtured by this region. Attending not only to the past, BCMA has developed programs which address this musical heritage in the present and promote its continuation into the future. Visit birthplaceofcountrymusic.org for further information.\nLoretta Lynn’s Museum, Hurricane Mills in Tennessee\nLoretta Lynn's 18,000-square-foot museum is complete with a mini-theater, Loretta's tour bus, personal vehicles, The Coal Miner's Daughter Dress, memorabilia and awards from the \"Living Legends\" entertainment career and personal life. Visit lorettalynn.com for further information.\nThe Museum of Appalachia, Clinton in Tennessee\nThe Museum of Appalachia, an affiliate of the Smithsonian Institution, is described as \"an American treasure\" that \"stands alone as a tribute to the American spirit.\" The Museum celebrates a Tennessee Fall Homecoming on the second full weekend of October. The Homecoming, stocked with great talent, is one of the nation’s largest and most authentic old-time music, craft and folk festivals. Visit museumofappalachia.org for further information."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:5a09cd51-ba52-409d-a34a-2c9977867ba8>","<urn:uuid:25076705-9a4a-41d1-8c84-1576a51f6f20>"],"error":null}
{"question":"What were the key differences between Federalists and Anti-Federalists in their views of the Constitution?","answer":"The Federalists and Anti-Federalists had opposing views on the Constitution. The Federalists supported a strong central government and the Constitution, believing it was necessary for the states to unite as a nation. In contrast, the Anti-Federalists opposed the Constitution's ratification because they feared the new national government would be too powerful and threaten individual liberties. They were particularly concerned about the absence of a bill of rights and argued against the expansion of national power in favor of small localized government.","context":["What were the groups opposed to the constitution?\nThe Anti-Federalists opposed the ratification of the 1787 U.S. Constitution because they feared that the new national government would be too powerful and thus threaten individual liberties, given the absence of a bill of rights. …\nWho were the opponents of the Constitution?\nIn time, the various opponents to the new Constitution came to be known as the Anti-Federalists. Their collected speeches, essays, and pamphlets later became known as the “Anti-Federalist Papers.”\nWho of the following was opposed to the ratification of the Constitution?\nWhat was the viewpoint of the Anti-Federalists concerning the ratification of the Constitution? People opposed to the ratification of the Constitution were called the Anti-Federalists. They were concerned that the Constitution gave too much power to the national government at the expense of the state governments.\nWhat groups may have been opposed to ratification and why?\nThose who supported the Constitution and a stronger national republic were known as Federalists. Those who opposed the ratification of the Constitution in favor of small localized government were known as Anti-Federalists. The Anti-Federalists argued against the expansion of national power.\nWho opposed the new constitution?\nWhich is a main idea in the Ninth Amendment?\nWhich is a main idea in the Ninth Amendment? Privacy rights must be respected, unless forbidden by the state law. Some rights are not included in the Constitution, but are still protected.\nIs right to privacy?\nArticle 12 of the Universal Declaration of Human Rights, 1948, and Article 17 of the International Covenant on Civil and Political Rights (ICCPR), 1966, legally protect persons against “arbitrary interference” with their privacy, family, home, correspondence, honour and reputation.\nWhy is privacy an important right?\nPrivacy is important because: Privacy gives us the power to choose our thoughts and feelings and who we share them with. Privacy protects our information we do not want shared publicly (such as health or personal finances). Privacy helps protect our physical safety (if our real time location data is private).\nDo we have the right to privacy?\nThe right to privacy is alluded to in the Fourth Amendment to the US Constitution, which states, “The right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated, and no Warrants shall issue, but upon probable cause, supported by Oath …\nWhat qualifies as invasion of privacy?\nInvasion of privacy is the considered the intrusion upon, or revelation of, something private. One who intentionally intrudes, physically or otherwise, upon the solitude or seclusion of another or his/her private affairs or concerns, is subject to liability to the other for invasion of privacy.\nWhat would be considered an invasion of privacy?\nInvasion of privacy is the unjustifiable intrusion into the personal life of another without consent. The four most common types of invasion of privacy torts are as follows: Appropriation of Name or Likeness. Intrusion Upon Seclusion.\nWhat Are The 4 Privacy Torts?\nProsser identified four privacy torts: Intrusion upon seclusion, public disclosure of private facts, false light and misappropriation of name and likeness. Today, in California there are also several common law privacy torts partially codified in the Civil Code.\nWhat is one example of a physical invasion of privacy?\nFor example, if you tape a private customer conversation without approval and use the remarks on your website, you could face an invasion of privacy lawsuit. Portraying someone in a false light.\nWhat is an example of privacy?\nPrivacy is the state of being free from public scrutiny or from having your secrets or personal information shared. When you have your own room that no one enters and you can keep all of your things there away from the eyes of others, this is an example of a situation where you have privacy.\nWhat are the three key aspects of privacy?\nAccording to Ruth Gavison, there are three elements in privacy: secrecy, anonymity and solitude. It is a state which can be lost, whether through the choice of the person in that state or through the action of another person.\nWhat are Westin’s four states of privacy?\nAlan defined the four states of privacy as solitude, intimacy, anonymity and reserve.\nWhat Are The Many Lives of privacy?\nThe Many Lives of PII\n- Social Security number,\n- Driver’s license or state identification card number, or.\n- Financial account number or credit card number, with or without any required code/number/password that would permit access to a financial account.\nWho were the 2 big anti-federalists at the Virginia Constitutional Convention?\nMany individuals, such as Patrick Henry, George Mason, and Richard Henry Lee, were Anti-Federalists. The Anti-Federalists had several complaints with the Constitution. One of their biggest was that the Constitution did not provide for a Bill of Rights protecting the people.\nWhat two groups were formed because of the ratification arguments over the constitution?\n143-44. Almost immediately upon the adjournment of the Convention and the publication of the Constitution, people divided themselves into two groups: those favoring ratification were called Federalists and those opposed to ratification were known as Anti-federalists.\nWhat two states did not ratify the Constitution?\nThe Constitution was not ratified by all states until May 29, 1790, when Rhode Island finally approved the document, and the Bill of Rights was not ratified to become part of the Constitution until the end of the following year.\nWhich state signed the Declaration of Independence first?\nPennsylvania State House\nAre there two declarations of independence?\n2. More than one copy of the Declaration of Independence exists. After the adoption of the Declaration of Independence, the “Committee of Five”—Thomas Jefferson, John Adams, Benjamin Franklin, Roger Sherman and Robert R.\nHow many founding fathers are there in total?\nMorris in 1973 identified the following seven figures as key Founding Fathers: John Adams, Benjamin Franklin, Alexander Hamilton, John Jay, Thomas Jefferson, James Madison, and George Washington based on the critical and substantive roles they played in the formation of the country’s new government.","What is the main point of Federalist 47?\nAbstract: Madison addresses concerns that the United States Constitution does not adequately provide for the separation of powers among the three branches of government. He argues that limited overlap of authority between the branches of government does not result in the tyranny of a single branch.\nWhat was the Anti-Federalists position?\nThe Anti-Federalists opposed the ratification of the 1787 U.S. Constitution because they feared that the new national government would be too powerful and thus threaten individual liberties, given the absence of a bill of rights.\nWhat was the purpose of the Federalist 46?\nThis essay examines the relative strength of the state and federal governments under the proposed United States Constitution. It is titled “The Influence of the State and Federal Governments Compared”.\nWhat does federalist 47 say?\nFederalist #47 affirmed the principle upon which the Federalists and Anti-Federalists agreed: “The accumulation of all powers, legislative, executive, and judiciary, in the same hands, whether of one, a few, or many, and whether hereditary, self-appointed, or elective, may justly be pronounced the very definition of Mar 6, 2011.\nWhat does federalist 51 say about judiciary?\nIn Federalist 51, James Madison urged that, to keep the powers separate, each branch “should have as little agency as possible in the appointment of the members of the others.” But this presented a problem for the judicial branch, which was intended to be apolitical and therefore could not have its members Nov 17, 2016.\nWhat is the main idea of Federalist 50?\n50 opens with the following premise: “IT MAY be contended, perhaps, that instead of OCCASIONAL appeals to the people, which are liable to the objections urged against them, PERIODICAL appeals are the proper and adequate means of PREVENTING AND CORRECTING INFRACTIONS OF THE CONSTITUTION.” The key to the opening is the Mar 6, 2011.\nWhat did the Federalists want?\nFederalists wanted a strong central government. They believed that a strong central government was necessary if the states were going to band together to form a nation. A strong central government could represent the nation to other countries.\nDid the Federalists support the Constitution?\nLed by Alexander Hamilton, albeit secretly at first, the Federalists were the first political party of the United States. They supported the Constitution, and attempted to convince the States to ratify the document.\nWhy did the Federalists win?\nIn 1787, toward the end of the Constitutional Convention in Philadelphia, Mason proposed that a bill of rights preface the Constitution, but his proposal was defeated. Why did the Federalists win? Federalists seized the initiative and were better organized and politically shrewder than Anti-federalists.\nWhat were the three main ideas in the Federalist Papers?\nSeparation of powers of the national government by dividing it into 3 branches : The legislative, the executive, and the judiciary.\nWhy did John Jay only write 5 essays?\nAfter writing the next four essays on the failures of the Articles of Confederation in the realm of foreign affairs, Jay had to drop out of the project due to an attack of rheumatism; he would write only one more essay in the series. Madison wrote a total of 29 essays, while Hamilton wrote a staggering 51.\nWhat is the main purpose of the Federalist Papers?\nThe Federalist Papers were written and published to urge New Yorkers to ratify the proposed United States Constitution, which was drafted in Philadelphia in the summer of 1787.\nWhat does Brutus 1 say?\nWhat did Brutus 1 say? He believed that the Constitution and laws of every state would nullified and declared void if they were, or shall be inconsistent with the Constitution. Brutus argued that under the Necessary and Proper Clause, Congress would be able to repeal state fundraising laws.\nWhat is the main idea of Federalist Paper 49?\nJames Madison wrote Federalist 49 in part as a response to Thomas Jefferson’s idea that a constitutional convention should be called whenever one of the departments of government oversteps its delegated constitutional authority.\nWhat is one example of checks and balances?\nExamples of checks and balances include: The president (Executive) is commander in chief of the military, but Congress (Legislative) approves military funds. The president (Executive) nominates federal officials, but the Senate (Legislative) confirms those nominations.\nWhat is the most famous line from The Federalist Papers No 51 about?\nFederalist No. 51 addresses means by which appropriate checks and balances can be created in government and also advocates a separation of powers within the national government. The idea of checks and balances is a crucial part of the modern U.S. system of government.\nWhat is Madison’s argument in Federalist 51?\nIn Federalist 51, Publius (James Madison) argues that the separation of powers described in the Constitution will not survive “in practice” unless the structure of government is so contrived that the human beings who occupy each branch of the government have the “constitutional means and personal motives” to resist “ Sep 16, 2013.\nWhy is the judicial branch peculiar to Madison Federalist 51?\nMadison saw the judiciary as having “peculiar qualifications”: at is, goals and objectives different from those of members of the legislature and executive branches. Second, because of life tenure, members of the judiciary would come to understand the value of their independence of the other two branches.\nWhat is fed 71?\nIt was published in the New York Packet in an effort to convince the people of New York to ratify the new Constitution. The papers were meant to urge New York and other states to ratify the proposed Constitution, which was a success at the end.\nWhy is Federalist 51 important?\nGovernment Must Furnish the Proper Checks and Balances Between the Different Departments.” Madison wrote Federalist 51 to explain how separation of powers with checks and balances protects liberty. Madison borrowed the concept of separation of powers from Montesquieu, a French political philosopher.\nWhy is Federalist 78 important?\nFederalist No. 78 discusses the power of judicial review. It argues that the federal courts have the job of determining whether acts of Congress are constitutional and what must be done if government is faced with the things that are done on the contrary of the Constitution."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:7b5c6446-40d1-4cf3-b85e-94749f05af9d>","<urn:uuid:40917716-d8a0-4dbe-a3bc-244c36f44395>"],"error":null}
{"question":"When it comes to insect resistance, how do teak and white oak compare in their natural ability to repel pests?","answer":"Teak wood has superior natural insect resistance compared to white oak. Teak contains natural oils that serve as an effective insect repellent, specifically protecting against termites and marine bores. While white oak does have some resistance against termites because 'they don't like the taste of oak wood,' teak's protection is more comprehensive due to its natural oils. Additionally, teak's pest resistance is part of its complete package of natural immunities that includes resistance to insects, water, and wood rot, making it more thoroughly protected than white oak.","context":["Drugs A - Z\nGeneric Name: Red oak\nCategoryHerbs & Supplements\nBlack oak (Quercus tinctoria), British oak, common oak (Quercus pedunculata), durmast oak (Quercus sessiliflora), English oak (Quercus robur), Fabaceae (family), gallotannins, green oak (Quercus virens), holm oak (Quercus ilex), live oak (Quercus virens), Quebec oak, quercetin, red oak (Quercus petraea, Quercus rubra), royal protector, sessile oak, tanner's bark, tannins (phlobatannin, ellagitannins, gallic acid), turkey oak (Quercus cerris).\nOf the many species of oak found all over the world, the white oak (Quercus alba) is found primarily in North America. Although there are many species of the Quercus genus, many are thought to have similar properties. The parts of this tree used medicinally are the inner bark and the galls (growths that are produced in reaction to fungi or insects).\nTraditionally, Native Americans and European settlers have used white oak for its astringent and anti-inflammatory properties. White oak was listed in the United States Pharmacopoeia from 1820 to 1919, and also in the National Formulary from 1916 to 1936.\nDue to a lack of available scientific evidence, it is difficult to determine the safety of white oak. Adverse effects associated with white oak include gastrointestinal irritation, nausea and vomiting, which are theoretically due to its tannin content.\nEvidenceDISCLAIMER: These uses have been tested in humans or animals. Safety and effectiveness have not always been proven. Some of these conditions are potentially serious, and should be evaluated by a qualified healthcare provider.\nTraditionWARNING: DISCLAIMER: The below uses are based on tradition, scientific theories, or limited research. They often have not been thoroughly tested in humans, and safety and effectiveness have not always been proven. Some of these conditions are potentially serious, and should be evaluated by a qualified healthcare provider. There may be other proposed uses that are not listed below.\nAntibacterial, antioxidant, antiseptic, antiviral, appetite stimulant, astringent, bleeding gums, bronchitis, burns, cancer, chilblains (inflammation caused by the cold), colds, cough, diabetes mellitus, diarrhea, digestive aid, dysentery (severe diarrhea), fever, gastrointestinal disorders, growth disorders, hemorrhage, hemorrhoids, inflammation, leukorrhea (vaginal discharge), lung conditions, rash (poison ivy), rheumatism, skin conditions, sore throat, strep throat, tonics, tuberculosis (prevention), ulcers, varicose veins, wound healing.\nAdults (18 years and older):\nThere is no proven safe or effective dose for white oak. Traditionally, oak bark has been ground to a fine powder and inhaled freely to treat tuberculosis. For diarrhea or dysentery, a decoction of 1 ounce of bark in a quart of water, boiled down to a pint and taken in \"wineglassful\" doses has been used for no longer than 3-4 days. For chronic sore throat, a decoction of 1 ounce of bark in a quart of water boiled down to a pint and gargled has been used.\nWhite oak has also been applied on the skin. Traditionally, a preparation has been formulated by mixing 2 teaspoons of coarsely powdered bark in 500 milliliters of water, followed by straining the solution. Oak bark is not recommended for topical use longer than 2-3 weeks. A decoction of 1 ounce of bark in a quart of water, boiled down to a pint has been applied topically for bleeding gums or piles.\nChildren (younger than 18 years):\nThere is no proven safe or effective dose for white oak, and use in children is not recommended.\nSafetyDISCLAIMER: Many complementary techniques are practiced by healthcare professionals with formal training, in accordance with the standards of national organizations. However, this is not universally the case, and adverse effects are possible. Due to limited research, in some cases only limited safety information is available.\nAvoid white oak in individuals with a known allergy or hypersensitivity to white oak. Caution is advised in those with allergies to peanuts or tree pollens. One patient experienced an anaphylactic reaction after eating acorn nuts, fruit of the holm oak (a related species, Quercus ilex), who was also allergic to peanuts.\nSide Effects and Warnings\nWhite oak is generally regarded as safe (GRAS) in the United States, and is often used on an as needed basis, orally and topically. However, there is very little scientific information available supporting the safety of white oak.\nGastric irritation, nausea, and vomiting have occurred in association with the ingestion of large doses of white oak.\nAlthough not well studied, white oak may cause hepatic (liver) dysfunction; plants with at least 10% tannins may cause necrotic (dead tissue) conditions of the liver. Tannins found in white oak may have adverse effects on the kidneys. Use cautiously in patients with renal (kidney) or hepatic (liver) dysfunction. Avoid oak bark baths in those who have cardiac insufficiency (stages III and IV (NYHA)), eczema, hypertonia (muscle tension) or infection.\nPregnancy and Breastfeeding\nInteractions with Drugs\nCombining white oak with medications that may damage the liver (hepatotoxic) should be avoided due to possible additive side effects. Caution is advised.\nTannins found in white oak may theoretically have adverse effects on the kidneys; plants with at least 10% tannins may cause kidney damage. Consult with a qualified healthcare professional, including a pharmacist, before combining therapies.\nInteractions with Herbs and Dietary Supplements\nCombining white oak with herbs and supplements that may damage the liver (hepatotoxic) should be avoided due to possible additive side effects. Caution is advised.\nThe primary chemical constituents of oak bark include tannins (phlobatannin, ellagitannins, gallic acid), gallotannins and quercetin. Patients taking quercetin should use caution as there may be additive effects. Tannins found in white oak may theoretically have adverse effects on the kidneys; plants with at least 10% tannins can cause kidney damage. Herbs or supplements with a high tannin percentage may also cause precipitation of constituents of other herbs. Consult with a qualified healthcare professional, including a pharmacist, before combining therapies.\nThis information is based on a systematic review of scientific literature, and was peer-reviewed and edited by contributors to the Natural Standard Research Collaboration (www.naturalstandard.com): Helena Kim, PharmD (Northeastern University); Shaina Tanguay-Colucci, BS (Natural Standard Research Collaboration); Catherine Ulbricht, PharmD (Massachusetts General Hospital); Mamta Vora, PharmD (Northeastern University); Wendy Weissner, BA (Natural Standard Research Collaboration).\nBibliographyDISCLAIMER: Natural Standard developed the above evidence-based information based on a thorough systematic review of the available scientific articles. For comprehensive information about alternative and complementary therapies on the professional level, go to www.naturalstandard.com. Selected references are listed below.\nBlumenthal M, et al, ed. The CompleteGerman Commission E Monographs: Therapeutic Guide to Herbal Medicine. Trans. S. Klein. Boston, MA: American Botanical Council, 1998\nBrinker F. Herb Contraindications and Drug Interactions. 2nd ed. Sandy, OR: Eclectic Medical Publications, 1998.\nGarg SK, Makkar HP, Nagal KB, et al. Oak (Quercus incana) leaf poisoning in cattle. Vet.Hum.Toxicol. 1992;34(2):161-164.\nKinde H. A fatal case of oak poisoning in a double-wattled cassowary (Casuarius casuarius). Avian Dis. 1988;32(4):849-851.\nLeung AY, Foster S. Encyclopedia of Common Natural Ingredients Used in Food, Drugs and Cosmetics. 2nd ed. New York, NY: John Wiley & Sons, 1996.\nLin RY, Clauss AE, Bennett ES. Hypersensitivity to common tree pollens in New York City patients. Allergy Asthma Proc 2002;23(4):253-258.\nLoria RC, Wilson P, Wedner HJ. Identification of potential allergens in white oak (Quercus alba) pollen by immunoblotting. J Allergy Clin Immunol 1989;84(1):9-18.\nMaciejewska A, Wojtczak J, Bielichowska-Cybula G, et al. [Biological effect of wood dust]. Med Pr 1993;44(3):277-288.\nMammela P, Tuomainen A, Vartiainen T, et al. Biological monitoring of wood dust exposure in nasal lavage by high-performance liquid chromatography. J Environ.Monit. 2002;4(2):187-189.\nMcCune LM, Johns T. Antioxidant activity in medicinal plants associated with the symptoms of diabetes mellitus used by the indigenous peoples of the North American boreal forest. J Ethnopharmacol. 2002;82(2-3):197-205.\nMcGuffin M, et al., ed. American Herbal Products Association's Botanical Safety Handbook. Boca Raton, FL: CRC Press, 1997.\nSchultz V, Hansel R, Tyler VE. Rational Phytotherapy: A Physician's Guide to Herbal Medicine. Terry C. Tegler, transl. 3rd ed. Berlin, Germany: Springer, 1998.\nVega A, Dominguez C, Cosmes P, et al. Anaphylactic reaction to ingestion of Quercus ilex acorn nut. Clin Exp Allergy 1998;28(6):739-742.\nWeiss RF. Herbal Medicine. Beaconsfield, UK: Beaconsfield Publishers Ltd., 1988, 328-29.\nWichtl MW. Herbal Drugs and Phytopharmaceuticals. Ed. N.M. Bissett. Stuttgart Medpharm GmbH Scientific Publishers, 1994.\nRemember, keep this and all other medicines out of the reach of children, never share your medicines with others, and use this medication only for the indication prescribed.","When you live in an area near water or a wet climate, then the best kind of wood to get would be waterproof wood. While no wood is completely waterproof, there are some great water-resistant woods that are perfect when working on projects where the wood will be exposed to the elements.\nOne of the most waterproof woods you can find is teak wood. Teak wood has a lot of natural oils that make it less water-absorbent than any other wood. Another type of wood that is water-resistant is afrormosia. It is usually used for making boats and outdoor furniture and is half the price of teak.\nWhen you buy any wood that you will use outdoors or in water, you need to find wood that won’t bend, warp or expand in moist or wet conditions. Some woods need to be treated to work best, and some don’t. So what is the best wood to use in wet conditions? I searched for the most waterproof wood, and here is what I found.\nThe Most Waterproof Woods\nAs I said before, there are no true waterproof woods, but some woods are water-resistant, and some woods need to be treated to have more water resistance. Hardwoods are the kinds of wood that offer the most water resistance and don’t need to be treated to use outdoors.\nThere are different types of hardwood, and their level of water resistance differs a little. Hardwoods are more water-resistant because they are extremely dense woods and have straight grooves that prevent water from penetrating the wood.\nHardwoods are low maintenance, water-resistant, warp, and rot-resistant, and are great to use on outdoor furniture, wooden cabins, and even boats and saunas. The only downsides to hardwoods are the availability and price.\nBecause hardwoods are in high demand but take a long time to grow, they are expensive. And for the same reason, it’s very important to get wood sustainably sourced from reliable sellers.\nWhat Is Sustainably Sourced, Woods?\nSustainable wood is wood that comes from properly managed sustainable forests. These forests are not stripped of all their trees, and new trees are planted to replace those cut-down. These forests are sustainably managed, and they don’t damage the eco-system, landscape, natural water systems, or animals in the forest.\nIllegal deforestation is a major concern in the fight to keep our forests thriving. These trees are cut down without consideration for the delicate eco-systems, animals, or water systems. The forest never recovers to its former glory if they are not correctly managed and treated after such a disaster.\nBelow are the most naturally water-resistant woods.\nTeak wood is the most water-resistant wood you can find. Teak wood doesn’t need to be stained, sealed, or treated to be water-resistant. It has a glossy natural oil that lubricates and protects the wood from water and insects.\nTeak Wood Durability And Weather Resistance\nTeak wood is by far the best wood to use in any outdoor project because it has natural immunity to insects, water, and wood rot. Teak wood is the most durable wood and was used in the hay days of shipbuilding as the perfect decking material due to its durability and water resistance.\nTeak wood is one of the hardest woods in the hardwood family and has features other than its natural resistance to water. Teak wood doesn’t bend, warp, become brittle or rot like other woods.\nThese qualities make teak wood able to withstand harsh conditions or the weather such as hail, rain, wind, and even the sun. It might get a bit dull (but it takes a long time to get dull), but a coat of its natural oil will fix it up nicely.\nTeak Wood Pest Resistance And Maintenance\nThe same natural oil that protects teak wood from water and the elements is a natural insect repellent and works wonders at keeping insects like termites and marine bores away. Teak wood is also low maintenance.\nWhen teak wood is harvested, it doesn’t need to be pressure treated, processed, stained, or painted. The only maintenance needed is a thin annual coat of teak oil to restore the shine. Because teak wood is so durable, it has the longest lifespan of all the hardwoods and will last 75-100 years if left outdoors in the elements.\nAfrormosia Wood (African Teak)\nAfrormosia wood, or African Teak, is a hardwood that comes from Africa. It is called African Teak because of its similarities to teak, even though it is not a type of teak. It is the perfect alternative to teak wood.\nIt possesses many of the same qualities that teak has but is much more affordable. It is not widely known in the US, but it is a beautiful wood to work with for those who do. When you want to buy afrormosia, please take caution and only buy from sustainable US sellers as afrormosia is highly regulated.\nIn Africa, it is not on the endangered species list yet; it might be classified as endangered in the future. Even though there are strict regulations in place, there are reliable resources of this wood found in the US.\nAfrormosia Wood Durability And Water-Resistance\nAfrormosia is very durable and water-resistant. It has been used for centuries to build boats in Africa and is a favorite in many European homes that want quality wood that will last years with little to no hidden maintenance costs.\nIt is just as durable as teak but a lot easier to work with. It is naturally resistant to wood rot, won’t become brittle or warp with time. When afrormosia is harvested, it already has the color of teak that is a bit aged and even, making it a gorgeous wood for indoor and outdoor high-moisture areas.\nAfrormosia Wood Pest Resistance And Maintenance\nAfrormosia is highly resistant to termites and other insects. The wood will not rot, bend or warp with time. It only needs minimal maintenance and costs a lot less than most hardwoods. You only need to wash and restain afrormosia wood once a year.\nThe one downside to afrormosia is that metal (furniture, BBQ’s and metal railings) can leave marks on the wood when there is rain, and the wood is unstained.\nUnder proper care and maintenance, this wood can last 20-30 years, if not longer.\nIpe Wood (Brazilian Walnut)\nIpe wood is a hardwood that is becoming popular in the US for its durability, water resistance, and scratch-resistant properties, to name a few. It is seen as a much more affordable teak wood substitute and comes from South America. It’s especially favored for its dense hardness as flooring or decking wood.\nLike teak wood, Ipe wood has a natural oil that protects it from harsh conditions and keeps it strong. It is harvested from sustainable well-managed forests and is famed to be 8 times harder than the WRC mentioned below.\nIt is five times denser than most other hardwoods and will last longer than any treated softwood. It can be left to age and brought back to its beautiful rich walnut coloring with just a pressure wash.\nIpe Wood Durability And Water-Resistance\nIpe wood is very durable and can last over 20 years before you have to stain it. Because it is considered the hardest wood in the hardwood family, it is rot-resistant, won’t warp or become brittle. It is very hard to work with as many consider it the hardest wood out there.\nWater won’t penetrate into the wood like it does some other woods because of how dense, hard, and tightly compressed the grain of the wood is. It has a straight grain and uniform texture.\nWhile it is not cheap in any sense, it is less expensive than teak. It can withstand extreme weather conditions such as rain, hail, wind, and even snow. This wood also does well under the hot gaze of the sun and won’t permanently fade to a grey if washed with a pressure washer a few times a year.\nIpe Pest Resistance And Maintenance\nIpe wood is naturally resistant to termites, beetles, and other wood-loving insects. The wood is too hard for them to live in, and they can’t stand the natural oil in the Ipe wood. This wood is very low maintenance and can last 20 years without any treatment or preservative.\nAll you need to do is pressure wash it a few times a year, and after 20 years, if needed, stain the wood. After that, you don’t have to worry about restaining it for another 20 years. That means if the wood is stained or treated after a few years, it can last well over 50 years.\nCedar Wood (Western Red Cedar)\nCedarwood is also a fantastic water-resistant hardwood. Cedarwood is usually used in outdoor furniture, structures, and even flooring. It is known for its natural ability to withstand most weather conditions.\nCedar Wood Durability And Water-Resistance\nThe Western Red Cedar (WRC for short) is known to be the most durable and water-resistant out of all the cedar hardwoods. The WRC has a fine, straight grain and uniform texture. Because of its low density, cedarwood is naturally water-resistant and won’t rot when exposed to rain, hail, wind, and the heat of the sun.\nMany people use it to build fences because of its durability outdoors. If properly maintained, this cedar wood will stay water-resistant, won’t warp, bend or become brittle. It makes a great base for most stains or paints.\nCedar Wood Pest Resistance And Maintenance\nBecause of its distinctive smell, red cedar wood is naturally resistant to insects such as carpet beetle larvae infestation and moths. Cedarwood is a bit more expensive than softwoods, but it is very low maintenance and only needs to be cleaned and restained or painted one to two times a year.\nWith these maintenance issues taken care of, the cedar wood will last between 15-20 years.\nOut of all the woods we have looked at so far, I had to mention the mighty White Oak, also known as a great hardwood to use outdoors and indoors.\nWhite Oak Wood\nWhile there are about 20 different oak species, it is common to group them in either the white oak or red oak category. White oak is known to be rot resistant and more water-resistant than softwoods. The red oak variety is rather porous and not the best suited to use outdoors.\nWhite oak is great for floors as it is highly scratch, dent, and water-resistant. It is not easy to work with, but it is well worth the effort at a fraction of the cost of other hardwoods. It has a closed-grain pattern that doesn’t allow water to damage it easily.\nRed oak is open-grained and is thus used in sculptures or furniture that doesn’t require water-resistant.\nWhite Oak Wood Durability And Water-Resistance\nWhite oak is rot-resistant and very durable. The European white oak is a bit more durable than its American cousin. White oak is weather resistant and, with proper maintenance, can withstand rain, hail, wind, and the sun’s hot rays.\nThe one drawback to white oak is that it shrinks and expands a little. It is much more stable and won’t shrink as much as red oak, and is thus more stable to use in flooring.\nWhite Oak Wood Pest Resistance And Maintenance\nWhite oak is very resistant against termites, as is the red oak; they don’t like the taste of oak wood. White oak will last longer with regular maintenance. If you stain white oak at least once a year, it can last 10-15 years.\nTeak might be the most expensive wood, but it is made to last. It is the most durable and water-resistant wood in the world can last up to 75 years without any staining or maintenance. So if you are interested in building a boat, cabin, sauna, or installing a deck on your property, teak is your best option.\nHardwood is not always easy to work with, but the results speak for themselves. Please make sure the wood you are buying comes from a sustainable source as deforestation is a real threat to our wildlife and eco-systems."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:19c46974-e053-4be9-aa8c-09d2b39fb52a>","<urn:uuid:33c23852-10d3-4993-a1ea-84a3ec8733b2>"],"error":null}
{"question":"Were slingers and archers used together in ancient battles, and did they have similar roles in attacking fortified cities?","answer":"Yes, slingers and archers were used together in battle, with slingmen serving close to archery units. Both were particularly effective in attacking fortified cities, but for slightly different reasons. Slingers could direct high-angle fire up steep slopes to target defenders on city walls, as seen in the attack on Lachish. Similarly, archers were crucial in siege warfare, as evidenced by Assyrian palace sculptures showing archers working in coordinated groups, sometimes with shield-bearers for protection while attacking enemies.","context":["Pick up a stone. Throw it. How far will it go? A fair way if you’re strong. But loop the stone into a sling and you can make it much more powerful. It’s a deadly weapon, in fact, as young David showed.\nWho used a sling for the first time?\nA modern David?\nThere is no way of knowing. But the sling was probably invented by shepherds to scare off predatory animals circling their ﬂocks.\nGradually the sling made its appearance on the battleﬁeld as a weapon of war. It let the slinger throw a stone a long way in any terrain, hilly as well as ﬂat.\nIt could also be fired up a slope, making it useful in assaults on fortiﬁed cities.\nA slingshot: easy to make, not easy to use\nThe sling had the great advantage of being easy to make, and its ammunition, slingstones, was provided by nature. This was important to the Israelite tribesmen, because the Phoenicians and Canaanites, lords of the land, forbade them the use of smelting and iron-mongery. This meant that they could not produce metal weapons, a vastly limiting factor when facing an enemy with metal swords, shields and armor.\nIts main disadvantage was that considerable training and experience were required to operate it with effective accuracy. It is one thing to shoot off a stone from a sling, but quite another to hit a small target at a distance – as David had to do when he faced Goliath.\nWhat did a slingshot look like?\nThe early sling looked rather like a large eye-patch. It consisted of a small piece of leather or cloth with two cords attached to opposite edges.\nThe stone missile was placed on the material and the cords pulled taut so that the material became a kind of bag containing the stone.\nThe bag was held by the left hand and the ends of the two cords held together by the right.\nThe bag was then swung round and round several times above the head until it gained the required momentum, at which point one of the cords would be released, shooting the missile forward.\nThe function of the sling was often complementary to that of the bow. Whenever they were used in battle, the slingmen always served close to the archery units.\nThey were particularly useful in an attack on a fortified city, because they could direct high-angle fire up a steep slope.\nThis wall painting from the tomb of Khety shows a variety of Egyptian warriors attacking a city. The red-helmeted soldiers in the top right of the mural are the slingers.\nTell Halaf slingman. The sling was made of a pad, usually of leather, attached to two thongs. After placing the slingstone in the pad, the thongs were pulled taut, converting the pad into a bag. The slinger held this bag in his left hand and the ends of the taut thongs in his right, above his head. He then whirled the sling with his right hand round and round to give it momentum, and at the crucial moment he released the end of one thong, freeing the stone.\nThe Assyrians use slings\nThe use of the sling came fairly late to the Assyrian army, making its appearance on the monuments only in the 8th century BC, in the time of Tiglath-pileser III. Perhaps it seemed too primitive for sophisticated Assyrian tacticians, and was over-looked.\nAssyrian slingers. These are offensive rather than defensive fighters, since their armor is designed to protect them from long-range weapons rather than swords and clubs.\nA reconstruction of the Assyrian attack on the Israelite city of Lachish. The slingers are at the bottom of the ramp built for battering rams. They are aiming at the defenders on the walls of the unfortunate city, soon to be overrun and destroyed.","Notes on Ancient Archery\nPart 1 of 2\nIT is obviously impossible within the limits of the present volume to attempt a history of archery as practised among the ancient inhabitants of the Mediterranean lands, and this chapter merely aims at supplying a few notes on some points of interest to archers connected with the use and structure of the bow in ancient times.\nIt appears that the bow was in use in all the lands bordering on the Mediterranean, in greater or less degree, from a very early date it was, however, among the Assyrians and the Egyptians that it assumed its highest position as a military weapon It is evident from the mural sculptures discovered by Sir A. H. Layard in the palaces at Nimroud and Kouyunjik that archery was as important an arm in the Assyrian hosts as it was in the English armies in the Middle Ages The mere fact that the king himself is generally represented in battle armed with the bow, sometimes even dismounted from his chariot, and shooting at his enemies on foot, shows that the weapon was held in the highest repute so important was the archer considered, that we find him accompanied by a shield-bearer whose business it was to ward off the arrows of the enemy. Sometimes the Assyrians fought in groups of three, consisting of an archer, a shield-bearer, and a swordsman. At other times we find one shield-bearer allotted to two archers, as in the illustration (fig 49) Frequently the archers fought from chariots, and here, again, we find them protected by a shield-bearer. Horse-archers were sometimes employed, also in pairs, one horseman holding the reins and guiding both horses, while the other used his how (fig. 50).\nIt would be not unnatural to suppose that, considering the large number of representations of archers and of bows that have come down to us, little difficulty would be found in recognising the structure of the bow used by the Assyrians. This, however, is very far from being the case, as the Assyrian bow, and to some extent the Egyptian bow, has been the cause of great perplexity in the minds of inquirers. It is evident that the Assyrian bow was an efficient and powerful one, not only from the fact that it was the principal weapon of war, but also because their kings and nobles appear to have relied on i, largely in hunting even so formidable a beast as the lion. Fig 51 shows King Asshur-na-zirpal He has apparently slain one lion, and is shooting at another. Now it is evident that if the bow was a trustworthy weapon against lions, it must have been capable of delivering an arrow with great force. Yet, to judge by the sculptured representations, the bow was as ill-made a weapon as can be conceived. We must therefore conclude, either that the sculptures are inaccurate, or that the bow was of a construction somewhat different from any that we are accustomed to, and was capable of doing better work than its appearance would lead us to believe. The former alternative is the one which at first seems most probable. the Assyrian sculptors, though obviously artists of great skill, were unacquainted with many of the elements of drawing, and frequently made the sort of mistake which children make in their first efforts. For instance, they delight in showing in a picture more than the eye can see at one view In depicting all archer in profile, with his back towards the spectator, they cannot resist introducing the drawing hand, as well as the back of the bow hand, when it would in fact be hidden by the body of the archer. Again, the artists frequently show no appreciation of the relative sizes of objects. It might therefore be argued, that if they make such obvious mistakes about matters of which we are able to judge, their representations of objects such as bows are likely to be equally faulty. This would be an easy way of dismissing the question, but on the whole it does not seem the right view to adopt.\nIn the first place, though the sculptures abound in instances of ignorance of perspective, yet they appear to be singularly accurate and exact in the representation of details. Secondly, if we are to assume that the representations of bows are ill done, and drawn without any attempt at accurate delineation, it seems certain that they would vary considerably from each other. This, however, is not the case. Great numbers of representations of bows have come down to us, executed at periods distant from each other by hundreds of years, but the type of bow is remarkably constant. It is impossible to believe that this uniformity can be due to any other cause than the fact that the pictures were accurately drawn from the bows in common use throughout this period.\nFig. 52 represents King Asshur-na-zirpal with a strung bow in his left hand. At first sight this appears to be a bow consisting of a single wooden stave about five feet long, with almost every fault that a how can possess. The curious angular shape which it shows violates the first principle of the bowyer's craft (according to our ideas), namely, that a bow shall have a stiff, unbending centre of a foot or eighteen inches, according to the length of the bow. This angular shape is very typical of Assyrian bows, and is also frequently found in Egyptian art, especially when Asiatic foes or mercenaries are depicted. Frequently, however, the bows are represented not absolutely angular, but always bending freely from the centre, and this is especially the case in the later sculptures of the time of Asshur-na-nipal. Bows with stiff centres occur in Egyptian art, but not, so far as I know, in Assyrian. Again, the bow appears to be of the same thickness all the way down, instead of gradually diminishing towards the ends. It is beyond all doubt that if this really represents, as it appears to do, a single stave bow of wood, it is a bad bow.\nIf we now refer to fig. 51, representing the same monarch, Asshur-na-zirpal, lion-hunting, we see what is presumably the same bow, or a bow of the same kind, fully drawn. This picture is as typical of the fully-drawn bow throughout the Assyrian sculptures as fig. 52 is of the bow when merely strung. This bow, again, bends very badly, judged by the standard of English wooden bows, as it bends right through the hand. The curve is, however, such as might be expected from the shape of the bow as depicted when strung, without any rigid centre. The length of the arrow, which is fully drawn to the head, is, moreover, so great when compared with the length of the bow that the two ends are brought much closer together than would be possible with any modern wooden bows without fracturing the bow. The curve described seems, in fact, to be only practicable with a bow made of a material far more elastic and less liable to fracture than any wood which, in modern times at any rate, has been used for bow-making. It is possible that the Assyrians knew of a wood which possessed the necessary qualities, which has long since disappeared or been forgotten, but it is improbable. Indeed, no ' self' bow, unless it were made of whalebone, could be expected to bend in the fashion of these Assyrian bows. The only remaining alternative, if we are to accept the evidence of the sculptures, is to assume that the Assyrian bow was in fact a composite bow. The appearance of the bow when strung affords little support to this theory, and, unfortunately, the ruins of Nineveh have not produced a single example of the Assyrian bow by which the question might be definitely settled. Fortunately, in the dry climate of Egypt a weapon has survived which may, perhaps, throw some light on the subject.\nIt will be remembered that the composite bow is, and has been from a remote period, essentially the weapon of Asia and of Eastern Europe; while in Africa the simple wooden arcus is the type of bow in general use. Consequently, it would be in accordance with what is generally known of the distribution of the bow if the Assyrian bow should turn out to be composite, while the occurrence of the composite bow in ancient Egypt would require some explanation. A considerable number of bows have been found in the tombs of ancient Egypt which are simple wooden bows of the typical African character. Fig. 53, from Rosellini's ' Monuments,' represents a bow of this type being drawn the stiff centre will be noted in comparison with the arch of the Assyrian bow. It dates probably from B.C. 1600, or somewhat earlier. It was therefore with great surprise, in the spring of 1893, that the present writer observed in the Egyptian section of the Royal Museum in Berlin what appeared to be undoubtedly a considerable fragment of a composite bow. I he curator of the department had not closely examined this piece, which came from a tomb at Thebes which is said by experts to be of the time of Rameses II. the writer, however, called the attention of Dr. von Luschan, the head of the Berlin Ethnographical Museum, to the bow, and he, recognising its importance, made a careful examination and dissection of it, and subsequently published a brochure on the subject. The illustrations (p. 65) of this unique bow are taken from drawings kindly supplied by Dr. von Luschan.\nThe bow is not perfect, one end being wanting, which has been restored by the dotted lines in the illustrations. The portion preserved measures 1.025 metre in length; the complete bow, as restored, would measure 1.245 metre. It will be seen that a deep groove runs the whole length of the bow, which is enclosed on each side by wood. Dr. von Luschan says that this consists of three strips on each side, though in his drawing there appears to be only one strip on each side in the centre at B. and two strips at A. These are the only portions of the bow which are of wood, the most important part of the bow probably its back being a hard, shiny, fibrous tissue of a pale yellow colour, of animal origin. Dr. von Luschan considers that this substance consists of the sinews from some large beast, probably cattle. The groove was in all probability filled with horn, which is known to be very perishable, even in the dry climate of Egypt. In some places traces can be found of a covering of leather and another, outer skin, probably of birch bark. Here we have a true composite bow similar in many respects to the modern Asiatic bow.\nThe groove in this bow is on the convex side, while the sinew back is on the concave side, as the bow now exists. The universal practice in building composite bows is to follow the natural shape of the horns which form their basis, the maker adding a stiffening of wood and overlaying the concave side with elastic sinew. When the bow is strung the natural shape of the horns is reversed, so that the outer, or convex, curve becomes the belly, or concave curve, in the weapon when ready for use. It appears that this usage was followed by the unknown bowyers who lived in the days of Rameses the Great. The main difference between this bow and a modern Turkish or Persian bow lies in the fact that in no part of the bow does there appear to be enough wood to render that part rigid. The backbone of the bow from end to end was horn and sinew: if, that is to say, we are right in conjecturing that the missing substance from the groove was horn. There is no stiff section in the centre of the bow, as is now customary, and there are no stiff ears at each end, turning on a natural hinge when the bow is strung. On the contrary, the bow would no doubt bend when drawn in one continuous curve throughout from end to end. Now this is precisely what those bows do in the Assyrian sculptures, which are represented as fully drawn, and precisely what the bow figured in the cut from Rosellini does not do. If, as appears at any rate possible, this bow was an Asiatic how, one difficulty as regards these sculptures disappears.\nThe difficulty of the angular form of the bow when strung but not drawn remains to be considered. The structure of the bow of Rameses II. at once makes this easier to understand. The absence of a stiff centre would naturally cause the bow when strung to fall away rapidly from the middle. In the example under consideration the wood stretches from end to end, so that although there would be no straight centre, which we nowadays expect, yet there would not be an actual angle. It is, however, possible that ill some cases the strips of wood did not actually join in the centre, in which case, when the pressure of the string was applied this curious angular shape Fig.. 54. Composite bow of the time of Rameses II would necessarily be produced. Should more bows of this character be subsequently discovered, this theory may be confirmed, or it may be upset, but in the meantime it is submitted tentatively as a possible explanation of this very curious weapon.\nDr. von Luschan supposes that this was either a bow of one of the Asiatic mercenaries of Egypt, or of one of the captives taken in war. He conjectures that it may possibly be Hittite, and the accompanying figure of a Hittite archer may be compared with it. This figure is taken, by Dr. von Luschan's permission, from a photograph of a Hittite sculpture recently discovered by him, and hitherto unpublished. Rameses II. conquered the Hittites or Khita, so that this conjecture is not improbable; and it is to some extent confirmed by a battle scene between Scti, the father of Rameses II., and the Hittites, engraved in Sir J. Gardner Wilkinson's work on the ancient Egyptians. In this picture the Hittites are armed with a short angular bow very similar to the Assyrian bow. However this may be, the likeness of the bow of Rameses II. toe the Assyrian bows and its undoubtedly composite nature seem to leave little room for doubt that the bows of the Assyrian sculptures are also composite.\nNo doubt the ordinary bow in use among the ancient Egyptians was the single-stave wooden how, of which several examples have been found in the tombs. These bows do not appear to have been very strong, and possibly they were not war bows, but were used for shooting birds and the smaller quadrupeds. Bows of unmistakably composite form are occasionally represented in the sculptures, and the fact that one composite bow has been discovered in an Egyptian tomb affords fair ground for believing that bows of this character were also in use, and were probably introduced by the Asiatic mercenaries who were employed by Egypt. The Sharu, who are identified by Birch with the Syrians, supplied the Egyptians with bows in the reign of Thothmes III,. which seems to show that they were not content with the indigenous African wooden bows.\nFig. 56 represents a hunting scene. It is taken from a green stone plaque in the British Museum from Tel-el-Amarna, which Dr. Wallis Budge believes to have been sent to Amenophis III. (B.C. 1450) as a gift from one of his Mesopotamian kinsfolk. I he bows bear a considerable resemblance to modern Oriental composite bows, far more so, indeed, than the bows of the Assyrian sculptures. Wooden bows are, however, found in Africa now curiously resembling the form of these bows, one of which is figured in Dr. Ratzel's monograph on African bows. It is possible that this form of composite bow may have been copied in wood by Nilotic tribes, and handed down to the present day. The shape is of course a bad one for wooden bows."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:ecd4fed1-bab6-462f-916e-0efac2d1b224>","<urn:uuid:d715d600-acb8-4be3-9891-61d4a2f78b1e>"],"error":null}
{"question":"How to structure a policy memo header?","answer":"A policy memo header should include the following elements: the addresser, the addressee, the date, and the subject of the writing.","context":["How to Write a Policy Memo: Writing Guide + Topics\nDo you want to try a role of an adviser? Then, you have such a great opportunity when writing a policy memo. Few people know what it is until they face a need to craft this kind of a document. So, what is a policy memo? That is a clear and concise piece of writing, the primary purpose of which is to consider a particular issue from different angles and provide a detailed analysis of the problem. The target audience of a memo is a decision maker, for whom you are going to collect all necessary pieces of information and put them into one puzzle.\nTake into account that the decision maker doesn't know about the issue at all or he/she knows little about the problem. Your task is to process the information on the problem and present it effectively. What does this mean? You are allowed to provide the reader not only with the brief description of the problem but also with the recommendations on how to deal with it. You are supposed to give the guidelines and promote the strategy that seems more favorable for the problem solution.\nHow to Write a Policy Memorandum: Expert Tips\nIn order to write a policy memo the right way, you need to check the format requirements. Before you get started with the process of memo writing, find out how it differs from the standard pieces of writing. If it is the first time you have faced a need to craft a policy memo, it is not surprising that you have a question “How to write a policy memorandum?” Everyone who is going to write a memo should ask himself/herself this question instead of starting writing it as a simple essay.\nThere is one major distinctive feature the policy memo structure has. The target audience is one of the key factors influencing the memo content and length. When crafting a memo, you should divide it into separate sections, where each one of them should be dedicated to a certain thought. Following the structure, you should provide the following information in the header: the addresser, the addressee, the date, and the subject of your writing. Then, you are expected to tell about the issue by providing a summary. Include the background information as well.\nPolicy Memo Format You Should Stick To?\nWe have already touched upon the structure of the memo above. Now, let's have a closer look at the policy memorandum format. In accordance with the specific memo format, you should follow the guidelines below when writing your policy memo:\n- Write the header line;\n- Start with the engaging sentence;\n- Describe the problem you are going to discuss;\n- Highlight the significance of the issue;\n- Share the pieces of advice on how to deal with the problem effectively;\n- Provide both your arguments and counter-claims against your opinion;\n- Prove that the counterarguments have no ground;\n- Do your best to prove that the solution offered by you is worth attention;\n- Conclude the memo.\nPolicy Memo Topics: Pick the Best One\nDo you lack the policy memo topic ideas? Check the ten best topics below:\n- The prevention of political conflicts and ways out of them.\n- Political management and its importance in the management of the modern state.\n- The effective means of saving money in the country's budget.\n- The problem of political culture in the history of political thought.\n- Political norms and value orientations of the modern society.\n- The necessity to implement budget amendments.\n- The ways of managing the money so that there are no any debts to other countries.\n- The tense relationships between Russia and the US.\n- How to provide poor countries with the sufficient food and water resources?\n- The necessity to implement a new visa policy.\nPolicy Memo Outline Sample\nHaving a detailed well-written policy memo outline, you can be sure that you won't miss anything when composing the final memorandum version. The well-structured outline is a backbone of any paper, and a memo isn't an exception. Because the main requirement for this kind of writing is to demonstrate the transparent flow of thoughts, an outline is of the utmost importance. It is a good idea to have two outlines: a draft and the edited version.\nIn case, it is the first memo writing in your life, and it will be difficult to formulate your ideas according to the policy memo requirements the first time. So use a draft, which allows having the free flow of thoughts, and then analyze what should be included in your work, what kind of information should be excluded from it because it doesn't add any value. Here, you will find a good memo outline example. You will save much time for crafting your outline if having this one in front of you.\n- Cover page\n- A summary of the problem;\n- The background information on the issue;\n- The presentation of the justification that the problem is really serious and it requires brainstorming solutions.\n- Give recommendations on how to deal with the problem;\n- Provide arguments;\n- Present the existing counter-claims;\n- Recommend the best possible solution by the solid evidence.\nThese are general guidelines for writing a policy outline. When writing your own memo, take into account that your teacher may provide you with the additional specific requirements for your assignment. Depending on the topic and the teacher's instructions, you may be assigned to write either a 1-page memo or the paper consisting of tenths of pages."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:27fa3c08-7fdf-4612-8e17-3b6679402871>"],"error":null}
{"question":"How did the relationship between the Kumeyaay people and European settlers evolve from first contact through Spanish missionary period, and how did Serra's missionary work intersect with this history?","answer":"The relationship began peacefully when the Kumeyaay, numbering 20,000-30,000 people, welcomed Portuguese explorer Juan Rodrigues Cabrillo in 1542. Initially, under Serra's arrival in 1769, the relationship remained cordial, with the Kumeyaay sharing fish and performing dances for the missionaries. However, relations deteriorated significantly by 1775, when the Kumeyaay attacked and burned the mission. This marked the beginning of a period of deep mistrust between the Kumeyaay and Spanish missionaries that lasted until Mexican independence in 1821. The Kumeyaay became known as the most resistant of all California Indians to European intrusion and subjugation. Despite this resistance, by 1797, Serra's mission had attracted the largest native population in Alta California. Serra himself continued his missionary work until his death in 1784 at the San Carlos Mission.","context":["San Diego has been inhabited by humans for perhaps 20,000 years along the coast, and 12,000 years in the desert. The primary Native American people to settle in San Diego's coastal and inland regions were the Kumeyaay, seasonal hunters and gatherers. These people were characterized as \"…fine in stature and carriage, affable and gay,\" according to Father Junipero Serra, who built the first California mission in the San Diego area in 1769. \"They brought fish and mollusks to us, going out in their canoes just to fish for our benefit. They have danced their native dances for our entertainment,\" he wrote in his journal.\nThese peaceful people greeted the first European to visit the region in 1542, Portuguese explorer Juan Rodrigues Cabrillo who sailed his ship the San Salvador from Navidad, New Spain. Cabrillo named the area San Miguel and declared it a possession of Spain. An estimated 20,000 to 30,000 Kumeyaay lived in the San Diego area at the time.\nAfter Cabrillo, Spain's Sebastián Vizcaíno arrived in his flagship San Diego in 1602 to map the California coast. Vizcaíno in turn named the area San Diego, not for his ship but for the Catholic saint San Diego de Alcalá.\nThe present Old Town had its origins in the 1700s when the Spanish ships San Antonio and San Carlos sailed into San Diego Bay in 1769, and the Spaniards established a military post, Presidio of San Diego on Presidio Hill near the present site of Old Town. Immediately afterward, the first of 21 California missions was then founded there on the hill, Mission San Diego de Alcalá.\nBy 1774, the first colonists arrived in San Diego, and one year later, the Kumeyaay people attacked the mission which had by then relocated a few miles east, and set it on fire. This was the first act revealing a deep mistrust between the native Kumeyaay and the Spanish missionaries, a mistrust that would continue until Mexican independence in 1821. Indeed, the Kumeyaay became the most resistant of all California Indians to subjugation and European intrusion. Even so, by 1797, the mission included the largest native population in Alta California.\nIn 1821, New Spain – or Mexico – won independence from Spain, and San Diego came under Mexican rule. In 1846, the United States declared war on Mexico, and in 1848 the Treaty of Guadalupe Hidalgo was signed, ending the war and defining the boundary between the U.S. and Mexico – all in the midst of the Gold Rush of 1848. San Diego officially became an American city.\nBy the 1850s, the non-native population reached 650, and San Diego County was created, one of California's original 27 counties. The population increased quickly, and by 1900, it was over 17,000. That year, the U.S. Navy founded its first establishment, the Navy Coaling Station, which further fueled the town's development. In 1903, the Marine Biological Association of San Diego formed, later becoming the world-famous Scripps Institution of Oceanography, the first of many scientific institutions in the area.\nSan Diego then hosted two World's Fairs, the Panama-California Exposition in 1915, and the California Pacific International Exposition in 1935. Balboa Park was the home of many Spanish colonial-style buildings built for these fairs, many of which have been restored to their original facades.\nThe military continued to expand its presence after World War II, but cutbacks after the Cold War prompted San Diego to diversify its economy. Since that time, the biotechnology industry has become an increasing player in the local economy, including the Jonas Salk Institute for Biological Studies, opened in 1963.\nUrban renewal is a continuing theme in Downtown San Diego, including the restoration of the Gaslamp Quarter and the construction of a large number of waterfront skyscrapers, hotels, restaurants, and other buildings.","Junípero Serra was a Spanish Franciscan friar who established the a series of missions among the native peoples of what is today California. He is considered by many to be the founding father of California and is recognized as a saint by the Catholic Church.\nJunípero Serra was born Miguel José Serra and baptized on November 24, 1713, in the town of Petra on the island of Mallorca, Spain. Though small for his age and somewhat sickly, as a boy he was filled with grand aspirations. The young José had developed an interest in reading the lives of the saints, fascinated in particular by the descriptions of St Francis of Assisi.\nAt the age of fifteen, José left home to enter the Franciscan University in nearby Palma to study philosophy. When he was seventeen, his intelligence and maturity permitted him to be admitted into the Franciscan Order, in spite of his superiors’ concerns about his health. Upon assuming the habit of St Francis, he took the name Junípero, which means “Jester of God,” his namesake being a real-life companion of St Francis of Assisi. In 1737 Serra was ordained a priest, and taught theology for seven years at the Llullian University of Mallorca.\nThough he received great acclaim as a professor, Serra was not content with an ordinary academic career. He was imbued with the zeal to visit other lands, so common to Mallorcans like him, who had been seafarers and cartographers for centuries, and indeed common to many Spaniards of Serra’s own time who wished to sail to the far-off “Indies” (as the Americas were commonly called). In addition, he could not forget the heroic tales of the saints that he read as a young man.\nSerra’s dream did not involve discovering wealth or gaining fame as a soldier, but rather announcing the Christian event to those who had not yet encountered it. He was aware, however, that the first disciples had been sent by Christ in teams of two, and so for many months Serra prayed that God would send him a companion.\nIn 1749, his dream became reality, when he met another Franciscan of his province who wished to become a missionary in the Americas. His name was Francisco Palou, who would accompany Fr Serra on many of his journeys and eventually become the author of his posthumous biography. Together with several other Franciscan missionaries, they sailed for the Americas.\nDespite the arduous overseas voyage, when Friar Junipero and his companions finally landed at Vera Cruz on the coast of Mexico, he decided to traveled on foot to Mexico City, while the others et off on horseback. On that journey, Serra’s leg became swollen from an insect bite. This would physically hinder him for the rest of his life, especially in walking.\nWhen he arrived in Mexico City, he studied and prepared for missionary work at the College of San Fernando. Not long afterwards, Serra began missionary work with other Franciscan friars in Mexico’s Sierra Gorda Mountains, where he preached to the native populations and founded new missions in territories that had earlier been extremely hostile to the Christian faith.\nWhile in the Sierra Gorda, Serra earned the respect of his superiors and was named ‘Presidente’ of the missions of the region. He then returned to San Fernando College in 1758, where he once again took up the mantle of professor and taught philosophy for nine years until receiving another call to the missions, in what was considered at that time to be the farthest reaches of the world: Baja California.\n1768 he was appointed to lead a group of fellow Franciscan missionaries to take over the missions of Baja California that had been founded by the Jesuit order. He arrived in Loreto, Baja California in April of that year. In March of 1769 he founded his first mission, San Fernando, Rey de España de Velicatá, in the northern part of the Baja California Peninsula.\nIn July of 1769 he joined an expedition to Alta (upper) California. He consecrated the first mission there on July 16 in what is today San Diego.\nDuring his time in Alta California, Serra oversaw the founding of nine missions:\nHe died at the San Carlos (Carmel) Mission, on August 28, 1784. His earthly remains are interred there.\nOn Sep. 23, 2015, Junipero Serra was declared a saint of the Catholic Church by Pope Francis."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:7e3bae57-4316-4244-b9f2-f49e907856ef>","<urn:uuid:0c41cbcf-4a69-457d-9eae-e09c274a868f>"],"error":null}
{"question":"What was Pericles' role in shaping Athenian democracy, and how did his policies impact citizen participation in government?","answer":"Pericles played a crucial role in strengthening Athenian democracy while serving as strategos from 443-429 BCE. He instituted significant democratic reforms including pay for jury service, which increased citizen participation in government. Under his leadership, he pursued great building projects on the Acropolis and guided Athens during the early years of the Peloponnesian War. While some viewed him as autocratic, Aristotle considered him as making the city more democratic. Pericles believed citizens should be free in their private lives while following law in public affairs, and notably declared that those who did not participate in politics 'had no business' in Athens. His vision of democracy emphasized both individual liberty and civic duty.","context":["Alcibiades: alcibiades, brilliant but unscrupulous athenian politician and military alcibiades' guardian, the statesman pericles, a distant relation, was too he was recalled by the athenian fleet, which remained loyal to the democracy and portrayed in plato's “symposium” view biographies related to categories. The athenian government was a form of 'direct' democracy, in which all citizens source l — pericles' funeral oration: an idealised view of athenaian source n — plato's forms of political governance and the best form. He grew up during the golden age of pericles' athens, served with distinction as a plato's ideas, but in the earlier dialogues—considered by historians to be the three years later, when a tyrannical athenian government ordered socrates. For her romantic relationship with pericles, the leader of democratic athens pericles and aspasia were never married she lived as his companion and was socrates and plato both noted aspasia's influence, which was evident in pericles's children of athena: athenian ideas about citizenship and the division. For, as aristotle concluded 100 years into the athenian democracy in the years around 450 bc the populist athenian general pericles presided be more representative of the opinions and interests of the general he followed his predecessor plato in criticising democracy as a poor form of government.\nLecaire, lucas d, tyranny and terror: the failure of athenian democracy and the reign of the thirty tyrants (2013) plato commented that the reforms of both ephialtes and pericles, athenian political life was dominated by ordinary it also pertains to an interesting theory which explains the creation of the three. Pericles was born into one of athens' leading families cimon for allegedly betraying athens and emerged as the leader of athens' democratic party the glory of ancient greece was far from over—plato was born a year after pericles'. Plato • aristotle 3 setting the stage for close to 50 years (from 477 to pericles he had three goals: (1) to strengthen athenian democracy, (2) to hold as athens grew in wealth, prestige, and power, other city-states began to view it. Ancient greece and more specifically to ideas that prevailed in athens in the fifth and fourth in plato's republic, socrates depicts democracy as the permissiveness of athenian social norms that pericles praises was seen by plato and.\nIntroduction to the greek philosophers socrates, plato and aristotle it through, they tried to talk it through, think about it from a rational point of view this golden age of athens, a time when democracy was flourishing, the age of pericles. Fascinating worns we find his political ideas, ambitions, adventures, approvals and aspects of athenian democracy of ancient greens and plato's reasons for disapproval through his stepfather he had a linn with pericles, who gave his. Furthermore, pericles fostered athenian democracy to such an extent that critics by ancient writers to support a tendentious view of pericles' shiftiness gorgias, in plato's homonymous dialogue, uses pericles as an.\n7 plato, another critic of athenian democracy, laments that there is “no managed to spend a long career unmolested for his views until fairly. Plato's view of the athenian democracy like pericles, plato was an athenian citizen he lived during the peloponnesian war plato had been a student of. With the great athenian statesman pericles the third century he did come to athens: plato's that protagoras held rather democratic views furthermore, a. At all events, plato9 represented some one as inquiring of him thus:— of pericles as rather aristocratic, — in name a democracy, but in fact a wherefore sore repentance fell upon the athenians, and a longing desire for cimon, and that not without a measure of hope, in view of the magnitude of their. Pericles and the causes of the peloponnesian war 127 athens as a model for contemporary democratic theory or practice, classical scholars in the past century view that (arguably) dominated ancient greeks like plato, xenophon, and.\nInstitutions, ideology, and political topography of athenian democracy athens participatory democracy—the view is decidedly mixed in thucydides' history of the peloponnesian war, the politician pericles, in his famous praise of plato's contemporary xenophon (mem 1259) argues that ancient writers and even the. In the speech pericles relates the special qualities of the athenians, redefining pericles' view was obviously a very idealized one, and it ignored the realities of. Parthenon – temple dedicated to goddess athena in athens a new era of radical democracy pericles followed a populist policy in order to gain power socrates sought to uncover errors in conventional views, and by the use of plato student of socrates he rejected democracy because of the death of socrates.\nPlato was far from the only person to have concerns about the athenian democracy many other writers portray the demos as being easily swayed or even bribed. Contrasting views of classical athens: pericles and plato as a result, under his leadership democracy was, in thucydides' judgment, in words (logoi) a. At first, it will have a look at the history of athenian democracy in the related in this circumstance, pericles made a drastic change by enabling 'thousands of plato must have formed his political views based on some. Real democracy cannot be fully understood without knowing its history, indeed, without in which the mass of the people were fully sovereign in fact as well as theory plato attacked pericles directly and blamed the athenians for praising.\nThe play focuses on how cleon stokes mass suspicion of pericles and the fall of athens'31 in his view, thucydides offered a cautionary tale: tragic losses in anderson's view, 'plato was an aristocrat and a homosexual. 'plato and aristotle: the historical background' from 'the greek and by the initiative of her great democratic statesman pericles, the city itself was so adorned as of athenian politics in his early years, culminating (in his view) as it did in the. This is a companion-piece to “the development of athenian democracy,” also the character socrates in plato's protagoras says that “when the athenian.","Constitution of Athens From Cleisthenes to the contemporary constitution Post-Cleisthenic reforms Archons: selection by lot from group preelected by tribes (less aristocratic); Aeropagus (retired archons) lost powers, therefore lost collective identity; More citizens involved; not necessarily wellconnected, or elite; Boule, council, determines agenda of Ekklesia (assembly); 457/6: all offices open to thetes; 462: pay for gov’t service and lowering property qualifications; Post-Cleisthenic reforms, cont. Pay for government service; Most offices not powerful, so significance of these changes difficult to assess; Symbolic significance of masses doing daily business of the city; Elites led reforms and competed with each other in the democratic arena; They displayed their wealth and even gave it away, e.g Cimon, whose lands were unfenced and who could afford huge largesse. Pericles, 495?-429 Wealthy and of good birth (but not as wealthy as his rival Cimon); Strategos, 443-429, only remaining elective post; Great orator: “…first of the Athenians, the most powerful in speech and in action” (Thucydides, 1.139.4); famous “Funeral Oration,” 430; Dies of plague, 429. Seen by some (e.g. Thuc.) as autocratic, even as a kind of monarch; Aristotle saw him as making city more democratic. Pericles’ policies Used influence with other generals (9) to avoid calling Ekklesia; Instituted pay for jury service; Pursued great building projects, e.g. on the Acropolis; Guided Athens during first years of the Peloponnesian war, 431-404 BCE (Athens and her allies vs. Sparta) (Thuc.: war started because of fear of Sparta). Oligarchic Interregnum (late Fifth century) The Four Hundred, founded to make alliance with Athens more attractive to despotic Persia; Deposed by the people; Thirty overthrew democracy, undermined the laws on pretext of restoring the original constitution and killed 1500 people; Thirty overthrown and democracy restored; still in place at time of writing (mid fourth century). Athenian political values: Isegoria [right to speak] Cornerstone of Athenian democracy (Ober, 79); Masses move from passive to active role in Fifth century; Ekklesia: decisions made on the basis of speeches; Led to importance of rhetoric not just for the elite—Sophists, teachers of how to argue (criticized by Plato for twisting falsehood into truth and vice versa). Athenian political values: Isonomia [equality] Democratic cities “aim at equality over anything else” (Pol. 1284a19); Equal opportunity to show one’s merit, and to be honored for it (Thuc., “Pericles’ Funeral Oration); Equality before the law: rich treated same as poor; Equal by nature? No, according to M.H. Hansen (1989), pace Plato and Aristotle; Hansen says term little used; equality never deified as demokratia was, no trireme named for it, Herodotus speaks of it in Greek, not Athenian context; Raaflaub: Herodotus, Thucydides and many other sources attest its importance. Athenian political values: Eleutheria [liberty] Free, as opposed to enslaved; Independence of city from foreign rule; Constitutional: political participation in public and freedom in private (slave could have latter, PseudoXenophon; see also Pericles’ Funeral Oration); Individual right to freedom? Mulgan says no; Hansen says yes; Cites exemption of citizens from corporal punishment, inviolability of the home, no infringement of private property (CA, XLVI); So how did Thirty legally kill Athenian-born persons? Pericles’ formulation Free to live and do as one chooses in private: – “We do not get into a state with our…neighbor if he enjoys himself in his own way….We are free and tolerant in our private lives; but in public affairs we keep to the law. This is because it commands our deep respect.” However, – “each individual is interested not only in his own affairs but in the affairs of the state as well…we do not say that a man who takes no interest in politics is a man who minds his own business; we say that he has no business here at all” (Thuc., Hist of Pelop. War). Concrete example of obligation to participate Refusal to participate carries a price: For example, “…if any man fails to serve as an Arbitrator when his age-group is performing this duty he shall lose his citizen rights, unless he happens to hold public office that year or to be abroad; only these categories are exempt” (C, LIII). One scholar (M.H. Hansen) asserts that complete withdrawal into private life was acceptable, but his is a minority view. Eleven constitutional changes “The eleventh…has lasted to the present day with ever-increasing power being assumed by the people. They have made themselves supreme in all fields,; they run everything by decrees of the Ekklesia and by decisions of the dikasteria in which the people are supreme. For the judicial powers of the Boule have passed to the people, which seems a correct development, for a small number are more open to corruption…than a large [number]” (CA, xli). Aristotle on Citizenship Criteria Cities decide criteria (states do today); “citizen…differs under each form of government; and our definition is best suited to a democracy” (3.1); State = partnership of citizens in a constitution (3.3) Citizen “shares in the administration of justice, and in offices” (3.1); Offices may be continuous (assembly, juries) (3.1); Or discontinuous: general, archon (3.1); Residence alone does NOT confer citizenship (3.1). Good Man/Good Citizen (3.4) Good man: Possesses perfect excellence (arete) Excellence is rule over himself and others (as master of household) Good ruler is both wise and good Good man and good citizen do not necessarily coincide; Good man can be good citizen; He is most likely to share in “honours of the state”. Good citizen Possesses a citizen’s excellence Relative to the constitution Democratic citizen: – Obey like a freeman; – Rule like a freeman; Citizen need not be wise (why?) Good citizen not necessarily a good man (esp. in democracies!). How would Athenian and Spartan citizens differ? Citizenship of “Mechanics” (manual workers), Pol. 3.5 “Is he only the true citizen who has a share of office, or is the mechanic to be included?…if none of the lower class are citizens, in which part of the state are they to be placed?” “….no more absurdity in excluding them than in excluding slaves and freedman….In ancient times, and among some nations, the artisan class were slaves or foreigners…The best form of state will not admit them to citizenship”; “but if they are admitted, then our definition of the excellence of a citizen will not apply to every free man…but only to those who are freed from necessary services” (par. 1277-8). Objections to “Mechanics” as Citizens (Pol. 3.5) Some are rich and meet property qualifications in oligarchies! Thebans only admit businessmen (mechanics) after ten years’ retirement; “no man can practise excellence who is living the life of a mechanic or labourer”; “such a life is ignoble and inimical to excellence” (7.9) Only those who share in the “honours” of the state should be citizens in full sense; Why does Aristotle think this? Aristotle and Athens on Citizenship Aristotle, Pol., Book III Cities determine criteria; Workers = citizens in extreme democracy Best if workers excluded (1278a1); B/c they lack leisure and education to participate in ruling and being ruled; They may serve as oarsmen! Const. Ath. Rigorous examination process Male offspring Two Athenian parents; law on epigamia Free-born; faking carries heavy penalty; Military service Included citizens from lowest classes; No occupational requirement. Citizenship question 1 Aristotle said that “he who has the power to take part in the deliberative or judicial administration of any state is said by us to be a citizen of that state” (pp.63). If we apply this definition to Hong Kong, we, as residents of Hong Kong (as living in a place does not make a person a citizen, I use the word resident), are neither government officials nor legislative councilors; what can we do to fulfill the requirement of “taking part in the deliberative or judicial administration”? Does “voting in Legislative Councilor elections” or “giving opinions to the government about new laws” satisfy the definition? Citizenship question 2 Aristotle mentioned in Book 3.4 that in some states, the excellence of a good citizen is not the same as the excellence of a good man, where the former includes ruling and obeying, the latter includes only ruling; in some states, the excellence of a good citizen and that of a good man is the same, where the good citizen knows 'how to govern like a freeman, and how to obey like a freeman' (p.67 line 14). 1. Do you think the excellence of good men and the excellence of good citizen can coincide with each other? Do you think a ruler can possess both the excellence of a good man and the excellence of a good citizen? 2. Consider the war in Iraq. Do you regard the U.S. soldiers good men? good citizens? Do you regard president George Bush as a good man? a good citizen?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:0e4ac478-023d-4818-940c-f54807eb6b56>","<urn:uuid:a1d5c523-6fc7-4e4a-a16e-c067766fb8ae>"],"error":null}
{"question":"How do high pressure die casting and investment casting compare in terms of surface finish quality?","answer":"Both processes can achieve excellent surface finishes, but in different ways. High pressure die casting produces sharply defined, smooth or textured-surface metal parts through the use of steel molds. Investment casting, due to the hardness of refractory materials used, can produce products with exceptional surface qualities that can reduce the need for secondary machine processes, achieving surface finishes of 1.3–4 micrometres RMS.","context":["High Pressure Die Casting (HPDC) technology\nFAIST Light Metals specialises in high pressure die casting (HPDC) technology for the production of aluminium components. Here we are going to explain how this process works and its features.\nSo, casting means forcing molten metal under high pressure into reusable metal dies. It is often described as the quickest route between raw material and finished product. The finished product also called “die casting” is an accurately dimensioned, sharply defined, smooth or textured-surface metal part.\nThe process has a number of phases:\n- the production of a steel mould able to produce tens of thousands of castings in a few seconds, which is divided into at least two sections to allow the removal of the castings.\n- Mounting of the two sections onto a specific machine where one will be stationary (fixed die half) while the other is moveable (injector die half). They are then clamped tightly together.\n- Injection of molten aluminium into the die cavity where it quickly solidifies.\n- The two sections are drawn apart and the casting is ejected.\nOf course, depending on the complexity of the final part, die casting dies can have moveable slides, cores, or other sections. The complete process is the fastest currently known able to produce precise non-ferrous parts.\nLet’s focus now on the die castings die composition. They are made of alloy tool steels and they have at least two sections:\n- The fixed die half, which is mounted on the side toward the molten metal injection system. It is specifically designed to contain the sprue hole through which molten aluminium enters.\n- The ejector die half, which is mounted on the moveable platen of the machine. It adheres to the other section and it is removed when the die is opened. Usually, it contains the runners (passage ways) and gates (inlets) which route molten metal to the die cavity (or cavities). It is also connected to an ejector box, which holds the mechanism to eject the casting from the die.\nHow ejection works?\nThe opening stroke of the machine involves the pins which are connected to the ejector plate moving forward thus they force the casting from the cavity. They must be carefully arranged so that any force placed upon the casting during ejection will not cause deformation.\nThen, when the die closes, return pins attached to the ejector plate return it to its casting position.\nThe die casting can be adjusted dependent on requirements. If the side of a die casting design requires a depression, one or more slides can be used to obtain the desired result without affecting the ejection of the casting.\nIndeed, if the slides and cores aren’t carefully fitted and securely locked into position during the process, molten metal could be forced into their slideways causing a disruption of operations.\nFixed and moveable cores are often used in dies. If fixed, the core axis must be parallel to the direction of the die opening. If moveable, they must be attached to core slides.\nIn conclusion, even though slides and cores increase the complexity and the cost of die construction, they allow adaptation of die castings to a wide variety of configurations, usually more economically than any other metalworking process.","Investment casting is an industrial process based on lost-wax casting, one of the oldest known metal-forming techniques. The term \"lost-wax casting\" can also refer to modern investment casting processes.\nInvestment casting has been used in various forms for the last 5,000 years. In its earliest forms, beeswax was used to form patterns necessary for the casting process. Today, more advanced waxes, refractory materials and specialist alloys are typically used for making patterns. Investment casting is valued for its ability to produce components with accuracy, repeatability, versatility and integrity in a variety of metals and high-performance alloys.\nThe fragile wax patterns must withstand forces encountered during the mold making. Much of the wax used in investment casting can be reclaimed and reused. Lost-foam casting is a modern form of investment casting that eliminates certain steps in the process.\nInvestment casting derives its name from the pattern being invested (surrounded) with a refractory material. Many materials are suitable for investment casting; examples are stainless steel alloys, brass, aluminium, carbon steel and glass. The material is poured into a cavity in a refractory material that is an exact duplicate of the desired part. Due to the hardness of refractory materials used, investment casting can produce products with exceptional surface qualities, which can reduce the need for secondary machine processes.\nInlet-outlet cover of a valve for a nuclear power station produced using investment casting\nWater glass and silica sol investment casting are the two primary investment casting methods nowadays. The main differences are the surface roughness and cost of casting. Water glass method dewaxes into the high-temperature water, and the ceramic mold is made of water glass quartz sand. Silica sol method dewaxes into the flash fire, and silica sol zircon sand makes the ceramic mold. Silica sol method costs more but has the better surface than water glass method.\nThe process can be used for both small castings of a few ounces and large castings weighing several hundred pounds. It can be more expensive than die casting or sand casting, but per-unit costs decrease with large volumes. Investment casting can produce complicated shapes that would be difficult or impossible with other casting methods. It can also produce products with exceptional surface qualities and low tolerances with minimal surface finishing or machining required.\nA wax pattern used to create a jet engine turbine blade\nCastings can be made from an original wax model (the direct method) or from wax replicas of an original pattern that need not be made from wax (the indirect method). The following steps describe the indirect process, which can take two to seven days to complete.\n- Produce a master pattern: An artist or mould-maker creates an original pattern from wax, clay, wood, plastic, or another material. In recent years the production of patterns using 3D printing has become popular using either standard PLA filament or custom made 'casting wax' filament, in which case one goes directly to step 5.\n- Create a mould: A mould, known as the master die, is made to fit the master pattern. If the master pattern was made from steel, the master die can be cast directly from the pattern using metal with a lower melting point. Rubber moulds can also be cast directly from the master pattern. Alternatively, a master die can be machined independently—without creating a master pattern.\n- Produce wax patterns: Although called wax patterns, pattern materials may also include plastic and frozen mercury. Wax patterns can be produced in one of two ways. In one process, the wax is poured into the mold and swished around until an even coating, usually about 3 mm (0.12 in) thick, covers the inner surface of the mould. This is repeated until the desired pattern thickness is reached. Another method involves filling the entire mould with molten wax and letting it cool as a solid object.If a core is required, there are two options: soluble wax or ceramic. Soluble wax cores are designed to melt out of the investment coating with the rest of the wax pattern; ceramic cores are removed after the product has hardened.\n- Assemble wax patterns: Multiple wax patterns can be created and assembled into one large pattern to be cast in one batch pour. In this situation, patterns are attached to a wax sprue to create a pattern cluster, or tree. To attach patterns, a heating tool is used to slightly melt designated wax surfaces, which are then pressed against each other and left to cool and harden. As many as several hundred patterns can be assembled into a tree. Wax patterns can also be chased, which means parting lines or flashings are rubbed out using the heated metal tool. Finally, patterns are dressed (by removing imperfections) to look like finished pieces.\n- Apply investment materials: The ceramic mould, known as the investment, is produced by repeating a series of steps—coating, stuccoing, and hardening—until a desired thickness is achieved. Coating involves dipping a pattern cluster into a slurry of fine refractory material and then draining to create a uniform surface coating. Fine materials are used in this first step, also called a prime coat, to preserve fine details from the mould. Stuccoing applies coarse ceramic particles by dipping patterns into a fluidised bed, placing it in a rainfall-sander, or by applying materials by hand. Hardening allows coatings to cure. These steps are repeated until the investment reaches its required thickness—usually 5 to 15 mm (0.2 to 0.6 in). Investment moulds are left to dry completely, which can take 16 to 48 hours. Drying can be accelerated by applying a vacuum or minimizing environmental humidity. Investment moulds can also be created by placing the pattern clusters into a flask and then pouring liquid investment material from above. The flask is then vibrated to allow entrapped air to escape and help the investment material fill any small voids. Common refractory materials used to create the investments are: silica, zircon, various aluminium silicates, and alumina. Silica is usually used in the fused silica form, but sometimes quartz is used because it is less expensive. Aluminium silicates are a mixture of alumina and silica, where commonly used mixtures have an alumina content from 42 to 72%; at 72% alumina the compound is known as mullite. During the primary coat(s), zircon-based refractories are commonly used, because zirconium is less likely to react with the molten metal. Prior to silica, a mixture of plaster and ground up old molds (chamotte) was used. The binders used to hold the refractory material in place include: ethyl silicate (alcohol-based and chemically set), colloidal silica (water-based, also known as silica sol, set by drying), sodium silicate, and a hybrid of these controlled for pH and viscosity.\n- Dewax: Once ceramic moulds have fully cured, they are turned upside-down and placed in a furnace or autoclave to melt out and/or vaporize the wax. Most shell failures occur at this point because the waxes used have a thermal expansion coefficient that is much greater than the investment material surrounding it—as the wax is heated it expands and introduces stress. To minimize these stresses the wax is heated as rapidly as possible so that outer wax surfaces can melt and drain quickly, making space for the rest of the wax to expand. In certain situations, holes may be drilled into the mold before heating to help reduce these stresses. Any wax that runs out of the mold is usually recovered and reused.\n- Burnout preheating: The mold is then subjected to a burnout, which heats the mold to between 870 °C and 1095 °C to remove any moisture and residual wax, and to sinter the mold. Sometimes this heating is also used to preheat the mould before pouring, but other times the mould is allowed to cool so that it can be tested. Preheating allows the metal to stay liquid longer so that it can better fill all mould details and increase dimensional accuracy. If the mold is left to cool, any cracks found can be repaired with ceramic slurry or special cements.\n- Pouring: The investment mold is then placed open-side up into a tub filled with sand. The metal may be gravity poured or forced by applying positive air pressure or other forces. Vacuum casting, tilt casting, pressure assisted pouring and centrifugal casting are methods that use additional forces and are especially useful when moulds contain thin sections that would be otherwise be difficult to fill.\n- Divesting: The shell is hammered, media blasted, vibrated, waterjeted, or chemically dissolved (sometimes with liquid nitrogen) to release the casting. The sprue is cut off and recycled. The casting may then be cleaned up to remove signs of the casting process, usually by grinding.\nAdvantages of investment casting\n- Excellent surface finish\n- High dimensional accuracy\n- Extremely intricate parts are castable\n- Almost any metal can be cast\n- No flash or parting lines\nDisadvantages of investment casting\nThe main disadvantage is the overall cost, especially for short-run productions. Some of the reasons for the high cost include specialized equipment, costly refractories and binders, many operations to make a mould, a lot of labor is needed and occasional minute defects. However, the cost is still less than producing the same part by machining from bar stock; for example, gun manufacturing has moved to investment casting to lower costs of producing pistols.\n- It can be difficult to cast objects requiring cores.\n- This process is expensive, is usually limited to small casting, and presents some difficulties where cores are involved.\n- Holes cannot be smaller than 1/16 in. (1.6 mm) and should be no deeper than about 1.5 times the diameter.\n- Investment castings require longer production cycles compared to other casting processes.\nThe variation on the gravity pouring technique is to fill the mold using a vacuum. A common form of this is called the Hitchiner process after the Hitchiner Manufacturing Company that invented the technique. In this technique, the mold has a downward fill pipe that is lowered into the melt. A vacuum draws the melt into the cavity; when the important parts have solidified, the vacuum is released, and the unused material leaves the mold. The technique can use substantially less material than gravity pouring because the sprue and some gating need not solidify.\nThis technique is more metal efficient than traditional pouring because less material solidifies in the gating system. Gravity pouring only has a 15 to 50% metal yield compared to 60 to 95% for counter-gravity pouring. There is also less turbulence, so the gating system can be simplified since it does not have to control turbulence. The metal is drawn from below the top of the pool, so the metal is free from dross and slag (which are lower density (lighter) and float to the top of the pool). The pressure differential helps the metal flow into every intricacy of the mold. Finally, lower temperatures can be used, which improves the grain structure.\nThis process is also used to cast refractory ceramics under the term vacuum casting.\nVacuum pressure casting\nVacuum pressure casting (VPC), properly referred to as vacuum assist direct pour, uses gas pressure and a vacuum to improve the quality of the casting and minimize porosity. Typically VPC machines consist of an upper and a lower chamber—the upper chamber, or melting chamber, housing the crucible, and the lower casting chamber housing the investment mould. Both chambers are connected via a small hole containing a stopper. A vacuum is pulled in the lower chamber, while pressure is applied in the upper, and then the stopper is removed. This creates the greatest pressure differential to fill the molds. The most common materials for vacuum casting process are the high nickel-based alloy and super alloys. Turbocharger products are a common applications for this casting process, though it is also regularly used in the manufacture of silver and gold jewellery.\nInvestment casting is used with almost any castable metal. However, aluminium alloys, copper alloys, and steel are the most common. In industrial use, the size limits are 3 g (0.1 oz) to several hundred kilograms. The cross-sectional limits are 0.6 mm (0.024 in) to 75 mm (3.0 in). Typical tolerances are 0.1 mm for the first 25 mm (0.005 in for the first inch) and 0.02 mm for the each additional centimeter (0.002 in for each additional inch). A standard surface finish is 1.3–4 micrometres (50–125 μin) RMS.\nThe history of lost-wax casting dates back thousands of years. Its earliest use was for idols, ornaments and jewellery, using natural beeswax for patterns, clay for the moulds and manually operated bellows for stoking furnaces. Examples have been found across the world, such as in the Harappan Civilisation (2500–2000 BC) idols, Egypt's tombs of Tutankhamun (1333–1324 BC), Mesopotamia, Aztec and Mayan Mexico, and the Benin civilization in Africa where the process produced detailed artwork of copper, bronze and gold.\nThe earliest known text that describes the investment casting process (Schedula Diversarum Artium) was written around 1100 A.D. by Theophilus Presbyter, a monk who described various manufacturing processes, including the recipe for parchment. This book was used by sculptor and goldsmith Benvenuto Cellini (1500–1571), who detailed in his autobiography the investment casting process he used for the Perseus with the Head of Medusa sculpture that stands in the Loggia dei Lanzi in Florence, Italy.\nInvestment casting came into use as a modern industrial process in the late 19th century, when dentists began using it to make crowns and inlays, as described by Barnabas Frederick Philbrook of Council Bluffs, Iowa in 1897. Its use was accelerated by William H. Taggart of Chicago, whose 1907 paper described his development of a technique. He also formulated a wax pattern compound of excellent properties, developed an investment material, and invented an air-pressure casting machine.\nIn the 1940s, World War II increased the demand for precision net shape manufacturing and specialized alloys that could not be shaped by traditional methods, or that required too much machining. Industry turned to investment casting. After the war, its use spread to many commercial and industrial applications that used complex metal parts.\nUnveiling the titanium integral space bus satellite by Planetary Resources in February 2014. The sacrificial mold for the investment casting was 3D-printed with integral cable routing and toroidal propellant tank. From left: Peter Diamandis, Chris Lewicki, and Steve Jurvetson.\nInvestment casting is used in the aerospace and power generation industries to produce turbine blades with complex shapes or cooling systems. Blades produced by investment casting can include single-crystal (SX), directionally solidified (DS), or conventional equiaxed blades. Investment casting is also widely used by firearms manufacturers to fabricate firearm receivers, triggers, hammers, and other precision parts at low cost. Other industries that use standard investment-cast parts include military, medical, commercial and automotive.\nWith the increased availability of higher-resolution 3D printers, 3D printing has begun to be used to make much larger sacrificial molds used in investment casting. Planetary Resources has used the technique to print the mold for a new small satellite, which is then dipped in ceramic to form the investment cast for a titanium space bus with integral propellant tank and embedded cable routing."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:4af0ee8f-5753-429d-ae1c-0dee481c4fe3>","<urn:uuid:1e2ec118-db68-43ed-8f39-f3aa0697ef5c>"],"error":null}
{"question":"How does the Antonov An-28's fuselage length compare to the American Flea Ship's wingspan?","answer":"The American Flea Ship had a wingspan of 21 feet, while the Antonov An-28 had a fuselage length of 12.98 meters (42 ft 6 inches), making the An-28's fuselage approximately twice the length of the Flea Ship's wingspan.","context":["It is a chapter of Fort Worth history that began, literally, with a wing and a prayer.\nWell, okay, three wings.\nFort Worth has long been a center of aircraft production. Beginning, of course, with Consolidated Aircraft Corporation at 1 Lockheed Boulevard.\nAnd Bell Helicopter at 3255 Bell Flight Boulevard.\nAnd Globe Aircraft Corporation at 180 South Blue Mound Road.\nAnd Universal Aircraft Company at 2826 East Vickery Boulevard.\nThat’s right. An aircraft plant was located in what today is a house on East Vickery Boulevard just north of William James Middle School.\nAs you might guess, Universal Aircraft Company, unlike Consolidated, Bell, and Globe, was not a big company. In fact, as we shall see, ’twas no bigger than a Flea.\nIn August 1927 Cassell D. Hibbs, Clarence C. Holden, and Nathaniel T. Mazza chartered Universal Aircraft Corporation. Hibbs was an entrepreneur: He owned a rubber company, sold real estate and Black Diamond soil conditioner, and manufactured gold-finished derringers. Holden was an auto mechanic. Mazza was president of Fort Worth Macaroni Company. This eclectic trio had big plans for Universal Aircraft: They would build a flying field, flying school, airplane mechanics school, and a plant to produce airplanes. Holden was building an airplane that would be the prototype for production.\nHibbs, Holden, and Mazza intended to capitalize on a fever that was epidemic in America in 1927: flying fever. Flying had captured the imagination of the country, just as the automobile and radio had earlier. In 1927 Fort Worth’s Meacham Field was just two years old. United Airlines was one year old. Pan American Airways had been founded in March. Just weeks before Universal Aircraft was chartered, Charles Lindbergh had made the first solo, nonstop transatlantic flight. Before the end of the year, at least three more transatlantic flights would be made.\nSoon after chartering Universal Aircraft, Hibbs, Holden, and Mazza began advertising sightseeing flights and flying lessons at “our field” at Lake Worth. The company said its first monoplane (with a single wing) would be on the field.\nUniversal Aircraft also made propellers for people who built their own airplanes. The propeller plant was a wood-frame building of 1,300 square feet at 2826 East Vickery Boulevard. Tarrant Appraisal District says the building was built in 1930. The company’s Clarence Holden and wife Lillian lived next door at 2816 in a house built in 1910. Both buildings still stand.\nIn that small wood-frame building at 2826 East Vickery Boulevard workers hand-sawed the lumber and carved the wooden propellers, selling them through the mail from Florida to Michigan.\nLillian Holden’s job in the company was purely clerical: She kept the books.\nBut that was about to change.\nAt some point in the mid-1930s Clarence and Lillian Holden divorced.\nAlso at some point in the mid-1930s Cassell Hibbs got an idea: Instead of making and selling just propellers for people who wanted to build their own airplanes, why not make and sell the entire airplane? He told Lillian Holden that he thought the two of them could design, build, and sell an airplane—in kit form—for just $100 (about $2,000 today). That was far less than the cost of most airplanes at the time.\nLillian Holden was intrigued by Hibbs’s idea.\n“There were so many young men who wanted to buy airplanes then,” Holden recalled in an interview with the Fort Worth Press in 1962. “But they just didn’t have the dollars to buy them.”\nHolden and Hibbs began studying aeronautics through magazines, correspondence courses—whatever could advance their understanding.\n“We drew plans and read and talked to each other and read, and we studied books on aircraft. . . . Nobody helped us,” Holden recalled.\nHolden was an unlikely aeronautical designer. She had dropped out of school after the tenth grade. She had no aeronautical training. She had never piloted an airplane. In fact, she told the Press in 1962: “I have never in my life been off the ground.”\n“But I always had an inventive mind,” she recalled in an interview with the Star-Telegram in 1983.\nHolden felt the weight of her undertaking. She recalled in 1983: “One night, after everybody had gone [from the plant on Vickery Boulevard], I got down on my knees and said, ‘Lord, now listen. Lord, you know I’ve always been afraid of airplanes. Don’t let me build one, not one that somebody gets hurt in. Now you look after it, Lord. I mean it.’”\nSlowly, by trial and error, Holden and Hibbs designed and built their “little cheap airplane.”\nTheir original concept was a triplane. Triplanes, which had been flown since before World War I (Baron von Richthofen flew a triplane), were maneuverable and had low landing and stall speeds. Holden and Hibbs tinkered for months. They experimented with the plane’s center of gravity. They designed the three wings to be interchangeable to reduce costs. They experimented with the degree of stagger of the three wings. Their design had no ailerons on the trailing edges of the wings. Instead the plane used variable-incidence wings for roll control.\nThey also considered building a monoplane and a biplane but always returned to the triplane.\nAt last Holden and Hibbs had translated their blueprints into a prototype. The moment of truth had come. Now, what Kitty Hawk had been to the Wright brothers, Sycamore Park was to be to Holden and Hibbs: Their workers trucked the airplane’s fuselage and three wings to the nearby park to test the airplane’s maneuverability on the ground. They had no intention of trying to get the prototype into the air on that first test. They attached the wings. One of the men climbed into the tiny cockpit. Another spun the propeller. The motor coughed and caught. The man in the cockpit released the brakes, revved the engine. The plane moved.\nIt moved a lot.\n“You just couldn’t keep it on the ground,” Holden recalled. “It would roll forward and then just seem to jump up into the air and sail along, come back to earth and then jump off again. I’ll never forget it. I stood there and watched it, and the tears just rolled down my face.”\nEncouraged, Holden and Hibbs built a second airplane. This one they hoped to get into the air.\nHolden recalled: “Mr. Hibbs wanted to name it the Flea after he saw the first one hopping off the ground at Sycamore Park. There had been a Flea plane in Europe—it killed six people, so I insisted we call it the American Flea Ship.”\nThe European airplane to which Holden referred was Frenchman Henri Mignet’s Flying Flea, a biplane with staggered wings. Some aviation historians say the American Flea was adapted from Mignet’s 1933 design. (Photo from Wikipedia.)\nHolden, encouraged by her success so far and despite keeping both feet on the ground, now dreamed of becoming the Henry Ford of aviation, of bringing affordable, simple, safe private airplanes to Americans. She was convinced that her Flea would suit amateur aviators to a (Model) T.\nUniversal Aircraft began to sell the Flea. Although Universal Aircraft built and sold a few ready-to-fly Fleas, the company mostly sold Fleas as kits. The company also sold just the plans for the Flea. Universal Aircraft began to advertise in aviation magazines: a free airplane with the purchase of a $135 ($2,400 today) motor.\nLater deluxe models (with safety belt, windshield, upholstery, better fabric cover of any color) of the Flea sold for $375.\nHolden’s Flea was built of metal, wood, and cloth. The frame was tubular steel. The Flea had a forty-horsepower Ford or a sixty-five-horsepower Continental motor, a wing span of twenty-one feet, a length of sixteen feet, an empty weight of 460 pounds. The airplane had one seat but could be adapted to seat two or three. The instrument panel contained only three or four instruments. The Flea could climb at six hundred feet a minute to a ceiling of twelve thousand feet and had a range of 225 miles. It could take off in one hundred feet or less of runway and land at thirty miles per hour. Maximum speed was one hundred miles per hour.\nUniversal Aircraft also sold airplane parts and supplies piecemeal. Need an air-cooled cylinder head to convert a Ford Model A motor to aviation use? $22.50. Need a gallon of nitrate dope to cover your airplane’s cloth surfaces? $1.85.\nLillian Holden and the assembly line of Universal Aircraft.\nIn addition to making and selling the Flea, in 1937 the company continued to make propellers. Note that the company made propellers for Harley and Indian motorcycle motors and Ford automobile motors. A motor could be sixty percent of the cost of building an airplane, and amateur builders often turned to motors intended for ground vehicles. (Wiley Post Aircraft Corporation was among companies building airplanes powered by converted Ford Model A motors.)\nIn April 1938 a new company with a familiar name—Universal Aircraft Company—was chartered with two familiar names: Hibbs and Holden. But this time the Holden was Lillian. She was credited as the designer and co-inventor of the Flea.\nLillian retained her ex-husband as an employee.\nAlso in 1938 Universal Aircraft Company patented the American Flea.\nLillian Holden believed in her invention. She claimed that the Flea would never nosedive. Also, because it was a triplane, she said, if the motor failed, the airplane would glide safely to ground.\n“No one,” she told the Press in 1962, “has ever been killed while flying an American Flea Ship.”\nThe Flea also was simple to assemble, she said, “almost like a baby buggy.”\nAnd the Flea was versatile: With the wings off and runners on instead of wheels, it became a motorized sled for ice or snow.\nIn fact, because freight rates for crated aircraft were much higher than for crated ice sleds, Holden and Hibbs shipped many Fleas marked as “ice sled.” Hibbs recalled that once on a New Orleans dock he was questioned about an ice sled marked for shipment to tropical Nicaragua. Only after Hibbs assured the questioner that Nicaragua is in Switzerland was the shipment approved.\nHolden and Hibbs were able to process (assemble, disassemble, crate, and ship) about one Flea a week. They soon were receiving more orders than they could fill. Fleas were shipped to Canada, Mexico, South Africa, Asia, Europe. A Czechoslovakian pilot was so pleased with his Flea that he ordered fifty more, which he wanted to sell, and asked for the Flea franchise for his country.\n“But we just couldn’t do it,” Holden recalled. “We didn’t have the money to mass produce the plane. We had orders from everywhere, but we couldn’t expand, and we couldn’t get a loan because we had no real collateral.\n“They didn’t pay us enough, and all I’d get out of one plane was enough to build the next one,” Holden said.\nHolden and Hibbs built and sold about four hundred Fleas.\n“We were tired, Mr. Hibbs and I. We knew it was ready for somebody to take it over.”\nSo, Holden and Hibbs began looking for a manufacturer who could mass produce the Flea.\nDuring more than forty years a few manufacturers were interested in or actually did buy the rights to the Flea, Holden said. For example, she said, Holden and Hibbs sold the rights to their design to Ace Aircraft Manufacturing Company.\nTo Holden’s consternation, Ace Aircraft never produced her design. After a long legal battle, Holden recovered the rights to her design. She also bought out Cassell Hibbs’s share of the company, became the sole patent owner, and continued to seek financing elsewhere.\nIn 1952 Holden moved to 2809 Meadowbrook Drive.\nIn her new home Holden did business as American Tri-Plane Enterprise. Predictably, as a woman working in a “man’s field,” she encountered some strong headwinds. But she persevered. She sold “a good many” of her plans at $99.95. She estimated it would cost $2,000 to $3,000 to build her plane to Federal Aviation Administration specifications.\n“I know it flies. I know it’s safe,” she said. “No other little plane can come anywhere near it. I’d be a fool to sit down and let it rot. And I’m not going to do it.”\nShe was proud when her airplane was featured on the cover of Trade-A-Plane magazine.\nAnd in 1970 the American Aircraft Association presented her with a plaque recognizing her as the “aircraft designer and co-inventor of the American Flea.”\n“I’m a co-inventor of this plane. Some people say, ‘Oh, what a nice souvenir.’ ‘Souvenir’ my eye. I want to get it going. I haven’t given it up by any means. I’m just slow because I’m alone. I just feel that it’s something that I can’t let go.”\nBut in 1970 the 1,300-square-foot plant of Universal Aircraft at 2826 East Vickery Boulevard was for sale, lock, stock, and propeller. The wood shop was later used to make dollhouses.\nHolden continued to advertise the Flea, to sell plans, and to seek mass production.\nBy the time Lillian Holden was interviewed by the Star-Telegram in 1983, this aviation pioneer had flown all of two times in her ninety-two years.\nBut even at age ninety-two she retained an evangelistic zeal for her invention, convinced that with financing she and the Flea could bring flying to the general public.\n“This plane has a history of forty years. Now, some people probably think, ‘Oh, that old woman don’t know what she’s doing’—those that don’t understand aircraft. I’m not after money. I want to give somebody something from my life. I want to give it to somebody, and they can get the FAA approval, and then they can build it on an assembly line. What I want to do before I pass out, the Lord willing, is to put this thing on the market like Ford did with his car. If it catches on, it’ll spread, just like cars did.\n“It’s a fortune for somebody. People don’t want to buy those big, expensive planes. The country today—they’re begging for a cheap, light plane, and I’ve got it. And here I sit.”\nLillian Holden in 1983 at age ninety-two.\nLillian Holden died soon after her 1983 interview, her dream of becoming the Henry Ford of aviation unfulfilled. She is buried in Mount Olivet Cemetery.\nHer Flea was never mass produced.\nBut a few of Lillian Holden’s airplanes are still flying. Others are on display in museums. In fact, this one hangs from the ceiling of Vintage Flying Museum at Meacham Field.\nFIFI and the Flea: At Vintage Flying Museum, above that yellow forklift the tiny Flea hangs next to the behemoth B-29 FIFI. The B-29’s ninety-nine-foot fuselage is long enough for the Flea to take off from. The length of the Flea’s fuselage and the diameter of the B-29’s four-blade propeller are the same: sixteen feet.\nFarther afield, this Flea is in the Wings of a Dream Museum in Brazil. (Photos from Wikipedia.)\nSuch survivors are snapshots of a time when a Flea and even a woman who had both feet on the ground could reach the sky.\n(Thanks to Earl Belcher for the tip and to the staff of Vintage Flying Museum for the assistance.)\nPosts About Aviation and War in Fort Worth History\nPosts About Women in Fort Worth History\nHello! I am preforming a research on Lillian Holden and would love to know your sources for it! If you still have them feel free to shoot me a email (if you can see it) with them if you can (or add them to the blog).\nMany Thanks for the read!\nMost of the newspaper clips are from the Record and Star-Telegram. The city directory is, as shown, from 1930. The Vintage Flying Museum has some blueprints, ads and other printed material. Other material was found the old-fashioned way–by Googling terms such as Flea, Holden, Universal Aircraft.\nI met Lilian when she was 82 and had Many wonderful conversations with her the last years of her life. I was a twenty eight-year-old man with a fascination for the elderly and their life’s stories. Lilian did not disappoint. She was the most interesting character I have ever met. Feel free to contact me and maybe I can fill in a few blanks for you. She inspired me to become a writer. I am now 69 and have written a novel loosely based on her amazing life and stories she confided to me. The name of my novel is Lily Of The Sky. It is available on amazon","Antonov An-28 / An-38\nAntonov An-28 / An-38\nThe Antonov An-28 and An-38 are small twin-engine turboprop aircraft intended as short-range airliners and suitable for a number of other duties.\nThe An-28 (NATO reporting name 'Cash') was developed from the An-14 'Pchelka' piston engine aircraft (NATO: 'Clod'), a small twin-engine utility aircraft developed during the 1950s as a replacement for the Antonov An-2 biplane. It seated up to eight passengers and first flew on 15 March 1958. Serial production started in 1966 and until 1972 around 300 aircraft were built. The An-14 was fitted with two Ivchenko AI-14RF radial piston engines and twin rudders. The aircraft had excellent short take-off and landing (STOL) capabilities.\nAntonov became the winner of an Aeroflot competition between the An-28 and the Beriev Be-30 for a small, short-range airliner during the late 1960s. The An-28 was originally designated 'An-14A' and the prototype was named 'An-14M'. The aircraft has a stretched fuselage and turboprop engines but it kept its predecessor's wing structure and twin rudders. It can be equipped with skis and float-type landing gear. The aircraft seats up to 20 passengers and is flown by a two-man crew. It can also be used as a cargo aircraft, for search and rescue operations, as an air ambulance and other tasks.\nThe An-28 first flew in September 1969 and the first pre-production model made its maiden flight in 1975. After a short pre-production series built by Antonov itself, al subsequent aircraft were built under licence in Poland by PZL Mielec. The first Polish built aircraft flew in 1984. PZL Mielec also developed its own improved variant, the PZL M28 Skytruck (An-28PT), with Pratt & Whitney PT6A engines. It first flew on 22 July 1993 and is still in production. The original An-28 remained in production until 1993 after around 200 were built.\nThe AN-38 is a further stretched and improved version of the An-28 seating up to 27 passengers or carrying 2,500 kg of cargo. It has more powerful engines, an improved cockpit and better passenger cabin comfort than the An-28. The crew can easily convert the cabin from cargo to passenger layout.\nThe An-38 has a fixed tricycle landing gear, fitted with low-pressure tyres to enable it to land and take off on unpaved and ice and snow covered runways. The aircraft is equipped with a hand-driven overhead-track hoist for handling cargo without the support of airport ground equipment.\nThe An-38 can be fitted with Honeywell TPE331 or Omsk TVD-20 turboprops, and with Western or CIS avionics. The first flight with the TPE331-version, the An-38-100, took place on 23 June 1994 and certification followed in April 1997. The Omsk TVD-20 version, the An-38-200, first flew on 11 December 2001 and was certified in November 2002. Subversions of the An-38-100 are the An-38-110 and An-38-120 with different avionics suites. The AN-38K is a freighter version. There are also variants for VIP transport, surveillance and other tasks.\nThe An-38-100 is produced by NAPO in Russia. Around a dozen of the type have been built.\nAntonov An-28 Specifications\nWingspan: 22,06 m (72 ft 4 in). Length: 12,98 m (42 ft 6 in). Height: 4,60 m (15 ft 1 in).\nEmpty weight: 3,900 kg (8,600 lb). Max. takeoff weight: 6,100 kg (13,450 lb).\nCapacity: 18 passengers. Range: 510 km (270 nm). Cruise speed: 350 km/h (190 kts).\nEngines: two Glushenkov TVD-10B or Pratt & Whitney Canada PT6A-65B turboprops (960 shp - 720 kW).\nM28 Skytruck Specifications\nWingspan: 22,06 m (72 ft 4 in). Length: 13,10 m (43 ft). Height: 4,90 m (16 ft 1 in).\nEmpty weight: 4,100 kg (9,309 lb). Max. takeoff weight: 7,500 kg (16,535 lb).\nCapacity: 19 passengers. Range: 1,500 km (810 nm). Cruise speed: 270 km/h (146 kts).\nEngines: two Pratt & Whitney Canada PT6A-65B turboprops (1,100 shp - 820 kW). (Photo: PZL Mielec/United Technologies)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:03fd6bf9-0284-455f-8bd0-6866456271d4>","<urn:uuid:4f009943-1566-4fe9-bf40-d1d5fa5fca4b>"],"error":null}
{"question":"What is the legend associated with Castel dell'Ovo in Naples and why is it named that way?","answer":"Castel dell'Ovo (Egg Castle) in Naples derives its name from a legend about the Latin poet Virgil placing an egg in its foundations. According to the legend, this egg keeps the entire structure standing, and if it were to break, a series of catastrophes would befall the entire city. This is the oldest castle in Naples and stands on an islet of tuff.","context":["Italy is full of castles and fortresses visited every year by thousands of tourists, eager to learn more about their history and their mysteries while they stroll through the great halls and silent courtyards. Great towers perched on the top of high slopes, mighty stone fortresses, and majestic manors surrounded by vast landholdings are capable of catapulting us into a world of other times, between history and knightly legends. Castles are places that always retain something mysterious and that have a lot to tell. So let’s see which are the most beautiful castles in Italy.\n- 1 Scaliger Castle of Malcesine – Veneto\n- 2 Scaligero Castle of Sirmione – Lombardy\n- 3 Aragonese Castle of Ischia – Campania\n- 4 Miramare Castle – Friuli Venezia Giulia\n- 5 Castel dell’Ovo – Campania\n- 6 Castel Nuovo – Campania\n- 7 Fort of Bard – Aosta Valley\n- 8 Fenis Castle – Aosta Valley\n- 9 Castel Sant’Angelo in Rome – Lazio\n- 10 Castel del Monte – Puglia\n- 11 Fortress of Brisighella – Emilia Romagna\n- 12 Bardi Castle – Emilia Romagna\n- 13 Marostica Castle – Veneto\n- 14 Gradara Castle – Marche\n- 15 Manta Castle – Piedmont\n- 16 Gabiano Castle – Piedmont\n- 17 Lucera Castle – Puglia\nScaliger Castle of Malcesine – Veneto\nThis medieval castle stands on a rocky outcrop in Malcesine, on the shores of Lake Garda. Rebuilt by the Scaligeri in the XIII century on a previous building, the castle keeps with the typical dovetail battlements. It also has a privileged position because it enjoys a wonderful view of Lake Garda. The castle, which passed over the centuries under the dominion of various powers, was also made famous by the writer Goethe’s drawings and descriptions in his “Journey to Italy” at the beginning of the XIX century. A Natural History Museum has been set up inside the building at the current entrance to the castle. In addition, the castle also houses a museum dedicated to Goethe.\nScaligero Castle of Sirmione – Lombardy\nAlso, on Lake Garda, you can admire another Scaligero castle: that of Sirmione, in Lombardy. Its historic center is a peninsula stretching out over the lake and offers breathtaking views. The majestic and beautiful castle dominates the landscape and is one of the best-preserved examples of lake fortification. It is probably dating back to the XIII century. The fortress of Sirmione is one of the most beautiful castles in Northern Italy and is characterized by the fact that it is bathed on all sides by Lake Garda’s waters. The castle now houses a museum that tells its history, a Roman and medieval lapidary, and offers the possibility of walking on the walkways.\nLike many Italian castles, the Scaliger Castle also has its own legend, which wants that a knight’s soul still wanders within its walls in search of his beloved. One more reason to visit this fortification on Lake Garda.\nAragonese Castle of Ischia – Campania\nThe first castle on the island of Ischia dates back to the V century BC. Later, after many historical events, the Aragonese – in the XV century – gave the fortress its current appearance, which looks like the Maschio Angioino of Naples. The castle stands on an islet and is connected by a 220 m long brick bridge to the ancient town of Ischia. The period of maximum splendor of the castle occurred at the end of the XVI century: at the time, the castle housed 1892 families, the convent of the Poor Clares, the abbey of the Basilian monks, the bishop, the seminary, the prince with his garrison. There were also 13 churches, including the cathedral. Today buildings cover a small part of the islet’s surface, which is mostly occupied by ruins, vegetable gardens, and vineyards.\nMiramare Castle – Friuli Venezia Giulia\nThis elegant and refined castle was the Habsburg court’s residence: it was built between 1856 and 1860 in Miramare, not far from Trieste, as the home of Maximilian of Habsburg-Lorraine, Archduke of Austria and later emperor of Mexico, and his consort Charlotte of Belgium. Lapped by the Adriatic Sea waters, the castle is surrounded by a luxuriant park full of rare plants. In the park, there is also the “Castelletto” (little castle), a smaller building that served as a residence for the two royal spouses during the construction of the castle itself but which became, in fact, a prison for Carlotta when she lost her reason after the murder of her husband in Mexico. Inside, the castle is divided into numerous rooms and still retains the original furnishings. The ground floor was intended as the residence of Emperor Maximilian I and his wife, Carlotta. In contrast, the upper floor was later used as Duke Amedeo d’Aosta’s residence, who lived there for about seven years.\nCastel dell’Ovo – Campania\nCastel dell Ovo is the oldest castle in Naples. Powerful and beautiful, it seems to derive its name from the legend of an egg placed in its foundations by the Latin poet Virgil: the egg keeps the whole structure up, and if by chance it were to break down, a series of catastrophes would begin for the whole city. The fortress stands on an islet of tuff, and its origin is lost in the mists of time: from the Romans to the Angevins, passing through Emperor Frederick II, this fort tells the great history of the city of Naples and beyond.\nCastel Nuovo – Campania\nAlso called “Maschio Angioino,” the imposing Castel Nuovo (New Castle) of Naples is one of the historic center’s main monuments, as well as one of the symbols of the city. Built-in the XIII century and enlarged in the XV century, the castle is characterized by late medieval and Renaissance architecture. The “New” castle was built to guard the city against enemy raids; in fact, the position in which it was built was of strategic importance. The castle shows itself today as a stratification of many eras, and today it hosts cultural events and shows. The castle is also the permanent seat of the Civic Museum and the library of the Neapolitan Society of Homeland History.\nFort of Bard – Aosta Valley\nBard is a beautiful town in the Aosta Valley dominated by a truly imposing Fort, an intact example of an early XIX century fortress. At the end of the XIX century, the fort began to decline, first used as a prison and then as an ammunition depot. Decommissioned in 1975 from the military state property, it was acquired by Aosta Valley Region in 1990 and completely renovated in 2006. The complex consists of three different buildings, located on three levels: a bulwark of almost 15,000 m of surface, inside the beautiful Museum of the Alps, dedicated to mountains. In addition to the Museum of the Alps, there are the prisons, which host a multimedia thematic itinerary on the history of the Fort.\nFenis Castle – Aosta Valley\nAnother masterpiece to see in Aosta Valley is absolutely the Fenis Castle, one of Italy’s most famous and most beautiful manors. It has a pentagonal plan, with towers at each corner and a double enclosure of crenelated walls: a scenographic building which, combining the characteristics of the fortification with those of the noble residence, made it a prestigious representative place for the greatest exponents of the Challant family, who they endowed the castle with an imposing defensive apparatus, as well as elegant pictorial decorations, symbols of power and prestige. Today the building is owned by Aosta Valley Region and can be visited. The visit to the ground floor develops through the armory, the refectory for soldiers and servants, the pantry, and the kitchen with a large fireplace. Continue up to the first floor, where you can see the chapel with the adjoining boardroom, the lord’s bedroom, the great kitchen, the dining room, and the hall of justice.\nCastel Sant’Angelo in Rome – Lazio\nThe Castle rises along the Tiber River and can be reached by crossing the famous Ponte degli Angeli (Angels’ bridge). Castel Sant’Angelo, also called Hadrian’s Mausoleum because it was born as a funeral monument of the emperor, was modified many times during the Middle Ages and the Renaissance until it reached its current appearance. It was a prison, a Renaissance residence, and a museum from a funerary monument to the fortified building.\nConsidered one of the leading Italian fortresses, Castel Sant’Angelo is also known for its proximity to Vatican City and being a witness to two thousand years of history. Over time, it has experienced the most disparate political and religious vicissitudes, mainly as a papal fortress and as a prison and barracks. Today it houses the Castel Sant’Angelo Museum, with its Renaissance halls, walkways, the library, the Treasure Room, and gardens.\nCastel del Monte – Puglia\nWanted by Emperor Frederick II, Castel del Monte, in Andria, is one of Italy’s symbols, so much so that it has also been declared a World Heritage Site by Unesco. The castle has an octagonal plan, and again, the towers, at each of the corners, have an octagonal shape. The castle once enjoyed rich plant and mythological decorations. Simultaneously, the use of limestone, white marble, and coral breccia was specially designed for the chromatic effect that still strikes the visitor today. Infused with symbolism, this XIII century fortress lived through the troubled history of the Norman and Lombard conquests in Southern Italy and the Kingdom of Naples’ vicissitudes. The fortress can be visited freely and offers the visitor the opportunity to dive into the past of one of the most famous and important fortified structures in Italy.\nFortress of Brisighella – Emilia Romagna\nIn Brisighella, a few kilometers from the city of Faenza, the majestic fortress stands, together with the Clock Tower, built in the XIV century by the Manfredi family, then passed to the Borgias, then to the Venetians, and finally to the Papal State. This building recalls the medieval centuries. Articulated around a vast internal courtyard with a trapezoidal shape, the fortress has two circular towers at the north-east and south-east corners, which flank the entrance, whose superimposed rooms have lent themselves to being set up as exhibition and multimedia spaces. In fact, today, the fortress is home to the museum dedicated to the relationship between man and plaster.\nBardi Castle – Emilia Romagna\nIn the province of Parma, the Castle of Bardi is one of the most significant examples of a medieval fortress in Italy and can be fully considered one of the most beautiful castles in Europe. The manor dates back to the IX century and is entirely made of stone; it is located on a hill, in a dominant position over the valley below. In addition, like all the best castles, even that of Bardi is associated with the legend of a ghost. According to some local inhabitants, in fact, the manor is haunted by the spirit of a handsome knight, who committed suicide on his return from the war after learning of the untimely death of his beloved woman.\nMarostica Castle – Veneto\nMarostica is a beautiful town in the province of Vicenza. Under the dominion of Cangrande della Scala, it was redeveloped. It took on the aspect it largely remains today, with the center within the walls that connect the two castles, the Upper Castle and the Lower Castle. Marostica is also famous worldwide for the Chess Game with Living Characters, an event that takes place every two years and involves about 600 characters in period costumes.\nGradara Castle – Marche\nIn the Marche region, in the province of Pesaro-Urbino, stands the fairytale village of Gradara, which develops around the Malatesta fortress, dating back to 1150 and the subject of successive extensions and renovations. The fortress and the adjacent village are protected by an external wall that extends for almost 800 meters, making the entire structure imposing. The village also offers enchanting views from its narrow streets and alleys that rise to the castle. Over time, the castle has gradually become one of the most visited monuments in the region and is home to museums, musical and artistic events.\nManta Castle – Piedmont\nThis spectacular medieval fortress at the foot of Mount Monviso deserves to be visited above all for the amazing late Gothic frescoes in its baronial hall. The paintings inspired by the tales of chivalry, in fact, leave you breathless and transport the visitor into a world of knights and bridesmaids. After exploring the fortress rooms and gardens, do not forget also to visit the castle church attached to the main building. Here, in fact, you can admire equally beautiful and well-preserved frescoes, this time obviously with a religious theme.\nGabiano Castle – Piedmont\nIt is an ancient manor located on a hill, surrounded by vast lands and vines, in Alessandria. This majestic castle nestled among the gentle slopes of Monferrato, in fact, boasts a magnificent view over the surrounding valley and is particularly renowned for its wine production and the grandiose hedge maze of its historic garden. The location of the labyrinth in the heart of the park emphasizes the contrast between the rigid and geometric lines of the labyrinth and the natural park that surrounds it, recalling the medieval concept of a forest as a natural labyrinth and a labyrinth as an artificial forest where nature is rigorously manipulated and controlled by man.\nLucera Castle – Puglia\nThe Castle of Lucera is a magnificent Swabian-Angevin fortress in the province of Foggia, which dominates the surrounding area thanks to its elevated position. The central structure dates back to 1233, while the imposing external fortified walls are from just a later period. The castle’s mighty walls are certainly the most impressive component of the entire structure: 900 meters long, 13 meters high, and interspersed with numerous towers, buttresses, and bastions, it is clearly visible from many kilometers away. Finally, the entire hilly area on which the fortress rests is a very interesting archaeological area, with Neolithic and Roman remain."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:40b0e16a-972a-4bd2-927d-708dd0178251>"],"error":null}
{"question":"What are the essential requirements for vaccine cold storage according to CDC guidelines, and how do global infrastructure differences impact cold chain management?","answer":"According to CDC guidelines, vaccines must be stored in medical grade laboratory, pharmacy and biomedical refrigerators/freezers to maintain the cold chain from manufacturing through administration. The entire process requires consistent temperatures of 2-8°C, with manufacturers, distributors, and healthcare providers sharing responsibility for maintaining proper storage. However, the global infrastructure for cold chain management varies significantly between countries and locations. Despite advances in cold chain technologies, basic infrastructure limitations in different regions can compromise temperature control. This lack of uniform infrastructure worldwide poses a significant challenge to maintaining consistent cold chain requirements across international borders.","context":["CDC Recommendations for Approved Laboratory, Medical, and Biomedical Refrigerators and Freezers for Vaccine Storage\nMaintaining a cold chain through-out the entire vaccine supply chain is critical to the safety and security of vaccines. According to the CDC, the cold chain begins with the cold storage unit at the manufacturing plant, extends to the transport and delivery of the vaccine and proper storage at the provider facility, and ends with the administration of the vaccine to the patient.\nAdditionally, the CDC stresses in its Vaccine Storage and Handling Pink Book that manufacturers, distributors, public health staff, and health care providers share responsibility to ensure the vaccine cold chain is maintained from the time vaccines are manufactured until they are administered.\nTo ensure safe vaccine storage, medical grade laboratory, pharmacy, and biomedical refrigerators and freezers must comply with … Read the rest\nSelecting the best\nultrasonic cleaner for your job should involve the same careful thought\nprocesses you'd go through when making other important purchasing decisions -\nsuch as an automobile or computer. What\nfeatures are important to you? Balancing\n\"needs\" with \"nice to have\" applies to any significant\nWhen you arm yourself with these 10 tips you are on the way to select the ultrasonic cleaner to do the jobs you need to do at the best price. The 10 tips are as follows:\n- Ultrasonic Cleaner Tank Dimensions\n- How are you using your ultrasonic cleaning equipment?\n- Add Baskets to the Spec Sheet for your Ultrasonic Cleaner\n- How to Specify Ultrasonic Frequency\n- How to Manage Ultrasonic Cleaning Solution Temperature\n- A Degas Mode Speeds Solution Preparation\n- The Benefits of the Sweep Mode\n- The Pulse Mode Boosts Ultrasonic Power\n- Understanding Ultrasonic Power\n- Selecting Accessories\nNo time to review the complete article? No problem.\nSample preparation is defined by the International Union of Pure and Applied Chemistry (IUPAC) as the process used to transform analytes into a measurable form. Several terms are used to describe the equipment used for sample prep, among which are sonicator baths.\nFor example, in Sample Preparation of Pharmaceutical Dosage Forms, “sonicators, also referred to as ultrasonic extraction, can be considered both particle size reduction techniques and an agitation technique. Because of its ability to disperse, mix and dissolve samples, it is widely used in the sample preparation of dosage forms.”\nSonicator baths, sonicators and ultrasonic cleaners are terms describing equipment commonly employed in drug design and development. Because the term “ultrasonic cleaner” is a bit misleading in this context (normally associated with thorough cleaning of parts) we’ll use sonicator bath in this post.\nThe Purpose of Sonicators\nSonication is one of the processes used in sample preparation to … Read the rest\nWhen it comes to explaining how to calibrate a digital scale, here’s a bit of reassurance: Calibrating a digital scale is not that complicated. While scale calibration is important, here you will learn why and how often digital scales should be calibrated.\nWe’re talking not only precision scales and analytical balances able to weigh in grams and milligrams but also industrial scales weighing in pounds, sales used for recipes, determining percentages, totalizing, counting and other functions used in industry today.\nWhy you Should Know How to Calibrate a Digital Scale\nFirst of all, in many instances it’s required by law. For example, calibration standards used for inspection, measuring, and test equipment shall be traceable to national or international standards.\nOne of these is the National Institute of Standards and Technology’s Weights and Measures Program Requirements, which provides detailed information on this topic.\nThe FDA document observes that if national … Read the rest\nLet’s start out by explaining why you may need a moisture analyzer. One example (of many) is in the food industry. As described in Food Quality and Safety “International and national standards define the permitted thresholds for moisture content in commercially sold products.”\nAfter listing some of these standards, the article continues “….this translates into increased workload around quality assurance and the development of efficient and cost-effective solutions. According to the stated legal requirements, methods of analysis and procedures must be clearly described and tested.”\nMoisture analyzers are widely used in meeting these standards. They go far beyond the food industry – affecting such diverse products as pharmaceuticals, plastics, paint, wood, birdseed, soybeans, fertilizer, cement, detergent, shampoo…..etc.\nSo, What is Moisture?\nExcellent Question. For the subject of this post, moisture is anything that evaporates on heating.\nIn addition to water this may include fats, oils alcohol and solvents.\nPlease note … Read the rest\nUltrasonic cleaning with volatile (flammable) solvents creates what the NEC and NFPA term a hazardous location. Conformance requires that you follow strict procedures including selecting an explosion proof ultrasonic cleaner or taking special precautions when using low flash point volatile solvents for cleaning. This post describes volatile solvents, where they are used in ultrasonic cleaning operations and how to comply with regulations including explosion-proof ultrasonic cleaners.\nWhat is a Flash Point?\nA flash point is the temperature at which a particular organic compound gives off sufficient vapor to ignite in air when given an ignition source. Gasoline fumes are an example. Although you can extinguish a match by dunking it in gasoline, such an exercise is not recommended because it is the gasoline fumes that ignite.\nIn view of this, selection of an ultrasonic cleaner must take into account that not only do volatile solvents evaporate, but the heat … Read the rest\nUltrasonic cleaning is among recommendations in the CDC’s Summary of Infection Prevention Practices in Dental Settings to remove contaminants from reusable dental instruments. Cleaning before sterilization avoids heat “baking on” blood, tissue and other organic residues that cause infection problems. As described in this post dental instrument ultrasonic cleaning systems are also ideal for cleaning molds, implant hybrid prosthesis, and removing plaster and cement from bridgework.\nWhy You Need an Ultrasonic Dental Instrument Cleaner\nUltrasonic energy creates billions of minute vacuum bubbles in an ultrasonic cleaning bath that implode with tremendous force when they contact dental instruments.\nThe process, called cavitation, reaches into tiny cracks and crevices, quickly and safely blasting loose and carrying away contaminants on dental instrument surfaces. It is a much faster … Read the rest\nLiposomal encapsulated vitamin C offers a high absorption efficiency vs. taking vitamin C pills and is much more convenient than intravenous administration. Due to the Coronavirus pandemic there has been an increase in demand for vitamin C as a means of protection or treatment while a recent report states that demand for liposomal vitamin C has outpaced that for hand sanitizers*. Because liposomal vitamin C has a short shelf life, this post tells you how to make it yourself using an ultrasonic bath and a beaker kit.\nWhy Ultrasonic Energy Delivers Superior Liposomal Vitamin C\nWhile it uses a benchtop ultrasonic cleaner, the process employs the power of what is called ultrasonic cavitation to encapsulate ascorbic acid within liposomes. Encapsulation improves the bioavailability of the vitamin C in your body.\nUltrasonic processing is also preferred as a means of reducing … Read the rest\nWhile the search for a Coronavirus vaccine proceeds at full throttle a crucial issue is proper collection, labeling, storage and shipping of COVID-19 specimens to CDC laboratories for testing. Among procedures spelled out in the CDC’s Interim Guidelines is that coronavirus specimen shipments must be stored at 2-8⁰C and shipped overnight on icepack for testing within 72 hours. If a delay in shipping is expected, specimens must be stored at -70⁰C or lower. These agree with a companion guideline for post-mortem testing.\nTemperature Guidelines for COVID-19 Test Kits\nBecause there are so many sites collecting specimens there is a national shortage of reagents for the tests. Test kits are available for real-time PCR and in vitro diagnostic procedures. One manufacturer specifies storage temperatures from -10⁰ to -30⁰C down to ≤-70⁰C depending on the components of the kits.\nTo meet these storage requirements you should be familiar with\n- Scientific Refrigerators\n- Scientific Freezers\nThe American Dental Association releases guideline updates on how dental technicians can protect themselves during the current Corona Virus pandemic. The ADA on March 16 encouraged dental practitioners to postpone elective procedures for the next three weeks. It also refers dentists to conform to CDC guidelines for sterilizing and disinfecting patient care items and devices.\nThese guidelines refer to reusable dental instruments, and among other techniques recommend the use of ultrasonic cleaners as a pre-sterilization or disinfecting procedure to render them safely for reuse.\nUltrasonic Cleaners In CDC Recommendations for Covid-19 Infection Control\nThe CDC’s 2008 Infection and Sterilization Guideline notes that “The most common types of mechanical or automatic cleaners are ultrasonic cleaners…” and that “Ultrasonic cleaning removes soil by cavitation and implosion in which waves of acoustic energy are propagated in aqueous solutions to disrupt the bonds that hold particulate matter to surfaces.”\nNote that ultrasonic cleaning … Read the rest","Cold Chain Logistics is no different than any other logistics operation having all the perils of the normal logistics and to add on it also has its big share of dealing with all the usual adversaries that affect a normal logistics as well as many others of its own. Many important aspects have to be considered as a risk by a freight forwarding company during handling cold chain transporation.\nThere can be seemingly unrelated events in providing cold supply chain logistics solutions that can impact a shipment out for delivery and which in no way can be handled by the logistics team. This is more when it comes to temperature-controlled logistics and that is the inherent volatility of a supply chain. Some of the many things that could affect changes in any shipping and logistics strategies can be socio-political unrest, wars, geological events including labor shortages.\nHence from a shipper’s perspective what one can do is to mitigate the risk and expect the worst and strategically evaluate the best options particularly while handling high value and volume shipments. As it’s imperative to say that the supply chain is a costly business in particular when you need to handle a cold chain.\nThe cost involved while handling cold chain is one that cannot be ignored especially when you evaluate the importance of transporting highly valuable and life-saving items. Hence means and modes have to be taken into consideration and evaluated meticulously to reduce cold chain management risk while still managing your supply chain expenses.\nLet us understand the 5 things that can go wrong despite cold chain risk management.\nGlobally uniformity in infrastructure: Logistics is all about transporting goods from one location to the other and many a time it also crosses international borders. Though there have been scientific advances that have given us the leverage of more efficient and reliable cold chain technologies yet one cannot expect the same kind of uniform infrastructure in every country or location. Hence many times cutting edge tech can often stumble due to the lack of the simplest requirement of infrastructures.\nRegulations implemented on cold chain management :\nSupply chain managers must be well updated with the global scenario of events and regulations across countries. This always includes the various political and socio-economic developments that many times can force the logistics company to even change their route plans. This can have serious effects when you are handling cold chain transportation. It is not only the political and economic scenarios that can post a threat to your transportation but also the climatic and weather patterns that also can contribute to disruption in operations.\nHigh-security risk in transportation and handling :\nImagine you have a temperature-controlled and sensitive vaccine that needs to be transported and in particular, one that has reentered a supply chain operations after been once stolen. In such cases, improper security measures can be a daunting task as it will affect the likelihood of damages that will go unnoticed. This can be a danger to human life as well.\nRetailer handling in the cold chain:\nLogistics is never completely free from human handling and one of the major challenges is to be sure how the retailers handle and store cold chain products when it comes to food and vaccines. Retailer’s understanding of how the products need to be handled and the way it has to be stored is a major threat to the quality of such products as this if not properly done can be harmful to the end consumers.\nDistribution and delivery risk:\nThere are two aspects in a distribution process which is one that of transporting of goods and the second being how you store the products while in transit and at the end destination. One can be sure about the storage aspect of the cold chain management as various warehouses are fully equipped today but the concern remains about temperature management.\nThe challenge in any cold chain management is to deal with the various risks involved with strategic planning and dealing with unforeseen events that can disrupt your operations. There is no scope for errors in this line of operations as many times you will require to transport goods such as life-saving vaccines and medicines. Hence a proper real-time monitoring system is quite essential to keep track of your temperature-controlled shipments from transit till it reaches the warehouse."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:af71d6a9-325f-486e-a52d-007828465158>","<urn:uuid:bf586905-fbc0-4d82-8f69-ad37266a2e24>"],"error":null}
{"question":"What are the key differences between long-term memory delay and anomic dysphasia in terms of their effects on memory and communication?","answer":"Long-term memory delay and anomic dysphasia affect memory and communication differently. Long-term memory delay is a reception-transmission developmental delay that makes it challenging for children to memorize things for long-term and recall them later. In contrast, anomic dysphasia, also called amnesic dysphasia, is caused by damage to the temporal parietal area and/or angular gyrus region, specifically disrupting word-retrieval skills. People with anomic dysphasia are unable to correctly name people or objects, causing them to pause or substitute generalized words (like 'thing'), but otherwise exhibit few language impairments.","context":["Parenthood becomes more challenging than ever when it comes to dealing with children with developmental delays. It is not only frustrating for the child but parents also have to protect their child from the piercing eyes of the world. It is difficult for children to understand why they can’t do simple things that other children of their age can do easily. This is the reason that children with developmental delays have poor communication and interaction skills (Ramous, 2008).1\nHowever, with the right parenting strategy, parents can boost the confidence of their children and help them overcome their weaknesses. This blog post will guide readers about the various types of developmental delays in children and highlight some useful parenting tips to help parents deal with children with learning disabilities.\nWhat is Developmental Delay?\nIn a research article published by University of Michigan Health Systems and written by Boyse (2010)2 development delay is defined as a mental and physical disability due to which a child can’t accomplish his/her developmental objectives at the right age. Developmental delay can be major or minor depending upon the extent of disability. However, if a child is suffering from temporary learning disabilities that can be cured then it won’t be considered as a developmental delay.\nIn an information guide for parents published by the Educational Resource Centre, the Royal Children’s Hospital (2009), 3 doctors follow strict guidelines for diagnosing developmental delay in children, as temporary lag and major/minor learning disabilities are characterized by same symptoms but have different treatments. This is the reason that it might take more than one visit for physicians to figure the type of developmental delay. However, parents are strictly guided to immediately consult a child health expert if they suspect that their child is not progressing at a normal pace.\nTypes of Developmental Delays\nHoopes, Adams & Alexander, PLC (2009)4 published an article about developmental delays, in which readers are informed about various forms of developmental delays. It is very important for parents to have adequate knowledge about different cognitive and other learning disabilities to provide their children with the right treatment at the right time.\nHere are some of the most frequently encountered developmental delays that impact learning abilities and cognitive skills of children:\n1. Reception-Transmission Developmental Delays\nLearning Rx (2014), 5 an authentic child care center declared that there are certain developmental delays caused due to improper functioning of transmission and reception neurons. However, these delays can be overcome to some extent by improving the cognitive skills of a child. Here is a list of some of these delays:\n· Long-Term Memory Delay: Children suffering from this developmental delay find it challenging to memorize things for long-term and recall them later.\n· Focus Delay: this ability hinders children to concentrate and focus on a particular task. Children suffering from this disorder can pay undivided attention to a task.\n· Short-tem Memory Loss: Children born with this disability can memorize anything for short time periods.\n2. Processing Delays\nAs the name implies these delays are associated with processing raw information to get useful results. Some of the common processing disorders are:\n· Slow Processing: Although it is a permanent disability, the condition can be improved by various mental therapies and exercises. A person suffering from this delay can’t think simultaneously in multiple dimensions.\n· Auditory Processing: It means that the brain is unable to quickly process and retrieve information from what is heard.\n· Sight Processing: A person with delayed sight processing disorder can’t conceptualize and analyze 2D and 3D images quickly.\n3. Other Developmental Delays\nOther developmental delays include:\n· Dyslexia: A learning disability characterized by difficulty in reading and pronouncing.\n· Delayed Speech (Aphasia): Children with this delay can’t speak fluently.\n· Dyspraxia: This developmental disability impacts the gross-motor skills of children.\nFollowing are some easy and helpful tips that can help parents strengthen cognitive skills of their children:\n· Educate- Parents must have complete knowledge about the symptoms of various disorders. This not only helps in early and timely diagnosis, but also allows parents to figure their options at initial stage.\n· Attention- A child suffering form developmental delays time and special care. This is necessary to prepare them to face the world and learn to live on their own.\n· Therapies and Counseling- Boyse (2010) emphasizes regular and proper therapies and counseling sessions to develop and strengthening cognitive skills in children suffering from developmental delays.\n· Special Education- Special schools and a proper learning environment also help children with developmental delays to cope with challenges.\n1. Ramous TM. (2008). Raising A Child With Developmental Delays. From One Parent to Another.\n2. Boyse K. (2009). Developmental Delay. University of Michigan Health System.\n3. Developmental delay: An information guide for parents. (2009). The Educational Resource Centre, The Royal Children’s Hospital.\n4. Hoopes, Adams & Alexander, PLC. (2009). Tips for Parents of a Developmentally Delayed Child.","dysphasia(redirected from Speech problems)\nAlso found in: Dictionary, Thesaurus, Encyclopedia.\nRelated to Speech problems: aphasia, Speech disorders\nDysphasia is a partial or complete impairment of the ability to communicate resulting from brain injury.\nApproximately one million Americans currently suffer from one of the various forms of dysphasia, and an additional 80,000 new cases occur annually. The term \"dysphasia\" is more frequently used by European health professionals, whereas in North American the term, aphasia is more commonly preferred. These two terms, however, can be and are used interchangeably. They both refer to the full or partial loss of verbal communication skills due to damage or degeneration of the brain's language centers. Developmental Dysphasia is considered to be a learning disability, but will not be the focus of this article.\nVerbal communication is derived from several regions located in the language-dominant hemisphere of the brain. These include the adjacent inferior parietal lobe, the inferolateral lobe, and the posterosuperior temporal lobe, as well as the subcortical connection between these areas. Disease, direct trauma, lesion, or infarction involving one or more of these regions can disrupt or prevent proper language function. Dysphasia does not necessarily prevent proper cognitive function, so the patient can think and feel with perfect clarity. This can be extremely frustrating for the patient, as they cannot express these thoughts and feelings to others.\nDysphasia can occur in a variety of forms, depending on how the communicative disruption manifests. Classically, dysphasia can affect one or more of the basic language functions: comprehension (understanding spoken language), naming (identifying items with words), repetition (repeating words or phrases), and speech. Although there are several subtypes of dysphasias, they most commonly manifest in one of three syndromes: expressive dysphasia, receptive dysphasia, or global dysphasia.\nExpressive dysphasia, also known as motor dysphasia, produces a conscious and recognizable disruption of a patient's speech production and language output. This includes the impairment of speech initiation, proper grammatical sequencing, and proper word forming and articulation. Although patients can perfectly understand what is said to them, they have great difficulty communicating their thoughts.\nBROCA'S DYSPHASIA. Broca's dysphasia is the most common type of expressive dysphasia. It is caused by damage to the lower area of the premotor cortex, located just in front of the primary motor cortex. This region is most commonly referred to as the Broca's area. Speech for patients suffering from Broca's dysphasia may be completely impossible. Others may be able to form single words or full sentences, but only through great effort. \"Telegraphing,\" the omission of articles and conjunctions, may also be exhibited.\nTRANSCORTICAL DYSPHASIA. Also known as isolation syndrome, transcortical dysphasia is caused by damage to the language-dominant brain that separates all or parts of the central region from the rest of the brain. There are three sub-classes of transcortical dysphasia, which define the impairments to a patient's ability to repeat words, sentences, and phrases: transcortical motor dysphasia, transcortical sensory dysphasia, and mixed transcortical dysphasia. Additional impairments may occur depending on the extent and location of the damage.\nReceptive dysphasia, also known as sensory dysphasia, impairs the patient's comprehension and meaning of language. Unlike expressive dysphasia, the patient can speak fluently and articulately, but will utilize meaningless words, nonsensical grammar, and unnecessary phrases to the point of becoming incomprehensible. However, they will be completely unaware of their mistakes. Additionally, the patient will find it difficult to comprehend spoken language and/or word-object relation.\nWERNICKE'S DYSPHASIA. Also known as semantic dysphasia, Wernicke's dysphasia is the most common of the receptive dysphasia. It is caused by damage to the Wernicke's area, located in the posterior superior temporal lobe of the language-dominant hemisphere. Although the patient can speak clearly and at length, many of their words, phases, and sentences will be nonsensical in nature. Additionally, they will experience difficulty in understanding spoken language, if not suffer a complete lack of comprehension. Semantic distinctions between words may become mixed up and jumbled, furthering confusion.\nANOMIC DYSPHASIA. Anomic dysphasia, also referred to as amnesic dysphasia, is caused by damage to the temporal parietal area and/or the angular gyrus region. Although very similar to Wernicke's dysphasia, anomic dysphasia is distinguished by its disruption of a patient's word-retrieval skills. They will be unable to correctly name people or objects, causing them to pause or substitute generalized words (like \"thing\"). Otherwise, the patient will exhibit few, if any, language impairments.\nCONDUCTION DYSPHASIA. Also known as associative dysphasia, conduction dysphasia is a relatively uncommon disease (representing only 10% of the cases). Damage to the upper temporal lobe, lower parietal, or connection between the Wernicke's and Broca's areas can result in the inability to repeat words, phrases, or sentences. The patient may also suffer the inability to describe people or objects in the proper terms.\nGlobal dysphasia, the third most common form of dysphasia, results from damage to both the anterior and posterior regions of the language-dominant hemisphere. In global dysphasia, all of the patient's language skills are disrupted; however, some may be disrupted more severely than others.\nCauses & symptoms\nCurrently, over one million people in the United States suffer a permanent type of dysphasia. Although dysphasia may manifest in several ways, the common cause for its onset is damage or trauma to the brain. Stroke, in particular, is the most common cause for dysphasia. Of the half million stroke victims reported annually in the United States, approximately 100,000 will suffer some form of dysphasia. Infection, direct trauma, transient ischemic attack (TIA), brain tumors, and degeneration can also instigate the onset of dysphasia.\nSymptoms of dysphasia will quickly manifest after damage to the brain has occurred, and will present in accordance to the particular type of dysphasia suffered. Due to the proximity to areas of the brain that control motor function, expressive dysphasias can be accompanied by noticeable motor impairment. The majority of symptoms will be language related, including:\n- Difficulty remembering words\n- Difficulty naming objects and/or people\n- Difficulty speaking in complete and/or meaningful sentences\n- Difficulty speaking in any fashion\n- Difficulty reading or writing\n- Difficulty expressing thoughts and feelings\n- Difficulty understanding spoken language\n- Using incorrect or jumbled words\n- Using words in the wrong order\nDysphasia is frequently diagnosed while the patient is being treated for injury to the brain, be it from trauma or disease. The health professional, typically a neurologist, will conduct standard cognitive tests, including tests to determine whether the patient's language centers have been affected. If the patient exhibits signs of difficulty communicating, they will often be referred to a speech-language pathologist. In turn, the pathologist will conduct a comprehensive examination of the patient's ability language and comprehension skills. This examination may begin with evaluating the patient's ability to repeat words and phrases, recognize and describe objects, and comprehend what is said to them. More extensive and standardized language-based tests may be required, including the Porch Index of Speech Ability and the Boston Diagnostic Aphasia Examination. Based on the result of the examinations, the health professional will be able to determine the type of dysphasia inflicting the patient. More extensive damage may require the use of computed tomography or magnetic resonance imaging for an effective diagnosis.\nInitially it is necessary to treat and stabilize the injury underlying the development of the patient's dysphasia. In some cases, such as with damage caused by TIA, a full recovery can be expedient and take only a few days. Unfortunately, most dysphasias can take months, if not years, to recover from. Even after prolonged therapy, many patients never achieve a full recovery. Efficacy of treatment greatly depends on the promptness with which it begins. For this reason, many medical facilities have speech-language pathologists on staff to begin the initial treatment process as quickly as possible.\nThere is no medical or surgical cure for dysphasia. Treatment, instead, relies strongly upon the use of various speech therapies. Much like physical therapy strengthens muscles and bones back to normalcy, speech therapy allow the patient to regain language function, as well as rebuild their communications skills. Treatment is typically conducted with a trained speech therapist. However, group sessions are common and allow the patient to practice their language skills in a non-threatening environment with others sharing their disability. Although much of therapeutic work is conducted by a speech therapist, friends and family also play a vital role in the patient's recovery. They can help the patient continually practice and exercise language skills while outside the therapeutic setting. Many times, family members are included on therapy sessions to teach them how to communicate with and understand the patient.\nThere are several treatments available, which utilize the patient's remaining language abilities to rebuild and compensate for those that were lost. These include out-put focused therapy (stimulationresponse), psycholinguistic therapy (cognitive), cognitive neurorehabilitation, and combinations thereof. Although these treatments approach aphasia differently, they all share a common thread by identifying the specific communication deficits and then targeting them with various modalities (computer-aided therapy, picture cards, reading and writing exercises, speech practice, etc.). These techniques stimulate the various parts of the brain associated with language, memory, and understanding, and thus allow it to heal.\nFortunately, about half of patients will suffer from transient dysphasia, in which the symptoms fade completely after only a few days. However, a patient's prognosis will greatly depend on several factors, such as the location and extent of the underlying damage. Additional factors of importance are the patient's age, general health, and mental health and motivation. Handedness may also be an indicator for recovery, as left-handed individuals have language centers located in both hemispheres of the brain (not just the left). As such, left-handed patients have access to language skills from either side of the brain, which can expedite their recovery. Even with therapy, dysphasia may take several years to overcome. Indeed, some patients will never regain their pre-trauma skill level of communication and speech.\nDysphasia can be prevented by avoiding the causes of brain injury and stroke, such as high blood pressure. In particular, eating a healthy diet and not smoking to maintain proper blood pressure will help prevent damaging strokes. Although it is impossible to predict head trauma, the use of head protection while participating in dangerous sports or activities can reduce the risk of serious brain damage.\nTransient ischemic attack — Also known as a ministroke, a transient ischemic attack is caused by a temporary interruption of blood flow in an area of the brain. Unlike in a true stroke, normal brain function will return with 24 hours.\nBrookshire, R. Introduction to Neurogenic Communication Disorders (6th edition) St. Louis, MO: Mosby, 2003.\nDarley, F. Aphasia. Philadelphia, PA: WB Saunders, 1982.\nNewman, S., and R. Epstein (eds). Current Perspectives in Dysphasia. New York: Churchill Livingstone, 1985.\nAlbert, M.L.. \"Treatment of Aphasia.\" Archives of Neurology 55 (November, 1998): 1417-1419.\nNational Aphasia Association. 29 John Street, Suite 1103, New York, NY 21108. (800) 922-4622. http://www.aphasia.org.\nSpeakability. 1 Royal Street, London, UK SE1 7LL. 020-7261-9572. http://www.speakability.org.uk/.\n\"Aphasia.\" The Merck Manual (Section 14. Neurologic Disorders) http://www.merck.com/mrkshared/mmanual/section14/chapter169/169b.jsp.\n\"Aphasia.\" National Institute on Deafness and Other Communication Disorders http://www.nidcd.nih.gov/health/voice/aphasia.asp.\n\"CMSD 336 Neuropathologies of Language and Cognition.\" The Neuroscience on the Web Series 〈http://www.csuchico.edu/∼pmccaff/syllabi/SPPA336/336unit5.html〉.\nImpairment in the production of speech and failure to arrange words in an understandable way; caused by an acquired lesion of the brain.\n[dys- + G. phasis, speaking]\ndysphasia/dys·pha·sia/ (-fa´zhah) impairment of speech, consisting in lack of coordination and failure to arrange words in their proper order; due to a central lesion.\nImpairment of speech and verbal comprehension, especially when associated with brain injury.\ndys·pha′sic (-zĭk) adj. & n.\ndysphasiaDysphrasia Neurology A speech impairment and/or inability to produce recognizable speech Clinical Defects in perception, sound discrimination, auditory memory, comprehension, word-finding, dysplexia Etiology Tumors of dominant cerebral hemisphere–ie, frontal, temporal, parietal lobes. See Aphrasia.\nImpaired or absent comprehension or production of, or communication by, speech, writing, or signs; due to an acquired lesion of or injury to a language center of the brain; may be transient if cerebral swelling subsides.\nCompare: alalia, aphonia\nSynonym(s): alogia (1) , dysphasia, dysphrasia, logagnosia, logamnesia, logasthenia.\nCompare: alalia, aphonia\nSynonym(s): alogia (1) , dysphasia, dysphrasia, logagnosia, logamnesia, logasthenia.\n[G. speechlessness, fr. a- priv. + phasis, speech]\ndysphasiaImpairment of speech or of the production or comprehension of spoken or written language. Dysphasia is due mainly to damage to the temporoparietal and prerolandic parts of the brain, usually from STROKE. Seven major sub-types of aphasia, including motor, sensory, conduction and ANOMIC DYSPHASIA, have been described but it is a complex disorder which cannot readily be divided into neat categories. Much can often be done to help by intensive devoted therapy. See also APHASIA.\nImpairment in the production of speech and failure to arrange words in an understandable way.\n[dys- + G. phasis, speaking]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:a763db98-3ae3-4fc7-852a-79facbb1e379>","<urn:uuid:8162f954-d39b-453c-8859-71d876580e35>"],"error":null}
{"question":"How many rainwater harvesting structures are planned under the Master Plan 2020, and what is their expected cost?","answer":"The Master Plan - 2020 plans to construct about 1.42 crore rainwater harvesting and artificial recharge structures in the country. These structures aim to harness 185 Billion Cubic Metre (BCM) of monsoon rainfall, with an estimated cost of about Rs. 1.33 Lakh Crores.","context":["Delhi and major cities show more than 4 mtr decline in ground water levels\nNEW DELHI, Nov 29 (The CONNECT) - Ground water levels are going down!\nAbout 33% of the wells monitored have registered decline in ground water levels in the range of 0 – 2 mtr. Decline of more than 4 m has also been observed in few pockets of Delhi, Chennai, Indore, Coimbatore, Madurai, Vijayawada, Dehradun, Jaipur, Allahabad, Ghaziabad, Kanpur, and Lucknow.\nThis Information was given by the Minister of State for Jal Shakti Bishweswar Tudu in Rajya Sabha today.\nMinistry of Jal shakti, Department of Water Resources, RD & GR (DoWR, RD & GR) is implementing Atal Bhujal Yojana (Atal Jal), a Rs.6,000-crore Central Sector Scheme, for sustainable management of ground water resources with community participation.\nAtal Jal is being implemented in 81 water stressed districts and 8774 Gram Panchayats of seven States viz. Gujarat, Haryana, Karnataka, Madhya Pradesh, Maharashtra, Rajasthan and Uttar Pradesh.\nCentral Ground Water Board (CGWB) in consultation with States/UTs has prepared ‘Master Plan for Artificial Recharge to Groundwater - 2020’, which is an improvement of the earlier Master Plan – 2013. The Master Plan – 2020 is basically a macro level plan indicating various structures for the different terrain conditions of the country including estimated cost. The Master Plan - 2020 envisages construction of about 1.42 crore Rain water harvesting and artificial recharge structures in the Country to harness 185 Billion Cubic Metre (BCM) of monsoon rainfall with an estimated cost of about Rs. 1.33 Lakh Crores\nThe Central Ground Water Board (CGWB) is periodically monitoring the ground water levels throughout the Country including metro cities on a regional scale, through a network of monitoring wells. In order to assess the decline in water level on a long-term basis, the water level data collected by CGWB during November 2020 has been compared with the decadal average (2010-2019).\nCGWB has, however, not carried out any survey to measure the total demand and the existing supply of water in big cities.\nThe government said water being a State subject, initiatives on water management including water conservation and water harvesting and making available adequate drinkable water to citizens in the Country is primarily States’ responsibility.\nHowever, important measures taken by the Central Government for conservation, management of ground water and effective implementation of rain water harvesting in the country including making available adequate drinkable water can be accessed at:\nThe Union has launched Jal Shakti Abhiyan (JSA) in 2019, a time-bound campaign with a mission mode approach intended to improve water availability including ground water conditions in the water stressed blocks of 256 districts in India. In this regard, teams of officers from Central Government along-with technical officers from Ministry of Jal Shakti were deputed to visit water stressed districts and to work in close collaboration with district level officials to undertake suitable interventions.\nIn addition, Ministry of Jal Shakti has taken up the “Jal Shakti Abhiyan: Catch the Rain” (JSA:CTR) with the theme “Catch the Rain - Where it Falls When it Falls” to cover all the blocks of all districts (rural as well as urban areas) across the country during March 22 2021 to November 30 2021.\nThe National Aquifer Mapping and Management program (NAQUIM) is being implemented by CGWB as part of Ground Water Management and Regulation (GWM & R) Scheme, a Central Sector scheme. NAQUIM envisages mapping of aquifers (water bearing formations), their characterization and development of Aquifer Management Plans to facilitate sustainable management of Ground Water Resources in the country. NAQUIM outputs are shared with States/UTs for suitable interventions.\nCentral government generally supports artificial groundwater recharge/water harvesting works in the country through Mahatma Gandhi National Rural Employment Guarantee Scheme (MGNREGS) and Prime Minister Krishi Sinchayee Yojana - Watershed Development component (PMKSY-WDC), ‘Surface Minor Irrigation (SMI) and Repair, Renovation and Restoration (RRR) of Water Bodies schemes’ a component of PMKSY (launched in 2015-16).\nMinistry of Housing & Urban Affairs (MoHUA) has formulated guidelines for the States to adopt measures suitable to local conditions, such as Unified Building Bye Laws (UBBL) of Delhi, 2016, Model Building Bye Laws (MBBL), 2016 and Urban and Regional Development Plan Formulation and Implementation (URDPFI) Guidelines, 2014, wherein adequate focus has been given on requirement of rainwater harvesting and water conservation measures. As per MBBL, all buildings having a plot size of 100 sq.m. or more shall mandatorily include the complete proposal of rainwater harvesting. 33 States/ UTs have adopted the features of these Bye Laws.\nA number of States have done notable work in the field of water conservation/harvesting for sustainable management of water resources. In this regard mention can be made of ‘Mukhyamantri Jal Swavlamban Abhiyan’ in Rajasthan, ‘Jalyukt Shibar’ in Maharashtra, ‘Sujalam Sufalam Abhiyan’ in Gujarat, ‘Mission Kakatiya’ in Telangana, Neeru Chettu’ in Andhra Pradesh, Jal Jeevan Hariyali in Bihar, ‘Jal Hi Jeevan’ in Haryana among others.\nGovernment of India in partnership with States, is implementing Jal Jeevan Mission (JJM) since August, 2019 to provide potable tap water supply of prescribed quality to every rural household in the country by 2024. Under JJM, while planning water supply schemes to provide tap water supply to house-holds, priority is given to quality-affected habitations. While allocating the funds to States/ UTs in a particular financial year, 10% weightage is given to the population residing in habitations affected by chemical contaminants including Arsenic and Fluoride, as on 31st March of the preceding Financial Year.\nMoHUA supplements the efforts of the States/ UTs through its programmes and policies. Atal Mission for Rejuvenation and Urban Transformation (AMRUT) is one of such programmes, which was launched on June 25, 2015, in selected 500 cities and towns across the country. The Mission focuses on development of basic urban infrastructure in the AMRUT cities, such as water supply, sewerage & septage management, storm water drainage, green spaces & parks, and non-motorized urban transport."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:a3790414-59df-4034-9759-418af2df6d5e>"],"error":null}
{"question":"For my environmental studies thesis: What approaches are being taken to protect aquatic ecosystems in Northern California - specifically comparing the legal/advocacy methods used for Mono Lake versus SF Bay's algae bloom crisis?","answer":"The protection approaches for these water bodies show different strategies. For Mono Lake, the primary method was through litigation that successfully halted water withdrawals after the lake level dropped 40 feet from Los Angeles' water diversions. For the SF Bay's algae bloom crisis, multiple advocacy approaches are being pursued, including legal action through organizations like EarthJustice suing the EPA, scientific research by the San Francisco Estuary Institute to understand the blooms, and advocacy work by Bay Keeper to stop polluters through litigation and policy work. Both cases involve legal intervention, but the SF Bay situation has a broader approach incorporating scientific research, public advocacy, and direct action against polluters.","context":["We are working on a photographic series exploring the connection between the reclamation of ponds and marshes and their promotion for recreational activity. The Mountain View Cemetery in Oakland was created by visionary landscape architect Frederick Law Olmstead whose designs “stage nature” by directing the gaze of the viewer and engaging contemplation of our place in the living world. The introduction of our miniature tent into this setting considers the compromise between anthropogenic interests and non-human nature.\nCovered in dried algae and hitchhiking seed burrs, we pack our gear into the car. It is getting late, we want to get to Eden’s Landing before sunset.\nEden’s Landing emergency relief tent celebrates a Bay Area environmental victory; the restoration of the artificially made salt ponds flanking the southern shores of the bay back to its original wetlands eco system. As far as changing the physical structure of Southern San Francisco Bay, no industry, not even waste disposal, has had as great an impact on the environment. In the past, more of the south bay had been diked and ponded for salt than not. Eden’s Landing emergency relief tent, as an intervention in this landscape, becomes a celebratory marker for this important transition of the land and water back to their original states.\nThe return of natural tidal flows along the South San Francisco bay constitutes one of the largest wetland restoration projects in history, turning stagnant industrial ponds into vital sustainable ecologies. Salt has been harvested from the San Francisco Bay since the mid-19th century in a patchwork of salt evaporation ponds. Cargill, an agricultural chemical company based in Minneapolis is the contemporary entity overseeing this process. Cargill works with Morton salt which processes the harvest.\nThings changed in 2003 when a large swath of coastline was returned to the public as a wildlife preserve. Since that time, the wetlands are recovering and now support migratory birds, brine shrimp, fish, and people! The restoration areas are now popular recreation sites for hiking, kayaking and photography.\nVideo: Electrical towers stretching over the restored salt marsh at the Don Edwards National Wildlife Refuge. Human infrastructure shares the environment with endangered species such as the California gray fox and the western snowy plover.\nThe next day we crossed through Yosemite arriving at Mono Lake in time to witness the setting sun glowing hot magenta, hurling shimmering embers across the surface of the water before disappearing behind the Sierra Nevada mountains. Tufa formations along the lake banks and extending into the shallows reinforced a sense of an otherworldly landscape. Tufas are calcium carbonate columns, the result of freshwater mineral springs beneath the surface reacting with the alkaline water of the lake. Their visibility is evidence of an incomplete recovery; they should be underwater. And the dramatic color, amplified as light scattered over atmospheric particulates from the wildfires in nearby Mariposa, was a consequence of drought and human negligence. Sometimes beauty is deceptively complicated.\nLocated 350 miles north of Los Angeles in the Eastern Sierras, the tributaries that feed Mono Lake were diverted for city use for over seven decades dropping the lake level 40 feet until successive litigations finally halted withdrawals. Mono Lake is part of an Endorheic basin, a system with no outlet save evaporation. Normally the lake is already three times saltier than the ocean. But evaporation without adequate replenishment precipitated a near ecological collapse in one of North America’s oldest lakes. Conditions have improved, but the vicissitudes of climate change are still a threat.\nThe next day was hot. Carrying awkwardly shaped, heavy, three-foot steel armature tent sculptures a mile through scrub brush to photograph under the dessert sun is a sweaty business, so we went for a swim. Swimming in Mono Lake is encouraged. The water is warm with a distinctive slippery feel. Pinkish clouds in the blue water are formed by trillions of tiny Artemia monica, a species of brine shrimp unique to Mono Lake. Small brown/black flies rested along the shore. The lake biome is contingent on brine shrimp and alkali flies; the shrimp and flies eat algae and the birds eat the shrimp and flies; as long as there is an inflow of water and stable pH levels the system works. Simple.\nTourists from all over the world passed us by as we worked. A few paused to watch with puzzled looks. A group of visitors from Germany asked: Are you shooting an advertisement? We thought, yes, in a way, and replied: We are making images that encourage recreational use of the lake to build support for conservation. One of the men smiled appreciatively: Oh yes, we were just reading about this.\nBeauty and rarity alone are never enough. Humans need a reason, a benefit.\nReclamation + Recreation = Water.\nRobin Lasser and Marguerite Perret\nOakland and Topeka","- Nicolette Beck\nA Quick Guide to the Red Tide: Harmful Algae Blooms in the SF Bay\nby Nicolette Beck, Philanthropic Advisor\nYou may have heard the news this summer of toxic red algae blooms in the SF Bay. You may have personally seen the effects: thousands and thousands of dead fish washing up on shores or floating limply in the water. You might also be wondering what is up and what we can do to help.\nAlgal blooms have their role in a healthy aquatic ecosystem, but lately, there has been an overabundance of one particular variety: Heterosigma akashiwo. It has been depleting oxygen in the water, causing fish and other sea life to suffocate and drown. The blooms also block out light from the water so the sea grass and other types of seaweed die as well.\nScientists believe that this overabundance is caused by excessive nutrients in the water, which primarily comes from our treated sewage. Other sources of nutrients that feed Harmful Algal Blooms (HABs) can be artificial fertilizers for agriculture and landscaping.\nThe good news is that the Red Tide has subsided in the SF Bay– for now. The bad news is that it will happen again if we don’t change our ways, and the impacts on sea life will continue to be devastating.\nWe looked into it, and the best solution is for the SF Bay Area to invest in water treatment facilities to remove these excessive nutrients from our wastewater.\nWays to help:\nWrite to your political officials, urging them to invest in water treatment facilities. There is a pre-written letter at this link here to write to the SF officials: https://baykeeper.org/actionalert/algalbloom\nSign this letter to the SF Bay Regional Quality Control Board, urging them to invest in more nutrient pollution research and to reduce such pollution. https://baykeeper.org/action-alert/algae-blooms-new-normal\nYou could also donate to the following charities:\nBay Keeper - www.baykeeper.org\nBay Keeper is an environmental charity that works to protect the SF Bay and its watershed. They\nuse research, litigation, advocacy, and policy work to stop polluters and protect the SF Bay’s\necosystem. From their website: “Baykeeper uses science, advocacy, and law to hold polluters\naccountable and stop destructive activities in San Francisco Bay and throughout its watershed.”\nEarthJustice - www.earthjustice.org\nEarthJustice is a charity made of lawyers who sue and advocate on behalf of the Earth. Though not\nspecific to the SF Bay area, EarthJustice is currently suing the EPA for not doing enough to stop\nHarmful Algal Blooms in Florida, leading to thousands of manatee deaths as well as the deaths of\nother sea life. If they win it will have implications for the EPA’s responsibility in preventing this all\naround the country. This is a great organization to support. About their lawsuit here:\nSan Francisco Estuary Institute - https://www.sfei.org\nSFEI is currently doing the research for the The SF Water Board to learn more about the harmful\nalgal blooms. From their website: “The San Francisco Estuary Institute (SFEI) is one of California’s\npremier aquatic and ecosystem science institutes. Our mission: provide scientific support and tools\nfor decision-making and communication through collaborative efforts. We provide independent\nscience to assess and improve the health of the waters, wetlands, wildlife and landscapes of San\nFrancisco Bay, the California Delta and beyond. SFEI’s 50 scientists and experts provide data,\ntechnology and tools that empower government, civic and business leaders to create cost-effective\nsolutions for complex environmental issues--from cleaner water and sustainable communities to\nclimate change. We have three primary programs: Clean Water, Resilient Landscapes, and"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:13dcdb00-e090-4b0c-9e8c-bf88d789a89e>","<urn:uuid:ee2fe746-d29a-4463-bf76-94cecfb8eb32>"],"error":null}
{"question":"What specific facial features become visibly altered after sleep deprivation?","answer":"Sleep deprivation causes multiple visible changes in facial features, including: hanging eyelids, redder eyes, swollen eyes, darker circles under the eyes, paler skin, more wrinkles and fine lines around the eyes, and droopier corners of the mouth.","context":["Q: The whole “beauty sleep” thing is an old myth, right?\nA: Nope. There is actual scientific evidence suggesting that you look your best when you've had a good night's rest.\nAn experiment was published in 2010 in the British Medical Journal that examined the relationship between sleep and attractiveness.\nA group of 23 adults (men and women) had photographs of their faces taken at two times:\nAfter a normal night's sleep (8 hours)\nAfter being sleep deprived (a reduced night of 5 hours of sleep followed by 31 straight hours of being kept awake)\nThe participants were asked to have a “neutral, relaxed expression” in all the photos.\nThe sleep happened in the participants' own homes and they tracked their sleep using sleep diaries and texting the laboratory.\nThen, the photographs were shown randomly to 65 observers who were unaware of the purpose of the study.\n- The observers saw each face for 6 seconds over three sessions.\n- The observers rated the faces on three characteristics:\nThe sleep deprived photographs were rated as:\n- Less healthy\n- Less attractive\n- More tired\nSome of the same researchers of the previous study then followed up with a second study, published in 2013 in the journal Sleep.\nThis time they wanted to know exactly what features were affected by sleep deprivation and had the biggest effect on ratings.\nAgain, the procedure was the same. Participants were photographed either after a good 8-hour night of sleep, or a 31-hour period of sleep deprivation.\nThis time, 50 students and 10 researchers from several countries were asked what “cues” they associate with losing sleep. The respondents answered:\n- Hanging eyelids\n- Red eyes\n- Swollen eyes\n- Glazed eyes, dark circles under the eyes\n- Pale skin\n- Droopy corners of the mouth\nThen once again, they got 40 participants to rate the photographs. This time the photographs were rated on the “cues” that were given above (red eyes, pale skin, etc.) and also Fatigue and Sadness.\nThe sleep-deprived photographs were rated as:\n- More fatigued\n- More hanging eyelids\n- Redder eyes\n- More swollen eyes\n- Darker circles under the eyes\n- Paler skin\n- More wrinkles and fine lines around the eyes\n- Corners of the mouth more droopy\n- More sad\nThus, being sleep deprived was associated with not only a large number of unattractive features, but also psychological characteristics like sadness and obvious fatigue.\nHave a big day tomorrow where you'll need to look your best? You could either:\n- Stay up all night worrying about it, studying (cramming), drinking, practicing what you'll say, etc. OR\nThe research says you need to SLEEP. Not only does sleep deprivation cause all sorts of behavioral and cognitive problems, it also makes you look WORSE. And let's face it – looking your best is a huge part of making a good impression.\nAnd I don't want to hear, “But wait, what if I need to study and practice?” Guys – you shouldn't have waited until the night before to do this.\nAxelsson, J., Sundelin, T., Ingre, M., Van Someren, E. J. W., Olsson, A., & Lekander, M. (2010). Beauty sleep: Experimental study on the perceived health and attractiveness of sleep deprived people. British Medical Journal, 341(7786), 1287-1289. Link: https://www.bmj.com/content/341/bmj.c6614\nSundelin, T., Lekander, M., Kecklund, G., Van Someren, E. J. W., Olsson, A., & Axelsson, J. (2013). Cues of fatigue: Effects of sleep deprivation on facial appearance. Sleep, 36(9), 1355-1360. Link: https://dx.doi.org/10.5665/sleep.2964"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:3570323b-2580-4e65-90f3-33f1a7a399f8>"],"error":null}
{"question":"What is the difference between CCleaner's data cleaning capabilities and professional computer forensics methods for data recovery? Is CCleaner enough to completely hide computer activity?","answer":"CCleaner and similar cleaning tools provide only basic data removal capabilities like clearing cookies, browser history, temporary files and cache. However, they are not foolproof since deleted files can potentially be recovered unless the free space is specifically overwritten. In contrast, professional computer forensics specialists can use advanced methods to find and recover deleted files, access hidden/encrypted files, analyze special disk areas like unallocated space and slack space, and examine system components like the registry and paging files. Even if CCleaner is used with its free space wipe feature, forensics experts can still potentially recover evidence through analysis of the system's complex data storage locations and through network monitoring. The only way to truly remove all activity traces is to completely erase the hard drive or reinstall the operating system with reformatting.","context":["There’s so much more to your computer, as well as your activity history, than just cookies and whatever tools like CCleaner can clean.\nSo much more.\nI’ll review a few of the more obvious ways employers can recover or collect information about your activity. Realize, though, it’s not with the intent that you be able to hide what you’re doing, but to illustrate the futility of even trying.\nBecome a Patron of Ask Leo! and go ad-free!\nCookies and CCleaner\nCookies are, effectively, small data files left on your computer by some of the websites you visit. As you might imagine, while the contents of the files might not be useful (they’re specific to each site), the fact that there exists a cookie from a specific site means your web browser has at some point fetched a page from that site. In other words, it’s one way to see where you’ve been.\nTools like CCleaner can easily and quickly clear cookies.\nIn addition, such tools also clear other traces of activity, like your explicit browser history, temporary files, the contents of your browser’s cache, and much more. I’ll refer to them as an interesting first step to removing some traces of your activity.\nBut they’re in no way complete or foolproof.\nThe easiest thing to overlook is the fact that deleting a file doesn’t delete its contents. Unless the data is actually overwritten by subsequent writes to the disk, there’s a possibility it can be recovered and restored. This is what “undelete” is all about — the attempt to recover files that have been deleted.\nSimply clearing your cookies or history or whatever else data-cleaning tools might remove does nothing more than delete the file(s) containing the information. There’s a chance the files could still be recovered with an undelete tool.\nThe only way to avoid this is to ensure that the data is overwritten after it’s been deleted. CCleaner and similar tools have something called a “free space wipe” which does exactly that: it overwrites all the free space on your hard drive with random data, rendering what was there practically1 unrecoverable.\nThat may still not be enough to erase all of your tracks, however.\nData you’re unaware of\nWindows is an incredibly complex operating system, as are many of the applications that run on it, including web browsers. It stores information in places you might not know about, or in places you know about — like the registry — but have no way to remove. Even so-called registry cleaners only remove or correct certain types of information, and are more about the health of your system than removing evidence of your activities.\nEven the paging or hibernation files could be analyzed by someone knowledgeable to collect or infer information about what the computer was used for, where you visited, or what you did.\nThere’s simply no way to know that there isn’t some amount of evidence of your activities left somewhere.\nSo, honestly, the only way to truly remove all evidence of your activity from your computer is to erase it completely. There are two approaches: using tools like DBan to literally erase the hard disk, or to reinstall Windows from scratch, ensuring you do a reformat of the hard disk as part of the process, and a free space wipe when the install is complete.\nBoth of those things are likely to trigger warning signs from an employer.\nBesides, they may still not be enough.\nYour computer isn’t needed to keep tabs on you\nWhen we think about tracking and evidence of our activities, we immediately think of all the data that’s stored on our device.\nWhile I’m sure your company would love it if you left all your tracks on the machine, it’s very possible that they don’t need ’em.\nRemember, they provide you with your internet connectivity and local networking. That means they can monitor where you go, what you do, and what you access from another computer, such as their internet gateway.\nThey don’t need access to your machine; all they need do is monitor your online activity through the devices they control.\nAnd it’s not your computer, to boot\nFinally, it’s important to realize that when you use a computer provided to you by your workplace, it’s not your computer. In most jurisdictions2, you don’t have a right to privacy on workplace-provided equipment.\nThe most obvious implication is that your employer has a right to snoop on what you’re doing by examining your computer or monitoring your internet traffic.\nMore concerningly, though, your employer could install spyware on your machine, or interfere with the “privacy” implied by https secure web sites. That means that even if you completely erase what’s on your computer, they may have already collected information about your activity and sent it to their own servers for storage and analysis.\nTrust. It’s complicated.\nHonestly, it really all boils down to trust.\nYou may or may not trust your employer not to spy on you, but it’s important to realize that they can. Much of how you use their equipment could be recovered forensically.\nSimilarly, they may or may not trust you to use their equipment according to their rules and guidelines. Once again, it’s important to realize that they have ways of verifying their trust in you.\nI can’t say how much invasive spying or forensic analysis is common, and I certainly can’t say how much is ethical or justified. What I can say, though, is that it’s possible, and that if you have any concerns at all, you should act as if your employer can monitor every little thing you do with their equipment. It’s by far the safest thing to do.","Computer Forensics and Data Recovery\nComputer forensics and Data Recovery is the science of analyzing a computer system hard disk using forensically sound methods and tools that have been tested and have had publications released such as the Department of Justice, NIST, Homeland Security and other approved forensic associations. The computer examination and analysis strategies might vary from case to case depending upon the evidence that is being attempted to be discovered to establish legal proof of the examination for legal cases. Computer Forensics and Data recovery can be utilized in a wide range of computer system criminal activity or abuse, consisting of but not limited to theft of data, theft of or violation of copyrights, and fraudulence. Computer experts could draw on a collection of approaches for uncovering data that resides in a computer system, and recovering deleted, secured, or damaged file details.\nBENEFITS OF PROFESSIONAL Computer system Forensics and Data Recovery\nThe unbiased computer Forensics professional that helps during a legal case will have experience on a large range of computer and software applications. This is constantly beneficial when your case has hardware and software applications with which this specialist is directly knowledgeable. Key computer components and software application execution is commonly comparable from one technology to an another, in which experience in one application or operating system area is frequently quickly transferable to a brand-new technology in a computer operating system.\nUnlike paper proof, computer system evidence can typically exist in numerous types, with earlier variations still easily accessible on a computer system disk. Understanding the possibility of their existence, also alternative formats of the same information can be uncovered. The discovery procedure could be offered well by a knowledgeable specialist recognizing additional opportunities that could be requested as perhaps relevant proof. Moreover, throughout on-site premises assessments, for situations where computer disks are not actually taken or forensically copied (see listed below), the forensics specialist could more quickly identify areas to look, indications to look for, and added information resources for relevant evidence. These could take the form of earlier versions of data documents (eg. memos, spreadsheets) that still exist on the computer’s disk or on backup media, or differently formatted versions of data, either developed or addressed by various other application software programs (eg. data processing, spreadsheet, email, timeline, organizing, or graphic).\nPreservation of data from changing is critical in computer forensic examinations. An experienced computer forensics professional will ensure that a subject computer system is carefully handled, documented to ensure that:\n- No feasible evidence is deleted, changed, or otherwise endangered by the procedures used to examine the computer system.\n- No feasible computer virus is presented to a subject computer system throughout the examination process.\n- Extracted and potentially relevant evidence is properly handled and safeguarded from later mechanical or electromagnetic damage.\n- A proceeding chain of custody is developed and kept.\n- Company procedures are documented for each case.\n- Any client-attorney details that is inadvertently discovered during a forensic examination is ethically and legitimately not disclosed.\nACTIONS TAKEN BY COMPUTER SYSTEM FORENSICS SPECIALISTS\nThe computer forensics professional will certainly take a number of mindful steps to determine and attempt to recover feasible evidence that could existing on a subject computer system:.\n- Secures the subject computer system during the forensic examination from any type of possible modification, damage, data corruption, or infection intro.\n- Discovers all files on the subject system. This includes existing typical documents, removed yet continuing to be archived documents, concealed data, password-protected documents,.and encrypted files.\n- Recovers all (or as much as possible) of discovered deleted files.\n- Reveals (to the extent possible) the contents of hidden files as well as temporary or swap files used by both the application programs and the operating system.\n- Accesses (if possible and if legally appropriate) the contents of protected or encrypted files.\n- Analyzes all possibly relevant data found in special (and typically inaccessible) areas of a disk. This includes but is not limited to what is called ‘unallocated’ space on a disk (currently unused, but possibly the repository of previous data that is relevant evidence), as well as ‘slack’ space in a file (the remnant area at the end of a file, in the last assigned disk cluster, that is unused by current file data, but once again may be a possible site for previously created and relevant evidence).\n- Prints out an overall analysis of the subject computer system, as well as a listing of all possibly relevant files and discovered file data. Further, provides an opinion of the system layout, the file structures discovered, any discovered data and authorship information, any attempts to hide, delete, protect, encrypt information, and anything else that has been discovered and appears to be relevant to the overall computer system examination.\n- Provides expert consultation and/or testimony, as required.\nWHO CAN USE COMPUTER FORENSIC EVIDENCE?\nMany types of criminal and civil proceedings can and do make use of evidence revealed by computer forensics specialists:\n- Criminal Prosecutors use computer evidence in a variety of crimes where incriminating documents can be found: homicides, financial fraud, drug and embezzlement record-keeping, and child pornography.\n- Civil litigations can readily make use of personal and business records found on computer systems that bear on: fraud, divorce, discrimination, and harassment cases.\n- Insurance Companies may be able to mitigate costs by using discovered computer evidence of possible fraud in accident, arson, and workman’s compensation cases.\n- Corporations often hire computer forensics specialists to ascertain evidence relating to: sexual harassment, embezzlement, theft or misappropriation of trade secrets and other internal/confidential information.\n- Law Enforcement Officials frequently require assistance in pre-search warrant preparations and post-seizure handling of the computer equipment.\n- Individuals sometimes hire computer forensics specialists in support of possible claims of: wrongful termination, sexual harassment, or age discrimination."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:b10bb820-2f90-46ff-9b89-0ceb0ff8b7e5>","<urn:uuid:c82ba33d-be8b-44ed-b168-265775e028b1>"],"error":null}
{"question":"How do schools' approaches to supporting social-emotional development differ between advantaged and disadvantaged students in terms of their psychological wellbeing outcomes?","answer":"Schools play a particularly crucial role in supporting social-emotional development for disadvantaged students who may have limited support structures available to them. The data shows that socioeconomically advantaged students report higher levels of psychological wellbeing than disadvantaged students - with a 10 percentage point gap at age 10, reducing to 5 percentage points by age 15. Schools can help address these differences by focusing on specific skills most strongly associated with psychological wellbeing: stress resistance, optimism, emotional control, trust and energy. Through initiatives like SEL programs integrated into the classroom, schools can provide structured support through activities like journal writing, class meetings, art activities, and explicit instruction in managing emotions and problem-solving skills.","context":["Disadvantage hindering social and emotional development\nFor decades, large-scale international assessments have shown a strong relationship between disadvantage and educational outcomes. This has traditionally focused on socioeconomic differences in cognitive outcomes, such as literacy, numeracy, science, ICT proficiency and other knowledge areas, but more recently, socioeconomic differences have also been found in other domains such as global competence.\nHowever, these assessments may not be capturing the underlying traits that really matter when it comes to student outcomes. According to the Organisation for Economic Cooperation and Development (OECD), ‘children need a balanced set of cognitive, social and emotional skills to adapt to today’s demanding, changing and unpredictable world’ (OECD, 2015).\nResearch over many years has resulted in a taxonomy of personality factors called the Big Five inventory. It identifies five basic dimensions of personality related to task performance, emotional regulation, engaging with others, collaboration and open-mindedness (Kankaraš & Suarez-Alvarez, 2019). Each dimension includes a number of sub-dimensions, which more clearly describe the associated thoughts, behaviours and feelings.\nFigure 1: The OECD Survey of Social and Emotional Skills assessment framework (Kankaraš & Suarez-Alvarez, 2019).\nBut social and emotional skills, unlike cognitive skills, cannot be directly measured. Instead, personality psychologists primarily measure these skills through self-reported surveys.\nThe OECD Survey on Social and Emotional Skills (SSES) is one of the first international collections of students’ self-reported skills across the Big Five domains. Led by the Australian Council for Educational Research, the SSES measured the conditions and practices that foster or hinder the development of social and emotional skills among 10- and 15-year-old students.\nData from 3000 students in each age group living in 10 cities around the world were collected in late 2019. Releasing the results in September 2021, the OECD emphasised that socioeconomic status, along with age, gender and migration background, matters when it comes to students’ social and emotional skills.\nIn all cities participating in the survey, socioeconomically advantaged students reported higher levels of social and emotional skills than their disadvantaged peers. The differences were particularly pronounced in six of the 17 skills being measured: tolerance, curiosity, creativity, empathy, assertiveness and self-efficacy.\nResearch has shown that young people’s social and emotional skills decline as they enter adolescence (Soto et al., 2011). The SSES confirms that, regardless of socioeconomic background, 15-year-olds report lower social and emotional skills than 10-year-olds on all measures except tolerance and assertiveness.\nInterestingly, this age-based decline in students’ social and emotional skills is slightly larger among socioeconomically advantaged students than it is for disadvantaged students. This means the differences in social and emotional skills between advantaged and disadvantaged students are generally smaller for 15-year-olds than for 10-year-olds. The exceptions are, again, tolerance and assertiveness skills, where the socioeconomic gap increased slightly between age 10 and 15.\nAddressing the socioeconomic gap in social and emotional skills is important because these skills are strongly related to young people’s psychological wellbeing (Strickhouser et al., 2017). The SSES found a strong association between skills related to emotional regulation and students’ current psychological wellbeing, as measured by how they felt during the previous two weeks.\nOn average, socioeconomically advantaged students also reported higher levels of psychological wellbeing than disadvantaged students. Among 10-year-olds, the average difference was around 10 percentage points. The difference among 15-year-olds was again smaller, at around five percentage points.\nThe driving forces behind these socioeconomic differences in students’ social and emotional skills are, as with cognitive outcomes, largely related to the resources available and the home environment. Some research has suggested that parenting style and family structure are potential channels through which parents’ socioeconomic status can affect their children’s social and emotional skills (Deckers et al., 2015).\nSchools are therefore important resources for promoting young people’s social and emotional skills and, in turn, psychological wellbeing. This is particularly the case for disadvantaged students, who, otherwise, may have little in the way of support structures available to them.\nUnderstanding which social and emotional skills are related to higher (and lower) levels of psychological wellbeing provides schools and teachers with crucial tools for supporting students. The SSES shows that stress resistance, optimism, emotional control, trust and energy are most strongly associated with higher psychological wellbeing. Providing teachers with training on how to best support their students to improve these skills may therefore be invaluable.\nDeckers, T., Falk, A., Kosse, F., & Schildberg-Hörisch, H. (2015). How does socio-economic status shape a child's personality? IZA Discussion Papers, No. 8977. Institute for the Study of Labor (IZA). https://www.iza.org/publications/dp/8977/how-does-socio-economic-status-shape-a-childs-personality\nKankaraš, M., & Suarez-Alvarez, J. (2019). Assessment framework of the OECD Study on Social and Emotional Skills. OECD Education Working Papers, No. 207. OECD Publishing. https://doi.org/10.1787/5007adef-en\nSoto, C. J., John, O. P., Gosling, S. D., & Potter, J. (2011). Age differences in personality traits from 10 to 65: Big Five domains and facets in a large cross-sectional sample. Journal of Personality and Social Psychology, 100(2), 330-348. https://doi.org/10.1037/a0021717\nStrickhouser, J. E., Zell, E., & Krizan, Z. (2017). Does personality predict health and well-being? A metasynthesis. Health Psychology, 36(8), 797–810. https://doi.org/10.1037/hea0000475\nOECD. (2015). Skills for Social Progress: The Power of Social and Emotional Skills. OECD Skills Studies. OECD Publishing. https://doi.org/10.1787/9789264226159-en.\nOECD. (2021). Beyond Academic Learning: First Results from the Survey of Social and Emotional Skills. OECD Publishing. https://doi.org/10.1787/92a11084-en.","As a burgeoning, important field of education research, SEL links students’ social and emotional skills with their academic achievement. … The key concept is creating a school environment that is supportive of and conducive to healthy social-emotional development of children and healthy mental habits.\nWhat is social emotional learning and why is it important?\nEducators and community agencies serve students with different motivation for engaging in learning, behaving positively, and performing academically. Social and emotional learning (SEL) provides a foundation for safe and positive learning, and enhances students’ ability to succeed in school, careers, and life.\nWhy is social emotional learning important for children?\nBeginning early in life, social and emotional learning (SEL) is highly important for helping preschool children to understand and manage their emotions, feel and show empathy for others, establish healthy relationships, set positive goals, and make responsible decisions.\nWhat is social emotional learning in the classroom?\nSocial and emotional learning takes academic learning a step further, helping students to develop resilience in the face of adversity. When combined with a PBIS initiative, SEL can help improve behavior, develop communication skills, and improve leadership among students.\nWhat are the 5 components of social emotional learning?\nWe use CASEL’s five core competencies of social emotional learning.\n- Self-Awareness. Understanding your emotions and thoughts and how they influence your behavior. …\n- Self-Management. …\n- Responsible Decision-Making. …\n- Social Awareness. …\n- Relationship Skills.\nWhat are the main components of social and emotional learning?\nCASEL has organized the components of Social Emotional Learning into five core components: self-awareness, self-management, responsible decision-making, relationship skills, and social awareness. These components can be implemented in the district, school, and classroom in different ways.\nWhat is the goal of social emotional learning?\nSocial and emotional learning (SEL) is the process through which children and adults acquire and effectively apply the knowledge, attitudes, and skills necessary to understand and manage emotions, set and achieve positive goals, feel and show empathy for others, establish and maintain positive relationships, and make …\nHow can you support social and emotional development in the classroom?\nHere are 25 ways to integrate social emotional learning into your classroom:\n- Use Journal Writing. …\n- Use Read Alouds. …\n- Do Daily Greetings. …\n- Hold Class Meetings. …\n- Incorporate Art Activities. …\n- Talk About Managing Emotions. …\n- Give Responsibilities. …\n- Practice Problem-Solving Skills.\nHow can you support children’s social and emotional development?\nPromoting Young Children’s Social and Emotional Health\n- Are usually in a positive mood.\n- Listen and follow directions.\n- Have close relationships with caregivers and peers.\n- Care about friends and show interest in others.\n- Recognize, label, and manage their own emotions.\n- Understand others’ emotions and show empathy.\n- Express wishes and preferences clearly.\nHow does social development affect learning?\nHow does social and emotional development affect learning? By providing a kind environment, it helps to encourage optimal brain development as well as social connection and collaboration. In other words, SEL affects learning by shaping children’s developing neural circuitry, particularly the executive functions.\nWhy social emotional skills are important?\nPositive social and emotional development is important. This development influences a child’s self-confidence, empathy, the ability to develop meaningful and lasting friendships and partnerships, and a sense of importance and value to those around him/her.\nWhat are the three pillars of social and emotional learning?\nThree Pillars: Culture, Adult Skills, Curriculum. A culture where social emotional learning can thrive is one that provides a safe and healthy place for children to learn and grow.\nWhat are some examples of social or emotional needs?\nExamples of Social and Emotional Skills Include:\n- • Displays self-control.\n- • Expresses feelings with words.\n- • Listens and pays attention.\n- • Pride in accomplishments.\n- • Has a positive self image.\n- • Asks for help when needed.\n- • Shows affection to familiar people.\n- • Aware of other peoples feelings.\nWhat are the main building blocks for social and emotional development?\nSocial Skills Building Blocks\n- Awareness. Children need to know the messages their body language, tone, actions, and words are sending. …\n- Emotional Self-Control. Children need to be calm to think through social situations. …\n- Getting the Big Picture. …\n- Adaptability. …\n- Predicting Outcomes. …\n- Social Recovery. …\n- Related Smart Kids Topics:\nWhat are the 5 core competencies?\nThe Five Core SEL Competencies\n- Social Awareness.\n- Relationship Skills.\n- Responsible Decision-Making.\nWhat is SEL and why it matters?\nSocial and emotional learning, or SEL, is “the process through which children and adults acquire and effectively apply the knowledge, attitudes, and skills necessary to understand and manage emotions, set and achieve positive goals, feel and show empathy for others, establish and maintain positive relationships, and …"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:2358260b-83a3-4fb2-b001-6674c6ec494e>","<urn:uuid:abcab6b2-37f6-48d9-92ad-cfd273a9aab0>"],"error":null}
{"question":"¿Cuál es la ruta de señalización específica para la activación de GADD45 beta y gamma en la producción de IFN-gamma? Please provide the detailed pathway including IL-18 and TCR alpha/beta involvement.","answer":"The signaling pathway involves multiple steps: 1) IL-18 promotes GADD45 beta expression through NF-kB: IL-18 activates IL-18R1, which recruits IRAK1 and TRAF6. TRAF6 then activates NIK(MAP3K14), leading to NF-kB activation and GADD45 beta transcription. 2) GADD45 beta and GADD45 gamma activate MEKK4(MAP3K4). 3) MEKK4 phosphorylates MEK3 and MEK6, promoting p38alpha activation. 4) p38alpha then induces STAT4 Ser 721 phosphorylation and phosphorylates ATF-2, both contributing to IFN-gamma production. 5) Additionally, IL-18 and TCR alpha/beta signaling, with IL-12 co-stimulation, activates JNK through GADD45/MEKK4/MEK4 pathway, leading to c-Jun phosphorylation, which cooperates with STAT4 in IFN-gamma promoter activation.","context":["IL-12-induced IFN-gamma production\nIL-12 is an important cytokine involved in production of IFN-gamma by T cells and NK cells, and plays an important role in T-helper cell differentiation. IL-12 signaling works together with IL-18 and TCR alpha/beta in inducing IFN-gamma production .\nUpon binding to its receptor, IL-12 activates Janus family kinases Tyk2 and JAK2. IL-12RB1 binds Tyk2, whereas IL-12RB2 associates with JAK2, leading to phosphorylation of tyrosine residues of STAT3 and STAT4. These tyrosine phosphorylations are responsible for formation of STAT4/STAT4 homodimers and STAT3/STAT4 heterodimers. These dimers then translocate to the nucleus and bind to IFN-gamma promoter , , , .\nUpon IL-12 action, STAT4 induces transcription of ERM and T-bet, which also promote IFN-gamma gene transcription , .\nIL-12 in cooperation with IL-18 and TCR alpha/beta signaling induces expression and activation of GADD45 beta and GADD45 gamma.\nIL-18 promotes expression of GADD45 beta in a NF-kB-dependent manner. IL-18R1, activated by IL-18, recruits IRAK1 and TRAF6. TRAF6 activates NIK(MAP3K14). This leads to subsequent activation of NF-kB, which promotes transcription of GADD45 beta , , .\nGADD45 beta and GADD45 gamma activate MEKK4(MAP3K4). MEKK4(MAP3K4) phosphorylates MEK3(MAP2K3) and MEK6(MAP2K6) and promotes p38alpha (MAPK14) activation. p38alpha (MAPK14) induces STAT4 Ser 721 phosphorylation, which is important for IFN-gamma production. p38alpha (MAPK14) also phosphorylates ATF-2 that regulates IFN-gamma production independently of STAT4 , , , , .\nIL-18 and TCR alpha/beta signaling, depending on co-stimulation of IL-12 leads to activation of JNK(MAPK8-10), also likely via GADD45 beta and/or GADD45 gamma/ MEKK4(MAP3K4)/ MEK4(MAP2K4) pathway. JNK(MAPK8-10) phosphorylates c-Jun. Activated c-Jun cooperates with STAT4 in IFN-gamma promoter activation , , , .\nIn addition, IL-12 action in NK cells promotes iNOS expression and iNOS production, which is a prerequisite for activation of Tyk2, tyrosine phosphorylation of STAT4, and production of IFN-gamma , .\n| ATF-2 || Cyclic AMP-dependent transcription factor ATF-2 |\n| ERM || ETS translocation variant 5 |\n| GADD45 beta || Growth arrest and DNA damage-inducible protein GADD45 beta |\n| GADD45 gamma || Growth arrest and DNA damage-inducible protein GADD45 gamma |\n| IFN-gamma || Interferon gamma |\n| IL-12 || IL-12 Complex |\n| IL-12RB1 || Interleukin-12 receptor subunit beta-1 |\n| IL-12RB2 || Interleukin-12 receptor subunit beta-2 |\n| IL-18 || Interleukin-18 |\n| IL-18R1 || Interleukin-18 receptor 1 |\n| IRAK1 || Interleukin-1 receptor-associated kinase 1 |\n| JAK2 || Tyrosine-protein kinase JAK2 |\n| JNK(MAPK8-10) || c-Jun N-terminal kinases Protein group |\n| MEK3(MAP2K3) || Dual specificity mitogen-activated protein kinase kinase 3 |\n| MEK4(MAP2K4) || Dual specificity mitogen-activated protein kinase kinase 4 |\n| MEK6(MAP2K6) || Dual specificity mitogen-activated protein kinase kinase 6 |\n| MEKK4(MAP3K4) || Mitogen-activated protein kinase kinase kinase 4 |\n| NF-kB || NF-kB Group of complexes |\n| NIK(MAP3K14) || Mitogen-activated protein kinase kinase kinase 14 |\n| STAT3 || Signal transducer and activator of transcription 3 |\n| STAT4 || Signal transducer and activator of transcription 4 |\n| T-bet || T-box transcription factor TBX21 |\n| TCR alpha/beta || TCR alpha/beta Group of complexes |\n| TRAF6 || TNF receptor-associated factor 6 |\n| Tyk2 || Non-receptor tyrosine-protein kinase TYK2 |\n| c-Jun || Transcription factor AP-1 |\n| iNOS || Nitric oxide synthase, inducible |\n| p38alpha (MAPK14) || Mitogen-activated protein kinase 14 |\n- Watford WT, Hissong BD, Bream JH, Kanno Y, Muul L, O'Shea JJ\nSignaling by IL-12 and IL-23 and the immunoregulatory roles of STAT4.\nImmunological reviews 2004 Dec;202:139-56\n- Jacobson NG, Szabo SJ, Weber-Nordt RM, Zhong Z, Schreiber RD, Darnell JE Jr, Murphy KM\nInterleukin 12 signaling in T helper type 1 (Th1) cells involves tyrosine phosphorylation of signal transducer and activator of transcription (Stat)3 and Stat4.\nThe Journal of experimental medicine 1995 May 1;181(5):1755-62\n- Thierfelder WE, van Deursen JM, Yamamoto K, Tripp RA, Sarawar SR, Carson RT, Sangster MY, Vignali DA, Doherty PC, Grosveld GC, Ihle JN\nRequirement for Stat4 in interleukin-12-mediated responses of natural killer and T cells.\nNature 1996 Jul 11;382(6587):171-4\n- Kusaba H, Ghosh P, Derin R, Buchholz M, Sasaki C, Madara K, Longo DL\nInterleukin-12-induced interferon-gamma production by human peripheral blood T cells is regulated by mammalian target of rapamycin (mTOR).\nThe Journal of biological chemistry 2005 Jan 14;280(2):1037-43\n- Ouyang W, Jacobson NG, Bhattacharya D, Gorham JD, Fenoglio D, Sha WC, Murphy TL, Murphy KM\nThe Ets transcription factor ERM is Th1-specific and induced by IL-12 through a Stat4-dependent pathway.\nProceedings of the National Academy of Sciences of the United States of America 1999 Mar 30;96(7):3888-93\n- Yang Y, Ochando JC, Bromberg JS, Ding Y\nIdentification of a distant T-bet enhancer responsive to IL-12/Stat4 and IFNgamma/Stat1 signals.\nBlood 2007 Oct 1;110(7):2494-500\n- Kojima H, Takeuchi M, Ohta T, Nishida Y, Arai N, Ikeda M, Ikegami H, Kurimoto M\nInterleukin-18 activates the IRAK-TRAF6 pathway in mouse EL-4 cells.\nBiochemical and biophysical research communications 1998 Mar 6;244(1):183-6\n- Dinarello CA\nMethods (San Diego, Calif.) 1999 Sep;19(1):121-32\n- Yang J, Zhu H, Murphy TL, Ouyang W, Murphy KM\nIL-18-stimulated GADD45 beta required in cytokine-induced, but not TCR-induced, IFN-gamma production.\nNature immunology 2001 Feb;2(2):157-64\n- Zhang S, Kaplan MH\nThe p38 mitogen-activated protein kinase is required for IL-12-induced IFN-gamma expression.\nJournal of immunology (Baltimore, Md. : 1950) 2000 Aug 1;165(3):1374-80\n- Visconti R, Gadina M, Chiariello M, Chen EH, Stancato LF, Gutkind JS, O'Shea JJ\nImportance of the MKK6/p38 pathway for interleukin-12-induced STAT4 serine phosphorylation and transcriptional activity.\nBlood 2000 Sep 1;96(5):1844-52\n- Morinobu A, Gadina M, Strober W, Visconti R, Fornace A, Montagna C, Feldman GM, Nishikomori R, O'Shea JJ\nSTAT4 serine phosphorylation is critical for IL-12-induced IFN-gamma production but not for cell proliferation.\nProceedings of the National Academy of Sciences of the United States of America 2002 Sep 17;99(19):12281-6\n- Chi H, Lu B, Takekawa M, Davis RJ, Flavell RA\nGADD45beta/GADD45gamma and MEKK4 comprise a genetic pathway mediating STAT4-independent IFNgamma production in T cells.\nThe EMBO journal 2004 Apr 7;23(7):1576-86\n- Barbulescu K, Becker C, Schlaak JF, Schmitt E, Meyer zum Buschenfelde KH, Neurath MF\nIL-12 and IL-18 differentially regulate the transcriptional activity of the human IFN-gamma promoter in primary CD4+ T lymphocytes.\nJournal of immunology (Baltimore, Md. : 1950) 1998 Apr 15;160(8):3642-7\n- Zhang F, Nakamura T, Aune TM\nTCR and IL-12 receptor signals cooperate to activate an individual response element in the IFN-gamma promoter in effector Th cells.\nJournal of immunology (Baltimore, Md. : 1950) 1999 Jul 15;163(2):728-35\n- Lu B, Yu H, Chow C, Li B, Zheng W, Davis RJ, Flavell RA\nGADD45gamma mediates the activation of the p38 and JNK MAP kinase pathways and cytokine production in effector TH1 cells.\nImmunity 2001 May;14(5):583-90\n- Park WR, Nakahira M, Sugimoto N, Bian Y, Yashiro-Ohtani Y, Zhou XY, Yang YF, Hamaoka T, Fujiwara H\nA mechanism underlying STAT4-mediated up-regulation of IFN-gamma induction inTCR-triggered T cells.\nInternational immunology 2004 Feb;16(2):295-302\n- Diefenbach A, Schindler H, Rollinghoff M, Yokoyama WM, Bogdan C\nRequirement for type 2 NO synthase for IL-12 signaling in innate immunity.\nScience (New York, N.Y.) 1999 May 7;284(5416):951-5\n- Schindler H, Lutz MB, Rollinghoff M, Bogdan C\nThe production of IFN-gamma by IL-12/IL-18-activated macrophages requires STAT4 signaling and is inhibited by IL-4.\nJournal of immunology (Baltimore, Md. : 1950) 2001 Mar 1;166(5):3075-82"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:4d813c49-b659-4795-b1d1-aa7bea2cc8ad>"],"error":null}
{"question":"During flu season, I want to protect myself and my family. What are the recommended preventive measures against flu, and what psychological support factors help during health crises?","answer":"For flu prevention, there are five key measures: getting vaccinated when available, washing hands for at least 20 seconds and avoiding face touching, coughing and sneezing into your elbow, avoiding crowds, and staying home when experiencing flu symptoms until the contagious period passes. Regarding psychological support during health crises, research indicates that people cope better when they have better health, stronger social support networks, a feeling of connectedness, a sense of purpose and meaning in life, and good emotion regulation abilities. These factors tend to result in fewer adverse impacts on psychological wellbeing during challenging times.","context":["‘Know What’s True about the Flu’\nTo improve communication with NJACCHO stakeholders, including the general public, part of a $2.4 million grant from the State Department of Health and Senior Services in January helped fund this NJACCHO website; a video, “Pandemic Influenza: A Guide to Home Preparedness”; and, in conjunction with the New Jersey Immunization Network, the production and distribution of a tool kit for public health professionals.\nThe tool kit, “Know What’s True about the Flu,” includes English and Spanish versions of reports, fact sheets, posters, video public service announcements, customizable PowerPoint slide presentations, and QR links to additional resources.\nExplore the Tool Kit Resources [click here]\nFollowing are the videos from this new tool kit\nWhat Can You Do?\nEvery year, more than 200,000 people are hospitalized in the United States with respiratory and heart conditions stemming from seasonal flu, and tens of thousands of people die from related complications.\nThose most at risk are young children and adults over the age of 65. In the New Jersey Health Officers Association’s informative video, “Pandemic Influenza: A Guide to Home Preparedness,” we give you the facts about the dangers of flu—but more importantly, we give you helpful tips about how to protect yourself from this highly contagious disease.\nWhen it’s flu season, here are five ways to help you avoid catching the flu:\n- Get vaccinated when vaccine is available.\n- Wash your hands often, for at least 20 seconds, and avoid touching your face.\n- Cough and sneeze into your elbow and cover your mouth and nose completely.\n- Avoid crowds when possible.\n- If you have flu symptoms, stay home and avoid contact with healthy people until your contagious period has passed.\nEmergency Preparedness: Ready Together New Jersey\nIn addition, New Jersey’s Department of Health and Senior Services has created a booklet, “Ready Together New Jersey,” as a family guide for emergency preparedness. The booklet is available in English and Spanish; download it and keep it handy.\nFollowing are highlights from “Ready Together New Jersey” about steps you can take for your family.To prepare for an emergency, NJDHSS recommends that you first compile an emergency kit that will provide you with the basics in the event you are without water, electricity, or other essential services for a few days.\nYour emergency kit should include:\n- Three days’ worth of water: one gallon per person per day\n- Canned foods, dried fruits, and granola bars\n- Pet food and water\n- Manual can opener\n- One change of clothing and footwear per person\n- One blanket and one flashlight per person\n- An extra set of keys, a credit card, and cash or traveler’s checks\n- Battery-powered radio plus extra batteries\n- Paper goods and personal hygiene products\n- Important family documents, such as copies of birth certificates, in a waterproof container\n- An extra pair of eyeglasses and/or contact lenses\n- Cash in small denominations\n- A First Aid Kit containing:\n- Adhesive bandages and gauze pads, assorted sizes\n- Antiseptic wipes\n- Antibacterial ointment\n- Tweezers, scissors and a thermometer\n- Aspirin and non-aspirin pain relievers\n- Anti-diarrhea medication\nFind Out Where to Go and What to Do\n- Know your local TV and radio emergency broadcasting stations; tune in for information and emergency instructions.\n- Find out if your community has a system of warning signals and what you should do when you hear them.\n- In certain emergencies, the state will use its telephone notification system, known as reverse 9-1-1, to call into homes to notify residents of a specific alert. Find out whether your municipality has such a system and sign up for it, listing your cell phone number and your land line.\n- Find out from your municipality whether you live on an emergency evacuation route; if so, you’re required to move vehicles off the street for snowplowing or a fire in the area.\n- If you have questions about specific health emergencies, contact your local health department. Representatives will have up-to-date information about disease incidents in your area. You can find a Directory of Local Health Departments on the New Jersey Department of Health and Senior Services website, www.state.nj.us/health .\nCreate a Personal Action Plan\nYour personal action plan is your investment in family security. Phone lines may be down; circuits may be busy. Plan ahead.\n- Identify a meeting place for your family near your home and another outside your neighborhood.\n- Pick an out-of-state friend and another who lives near you as your family’s contacts.\n- Develop a contact list for every family member, including work, school, and cell phone numbers.\n- Give your family’s contact information to your family contacts.\n- Post clear directions to your home in a convenient location in the event you need to call emergency services.\n- Show each family member how to turn off water, gas, and electricity.\n- Find out how your children’s schools will handle emergency situations.\n- Give the school your contact information, including that of both your friends.\n- Learn where students will be taken if they are evacuated, how the school will notify you, and how you will meet your child.","The mental health impact of COVID-19\nHow can quarantine affect your mental health? How can we better manage the anxiety that lockdown can provoke? Professor Cherie Armour from the Stress, Trauma, and Related Conditions (STARC) lab at Queen’s is working to find out.\nAs isolation, lockdown and quarantine measures are enforced throughout the UK and Ireland, a team of researchers from the Stress, Trauma, and Related Conditions (STARC) lab at Queen’s University School of Psychology are conducting a study which aims to better understand the impact that COVID19 is having on the psychological wellbeing of the people in Northern Ireland and the Republic of Ireland.\nProfessor Cherie Armour, who is leading the project says: “COVID-19 and the restrictions it places on the population as a whole, such as the need to self-isolate, is and will continue to be a stressful life event for many people across the country.\n“In Northern Ireland, Universities have moved their academic activities off campuses to online environments, schools and childcare settings have seen closures, as have pubs, restaurants, leisure venues, and shops selling non-essential goods. Businesses across many sectors are supporting their employees to work from home. With government rules stating you must stay at home, the daily escalation of the seriousness of the situation will of course be anxiety-provoking for many people.”\nProfessor Cherie Armour\nCOVID-19 and mental health\nProfessor Armour’s research focusses on adverse and/or traumatic life events, including those that occur because of someone’s occupational role (e.g. Police, Military & Ambulance services), and how those impact on psychological well-being. She is particularly interested in the psychological disorders of Posttraumatic Stress Disorder, Depression, Anxiety and Dissociation.\n“My focus is on biopsychosocial factors that exist pre-during- and post-trauma and how they predict someone’s risk of developing, or resilience against developing, mental ill health outcomes,” she explains.\nAs part of her research, Professor Armour examines the role of a number of things that might impact on the trauma exposure and mental health outcome relationship such as sleep, social support and emotion regulation.\nShe is now applying her research skills to examine the impact of the COVID-19 pandemic on the overall psychological wellbeing of the population and what people can do to ensure that this impact is minimised for themselves and their families?\nThe impact of quarantine\nThe project follows research from Kings College London relating to the psychological impact of quarantine.\n“The research found that those quarantined reported heighted psychological distress including confusion, fear, anger, anxiety and difficulties with sleeping,” says Professor Armour. “One study reported that some longer lasting behavioural changes were seen such as vigilant hand washing and the avoidance of large crowds.”\nShe adds: “The research tells us that some of the key stressors during self-isolation / quarantine relate to fears about becoming infected, having inadequate supplies and inadequate information, a sense of loneliness through isolation, and feelings of boredom and frustration. Research has also found that longer durations of self-isolation / quarantine have a more adverse impact on psychological wellbeing.”\nWhile the Kings College project reviewed a relatively small number of studies (26 in total), it provided the research community with a good evidence base for further research. Professor Armour says it is therefore now vital that the research community mobilise to collect robust and reliable data that will allow us to understand what the psychological impact of COVID-19 is but also how that impact may change over time.\nGathering vital data\nTo that end, researchers at STARC have designed a longitudinal psychological wellbeing survey that is administered online.\n“This survey will ask people a wide variety of questions about their life experiences, physical health, living environments, exposures and worries related to COVID-19, sleeping habits, what social support networks they have, whether they are experiencing loneliness, how they are regulating their emotions, whether they can see meaning and purpose to their life, and whether they are experiencing any symptoms of depression, anxiety, or PTSD in response to the COVID19 situation,” says Professor Armour.\nThe first survey takes just 20 minutes to complete and then subsequent surveys will take approximately 10 minutes to complete.\nThe survey will allow researchers to examine the psychological wellbeing of subgroups, such as healthcare professionals and other key workers.\nProfessor Armour says “Previous research tells us that when people experience adverse life events those with better health, fewer past trauma experiences, more social support, a feeling of connectedness (i.e., not feeling lonely), those who have a sense of purpose and meaning in their life and those who can regulate their own emotions tend to experience fewer adverse impacts on their psychological wellbeing.\n“The survey will allow us to understand if this is indeed the case for COVID-19 and for all those currently self-isolating from within our Queen’s University community and from across the wider community of Northern Ireland. Ultimately, this will inform us in how to best offer pragmatic help, with a view to alleviating some of the pressure that services face during these unprecedented times.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:77b49e78-f41d-464b-899e-7ba899bf6773>","<urn:uuid:ef55af05-b093-467b-b6c2-64ebce94f5fa>"],"error":null}
{"question":"How do pollination requirements compare between blueberries in a garden and Haskap berries?","answer":"While Haskap berries require two unrelated varieties that bloom at the same time for fruit production and should be planted close together (3-5 feet apart), blueberries have more complex requirements. Some blueberry varieties need multiple plants for cross-pollination by bees, and they also require specific temperature conditions - they need a certain number of 'chill hours' below 45 degrees Fahrenheit during winter. These chill hours can range from 150 to 1,000 hours depending on the variety, with Southern highbush varieties requiring fewer hours (150-800) compared to Northern highbush varieties (800-1,000).","context":["I am noticing a huge increase in popularity among gardeners to plant edibles. There is an upswing in popularity among designers to include fruits, herbs and other edibles in the landscape. Parents want their kids to actually see where their food comes from, say a tree or a bush and not a cello wrapped bag from the grocery store. Young couples in apartments want to grow fruit trees and veggies in containers on their balcony or allotment garden. People new to Canada are coming to nurseries asking for new and unusual favorites. I am excited to see this happening.\nPlanting stuff you can eat is a great way to encourage gardeners. Plants that ‘pay rent’ are a lovely thank you for all of that work we put in to fertilizing, watering and oh my goodness the weeding. Planting edibles…fruit and berries in particular, is not very difficult and like all gardening, put the right fruit in the right place and in some cases with the right partner and you can make even the most mediocre of gardeners into a proud, fruit toting zero mile foodie! Pollination! This is viewed as a deep dark secret by some and is a too often stumbling block to many.\nSo I’m going to try to simplify this subject and help you to be a successful fruit grower. To get you on the right track, I will clarify a couple of terms. Pollinizer is the tree or shrub that provides the pollen. A pollinator is the bee, fly, wasp, butterfly, bat, hummingbird or other critter that transfers the pollen from one plant to another.\nApple Tree Pollination\nApples, it takes two DIFFERENT apple varieties that bloom around the same times to get apples. As long as the blossoms open within about a week of one another and you have pollinators present and hard at work you will get apples. Crabapples in particular have a nice long bloom time and will often cover a wide range of pollination days.\nDo you need your apple trees to be in the same yard and right next to one another? No. As long as the apple or crabapple tree are within about 4 city lots of one another you will likely get apples. It used to be a square city block of one another but our pollinators are getting scarce. Certainly the closer the better. There a number of apples known as triploids. These apples are like the greedy stepsisters of the pollination world…all take, take, take and no give. The can be pollinated by another tree but will not pollinate anyone else, so you would need a third variety in the mix if you wanted apples on all of the trees. Why plant a triploid? They are often pretty tasty varieties like: King of Thompkins County, Bramley’s Seedling and Gravenstein.\nCherry Tree Pollination\nCherries, there are a number of varieties like the sour cherries, bush cherries and some sweet cherries that are self- fruitful. There are also a number of varieties that will require a DIFFERENT cherry variety nearby as a pollinizer to produce cherries. It will usually say so on the label. All cherries will benefit from having a DIFFERENT cherry variety nearby and you will often get bigger yields if this is the case. You have 2 different cherries nearby and wonder why you don’t get fruit some years? Watch what happens at bloom time…if it is raining up, down and sideways…the pollinators can’t do their job, they stay home and wait for the rain to stop like sensible bees.\nPlum Tree Pollination\nPlums, we’ll cover European and Japanese varieties. Many European varieties are self fruitful. For those that are not you will need another DIFFERENT European plum as a pollinizer. Most Japanese plums will need another DIFFERENT Japanese plum as a pollinizer. It’s not that the European or Japanese plums are unable to pollinize one another it is simply that the Japanese plums often bloom too early to catch the bloom time of the European plums. It is also better to have the Japanese plums at least in the next yard or two as they bloom at a cooler time of spring and we want to make it a bit easier on our hard working pollinators.\nPear Tree Pollination\nPears, there are European varieties and Asian varieties. With pears you will need two DIFFERENT varieties of pears for pollination to occur. Asian varieties tend to bloom earlier and European pears tend to bloom later. There is occasional overlap as with Bartlett European Pear and Twentieth Century Asian pear. Pear blossoms are lower in sugar so not particularly attractive to many pollinators. Believe it or not the common housefly helps to pollinate pears! Do try to keep your pears within 10-20 feet of one another to aid pollination.\nThere are a number of varieties that will produce a few with just one plant but you will be rewarded handsomely if you plant another blueberry close by. I like to have all my berries in one patch. Did you know the bell shaped blossoms will only release their pollen with the specific vibration of a bees wings? For all you music buffs I believe it is the key of C. If you have a tuning fork and you hit it and place it next to a blossom, you can see the pollen release…cool eh?\nHaskap / Honey Berry Pollination\nHaskap or Honeyberry – You will need two unrelated varieties that bloom at the same time to get fruit. The little yellow flowers are early but seem to be very attractive to our pollinators. I would plant them closer together, say about 3-5 feet.\nYou will need one female plant and one male plant to produce fruit. No you don’t have to look under the leaves to tell…it’s on the label\nOne plant is fine but you will produce more fruit if you get multiples…perhaps it just makes it more attractive to the pollinators.\nOther Small Fruits\nRaspberries, Blackberries, Currants, Figs, strawberries. You are fine with just the one. Though in the case of raspberries and strawberries it won’t be just one for long!\nNow get out there and get cracking! The sun is out and the time is just right. Try something new, challenge yourself and plant an edible. If you have questions still, come in and say Hi! Wow me with your awesome knowledge of the difference between and pollinizer and pollinator and I will be thrilled to bits and answer all of your questions. Actually, if you forget and don’t know the difference, I will still answer your questions…but I might sneer a little…inside ;).","U.S. Department of Agriculture hardiness zone 8 plants tolerate wintery temperatures down to 10 degrees Fahrenheit, but cold hardiness isn't typically what prevents zone 8 growers from seeing thriving blueberry bushes. If temperatures do not drop below 45 degrees Fahrenheit for certain periods of time, some blueberry bushes will not bear fruit the following year. Choosing blueberry varieties better acclimated to a warmer climate is your best option.\nAll blueberries have a chill-hour requirement, which is defined as the number of hours the temperature falls under 45-degrees Fahrenheit during the course of the winter. The chilling promotes growth during the next growing season. The problem with chilling hours is that they can vary drastically within a small area. Take, for example, California. Palo Alto experiences over 1,000 chill hours, while 20 miles away near San Francisco Airport, the chill hours are reduced to a little over 700. Regardless of whether a blueberry bush is designated for zone 8, always check the chilling requirements.\nRabbit-eye and many of its cultivars thrive in zones 6 to 9. The variety has among the lowest chilling requirements, according to the University of California's Cooperative Extension, at an estimated 100 to 200 hours. Some cultivars may require slightly more, but the number is well within zone 8 locations. Rabbit-eye grows up to 3 feet tall and usually blooms in late spring.\nNorthern highbush blueberries (Vaccinium corymbosum) generally require 800 to 1,000 chill hours, and usually are not suitable for many zone 8 areas. However, the \"Elliot\" cultivar (Vaccinium corymbosum \"Elliot\") has been shown to grow in areas with chill hours fewer than 300, according to the University of California's Cooperative Extension. Southern highbush blueberries -- a hybrid of northern highbush -- require far fewer chill hours and are more suitable to warmer climates. Most types of southern highbush require between 150 to 800 chill hours, which is usually easily met by most zone 8 areas. The \"Jubilee\" cultivar (Vaccinium corymbosum \"Jubilee\"), for example, requires 500 chilling hours, while the \"Misty\" cultivar (Vaccinium corymbosum \"Misty\") requires only 300. \"Jubilee\" is hardy in USDA hardiness zones 5 to 9, while \"Misty\" is hardy in zones 5 to 10.\nMany types of blueberries require the help of bees to cross-pollinate before they will fruit. Successful cross-pollination requires multiple varieties. Blueberry bushes do not like anything except acidic soil. They grow best in full sun, but partial shade may be necessary in the cusp of summer, especially for non-southern highbush varieties. Well-drained soil is a must, especially since blueberries enjoy moist soil at all times. Their roots are shallow, which can lead to problems when grown near other plants. Placing plants outside of the blueberry's root zone avoids potential damage.\n- University of California Agriculture and Natural Resources: A Quick Method of Estimating Chill Hours\n- University of California Cooperative Extension: Blueberries\n- Plants for a Future: Rabbit-Eye Blueberry\n- Monrovia: Jubilee Blueberry\n- Four Winds Growers: Care of Southern HIghbush Blueberries\n- University of California Agriculture and Natural Resources: Growing Blueberries in the Sacramento Region\n- Photos.com/Photos.com/Getty Images"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:44b52e56-c518-4de7-9f56-5b9ee8a78443>","<urn:uuid:e21b6886-0124-4d00-a13c-3849c6bf9b0f>"],"error":null}
{"question":"How does the Cavendish Community Food Hub plan to distribute the food they produce?","answer":"The Cavendish Community Food Hub follows a thirds model for food distribution: one third is sold at full price, one third at a discounted price, and one third is given as gifts to people and organizations in need.","context":["The Social Enterprise Incubator, sponsored by Libro Credit Union (a Certified B Corporation), is as much an entrepreneurial support program as it is a community of people creating positive change through work at the intersection of purpose, prosperity, and measurable benefit. The Spring 2020 cohort brings together change makers who are passionate about strengthening community, food systems, and personal resiliency through their work. During program orientation, Pillar team members observed that a core theme shared between the social enterprises was a focus on United Nations Sustainable Development Goal 3 - Good Health and Well-Being, and this is just the start. These growing organizations address a web of challenges that exist in the world today.\nMeet the Social Enterprises\nA.R.I.S.E. is a social enterprise dedicated to developing and sustaining a community seed network. Seed saving is a way to preserve the biodiversity which is at risk, while helping us to remember our deeper connection to the land. A.R.I.S.E tries to provide ethically sourced seeds native to the Antler River and the Carolinian zone in which we are located.\nBeecuz develops and facilitates innovative, skill-based workshops and educational curricula that provide youth with the tools and resources needed to proactively address and care for their mental health. Currently, Beecuz facilitates a 12-week school program for students in grades 4-6 that consists of six, two-week modules. Students start the program by learning about the distinction between mental health and mental illness (Module 1). They are then taught about the power of passion and perseverance (Module 2) before given the opportunity to develop their self-belief and confidence (Module 3). Next, Beecuz introduces students to the basics of neuroplasticity and the ability to rewire your brain (Module 4) before talking about mindfulness and gratitude (Module 5). Finally, the program concludes by teaching students about strong relationships, safe spaces and everyday leadership in order to translate knowledge into action (Module 6). To date, Beecuz’ school program has reached more than 800 students and over 20 teachers in London, Kitchener and Waterloo. In the last two years of operation, Beecuz has acquired funding from the KW Awesome foundation, TakingITGlobal, and the Mental Health Commission of Canada, and received support from School Mental Health Ontario.\nByClaire curates and sells pre-loved items and is currently reselling books, clothing, and accessories. Founder Claire Crossley launched this social enterprise in January 2020, after her job in the nonprofit sector was eliminated due to provincial funding cuts in June 2019. This social enterprise started based on Claire's immediate financial need and with zero financial investment; ByClaire is now a self-sustaining social enterprise and sources items through consignment (60% to clients) and donations. Claire will also offer WorkshopsByClaire teaching others how to create a part- or full-time income; too many people are living precariously with only part-time or no employment options and especially now with Covid. ByClaire will hire 2-3 Ontario Works (OW) receiptients by Summer 2020 paying each contract employee enough to complement their monthly support. Monthly rates for Ontario Works leave individuals without food and basic necessities, and this small but extra part-time income will feed one indiviual. Finally, reselling pre-loved books and clothing is good for the environment, as fast fashion is one of our greatest consumables and therefore contributors to our landfills.\nFounder Jen Hewson is an Intuitive Counsellor and Inspirational Speaker. Her mission and vision for Canadian Women Healing Our World (CWHOW) is to increase equitable opportunities for women to access integrated wellness services for improved personal growth and emotional healing. Through collaborative partnerships with other wellness professionals, and Jen's own expertise, women that face social inequities are able to attend workshops, classes and events, that help improve their overall health and wellness. Women that complete CWHOW's Life Alignment Program are offered the opportunity to take the stage and tell their courageous stories in a bi-annual Speakers Series event each year. These events are in support of organizations that directly help improve the lives of women.\nThe Cavendish Community Food Hub (CCFH) project is a community driven project that intends to repurpose a City of London storage site located in Cavendish Park as a vibrant center for urban agriculture. The Food Hub will consist of multiple hoop-houses with raised, wheelchair accessible beds for vegetable production, tool sharing library, composting facility and outdoor meeting and celebration space for the community. The infrastructure will be used for training people with a wide range of abilities how to grow food in raised bed gardens. The food produced at the Food Hub will be distributed along the thirds model of (1) one third at full price, (2) one third discounted and the (3) the last third as gifts to people and organizations in need. The Food Hub will also provide education opportunities for children of all ages on what it takes to grow food in an urban setting as well as provide the community to come together and celebrate local food. Ultimately the Food Hub will serve as an incubator for innovation in urban agriculture in London and its region. It will serve as a model to be replicated across the city.\nCliff Gliders is built on the belief that everyone is capable of achieving amazing things with the right support, and “amazing” looks different for each person! Unfortunately for adults living with disabilities in Ontario, many services end upon finishing high school. The transition to adulthood is so drastic, many people with and without disabilities describe it as “falling off a cliff.” For that reason, Cliff Gliders offers support for youth beginning in grade 7 (approximately 12 years of age) through the age of 29 (though no one will be denied assistance due to age), as well as their caregivers. Services include: skill-building and social activities (by age group and, in some cases, developmental stages), mixed-age social events, voluntary leadership opportunities for older youth, and amplification of youth voices through individual and systems-level advocacy.\nCollective Motion (co.motion)\nCollective Motion (co.motion) is a community based, holistic health initiative that takes physical health out of the gym and into the community, while also providing a helping hand. Collaborating with community organizations to support their needs through Co.Motion initiatives where participants will aid a community organization in a task joined with functional fitness to get the same benefits you’d receive at a traditional gym class. Collective Motion is a holistic approach to physical health, community care and an inclusive way for Every Body to get involved.\nFeed London is positioned as a for-profit venture, with the mandate to make a social impact in the London, ON community. Our primary social impact goal is to contribute to the local food security systems in London through reprocessing foods which are going to waste, repackaging, and distributing meals. This will all be executed, while encouraging sustainable lifestyles to safeguard the environment, by using social media to spread education but also through the opportunities to individuals in the London population looking to enhance their skills in management and the culinary arts through partnerships with various organizations and schools.\nImpact London is a peer-led non-clinical organization that serves the community to reduce poverty and addiction. Their team is made up of an amazing group of people that have lived experience with poverty, addiction and mental illness. They are passionate about helping others move through their journey without stigma and isolation. Impact London offers a wide range of programs to assist clients in moving forward in their recovery and educational programming to assist them with obtaining employment. Impact London also offer employment opportunities while building the necessary skills and routines to retain future employment. Currently, Impact London is leading efforts to support people experiencing homelessness during COVID-19.\nPermaculture for the People is a newly developing urban micro farm, apiary, and learning hub based on permaculture and social justice principles. They hope to offer people hands-on experiences in urban agriculture, medicinal herbal gardening, and beekeeping while building a sense of community and neighbourliness. In 2020, they are offering a series of learning opportunities and consulting services related to permaculture and gardening. Permaculture for the People will also be sourcing potential sites for the micro farm and apiaries.\nSeniors Community Talent Network\nInspired by statistically recognized wicked challenges for Seniors, charities and business talent recruiters, the Seniors Community Talent Network will be dedicated to diffusing theproblem of ageism within the workplace and power up the wolunteer workforce for local charities \"stuck' without funding to recruit excellent, well-matched talent. Just as importantly, the Talent Network will offset the pernicious and burgeoning crisis of social isolation and loneliness within the Senior population, supporting wellness in terms of both emotional and financial currency for a collage of community members. The idea is seamlessly bonded with the global vision and values of diversity and inclusivity.\nFounder Tawee Donchai grew up surrounded by textiles in Northern Thailand, as well as communities affected by poverty and other social issues. As he became an adult, he wanted to make a positive difference in communities such as these and partnered with local experts and artisans to develop sustainable businesses. In 2018, he moved to Canada with his family (including Co-Founder and wife, Beth) and still feels compelled to do what he can to connect the two worlds - Asia and North America - in order to make positive and lasting change. Together, Tawee and Beth want to be a bridge to provide a way for socially conscious citizens to celebrate Asian artisans, helping them earn fair wages so they can raise resilient families while also providing North Americans with ethically- sourced products."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:8f170f3b-a12a-41e1-b4e0-14e1ec3ef52c>"],"error":null}
{"question":"Which disinfection method is more cost-effective for wastewater treatment - UV radiation or ozone?","answer":"Based on the information provided, UV radiation appears more cost-effective overall for wastewater treatment. While ozone is very effective at destroying viruses and bacteria, it is relatively expensive due to high energy costs. UV systems have lower operating and capital costs, especially with recent technological improvements allowing treatment of higher flows with less energy and smaller footprint. However, UV effectiveness depends heavily on water clarity (UVT%), peak flow rates, and target organisms. Ozone, while being the strongest commercially available oxidizer that can kill all organisms, is described as unreasonably expensive for bulk disinfection applications.","context":["Waste Water Treatment\nModular “Plug & Play” Waste Water Treatment Solutions\nWaste Water Treatment Overview\nThe principal objective of wastewater treatment is generally to allow human and industrial effluents to be disposed of without danger to human health or unacceptable damage to the natural environment. The three basic types of wastewater are classified by source:\n- Industrial Wastewater\n- Domestic Wastewater\n- Storm Wastewater\nThe composition of industrial wastewater varies with the type of industrial process discharging the wastewater. Some industrial wastewater can be readily treated in conventional wastewater treatment plants. Other more contaminated sources may have to be pre-treated to remove or limit any process specific contaminants to acceptable levels before being accepted at a municipal treatment facility.\nDomestic wastewater comes from normal day to day activities occurring in homes, business and institutions. It is composed of a variety of organic and inorganic substances. Organic substances consist of molecules that are based on carbon and include fecal matter, detergents, soaps, fats, greases and food particles. Domestic wastewater is readily treatable in publically owned treatment facilities, which have been specifically designed to treat domestic wastewater in accordance with prescribed water quality regulations and standards.\nStorm water is also readily treated in these municipal facilities as it is relatively low in contaminants. Too much storm water however can overload these facilities and also dilute the sewage so that biological treatment processes are compromised. Most modern municipal wastewater system designs, channel domestic and storm sewage in separate distribution lines.\nMeasured and Regulated Constituents of Wastewater\nBiochemical Oxygen Demand\nOne of the most widely measured characteristics of wastewater is Biochemical Oxygen Demand or BOD. It is defined as the amount of oxygen aerobic microorganisms must consume to breakdown the organic material present in the wastewater. BOD is determined by measuring the quantity of oxygen consumed by microorganisms during a five-day period. For example, a regulated effluent discharge limit of say 20 ml BOD/l, means that the concentration of oxygen in a water sample diminishes by no more than 20 mg/l over 5 days. Also referred to as BOD5, it is the most common measure of the biodegradable organic material content of wastewater. BOD of effluent is tightly regulated, because effluent high in BOD can deplete oxygen in receiving lakes and rivers, causing fish kills and ecosystem imbalances.\nTotal Suspended Solids\nMost waste water contains large quantities of undissolved organic and inorganic materials. Referred to as Total Suspended Solids or TSS, these solids are problematic because most are in the form of fine particles, which not only support BOD but also plug up or clog septic systems and mechanical wastewater treatment equipment. TSS are removed from waste streams by various means such as screening, granular filtration, and induced settling/flotation schemes. TSS values are expressed as mg TSS/liter.\nOne of the major concerns regarding constituents in wastewater effluent is the concentration of nutrient compounds, particularly nitrogen and phosphorus. When released into receiving waters these nutrients concentrate and stimulate the excessive growth of algae and other aquatic plants, leading to decreased oxygen levels that can lead to hypoxic conditions. Hypoxia occurs when dissolved oxygen concentrations fall below the level necessary to sustain most animal life; generally considered to occur at levels below 2mg/l.\nNitrogen is present in many forms in the waste water stream. Most nitrogen excreted by humans is in the form of organic nitrogen (dead cell material, proteins, amino acids) and urea. Ammonia (NH3) is the primary form of nitrogen in influent entering treatment facilities. Nitrogen removal is accomplished by the biological processes of nitrification and denitrification, which convert ammonia (NH3) into gaseous nitrogen ( N2 ), an inert gas suitable for release into the atmosphere.\nNitrification is a two-step aerobic process facilitated by two different species of nitrifying bacteria. The oxidation of ammonia (NH3) to nitrite (NO2−) occurs first followed by the oxidation of nitrite (NO2−) to nitrate (NO3−). Then the NO3− is reduced to N2 through anaerobic denitrification. A wide array of bacterial populations are employed in the denitrification process.\nThe average phosphorus content of most sewage is estimated at around 10 mg/liter. Synthetic detergents are the largest source followed by human waste in the form of urine and feces. While legislated requirements for more environmentally friendly detergent formulations have reduced the amount of phosphorus entering the waste stream, the contribution from human waste remains constant. Phosphorus removal can be achieved by chemical precipitation or biologically through the enhanced biological phosphorus removal process (EBPR).\nIn chemical precipitation soluble phosphorus is transformed to a solid that settles and can be removed with the sludge. Several different metal salt additives are commonly used to chemically precipitate phosphorus, they include : Ferric Chloride (FeCl3), Ferrous Chloride (FeCl4), Ferrous Sulfate (FeSO4) and Aluminum Sulfate (alum) ( Al4 (SO4) 3 ) .\nEnhanced Biological Phosphorus Removal ( EBPR) is a process that uses alternating anaerobic and aerobic zones to provide an environment that encourages the growth of Phosphorus\nAccumulating Organisms (PAO). PAOs store excess polyphosphate in their cell mass and captured phosphorus is removed with the waste sludge. Essentially EBPR relies on the selection and proliferation of microbial populations capable of sequestering phosphates in greater amounts than would normally be required to sustain their growth. The resulting sludge high in phosphate can be used as fertilizer or disposed of in a conventional land fill.\nReview of Basic Wastewater Treatment Processes\nThe most basic form of wastewater treatment involves the removal of contaminants by allowing or inducing them to either settle to the bottom or to float to the top of reservoirs or clarification tanks for collection and removal. Pollutants are then subjected to biological treatment, either in natural settling ponds or in bio-reactors specifically designed to optimize certain microbial populations and metabolic processes in order to remove or chemically transform the targeted contaminants.\nWastewater flowing into modern treatment facilities generally passes through five common stages:\n- Influent collection & delivery\n- Primary treatment\n- Secondary treatment\n- Tertiary treatment\n- Disinfection & Effluent Discharge\nInfluent Collection & Delivery\nInfluent as the name implies, is wastewater flowing into a wastewater treatment facility. It contains all of the water, debris, and waste that entered the collection system feeding the facility.\nPrimary treatment involves basic processes to remove suspended solid waste from influent and to reduce biochemical oxygen demand (BOD) – the amount of oxygen microorganisms must consume to breakdown the organic material present in the wastewater. First, influent is passed through a series of raked bar screens to mechanically remove large objects such as bottles, plastic materials, pieces of wood, trash and other forms of suspended solid waste. The water is then passed through a grit removal system to take out smaller inorganic particles like sand and gravel. Finally the water flows into large primary clarification tanks or clarifiers, where suspended organic solids either settle by gravity or float to the surface as scum and grease. The floating scum is skimmed off, the settled solids, known as primary sludge, is removed and the primary effluent moves onto the next stage for secondary treatment. Primary treatment can reduce BOD by 20 to 30 percent and suspended solids by up to 60 percent.\nSecondary treatment employs aerobic biological treatment processes to remove biodegradable organic matter from the primary influent. Aerobic biological treatment is performed in the presence of oxygen by microorganisms (principally bacteria) that metabolize the organic matter in the wastewater, thereby producing more microorganisms and inorganic end-products (principally CO2, NH3, and H2O). Several aerobic biological processes are used for secondary treatment differing primarily in the manner in which oxygen is supplied to the microorganisms and in the rate at which organisms metabolize the organic matter.\nSecondary treatment systems are classified as fixed-film or suspended-growth systems. Fixed-film or attached growth systems, include trickling filters, bio-towers, and rotating biological contactors, where the biomass grows on media and the sewage passes over its surface. Fixed-film systems have been further perfected with the advent of Moving Bed Biofilm Reactors (MBBR). MBBR systems are the treatment method of choice here at Sapphire water.\nSuspended-growth systems include conventional activated sludge processes (also aerated lagoons and aerobic digestion), where the waste flows around and through the free-floating microorganisms, gathering into biological flocs that settle out of the wastewater. The settled flocs retain the microorganisms, meaning they can be recycled for further treatment.\nTypically secondary treatment systems can remove up to 85 percent of BOD and total suspended solids.\nThe purpose of tertiary treatment also referred to as effluent polishing, is to provide a final treatment stage to further improve the effluent quality before it is discharged to the receiving environment. Specifically, tertiary treatment targets remaining bioavailable organics (contributors to BOD), nutrients ( nitrogen and phosphorus ) and toxins ( pesticides, solvents, petroleum, metals – lead, cadmium, mercury). Treatment methods vary and include granular filtration, biological processes such as nitrification and denitrification, enhanced biological phosphorus removal ( EBPR), chemical precipitation, coagulation, flocculation and toxin specific treatment strategies. All in all, tertiary treatment can remove up to 99 percent of all impurities from sewage, but it is a very expensive process.\nDisinfection & Effluent Discharge\nThe purpose of disinfection is to destroy or inactivate pathogenic bacteria, viruses and parasites remaining in treated wastewater before it is discharged back into the environment in order to prevent the spread of waterborne diseases.\nVarious monitoring systems have been established that use the presence of indicator bacteria to gage the effectiveness of disinfection systems. The most commonly used indicators are total and faecal coliforms, E. coli and faecal streptococci. The presence of the indicator organisms in numbers greater than a specific target level suggests an increased probability of the presence of pathogenic organisms. The Water Environment Federation reported in 1996 that high levels of coliform indicators have been detected in 88% of the waterborne disease outbreaks in North America.\nChlorine is the most widely used disinfectant for wastewater, followed by ozone and ultraviolet radiation. Chlorine kills microorganisms by destroying cellular material. This chemical can be applied to wastewater as a gas, a liquid or in a solid form similar to swimming pool disinfection chemicals. However, any free (uncombined) chlorine remaining in the water, even at low concentrations, is highly toxic to aquatic life. Therefore, removal of even trace amounts of free chlorine is often needed to protect fish and aquatic life.\nOzone is produced from oxygen exposed to a high voltage current. Ozone is very effective at destroying viruses and bacteria and changes back to oxygen rapidly without leaving harmful by products. Ozone is a relatively expensive approach to disinfection due to high energy costs.\nUltra violet (UV) disinfection occurs when electromagnetic energy in the form of light in the UV spectrum penetrates the cell wall of exposed microorganisms. The UV radiation degrades the ability of the microorganisms to survive by damaging their genetic material. UV disinfection is a physical treatment process that leaves no chemical traces.\nEmerging Trends in Water and Wastewater Treatment\nThe overarching trends driving global demand for improved water and wastewater treatment systems are: population growth, increasing water scarcity, aging infrastructure along with the associated funding gap and the enactment of stricter water quality regulations to address rising concerns over the effects of inadequately treated water on human health and the environment.\nWith a large portion of existing water treatment systems reaching the end of their service lives many countries are faced with the urgent need to fund the replacement of this aging infrastructure. The situation is further exacerbated as new more stringent water quality regulations, render many middle aged treatment facilities incapable of compliance without major upgrades and overhauls.\nAs a result of this situation we are seeing the emergence of smaller more economical decentralized water and wastewater treatment systems capable of meeting current and future water quality standards. Often these decentralized systems are being funded in part by Public Private Partnership (3P or PPP ) finance schemes in an effort to bridge the infrastructure funding gap. From a technology stand point these modular packaged systems are generally scalable to accommodate future growth and rely heavily on advanced biological treatment technologies as opposed to chemical treatment options. (Decentralized WWTP Video)","“What is the most effective method of disinfection for treated sewage water, Ozonation or Ultra Filtration? How much can we expect from Ozonation?” asks Tajuddin Shaik, Assistant Manager-Sales and Proposals, Zion Enviro Systems Pvt Ltd. Excerpts of a Linkedin Discussion.\nMisha Shifrin, Ozone Application Engineer-Absolute Ozone®: Ozone is the most effective and economical way to disinfect water; however it is unreasonably expensive to be used instead of filtration or bulk BOD/COD removal. That is the place where mechanical filtration, bio-filtration, aeration etc. are most efficient and cost effective.\nUltra filtration is way too expensive to be used for waste water, because of large power consumption and maintenance costs. Most of the ultra-filtration systems could easily become breeding grounds for bacteria and viruses.\nJim Wark, Application Specialist-Neotech Aqua Solutions: Depending on the clarity, (actually UVT%) UV could be a way. We have several companies we are working with in water reuse using UV as the primary disinfectant and if needed chlorine for residual. The three most important things needed are Peak flow rate, UVT % and what one is treating.\nAlfonso Carpio, Water Treatment Professional: UV is usually 4 log reduction but can be sized for as many log reduction as needed. I would use mixed oxidant disinfectant generator as it leaves a good amount of residual that can be an assurance for a complete kill of the entire population in the spectrum.\nOrlando D. Gutierrez Coronado, Project Engineer-GEOFUTURE: In general, the ozone could give the best cost-effective performance in the long run. If you will evaluate the project just for a few months, then you could disinfect with any other chemical substance as PAA, or even rent an ozone machine.\nIn this case we are considering the sewage treated water that could have some of the above described substances to interfere with the UVT.\nAlfonso: I suppose UF has been mistakenly mentioned to be UV. UF is not a disinfecting equipment but a filtration medium. To further advance my advocacy on mixed oxidant, it’s more advantageous in providing the residual especially in long distribution pipelines where one would like to measure the amount of residual present to give you peace of mind. It also eliminate the biofilm which can house those organisms that we don’t usually see in any bacterial count in the field.\nColin Deakin, Process Engineer UF/MBR, GE Water & Process Technologies: I agree with Alfonso. UF is a filtration system, but has an extremely high degree of retention of particles including parasites, bacteria and viruses. Typically current system designs will clean and therefore sanitise themselves daily, and can also test themselves for integrity at the same frequency. These two factors go together in maintaining the desired quality. However, meeting something like title 22 for recycle water quality involving human contact requires demonstration of disinfection to 5 log virus. For something like this, UF and disinfection is likely to be required. UF does the bulk of the work removing the major part of the particulate load allowing the disinfection step to work optimally and reliably.\nØyvind Thorsen, Sales Engineer at Sterner AquaTech AS: I support those who have noticed the importance of not forgetting UV as an alternative. There is a huge development in UV for the time being. Look at Wedecos Duron system which we already have installed in Norway where you have longer life time on lamps and much higher UVC output per lamp than we had previously.\nJim: UV is very effective in wastewater treatment when the proper pretreatment and analysis is done. It in fact is one of the least expensive in overall operating and capital costs. The three things which are absolutely needed for UV to be considered:\n1. UVT% which is the transmissivity of UV through water, unlike turbidity or colour, can be very tricky. Ocean water for example handles UV well, bottled water with a pinch of sugar does not.\n2. Peak Flow rates: UV must be sized for the highest flow, although in most cases, it can be run at lower flow rate.\n3. What you are trying to reduce? Is it crypto? Giardia? E-Coli? or something else, it then must be dosed properly.\nUV has changed dramatically over the last few years; we can now treat higher flows with less energy and a much smaller footprint if the water clarity is good. If not, calculations need to be made for proper sizing and equipment choices.\nSimon Tarlovsky, Regional Sales manager-Aquatech: Just as an additional comment, Ozone and UV are in the same family of treatment, they will kill but not remove, UF is different, will remove but not kill. You need to also consider the failure option in your analysis, what if your execution fails and your kill or removal are not complete.\nThe problem with UF is that it can be a breeding ground and it can just stop working when slimed.\nThe problem with UV and Ozone is that it kills but does not remove some spores which are resistant to this treatment. If you don’t keep track of the system, you can have local or general under kill.\nIn the beverage industry, reuse of any water that has been in contact with waste water, is not yet acceptable, but with enough care and supervision, waste water is not worst than any other water for drinking, when properly treated.\nMisha Shifrin: Most of the people here not only know how UV works, but have actual experience with using UV. Without cost comparison UV is only good for people who sell it as they make money on service and light bulb replacement for years to come.\nUnlike UV, zone will kill, remove bacteria, viruses, etc., and there are no ozone resistant organisms as ozone is the strongest commercially available oxidizer and it simply burns all organic materials and oxidizes minerals if applied correctly.\nJim: Why then are there more UV systems in use for wastewater treatment than O3 in the US? Ozone is a fine treatment method, no question…But others have abilities too.\nLouis Coulthard, Cricket Manager-USRC-Millennium: I have used UV, O3 & straight up simple chlorine to good effect depending on the circumstance. In industrial waste water treatment, turbidity is often a problem negating UV and associated lamp replacement costs and clyptosporidium are unheard of negating the need for & dangers of O3 in unskilled hands. Most cases, straight up chlorination & de-chlorination is simplest & best.\nMisha: UV today in the long run will considerably be more expensive than ozone and other alternatives and does not really present an economical solution for large spectrum of applications. Especially wastewater treatment. However it is a great business for people who sell UV tubes every year and I perfectly understand why UV is so vigorously promoted. Water treatment in general has taken a hard beating by\nthose who used to go around and make claims that couldn’t be justified and took people’s money. Education and 3rd party certifications are taking us at least to a better medium."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:5c51ae6f-0bf1-4201-950d-42b398ba57da>","<urn:uuid:01d9fb9f-9b55-482f-9a49-038d6f4c214c>"],"error":null}
{"question":"What are the key trust-related security weaknesses identified in software development, and how does Zero Trust strategy address these vulnerabilities?","answer":"The key trust-related security weaknesses include missing authentication for critical functions, improper authentication, use of hard-coded credentials, missing authorization, incorrect default permissions, exposure of sensitive information, insufficiently protected credentials, and incorrect permission assignment for critical resources. These weaknesses are growing in frequency and severity, with three trust-related issues being the fastest risers since 2020. The Zero Trust strategy addresses these vulnerabilities by assuming network compromise and implementing continuous validation of user permissions, devices, and data. This approach has been formally defined by NIST and is being adopted by both government agencies and private organizations to prevent sophisticated threats from state actors and organized criminal gangs.","context":["Differences between the SANS Top 25 and OWASP Top 10\nWhile they both serve as a reference point for software security and are partly based on the same source data, the SANS/MITRE CWE Top 25 and the OWASP Top 10 differ in scope and purpose. The OWASP list groups the most prevalent web application security weaknesses into ten categories corresponding to broader cybersecurity concerns. With each subsequent edition, the categories have been moving away from specific vulnerabilities or even common vulnerability classes and towards a more strategic view – see our post on the 2021 OWASP Top 10 to learn what this means in practice.\nThe SANS/CWE Top 25 lists the most prevalent issues from the Common Weakness Enumeration (CWE). In a way, CWE takes the opposite approach to the OWASP list, focusing on specific weaknesses rather than more abstract classifications. This makes the list more directly useful for developers and security engineers, as each item relates to concrete implementation flaws that can be found and addressed. Interestingly, although the SANS/CWE Top 25 applies to all types of software while the OWASP list is limited to web applications, with each edition there is more and more common ground between web and non-web software security.\nWeaknesses vs. vulnerabilities: Both the SANS Top 25 and the OWASP Top 10 deal solely with CWEs, i.e. security weaknesses that commonly occur during software development. These are different from CVEs, which are confirmed security vulnerabilities in specific products. In simple terms, exploitable weaknesses reported in production become vulnerabilities.\nCommon themes in software security weaknesses in 2021\nThe SANS Top 25 list is based on the prevalence of specific weaknesses in real-life vulnerabilities taken from the NIST NVD. Each CWE that has led to a vulnerability gets a score that reflects its frequency and severity (see here for the actual formula), and this score determines its position on the list. A dry technical list doesn’t seem particularly useful or exciting, but if you read closely, the CWE codes, scores, and trends tell the story of modern software development and security – a tale of trust, deceit, and demons of the past, all set firmly in the cloud. Let’s look at the four common themes running through the Top 25.\nWeb application security is everywhere\nIf you came to the SANS TOP 25 CWEs from the OWASP Top 10, you’d be forgiven for having a sense of deja vu, as eight of the top 25 weaknesses are either web-specific or most commonly found in web applications. It’s no secret that as software development moves to the web, so does application security. Here are the four web-specific weaknesses on the list, along with their official names and overall positions:\n- #2: Cross-site scripting (XSS), officially Improper Neutralization of Input During Web Page Generation [CWE-79]\n- #9: Cross-site request forgery (CSRF) [CWE-352]\n- #23: XXE injection, officially Improper Restriction of XML External Entity Reference [CWE-611]\n- #24: Server-side request forgery (SSRF) [CWE-918]\nApart from these, several other weaknesses in the list usually occur in web security contexts, notably SQL injection, OS command injection, and path traversal (a.k.a. directory traversal). While these can apply to other types of software, they are easiest to exploit in web applications. Again, the position reflects the frequency and severity of vulnerabilities linked to a specific weakness, so having XSS way up at #2 means there is a lot of cross-site scripting going on.\nMemory management issues never go away\nOn the one hand, we see that all the cloudy headlines are true – software development is increasingly web development, and software security is increasingly web application security. However, the #1 weakness (along with five relatives) serves as a stark reminder that a lot of critical software relies on lower-level programming languages that need careful memory management. The top software security weakness of 2021 is essentially buffer overflow, though this specific term is considered too general for CWE. Here are the weaknesses related to low-level memory operations:\n- #1: Out-of-bounds write (code can write to memory that shouldn’t be accessible) [CWE-787]\n- #3: Out-of-bounds read (code can read memory that shouldn’t be accessible) [CWE-125]\n- #7: Use after free (code uses a variable that shouldn’t be used anymore) [CWE-416]\n- #12: Integer overflow or wraparound (mismanagement of large numeric values) [CWE-190]\n- #15: NULL pointer dereference (code attempts to access a non-existent value) [CWE-476]\n- #17: Improper restriction of operations within the bounds of a memory buffer (code can operate on memory that shouldn’t be accessible) [CWE-119]\nTrust no one with your inputs\nThe other overarching theme of this software security tale is trust. It is difficult enough to write software that works correctly with the expected data and users. When every user could be malicious and every input could be an attack attempt, writing even the simplest piece of code is like walking through a minefield. How can you do anything when everyone is suspicious? How can you check every piece of data? And yet this is the reality of application security, as shown by over a quarter of the top 25 being weaknesses related to blindly trusting your inputs:\n- #4: Improper input validation [CWE-20]\n- #5: OS command injection, officially Improper Neutralization of Special Elements used in an OS Command [CWE-78]\n- #6: SQL injection, officially Improper Neutralization of Special Elements used in an SQL Command [CWE-89]\n- #8: Path traversal/directory traversal, officially Improper Limitation of a Pathname to a Restricted Directory [CWE-22]\n- #10: Unrestricted upload of file with dangerous type [CWE-434]\n- #13: Loading saved data without checking, officially Deserialization of Untrusted Data [CWE-502]\n- #25: Code injection, officially Improper Neutralization of Special Elements used in a Command [CWE-77]\nIn all these cases, failure to sanitize user-controlled inputs can have devastating consequences, from software crashes to information exposure or code execution. And as mentioned earlier, many of these are typically found in web application security, where user-controlled inputs make up most of the data your application uses.\nTrust no one with access\nThe threat landscape is easily the biggest change across the history of software security. With threat actors now active at every stage of the application lifecycle, access control should be an integral part of software and data design – except that it’s not. All the remaining weaknesses from the Top 25 are related to implicit trust or failures to protect sensitive data at all times, showing that, all too often, security is still an afterthought during development:\n- #11: Missing authentication for critical function [CWE-306]\n- #14: Improper authentication [CWE-287]\n- #16: Use of hard-coded credentials [CWE-798]\n- #18: Missing authorization [CWE-862]\n- #19: Incorrect default permissions [CWE-276]\n- #20: Exposure of sensitive information to an unauthorized actor [CWE-200]\n- #21: Insufficiently protected credentials [CWE-522]\n- #22: Incorrect permission assignment for critical resource [CWE-732]\nThe importance of such trust-related issues is also reflected in the OWASP Top 10, where the top categories are now Broken Access Control and Cryptographic Failures. Ensuring application security means encrypting sensitive data (or all data, in many cases) at rest and in transit using secure algorithms while also thinking of authentication and authorization when designing user roles and function access.\nTo be effective, security must come first\nWith over half of the SANS Top 25 security weaknesses being related to trust and access control, it’s no coincidence that CISA is calling for organizations to implement zero trust principles in their systems. What’s more, the three fastest risers on the list since 2020 are all trust-related: Incorrect Default Permissions, Missing Authentication for Critical Function, and Deserialization of Untrusted Data. And remember that the list is based on prevalence in real-life vulnerabilities, so these weaknesses are out there and growing in frequency or severity (or both).\nThere are no shortcuts to avoiding software vulnerabilities, only hard work to build a security-first mindset and embed security into every stage of the software development lifecycle (SDLC). Vulnerability testing, mitigation, and remediation all need to be a routine part of the development workflow, built on a solid foundation of education and security awareness.\nOrganizations can no longer afford to compromise on security or accept security risks as the price of rapid development and growth. When anything can be a target and anyone can be an attacker, security must come first.","A concept receiving much attention lately in public policy circles is Zero Trust. John Kindervag, the Father of Zero Trust, has been getting a lot of speaking engagements suddenly. And for a good reason. Risk Management – or what John refers to call “Danger Management” as ZT and the objects of its attention are only specific dangers posed to an organization ahead of risk management.\nBut as long as it ends up as part of the solution or the driving motivation for a ZT implementation.\nSo, What Is Zero Trust?\nJohn will call it a strategy but for me, I see it in application, as an essential principle of Risk Management. A core tenet of the Zero Trust model is to assume that the network has been compromised and includes hostile intruders, which implies an obligation to authenticate and authorize every connected person or device.\nFormally defined by NIST, from John Kindervag’s original work in 2008, Zero Trust (ZT) is the term for an evolving set of cybersecurity paradigms that move defenses from static, network-based perimeters to focus on users, assets and resources. A Zero Trust Architecture (ZTA) uses Zero Trust principles to plan industrial and enterprise infrastructure and workflows. Zero Trust assumes there is no implicit trust granted to assets or user accounts based solely on their physical or network location (i.e., local area networks versus the internet) or based on asset ownership (enterprise or personally owned). See Zero Trust Architecture | NIST.\nThe acute crisis that generated the need to create a Zero Trust model was the ever-growing number of cyber attacks targeting both the public and private sectors, including critical infrastructures such as water plants or healthcare facilities and even governmental agencies.\nThe high-profile ransomware attack on the Colonial Pipeline and the Solar Winds breach shone a bright light on the dangers caused by vulnerabilities, and the government responded.\nThe fact that cyber attackers were lying hidden in Solar Winds networks for over a year before being discovered, and could have had lateral access through networks, demonstrated the crucial need for a change in the way to prevent the growing and sophisticated threats from state actors and organized criminal gangs, from wreaking havoc across all industries and jeopardizing critical infrastructure.\nThrough executive orders and memorandums, the U.S. government is now prioritizing and setting milestones for implementing the Zero Trust approach. See Executive Order on Improving the Nation’s Cybersecurity | The White House and also OPM’s Federal Zero Trust Strategy – Moving the U.S. Government Towards Zero Trust Cybersecurity Principles.\nWhile the Department of Homeland Security is leading the civilian side of exploring and optimizing the Zero Trust approach, on the defense and intelligence side of government, a Zero Trust pilot is being undertaken as a joint effort between the U.S. Cyber Command, the Defense Information Systems Agency and the National Security Agency where they are lab testing various technologies. According to Neal Ziring, the technical director for NSA’s Cybersecurity Directorate, “The team has been able to demonstrate the effectiveness of Zero Trust at preventing, detecting, responding and recovering from cyber attacks,” DHS, NSA creating reusable pieces to Zero Trust foundation | Federal News Network.\nRecently, the Pentagon’s top IT office issued a contract to develop a Zero Trust IT architecture capable of continuously validating user permissions, devices, services and data innocuousness. The project, called Thunderdome, will test how to implement Zero Trust architecture involving technologies such as Software Based Segmentation, Secure Access Service Edge and Software Defined-Wide Area Networks. See Welcome to Thunderdome: Pentagon awards Zero Trust architecture prototype (navytimes.com)\nDavid McKeown, DOD’s senior information security officer/chief information officer for cybersecurity, summed up the challenge and the need for a Zero Trust model: “We feel like Zero Trust is the only solution out there right now that gives us a fighting chance on detecting these folks that may have a foothold on our network or this anomalous software that we’ve allowed in.” Pentagon ‘Zero Trust’ cyber office coming in December (yahoo.com).\nPublic And Private\nZero Trust will not only be implemented by governments but also by many companies in the private sector. Gartner predicts that spending on Zero Trust network access solutions will grow from $820 million this year to $1.674 billion in 2025, attaining a 26% Compound Annual Growth Rate (CAGR). Zero-trust trends for 2022 | VentureBeat\nIn view of the growing number of threats, a refocus on strengthening cybersecurity requires sound investments, resources, expertise and capabilities. And it requires creating a framework that will assess situational awareness, align policies & training, optimize technology integration and privileged access management, promote information sharing, establish mitigation capabilities and maintain cyber resilience in the event of incidents.\nThe 5 Steps Simulated\nBut first, leaders need to know where and what vulnerabilities they face. They need to determine upfront what linked devices, people, software and hardware should be trusted to best protect digital systems and networks.\nSimulation platforms are invaluable tools to initiate a Zero Trust approach for a security framework, as they provide a bird’ eye view on the security gaps attackers could use to gain a foothold and the path they could use to escalate or move laterally.\nAgencies and companies alike draw enormous benefits from testing their security controls against the full cyber attack kill chain. The combination of the chronic shortage of skilled cybersecurity workers and the ever-increasing volume and complexity of threats on connected attack surfaces makes automating breach and attack simulation the only effective way to help evaluate security gaps and check that remediation measures are effective.\nThe crux of a Zero Trust model is to reach and maintain the highest achievable level of segmentation and fortification and best prepared and fortified to minimize the odds of experiencing a breach and reduce the potential damage of such a breach to a strict minimum by preventing escalation and lateral movement.\nOptimizing privileged access management and security controls effectiveness is the first step and needs to be continuously fine-tuned to account for modification of the threat landscape and the unavoidable frequent changes in the environment resulting from agile development. Continuous security validation processes are indispensable in providing the extended security posture management needed to maintain Zero Trust Architecture in a constantly fluctuating reality.\nDanger And Risk Management\nThe Zero Trust model may not be a panacea for everything cybersecurity, and the founders of the CyberTheory Institute have it right when they remind us that their mission statement for the Institute is to convince CSIO’s that the downside of ZT is very slight against an upside that expects to cut successful breaches in half. Those thought leaders, all experts in Zero Trust are all convinced that a ZT implementation will remove excessive trust from a network, significantly reduce the attack surface and increase the granularity and frequency of identity validation, authentication and proofing, while continuous monitoring and alerting will lower the successful breach events by 50%.\nIn addition, the “never trust, always verify” motto is an essential element of Risk Management. It is encouraging to see government agencies and companies look first within their networks and operations to discover threats and help mitigate vulnerabilities. Whether folks think of cyber attacks in the “danger” or “risk” categories, that, in and of itself, is a big step in making our nation and industries more secure."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:8aeaf7b5-d75a-4b60-bbba-d0bb596a8aad>","<urn:uuid:c8f665ca-7905-4bfd-b23a-4f6a26d30c31>"],"error":null}
{"question":"What's the main difference between government and NGO roles in peace work?","answer":"While both governments and NGOs contribute to peace processes, they have distinct roles. Governments can create formal structures like truth commissions (as South Africa did) and provide opportunities for people to address past conflicts, but cannot directly impose reconciliation since it is fundamentally a bottom-up process. NGOs, on the other hand, typically focus on grassroots peacebuilding activities - for example, organizations like Search for Common Ground develop programs like soap operas with conflict resolution themes, while the Mennonite Central Committee works on reconciliation in Central and South America. NGOs are particularly effective at peacebuilding work that involves creating spaces for interaction, capacity building, and relationship transformation at multiple levels of society.","context":["Charles (Chip) Hauss\nIn the last few years, reconciliation has become one of the \"hottest\" topics in the increasingly \"hot\" field of conflict resolution. It refers to a large number of activities that help turn the temporary peace of an agreement which ends the fighting into a lasting end to the conflict itself. Through reconciliation and the related processes of restorative and/or transitional justice, parties to the dispute explore and overcome the pain brought on during the conflict and find ways to build trust and live cooperatively with each other.\nWhat is Reconciliation\nReconciliation is a rather new concept in the new field of conflict resolution. It is not mentioned once in a book I wrote in 1995. In the one I published in 2001, it was the most frequently cited concept.\nAs is the case with any new concept, there is no standard definition that all scholars and practitioners rely on. However, almost everyone acknowledges that it includes at least four critical components identified by John Paul Lederach -- truth, justice, mercy, and peace.\nLederach's use of the term \"mercy\" suggests that the ideas behind reconciliation have religious roots. It is a critical theological notion in all the Abrahamic faiths and is particularly important to Evangelical Christians as part of their building a personal relationship with God. For those who ask \"what would Jesus do,\" reconciliation is often not just an important issue, but the most critical one in any conflict.\nIn recent years, reconciliation has also become an important matter for people who approach conflict resolution from a secular perspective. For them, the need for reconciliation grows out of the pragmatic, political realities of any conflict resolution process (see the next section).\nConflict resolution professionals use a number of techniques to try to foster reconciliation. By far the most famous of them is South Africa's Truth and Reconciliation Commission that held hearings into the human rights abuses during the apartheid era and held out the possibility of amnesty to people who showed genuine remorse for their actions. Since the TRC was created in 1995, as many as 20 other such commissions have been created in other countries, which experienced intense domestic strife. These projects bring people on both sides of a conflict together to explore their mutual fear and anger and, more importantly, to begin building bridges of trust between them. Despite the violence in the region since 2000, some of the most promising examples of this kind of reconciliation have occurred between Israelis and Palestinians. For more than a decade, Oases of Peace (Neve Shalom/Wahat al-Salaam) have been bringing together students and teachers from both sides of the divide. Similarly, the Seeds of Peace summer camp in Otisfield, Maine (U.S.) has served as a \"safe place\" for Israeli and Palestinian teenagers to spend extended periods of time together. Yet others have tried more unusual strategies. At Search for Common Ground, we make soap operas with conflict resolution themes for teenagers aired on radio in Africa and on television in Macedonia. Similarly, Benetton sponsored a summer camp for teenage basketball players from the former Yugoslavia, one of many examples in which people have tried to use sports to build bridges, ironically, in part through competition. Last but by no means least, it should be obvious from the above that many people have used religion as a vehicle to help forge reconciliation. Thus, the Rev. John Dawson has made reconciliation between blacks and whites the heart of his 20-year ministry in South Central Los Angeles. Similarly, Corrymeela is an interfaith religious retreat center, which has spent the last 25 years facilitating meetings between Catholics and Protestants in Northern Ireland.\nThere is at least one common denominator to all these approaches to reconciliation. They all are designed to lead individual men and women to change the way they think about their historical adversaries. As a result, reconciliation occurs one person at a time and is normally a long and laborious process.\nWhy Reconciliation Matters\nReconciliation matters because the consequences of not reconciling can be enormous. In Fen Osler Hampson's terms, too many peace agreements are \"orphaned.\" That is, the parties reach an agreement that stops the fighting but does little to take the parties toward what Kenneth Boulding called stable peace, which can only occur when the issues that gave rise to the conflict in the first place are addressed to the satisfaction of all.\nWithout reconciliation, the best one can normally hope for is the kind of armed standoff we have seen in Cyprus for nearly 30 years. In 1964, the rival Turk and Greek forces agreed to a cease fire, a temporary partition of the island, and the introduction of United Nations Peacekeeping forces. Since then, little progress has been made toward conflict resolution; in fact, it is all but impossible for Greek Cypriots to visit the Turkish part of the island and vice versa.\nAt worst, without reconciliation, the fighting can break out again, as we have seen since the tragic beginning of the second Intifada in Israel/Palestine since 2000. Despite Oslo and other agreements and despite some serious attempts at reconciliation at the grassroots level, the parties made little progress toward achieving stable peace until 2000 when Palestinian frustrations finally boiled over in a new and bloodier round of violence.\nMost examples fall somewhere between Cyprus and Israel/Palestine. For instance, because Catholics and Protestants have not made much progress toward reconciliation, every dispute between them since 1998 has threatened to undermine the accomplishments of the Good Friday Agreement which put at least a temporary end to \"the troubles\" in Northern Ireland.\nWhat Individuals Can Do\nAt the most basic level, reconciliation is all about individuals. It cannot be forced on people. They have to decide on their own whether to forgive and reconcile with their one-time adversaries.\nNothing shows this better than the remarkable documentary, \"Long Night's Journey Into Day\" which chronicles four cases considered by the South African Truth and Reconciliation Committee. The final one involves a young black man who had been a police officer and helped lure seven activists into a trap in which they were all killed by the authorities. The last scene of the sequence shows a meeting he held with the mothers of the seven boys in which he begs for their forgiveness. It is clear that, unlike one of his white colleagues who is interviewed earlier, his confession and his remorse are heart-felt. Still, at first the mothers, whose pain remains raw more than a decade after the murders, refuse to forgive him. Then, one of them asks if his first name means \"prayer\" and when he says it does, you can literally watch the mothers draw on their own Christianity and find the mental \"space\" to forgive the former officer.\nWhat States Can Do\nBy its very nature, reconciliation is a \"bottom up\" process and thus cannot be imposed by the state or any other institution. However, as the South African example shows, governments can do a lot to promote reconciliation and provide opportunities for people to come to grips with the past.\nIn South Africa, the TRC heard testimony from over 22,000 individuals and applications for amnesty from another 7,000. The TRC's success and the publicity surrounding it have led new regimes in such diverse countries as East Timor and Yugoslavia to form truth commissions of one sort or another. The idea of restorative justice, in general, is gaining more widespread support, especially following the creation of the International Criminal Court. And, truth commissions need not be national. A number of organizations in Greensboro, North Carolina, have come together to try to achieve reconciliation in a city which has been at the forefront of many violent racial incidents since the first sit-ins there in 1960.\nWhat Third Parties Can Do\nIt is probably even harder for outsiders to spark reconciliation than it is for governments.\nMost successful efforts at reconciliation have, in fact, been led by teams of \"locals\" from both sides of the divide. Thus, the TRC was chaired by Desmond Tutu, a black clergyman, while its vice president was Alex Boraine, a white pastor. Both were outspoken opponents of apartheid, but they made certain to include whites who had been supporters of the old regime until quite near its end.\nThe one exception to this rule is the role that NGOs can play in peacebuilding. The Mennonite Central Council, in particular, has focused a lot of its work in Central and South America on reconciliation. And even though it rarely uses the term, Search for Common Ground develops news programs and soap operas with conflict resolution themes in such countries as Macedonia and Burundi.\nResolution Isn't Cozy\nEven though reconciliation mostly involves people talking to each other, it is not easy to achieve. Rather it is among the most difficult things people are ever called on to do emotionally. Victims have to forgive oppressors. The perpetrators of crimes against humanity have to admit their guilt and, with it, their arrogance.\nBut perhaps the difficulty of reconciling can best be seen in the case of the former police officer and the seven mothers mentioned above. Most of them broke down and had to be escorted out of the room during the hearing at the TRC on the request for amnesty by two of their killers. And, their pain and anger are inescapable at the beginning of their meeting with the officer. It is clear that it is not easy for them to forgive him; but it is also abundantly clear how far doing so relieves them of the pain they have carried inside them for years.\n Fen Osler Hampson, Nurturing Peace: Why Peace Settlements Succeed or Fail (Washington, D.C.: United States Institute of Peace, 1996).\n Kenneth Boulding, Stable Peace Austin, TX: University of Texas Press, 1978.\n Long Night's Journey Into Day, a documentary film written and directed by Frances Reid and Deborah Hoffmann, produced by Frances Reid, Iris Films. Information about the film and a lot of associated information can be found at http://www.irisfilms.org/longnight/index.htm\nUse the following to cite this article:\nHauss, Charles (Chip). \"Reconciliation.\" Beyond Intractability. Eds. Guy Burgess and Heidi Burgess. Conflict Information Consortium, University of Colorado, Boulder. Posted: September 2003 <http://www.beyondintractability.org/essay/reconciliation>.","Selected Definitions of Peacebuilding\nThe following list is not exhaustive and only provides some of the most prominent academic and institutional definitions of peacebuilding.\nConflict Information Consortium, University of Colorado\nPeacebuilding is a process that facilitates the establishment of durable peace and tries to prevent the recurrence of violence by addressing root causes and effects of conflict through reconciliation, institution building, and political as well as economic transformation. This consists of a set of physical, social, and structural initiatives that are often an integral part of postconflict reconstruction and rehabilitation.\nPeacebuilding is the process of creating self-supporting structures that “remove causes of wars and offer alternatives to war in situations where wars might occur.” Conflict resolution mechanisms “should be built into the structure and be present there as a reservoir for the system itself to draw upon, just as a healthy body has the ability to generate its own antibodies and does not need ad hoc administration of medicine.”\nGlobal Partnership for the Prevention of Armed Conflict\nPeacebuilding involves addressing social and political sources of conflict as well as reconciliation.\nJoan B. Kroc Institute for Peace & Justice, University of San Diego\nStrategic Peacebuilding Principles:\n- Peacebuilding is complex and has multiple actors.\n- Peacebuilding requires values, goals, commitment to human rights and needs.\n- Peacebuilding goes beyond conflict transformation.\n- Peacebuilding cannot ignore structural forms of injustice and violence.\n- Peacebuilding is founded on an ethic of interdependence, partnership, and limiting violence.\n- Peacebuilding depends on relational skills.\n- Peacebuilding analysis is complex; underlying cultures, histories, root causes, and immediate stressors are essential.\n- Peacebuilding creates spaces where people interact in new ways, expanding experience and honing new means of communication.\n- Peacebuilding heals trauma, promotes justice and transforms relationships.\n- Peacebuilding requires capacity and relationship building at multiple levels.\nLederach, John Paul\n[Peacebuilding] is understood as a comprehensive concept that encompasses, generates, and sustains the full array of processes, approaches, and stages needed to transform conflict toward more sustainable, peaceful relationships. The term thus involves a wide range of activities that both precede and follow formal peace accords. Metaphorically, peace is seen not merely as a stage in time or a condition. It is a dynamic social construct.\nOrganization for Economic Cooperation and Development\n[Peacebuilding] includes activities designed to prevent conflict through addressing structural and proximate causes of violence, promoting sustainable peace, delegitimizing violence as a dispute resolution strategy, building capacity within society to peacefully manage disputes, and reducing vulnerability to triggers that may spark violence.\nSchool of Conflict Analysis and Resolution at George Mason University\nPeacebuilding is a term used within the international development community to describe the processes and activities involved in resolving violent conflict and establishing a sustainable peace. It is an overarching concept that includes conflict transformation, restorative justice, trauma healing, reconciliation, development, and leadership, underlain by spirituality and religion. It is similar in meaning to conflict resolution but highlights the difficult reality that the end of a conflict does not automatically lead to peaceful, stable social or economic development. A number of national and international organizations describe their activities in conflict zones as peacebuilding.\nUnited Nations Development Program\nPeacebuilding involves a range of measures targeted to reduce the risk of lapsing or relapsing into conflict by strengthening national capacities at all levels for conflict management, and laying the foundations for sustainable peace and development. Peacebuilding strategies must be coherent and tailored to the specific needs of the country concerned, based on national ownership, and should comprise a carefully prioritized, sequenced, and therefore relatively narrow set of activities aimed at achieving the above objectives. This office works specifically with peacebuilding in the context of conflict prevention.\nUnited Nations: Peacebuilding Support Office\nPeacebuilding is rather the continuum of strategy, processes and activities aimed at sustaining peace over the long-term with a clear focus on reducing chances for the relapse into conflict…. [It] is useful to see peacebuilding as a broader policy framework that strengthens the synergy among the related efforts of conflict prevention, peacemaking, peacekeeping, recovery and development, as part of a collective and sustained effort to build lasting peace. This office works specifically with peacebuilding in the context of postconflict reconstruction.\nUnited States Institute of Peace\nOriginally conceived in the context of postconflict recovery efforts to promote reconciliation and reconstruction, the term peacebuilding has more recently taken on a broader meaning. It may include providing humanitarian relief, protecting human rights, ensuring security, establishing nonviolent modes of resolving conflicts, fostering reconciliation, providing trauma healing services, repatriating refugees and resettling internally displaced persons, supporting broad-based education, and aiding in economic reconstruction. As such, it also includes conflict prevention in the sense of preventing the recurrence of violence, as well as conflict management and postconflict recovery. In a larger sense, peacebuilding involves a transformation toward more manageable, peaceful relationships and governance structures—the long-term process of addressing root causes and effects, reconciling differences, normalizing relations, and building institutions that can manage conflict without resort to violence. The US Government does not have a publicly available definition of peacebuilding, other than the definition provided by USIP."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:63a8595d-eb87-4745-b5e1-dcf3dc608481>","<urn:uuid:7a473c9a-2a9f-45a1-8356-9071b7f65836>"],"error":null}
{"question":"What are the key differences between social event accessibility at conferences versus digital content accessibility standards?","answer":"For conference social events, accessibility focuses on physical aspects like wheelchair accessible venues, available transportation, sign language interpretation, and informing participants about walking distances. In contrast, digital content accessibility under WCAG focuses on technical aspects across four principles (POUR): making content perceivable through text alternatives, ensuring operability through keyboard navigation, making information understandable, and ensuring robustness across different technologies and assistive tools.","context":["Shari Trewin 1, with additional input from Vicki Hanson, Jen Mankoff, and Donal Fitzpatrick\n- IBM T.J. Watson Research Center, Hawthorne, New York, USA\nThis document contains information for organizers of academic conferences who wish to make their events as accessible as possible, so that people with disabilities can participate fully. It is not intended as an accessibility checklist or requirements document, but offers general ideas and information that conference planners may wish to consider. These ideas are based on experiences in running the ACM SIGACCESS Conference on Computers and Accessibility (ASSETS). ASSETS is a conference of around 130 people, consisting of technical paper presentations, posters and demonstrations. The conference proceedings are published in the ACM digital library. Most attendees are non-local, and stay in the conference hotel. ASSETS is often attended by people with disabilities. In a typical year, there might be attendees who are blind, have low vision, are deaf or hard of hearing, use a power wheelchair or an electric scooter, have limited dexterity, and limited mobility. ASSETS strives to create an environment in which all attendees can participate and socialize together.\nTable of Contents\n- Planning an Accessible Conference\n- Location and Hotel Selection\n- Online Registration\n- Local Arrangements\n- Conference Sessions\n- Social Activities\n- More Information and Resources\nLocation and Hotel Selection\nWhen selecting a location and a specific hotel, consider the following:\n- Is there accessible public transport from the airport (or other likely arrival points) to the hotel? How complex is the journey?\n- Is the hotel ADA-compliant (or equivalent accessibility regulation for non-US sites)? Even if the hotel claims to meet the appropriate standards, implementation varies widely, so it is still valuable to ask more detailed questions and visit the site in person wherever possible. Hotels in the US should be able to provide an ADA compliance statement indicating how they meet the accessibility requirements of the law.\n- Is the hotel willing to reserve the wheelchair accessible rooms and rooms on the ground floor and near elevators for conference attendees to book?\n- Is the building wheelchair accessible through the main entrance?\n- Large, open public spaces with few landmarks are difficult to navigate without vision.\n- Are there at least 2 wheelchair accessible guest rooms? Specific adaptations include easily operable door handles and room keys, lowered spyhole and light switches, and room to maneuver bulky electric wheelchairs.\n- If the event is more than one day, is there a wheelchair-adapted room with a roll-in shower?\n- Are the elevators accessible? (Ask about tactile buttons, auditory feedback, visual feedback, wheelchair-height controls).\n- Does the hotel have Braille labeling, especially in elevators? (For ASSETS, we have even provided adhesive Braille labels for elevators.)\n- Does the hotel have alternative emergency devices for Deaf and hard of hearing guests?\n- How much walking is required to get between the nearest disabled parking space, the lobby, guest rooms, meeting rooms, nearest restrooms and lunch location? What is the wheelchair route between these locations?\n- Are there accessible restrooms near the meeting rooms, on the same level or with an elevator very near? The accessible stall should accommodate a large power wheelchair.\n- If there is a raised stage in the meeting room, is there a way for a wheelchair user to get to the podium? Are there railings on steps up to the stage?\n- Do the meeting rooms have an induction loop?\n- Will there be room to seat wheelchair users in the meeting sessions?\n- Obstacle-free environment (free of protruding objects and trip hazards)\n- Have the hotel staff had any disability awareness training?\n- How willing are the hotel staff to accommodate special requests?\n- Captioning and sign language translation are expensive. Plan for approximately $1500 per day in the budget to cover this service, in case it is requested. Sign language interpretation should include coverage of breaks and social events. The choice of whether to provide interpreters or captioning will depend on the requests made by attendees. For conferences where there will be attendees who sign in different sign languages, captioning can be used to accommodate everyone with a single service.\n- The audio-visual budget should include microphones for speakers, and for asking questions.\n- When deciding how many student volunteers are needed, consider that student volunteer duties might include assisting attendees with accessibility requests, such as guiding people to the restrooms or helping at the buffet.\n- Some attendees may require helpers or assistants to accompany them at the conference for care giving and/or language interpreting. These helpers should not have to pay the full conference fee. A suggested alternative is to have them pay for a â€˜meal-onlyâ€™ fee if they will be eating at the conference.\n- The conference website should meet W3C’s WCAG 2.0 accessibility guidelines and be tested for usability when fonts are enlarged, when style sheets are turned off, when images are turned off, and without using a pointing device (keyboard-only access). Testing with a screen reader and screen magnifier is also beneficial.\n- The website can provide information about accessibility of the conference hotel, accessibility of transportation to the hotel (including for bulky electric wheelchairs), details of local accessibility information services, and contact people at the conference hotel, and on the organizing committee for accessibility questions.\n- The website can offer information on how to create an accessible submission to the technical program.\n- Posting details and advance registration for social events on the website can facilitate access for those who are not able to see onsite notices.\n- Include a place for participants to indicate accessibility requirements.\n- Include a place to indicate dietary requirements\n- Allow registrants to request electronic proceedings, if the default is paper.\n- Check that the online registration process is accessible, using the same process as for the website.\n- Follow up with registrants to clarify accessibility requests\n- It is helpful to have a system (e.g. colored stickers on the conference badges) to identify people who have requested special meals, if these meals are served separately by hotel staff\n- Ask the hotel to provide labels on buffet dishes listing ingredients or indicating the presence of gluten/meat/dairy products/fish\n- Make sure that non-sugar beverage and healthy snack options are available\n- Ask the hotel staff to make drinking straws available\n- Have student volunteers assist with buffet-style food where needed.\n- Gather information on local emergency doctors, hospital facilities, wheelchair repair, physiotherapist and veterinarian.\n- Make sure the room seating allows for wheelchair access to the podium and the microphone for asking questions.\n- Make sure there are sufficient wheelchair seating places.\n- Offer an orientation session before the conference, for attendees and presenters to get to know the meeting room layout and seating arrangements. The room should already be set out as it will be for the meeting.\n- Make sure there is a connection into the PA for laptops\n- Make student volunteers available to attendees who request assistance.\n- If the conference provides an attendee list, have an electronic version available for reference at the registration desk, so that visually impaired attendees can browse the list for people they may wish to talk with. An alternative is to have attendees ask at the registration desk whether specific people are present.\n- Communicate with hotel staff about what to expect, and what accessibility requests they may receive from conference attendees. Specifically, staff should be available to show blind attendees to their rooms, and around the inside of their rooms to familiarize them with the layout. They should also be prepared to show attendees how to get to the meeting rooms. Make staff aware that Deaf attendees may communicate in different ways. Some may speak, others may write.\n- In advance of the sessions, presenters should be encouraged to prepare as accessible a presentation as possible, including captions for video. If presentations can be made available ahead of time, this is very helpful to attendees with visual impairment, captioners and sign language interpreters.\n- The printed session schedule should also be available electronically, in large print, and if possible Braille (1 Braille copy is sufficient).\n- At the start of the conference, if there are attendees in the audience with vision impairments, all slides, videos and visual demos will need to be described as part of the spoken presentation.\n- Remind the audience to use a microphone to ask questions, so that everyone can hear, and should state their name before speaking, for the benefit of those who cannot see who is speaking.\n- If there is an interpreter or captioner present, the lighting should be good enough that they can be easily seen by the deaf attendee(s). Explain to presenters that they should speak with a normal tone and pace, unless asked to slow down by the interpreter. For personal conversations, the attendees should be reminded to speak directly to the person, not to the interpreter.\n- A presenter with a visual impairment may request assistance from a student volunteer to advance slides. The session chair, or a volunteer, can also facilitate question asking.\n- Work with the proceedings publishers to ensure that the index and table of contents of the electronic proceedings are available in an accessible format. If they are html, the W3C WCAG 2.0 guidelines apply. For pdf documents, the Adobe Acrobat accessibility check applies (see resources below for more information about creating accessible pdf documents).\nThe social side of a conference is equally as important as the technical content. Try to avoid events that exclude some attendees. Consider whether:\n- Accessible transportation is available to any off-site events\n- Offsite venues (e.g. restaurants) are wheelchair accessible with an accessible restroom\n- Sign language interpretation is available for the social event.\n- Participants will need to be informed of the walking distance to nearby events, so they can decide how best to get there.\n- The event will have broad appeal (e.g not purely visual or auditory)\nMore Information and Resources:\n- Producing accessible pdf files\n- Planning and giving an accessible presentation\n- Planning Accessible Conferences and Meetings: An ERIC/OSEP Information Brief for Conference Planners\n- ADA Requirements checklist\n- W3C Web Content Accessibility Guidelines (WCAG 2.0)\n- Testing Web sites for accessibility: www.webaim.org is a good starting place. The WAVE Firefox plugin is a useful tool.","The Web Content Accessibility Guidelines (WCAG)\nWhat are the Web Content Accessibility Guidelines (WCAG)?\nThe guidelines offer technical recommendations on how to make website content accessible. The guidelines are also the standard reference for most website accessibility-related legislation like the Americans with Disability Act (ADA) in the US, and the European Web Accessibility Directive.\nWCAG Versions - 2.0, 2.1, 2.2 and 3.0\n- WCAG 2.0 - published 11 December 2008.\n- WCAG 2.1 - published on 5 June 2018 and is now the W3C recommended version\n- WCAG 2.2 - not yet in effect, scheduled to be published in 2021\n- WCAG 2.0 had 61 success criteria\n- WCAG 2.1 introduced 17 more success criteria to address mobile accessibility, people with low vision, and people with cognitive and learning disabilities.\n- WCAG 2.2 will be expanding on 2.1 with nine new success criteria, plus an update to one, with the goal of making content more accessible to a wider range of users.\nThe current standing WCAG versions 2.0 and 2.1 are categorized according to four principles, perceivable, operable, understandable, and robust (POUR).\nElements that convey information or components of a website’s user interface must be presented in a way that users are able to find, process, and understand.\nAll functionality and navigation on the website should be usable.\nInformation and the operation of the user interface must be clear and understandable to users of all abilities.\nThe website should be capable of adapting and developing itself to support a variety of current and potential future user agents, including assistive technologies.\nUnder each principle are testable success criteria that provide recommendations on how to make digital content more accessible. The success criteria are classified by three levels A, AA, and AAA, with A being the most basic level of WCAG compliance and AAA being the hardest.\nLearn more about the elements of WCAG and how to comply with its success criteria.\nOn the 21st of January, 2021, the WAI released the first working draft of the WCAG 3.0. WCAG 3.0 is planned to be a major revision with the intention to make the guidelines more user-friendly than the WCAG 2 iterations, and more flexible, covering even more content, apps, and tools, as well as organizations and disabilities. WCAG 3.0 is still in development and is not expected to be finalized for the next few years. Learn more about WCAG 3.0.\nThe different WCAG compliance levels\nThe WCAG categorizes its conformance based on three levels: A, AA, and AAA. To conform to the guidelines, it is a requirement that one of these levels should be fully met.\nMinimal WCAG compliance (level A)\nExamples of Level A success criteria:\n- All non-text content like images or videos must have a text alternative, like alt text or captions, that serves the equivalent purpose.\n- Users can navigate the website effectively using only a keyboard\n- Avoid using color as the only visual means of conveying information or prompting an action, like having green buttons with no text on them to suggest that it is meant to be selected as a ‘yes’ response.\n- If there is audio that auto-plays on your website for more than 3 seconds, ensure that that you provide means of adjusting the volume, stopping, or pausing it.\nAcceptable WCAG compliance (level AA)\nExamples of Level AA success criteria:\n- Ensuring that text on a webpage can be resized without assistive technology up to 200 % without loss of content or functionality.\n- Provide descriptive headings and labels in content\n- Navigational elements on the site, like menus, should be in a consistent, repeated position across the website.\n- When executing an action on the site, like filling in forms or clicking on buttons, errors can occur on the user’s part. If an error should occur, suggestions for correction should be provided.\nOptimal WCAG compliance (level AAA)\nExamples of Level AAA success criteria:\n- The visual presentation of text and images of text must have a contrast ratio of at least 7:1.\n- Removing timing limitations from all content, unless it is for non-interactive synchronized media and real-time events.\n- When a user has to submit information on a webpage, the submissions must be reversible, checked for input errors (and offer suggestions for correction if errors do occur), and there is a confirmation mechanism in place to allow the user to review the submission and edit if needed.\n- Images of text should be avoided or only used for decoration.\nWho should comply with the WCAG?\n- Web content developers (page authors, site designers, etc.)\n- Web authoring tool developers\n- Web accessibility evaluation tool developers\nHow to check your website’s WCAG compliance level\nHow Monsido can help your website meet WCAG standards\nEach audit scans your site for machine-testable issues, provides detailed reports so you can review any errors that may arise, gives you targeted recommendations on how to address these errors based on the guidelines, and shows you your compliance based on levels A, AA, and AAA. You can track and prove your accessibility compliance progress via reports in the History Center. We also offer accessibility training to customers and support, all-inclusive, to ensure that you are well-versed in both automated and manual remediation methods, and can efficiently and consistently improve your website’s accessibility.\nMonsido also offers free tools to complement your web accessibility efforts, including a color contrast checker for web teams to test out compliant color combinations for their web design, and an accessibility statement generator, which helps you generate a public statement declaring your commitment to web accessibility and helps make your web accessibility policy transparent to all your users."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:4c029784-0eca-4a08-b424-f9bcf0b6cfeb>","<urn:uuid:a2ef5797-959c-4ba6-9b52-ee60633ac0ca>"],"error":null}
{"question":"What similarities exist in the religious diversity represented by sacred sites in both India and Indonesia?","answer":"Both India and Indonesia showcase remarkable religious diversity through their sacred sites. In India, places like Varanasi hold significance for multiple faiths including Buddhists, Jains, and Hindus, with sites like the Kashi Vishwanath Temple dedicated to Lord Shiva. Similarly, Indonesia demonstrates religious plurality through its monuments - despite being known as the country with the largest Muslim population, it houses two of the world's greatest Buddhist and Hindu shrines: Borobudur (Buddhist) and Prambanan (Hindu). Prambanan features impressive towers reaching nearly 50 meters in height, while both temples are UNESCO World Heritage sites, exemplifying Indonesia's national motto of 'Unity in Diversity' (Bhinneka Tunggal Ika).","context":["Unveiling the Rich Cultural Tapestry: Exploring Indian Heritage\nIndia has a unique legacy that dates back thousands of years. It is a place of many different cultures, languages, and traditions. India is home to a wealth of historical landmarks, old temples, and architectural marvels that preserve the history of earlier times. Varanasi and Khajuraho stand out as important locations that provide an immersive journey into India’s rich past.\nThere are many other locations that add to India’s diverse heritage. This piece intends to keep focus on places located in North India. Delhi, Agra, Jaipur, Jaisalmer, Udaipur, Pushkar, and Bodh Gaya are a few of important locations.\nVaranasi: The Spiritual Heartbeat of India\nVaranasi is one of the world’s oldest continually inhabited cities. It lies tucked away on the banks of the revered River Ganges. Varanasi is revered by Buddhists, Jains, and Hindus alike for its extraordinary spiritual significance. The city offers a special fusion of heritage and contemporary, and it vibrates with spiritual energy.\nTourists, pilgrims, and seekers travel to Varanasi to experience the mesmerising Ganga Aarti, a religious ritual that lights up the riverbank each evening.\nThe bustling ghats, ancient temples, and meandering streets of Varanasi capture the spirit of Hinduism. The Lord Shiva-dedicated Kashi Vishwanath Temple is a monument to the city’s profound religious significance.\nAlong with being a centre of spiritual activity, the Dashashwamedh Ghat is also a location where history may be experienced while strolling along its ancient steps. A morning boat trip on the Ganges presents an incredible scene as worshippers conduct rituals and give prayers, confirming the spiritual significance of the city.\nKhajuraho: A Marvel of Art and Architecture\nNestled in the heart of Madhya Pradesh, Khajuraho is famed for its picturesque collection of temples. These temples are well known for their complex sculptures and breathtaking depictions of a unique fusion of sensuality and spirituality.\nThe temples, which are included as a UNESCO World Heritage Site, were constructed in the 10th and 11th CE by the Chandela rulers. These temples are a testament to the sophistication of Indian art in the past.\nThe exquisite carvings that adorn the temples of Khajuraho explore various facets of life, from gods and goddesses to everyday activities. The most famous of these temples is Kandariya Mahadeva Temple, known for its towering spire and intricate sculptures.\nErotic sculptures on the walls of Khajuraho Temples often attract lots of attention. But they are just a small part of the larger narrative presented by the artists on the walls of the temples. These sculptures, depicting human relationships and desires, were intended to celebrate the full spectrum of human experience.\nA light and sound is there every evening. This is a must see to delve deeper into the history and significance of Khajuraho’s temples. The show, through vivid narration, brings the stories of the temples to life, adding a new dimension to your understanding. This is a tremendous help in understanding the art works seen on the walls of the temples during your sightseeing tours.\nAgra: The Majestic Mughal Heritage\nNo exploration of North India’s heritage is complete without a visit to Agra. This is home to the iconic Taj Mahal.\nCommissioned by Mughal Emperor Shah Jahan to show his love for his wife Mumtaz Mahal, Taj Mahal, made in white marble, is adorned with intricate inlay work and geometric patterns.\nTaj Mahal reflects a symphony of architectural brilliance and emotional resonance. Its intricate detailing, from its ornate calligraphy to its symmetrical gardens, leaves visitors awe-struck.\nBeyond the Taj Mahal, Agra hosts other architectural gems, such as the Agra Fort, a UNESCO World Heritage site, that chronicles India’s history through its imposing walls and magnificent palaces.\nDelhi: The Historical Crossroads\nDelhi, India’s capital, ties together the country’s varied past and present. A prime example of Indo-Islamic architecture is the Qutub Minar, a majestic minaret built in the 13th century. Nearby is Humayun’s Tomb, an architectural precursor to Taj Mahal, showcases the magnificence of Mughal architecture.\nThe vibrant markets of Old Delhi, the opulence of the governmental structures in New Delhi, and the cutting-edge architectural wonder of the Lotus Temple are all part of Delhi’s rich legacy.\nJaipur: The Pink City’s Royal Resplendence\nJaipur, the capital of Rajasthan, is a picture of imperial splendour and exquisite architecture. The distinctive pink colour of the city’s buildings is what gives it the moniker “Pink City.” A testimony to the wealth of Rajputana kings is the expansive Amber Fort, which features exquisite mirror work and spectacular views.\nThe Palace of Winds, also known as the Hawa Mahal, is a masterpiece of architecture that was built to allow royal ladies to watch street processions while keeping out of sight of the general public.\nJaisalmer: The Golden Citadel\nThe desert city of Jaisalmer enchants visitors with its golden-hued architecture and enchanting sand dunes. Jaisalmer Fort, a living citadel, stands as a testament to the city’s history and resilience. The intricate latticework of the havelis (mansions) and the desert safari experiences make Jaisalmer a unique heritage destination.\nThe havelis of the traders of the past in Jaisalmer also show how much wealth Jaisalmer once saw.\nPushkar: The Colorful Melange\nA little village in Rajasthan called Pushkar is well-known for its lively culture and the Pushkar Camel Fair. This yearly occasion turns the sleepy village into a lively funfair, luring both vendors and visitors. The revered Pushkar Lake and the Brahma Temple provide spiritual refuge amidst the celebrations.\nBodh Gaya: Discovering Enlightenment\nBodh Gaya is a place of profound spiritual significance, revered as the site where Siddhartha Gautama attained enlightenment under the Bodhi tree. This holy site is extremely valuable historically and attracts pilgrims and tourists from all over the world.\nA UNESCO World Heritage site, the Mahabodhi Temple houses the Bodhi tree. The scenes from Buddha’s life, his teachings, and many episodes of his path to enlightenment are depicted on the walls’ beautiful stone carvings.\nThe Vajrasana is thought to be the location where Buddha pondered and attained enlightenment because it is situated beneath the Bodhi tree.\nSculptures, inscriptions, and artefacts that chart the evolution of Buddhist art and culture can be seen in the Bodhgaya Archaeological Museum.","Thursday, September 29, 2011\nIndonesia: An abundance of wonders and beauty\nIndonesia has a national motto which reads in Bahasa Indonesia - Bhinneka Tunggal Ika; translated to English it means Unity in Diversity. For anyone in Indonesia this is familiar but for those less familiar with this country it is an interesting insight into the aspirations of the nation. The word ‘diversity’ is a particularly good representation of this country. It is a country with remarkable diversity.\nThat diversity manifests itself in a wealth of wonders and beauty that are the product of a varied range of natural environs and social and ethnic groups and their cultures. It is something of a cliché to think of Indonesia as a string of thousands of islands and the world’s greatest archipelago; a person could spend a lifetime island hopping here and be consistently amazed or impressed.\nSome of Indonesia’s more renowned tourist destinations are a good place to start in appreciating this country. Java is, it is reasonable to state, the foremost of Indonesia’s islands both historically and in terms of the concentration of the nation’s population. Within Java there are a significant number of tourist destinations of repute.\nThe string of volcanic mountains that line Java presents numerous natural wonders to be seen and explored, but perhaps the most dramatic of the mountain regions is Bromo in the east of the island. This range of mountains creates stunning vistas and combined with the vast ‘Sea of Sand’, that sits in the area too, makes an area for rugged trekking and appreciation of and awe at the might of nature.\nThe Indonesian archipelago is noted as one of the more volatile regions of the world for seismic and volcanic activity and so visits to the likes of Bromo sometimes have to be timed to coincide with less active periods in the life of the volcanoes. Even during a relatively calm period care may be needed as the stench of sulfur pouring up from the craters can be quite overpowering.\nBut the stunning views and wonder of viewing the Earth’s steaming, bubbling and smoking essence is an experience to be remembered. Not so far away from Bromo, but in Central Java, are more peaceful wonders that were created long ago by human hands and today represent wonders of world heritage. The temples at Borobudur and Prambanan are places of religious and spiritual importance.\nBorobudur is quite simply staggering to behold in its beauty and scale. This hill-like Buddhist temple seems to be covered from top to bottom in statuary and relief carving. The numbers at Borobudur are staggering to behold – there are more than 2,500 panels of relief carving and over 500 statues of the Buddha. It all amounts to a wealth of craftsmanship and a day’s visit can never be enough.\nMeanwhile, the nearby Prambanan temple is Hindu in origin and like Borobudur is a UNESCO World Heritage site. Prambanan is quite different to Borobudur though. Prambanan is a series of towers that when built must have been the skyscrapers of that time. The tallest central one reaches up to near 50 meters in height.\nBorobudur and Prambanan stand as exemplars of Indonesia’s diversity. Indonesia is so often referred to as the country with the biggest Muslim population and this is true but here stand two of the world’s greatest Buddhist and Hindu shrines.\nThe rest of the world is familiar with another Hindu part of Indonesia – namely Bali – and often this jewel of an island is mistakenly thought of as separate to Indonesia. Bali is though one small part of this great nation. Destinations abound and are abundant in what they have to offer to the traveler. Simon Marcus Gower, Contributor, Jakarta Post"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:c6955415-80d5-4aae-8954-b204ba8091ae>","<urn:uuid:de7a7ffb-d006-4b64-a6b3-536614f8af6c>"],"error":null}
{"question":"I'm planning to travel internationally. What's the main difference between how Hepatitis A and Hepatitis B are transmitted, and how does this affect prevention methods?","answer":"Hepatitis A and B have different transmission routes. Hepatitis A is primarily spread through contaminated food or water, while Hepatitis B is transmitted through contact with infected blood, semen, or other body fluids. For prevention, Hepatitis A requires careful attention to food and water hygiene, especially when traveling to areas with poor sanitary conditions. Hepatitis B prevention focuses on avoiding contact with infected bodily fluids, using protection during sexual contact, and not sharing items that could have blood contact like needles, razors, or toothbrushes. Both diseases can be prevented through vaccination, which is recommended before international travel.","context":["Hepatitis B is a liver infection and is spread by sharing infected bodily fluids. This condition often does not show symptoms but chronic cases can cause serious damage.\nWhat is hepatitis B?\nHepatitis B is an infection of the liver caused by the hepatitis B virus (HBV). There are other hepatitis viruses (A, C, D, E) that may behave and be transmitted differently. HBV causes swelling and inflammation of the liver that prevents its normal function. HBV can cause a short-term, acute illness that lasts up to a few months. It can also cause a lifelong illness, which more commonly develops in children.\nSince many people may not have symptoms or do not realize they are infected, their illness is often not diagnosed. Presenting symptoms include fever, fatigue, loss of appetite, nausea, vomiting, abdominal pain, and dark urine among others.\nTreatment is much like that for a regular infection, with plenty of rest, fluids, and adequate nutrition provided at home or from a medical team in more severe cases. Hepatitis B can be prevented by a variety of measures.\nYou should visit your primary care physician. Hepatitis B infection, if confirmed by a blood test, is treated with prescription antiviral medication.\nFree, private, and secure to get you the best way to well. Learn about our technology.\nHepatitis b symptoms\nAccording to the Centers for Disease Control and Prevention (CDC), in 2016, more than 3000 cases of acute hepatitis B were reported in the USA. Just 30 to 50 percent of adults who contract hepatitis B experience symptoms, and many people infected with hepatitis B virus do not realize they are infected, and therefore commonly not reported. The CDC estimates the actual number of acute hepatitis B cases was almost 21,000 in 2016.\nThese symptoms can appear from eight weeks to five months after initial exposure to the virus. The average time from exposure to onset is three months. It is important visit your healthcare professional immediately if there is any suspicion of infection. This is also important because the virus can still be unknowingly transmitted to others. Symptoms of acute hepatitis B include:\n- Loss of appetite\n- Abdominal pain\n- Dark urine\n- Clay-colored bowel movements\n- Joint pain\n- Jaundice: This is yellow coloring of the skin or eyes.\nDeveloping the lifelong form\nIn the acute form of HBV, symptoms usually last several weeks, but can last as long as six months. About 95 percent of adults exposed to HBV have the acute form, while the remaining 5 percent develop the lifelong form. In children, the statistics are reversed; 95 percent of children will develop the lifelong form of hepatitis B, while just 5 percent will have the acute form of the infection.\nHepatitis b causes\nHepatitis B is caused by a variety of factors that contribute to its spread. However, it is important to also understand how HBV is not spread. Certain individuals do have characteristics that place them at a higher risk of developing HBV.\nHow HBV is spread\nHepatitis B results when the hepatitis B virus enters the bloodstream. The only way to become infected is to be exposed to blood, semen or other body fluid that is infected. People can become infected from the virus via:\n- Birth: Spread can occur an infected mother to her baby during birth\n- Sex with an infected partner\n- Sharing needles or syringes with an infected person\n- Sharing items: This includes toothbrushes, razors or other medical equipment that accesses the bloodstream (a glucose stick, for example) with an infected person\n- Direct contact: Spread can occur from touching blood or open sores of an infected person\n- Exposure to blood: Spread can occur from needlesticks or other sharp instruments of an infected person\nHow HBV is not spread\nThe common thread among the causes above is that they involve contact/transmission of body fluids. Hepatitis B virus is not spread via airborne transmission or contact transmission if there is no blood contact involved. For example, hepatitis B virus is not spread via:\n- Food or water\n- Sharing eating utensils\n- Hand holding\nWho is most likely to be affected\nGiven the ways that HBV is spread from person-to-person, there are specific populations that are at a greater risk for contracting the disease even though anyone can get HBV. These populations include:\n- Infants born to infected mothers\n- People who inject drugs or share needles, syringes, or other drug equipment\n- Sex partners of people with HBV\n- Men who have sexual contact with men\n- People who live with a person who has HBV\n- Healthcare and public safety workers exposed to blood\n- Hemodialysis patients\nFree, secure, and powered by Buoy advanced AI to get you the best way to better. Learn about our technology.\nTreatment options and prevention for hepatitis b\nTreatment options include methods of testing and methods to restore fluid and nutrition levels as well as hospitalization, if necessary. There are also important preventative methods that are highly effective.\nThe first step to treating or preventing HBV infection is to see a doctor for testing. A blood test can help to tell whether you have immunity, or an acute or chronic hepatitis B infection.\nRestoring fluids and nutrition\nThere is no curative medication for HBV, and most infections in adults resolve on their own. Rest, fluids, and adequate nutrition will help this process. More severe cases of acute hepatitis B will require hospitalization to provide appropriate fluid and nutrition supplementation while the infection resolves.\nFortunately, HBV is a preventable disease, and an effective vaccine has been available since 1981. The hepatitis B vaccine is safe and effective in all age groups. It is a series of shots given over several months. You can also do the following in to reduce your risk of HBV:\n- Safer sex: Condoms act as a protective barrier against bodily fluids such as blood and semen that could be infected with the virus. Condoms can reduce your risk of contracting HBV, they do not eliminate the risk completely.\n- Know the HBV status of sexual partners: Don't engage in unprotected sex (sex without a condom) unless you're absolutely certain your partner isn't infected with HBV or any other sexually transmitted infections.\n- Be cautious about body piercing and tattooing: If you get a piercing or tattoo, look for a reputable shop. Ask about how the equipment is cleaned. Make sure the employees use sterile needles. If you can't get answers, look for another shop.\n- Avoid shared, re-used, or dirty needles: Use a sterile needle each time you inject anything and never share needles.\nIf a HBV infection does resolve on its own, the affected individual becomes immune and cannot become infected with HBV again.\nIndividuals with chronic HBV infection are at an increased risk for liver damage, cirrhosis, liver failure, liver cancer, and death. Chronically infected individuals will need to follow up closely with a liver doctor. Some antiviral medications are available that can help to suppress the infection.\nWhen to seek further consultation for hepatitis b\nIf you think that you have been exposed to HBV but are unsure of your vaccination status, call your physician immediately. They may be able to perform tests that will indicate your vaccination and exposure status.\nYou may be eligible for an injection of an antibody that may help protect you from HBV if it is given to you within 12 hours of exposure. Because this treatment only provides short-term protection, you also should get the hepatitis B vaccine at the same time, if you never received it.\nQuestions your doctor may ask to determine hepatitis b\n- Have you lost your appetite recently?\n- Have you experienced any nausea?\n- Are you sick enough to consider going to the emergency room right now?\n- Any fever today or during the last week?\n- Have you been feeling more tired than usual, lethargic or fatigued despite sleeping a normal amount?\nSelf-diagnose with our free Buoy Assistant if you answer yes on any of these questions.\n- Surveillance for Viral Hepatitis - United States, 2016. Centers for Disease Control and Prevention. Published October 1, 2018. CDC Link\n- Trpo C, Chan HLY, Lok A. Hepatitis B Virus Infection. The Lancet. Published June 18, 2014. The Lancet Link\n- Elgouhari HM, Abu-Rajab Tamimi TI, Carey WD. Hepatitis B Virus Infection: Understanding Its Epidemiology, Course, and Diagnosis. Cleveland Clinic Journal of Medicine. 2008;75(12):881-889. NCBI Link\n- Komatsu H, Inui A. Hepatitis B Virus Infection in Children. Expert Review of Anti-Infective Therapy. 2015;13(4):427-450. NCBI Link\n- Wilkins T, Zimmerman D, Schade RR. Hepatitis B: Diagnosis and Treatment. American Family Physician. 2010;81(8):965-972. AAFP Link\n- Tang LSY, Covert E, Wilson E, Kottilil S. Chronic Hepatitis B Infection: A Review. JAMA Network. 2018;319(17):1802-1813. JAMA Link","Safety And Adverse Events\nCommon and local adverse events\nHA vaccine is well tolerated. Reactions are generally mild and transient, and are usually limited to soreness and redness at the injection site. Other less frequent reactions include headache, irritability, malaise, fever, fatigue and gastrointestinal symptoms. Injection site reactions occur less frequently in children than in adults as do mild, systemic events . No significant difference in reactions is evident between initial and subsequent doses of vaccine or in the presence of pre-existing immunity.\nRefer to Hepatitis B Vaccine in Part 4 for information about HAHB vaccine.\nInjection site reactions following receipt of standard human Ig include tenderness, erythema and stiffness of local muscles, which may persist for several hours. Mild fever or malaise may occasionally occur.\nLess common and serious or severe adverse events\nLess common side effects following receipt of standard human Ig include flushing, headache, chills and nausea. Urticaria, angioedema and anaphylactic reactions may occur rarely.\nGuidance on reporting Adverse Events Following Immunization\nVaccine providers are asked to report, through local public health officials, any serious or unexpected adverse event temporally related to vaccination. An unexpected AEFI is an event that is not listed in available product information but may be due to the immunization, or a change in the frequency of a known AEFI.\nContraindications and precautions\nWhy It Is Used\nHepatitis B virus causes a liver infection that can lead to serious complications, including liver cancer. It is common in people throughout the world, particularly in Asia and sub-Saharan Africa.\nThe Canadian National Advisory Committee on Immunization recommends hepatitis B immunization for all children. Pregnant women and other adults who do not have immunity and who have a high chance of exposure should be vaccinated.\nHow Hepatitis Is Spread\nHepatitis A: About 20,000 people in the U.S. contract hepatitis A each year. The hepatitis A virus is found in the stool of the infected person. It is spread through contaminated food or water or by certain types of sexual contact.\nChildren who get hepatitis A often don’t have symptoms, so they can have the virus and not know it. However, they can still spread it easily. Fortunately, children are now routinely vaccinated against hepatitis A.\nMost people who get hepatitis A recover completely within two weeks to six months and don’t have any liver damage. In rare cases, hepatitis A can cause liver failure and even death in older adults or people with underlying liver disease.\nHepatitis B: Every year, about 40,000 people in the U.S. become infected with hepatitis B. Acute hepatitis lasts from a few weeks to several months. Many infected people are able to clear the virus and remain virus-free after the acute stage. However, for others, the virus remains in the body, and they develop chronic hepatitis B infection, which is a serious, lifelong condition. About 1.2 million people in the U.S. have chronic hepatitis B. Of these, 15% to 25% will develop more serious health problems, such as liver damage, cirrhosis, liver failure, and liver cancer, and some people die as a result of hepatitis B-related disease.\nYou May Like: Difference Between Hepatitis B And C\nHow Does Hepatitis A Spread\nContaminated food or water is the most common source of hepatitis A infection. Contamination can happen at any point in the food growing, processing or cooking process. Travelers are at an increased risk. Take extra precautions in developing countries with poor sanitary conditions.\nIt is possible for the disease to spread through close contact with an infected person. This includes sex or caring for an infected person.\nVaccination is the best form of protection.\nVaccine For Hepatitis B\nHepatitis B Vaccine\nIt takes only a few shots to protect yourself and your loved ones against hepatitis B for a lifetime.\nThe hepatitis B vaccine is a safe and effective vaccine that is recommended for all infants at birth and for children up to 18 years. The hepatitis B vaccine is also recommended for adults living with diabetes and those at high risk for infection due to their jobs, lifestyle, living situations, or country of birth. Since everyone is at some risk, all adults should seriously consider getting the hepatitis B vaccine for a lifetime protection against a preventable chronic liver disease.\nThe hepatitis B vaccine is also known as the first anti-cancer vaccine because it prevents hepatitis B, the leading cause of liver cancer worldwide.\nYou cannot get hepatitis B from the vaccine. All hepatitis B vaccines that have been used since 1986 are made synthetically meaning the hepatitis B vaccines do not contain any blood products. Learn more.\nIf you have a current HBV infection or have recovered from a past HBV infection, the hepatitis B vaccine series will not benefit you or clear the virus. However, the vaccine can provide a lifetime of protection for loved ones who do not have hepatitis B and get the vaccine as soon as possible. Testing is the only way to know if you or your loved ones have a current infection or have recovered from a past infection.\nHepatitis B Vaccine Recommendations\nThree-Dose Hepatitis B Vaccine Schedule\nYou May Like: How To Read Hepatitis C Test Results\nWhat Is Hepatitis B Virus\nHepatitis B virus attacks the liver. Hepatitis B virus infections are known as the “silent epidemic” because many infected people don’t experience symptoms until decades later when they develop hepatitis , cirrhosis , or cancer of the liver . Every year in the United States about 22,000 new hepatitis B infections occur and about 2,000 people die from their infections.\nSymptoms Of Hepatitis B\nMany people with hepatitis B will not experience any symptoms and may fight off the virus without realising they had it.\nIf symptoms do develop, they tend to happen 2 or 3 months after exposure to the hepatitis B virus.\nSymptoms of hepatitis B include:\n- flu-like symptoms, including tiredness, a fever, and general aches and pains\n- loss of appetite\n- tummy pain\n- yellowing of the skin and eyes\nThese symptoms will usually pass within 1 to 3 months , although occasionally the infection can last for 6 months or more .\nYou May Like: How Do You Pass Hepatitis C\nPrevent Hepatitis B Infections In Newborns\nIf you are pregnant and have hepatitis B, talk with your doctor about lowering the risk that the infection will spread to your baby. Your doctor will check your virus levels during pregnancy. If virus levels are high, your doctor may recommend treatment during pregnancy to lower virus levels and reduce the chance that hepatitis B will spread to your baby. Your doctor may refer you to a liver specialist to find out if you need hepatitis B treatment and to check for liver damage.\nWhen it is time to give birth, tell the doctor and staff who deliver your baby that you have hepatitis B. A health care professional should give your baby the hepatitis B vaccine and HBIG right after birth. The vaccine and HBIG will greatly reduce the chance of your baby getting the infection.\nWhat To Think About\nIf you are exposed to HBV before you have received all three shots in the vaccination series, a dose of hepatitis B immune globulin usually will prevent infection until the vaccine takes effect.\nIf you have already had hepatitis B and have developed protective antibodies to the virus, you do not need the vaccine because you have lifetime protection against the infection. If you are not sure whether you have had hepatitis B, you can be tested, or you can be vaccinated without testing. The vaccine is not harmful for you if you are already immune.\nIf you have chronic HBV infection, the vaccine will be ineffective, although it is not harmful.\nThe vaccine is safe for women who are pregnant or breastfeeding.\nYou May Like: Can Hepatitis B Be Cured With Antibiotics\nWhat Are Clinical Trials For Hepatitis B\nClinical trialsand other types of clinical studiesare part of medical research and involve people like you. When you volunteer to take part in a clinical study, you help doctors and researchers learn more about disease and improve health care for people in the future.\nResearchers are studying many aspects of hepatitis B, such as\n- progression of hepatitis B and long-term outcomes\n- new treatments for hepatitis B\n- prevention of reactivated or worsening hepatitis B in people receiving cancer treatment\nHow Do Doctors Treat Hepatitis B\nDoctors typically dont treat hepatitis B unless it becomes chronic. Doctors may treat chronic hepatitis B with antiviral medicines that attack the virus.\nNot everyone with chronic hepatitis B needs treatment. If blood tests show that hepatitis B could be damaging a persons liver, a doctor may prescribe antiviral medicines to lower the chances of liver damage and complications.\nMedicines that you take by mouth include\nA medicine that doctors can give as a shot is peginterferon alfa-2a .\nThe length of treatment varies. Hepatitis B medicines may cause side effects. Talk with your doctor about the side effects of treatment. Tell your doctor before taking any other prescription or over-the-counter medicines.\nYou May Like: Is Hepatitis C Contagious Sexually\nConcurrent Administration Of Vaccines\nHB-containing vaccines may be administered concomitantly with other vaccines or with HBIg. Different injection sites and separate needles and syringes must be used for concurrent parenteral injections.\nRefer to Timing of Vaccine Administration in Part 1 for additional information about concurrent administration of vaccines.\nOutlook For Hepatitis B\nThe vast majority of people infected with hepatitis B in adulthood are able to fight off the virus and fully recover within 1 to 3 months.\nMost will then be immune to the infection for life.\nBabies and children with hepatitis B are more likely to develop a chronic infection.\nChronic hepatitis B affects around:\n- 90% of babies with hepatitis B\n- 20% of older children with hepatitis B\n- 5% of adults with hepatitis B\nPage last reviewed: 30 January 2019 Next review due: 30 January 2022\nRead Also: Can You Get Hepatitis From Saliva\nWhat Are The Side Effects\nThe most common of the hepatitis B vaccine are mild and include:\n- Low fever or,\n- Sore arm from the shot.\nPrepare for your child’s vaccine visit and learn about how you can:\n- Research vaccines and ready your child before the visit\n- Comfort your child during the appointment\n- Care for your child after the shot\nHepatitis A Vaccine And International Travel\nWho should get the hepatitis A vaccine before traveling internationally?\nAll unvaccinated people, along with those who have never had hepatitis A, should be vaccinated before traveling to countries where hepatitis A is common. Travelers to urban areas, resorts, and luxury hotels in countries where hepatitis A is common are still at risk. International travelers have been infected, even though they regularly washed their hands and were careful about what they drank and ate. Those who are too young or cant get vaccinated because of a previous, life-threatening reaction to the hepatitis A vaccine or vaccine component should receive immune globulin. Travelers to other countries where hepatitis A does not commonly occur are not recommended to receive hepatitis A vaccine before travel.\nHow soon before travel should I get the hepatitis A vaccine?\nYou should get the first dose of hepatitis A vaccine as soon as you plan international travel to a country where hepatitis A is common. The vaccine will provide some protection even if you get vaccinated closer to departure. For older adults , people who are immunocompromised, and people with chronic liver disease or other chronic medical conditions the health-care provider may consider, based on several factors, giving an injection of immune globulin at the same time in different limbs.\nWhat should I do if I am traveling internationally but cannot receive hepatitis A vaccine?\nAlso Check: How Do People Catch Hepatitis B\nWhat Is The Hepatitis A Vaccine\nThe hepatitis A vaccine is a dose of inactive virus that stimulates your natural immune system. After the hepatitis A vaccine is given, your body makes antibodies that will protect you against the hepatitis A virus.\nVaccination for hepatitis A requires 2 shots, 6 months apart. The vaccine is given with an injection, into the muscle of the upper arm. If for some reason the second injection doesn’t take place at 6 months, you can receive the second dose at a later time.\nIf you need hepatitis B vaccination in addition to hepatitis A, you can do these individually or as a combined vaccine that covers both. The combination vaccine is given as 3 injections over a 6-month period–an initial dose, followed by a second dose 1 month later, and then a third dose 5 months after the second.\nWhy Is The Hepb Vaccine Recommended\nPeople who dont know they’re infected can spread the hepatitis B virus. So it cant be avoided just by being careful. That’s why health experts recommend that all babies get the vaccine right from birth.\nThe HepB injection usually creates long-term immunity. Most infants who get the HepB series are protected from hepatitis B infection beyond childhood, into their adult years.\nEliminating the risk of infection also decreases risk for cirrhosis of the liver, chronic liver disease, and liver cancer.\nAlso Check: What Is Hepatitis C Genotype 1a\nIf I Already Have Hepatitis B Can The Vaccine Treat It\nNo. The hepatitis vaccine prevents hepatitis, but doesnt cure it if you already have it. If you have hepatitis B, there are other treatment options.\nHowever, if you recently got exposed to the hepatitis B virus and you havent had the vaccine yet, tell your doctor right away. The vaccine and possibly other treatment can reduce your chances of getting hepatitis B if you get it within 2 weeks after you came into contact with the virus. The sooner you seek care after being exposed to hepatitis B, the better, so try to get there right away.\nWho Should Not Get The Vaccine\nSpeak with your health care provider if you have had a life-threatening reaction to a previous dose of hepatitis A vaccine, or any component of the vaccine including neomycin, or to latex.\nThere is no need to delay getting immunized because of a cold or other mild illness. However, if you have concerns speak with your health care provider.\nAlso Check: Who To Screen For Hepatitis C\nWho Should Get The Hbv Vaccine\nThe Centers for Disease Control and Prevention recommends that children should get their first hepatitis B vaccine at birth and complete the doses by 6 to 18 months of age. However, the HBV vaccine is still recommended for all children if they havent already gotten it, from infanthood up to 19 years old. Most U.S. states require a hepatitis B vaccine for school admittance, however.\nIts also recommended for adults at an increased risk of catching the HBV infection, or anyone who fears they have or will be exposed to it in the near future.\nThe HBV vaccine is even safe to administer to pregnant women.\nWho Is The Vaccine Recommended For\nThe HBV vaccine is recommended for adults who:\n- Are sexually active with or live in the same house as a person with HBV\n- Are sexually active with more than one partner\n- Seek care in a clinic for sexually transmitted diseases, HIV testing or treatment, or drug treatment\n- Men who have sex with other men\n- People who inject drugs or have a job that involves contact with human blood\n- People who are on the staff of or a client in an institution for the developmentally disabled\n- Hemodialysis patients or those with end-stage renal disease\n- People with HIV\n- Dialysis patients\n- Those with chronic liver disease\n- Those who live or travel for more than six months a year in countries where Hepatitis B is common\n- Prisoners in a correctional facility\nAlso Check: Can Hepatitis C Be Passed From Mother To Child\nWhat Is Hepatitis A & B\nHepatitis A and B are two viruses that affect your livers ability to function. Hepatitis A is usually spread through the ingestion of contaminated food or water or close contact including sexual relations with someone who is already infected. Hepatitis B is spread through contact with the blood or other body fluids of an infected person, including contact with objects that could have blood or body fluids on them such as toothbrushes and razors.\nThe hepatitis A virus can cause a flu-like illness, a yellowing of the skin or eyes , along with severe stomach pains and diarrhea. The hepatitis B virus can cause a short-term flu-like illness, or long-term infection that can lead to liver damage, liver cancer or death. Babies and young children infected with hepatitis B are more likely to get this chronic form of the disease.\nWhat Are Dosages Of Hepatitis B Vaccine\nDosages of Hepatitis B Vaccine:\n- 40 mcg/ml\n- 5 mcg/0.5 ml\n- 10 mcg/0.5 mg\nDosage Considerations Should be Given as Follows:\n- Engerix B: 1 mL intramuscularly at 0, 1, and 6 months\n- Recombivax HB: 1 mL intramuscularly at 0, 1, and 6 months\n- Adults receiving dialysis or other immunocompromising conditions\n- Recombivax HB : 40 mcg intramuscularly at 0, 1, and 6 months, OR\n- Engerix-B : 40 mcg intramuscularly at 0, 1, and 6 months\n- Unvaccinated children should complete a 3-dose series\n- Children aged 11-15 years: 2-dose series of adult formulation Recombivax HB is licensed for use in children aged 11 through 15 years\n- low blood pressure\n- pain when urinating\nSuspected adverse events after administration of any vaccine may be reported to Vaccine Adverse Events Reporting System , 1-800-822-7967\nThis document does not contain all possible side effects and others may occur. Check with your physician for additional information about side effects.\nYou May Like: How Is Hepatitis C Test Done"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:d843a744-a7e6-4e74-a027-4b893c95b293>","<urn:uuid:7e73c843-507d-43be-8c79-314b79052f45>"],"error":null}
{"question":"As a healthcare administrator, I need to understand how correctional facilities manage both patient privacy and inmate medical complaints. What are the key protocols for each?","answer":"For patient privacy, HIPAA requires covered entities including correctional healthcare facilities to protect Protected Health Information (PHI), provide Notices of Privacy Practices (NPP), obtain proper authorizations for sharing information, and ensure staff are trained on privacy protocols. For inmate medical complaints, facilities must have formal grievance procedures with clear written steps and timelines. Medical staff responding to grievances must document responses in medical records while adhering to HIPAA privacy rules about information sharing. The grievance system requires inmates to exhaust administrative remedies before filing lawsuits, while proper documentation that respects privacy helps create helpful administrative records. Staff should address specific medical issues raised in grievances while maintaining patient confidentiality, and work to improve both medical services and communication to prevent future complaints.","context":["Don’t Fight the Grievance System – Take What the Grievance System Gives You\nA. Compliance with the Grievance Procedure is Required Before an Inmate May Litigate a Claim\nYou provide quality care to a large, complicated, and oftentimes thankless patient population. An inmate grievance is then lodged against you – the content of which may have no basis in fact or medicine. This is annoying. You feel confident that you provided any and all medically necessary care and your charting is on point, so you brush off the grievance knowing it to be unfounded. This is where you go wrong. While responding to grievances is time-consuming, when done properly, you might avoid a lawsuit or you can lay the foundation for getting the inevitable lawsuit dismissed early on.\nThis article discusses the Prison Litigation Reform Act (PLRA) grievance requirement and instances where grievances that have been poorly responded to allowed inmates to overcome grievance challenges and proceed to full-blown litigation. The article will conclude with practice tips and recommendations.\nThe PLRA provides that \"[n]o action shall be brought with respect to prison conditions” under Federal law by “a prisoner confined in any jail, prison, or other correctional facility until such administrative remedies as are available are exhausted.” 42 U.S.C. 1997e(a). The plain language of the PLRA contains only one exception: the grievance procedure must be “available.” This caveat has been interpreted to excuse compliance with the grievance procedure in instances where, inter alia, a grievance was not responded to (or was insufficiently responded to) by prison officials, prison officials interfered with the prisoner’s attempt to submit a grievance, and where the inmate lacked the mental capacity to utilize the grievance procedure.\nThe grievance requirement gives inmates an incentive to make full use of the facility’s grievance process and provides the administration with an opportunity to correct any errors. It also reduces the quantity of inmate suits and improves the quality of suits that are filed because proper exhaustion is believed to result in the creation of an administrative record helpful to the court. The PLRA has been interpreted to require strict compliance with the facility’s grievance procedure, i.e., utilization of appeals, compliance with deadlines, must be submitted to the appropriate person.\nIf an inmate files a Section 1983 claim against you, one of the first issues your defense attorney will address is whether the inmate exhausted the facility’s grievance procedure before filing suit. If not, your attorney will file a motion with the court asking the judge to dismiss the case on that basis. Discovery on the merits of the claims asserted against you will be stayed pending the court’s ruling on this threshold issue. If there is an issue of fact as to whether the inmate exhausted the grievance procedure, the judge may hold an evidentiary hearing where testimony and evidence will be elicited and submitted on the grievance issue. If there are no issues of fact, the case may be dismissed based upon written submissions filed by the parties. If the judge concludes that the inmate exhausted the grievance procedure or that the grievance procedure was not available to the inmate, i.e., because the inmate did not received a sufficient response to the grievance, the parties will proceed to full-blown litigation which addresses the merits of the claims asserted against you. More often than not, litigation will go on for years and will require you to assist in responding to written discovery, execute an affidavit, sit for a deposition and/or testify at trial. Time spent properly responding to a grievance is well worth your time when viewed in this light.\nB. The Courts are Inclined to Find that a Pro Se Inmate has Complied or that Exhaustion is Unnecessary Under Certain Circumstances\nThe Seventh, Third, and Eighth Circuits have held that administrative remedies are not available, and exhaustion is therefore not required, where prison officials refuse to give a prisoner the forms necessary to file an administrative grievance. See Dale v. Lappin, 376 F.3d 652 (7th Cir. 2004); Mitchell v. Horn, 318 F.3d 523 (3d Cir. 2003); Miller v. Norris, 247 F.3d 736 (8th Cir. 2001). Similarly, the Seventh Circuit has held that where prison officials invite noncompliance with a procedure the prisoner is not required to follow it. See Swisher v. Porter Co. Sheriff’s Dept., 769 F.3d 553 (7th Cir. 2014) (prison officials told inmate not to file a grievance because problem would be resolved without the need for formal grievance procedure). The Seventh Circuit has also held that prison officials' failure to respond to a properly filed grievance makes remedies unavailable and therefore excuses a failure to exhaust. See Dole v. Chandler, 438 F.3d 804 (7th Cir. 2006). The Third Circuit has held that exhaustion was excused where guards erroneously informed an inmate that he had to wait until an investigation was complete before filing a grievance. See Brown v. Croak, 312 F.3d 109 (3d Cir. 2002). The Ninth Circuit has excused exhaustion where the prisoner was prevented from doing so by a prison official’s mistake. Nunez v. Duncan, 591 F.3d 1217 (9th Cir. 2010). And several circuits have held that prison officials' threats of retaliation can render administrative remedies effectively unavailable such that a prisoner need not exhaust them. See Turner v. Burnside, 541 F.3d 1077 (11th Cir. 2008); Macias v. Zenk, 495 F.3d 37 (2d Cir. 2007); Kaba v. Stepp, 458 F.3d 678 (7th Cir. 2006). Finally, courts have found a prison grievance process to be unavailable because of an inmate’s physical infirmity. See Days v. Johnson, 322 F.3d 863 (5th Cir. 2003) (broken hand); Hurst v. Hantke, 634 F.3d 409 (7th Cir. 2011) (prisoner had a stroke leaving him totally incapacitated); Johnson-Elster v. Elyea, 2009 U.S. Dist. LEXIS 18049 (N.D. Ill. 2009) (prisoner had advanced multiple sclerosis and moved in and out of lucidity).\nC. Correctional Healthcare Staff Should use the Grievance Process as Another Way of Improving Healthcare and Communication with Inmate/Patients\nCorrectional facilities may or may not have formal grievance procedures. To the extent they exist, there should be a written grievance procedure advising the incarcerated person the exact steps that must be followed in order to complete the grievance process. The responsibility for answering a grievance may vary by institution. Jails may not have a designated person who answers all grievances. Larger institutions may give this task to designated persons regardless of the issues. It is likely in your best interest to be aware of that procedure when approached to assist with responding to any grievance or contributing to any step in the grievance process.\nWhen faced with participating in the grievance process at your facility, consider the following:\n1. Is this grievance actually related to the provision of medical services?\nInmate grievances may contain any number of issues when drafted. Typically, a facility will have a person designated to respond to grievances. In such cases, the official grievance examiner will conduct some fact investigation that may require information from medical staff in order to properly respond. If the issue is not medically related, there is likely no need to become involved. Your response or input may be limited to stating that the matter is not medical but relates to a security issue for example.\n2. What should or can my role be as relates to answering grievances?\nIf you are asked to give medical information to the grievance examiner, ask to review the grievance personally. The grievance examiner may simply ask you, “How is Mr. Jones doing?” You might read an assessment from the physician which states the patient is stable and respond with “stable.” However, if you knew that the grievance was actually related to an issue concerning medication, or an issue relating to whether or not the patient should be weight bearing at the time, you would tailor your response to address the issue raised in the grievance rather than providing information that may make the matter worse by providing a vague response that may be incorrectly interpreted by the person who actually writes the response. If you are not sure about how your comments or input are being conveyed to the grievant, keep a personal record or better yet document your response in the patient’s chart as to what was asked and your response, for reference in the future.\nConsider contacting a supervisor if you are asked to prepare a written report of a patient’s care and treatment if it is to be sent for reasons not related to treating a patient. Statements regarding medical treatment for a patient may or may not be shared with others under the law (HIPAA). If information is needed to answer a grievance, you will want to know the nature of the complaint so it can be addressed specifically. Are you the person being complained about in the grievance? This is important because, if you may be named as a party in a case, any written or oral statements by you could be used against you as evidence. The manner in which your response is documented may be very important later. Sometimes the grievance staff may be asking for your input when you are already represented by counsel in a pending matter. In such cases, you should relay that communication to your attorney before answering.\nIn smaller facilities, medical staff may be asked to directly respond to a grievance in writing. Again you want to address the specific issue referenced by the grievance within the guidelines of the policy in place at your facility.\nBe familiar with the terms of the policies at your facility.\nIf the policy requires a response from you in 30 days, there may be no room for delay.\nIf the grievance suggests that a person did not receive a certain medication, be sure to address the issue raised after confirming the facts.\nWas there an order for that medication in the chart?\nDoes the patient’s chart reflect whether the medication was given or not?\nDoes the chart reflect the medication was refused?\nWhat are the circumstances when medical staff will write “refused”?\nFind out what the circumstances were in this instance.\n3. What else can or should I do about the complaints made in grievances regarding medical care and treatment?\nIf the grievance you just reviewed makes reference to an ongoing issue, be sure to take steps to document the complaint and address the issue in the patient’s medical records if necessary. If you determine that a medication error did occur and would continue to occur without a corrected order, be sure to note that you are now aware of the issue, and that you have taken steps to address it. It is not enough to just respond to the grievance. You will want to follow through to correct any issue that is discovered through the grievance process.\nAt the same time, if you determine that the patient’s complaint is not founded, you will want to document that as well to show that you have reviewed the record, discussed the issue with the physician, and resolved the issue using your best professional judgment. For example, if a patient believed that he was not getting his medication and the medication administration record (MAR) also showed the medicine was not being given, you might want to document the reason for withholding the medication. Maybe the patient/inmate was on a hunger strike during the time at issue and the medication is one that must be taken with food or on a full stomach; maybe the patient refused the medication (list the witnesses and/or circumstances for the refusal); or maybe the patient is scheduled for an upcoming procedure such that the medication was discontinued for a short period of time. This information is helpful as documentation to remind yourself as to the circumstances of any modification in a patient’s care and treatment. Correctional healthcare poses many challenges for the practitioner. Patients/inmates are often transferred from one facility to another without any notice to the medical staff at either facility. Documentation of complaints in the patient’s medical records may prove vital to addressing pressing concerns and changes in a patient’s care.\nAddressing medical grievances is a process that requires coordination between medical and correctional staff in most instances. Work together to address the issues raised in a way that provides appropriate resolution for the patient/inmate, and documents your efforts and level of involvement in addressing the situation. The goal should be to not only address the issues raised in the grievance for the inmate, but also to improve the provision of medical services and communication to avoid such grievances and complaints in the future.","Healthcare facilities gather and manage volumes of critical patient information that, if lost or stolen, could result in patient identity theft and delayed care. In 1996, the Health Insurance Portability and Accountability Act, or HIPAA, prompted lawmakers to build a set of privacy laws governing the management and security of patient information.\nUsing this HIPAA security rule checklist, you can see how these standards apply to your organization and take steps to obtain compliance.\nWhat is the HIPAA Privacy Rule?\nThe Department of Health and Human Services issued a set of orders that standardized privacy law for all individuals and organizations that would manage patient health data. These accountable organizations are known as covered entities and are liable for all mandates expressed in the Standards for Privacy of Individually Identifiable Health Information, also known as the HIPAA Privacy Rule.\n“A major goal of the Privacy Rule is to assure that individuals’ health information is\nproperly protected while allowing the flow of health information needed to provide\nand promote high quality health care and to protect the public’s health and well being.” – United States Department of Health and Human Services\nThese privacy standards arrived as medical professionals started to digitize medical records. Taking advantage of digital documentation allows all healthcare-related organizations to better serve patients, since managing digital records is far more efficient than managing hard copies of medical records.\nTo Whom Does the HIPAA Privacy Rule Apply?\nThe HIPAA Privacy Rule applies to what are referred to as covered entities. These agencies assist in the administration of healthcare services, to include treatments, insurance payments, and more.\nWhat are Covered Entities?\nA covered entity includes private medical practices, hospitals, and any auxiliary organization that must access protected health information to operate. Often, there are several healthcare-related agencies working together to assist a patient in receiving the medical care that they require.\nThe Privacy Rule identifies a covered entity as one of the following:\n- Health Plans\n- Insurance providers\n- Medicare/Medicaid insurers or supplemental insurers\n- Employer-sponsored plans\n- Government-sponsored plans\n- Church-sponsored plans\n- Coop plans\n- Healthcare Providers\n- Healthcare Clearinghouses\nThe Privacy Rule also applies to non-covered entities that serve as third-party vendors or business associates to a covered entity.\nWhat is Protected Health Information (PHI)?\nProtected Health Information, or PHI, is the formal term for “individually identifiable health information.” Covered entities manage PHI in accordance with their duties and are under scrutiny to protect patient identities and privacy by abiding by all HIPAA compliance standards pertaining to lawful use of PHI.\nDownload Our HIPAA Compliance Checklist\nWhat are HIPAA Authorizations?\nIn the event that a covered entity needs to share PHI with another individual or agency, but that individual or agency is not otherwise permitted access to a patient’s PHI under HIPAA Privacy law, covered entities may seek a patient authorization.\nHIPAA authorizations must be signed by the patient and lay out clearly who the authorization is for, the purpose of the authorization, and when the authorization expires. The covered entity should also define any contingencies or parameters laid out by the patient to meet the authorization’s purpose.\nAn example of a HIPAA authorization could be a mental health patient that agrees to share his/her therapy notes in a full psychological evaluation. This is a common scenario for veterans providing medical evidence during a PTSD disability claim. Even though filing a disability claim involves due process and legal discovery, investigators may not access those medical records without a signed patient authorization.\nHIPAA Protected Health Information Uses and Disclosures\nWhat is a Notice of Privacy Practices (NPP)?\nAll covered entities must disclose a notice of privacy practices, or NPP, that outlines the patient’s rights according to HIPAA privacy law and PHI. The NPP should also explain how a patient may file a complaint against a covered entity that they feel violated their rights under HIPAA privacy law.\nNPPs are items commonly found in registration paperwork when a patient sees a medical professional for the first time. The documentation explains how a covered entity may use the patient’s PHI within the bounds of HIPAA compliance.\nYour HIPAA Security Checklist:\nA HIPAA security checklist can help you identify where your business operations fail to meet HIPAA privacy requirements. You can use the checklist below to perform an internal audit. Or you can use the checklist as a way to gauge how seriously your organization takes HIPAA compliance.\nPatient Access and Consent\n- Have you established a process to help patients access their PHI? In this day and age, many covered entities make sure that patients can access their PHI safely online, even if another covered entity maintains the PHI database. Regardless, your organization should have clear policies and procedures to help patients view their PHI.\n- Do you have a process for accepting and fulfilling PHI copy requests from patients? When a patient requests copies of their PHI, HIPAA compliance dictates that you give the patient a copy in the requested format (hard copy or digital) within 30 days of their request.\n- If your firm decides to charge patients a fee for copies of their PHI, do you make those fees accessible? HIPAA compliance requires covered entities to fulfill PHI copy requests to patients at a reasonable cost. Prohibitive costs do not properly reflect the amount of labor and expenses required to fulfill PHI requests. Agencies that charge too much may be doing so intentionally to keep from having to be HIPAA compliant.\n- Are your authorizations specific, to include uses, recipients, disclosures, and expiration dates? Vague HIPAA authorizations do not protect your organization or the patient. All critical details of the authorization should be clearly spelled out according to the patient’s expectations.\n- Do your authorizations use “plain English,” as opposed to medical jargon and elusive clinical terms that the patient will not understand? If it appears that the patient had no clue of what they were signing because of convoluted words and phrases, your authorization could be in violation of HIPAA privacy law. It’s critical that your authorizations use language that is understandable to the average patient.\n- Do you secure the patient’s signature and date on every authorization? HIPAA authorizations are invalid unless they have the patient’s signature, as well as the date on which it was signed.\n- Do you store your HIPAA authorizations in a secure location and properly dispose of them once they are no longer needed? Losing a HIPAA authorization could open your organization up to legal action from the patient. It’s critical that you properly store and share authorizations in accordance with HIPAA privacy law.\nNotice of Privacy Practices (NPP)\n- Do you have an NPP included in your new patient paperwork? To be HIPAA compliant, you should onboard every new patient or client with an NPP so that those individuals understand their PHI rights from the start of your services.\n- Do you have your patients or clients confirm that you informed them of their rights according to HIPAA privacy law? Having your patients or clients confirm in writing and with a signature that they have read and understood your NPP protects you as a covered entity.\n- Do you prominently display your NPP on the premises and/or clearly on your website? Demonstrating that you publicize your NPP for all to see further protects you against patients claiming that you did not inform them of their rights under HIPAA privacy law.\n- Do you have policies and procedures in place to manage patients with concerns that you’re not complying with your NPP? It is possible that some patients may accuse you of not advising them of their rights per HIPAA privacy law. More importantly, a patient or client may fall through the cracks. Either way, you should have a clear process on how to manage those complaints and rectify them immediately.\n- Do your day-to-day operations align with your NPP and HIPAA Privacy Law? You should perform routine audits of your business operations to ensure that you’re not merely paying lip service to HIPAA privacy law.\nEmployees and Business Associates\n- Do all of your staff members understand HIPAA privacy law, as well as workplace policies and procedures relevant to PHI? Much of your HIPAA compliance pertains to consistent adherence by your staff. As a covered entity, it is your responsibility to ensure that every employee understands HIPAA privacy law and how they must manage PHI in their current role.\n- Have you trained your employees and collected proof (such as signed documentation) that they received the proper HIPAA compliance training? Similar to how you have patients sign and confirm that they had read and understood your NPP, you should include attestation documentation at the conclusion of HIPAA compliance training for your staff.\n- Do you have a process in place for employees to report HIPAA non-compliance without fear of reprisal? Ideally, you should create a way for employees to report non-compliance anonymously. This approach ensures that managers and lower-level employees alike are held accountable in accordance with HIPAA privacy law.\n- Do you collect confidentiality agreements from your employees and independent contractors? Employees and independent contractors of covered entities are known as non-business associates. Since it is likely that these people will come into contact with or manage PHI as part of their job description, it’s important that you collect confidentiality agreements from each of them.\n- Do you choose your business associates carefully, to include carrying out due diligence on that organization’s privacy policies and procedures? You could be held liable if one of your vendors mismanages PHI that your business maintains. Part of being HIPAA-compliant is ensuring that you only work with vendors that also understand HIPAA privacy law.\n- Do you maintain a list of all your business associates and third-party vendors? If your organization manages PHI, it’s very likely that most or all of your business associates and third-party vendors may come into contact with that PHI. It’s critical that you maintain an up-to-date record of all external parties with whom you do business.\n- Have you established the proper contracts (business associate agreements) with your business associates and third-party vendors that contain HIPAA-compliant directives on all matters pertaining to PHI? You should disclose to your business associates that managing PHI is part of your business operations. This informs your vendors that they must maintain HIPAA compliance, especially if their services also involve the use of PHI.\n- Do you reexamine your business associate agreements every year, to include updating your list of business associates? The nature of your relationship with third-party vendors and business associates can change year-over-year as you and the other party scale your respective operations. As such, you may need to update portions of your business associate agreements to remain HIPAA-compliant.\n- Do you have an up-to-date network diagram? Network diagrams show you all possible attack vectors from which hackers and malware might enter and try to steal or destroy PHI.\n- Do you have basic cybersecurity protocols in place? Due to the sensitivity of PHI, it is critical that you maintain all the necessary firewalls, malware protection, and monitoring to keep your and your patient’s information secure.\n- Do you have a plan to respond to an incident or breach? The initial moments after a security breach are often the most critical. Having a plan in place to quarantine the incident, diagnose the root cause, patch the intrusion, and report any damage will protect PHI under your organization’s care. Also, it will help your cybersecurity team update its tools, policies, and procedures to deal with similar intrusions more efficiently.\n- Has your staff received training on phishing attacks and how to prevent them? Sometimes the biggest threat to your organization is an employee clicking on an unknown link and releasing malware onto the company network. Making sure that your employees know how to safely deal with phishing attacks can drastically reduce your cyber risk.\nKey Takeaways: HIPAA Security Rule Compliance Checklist\nUsing the checklist above, you can take initial steps to become HIPAA-compliant in accordance with privacy laws pertaining to PHI. Failing to comply with HIPAA Privacy Law can result in financial penalties and patient lawsuits.\nRSI Security is an agency dedicated to assisting covered entities in their quest to acquire and maintain HIPAA security compliance. Our team of cybersecurity specialists can help you create a personalized HIPAA security rule compliance checklist and establish the necessary safeguards to protect your PHI against negligence or abuse.\nDownload Our Complete Guide to Navigating Healthcare Compliance Whitepaper\nNot sure if your HIPAA or healthcare compliance efforts are up to snuff? Unsure about where to even start? Download RSI Security’s comprehensive guide to navigating the HIPAA and healthcare compliance labyrinth. Upon filling out this brief form you will receive the whitepaper via email."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:b7bff0e5-c0d1-44e8-b0d1-46b037cf2cfb>","<urn:uuid:e152642d-e4fa-4a0b-9ec7-584621db2de8>"],"error":null}
{"question":"What fundamental differences exist between Band and Bandwidth concepts in vibration analysis versus narrow bandpass filter design?","answer":"In vibration analysis, a Band is simply defined as a range of frequencies (e.g., 0 Hz to 5 Hz is a 5 Hz wide band). In contrast, in narrow bandpass filter design, bandwidth (BW) is more specifically related to the filter's selectivity and is mathematically linked to the quality factor Q (where Q>10) and center frequency fc. The narrow bandpass implementation also includes specific design calculations involving multiple feedback paths and component relationships through formulas involving Q, center frequency, and gain parameters.","context":["Average – In vibration analysis, an average usually refers to the process whereby the vibration software will, after converting waveforms into spectrums via FFT, add the resultant spectrums together and then divide by the number of spectrums added together. The result is an “averaged spectrum”.\nBand – A band is simply a range of frequencies e.g. A band from 0 Hz, to and including 5 Hz, is a band that is 5 Hz wide.\nBand Pass Filter – A filter that blocks all data above and below its defined band.\nCondition Monitoring – The use of specialized equipment to deduce the actual condition of equipment as pertains to its fitness for continued use. Condition monitoring is the foundation of Predictive Maintenance and the two terms are sometimes used interchangeably.\nCycles per minute (CPM) – In vibration analysis, cycles per minute refers to the number of vibratory cycles that occur in one minute of time. Cycles per minute is a quantity of frequency.\nCycles per second (CPS) – In vibration analysis, cycles per second refers to the number of vibratory cycles that occur in one second of time. Cycles per second is a quantity of frequency.\nEnveloping – Enveloping, also known as envelope de-modulation, is a data processing technique whereby a spectrum is created from a demodulated or filtered waveform.\nFFT – Fast Fourier Transform (FFT) is a mathematical process that transforms a waveform into the components of its frequency spectrum.\nFmax – Fmax stands for “maximum frequency”. It is the high frequency boundary for a set of data.\nFmin – Fmin stands for “minimum frequency”. It is the low frequency boundary for a set of data.\nFrequency Markers – These are visible marks that can be overlaid on a spectrum at specific frequencies to help identify likely machinery problems.\nHertz (Hz) – The same as “cycles per second”.\nHigh Pass Filter (HP) – This filter blocks all data below it and only allows the data that is higher to “pass” and be recorded. It determines the “low frequency cutoff” or Fmin.\nLOR – LOR stands for “Lines of Resolution”. It is the number of digital bins of amplitude information a spectrum will be constructed from.\nLow Pass Filter (LP) – This filter blocks all data above it and only allows the data that is lower to “pass” and be recorded. It determines the “high frequency cutoff” or Fmax.\nNegative Averaging – A procedure whereby, having previously taken a reading, a second reading is taken and all data in the first reading that matches data in the second reading is subtracted.\n% Overlap – % overlap is the amount several otherwise sequential waveforms, being sampled and averaged into a spectrum, will “overlap one upon the other” as they are being sampled.\nPeak-to-Peak – The measure of vibration amplitude in a waveform, from the negative peak to the positive peak.\nUnbalance – A measure that quantifies how much the rotor mass centerline is displaced from the centerline of rotation.\nWaveform – In vibration analysis, a waveform is a display of vibratory energy over time.","Band Pass Filter\nA band-pass filter is a circuit which is designed to pass signals only in a certain band of frequencies while attenuating all signals outside this band. The parameters of importance in a bandpass filter are the high and low cut-off frequencies (fH and fl), the bandwidth (BW), the centre frequency fc, centre-frequency gain, and the selectivity or Q.\nThere are basically two types of bandpass filters viz wide bandpass and narrow bandpass filters. Unfortunately, there is no set dividing line between the two. However, a bandpass filter is defined as a wide bandpass if its figure of merit or quality factor Q is less than 10 while the bandpass filters with Q > 10 are called the narrow bandpass filters. Thus Q is a measure of selectivity, meaning the higher the value of Q the more selective is the filter, or the narrower is the bandwidth (BW). The relationship between Q, 3-db bandwidth, and the centre frequency fc is given by an equation\nFor a wide bandpass filter the centre frequency can be defined as where fH and fL are respectively the high and low cut-off frequencies in Hz.In a narrow bandpass filter, the output voltage peaks at the centre frequency fc.\nWide Bandpass Filter\nA wide bandpass filter can be formed by simply cascading high-pass and low-pass sections and is generally the choice for simplicity of design and performance though such a circuit can be realized by a number of possible circuits. To form a ± 20 db/ decade bandpass filter, a first-order high-pass and a first-order low-pass sections are cascaded; for a ± 40 db/decade bandpass filter, second-order high- pass filter and a second-order low-pass filter are connected in series, and so on. It means that, the order of the bandpass filter is governed by the order of the high-pass and low-pass filters it consists of.\nA ± 20 db/decade wide bandpass filter composed of a first-order high-pass filter and a first-order low-pass filter, is illustrated in fig. (a). Its frequency response is illustrated in fig. (b).\nNarrow Bandpass Filter.\nA narrow bandpass filter employing multiple feedback is depicted in figure. This filter employs only one op-amp, as shown in the figure. In comparison to all the filters discussed so far, this filter has some unique features that are given below.\n1. It has two feedback paths, and this is the reason that it is called a multiple-feedback filter.\n2. The op-amp is used in the inverting mode.\nThe frequency response of a narrow bandpass filter is shown in fig(b).\nGenerally, the narrow bandpass filter is designed for specific values of centre frequency fc and Q or fc and BW. The circuit components are determined from the following relationships. For simplification of design calculations each of C1 and C2 may be taken equal to C.\nR1 = Q/2∏ fc CAf\nR2 =Q/2∏ fc C(2Q2-Af)\nand R3 = Q / ∏ fc C\nwhere Af, is the gain at centre frequency and is given as\nAf = R3 / 2R1\nThe gain Af however must satisfy the condition Af < 2 Q2.\nThe centre frequency fc of the multiple feedback filter can be changed to a new frequency fc‘ without changing, the gain or bandwidth. This is achieved simply by changing R2 to R’2 so that\nR’2 = R2 [fc/f’c]2"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:dac12ee0-8b6b-4e5f-85a2-10a5cfa50c6a>","<urn:uuid:fc5ca938-4319-496a-922f-9c599560beef>"],"error":null}
{"question":"I collect comic graded by CGC. What is CGC's standard grading process and how does it affect comics like Devils & Deaths by Edvin Biukovic?","answer":"CGC employs a detailed 6-step grading process. First, they check for large problems like stains or holes. Second, they identify variants and pedigree status. Third, they count pages to ensure completeness. Fourth, they assess page condition, looking for tears, stains, creases, and issues like foxing. Fifth, they check for restorations. Finally, a team of graders examines the item to reach consensus on a final grade from 0.5 to 10. For a work like Devils & Deaths, this grading would affect its market value and preservation, as graded comics are encapsulated in protective holders and often command higher prices than ungraded versions. However, this encapsulation means the comic cannot be read, which is particularly significant for artistic works like Devils & Deaths, which features Biukovic's intimate character portrayals and masterful page staging that made it one of comics' great works.","context":["A Great Lost Artist And His Unsung Masterpiece: Edvin Biukovic And ‘Devils & Deaths’\nComic artist Edvin Biukovic died fifteen years ago this month at just 30 years old. His death was obviously a terrible loss to those who knew and loved him. It was also a terrible loss to the comic industry; Biukovic never received the level of lasting acclaim or recognition that his talent deserved, and produced relatively few works. Yet he was one of the finest comic artists of his generation.\nBiukovic published several works in his native Croatia that have sadly never been translated. His finished English-language works include a couple of Star Wars stories published at Dark Horse, and the first of Peter Milligan's Human Target stories for Vertigo. One work stands as his masterpiece; Devils And Deaths, written by his long-time friend and collaborator Darko Macan, and published by Dark Horse, is a science fiction story about a country torn apart by ancient grudges and tribal conflicts, and of the desperate people trying to eke out a purpose in the midst of war.\nThis choice of subject matter for Devils & Deaths was not an abstract meditation on the horrors of war. Buikovic and Macan were born in Croatia when it was still a Socialist Republic. The Croatian War of Independence was ongoing at the point that these stories were conceived. In her introduction to Devils And Deaths, co-editor Diana Schutz recalls Macan telling her quite casually that the war was sixty miles away when they spoke on the phone.\nThe collected Devils And Deaths is actually two stories -- a two-parter, 'Devils And Deaths', about a dying soldier trying to do one last honorable thing; and a four-parter, 'Devil's Choices', about his brother trying to be a good soldier in a bad war. Other stories weave around them, most notably the rise and fall of a warlord, and a young family forced to choose between their own peace and being a small part of something greater. It's smart, raw, and beautifully observed storytelling.\nBiukovic excelled at finding the intimacy in the story; in drawing the reader in to discover the passion and intensity of his characters. His pages breathed because the characters lived from moment to moment and their expressions and their bodies shift to show frustration, rage, doubt, mania.\nBiukovic also understood how to stage his pages. He could draw the reader in to a kiss by a tree, hold us steady in a moment of reverence by the ocean, or hurl us into the action of a battlefield, all with equal assurance. The way Biukovic angled his action scenes allowed him to be relatively sparing with motion lines; the characters and the environment sold his action, just as they sold his emotion. One character's execution by firing squad is presented as powerfully as such a scene could be; the next page offers four panels that wordlessly show how the executed party immediately becomes a martyr.\nDevils And Deaths is a truly extraordinary work, and one of the great science fiction testimonies of life in the late 20th century. Schutz, again in her introduction, notes that the first story came to Dark Horse as an unsolicited submission. Very few publishers will now look at such submissions, because few great works ever emerge from the slush pile. Few, but not none. Devils And Deaths is proof of that.\nYet the book is not as celebrated as it deserves to be. Its full title is Grendel Tales: Devils And Deaths, and that title may have limited the book's reach. I don't mean that as any slight to the Grendel series or to its creator Matt Wagner, but anyone unfamiliar with Grendel may think the book isn't for them, and anyone who loves Grendel may not have given time to Grendel stories by authors other than Wagner.\nYet one does not need to know Grendel to understand Devils And Deaths; I had never read another Grendel story before I read it. The world is informed by Wagner's creation, but Devils And Deaths establishes its own story and is complete in itself. If the work were better known and better read, it would be in the pantheon of comics' great works; and Biukovic would be remembered as one of the medium's great lost talents.\nOne other Biukovic comic deserves mention; 'A Prayer To The Sun' is a short story also written by Macan, and published after Biukovic's death in the Vertigo anthology Weird War Tales. It serves the same themes as Devils And Deaths; it's a heartbreaking contemplation of the tragedies of war and the futility of hope, covering centuries of conflict in eight pages.\nBiukovic was taken from us too soon, and his work deserves to have a lasting legacy. We will never see the comics he could have made; he still has the power to inspire the storytellers that follow him.","A cornerstone of comic book collecting is the grade that a book is assigned by the CGC, or Certified Guaranty Company. Despite the CGC’s importance to modern collectors, many still don’t understand what it means for a comic book to be graded. By understanding CGC’s process, aspiring collectors will better understand why this grading process is so important.\nBut First, What Is the CGC?\nBased in Sarasota, Florida, the CGC was founded in 2000 and quickly became a global standard for determining a comic book’s physical condition. Prior to CGC, the standard was the Overstreet Guide’s 10 Point Grading Scale, which ranged from a 0.5 (Poor) to 10 (Gem Mint). CGC built upon Overstreet’s grading by developing a more standardized process, and incorporated new elements.\nHow Do You Know if Your Comic Is Worth Sending In for Grading?\nThough you may be tempted to submit every comic book you purchase for CGC Grading, this process must be paid for. The grading fee varies depending on the age and value of the comic, ranging from $22 to $120 for most books. Comics with a fair market value above $3,000 however, are charged on a sliding scale of 3% of the book’s value (minimum $150).\nGiven these costs as well as the costs of shipping, you should only seriously consider submitting a comic book for grading if you already know it’s valuable or if you believe that it will become valuable in the future. A general rule of thumb that many collectors follow is that a book’s expected value needs to be worth about $200 before it makes sense to have the book graded from a financial perspective.\nAlso note that the grading process often takes some time. CGC states that grading can take 106 to 133 days. A fee of $75 can reduce this time to 44 days, and an express fee of $120 can reduce this time to 17 days. Books over $3,000 in value are turned around in an expected 10 days.\nWhat Are the Benefits of Sending Your Comic In for Grading?\nA graded comic book often has significantly more value than a non-graded version, since a graded comic book has been authenticated by an industry respected organization. Additionally, collectors are much more willing to pay top dollar for high conditioned books, if that condition has been verified and quantified by CGC. Finally, with a CGC certification number, a graded comic has a provenance—meaning that this book’s ownership history will now be documented for future owners to appreciate.\nAnother benefit of grading is that your comic will be encapsulated in a crystal clear holder. In addition to the item being better preserved by this shell, the holder helps to guard against counterfeiting and tampering.\nBeyond preservation, the CGC slabs themselves have a positive aesthetic quality that fans and collectors have come to appreciate. These containers make it easier to handle the collectibles and to display them. Instead of keeping a comic book hidden in a box, they can be showcased like the works of art they are. One drawback is that encapsulated books can’t be read.\nWhat Are the CGC Labels?\nEach comic book receives a numerical grade of 0.5 to 10 as well as a designation related to the page quality of the comic. It’s worth noting that for many books (save for the most modern) the highest grade in existence is often a 9.8, with 9.9s and 10s for older books exceedingly rare. In addition to the number grade and page quality designation, each submission will have a color-coded label. There are 12 colors, and out of these, there are 5 major colors: Universal Label (Blue), CGC Signature Series Label (Yellow), Qualified Label (Green), Restored Label (Purple), and Pedigree Label (Gold).\nThe universal blue label is given to comic books that are graded as marked with no qualifiers. It has no autographs, no evidence of restoration, nor anything massively missing.\nThe yellow signature series is for a submission that has a significant person’s autograph, which has been authenticated. While one might assume that signed books are more valuable (and while this is often the case), in certain cases, especially with older comics, unsigned books can sometimes command a higher price.\nThe qualified green label is for any collectible that has a significant defect. The green label communicates that something is substantially wrong with the book despite being graded. This could be a missing page or an un-authenticated signature or missing staples. According to Midtown Comics, the green label allows a damaged book to be graded on a curve; “For example, a book missing staples may receive a blue label grade of 0.5 (POOR grade) but would receive a grade of 6.8 or 7 through the qualified green label.”\nThe purple restored label is for any comic book that has been repaired. In general, restoration significantly harms the value of a comic and some have dubbed this CGC designation as PLOD or “the purple label of death.”\nThe pedigree gold label is applied to any item that is part of a collection CGC recognizes as being of exceptional quality. A pedigree label not only verifies the quality of a book, but highlights the quality of the collection it comes from. As GoCollect.com writes, “a pedigree is an exceptional collection that they are willing to recognize with unique labeling.”\nTo date, there are only 61 pedigree gold collections recognized by CGC. Not only do their pedigree statuses signify quality, this label also increases the value of a book. A pedigree will increase a comic book’s value as a collectible because it adds to the story the item. After all, it is not some random book found in an attic. Instead, it was part of a collection that was preserved for future generations.\nAs ComicBookPedigree.com explains, when it comes to pedigree collections, “all of the owners had one thing in common; they were conscientious about the condition of their comic books. By luck or design, each of the collections featured here survived many tests of time; paper drives, puberty, and especially Mom, who never understood why her child would save such things.”\nHow Are Comics Graded by the CGC?\nWhile one might think that this grading process would be a company secret, CGC is completely transparent about how a comic book’s quality is determined.\n(1) First impressions\nA first impression is typically reserved for finding large problems; such as stains or holes on the cover as well as missing pieces and creases.\n(2) Identify variants or pedigree\nA submission’s pedigree is determined by the collection it comes from. While different organizations have various criteria for pedigree, there are three near-universal standards:\nA)The quality of an overall collection — does it already have high value items?\nB)The collection’s origin — were they collected by the same person/people and stored in a similar space at the same time? This is of value because it signifies that all the items will be in a similar condition; and\nC)The completeness of the collection — is it just a collection of random books or does it contain completed series?\nIn short, two of the same comic books in similar conditions can be worth different amounts if one comes from a trusted and vetted collection.\n(3) Count pages\nSelf explanatory — each page of a book is counted to make sure a submission isn’t missing any.\n(4) Determine page condition\nWhen it comes to specific condition, graders will look for tears, stains, and creases.\nOne major issue in page quality is “foxing.” Foxing is typically a degradation of paper that comes from within the paper as it ages. Foxing is typically found in books that have not been properly protected from the environment.\nAnother source of damage can come from the book’s staples. Staples rust if exposed to moist air and that rust can expand into paper. Staples might also cause tears.\nIn addition to keeping comic books away from moisture and humidity, it’s also important to keep them away from direct sunlight. Sunlight can bleach an image and tone down the vibrancy of a cover’s colors.\nFinally, with so many comic books including posters or other features that encourage readers to cut them out, graders will determine if these extra materials are still present.\nExamples of such materials include advertisements encouraging readers to cut out stamps and coupons to be sent in, or posters to be taken out and put on display. One of the many reasons X-Men #1 sold so many copies was due to variant covers, one of these variants being a cover that functioned as a pull out poster. Similar to Playboy, dozens of different titles were published with centerfold posters in a comic book that were designed to be removed from the book and hung up on walls—doing so would unfortunately significantly harm the value of the specific issue.\nTo make sure that pages or covers weren’t swapped out, graders will also examine a book’s staples to make sure they weren’t opened and closed again.\nA surprising source of a comic book’s imperfections is the poly bag and board that it might be sealed in. For instance, a back cover might stick to the board it is sealed with. This means that a back cover could be damaged due to ink transferring to the board or the page itself becoming damaged from parts of it becoming permanently stuck to the board. Poly bags for comic books are incredibly common because they are affordable. However, they offer less protection than mylar bags. And while mylar bags are more expensive, they offer superior protection from moisture and humidity, insects, mold, and other chemicals.\n(5) Check for restorations\nTo salvage/improve the appearance of a comic book, some collectors will try to restore it. CGC currently identifies 11 types of restorations that can be done on comic books: color touch, piece fill, tear seals, spine split seals, reinforcement, piece reattachment, cleaning, staple replacement or cleaning, re-glossing, glue, and trimming.\nBecause of the complexities surrounding restorations, CGC has a seperate scale to grade restored books. And while it is tempting to want someone to restore books to mint condition in the hopes that their new value will be dramatically higher, restorers can often do more damage than good to a book’s value depending on the specific situation .\nTwo restoration details to keep in mind are the slight differences between restoration and conservation. For instance, CGC defines restoration as “the act of adding foreign material to a comic book through certain techniques to return its appearance to an ideal or original state.” In contrast, “the goal of conservation is to preserve the structural integrity of the comic while removing all things that are detrimental to its longevity.”\nOnce a grader has documented and considered all of a book’s imperfections, they will determine a grade. Similar defects do not always have a similar impact on a book’s final grade. For instance, a crease being severe enough that it breaks the color will have a greater negative impact than a crease that doesn’t break a page’s colors.\n(6) Assess all defects and determine a final grade.\nTo make sure the quality of a book is properly assessed, grading is done by a team and multiple CGC professionals examine every item as a means to guarantee consistency and accuracy. This process of multiple graders concludes once a consensus for a final grade is reached.\nHow Are Comic Books Authenticated by the CGC?\nBefore the intensive work of grading is done, the book is authenticated. With some comic books valued so highly, there are many attempts to pass off fakes as the real things. Stephen Fishler, the CEO and cofounder of Metropolis Collectibles and ComicConnect.com, discusses some of these attempts in Fraud Magazine:\n“We’ve seen two notable attempts to counterfeit modern, or Bronze Age, comic books: Cerebus No. 1 (1977) and Teenage Mutant Ninja Turtles No. 1 (1984). Why did counterfeiters pick them? Both were printed with unusual black-and-white interiors, which the fraudsters assumed would be easier to replicate. After all, 99% of the comics produced in the last 80 years feature full-color interiors,” Fishler said.\n“Regardless, industry professionals quickly spotted the counterfeits,” Fishler continued, “and no one ever profited from them. Case closed.”\nAlternatives to CGC: CBCS and PGX\nWhile CGC is undoubtedly the gold standard in the comic grading industry, there are other players in the space, most notably CBCS which is owned by Beckett Media. CBCS is generally considered a reputable player in the space and some collectors choose to use the company because of its slightly lower fees and often much faster turnaround times. The grading scales used by CBCS are nearly identical to those of CGC; however, it’s worth noting that CGC comics of the same grade generally fetch a slight premium to those graded by CBCS on the resale market.\nPGX is another comic grading company out there; however, many collectors claim that their grading quality is suspect and stay away from either having their books graded by them or buying PGX graded books. For this reason, until something changes reputation wise, you might want to steer clear of PGX grading as an option.\nWith the comic book collector’s market increasing every year, the importance of grading will only grow. And this importance is not a hyperbolic statement. Grading has become so common that CGC and similar companies have needed to hire more people. As Jim McLauchlin wrote for GamesRadar.com, “Collectibles markets are surging, and as old comics get more expensive, collectors and dealers turn to third-party services to ensure authenticity and grade. The phenomenon has led to an odd crimp in the labor market – grading services and high-end auction houses are desperate to hire graders, even paying cash bonuses to get people in the door.”\nAs it becomes more common for comic books to get graded, it is important for collectors, fans, and others interested in comic books to understand what it means to purchase a CGC (or CBCS) graded collectible."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:bd8d2f98-0307-4e36-988d-6f09a96f0f12>","<urn:uuid:67b3d9cd-b743-4124-b590-c2b2ff2da349>"],"error":null}
{"question":"How did heavy metals cause a health crisis in Minimata, Japan in 1932?","answer":"In 1932, people in Minimata, Japan experienced what became known as the Minimata Syndrome, caused by mercury contamination of marine life that the Japanese consumed. This disruption in the food chain resulted in over 500 deaths.","context":["All living things are part of the food chain… no exceptions. Although we experience this principle of dependency daily, it usually gets ignored unless something disrupts the chain and results to detrimental consequences such as food poisoning that may lead to human fatalities or illnesses. Heavy metals are a part of nature and some are even beneficial to the human body but as with most things, too much concentration can have its adverse effects. What Are Heavy Metals? Heavy metals are found in rocks and soils but these may also accumulate in water forms\nwhen these solid objects get broken down in time. The term heavy metal means any “metallic chemical element that has a relatively high density and is toxic or poisonous at low concentrations. (ILPI, 2007, par. 1) Some of the heavy metals that are being considered as threatening are arsenic, cadmium, chromium, copper, lead and mercury. These heavy metals which can also be called minerals have been present in the environment in safe amounts until industrialization has initiated mining activities and technological advances that altered their presence in the food chain.\nSome of the products that elevated these heavy metals to possible intolerable levels are leaded gasoline and silver-mercury tooth amalgam. (Haas, n. d. , par. 5) Unfortunately, heavy metals cannot be destroyed except through radiation. It is therefore necessary to control its content and the way it is distributed in man’s environment especially when the food chain is concerned. How Do Heavy Metals Seep Into The Food Chain? There are many natural ways by which heavy metals enter the food chain.\nNature can release heavy metals from boulders and other rock formations by weathering and eventual breakage or erosion. Volcanic eruptions can also release these heavy metals into the air which will eventually accumulate around the surrounding habitat of many living organisms. Acidic rains can also break down soils and release heavy metals into streams, lakes, rivers and ground water. (Lenntech, 2006, par. 4) Man also has had a hand in the spillage of heavy metals in his environment. Lead paint and water pipes have been a source of many health problems and intoxications.\nCopper, for example, can contaminate drinking water because of the copper-based pipes it passes and also from purifying agents that are intended to control the presence of microorganisms. Sewage sludge disposal unto farms is also a means that was originally a good way of garbage disposal but it can also be a path for certain nutrients to seep through the soil in larger quantities that may not be so beneficial after all. In Ohio, for example, sludges are considered rich in nitrogen and phosphorus which are significant sources of nutrients to farms in the vicinity of the sewage treatment plants.\n(The Ohio State University, n. d. , par. 1) However, it has been found that it also contains unnecessary amounts of cadmium, copper, nickel, zinc and lead that can deter the healthy growth of botanical organisms and contaminate the food chain. Cadmium is the one that is easily absorbed by plants and copper is toxic to livestock. These heavy metals can also contaminate the soil when these are applied as fertilizers and pesticides. Automobile exhausts, although not directly affecting the food chain, can also cause illnesses because of its lead content.\nBurning fossil fuel like coal, garbage and even tobacco can also release heavy metals like cadmium into the environment. (Contaminants Division, n. d. , par. 5) Man and nature can go hand in hand in releasing heavy metals in the environment. A factory in China, for example, can emit these toxic minerals in the air and the wind can carry it over to bodies of land and water in Russia. Because of this fact, it is harder to study the sources of heavy metal contamination in different locations. Bioaccumulation and Biomagnifications\nThere are two terms that are being used to show the problems that we undergo due to heavy metal contamination. These are biomagnifications and bioaccumulation. Biomagnification is the addition of intoxicating quantities of minerals when animals eat prey that could have been contaminated by heavy metals. Bioaccumulation, on the other hand, is an increase in the concentration of a chemical in a biological organism over time, compare to the chemical’s concentration in the environment. (Wageningen University, 2007, par.\n2) This means that the bodies of the contaminated organisms accumulate more of these toxins than what they are able to secrete or eliminate and the build up can cause many negative consequences. Biomagnification, as implied, is a step in the food chain while bioaccumulation pertains to the heavy metal content within a living organism in relation to its habitat. Methyl mercury, for example, is a heavy metal that can contaminate rivers. Marine organisms may make the mistake of taking the heavy metal. These organisms are usually eaten by fish.\nWhen a fish eats a contaminated marine organism, it cannot excrete the methyl mercury from its body anymore. The more contaminated organisms it eats, the more the methyl mercury accumulates within its body. Little fishes that have eaten these organisms can also be eaten by bigger marine creatures. The animals that are on top of the food chain stands to receive the consequences. Should a human, catch a contaminated fish, mercury poisoning will be inevitable and probably be even fatal. It was once assumed that heavy metals on the ground can bind tightly with soil particles and thus avoid being taken in by organisms eating on the soil.\nHowever, a study of snails on cadmium-contaminated soil was made by Renaud Scheifler and his team of soil biologists at the University of Franche-Comte which showed that after two weeks, the snail’s tissues were found to have absorbed 16 percent of cadmium. (Coghlan, 2002, par. 4) In other words, it is not true that heavy metals cannot be accessed by organisms when it is bound to soil particles. Thus, heavy metals can also enter the food chain through the soil. The human body can easily eliminate toxic substances taken in low doses but it can also sometimes confuse heavy metals for other helpful minerals.\nCadmium, for example, can be mistaken for zinc and lead for calcium. Although some of these minerals’ properties are the same, cadmium and lead cannot replace the functions or benefits that zinc and calcium provide. Thus, some essential body functions may not be carried out and heavy metals may also accumulate within. Humans are equipped with a good maintenance system but if our natural means of eliminating these heavy metals from our system is hampered because of overdose and continued exposure, there is a possibility of abnormalities or even fatalities if not treated properly.\nEffects of Heavy Metals to Humans Different heavy metals have unique effects on the living organisms that intake it. In the 1950s, many people in Japan began to experience a painful disorder called “itai-itai” which they discovered was a result of cadmium exposure due to industrial wastes. Cadmium can also have other effects such as the following: damage to the kidney, susceptibility to anemia, renal dysfunction, lung diseases that can lead to lung cancer, osteoporosis and osteomalacia.\nAnimals have been studied and showed that cadmium also increases their blood pressure but it has not been proven enough to affect humans the same way. Mercury and lead can both accumulate in the kidney, liver and spleen which can affect these organs’ functions. Low-level exposure can irritate the skin and cause ulceration. (Lenntech, par. 19) In 1932, people from Minimata Japan experienced what is now known as the Minimata Syndrome which was a direct effect of mercury contamination of marine life which the Japanese loved to eat.\nOver 500 people died because of this disruption in the food chain. Copper is needed by the body but unusual doses can result to anemia, damages to the liver and kidney, plus stomach pain. Chromium can also accumulate in marine creatures and can cause considerable damages to the human kidney, liver and nerve tissues. Selenium is needed for some human body processes but overdose can damage the nerves, result to over fatigue and increased moodiness. Harder to correct health problems that may arise include loss of fingernails, damages to organ tissues and the nervous system.\nOne of the most important links in the general food chain is the plant species. However, heavy metal contamination can cause very sensitive kinds of plants to die. (Velky, n. d. par. 1) When this happens, organisms that rely on these botanical creatures will have a food shortage. Plants who may survive contamination get the chance to be eaten by other living creatures that can start the problems of biomagnifications and bioaccumulation. Humans are susceptible to higher levels of pollutants through ingestion because we are at the top of the food chain.\nWhen plants and the soil from which many living things get food from becomes contaminated with toxic chemicals, these pollutants can be carried and accumulated in each animal that enters the food chain before man finally eats the one with the most quantity at the end of the chain. Heavy metals, as previously mentioned, can affect internal organs, the brain and even the development of young children. Pregnant women can bear ill or handicapped children as a result of heavy metal contamination of their food. For these innocent babies, the effect of these pollutants will remain with them for the rest of their lives.\nPrevention or Solution Since these heavy metals are important for other natural reasons and can only be broken down through radiation, we cannot easily just find technology that will eradicate them. We can only try to control them by studying more about how these affect the human population and how they become a dangerous part of the food chain. We can develop more studies as to which plants are more resistant to heavy metal contamination so that we can adapt our choices of agricultural products in industrial territories. (Velky, par. 3)\nGovernments concerned with the environment can also form agreements such as the “Heavy Metal Protocol” initiated by Canada which seeks to necessitate the use of environmental-friendly technology to lessen heavy metal pollution from industrialized factories. To avoid direct poisoning from toxic materials, it would be wiser to eat younger fish or smaller marine animals that could not have accumulated many chemicals yet from the food chain. It might also be smarter for people to eat animals that are not predators to avoid being the last link in a contaminated food chain.\nIt would also be better to listen and follow health advisories regarding food toxicity rather than risk food poisoning. Conclusion Technology and nature can cook up a dangerous recipe when heavy metals are concerned. Man has to learn to control wastes and learn more about pollution to be able to make wiser decisions regarding many things that affect the food chain. If the food chain is not properly secured, it can be assumed that the human population will experience short-term, long-term and fatal consequences that could have been otherwise avoided with good principles and information. Rough Draft\nIntroduction: All living things are essential in the food chain. Heavy metals are natural necessities but can be poisonous if taken in overdosed amounts. What are Heavy Metals: Nature provided man with heavy metals in natural land forms like rocks and soil. These minerals can be released as the land forms are broken down by weather and time. Man also has increased heavy metal contamination through technology, mining and industrialization. Heavy metals are not easily destroyed unless by radiation. Therefore, man has to learn how to control the quantity of these heavy metals in his environment.\nHow Do Heavy Metals Seep Into the Food Chain? Nature can release these when rock formations are broken, through erosion, volcanic eruptions and acidic rain. Man can contaminate his environment by improper disposal of wastes such as sewage sludges, irresponsible use of technology which emits poisonous gas like lead. Nature and man can also assist each other in contaminating the earth too. Bioaccumulation and Biomagnifications: Biomagnification happens when a contaminated prey is eaten by another living creature.\nEach of the biomagnifications in the food chain allows the accumulation of the heavy metal to increase until the end eater of the chain suffers the most quantity of toxic material ingested. This is bioaccumulation. Scientists are also proving that organisms can get these toxic wastes from the soil even when it was believed that heavy metals can bind themselves well to soil particles so that they become non-bio-accessible. This was refuted by a study in France. The human body also accepts heavy metals like cadmium and lead because these are mistaken for zinc and calcium which have similar properties.\nHowever, due to its differences, the functions aided by zinc and calcium are not met by cadmium and lead which results to devastating consequences. Effects of Heavy Metals to Humans: Different minerals have different effects. Plants, which are a very important part of the food chain, may also die because of contamination and this can affect those who eat it. Humans can die because of contamination. Should a victim live, serious consequences like birth defects for babies and handicaps may form.\nPrevention or Solution: Research must be done to find which plants can adapt to contamination and not before crops are planted near industrialized areas. Prevention and finding ways to control waste disposal are also recommendable. Learning to choose which food to eat can also save one from food poisoning. Conclusion: Man needs to learn how to control or manage waste disposal to protect and survive in the food chain.\nCoghlan, A. (23 December 2002). Danger of toxic metals in soils underestimated. Retrieved 15 October 2007 from http://www. newscientist. com/article/dn3196. html.Contaminants Division. (n. d. ). Northwest Territories contaminants fact sheets. Retrieved 15 October 2007 from http://64. 233. 183. 104/search? q=cache:3JoD8cM9h6cJ:nwt-tno. inac- ainc. gc. ca/pdf/contaminants/HeavyMetals_e. pdf+heavy+metals+in+food+chain&hl=tl&c t=clnk&cd=17&gl=ph. Haas, E. M. (n. d. ). Heavy Metals and Minerals. Retrieved 15 October 2007 from http://www. healthy. net/scr/article. asp? Id=1660. ILPI. (30 June 2007). Heavy Metals. Retrieved 15 October 2007 from http://www. ilpi. com/ msds/ref/heavymetal. html. Lenntech Water Treatment and Air Purification Holding.\n(2006). Heavy Metals. Retrieved 15 October 2007 from http://www. lenntech. com/heavy-metals. htm. The Ohio State University (n. d. ). Background Levels of Heavy Metals in Ohio Farm Soils Retrieved 15 October 2007 from http://ohioline. osu. edu/rc275/rc275_1. html. Velky, P. (n. d. ). The heavy metals in human food chain. Retrieved 15 October 2007 from http://bionet. informatik. uni-oldenburg. de/schulen/novaky/heavy_metals/en/hm08. htm. Wageningen University. (24 September 2007). Heavy Metals. Retrieved 15 October 2007 from http://www. food-info. net/uk/metal/intro. htm."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:46e58331-5b8d-483f-9fa9-8186629815a2>"],"error":null}
{"question":"How long does an asphalt shingled roof typically last?","answer":"An asphalt shingled roof typically lasts between 15 to 25 years, depending on the quality of the shingles used. The first signs of deterioration usually include loosened shingles or curling edges on the shingles.","context":["How to Make the Best Choice From a List of Roofing Companies\nPeople who purchase a new home typically have a home inspection done before they settle on submitting their final offer. A professional inspector will include a roof inspection in their report, so most homes for sale will not have any current problems with the roof. The new owner of the home will probably not need to think about repairing or replacing the existing roof for at least 10 years or more.\nThe average life of an asphalt shingled roof ranges from 15 to 25 years depending on the quality of the shingles used. One of the first signs that a shingled roof needs attention is loosened or curling edges on the shingles, or a serious windstorm may blow several off if they have begun to loosen. Any of these issues is a sign that a roof may need to be repaired or even totally replaced.\nAlthough asphalt shingles are the most common type of roofing material used by modern roofing companies, other roofing materials have gained popularity within the past couple decades. Metal roofing materials are now manufactured to resemble various types of shingles. While metal roofs have at least double and sometimes triple the lifespan of asphalt shingles, the initial cost is considerably higher. A couple important advantages of metal roofing is that it is more resistant to wind damage and can prevent fire damage in the case of a nearby fire. Many home insurance companies offer a policy discount for metal roofed homes.\nSome other types of roofing materials include clay or concrete tiles, slate and wood shakes or shingles. Slate and tile roofing is heavier than asphalt or metal roofing, so care must be taken to ensure the structure of the building can support the extra weight. The advantages of these tiles is that they are nearly indestructible and will perform well as long as the building stands.\nWood shakes are often made of cedar since this type of wood is rot resistant, although there is an increased fire danger with wood roofing. Wood shingles were used extensively before the availability of asphalt or metal roofing, and wood is very aesthetically appealing.\nWhen a homeowner realizes the roof on their home is beginning to deteriorate and needs attention, they need to decide which type of roofing material is their best choice. Asphalt shingles is the material most roofing companies work with these days, and it is likely the type of shingles used for the existing roof. The homeowner may also consider replacing the roof with some other type of roofing than is currently in place.\nLocal home improvement stores usually have a list of professional contractors in the area, so they are probably the first place to look for qualified roofing companies. The store employees may also be able to recommend the most qualified contractors. The yellow pages and the internet are also good sources of contractor listings. One advantage of online searches is the company sites typically include reviews and comments from past customers.\nOnce the homeowner has found information about several professional roofers, they should call at least three to give them a bid on their project. The contractor will first inspect the existing roof to identify the severity of the problem. They may recommend repairing a problem area rather than replacing the entire roof. They should be able to give the homeowner specific advice if they are considering other types of roofing materials.\nLocation and climate have a lot to do with which type of roofing material is best. Homes in very cold climates where a lot of snow falls each year often do best with metal roofing. This is because snow slides off easily and eliminates damage from a heavy snow load. Homes located in the desert southwest are often roofed with clay tiles because they resist sun fading. Tiles can be designed to compliment the adobe homes that are common in that area.\nAlthough some people don’t think much about their roof until they notice a problem, it really is important to the integrity of their home. A leaky roof can allow deterioration of the interior structures of a home which can cause serious and expensive damage.\nWhat to Know About Commercial Roofing Contractors and Designs\nIt is unfortunate that the majority of roofs on commercial properties are often ignored until there is a large problem present, such as a leak or missing shingles. Unlike residential roofs, these commercial roofs will be neglected until the problem is large and must be repaired by professionals. While it is possible to complete roofing repairs independently, the majority of commercial roofing maintenance is difficult to manage because these roofs are not the easiest to access. Furthermore, the longer a business allows a problem to persist, the greater the need for professional skills and equipment to be used in the repair procedure.\nNeedless to say, having regular preventative commercial roof maintenance will save a great deal of money because the repairs will be kept to a minimum. Regular maintenance of both residential and commercial roofs will maintain the longevity of the roof meaning the roof will not need to be replaced as often. Of course, to find the correct contractors to provide this suitable maintenance it is necessary to take certain considerations into account. This article will examine the points to take into account when looking at commercial roofing contractors; as well as the different types of roofing designs used. It is important to look at the designs available because roofing style can influence the longevity and maintenance of the roof.\nWhat Are The Considerations to Make When Choosing a Commercial Roofing Contractor?\nRegardless of whether you are looking at having a commercial roof installed or repaired, it is essential that you choose the correct roofing contractor. By using the correct contractor, you can be sure that the job is on the way to being a success and not a failure. The first point to take into account is that the contractor has several years of experience with a strong reputation. Professionalism is essential and if the company has a bad reputation, the chances are that they will not provide the correct quality of services.\nOne method to identify the company or contractor’s standard of services is to read testimonials or speak to colleagues regarding this issue. A contractor presenting with positive testimonials on their official website demonstrates a degree of professionalism with high quality of customer service. Unfortunately, many official testimonials may be dishonest and misrepresent the company; therefore, it is recommended that you read reviews on a third-party review website before making a final decision.\nAnother factor to take into account is the cost of the service. Cost is a sensitive topic nowadays, irrespective of whether you are operating in the commercial or residential sector. It is highly recommended that you draft a budget beforehand as this will help determine which service is affordable and which is not. Furthermore, it is recommended that you obtain quotes from different contractors before making a final decision. The quotes should be customized and detailed including the discussion of insurance as part of a final payment.\nA person may question why insurance is necessary when paying for commercial roofing repairs, but insurance is imperative as a safety procedure. When a contractor provides work insurance, they will provide coverage for you as an employer removing any liability should the worker experience injury “on the job”. No individual wants to experience injury, but accidents are unpredictable and it is better to be safe rather than sorry.\nWhat Are The Different Types of Corporate Roofing Systems Available?\nWhen choosing a roofing contractor, it is important that you consider the contractor in addition to the roofing system available. By understanding the different roofing systems on offer, you will be able to choose the contractor with the correct skills to deal with your problem. Below are the different commercial roof designs available:\n• The Green Roofing System\nOne of the most popular roofing systems in today’s society is the green roofing system. This is a unique design because it utilizes vegetation as its primary covering. A beneficial design for smaller businesses because the cost of heating and cooling is reduced using the vegetation and its insulation. Unfortunately, the roofing maintenance is labor-intensive and requires a contractor with skills in this particular design for effective maintenance.\n• The Metal Roofing Design\nArguably the most common roofing style is the metal roofing design. Metal roofs are designed using various types of metal materials including steel, copper, metal tile sheeting, and aluminum. This is beneficial option due to the reduced cost of installation, but it can be disadvantageous because of the vulnerability to rust when exposed to sunlight. This means that the steel will require additional layers of protective chemicals to prevent corrosion from sun exposure. Contractors must be aware of this and be willing to deal with these chemicals.\nSigns It’s Time for Roof Repair\nA solid roof is crucial for a cozy, warm, and leak-free home. Although the roofer may have given a service guarantee of at least 25 years, the roof still needs to be maintained and serviced regularly to prevent roofing emergencies. Simple tasks such as having broken shingles replaced, sweeping leaves and twigs off the roof, as well as washing it immediately after winter are some of the roof maintenance tips that can help your roof last much longer. Having your roof inspected at least twice a year also helps detect issues and potential problems should be addressed immediately to avoid a disaster. Some of the issues that call for immediate roof repair are discussed below.\n1. Curling Shingles\nCurling shingles, also known as cupping, are a clear indicator that you may need to have the shingles replaced, or even a new roof. Cupping is an initial sign of individual shingles wearing off and could lead to leaks into the house. Temperature fluctuations may cause cupping among other external factors. Consider calling a roof repair expert as soon as you start noticing roofing shingles cupping or clawing. The contractor may have to inspect the level of damage to determine whether they need replacing or its time for a new roof.\n2. Water Stains in the Attic\nA simple leak in the attic can cause massive structural damage to both the roof and the house. Detecting leak from the attic is relatively easy. You can either have the attic tested or just look for streaks of light getting in through the roof. Testing for dampness in the attic especially past the insulation points is also recommended. If there are water stains in the attic or can see light rays through the roof, it’s then time to call a roofing expert. Although small and simple leaks can be patched up, the contractor will need to test the entire roof for structural damage. Larger leaks may call for a part of the roof taken off and reinstalled afresh.\n3. Granules in the Gutter\nWhile your roof may seem perfectly ok and no signs of leaks, finding a bunch of granules in the gutters is an indication that the roof is deteriorating, fast. Unless it is a freshly installed roof, granules in the gutters mean shingles are wearing off fast and that they aren’t protected from direct sunlight. Manufacturers coat roofing shingles with granules to help improve their durability by preventing baking by the sun. If you notice granules in the gutter, then chances are your roof is halfway through its lifespan, and that you should start planning for a new roof.\n4. Cracked and Missing Shingles\nAlthough a common occurrence, cracked or missing shingles call for immediate fixing. This is particularly important if one part of the roof has broken shingles. The cracked shingles may be as a result of a poor installation job, or wind damage. A good roofing contractor should be able to identify the root cause and have it addressed in no time. Should you however find entire shingles missing, you then need more than just roof repair.\n5. A Sagging Roof\nA sagging roof may be due to a structural issue that needs to be addressed as soon as possible. The problem could be with decking in the attic, or even worse, foundation supports. While you may not be in danger (yet), it would be wise to call your roofer immediately (if the roof is still under warranty). The roofer may have to reroof the house afresh or take on the necessary repair measures.\nMoss and algae may create a habitat on your roof as well. While there may be no cause for alarm, it would be best if you considered having the roof washed to get rid of these microbes. Moss attract moisture onto the roof which not only makes it heavier but could also start decomposing the shingles.\nHiring a Roofer: What to Look For\nOnce confident that the roof requires fixing, it’s important that you find a good roofer to handle the repairs. If the roof is still under warranty, you can then call the roofer to have it fixed ASAP. If it is way past the guarantee provided, you will then have to hire a roofer yourself. Some of the basic qualifications to look for in a roofer include licensing, insurance, experience, expertise, and proper communication. Be sure to get quotes from several service providers to find the right one for the job."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:6da2b4c3-f093-4884-9972-8229116cd96a>"],"error":null}
{"question":"What immediate steps should be taken if a smartphone with mobile wallet services is stolen?","answer":"If your smartphone with mobile wallet services is stolen, you should immediately: 1) use security software to lock the device and wipe sensitive information if installed, 2) report the theft to your wireless carrier and provide the IMEI/MEID number to disable the device and mobile wallet services, 3) report the theft to the police with the phone's make, model, and IMEI/MEID number, and 4) change all passwords for mobile wallet services and banking accounts if you cannot lock the phone.","context":["Consumers are increasingly using their smartphones, tablets and other mobile devices as \"mobile wallets\" to pay for goods and services, downloading software that allows them to complete both mobile and in-person transactions. As the use of mobile wallet services increases, consumers need to protect their smartphones, mobile wallet applications, associated data, and mobile wallet services from theft and cyber attacks.\nHow to Safeguard Your Mobile Wallet Smartphone\n- Consider your surroundings and use your smartphone or mobile device discreetly.\n- Do not use mobile wallet services to conduct financial transactions over an unsecured Wi-Fi network.\n- Never leave your smartphone unattended in a public place. Don't leave it visible in an unattended car; lock it up in the glove compartment or trunk.\n- The police may need your smartphone's unique identifying information if it is stolen or lost. Write down the make, model number, serial number, and unique device identification number (either the International Mobile Equipment Identifier (IMEI) or the Mobile Equipment Identifier (MEID) number). Some phones display the IMEI/MEID number when you dial *#06#. The IMEI/MEID can also be found on a label located beneath the phone's battery or on the box that came with your phone.\n- Review the service agreement for the financial account used in your mobile wallet to find out what will happen and who to contact if your smartphone is stolen or lost, or if your mobile wallet application is hacked.\n- Monitor the financial account used in your mobile wallet for any fraudulent charges.\n- Choose a unique password for your mobile wallet. Should your smartphone be lost or stolen, this may help protect you from both unwanted charges and from theft and misuse of your personal data.\n- Install and maintain security software. Apps are available to:\n- Locate your smartphone from any computer;\n- Lock your smartphone to restrict access;\n- Wipe sensitive personal information and mobile wallet credentials from your smartphone; and\n- Make your smartphone emit a loud sound (\"scream\") to help you or the police locate it.\n- Adjust your \"locked screen\" display to show your contact information so that your smartphone may be returned to you if found.\n- Be careful about what information you store. Social networking and other apps may pose a security risk and allow unwanted access to your personal information and mobile wallet data.\nWhat to Do if Your Mobile Wallet Smartphone Is Stolen\n- If you are not certain whether your smartphone or mobile device has been stolen or if you have simply misplaced it, attempt to locate the smartphone by calling it or by using the security software's GPS locator. Even if you may have only lost the smartphone, you should remotely lock it to be safe.\n- If you have installed security software on your smartphone, use it to lock the device, wipe sensitive personal information, and/or activate the alarm.\n- Immediately report the theft or loss to your wireless carrier. You will typically be responsible for any charges incurred prior to when you report the stolen or lost smartphone. If you provide your carrier with the IMEI or MEID number, your carrier may be able to disable your smartphone, your mobile wallet services, and block access to your personal information and sensitive mobile wallet data. Request written confirmation from your carrier that you reported the smartphone as missing and that the smartphone was disabled.\n- If your smartphone or mobile device was stolen, also immediately report the theft to the police, including the make and model, serial and IMEI or MEID number. Some carriers require proof that the smartphone was stolen, and a police report can provide that documentation.\n- If you are unable to lock your stolen or lost smartphone, change all of your passwords for mobile wallet services and banking accounts that you have accessed using your smartphone service.\nFor more information about what to do if your wireless device is lost or stolen, and contact information for service providers, go to: www.fcc.gov/guides/stolen-and-lost-wireless-devices\nConsumer Help Center\nFor more information on consumer issues, visit the FCC’s Consumer Help Center at https://consumercomplaints.fcc.gov.\nTo request this article in an accessible format - braille, large print, Word or text document or audio - write or call us at the address or phone number at the bottom of the page, or send an email to email@example.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:f0e2a7da-2b49-4bca-b8ef-69e8e887b937>"],"error":null}
{"question":"how do scientists check if quantum messages are secure?","answer":"Scientists can check message security because quantum particles cannot be measured without destroying the information they contain. If photons arrive in the same state they were sent in, no eavesdropper has extracted the information. If they arrive in a different state, it indicates the information has leaked into the environment and the message is not secure.","context":["Back in the 1980s, quantum physicists discovered that the strange rules of quantum mechanics allowed information to be sent from one part of the universe to another with complete privacy. This so-called “quantum cryptography” would be perfect, they said, because the security of the message would be guaranteed by the laws of physics themselves.\nWithin a few years, researchers demonstrated the technique in the lab, and today quantum cryptography is becoming commercially viable thanks to companies such as ID Quantique in Geneva, Switzerland.\nBut the entire mechanism is a little counterintuitive. The private message is not sent using quantum mechanics at all. Instead, physicists use quantum processes to send a code called a one-time pad that is used to encrypt the original message. The encrypted message is then sent over an ordinary telecommunications channel and decoded in the usual way. The technique is called quantum key distribution.\nComputer scientists know that a message encoded using a one-time pad cannot be broken. So the security comes from the ability to send the one-time pad with perfect privacy, which is what this approach guarantees.\nAnd that raises an interesting question. If it’s possible to send the one-time pad securely using quantum mechanics, why not just send the original message that way?\nToday, Wei Zhang at Tsinghua University in Beijing and a few pals say they have done just this. The new process is called quantum secure direct communication, and the Chinese team have used it through 500 meters of fiber-optic cable for the first time.\nThe reason physicists have relied on one-time pads in the past is simple. At issue is whether a message has been overheard. Physicists can check this because quantum particles cannot be measured without destroying the information they contain.\nSo when photons are transmitted, if they are arrive in the same state they were sent in, an eavesdropper cannot have extracted the information they contain. But if they arrive in a different state, that is clear evidence that the information has leaked into the environment and the message is not secure.\n(In practice, physicists can be sure that a message is secure as long as this leakage is below some critical threshold.)\nThe problem is that the leakage becomes apparent only after it has occurred. So an eavesdropper would already have the information by the time physicists found out about the ruse.\nThat’s why they use this process to send a one-time pad, a set of random numbers that can be used to encrypt a message. If the one-time pad is overheard, physicists simply disregard it and send another, until they can be sure that the process was completely private.\nBut physicists would dearly love to do away with the one-time pad if they could find a way to ensure the secrecy of a message before it is sent. And some years ago, theorists worked out a way to do this.\nThe method exploits the quantum phenomenon of entanglement. This occurs when quantum particles are so closely linked that they share the same existence—for example, when they are both created at the same time and place.\nWhen this happens, the particles remain linked, even when they are separated by vast distances. And a measurement on one particle immediately influences the state of the other.\nSo the trick is to create a set of entangled particles, such as photons, and encode information in their polarization state. So vertical polarization could represent a 1 and horizontal polarization a 0, for instance.\nThe sender, Alice, keeps one half of each pair and sends the others to Bob, who then has a set of photons that are entangled with Alice’s photons.\nBob separates his photons randomly into two groups. He measures the polarizations of one set and sends the results back to Alice. She then checks whether the states have changed during transmission—in other words, whether Eve has been listening in.\nIf not, then Alice and Bob know Eve cannot have seen the other photons either, because they have been separated at random. And that means Alice and Bob can use the remaining photons to transmit data using the normal process of quantum communication, which is perfectly private.\nAnd that’s exactly what Zhang and co have done. One reason the experiment is difficult is that the photons have to be stored while this checking process is ongoing. Zhang and co do this by sending the photons around a two-kilometer loop of optical fiber and carrying out the checks as quickly as possible. The longer it takes, the more likely the photons are to be absorbed or scattered by the optical fiber.\nThe results clearly show the potential of the technique. “This fibre based QSDC system has the potential to realize a transmission rate close to security key rates of current commercial quantum key distribution systems,” say Zhang and co. “The advantage [is] that the QSDC system could transmit not only secure keys but also the information directly.”\nOf course, various improvements are needed to make this kind of system commercially viable. But the work is an important stepping stone toward entirely quantum-based secure communication. Banks, governments, and military agencies will be watching eagerly.\nRef: arxiv.org/abs/1710.07951 : Experimental Long-Distance Quantum Secure Direct Communication"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:8aba708a-3afb-417b-a0ef-4d7c9a825161>"],"error":null}
{"question":"How do the trigger mechanisms of the Canjar set trigger and the Walther PPQ SC's Quick Defense Trigger compare in terms of weight and functionality?","answer":"The Canjar set trigger and Walther PPQ SC's Quick Defense Trigger have different mechanisms and weights. The Canjar set trigger can be adjusted to release at just under one pound (around 16 ounces) when set, and requires pushing the trigger shoe forward until the set trigger lever protrudes from a slot. In contrast, the Walther PPQ SC's Quick Defense Trigger has a consistent 5.6-pound break with a short 0.4-inch travel and quick 0.1-inch reset, without any setting mechanism.","context":["Volume: 53 | Back to issueSubscribe Now\ncolumn By: Gil Sengel | July, 21\nOnce popular on pre-1930 single-shot target rifles, all European cartridge rifles and most muzzleloaders, this trigger is rarely seen on new cartridge guns today. Why? Simply because today’s primers are more sensitive and uniform than ever. Extremely heavy mainsprings to guarantee ignition are no longer needed, so trigger pulls can be adjusted downward to the point where the added expense of a set trigger is no longer necessary.\nNevertheless, anyone who likes to shoot older guns, or their new reproductions, will eventually come across a rifle and pass it up because of fear of the set trigger. As noted in the previous column, there is nothing to go wrong except dirt or powder fouling buildup, which is easily remedied. If one is disassembled for entertainment, or someone starts filing on parts, then all bets are off!\nThe foregoing also applies to a slightly different set trigger. As one would expect, humans could not leave well enough alone. They also created two-, three-, four- and even five-piece triggers! Published drawings show these pieces to be tiny, oddly shaped things, no two of which are alike. Each pivots on a pin through the trigger box. This provides a simple way to determine the number of parts in any set trigger – just count the pins and subtract one for the setting trigger.\nThese set triggers do not have ground sear surfaces that slide apart in a normal sear/hammer relationship. Thus, there is nothing to wear, chip or break. When the setting trigger is pulled, tiny flat springs push each piece into proper alignment with the next to produce a state of equilibrium. A tiny movement of the firing trigger then disrupts this balance, causing the pieces to rotate out of the way, releasing an arm on the setting trigger to spring upward, firing the rifle. One early competitor remarked that his set trigger was so finely made that the rifle would fire if simply tipped up from horizontal to vertical!\nSet triggers consisting of several parts are probably quite rare. I have seen very few. All worked perfectly except one that appeared to contain congealed oil. It took several weeks of soaking, but eventually all the little parts moved and the trigger functioned normally. Nevertheless, since one-piece set triggers can be set so light that most folks can’t feel contact with the trigger before it releases, what’s the point?\nAnother series of set triggers is that produced by Winchester. There were three in the group, with the Schüetzen Double-Set available on the Winchester single-shot rifle. It is the same as the one-piece triggers covered in the last issue. The two others in the series, the single set and close-coupled double set have small sear surfaces that slide apart, rather than being held in equilibrium. These wear and this is probably the reason many of them are found in non-working condition. If backing out the adjustment screw and cleaning doesn’t allow the trigger to set, it is worn, or worse, and will require parts to be made. This is far beyond the realm of the home gunsmith.\nNow comes a modern set trigger. It was made by the M.H. Canjar Company in Denver, Colorado. This outfit produced many accessories for high-power and small-bore target shooters, but its main product was the Canjar Precision Trigger.\nThe Canjar trigger itself, however, was not a set trigger. It was a replacement for the military designs in the dozens of surplus bolt guns being imported in the 1960s through the 1980s. Canjar triggers were better than needed on sporting rifles, but at the time, many target guns were being built on surplus Mauser and Springfield actions. Virtually all custom varmint rifles were also built on these actions, and they needed triggers as well.\nThe reason for covering the Canjar non-set triggers is because their set trigger involves only the addition of a set trigger shoe in place of the normal curved finger piece. When unset, the trigger looks like one fitted with a clamp on, half-inch wide trigger shoe so popular at the time. A closer look shows that the curved finger piece has been removed and the shoe is attached by a single cross pin. In the front of the shoe a thin, vertical slot has been milled.\nTo set this trigger, a fingernail is hooked behind the shoe and pushed forward until the set trigger lever protrudes from the slot in the front of the shoe. There is no sound produced when the trigger sets. Touching the small, exposed piece releases the shoe, which snaps back with enough force to jar the sear out of engagement and fire the rifle.\nThe only problem I have seen with a Canjar set trigger is dirt or gun case lint getting into the mechanism because it is exposed outside the stock. Simple cleaning as described in the last column corrects this. Other complaints were just what you would expect – set trigger pull was too heavy. When newly installed, I always set the trigger to release at just under one pound. Nevertheless, more than one owner wanted the set trigger to release when the side of the stock was rapped with the knuckles!\nPhotos show a Canjar set trigger installed on a Ruger No. 1 built with a very heavy 30-inch barrel as a Schüetzen-style cast bullet rifle. From a benchrest, there is no real difference in group size between an unset 2.5-pound pull and set 10-ounce let off. Offhand is a different story! The light pull cuts scores in half.\nSadly, except for muzzleloaders or a very few reproductions of old long-range rifles, newly made set triggers are a thing of the past. Yet with proper practice, they provided a delightful solution to a very real problem for competition riflefolk at the time. They are from an era when target shooting was a more leisurely pastime. Do not fear the set trigger. Just don’t disturb it so long as it functions properly, but clean if necessary – and never file, bend or disassemble one.","Walther has expanded its handgun lineup with its new PPQ SC, a subcompact version of its popular PPQ.\nWhat to know about Walther's new PPQ SC:\n- The PPQ SC scales down the popular PPQ for a more comfortable carry option.\n- It's 6.6 inches in overall length, 4.4 inches in height and 1.3 inches in width.\n- Controls (slide stop and magazine release) are ambidextrous.\n- It uses Walther's Quick Defense Trigger, a 5.6-pound trigger with a quick reset.\n- It's available with a variety of magazine options and has an MSRP of $649.\nToday Walther has announced its newest addition to its handgun lineup, and it's something fans of the brand have been wanting for some time — a sub-compact option in the PPQ line. The new Walther PPQ SC (for Sub-Compact) takes the popular PPQ down to a more manageable size for comfortable carry, while keeping all of the great standard PPQ features shooters love.\nThe new PPQ SC shaves off almost an inch in length from the PPQ M2 and roughly three ounces in weight. In terms of height, the PPQ SC knocks off about another inch, taking it from 5.3 inches to 4.4 inches. Those figures might not sound like a lot, but as most who carry know, an inch or two and a few ounces here and there can make all the difference when it comes to comfort and concealment.\nLike the earlier PPQ models, the new PPQ SC features an excellent trigger (Walther refers to it as a Quick Defense Trigger) that breaks cleanly at 5.6 pounds and a short 0.4-inch travel and quick 0.1-inch reset. The trigger guard is also serrated.\nAs with all of Walther's handguns, there is still an emphasis on ergonomics with the PPQ SC. This starts with the grip, which features a non-slip, cross-directional surface for improved control, even under quick and repetitive recoil. It continues with the gun's interchangeable backstraps designed to fit a variety of hands.\nIf that weren't enough, Walther's PPQ SC also features ambidextrous controls. The slide stop and thumb-activated magazine release are both left- and right-hand friendly. And the slide stop is extended for easy activation, even with gloves. The PPQ SC's Tenifer-coated slide also wears front and rear serrations for more efficient manipulation.\nSights are simple but effective low-profile three-dot polymer combat sights. The sights facilitate quick aiming, yet remain unobtrusive to prevent snagging on the draw. The rear is also adjustable for windage.\nOther noteworthy features include a MIL-STD 1913 Picatinny forward rail for accessories such as a laser or light and a Tenifer-coated barrel and inside action parts. In addition, the PPQ SC will be available with a few different magazine choices, including flush fit, finger extender and extended sleeve options.\nI had the opportunity to test the new PPQ SC several months back at a Walther media event, and though I can't provide a full review here, I can say that it performed very well. The event was a training program at a Deliberate Dynamics facility outside of Price, Utah, and Inceptor Ammunition provided a ton of its ARX defensive and RNP training loads for our use. And Clinger Holsters offered holsters for the event. Our group of writers and industry professionals put a ton of rounds through the guns and they all proved quite reliable.\nThe Quick Defense Trigger was pleasant, and the gun handled well, even with the flush-fit magazines, though I did tend to shoot better with the finger extension magazines. The sights were fairly easy to acquire and put on target — perfectly suitable for an everyday carry piece. Operating the gun was also simple, with the mag release activating seamlessly and dropping magazines freely.\nI did have a little trouble with the slide not locking back on an empty magazine, not because there was anything wrong with the gun, but simply because of the placement of my thumbs on the slide. Once I made an adjustment, it locked back each and every time.\nOverall, I came away very impressed with the gun in my time with it. It's everything you might need in a daily carry gun — highly concealable, comfortable in the hand and plenty shootable at appropriate distances.\nMSRP on the new PPQ SC is currently listed at $649. For more on this new subcompact, keep an eye out for a full review in an upcoming issue of Gun Digest the Magazine.\nWalther PPQ SC (Sub-Compact)\nBarrel: 3.5 in.\nBarrel Twist: 1:10\nOverall Length: 6.6 in.\nHeight: 4.4 in.\nWidth: 1.3 in.\nWeight (empty mag): 21.2 oz.\nSight Radius: 5.6 in.\nTrigger: 5.6-lb. Quick Defense Trigger\nFinish: Matte Black\nSights: Low-profile, three-dot combat iron sights\nCapacity: 10 rounds (standard)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:fbd949d2-fb32-425c-86cb-1d1fbb559d5e>","<urn:uuid:427f0ad2-816a-4cde-abd2-ca080d9b87ae>"],"error":null}
{"question":"How do Solution Focused Therapy and Mindfulness-based approaches differ in their approach to addressing personal change?","answer":"Solution Focused Therapy focuses on the here and now, working on finding immediate solutions without exploring past problems, aiming for quick practical results. In contrast, mindfulness-based approaches emphasize slowing down, reflecting, and learning to cope with experiences rather than seeking quick fixes. Mindfulness approaches work with people to change their attitude and relationship to conflicts and suffering, integrating spiritual, mental, and physical levels, while acknowledging that difficulties cannot be avoided but must be worked through.","context":["[Key Words: change; human change processes; theories of change; mindfulness; neuroscience]\nEveryone who is a Trekky fan remembers the opening to the 1960’s television extravaganza, Star Trek, in which the narrator boldly proclaims space as the final frontier. At the time that television show premiered, we had yet to land a man on the moon. In the 21st Century now, although far from finished, we have explored space in ways only imagined in the 1960’s. Likewise, from Sir Arthur Evans on, we have dug through the ages of the earth’s core to explore our archaeological and geological pasts. And with the adventurous spirit of Jacque Cousteau, we have descended to the the depths of the ocean and continue to explore that unknown region as well. I would like to pose a new possible region of exploration, one that some people may think we have already figured out. With the emergence of cognitive science, neuroscience, and technologies that aid in our understanding of our neurological system, we are now at the edge of a new frontier: human change processes. Our previous simple formulas that sought to explain humans change no longer appear to apply, given the new data that the neurosciences and brain technologies provide. Human change processes, a notion emphasized in research done by Michael Mahoney, now appear more complex than we ever thought possible. The repercussions felt by such technologies will resound throughout several fields, including not only neuroscience and neurology, but also the philosophy of mind/body, and the science of human change processes, specifically psychotherapy.\nSeeking to Understand Change\nWhat is change? How do people change? What exactly changes when people do change? I would like to claim that I’m clearly and succinctly about to answer all these questions in the essay that follows. But forgive me if I don’t make such a fool of myself. I believe strongly in the idea that personal change is spiritual and mysterious in many ways, and that although the sciences can help us come to grips with important matters regarding human change, they cannot illumine the whole process. Because human change processes are still a frontier for discovery, change is a phenomenon to explore for the sake of adventure, about which to make guesses just for the fun of it, and for which to pretend we know what we’re really talking about so as to impress ourselves. So I’m going to have some fun and throw out some ideas regarding change for the sake of exploration and focus on an emerging view of change that a variety of practitioners, from psychotherapists and pastors to yoga instructors and meditation trainers are discussing today – mindfulness training.\nPsychotherapy and Change\nPsychotherapy is often about some form of change. People contact therapists, generally, to alter something going on in their lives – i.e. to make changes. They may desire their environment or the world around them to change. They may want other people to change. Or they may hope that they can bring about some kind of change in themselves. So the word, change, is packed with melded perceptions, ambiguous meanings, and even mysterious connotations. When we delve into and seek to clarify exactly what we mean by change, suddenly we encounter just how difficult it is to describe this human experience. But whether or not clients realize it, when they enter a counseling room, personal change is the territory onto which they have stepped. Rarely do people’s environments and relationships change unless personal change occurs as well.\nObviously, physical change is the easiest phenomenon to recognize, but when someone says, Bill is a different man, rarely are they referring to anything physical. Perhaps Bill’s core values have changed. Maybe it’s his overall demeanor that’s different. Or it could be his emotional make up, or his modus operandi for engaging life. Somehow, it’s clear that Bill is not the same person he used to be. When people talk to Bill now, they are acutely aware that it is not the same Bill with whom they used to converse. But if Bill, indeed, has changed, what exactly is the difference? And how did such a difference come about?\nConceptualizations of Change\nFor centuries, philosophers, scientists, spiritualists, and religionists have theorized about the phenomenon and experience of human change. In the field of psychotherapy alone, we encounter a plethora of theories about what change is, and how personal change comes about. In this essay, I’m going to explore human change as a mystery, without any promise that I’m going to clarify much at all. Although I promise no clear-cut answers or foregone conclusions regarding change, I do recognize that one’s view of change is premised on one’s view of human nature. My personal bias or leaning is toward an understanding of change that proposes some type of spirit/mind/body interaction. I also believe that our lived-experience allows us to make a distinction between minor and major, or superficial and deep change. Such a distinction is theorized in a number of ways. Moreover, I believe we can understand change only in holistic terms.\nTheories of Change: A Quick Overview: The First & Second Forces\nPsychotherapy formally began in the 19th Century, although human beings have explored the notion of the psyche and spirit for centuries, dating back before Biblical and Greco-Roman times. Likewise, traditions in the East exploring the notion of change date back centuries. But what some have designated as the first two forces of psychotherapy, psychodynamic and behavioristic, had their beginnings in the 19th Century with the work of Sigmund Freud and Ivan Pavlov. For decades these two schools of therapy offered contrasting views of human nature and change. For the psychoanalytic and psychodynamic theorizers and practitioners, people must undergo deep intra-psychic restructuring for long-lasting change to occur. Another way to put it is that an individual’s personality must change although defining what is meant by personality is not an easy task. In contrast, the behaviorists, following Pavlov, and culminating in the 1950’s and 1960’s with the work of B.F. Skinner, sought to simplify the notion of change. If people change their behavior, then they have changed. Both of these schools of therapy generated methodologies and technologies they used to put to work their respective theories of change.\nThe Third Force\nIn the late 1950’s and throughout the 1960’s into the 1970’s, what was called the third force in psychology emerged, a school of thought called by a variety of names – phenomenological, Rogerian, existential, and humanistic among others. People such as Rollo May, Viktor Frankl, Irvin Yalom, Carl Rogers, and more recently, Emmy van Deurzen explicated the multifarious theoretical foundations of this school. Their understanding of human nature countered both the psychodynamic and behavioral schools, which led to their name of the third force. Emphases, such as human-beings-in-context, enduring suffering and conflict, values clarification, personal meaning, and self-actualization became some of the major themes of this school, with each theorist focusing on a particular theme or set of themes. This third force also emphasized the place of the human will, values, and spirituality in contrast with the more deterministic viewpoints of the first two schools. Human change came about when people explored who they are, who they wanted to be, and what values they decided to own for themselves. This school challenged people to decide for themselves what they valued, and how they wanted to live.\nEvolutions and Permutations\nThe above description of the three forces of psychotherapy is a necessary brief and over-simplified one. There are a number of permutations in thought, evolution of ideas, and continued research in a variety of fields that have continued to address human change. The behaviorists school, for example, through research in cognitive science, cognitive psychology, and Artificial Intelligence evolved into the cognitive-behavioral school (CBT), emphasizing that human change involves a change in one’s belief structures. Major change comes about when people alter their core beliefs. CBT is a major force in the field today with prolific research in the treatment of psychological disorders. And CBT has evolved with the development of what is called the third wave of cognitive and behavioral approaches (touched upon below). Likewise, there are a variety of psychoanalytic and psychodynamic schools that have been influenced by neuroscience and studies in human development and attachment.\nAnd So It Continues: The Fourth Force\nPsychotherapy has now witnessed the evolution of what can be considered a fourth force in psychology: the postmodern school. This school of thought, emerging from such philosophies as deconstruction, has sought to alter the power in therapy, giving prominence to the client. Its philosophical foundations also run counter to the medical model exercised in the psychoanalytic and cognitive behavioral schools. Its relationship to the DSM-V is cautious at best, antagonistic at worse. Although it runs counter to much of what has preceded it, we are seeing now an incorporation of postmodern thought in other schools. Lacan in psychoanalysis and the cognitive constructivism of Michael Mahoney are such two examples. The school is likewise multifarious, given rise to such approaches as constructivism, social constructionism, and narrative therapies. And so it continues.\nMindfulness as a Counter to Technologies of Change\nAll these schools of thought, individual theorists within each school, have their own take on human change, what it is, and how it occurs. Not only will I not enter the debates here, but also I’ve experienced that the debates can be rather divisive, fruitless, and ultimately pointless. Although I lean more toward the third force with some smatterings of the fourth, and some residuals of the first two, I do not believe any one school of thought has the corner on the truth. I tend to agree with Karl Popper – let’s become more enamored with what we don’t know rather than with what think we know. Returning to the beginning of this essay, for me the notion of human change, particularly revolutionary personal change, is a rather spiritual, mysterious, awe-inspiring experience. Having said that, I would like to simply throw out an idea for possible brainstorming and exploration. A view of change has emerged that, paradoxically, tends to focus less on trying to change, and more on learning to cope, which in some unquantified way ultimately leads to change.\nAs I stated above, my personal view of change, although far from clear in my own thinking, involves some type of spiritual/mental/physical interaction. I believe we are holistic beings, and we need a conceptualization of change that goes beyond mere techniques in therapy – technologies of change. Mindfulness approaches have emerged in the field of therapy from Eastern traditions, particularly Zen and other forms of Buddhism. (I am neither a Zen nor any form of Buddhist.) A mindfulness approach has presented itself as possible foundation on which all the schools of thought can rest. From my perspective, it is spiritual in nature. The question emerges: Might mindfulness be a unifying force to many of the school or therapy wars that the field has had to endure? I personally believe that mindfulness offers something valuable that can be integrated with a variety of conceptualizations of human nature. Mindfulness has already influenced all four forces on some level. Much of existential thought meshes well with mindfulness. Additionally, its impact has been felt in the cognitive-behavioral school in the form of what is called the third wave of that school, seen in the development of Dialectical Behavior Therapy (DBT), Mindfulness-Based Cognitive Therapy (MBCT), and Acceptance & Commitment Therapy (ACT). Several postmodern approaches have sought to incorporate mindfulness along holistic lines of mind/body, drawing on the work of Daniel Siegel. Neuroscience has indicated that mindfulness practices can alter brain structure and increase healthy brain functioning. Once again, at the risk of over-simplifying, mindfulness approaches basically work with people to change their attitude and relationship to the conflicts and suffering they undergo. For me personally, unlike some of the Eastern thinking that holds suffering is illusionary, I believe suffering is very real and realistically and holistically painful, influencing spiritual, mental, and physical levels.\nThe Quick Fix\nI also believe, however, we live in an age of the quick fix. We want a pill to make us feel better – NOW. We ask therapists, doctors, pastors, and politicians to end our suffering – ASAP. What mindfulness perspectives can possibly bring to the table is the importance of slowing down, reflecting, taking stock, and learning to cope with what we are experiencing – and if possible, to find meaning in such experiences. Such a view integrates with my Christian worldview, and I find it workable with the way I want to engage living. Particularly, third wave approaches, such as ACT, address the importance of clarifying values and making a commitment to align our lives with values we claim to hold. Again, what mindfulness brings to the table is a counter attitude that says, as we clarify who we are, what we believe, and how we want to live, then we will be more able to take on the real difficulties in life that come our way. Such difficulties are not necessarily resolved quickly, and we simply cannot avoid them. Mindfulness does not say to us to give in to the difficulties of life, but that when they come, not to avoid them in ways that prevent us from working through them.\nThe above discussion, for sure, is an over-simplication of the major forces and ideas I have sought to merely touch upon. There are numerous schools of thought, conceptualizations, and research from a variety of angles presently underway on human change. But the one thing I believe we do face in this culture is the idea of the quick fix. I, too, am susceptible and have succumbed to it too many times. We are an over-medicated society, which has raised healthcare in terms of the medical model to the level of an altar at which we grasp for hope and meaning. Perhaps it’s time to look at a different angle on how to work through and find meaning in the way we relate to and face our struggles in living. Perhaps a mindful way of living can help us do that. And as we find ways through our values to face our personal difficulties, maybe we will glean a little more understanding of the nature of what it takes to be human and experience personal change.\nJohn V. Jones, Jr., Ph.D., LPC-S/May 14, 2014","What is Solution Focused Therapy?\nWhen we work from solution-focused therapy, we focus on the here and now and what we need to do to reach our goals. Moreover, we are working together on finding your solutions instead of exploring and reinforcing your problems. This approach is very practical and can help clients find their solutions quickly. Suppose you have been in therapy before and are used to telling your story repeatedly and relieving these problematic experiences. In that case, this will feel very different and uncomfortable. This intervention aims not to focus on the problem and to focus on your solutions to these problems.\nWhat is Reality Therapy?\nWhen we work from reality therapy, we focus on the here and now and the choices we have in front of us. Often, we are in difficult situations with and we feel as if we have “no choice” therefore, we feel trapped, depressed, anxious, angry, powerless, and helpless. In reality, we have choices, the choices before us may not be great choices or the best choices we want to make, yet we have found ourselves having to choose from the lesser of two options. Therefore, we will evaluate how you see the goals you have achieved and those you have not, deciding if your goals are realistic and ready and willing to make a change to accomplish this goal? This theory’s foundation is taking responsibility for the choices in front of us and choosing more practical actions to help us. Again, this is not an approach to explore the past in detail. Instead, we want to understand where we are right now and what we need to do to reach your goals.\nWhat is Cognitive Behavioral Therapy (CBT)?\nWhen we work from cognitive-behavioral therapy (CBT), we are exploring what is going on right now, sometimes requiring us to understand the past better, so we can be healthier for the future. Often, we struggle with depression, anxiety, substance abuse, eating disorders, trauma, relationship issues, and familial problems. We experience many problems because we have unhelpful ways of thinking or seeing a situation or learned maladaptive (bad) coping skills, behaviors, and patterns to these problems.\nOur goal in using CBT is to change your unhelpful ways of thinking, feeling, and behaving to help you learn useful coping skills for these complex problems. This theory allows you to take concrete steps to change what you are currently feeling, thinking, and doing that is not working and replace it with more valuable skills to reduce or solve the problematic behaviors.\nThis intervention requires learning and reviewing the coping skills, tools, and steps for you to take then to leave our session and practice these tools outside of the counseling session as “homework.” If you do not practice using these new skills outside the session, you will likely not see any meaningful positive results.\nIn order to proceed with booking an appointment, you will need to fill out and sign the online fillable PDF forms listed below. Once you have completed and submitted the forms to our HIPAA approved and encrypted email address, we will process your documents and get in touch with you within 24 business hours.\nProceed with Booking\nIf You are In A Crisis, Get Help Immediately!\nIf you’re in pain, struggling, and needing immediate help then call 911, go to the nearest emergency department, call lifeline at 1-800-273-8255, or chat through their Lifeline Chat services. Additional information can be found on their website www.suicidepreventionlifeline.org\nIf, for unseen reasons, you do not hear from me or I am unable to reach you, and you feel you cannot wait for a return call or feel unable to keep yourself safe, please go to your local hospital Emergency Room or call 911. You can also call the National Suicide Prevention Lifeline: 1-800-273-8255 and ask to speak to the mental health worker on call. I will make every attempt to inform you in advance of planned absences and provide you with the name and phone number of the mental health professional covering my practice.\nThe National Suicide Prevention Lifeline is a national network of local crisis centers that provides free and confidential emotional support to people in suicidal or emotional distress 24 hours a day, seven days a week. They are committed to improving crisis services and advancing suicide prevention by empowering individuals, promoting professional best practices, and building awareness https://suicidepreventionlifeline.org.\nPresident Signs National Suicide Prevention Designation Act Into Law.\nThe President recently signed the National Suicide Hotline Designation Act into law. 988, the new three-digit number for the National Suicide Prevention Lifeline, is to be completed by July 2022. In the meantime, please continue to share 1-800-273-TALK (8255) with anyone wishing to connect to the Lifeline. 988 is NOT CURRENTLY ACTIVE nationally and may not connect callers to the Lifeline. https://www.vibrant.org/president-signs-988-into-law"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:5fd71570-ff51-413e-86dc-1a62009a19d9>","<urn:uuid:759c7019-9f8b-4e23-b4d0-2834181d1fc0>"],"error":null}
{"question":"I work with race cars but keep having electronic problems. What's worse for damaging electronics: power spikes when turning solenoids OFF or voltage drops from brown-outs?","answer":"Power spikes when turning solenoids OFF are generally more damaging than brown-outs. When solenoids are turned OFF, they can create voltage spikes of hundreds of volts due to energy stored in magnetic fields being converted back to electrical energy. For example, even a small air control solenoid can produce a 396V transient voltage spike upon turning off, which can destroy electronics or cause data corruption in microcomputers. While brown-outs (prolonged low voltage conditions) can cause equipment malfunction and eventual damage over time, they don't typically cause the immediate, severe damage that high-voltage transient spikes can cause to sensitive electronic components.","context":["All electrical and electronic equipment, connected to the mains supply is at risk of being damaged from spikes, surges, lightning, power cuts, brown-outs, power-cuts (blackouts), power back surges, and over-voltage. The following is a summary of the main types of power problems, causes and how these affect electrical and electronic equipment.\nVery short, (one millisecond) event of very high surge in voltage to thousands of volts and amps. Spikes are common in all parts of the world and repeated exposure to spikes will damage electronic equipment and corrupt data.\nWhat causes it? Switching on/off of nearby equipment, lightning, motors starting etc.\nLong duration (milliseconds, seconds, minutes, hours or days) rise in the voltage above acceptable limits. Depending on the level of the over-voltage, the damage can be instantaneous, severe and irreparable.\nWhat causes it? On return of mains supply after power cuts, under-sized utility oscillating between periods of brown-outs and over-voltage or accidental (e.g. accidental connection between two phases).\nBrown-Out / Under-Voltage:\nLong duration of low voltage (milliseconds to seconds, minutes, hours or days). Very common in parts of the world especially where the power utilities are over-stretched. Prolonged and frequent brownouts cause the equipment to malfunction or not work at all. Repeated episodes are certain to cause damage. Motors and compressors (and therefore fridges, freezers, coolers, air-conditioners and pumps) are especially at risk. In time, damage is certain.\nWhat causes it? Most commonly an over-stretched utility, especially in areas of poor power distribution infra-structure and remote areas. Common in dry seasons where water is used for electricity generation.\nDirect or nearby strikes can cause minor problems or severe disturbances and damage. Lightning produces spikes/surges, over-voltage or power cuts.\nWhat causes it? The surge is generated by either a direct hit, or indirectly striking underground or overhead lines and transmitting high surges to connected equipment in nearby buildings. For more information, come back soon.\nCommon in every country in the world, especially in areas of frequent voltage problems. Sudden loss of power can cause damage ranging from corruption of data to mechanical faults as equipment is stopped while in operation.\nWhat causes it? Power or sub station failure, breakdown in the distribution network, or simply a plug being pulled out accidentally.\nThese typically occur when power returns after a power-cut and connected equipment receives a surge of electricity at an over-voltage level, which can be very damaging (see above).\nWhat causes it? Power back surges are created by the utility, when it restores supply at an above normal voltage in order to compensate for the demand as connected equipment re-starts simultaneously.\nTelecom surges, spikes and lightning:\nShort term, high voltage and current phenomena occurring on the telephone lines. Can cause irreparable damage to any piece of equipment connected to the incoming line. The telephone line itself may even be damaged or destroyed in severe cases.\nWhat causes it? Telecom spikes are caused by lightning striking either the telephone line directly or an object near it.\nRFI (Radio Frequency Interference)/Noise:\nHigh frequency disturbances that occur within a short period of time (milliseconds). RFI & noise are very common in all parts of the world and are the main cause of data corruption.\nWhat causes it? Generated by high frequency noise from nearby equipment like TV, radio equipment, transmitters, mobile phones, switching on/off of certain loads, fluorescent lights, motor speed controls, light dimmers.","Proper Race Car Wiring Principles\nWith the never-ending quest for more horsepower and faster cars, today’s racecars have more electronics than ever. To create more horsepower, cylinder pressures must rise. To fire the cylinders under these higher pressures, modern ignition systems are more powerful than ever. This powerful spark, requires powerful drivers. Ignition boxes can easily pull 40 amps or more from your electrical systems. This combined with other systems in your car like solenoids used to control converters, nitrous, boost, etc. put an extremely high demand on your car’s electrical system. This has forced us to rethink the way the cars are wired and to consider basic electrical principles like Ohm’s Law.\nSome wiring guys or shops know what they’re doing and do nice neat work that also follows the techniques outlined here to insure a proper install of all your equipment. However, many think that a ground is a ground and power is power, as long as it turns on/off then the job is done. If this is the case with your installer, then get ready for problems. We often see beautiful wiring jobs that do not follow these basic electrical engineering practices, leaving the customer with problems at the track that waste thousands of dollars in missed runs, or even damaged components and engines!\nThe way that power is distributed between your electrical systems and your electronics can be critical to the proper operation of your electronic devices. Wire size is important to prevent voltage drops or voltage “brown-outs”, but the way the various instruments and electrics are connected and the connection paths are also critical.\nProblems can occur when an electronic device and an electrical device such as a motor or solenoid share a common electrical path (power, ground or both).\nWhile an electric motor or light bulb might draw 5 amps when it’s running, it can draw much more current when starting. A 5-amp solenoid may draw many times that when engaging, causing a momentary spike on the power and ground buses where it’s connected. This voltage drop or “transient” might last only a few thousandths, hundredths, or tenths of a second and you might not even notice it. The solenoid will likely operate just fine, with no known issues, but some of the other electronics may not like the voltage drop and malfunction, causing reset or lockup. This may be a very brief event but can ruin your day at the track for sure.\nTransients can occur when you turn OFF devices such as motors or solenoids. Energy stored in the magnetic fields of these devices can create voltage spikes of hundreds of volts when the power is suddenly removed as the magnetic energy is instantly converted back to electrical energy by the collapsing magnetic field. These spikes occur when the motor or solenoid is turned OFF not ON. These very fast voltage spikes usually do no damage to electrical devices but can destroy electronics or cause them to do things you don’t expect or want. Data corruption of memories or corruption of the software being run in microcomputers and even destruction of parts of the electronics are often the result of poorly wired circuits.\nDevices installed in a race car should be divided into two categories: power devices and control devices.\nPower devices are things that use a lot of current for brute-force operations like turning on solenoids and generating ignition sparks. They will generally have large-gauge, “fat” wires for power connections.\nControl devices are things that make decisions and send and receive low-current signals like logged data and ignition timing triggers. They will generally have smaller-gauge wires for power connections.\nSome devices will fall into both categories, like an ignition box that both generates sparks and receives a low-current signal to know when to generate said sparks. Often you will find that these devices have two sets of ground and/or power wires. There will be a large-gauge set for the power section and a small-gauge set for the control section.\nMost of us have used an ohmmeter to measure resistance. We learned that a very low resistance is a “dead short”. In today’s race environment, however, it might serve us to revise that perception. When very high currents are mingling with very precise, microprocessor-controlled electronics, there’s no such thing as a “dead short”. Even one tenth of an ohm can cause major problems when it’s dismissed as not enough to worry about. When the current consumption of a high-power ignition system is combined with a bunch of high-current solenoids, the total current can exceed 30 Amps, or much higher. When you pass 30 Amps through a cable with a resistance of one tenth ohm, the voltage drop between the ends of that cable will be 3 VOLTS. A 3 volt change in sensor reference voltage rails can have many unpredictable results.\nUnfortunately, one tenth of an ohm is as good as it gets in many situations. Even if you use a cable the size of your wrist, the crimped terminals on each end of it will result in a tiny bit of resistance. We must change our goal from avoidance to management. Given that some resistance is unavoidable, how does one minimize its impact on the operation of electronic devices?\nA Little Science- OHM’s Law\nThe second demon in racing electronic systems is transient voltage. In electronics, the word “transient” is used to describe a short-duration condition triggered by an event. For example, when you remove power from an energized solenoid, a short duration (microsecond) pulse of high voltage is generated. This “spike” of voltage, when left uncontrolled, can wreak havoc on the microcontrollers in the system.\nThere are a lot of transient events in an automotive system, but the most concerning are those that generate a high voltage. This includes the ignition spark. Unmanaged, high-voltage spikes can cause problems that are very hard to troubleshoot. The effects are unpredictable and difficult to reproduce in testing. Transients are managed with preventative measures, rather than diagnostics.\nTransients are generated by power devices, such as solenoids, relays and coils, and most harmful to control devices. Transients can be managed with the use of diodes across the coils, as well as proper wiring techniques. Every solenoid or relay in the car should have a suppression diode installed, some relays, or controllers, MAY have the diode built, others may not. Simply changing a relay, could cost you weekends of failed racing. We recommend, and there is no harm in, adding a diode as insurance, not to mention it may improve the performance of a coil-driven device.\nExamples of a couple of ways to possibly install a protection diode.\n(Diode # 1N4002, 1N4004, 1N4006 or similar)\nExample of a small air control solenoid producing a 396V transient voltage spike upon turning off.\nNot hard to see how these voltage spikes can cause problems. These spikes can happen on either the positive or negative side, depending on the circuit. Just think of how your electronics feel about a 400 volt spike running through them.\nThe general rule when wiring power and ground is to avoid sharing the connections between power devices and control devices. This is especially important for ground connections, but power connections should also be separate wherever possible. It is safe to assume that power devices generate the problems that control devices can’t tolerate. If the two categories of devices are connected to separate ground/power busses, it limits the transfer of problems from one to the other. Adding an additional ground wire to connect the 2 buses, a “common ground” like your “friend” suggested, is actually horrible. This may cause problems as opposed to fixing them— you CAN have too many grounds- especially if they go to the wrong places.\nTo this end, the best plan is to create a “Clean Bus” for the control devices and a “Dirty Bus” for the power devices. The goal of the clean bus is to minimize voltage drops, so it needs to be built with low-resistance cable and terminals. The goal of the dirty bus is to handle large currents, so it also needs to be built with low-resistance cables and terminals. In other words, use high-quality, low-resistance materials and methods for both. The 2AWG cable that handles 100 Amps without heating up will provide a quality ground reference when carrying 3 Amps for the control devices, a #10 AWG WILL NOT provide a sufficient ground path to the battery. This is not the place to save weight at the expense of unreliable operation of your electronics.\nDependence on chassis ground should be minimized. The chassis may be grounded to the battery negative terminal, providing a definite ground path to the chassis, not accidentally grounded through the engine or other purposefully grounded part. Grounding devices to the chassis should be avoided. Instead, ground them to the appropriate bus. The Clean Bus ground should not connect to chassis, ONLY TO BATTERY NEGATIVE TERMINAL!\nIn a perfect world, if the battery were safely mounted and protected in a central location, this would be best. As we all know, it is usually remotely located do to weight, rules, safety or other overriding factors. To be clear, every electrical device in the car should be grounded directly to the battery terminal. With the battery within reach, connecting the grounds properly would be easy, however, this is not realistic, so some compromise must be found.\nThe best compromise is the separation of the grounds in the form of a “Dirty Bus” and a “Clean Bus”. The Clean bus can allow for a distribution block located in a more central area of the car for simpler wiring and neatness. This distribution block should be supplied with a large cable, like 1/0 gauge cable. Again, you may compromise and “get away” with a smaller gauge, but cable smaller than a minimum of 2 gauge should NOT be used. The larger wire size and a few ounces is cheap insurance against problems, aggravation and lost weekends. Plus, chicks dig big wires!\nThis Clean Bus Distribution block can be used as the voltage source for the various toggle switches and devices as well as the power source for the low current draw control devices like boost controllers, sensors, data loggers, traction control or ignition controls.\nThe Dirty Bus can allow for a distribution block located in a more central area of the car for ease of access. This Dirty Bus Distribution block can be used as the voltage source for things like solenoids, coils, or other high current draw devices. This distribution block should be supplied with a large cable (2/0 gauge or larger). Compromise here and suffer!\nThe main power leads for devices such as EFI boxes, Ignition Boxes or Coil Drives MUST be connected to the battery terminals or at worst to the Master kill switch located near the battery! Almost every manufacturer specifies this and it is very important for reasons you will see below.\nHere are a few different ways of wiring a simple circuit with a battery, a motor or solenoid and an electronic device. To keep things simple, only the major components are shown. Electrically they are all the same in that they are all connected to positive and negative and will turn on/off. Electronically, however, the example labeled Bad, and Worst are likely to cause problems and lots of aggravation! The one labeled Good will probably work, but Best is the way it should be done. If your wiring guy is not following the simple methods shown in the Best example- get someone else to do it.\nRunning a pair of large gauge wires from the battery to distribution blocks and connecting EVERYTHING to those blocks is VERY BAD!!!\nIt may look good and be easy- but, it will NOT work properly. As shown in the Best example, the proper method is to have a “Clean Bus” with only electronics connected to that bus, and a “Dirty Bus” with the higher current draw and noisier devices connected to that bus.\n- Use the proper gauge wire\n- EVERY solenoid should have a protection diode installed- even if you have never heard of that!\n- Do Not depend on the chassis to be a proper ground\n- Ground each cylinder head to the battery ground (Dirty Bus)\n- connect each cylinder head to coil negative post if compatible.\n- Since the chassis will probably get a partial ground from the engine- Run a grounding strap from Dirty Bus ground to chassis to make sure it is properly grounded.\n- Stay away from Butt splices- just don’t use them.\n- Ignore your buddy who says “I never did that and mine works fine”\n- When connecting or disconnecting a battery charger the Master switch should be off. Do not leave it on and all electronics connected and powered up when connecting and disconnecting the battery charger. Furthermore powering up electronics rated for 12-16 volts with a battery charger that’s humming along at a much higher voltage is not good for the electronics.\nThe main power wires of these devices should be connected to the Battery Terminals.\n- Ignition Boxes (main power wires- not the on/off ignition wire)\n- Injector Drivers (main power wires- not the on/off ignition wire)\n- EFI (main power wires- not the on/off ignition wire)\nHere is a list of devices commonly found in racecars that can cause problems to electronics wired to the same circuit. The main power wires of these devices should be connected to the Dirty Bus.\n- Starter motors/starter solenoids\n- Electric Blowers\n- Electric fans\n- Headlight bulbs\nThese devices should be connected to the Clean Bus.\n- Traction Control\n- Ignition On/Off\n- EFI On/Off\n- Boost Controllers\n- Shift Controller\n- Data Loggers\n- Sensors / Instruments"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:54eafd10-98c6-4fd6-8db0-88c55b34a7b0>","<urn:uuid:c5dec07a-ee00-4ffa-bfab-230c01a418aa>"],"error":null}
{"question":"How does DRM technology impact fair use rights in educational contexts?","answer":"DRM technology can restrict educational fair use by creating 'walled gardens' within higher educational institutions, forcing educators to use DRM to limit access to digital content. This makes materials prohibitively expensive and creates virtual barriers to accessing knowledge, even when copyright law would normally permit such use. The technology cannot accurately determine what constitutes fair use, as there is no precise algorithm for deciding whether a particular use is fair or not.","context":["Digital Rights Management (DRM) is an orderly way to deal with copyright security for computerized media. The motivation behind DRM is to counteract unapproved redistribution of computerized media and limit the ways buyers can duplicate substance they’ve bought. DRM items were produced in light of the quick increment in online theft of economically promoted material, which multiplied through the broad utilization of peer-to-peer document trade programs. Regularly DRM is actualized by inserting code that averts duplicating, determines a day and age in which the substance can be gotten as far as possible the number of gadgets the media can be introduced on.\nAlbeit computerized content is ensured by copyright laws, policing the Web and getting culprits is extremely troublesome. DRM innovation concentrates on making it difficult to take content in any case, a more proficient way to deal with the issue than the hit-and-miss procedures went for catching on the web poachers sometime later.\nDRM speaks to the controls by which you can keep somebody from replicating or printing or altering or generally making accessible your favored data to other individuals.\nWhat one needs to recollect is that the even distribution of power has changed in the last ten or so years. Before that, duplicating and re-printing a physical book was non-insignificant. On the off chance that you needed to take another person’s work you expected to discover somebody who was eager to distribute the duplicate, and the distributor knew they would be sued without kindness. So there were some extremely clear physical controls in the physical print industry.\nSo DRM is a basic element on the off chance that you are making secret data accessible to others, regardless of whether it is a value list, an agreement detail, an investigator’s report, a pamphlet or a book. Without DRM you have no chance to get of counteracting individuals who approach your licensed innovation from doing just precisely what they need with it. So is the reason it is imperative.\nWho all use Digital Rights Management?\nDRM is utilized by numerous associations inside to ensure private data, share reports safely with outsiders, and review records to recognize spills. DRM inside an association is otherwise called Enterprise Rights Management or Information Rights Management. Some online frameworks where report controls and access can be adjusted continuously are here and there alluded to as Active Rights Management frameworks, yet basically, they are all a similar thing. DRM is used by a number of industries to control the use of paid digital content. It is popular in the music, movie, and e-book industries to control content distribution, prevent unauthorized use, and protect revenue streams. Whilst many end users paying a few dollars for a song, movie, or e-book, are against the use of DRM it is more readily acceptable in the business world protecting high-value reports and content available to members only. This is because if someone has paid $9690 for something they would not necessarily be happy if other users were to receive the same thing for free.\nDRM empowers distributors to log utilize so they can see information on when the content was utilized by whom and when.\nApplication of DRM\nRegarding copyright law, the scholarly resources of the copyright holders are overseen and controlled carefully by copyright holders through an innovative instrument. So DRM essentially accommodates an extra security to a work, which is in any case ensured by the Copyright law. It is a sort of extra defensive measure which one applies to his/her licensed innovation and by this the work may stay out of the span of the general population, and the utilization which is generally allowed by the tradition that must be adhered to may not be finished by the overall population, particularly as if there should arise an occurrence of Fair utilize allowed in copyright law.\nFAIR USE and Digital Rights Management\nTo have a portion from a work and utilize it for the reasons and exercises characterized as a reasonable utilize action is passable in the copyright and it additionally energizes production of new works, and this is very much settled in the copyright that unfavourable financial motivating forces will be made if unlimited and total copyright is made, because of this a shared objective of the copyright law with an investigate reasonable utilize teaching is formation of new work and furthermore advancement of knowledge. The limitation of the work into the bolted compartments of the Digital Rights Management secured copyrighted substance won’t enable one to utilize, despite the fact that reasonable as clarified by the law. The use of Digital Rights Management confines the clients to draw in with the current works, and utilize them likewise in the terms of reasonable utilize, and furthermore these computerized bolt ups will make restraining infrastructure regarding rights practiced by the proprietors, such a making of imposing business model has never been the targets of copyright law.\nSo as to save fair use exemptions, DRM frameworks would need to suit for unapproved employments of copyrighted works, yet the ease of the teaching implies that it can’t be characterized with accuracy. Thusly, trouble lies in communicating the factors that may emerge for each situation in PC code; from a mechanical point of view, there is no exact calculation for choosing whether the use is reasonable or not.\nAdvanced innovation may assume a positive part in the circle of instruction. Arrangements for DRMs may not just focus the copyright materials with capable enterprise and the media outlets, however, may truly limit instruction, making ‘walled patio nurseries’ inside higher instructive foundations by driving teachers to utilize DRM keeping in mind the end goal to confine access to an advanced substance or bolt away computerized works. Presentation of DRM advances in the different field of training brings about making the materials unreasonably expensive and past the range of people by making a virtual restriction to access to information disregarding having the arrangements to utilize them by copyright law. As far as DRM the greatest concern is the buyer appropriate to reasonable utilize, which is in question in the present civil argument of having or not having DRM arrangements in our law. These arrangements must need to experience a money-saving advantage investigation one again and the opportunity of Fair utilization of the work as specified in copyright law must not be reduced.\nCopyright and the fair use principle serve to give creators motivating forces to make while additionally enabling clients to draw in with innovative substance and rousing the making of new works, in this way profiting society by and large."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:819501b5-55dc-413c-a7c4-3592c4691126>"],"error":null}
{"question":"How can one calculate whether an agricultural system is acting as a greenhouse gas source or sink?","answer":"To calculate whether an agricultural system is a source or sink of greenhouse gases, one must: 1) Measure carbon accumulation in the soil. 2) Calculate the quantities of methane (CH4) and nitrous oxide (N2O) released into the atmosphere. 3) Account for all CO2 emissions from agricultural practices including plowing, planting, harvesting, irrigation installation and use, and application of agrochemicals, fertilizers, and lime. 4) Convert all emissions into CO2 equivalent, considering that CH4 is 34 times and N2O is 298 times more potent than CO2. 5) Compare the total gases drawn from the agrosystem against those emitted to determine the net global warming potential.","context":["Agriculture and land use change contribute 24% of greenhouse gas emissions. Researchers estimate that this share will increase by 30 to 40% by 2050. The challenge is therefore to counterbalance this with greater carbon sequestration in order to transform agrosystems into greenhouse gas sinks.\nTo determine this role, it is sufficient, in broad terms, to take into account the gases drawn from the agrosystem and to compare them to those emitted by the same agrosystem. To do this, it is necessary to measure the accumulation of carbon in the soil, the quantities of methane (CH4) and nitrous oxide (N2O) released into the atmosphere, as well as all the CO2 coming from agricultural practices: plowing, planting, harvesting, installation and use of irrigation, application of agrochemicals, fertilizers and lime.\nKnowing that CH4 and N2O are greenhouse gases 34 and 298 times more potent than CO₂ respectively, it is sufficient to convert all these emissions into CO2 equivalent to estimate the net global warming potential and compare between different agricultural management methods.\nTo calculate the carbon cost of today's dominant agricultural practices, one must also consider their direct CO2 emissions. According to studies conducted in southern Brazil and the United States, the carbon cost of agricultural practices represents a significant amount of CO2, up to 800 and 2,000 kg ha-1 year-1.\nDiagram showing how to estimate whether an agrosystem is a source or sink of greenhouse gases: measure carbon fixation in the soil, the amount of CH₄ and N₂O released into the atmosphere, and all the CO₂ from agricultural practices: tilling, planting, harvesting, irrigation installation and use, and application of agrochemicals, fertilizers, and lime.\nBased on this observation, the first thing to do is to determine which agricultural practices to adopt in order to transform the soil into a greenhouse gas sink - and not a source.\nThe first element to take into account is that the soil can become a source of greenhouse gases when there is an excessive input, because telluric microorganisms feed on all these inputs and spit them out in the form of GHG.\nEven more so when the land is saturated with water by irrigation and compacted by machinery, as oxygen availability becomes scarce, leading to methanization (a process responsible for the production of CH₄) and denitrification (one of the processes responsible for the production of N₂O). Nevertheless, these methane fluxes remain low or even sometimes negative if the soils have a good structure and are not flooded.\nIn addition, the establishment and use of irrigation accounts for 47-63% of the carbon footprint while fertilization and limestone application rise to 35%. These proportions vary according to the inputs used and their annual input. To mitigate the climate change caused by agriculture, the most obvious solution seems to be the reduction of inputs, with more reasoned spreading.\nAgroforestry and legumes\nOther agricultural practices are just as important in moving from a source to a sink for greenhouse gases.\nThis is particularly true of agroforestry, which has been adopted by some farms. It consists of using cover crops, for which carbon storage exceeds its CO₂ equivalent released into the atmosphere in terms of N2O emissions, and improves soil structure with negative CH4 emissions.\nTwo contrasting agricultural systems, behaving as source (left) and sink (right) of greenhouse gases, according to CO₂, CH₄ and N₂O emissions as well as soil carbon sequestration. Murilo Veloso\nThis is also the case for the use of legumes. As these plants associate with nitrogen-fixing bacteria, they promote carbon storage in the soil and make it possible to substitute part of the mineral fertilization, thus reducing nitrous oxide emissions.\nFurthermore, by maintaining soil moisture and structure, practices such as agroforestry and cover crops provide an alternative to decrease irrigation.\nHowever, transforming an agrosystem into a greenhouse gas sink is not always obvious. For example, plowing, the practice of turning over the soil before seeding, does not necessarily have the same impact on GHGs.\nIn temperate environments, plowing has little effect on soil carbon stocks because low temperatures in early spring slow down microbial activity and the decomposition of soil organic matter.\nIn contrast, in tropical environments where temperatures remain favorable, soil destructuring by tillage stimulates microorganisms to decompose soil organic matter, which releases GHGs.\nExample of conventional agriculture and its practices such as monoculture, tillage and heavy reliance on inputs that contribute to increased greenhouse gas emissions and reduced soil carbon sequestration, making the agricultural system a source of greenhouse gases (left). Example of agroecological practices such as cover crops, no-till, intercropping, legumes and agroforestry that contribute to decrease greenhouse gas emissions and increase soil carbon sequestration, making the agricultural system a greenhouse gas sink (right).\nFor no-till farming practices, N2O emissions are largely offset by CO2 storage, counterbalancing the 375 to 616 kg of carbon equivalent emitted per hectare per year by the 2063 to 3940 kg of carbon equivalent fixed in the soil per hectare per year. This represents nothing less than a carbon storage five to six times higher than the emissions! Ploughing, on the other hand, clearly reduces the soil's capacity to draw carbon from these tropical zones.\nOn the other hand, plowing also constitutes an additional cost of CO₂ when the use of diesel for machinery is taken into account (35.3 kg of carbon equivalent per hectare per year in conventional and 5.8 kg in direct seeding).\nFrom field to plate\nLet's not forget that greenhouse gas emissions do not stop at production, but also continue during transportation, processing, packaging and redistribution of products.\nIt is therefore necessary to radically change agricultural practices, starting with the fields. The practices mentioned above - the use of cover crops, legumes, intercrops, agroforestry, and the abandonment of tilling the soil - would have a triple effect: strengthening an organic and sustainable agri-food system, respecting biodiversity, allowing a balanced cohabitation between agriculture and the environment, while making farmers less dependent on large industrial companies"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:af9dd96c-4d30-4a93-9431-853dd63dbf04>"],"error":null}
{"question":"As a database developer, I need to understand the historical evolution: ¿cuál fue liberado primero, Delphi o PostgreSQL?","answer":"PostgreSQL (initially released as Postgres in 1989) was released before Delphi, which was first released in 1995. PostgreSQL began as an object-oriented DBMS and gradually enhanced with SQL standards, while Delphi was released as an Object Pascal programming language and RAD development environment with database access capabilities.","context":["The (then) Borland (today: Embarcadero) Delphi programming language & RAD development environment has been accessing databases with objects since at least 1995.\nDelphi is both the name of an the \"Object Pascal\" programming language and a RAD Object Oriented Integrated Development Environment, which since it's first version provided objects for interacting with tables, queries and or views in a relational databases.\nThe original 1995 version Delphi 1.0 client/server based objects (TTable, TQuery, TDataset, TDatabase, TSession) serves as sufficient prior art to for the claims in question, however the \"Midas\" (now \"Datasnap\") multi-tier technology that was released around 1997 with the advent of Delphi 3.0 provides additional prior art should there be quibbles with the former.\nIn particular, I submit the following comments in response to the claims above:\n- A method for enabling an object oriented user application to access a relational database having one or more physical tables segmented into rows and columns, comprising:\nGenerally Delphi's database components since version 1.0 has had this exact goal, of allowing an object oriented user application to access a relational database. In particular, the TTable (1 physical table or view) or TQuery (1 or more tables or views) are therefore 2 concrete classes providing access to an underlying relational database and therefore direct prior art.\ndefining a logical table comprising a subset of columns from at least\none of the one or more physical tables;\nBoth TQuery and TTable instances define logical tables for use in a Delphi application and so serve as prior art here as well.\ndesignating one column of the\nlogical table as a logical primary key column;\nFirst note that in general, keys need not be single column, so the claim here is oddly limiting for something that is supposed to be for general use and expanding the state of the art.\nNonetheless, Delphi supports several ways of defining both single and multi-part keys for its data access objects that serves as direct prior art here.\n1) TField objects (which are contained in TQuery/TTable etc) support (at least) the TField.ProviderFlags property (e.g. property \"pfInKey\") which allows one to specify that a particular TField is a key field for the containing logical table object.\n2) TTable components (amongst others) can additionally store both field and index definitions which imply whether a field is part of a logical primary key or not: TTable.IndexDefs for example contains a list of index definitions, each index definition has several properties, including the list of fields making up the index, and properties to specify various aspects of the index, including whether it defines a primary key (Index.Options include enumerated value ixPrimary)\n3) Midas TClientDataset similarly is a completely logical dataset class that allows a user to define field, field properties (including key specification) as well as having an IndexDefs property that can define logical indexes and primary key, and which can be used to access data from a back-end relational database.\nforming a normalized\nrelational schema object representing the logical table;\nThe term \"relational schema\" usually refers to a database schema, which usually includes multiple tables (or relations), so this statement referring to a singular \"logical table\" whilst simultaneously referring to a \"normalized relational schema\" seems on it's face confused and at best poorly worded.\nMaybe the author meant \"relation schema\", where \"relation\" is a formal name for what we would otherwise call a table, and the \"relation schema\" is then informally the description of the structure of the table (rows, columns) and so on.\nEither way, as mentioned before, any of the many TDataset descendant object included in Delphi, and in particular TTable, TQuery and TClientDataset serves as prior art as objects representing a logical table (relation) in a Delphi application.\nFor more on this apparent confusion of terminology, see:\nresponsive to the normalized relational schema object, one or more\nobject classes associated with the normalized relational schema\nPart of the work done by the Delphi environment is to generate instances of TQuery, TTable, TClientDataset and so on, and to populate these objects with the attendant field definitions, index definitions, keys and so on, or to allow the developer to specify these manually by hand if so desired.\nThese objects form part of a so called datamodule object, which itself is an instance of the TDatamodule class. Delphi allows one to define relationships between various \"TDataset\" objects (TDataset being the ancestor of several of the classes previously mentioned and so being a collective term for any Delphi application \"logical table\"), thereby establishing relationships between the logical objects and thus effectively a relational schema.\nThe TDatamodule then is prior art for the \"relational schema object\", and whether it is normalized or not depends on the developer and how he/she designs the logical tables in question, but suffice it to say that a normalized form is supported if desired.\nand employing an object of an object class including the one\nor more object classes associated with the normalized relational\nschema object and a respective corresponding logical primary key value\nto access data in the at least one of the physical tables in the\nAs a concrete prior art example: A TClientdataset class instance named \"cdsCustomer\", housed with other TClientDatasets (say \"cdsOrder\" with a master-detail relationship with cdsCustomer) in a TDatamodule instance, with a suitably defined primary key field & index on the \"CustomerId\" logical field (together with suitable supporting Midas backend) will allow the Delphi application containing this clientdataset to retrieve data in a back-end relational database table called say \"tblCustomers\", locate a specific CustomerID in the logical table, modify values in the logical table (without at that point having any link with the back-end) and eventually sync the update back to the database.\nIn summary, this claim describes nothing more than what Delphi's object access classes have already been providing since at least 1995.\nDelphi release dates:\nDelphi Developers guide (c) 1995 Borland:","DBMS > HyperSQL vs. PostgreSQL vs. Prometheus\nSystem Properties Comparison HyperSQL vs. PostgreSQL vs. Prometheus\nPlease select another system to include it in the comparison.\n|Editorial information provided by DB-Engines|\n|Name||HyperSQL also known as HSQLDB Xexclude from comparison||PostgreSQL Xexclude from comparison||Prometheus Xexclude from comparison|\n|Description||Multithreaded, transactional RDBMS written in Java also known as HSQLDB||Widely used open source RDBMS Developed as objectoriented DBMS (Postgres), gradually enhanced with 'standards' like SQL||Open-source TimeSeries DBMS and monitoring system|\n|Primary database model||Relational DBMS||Relational DBMS with object oriented extensions, e.g.: user defined types/functions and inheritance. Handling of key/value pairs with hstore module.||Time Series DBMS|\n|Secondary database models||Document store|\n|Developer||PostgreSQL Global Development Group www.postgresql.org/developer|\n|Initial release||2001||1989 1989: Postgres, 1996: PostgreSQL||2015|\n|Current release||2.7.0, July 2022||14.5, August 2022|\n|License Commercial or Open Source||Open Source based on BSD license||Open Source BSD||Open Source Apache 2.0|\n|Cloud-based only Only available as a cloud service||no||no||no|\n|DBaaS offerings (sponsored links) Database as a Service|\nProviders of DBaaS offerings, please contact us to be listed.\n|Server operating systems||All OS with a Java VM Embedded (into Java applications) and Client-Server operating modes||FreeBSD|\n|Typing predefined data types such as float or date||yes||yes||Numeric data only|\n|XML support Some form of processing data in XML format, e.g. support for XML data structures, and/or support for XPath, XQuery or XSLT.||no||yes specific XML-type available, but no XML query functionality.||no Import of XML data possible|\n|SQL Support of SQL||yes||yes standard with numerous extensions||no|\n|APIs and other access methods||HTTP API JDBC via HTTP|\nnative C library\nstreaming API for large objects\n|RESTful HTTP/JSON API|\n|Supported programming languages||All languages supporting JDBC/ODBC|\n|Server-side scripts Stored procedures||Java, SQL||user defined functions realized in proprietary language PL/pgSQL or with common languages like Perl, Python, Tcl etc.||no|\n|Partitioning methods Methods for storing different data on different nodes||none||partitioning by range, list and (since PostgreSQL 11) by hash||Sharding|\n|Replication methods Methods for redundantly storing data on multiple nodes||none||Source-replica replication other methods possible by using 3rd party extensions||yes by Federation|\n|MapReduce Offers an API for user-defined Map/Reduce methods||no||no||no|\n|Consistency concepts Methods to ensure consistency in a distributed system||Immediate Consistency||Immediate Consistency||none|\n|Foreign keys Referential integrity||yes||yes||no|\n|Transaction concepts Support to ensure data integrity after non-atomic manipulations of data||ACID||ACID||no|\n|Concurrency Support for concurrent manipulation of data||yes||yes||yes|\n|Durability Support for making data persistent||yes||yes||yes|\n|In-memory capabilities Is there an option to define some or all structures to be held in-memory only.||yes||no||no|\n|User concepts Access control||fine grained access rights according to SQL-standard||fine grained access rights according to SQL-standard||no|\nMore information provided by the system vendor\nWe invite representatives of system vendors to contact us for updating and extending the system information,\n|Related products and services|\n|3rd parties||Fujitsu Enterprise Postgres: An Enterprise Grade PostgreSQL with the flexibility of a hybrid cloud solution combined with industry leading security, availability and performance.\nDbVisualizer is equipping database professionals with the tools they need to build, manage and maintain state-of-the-art database technologies. It has sky-high user satisfaction and is the preferred tool for database professionals around the world. Customers include Tesla, Apple, Facebook, Deutsche Bank, NASA, and 25,000 others in 145 countries.\nInstaclustr: Fully Hosted & Managed PostgreSQL\nEDB: BigAnimal is fully-managed PostgreSQL in the cloud from the database fanatics at EDB.\nSQLFlow: Provides a visual representation of the overall flow of data. Automated SQL data lineage analysis across Databases, ETL, Business Intelligence, Cloud and Hadoop environments by parsing SQL Script and stored procedure.\nPercona: Database problems? Not on your watch.\nDatabases run better with Percona.\npgDash: In-Depth PostgreSQL Monitoring.\nThe State of PostgreSQL 2022 survey is now open!\nDatasentinel: Platform for Monitoring and Analyzing the performance problems of your PostgreSQL Clusters\nNavicat for PostgreSQL is an easy-to-use graphical tool for PostgreSQL database development.\nAiven for PostgreSQL: The familiar open source PostgreSQL® that you know and love, with no proprietary features or add-ons that will lock you in.\nCYBERTEC is your professional partner for PostgreSQL services and Data Science since 2000. With offices in Austria, Uruguay, Estonia, Poland, South Africa and Switzerland, CYBERTEC operates worldwide and is here for you 24/7!\nWe invite representatives of vendors of related products to contact us for presenting information about their offerings here.\n|HyperSQL also known as HSQLDB||PostgreSQL||Prometheus|\n|DB-Engines blog posts|\nSnowflake is the DBMS of the Year 2021\n|Recent citations in the news|\nOpen-source DBMS becoming battleground of public cloud\nprovided by Google News\nTimescale Releases Third State of PostgreSQL Report\nHow to Work With a PostgreSQL Database Using psql Commands - MUO\nInstall PostgreSQL pgAdmin 4 on Ubuntu 22.04 LTS Jammy Linux\nHow to return data from Postgresql pool in NodeJS?\nHow to Create a CRUD API Using Node, PostgresSQL, and Express - MUO\nprovided by Google News\nThe Great Grafana Mimir and Cortex Split\nConsider Grafana vs. Prometheus for your time-series tools\nThe Mechanics of Metrics: Aggregation across Dimensions\nQuery Optimization in the Prometheus World – The New Stack\nPrometheus vs Zabbix: Network monitoring tools comparison\nprovided by Google News\nBusiness Support Specialist\nPostgreSQL Database Administrator\nPostgreSQL Apps DBA\nSenior PostgreSQL Database Engineer - Modernization\nSenior Software Engineer - (Remote, EMEA)\nVice President, Software Engineer - Java\nStorage Software Engineer/Researcher, Infrastructure Monitoring and Admin\nShare this page"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:6ae34e3a-ca2e-4041-a159-9c23ec4c079e>","<urn:uuid:a287ddeb-f50b-4b55-8df2-0e2d022785d7>"],"error":null}
{"question":"How did ancient Greek musical instruments like the flute compare to early reed instruments from other civilizations, particularly regarding their social acceptance and cultural significance?","answer":"While flutes were widely used in Greek religious and civic ceremonies, they faced some cultural resistance. They were opposed in Greece due to their 'tantalizing effect' and association with Dionysian Mysteries and orgiastic cults, leading Plato to want them banned from his ideal republic. In contrast, early reed instruments had broader acceptance across different civilizations. For instance, in Egypt around 2700 B.C., they developed the zummara, a single-reed instrument with double bore, and in India, they had the pungi or magudi, a double-clarinet with an enclosed reed. Single reed instruments were found in many cultures worldwide as they provided a simple way of producing sound, with early mentions appearing in French dictionaries in the 16th century.","context":["GREECE. MUSICAL INSTRUMENTS. LYRAS AND FLUTES.\nThe lyre and the flute were the most popular musical instruments of the Greeks. Although the invention of the former is attributed to Hermes, who is said to have spanned the accidentally found shell of a dried up tortoise with its skin stripes, the lyre is of Asian origin as well as the flute. However, while the former quickly settled in Greece, the latter was opposed to its introduction because it had a tantalizing effect.\nShe was used in Dionysian Mysteries and other orgiastic cults, which is also indicated by the legend that the satyr Marsyas picked up the flute thrown away by Athena, whose facial features had been distorted during its playing, and challenged Apollo to a competition with Kithara. That is why Plato wanted the flute banished from his ideal republic.\nEven in the historical period that the monuments tell us about, the back and chest shell of the turtle was used to make the soundboard. “In the natural openings of this shell, from which the forelegs protrude, the winding horns of the goat were fixed with their root ends, which were connected by a yoke near their tips. The strings were stretched over this frame in the following way: a bridge was fastened to the breastplate of the turtle shell, over which the strings, which were fastened somewhat lower in the sound box by means of knots, continued to the yoke and were here either simply looped around or kept in tension by means of swivels”.\nLater the arms of the lyre were made of wood, ivory, metal and other materials. But the form of these remained the tortuous one of the earlier animal horns. The strings were made of sheep intestine, flax threads and later metal wires. When the lyricist sat, he put the instrument on his knees; when he stood, he wore it on a strap hung around his neck. The number of strings went up to twenty and forty. Magadis, who came from Lydia, was a lyre with twenty strings and two full octaves. It is mentioned by Anakreon, while Sappho is said to have used the pectis.\nA special kind of lyres, which is handled in vase pictures especially by Sappho and Alkaios, is designed as follows: “From the soundbox, which is formed by a small turtle bowl, two straight wooden arms rise in diverging directions, which bend against each other at their upper ends, where the yoke connects them”. Archaeologists believe that this lyre contains the barbitone, a deep-sounding instrument that Terpandes from Lydia is said to have introduced to Greece. Another type of lyre was the epigoneion, which had forty sides. Magadis and Epigoneion were played with both hands. So the plectron was not necessary.\nAccording to Pollux, the plectron with which the strings were struck originally consisted of the nail of an animal’s claw or horn, usually the goat. Then it was made of precious fabrics, especially ivory. In former times it must have been very heavy, because the legend says that Hercules killed his teacher, the singer Linos, with it.\nOver time, the lyras became more and more precious and richer in artistic decoration, for which vase pictures and reliefs provide us with numerous examples. Lucian tells us that a certain Euangelos of Taranto appeared in the pythic games with a lyre made of the finest gold and decorated with rings, precious stones and beautiful sculptures representing Apollo, the Muses and Orpheus.\nThe flutes were used at religious and bourgeois festivals and ceremonies. Their different forms, which were the same in Romans and Greeks, are depicted on the panel with the violin under no. 1, 2, 3, 4, 7 10. When blowing the double flute, both Roman and Greek flute players made use of a leather cheek and lip bandage, “through the metal mouthpiece of which the mouthpieces of the double clarinet were inserted. The purpose of this bandage was to prevent excessive breathing during blowing, which would have made it impossible to produce softer sounds.”\nNo. 2: Wandering musician with double flute and lyre suspended from the walking stick.\nNo. 13: Zither player. The object she is holding in her right hand seems to be a reserve string.\nNo. 1: Ordinary form of the lyre.\nNo. 3: A stringed instrument similar to a guitar.\nNo. 4 and 6. The syrinx or panpipe made of reed. It contained seven to nine, exceptionally also eleven longer and shorter pieces of reed, which were glued together with wax and fastened even better to each other by crosspieces.\nNo. 5: Zither player with the plectron in her right hand.\nNo. 7, 10, 11, 12, 14-21. Various forms of lyre. No. 11 is a two-sided instrument. No. 12 tensions the strings. No. 14 shows the carrying strap, No. 15 how the plectron was fastened when it was not needed. In 16, 17, 9 the plectrons are added.\nNo. 8: Trigonttin or harp.\nNo. 9. Eleven-stringed lyre after a mural from Herculaneum. The ends of the strings are loose, not fastened as in No. 21. The tubes of a syrinx are attached to the bridge connecting the two horns at the top.\n(After Willemin, Costumes de l’Antiquiti. Guhl and Koner, the lives of the Greeks and Romans.)\nSource: History of the costume in chronological development by Auguste Racinet. Edited by Adolf Rosenberg. Berlin 1888.","==The History of the Clarinet== About 2700 B.C., the Egyptians created an instrument called the zummara (sometimes referred to as the memet). The zummara was a single-reed instrument, much like the modern day clarinet, but it had a double bore like the double-reeded Greek instrument aulos. The zummara’s two pipes were parallel so that with each finger the player covered two holes, one on each pipe. The pipes were said to be out of tune with each other and produced a very dissonant beating sound.\nInvented around 1690, the clarinet is a single-reed woodwind instrument with a cylindrical tube. The clarinet evolved from an earlier instrument called the chalumeau, the first true single reed instrument. Johann Christoph Denner of Nuremberg with the help of his son Jacob improved the chalumeau, creating a new instrument called the clarinet. Denner added two keys to the chalumeau and increased that instruments range by over two octaves. He also created a better mouthpiece and improved the bell (end) of the instrument. In India, there was an instrument people now call the double-clarinet but in India it was called the pungi or the magudi. The difference between this instrument and the zummara was that the reed was enclosed in a wooden chamber and the left-hand pipe was drone, and the right-hand pipe was melodic.\nSingle reed instruments of this type have been found in many cultures throughout the world, as it is a simple way of producing sound. The reeds are called idioglot reeds and are cut out of part of the instrument that is placed inside the mouth to sound. A simple way of reconstructing one of these idioglot reeds is to cut a small triangular slit in a straw, when this is placed in the mouth it produces a buzzing sound. Instruments with these idioglot reeds are first mentioned in dictionaries in France in the Sixteenth century (for example Estienne (1511)) and are described in more detail in the Seventeenth Century treatises of Mersenne and Trichet.\nThe man universally credited for actually inventing, or making, the clarinet was Johann Christoph Denner (1655-1707) with the help of his son, Jacob, of Nuremberg, Germany. J.C. Denner was well-known and well-respected for the high quality woodwind instruments he made. Denner was said to have a creative mind; he toyed with the instruments he so finely crafted and it would seem the clarinet was the result of such tinkering. There is no documented proof that Denner alone developed the clarinet since two of his contemporaries, Klenig and Oberlender, also made clarinets.\nHowever, it must also be mentioned that (apart from a single instrument at Berkley whose attribution is much disputed) there are no extant clarinets by J.C.Denner, only chalumeaux.\nScholars first believed the clarinet was developed around 1690, but further research has lead scholars and music historians to believe it was developed around 1701-1704. It is also believed the clarinet was first called a mock trumpet. There is a music book discovered by a scholar, Thurston Dart, for mock trumpet which was published in 1698, and this was followed by three similar volumes throughout the next decade. Doubt still remains about who made the clarinet and how.\nThe accepted hypothesis is that Denner crafted the clarinet, and he did so sometime around 1700. J.G. Doppelmayr, a contemporary of Denner, wrote a Report on the Mathematicians and Artisans of Nuremberg in 1730, twenty-three years after Denner’s death. The report indicates Denner developed the clarinet a little after 1700. Before this historical text was discovered, historians and scholars only had a piece written in 1778 by C.G. Murr, \"Description of the Distinguished Features of Nuremberg,\" wherein Murr wrote that Denner created the clarinet in 1690.\nWhat made the clarinet a clarinet was Denner's great improvement to the old chalumeau. The usual material used for early examples of both instruments is boxwood (a common material in instrument making) with a heteroglot reed (that is separate to the instrument)tied to the upper side of the mouthpiece therefore vibrated by the upper lip . To change the chalumeau to a clarinet he added a 'speaker key' (also known as the register key) causing the instrument to over-blow , creating a new and higher register for the instrument. This register is known as the clarino register (a reference to a style of trumpet playing) and is thought to be the origin of the name of the instrument. Many modern clarinettists still refer to the overblown register of the clarinet as the clarino register, and to the lower register as the chalumeau.\nAcoustically the clarinet acts like a closed cylindrical tube and overblows at the twelfth. He also equipped it with a bell by enlarging the bore.The mouthpiece and barrel joint are made in one piece and, with Denner's additional key the earliest clarinets are two-keyed instruments. Extant chalumeaux have single keys at the front of the instrument.\nThe Clarinet's Early DaysEdit\nThere is no mention of the name clarinet until 1690, right after Denner made the first playable clarinet. In this year, the Graf (Duke) of Gronsfeld in Nuremberg ordered two clarinets from Jacob Denner for the use of his musicians. In 1712, four clarinets that were made out of boxwood were bought by the Nuremberg Town band (Ratsmusik). Since Denner worked in Nuremberg and that’s where they first appeared in the band, it is said that Denner gave the instrument its name.\nHistory of the clarinet from 1680-1751Edit\nThe history of the clarinet is a long history beginning in 1690. During that year a man named Johann Cristoph Denner invented the clarinet. These clarinets only had two keys that were mostly made from brass along with the springs. The clarinet however was made from boxwood, plum, ebony, ivory or pear. Around 1700 he added the register key to his instrument. In 1740 a third key was added ableing the players to play low E. Nine years later a man named Rameau uses the clarinet in his opera in Paris. Shortly after in 1750 Barthold Fritz added the 4th & 5th key to the clarinet. The clarinet is introduced to London by Bach in 1751."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:40b5e587-7d2e-4845-bf1c-4964b5f6e93d>","<urn:uuid:18e3ead8-de03-4399-9d22-c48064fe9813>"],"error":null}
{"question":"Looking to understand habitat preservation methods: what are the best practices for minimizing human impact on natural camping areas, and how does this relate to protecting aquatic ecosystems?","answer":"For minimizing human impact while camping, key practices include planning ahead to avoid high-use periods, camping on durable surfaces, using established campsites, properly disposing of waste, and keeping group sizes small. Campers should also minimize campfire impacts by using only designated fire rings and burning wood completely to ash. These practices help protect both terrestrial and aquatic ecosystems, as human activities near water bodies can impact fish habitats through increased sedimentation, nutrient loading, and pollution. The removal of riparian vegetation and disturbance of riverbanks can particularly affect aquatic habitats, leading to changes in water quality and fish community composition.","context":["The seven “Leave No Trace” (LNT) concepts have been formed by the Go away No Trace group as a set of tips intended to teach campers and secure the environment.\nJust one of the tenets of outdoor recreation—camping, specifically—is the plan of making the most of the purely natural planet although minimizing effect as much as probable. For long run generations to be ready to take pleasure in spaces that we use right now, and for those purely natural spaces to prosper, it is very important that we get the job done towards defending and preserving wilderness.\nThe Go away No Trace concepts are not new awareness in actuality, any one who has used time in the outdoors will take into account the the greater part of these to be widespread sense. For the professional outdoor fanatic, the tips are a useful reminder of the duty we each individual have for newcomers, they are a road map to making the most of the wild responsibly and respectfully.\nSystem in advance and put together: Helps make sense, suitable? Though many of us take into account organizing in advance in an introspective trend (packing clothes, foodstuff and supplies that you will be working with), it’s also essential to take into account the other side of the tenting equation: the area you are touring to. Are there constraints or rules you should really know about? Climate conditions pertinent to the area?\nThe Go away No Trace concepts propose that, when organizing for a vacation, a person should really foresee squander-disposal units, maintain teams to a minimum amount to cut down effect on the environment and attempt to schedule your take a look at to prevent times of higher use.\nJourney and camp on strong surfaces: According to the LNT internet site, the most effective campsites are found, not produced. Stick to spots that are previously recognized for setting up camp, and prevent altering present campsites.\nDispose of squander properly: What you convey in, you must acquire out. This applies to squander as effectively as supplies. Convey trash receptacles, pack out rest room paper and other cleanliness products and make sure you handle human squander properly.\nGo away what you obtain: You may possibly have stumbled upon the most great wildflower ever, but don’t decide it. Normal environments continue to be purely natural only if we retain their harmony. This goes for historical objects as effectively.\nReduce campfire impacts: Campfires, when mismanaged, can have critical, lasting effects on the purely natural environment. To minimize harm, gentle fires only exactly where permitted, in hearth rings and mounds, and maintain blazes little. Make sure to burn off wood all the way to ash and scatter neat ashes.\nRegard wildlife: The LNT internet site cautions not to technique wildlife, or to follow it. Observe nearby creatures from a distance, and do not feed animals.\nIf you convey your pet on a vacation, make sure you command it at all times. If that cannot be finished, it is greater to go away your companion at home.\nBe considerate of other visitors: In most scenarios, there will be other campers close to as you embark on your vacation. Be courteous when you come across them on the path, and maintain loud noises to a minimum amount.\nFor entry to exceptional gear films, celebrity interviews, and much more, subscribe on YouTube!","Effects on aquatic habitat and fish\nStudy unit, which have an associated effect on fish community composition effects of water quality and habitat on composition of fish communities in the upper colorado river basin —jeffrey r deacon and scott v mize natural and human factors in the upper colorado river basin result in differences in water quality and fish habitat. The effects of chanellization and channel restoration on aquatic habitat fish assemblages and aquatic habitat of chanellization and channel restoration on. The effects of the flood on aquatic habitats in the delta provided insight for interpreting fish community trends main channels, side channels, and backwaters became narrower, smaller, and less numerous after the flood, whereas sandbar occurrence increased. These threats impact mangroves, seagrass, saltmarsh and coastal lagoon communities which serve as refuge to marine species land clearing, dredging, agriculture, reclamation and estuarial developments as well as removal of riparian vegetation, increased sediments, nutrients and pollutants in riverbanks and beds and the removal of organic. Effects of water-level fluctuations on the fisheries of we were able to evaluate the effects of low water level on fish but assessing effects of water. Drought’s impact on fish and wildlife warm water temperatures also increase predation on juvenile salmon and steelhead by warm water fish species. This special issue of the journal water will be devoted to “the effects of aquatic habitat restoration or degradation on fish production” aquatic habitats worldwide are in a state of flux sea level is rising, the oceans are becoming more acidic and patterns of precipitation are changing dramatically. Potential effects of sediment on fish and their habitat it also refers to criteria and guidelines that were developed for the protection of aquatic resources from sediment.\nOther aquatic vertebrates which eat fishes (mainly reptiles, amphibians and birds) may also disappear image: preparing land for cultivation after the forest was burnt this practice in northeastern india leads to soil erosion and siltation in headwaters of major rivers, thus destroying fish habitats photo: waikhom vishwanath. Involving phytoplankton, zooplankton, insects, freshwater molluscs, and fish at each trophic level, excessive concentration of clay can cause direct effects (mortality, reduced physiological function, and habitat alienation) and indirect effects (decreased rates of growth, reproduction and recruitment) linked to reduced food supply. Effects of stream acidification and habitat on fish populations of a north american river barry p baldigo and gregory b lawrence us geological survey, water resources division, 425 jordan road, troy, ny 12180, usa. Jacob wittman will present his honors presentation on thursday (4/14) from 4 pm in 370 kottman hallhis presentation is effects of aquatic habitat degradation on hybridization between bluegill and green sunfish. Effects of multiple low-head dams on fish, macroinvertebrates, habitat abstract—we examined the effects of low-head dams on aquatic biota, habitat.\nFish assemblages, habitat conditions and changes in aquatic effects of livestock on fish assemblages can be exacerbated. Effects of suction dredge mining on oregon fishes and aquatic habitats, supplemental information oregon chapter american fisheries society march 2015. About half of fishes live in freshwater terrestrial to assess the effects of angling-induced and among separate bodies of terrestrial aquatic. 10 background of the study in new south wales, the department of primary industries (dpi) is responsible for execution of the fisheries management act 1994 as such, dpi functions to maintain and protect aquatic habitats through sound and effective habitat.\nIssue indicator #1 ‐ sediment effects on aquatic habitat and biota iron creek contains fish because the type of aquatic habitat required for. Aquatic habitats and species the blunt face and bluenose shiners and the effects of habitat changes on their concerns regarding fish and wildlife habitat.\nEffects on aquatic habitat and fish\nAquatic habitat is a unique investigation designed to study small fresh water fish including the medaka the effects of radiation and the biological.\n- Climate change is one of the most critical long-term threats to fish population and habitat population resilience aquatic habitat effects of fire on habitat.\n- Urban development results in multiple stressors that can degrade aquatic ecosystems by altering the hydrology, habitat, and chemistry of streams results of the usgs investigation of the effects of urbanization on stream ecosystems (euse) found that no single environmental factor was universally.\n- Effects of changing climate on aquatic habitat and connectivity ecology and evolution frog observation data were compiled by the us fish and.\n- Habitat management aquatic habitats if we want to catch fish and to enjoy our natural environment we must all look after our aquatic habitats.\nFish and wildlife response to farm bill conservation practices 83 effects of conservation practices on aquatic habitats aquatic habitat conditions. Scientific data leads the way the refuge, and its partners, assess, manage, and monitor aquatic habitat conditions and distribution of fish species, including biological characteristics develop management activities to protect and restore habitats and assess effects of implementing aquatic habitat management activities at the. The effect of fish and aquatic habitat complexity on amphibians the effects of fish on assemblages of effects of habitat complexity and predator identity on. Aquatic habitats and species effects on the aquatic and terrestrial populations that depend on them will be project impacts on essential fish habitat. Effects of multiple low-head dams on fish, macroinvertebrates, habitat, and water quality in the fox river, illinois can have dramatic effects on rivers and aquatic."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:365597f5-24b2-40f2-8ebd-3674d44ca87a>","<urn:uuid:5361d42f-af05-4487-bef7-bd8004870068>"],"error":null}
{"question":"Could you compare the data collection methods used to assess learning in museums with the assessment approaches used in 7th grade mathematics?","answer":"Museum learning assessment uses various data collection methods including audio/video recording (requiring special consent and technical solutions for ambient noise), time-based measures (holding time), and group interviews that capture natural social interactions. In contrast, 7th grade mathematics assessment focuses on measuring specific competencies through structured evaluation of students' ability to solve problems involving ratios, proportions, geometric concepts and rational numbers. While museum assessment often deals with the complexity of capturing informal group learning experiences and relies heavily on observational data, mathematical assessment in grade 7 uses more standardized approaches to measure individual student mastery of clearly defined skills and concepts as outlined in the core standards.","context":["Below is the uncorrected machine-read text of this chapter, intended to provide our own search engines and external engines with highly rich, chapter-representative searchable text of each book. Because it is UNCORRECTED material, please consider the following text as a useful but insufficient proxy for the authoritative book pages.\n322 APPENDIX B Some Technical Considerations in Assessment Interviewing Groups Versus Individuals Learners in informal environments, such as museums, generally partici- pate in multigenerational groups rather than as individuals, and these groups often move loosely through the environment, splitting and reforming as members make new discoveries and share what they are experiencing with one another. This makes interviewing particularly challenging. It is difficult to craft interview questions that are suitable for a broad range of people. Also, the learning experience varies frequently between an individual and group focus. There are trade-offs between interviewing individuals and groups: (a) Interviewing individuals has the advantage that participants do not influ- ence each otherâs opinions, that the resulting data are amenable to statistical methods with equal weighting for each person, and that the time taken to conduct an interview is relatively short. However, such interviews require selecting an individual interviewee (and often individuals prefer to self-nomi- nate rather than accept a random sampling method), locating the rest of the group to explain what is happening, ensuring that minors have appropriate child care, and finding a nearby yet quiet location to conduct the interview. Also, parents often respond to questions by reporting on what they think their children learned rather than what they themselves learned, because their childrenâs experience is often their framing reason for attending. Unless parentsâ interpretations of childrenâs experience are the focus of the study or the child is too young to be interviewed, this approach is problematic because it relies on indirect inferences rather than self-report.\nAppendix B 323 (b) Interviewing groups has the advantage of not separating group mem- bers, so families are more likely to agree to participate. Also, the responses they give, as a group, reflect the actual learning when the group members were jointly engaged in activity. One disadvantage of group interviews is that one member may dominate (typically an adult), and group members will often fall into agreement with each otherâs opinions. A way to reduce this tendency is to question members individually but in inverse order of status. However, some researchers feel that unequal power dynamics are likely to be representative of the learning dynamic, and therefore interviews with asymmetrical participation are authentic. Group interviews also pres- ent the problem of how to quantify data from groups of different sizes, particularly if the study attempts to characterize frequencies of responses. Some researchers code response frequencies into 3 categories: â1,â â2,â and âmany.â Finally, although group interviews are more relaxing for partici- pants, there is rarely time to ask all questions equitably before the group becomes restless, so most persons in the group typically do not complete the interview. Alternatively, interviews conducted from a more qualitative or naturalistic perspective may allow for a much looser participation structure by the group members, but they require extended and careful analysis by the researcher afterward. Control Groups Because informal environments emphasize learning by choice, using random assignment of learners to treatment and control groups may some- times be logistically impossible, upsetting to the learners, threatening to the study validity, or all of the above. In such cases, it may be desirable to refer- ence a comparison group that is not a strict control but that provides some sense of plausible baseline behavior (data from visitors to other museums or exhibitions, literature that cites common knowledge, behaviors, or attitudes to a topic, etc.). Video- and Audiotaping With increasing interest in such process-based outcomes as engagement, conversations, and actions, research in informal environments has made increasing use of recording systems, such as audio- and videotape. These raise technical and ethical issues. Technically, the main challenge is often to obtain audio of sufficiently high quality to hear what people are saying above the ambient noise. Attempts at solution include using a Dictaphone (Borun, Chambers, and Cleghorn, 1996), wearing of cordless microphones (e.g., Leinhardt and Knutson, 2004), or placement of microphones on individual exhibits (e.g., Gutwill, 2003). The ethical issues, namely the need to have visitors give informed consent to being recorded, have been addressed by posting signs, augmenting posting-signs (Gutwill, 2003), asking for consent\n324 Learning Science in Informal Environments when visitors arrive and placing a sticker on their clothing to alert the videog- rapher (Crowley and Callanan, 1998), or getting explicit consent as visitors enter a space. Such methods are generally compromises, and researchers should always refer to their local institutional review board for approval of their specific data collection method. Time as a Measure of Learning In environments such as museums, botanical gardens, and zoos, where learners move freely through a physical space of options, time spent (âholding timeâ or âdwell timeâ) is a commonly used measure of impact in summa- tive evaluations. At the same time, there is controversy about what exactly it assesses in relation to learning. There are various approaches to thinking about time, including: (a) Some researchers regard it as a necessary but not sufficient condition for learning. In this view, learners need to pause and engage with objects, people, or activities in order to have a chance to learn from them, but learn- ing is not necessarily linearly related to time spent. Some researchers have interpreted histograms of holding time as bimodal or multimodal, reveal- ing different audience characteristics in terms of background or motivation (browsers, grazers, etc.), but these are controversial: most exhibitions show a single peak at the short end of the spectrum of time spent (Serrell, 1998, 2001). (b) Some regard it as an indicator of learning, using the well-established principle that time on task is the most universal correlate with learning across contexts. However, the meaning of âon taskâ is particularly am- biguous in free-choice environments (Shettel, 1997), as is the definition of learning. A few studies have shown direct evidence that time spent in exhibitions correlates with learning, as measured by previsit questionnaires on the exhibit topic (Abler, 1968) or free recall of objects seen (Barnard and Loomis, 1994). (c) Some regard time spent as a direct measure of learning, defined as engagement in socially sanctioned collaborative activity. From this socio- cultural perspective, participants are learning throughout their engagement, although the exact nature of what they learn may be quite different from institutional expectations. Internet Surveys Increasingly, the Internet is being used to conduct surveys of learners. These may be assessments of online resources or may ask about previous experiences in another setting (such as a museum visit, viewing of a TV series, etc.). They may be contained within emails, or, increasingly, be web-\nAppendix B 325 based. Compared with paper surveys, Internet surveys are inexpensive and generate quick responses, but often raise concerns about response rates and biased populations of respondents. For recent reviews of the literature on web surveys in informal science learning environments, including suggestions for effective design and usage, see Parsons (2007), Yalowitz and Ferguson (2007), and Storksdieck (2007). REFERENCES Abler, T.S. (1968). Traffic patterns and exhibit design: A study of learning in the museum. In S.F. DeBorhegyi and I. Hanson (Eds.), The museum visitor (vol. 3, pp. 103-141). Milwaukee, WI: Milwaukee Public Museum. Barnard, W.A., and Loomis, R.J. (1994). The museum exhibit as a visual learning museum. Visitor Behavior, 9 (2), 14-17. Borun, M., Chambers, M., and Cleghorn, A. (1996). Families are learning in science museums. Curator, 39 (2), 123-138. Crowley, K., and Callanan, M. (1998). Describing and supporting collaborative scientific thinking in parent-child interactions. Journal of Museum Education, 17 (1), 12-17. Gutwill, J. (2003). Gaining visitor consent for research II: Improving the posted sign method. Curator, 46 (2), 228-235. Leinhardt, G., and Knutson, K. (2004). Listening in on museum conversations. Walnut Creek, CA: AltaMira Press. Parsons, C. (2007). Web-based surveys: Best practices based on the research literature. Visitor Studies, 10 (1), 13-33. Serrell, B. (1998). Paying attention: Visitors and museum exhibitions. Washington, DC: American Association of Museums. Serrell, B. (2001). In search of the elusive bimodal distribution. Visitor Studies Today, 4 (2), 4-9. Shettel, H.H. (1997). Timeâis it really of the essence? Curator, 40, 246-249. Storksdieck, M. (2007). Using web surveys in early front-end evaluations with open populations: A case study of amateur astronomers. Visitor Studies, 10 (1), 47-54. Yalowitz, S., and Ferguson, A. (2007). Using web surveys in summative evaluations: A case study at the Monterey Bay Aquarium. Visitor Studies, 10 (1), 34-46.","Mathematics Grade 7\n(1) Students extend their understanding of ratios and develop understanding of proportionality to solve single- and multi-step problems. Students use their understanding of ratios and proportionality to solve a wide variety of percent problems, including those involving discounts, interest, taxes, tips, and percent increase or decrease. Students solve problems about scale drawings by relating corresponding lengths between the objects or by using the fact that relationships of lengths within an object are preserved in similar objects. Students graph proportional relationships and understand the unit rate informally as a measure of the steepness of the related line, called the slope. They distinguish proportional relationships from other relationships.\n(2) Students develop a unified understanding of number, recognizing fractions, decimals (that have a finite or a repeating decimal representation), and percents as different representations of rational numbers. Students extend addition, subtraction, multiplication, and division to all rational numbers, maintaining the properties of operations and the relationships between addition and subtraction, and multiplication and division. By applying these properties, and by viewing negative numbers in terms of everyday contexts (e.g., amounts owed or temperatures below zero), students explain and interpret the rules for adding, subtracting, multiplying, and dividing with negative numbers. They use the arithmetic of rational numbers as they formulate expressions and equations in one variable and use these equations to solve problems.\n(3) Students continue their work with area from Grade 6, solving problems involving the area and circumference of a circle and surface area of three-dimensional objects. In preparation for work on congruence and similarity in Grade 8 they reason about relationships among two-dimensional figures using scale drawings and informal geometric constructions, and they gain familiarity with the relationships between angles formed by intersecting lines. Students work with three-dimensional figures, relating them to two-dimensional figures by examining cross-sections. They solve real-world and mathematical problems involving area, surface area, and volume of two- and three-dimensional objects composed of triangles, quadrilaterals, polygons, cubes and right prisms.\n(4) Students build on their previous work with single data distributions to compare two data distributions and address questions about differences between populations. They begin informal work with random sampling to generate data sets and learn about the importance of representative samples for drawing inferences.\nCore Standards of the Course\nStrand: MATHEMATICAL PRACTICES (7.MP)\nThe Standards for Mathematical Practice in Seventh Grade describe mathematical habits of mind that teachers should seek to develop in their students. Students become mathematically proficient in engaging with mathematical content and concepts as they learn, experience, and apply these skills and attitudes (Standards 7.MP.1–8).\nMake sense of problems and persevere in solving them. Explain the meaning of a problem and look for entry points to its solution. Analyze givens, constraints, relationships, and goals. Make conjectures about the form and meaning of the solution, plan a solution pathway, and continually monitor progress asking, \"Does this make sense?\" Consider analogous problems, make connections between multiple representations, identify the correspondence between different approaches, look for trends, and transform algebraic expressions to highlight meaningful mathematics. Check answers to problems using a different method.\nReason abstractly and quantitatively. Make sense of the quantities and their relationships in problem situations. Translate between context and algebraic representations by contextualizing and decontextualizing quantitative relationships. This includes the ability to decontextualize a given situation, representing it algebraically and manipulating symbols fluently as well as the ability to contextualize algebraic representations to make sense of the problem.\nConstruct viable arguments and critique the reasoning of others. Understand and use stated assumptions, definitions, and previously established results in constructing arguments. Make conjectures and build a logical progression of statements to explore the truth of their conjectures. Justify conclusions and communicate them to others. Respond to the arguments of others by listening, asking clarifying questions, and critiquing the reasoning of others.\nModel with mathematics. Apply mathematics to solve problems arising in everyday life, society, and the workplace. Make assumptions and approximations, identifying important quantities to construct a mathematical model. Routinely interpret mathematical results in the context of the situation and reflect on whether the results make sense, possibly improving the model if it has not served its purpose.\nUse appropriate tools strategically. Consider the available tools and be sufficiently familiar with them to make sound decisions about when each tool might be helpful, recognizing both the insight to be gained as well as the limitations. Identify relevant external mathematical resources and use them to pose or solve problems. Use tools to explore and deepen their understanding of concepts.\nAttend to precision. Communicate precisely to others. Use explicit definitions in discussion with others and in their own reasoning. They state the meaning of the symbols they choose. Specify units of measure and label axes to clarify the correspondence with quantities in a problem. Calculate accurately and efficiently, express numerical answers with a degree of precision appropriate for the problem context.\nLook for and make use of structure. Look closely at mathematical relationships to identify the underlying structure by recognizing a simple structure within a more complicated structure. See complicated things, such as some algebraic expressions, as single objects or as being composed of several objects. For example, see 5 – 3(x – y)2 as 5 minus a positive number times a square and use that to realize that its value cannot be more than 5 for any real numbers x and y.\nLook for and express regularity in repeated reasoning. Notice if reasoning is repeated, and look for both generalizations and shortcuts. Evaluate the reasonableness of intermediate results by maintaining oversight of the process while attending to the details.\nCompute unit rates associated with ratios of fractions, including ratios of lengths, areas and other quantities measured in like or different units. For example, if a person walks 1/2 mile in each 1/4 hour, compute the unit rate as the complex fraction 1/2/1/4 miles per hour, equivalently 2 miles per hour.\n- Decide whether two quantities are in a proportional relationship, e.g., by testing for equivalent ratios in a table or graphing on a coordinate plane and observing whether the graph is a straight line through the origin.\n- Identify the constant of proportionality (unit rate) in tables, graphs, equations, diagrams, and verbal descriptions of proportional relationships.\n- Represent proportional relationships by equations. For example, if total cost t is proportional to the number n of items purchased at a constant price p, the relationship between the total cost and the number of items can be expressed as t = pn.\n- Explain what a point (x, y) on the graph of a proportional relationship means in terms of the situation, with special attention to the points (0, 0) and (1, r) where r is the unit rate.\nUse proportional relationships to solve multistep ratio and percent problems. Examples: simple interest, tax, markups and markdowns, gratuities and commissions, fees, percent increase and decrease, percent error.\nApply and extend previous understandings of addition and subtraction to add and subtract rational numbers; represent addition and subtraction on a horizontal or vertical number line diagram.\n- Describe situations in which opposite quantities combine to make 0. For example, a hydrogen atom has 0 charge because its two constituents are oppositely charged.\n- Understand p + q as the number located a distance |q | from p, in the positive or negative direction depending on whether q is positive or negative. Show that a number and its opposite have a sum of 0 (are additive inverses). Interpret sums of rational numbers by describing real-world contexts.\n- Understand subtraction of rational numbers as adding the additive inverse, p – q = p + (–q). Show that the distance between two rational numbers on the number line is the absolute value of their difference, and apply this principle in real-world contexts.\n- Apply properties of operations as strategies to add and subtract rational numbers.\n- Understand that multiplication is extended from fractions to rational numbers by requiring that operations continue to satisfy the properties of operations, particularly the distributive property, leading to products such as (–1)(–1) = 1 and the rules for multiplying signed numbers. Interpret products of rational numbers by describing real-world contexts.\n- Understand that integers can be divided, provided that the divisor is not zero, and every quotient of integers (with non-zero divisor) is a rational number. If p and q are integers, then –(p/q) = (–p)/q = p/(–q). Interpret quotients of rational numbers by describing real-world contexts.\n- Apply properties of operations as strategies to multiply and divide rational numbers.\n- Convert a rational number to a decimal using long division; know that the decimal form of a rational number terminates in 0s or eventually repeats.\nSolve real-world and mathematical problems involving the four operations with rational numbers. Computations with rational numbers extend the rules for manipulating fractions to complex fractions.\nStrand: EXPRESSIONS AND EQUATIONS (7.EE)\nUse properties of operations to generate equivalent expressions (Standards 7.EE.1–2). Solve real-life and mathematical problems using numerical and algebraic expressions and equations (Standards 7.EE.3–4).\nUnderstand that rewriting an expression in different forms in a problem context can shed light on the problem and how the quantities in it are related. For example, a + 0.05a = 1.05a means that “increase by 5%” is the same as “multiply by 1.05.”\nSolve multi-step real-life and mathematical problems posed with positive and negative rational numbers in any form (whole numbers, fractions, and decimals), using tools strategically. Apply properties of operations to calculate with numbers in any form; convert between forms as appropriate; and assess the reasonableness of answers using mental computation and estimation strategies. For example: If a woman making $25 an hour gets a 10% raise, she will make an additional 1/10 of her salary an hour, or $2.50, for a new salary of $27.50. If you want to place a towel bar 9 3/4 inches long in the center of a door that is 27 1/2 inches wide, you will need to place the bar about 9 inches from each edge; this estimate can be used as a check on the exact computation.\n- Solve word problems leading to equations of the form px + q = r and p(x + q) = r, where p, q, and r are specific rational numbers. Solve equations of these forms fluently. Compare an algebraic solution to an arithmetic solution, identifying the sequence of the operations used in each approach. For example, the perimeter of a rectangle is 54 cm. Its length is 6 cm. What is its width?\n- Solve word problems leading to inequalities of the form px + q > r or px + q < r, where p, q, and r are specific rational numbers. Graph the solution set of the inequality and interpret it in the context of the problem. For example: As a salesperson, you are paid $50 per week plus $3 per sale. This week you want your pay to be at least $100. Write an inequality for the number of sales you need to make, and describe the solutions.\nStrand: GEOMETRY (7.G)\nDraw, construct, and describe geometrical figures, and describe the relationships between them (Standards 7.G.1–3). Solve real-life and mathematical problems involving angle measure, area, surface area, and volume (Standards 7.G.4–6).\nDraw (freehand, with ruler and protractor, and with technology) geometric shapes with given conditions. Focus on constructing triangles from three measures of angles or sides, noticing when the conditions determine a unique triangle, more than one triangle, or no triangle.\nKnow the formulas for the area and circumference of a circle and use them to solve problems; give an informal derivation of the relationship between the circumference and area of a circle.\nSolve real-world and mathematical problems involving area, volume and surface area of two- and three-dimensional objects composed of triangles, quadrilaterals, polygons, cubes, and right prisms.\nStrand: STATISTICS AND PROBABILITY (7.SP)\nUse random sampling to draw inferences about a population (Standards 7.SP.1–2). Draw informal comparative inferences about two populations (Standards 7.SP.3–4). Investigate chance processes and develop, use, and evaluate probability models (Standards 7.SP.5–8).\nUnderstand that statistics can be used to gain information about a population by examining a sample of the population; generalizations about a population from a sample are valid only if the sample is representative of that population. Understand that random sampling is more likely to produce representative samples and support valid inferences.\nUse data from a random sample to draw inferences about a population with an unknown characteristic of interest. Generate multiple samples (or simulated samples) of the same size to gauge the variation in estimates or predictions. For example, estimate the mean word length in a book by randomly sampling words from the book; predict the winner of a school election based on randomly sampled survey data. Gauge how far off the estimate or prediction might be.\nInformally assess the degree of visual overlap of two numerical data distributions with similar variabilities, measuring the difference between the centers by expressing it as a multiple of a measure of variability. For example, the mean height of players on the basketball team is 10 cm greater than the mean height of players on the soccer team, approximately twice the variability (mean absolute deviation) on either team; on a dot plot, the separation between the two distributions of heights is noticeable.\nUse measures of center and measures of variability for numerical data from random samples to draw informal comparative inferences about two populations. For example, decide whether the words in a chapter of a seventh-grade science book are generally longer than the words in a chapter of a fourth-grade science book.\nUnderstand that the probability of a chance event is a number between 0 and 1 that expresses the likelihood of the event occurring. Larger numbers indicate greater likelihood. A probability near 0 indicates an unlikely event, a probability around 1/2 indicates an event that is neither unlikely nor likely, and a probability near 1 indicates a likely event.\nApproximate the probability of a chance event by collecting data on the chance process that produces it and observing its long-run relative frequency, and predict the approximate relative frequency given the probability. For example, when rolling a number cube 600 times, predict that a 3 or 6 would be rolled roughly 200 times, but probably not exactly 200 times.\nDevelop a probability model and use it to find probabilities of events. Compare probabilities from a model to observed frequencies; if the agreement is not good, explain possible sources of the discrepancy.\n- Develop a uniform probability model by assigning equal probability to all outcomes, and use the model to determine probabilities of events. For example, if a student is selected at random from a class, find the probability that Jane will be selected and the probability that a girl will be selected.\n- Develop a probability model (which may not be uniform) by observing frequencies in data generated from a chance process. For example, find the approximate probability that a spinning penny will land heads up or that a tossed paper cup will land open-end down. Do the outcomes for the spinning penny appear to be equally likely based on the observed frequencies?\n- Understand that, just as with simple events, the probability of a compound event is the fraction of outcomes in the sample space for which the compound event occurs.\n- Represent sample spaces for compound events using methods such as organized lists, tables and tree diagrams. For an event described in everyday language (e.g., “rolling double sixes”), identify the outcomes in the sample space which compose the event.\n- Design and use a simulation to generate frequencies for compound events. For example, use random digits as a simulation tool to approximate the answer to the question: If 40% of donors have type A blood, what is the probability that it will take at least 4 donors to find one with type A blood?\nhttp://www.uen.org - in partnership with Utah State Board of Education (USBE) and Utah System of Higher Education (USHE). Send questions or comments to USBE Specialist - Lindsey Henderson and see the Mathematics - Secondary website. For general questions about Utah's Core Standards contact the Director - Jennifer Throndsen. These materials have been produced by and for the teachers of the State of Utah. Copies of these materials may be freely reproduced for teacher and classroom use. When distributing these materials, credit should be given to Utah State Board of Education. These materials may not be published, in whole or part, or in any other format, without the written permission of the Utah State Board of Education, 250 East 500 South, PO Box 144200, Salt Lake City, Utah 84114-4200."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:199dd62a-7613-49cb-bc89-b80a9f8de85e>","<urn:uuid:10b27653-c303-47f9-acfd-5fbef73d63f2>"],"error":null}
{"question":"What are the benefits of oat-pea intercropping for farming, and what environmental limitations does it face compared to conventional farming?","answer":"Oat-pea intercropping offers several benefits including reduced fungicide and synthetic nitrogen use, higher quality grain samples, lower grain shipping costs, and improved soil health through increased biomass production. However, it faces environmental limitations compared to conventional farming because organic-style practices like this have lower per-acre yields than conventional methods, which could increase pressure for land-use conversion with negative biodiversity and greenhouse gas implications. Additionally, without effective herbicides, farmers often rely on tillage for weed control, which makes it difficult to implement environmentally beneficial no-till farming practices.","context":["Luke Struckman, PhD, Oat-Pea Intercropping Project Lead, South East Research Farm – Summer (June) 2021 Pulse Beat\nDuring the past decade, mixed grain intercropping has become a commercially significant practice for some farm operations on the Canadian and Northern U.S. prairies. Proven mixed grain intercrop combinations can provide significant agronomic and financial\nbenefits. In 2020, the South East Research Farm and General Mills conducted a study to investigate the commercial viability of the oat- pea intercrop combination. The study combined farmer interviews with on-farm oat-pea trials in order to provide starting points for farmers interested in adopting oat-pea intercropping.\nMIXED INTERCROPPING ON THE CANADIAN AND NORTHERN U.S. PRAIRIES\nIntercropping practices, such as interseeding wide-row grain corn with cover crops or small grain-soybean relay cropping, are being used successfully in some regions of North America. Similarly, mixed intercropping has been relatively common on the Canadian and Northern U.S. prairies through small grain-clover or pea-brassica combinations. However, seeding two or three grain, oilseed and\npulse cash crops at the same time, harvesting them together at the end of the growing season, separating the seed and marketing each cash crop for grain, is a relatively new practice. This is referred to as mixed grain intercropping.\nFarmers practicing mixed grain intercropping have reported significant agronomic benefits, such as reduced fungal and insect pressure, overyielding and increased post-harvest residue. Some proven mixed grain intercrop combinations include canola-pea and chickpea-flax.\nThe oat-pea intercrop combination is typically used for greenfeed or hay. It has not been common for the oat-pea combination to be grown for grain oat and dry pea production for human consumption. As the study findings demonstrate, the oat-pea mixed grain combination can be commercially viable thanks to the agronomic benefits it provides, in addition to the relatively strong market demand for both grain oats and dry peas in recent years.\nTo better understand current oat-pea intercropping practices, twenty-five interviews were conducted with farmers in Canada, the U.S. and the U.K. on their experiences growing oat-pea intercrops. Interview questions focused on production methods, grain separation, obstacles and profitability.\nThe vast majority of farmers stated that synthetic nitrogen applications can be reduced or eliminated in oat-pea intercrops. Fungicides were shown to not be necessary. Herbicide use was significantly reduced, but a number of growers mentioned that this was due to a lack of available in-season herbicide options.\nSeeding rates were informed by the experiences of other farmers and on-farm experimentation. There were a wide range of seeding rates provided, but growers tended to favour peas and reduce oat seeding rates to 60% or less of monocrop. Otherwise, oats tend to dominate peas as the growing season progresses. Several growers have selected lodge-prone pea varieties to intercrop because varieties such as 4010 forage peas or Austrian winter peas can be incredibly difficult to grow as monocrops, and oats help to keep them standing until harvest.\nGrain cleaning and separation are typically done on-farm using a wide range of cleaning equipment, although rotary cleaners tend to be the most common. Growers cite higher quality grain samples, lower grain shipping costs, as well as significantly lower dockage, as benefits to cleaning and separating oat-pea mixed grain on farm. Additionally, screenings can be kept or sold as livestock feed. Grain separation costs can be offset by higher quality grain samples in addition to reduced input costs.\nAlthough oat-pea intercropping does provide attractive benefits, there are significant obstacles to making oat-pea intercropping practical for cash crop production. Crop insurance was cited as one major obstacle to oat-pea intercropping (and mixed intercropping in general), since most insurance policies only allow for a limited acreage of novel cash crops each season. Weed control can be a serious issue since no herbicides are labelled for use with both crops in-season.\nOn-farm storage can be another obstacle. Mixed intercrops must be stored separately from monocrops. Oat-pea mixed grain takes up significantly more storage space than other mixed grain intercrops (such as canola-pea) due to the bulkiness of grain oats. Along the same lines, separation is more difficult in comparison to other intercrops due to the large size of oat and pea seeds and the possibility of peas splitting during cleaning and separation.\nMarketing can pose an additional barrier, since some grain oat buyers will not purchase oats that have had peas separated out of them due to potential allergen cross contamination. Finally, a lack of information — both from other farmers and published research on the oat-pea combination — can make it difficult to implement.\nDespite significant obstacles, oat-pea intercrops have the potential be more profitable than monocrop oats. This is due to a reduction in synthetic inputs, lower grain shipping costs, higher quality grain samples and the possibility of growing higher- value, lodge-prone pea varieties. At the same time, oat-pea intercrops provide benefits to farm cash crop rotations and soil health, such as producing large amounts of biomass, increasing cash crop diversity and helping to mitigate adverse weather conditions through combining two different cash crops that thrive in varying soil moisture conditions.\nTwelve on-farm oat-pea trial sites were located in Saskatchewan, Manitoba and North Dakota during the 2020 growing season (see Figure 1). The trials placed 10-acre oat-pea plots adjacent to 10-acre monocrop oat plots, allowing for side-by-side comparisons of oat-pea intercrop production with monocrop oat production at each site. Appreciable variation exists across soil types and precipitation levels in the broad geographic area covered by the trial sites.\nParticipating farmers chose seeding rates along with fertility and herbicide treatments. The non-replicated, single-year nature of these trials does not provide enough data to make agronomic recommendations for oat-pea intercropping. However, some key findings can provide starting points for farm operations interested in oat-pea intercropping for grain oat and dry pea production.\nParticipating farmers provided information on seeding rates, input rates and yield. Across the 12 sites, the oat-pea intercrop trials yielded 3561 lbs/ac, on average, while the oat monocrop trials yielded 3907 lbs/ac, on average. Importantly, increased seeding rates did not lead to a linear increase in yield across sites.\nGeneral Mills conducted grain quality tests comparing intercropped oats to monocrop oats. Oat samples were tested for percentage of oat plumps and protein content. While the oat-pea intercrop plots scored consistently higher with regards to percentage of oat plumps and oat protein content versus the oat monocrop trials, the differences were not significant.\nA cost/benefit analysis demonstrated that the oat-pea intercrop was more profitable than the oat monocrop at Deloraine, MB, Boissevain, MB and Melfort, SK. There were no significant differences in profitability when comparing oat-pea intercrop plots to oat monocrop plots at Arborg, MB, Noonan, ND and Sheho, SK. At the six remaining trial sites, oat-pea intercrop plots were significantly less profitable than oat monocrop plots.\nAs this study demonstrates, the oat-pea combination can be a viable means for improving farm profitability, increasing cash crop diversity and building soil health. At the same time, appropriate seeding rates, the lack of in-season herbicides, post- harvest storage and separation, and marketing, along with other issues, need to be given serious consideration. Growers interested in adopting the oat-pea combination should consult with experienced intercropping farmers and experiment at small scales on-farm for at least one year before adopting oat-pea intercropping at a production scale.\nThis project was a team effort among researchers from the South East Research Farm, General Mills and Agriculture and Agri-Food Canada.","The guiding principal of organic is to rely exclusively on natural inputs. That was decided early in the 20th century, decades before before the scientific disciplines of toxicology, environmental studies and climate science emerged to inform our understanding of how farming practices impact the environment.\nAs both farming and science have progressed, there are now several cutting edge agricultural practices which are good for the environment, but difficult or impossible for organic farmers to implement within the constraints of their pre-scientific rules.\nThere was one window during which the rules for organic might have been adjusted to reflect a more modern understanding. In 1990 the US Congress charged the USDA with the task of setting a national standard for what products could be legally sold as Organic. That agency was inclined to include more science in a definition of “what is safest for us and for the environment,” but the organic community of that day was adamant that the rule should only reflect the purely natural definition embraced by their existing customer base. Long before the final Organic Standards were published in 2002, it was clear that the industry preference had prevailed and that the rules of organic would still reflect their pre-scientific origins. That is why the following six environmental issues exist for organic farming.\n1. Less Than Optimal Fungicides\nOrganic farmers use pesticides, but only those qualified as sufficiently natural. Thus, copper-based fungicides are among the few options available to an organic grower for the control of fungal plant diseases. These are high-use rate products that require frequent re-application and which are quite toxic to aquatic invertebrates. There are much more effective, and far less toxic, synthetic fungicide options without environmental issues, and which, unlike copper, break down into completely innocuous materials. Organic growers can't use those fungicides. Similarly there are many environmentally benign, synthetic insecticides and herbicides which cannot be used.\n2. A Surprisingly High Carbon Footprint for Compost\nThe greatest original contribution of the early organic movement was its focus on building soil health. One of the main ways that organic farmers do this is by physically incorporating tons of organic matter into the soil in the form of composts. Unfortunately, during the process of composting a substantial amount of methane is emitted which means that broad use of this soil-building approach would be problematic from a climate change point of view.\n3. Practical Barriers to Implementing No-till Farming\nThe best approach to building soil quality is minimizing soil disturbance (e.g. no plowing or tilling) combined with the use of cover crops. Such farming systems have multiple environmental advantages, particularly with respect to limiting erosion and nutrient movement into water. Organic growers frequently do plant cover crops, but without effective herbicides, they tend to rely on tillage for weed control. There are efforts underway to find a way to do organic no-till, but they are not really scalable.\n4. Difficulties Implementing Optimized Fertilization\nFertilizers are associated with many of the biggest environmental issues for agriculture because of the challenges in supplying all a crop needs without leading to movement of those nutrients into surface or ground water or to emissions of the highly potent greenhouse gas, nitrous oxide. The best practice is to “spoon feed” the nutrients through the irrigation system at levels designed to closely track the changing demands of the crop throughout the season.\nDrip Irrigated and Fertilized Grapes\nThis requires water-soluble forms of the nutrients and that is very expensive to do for the natural fertilizer sources allowed in organic. Since the plants absorb those nutrients in exactly the same molecular forms regardless of source, this cost barrier is a non-scientific impediment to doing the best thing from an environmental point of view. Organic fertilizers like composts or manures are also much less practical for variable rate application, an environmentally beneficial option for rain-fed crops in which different amounts of fertilizer are applied to different parts of the field based on geo-referenced soil and yield mapping data. Finally, the organic avoidance of \"synthetic fertilizers\" would mean that these growers would not be able to use what appear to be promising small-scale, carbon-neutral, renewable energy-driven systems for making nitrogen fertilizers.\n5. Lower Land-Use-Efficiency\nThe per-acre yields of organic crops are significantly lower than those for conventional. This has been well documented both by meta-analysis of published research comparisons and by public data generated through USDA commercial production surveys.\nThe shortfall is driven by limited pesticide options, difficulties in meeting peak fertilizer demand, and in some cases by not being able to use biotech traits. If organic production were used for a significant proportion of crop production, these lower yields would increase the pressure for new land-use-conversion - a serious environmental issue because of the biodiversity and greenhouse gas ramifications.\n6. Lack of an Economic Model to Move Beyond Niche Status\nFinally, agriculture needs to change in ways that accomplish both productivity and environmental goals. That optimal farming approach must become the dominant system over time. Even if organic had maintained its growth trend from 1995 to 2008, organic acreage in 2050 would still have represented less than 3% of US cropland.\n|Trend line for US organic cropland as of of 2008|\nThen, between 2008 and 2011, USDA survey data showed no net gain in US organic acreage. Environmentally desirable \"conventional\" practices like no-till, cover cropping and a variety of other precision agriculture innovations are already practiced on a much broader scale and have the potential to be economically attractive for farmers without any price premium mechanisms. Innovations in farmland leases could greatly accelerate the conversion process if growers could be guaranteed long-term control of fields so that they could profit from their investments in building soil quality.\nConsumers Who Want To Do The Right Thing\nThere are many consumers who are willing to spend more for organic food because they believe that they are making a positive difference for the environment. While it is commendable that people are willing to do that, the pre-scientific basis for the organic rules means that the environmental superiority of organic cannot be assumed. While “only natural” is appealing as a marketing message, it is not the best guide for how to farm with minimal environmental impact. Between rigorous, science-based regulation, public and private investments in new technology development, and farmer innovation, modern agriculture has been making excellent environmental progress. That trend, not organic, is what we need to encourage.\nYou are encouraged to comment here and/or to email me at email@example.com\nPennsylvania farm image from USDA Images. Vineyard image Agne27. Copper Sulfate image from Wikimedia commons. Organic yield and acreage information from the USDA-NASS."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:7733fc97-4635-4e4d-9b07-337812db5438>","<urn:uuid:af468a30-ba52-43e0-a071-b93d38314470>"],"error":null}
{"question":"How do PE and DBT differ in their approach to emotional regulation?","answer":"PE and DBT have different approaches to emotional regulation. PE focuses on emotional processing through direct exposure to trauma memories and situations, where individuals learn that their distress is temporary and trauma-related physiological responses are not dangerous. In contrast, DBT specifically includes emotion regulation as one of its four key skill areas, teaching individuals to identify, name, and understand the function of emotions while increasing their ability to regulate emotions. DBT also incorporates additional components like mindfulness, distress tolerance, and interpersonal effectiveness skills.","context":["This is Part 2 of a 3-part series on posttraumatic stress disorder (PTSD) and eating disorders by Dr. Mary Hill. To access the first post describing the co-occurrence of eating disorders and PTSD click here.\nEvidence-based trauma therapies reduce symptoms of posttraumatic stress disorder (PTSD) and common co-occurring problems like depression, and improve functioning and well-being.1 Prolonged exposure (PE), cognitive processing therapy (CPT), and trauma-focused cognitive behavioral therapy (CF-CBT) are well-established evidence-based treatments for individuals with PTSD.2,3,4,5 Descriptions of each treatment and how they work are presented below.\nPE has the strongest recommendation as a treatment for individuals with PTSD in clinical practice guidelines.2,3,4,5 It addresses problematic avoidance that develops as a result of the trauma. People with PTSD often try to avoid things that remind them of the trauma because encountering people, places, objects, and memories that remind them of the trauma is highly distressing. Avoidance reduces distress in the moment, but ultimately makes PTSD worse. Over time, people avoid more and more trauma reminders, even those that most people would consider safe. Not only does this avoidance keep PTSD symptoms going, it negatively impacts individuals’ ability to engage in meaningful activities and live the kind of lives they want.\nPE works by helping individuals limit problematic avoidance through gradual, systematic exposure to feared but safe stimuli (e.g., places, situations, memories). Essentially, PE helps individuals face their fears safely. PE involves two types of exposure: imaginal exposure and in vivo exposure. During imaginal exposure, individuals retell the trauma memory. The major goal of imaginal exposure is to help individuals emotionally process the trauma memory in order to reduce PTSD symptoms. By repeatedly telling the trauma narrative, individuals better organize and process the memory and their cognitive and emotional reactions to it.\nDuring in vivo exposure, individuals approach objectively safe trauma-related situations (e.g., going to crowded places) that they have avoided because of trauma-related distress. Through direct experience during exposure, individuals learn that the trauma memory, reminders, and physiological responses are not dangerous, and the distress experienced when encountering them is temporary. In addition, new information learned through direct experience that is incompatible with trauma-related beliefs (e.g., “people can’t be trusted,” “I can’t handle being out by myself”) promotes improved quality of life as individuals reengage in meaningful activities previously avoided because of trauma-related distress.6\nCPT is another evidence-based treatment that focuses on one’s beliefs about self, others, and the world that changed as a result of the trauma and how these beliefs affect emotions and behaviors. Specifically, unhelpful beliefs related to power and control, esteem, safety, trust, and intimacy are addressed. It is thought that these beliefs that developed after the trauma make PTSD worse.\nThe primary focus of CPT is to modify dysfunctional cognitions that impair functioning, known as “stuck points.” For example, the “just world belief” states that, “good things happen to good people, and bad things happen to bad people.” If that is believed to be true, it can be hard for trauma survivors to make sense of why something bad happened to them and may impact their ability to engage in their lives as they did before the trauma. CPT challenges stuck points by teaching individuals how to assess whether facts support their stuck points. If not, they can work with their therapist to create a new perspective. CPT is believed to work by developing more balanced and helpful beliefs which decrease difficult emotions and other PTSD symptoms.7\nFinally, TF-CBT is an evidence-based treatment for children with PTSD or traumatic grief and their parents. Treatment includes components of PE and CPT. TF-CBT is comprised of psychoeducation about trauma and PTSD, teaching relaxation skills and other coping skills, in vivo exposure to reminders of the trauma, creation of a trauma narrative (which is similar to imaginal exposure), processing trauma-related thoughts and emotions, and developing safety skills.8\nDespite research support for these treatments, many trauma survivors and therapists express concern that trauma-focused therapies will make people with PTSD feel worse.9 However, the literature shows that most who participate in evidence-based PTSD treatment improve and do not experience worsening of PTSD symptoms or depression.10,11 These treatments are safe and effective, although it is important to find a therapist who is trained in the treatment modality, and it can be beneficial to ensure that individuals have good coping skills prior to beginning treatment as addressing trauma in treatment can be emotionally intensive. Although there are currently no established guidelines for when to address trauma in treatment for individuals with eating disorders or which treatment may be most beneficial, evidence-based PTSD treatments can be used in combination with eating disorders treatments. It is important for future research to assess the best way to integrate treatment for both PTSD and eating disorders.\n- van Minnen, A., Zoellner, L. A., Harned, M. S., & Mills, K. (2015). Changes in comorbid conditions after prolonged exposure for PTSD: A literature review. Current Psychiatry Report, 17, 1-16, doi: 10.1007/s11920-015-0549-1\n- Bisson, J. & Andrew, M. (2007). Psychological treatment of post-traumatic stress disorder (PTSD). Cochrane Database of Systematic Reviews, 3, 1-122. doi: 10.1002/14651858.CD003388.pub3\n- Institute of Medicine (2008). Treatment of posttraumatic stress disorder: An assessment of the evidence. Washington, D.C. The National Academic Press.\n- Cusack, K., Jonas, D. E., Forneris, C. A., Wines, C., Sonis, J., Cook Middleton, J., …Gaynes, B. N. (2016). Psychological treatments for adult with posttraumatic stress disorder: A systematic review and meta-analysis. Clinical Psychology Review, 43, 128-141. doi: https://doi.org/10.1016/j.cpr.2015.10.003\n- Difede, J., Olden, M. & Cukor, J. (2014). Evidence-based treatment of post-traumatic stress disorder. Annual Review of Medicine, 65, 319-332. doi: 10.1146/annurev-med-051812-145438\n- Foa, E. B., Hembree, E. A., & Rothbaum, B. O. (2007). Prolonged exposure therapy for PTSD: Emotional processing of traumatic experiences. New York, NY: Oxford University Press.\n- Resick, P. A., Monson, C. M., & Chard, K. M. (2014). Cognitive processing therapy: Veteran/military version: Therapist’s manual. Washington, DC: Department of Veterans Affairs.\n- Cohen, J. A., Mannarino, A. P., & Deblinger, E. (2006). Treating trauma and traumatic grief in children and adolescents. New York, NY: Guilford Press.\n- Ruzek, J. I., Eftekhari, A., Rosen, C. S., Crowley, J. J., Kuhn, E., Foa, E. B., Hembree, E. A., and Karlin, B. E. (2014). Factors related to clinician attitudes toward prolonged exposure therapy for PTSD. Journal of Traumatic Stress, 27, 423-429. doi: 10.1002/jts.21945\n- Jayawickreme, N. J., Cahill, S. P., Riggs, D. S., Rauch, S. A. M., Resick, P. A., Rothbaum, B. O., and Foa, E. B. (2014). Primum non nocere (first do no harm): Symptom worsening and improvement in female assault victims after prolonged exposure for PTSD. Depression and Anxiety, 31, 412-419. doi: 10.1002/da.22225\n- Foa, E. B., Zoellner, L. A., Feeny, N. C., Hembree, E. A., & Alvarez-Conrad, J. (2002). Does imaginal exposure exacerbate PTSD symptoms? Journal of Consulting and Clinical Psychology, 70, 1022-1028. http://dx.doi.org/10.1037/0022-006X.70.4.1022","Dialectical behavior therapy (DBT) is an evidence-based psychotherapy that is founded on the principles of cognitive-behavioral therapy (CBT) and rooted in mindfulness practices based on Zen Buddhist teachings. The term “dialectical” derives from the idea that combining two opposites in therapy (acceptance and change) yields better results than either would on its own, as is described by WebMD. Psychologist Marsha M. Linehan developed DBT in the late 1980s as a means to more effectively treat chronically suicidal individuals diagnosed with borderline personality disorder (BPD). Currently, DBT is not only considered to be the gold standard form of treatment for individuals diagnosed with BPD but has become an effective and relied upon the psychotherapeutic method in the treatment of many other mental health conditions. The National Alliance on Mental Illness (NAMI) indicate that DBT can be helpful in treating:\n- Bipolar disorder\n- Attention-deficit/ hyperactivity disorder (ADHD)\n- Generalized anxiety disorder (GAD)\n- Eating disorders\n- Obsessive-compulsive disorder (OCD)\n- Major depressive disorder\n- Substance use disorder\n- Post-traumatic stress disorder (PTSD)\nDBT empowers a client to learn applicable social and emotional skills, healthy coping mechanisms, and use mindfulness techniques to enable a client to effectively cope with stress, live in the moment, regulate emotions, and improve relationships with others. Dialectical behavior therapy has become a mainstream form of psychotherapy, with providers spanning all over the world. Specifically, Southern California is home to a plethora of mental health clinicians that are qualified, DBT providers.\nDBT places primary emphasis on the psychosocial aspects of treatment. It is a multifaceted approach that is comprised of the following four components:\n- Individual therapy: focused on improving client motivation and helping clients apply and integrate skills learned to navigate specific life challenges.\n- Group DBT skills training: focused on improving and teaching healthy client behavioral skills, as well as provide a forum to practicing integrating the skills.\n- As-needed, distance/ phone coaching: in-the-moment coaching to assist in and provide crisis management support between sessions.\n- Consultation team: this component is solely for the mental health provider treating the client. A consultation team serves as support for the clinician in treating clients with severe, complex, and/ or challenging to treat disorders.\nClients also agree to complete out-of-session assignments (e.g., group skills homework, diary cards, etc.) to continue to practice implementing the information learned and integrating DBT skills into their daily life.\nDBT specifically focuses on providing therapeutic skills in the following four key areas, provided by the Linehan Institute:\n- Core Mindfulness: skills focused on improving an individual’s ability to accept, become more aware of oneself and others, and be attentive to the present moment.\n- Distress tolerance: skills focused on increasing an individual’s tolerance of negative emotions instead of reacting impulsively.\n- Interpersonal effectiveness: skills focused on increasing an individual’s communication strategies.\n- Emotion regulation: skills focused on helping an individual identify, name, and understand the function of emotions, and increasing one’s ability to regulate emotions.\nThe entire DBT program takes about six months to complete, as six weeks is allocated to each of the four modules. Longer DBT programs may elect to repeat one or more of the four skills modules. If each of the four modules is repeated it would extend the length of the program to last about twelve months long.\nThe information above is provided for the use of informational purposes only. The above content is not to be substituted for professional advice, diagnosis, or treatment, as in no way is it intended as an attempt to practice medicine, give specific medical advice, including, without limitation, advice concerning the topic of mental health. As such, please do not use any material provided above to disregard professional advice or delay seeking treatment."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:5fad2a10-a33d-4b16-ba4f-975d7b54b260>","<urn:uuid:a9f111a8-e7f4-4361-8378-32f71a410169>"],"error":null}
{"question":"How does operational independence from main grid compare: microgrid backup systems versus traditional central power distribution networks - key technical + economic factors?","answer":"Microgrids and traditional central power networks differ significantly in their operational independence. Microgrids are specifically designed to operate both connected to and independently from the main grid, providing resilient backup power to a smaller group of users during outages, as demonstrated in the Outer Banks case. In contrast, traditional central station generators rely on an interconnected transmission and distribution network to deliver power, making them vulnerable to transmission line failures. While traditional systems have lower operating costs due to economies of scale (with fossil fuel plants costing $0.02-0.10/kWh to operate), they lack the ability to isolate and operate independently when transmission infrastructure fails. This technical limitation of traditional systems is driving utilities to now view microgrids as an opportunity rather than a threat, despite the challenge of developing appropriate pricing structures.","context":["Microgrids: Keeping Your Lights on When the Power Goes Out\nWe have all experienced it before. You are at your home or work on a beautiful day without a cloud in the sky and then… the power goes out. For some, this is just a minor inconvenience but, for others, these random power outages can jeopardize their livelihood.\nSuch was the case last July on the Outer Banks Islands off the coast of North Carolina. An area that thrives on its tourism for about 3 months during the year. Losing even a few days of tourism can be devastating to the island’s businesses and residents.\nA construction crew in the area accidentally severed transmission lines that brought power to the islands. Many feared that the islands would be without power for days and tourists would leave. In fact, the outage and projected financial impact made national news. However, one thing that wasn’t widely publicized is that most of the island’s residents and tourist still had power.\nHow is this possible? North Carolina had just finished production on a microgrid earlier that year that could provide backup power to 1,000 year-round residents of the islands. The microgrid was installed because the area is prone to storms and hurricanes that can knock out traditional transmission power lines.\nWhat is a microgrid? Sometimes referred to as “smart grids”, a microgrid is either a third-party or utility-owned small electricity generation facility that is known for their resiliency and ability to provide backup power to a smaller group of homes and/or businesses. These facilities are normally connected to the electrical grid but are also capable of working independently in the event of a power outage.\nMicrogrids are a resource that is beginning to take off in the US. Early adaptors were essential services like police stations, fire stations, and hospitals. However, lately, more tech companies, universities, and financial companies have been investing into microgrids to ensure power stability in the event of an outage.\nMicrogrids can generate electricity using a variety of sources with diesel, solar and wind being the most common. With storage batteries becoming larger and cheaper to manufacture, microgrids are able to generate renewable energy and store more of it for when they need to use it. For instance, the microgrid that kept the power on in the Outer Banks uses a 500 kW/1 MWh lithium-ion battery for its storage.\nOnce seen as a threat that could cut into revenues, utilities are now seeing microgrids as a potential opportunity. Implementing microgrids to increase stability in an aging infrastructure as well as working with communities and businesses on a back-up power source. The problem is developing a pricing structure that will be approved by the DPU since the concept goes against tradition utility pricing.\nWhile the earliest microgrids simply powered backup generators, they have evolved to become fully independent smart grids. Some are predicting microgrids may leapfrog the need for traditional utilities in the same way cell phones leapfrogged landlines in the not too distant future.","Basic economics of power generation, transmission and distribution\nIn most industrialized countries, electric power is provided by generating facilities that serve a large number of customers. These generating facilities, known as central station generators, are often located in remote areas, far from the point of consumption. The economics of central station generation is largely a matter of costing. As with any other production technology, central station generation entails fixed and variable costs. The fixed costs are relatively straightforward, but the variable cost of power generation is remarkably complex. We will examine each of these in turn.\nThe fixed costs of power generation are essentially capital costs and land. The capital cost of building central station generators vary from region to region, largely as a function of labor costs and \"regulatory costs,\" which include things like obtaining siting permits, environmental approvals, and so on. It is important to realize that building central station generation takes an enormous amount of time. In a state such as Texas (where building power plants is relatively easy), the time-to-build can be as short as two years. In California, where bringing new energy infrastructure to fruition is much more difficult (due to higher regulatory costs), the time-to-build can exceed ten years. Table 5.1 shows capital cost ranges for several central-station technologies. Although the ranges in Table 5.1 are quite wide, they still mask quite a bit of uncertainty in the final cost of erecting power plants.\nOperating costs for power plants include fuel, labor and maintenance costs. Unlike capital costs which are \"fixed\" (don't vary with the level of output), a plant's total operating cost depends on how much electricity the plant produces. The operating cost required to produce each MWh of electric energy is referred to as the \"marginal cost.\" Fuel costs dominate the total cost of operation for fossil-fired power plants. For renewables, fuel is generally free (perhaps with the exception of biomass power plants in some scenarios); and the fuel costs for nuclear power plants are actually very low. For these types of power plants, labor and maintenance costs dominate total operating costs.\nIn general, central station generators face a tradeoff between capital and operating costs. Those types of plants that have higher capital costs tend to have lower operating costs. Further, generators which run on fossil fuels tend to have operating costs that are extremely sensitive to changes in the underlying fuel price. The right-most column of Table 5.1 shows typical ranges for operating costs for various types of power plants.\n|Technology||Capital Cost ($/kW)||Operating Cost ($/kWh)|\n|Coal-fired combustion turbine||$500 — $1,000||0.02 — 0.04|\n|Natural gas combustion turbine||$400 — $800||0.04 — 0.10|\n|Coal gasification combined-cycle (IGCC)||$1,000 — $1,500||0.04 — 0.08|\n|Natural gas combined-cycle||$600 — $1,200||0.04 — 0.10|\n|Wind turbine (includes offshore wind)||$1,200 — $5,000||Less than 0.01|\n|Nuclear||$1,200 — $5,000||0.02 — 0.05|\n|Photovoltaic Solar||$4,500 and up||Less than 0.01|\n|Hydroelectric||$1,200 — $5,000||Less than 0.01|\nBecause of the apparent tradeoff between capital and operating cost, comparing the overall costs of different power plant technologies is not always straightforward. Often times, you will see power plants compared using a measure called the \"Levelized Cost of Energy\" (LCOE), which is the average price per unit of output needed for the plant to break even over its operating lifetime. We will discuss LCOE in more detail in a future lesson - it is an extremely important (and often-used) cost metric for power plants, but it has its own problems that you will need to keep in the back of your head.\nIrrespective of technology, all generators share the following characteristics which influence the plant's operations:\n- Ramp rate\nThis variable influences how quickly the plant can increase or decrease power output, in [MW/h] or in [% of capacity per unit time]\n- Ramp time\nThe amount of time it takes from the moment a generator is turned on to the moment it can start providing energy to the grid at its lower operating limit (see below), in [h]\nThe maximum output of a plant, in [MW]\n- Lower Operating Limit (LOL)\nThe minimum amount of power a plant can generate once it is turned on, in [MW]\n- Minimum Run Time\nThe shortest amount of time a plant can operate once it is turned on, in [h].\n- No-Load Cost\nThe cost of turning the plant on, but keeping it \"spinning,\" ready to increase power output, in [$/MWh]. Another way of looking at the no-load cost is the fixed cost of operation; i.e., the cost incurred by the generator that is independent of the amount of energy generated.\n- Start-up and Shut-down Costs\nThese are the costs involved in turning the plant on and off, in [$/MWh].\n|Technology||Ramp Time||Min. Run Time|\n|Simple-cycle combustion turbine||minutes to hours||minutes|\n|Combined-cycle combustion turbine||hours||hours to days|\n|Nuclear||days||weeks to months|\n|Wind Turbine (includes offshore wind)||minutes||none|\n|Hydroelectric (includes pumped storage)||minutes||none|\nThe minimum run time and ramp times determine how flexible the generation source is; these vary greatly among types of plants and are a function of regulations, type of fuel, and technology. Generally speaking, plants that are less flexible (longer minimum run times and slower ramp times) serve base load energy, while plants that are more flexible (shorter minimum run times and quicker ramp times) are better-suited to filling peak demand. Table 5.2 and Figure 5.3 show approximate (order-of-magnitude) minimum run times and ramp times for several generation technologies. It is important to realize that, in some sense, these are \"soft\" constraints. It is possible, for example, to run a nuclear plant for five hours and then shut it down. Doing this, however, imposes a large cost in the form of wear and tear on the plant's components.\nThe cost structure for transmission and distribution is different than for power generation, since there is basically no fuel cost involved with operating transmission and distribution wires (and their associated balance-of-systems, like substations). At the margin, the cost of loading a given transmission line with additional electricity is basically zero (unless the line is operating at its rated capacity limit). Capital cost thus dominates the economics of transmission and distribution."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:ca4721f8-a246-48ef-9b5f-40b44f556591>","<urn:uuid:f0adad7d-89e2-4843-84d4-1efb3fae8547>"],"error":null}
{"question":"What are the key differences between immunotherapy and radiation therapy in terms of how they target cancer cells?","answer":"Immunotherapy and radiation therapy target cancer cells through fundamentally different mechanisms. Immunotherapy works by harnessing the body's immune system, specifically teaching T cells to recognize and destroy cancer cells like guided missiles while sparing normal tissues. In contrast, radiation therapy uses high-energy waves or particles to directly damage cancer cells' DNA and destroy their ability to grow and divide. While immunotherapy is a systemic treatment that can work throughout the body, radiation therapy is a localized treatment that affects only the specific areas where radiation is directed. However, both treatments can affect healthy cells - immunotherapy can lead to autoimmune-like side effects, while radiation causes collateral damage to normal tissues in the treated area.","context":["Immunotherapy—cancer's new frontier\nYour immune system never sleeps. Every moment of the day, immune cells monitor your body for disease, calling for backup when they detect a threat. It's a system that works elegantly—most of the time. It's not foolproof; if it were, we would never get cancer in the first place.\n\"The immune system is supposed to fight tumors, but it doesn't do it very effectively,\" says Dr. Daruka Mahadevan, director of the University of Arizona Cancer Center Early Phase Clinical Trials Program.\nIf the immune system is unable to suppress their development early, abnormal cells can blossom into cancers, and cancers can spread throughout bodies, eventually claiming lives.\n\"So many of my patients feel like cancer is a betrayal of their own immune system's ability to survey and protect them,\" says Dr. Julie Bauman, Cancer Center division chief of hematology and oncology.\nWhen tumors evade detection by the immune system, we turn to chemotherapy and radiation. However, these old standbys don't exclusively target cancer cells and are notorious for the collateral damage rained upon healthy cells, often causing side effects such as nausea and hair loss.\nWhat if we didn't have to subject normal cells to the friendly fire sprayed haphazardly by chemo and radiation? Many cancer researchers are optimistic about immunotherapy, a treatment that harnesses the power of the immune system, teaching it to recognize—and destroy—cancer cells.\n\"An educated immune cell, like a guided missile, can specifically recognize and destroy the tumor target, sparing normal tissues,\" says Protul Shrikant, professor of immunobiology at the UA College of Medicine – Tucson.\n\"The development of immunotherapy has revolutionized the way we treat cancer,\" says Dr. Clara Curiel, leader of the Cancer Center's cutaneous oncology team. \"It's a paradigm shift.\"\nThe idea that the immune system could do the job \"really appeals to people,\" Bauman says. \"There's this notion that we're restoring a natural process in the body, the ability to harness its wisdom to attack cancer.\"\nT Cells: A Crash Course in Immunity\nThe immune system evolved to distinguish between foreign invaders, such as viruses and bacteria, and friendly faces, namely a person's own \"self\" cells. The ability to recognize this difference is essential to keeping the system in balance.\nOne crucial component of the immune army are T cells, which cruise the bloodstream, on the lookout for \"bad guys.\" T cells can call in reinforcements by special-ordering customized proteins called antibodies, which are able to lock onto these \"enemy\" cells and mark them for destruction.\nWhen T cells see a \"non-self\" invader, such as a virus, they mount an attack. But when they see a \"self\" cell, they need to know to leave it alone—otherwise, the immune system would be attacking its own body, a misfire that can lead to autoimmune diseases such as rheumatoid arthritis and lupus. When all goes according to plan, the immune system's foot soldiers patrol the body for enemies while leaving innocent civilians alone.\nBut recognizing a cancer cell isn't as easy as it might seem.\n\"Cancer cells arise from our own cells, so they have a lot in common with our normal cells,\" Bauman explains. \"We have to exploit the things that are different about cancer and teach the immune system to recognize cancer as the enemy. Exploiting that difference is tricky.\"\nThe idea to harness the immune system in the fight against cancer is not new. In New York, as the 19th century was ending, a doctor named William B. Coley developed a rudimentary predecessor to immunotherapy. After noticing that some patients with cancers of the bones or soft tissues, called sarcomas, went into remission after coming down with strep infections, he hypothesized that the bacteria that caused strep also killed cancer.\nBased on this hunch, Coley started injecting strep bacteria into sarcoma patients, some of whom experienced dramatic remissions. Rather than directly killing the cancer, the bacteria probably provoked an immune response that was strong enough to wipe out the cancer along with the infection.\n\"He was hyperactivating the immune system by injecting these bacteria,\" says Dr. Emmanuel Katsanis, the Cancer Center's division chief of pediatric hematology and oncology. \"The activated immune system was trying to fight the infection—and, at the same time, fighting the cancer.\"\nIn 1899, Coley's treatment was licensed by Parke-Davis and Co. It certainly wasn't a cure-all, and it was eclipsed by radiation treatment in the early decades of the 20th century. Chemotherapy came into use during the post-World War II period, and by 1965 the American Cancer Society had put Coley's bacterial cocktail on a list of \"unproven\" treatments.\nBut the idea that the immune response to infection could be conscripted in the fight against cancer never completely vanished.\n\"When I first started in oncology,\" Katsanis recalls, \"some of my mentors were saying, 'This patient got a severe infection. Hopefully this has stimulated their immune system and they're not going to relapse.'\"\nThese days, instead of pinning our hopes on the ability of viruses and bacteria to incite the immune system into a cancer-fighting state, we can create more precise tools.\n\"We've learned how to turn antibodies that we engineer in the laboratory into proteins that fight cancer,\" Bauman says.\nScientists customize these so-called monoclonal antibodies to lock onto a specific cancer molecule, and physicians deploy these \"supersoldiers\" into the body. One subtype of monoclonal antibody, called a checkpoint inhibitor, can tinker with a specific component of a T cell, which is akin to a \"brake.\"\nShrikant likens the immune process to driving a sports car.\n\"A Ferrari owner needs to appreciate and understand the use of the accelerator and the brake to enjoy the car over the long haul,\" he explains. \"Otherwise, the Ferrari would be wrapped around a tree.\"\nWhile a normally functioning immune system needs those brakes to protect self cells from immune attack, some cancers can slam on the brakes to suppress the immune response. Checkpoint inhibitors cut the brakes—a boon in an environment in which cancer cells, rather than autoimmune diseases, pose the most serious threat.\nWhat's more, educating the immune system to recognize cancer cells can lead to long-lasting immune responses well after a patient completes treatment.\n\"When you prime the immune system to recognize a target, it stays as a memory in your immune system,\" Curiel explains. If, after treatment, a tumor cell starts replicating again, the immune system \"will remember that it is a 'foreign' target and will attempt to eliminate it,\" she says.\nCollaboration at the UA\nCancer Center researchers traverse the \"bench-to-bedside\" continuum, solving the immune system's riddles to develop new drug candidates. If successful in the lab, these therapies are taken into the clinic to test in patients.\n\"If we want to understand how to fight a disease, we have to do basic research to understand how that disease comes about, or how the body can naturally take care of that disease,\" says Michael Kuhns, associate professor of immunobiology at the College of Medicine – Tucson.\nKuhns' research focuses on the \"conversations\" between T cells and the rest of the immune system. T cells are able to respond to threats and coordinate attacks. Manipulating these messages can change a T cell's behavior.\nKuhns reports that his team already has achieved success in a basic lab setting.\n\"I can redirect T cells to kill targets,\" he says. \"One day, that could have immunotherapeutic applications.\"\nLikewise, Shrikant's research focuses on how T cells recognize and destroy cancer cells.\n\"Our laboratory studies span from detailed understanding of fundamental immunology, to validating in animal models, to early phase clinical trials,\" he says of the ongoing work in his laboratory.\nMahadevan specializes in the development of drugs for lymphomas, including peripheral T-cell non-Hodgkin lymphoma, or PTCL, a cancer of the T cells.\n\"PTCL is an aggressive form of lymphoma, and we don't have any good treatments,\" Mahadevan says. Because immunotherapy harnesses the power of T cells, T-cell lymphoma is an especially challenging target: \"The very cells that we are trying to activate are the cells that are abnormal,\" he says.\nMahadevan's lab has identified a combination treatment that is effective against PTCL tumors in lab mice. While the immune system is suppressed in these lymphomas, his team has been able to wake the immune system back up by blocking the ability of cancer cells to divide, significantly improving the survival of these mice. He is now working with pharmaceutical companies and the National Institutes of Health to find a combination that works in patients.\nAcross campus, collaborations between basic scientists and physician-scientists are helping to expand our understanding of how the immune system can be enlisted in the fight against cancer.\n\"There's incredibly good science on campus, and there are a lot of clinics,\" Shrikant says. \"There's tremendous potential.\"\nDespite the promise that immunotherapy holds, most patients don't experience the dramatic results that make headlines.\n\"It really depends on the disease and the drug, but I would say that 30 percent of the time, we see some efficacy,\" estimates Dr. Hani Babiker, associate director of the Cancer Center Early Phase Clinical Trials Program.\nFor her head-and-neck cancer patients, Bauman estimates immunotherapy's efficacy at about 15 percent.\n\"I've seen patients achieve deep remission that we never thought possible,\" Bauman says. \"I wish this worked for everyone, because if 15 percent of people have meaningful responses to immunotherapy, that means 85 percent come to the table with that level of hope and are disappointed.\"\nTo realize immunotherapy's full potential, it needs to work for more patients.\n\"I don't know whose T cells will be awakened in the right way and attack the cancer, and whose T cells will be awakened in the wrong way and attack the patient,\" Bauman says. \"My hope is that I learn how to unlock the right T cells at the right time for the right cancer.\"\nWhile it's tempting to imagine a future in which immunotherapy alone can vaporize tumors for good, chemotherapy and radiation aren't going away soon, and in fact they can be combined with immunotherapy to increase its effectiveness—leading to longer survival and less disappointment.\n\"I believe we can improve response rates to 70 to 80 percent with these rational combinations,\" Mahadevan says. \"A tumor that is resistant to one immune checkpoint therapy may be really sensitive (to) a combo.\"\nRadiation and chemotherapy directly kill tumor cells in a process that can release cellular debris into the body.\n\"Radiation breaks a cancer cell open and exposes all of its hidden 'guts' to the immune system,\" Bauman explains. \"Chemotherapy directly poisons the cancer cell. It dies an immunogenic cell death, and also shows more of its abnormal contents.\"\nWhen immune cells are activated by immunotherapy drugs, this debris can help train the newly empowered immune system to recognize the enemy.\n\"An army of T cells is waiting to recognize some part of that cancer cell as foreign and, therefore, worthy of attack,\" Bauman says. \"Immunotherapy is there to prime that response and help those T cells become more active.\"\nHope for the Future\nAlthough the idea of immunotherapy is not new, in the clinic it is still in its infancy. The therapy has been responsible for remissions that previously were unimaginable, and patients exhausted by the harsh effects of chemotherapy and radiation may find that immunotherapy is a gentler experience. But most patients fail to respond—while those who do see results also run the risk of potentially serious side effects in which an \"over-revved\" immune system attacks healthy cells.\nAlso in its infancy is precision medicine, a strategy that helps physicians match patients to drugs based on their genetic profiles. Researchers hope to use such an approach to predict which patients are more likely to respond to particular cancer drugs, including immunotherapy, and who might be at risk for side effects. Undertakings such as the National Institutes of Health's Precision Medicine Initiative, from which the UA Health Sciences and Banner Health have received funding, eventually will help physicians choose optimal treatments based on an individual patient's unique cancer genetics.\n\"We have the ability to sequence the human genome, and we have a multitude of immunotherapy candidates,\" Bauman says. \"When these two things come together, we will have powerful cancer therapy for every person. We can wake up their immune system to the unique antigens within their cancer. That is the holy grail.\"\nEnergized by the swift advances in the field over the last couple of decades, scientists and physicians look forward to what the future holds.\n\"It's a really exciting time in drug development,\" Babiker says. \"We're discovering a different modality in cancer treatment—using our patients' own immune system to target and fight the cancer.\"\n\"My hope for the future is that cancer will not be a word we are afraid of,\" Curiel says. \"We have made huge progress in a very short time—it is encouraging.\"\nAlthough there still is much work to do, scientists around the world are devoted to unraveling the secrets of the immune system. UA Cancer Center researchers are involved in laboratory research and clinical trials to plumb the depths of immunobiology, discover new immunotherapy drugs, find more effective treatment combinations and reduce the occurrence of side effects.\nThe hope is that, someday, the wisdom of scientists will join forces with the wisdom of the body to equip our immune cells to lead the charge in vanquishing cancer.\nProvided by University of Arizona","Radiation therapy is the use of penetrating beams of high-energy waves or streams of particles called radiation to treat disease. Radiation has been used to treat cancer since the late 19th century. In fact, the first successful radiation treatment for cancer was reported in 1898.\nIn its earliest state, radiation was given in single, large doses, which caused many complications. By 1940, doctors had begun dividing the total dose of radiation into several smaller doses. This process is known as fractionation of the dose. Fractionation is very important because it allows the oncologist to destroy tumor cells, while allowing normal tissues to repair the radiation damage.\nIn the last 50 years, technology has allowed for great advances in radiation therapy, including deeper penetration of the radiation and less scatter to healthy tissues. Currently, there exists a delicate balance between using radiation to treat cancer cells and minimizing its adverse side effects on the body's normal cells.\n- How does radiation therapy work?\n- What is radiation therapy used for?\n- What are the types of radiation therapy?\n- What adverse effects can occur with radiation therapy?\n- Which cancers is radiation therapy used to treat?\nRadiation therapy destroys the ability of cancer cells to grow and divide. When high-energy ionizing radiation is given as cancer therapy, some cells are directly damaged, but more cells are indirectly affected by the radiation. The rays or particles enter the cell's nucleus, interact with water present in the nucleus, and form a free radical called hydroxyl radical. The hydroxyl radical is unstable and causes damage to the cell's DNA. Due to this damage, some cells die immediately. Some cells will survive in the short-term, but are unable to divide and will die at the time of mitosis, or cell division.\nNormal, healthy tissues are affected by radiation therapy as well, and this accounts for the adverse side effects seen with this type of treatment. To help minimize side effects, radiation is divided into doses and spread out over time. In addition, radiation is targeted as much as possible to shield normal tissue and only irradiate the cancer cells.\nUnlike chemotherapy , which is a systemic treatment, radiation therapy is a localized treatment. This means it affects only the cells in the specific areas of the body where the radiation is directed. Radiation therapy is sometimes used in combination with other therapies, like chemotherapy or hormonal therapy , to help improve treatment results. Also, radiation therapy can be given before, during, or after surgery.\nRadiation therapy is given with the intention of completely destroying the disease. When this is not possible, controlling the growth and spread of the cancer is the goal. In addition, improving quality of life by controlling symptoms associated with cancer is an aim of radiation therapy. These symptoms include pain, uncontrolled bleeding, tumor obstruction around major blood vessels and organs, and spinal cord compression.\nDetermining if your cancer may be appropriately treated by radiation therapy is based on several factors, such as tumor type, location, size, and response to other modalities including surgery and chemotherapy. Your radiation oncologist is the only medical professional qualified to give an absolute opinion on the role of radiation therapy in the management of your disease.\nSome factors affecting how well radiation therapy will work include the following:\n- Phase of cell—A cell goes through five phases in its cycle of division—from resting to mitosis. Cells in the resting phase are less sensitive to radiation (and therefore less likely to be damaged) than cells that are actually dividing (in mitosis).\n- Division rate of the cell—Rapidly dividing cells are more sensitive than slowly dividing cells.\n- Oxygenation—Because oxygen is necessary to form the hydroxyl radical (which leads to cell destruction), highly oxygenated tissues are more sensitive to radiation therapy. This has been clearly demonstrated in cervical cancers and cancers of the head and neck. In fact, the presence of oxygen is so important that patients who smoke, and therefore lower the oxygen level to their tumor, suffer from a 50% reduction in the ability of the radiation to kill the cancer.\nIn external radiation therapy, rays are directed at the tumor from outside the body. Prior to treatment, your doctor will develop a plan to determine the best method for delivering treatment. In a process called simulation, you will be asked to lie on the examination table while the radiation therapist uses a special x-ray machine to define the area of treatment. Radiographic studies, like a CT scan , an MRI , or a barium enema , help the physician visualize the exact area that needs treatment. Using this information, the doctor can calculate the maximum radiation dose to the tumor, while minimizing the dose to the normal surrounding tissues.\nA CT scan is a type of x-ray that uses a computer to produce cross-sectional images of the inside of the body. An MRI scan uses magnetic waves to produce images of the inside of the body. Using a large magnet, radio waves, and a computer, an MRI produces 2D and 3D pictures. A barium enema is a rectal injection of barium, a substance that coats the lining of the colon and rectum. It is done before x-rays are taken in order to create better x-ray images.\nIf you receive external radiation therapy, you will go to the hospital or clinic each day for treatment. Usually, treatments are given 5 days a week for 2 to 8 weeks. The total dose of radiation and the number of treatments necessary will depend on the size, location, and type of cancer you have, as well as your general health and other medical treatments you may be having. This procedure is like having an x-ray. Actual treatment time (the time you are receiving radiation) ranges from 2 to 5 minutes.\nRadiation is usually given once a day with a dose based on the type and location of the tumor. In hyperfractionated radiation therapy, the daily dose is divided into smaller doses and given several times a day (usually twice a day). Treatments are separated by 4 to 6 hours.\nHyperfractionation is used in the treatment of head and neck cancers, some lung cancers , and in a situation where a patient has already had radiation therapy and needs more to that same area. Other cancer sites have not been conclusively shown to respond to radiation given more often than once daily.\nDifferent types of machines are used to deliver the radiation. The higher the energy produced by the machine, the greater the depth of penetration. In addition, there is less radiation scatter with higher energies.\nAt the end of the treatment regimen, the tumor site often gets an extra dose of radiation, called a boost.\nInternal radiation therapy, also called brachytherapy, places the radiation source as close as possible to the cancer cells. Radioactive material, sealed in a thin wire, catheter, or tube, is placed directly into the affected tissue. This method concentrates the radiation on the cancer cells and minimizes the radiation damage to the normal tissue nearby.\nThe radioactive substances used for internal radiation therapy include the following:\nThe type of implant and how it is placed depends on the size and location of the tumor. Methods include the following:\n- Interstitial radiation—The implant is placed directly into the tumor via catheters, seeds, or capsules. This is commonly used for prostate cancer.\n- Intracavitary radiation—Commonly used for cervical cancer, the implant is placed in special applicators inside a body cavity.\n- Intraluminal radiation—Commonly used for lung or esophageal cancer , the implant is placed in special applicators inside a body passage or lumen.\n- Surface brachytherapy—The implant is placed in or against the tumor. This is commonly used for skin cancer or melanoma of the eye.\nImplants may be removed after a short time or left in place permanently. When left in place, the implants become non-radioactive in a short time. For the placement of most types of implants, you will need to be in the hospital.\nIndications for internal radiation therapy include cancers of the head and neck, lung, breast, uterus, thyroid, cervix, rectum, bladder, and prostate. It is sometimes given in combination with external radiation therapy.\nIntraoperative radiation combines surgery and radiation therapy. During surgery, after as much of the tumor as possible is removed, a large dose of radiation is given directly to the tumor bed and nearby areas. This therapy is sometimes given in combination with external radiation therapy.\nIntraoperative radiation is used to treat locally advanced abdominal cancers such as stomach, pancreatic, colorectal, and retroperitoneal sarcomas.\nIn photodynamic therapy (PDT), photosensitizers, or light-sensitive molecules, are injected into the bloodstream and absorbed by cells throughout the body. These agents remain in cancer cells longer than in normal cells. When the cancer cells are exposed to laser light, the photosensitizers are activated and form oxygen radicals that affect cell membranes, the cytoplasm, and the DNA. This results in cell damage and death. PDT causes minimal damage to healthy tissue, but the laser light used in PDT cannot pass through more than 3 centimeters of tissue.\nPDT is mainly used to treat tumors on or just under the skin or on the lining of internal organs. Currently, PDT is used in the treatment of skin, lung, and esophageal cancers as well as superficial cancers of the bladder, head, and neck.\nIn addition, PDT is used for bone marrow purging . Bone marrow used for autologous transplantation must be relatively free of cancer cells. Before transplantation, the harvested marrow is often treated with PDT in a process known as \"purging\" to get rid of cancer cells.\nIn hyperthermia, body tissues are exposed to high temperatures (up to 106°F) to damage and kill cancer cells or to make cancer cells more sensitive to the effects of radiation. Methods include the following:\nLocal hyperthermia —Heat is applied to a small area, usually the tumor itself, either externally or internally. If heated externally, high-frequency sound waves are aimed at the tumor using a device (like an ultrasound machine) outside the body. Internal heating can be done by inserting sterile probes that are either heated or filled with warm water, implanted microwave antennae, or radiofrequency electrodes.\nRegional hyperthermia —Heat is applied to an organ or a limb. This can be done with magnets or other devices that produce high energy. Or, blood can be removed, heated, and returned to the affected region.\nWhole-body hyperthermia —Heat is applied to the entire body. This is used for metastatic cancer that has spread throughout the body. Warm-water blankets, hot wax, inductive coils, or thermal chambers are used to raise body temperature.\nRadiation therapy affects normal, healthy cells as well as cancer cells. When radiation is targeted at a cancerous site, there is destruction of normal tissue, in addition to cancer cells, in that area. A majority of the adverse effects are due to this phenomenon. The following are general side effects that can occur with radiation therapy.\nFatigue, or feeling tired, can be a symptom of the cancer itself, as well as a side effect of cancer treatment. It is experienced by many people receiving radiation therapy. The exact cause is not know, but it may result from a combination of lowered red blood cell counts, lack of sleep, pain, and poor appetite. Fatigue many occur after treatment each day and become chronic as treatment continues. Fatigue is not a life-threatening side effect, but it can be disruptive to daily life.\nThe level of fatigue varies among people, but generally, most people are able to continue work and light activities. To help combat fatigue, try not to do too much. If you start to feel tired, limit your activities. Try to get enough rest at night and take naps or short breaks throughout the day. Allow people to help you with daily responsibilities, like shopping, house cleaning, and child care.\nSome people have found that light exercise, such as walking, helps combat fatigue. Talk with your doctor about how much exercise is right for you both during and after radiation therapy.\nSkin reactions are normal and expected with radiation therapy. Reactions can occur as soon as 2 weeks into treatment. The skin becomes red and irritated and slight swelling may appear. After a few weeks of treatment, skin may become very dry, itchy, and flaky. In some types of radiation therapy, skin may develop a moist reaction. The skin starts to shed, leaving a raw, painful area. If this occurs, it is important to let your doctor know.\nSkin reactions are more common in areas receiving large doses of radiation. Also, certain areas of the skin are more sensitive, such as facial skin, skin over bony prominences, and skin with a pre-existing surgical wound. When radiation therapy is used along with chemotherapy, there is a higher risk of developing a skin reaction. Most skin reactions disappear a few weeks after treatment is completed.\nThe following suggestions may help you avoid skin problems:\n- Wash with lukewarm water and mild soap, and pat dry.\n- Do not wear tight clothing over the area.\n- Do not rub, scratch, or scrub the skin in the treated area.\n- Avoid putting anything hot or cold on the treated area.\n- Avoid exposing the radiated area to the sun during treatment.\n- Ask your doctor to recommend a cleanser and moisturizer for your sensitive skin.\nIf large areas of active bone marrow are treated with radiation therapy, a decrease in bone marrow function occurs. Such treatment areas include the pelvis, spine, sternum, ribs, long bones, and skull. This can lead to low levels of red blood cells ( anemia ), white blood cells (neutropenia), and platelets (thrombocytopenia). Resulting complications may include fatigue, serious infection, and uncontrolled bleeding.\nDuring radiation therapy, blood levels may be monitored, particularly if chemotherapy is also being delivered, or if a large part of the body is being irradiated. For many sites treated with radiation, there is no reason to get blood samples, unless you begin to feel poorly and the doctor wants to determine if low blood counts are to blame. If a blood test shows significant bone marrow effects, your doctor may wait until your blood counts increase before continuing treatment. Blood transfusions are sometimes necessary.\nRadiation therapy can cause hair loss, also known as alopecia . Hair loss occurs only in the area being treated with radiation. If you receive radiation to your head, you may lose some or all of the hair on your scalp. The amount of hair that grows back depends on how much and what kind of radiation you received. Some people find that when their hair grows back, the color or texture may be slightly different.\nAlthough not life-threatening, hair loss can be upsetting. Many people buy a wig or hairpiece, or use hats or scarves, to cover their heads. If you buy a wig because of cancer treatment, it is a tax-deductible expense and may be covered in part by health insurance.\nLoss of appetite can be a symptom of the cancer itself, as well as a side effect of cancer treatment. It is not unusual to lose one or two pounds a week during radiation treatment. You will be weighed weekly to monitor your weight.\nMany small meals, rather than three large ones, can help make eating seem less overwhelming. Ask your doctor for a referral to a registered dietitian (RD) to assist you in setting up a diet plan that helps you maintain weight. In addition, medications to increase your appetite are available. If it is painful to chew and swallow, you may want to try a powdered or liquid diet supplement. It is crucial that you get enough calories and protein so that your body has enough energy to fight the cancer. Studies have found that people who eat well cope better with their cancer and its treatment.\nFor details on the use of radiation therapy for specific cancers, see the following articles:Bladder cancerBrain tumorsBreast cancerCervical cancerColorectal cancerEsophageal cancerHodgkin's lymphomaKidney cancerLeukemiaLung cancerMelanomaMultiple myelomaNon-Hodgkin's lymphomaOvarian cancerPancreatic cancerProstate cancerStomach cancerTesticular cancerThyroid cancerUterine (endometrial) cancer\n- Reviewer: Mohei Abouzied, MD, FACP\n- Review Date: 09/2014 -\n- Update Date: 09/17/2014 -"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:6fd754d0-2764-4ae2-9ee1-b82acac69df4>","<urn:uuid:773fd8c3-f077-4739-a510-6f5f090b0eb6>"],"error":null}
{"question":"How does server-based recording technology enhance workflow efficiency and image quality in broadcast production compared to traditional camera recording methods?","answer":"Server-based recording technology streamlines workflow by enabling cameras and audio devices to send uncompressed media directly through a network switch to a digital recording server, which encodes signals into ready-to-edit files and sends them to network-attached storage devices. This eliminates the intermediate process of memory cards and manual offloading, while allowing instant access to files for quality assessment. Regarding image quality, modern cameras like the URSA Broadcast G2 capture high-quality 6K footage with 13 stops of dynamic range and advanced features like generation 5 colour science for accurate skin tones and faithful color reproduction. The system can simultaneously create both high-quality and proxy-grade video files with multiple copies in real-time, eliminating the need for time-consuming transcodes and providing instant redundancy.","context":["The new Blackmagic URSA Broadcast G2 is a broadcast camera for traditional and online applications, with a 6K digital film sensor and digital film characteristics such as low light performance with dual gain ISO up to +36dB, and 13 stops of dynamic range. The camera records ProRes, H.265 and Blackmagic RAW file formats and has Blackmagic generation 5 colour science.\nThe Blackmagic URSA Broadcast G2 works as a 4K field or studio broadcast camera, or a 6K digital film camera, and uses familiar lenses and battery types. The lens mount can also be changed. Users records common file formats onto regular SD cards, UHS-II cards, CFast 2.0 cards or external USB disks. These features make it compatible with most video software and broadcast media management systems as well.\nColour and Light\nThe combined 6K sensor and Blackmagic generation 5 colour science result in the same imaging system used in digital film cameras, resulting in accurate skin tones and faithful colour reproduction. The new dynamic 12-bit gamma curve captures extra colour data in the highlights and shadows.\nThe colour system also handles some of the complex Blackmagic RAW image processing, preserving colour and dynamic range data from the sensor via metadata for use in post production. The darker blacks and brighter whites recorded due to the 13 stops of dynamic range increase options when using colour correction to produce more dynamic, expressive footage. The URSA Broadcast G2 works with standard 17 and 33 point 3D LUT files, but it does have built-in LUTs such as Extended Video, Film to Video, Film to Rec.2020 and others.\nThe camera’s custom neutral density (ND) filters will quickly reduce the amount of light entering the camera, with 1/4, 1/16th and 1/64th stop filters specifically designed to match the camera's image sensor and colour science, enhancing latitude and colorimetry. The user can apply different combinations of aperture and shutter angle, accommodating diverse situations, and display the filter status on the LCD as a fraction, number or stop reduction. The IR filters have been built to evenly filter both optical and IR wavelengths, avoiding IR contamination.\nThe URSA Broadcast G2’s low light performance makes it possible to shoot under a wider range of ambient lighting, for example, moonlight. The gain ranges from -12dB (100 ISO) up to +36dB (25,600 ISO), optimising the capture quality to reduce grain and noise, while preserving the dynamic range of the sensor. Gain controls are accessed via a camera switch or the LCD menu, or remotely using the SDI remote camera control protocol.\nThe 6K sensor’s resolution of 6144 x 3456 allow enough flexibility for either broadcast or digital film work. When using B4 lenses, the camera sensor’s 4K window suits UHD broadcast applications and, by changing to a PL or EF lens mount, the full 6K resolution is available for digital film productions.\nThe B4 broadcast lens mount supplied with the camera incorporates optics matching the camera’s sensor, and supports modern Ultra HD lenses or lower cost HD lenses. Older HD lenses sometimes feature resolutions beyond HD, increasing their value, and B4 lenses are desirable because they are par-focal, maintaining focus as the lens is zoomed and the focal point (and magnification) changes. To use PL, EF, F and other types of lenses, the mount can be changed – a spare EF mount is included with the camera. Electronic lens control is also included, allowing focus, iris and zoom adjustment using the camera’s controls, or remotely.\nFurther to formats, the camera is designed to work with standard file formats, used generally by broadcast systems and editing software, recording ProRes 422 HQ and ProRes 422 into QuickTime files. For compressed files, users can record H.265 outputting images at 60:1 to 285:1 compression ratios, in 10-bit broadcast quality. Shooting Blackmagic RAW files is a further alternative, a format tuned to preserve the quality of the sensor data from the camera. In short, URSA Broadcast G2 users have a camera that suits most types of workflow.\nThe dual CFast 2.0 recorders and dual SD/UHS-II card recorders allow a choice of physical media. Both types are standard, inexpensive and readily available, and the dual slots give non-stop recording. When a card is full, recording automatically continues onto the next card, while the user replaces the full card in the inactive slot with an empty one.\nOn the rear of the camera, a high speed USB-C expansion records to external disks or connects to various accessories. For example, by plugging in an external USB flash disk, users can record ProRes, H.265 or 12-bit Blackmagic RAW files and remove the disk to a computer later on for editing and colour correction without copying the files over. Meanwhile the computer’s USB port recharges the disk.\nFor everyday HD broadcast work, where dynamic range and colour fidelity are less critical, the camera will scale the image sensor down to 1080HD video standards, which helps move stories on air quickly, in HD or UHD. The sensor also supports sub pixel image processing of HD video for anti-aliasing and sharpness.\nThe optional zoom and focus demands make it possible to use photography lenses for live production, and have USB-C connections for setting them up with the camera’s electronic lens control. Each zoom and focus demand also has two USB-C ports, allowing them to be daisy chained and then connected to the camera with a single USB connection. The precise design gives very fine lens control, allowing the user to frame and adjust the lens without moving their hands from the tripod handles.\nURSA Studio Viewfinder\nA shoulder mount kit, V-Lock battery plate and top handle are included so that users don't need to purchase extra gear, but a range of other accessories is available specifically for use with URSA Broadcast G2. These include the Blackmagic URSA Studio Viewfinder built for tripod-mounted studio camera use with a 7-inch 2,000 nit display, tally light and accessible controls.\nUsing a standard V-Lock mount, it mounts to the top of the camera body – or the top of the SMPTE fibre converter if one is installed. The display has a metal sunshade and grip handles for positioning independently of the camera, even while the camera is on air. It can be tilted up and down and rotated sideways, with lock knobs to vary the tension on the adjustments.\nThe framing and focus are adjusted with the on-screen or separate physical controls, using a menu dial to scroll through menus and change settings. Especially useful are the three knobs on the right side of the viewfinder that control the LCD brightness and contrast, and adjust the focus peaking. The focus peaking control makes focussing very quick at the user’s chosen zoom setting. Finally, the viewfinder’s three customisable function buttons can be programmed to control features like zebra, false colour, focus peaking, luminance waveform, LUTs and so on.\nThe large tally light on the back of the viewfinder shines red for on-air, green for preview and orange for ISO recording. Once the correct camera number is set in the URSA Broadcast G2’s menus, tally information is sent back to the camera over SDI as the director cuts between cameras on the ATEM switcher, illuminating the light when the camera is on the air.\nBlackmagic SDI Control Protocol\nThe tally light is one of the features that can be controlled over the SDI video connection. The Blackmagic SDI Control Protocol – which developers use to build devices that integrate with Blackmagic products – takes advantage of blanking space in the SDI data stream to add talkback, tally and other camera control information direct to the camera via the program return feed.\nThis feature also works with the viewfinder because it has the same control sent to it from the camera via SDI. The Blackmagic control protocol is an open standard and is documented in the URSA Broadcast and ATEM switcher manual. That means anyone can create custom applications for Blackmagic cameras, switchers and Blackmagic URSA Studio Viewfinder. www.blackmagicdesign.com","by James Delhauer\nOn a set, the job of the person who is tasked with acquiring the content that is shot throughout the day is incredibly stressful. Whether we’re discussing the tape operators of days gone by or the most modern media recordists, there are challenges that have stood the test of time. Somewhere between hundreds of thousands and hundreds of millions of dollars are spent assembling the production. Countless man-hours contribute to making it the very best that it can be. Literal blood, sweat, and tears are spilled to create what we all hope will be a veritable work of art. Then, after all of that, it falls on the shoulders of the one person who is tasked with handling the media. They are simply given very delicate assets that have been created throughout the day and which represent the sum total of the production as a whole. Just about anything can go wrong. Data can be corrupted. Hard drives can be damaged. Video tape can tear. Fortunately, these risks are being minimized by the advent of a new method of media acquisition: server-based recording.\nThough different productions utilize a vast array of workflows, every single one since the Roundhay Garden Scene was first filmed in 1888, has come down to the media. And every single production needs someone to manage it. In today’s digital era, the most common workflow goes a little something like this. Cameras or external recorders capture video and audio data to an internal storage device some sort. When that unit is full, it is ejected and turned over to a media manager. The production continues with another memory card while the media manager takes the first one and offloads, backs up, and verifies the files on it. This is usually done with an intermediate program such as Pomfort’s Silverstack or Imagine’s Shotput Pro—programs that can do file comparisons to ensure that what was on the source media is identical to what ends up on the target media. When all of that content is secured on multiple external hard drives, the original memory card is returned to the production so that it can be wiped and reused. Rinse and repeat. At the end of each day, the media manager turns over at least one set of drives containing the day’s work to someone who will bring it to a post-production facility.\nThere, the work is moved from these temporary storage drives onto work servers, where assistant editors can begin their work.\nWhile prominent, this workflow does come with a few inherent drawbacks. Most notably, the process is both fragile and time-consuming. Digital storage, no matter how sophisticated, is vulnerable to failure, damage, or theft. When the media manager receives a card with part of the day’s work on it, that card is often the only raw copy of the work in existence. Careers could end in a heartbeat if anything were to happen to it. So it becomes his or her job to create multiple copies. Unfortunately, the time during which data transfers from one storage system to another is the time at which it is most vulnerable. An accidentally yanked cable or sudden power surge is all it takes to corrupt the open files as they are transferring over. This vulnerability is compounded by the fact that transferring files is time-consuming and becoming ever more so. As our industry continues to push the boundaries of resolution, color science, and bit depths, video files are getting bigger and bigger. As such, they require more time to offload, duplicate, and verify, meaning that the period of vulnerability is growing longer.\nBut emerging technologies are creating new workflows that circumvent these drawbacks. Among the most promising is server-based recording.\nRather than relying on disparate components that must be passed back and forth between different individuals on a set, server-based recording allows productions to streamline their workflows and unify everything through one interconnected system. All of the components can be plugged into a single network switch and communicate with one another directly. Cameras and audio devices send uncompressed media directly into the switch. The network feeds them into a digital recording server (such as a Pronology’s mRes or Sony’s PWS 4500), which takes the uncompressed data and encodes the signals into ready to edit files. These files are then sent back into the network, which in turn sends them to any desired network-attached storage devices (such as SmallTree’s TZ5 or Avid’s ISIS & NEXIS platforms). The moment the recordist hits the Stop button, he or she can open the files on a computer and bring the newly created clips into a nonlinear editing application in order to assess their viability. This method eliminates the intermediate process of utilizing memory cards, transfer stations, and shuttle drives in favor of writing directly to external storage and thus removes both the time and risk associated with manual offloading. It also offers instant piece of mind to both the person handling the media and the production as a whole that the work that has been done throughout the day is, in fact, intact and ready for post-production.\nAnd this is only the most basic of network-based workflows.\nBy utilizing advanced encoder systems, such as the aforementioned mRes platform, multiple tiers of files can be distributed across multiple pieces of network-attached storage. This gives the recordist the ability to simultaneously create both high-quality and proxy-grade video files and to make multiple copies of each in real time as a scene is being shot. This eliminates the potential need for time-consuming transcodes after the fact and, more importantly, this instant redundancy removes the key period of danger in which only a single fragile copy of the production’s work exists. As a result, recordists can now unmount network drives mere minutes after productions wrap and turn them over for delivery to post with one hundred percent certainty that there are multiple functioning copies of their work from the day. There is no need to spend several hours after wrap each day offloading cards and making backups.\nOr, to take things a step further, productions can take advantage of the inherent beauty that is the internet to skip the need for the shuttle process altogether. It is possible to create files in a manner that sends them directly to a post-production edit bay. With low bitrate files or a high-capacity upload pipeline, recordists can set up their workstations using transfer clients (such as Signiant Agent or File Catalyst) to take files that are created in a particular folder on their network-attached storage and automatically upload them to a cloud-based server, where post-production teams can download them for use. This process has the distinct advantage of sending editors new files throughout the day in order to accommodate a tight turnaround.\nConversely, for productions where the post-production team may be located on site, a hard line can be run from the recording network directly to the edit bays. By assigning the post team’s ISIS server (or comparable network attached server) as a recording destination, editors gain access to files while they are recording. In cases such as this, the production may opt to use “growing” Avid DNxHD files. This format takes advantage of Avid’s Advanced Authoring Format in order to routinely “close” and “reopen” files, allowing editors to work with them while they are still being recorded. For productions with incredibly tight turnarounds, this is the single fastest production to post-production workflow possible.\nAll of this makes server-based recording an incredibly versatile tool. However, it is not without its limitations. At this time, network-based encoders are limited to encoding widely available intermediate or delivery codecs, such as Apple ProRes or Avid DNxHD. Without direct support from companies with their own proprietary formats, they cannot output in formats such as REDCODE or ARRIRAW. Furthermore, setting up a network of this nature requires persistent power and space. It is also worth considering that, like most new technologies, server-based recording often comes with a hefty price tag. These limitations make the process unsuited for productions hoping to take advantage of the full range of Red and Arri cameras, productions in remote or isolated locations, and low-budget productions.\nSo when is it most appropriate or necessary to take advantage of this emerging technology? While it can be of use in a single-camera environment, this method of recording truly shines in live or archaically termed “live to tape” multi-cam environments, where anywhere from three to several dozen cameras are in use. After all, if a show records twelve cameras for one hour, the media manager suddenly has to juggle twelve hours’ worth of content. It is much easier to write all twelve to a network-attached storage unit than to offload all twelve cards one by one. Also, due to the fact that network-attached storage drives can be configured to store hundreds of terabytes, the process is ideally suited for live events or sports broadcasts where stopping and starting the records risks missing key one time only moments. But above all, it is best used when time is critical. The ability to bring files into a nonlinear editing system as they are being recorded and work in real time is a game changer for media managers, producers, and editors alike.\nThis technology is already revolutionizing the way television productions approach on-set media capture and it is still in its infancy. It will continue to grow and evolve. Given time, it is my sincere hope that it will find its way into the feature film market and become more practical for smaller productions to adopt. For the time being, Local 695 Video Engineers should begin to take note of what is available and familiarize themselves with the technology so that they are prepared to take advantage of the technology in the future."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:277ca97c-1ff7-41f3-ae8f-0267ce9d86bc>","<urn:uuid:30810c78-ca98-4437-bb90-186bcefed5e2>"],"error":null}
{"question":"How do Moderna Museet's exhibitions showcase both Western and Latin American artists, and what distinguishes Latin American avant-garde from Western art practices?","answer":"Moderna Museet presents diverse exhibitions ranging from Western artists like Giacometti and Cindy Sherman to Latin American artists like Gabriel Orozco and Adrián Villar Rojas. While Latin American avant-garde art has been influenced by Western artistic practices, it remains distinct due to its unique hybridization of indigenous, European, and African ancestral elements. Latin American avant-garde is characterized by strong social and political awareness, expressing the realities experienced by its people, and shouldn't be considered derivative of Western practices but rather recognized alongside Western art as its own ground-breaking discipline.","context":["Alberto Giacometti forged a singular path within European modernism, restlessly seeking a new language for sculpture as a “double of reality”. Today, his elongated, pared-down figures have become iconic. Moderna Museet’s latest exhibition Giacometti – Face to Face is the first large-scale presentation of Giacometti’s work in Sweden in over twenty years and Moderna Museet’s Director Gitte Ørskou predicts the exhibition to be a future classic.\nYayoi Kusama is one of the most acclaimed artists working today. Since the 1950s, she has created art that is as personal as it is universal. Like few other artists she moves freely between painting, sculpture and installations, between art and design, and between East and West. In summer 2016, Moderna Museet and ArkDes are […]\nModerna Museet presents the first exhibition of the Argentinian artist Adrián Villar Rojas in Scandinavia. Villar Rojas compares his practice to a virus that lives in and off the art world. Although his works are rarely overtly political, they are ideologically charged in how they are produced and positioned outside any economy. Time as the […]\nAkram Zaatari’s work method can be compared to that of an archaeologist – he excavates images, memories and stories. His interest in subjective historiography delves deep into our recent history, focusing on documents that can be personal, not to say intimate. Zaatari often works with existing documents such as photographs, diaries and sound recordings. His […]\nA focused examination of thirteen large-scale masterworks presented in a series of telling juxtapositions, Sculpture After Sculpture traces the parallel developments of Katharina Fritsch (b. 1956), Jeff Koons (b. 1955), and Charles Ray (b. 1953). Beginning with iconic works from the late ’80s and early ’90s, which highlight the artists’ shared relationship to the commodity […]\nMid-Sentence at Moderna Museet is Nina Canell’s first solo exhibition in Sweden. She has previously exhibited at a number of prominent international museums and institutions. Canell’s works generally consist of materials that are normally used for specific purposes – such as nails, electricity, air, water, chewing gum and sound. In this exhibition, they trigger associations […]\nThe exhibition A Way of Life – Swedish Photography from Christer Strömholm until Today features more than 350 works by 29 photographers from Moderna Museet’s rich collection of photography. From the seminal Christer Strömholm, via influences such as Anders Petersen, Tuija Lindström and Kenneth Gustavsson, to three younger, current photographers, the exhibition highlights the intimate […]\nGabriel Orozco – Natural Motion\nStockholm 14 February, 2014 – 4 May, 2014\nGabriel Orozco is one of the best-known artists of his generation but has never before been shown in the Nordic countries. His reputation has been growing steadily since the late-1980s, and his oeuvre opens up for a new form of conceptualism that embraces both deep contemplation and precise craftsmanship. In Moderna Museet’s ambition to feature art from beyond the Western mainstream, Gabriel Orozco is essential to understanding the contemporary world.\nModerna Museet opens its 2014 art season with the exhibition Dance Machines – From Léger to Kraftwerk. The exhibition focuses on the modernists’ fascination with the machine and mechanised everyday life, and how Kraftwerk has explored the relationship between man and machine in multidisciplinary projects. Dance Machines – From Léger to Kraftwerk opens on 22 […]\nCindy Sherman – Untitled Horrors\n19 October, 2013 – 19 January, 2014\nCindy Sherman is one of the most important artists today. Since her breakthrough in the early 1980s, she has revolutionised both contemporary art and attitudes to photography. Sherman’s works have had a profound impact on our ideas of the construction of identity, and on the mythogenic power of pictures. The exhibition Cindy Sherman – Untitled Horrors emphasises the undercurrent of horror and fascination that runs through Sherman’s entire output.\nHilma af Klint – A Pioneer of Abstraction\nModerna Museet, Stockholm\n16 February – 26 May, 2013\nOne hundred years ago, Hilma af Klint painted pictures for the future. By 1906, she had developed an abstract pictorial imagery – some years before artists such as Kandinsky, Malevich and Mondrian. Moderna Museet is celebrating Hilma af Klint as a pioneer of abstract art and one of Sweden’s greatest artists.","Avant-garde is a French term originally used to describe “the foremost part of an advancing army or naval force” (Oxford English Dictionary Online-vanguard), but has since been appropriated to indicate “new and experimental ideas and methods in art” (Oxford English Dictionary Online-avant-garde). The Latin American avant-garde art has a rich and colourful history that often remains overlooked by Western academia. It is characterized by an awareness of and reaction to the region’s turbulent and sometimes violent social and political history.\nAvant-garde artists are seen as being on the forefront of the boundaries of artistic practice, experimenting before the public is able to catch up. They are not bound by the strict rules of academic realism that was so popular in the past and thus have the luxury of depicting subject matter that is not instantly recognizable. This opportunity for personal subjectivity promotes aesthetic experimentation leading to evolution of the artistic process (Ramirez and Olea, 2004). The avant-garde rises to the challenge of resolving various conflicts within the creation and appreciation of art. In Paraguay, for example, artists sought to resolve “the opposition between the autonomy of form and the expression of meaning” (Sullivan, 1996). Latin American avant-garde artists deserve the same level of acclaim awarded to Western artists.\nWhat is specifically Latin American cannot be easily designated, but the term has come to signify artists residing in Latin America and artists from Latin American who currently reside in other regions (Sullivan, 1996). The vast amount of ethnicities, cultures and experiences denies the possibility of a universal artistic style, thus all Latin American artists cannot be confined to a particular movement.\nA key element of Latin American culture, which in turn is represented in its art, is hybridization. A mix of ethnicities come together to contribute different elements, creating a rich and unique culture. Although Western artistic practices have been key in igniting artistic experimentation in Latin America, Latin American art remains its own distinct entity and is not derogative of or lesser than Western art. Indigenous, or pre-Colombian, art also contributed techniques and inspiration for avant-garde practices, but it too is a separate genre.\nArtistic experimentation in Latin America is known for expressing strong values concerning the social realities experienced by its people. To use Diego Rivera as an example highlights his method of involving the integration of his personal political values into his art. A supporter of communism, Rivera included Vladimir Lenin in his mural “Man at the Crossroads,” despite disapproval from his American clients. The mural was destroyed before its completion but Rivera went on to recreate it back in Mexico (Lucie-Smith, 1993). There also exists in Latin American culture a shared experience among many a sense of marginalization and dislocation, whether literal or not. Cuban artist Ana Mendieta, for example, was sent away from her family during her childhood to the United States. This sense of exile never left her and is a recurring theme in her work (Sullivan, 1996). These are two examples of artistic practice that push established boundaries and draw the public forward with it.\nAnother way that the political circumstances affect artistic development is the state of the government. The multiple coup-d’états in Brazil not only influenced the opinions of artists but also determined how free they were to express them. Under the rule of Brazilian dictator Getúlio Vargas from 1930 to 1945, for example, there was state support for art because it was seen as a promotion of nationalism (Sulllivan, 1996). Consider a different region: Rivera’s murals in Mexico may not have been so successful if they had not been sanctioned by the state. The political circumstances have a significant effect on the direction and strength of artistic experiment.\nWriter Ferreira Gullar said that “avant-garde art should spring from an analysis of a given country’s social and cultural characteristics, never from advanced ideas imported intact from the developed countries to which they properly apply” (Traba 1994). This sentiment describes the appropriation of Western art into the Latin American avant-garde. Artists work with and further develop movements that are prominent in Western art into their own experimentation, changing Western practices into one that reflects Latin America. As well as partaking in these established movements, the Latin American avant-garde creates some of its own, such as Constructive Universalism – the combination of a variety of movements such as Americanism; Primitivism and Classicism – and Muralism (Sullivan, 1996).\nBecause of the many influences of different ethnicities from within the continent, Latin American art production is a fluid and malleable process of hybridization, using the land’s colonial history and the indigenous, European and African ancestral elements of the culture and people to contribute to the overall artistic practices. Foreign influence also comes in the form of immigration and international study. This hybridity is what makes the Latin American avant-garde unique from the artistic experimentation present in other areas of the world. Avant-garde art in Latin America pushes the limits through a process of being inspired and influenced from both within and beyond its own culture.\nAvant-garde art in Latin America is its own ground-breaking discipline, not a derivative of Western practices. As a result of the varied ethnicities and influences that became a part of the culture as the region broke away from its colonial past, this hybridization unique to Latin America presents a particular cultural experience that influences artistic practices. Avant-garde experimentation also benefits because of a personal experience with the shared political circumstances that often lead to social unrest. This awareness is expressed in many of the works of the region. Latin America possesses a strong avant-garde that deserves its own recognition alongside Western art with artists who use their political awareness and personal or cultural hybridized backgrounds to push artistic experimentation forward.\nChaplik, Dorothy. Defining Latin American Art / Hacia una definicícion del art latinoamericano. Jefferson, North Carolina: McFarland and Company, Inc., Publishers, 2005. Print.\nDezeuze, Anna. “Border Crossing.” Tate etc. Summer 2005. Web.\nLucie-Smith, Edward. Latin American Art of the 20th Century. London: Thames and Hudson Ltd., 1993. Print.\nLucie-Smith, Edward. Visual Arts in the Twentieth Century. New York: Harry N. Abrams, Incorporated: 1997. Print.\nRamirez, Mari Carmen, and Héctor Olea. Inverted Utopias: Avant-Garde Art in Latin America. New Haven and London: Yale University Press, 2004. Print.\nRodríguez, Eduardo Luis, John Beusterien, and Narcisco G. Menocal. “The Architectural Avant- Garde: From Art Deco to Modern Regionalism.” The Journal of Decorative and Propaganda Arts 22 (1996): 254-277. Web.\nSullivan, Edward J. Latin American Art in the Twentieth Century. New York: Phaidon Press Limited, 1996. Print.\nTraba, Marta. Art of Latin America: 1900-1980. Baltimore: John Hopkins University Press, 1994. Print."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:082ca5c7-716a-4039-85fa-45e8f9b50abf>","<urn:uuid:e03b0e63-fe19-4363-b8e1-8f6e10c5661f>"],"error":null}
{"question":"How does footwear affect the quality of tango dancing?","answer":"Proper footwear is essential for tango dancing as it directly impacts the connection with the music and partner. Good shoes enable easy movement across the dance floor, help maintain proper body alignment, and develop strength and balance. Dancers spend most of their time with feet close to the ground, balancing on the balls of their feet, so shoes must be flexible enough to support small movements while rotating on the heel. The material choice also matters - leather shoes are more sturdy but slippery, while suede shoes provide more grip and flexibility but require more maintenance.","context":["Tango is an emotional and instinctive music, which means that easy movement across the dance floor is crucial for establishing a good connection with the music and reacting to the partner's moves. Since the tango first emerged from the working-class establishments in Buenos Aires and Montevideo and reached higher-end dance floors in both Argentina and the rest of the world, the dancers of tango preferred to have a softer type of shoes that eventually solidified into purpose-built dance shoes that are perfect for enjoying tango music and tango dance.\nBeginners in the world of tango dancing will need to ensure that their feet are well protected in comfortable footwear that will enable them to effortlessly move across the dance floor during practice sessions, social dancing, and competitions. Choosing a right shoe is crucial for getting a sense of awareness how your body is interacting with the ground, reaching the proper alignments of the body, and developing the strength, balance, and confidence that is needed for execution of perfect tango dance.\nAs a rule of thumb, the tango dancing beginners do not need to purchase purpose-built tango shoes immediately, since they can get the most of the benefits of such shoes by getting a good jazz dancing shoes. Men's jazz shoes are usually made of leather, with not to still uppers and a flexible sole that will enable them to keep a good awareness of the dancing floor. If jazz shoes are also unavailable, men can also go for ordinary leather shoes, but it is highly recommended to practice in models that have softer soles . Some beginners (and even seasoned veterans) of tango dance also prefer to do some training sessions wearing nothing but socks, which enables them to train how to keep certain positions and alignments of their body.\nTango can be danced by anyone, but good footwear is essential for proper execution of its moves\nWomen’s tango footwear is famous for being inclusive to higher heels that go all the way to 9 cm of height, but beginners are recommended to stick to dancing shoes that are more closer to the ground. The novice female dancers are even recommended to take flat dancing shoes with the minimal heel, which will enable them to gain needed confidence when practicing faster moves, more elaborate tango dance forms, and proper body alignments. When the basics are covered, women are encouraged to switch to shoes with a higher heel (which have to be strong and durable) because such shoes will enable them to more easily place their weight on the ground.\nAnd of course, one of the most important things that you need to take in consideration when picking tango shoes for yourself is to make sure that you feel comfortable in them. They would not be so tight that they prevent circulation and cause sores, but it is not recommended to practice in too loose footwear. Both men and women will spend a majority of time dancing with feet being very close to the ground, and their balance being held on the balls of their feet. Having proper shoes that are flexible enough to support small feet movements while you rotate on your hell and the uppers are not stiff and causing you irritation is crucial to your feeling great while dancing the tango. Novices are also discouraged to use footwear that is slippery and that have opened toe (for added protection). Commonly, leather shoes are more sturdy but slippery, while suede shoes provide more grip and flexibility but do require more maintenance.\nDifferences between regular street shoes and tango dancing shoes for men is less apparent than in female shoes, but while these differences are not immediately visually apparent, their impact on the feel in the foot is easily noticeable by tango dancers. Having good tango shoes means not only that men will feel more comfortable in them during their practice, social dances, and competitive dance, but also that they will get an immediate advantage that will help them to achieve a better connection with their partner, dance floor, and music.\nMen tango footwear should have classic form\nHere are the most important features of men tango shoes:\nThe differences between regular street shoes and tango dancing shoes are of course much stronger for females than for males. The changes are both visual and structural. Tango is well known to be a highly elegant dance, and female dancers enjoy to find extravagant and stylish shoes that will also fit the requirements of their dance. Female dancers also need to take special care about preserving their feet because their move sets will put a lot of strain on their heels and ball of the foot when performing “pointe” moves, pivoting and taking numerous powerful steps in quick succession. If female dancer starts feeling any foot issues or aggravating existing foot issues, she should immediately contact a doctor or a specialist about picking proper shoes that will provide better comfort.\nWomen tango shoe has to have enhanced heel\nFemale tango shoes are distinguished by the following features:"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:aa0401a2-7f53-43f4-8990-84c9d793e3ca>"],"error":null}