{"question":"I've been researching historical weather patterns. Could you explain how people's understanding and descriptions of hurricanes differed between the 1815 New England region and 1824 Georgia based on available records?","answer":"The records show distinct differences in how these storms were described and understood in different regions. In 1815 New England, people typically called such events 'gales' or 'tempests' and viewed them through a holistic lens that connected them to other natural phenomena like earthquakes and volcanism. They understood weather through folklore and almanacs, believing in an 'electric fluid' that connected all natural phenomena. In contrast, by 1824 in Georgia, records show more specific hurricane-related terminology and detailed observations of storm characteristics, as evidenced by precise descriptions of wind direction changes, timing of the eye passing over Darien, and specific measurements of storm surge heights ranging from 6-12 feet in different locations.","context":["This piece originally appeared in Zócalo Public Square.\nTwo hundred years ago this week, the Great September Gale struck New England. The “gale” swamped the coastlines of five states with storm surges up to 15 feet. It reduced dozens of ships in Boston; Providence, Rhode Island; and other harbors to matchsticks and destroyed houses, churches, and barns from Long Island to New Hampshire. Forests were leveled, with trees torn up at the roots. High winds hurled broken glass, bricks, and slate roof tiles through the streets of urban areas. The storm bent the steeple of Old South Church in Boston. Most modern sources record the death toll at 38, but it could have been considerably higher.\nThough people didn’t use the word hurricane much back then, it assuredly was one, most likely a Category 4. The Great September Gale was the second major hurricane to strike New England since the coming of white settlers, and since that time, only the famous “Long Island Express” hurricane of 1938 is believed to have been more destructive.\nI came upon the Great September Gale while I was researching Americans’ perceptions of climate during the 1810s. This period is particularly interesting because the climate all around the world temporarily cooled for about a decade. And at the same time, many social institutions, including science, were in significant transition. The September Gale is one of numerous weather and climate events that people remembered and recorded, especially in their diaries. Some peoples’ descriptions of the 1815 gale took me back to a tornado I experienced as a teenager in Nebraska in 1988. After that storm, the streets of Omaha filled with branches and bricks, much like Boston’s after the gale. When I read a newspaper account, the centuries just telescoped into a common moment in time.\nHurricanes have loomed large in our consciousness lately. Last month, the nation observed the 10th anniversary of Hurricane Katrina, an event whose meaning and legacy we are still quite painfully trying to evaluate. Unlike Katrina, the New England hurricane of September 1815 did not cause significant demographic shifts or provoke hard questions about our responsibilities to care for each other in disaster situations, but it does provide an opportunity to consider Americans’ attitudes toward storms—and our climate—then and now.\nIn 1815, most New Englanders called events of this size and intensity a gale or tempest. In an era before detailed weather forecasting became widespread, people tended to view extreme weather events as simply a natural feature of the places where they lived. Their understanding of weather and scientific phenomena came from the kind of earthy knowledge, colored by folklore and astrology, found in popular farmers’ almanacs. For example, the Boston-published Physician’s Almanac for June 1817 is a dizzying jumble of planetary and zodiac symbols and precise notations of tides and solar positions. Simple advice such as “Good weather for vegetation, if not too cold” and “Look for much rain” is interspersed with platitudes and sayings such as “Weakness is the only incorrigible fault men have.”\nAmericans’ views about weather and climate in the 1810s were more holistic than ours today. Gales and tempests were related, many people thought, to phenomena like lightning, volcanism, and earthquakes. The 1815 storm occurred only a few years after the powerful New Madrid earthquakes in Missouri commanded public attention and less than a year before the bizarre climate anomalies of the “Year Without Summer” (1816)—which involved snowstorms in June and killing frosts in the dog days of August. In the 1810s, the idea of an “electric fluid” surrounding and suffusing the world—disturbances in which manifested themselves as earthquakes, waterspouts, hurricanes, and thunderstorms—was quite mainstream. “[A]ll observation convinces us,” wrote one author in an 1812 article in the Port-Folio, referring to this electric fluid, “that these phenomena are marked by its presence and arise from its agency.” For most people the Earth, the oceans, the atmosphere, and the heavens above it were all interconnected systems like those of a human body.\nWe ask very different kinds of questions today about the weather—and they mostly concern the future. Is that tropical depression going to become a hurricane? How strong is it going to be? Where will it come ashore? Many of the factors that made Katrina such a painful experience involved this future dimension, centering upon questions about what authorities should have known or done in advance of the storm’s landfall. It’s right that we ask these questions; the people of Boston and Providence could not prepare for the 1815 gale in advance. When they talked about weather, their preoccupations centered mostly around agricultural concerns: when to plant your winter wheat and when frosts could be expected that might damage your corn. Their conception of future weather was vague and mostly cyclical, tethered very tightly to natural rhythms observed in previous seasons. For them, the most important weather was that of the past.\nOur current emphasis on weather in the future tense—and our reliance upon minutely specialized fields of expertise to explain scientific questions—has made it harder for us to appreciate specific weather events as part of a global ecosystem. Hurricanes are not a singular phenomenon. They are powered by warm sea-surface temperatures and set in motion by wind patterns. The scientific consensus is that industries and other human activity are affecting these processes, driving up sea-surface temperatures, increasing the intensity of hurricanes. Yet the connection between climate change and hurricanes is somewhat murky in the public consciousness because it’s impossible to prove that this individual hurricane or that one was “caused” by global warming. The compartmentalization and specialization of modern scientific disciplines has greatly increased our overall scientific understanding, but sometimes it makes it harder for us to see the forest for the trees.\nFor many New Englanders in 1815, it was intuitively obvious that everything in the sky, and most everything on the ground, was connected somehow to everything else. It would have come as no surprise to vintners in 1815 New England that wine grapes harvested in the aftermath of the Great September Gale tasted like salt. Some took the interconnectedness a step further, seeing in it a higher meaning. Shortly after the gale, a deeply religious young woman wrote in her diary, “[W]ho can tell what providence has designed by this judgment? We cannot think ’tis sent for nothing.”\nIn 2015, we obviously know much more about hurricanes than we did 200 years ago, but looking back closely and carefully can be useful, too. History is more than a laundry list of problems or mistakes that we keep solely so we can avoid repeating them. Sometimes it’s a reminder that the view from the present can be more narrow and restrictive than the view from the past.","Darien in McIntosh County, Georgia — The American South (South Atlantic)\nSugar Mill - Rum Distillery Ruins\nThese buildings, constructed of tabby by William Carnochan on his huge sugar plantation at \"The Thicket,\" followed closely plans laid out by Thomas Spalding of Sapelo. The sugar works and rum distillery were operated successfully on a commercial scale until 1824, when a hurricane tore off the roof and upper story of the mill and cane barn, and destroyed other buildings.\nErected 1957 by Georgia Historical Commission. (Marker Number 095-27.)\nMarker series. This marker is included in the Georgia Historical Society/Commission marker series.\nLocation. 31° 25.87′ N, 81° 23.659′ W. Marker is in Darien, Georgia, in McIntosh County. Marker is at the intersection of Ridge Road (State Highway 99) and Tolomato Causeway (Mission Road) on Ridge Road. Touch for map. Marker is in this post office area: Darien GA 31305, United States of America.\nOther nearby markers. At least 8 other markers are within 5 miles of this marker, measured as the crow flies. Sapelo Island (approx. 2.4 miles away); Ardoch (approx. 3.6 miles away); Ashantilly St. Andrew's Cemetery (approx. 3.8 miles away); Fort Barrington (was approx. 3.9 miles away but has been reported missing. ); Old River Road (was approx. 3.9 miles away but has been reported missing. ); Birthplace of John McIntosh Kell (approx. 4.4 miles away); Old Meeting House (approx. 4.6 miles away). Touch for a list and map of all markers in Darien.\nAlso see . . . NOAA, National Weather Service, Southern Region. Year:1824 Date(s): 14-15 September Principle Affected Area(s): Upper Georgia - major hurricane\nUpper Georgia coastal waters - major hurricane\nLower Georgia - major Hurricane\nLower Georgia coastal waters - major hurricane\nNortheast Florida - hurricane\nNortheast Florida coastal waters - major hurricane\nLandfall Point(s): Near Darien and St. Simons Island; The eye moved directly over the town of Darien, Ga. Remarks: Center probably skirted north of Caribbean islands and along the outer Bahamas. Exceeded 1804 storm in flooding and damage. St. Simons Island completely overflowed. 83 persons killed on this island. (not listed in Georgia by Dunn and Miller and is only indicated as being “Along\nRobert Durfee’s Journal and Recollections of Newport, Rhode Island, Freetown,Massachusetts, New York City & Long Island, Jamaica & Cuba, West Indies & Saint SimonsIsland, Georgia ca. 1785-1810. Edited by Virginia Steele Wood, 1990, NOTES, Page 101.\nJohn Couper’s losses were estimated to be between 50 and 60 thousand dollars. Thomas Spaulding of Sapelo island reported, a wall of water six feet high sweeping across the island and losses of 40 to 50 thousand dollars. The lighthouse on the south end of Sapelo was destroyed, with the sea running from 6 to 8 feet inside the structure. The beacons on Wolfs Island were likewise destroyed. The keeper of the Wolf’s Island beacon reports the door “Stove in, the sea breaking from 10 to 12 feet, which destroyed the whole of the oil.” Sullivan, Buddy. Early Days on the Georgia Tidewater:\nWilliam Carnochan’s Sugar Mill and rum distillery near Darien (at the Thicket) was destroyed and never rebuilt. “The islands on the coast were all under water. On the mainland at Carnochan’s flat, where Mr. [Richard L.] Morris and Mr. [T.P.] Pease now reside, at the Hudson place, and at Colonel Harrison’s (a continuation of the same flat ground) the tide rose over ten feet above the surface.” Ibid, Page 210.\nThe eye passed directly over the town of Darien, “The wind was first from the north and east. It blew with the greatest violence between eleven and twelve o’clock at night, after which it suddenly ceased. For half an hour it was nearly calm. Then it shifted to the west, and for some hours it blew more fiercely than ever. Under this change of direction the water fell rapidly, and many captives in trees and homes were permitted to come down and out.” Ibid., Page 156.\nThe Savannah Georgian, of 25 September 1824, reported, “The storm at Darien, and its neighborhood. Exceed that of 1804, both in violence and destruction.” This is confirmed by reports from the Darien Gazette over the next two weeks. The beaches of Cumberland Island, near the St Johns bar and at St Augustine were covered with wreckage.\nSummary: Storm Tide values support major status and based (Submitted on September 12, 2008, by Mike Stroud of Bluffton, South Carolina.)\nCategories. • Industry & Commerce • Landmarks • Notable Buildings •\nCredits. This page was last revised on June 16, 2016. This page originally submitted on September 12, 2008, by Mike Stroud of Bluffton, South Carolina. This page has been viewed 1,388 times since then and 40 times this year. Photos: 1. submitted on April 4, 2011, by Mike Stroud of Bluffton, South Carolina. 2, 3, 4, 5, 6. submitted on September 12, 2008, by Mike Stroud of Bluffton, South Carolina. • Kevin W. was the editor who published this page."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:51d6d7cf-374f-41de-9ca4-0e5739a0ee09>","<urn:uuid:27e4b54b-9bf0-40d3-bc04-98389d8b8cbc>"],"error":null}
{"question":"How do string theory and Leibniz's Analysis Situs compare in their mathematical ambitions?","answer":"Both string theory and Leibniz's Analysis Situs represent ambitious mathematical projects, but with different approaches. Leibniz's work aimed to establish a new geometrical science focused on understanding the philosophy of space and metaphysics. String theory, while mathematically sophisticated and leading to exciting mathematical discoveries (even earning Witten a Fields Medal), has been criticized for being pure mathematics disguised as physics, with no experimental basis. The key difference is that Leibniz's work was explicitly presented as a philosophical-mathematical investigation, while string theory claims to describe physical reality despite lacking experimental validation.","context":["Geometry and Monadology by Vincenzo De Risi, , available at Book Depository with free delivery worldwide. But even so, De Risi’s Geometry and Monadology is a brilliant work, a virtuoso performance. He presses his larger argument with intelligence. Download Citation on ResearchGate | Geometry and Monadology | This book reconstructs, from both historical and theoretical points of view, Leibniz’s.\n|Published (Last):||4 November 2005|\n|PDF File Size:||13.6 Mb|\n|ePub File Size:||13.66 Mb|\n|Price:||Free* [*Free Regsitration Required]|\nDispatched from the UK in 3 business days When will my order arrive? Home Contact Us Help Free delivery worldwide. Leibniz’s Analysis Situs and Moadology of Space. Description This book reconstructs, from both historical and theoretical points of view, Leibniz’s geometrical studies, focusing in particular on the research Leibniz carried out in his final years.\nThe work’s main purpose is to offer a better understanding of the philosophy of space and in general of the mature Leibnizean metaphysics.\nThis is the first ever, comprehensive historical reconstruction of Leibniz’s geometry. The Best Books of Check out the top books of the year on our page Best Books of Product details Format Hardback pages Dimensions x x Illustrations note XX, p. Looking for beautiful books? Visit our Beautiful Books page and find lovely books for kids, photography lovers and more.\nGeometry and Monadology : Vincenzo De Risi :\nOther books in this series. Patterns of Change Ladislav Kvasz.\nScenes from the History of Real Functions F. The Moon that Wasn’t Helge Kragh.\nExpounding the Mathematical Seed: Expounding the Mathematical Seed. The Translation Translation v. Gnomes in the Fog Dennis E. Symmetrie Gruppe Dualitat E.\nGeometry and Monadology : Leibniz’s Analysis Situs and Philosophy of Space\nConvolutions in French Mathematics, Back cover copy This book reconstructs, from both historical and theoretical points of view, Leibniz’s geometrical studies, focusing in particular on the research Leibniz carried out in the last years of his life. Monadollgy is indeed the gemetry ever comprehensive historical reconstruction of Leibniz’s geometry that meets the interests of both mathematicians and philosophers.\nThe main purpose of the work is to offer a better understanding of the Leibnizean philosophy of space and mature metaphysics, through a pressing confrontation with the problems of geometric foundations. Regarding the scope of these problems, the book also deals in depth with Leibniz’s theory of sensibility, thus favouring the comparison and contrast between Leibniz’s philosophy and Kant’s transcendentalist solution.\nThe Appendix references to a number of previously unpublished manuscripts on geometry from the Leibniz Archiv in Hannover, which disclose new theories, points of view and technicalities of Leibniz’s thought.\nI find anv [De Risi’s] contribution to the debate on the reality of corporeal substances to be at once original and decisive. And finally, I am hugely impressed by the expertise he has brought to bear on both the purely formal and the deeply metaphysical sides, each requiring vastly different but mohadology considerable competences.\nI am impressed by the original way in which he makes sense of the phenomenalistic strains geometryy Leibniz’s thought by connecting them with the metaphysics of expression, and this in turn with the foundation of real space.\nIn sum, this [book] is an extraordinary accomplishment. Arthur, McMaster University I believe that this is an extraordinary [book] which sets geometrt standards for Leibnizean scholarship–and, in particular, for historical and philosophical investigation into the relationship between Leibniz and Kant.\nGeometry and Monadology: Leibniz’s Analysis Situs and Philosophy of Space by Vincenzo De Risi\nFriedman, Stanford University show more. Table monwdology contents Historical Survey. Review quote From the reviews: I find his contribution to the debate on the reality of corporeal substances to be at once original and decisive. In sum, this dissertation is an extraordinary accomplishment. Arthur, McMaster University I believe that this is an extraordinary dissertation which sets new standards for Leibnizean scholarship-and, in particular, for historical and philosophical investigation into the relationship between Leibniz and Kant.\nIt deals with G. Leibniz’s lifelong project of founding a new geometrical science, the yeometry situs. In sum the author has provided a great survey monaodlogy Leibniz’s writings on geometry presented with deep analyses in several fields of mathematics and metaphysics, providing a vivid impression of the interconnectedness of Leibniz’s general system of science. Book ratings by Goodreads.\nGoodreads is the world’s largest site for readers with over 50 million reviews. We’re featuring millions of their reader ratings on our book pages to help you find your new favourite book.","Can ever more abstruseness and distance from experimental results reveal physical truths? asks Nobel laureate Philip Anderson.\nFour centuries ago Francis Bacon, in his Novum Organum , outlined the philosophy that came to be the distinguishing characteristic of modern science. This philosophy held that knowledge of the nature of things was to be gained by the acute observation of nature, not by the study of authoritative texts or of holy books, or from imaginative flights of human fancy. The resulting explosive growth in our understanding of the universe and of our ability to manipulate it cannot be gainsaid; whatever one may say about the technical ingenuity of the medieval Chinese or the early mathematical discoveries of the Indians and the Arabs, one has to concede that nothing remotely resembling modern systematic science developed in those cultures.\nMany of us in the physics community have become increasingly disturbed by the growing hegemony in a major subfield of our subject of what we see as a revival of the medieval, pre-Baconian mode of thinking: that the universe is designed on some simple basic principle that can be discovered by the exercise of pure reason, unaided, in fact unencumbered, by experimental study. Such would seem to be the thinking of the community of \"string theorists\", who in the past two decades have achieved a dominant position in theoretical physics. Every research physics department worldwide with any pretensions looks to have its own string-theory group of at least two, since only a few string theorists talk physics to anyone else. String theory has produced at least three media superstars, whose books and TV interviews bring to physics departments flocks of students ambitious to join this brilliant enterprise, which, it has been said, reveals \"the language in which God wrote the world\".\nPeter Woit, in a strongly argued and serious book, has taken on the task of analysing this situation. Woit, a mathematician at Columbia University, is neither a sour-grapes sorehead nor a sensationalist amateur, as he makes clear by describing the complex mathematics of string theory in what may be excessive detail for many readers without, so far as I can tell, many really serious errors. He is an admirer of Ed Witten, the fabulous mathematician-physicist who has been the guru of string theory for several decades. Woit took his degree at Princeton University, learning quantum-field theory from several of its most influential modern exponents, and he keeps in touch with the field through numerous contacts. Indeed, after spending several chapters on the history of modern particle theory, Woit adds a long and highly technical chapter praising the many exciting mathematical discoveries that have resulted from the complications of superstring theory - mathematical developments that, inter alia , earned Witten an unprecedented and well-deserved Fields Medal and contributed to the stellar career in mathematics of Sir Michael Atiyah, former president of the Royal Society.\nHow then can the enterprise on which all these unquestionably brilliant people are engaged have gone so far off the rails as to merit the physicist Wolfgang Pauli's famous put-down, \"That's not even wrong!\"? This needs a little history, which Woit describes in the early chapters. (The experimental bits are a bit shaky, though.) The history concludes with an unexpected and glorious success: the so-called standard model, which was constructed in roughly 1965-75, by many of the same theorists who carried on into string theory. This marvellous structure classifies all of the bewildering array of known \"elementary\" particles and fields by means of three families of four \"particles\" (fermions) each, and two \"gauge\" symmetry groups that imply two sets of \"fields\" (bosons) that are responsible for interactions of a particularly simple type among the particles.\nThe way in which this structural classification fell into place, and the great leaps of imagination involved, justifies a degree of hubris among the few dozen truly extraordinary individuals who discovered it. It has also the very special feature that the ostensible world picture we see at the ordinary chemical scale has very little resemblance to the underlying structure, but instead is emergent from it: for instance, neither nucleons nor light quanta are fundamental to the standard model (the electron happens to be). However, both this hubris, and the complexity of the result, fed the temptation to go on leaping, and to forget that each of these earlier leaps, without exception, had taken off from some feature of the solid experimental facts laboriously gathered over the years.\nIt is conventional to say that the standard model vitiated all of particle theory as we had come to know it because everything measured since 1975 has \"agreed\" with it. This is problematical; what is true is that measurements since 1975 have left the structure intact, but with modifications in detail. Some of these modifications are very ad hoc, such as those necessary to fit the observed weak-time asymmetry, and the neutrino masses and oscillations. As so modified, there remain at least 25 arbitrary constants that must be determined from experiment.\nWoit begins the story of strings with a cautionary tale: the sad fate of the fad known as \"particle democracy\" that overtook the theory world in the 1960s. The hope it offered - as later did string theory - was that the mere structure of certain equations would lead to a unique theory of the world. The dream lives on in the minds of a few adherents and in a couple of books still popular among New Agers. It was in the course of fumbling around with this mathematics that the defining ideas of string theory emerged and eventually began to catch the fertile imaginations of the particle-theory community.\nThe main ideas behind string theory are these. First, that what we had been treating as particles (or fields defined at space-time points) are not that, but are little wiggly strings that define a surface in space-time as they move. Second, that (for reasons related to working out a consistent theory of these objects) space is really at least ten-dimensional, with most of the dimensions curled up so tightly we cannot see them. Third, that, by similar logic, the underlying symmetry of space-time must be supersymmetry, a generalisation of the ordinary relativistic symmetry that requires, among other things, that for every fermion there must be a corresponding boson (in strict supersymmetry, of the same mass) and vice versa, since the generators of the supersymmetry group switch the two. Hence, super strings.\nI need hardly say that none of these ideas has any experimental basis. So why then are they taken seriously? Perhaps there are several reasons - reasons that do not include the possibility, or even the hope, of experimental confirmation.\nAt the outset, the main task was perceived as being to bring general relativity into the fold, a task at which ordinary quantum field theory has always failed. With this visionary postulate, it did seem possible to make a reasonable supergravity - in ten supersymmetric dimensions, of course.\nClassical gravity is satisfactory all the way down to the \"Planck\" scale, 10¹5 smaller than we can at present measure, so we can imagine - and the string theorists do imagine - that all kinds of strange things, such as the disappearance of the extra dimensions and the breaking of supersymmetry, happen at the intermediate scales.\nGravity has always had a very special cachet among mathematically inclined theorists because of its gorgeous mathematical expression, accompanied by the myth that it sprang full blown from Einstein's brain (which it did not; he saw its outlines already in 1907 as a consequence of experimental arguments and spent eight years learning the maths to do it right). Even though we now know that there are other ways to converge gravity, one may nonetheless concede that it was a useful exercise to create an example of a theory that does not fail at the Planck scale.\nTo my mind, the most valid point in favour of string theory is that the standard model exhibits several \"internal local symmetries\". These receive a natural interpretation in string theory as rotations in the extra dimensions - an idea that dates back to Einstein's time.\nI am far less sympathetic to a line of thinking in the theory that seems to be purely aesthetic. String theory (in some one of its many versions) has actually been justified as \"just too beautiful not to be true\". Woit is particularly dismissive of this claim, suggesting that intricacy, abstruseness and novelty are no substitutes for the simplicity of, for instance, the standard model. There is doubtless a fascination and excitement in being in possession of a particularly esoteric and complex body of knowledge or ritual. String theory has begun to seem an obsession, even a cult - the kind of thing that leads the young to wear T-shirts with slogans such as \"Why be stuck with only four dimensions?\"\nThe leading source of optimism, however, was the dream that superstrings would furnish a unique theory because of the constraints found on usable versions of the ideas. It is this dream that has evaporated as the mathematical understanding has increased - in fact, to the point that string theorists have come to accept that there are almost no uniqueness properties at all. In other words, one may have whatever universe one pleases. It may be said that there is so much freedom in string theory that our present universe, even at the elementary particle level, would be the result of historical contingency.\nFinally, Woit points out, a motivation for pursuing superstrings is often quoted as \"it's the only game in town\". The implication is that if you want a job in theoretical physics or a position at a prestigious institute, or even a MacArthur genius award, you had better learn string theory. Woit notes that with string theory, physicists are getting perilously close to validating the thesis of radical sociologists of science that at least this portion of science is socially constructed.\nIt is time to sum up. What is Woit's argument? He is not accusing the string theorists of egregious mathematical error - of course they are superb mathematicians. Rather, he accuses them simply of doing pure mathematics in physics departments, of redefining \"science\". One could not possibly object to the existence of an active mathematical community pursuing such an exciting, original line of work. The objection is to the claim that this work is physics, that it possibly, or even probably, will tell us how the real world constructs itself. One may particularly cavil at the high level of hype around string theory, to the point of monopolising popular attention, and that the gigadollars of a number of philanthropists, as well as numerous physics department employment slots, are being farmed out to what is really an esoteric branch of algebraic geometry.\nNot Even Wrong is written for the mathematically inclined. Woit seems to feel that it is essential for this very complex subject to be covered in a serious manner. The maths in the book may have been a mistake. It will open him to nitpicking on every issue he discusses and perhaps obscure the central issue, which may not be so very complicated: just what is the emperor actually wearing? He writes the non-mathematical parts of the book well and clearly, although not always without attitude. Still, as a solid-state theoretical physicist, I am pleased the book has been written and to have had the opportunity to speak my mind about its subject.\nPhilip W. Anderson is emeritus professor of physics, Princeton University, New Jersey, US. He was awarded the National Medal of Science by the US Government and is a Nobel laureate in physics.\nNot Even Wrong: The Failure of String Theory and the Continuing Challenge to Unify the Laws of Physics\nAuthor - Peter Woit\nPublisher - Cape\nPages - 290\nPrice - £18.99\nISBN - 0 224 07605 1"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:1812f7dc-dedc-45c4-b8d1-f4fdc04938e5>","<urn:uuid:15f85006-3db7-4388-9e2f-fa40746a3f73>"],"error":null}
{"question":"What is the relationship between anatomical factors and treatment approaches in pediatric sleep-disordered breathing?","answer":"The main anatomical causes of sleep-disordered breathing in children are enlarged tonsils and adenoids, small jaw, small airway, and in overweight children, fat deposits that narrow the airway. Correspondingly, the primary treatment approach is tonsillectomy and adenoidectomy (removal of tonsils and adenoids), which is considered first-line therapy and helps many children improve both their sleep and behavior. In cases where this isn't sufficient, alternative treatments include weight loss, using a CPAP machine that blows air into the nose via a mask to keep the airway open, or additional surgical procedures.","context":["Most of us consider snoring just a normal part of life. Maybe we have a partner who snores, or a grandpa who falls asleep in his easy chair and snores so loud it's hard to carry on a conversation in the room. We think of it as a common condition, and while it might be irritating trying to sleep beside a snorer, it usually isn't anything to worry about.\nBut is snoring normal for young kids?\nAccording to the National Sleep Foundation, most children snore once in a while, but only 10 percent snore on a regular basis, compared to 30 to 40 percent of adults. Snoring is caused by a lack of air moving freely through your nose and mouth during sleep. In adults, it often happens when the airway narrows because of an awkward sleeping position or because of abnormalities in the soft tissue of the throat. It's less common in kids, so if a child is snoring, there's sometimes another reason for it.\nSnoring in kids can be caused by:\nA cold or some other respiratory infection\nA small jaw or a small airway\nEnlarged tonsils and adenoids\nSome of these are minor conditions that will pass soon enough, but consistent snoring could point to a more serious issue like sleep apnea.\nWhat is sleep apnea?\nAbout 3 percent of all children between the ages of 1 through 9 have sleep apnea or upper airway resistance syndrome, which can be a more serious issue. So much so that in 2002, the American Academy of Pediatrics recommended that all children be screened for snoring to see if it's associated with sleep apnea.\nApproximately 1 to 3 percent of children suffer from breathing problems while they sleep. When snoring is accompanied by gasps or pauses in breathing, the child may have sleep apnea. This occurs when kids' muscles, which are usually relaxed during sleep, become so relaxed that the airway becomes obstructed and the child can't breathe properly, creating a pause that can last anywhere from a few seconds to a full minute. The brain then alerts the body that it's not breathing properly and the child will usually gasp or snort and start to breathe again.\nThis can be exhausting for a child's body, and because of all the waking in the night it can seriously affect quality of sleep and make for cranky kids who have trouble focusing. The American College of Chest Physicians claims that children who snore loudly are twice as likely to have learning problems. It only makes sense; tired kids are way more likely to have trouble concentrating and tend to be overtired and hyperactive.\nThere are treatments for sleep apnea that can solve the problem. Often, simply removing the child's tonsils or adenoids may take care of the issue. Some kids might need to use a machine that will blow air into their nose via a nose mask to keep the airway open and unobstructed.\nHaving a child who snores regularly isn't necessarily cause for alarm, but make sure you take your child in to your family physician to rule out any underlying conditions that might be affecting sleep quality. This will help ensure a lifetime of sweet (and quiet) dreams.","Overview of Sleep Disordered Breathing\nSleep-disordered breathing (SDB) is a general term for breathing difficulties that occur during sleep. SDB can range from frequent, mild snoring to obstructive sleep apnea (OSA), a condition involving repeated episodes of partial or complete blockage of the airway during sleep. Blockage of the airways during sleep disrupts the natural pattern of sleep. Such interruptions can result in a decrease in heart rate, rise in blood pressure, and arousal into a lighter stage of sleep. Oxygen levels in the blood can also drop. Approximately 10% of children snore regularly, and about 2-4 % of the pediatric population has OSA. Recent research indicates that even mild SDB or snoring may cause many of the same problems as OSA in children.\nSymptoms of Obstructive Sleep Apnea\nSnoring that is loud and present on most nights is the most obvious symptom of sleep-disordered breathing in children. The snoring may be loud and irregular. Gasping and snorting noises may also be present. Because of the lack of good-quality, restful sleep, a child with SDB may be irritable or have difficulty concentrating in school. Some children seem sleepy during the day while others may seem overly busy or exhibit hyperactivity. Bed-wetting is also frequently seen in children with sleep apnea.\nEnlarged tonsils and adenoids are a common cause of airway narrowing that causes or contributes to SDB. Overweight children are at increased risk for SDB and OSA because of airway narrowing from fat depositis. Children with abnormalities involving the lower jaw or tongue or neuromuscular deficits have a higher risk of developing sleep disordered breathing.\nPotential Consequences of Untreated Pediatric Sleep Disordered Breathing\n- Social: Loud snoring can become a significant social problem if a child shares a room with siblings or at sleepovers and summer camp.\n- Behavior and learning: Children with SDB may become moody, inattentive, and disruptive both at home and at school. Sleep-disordered breathing can also be a contributing factor to attention deficit disorders in some children.\n- Enuresis (bed wetting): SDB can cause increased nighttime urine production, which may lead to bedwetting.\n- Growth: Children with SDB may not produce enough growth hormone, resulting in abnormally slow growth and development.\n- Obesity: SBD may cause the body to have increased resistance to insulin or daytime fatigue with resultant decrease in physical activity. These factors can contribute to obesity.\n- Cardiovascular: OSA can be associated with an increased risk of high blood pressure or other heart and lung problems.\nDiagnosis of Sleep Apnea\nSleep disordered breathing should be considered in a child who has frequent loud snoring, gasping or snorting while sleeping. Sleep may be unusually restless or the child may have unexplained bed wetting in conjunction with snoring. Behavioral symptoms can include changes in mood, misbehavior, and poor school performance. Not every child with academic or behavioral issues will have SDB, but if a child snores loudly on a regular basis and is experiencing mood, behavior, or school performance problems, sleep disordered breathing should be considered.\nThese symptoms should be discussed with the child’s pediatrician or an otolaryngologist (ear, nose and throat specialist). In most cases, the diagnosis of sleep-disordered breathing is made by history and physical examination.\nIn certain cases, the physician may recommend further testing. A sleep study, or polysomnography, is a test for the diagnosis of sleep apnea. This study is performed while the child is sleeping in a sleep laboratory or hospital. Several measurements are taken through the night including brain waves, oxygen levels, airflow, muscle movement and breathing.\nTreatment for Sleep Disordered Breathing\nBecause enlarged tonsils and adenoids are the most common cause for SDB in children, tonsillectomy and adenoidectomy (removal of the tonsils and adenoids, T&A) are typically first-line therapy. Many children with sleep apnea show both short and long- term improvement in their sleep and behavior after T&A. Most children do not require overnight admission in the hospital and go home a few hours after surgery. Acetaminophen and ibuprofen are typically used to control pain.\nOther treatment options include weight loss, the use of Continuous Positive Airway Pressure (CPAP) or additional surgical procedures may sometimes be required. These are used less often in children and may be used if sleep apnea persists after T&A."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:2daa7126-f44d-470e-8041-f3f8e30dbe45>","<urn:uuid:1c65d884-1552-484a-b284-e5c385e52f62>"],"error":null}
{"question":"I'm doing research on aircraft safety. How do the safety features of personal flying devices compare - specifically between the Gravity jet suit's thrust control system and the Cessna 172's landing gear design?","answer":"The Gravity jet suit and Cessna 172 use different approaches to safety in flight control. The jet suit allows control through arm angle adjustment, where the pilot can direct thrust partially downward by changing arm position - at 40° from vertical, the thrust is split between upward and inward components for stability. The Cessna 172's tricycle landing gear design provides inherent stability during landings - with the center of gravity in front of the main wheels, the aircraft naturally straightens itself out if landing crooked, making it more forgiving for pilots. The Cessna 172's design has proven very safe, with a fatal accident rate of just 0.56 per 100,000 hours, about half the industry average.","context":["This isn't actually a real Iron Man suit. But it does fly. It's a flying suit made by Gravity Industries, a young British startup that builds what they call 'jet suits.' The system uses six kerosene-powered jet thrusters to let a human fly around. Honestly, it just looks cool.\nThis tweet states that it takes 1,000 horsepower to fly—how about an estimation to check this number?\nThe Physics of Flight\nLet's start off with some fundamental physics. How does this jet suit fly? I'm going to say it's all about the momentum principle. This says that the net force on an object changes its momentum where momentum is the product of mass and velocity. Here is the equation form of this idea.\nThere is one other important idea about forces—they are an interaction between two objects such that for every force there is an equal and opposite force.\nOK, now for flying. Suppose I have a human that is hovering above the ground. There is of course the gravitational force pulling down on the human so that there must also be an upward force to make the total force zero (so the human stays hovering). This upward force comes from the thrust of the micro jets. But how does a jet produce thrust? The answer comes from the momentum principle.\nBasically, this jet engine takes stationary air from above the engine and pushes it down so that it is moving with some new speed. This change in speed means that there is a change in momentum of the air such that it requires a force. If you push down on the air, the air pushes up on the human—and that is the trust.\nIt's not too difficult to derive (and I did so here if you want to see it), but this thrust force depends on a number of factors:\n- The density of air (this will probably be some constant value around 1.2 kg/m3).\n- The speed of the air coming out of the jet engines—I will call this \"thrust speed.\"\n- The area of the jet thrust (that comes out of the engine).\nNotice that all three of these factors change either the mass or speed of the air—which changes the momentum of the air. As an equation, it would look like this:\nIf you want a flying human to hover, this thrust force would have to be equal to the human's weight. But I don't really care so much about the thrust force: What I want is the power. Power is a measure of the rate at which you do work—the work in this case is going into the increase in kinetic energy of the air. Putting this together (again, refer to the human-powered helicopter post for the details), I get the following expression for power.\nYou can use these two expressions together to calculate the hovering power. First use the thrust force to calculate the speed of the air to hover and then use this speed to calculate the power.\nNow I need some values to calculate the power. Here are my estimations.\n- Mass of human (plus all the gear) = 90 kg (total guess).\n- Number of jet engines = 6. Technically, I think the newest suit has five jet engines and one of them is larger.\n- Area of jet engine = 0.0079 m2 (based on a engine diameter of 10 cm).\nWith these values, I get a thrust air speed of 176 m/s (394 mph)—just in case you want to see, here are my calculations in python. I'm embedding them right in this page to help promote the idea that python makes an excellent calculator. You can even change the values and rerun it to get new values. It's awesome.\nUsing this thrust speed, I get a power of 77,889 Watts or 104 horsepower. Yes, this is quite a bit lower than the listed 1,000 hp in the video but I think this is OK. I have calculate the hovering power, not the flying power. But there is another reason that I will now describe.\nComponents of Thrust\nOne of the cool things about this flight suit is the method that is used to control vertical thrust. Of course there is a throttle for the jet engines so that you could increase or decrease the thrust, but you don't need to do that. Instead, the human pilot can increase the angle of arms so that the jet engine thrust is directed only partially down. Here, let me draw a force diagram.\nEach of these hand jets has a thrust force in which part of the force (the x-component) pushes inward and part (the y-component) pushes upward. If the arm angle is θ degrees (as measured from the vertical), then the vertical component of force would be the total force multiplied by the cosine of θ. Yes, you need to be careful here. I see physics students make this mistake quite often. Just because it's a y-component doesn't automatically mean that it depends on the sine of θ—you have to look to see how the angle is measured. Just be careful.\nOK, let's assume that the arm angle is at 40° from the vertical. That means the total thrust (ignoring the jet engines on the back) would have to be greater in total magnitude to get a component to balance out the gravitational weight. If I include this in the power calculation, I get a thrust speed of 202 m/s with a power of 116 thousand Watts (115 horsepower).\nThat's still lower than the listed power, but this is a calculation based on a bunch of estimates. I suspect my value for the diameter of the jet engine is too large—but you can change that in python calculations if you like (see above). Also, this is the theoretical power with no energy losses. I assume that an actual engine wouldn't be perfect. But even if I get the wrong answer, it's still fun to make these estimations.\nOh, how about one homework question? If you assume my estimations are close to being legitimate, how high could this jet suit fly? Hint: As you increase in altitude, the density of air decreases.\nMore Great WIRED Stories\n- Playing Monopoly: What Zuck can learn from Bill Gates\n- A frolicking polar bear and other gorgeous drone photos\n- Sorry, nerds: Terraforming might not work on Mars\n- No solar-powered EV? You can still drive on sunshine\n- How a bunch of lava lamps protect us from hackers\n- Get even more of our inside scoops with our weekly Backchannel newsletter","Cessna 172 (10 Things You Need to Know)\nAsk a pilot to name a common single-engine fixed-wing aircraft, especially one that is good to learn on, and the Cessna 172 will quickly rise to the top of the list. From the original first flight in 1955 to its latest model still in production today, the Cessna 172 has been a reliable, dependable, easy-to-fly aircraft that is appreciated by students and seasoned pilots alike.\nIn this post we will explore the Cessna 172 and share ten things you need to know plus answers to the most frequently asked questions about this storied aircraft.\nBefore we delve into the things you need to know about the Cessna 172, let’s start with the basic specifications. Although there are many variations of the 172, we will share the specs for the current production model – the 172S.\nCessna 172S Specifications\n- Maximum occupants: 4\n- Maximum speed: 126 knots\n- Cruising speed: 124 knots (75% power, at 8000 feet)\n- Fuel capacity: 56 gallons\n- Maximum Range: 640 nm\n- Engine: Lycoming IO-360-L2A, 180 hp\n- Propeller: McCauley 2 blade metal, fixed pitch\n- Service ceiling: 14,000 feet\n- Maximum rate of climb: 730 fpm\n- Takeoff distance: 960 feet (ground roll), 1630 feet (total over 50-foot obstacle)\n- Landing distance: 575 feet (ground roll), 1335 feet (total over 50-foot obstacle)\n- Stall speed: 48 KCAS\n- Maximum ramp weight: 2558 pounds\n- Maximum takeoff weight: 2550 pounds\n- Maximum landing weight: 2550 pounds\n- Basic empty weight: 1680 pounds\n- Maximum useful load: 878 pounds\n- Baggage capacity: 120 pounds\n- Length: 27 feet 2 inches\n- Height: 8 feet 11 inches\n- Wingspan: 36 feet 1 inch\n- Wing area: 174 square feet\nNow that you know what to expect from the current Cessna 172 model, let’s take a look at the 172’s history and discover 10 must-know pieces of information about this aircraft.\n1. The Cessna 172 is the most-produced aircraft ever\nSince production began in 1956, over 43,000 Cessna 172s have been built with more still on the way. This legendary aircraft is well-loved, well-respected and still in demand more than sixty years after it first came on the market.\n2. There have been over 20 variations of the Cessna 172 (plus some special versions)\nOver the course of its long life, the Cessna 172 has naturally undergone a series of modifications, upgrades and enhancements while remaining true to the heart of her original solid design.\nAmong the most notable are the 172B which introduced the Skyhawk name, the 172D with its Omni-Vision rear wraparound window, and the 172S which incorporated Garmin G1000.\n3. Not all Cessna 172s were built in America\nIt is known as an American aircraft, but you may be surprised to learn that 1,436 Cessna 172s were actually built in France. From 1965-1971, Reims Cessna constructed the Cessna 172F. This model replaced a lever-operated flap system with electric flaps. It was also the basis for the U.S. Air Force’s T-41A Mescalero – a primary trainer.\n4. The “Skyhawk” name originally referred to a deluxe option package\nAlthough you may be used to hearing the name “Cessna Skyhawk,” originally, the Cessna 172 was called just that – the Cessna 172. It had no fancy name. The name Skyhawk came about in 1960 with the release of the 172B. This model had the option for a standard package or a deluxe option package – named the Skyhawk – with extra equipment and full exterior paint. Eventually the name stuck and came to refer to the entire Cessna 172 line-up.\n5. The 172’s tricycle landing gear design helps you ace your landings\nOne of the key design features of the 172 lineup has always been the tricycle landing gear design. This design was tested out on the WWII B-24, B-25 and B-26 bombers. The combination of a nosewheel in the front and moving the main wheels aft was found to make a tricycle landing gear aircraft easier to land.\nIn this configuration, the center of gravity (COG) is in front of the main wheels. What this means to you is that when you land in a tricycle gear aircraft, if you are crooked, the airplane’s weight will help to straighten you out. By contrast, a conventional landing gear setup has a center of gravity that is aft of the main wheels. Land crooked in this type of aircraft, and your mistake will just get exacerbated by the influence of the aircraft’s weight.\n6. Cessna 172s are used as trainer aircraft for many flight schools\nIf you have a private pilot’s license, odds are that you trained on a 172 at some point. Flight schools love this aircraft for several reasons. The ease of flying a tricycle landing gear configuration is a main selling point as are the forgiving handling characteristics and increased visibility of the high wing design plus rear “Omni-Vision” window.\nIn fact, the 172 is so stable that its built-in aerodynamic stability can often return a plane to straight and level flight after a spin even without input from the pilot.\n7. The world record for the longest endurance flight is held by a Cessna 172\nImagine what it would be like for a small aircraft to fly non-stop for over 64 days. Bob Timm and John Cook did just that from December of 1958 to February of 1959. Their 64-day, 22-hour, 19-minute flight out of Las Vegas still holds the endurance record.\nTimm and Cook flew a modified Cessna 172 which was named The Hacienda after the Hacienda Hotel who sponsored the $100,000 flight as a combined publicity stunt and fundraiser for cancer research.\nTo prepare for the flight, the team added a 95-gallon belly fuel tank to augment the 47 gallons carried in the wing tanks. An electric pump in the belly tank allowed fuel to be pumped into the wing tanks for refueling. Oil lines were replumbed to allow inflight oil and oil filter changes. In the cabin, the pilot seat stayed, but everything else was removed and a 4’x4’x4” foam pad was added as a bed for the off-duty pilot. An added platform off the co-pilot side allowed for better footing during refueling. A small sink in the rear was used for washing and shaving.\nAn FAA waiver allowed the overweight aircraft to operate at 350-400 pounds over the normal limit. Twice a day the Hacienda would rendezvous with a Ford truck over a straight stretch of closed off highway. Flying 20 feet above the ground, the Hacienda used an electric winch to hook a refueling hose from the truck. Three minutes later with its belly tank full, the Hacienda would release the hose and continue her flight.\nThe next time you pass through the Las Vegas McCarren Airport’s baggage claim area, look up – the Hacienda is hanging from the ceiling at the airport where her historic flight began and ended.\n8. There was (briefly) a diesel-powered Skyhawk on the market\nGiven the lower cost of Jet A and its more readily accessible nature overseas, Cessna decided to release a diesel version of the Skyhawk in 2017. The Turbo Skyhawk JT-A came powered by a Continental CD-155 diesel engine and received high marks in fuel efficiency and power.\nUnfortunately, the diesel engine also resulted in a higher purchase price ($420,000 compared to $307,000 for the 172S) and reduced payload capacity because of the added weight. Just 11 months after receiving FAA certification, the JT-A was discontinued due to poor sales.\n9. The triangle gear Skyhawk almost didn’t happen\nRumor has it that when a group of Cessna engineers approached the Cessna marketing department with their idea for a triangle gear aircraft, they were shot down. Marketing wanted to focus on tailwheel aircraft. Thankfully for Cessna and for all of us, the engineers were undaunted, and they continued work to build out the first concept version of the tricycle-geared Cessna 172 that we all know and love. Ultimately it met with approval and went into production.\n10. The Cessna 172 can be powered electrically (at least as a proof-of-concept)\nElectric Skyhawks may be in our future. In 2012, Cessna and Beyond Aviation worked together to develop an electrically powered variation of the Cessna 172. The aircraft took multiple successful test flights powered by the Panacis batteries that take the place of the back seats. One of the big advantages of an electric aircraft would be imperviousness to density altitude changes.\nBonus: Answers to Frequently Asked Questions\nFinally, we have rounded up the answers to some of the most frequently asked Cessna 172 questions.\n1. How safe is the Cessna 172 compared to other aircraft?\nThe 172 has an impressive safety record, especially when compared to the industry average. Statistically, the Cessna 172’s fatal accident rate is .56 per 100,000 hours. This is about half of the industry average rate of 1.2-1.4.\n2. How much does it cost to own and operate a Cessna 172?\nCost is an important consideration for most pilots when they are contemplating the purchase of an aircraft. Beyond the purchase price, you must also account for operational costs, storage, insurance, maintenance and more. AOPA crunched the numbers for you and put together a hypothetical operating cost calculation for a Cessna Skyhawk\nThis particular calculation was based on a 1975 Cessna 172M with a purchase price of $39,000. For comparison, the purchase price of a new Cessna 172S is around $307,000.\nLoan payments in the AOPA scenario were calculated at $290 for a $35,000 loan over 20 years at 7.9%. AOPA insurance payments came to $1200 a year, and hangar space rental was $250 a month, which equates to $3000 a year.\nYearly maintenance was estimated to be $2500 or $25/hr. for flying 100 hours or $4000 ($13.34/hr.) over 300 yearly hours.\nThe final factor to consider is any kind of projects you want to do, especially to an older aircraft.\nThe final AOPA estimate comes out to $108.10 per hour if flying 300 hours a year or $225.30/hr. for 100 flight hours.\nThe total annual cost is around $22,530 for 100 yearly hours of flight and $32,430 for 300 hours.\nThat said, keep in mind that you can save money by opting for tie-downs rather than hangars, and hangar costs vary by location. The amount you owe on your aircraft loan (should you need to take one out) is also an important variable component of this calculation.\n3. What kind of engine does the 172 have?\nThe original 1955 Cessna 172 was powered by a Continental O-300 145 hp engine. In 1968, the 172I debuted the Lycoming O-320-E2D 150 hp engine. Previous engines had used 80/87 fuel, but the 1977 Skyhawk N (172N) tried out a Lycoming O-320-H2AD 160 hp engine running on 100-octane. The performance wasn’t favorable, so starting in 1981, the 172P switched to the Lycoming O-320-D2J.\nFactory-fitted fuel-injected engines arrived with the 1996 172R’s Lycoming 160 hp Lycoming IO-360-L2A. The current 172S model was introduced in 1998 with an upgraded 180 hp Lycoming IO-360-L2A.\n4. When will Cessna release a new model of the 172?\nCessna has been exploring both diesel and electric options, but for now, the 172S is the only Skyhawk on the market.\nThe Cessna 172 is a classic plane with a long and storied history. Dig into its past and you will find a series of well-loved, reliable, safe aircraft that are easy and enjoyable to fly. Memorize some of our top 10 facts and soon you will soon be more well-versed on the 172 than your fellow pilot friends.\nPilotMall.com offers a full selection of Cessna Aircraft Manuals.\n- PilotMall.com Editor"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:4af66278-dae2-4e30-9f60-ff2847d59833>","<urn:uuid:f1d36dc0-b956-47fa-a147-896672743de1>"],"error":null}
{"question":"Is the reproductive process similar between sea hares and red sea urchins, especially in terms of how they release their eggs into the environment?","answer":"Both sea hares and red sea urchins use external fertilization, but there are some differences in their reproductive processes. Sea hares are hermaphrodites, with individuals both receiving and donating sperm during copulation, often forming chains of multiple animals. Red sea urchins, on the other hand, are either male or female, and are broadcast spawners that aggregate during spawning season. Female red sea urchins can produce 100,000 to 2,000,000 eggs into the sea where they are fertilized. Both species tend to reproduce in aggregations, with sea hares forming seasonal reproductive groups and red sea urchins reproducing best when in dense groups.","context":["In December, you heard from BHL User Dr. Thomas Carefoot, a marine biologist specializing in Sea Hares. We asked Dr. Carefoot to write another post for us about sea hares and his work on this topic. We send a special thanks to Dr. Carefoot for his participation in our blog and enthusiasm for educating the public about these amazing, and often overlooked, sea creatures!\nThe Formidable and Terrible, yet a-trembling, Sea Hare Aplysia\nBy Thomas Carefoot and Kevin Lee\n|Figure 1: sea-hare drawing. Anonymous.|\nSea Hares: Introduction\n|Figure 2: Juvenile Aplysia californica. Kevin Lee, Fullerton, CA.|\n|Figure 3: Black Sea Hare Aplysia vaccaria. Roeland Papen, CA.|\nSea hares of the genus Aplysia are a type of opisthobranch mollusc represented world-wide by some 35 species. They are herbivores, eating various types of seaweeds and some seagrasses (See figure 2). They have voracious appetites, grow quickly, and have life spans ranging from 1-1.5 years depending on the species and the time of year of hatching and settlement of the larvae. Most species range from thumb to hand size, but some grow much bigger. The largest species Aplysia vaccaria lives in southern California and ranks as one of the largest gastropods in the world. It is unique in reaching bucket-sized proportions, with lengths commonly attaining 0.75m (30in) and live masses exceeding 15kg (33lb). The one featured in the photograph is about 60cm in length (See figure 3).\nSea hares mostly inhabit tropical/subtropical regions, with a handful being circum-tropic. In North America there are about 6 species, with two, Aplysia californica and A. vaccaria, being indigenous to southern California (See figure 4). Sea hares spend most of their time feeding, defecating, copulating, and resting…not a bad life!\n|Figure 4: Diver holding Aplysia californica. Steve Pennings, University of Houston, TX.|\nSea Hares in Ancient Literature\nSea hares have been known in writings for 2000 years, from Pliny’s imaginative observations in the first century AD that credited this novel “fish” with a variety of noxious and other abhorrent characteristics, to the fanciful description by the Swedish ecclesiastic Olaus Magnus in the 16th Century. This author comments on the venomous features of Lepus marinus formidiabilis (“formidable marine hare”) and concludes:\n“This hare doth cause terror in the sea; on land he is as the poor little hare, fearful and a-trembling.”\n|Figure 5: Close view of head of Aplysia dactylomela. Anne Dupont, Florida|\nThe name “sea hare” is derived from Pliny’s original designation Lepus marinus, so named because of its hunched posture when resting and its prominent sensory appendages, or rhinophores, on its head (See figure 5). The name Aplysia appears in the 1767 edition of Linnaeus’ Systema Naturae. The meaning of the word aplysia is not perfectly clear, but in Greek it translates as “dirty sponge, unwashed.”\nSea Hares and Neurobiological Research\nIn view of their large sizes, shallow-water habitats, interesting attributes, and long history with humans, it’s surprising that their general biology and ecology are not better studied. In fact, most current research is neurobiological, with focus on the neuronal bases of learning and behaviour. Aplysia’s value in these respects is in its neatly ordered, comparatively simple nervous system that contains cells large enough to be easily penetrated with microelectrodes. A few relatively simple behaviours such as siphon- and gill-withdrawal reflexes, escape locomotion, and feeding, coupled with an accessible nervous system, has led to several fundamentally important discoveries on the neural integration of learning and behaviour. Such research has required so many specimens that an entire culture facility was created at the University of Miami (NIH National Resource for Aplysia) to supply sufficient stock. In fact, the original culture of A. californica from egg to adult by scientists at the New York School of Medicine in 1977 represented a break-through research discovery, as it was one of the earliest species possessing a long-lived plankton-feeding larva to be cultured.\nSea Hares Reproduction\n|Figure 6: Line of copulating Aplysia dactylomela. Anne Dupont, FL.|\n|Figure 7: Spawn of Aplysia punctata. Tom Carefoot, UBC.|\nSea hares are functional hermaphrodites, that is, with both sexes operating simultaneously within an individual. Copulation involves one individual approaching another, mounting from behind, gripping tightly with the front of its foot, and donating (but not receiving) sperm. During copulation the head of the sperm-donating individual is buried within the fleshy mantle folds of the sperm-receiving one. This orientation permits other individuals to be involved, and long chains may be formed (See figure 6), with all but the first and last individuals both receiving and donating sperm. Note in the photograph that the front individual is free to feed. The whole chain may amble slowly along the sea bottom for hours or even days, with individuals dropping in and out of formation. Should the sea hare in front come upon the one at the end, the “daisy chain” may become a “daisy circle”.\nSome species form large seasonal reproductive aggregations and their spawn, in the form of tangled masses of egg-bearing threads, becomes heaped up on rocks and seaweeds on the sea bottom. Pigments from seaweed foods are incorporated into the spawn, coloring each uniquely and creating the means for some chemical-detective work (See figure 7). After becoming familiar with what colour matches what algal food, egg strings as shown in the photograph can be identified as to the seaweed species eaten within the last 3-4 days before laying.\nSea Hares Defenses\n|Figure 8: Aplysia californica inking. Genevieve Anderson, Santa Barbara Community College, California|\nAlso incorporated from the seaweed foods are defensive chemicals of a huge variety, including mycosporine-like amino acids for UV protection, phycoerythrin pigments transformed into purple ink-secretions for warding off predators, and a myriad of other chemicals sequestered in the skin and digestive gland to make consumption of a sea-hare’s body a potentially sickening or even deadly experience (See figure 8). In addition to release of purple inks by most Aplysia species, some have white ink, and all have a white opaline-gland secretion. At least half a hundred chemical compounds have been isolated from different species of Aplysia, mostly types of diterpenes, all derived from red-algal foods. If a sea hare that normally eats red algae is fed for a time on green or brown algae, it becomes facultatively de-inked. Regular production of purple ink is restored if the diet is changed back to red algae. If you click here you can see 3 video clips made by researchers at Georgia State University, Atlanta, showing a spiny lobster attacking A. californica and getting a face-full of purple ink. The ink is certainly aversive to different types of potential predators although other functions, such as creating a camouflaging “smoke”-screen, have been proposed for it.\nSea Hares Swimming\nAbout half a dozen species can swim, the actual number depending on the extent of taxonomic separation of the species. For example, Aplysia fasciata in the Mediterranean and A. brasiliana in the Gulf of Mexico are both capable swimmers, but may be closely related. In North America there are only 2 swimming species, the less common A. moria, and the more common A. brasiliana. The latter is found throughout the Gulf States and along the south-eastern seaboard of the U.S.\nSwimming involves the rhythmical flapping of large, fleshy projections or parapodia on either side of the body. The actual mechanism of swimming has been debated in the scientific literature for over 80 years, but probably generally discussed for millennia, given the curiosity of humankind from fisher-folk to author/philosophers like Pliny, who would naturally have wondered what the heck that was that that just swam by. First, check out these 2 videos, one of A. pulmonica gently swimming on the side of an aquarium tank in Honolulu, and one of A. brasiliana swimming vigorously in a large, open tank in the Marine Science Institute of the University of Texas at Port Aransas.\nNow consider what mode of swimming is being used. Here’s a clue: humans and fishes swim by sculling, that is, displacement of water by arms, legs, pectoral fins, and tails, but other mechanisms are used by other animals. Possible answers will be discussed later. Most swimming by sea hares is done at or close to the sea surface and at least one species, A. brasiliana, regularly bobs its head out of the water.\nWhat is the function of swimming? One stimulus that will stop a swimming A. brasiliana in its “tracks” is a floating tuft of seaweed, and a sea hare deprived of food overnight will swim 12 times longer than one that has been fed. In comparison, contact of one swimming A. brasiliana with another does not stop or, for that matter, even slow down either individual’s swimming. Food-finding, then, may be a primary function, but seemingly not mate-finding. If the tail of a sea hare is pinched to mimic predatory attack, it swims significantly longer than one not pinched. So, even though sea hares have relatively few predators, escape from aversive stimuli may be another possible function. Another possibility, long-distance migration, has been proposed by some researchers but is less likely for reasons not worth discussing here.\nNow, back to our quiz on propulsive means: as you watched the videos did you think of jet propulsion from water being funneled along the curving parapodia and squirted out the back? This idea was proposed in the 1930s for a Mediterranean sea-hare species, and is the way that octopuses, squids, and scallops swim. Alternatively, what about hydrodynamic lift, as is thought to explain the smooth swimming of penguins and the lift generated by swimbladder-less sharks? This idea for sea hares originated from observations and analyses of parapodial movements of A. brasiliana by researchers at the Marine Biomedical Institute in Galveston, Texas. Their results suggest that the leading edge of each parapodium is airfoil-shaped and lift is generated during each cyclical stroke. Now check out the comparatively stubby parapodia of Aplysia gigantea, photographed swimming at Rottnest Island, Australia (See figure 9). This species swims with a more jerky motion and is not known for long-duration swims. Could sculling be involved here? Finally, is it possible that the modes used are species-specific? Have hybrid modes perhaps evolved? Or, does the mechanism involve something else entirely?\n|Figure 9: Aplysia gigantea swimming. Tom Carefoot, University of British Columbia|\nOne guesses that there is much left to find out about these formidable and terrible creatures, not just about their swimming, but also about other aspects of their biology and ecology.\nAbout the Contributors\n|Tom Carefoot. Margot Spence, Capilano College, British Columbia|\nI am retired from a 35-year teaching and research position in marine biology at the University of British Columbia, Canada. My research specialties have included, among other things, the study of sea hares, and I have sought them out in many areas of the world. I am well versed on invertebrate marine life, most notably on the Pacific west coast, but also throughout the Caribbean, Indo-Pacific, and other tropical areas. I have written 2 books on marine ecology, authored 90 research papers, and have recently produced a large educational website on west-coast marine invertebrates called A SNAIL’S ODYSSEY. Vancouver, British Columbia.\n|Kevin Lee. Kim Jin-Soo, California.|\nIn order to share the wonders of the underwater world, I took up photography a few years ago and have traveled to seven continents, sometimes diving in below-freezing waters, such as in Antarctica, in search of marine animals, especially opisthobranchs. My images are on permanent display at Chapman University, Orange California, and have been exhibited at various venues such as the Branford House, University of Connecticut and Scripps Institution of Oceanography, and have been published widely in periodicals and online. Visit my website at www.diverkevin.com. Fullerton, California.","Red Sea Urchin\nDid You Know?\nRed sea urchins have feet.\nRed sea urchins, Mesocentrotus franciscanus, belong to the Phylum Echinodermata and class Echinoidea. This phylum also includes sea cucumbers and sand dollars, one of their common traits is their radial symmetry. They vary in color between a uniform red and dark burgundy and crawl slowly over the sea bottom using their spines as stilts.\nThe red sea urchin is the largest of the sea urchins, with a maximum \"test\", or outer skeleton, diameter of more than 18 cm and a maximum spine length of 8 cm. The test is made up of 10 fused plates that encircle the sea urchin like the slices of an orange. Every other section has holes through which the sea urchin can extend its tubed feet. These feet are controlled by a water vascular system. By changing the amount of water inside, the animal can extend or contract the feet. The tip of the tube foot is shaped so it can act like a suction disc. Spines can also be used for locomotion.\nGrowth and Reproduction\nReproduction occurs between March and September in Southeast Alaska. Urchins are broadcast spawners with external fertilization and aggregate during spawning. Female urchins may produce 100,000 to 2,000,000 eggs into the sea where they are fertilized. After fertilization, they develop into a morula and eventually become 8-armed echinopluteus larvae which are herbivorous, feeding on phytoplankton. After the larvae stage, they develop into juveniles and eventually settle onto the substrate. After settling, a rapid metamorphosis occurs including development of spines and tube feet and then internal organs form similar to an adult sea urchin. They seem to reproduce best when in dense aggregations.\nSmall urchins (less than 5 cm test diameter) often hide under the adults. Adult urchins can release a chemical cue that causes the young to aggregate underneath them when the adults detect the presence of certain kinds of starfish. Some research suggests that urchins can live over 100 years, and found some near Vancouver Island that may be 200 years old.\nField studies of annual growth rates in Southeast Alaska indicate an annual growth increment between 0 and 20 mm. Growth rate is generally greatest among urchins between 20 and 40 millimeters, with large variation among locations and years. Slower growth occurs in areas exposed to open ocean conditions. By age ten urchins have almost stopped growing in diameter and growth slows considerably.\nThe mouth of the urchin is on the bottom (oral) part of the test. Food is chewed by 5 teeth which are part of a complex mechanism called Aristotle’s lantern. The chewed food then moves through the esophagus, stomach, and then intestine. When food enters the intestine and forms round pellets which are later excreted through the anus, which is at the top (aboral) side of the test.\nRed sea urchin larvae feed on microorganisms using cilia to sweep them into their mouths. As juveniles, sea urchins feed on diatoms and smaller food. Adults feed primarily on kelp (especially Nereocystis or Macrocystis) but can eat sessile invertebrates. Often forms large subtidal aggregations in or near kelp beds.\nDuring reproduction, red sea urchins aggregate together.\nRange and Habitat\nTheir range occurs from Northern Japan and Alaska to Baja California. In Southeast Alaska, the red sea urchin occurs primarily on rocky shorelines of the outside coast with largest concentrations in southern southeast. They can inhabit intertidal depth to up to 90 m.\nStatus, Trends, and Threats\nIn Southeast Alaska, the red sea urchin population is kept at very low levels by sea otters in many areas of the outside coasts, including, the Barrier Islands, Baker Island, Chichagof Island, Dall Island, Kuiu Island, Lulu Island, Maurelle Islands, Noyes island, Sumez Island, southern Prince of Wales Island, and nearby areas. Sea stars are also common predators of sea urchins.\n0 – 18 cm in diameter\nFrom Northern Japan and Alaska to Baja California, intertidal to 90 m depth\nSea otters and star fish\nMales and females, broadcast spawners, external fertilization\nManaged by the Alaska Department of Fish and Game\nDid You Know?\n- Red sea urchins have feet.\n- Red sea urchins are radially symmetric.\n- Red sea urchins live on sand and rocks.\n- Red sea urchins are either male or female.\n- Sea otters like to eat red sea urchins.\nCommercial and subsistence fishers harvest red sea urchins in Southeast Alaska. Urchins are harvested for their gonads, commonly called roe or uni, with no distinction made between males or females. The commercial fishery mostly markets the uni fresh and primarily in Japan.\nIn 1996, ADF&G and the sea urchin industry developed interim regulations and a management plan for the commercial urchin fishery in Southeast Alaska. This plan was implemented during the 1996/1997 season. The regulations were adopted by the commissioner under authority of 5 AAC 39.210 for High Impact Emerging Fisheries and became effective in December 1996. In 1997, the Alaska Board of Fisheries adopted the red sea urchin management plan. The core elements are:\n- Annual guideline harvest levels are 6% of the biomass estimate. Fisheries will only be opened where biomass surveys have been conducted in the previous six years.\n- Harvest opportunities are to be distributed to each week of every month that the fishery is open. The fishery is to be managed to span approximately four months, subject to needs for conservation, law enforcement, reducing waste, and promoting fishery development. Size limits and trip limits may be imposed if needed to slow the pace of the fishery.\n- In addition to fish ticket requirements, processors must submit records of the roe recovery within 30 days of landing.\nAdditional information about sea cucumber management can be found on our Commercial Sea Urchin Dive Fisheries webpage.\nSee Southeast Commercial Dive Fisheries for red sea urchin research."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:4d40bfbb-f288-432e-becf-4712877df628>","<urn:uuid:7147a1d9-b759-4c9c-b875-f882d8501701>"],"error":null}
{"question":"When did HACCP certification officially begin in China?","answer":"HACCP certification in China officially began on March 20, 2002, when the national CNCA issued the 3rd Bulletin 'Food Production Enterprises hazard analysis and Critical Control Point (HACCP) Management System Certification Management Regulations', which was implemented from May 1, 2002.","context":["The Food Safety Management system-HACCP\nHACCP is an abbreviation for English hazard Analysis Critical control point. HACCP is a widely recognized international food safety management system, in the food production, processing, and follow-up services, to help food enterprises in line with the relevant national laws and regulations, to ensure the safety of food.\nHACCP has become a mandatory norm under the law in many countries. HACCP management system has been gradually extended from the original food processing to the entire food supply chain (from raw material supply, food production/processing, packaging to storage and transportation) and food consumption in all aspects.\nThe standard requires food-related enterprises to identify possible biological/chemical/physical hazards in their business processes, to conduct hazard analysis of these factors, to identify critical control points, to develop control ranges in accordance with regulatory requirements, to propose necessary preventive and corrective measures, and to carry out programmatic controls to eliminate hazards or control hazards to a certain extent.\nChina's HACCP certification began on March 20, 2002, the national CNCA issued the 3rd Bulletin \"Food Production Enterprises hazard analysis and Critical Control Point (HACCP) Management System Certification Management Regulations\", and implemented from May 1, 2002.\nThe implementation of this Regulation further regulates the implementation of HACCP system certification supervision and management work in food production enterprises, HACCP system certification management to achieve a legally compliant. April 19, 2002, the State administration of quality Supervision and inspection issued the 20th order \"Export food production enterprises health registration and Registration Administration Regulations\", with effect from May 20, 2002.\nThe regulations require that the export food producers listed in the catalogue of products to be reviewed in the HACCP system for health registration need to establish the HACCP system in accordance with the health requirements of the export food production enterprises and the International Codex Alimentarius Commission's Hazard analysis and Critical Control Point (HACCP) system and its application guidelines. In accordance with the above regulations, the HACCP system must be established at present there are six types of production and export enterprises, respectively, the production of aquatic products, meat and meat products, frozen vegetables, fruit and vegetable juice, meat and aquatic products of frozen food, canned products enterprises, this is China's first mandatory requirements for food production enterprises to implement HACCP system.\nIt marks the application of HACCP into a new stage of development in China. HACCP certification in China has experienced 10 years (2002-2011) of development, the number of certified enterprises has exceeded 5,000, but, with the development of HACCP certification, HACCP certification basis and certification procedures are not uniform, to standardize HACCP certification activities, the negative effects of improving the effectiveness of certification are becoming increasingly apparent. There have been \"CNAB-SI52:2004 based on HACCP food safety management System norms (trial)\", \"SN/T1443.\" 1-2004 Food Safety Management system requirements, \"general evaluation guidelines\" food Safety Management System Requirements \"(HACCP-EC-01),\" CAC general principles of food hygiene and HACCP Application Guidelines \",\" GB/T 27341 Hazard Analysis and Critical Control point system food production enterprises common requirements \" And so on a number of standards.\nIn order to standardize HACCP certification, the CNCA has issued the \"Enterprise Hazard Analysis and Critical Control Point (HACCP) system certification implementation Rules\", which has been in effect since May 1, 2012. The new HACCP certification implementation rules, based on China's HACCP standards and certification system, consider the global Food Safety Action Plan, which includes the International Codex Alimentarius Commission (CAC) standards and GFSI standards of the International Food Business Forum (CIES), determine the basis and mode of certification:\nThe certification is based on: GB/T27341 \"Hazard Analysis and Critical Control point system food production enterprises general Requirements\" and GB14881 \"General hygiene practice for food manufacturing”."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:108dab09-dc09-4b26-8048-c7e39723e542>"],"error":null}
{"question":"How do the speeds and accuracy levels compare between Patriot missiles and Iran's medium-range ballistic missiles?","answer":"The Patriot missile system and Iran's medium-range ballistic missiles have different performance characteristics. Iran's Qadr H missiles can reach speeds of around Mach 14 and have an accuracy of between 50-100m circular error probable. While the exact speed of Patriot interceptors isn't specified, they are designed to counter these high-speed threats and have demonstrated a 95% hit ratio in modern deployments, a vast improvement from their initial 25% success rate in the Gulf War. The Patriot system is considered highly effective, with Navy pilots noting that aircraft cannot escape once targeted. The system is so capable that it can launch interceptors before securing a weapons-grade lock, using initial fly-out guidance followed by onboard systems for terminal guidance.","context":["Will the Patriot Air Defense System Be a Lifesaver for Ukraine?\nThe United States is officially sending Ukraine its long-serve Patriot air defense system to help defend the embattled nation’s civilian infrastructure.\nThe United States is officially sending Ukraine its long-serve Patriot air defense system to help defend the embattled nation’s civilian infrastructure against the ongoing Russian assault. This decision represents what could be a significant leap in Ukraine’s defensive capabilities, while also serving as a powerful political message about America’s deepening support for the wartorn nation.\nHowever, America’s Patriot air defense system, like the air defense enterprise itself, is a widely misunderstood topic online. So we set out to offer a better understanding of the system itself, its capabilities, and perhaps most importantly, its limitations.\nIt’s a near certainty that Russia will leverage misconceptions about both the system and its role to advanced narratives meant to undermine faith in American equipment and in Ukraine’s chances at emerging from this conflict victorious.\nSo, in order to innoculate yourself against the flood of disinfomation that’s sure to ensue, here’s a crash course in America’s MIM-104 Patriot Air Defense system.\nWhat is the Patriot air defense system?\nThe MIM-104 Patriot system is comprised of multiple assets but serves a singular purpose: identifying and intercepting inbound threats ranging from aircraft to both cruise and ballistic missiles.\nDeveloped by Raytheon, the MIM-104 Patriot Air Defense System first entered service in the early 1980s as a replacement for both Nike Hercules high-to-medium air defense and MIM023 Hawk medium tactical air defense systems.\nDespite being developed with a focus on defending against high-performance aircraft, there was a clear need for countering tactical missiles by the mid-80s. In this context, tactical missiles refer to short-range ballistic missiles, cruise missiles, and air-to-surface missiles deployed by air and rotorcraft.\nBallistic and cruise missiles offer different challenges for air defenses due to their inherent differences in their operation. Ballistic missiles like Russia’s air-launched Kh47M2 Kinzhal or even its nuclear-armed RS-28 Sarmat ICBM could be thought of as similar to rockets. They’re commonly launched using a conventional rocket booster along a high, arcing ballistic flight path before separating from the booster and careening back toward their target at extremely high rates of speed. Nearly all ballistic missiles achieve hypersonic velocities as they approach their targets, but unlike modern hypersonic missiles, they rarely maneuver during their descent, making their trajectories fairly predictable.\nCruise missiles, on the other hand, can be thought of as more akin to suicide drones. They’re often powered by air-breathing jet engines, not unlike tactical aircraft, which allow them to fly under power along a more horizontal and unpredictable trajectory. These weapons fly at much lower speeds than ballistic missiles but can be more dangerous due to their maneuverability and the ability to use to curvature of the earth to mask their approach.\nTo this end, the U.S. soon fielded two modifications to the Patriot system before it ever even saw combat. The first, Patriot Advanced Capability 1 (or PAC-1), was a software upgrade while the second, PAC-2, included changes to the hardware itself, including a new fuse and larger fragments within the warhead. By the time the first Patriot systems were deployed in the Middle East for the Gulf War in 1991, both of these modifications had been fielded, which made the Patriot system more adept at engaging missiles than it had been at its onset.\nHowever, the Patriot system failed to live up to expectations during its first combat deployment, and as a result, it is often dismissed by those who only recall those early controversies. However, mistaking today’s MIM-104 Patriot system for the same one America fielded over three decades ago would be a mistake.\n“In the first Gulf War Patriot was right around 25%. It was doing something it wasn’t necessarily designed for. It was actually built for planes but they decided to throw it at missiles and it sometimes hit. Since then, we have vastly improved the system — like hundreds of upgrades,” explained U.S. Army Patriot Fire Control Enhanced Operator-Maintainer, Sergeant First Class Long. “Now days, Patriot has right around a 95% hit ratio.”\nWe’ll dive much further into the controversy surrounding the Patriot’s early performance in an another piece.\nThe Patriot system is usually deployed in batteries that are made up of six primary components as well as some others depending on circumstance:\n- An electrical power plant\n- A radar set\n- An engagement control station\n- Launching stations\n- An antenna mast group\n- The Patriot interceptor missiles themselves\nToday, the Patriot is operated by 18 nations, with the United States operating the largest fleet of systems, with 16 Patriot battalions operating upwards of 50 Patriot batteries with more than 1,200 interceptors in the field.\nHowever, it’s important to understand that the Patriot system does not operate as an island unto itself under normal circumstances. In America’s missile defense apparatus, the Patriot serves as one portion of a layered defense strategy, something Long and the Army refer to as “defense in depth.”\n“Defense in depth is defined as having increasing levels of firepower as the threat gets closer to you,” Long explains, “which means the enemy runs into an increasing number of fires as they approach friendly forces.”\nWhat can Patriot missiles (PAC-2 and PAC-3 Interceptors) really do?\nToday’s American Patriot systems operate interceptors from two primary families: PAC-2 and PAC-3, though even the PAC-2 missiles are a far cry from their siblings employed in the 1990s.\nIncredibly, both PAC-2 and PAC-3 interceptors are actually launched by the Patriot system before it has even secured what’s known as a “weapons-grade lock,” or a targeting solution for the inbound aircraft or missile. Instead, the weapon is deployed in what’s called the “initial fly out” stage of its guidance approach where it is then fed active guidance information from the Patriot’s radar array until it gets close enough to the target to transition from the Patriot’s radar system to its own onboard guidance systems.\nThis results in an extremely short window of time between an aircraft, for instance, being notified of a radar lock and the weapon itself actually reaching its target.\n“The Patriot is by far the most lethal SAM system in the world, and there is no aeroplane in existence that is going to get away from it. The missile itself is also designed to bias its impact on the nose of the aircraft so as to kill the pilot. If a Patriot is fired at your aircraft, you might as well eject, as there is nothing you can do to get away from it,” explained Navy Lt. Cdr. Rod Candiloro, an F/A-18 Hornet pilot who flew during Operation Iraqi Freedom.\nHis concerns about the system were warranted. At the time, the system was proving very effective at intercepting enemy missiles with its processes automated, but that automation ultimately led to two friendly-fire incidents against a Royal Air Force Tornado and a U.S. Navy Hornet. All three crewmembers involved in those intercepts were killed.\nPAC-2 missiles are interceptors that benefit from the PAC-1 and 2 updates discussed above, however, the modern PAC-2 GEM-T, or Guidance Enhanced Missile – Tactical (as opposed to GEM-C with the “C” denoting cruise missile), is a modernized iteration with a number of further enhancements to improve its performance against tactical ballistic missiles.\nThese interceptors come equipped with a new proximity fuse for their explosive fragmentation warheads, which represents one of the significant operational differences between these weapons and the kinetic-based PAC-3 interceptors.\n“As the interceptor missile approaches the target, its active seeker will steer the missile to the target. A PAC-2 Patriot missile will detonate in the vicinity of the threat missile whereas a PAC-3 will seek to impact the warhead of the threat ballistic missile,” says NATO’s “Patriot Deployment Fact Sheet.\nThe PAC-2 GEM-T is also equipped with a new low-noise oscillator in the nose that allows for improved targeting of aircraft or missiles with a low radar cross-section. These interceptors entered service in 2002 and saw significant success in Iraq the following year.\n“In contrast with the experience of Desert Storm, Patriot interceptors defeated every ballistic missile they engaged during the 2003 Operation Iraqi Freedom. Since 2015, Patriot has successfully engaged scores of missiles and drones in the Yemen Missile War. Israel has likewise used it on a number of occasions to defeat drones, aircraft, and other threats,” write Mark Cancian and Tom Karako for the Center for Strategic & International Studies.\nPAC-3 MSE and CRI\nWhile the PAC-2 missiles used blast fragmentation warheads to take out incoming missiles or aircraft, the smaller and more modern PAC-3 missiles leverage “hit to kill” technology to destroy targets with sheer kinetic force. Another important difference is manufacturer — while PAC-2 missiles come from Raytheon, PAC-3 interceptors come from Lockheed Martin. As such, the PAC-3 missiles are completely new “clean sheet” designs meant to maximize the Patriot air defense system’s capability set.\nPAC-3 missiles use an active Ka-band radar seeker for terminal guidance into the target, with 180 solid-fueled attitude control motors (ACM) in its forward section to allow for heightened maneuverability.","Missiles with range of 300-2,000km launched from silos in multiple locations in the country, Revolutionary Guards says.\nThe recent launch of several medium-range ballistic missiles (MRBMs) by Iran has focused international attention on the Islamic Republic’s considerable ballistic missile arsenal – for years, an important factor in regional security concerns.\nSo how much of a threat are these weapons to Iran’s rivals in the Middle East?\nBallistic missiles have significant military advantages over more traditional aircraft or cruise missiles when it comes to delivering explosives over long distances – as well as significant limitations.\nFirst used by Nazi Germany against British cities during the final years of the Second World War, ballistic missiles are launched vertically to an extremely high speed. After their rocket motors have burnt out, they follow a ballistic trajectory until they fall onto their targets.\nThe most obvious military advantage ballistic missiles possess is the extremely high speed at which their warheads fall onto their targets. This makes them extremely hard to intercept and give little warning time for an opponent.\nThe longer the range of a missile, the higher it must arc upwards on its trajectory – and so the higher speed it picks up on its descent onto the target.\nThe Qadr H missiles launched by Iran on Tuesday have a range of around 2000km and a maximum speed of around Mach 14.\nThis makes intercepting them a very complex operation – even with specialist ballistic missile defence systems like the American Patriot PAC-3. It also limits the area defensive systems can cover.\nThe only systems able to reliably counter medium-range ballistic missiles are large and extremely costly. They include the Standard Missile 3 – used by the US Navy’s Aegis system – and the Terminal High Altitude Area Defence system, which has been procured by the United Arab Emirates (UAE) and Oman.\nAs a result, they are deployed in limited numbers, even by the US, and could be saturated by a reasonable number of potential simultaneous Iranian launches.\nIt is reasonable to assume that if Iran were to launch a substantial proportion of its medium-range ballistic missile stocks at targets in Saudi Arabia or Israel, for example, the existing defence systems in the Gulf would not be able to shoot down all the incoming threats.\nBut, what would be the impact of the missiles on their potential targets?\nIt is worth noting that accuracy is a fundamental limiting factor for ballistic missiles.\nThe most advanced Iranian MRBMs can reportedly achieve an accuracy of between 50m and 100m circular error probable – the radius within which half of the rounds fired would be expected to land.\nThis is sufficient to target cities and large military bases, such as airfields and oil installations, but not for hitting moving targets or individual buildings.\nThis is why ballistic missiles have traditionally been associated with nuclear, chemical or biological warheads, which do not require pinpoint accuracy to have devastating effects on targets.\nHowever, with the nuclear deal appearing to hold for now, Iran insists that its ballistic missiles are armed with conventional explosive warheads.\nThis greatly limits the military impact that they could have in a potential confrontation with the Islamic Republic’s neighbours.\nThe warheads which are fitted to the various Shahab and Qadr series ballistic missiles are believed to be limited to between 700kg and 1000kg.\nThat translates to a reasonably powerful high-explosive (non-nuclear) warhead, roughly comparable to the heavy 2000lb-class laser and GPS guided bombs which have become the backbone of Western-made fighter bombers’ ground attack capabilities.\nHowever, since each ballistic missile only carries a single 1000kg payload, whereas a fighter can carry multiple bombs and rockets, the impact of a conventional-warhead from a Qadr or Shahab missile would be less than the strike power of a single F-15E or F-16 fighter bomber.\nThe Qadr H can reportedly carry a multiple independently targetable re-entry vehicle (MIRV) warhead.\nThis enables multiple targets to be hit by the same missile but with smaller individual warheads. It also greatly complicates any attempt to shoot down the incoming threat by missile defence systems.\nHowever, the overall payload weight limit remains unchanged and so the destructive capacity of even a MIRVed Qadr H with conventional explosives is probably no greater than a Western strike fighter.\nIn the event of a conflict, Iran’s ballistic missile arsenal could almost certainly place warheads with nearly 1000kg of explosives on cities and military bases as far afield as Israel, despite the presence of ballistic missile defence systems in the region.\nThe military impact that they could have by disabling runways and facilities at key military bases in the UAE, Saudi Arabia, Qatar and even Israel would be inconvenient and costly – but would certainly not turn the tide of any military operation against Iran by the Gulf Cooperation Council, Israel or the US.\nWithout nuclear, chemical or biological warheads, the international community should view Iran’s ballistic missile capabilities for what they are – a potentially dangerous but not decisive or hugely effective conventional deterrent.\nJustin Bronk is a Research Fellow in Military Sciences at the Royal United Services Institute.\nThe views expressed in this article are the author’s own and do not necessarily reflect Al Jazeera’s editorial policy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:40ec2bb7-c53c-49ff-8bf8-62d1a1fbedac>","<urn:uuid:ce7c6fc1-9a72-407b-a108-73e4be16d4c0>"],"error":null}
{"question":"What's the difference between Gridster's layout functionality and User Security Check's user management capabilities?","answer":"Gridster is a jQuery plugin that enables creation of draggable layouts where elements can span multiple columns, focusing on interface design. The User Security Check, on the other hand, is a comprehensive security audit tool that manages five critical elements of user security: Two-Factor Authentication Status, Password Age & Strength, Last Time Active, Active WordPress Sessions, and User Role.","context":["Waypoints is a jQuery plugin that makes it easy to execute a function whenever you scroll to an element.\nParallax scrolling for the masses. Skrollr helps with all kinds of different scrolling effects including color shifts, transforms and more.\nA graph visualization library using web workers and jQuery\nA dynamic column grid.\nA dynamic layout plugin for jQuery The flip-side of CSS floats\nIsotope: An exquisite jQuery plugin for magical layouts\nA lightweight, easy-to-use jQuery plugin for fluid width video embeds.\nGridster is a jQuery plugin that allows building intuitive draggable layouts from elements spanning multiple columns.\nTextExt’s modular design allows you easily turn a standard HTML text input into a wide range of modern, tailored to your needs input field without bloating your source code\nEdit In Place Plugin For jQuery\nFitText makes font-sizes flexible. Use this plugin on your fluid or responsive layout to achieve scalable headlines that fill the width of a parent element.\nNice, downward compatible, touchable, jQuery dial\nNoty is a jQuery plugin that makes it easy to create alert, success, error, warning, information and confirmation messages as an alternative the standard alert dialog. Each notification is added to a queue.\nA jQuery plugin which slides a webpage over to reveal an additional interaction pane\nThis plugin provides you an accessible and lightweight solution to a widely adopted interface pattern known as progressive disclosure.\nAvgrund is a jQuery plugin for modal boxes and popups. It uses interesting concept showing depth between popup and page. It works in all modern browsers and gracefully degrade in those that do not support CSS transitions and transformations\nSmooth scrolling navigation and animation for single page sites. Super simple, incredibly handy.\njqPagination is a jQuery plugin that provides a newer method of pagination for your web site or application.\nJoyride is extremely flexible and lets you take control of how people interact with your tour. We programmed it to be cross-browser compatible with modern browsers and even used some fancy CSS to avoid images. Now let’s see just how easy it is to take your first ride without getting the fuzz involved.\nPowerTip features a very flexible design that is easy to customize, gives you a number of different ways to use the tooltips, has APIs for developers, and supports adding complex data to tooltips. It is being actively developed and maintained, and provides a very fluid user experience.\nTipsy is a jQuery plugin for creating a Facebook-like tooltips effect based on an anchor tag’s title attribute.\nSlideshows, Images & Sliders\nThe most complete responsive jQuery / WordPress slider.\na simple jQuery plugin that allows you to add a dynamically-resized, slideshow-capable background image to any page or element\nAnystretch is a jQuery plugin that allows you to add a dynamically-resized background image to any page or block level element. The image will stretch to fit the page/element, and will automatically resize as the window size changes.\nA simple lightbox based on prototype and scriptaculous.\nAn awesome, fully responsive jQuery slider toolkit.\nA lightweight customizable lightbox plugin for jQuery\nWOW Slider is a responsive jQuery image slider with amazing visual effects (Rotate, Blur, Flip, Blast, Fly, Blinds, Squares, Slices, Basic, Fade, Ken Burns, Stack, Stack vertical and Basic linear) and tons of professionally made templates.\nTimeago is a jQuery plugin that makes it easy to support automatically updating fuzzy timestamps\nThe jQuery Validation Plugin provides validation for your existing forms.\nFile Upload widget with multiple file selection, drag&drop support, progress bars and preview images for jQuery.\nA simple jQuery plugin to show a list of Instagram photos.\nMotionCAPTCHA is a jQuery CAPTCHA plugin, based on the HTML5 Canvas requiring users to sketch the shape they see in the canvas in order to submit a form.\nSuper-smooth CSS transitions & transformations for jQuery","In the Feature Spotlight posts, we will highlight a feature in the iThemes Security Pro plugin and share a bit about why we developed the feature, who the feature is for, and how to use the feature.\nToday we are going to cover the User Security Check, an easy way for you to audit the strength of your user’s security.\nWhy Does The Security of My Website’s Users Matter?\nSimply put: a single Admin user with a weak password could undermine all of the other website security measures you have put into place. That is why it is so important for you to audit the strength of security used by the Administrator users on your website.\nThe iThemes Security Pro User Security Check allows your quickly audit and modify 5 critical elements of user security:\n- Two-Factor Authentication Status\n- Password Age & Strength\n- Last Time Active\n- Active WordPress Sessions\n- User Role\nWhy Should I Use the User Security Check in iThemes Security Pro?\nThe iThemes Security Pro plugin has a ton of tools that you can use to increase the WordPress user security on your website. The Two-Factor Authentication and Password Requirements features alone protect your WordPress users from 100% of automated bot attacks.\nHowever, these two user security tools are only effective if the users on your website are actually using them. A single admin with a weak password could undermine all of the other security measures you have put into place.\nThat is why it is so important for you to audit the security of the Admins and Editors on your website.\nWhat Does the User Security Check Audit?\nThe User Security Check in the iThemes Security Pro plugin allows you to audit 5 different security elements for each user on your website:\n1. Two-Factor Authentication\nThe Two- Factor section of the User Security Check lets you view if a user has enabled two-factor authentication and whether it has been configured.\n- Gray Padlock – If a user has a gray padlock, it means they have enabled and configured two-factor authentication.\n- Orange Padlock – An orange padlock means that the user has enabled but not configured two-factor authentication. Even though this user has configured 2fa, they are likely being forced to use the email method of 2fa when logging in.\n- Red Padlock – A red padlock means the user hasn’t enabled or configured two-factor authentication.\nHovering over a red padlock in the User Security Check will display an option to send that user an email reminding them to configure two-factor authentication.\n2. Password Strength and Age\nThe Password section of the Users Security Check displays the strength and age of each user’s password.\nIf the password strength of a user is Unknown, that means the user hasn’t logged in since the User Security Check was enabled.\n3. Last Active Time\nThe Last Active section of the User Security Check displays the last time a user was active on the website.\nIf the Last Active time for a user is Unknown, that means they haven’t logged in since the User Security Check was enabled.\n4. Active Sessions\nThe Session section of the User Security Check displays the number of active sessions for each user.\nWordPress generates a session cookie every time you log into your website. Having multiple active sessions could simply be from a user not signing out from their laptop before signing into the website from their phone.\nHowever, a user with multiple active sessions could be a sign of a session hijacking attack. If a user has multiple unexpected sessions, you can click the Log Out Everywhere button to end all active sessions.\n5. User Role\nThe Role section of the User Security Check allows you to view and modify each user’s role.\nHow to Use the User Security Check in iThemes Security Pro\nEnable the User Security Check on the iThemes Security Pro settings’ main page to get started auditing your user’s security.\nOnce enabled, click the Configure Settings button to view the User Security Check.\nHow to Manage User Security From Your WordPress Security Dashboard\nThe iThemes Security WordPress Security Dashboard is a dynamic dashboard with all your WordPress website’s security activity stats in one place.\nThere are 2 security cards that let you manage the User Security Check from your security dashboard.\n1. User Security Profiles\nSee a list of every admin user on the site. Click on any username to get their user security check overview.\n2. User Security Profile\nPin a single user’s profile to your dashboard, and see their user role, password strength and age, whether or not they have two-factor enabled and when they were last on the site.\nWrapping Up: Make User Security a Priority\nHackers have better tools, and the bar for minimum security has been raised. WordPress security starts with user security, and in less than a minute, you can audit the security of every user on your website with the User Security Check."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:7586a48c-3b74-4110-98f1-455042b5fa0b>","<urn:uuid:500ad07b-1101-45a8-87ae-c007207ca7d9>"],"error":null}
{"question":"How many solar panels does Avon Valley Park have installed, and what percentage of their power needs does it cover?","answer":"Avon Valley Park has 265 panels installed on the roof which generates 90% of their power needs.","context":["Avon Valley Park Sustainability Policy.\nAt Avon Valley, we recognize the importance of sustainability and are committed to minimising our environmental impact, promoting social responsibility, and ensuring the long-term viability of our business. This Sustainability Policy outlines our guiding principles and commitment to sustainable practices in all aspects of our operations.\n1.1. Energy Efficiency: We will strive to reduce energy consumption and increase energy efficiency throughout our facilities. This includes investing in energy-saving technologies, promoting responsible energy use among employees, and regularly monitoring and assessing our energy performance. We have 265 panels placed on the roof which generates 90% of the power at Avon Valley Park.\n1.2. Waste Management: We are dedicated to minimising waste generation, promoting recycling and reuse, and responsible disposal of any waste we generate. We will implement waste reduction strategies, such as recycling programs, and encourage our employees and visitors to adopt sustainable waste management practices. We currently operate a zero to landfill policy through our waste provider.\n1.3. Water Conservation: We will actively monitor and reduce water consumption across our operations through the implementation of water-efficient technologies and awareness programs. We will also strive to protect local water resources and promote responsible water use.\n1.4. Biodiversity and Ecosystem Protection: We recognize the importance of biodiversity and will work to protect and enhance the natural environment around our facilities. We will undertake initiatives to preserve local ecosystems, support wildlife conservation efforts, and promote sustainable land management practices. We have bee hives on site, plant wildflowers and herbs and have bird and bat boxes.\n1.5. Sound Monitoring: We prioritise the implementation of sound monitoring systems that adhere to industry standards and guidelines while minimising noise pollution and protecting the well-being of attendees, neighbouring communities, and local wildlife. By monitoring sound levels throughout the event, we aim to strike a balance between providing an exceptional experience for our guests and maintaining a harmonious relationship with the environment. Our park opening and events include regular sound audits, the use of advanced technology for precise measurements, and collaboration with experienced sound engineers to achieve optimal sound quality without exceeding permissible sound thresholds.\n1.6. Pumpkin Patch: At the end of our event season local food banks are invited to take any surplus pumpkins, and further surplus is either given to our farm animals to eat or composted. We provide ideas online along with our event marketing about what to do with pumpkins after Halloween including recipes and wildlife friendly options.\n2.1. Employee Well-being: We value the health and well-being of our employees and will provide a safe and inclusive work environment. We will promote work-life balance, encourage and provide employee development and training, and prioritise employee health and safety. Providing private healthcare for contracted staff and organising regular staff socials.\n2.2. Community Engagement: We are committed to actively engaging with the local community and supporting initiatives that contribute to its well-being. We will collaborate with local organisations, employ local people, participate in community events, and contribute to local causes to foster positive relationships and support the community’s sustainability goals.\n3.1. Supply Chain Management: We will prioritise sustainable procurement practices by selecting suppliers and partners who share our commitment to sustainability. We will evaluate suppliers based on their environmental performance, ethical practices, and adherence to sustainability standards. We operate a paperless system with all suppliers.\n3.2. Product Sustainability: We will strive to offer sustainable and environmentally friendly products and services to our customers. We will seek to reduce the environmental impact of our products, consider their life cycle, and prioritise the use of eco-friendly materials and production processes. We operate a ticketless system with all bookings and guest communication carried out via email. We have a map of the site available via an app.\n4.1. Performance Measurement: We will establish clear sustainability targets and regularly measure our progress towards achieving them. We will track key performance indicators related to energy consumption, waste generation, water usage, and other relevant metrics to identify areas for improvement.\n4.2. Stakeholder Engagement: We will actively engage with employees, customers, suppliers, national farm parks and associations and the local community, to gather feedback, share best practices, and collaboratively address sustainability challenges.\n4.3. Compliance and Regulation: We will comply with all applicable environmental laws, regulations, and other requirements relevant to our operations. Additionally, we will strive to exceed these standards whenever possible and align with recognized sustainability frameworks and certifications.\nThis Sustainability Policy will guide our decision-making processes and drive our commitment to sustainable practices. We will regularly review and update this policy to reflect evolving sustainability priorities and ensure its effective implementation across all levels of our organisation.\nThis document is available to download HERE.."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:1b505f08-e269-4cfc-ad22-d8a46edd0d15>"],"error":null}
{"question":"What metrics are used to measure wealth inequality in the Sugarscape simulation?","answer":"The simulation uses two key metrics to measure wealth inequality: the Lorenz curve, which shows what percentage of wealth is held by what percentage of the population, and the Gini coefficient, which ranges from 0 (complete equality) to 1 (maximum inequality, where one agent has all the wealth).","context":["NetLogo Models Library:\n## WHAT IS IT?\nThis third model in the NetLogo Sugarscape suite implements Epstein & Axtell's Sugarscape Wealth Distribution model, as described in chapter 2 of their book Growing Artificial Societies: Social Science from the Bottom Up. It provides a ground-up simulation of inequality in wealth. Only a minority of the population have above average wealth, while most agents have wealth near the same level as the initial endowment.\nThe inequity of the resulting distribution can be described graphically by the Lorenz curve and quantitatively by the Gini coefficient.\n## HOW IT WORKS\nEach patch contains some sugar, the maximum amount of which is predetermined. At each tick, each patch regains one unit of sugar, until it reaches the maximum amount.\nThe amount of sugar a patch currently contains is indicated by its color; the darker the yellow, the more sugar.\nAt setup, agents are placed at random within the world. Each agent can only see a certain distance horizontally and vertically. At each tick, each agent will move to the nearest unoccupied location within their vision range with the most sugar, and collect all the sugar there. If its current location has as much or more sugar than any unoccupied location it can see, it will stay put.\nAgents also use (and thus lose) a certain amount of sugar each tick, based on their metabolism rates. If an agent runs out of sugar, it dies.\nEach agent also has a maximum age, which is assigned randomly from the range 60 to 100 ticks. When the agent reaches an age beyond its maximum age, it dies.\nWhenever an agent dies (either from starvation or old age), a new randomly initialized agent is created somewhere in the world; hence, in this model the global population count stays constant.\n## HOW TO USE IT\nThe INITIAL-POPULATION slider sets how many agents are in the world.\nThe MINIMUM-SUGAR-ENDOWMENT and MAXIMUM-SUGAR-ENDOWMENT sliders set the initial amount of sugar (\"wealth\") each agent has when it hatches. The actual value is randomly chosen from the given range.\nPress SETUP to populate the world with agents and import the sugar map data. GO will run the simulation continuously, while GO ONCE will run one tick.\nThe VISUALIZATION chooser gives different visualization options and may be changed while the GO button is pressed. When NO-VISUALIZATION is selected all the agents will be red. When COLOR-AGENTS-BY-VISION is selected the agents with the longest vision will be darkest and, similarly, when COLOR-AGENTS-BY-METABOLISM is selected the agents with the lowest metabolism will be darkest.\nThe WEALTH-DISTRIBUTION histogram on the right shows the distribution of wealth.\nThe LORENZ CURVE plot shows what percent of the wealth is held by what percent of the population, and the the GINI-INDEX V. TIME plot shows a measure of the inequity of the distribution over time. A GINI-INDEX of 0 equates to everyone having the exact same amount of wealth (collected sugar), and a GINI-INDEX of 1 equates to the most skewed wealth distribution possible, where a single person has all the sugar, and no one else has any.\n## THINGS TO NOTICE\nAfter running the model for a while, the wealth distribution histogram shows that there are many more agents with low wealth than agents with high wealth.\nSome agents will have less than the minimum initial wealth (MINIMUM-SUGAR-ENDOWMENT), if the minimum initial wealth was greater than 0.\n## THINGS TO TRY\nHow does the initial population affect the wealth distribution? How long does it take for the skewed distribution to emerge?\nHow is the wealth distribution affected when you change the initial endowments of wealth?\n## NETLOGO FEATURES\nAll of the Sugarscape models create the world by using `file-read` to import data from an external file, `sugar-map.txt`. This file defines both the initial and the maximum sugar value for each patch in the world.\nSince agents cannot see diagonally we cannot use `in-radius` to find the patches in the agents' vision. Instead, we use `at-points`.\n## RELATED MODELS\nOther models in the NetLogo Sugarscape suite include:\n* Sugarscape 1 Immediate Growback\n* Sugarscape 2 Constant Growback\nFor more explanation of the Lorenz curve and the Gini index, see the Info tab of the Wealth Distribution model. (That model is also based on Epstein and Axtell's Sugarscape model, but more loosely.)\n## CREDITS AND REFERENCES\nEpstein, J. and Axtell, R. (1996). Growing Artificial Societies: Social Science from the Bottom Up. Washington, D.C.: Brookings Institution Press.\n## HOW TO CITE\nIf you mention this model or the NetLogo software in a publication, we ask that you include the citations below.\nFor the model itself:\n* Li, J. and Wilensky, U. (2009). NetLogo Sugarscape 3 Wealth Distribution model. http://ccl.northwestern.edu/netlogo/models/Sugarscape3WealthDistribution. Center for Connected Learning and Computer-Based Modeling, Northwestern University, Evanston, IL.\nPlease cite the NetLogo software as:\n* Wilensky, U. (1999). NetLogo. http://ccl.northwestern.edu/netlogo/. Center for Connected Learning and Computer-Based Modeling, Northwestern University, Evanston, IL.\n## COPYRIGHT AND LICENSE\nCopyright 2009 Uri Wilensky.\n![CC BY-NC-SA 3.0](http://ccl.northwestern.edu/images/creativecommons/byncsa.png)\nThis work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License. To view a copy of this license, visit https://creativecommons.org/licenses/by-nc-sa/3.0/ or send a letter to Creative Commons, 559 Nathan Abbott Way, Stanford, California 94305, USA.\nCommercial licenses are also available. To inquire about commercial licenses, please contact Uri Wilensky at email@example.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:4da533f2-68e0-4db1-a7ab-5eba6980fb6d>"],"error":null}
{"question":"What are the potential dangers of trying to create a perfect society?","answer":"Attempting to create a perfect society (utopia) can ironically lead to dystopia. When trying to create absolute equality and perfection, societies often resort to eliminating imperfection and nonconformity, which has historically led to dictatorships and failed states. Examples range from China to Venezuela, where utopian dreams of absolute equality resulted in situations where the majority became equally poor while ruling elites lived in luxury. These states typically maintained 'equality' through ruthless efficiency, creating totalitarian dystopias. The fundamental issue is that human nature contains inherent flaws like greed, personal ambition, and capacity for evil that cannot be eliminated.","context":["(BEING CONTINUED FROM 08/01/18)\nby Anthony Egan\nWe do not live in a perfect world. Nor do we live in an absolute nightmare.\nThe word utopia is derived from a novel of the same name, written in 1516 by St Thomas More. A description of a perfect society, many commentators believe that this enthusiastic account of absolute equality, justice and surprisingly liberal cultural life – which includes religious freedom, legal euthanasia, easy divorce and even women priests – can only be read as satire. Thomas More, while he was still England’s Lord Chancellor before the Reformation, actively persecuted Protestants, and had no conception of social equality (let alone gender equality!). Even the name u-topia – from the Greek meaning ‘no place’ – suggests he is mocking the very idea.\nAs its polar opposition, dystopia means the very worst possible world: a place of absolute tyranny. It is most brilliantly expressed in George Orwell’s 1947 novel, 1984. The vision here is one of totalitarian government, surveillance of everybody, secret police operating at will and truth subverted by propaganda. Some folks may think that dystopia is all too real in our present world of surveillance cameras, state capture and alternative facts.\nThis is mistaken. Though all these manifestations of lack of freedom exist, the fact that we know about them – and to varying degrees resist them – suggests that such absolute tyranny does not exist. Insofar as we continue to resist, dystopia – though a possibility – is deferred.\nOddly enough, some might even say that utopia is the first step to dystopia. In trying to create a perfect society we may well accidentally create a disaster. Utopian dreams of absolute equality motivated the architects of dictatorships and failed states from China to Venezuela. Creating perfection means eliminating imperfection and nonconformity. Yet we cannot eliminate our capacity for sin: potential or real greed, power and revenge lies beneath even our noblest sentiments.\nSt Augustine of Hippo long ago warned that the Human City was by its nature flawed. However much we try to promote the best, we shall not achieve it this side of eternity. People are not equal, whether in intelligence, looks, abilities or moral character. Even economic equality is an impossibility: no society on earth has achieved this. In those that have tried we have seen situations where the vast majority are equally poor, while a ruling elite – the ‘guardians’ of equality – have lived in luxury. Normally the states that have emerged have also maintained ‘equality’ with ruthless efficiency, creating in the process totalitarian ‘dystopias’.\nWe must be on our guard against utopian visions, even as we should take the values of utopia seriously. The latter are the ideal. It is the moral benchmark against which we make social, political and economic decisions. In the best possible world (which, following Augustine, I argue cannot exist) this is what we’d like. In the real world where greed, personal ambition and the capacity for evil exists, we need to find better alternatives and workable compromises that advance the good as well as we can, while avoiding the terrible unintended consequences.\nWhile some well-regarded scholars argue that utopianism is a Western phenomenon and that utopias do not appear outside the West until the influence of More’s Utopia was felt, others have argued that utopianism developed independently in non-Western cultures. Thomas More invented a literary genre, but there are texts in the West and outside it that predate More’s Utopia that describe a nonexistent society that is identifiably better than the existing society. Probably the best-known early non-Western utopia is “The Peach Blossom Spring,” a poem of T’ao Yüan Ming (also known as T’ao Ch’ien) (365–427), that describes a peaceful peasant society, but there are golden ages, earthly paradises, and other forms of utopianism found in Sumerian clay tablets and within Buddhism, Confucianism, Hinduism, Islam, and Daoism.\nOnce it is established that there are utopian traditions that are certainly non-Western, there are problems that confront a scholar approaching the subject at the beginning of the twenty-first century. One is the issue of what is non-Western. Scholars disagree profoundly over what constitutes non-Western and Western. Some would limit Western to Europe, North America, Australia, and New Zealand and thereby exclude the substantial Portuguese and Spanish literatures published in Central and South America, which contain many utopias. Others would include these literatures. A second problem is that there are no good bibliographies of any non-Western utopianism not written in English. A related problem is that there are debates in a number of countries, even in countries such as India, where English is an official language, over the status of works written in English, particularly those written by authors who choose to live outside the country.\nIn ancient China, Moist and Legalist thought had utopian elements, and the same can be said for neo-Confucianism and Daoism. In twentieth-century China, Mao Zedong (1893–1976) was clearly utopian in his desire to transform Chinese society along the lines of his vision for it, and it can be argued that Mao’s Communism was both Marxist and rooted in Confucianism.\nThere have been a number of twentieth-century political movements with utopian dimensions. In India, Mohandas K. Gandhi (1869–1948) was a utopian and used the Hindu notion of Ramaraja (the rule of the Rama), the golden age, as a means of communicating his ideas. The vision of the Islamic republic developed by Ayatollah Ruhollah Khomeini (1900?–1989) and by the Taliban for Afghanistan were also clearly utopian and fit Popper’s analysis of the dangers of utopianism.\nThere are oral utopian traditions among the aborigines in Australia, the first nations in Canada, the Maori in New Zealand, and the Native American Indians in the United States. The struggle against colonialism produced millennial movements with strong utopian elements, such as the Taiping Rebellion (1851–1864) in China and the Ghost Dance movement in the United States. There were dozens of such movements in South America and movements among the Maori in New Zealand, some of whose successors still exist in the early twenty-first century, such as the Maori’s Ratana Church.\nAlso, there is a strong communitarian tradition in both Buddhism and Hinduism, and there is a traditional communitarianism among various indigenous peoples that has redeveloped since around 1980 as chosen, better ways of living, particularly among the Maori in New Zealand.\nMost non-Western utopianism is post-More and clearly connected with the genre of literature he invented, and as a result are deeply influenced by the West. Since China had the strongest pre-More utopian tradition, it is not surprising that it has the strongest post-More tradition. The Chinese utopias that are best-known in the West are Li Ju-Chen’s (c. 1760–c. 1830) Flowers in the Mirror (1828), which favors the rights of women, and Kang Youwei’s (1858–1929) Da T’ung Shu (1935), which is concerned with world unity.\nWorks that most nearly fit the genre of utopian literature appear to be most common in former colonies, and aspects of Chinese utopianism fit this model. There are utopias in English in various African countries, including South Africa, where utopias are in Afrikaans, English, and indigenous languages. In addition, there are utopias (because of limited research, how many is not known) in various indigenous languages in other African countries and in India.\nAfrican utopias in English are the works most widely read in the West. They come from many different countries and have a strong dystopian flavor. But as with many contemporary Western utopias, they often hold out hope of positive change. Ali A. Mazrui (1933–), who was born in Kenya, wrote The Trial of Christopher Okigbo (1971), which is mostly dystopian but still holds out hope. Authors born in Nigeria include Buchi Emechta (1944–), whose The Rape of Shavi (1983) shows the destruction of traditional utopia by colonialism; Wole Soyinka (1934–), whose Seasons of Anomy (1973) is primarily dystopian but includes the possibility of a better life; and Ben Okri (1959–), whose Astonishing the Gods (1995) presents the search for utopia. Bessie Head (1937–1986) was born in South Africa and lived in Botswana; her When Rain Clouds Gather (1969) presents a village that is both described as a utopia and is the location of an attempt to create a utopia.\nThe best-known Indian utopia in English is probably Salman Rushdie’s (1947–) Grimus (1975), which includes a society that is described in the text as “utopian” because it functions on a basis of rough equality and with no money. Other Indian utopias do not appear to have gained much of an audience outside India.\nComparative studies on Western and non-Western utopianism are only just beginning. (An early-twenty-first-century example is Zhang Longxi’s “The Utopian Vision, East and West” in the journal Utopian Studies .)\nAl-Azmeh, Aziz. “Utopia and Islamic Political Thought.” History of Political Thought 11 (1990): 9–19.\nBloch, Ernst. The Principle of Hope. 3 vols. Translated by Neville Plaice, Stephen Plaice, and Paul Knight. Cambridge, Mass.: MIT Press, 1986.\nJameson, Fredric. The Seeds of Time. New York: Columbia University Press, 1994.\nKumar, Krishan. Utopia and Anti-Utopia in Modern Times. Oxford: Blackwell, 1987.\nLevitas, Ruth. The Concept of Utopia. Syracuse, N.Y.: Syracuse University Press, 1990.\nLewis, Arthur O. Utopian Literature in The Pennsylvania State University Libraries: A Selected Bibliography. University Park: Pennsylvania State University Libraries, 1984.\nMannheim, Karl. Ideology and Utopia: An Introduction to the Sociology of Knowledge. Translated by Louis Wirth and Edward Shils. New ed. London: Routledge, 1991.\nManuel, Frank E., and Fritzie P. Manuel. Utopian Thought in the Western World. Cambridge, Mass.: Harvard University Press, 1979.\nMoylan, Tom. Demand the Impossible: Science Fiction and the Utopian Imagination. London: Methuen, 1986.\n——. Scraps of the Untainted Sky: Science Fiction, Utopia, Dystopia. Boulder, Colo.: Westview, 2000.\nNegley, Glenn. Utopian Literature. Lawrence: Regents Press of Kansas, 1977.\nPolak, Fred[erik] L. The Image of the Future; Enlightening the Past, Orientating the Present, Forecasting the Future. 2 vols. Translated by Elise Boulding. New York: Oceana, 1961.\nPopper, Karl R. The Open Society and Its Enemies. 4th rev. ed. 2 vols. London: Routledge and Kegan Paul, 1962. Originally published in 1945.\nPordzik, Ralph. The Quest for Postcolonial Utopia: A Comparative Introduction to the Utopian Novel in the New English Literatures. New York: Peter Lang, 2001.\nPordzik, Ralph, and Hans Ulrich Seeber, eds. Utopie und Dystopie in den Neuen Englischen Literaturen. Heidelberg, Germany: Universtätsverlag C. Winter, 2002.\nRoemer, Kenneth M. The Obsolete Necessity: America in Utopian Writings, 1888–1900. Kent, Ohio: Kent State University Press, 1976.\nSargent, Lyman Tower. British and American Utopian Literature, 1516–1985: An Annotated, Chronological Bibliography. New York: Garland, 1988.\n——. “The Three Faces of Utopianism Revisited.” Utopian Studies 5, no. 1 (1994): 1–37.\nSchaer, Roland, Gregory Claeys, and Lyman Tower Sargent, eds. Utopia: The Search for the Ideal Society in the Western World. New York: New York Public Library and Oxford University Press, 2000.\nSuvin, Darko. Metamorphoses of Science Fiction: On the Poetics and History of a Literary Genre. New Haven, Conn.: Yale University Press, 1979.\nWilde, Oscar. The Soul of Man under Socialism. Boston: Luce, 1910.\nZhang Longxi. “The Utopian Vision, East and West.” Utopian Studies 13 (2002): 1–20.\nLyman Tower Sargent\n(TO BE CONTINUED)"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b49e0c06-6158-49cc-9c7b-fe50fc7e34da>"],"error":null}
{"question":"Based on historical developments in authentication methods, how do digital tokens compare to knowledge factors in terms of their vulnerability to social engineering attacks?","answer":"Digital tokens (possession factors) and knowledge factors have different vulnerabilities to social engineering attacks. Knowledge factors like passwords, while familiar and easy to implement, can be compromised if users choose weak passwords or if the information is easily discoverable through public records. Digital tokens provide stronger security since they require physical possession of a device (for hard tokens) or access to specific electronic devices (for soft tokens). However, according to the documents, sophisticated social engineering attacks can still bypass both methods by taking advantage of human error, getting users to unwittingly reveal their authentication credentials. This was demonstrated in cases where chatbot services could be exploited to send messages appearing to come from trusted sources, potentially bypassing email protections and making employees more likely to follow malicious links.","context":["New to Chatbots? Understand Your Security Risk\nWith the increasing use of chatbots as a frontline tool for businesses, organizations need to take a closer look at the security of such services and include them in their threat model.\nLike any new technology, chatbots can also pose a potential security gap if vendors and organizations aren’t aligned in their deployment. At Tenable, we worked closely with our vendor to map any potential issues and ensure a secure user experience across our website. Organizations can get ahead of potential threats by understanding the security risks around these new tools and following common best practices.\nHow do Chatbots work?\nToday’s chatbots serve a variety of purposes. They’re great at:\n- Generating sales leads\n- Answering common support questions\n- Redirecting site visitors to other resources or contacts\nThat said, these bots can't do everything, so at some point, a human has to get involved. Many interactions can be handled directly through the chat service provided by the bot, such as a support person walking a customer through some common troubleshooting steps. In more complex situations, however, these bots will often schedule meetings or send messages to be handled outside of the chat session. For example, if a prospective customer wants to chat with a salesperson about terms for a potential deal, they'll have a quick back and forth with the bot, which will result in a meeting with a salesperson being scheduled. How does this work? Well, this is where the security gap comes in.\nCompanies are finding more ways to integrate these bots into their existing business models. These bot services have functionality built-in that gives them permission to schedule meetings and send messages as an individual from a given pool of users. These meeting invites and emails will appear to come directly from these individuals and not from a third party service. In some cases, these chatbots may need to collect personally identifiable information (PII) or payment information. This creates additional risk and raises several security concerns around data collected.\nLet’s first consider the impacts of collecting PII. Organizations need to be cautious in understanding how the chatbot platform integrates with their business. Understanding if the chatbot requires privileged access to backend systems for authentication or account authorization is a major security concern. Additionally, one has to consider if the traffic is encrypted, how information is stored and equally important and how the chats are logged. If an attacker were able to identify a vulnerability in a chatbot application, this could open an opportunity to access privileged and possibly sensitive data. If the data collected is only stored with the chatbot service provider, there’s the added risk of not being able to control how that data is secured or stored, both in transit and at rest. In 2018, Sears and Delta suffered a breach of payment data when a third party chatbot service they utilized was compromised.\nAdditional concerns arise in cases where the chatbot is used as a scheduling mechanism. In a best-case scenario, this causes nothing more than minor annoyance and embarrassment. In others, the consequences could be much more severe. An attacker could launch advanced social engineering attacks by essentially sending messages as a trusted insider for the company using the chatbot service.\nAs an example of an attack Tenable has previously observed in the wild, let’s say your company uses a chatbot service on their site to generate sales leads. If an attacker happens to know or guess the email addressing scheme or internal mailing lists the company uses (such as [email protected]), they could send messages on behalf of a salesperson to anyone within the organization. In the example below, the meeting notes section of a calendar invite could include any message an attacker wanted, even one containing malicious links. This email appears as a trusted source and is not marked as coming from the outside world -- effectively bypassing all existing email protections, such as DomainKeys Identified Mail (DKIM). As the author appears to be a trusted source, employees would be more likely to follow the malicious link in the invite.\nWhile this example is not a major breach of security, it does demonstrate how one of these chatbot services can be abused. What happens if the attacker starts filing IT requests on behalf of a salesperson? A sophisticated attacker with malicious intent could wreak havoc by abusing the functionality provided by the bot, perhaps by requesting ports to be opened on the company firewall or applications/services to be installed.\nSolutions and best practices\nAs chatbot use and scope of services continues to expand, the following solutions and best practices can help increase security and reduce an organization's risk:\n- Authentication: In scenarios where your chatbot needs to authenticate a user in order to provide specific solutions or fulfill requests, it is imperative to consider how the user will authenticate and how the chatbot system will handle these requests. Using two-factor authentication (2FA) adds an additional layer of security and if using a third-party chatbot service, single sign-on (SSO) solutions may be available and should be utilized. Additional consideration around authentication should include forcing timeouts after a set time period.\n- Encryption: While end-to-end encryption might seem like overkill for basic support questions, encrypting all traffic could help protect vulnerable users. Consider a use-case where a customer using public WiFi enters their account information and password in their query to the chatbot without being solicited for this information.\n- Logging: Careful consideration must be made in what data is collected and how this data is stored. There may be international laws to consider based on a user's geographic location, but also consider the above example, where a user inputs their username and password into the chat. One solution might be to delete all chat data once the session is complete or closed due to a time out condition. Other solutions might include matching specific keywords to scrub out any sensitive data from logs.\nThe easy solution in this scenario would be to simply blocklist or deny your own company from being able to receive these messages, which is not a default behavior or configuration for many of these services. This mitigation doesn't, however, prevent an attacker from sending these unsolicited messages to third-parties. Other solutions could include forcing these messages to come from a separate, designated domain and ensuring that no internal processes rely strictly on email. These solutions would allow more flexible monitoring and filtering.\nThe risks involved with chatbot attacks are likely to be more of an annoyance than anything else. Depending on a company’s configuration, sophisticated social engineering attacks could definitely occur, but to our knowledge at the time of this writing, social engineering hasn't been the sole cause of any major breaches. Despite the current risk profile for chatbots, it's still something security organizations within companies should be paying closer attention to and monitoring. It certainly isn't the type of attack we see everyday, and chatbots are a pretty innocuous piece of software that most people don't realize wields this sort of power. However, we believe it’s important to take a closer look at the chatbots your organizations utilize and make sure it's included in your threat models.\nGet more information\nSee vulnerabilities reported to vendors from Tenable's Research Teams on the Tenable Research Advisories page.\nLearn more about Tenable, the first Cyber Exposure platform for holistic management of your modern attack surface.\nGet a free 30-day trial of Tenable.io Vulnerability Management.\n- Vulnerability Management\nAre You Vulnerable to the Latest Exploits?\nEnter your email to receive the latest cyber exposure alerts in your inbox.","Multifactor Authentication: Access Control Made Easy?\nDIGITAL INFORMATION’S MODERN CONTEXT: A DATA SECURITY CRISIS\nMajor breaches to corporate information systems, such as Equifax’s 2017 data breach,1 have shown the vulnerabilities of many data security practices. These highly public failures have created some of the most significant headlines of the early twenty-first century,2 forcing data security experts to rethink their methods for securing personal information. One method receiving a renewed focus is multifactor authentication—a process by which institutions use multiple steps to verify a user’s identity. Some forms of multifactor authentication (such as requiring a personal identification number (PIN) when using a bank card) have been reliable for some time, and consumers now consider the practices to be second nature. However, the security of these methods has been substantially eroded in recent years, demonstrating the need for businesses and governments to respond to increasingly sophisticated threats that face a highly digitized society.\nData security threats are primarily the responsibility of governmental agencies and financial institutions, which collect and store significant amounts of confidential information. The government has recognized this responsibility and enacted guidance through the Federal Financial Institutions Examination Council (FFIEC), the National Institute of Standards and Technology (NIST), and the Department of Homeland Security (DHS). This guidance has raised the standards to ensure best practices are adopted to protect confidential data.3\nWHAT IS MULTIFACTOR AUTHENTICATION?\nMultifactor authentication is a process by which online accounts and services confirm the identity of a user through a series of verification steps (known as “factors”), each of which provides additional evidence that the user is who she claims to be. Each additional factor required for authentication significantly increases protection. There are three primary types of authentication factors: (1) knowledge factors (something the user knows), (2) possession factors (something the user has), and (3) inherence factors (something the user is).4 While some information systems use multiple steps of the same type of factor, the FFIEC, among others, has criticized such use as inadequate, recommending the use of different authentication factor types for more effective protection.5 Although multifactor authentication can also refer to non-digital factors (such as fingerprints or retinal properties), this paper uses the term in the digital sense unless otherwise specified.\nKnowledge factors are perhaps the most familiar form of authentication;6 a user must prove that they know a certain piece of information, such as a password, passphrase, or PIN to access an account. Although useful and easy to implement, knowledge factors can vary greatly in the level of protection they provide, depending on the strength of the password, or phrase. Greater length or variability in the types of characters used (i.e., uppercase/lowercase, special characters, numbers) and the exclusion of common words or names strengthen passwords and makes them more difficult for malicious third parties to crack. Knowledge factors that are easily discoverable through public record (a user’s legal name, for example) are best avoided, given the ease with which a third party may access that information and seek to breach the secured system.\nPossession factors are items within the user’s possession.7 A key, for example, would be a possession factor in the physical world for a user authenticating her identity at a locked door. With the advent of computer technology, possession factors known as tokens have become an important facet of multifactor authentication. A digital token is simply a form of code that represents the identity of the user and serves as a physical identifier for whichever system the user seeks to access.8 A token will often use constantly changing authorization credentials to match the constantly changing access code for the system. The token and program are synced to change their credentials just as two watches can be synced for time, and each follows the same sequence as its counterpart.\nThere are two primary types of tokens: Hardware (hard) tokens and software (soft) tokens, each with their own advantages and disadvantages. Hard tokens are physical items that can provide a user with access to a system, for example, a security access card.9 The credentials are stored in that device alone which must be physically carried with the user. In contrast, soft tokens are stored on electronic devices and can be duplicated and shared across devices.10 More frequently, smartphones are used to store soft tokens and provide users with a convenient way to authenticate their identity but can subject them to potential cyber threats if the device the token is accessed on is compromised.\nInherence factors are those inherent in the user as an individual.11 Biometric technology is used to analyze the user’s physical characteristics and authenticate their identity. This can include any number of identifiers; however, commonly used identifiers include fingerprints, retinas and irises, voice patterns, and facial recognition. These identifiers, usually subjected to a scan, are then compared to a database containing a stored sample for confirmation of the user’s identity. Inherence factors are incredibly difficult to duplicate because they require the unique physical features of the user for access. The systems used to implement biometric data scans can be costly, however, sometimes limiting the attractiveness of their use. TouchID, the fingerprint recognition technology used by Apple, Inc. on its devices, stores fingerprints locally on the device to prevent external access to fingerprint information while still providing the ease of access and protection of an inherence factor.12\nLocation and time, although not strictly considered authentication factors, may also be used to confirm the identity of a user. The user’s normal geographic location (for example, access from a user’s office) and the time of access can help identify a user and protect them from malicious third parties. For instance, if a user makes a withdrawal from their bank in New York at 6:00 PM, a subsequent, same-day withdrawal in Florida at 6:05 PM could alert the user’s bank to potentially fraudulent activity. As it is not possible that the user could be in both places at once, many systems automatically acknowledge this impossibility and react by shutting out the second user or by providing the user with an alert of the potentially unauthorized access.\nFor firms wishing to implement multifactor authentication, the primary concerns relate to the greater expenses associated with more sophisticated authentication systems. Often, deploying more sophisticated systems may require using several different software programs, as well as potential hardware in the issuance of tokens. Both are frequently issued on a subscription basis, requiring monthly or annual fees. In addition to deployment costs, multifactor authentication often carries significant additional support costs. The price of such a process being put in place increases based on the number and complexity of the factors implemented.13\nThere are also some practical concerns with the implementation of multifactor authentication: no system is free from human error. It is possible to forget a password, and tokens (or the hardware with access to tokens) can be lost, stolen, or otherwise compromised. Additionally, information may be properly secured by one institution, but the data may still be compromised if other firms sharing the information are breached. It is therefore important for entire industries to maintain a certain level of protection on their confidential data for their sake and the sake of their affiliates.\nA firm wishing to implement multifactor authentication will need to balance its interests in ease of access, expense, and data security to suit its needs. Although multifactor authentication provides great value to those seeking to protect digital information, criminals are still able to compromise systems implementing multifactor authentication. Phishing schemes, man-in-the-middle, man-in-the-browser, and other malware attacks can bypass certain processes by taking advantage of human error, gleaning authentication factors from unsuspecting users to infiltrate otherwise secure systems.14 While multifactor authentication can be effective, other measures, such as employee training on data security or the creation of a guide for technology best practices within the firm, should be taken to ensure proper treatment of authentication credentials and to provide greater protection for confidential information.\nMULTIFACTOR AUTHENTICATION’S OUTLOOK\nMultifactor authentication has been implemented across many platforms and is nearly ubiquitous in our modern society. Used as a security measure, it can help to protect users against identity theft, fraud, and brute force hacking. The practical approach to multifactor authentication requires finding the proper balance between expense and the need to keep information secure. Not every circumstance will necessitate an intensive multifactor system, nor will every institution be able to justify the costs of keeping one in place. Still, understanding the options available to protect confidential information can help create awareness about the potential risks and ways to manage them appropriately.\nGLTR Staff Member; Georgetown Law, J.D. expected 2018; University of Virginia, B.A. 2014. ©2018, Kyle Swan."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:558e91ff-8325-456f-b909-30379249667d>","<urn:uuid:21ba294e-831e-4daa-adc0-0c2d28dc60c2>"],"error":null}
{"question":"Need quick fact check - both Spanish Real de a ocho + Canadian Maple Leaf were made for international trade purpose?","answer":"Yes, both coins were created for international trade purposes. The Real de a ocho was introduced by King Carlos I in 1497 specifically to promote world trade, and it made a significant impact on the world's silver economy for centuries. Similarly, the Canadian Gold Maple Leaf coins, first struck in 1979, were designed as bullion coins for international investors, and their popularity with investors around the world continues today, being well-positioned for trading and selling.","context":["On 29th April, Spain released a one-kilo pure-silver coin featuring different historic coins that were circulated in Spain and the Americas. It is the second such coin in a special series that was introduced in 2017. The first coin from the series featured large silver pieces from the Americas which were the predecessors of silver dollars. Many coins depicted on the latest coin were circulated in the Hispanic territory. They belong to a time period that ranges from the economic and political unification under the Romans till the beginning of the 21st Century. Some were issued within the territory while others were issued under foreign occupation.\nThe obverse side depicts the following coins:\nThe first coin is a gold tremis of Visigoths from the Kingdom of Toledo. By the early 8th century, these coins were issued in various types depicting different legends. The obverse depicts a front-facing portrait of the king along with the legend: + RECCAREDVS REX. The reverse also depicts the same portrait along with the mint’s legend and the royal epithet: + TOLETO PIVS. The second one is a bronze coin from the city of Bilbilis. The obverse features a man’s head while the reverse depicts an Iberian horseman holding a weapon. The city name is also featured on the reverse in the province’s alphabet.\nThe third coin depicts a silver peseta. Queen Isabel II lost her powers after the revolution of 1868 and the Provisional Government wanted to change the Spanish monetary system. Spain wanted to be a part of the Latin Monetary Union promoted by Napoleon III from December 1865. The union was formed by France, Belgium, Italy, and Switzerland. Greece had joined the union in 1868 and unofficially by Spain. A new monetary system was created in October 1868 to celebrate the Cadiz uprising.\nThe fourth coin is the famous Real de a ocho. King Carlos I wanted to introduce a stable silver currency in 1497 to promote world trade. During that time, a lot of silver was discovered in the Americas and a large number of coins were minted. They were later termed as real de a ocho. They made a big impact on the world’s silver economy for many centuries. They were circulated in Spain until the reign of Isabel II.\nThe obverse side also features a portrait of King Don Felipe VI wearing the insignia of Captain General. Other inscriptions include the legends UNIDADES MONETARIAS ESPANOLAS and FELIPE VI REY DE ESPANA.\nThe Reverse side depicts the following coins:\nThe first one is an orichalcum sesterce of Hadrian. It was a divisor of the silver denarius and was struck in silver initially. Later, it became the lowest divisor in silver and was minted as the highest multiple in bronze. The obverse side depicts the emperor’s bust with cuirass, mantle, and laurel wreath, and the legend IMP CAESAR DIVI TRAIAN AVG F TRAIAN HADRIAN OPT AVG GER. The reverse depicts Concordia seated on a throne, holding a patera in her right hand. Other inscriptions include ONCORDIA and S.C. The second part of the legend is depicted on the obverse: DAC PARTHICO PM TR P COS PP.\nThe second coin is the Roman Republic as, meaning “bronze” in Latin. It comes from the Latin aerarium, the place where bronze coins, or asses, were kept in Rome. The obverse features the two-faced Roman god, Janus. The reverse side depicts a commonly found symbol of a ship’s prow and the value sign I.\nThe third coin is a Carolingian silver dinero of Louis the Pious, introduced by Charlemagne in 793. This coin was circulated for hundreds of years in the West. Silver content was gradually reduced. Finally, dineros de vellon was minted in an alloy made of silver and copper. The obverse features a Greek cross and the legend: + HLVDOVICVS IMP, while the mint’s name is featured in three lines on the reverse: BAR / CINO / NA (Barcelona).\nThe fourth coin is a Dirham of Abd al-Rahman I from Umayyad Dynasty in al-Andalus. The silver dirham was used for transactions on a large scale, but smaller divisors were not available. The coin was broken up for small transactions. The obverse side depicts the kalmia while the reverse features the Surah of the Quran.\nThe reverse side features two Hercules’ columns and the motto PLUS ULTRA. Other inscriptions include the Real Casa de la Moneda mintmark, a crowned “M”, legend ESPANA 2019 and face value 300 EURO.\nThe 1,007 grams .999 Silver Proof coin with a face value of 300 Euro, has a diameter of 100 mm and a mintage limit of 500 pieces. Every coin is numbered from 001 to 500 on its smooth edge. They are packaged in a wooden case along with a numbered certificate of authenticity.\nImage Courtesy: The Royal Mint of Spain","Royal Canadian Mint: A High Standard for Excellence and Innovation\nCanada’s official mint, The Royal Canadian Mint (RCM), makes some of the most popular gold bullion and silver bullion coins on the market. RCM operates two minting facilities which produce everything from Canada’s circulation coins to bullion coins, medals, and circulation coins on behalf of other nations.\nIt was founded in 1908, and operates as a Crown Corporation of Canada. These organizations are state owned but “generally enjoy greater freedom from direct political control than government departments.\" Canada’s mint is somewhat unusual because it is expected to turn a profit even though it is not privately owned.\nThe profit motive at least partially explains why the Mint has been so willing to innovate and launch a wider variety of superb quality coins and bars. Here’s a glimpse into the RCM’s history and achievements over the years.\nRoyal Canadian Mint Silver Products:\nRoyal Canadian Mint History\nCanada relied upon Britain’s Royal Mint to produce circulation coins for more than 50 years after the nation was founded. The nation’s quickly growing population put Canada in dire need of more coin production. However, the process of authorizing and building an independent national mint with the needed capacity would take decades.\nThe first step was an 1890 proposal for building a branch operation of Britain’s Royal Mint located in Canada. The proposal languished for 11 years, before authorization was finally granted in 1901 to build a facility in Ottawa. Seven years of delays, planning and construction followed. The Mint finally began production of Canada’s circulation coins in 1908. Lord Earl Grey himself activated the presses.\nThe Royal Mint in Ottawa operated for more than 20 years before gaining independence from Britain. It was renamed the Royal Canadian Mint in 1931 and began operations under the control of Canada’s Department of Finance.\nWhere are the mints in Canada?\nLate in 1960, the master of the mint of the Ottawa facility proposed the construction of a second facility. The Mint was operating at maximum capacity, and unable to keep up with demand. A large portion of circulation coins had to be made with the help of the Philadelphia branch of the US Mint.\nPoliticians wrestled over whether a second location was truly needed and where it should be built for 11 years. They eventually agreed to construct a new mint facility in Winnipeg. Land was purchased in 1972, and construction began. It was completed in 1976.\nDuring the 11-year debate over building a second location, the Royal Canadian mint was reorganized as a Crown corporation. The expectation was for it to operate more like a private corporation and turn a profit.\nThe original Ottawa facility was declared a National Historic Site of Canada in 1979.\nToday the Winnipeg facility produces circulation coins for Canada as well as for other nations such as Cuba, Norway, Colombia and Iceland. The Ottawa Mint is responsible for striking collectible coins and bullion coins.\nThe year 2007 saw another interesting milestone for the Royal Canadian Mint. To mark the launch of one-ounce Gold Maple Leaf bullion coins made from “five nines” (99.999%) pure gold, the Mint produced a handful of massive 100 kilogram gold Maple Leaf coins. The face value of each coin is $1 million CAN. Guinness World Records certified them as the world’s largest gold coins.\nThe record lasted until 2011 when Australia’s Perth Mint struck its 2,231-pound coin of 99.99% pure gold.\nRoyal Canadian Mint coins have been racking up awards over the past 30 years. The Mint has claimed prizes for artistry, innovation and “Coin of the Year” on multiple occasions. It’s 2012 Coin of the Year commemorated the Queen’s Diamond Jubilee. 1,500 coins were struck in 99.999 pure gold with a genuine Canadian diamond inset, viewable from both sides.\nPalladium Bullion Pioneers\nPalladium bullion has been growing in popularity in recent years. The Royal Canadian Mint was the first to strike a production coin using the platinum group metal in 2005. The decision to issue a palladium version of its iconic Maple Leaf was an attempt to meet the rising demand.\nTo date only two sovereign mints have produced these coins in quantity, with the other being the US Mint which began issuance in 2017.\nRCM produced the Palladium Maple Leaf, which had a fineness rating of .9995, between 2005 and 2007, and again in 2009. Production then went on hiatus until 2015 when it was resumed and continues to this day.\nCanadian Gold Maple Leaf\nThe Maple Leaf, Canada’s popular gold coins were first struck in 1979 and initially carried a fineness rating of .999. The purity was upgraded to “four nines” (.9999) in late 1982.\nThe Royal Canadian Mint became the world’s first to produce 99.999% pure gold coins in 2005 and it remains the only sovereign mint to do so. These higher purity coins have generally been part of a special series, with limited mintages. The total supply of 99.999% Gold Maple Leaf coins is tiny relative to those with 99.99% purity.\nThe 1911 Canadian Silver Dollar is only found in three examples. Interestingly, only two were made from actual silver, the other was struck in lead. One of the silver coins is on display in a museum in Ottawa while the other silver coin was sold at auction in 2003. Its price? Around 1 million dollars. These are the rarest and most valuable Canadian coins.\nThere were fewer than 30 coins struck in the “Dot Cent” group of pennies struck in 1936. Today only three of these coins have been identified in mint condition. During the most recent auction for one, the penny fetched $253,000 – more than 25 million times its face value.\nThe 1921 Canadian 50-cent piece is also sought after by collectors. Nearly 30,000 of these coins were struck and released, but only around 100 of them are believed to still exist. The vast majority have been melted, significantly increasing the value of those that remain.\nThe Royal Canadian Mint has a rich history. In recent decades it has cultivated a reputation of innovation and quality. RCM’s well-made coins honor the country’s wildlife, heroes, and national symbols, while paying homage to the Mint’s British heritage.\nThe interesting designs and unique features have made their bullion coins popular with investors around the world. Buyers can act with confidence knowing they will get a beautiful product which will be well positioned to appreciate and easy to sell."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:0b3a03b4-1de7-4c1d-a0b2-171c7a70eab0>","<urn:uuid:b110cb5a-f3c5-467e-9c03-fca10774d70b>"],"error":null}
{"question":"What's the role of CVE scores in package security decisions?","answer":"CVE (Common Vulnerability and Exposures) scores use the Common Vulnerability Scoring System to rate security risks. IT and Information Security teams use these scores, supplied by institutions like NIST and NVD, to evaluate and make decisions about which open-source packages can be safely used within their networks. For example, healthcare organizations might have standards prohibiting the use of packages with vulnerability scores above 7.","context":["The open-source ecosystem is the engine that drives digital innovation; no single technology vendor can match or exceed the open-source ecosystem’s pace. Open-source innovation in data science, machine learning, and artificial intelligence has revolutionized many of today’s leading-edge fields—emerging models and applications using predictive analytics, natural language processing, robotics, and other cutting-edge tools are rapidly changing the landscape.\nWhile these powerful open-source tools have become essential for differentiation and competitiveness, the adage “with great power comes great responsibility” applies. The open internet is the reason the open-source ecosystem thrive; it is also the reason open-source software has the potential to introduce countless points of failure in an organization’s infrastructure. For heavily-regulated industries such as government, healthcare, and finance, open-source innovation may seem out of reach due to security and compliance restrictions that forbid exposing infrastructure to the internet.\nHeavily-regulated industries can mitigate this risk by implementing “air-gapped” environments, where there is no inbound or outbound internet connection. Without an internet connection, air-gapped environments are physically separated from other computers and networks, eliminating an attack surface that can be open to vulnerabilities.\nHow organizations can safely use open-source software in air-gapped environments\nIf an organization has no outbound internet connection, how can they leverage the benefits of open-source packages and libraries found on the internet? Anaconda recommends two ways to leverage an air-gapped environment: No internet access, or one-way access.\nAir-gapped environments with no internet access\nWhen an organization requires their air-gapped environment to have no internet access:\n- The administrator accesses and downloads packages from a secure location, as provided by Anaconda. This access will allow the administrator to pull down the full package repository, which includes all associated package Common Vulnerability and Exposures (CVEs) metadata.\n- Once the administrator has pulled all the packages and CVEs down, they will be able to export them to a physical location, determined by the organization.\nFor one-way access, the air-gapped environment has a separation between the internet and the internal network:\n- In this case, the organization’s internal instance can be connected to the internet via a proxy or through a secure, unidirectional HTTP network connection, also provided by Anaconda.\n- The administrator is then able to pull down the full package repository, which includes all associated package CVE metadata.\nOrganizations do not have to sacrifice modern innovation for security. By implementing air-gapped solutions into your enterprise workflows, the organization can have complete control over what packages are ported and when they are ported into networks via a manual transfer from an external, physical medium (such as a hard drive or USB flash drive).\nHow do organizations decide which packages get ported?\nDue to the additional security and compliance requirements faced by heavily-regulated industries, IT and Information Security teams in these organizations must have the means to evaluate the integrity and potential vulnerabilities of open-source software.\nIT and Information Security teams can proactively create safe workflows for their data science teams by reviewing packages and their associated CVEs. CVEs use a common risk rating system, referred to as the Common Vulnerability Scoring System, and are supplied by government institutions such as the National Institute of Standards and Technology (NIST) and National Vulnerability Database (NVD). Organizations in the healthcare industry, for example, may have a security standard that stipulates no packages with a known vulnerability score above a 7 can be used within their networks.\nCVE scores make it easy for IT and Information Security teams to make decisions about what packages can be used to address the needs of their data science teams. By combining an air-gapped structure of separation and the manual mediation of packages, organizations can create a safe and compliant open-source software pipeline and secure their networks.\nIBM® Anaconda Repository for IBM Cloud Pak® for Data can be installed in air-gapped environments to provide organizations access to curated, open-source packages without connecting to the internet. Anaconda Repository allows enterprises to centralize their data science projects and confidently manage the security of their open-source packages and libraries used for AI.\nWatch this on-demand webinar to learn how you can secure open-source data science in the enterprise.\nLearn more about Anaconda Repository for IBM Cloud Pak for Data."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:7d14ef4b-3abc-477e-bc2d-0f839f14813e>"],"error":null}
{"question":"How does the concept of frequency harmonics relate across both musical instruments and electronic signal transmission, particularly in terms of their practical applications?","answer":"In musical instruments, harmonics are essential components that create distinctive tones, with each instrument producing specific whole-number multiples of the fundamental frequency. For example, some instruments like bottles that are open on one end can only produce odd-numbered harmonics (3000 Hz, 5000 Hz, etc., for a 1000 Hz fundamental). This harmonic content determines each instrument's unique timbre. In electronic signal transmission, frequency mixing can be deliberately utilized, such as in computer communications networks where high-frequency signals are superimposed on 60 Hz power-line conductors to transmit data along existing power cabling. However, when unintended, these frequency mixtures can cause interference and noise in signal transmission systems.","context":["If you play a musical instrument or have a favorite one, chances are you selected it because of its “tone color.” Tone color (or Timbre), as defined by musicologists, is what you hear being produced. You may have been attracted to the instrument’s richness or piercing quality or to how the sounds were being produced. Perhaps it was the strumming of a guitar, the plucking of the strings of a violin or the scraping sounds of a symbol. The physical characteristics of sounds that determine the perception of tone color include spectrum and envelope (1). It also depends on the “wave form, the sound pressure, the frequency location of the spectrum and the temporal characteristics of the stimulus (1).”\nThere are over twenty different kinds of musical instruments in an orchestra from violin to bass, trumpet to tube, and piccolo to bassoon. As you listen you may find yourself identifying instrumental tones with colors. For you, the clarinet may be producing a pastel shade of blue, the trumpet a brilliant red. Taken a bit further, you may associate different emotions to go with these colors. Arthur Elson (2) “came up with the following:\nViolin All emotions\nCello All emotions but more he mannish\nPiccolo A wild kind of gaiety\nOboe Rustic kind\nTrumpet Bold or martial\nTuba Power or brutality\nEnglish Horn Dreamy or melancholy\nClarinet Eloquent and tender”\nIt is noted that every sound source has an individual quality that is determined by its harmonic profile (3). As noted previously, tone color influences moods. Sound sources that have a simple harmonic profile are thought to have “darker” tone color and tend to sooth human emotions.\nIf simple tone colors are combined with loud dynamics (a term for gradations of amplitude) it will produce moods associated with vigor, trouble, and heroism. These same simple color tones combined with soft dynamics are associated with terror and mystery. Dynamics serve as a natural indicator for emotional mood. Composers writing for film and theatre use these techniques to create background.\nSimilar to changes in dynamics, changes in tone color provide the listener with contrast and variety (4). If a melody is played by one instrument and then another, it sounds different because of the differences in each instrument’s tone color. The use of two different instruments may be chosen to highlight a new melody. As mentioned previously, specific instruments can reinforce a melody’s emotional impact (2 ). The use of electronic techniques allows the composer to create unique color unlike those produced by traditional instruments.\nComposers of the Romantic period increased the use of tone color as a result of a move from “pure” or “absolute” music. They used tone color to write music that told a story, portrayed a mood or described an event. Classicists Hayden or Mozart wrote music for music’s sake. On the other hand, Berlioz, List, and Richard Strauss, nineteenth-century Romanticists employed program music thus concentrating on tone color (2). It has been suggested that Debussy’s music is credited with elevating the role of tone color in music as noted in Prelude a l après-midi d’un faun (5).\nFor a composition to be effective it must contain two of music’s basic elements – tone color and dynamics. They contribute to the feeling of motion and movement in music. Musical instruments provide the tone colors. The combination of rhythm, melody, harmony, dynamics, tone color and instruments form music.\nFor examples of the use of different tone colors log onto http://www.wonderofsound.com.\nAmerican Standards Association (1960). American Standard Acoustical Terminology. New York: American Standards Association.\nGoulding, Phil G. Classical Music. p.31. Fawcett Columbine, New York.\nErickson, Robert . The Structure of Music: A Listener’s Guide. University of California Press. 1975.\nGrey, John M. (1977). “Multidimensional Perceptual Scaling of Musical Timbres”. The Journal of the Acoustical Society of America 61(5): 1270–77.\nSamson, Jim (1977) Music in Transition: A Study of Tonal Expansion and Atonality, 1900-1920. New York: W.W.Norton & Company.","In our study of AC circuits thus far, we've explored circuits powered by a single-frequency sine voltage waveform. In many applications of electronics, though, single-frequency signals are the exception rather than the rule. Quite often we may encounter circuits where multiple frequencies of voltage coexist simultaneously. Also, circuit waveforms may be something other than sine-wave shaped, in which case we call them non-sinusoidal waveforms.\nAdditionally, we may encounter situations where DC is mixed with AC: where a waveform is superimposed on a steady (DC) signal. The result of such a mix is a signal varying in intensity, but never changing polarity, or changing polarity asymmetrically (spending more time positive than negative, for example). Since DC does not alternate as AC does, its “frequency” is said to be zero, and any signal containing DC along with a signal of varying intensity (AC) may be rightly called a mixed-frequency signal as well. In any of these cases where there is a mix of frequencies in the same circuit, analysis is more complex than what we've seen up to this point.\nSometimes mixed-frequency voltage and current signals are created accidentally. This may be the result of unintended connections between circuits -- called coupling -- made possible by stray capacitance and/or inductance between the conductors of those circuits. A classic example of coupling phenomenon is seen frequently in industry where DC signal wiring is placed in close proximity to AC power wiring. The nearby presence of high AC voltages and currents may cause “foreign” voltages to be impressed upon the length of the signal wiring. Stray capacitance formed by the electrical insulation separating power conductors from signal conductors may cause voltage (with respect to earth ground) from the power conductors to be impressed upon the signal conductors, while stray inductance formed by parallel runs of wire in conduit may cause current from the power conductors to electromagnetically induce voltage along the signal conductors. The result is a mix of DC and AC at the signal load. The following schematic shows how an AC “noise” source may “couple” to a DC circuit through mutual inductance (Mstray) and capacitance (Cstray) along the length of the conductors. (Figure below)\nStray inductance and capacitance couple stray AC into desired DC signal.\nWhen stray AC voltages from a “noise” source mix with DC signals conducted along signal wiring, the results are usually undesirable. For this reason, power wiring and low-level signal wiring should always be routed through separated, dedicated metal conduit, and signals should be conducted via 2-conductor “twisted pair” cable rather than through a single wire and ground connection: (Figure below)\nShielded twisted pair minimized noise.\nThe grounded cable shield -- a wire braid or metal foil wrapped around the two insulated conductors -- isolates both conductors from electrostatic (capacitive) coupling by blocking any external electric fields, while the parallal proximity of the two conductors effectively cancels any electromagnetic (mutually inductive) coupling because any induced noise voltage will be approximately equal in magnitude and opposite in phase along both conductors, canceling each other at the receiving end for a net (differential) noise voltage of almost zero. Polarity marks placed near each inductive portion of signal conductor length shows how the induced voltages are phased in such a way as to cancel one another.\nCoupling may also occur between two sets of conductors carrying AC signals, in which case both signals may become “mixed” with each other: (Figure below)\nCoupling of AC signals between parallel conductors.\nCoupling is but one example of how signals of different frequencies may become mixed. Whether it be AC mixed with DC, or two AC signals mixing with each other, signal coupling via stray inductance and capacitance is usually accidental and undesired. In other cases, mixed-frequency signals are the result of intentional design or they may be an intrinsic quality of a signal. It is generally quite easy to create mixed-frequency signal sources. Perhaps the easiest way is to simply connect voltage sources in series: (Figure below)\nSeries connection of voltage sources mixes signals.\nSome computer communications networks operate on the principle of superimposing high-frequency voltage signals along 60 Hz power-line conductors, so as to convey computer data along existing lengths of power cabling. This technique has been used for years in electric power distribution networks to communicate load data along high-voltage power lines. Certainly these are examples of mixed-frequency AC voltages, under conditions that are deliberately established.\nIn some cases, mixed-frequency signals may be produced by a single voltage source. Such is the case with microphones, which convert audio-frequency air pressure waves into corresponding voltage waveforms. The particular mix of frequencies in the voltage signal output by the microphone is dependent on the sound being reproduced. If the sound waves consist of a single, pure note or tone, the voltage waveform will likewise be a sine wave at a single frequency. If the sound wave is a chord or other harmony of several notes, the resulting voltage waveform produced by the microphone will consist of those frequencies mixed together. Very few natural sounds consist of single, pure sine wave vibrations but rather are a mix of different frequency vibrations at different amplitudes.\nMusical chords are produced by blending one frequency with other frequencies of particular fractional multiples of the first. However, investigating a little further, we find that even a single piano note (produced by a plucked string) consists of one predominant frequency mixed with several other frequencies, each frequency a whole-number multiple of the first (called harmonics, while the first frequency is called the fundamental). An illustration of these terms is shown in Table below with a fundamental frequency of 1000 Hz (an arbitrary figure chosen for this example).\nFor a “base” frequency of 1000 Hz:\n|1000||1st harmonic, or fundamental|\nSometimes the term “overtone” is used to describe the a harmonic frequency produced by a musical instrument. The “first” overtone is the first harmonic frequency greater than the fundamental. If we had an instrument producing the entire range of harmonic frequencies shown in the table above, the first overtone would be 2000 Hz (the 2nd harmonic), while the second overtone would be 3000 Hz (the 3rd harmonic), etc. However, this application of the term “overtone” is specific to particular instruments.\nIt so happens that certain instruments are incapable of producing certain types of harmonic frequencies. For example, an instrument made from a tube that is open on one end and closed on the other (such as a bottle, which produces sound when air is blown across the opening) is incapable of producing even-numbered harmonics. Such an instrument set up to produce a fundamental frequency of 1000 Hz would also produce frequencies of 3000 Hz, 5000 Hz, 7000 Hz, etc, but would not produce 2000 Hz, 4000 Hz, 6000 Hz, or any other even-multiple frequencies of the fundamental. As such, we would say that the first overtone (the first frequency greater than the fundamental) in such an instrument would be 3000 Hz (the 3rd harmonic), while the second overtone would be 5000 Hz (the 5th harmonic), and so on.\nA pure sine wave (single frequency), being entirely devoid of any harmonics, sounds very “flat” and “featureless” to the human ear. Most musical instruments are incapable of producing sounds this simple. What gives each instrument its distinctive tone is the same phenomenon that gives each person a distinctive voice: the unique blending of harmonic waveforms with each fundamental note, described by the physics of motion for each unique object producing the sound.\nBrass instruments do not possess the same “harmonic content” as woodwind instruments, and neither produce the same harmonic content as stringed instruments. A distinctive blend of frequencies is what gives a musical instrument its characteristic tone. As anyone who has played guitar can tell you, steel strings have a different sound than nylon strings. Also, the tone produced by a guitar string changes depending on where along its length it is plucked. These differences in tone, as well, are a result of different harmonic content produced by differences in the mechanical vibrations of an instrument's parts. All these instruments produce harmonic frequencies (whole-number multiples of the fundamental frequency) when a single note is played, but the relative amplitudes of those harmonic frequencies are different for different instruments. In musical terms, the measure of a tone's harmonic content is called timbre or color.\nMusical tones become even more complex when the resonating element of an instrument is a two-dimensional surface rather than a one-dimensional string. Instruments based on the vibration of a string (guitar, piano, banjo, lute, dulcimer, etc.) or of a column of air in a tube (trumpet, flute, clarinet, tuba, pipe organ, etc.) tend to produce sounds composed of a single frequency (the “fundamental”) and a mix of harmonics. Instruments based on the vibration of a flat plate (steel drums, and some types of bells), however, produce a much broader range of frequencies, not limited to whole-number multiples of the fundamental. The result is a distinctive tone that some people find acoustically offensive.\nAs you can see, music provides a rich field of study for mixed frequencies and their effects. Later sections of this chapter will refer to musical instruments as sources of waveforms for analysis in more detail.\n- A sinusoidal waveform is one shaped exactly like a sine wave.\n- A non-sinusoidal waveform can be anything from a distorted sine-wave shape to something completely different like a square wave.\n- Mixed-frequency waveforms can be accidently created, purposely created, or simply exist out of necessity. Most musical tones, for instance, are not composed of a single frequency sine-wave, but are rich blends of different frequencies.\n- When multiple sine waveforms are mixed together (as is often the case in music), the lowest frequency sine-wave is called the fundamental, and the other sine-waves whose frequencies are whole-number multiples of the fundamental wave are called harmonics.\n- An overtone is a harmonic produced by a particular device. The “first” overtone is the first frequency greater than the fundamental, while the “second” overtone is the next greater frequency produced. Successive overtones may or may not correspond to incremental harmonics, depending on the device producing the mixed frequencies. Some devices and systems do not permit the establishment of certain harmonics, and so their overtones would only include some (not all) harmonic frequencies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:9c4a4006-af83-42f8-8c2b-9cc124a909ba>","<urn:uuid:3272d07c-fa09-4729-b67b-e3859dd47357>"],"error":null}
{"question":"Which country is larger in terms of total area: Switzerland or the Dominican Republic?","answer":"Switzerland has a total area of 15,940 square miles, while the Dominican Republic is significantly larger, occupying two-thirds of the island of Hispaniola with a territory of 48,442 square kilometers (18,704 square miles).","context":["The Dominican Republic was once ruled by Spain, including its neighbor, Haiti, which was once a French colony. The country being so diverse, with mountains. Santo Domingo is the capital city of the Dominican Republic. WHO Region: Region of the Americas. Country Office Website Travel Advice. The Dominican Republic is a Caribbean country that occupies the eastern two-thirds of the Caribbean island of Hispaniola. The western one-third of Hispaniola is. The Dominican Republic is a presidential democratic republic. The government is divided in three branches: the Executive, the Legislative and the Judiciary. The. Here are 17 remarkable facts about this Caribbean nation: · Armed forces personnel and members of the national police aren't allowed to vote. · The DR is the only. Dominican Republic makes up one half of the island of Hispaniola – which it shares with Haiti in the west. The country is one of the most geographically diverse. In , the Dominicans voluntarily returned to the Spanish Empire, but two years later they launched a war that restored independence in A legacy of.\nThe Dominican Republic is the second largest Caribbean country and is known for its diverse culture and hospitable people. Located just a short flight from. The Dominican Republic Flag A centered white cross that extends to the edges divides the flag into four rectangles - the top ones are blue (hoist side) and. The Dominican Republic is the 2nd largest country in the Caribbean (by both landmass and population) and is the most frequently visited. It is divided into\nDominican Republic ; Caribbean · Upper middle income · 6, USD, GNI per capita · 10,, · Rank 92 of The Dominican Republic is the second largest Caribbean nation after Cuba, with an estimated 10 million people, and was discovered by Christopher Columbus in. National Geographic's latest travel stories about Dominican Republic. 5 Ways to Explore the Dominican Republic Off the Beaten Path. The Dominican Republic is situated on the eastern part of the island Hispaniola, the second largest island in the Greater Antilles and it's the second largest.\nThe Dominican Republic comprises two-thirds of the island of Hispaniola, one of the few islands in the world shared by two independent nation-states. COVID Vaccine Information: Are vaccines available in the Dominican Republic for U.S. citizens to receive? Yes. According to Dominican authorities, foreigners. You can find Dominican Republic-specific information on web05.ru For information about health in the Dominican Republic, see the CDC Yellow Book.\nA Democratic nation with a population of over 10 million people, the Dominican Republic has a territory stretching 48, square kilometers (18, square. Following both French and Spanish rule from as early as the 16th century, the island nation of the Dominican Republic declared itself an independent nation from. Following both French and Spanish rule from as early as the 16th century, the island nation of the Dominican Republic declared itself an independent nation from.\nThe Dominican Republic is a Caribbean country located on the eastern two-thirds of the island of Hispaniola. It shares the island with Haiti to the west. Country Facts Capital: Santo Domingo de Guzmán Population: (July ) Area: sq. Km. Currency: Dominican peso (RD$) Independence. The Dominican Republic is in the Caribbean. It is a part of the island of Hispaniola / la Española. The Dominican Republic shares the island with Haiti. The Dominican Republic is a country of origin of a large number of emigrants. Several estimates have shown that there are around one million Dominican.","Switzerland is located in the western region of Europe, where it covers a total area of 15,940 square miles. Of this area, approximately 4.2% is made up of various bodies of water. Some of the largest bodies of water in this country include Lake Geneva, Lake Constance, and the Rhine River. The entire country can be divided into 3 specific geographic regions: the Jura Mountains to the north, the Swiss Alps Mountains to the south, and the Central Plateau between the two. Switzerland has no direct access to the ocean or the sea, which means it is considered a landlocked country and must rely on friendly relations with neighboring countries that have access to sea ports. The territory of Switzerland is surrounded by nearly 1,180 miles of borders, which it shares with 5 other autonomous nations: Germany, Austria, Liechtenstein, Italy, and France. This article takes a closer look at each of these borders.\nCountries Bordering Switzerland\nThe border between Switzerland and Germany stretches for a total of 208 miles and is situated along the northern edge of Switzerland. The easternmost point of this border begins at the tripoint shared between Switzerland, Germany, and Austria at Lake Constance. From here, the boundary runs in an eastern direction until the town of Stein am Rhein, where it curves north and into German territory before turning south toward Switzerland once again. The majority of this border follows the course of the Rhine River, specifically the area known as the High Rhine. It ends at the tripoint location shared between Switzerland, Germany, and France. This particular location is distinguished by the Dreiländereck Monument, a tall pillar-like structure with one of the flags of each country located on one of its three sides.\nThe border between Switzerland and Austria runs for a total of 102 miles, along the eastern edge of Switzerland. The length of this border is separated into two sections due to the position of the country of Liechtenstein, which lies in the middle of the border with Austria to its east and Switzerland to its west. At its northernmost point, this boundary begins where Switzerland, Austria, and Italy meet. From here, it runs in a southern direction and moves through the Grison Alps mountain range. It is interrupted by Liechtenstein and begins again at the southern end of this country. From here, the border between Switzerland and Austria primarily follows the Alpine Rhine River Valley until it reaches Lake Constance in the south. On this border, the highest elevation is located at the Piz Fanga mountain peak, which stands at 11,148.29 feet above sea level. Its lowest point is at Lake Constance, which is 1,295.93 feet above sea level.\nThe border between Switzerland and Liechtenstein measures at only 25 miles in length, making it the shortest border in Switzerland. It runs along part of the eastern edge of Switzerland, effectively dividing the border between Switzerland and Austria into two section. Its position makes Liechtenstein a doubly landlocked country, which means it is surrounded by other landlocked countries. Switzerland and Liechtenstein share a long history of friendly border relations. In fact, these two countries enjoy control-free border crossings, meaning individuals can freely pass between these two autonomous nations. The collaboration between these two countries extends into their economies as well; Liechtenstein used the Swiss Franc for several years prior to the implementation of the Euro currency. As the larger country, the government of Switzerland has always acted in the best interest of both countries. It often represents Liechtenstein in negotiations and in international events, if representatives from Liechtenstein are not present.\nThe border between Switzerland and Italy stretches for a total of 460 miles, making it the longest international border in Switzerland. It is situated along the southern edge of this country and runs in an east to west direction. At its easternmost point, this border is located at the tripoint border between Switzerland, Italy, and France. This point is marked by Mont Dolent, a mountain peak in the Mont Blanc mountain range of the Alps Mountains. This boundary line then moves in an eastward direction through the High Alps region, which is known for its rugged terrain and extremely frigid climate. From this region, the border also runs through Lake Maggiore, which is the lowest elevation in Switzerland at 656 feet above sea level. This line continues until it reaches the tripoint boundary between Switzerland, Italy, and Austria in the east. As a result of the European refugee crisis, which began in 2015, the number of undocumented immigrants crossing the border from Italy into Switzerland has risen. In response the government of Switzerland has increased border patrol efforts, although it has turned down proposals to build a physical barrier between the two countries.\nThe border between Switzerland and France runs over a distance of 356 miles, making it the second longest border in Switzerland. It makes up the entire western edge of this country and passes through the Mont Blanc region of the Alps Mountain range. Its highest point in this mountain range is Aiguille d’Argentiere, which stands at 12,798.56 feet above sea level. Although the location of this border was agreed upon in 1815, it has experienced some changes over the years. One of the most recent changes occurred in 2002, when the territory of Switzerland gained a total area of 16,985.45 square feet. The border between these two countries begins at the tripoint shared between Switzerland, France, and Germany. From here, the border moves in a generally southwestern direction until it reaches the city of Ginebra in Switzerland. At this location, the border between these two countries takes a turn to the east, heads northeast, and passes through Lake Geneva (the largest lake in Switzerland by surface area). After passing through this lake, the border turns to the south until it reaches the tripoint shared between Switzerland, France, and Italy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2550a966-46ae-4f62-8e31-04af88d9c3e3>","<urn:uuid:924b21e4-78d1-418d-ba6d-659ea6e22683>"],"error":null}
{"question":"Can you clarify the specific relationship between CEC and soil amendments? I'm particularly interested in how compost applications affect CEC levels.","answer":"CEC (Cation Exchange Capacity) cannot be easily adjusted through soil amendments. While compost is an excellent soil amendment, it's not stable humus, and only 1-10% of the initial application may eventually become humus. To raise soil organic matter by just 1%, it requires two cubic yards of compost per thousand square feet (assuming 50% moisture content). Large surface applications of compost can actually be harmful, as abrupt soil layers can inhibit water movement and moisture retention. Natural/organic nitrogen sources are generally more effective than synthetic chemicals at raising or preserving organic matter levels due to their stimulation of biological activity.","context":["Cation Exchange Capacity (CEC) – First, what are cations (pronounced CAT-eye-ons)? Simply put, they are positively charged ions. The cations in the soil that concern us the most are calcium (Ca), magnesium (Mg), potassium (K), and hydrogen (H). The capacity of the soil to hold and exchange cations is determined by the amount of clay and/or humus that is present. These two colloidal (negatively charged) substances are essentially the cation warehouse or reservoir of the soil. Sandy soils with very little organic matter (OM) have a low CEC, but heavy clay soils with high levels of OM have a much greater capacity to hold cations.\nThe disadvantages of a low CEC include the limited availability of mineral nutrient to the plant and the soil’s inefficient ability to hold applied nutrient. Plants can exhaust a fair amount of energy (which might otherwise have been used for growth, flowering, seed production or root development) scrounging the soil for mineral nutrients. Soluble mineral salts (e.g. potassium sulfate) applied in large doses to soil with a low CEC cannot be held efficiently because the cation warehouse is too small.\nWater also has a strong attraction to colloidal particles. All functions that are dependent on soil moisture are also limited in soils with low CEC. Organisms such as plants and microbes that depend upon each other’s biological functions for survival are inhibited by the lack of water. Where there is little water in the soil, there is often an abundance of air, which can limit the accumulation of organic matter (by accelerating decomposition) and further perpetuate the low level of soil colloids.\nHigh levels of clay with low levels of OM would have an opposite effect (a deficiency of air), causing problems associated with anaerobic conditions. The CEC in such a soil might be very high, but the lack of atmosphere in the soil would limit the amount and type of organisms living and/or growing in the area, causing dramatic changes to that immediate environment. Oxidized compounds such as nitrates (NO3) and sulfates (SO4) may be reduced (i.e., oxygen is removed) by bacteria that need the oxygen to live, and the nitrogen and sulfur could be lost as available plant nutrients. Accumulation of organic matter is actually increased in these conditions because the lack of air slows down decomposition. Eventually, enough organic matter may accumulate to remedy the situation, but it could take decades or even centuries.\nThe CEC of a soil is a value given on a soil analysis report to indicate its capacity to hold cation nutrients. CEC is not something that is easily adjusted, however. It is a value that indicates a condition, or possibly a restriction that must be considered when working with that particular soil. Unfortunately, CEC is not a packaged product. The two main types of colloidal particles in the soil are clay and humus and neither is practical to apply in large quantities. Compost, which is an excellent soil amendment, is not necessarily stable humus. Over time compost may become humus, but the end product might only amount to 1-10 percent (in some cases, less) of the initial application.\nRemember that each percent of organic matter in the soil is equal to over 450 pounds per 1,000 square feet (20,000 lbs/acre). Compost normally contains about forty to fifty percent OM on a dry basis, and weighs approximately 1,000 pounds per cubic yard (depending on how much moisture it contains). If the moisture level is fifty percent, it would take two cubic yards of compost per thousand square feet to raise the soil OM level one percent (temporarily). Large applications of compost to the surface of the soil, however, can do more harm than good. Abrupt changes in soil layers can inhibit the movement of water and restrict the soil’s capacity to hold moisture. Obviously, building organic matter in the soil is not something that can or should be done overnight. Natural/organic nitrogen sources, in general, will do more than synthetic chemicals to raise or preserve the level of OM, because of the biological activity they stimulate. Colloidal phosphate contains a natural clay and is often used to condition sandy soils with a low CEC. Low phosphorus conditions should be present, however, to justify its use.\nIf a soil has a very low CEC, adjustments can and should be made, but not solely because of the CEC. A soil with a very low CEC has little or no clay or humus content. Its description may be closer to sand and/or gravel than to soil. It cannot hold very much water or many cation nutrients; plants, therefore, cannot grow well. The reason for the necessary adjustment is not the need for a higher CEC, but because the soil needs conditioning. A direct result of this treatment will eventually be a higher CEC. During the process of soil building, the steward must be aware of the soil’s limitations. Soil with a low CEC cannot hold many nutrients, so smaller amounts of fertilizer should be applied more frequently. Feeding a crop growing on soil with a low CEC is analogous to feeding an infant. It doesn’t eat a lot but must be fed often. As the CEC of the soil improves, larger doses of fertilizers can be applied less frequently."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f87f48e9-cfd4-4086-9d05-12115f368b81>"],"error":null}
{"question":"How do the mortality rates and treatment urgency compare between prostatitis and necrotizing fasciitis?","answer":"The mortality rates and treatment urgency differ significantly between these conditions. Prostatitis generally has a good prognosis with most men recovering fully when treated with antibiotics, and chronic prostatitis often gets better over time without serious complications. In contrast, necrotizing fasciitis has a much higher mortality rate of up to 80%, depending on associated factors. Regarding treatment urgency, while prostatitis can often be treated with antibiotics over several weeks, necrotizing fasciitis demands immediate intervention - early recognition and aggressive surgical debridement must be performed within 6 hours of symptom onset for optimal outcomes.","context":["Skip to Content\nHome > Wellness > Health Library > Prostatitis\nProstatitis is swelling or\ninfection of the\nprostate gland. It often hurts. The\nprostate gland sits just below a man's bladder and makes part of the fluid for\nsemen. In young men, the prostate is about the size of\na walnut. It usually grows larger as you grow older.\nseveral types of prostatitis. They vary based on how long a man has had the\nproblem and what kind of symptoms he has.\nSometimes prostatitis is\ncaused by bacteria, but often the cause is not known.\nSymptoms of long-term\n(chronic) prostatitis are often mild and start slowly over weeks or months.\nThey may include:\nSymptoms of acute prostatitis are the same, but they\nstart suddenly and are severe. They may also include a fever and chills.\nSome men may have no symptoms.\nA doctor can often\ntell if you have prostatitis by asking about your symptoms and past health. He\nor she will also do a physical exam, including a\ndigital rectal exam. In this test, the doctor puts a\ngloved, lubricated finger in your rectum to feel your prostate. You may also\nneed blood and urine tests to find out which type of prostatitis you have or to\nlook for another cause of your problems.\nProstatitis caused by bacteria\nis treated with antibiotics and self-care.\nHome treatment includes\ndrinking plenty of fluids and getting lots of rest. Taking\nover-the-counter pain relievers can also help.\nYour doctor may prescribe medicine to control pain and reduce swelling.\nHe or she may also prescribe medicine to soften your stool and relax your\nSurgery is rarely used to treat prostatitis.\nLearning about prostatitis:\nHealth Tools help you make wise health decisions or take action to improve your health.\nMost men with\nprostatitis have chronic prostatitis, which may also be called pelvic pain syndrome. The cause of this type of\nprostatitis is not known.\nBacteria infecting the prostate cause\nacute prostatitis and\nchronic bacterial prostatitis. Bacteria most likely\nenter the prostate by:\nThe symptoms are similar for all forms of\nprostatitis, with the exception of\nSymptoms of chronic prostatitis may include:\nWith acute prostatitis, symptoms are severe, come on\nsuddenly, and include fever and chills. Signs of\nchronic bacterial prostatitis may be milder and come\non suddenly or gradually over weeks or months, and the symptoms may come and\ngo. Symptoms alone cannot be used to determine the type of prostatitis you\nOther conditions, such as a\nbladder infection or\nbenign prostatic hyperplasia (BPH), can cause symptoms\nsimilar to those of prostatitis.\nespecially if it continues for a long time, can cause\nChronic prostatitis often gets better over time without\nserious complications. But the symptoms sometimes return\nacute prostatitis have severe pain and fever. Most men recover fully when treated with antibiotics.\nDelaying treatment increases the risk of complications, such as\nsepsis or an\nabscess in the prostate.\nChronic bacterial prostatitis can be difficult to treat, because some\nmedicines have a hard time reaching the prostate.\nchronic bacterial prostatitis commonly have repeated urinary tract infections.\nThe infection may spread to the\nThe presence of infected\nprostate stones (prostatic calculi) can make treatment of chronic\nbacterial prostatitis more difficult.\nThings that can increase\nyour risk for\nIf you have had\nchronic bacterial prostatitis, you have an increased\nchance of developing it again.\nContact your doctor immediately if you have sudden fever, chills, and\nurinary symptoms, such as pain or burning with urination or blood or pus in the\nurine. These symptoms may point to acute prostatitis.\nCall your doctor if you\nMost men will have some discomfort in their\nprostate (prostatitis) at some time during their lives. If you\ndo not have a fever and chills or extreme pain, you may try home treatment for\na few weeks. Take nonprescription pain medicines, such as aspirin, ibuprofen,\nor acetaminophen, to relieve pain. But if your urinary symptoms and pain\ncontinue, be sure to see a doctor.\nHealth professionals who can evaluate and treat your\nTo prepare for your appointment, see the topic Making the Most of Your Appointment.\nIf your doctor suspects\nthat you have\nprostatitis, he or she will begin with a complete\nmedical history and physical exam. The type of\nprostatitis that you have cannot be determined solely from your history and\nsymptoms. Your doctor will do tests to find out the cause of your\nAcute prostatitis is the least common\ntype but the easiest to diagnose. If acute prostatitis is suspected, a\nurine culture will be done to test for the presence\nand type of bacteria.\nIf your history and physical exam show that\nyou do not have acute prostatitis, a\npre- and post-massage test (PPMT) or\nexpressed prostatic secretions test may be done to\nfind out which type of prostatitis you have. An expressed prostatic secretions\ntest is not done if acute prostatitis is suspected, because when the prostate\nis inflamed or infected, massaging it to obtain a sample for tests is very\npainful and possibly dangerous. Some doctors believe that\nmassaging an infected prostate increases the risk of developing a bacterial\ninfection of the blood (septicemia).\nmay be needed if:\nTests that may be done include:\nprostatitis usually begins with taking an antibiotic\nfor several weeks. If you begin to feel better, you may have to take the\nmedicine for 2 to 3 months. If you do not get better while taking\nantibiotics, more tests may be done.\nYou may need to try more than one treatment. There isn't a standard treatment that works well for all men.\nacute prostatitis is aimed at curing the infection and\npreventing complications. Acute bacterial prostatitis is treated with\nantibiotics, pain and fever medicine, stool softeners, fluids, and\nchronic bacterial prostatitis is aimed at curing the\ninfection and preventing complications. Antibiotics are given for 6 to 12\nweeks. Long-term antibiotic treatment may be needed if the infection\nYou may be able to prevent\nProstatitis is usually treated with antibiotics and other medicines prescribed by your doctor. But there are some things you can do at home that may help you be more\nprostatitis usually begins with antibiotics and\npossibly other medicines to relieve symptoms. If you begin to get better, you\nmay have to continue taking antibiotics for 2 to 3 months. During this time, be sure to take the antibiotics as prescribed. If you do not begin to get\nbetter while taking medicines, your doctor may want you to have more\nChronic prostatitis is usually treated first with\nantibiotics based on the possibility that an infection was missed during\ntesting. But experts advise against long-term treatment with antibiotics\nunless an unusual bacterial infection is suspected.\nthat may be used to treat chronic prostatitis include:\nAntibiotics are central to treating\nchronic bacterial prostatitis. Your doctor may\nprescribe certain antibiotics based on your medical history, symptoms, and\nother factors such as your age. Other medicines may also be used to help\ncontrol symptoms, including:\nChronic bacterial prostatitis may require long-term\nantibiotics, especially if the symptoms return. Some men need treatment with\nlow doses of antibiotics over a long period to control infection and prevent\nurinary tract infections (UTIs).\nprostatitis may be needed to treat\nchronic bacterial prostatitis that does not respond to\nlong-term antibiotic treatment and that causes repeated\nurinary tract infections. Surgery may be done to\nremove part of the prostate or to remove infected prostate stones (prostatic calculi).\nBut this does not always cure the infection, and it may make the symptoms\nworse. Surgery is typically done only if all other treatments have\nSurgical removal of part of the prostate to remove prostate\nstones or to treat an\ninfection that does not respond to antibiotic treatment is called\nSurgery to remove part of the prostate that is blocking urine flow is called transurethral resection of the prostate (TURP). This type of surgery may be done in men with benign prostatic hyperplasia (BPH) who are having problems with prostatitis.\nProstatic massage for\nprostatitis (\"milking\" of the prostate by a doctor) is an old treatment that many doctors are beginning to use again\nbecause medicines do not always successfully cure prostatitis.\nTo massage your\nprostate gland, the doctor inserts a\nlubricated, gloved finger into your rectum and presses several times on your\nprostate. This may need to be done 2 or 3 times a week. Why this works is not\ncertain, but it is believed that the massage helps open blocked ducts in the\nprostate, improving circulation and antibiotic penetration into the\nProstatic massage is not done for acute prostatitis,\nbecause it could cause the bacteria to spread from the prostate and cause a\nwider infection (sepsis).\nOther treatments that may be helpful for prostatitis include:\nAnothaisintawee T, et al. (2011). Management of chronic prostatitis/chronic pelvic pain syndrome. JAMA, 305(1): 78–86.\nNickel JC (2012). Prostatitis and related conditions, orchitis, and epididymitis. In AJ Wein et al., eds., Campbell-Walsh Urology, 10th ed., vol. 1, pp. 327–356. Philadelphia: Saunders.\nOther Works Consulted\nGupta K, Trautner BW (2012). Urinary tract infections, pyelonephritis, and prostatitis. In DL Longo et al., eds., Harrison's Principles of Internal Medicine, 18th ed., vol. 2, pp. 2387–2395. New York: McGraw-Hill.\nKuritzky L (2013). Prostatitis. In ET Bope, RD Kellerman, eds., Conn's Current Therapy 2013, pp. 969–972. Philadelphia: Saunders.\nLe B, Schaeffer AJ (2011). Chronic prostatitis, search date August 2010. BMJ Clinical Evidence. Available online: http://www.clinicalevidence.com.\nByHealthwise StaffPrimary Medical ReviewerE. Gregory Thompson, MD - Internal MedicineSpecialist Medical ReviewerChristopher G. Wood, MD, FACS - Urology, Oncology\nCurrent as ofJune 4, 2014\nCurrent as of:\nJune 4, 2014\nE. Gregory Thompson, MD - Internal Medicine & Christopher G. Wood, MD, FACS - Urology, Oncology\nHow this information was developed to help you make better health decisions.\nTo learn more, visit Healthwise.org\n© 1995-2015 Healthwise, Incorporated. Healthwise, Healthwise for every health decision, and the Healthwise logo are trademarks of Healthwise, Incorporated.\n250 Pleasant Street\nConcord, NH 03301\nContact Concord Hospital\nView Quality Data\n© 2015 Concord Hospital","D Huljev. Necrotizing Fasciitis of the Abdominal Wall with Lethal Outcome: A Case Report. The Internet Journal of Plastic Surgery. 2005 Volume 2 Number 2.\nNecrotizing fasciitis is an acute surgical condition that demands a prompt and combined treatment. Early recognition, aggressive surgical debridement, and targeted antibiotic therapy significantly affect the overall course of treatment and, ultimately, survival. The authors present a case of a woman with necrotizing fasciitis of the abdominal wall and the course and methods of treatment. Because of comorbidy factors (extreme obesity, diabetes), and late proper diagnosis of necrotizing fasciitis (the clinical signs were \"hidden\" by celullitis, and phlegmona of abdominal wall), and then as the consequence, overdue adequate surgical treatment, unfortunately contributed to medical treatment failure, respectively lethal outcome.\nProgressive necrotizing inflammatory lesions of soft tissues are relatively rare entities in our everyday surgical practice. In USA there are approximately around 500 - 1500 cases per year.(1)\nThe term necrotizing fasciitis is used as the generic name for necrotizing infections of soft tissue. This term includes different syndromes of progressive necrotic infections of skin and subcutaneous tissue.(2) It concerns rapid progressive infection, which affects the fascia and subcutaneous tissue, with simultaneous development of thromboses of skin microcirculation, resulting in necrosis of soft tissue and skin, destruction of muscles and liquefaction of fat(2,3)\nMedical treatment is a very complex one. On the front burner is surgical debridement beside the aimed application of broad-spectrum antibiotics. Additionally, auxiliary measures can be applied, such as negative pressure wound therapy (NPWT) or hyperbaric oxygenation.(4,5) Infection outcome is always uncertain, and the mortality in these patients is up to 80 %, depending upon associated comorbid factors, the way of treatment and the development of complications (the acute renal insufficiency, acute respiratory distress syndrome - ARDS and multiorganic failure - MOF).(6,7,8,9,10,11,12) In this article has been presented the case of female patient with the necrotic fasciitis of bottom of abdominal wall, perineum and inquinofemoral region, who in spite of all the executed measures, resulted with the lethal outcome.\nClinical status and course of treatment\nA female patient 60 years old, obese (high 161 cm, weight 114 kg, BMI = 44), mother of five children, diabetic, had raised temperature, inflammatory changes in the area of groins and light pains in this area, which lasted in the past 3 days.\nDuring the examination, a surgeon made the diagnosis of excessive body weight, hanging stomach, erythema and cellulites beside minimal sore spots of the bent and hanging bottom of abdomen. Furthermore, he recommended antibiotic therapy and directed the patient to the dermatologist. Two days after she was examined by the dermatologist who described the skin erythema of abdominal wall and groins, which were warm and milder painful sensitive.\nBecause of the general plight, the patient was hospitalized at the Internal Clinic, and the same day was moved to the Department of Nephrology, under the diagnosis of abdominal walls phlegmona. At the admittance the patient was conscious, febrile 38,4°C with the expressed erythema of wall of lower half abdomen and groins, with skin bullae filled with yellowish liquid contents. At the admittance sedimentation was 132, and leukocytes 8,23 x 109 / l. From urinoculture were isolated\nIn spite of intensive antibiotic therapy, inflammatory and necrotic progression occurred and patient was moved to the Surgical Clinic until she died 3 months later.\nAt the time of admission to the Surgical Clinic, the patient was septic, in the state of borderline reversible septic shock, with marked high C-reactive protein (CRP), expressed heavy necrotic changes of soft tissue of right lower abdomen and perineal regions, complete destruction of labia majoris, and propagation of the process in perianal region and right gluteal region (Figures 1 and 2).\nImmediately, urgent surgery and extensive necrectomy of bottom of abdominal wall and perineal region were accomplished (Figure 3).\nAbundant decontaminations of wounds with the 3 % hydrogen peroxide and physiological solution were done. The defect was left opened. On places without the visible skin necrosis, incisions were done and drainage was introduced. Because of the general condition, the patient was placed in the Intensive Care Unit (ICU). During the operation, targeted samples were taken for microbiological examination. The combined therapy of daily doses of Tazocin 4 x 2,25 g with Diflucan a 100 mg was continued.\nThe following day, because of the doubt of streptococcal sepsis, the antibiotic therapy was changed, and in the agreement with the microbiologist, Clindamycin 3 x 900 mg, Penicilin G 4 x 5.000.000 U, and Mirocef 2x1g were introduced. Wound was daily writhing, beside the occasional necrectomy.\nThe seventh day after the surgery came to the more abundant bleeding from the wound area with the pressure loss, and deterioration of the general condition of the patient. The urgent wound revision was approached, and during the surgery the multiple bleedings in wound were found and the hemostasis was done. Postoperatively, patient remained on the respirator. Everyday wound writhing was continued, parallel with taking the material for micro- biological analysis, tracking laboratory parameters, especially the following ones: L (Figure 4), CRP (Figure 5), erythrocytes, potassium and sodium (Figure 6), urea and creatinine (Figure 7), central venous pressure (Figure 8) and diuresis (Figure 9). Antibiotic therapy was changed according to antibiograms findings, as agreed with the microbiologist (Figure 10). Patient is throughout the stay, the largest part febrile (Figure 11).\nAfter twenty days of staying in ICU, because of the development of acute renal insufficiency, hemofiltration was accomplished beside patient's bed during 4 days, until to the establishment of diuresis, and depreciation of urea, creatinine and potassium. Because of the impossibility of detachment from the respirator and adequate toilet of respiratory tracts, the 37th day of hospitalization tracheostomy was performed.\nAfter the obvious improvement, which appeared 6 weeks after the admittance, that included hemodynamic stability, the minimal support to the respiration, satisfactory diuresis and intestinal passage, as well as satisfactory local wound aspect, patient became continu- ously high feverish, that proved\nThe first clear definition of necrotizing infections of soft tissues was given by Joseph Jones in 1871, as hospital gangrene(13). The term necrotizing fasciitis was mentioned for the first time by Wilson in 1952 (14). Today's division is based upon tracking of more factors which in detail characterize the infection of soft tissue, determine the way of medical treatment and somehow predict the outcome. These predisposing factors include the period of incubation, etiologic causes, systemic toxicity, clinical course of infection, peculiarities of lesion and degree of tissue affection, respectively of gram stained tissues preparation, and the course of medical treatment (15). In this way we distinguish: necrotizing fasciitis, anaerobic streptococcal myositis, progressive bacterial synergistic gangrene, polymicrobical synergistic necrotizing cellulitis, nonclostridial or clostridial infections, and Fournier's gangrene (6,15).\nNecrotizing fasciitis is the acute surgical condition. Early recognizing and surgical debridement of necrotic tissue are exceptionally important, and are significantly influencing upon the course of the treatment and patient's survival (16,17,18,19). In spite of the aimed and on time initiated therapies, the outcome of necrotizing fasciitis is always doubtful. Factors which determine the bad outcome of medical treatments are: the delayed incision or the one, 30 hrs. after the development of symptoms, the inadequate surgical treatment (insufficient debridement or the inadequate repetition of debridement), localization on the body, age higher than 65 yrs., male gender, present endogenous diseases, index of body weight higher than 40, infection caused with\nDuring the initial examination, very frequently necrotic fasciitis is not recognized, even in 85% of cases (3), as the clinical signs are “hidden” with abscess or cellulitis (nonecrotic infection of soft tissues). Furthermore, in clinical signs only the light shape of cellulitis can dominate, while at the same time, necrotizing fasciitis can rapidly progress through the fascia and affects the muscle (19). In such patients the exact diagnoses can be obtained, only when the infection has rapidly progressed alongside the migration of edge of edemas, and skin induration in spite of intravenous application of antibiotics of wide spectrum.\nThe clinical signs are usually of acute course and develop in a couple of days. There are usually trials of the symptoms: pain, swelling and raised temperature. Sensitivity of skin, erythema, and locally the raised temperature of skin are frequently the only symptoms of an early phase of necrotizing fasciitis. Peculiarities of the developed clinical pictures are the system intoxication followed with fever, hemorrhagic skin bullas, celullitis in 90% cases, edema in 80%, necrosis skins, fluctuation, crepitations, sensory and motor deficit (6).\nPathophysiological process grips the skin, the subcutaneous tissue, fascia and muscles and has been characterized with angiothrombotic microbial invasion and liquefaction necrosis. Histologically the following is seen: necrosis of the superficial muscle fascia, infiltration of polymorfonuclears in deep dermis and fascia, thrombosis and suppuration veins and arteries which traverses fascia, and proliferations of microorganisms inside the destroyed tissue (5,22).\nFrom microbiological point of view, it is a heavy polymicrobical infection caused by mixed aerobic and anaerobic microflora. Etiological causes are dominantly aerobic and anaerobic bacteria but can be system of fungi too. Dominant species are:\nMedical treatment of necrotizing fasciitis is complex and combined one. On the front burner is aggressive surgical debridement of the complete necrotic tissue, as well as the initial administration of broad spectrum antibiotics. Revision of wounds must be done inside 24 - 48 hrs because of the evaluation for the additional debridement. In therapeutic procedures, time is highly important factor for the prognosis and outcome of medical treatments. The optimal time interval for the first debridement is inside 6 hrs according to the appearance of clinical symptoms. Moreover, supporting procedures such as hyperbaric oxygenation and NPWT can be applied too (4,32,39,40).\nBefore the development of clinical signs of necrotizing infections of soft tissues, probably sore spots in the area of bottom of abdomen have preceded. Predisposing factors to the development infections are: diabetes mellitus, age, and body index mass higher than 40. The development of infections was gradual with the development of celulitis, edemas and fever, the same was with the development of intoxication with the resultant development of sepses. Etiologic originators were aerobic and anaerobic bacteria\nIn the presented case, there have been a few failures in the course of medical treatments. The first diagnoses were celullitis and abdominal wall phlegmona. During the first examination surgeon, and later on dermatologist, respectively internists did not take in the consideration the differential diagnostic entity of necrotizing fasciitis. Initially, no incisions were done, and in the later phase when bullosis of skin developed the inadequate incisions and drainages were done. The adequate surgical treatment - the extensive necroctomy (when patient was admitted to Surgical Clinic) was done too late, still 2 weeks after the beginning of clinical signs development. At that moment, the patient was already significantly septic and in serious physical condition. Unfortunately, because of the impossibility, the auxiliary measures such as NPWT or hyperbaric chamber were not applied.\nBecause of comorbidity factors (extreme obesity, diabetes), and late proper diagnosis\nof necrotizing fasciitis (the clinical signs were “hidden” by celullitis, and phlegmona of abdominal wall), and then as the consequence, overdue adequate surgical treatment, unfortunately contributed to medical treatment failure, respectively lethal outcome. This of course don't means that the patient would survived, since the mortality under such circumstances brings out and to 80 %, but without this “failures” chances for surviving would be greater."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:089830b7-cf59-4406-b7d4-ba9aae0dbae8>","<urn:uuid:a14d8dbb-e808-451d-bb8b-b0514c4bda95>"],"error":null}
{"question":"How did the Keta (Chum) salmon get its name, and what distinguishing features does it have?","answer":"Keta salmon, also known as Chum salmon, got its name from the pidgin word 'tzum' meaning 'striped,' which was used in communications between early white traders and Pacific Northwest natives. The fish develops distinctive striped scales as it approaches its spawning grounds, with the scales darkening and often taking on dark green or purple hues.","context":["Our salmon season runs from mid-May through September. Of the five species of wild salmon that return to our fishing grounds each year, we predominately catch and market three species. We begin our season at the mouth of the Copper River targeting sockeye salmon. In June, we transition out to Prince William Sound for the keta salmon run. We finish our season back at the Copper, catching late season coho salmon. When you buy a Copper Valley Fish Collective wild Alaskan salmon share, you’re buying a share of our season’s catch. Every year is a little different, and your share will reflect this natural rhythm. This is truly a wild salmon run! We strive to include at least two different species of salmon in every share so that you can compare and contrast their flavor and texture.\nThe jury remains out, even in this household, as to which salmon is “best.” Each species has its own distinctive attributes, so you’ll have to decide for yourself! We often debate the merits of each species, so here are a few of our thoughts to help guide your own salmon exploration.\nSockeye Salmon (a.k.a. Red Salmon)\nScientific name: Oncorhynchus nerka\nThe sockeye of the Copper River are traditionally the first to return to Alaska each year, which marks the start of the commercial salmon season. On opening day, these prized fish get a first-class airlift to restaurants across the United States. From fishmongers to gourmet chefs, “Copper River reds” are celebrities of the salmon world.\nAs adults they feed on crustaceans, which gives their flesh a brilliant (surprise!) red color. Sockeye is undeniably the most firm of the salmons, and will retain its characteristic color even when cooked. Because of these features we love to have it grilled straight-up, and it also makes for a sharp-looking bowl of poke.\nKeta Salmon (a.k.a. Chum Salmon, Dog Salmon)\nScientific name: Oncorhynchus keta\nThe name chum is derived from the pidgin word tzum (meaning “striped”), used between early white traders and natives of the Pacific Northwest. A chum’s scales grow increasingly striped as they draw near to their spawning grounds, darkening and often taking on dark green or even purple hues.\nThis medium-sized Alaskan salmon is probably the most underrated for eating. A fine fish to fillet (and a staple meal on our boat in the summer!), it is also represented on most sushi menus: your salmon roe ikura was most likely harvested from a chum. The meat itself is less oily when compared to the other salmons, and should be cooked accordingly. Some like to use a savory marinade to maintain moisture, others prefer lightly cooked steaks.\nCoho Salmon (a.k.a. Silver Salmon)\nScientific name: Oncorhynchus kisutch\nCoho salmon are the largest fish we offer, and thus the thickest cuts you may find in your share. These wild fighters are the reason you’ll see rod-and-reel fisherman lining the banks of Copper River sloughs in September. On hook or in the net, their metallic silver-streaked tails are easy to spot.\nTheir fillets are hefty compared to sockeye or chum, making them well-suited to our favorite cut for the pan, the butterfly. We tend to use these versatile rose-colored portions in curries, chowders, and fish tacos. And as always, a little salt, pepper, and oil on these babies makes for an impressive meal.\nSummer: P.O. Box 1501\nCordova, AK 99574\nWinter: 2420 E. 16th Avenue\nAnchorage, AK 99508"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:8bfe73c2-09fd-4436-98fc-b7530db69336>"],"error":null}
{"question":"What are the genetic causes of Down syndrome, and how does it increase the risk of developing Alzheimer's disease?","answer":"Down syndrome is caused by genetic abnormalities involving chromosome 21, with the most common being trisomy 21 where there is an extra copy of chromosome 21, making the total chromosome count 47 instead of 46. This can occur due to nondisjunction during cell division or through translocation of chromosome 21 material. Regarding Alzheimer's disease, adults with Down syndrome have a 70-80% chance of developing clinical dementia by their 60s. This increased risk is linked to having three copies of the Amyloid-beta Precursor Protein gene on chromosome 21, which leads to a 1.5-fold increase in amyloid-beta protein. However, amyloid deposition alone doesn't immediately cause dementia, as individuals can harbor this pathology for over a decade before cognitive decline becomes apparent.","context":["Down syndrome disease is a disorder arising because of genetic abnormality and it leads to physical as well as mental disabilities of various kinds. Down syndrome occurs due to the presence of an extra chromosome, chromosome 21. This condition is also known as trisomy of chromosome 21. Down syndrome disease is one of the leading cases of genetic disorder worldwide. It is also a very prominent and very well-known disease.\nNamed after the physician Langdon Down, who first observed this disease, down syndrome disease affects not only physical characteristics but mental characteristics as well. Mental retardness is one of the effects of the disease. Along with all these they are also susceptible to other disease conditions like leukaemia (a form of blood cancer) and Alzheimer’s. Unfortunately, there is no cure for down syndrome disease. But the quality of life can be made better by taking extra care and providing training to the individuals to perform better in daily essentials. Noteworthy is that down syndrome can be diagnosed during early screens in pregnancy stages and this can be helpful in decreasing the occurrence of disease.\nReasons for Down Syndrome\nFrom the above information, it is clear that to define down syndrome the abnormality of the genetic disorder has to be mentioned. So, when asked, is down syndrome genetic, the answer is yes since the disease is caused by the presence of an extra copy of chromosome 21. Thus, it is a chromosomal disorder.\nWhen asked what causes down syndrome one of the most simple reasons for down syndrome is the chromosomal disorder due to the aneuploidy of the autosome. The rate at which the disease is prevalent is 1 in 1800 live births. It is found to be more occurring in cases when the age of the mother has exceeded 35.\nDown syndrome disease is caused due to abnormal cell division. Usually, during the mitotic and meiotic cell division, there is a duplication of the chromosomes and the chromosome pairs are separated so that each daughter cell can get a copy of each chromosome. Sometimes, when they are not able to separate properly, this gives rise to unequal distribution of the chromosomes in each daughter cell. This condition known as nondisjunction gives rise to down syndrome disease. The nondisjunction occurs during the anaphase before all of the replicated chromosomes’ centre points or kinetochores, which are attached to the microtubules originating from opposite ends of the cells, are segregated. Due to this condition, abnormal numbers of chromosomes appear in the daughter cells with one cell receiving one chromosome 21 while the other receiving three chromosomes 21.\nMany times there is the translocation of a part of chromosome 21 to another part of another chromosome. This also leads to down syndrome as even though the number of chromosomes is normal, there is an extra portion of chromosome 21 present in the cells.\nThus, the down syndrome disease can be characterized not by the absence of any genetic information but the presence of an abnormally extra genetic material from chromosome 21. This causes abnormalities in the physical and mental development of the person. The diagram of the genetic abnormalities of the down syndrome disease is given below:\n[Image will be Uploaded Soon]\nAs it is clear that down syndrome is genetic, it is also non-inheritable in most cases. In cases where the abnormality is caused due to a problem in genetic translocation rather than an entirely new chromosome being present, down syndrome is inheritable. In such cases, the signs and symptoms of down syndrome may be absent in the parent but can be passed on to the child. The child can go on to live for another 60 years. But this also depends on the number of other health complications present.\nTypes of Down Syndrome\nDown syndrome disease can be classified into three types. All the three types explained above as part of the theory are distinctly given below in points:\nTrisomy of Chromosome 21:\nThis is the general case when there is an extra copy of chromosome 21 present in the cells. This makes the total number of chromosomes from 46, the normal case, to 47, the abnormal one. This as explained earlier happens because of the nondisjunction of chromosome 21 during the anaphase segregation of chromosomes. Because of this down syndrome is also known as Trisomy or Trisomy 21.\nThis accounts for one percent of the total cases of down syndrome disease. Usually, this case arises when the nondisjunction's happen during the mitotic division of the zygote into blastomeres. It causes some cells to have a normal number of chromosome 21 while some cells have an abnormal number of chromosome 21.\nTranslocation Down Syndrome:\nAs said, this condition arises when a part of chromosome 21 is translocated to another chromosome (usually chromosome 14). It does not end in an extra chromosome 21 copy but some part of the long arm of chromosome 21 as part of chromosome 14. Nevertheless, it can cause down syndrome disease when expressed in a dominant form. The translocation of the down syndrome doesn't have any correlation with the mother’s age.\nSigns and Symptoms of Down Syndrome\nThere are various physical and mental abnormalities in a person suffering from down syndrome. One example is those that have poor immunity and hence are subjected to many diseases. Their development takes place at an age later than normal. Many of the patients are born with congenital heart defects, thyroid disease, sleep arena, and gastrointestinal defects. They are also susceptible to diseases like leukaemia and Alzheimer’s more than normal people.\nThe physical and facial features are the easiest signs of down syndrome. After the child is born, the signs and symptoms of down syndrome start appearing. At first, the infants may be of normal size but when they grow slowly, their height grows much less than those of the same age. The prominent signs and symptoms of down syndrome are:\nShort height and undersized growth\nSlanty eyes and folding in the skin above the eye\nFlattened nose and furrowed tongue\nBroad and short hand\nPoor toning of the muscle with excessive flexibility\nShort neck, small head and abnormal teeth\nLonger time in developing linguistic abilities\nMild or moderate cognitive impairment","Individuals with Down syndrome (DS) have been largely neglected in therapeutic and biomarker studies of Alzheimer's disease (AD). Adults with DS are uniformly affected by AD pathology by their 30's and have a 70-80% chance of clinical dementia by their 60's. In 95% of cases, DS is associated with three copies of chromosome 21, each containing of copy of the Amyloid-beta (A?) Precursor Protein gene (leading to a 1.5-fold increase in A? protein). Yet, nowhere is it clearer than in DS that A? deposition is not sufficiet to produce dementia, as individuals harbor this pathology for over a decade before cognitive decline is apparent. DS can be seen as a setting of amplified sensitivity to risk and protective factors that moderate the relationship between A?, neurodegeneration and clinical dementia. Understanding the factors that moderate this relationship in DS and biomarkers for those factors is critically important in the design of therapeutic trials for AD in DS and in general. Thus, this longitudinal study of Neurodegeneration in Aging DS (NiAD) and its relationship to cognition has the potential to: 1) identify critical factors that link A? deposition to neurodegeneration and, ultimately, dementia; 2) define biomarkers for these factors; and, most importantly, 3) set a foundation for an efficient transition from this biomarker study to a therapeutic trial to combat A in DS augmented by biomarker outcomes. For the past 5 years, the three independent research groups included in this application have been studying the course of A? deposition and other imaging biomarkers and their impact on cognitive/functional measures in adults with DS [(a) the combined Pittsburgh/Madison study; (b) the Banner Alzheimer's Institute study; and (c) the Cambridge study]. In their ongoing work, 140 adults with DS (including 23 with DS/AD-dementia) have undergone magnetic resonance imaging (MRI) and amyloid-positron emission tomography (PET) scans and neuropsychological/ functional assessments. These three research groups now propose to combine resources and harmonize all protocols in response to the request from NIA/NICHD to develop a large AD biomarker study in DS. This study will be further strengthened by aligning NiAD with the three largest ongoing longitudinal studies of AD biomarkers in the general population: the Alzheimer Disease Neuroimaging Initiative (ADNI), the Dominantly Inherited Alzheimer Network (DIAN) and the Alzheimer Prevention Initiative (API). All data will be made available in an open-access format using a model similar to ADNI. The established DS cohort is a significant advantage that will shorten the recruitment phase, maximize longitudinal data that can be acquired and allow for addition of new biomarkers to be compared to longitudinal clinical and imaging measures. The proposed 5-year longitudinal study will examine progression of AD related biomarkers (A?-, tau- and fluorodeoxyglucose-PET, structural and functional MRI, cerebrospinal fluid A? and tau, plasma A? and proteomics, genetics, neuropathology) and cognitive/functional measures in 180 adults with DS (>25 yrs. of age) and 40 biomarker-controls. Subjects will be re-evaluated every 15 months to assess changes in cognition/adaptive functioning and every 30 months to detect biomarker changes.\nAdults with Down syndrome (DS) are at an extremely high risk for developing Alzheimer's disease (AD), with most individuals over age 40 evidencing neurofibrillary tangles and neuritic plaques (which are thought to be associated with the eventual appearance of AD symptoms). The goal of the current application is to recruit and follow 180 adults with DS and 40 biomarker controls to enable the identification of the longitudinal progression of AD in adults with DS using clinical, cognitive, imaging and genetic and biochemical biomarkers. This data is not only necessary to deepen our understanding of the pathophysiology of AD in DS, but may also offer information that will prove useful in the design of treatment trials to slow or prevent AD in DS.\n|Hu, Ziheng; Wang, Lirong; Ma, Shifan et al. (2018) Synergism of antihypertensives and cholinesterase inhibitors in Alzheimer's disease. Alzheimers Dement (N Y) 4:542-555|\n|Handen, Benjamin L; Mazefsky, Carla A; Gabriels, Robin L et al. (2018) Risk Factors for Self-injurious Behavior in an Inpatient Psychiatric Sample of Children with Autism Spectrum Disorder: A Naturalistic Observation Study. J Autism Dev Disord 48:3678-3688|\n|Lao, Patrick J; Handen, Ben L; Betthauser, Tobey J et al. (2018) Alzheimer-Like Pattern of Hypometabolism Emerges with Elevated Amyloid-? Burden in Down Syndrome. J Alzheimers Dis 61:631-644|\n|Lao, Patrick J; Handen, Ben L; Betthauser, Tobey J et al. (2018) Imaging neurodegeneration in Down syndrome: brain templates for amyloid burden and tissue segmentation. Brain Imaging Behav :|\n|Cohen, Ann D; McDade, Eric; Christian, Brad et al. (2018) Early striatal amyloid deposition distinguishes Down syndrome and autosomal dominant Alzheimer's disease from late-onset amyloid deposition. Alzheimers Dement 14:743-750|\n|Hartley, Sigan L; Handen, Benjamin L; Devenny, Darlynne et al. (2017) Cognitive decline and brain amyloid-? accumulation across 3 years in adults with Down syndrome. Neurobiol Aging 58:68-76|\n|Mihaila, Iulia; Hartley, Sigan L; Handen, Benjamin L et al. (2017) Leisure Activity and Caregiver Involvement in Middle-Aged and Older Adults With Down Syndrome. Intellect Dev Disabil 55:97-109|\n|Lao, Patrick J; Handen, Ben L; Betthauser, Tobey J et al. (2017) Longitudinal changes in amyloid positron emission tomography and volumetric magnetic resonance imaging in the nondemented Down syndrome population. Alzheimers Dement (Amst) 9:1-9|\n|Lao, Patrick J; Betthauser, Tobey J; Hillmer, Ansel T et al. (2016) The effects of normal aging on amyloid-? deposition in nondemented adults with Down syndrome as imaged by carbon 11-labeled Pittsburgh compound B. Alzheimers Dement 12:380-90|\n|Hartley, Sigan L; Handen, Benjamin L; Devenny, Darlynne A et al. (2014) Cognitive functioning in relation to brain amyloid-? in healthy adults with Down syndrome. Brain 137:2556-63|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:8758d64b-170a-4e95-8e02-58a74d14d190>","<urn:uuid:50a2013c-e96b-4b56-a8f9-3b95f71d35cd>"],"error":null}
{"question":"What do nuclear research at the Cavendish Laboratory and solar eclipse observation have in common in terms of revealing hidden layers of nature?","answer":"Both activities reveal normally invisible layers of nature: During a solar eclipse, as Sofia explains, observers can see different layers of the sun's atmosphere - the photosphere (yellow visible layer), the chromosphere (red cooler layer), and the corona (diffuse streaming layer), which are otherwise impossible to see with the naked eye. Similarly, at the Cavendish Laboratory, Rutherford's experiments, particularly his gold foil experiment in 1911, revealed hidden layers of atomic structure, leading to his atomic model which showed that atoms have a concentrated central nucleus rather than a uniform mass distribution.","context":["Keck Dean and Astrophysicist Ulysses J. Sofia Illuminates the Solar Eclipse\nIn anticipation of the first total eclipse of the sun visible from North America in nearly 40 years, we asked our own resident astrophysicist, Keck Science Department Dean Ulysses J. Sofia, for a couple of reflections on the moment when the moon passes between the earth and the sun, fleetingly changing the world as we know it. An expert on solar variation and the interstellar medium, Sofia will travel to Oregon to watch—safely, with proper eye wear and such—the sunless sky. Although he has seen a total solar eclipse twice before, he says the celestial event on August 21 is absolutely worth a 900-mile trek.\n“Anybody who has a chance to see a solar eclipse should do it,” Sofia said. “It is really phenomenal, something you can’t describe.”\nBut if you’re an astrophysicist, one who has worked for NASA and who started crunching satellite data in high school, you might still have a few thoughts to share about the upcoming uncommon alignment of heavenly bodies.\nQ: Why are solar eclipses such a big deal?\nSofia: Solar eclipses are a lot rarer for people to see than lunar eclipses because they have a very narrow path on the earth. During a lunar eclipse, the earth’s shadow is on the moon, so anyone on earth who can see the moon—which is half the earth—can see that happening. In contrast, the stripe across the earth of the moonshadow is very small.\nQ: The sun’s diameter is 400 times bigger than the moon’s—how can the moon block out the sun completely?\nSofia: The sun is 400 times farther away from the earth than the moon is. This coincidence of being 400 times bigger and 400 times farther away makes the moon and the sun almost exactly the same angular sizes (how big they look to us in the sky). It is one of the most random things in the world that they are almost precisely the same angular size. So close, in fact, that the small non-circularity of the earth’s and moon’s orbits means that the moon isn’t even always big enough to cover the entire surface of the sun.\nQ: During an eclipse, is there anything to see besides not seeing the sun?\nSofia: You can see different layers of atmosphere of the sun as the moon covers the sun itself. The yellow part we always see is called the photosphere; as the moon blocks that, you may be able to see the chromosphere, a red, cooler layer of the sun. Then, as that gets covered up, you see the corona, which is this really diffuse, fluffy-looking layer that streams away from the sun. All stars have these layers, but a solar eclipse is the only way you can actually see them with the “naked eye.”\nQ: What’s the moment of total eclipse like?\nSofia: There’s an eeriness to the light as the moon covers the sun. Everything gets very blue. Usually birds stop chirping. It’s beautiful.\nQ: Do you have one piece of advice for Monday’s eclipse?\nSofia: Protect your eyes and put your camera down. Experience it, live it rather than try to preserve it.\n–posted August 17, 2017","Ernest Rutherford, 1st Baron Rutherford of Nelson (born August 30, 1871 in Spring Grove near Nelson , New Zealand , † October 19, 1937 in Cambridge , United Kingdom ) was a British physicist who received the 1908 Nobel Prize in Chemistry . Rutherford is considered one of the most important experimental physicists .\nIn 1897, Rutherford recognized that the ionizing radiation from uranium consists of several types of particles. In 1902 he hypothesized that chemical elements change into elements with a lower atomic number through radioactive decay. In 1903 he differentiated radioactivity into alpha radiation , beta radiation and gamma radiation according to their increasing penetration capacity and introduced the term half-life . This work was awarded the Nobel Prize in Chemistry in 1908 .\nHis best-known contribution to atomic physics is Rutherford's atomic model , which he derived from his experiments in scattering alpha particles on gold foil in 1911 . Rutherford expanded the atomic model of Thomson , who had assumed a uniform mass distribution.\nRutherford demonstrated experimentally for the first time in 1917 that irradiation with alpha particles can convert one atomic nucleus (in his case nitrogen ) into another (in his case into the next heavier element oxygen ). During these experiments he discovered the proton . Under his guidance, John Cockcroft and Ernest Walton \"smashed\" an atomic nucleus with artificially accelerated particles; Lithium bombarded with protons was converted into two alpha particles, i.e. helium nuclei. Another scientist in Cambridge, James Chadwick , succeeded in 1932 in experimentally demonstrating the neutron , which Rutherford had theoretically postulated years earlier.\nLive and act\nOrigin and education\nErnest Rutherford was the fourth of twelve children of James Rutherford (1838-1928) and his wife Martha Thompson (around 1843-1935). His parents immigrated to New Zealand as a child . From Spring Grove , the family moved to Foxhill in 1876 . From March 1877, Rutherford attended the Primary School led by Henry Ladley there . In 1883 the family moved on to Havelock , where their father ran a flax mill he built on the Ruakaka River . For economic reasons, the family had to move again five years later, this time to Pungarehu on New Zealand's North Island . Supported by a grant from the Marlborough Education Board, Rutherford attended Nelson College from 1887 to 1889 . There he played in the rugby team and was head boy in 1889. His interest in mathematics and science was encouraged by his teacher William Still Littlejohn (1859-1933).\nFrom February 1890, Rutherford studied at Canterbury College in Christchurch . There, the professor of mathematics and natural philosophy Charles Henry Herbert Cook (1843-1910) promoted Rutherford's mathematical talent, while the professor of chemistry Alexander William Bickerton (1842-1929), who also taught physics, aroused Rutherford's interest in physics. In 1892 Rutherford passed the exams for the Bachelor of Arts , in 1893 he earned a Master of Arts degree and a year later a Bachelor of Science degree . Rutherford's first research dealt with the influence of high frequency Hertzian waves on the magnetic properties of iron and was published in the Transactions of the New Zealand Institute .\nDuring this time, Rutherford lived in the house of the widowed Mary Kate De Renzy Newton, a secretary for the Woman's Christian Temperance Union . There he met her daughter, his future wife Mary \"May\" Georgina Newton (1876–1945).\nRutherford applied in 1894 for the New Zealand place for an \" 1851 Exhibition Scholarship \", a scholarship funded from the surpluses of the Great Exhibition of 1851 in London. He was defeated with his application to the chemist James Scott Maclaurin (1864-1939) from Auckland University College . When Maclaurin did not accept the £ 150 scholarship intended for a study visit to Great Britain, Rutherford was awarded it as the second applicant.\nOn August 1, 1895, Rutherford left New Zealand on a steamship from Wellington . During a stopover, he demonstrated his Hertzian wave detector to William Henry Bragg at the University of Adelaide and received a letter of recommendation from Bragg. In October 1895 Rutherford began his work at the Cavendish Laboratory of the University of Cambridge , directed by Joseph John Thomson . Initially, he was concerned with improving the sensitivity of his detector, which he could soon use to detect radio waves at a distance of about half a mile. Thomson, who quickly recognized Rutherford's experimental talent, invited Rutherford at the beginning of the Easter semester in 1896 to help him investigate the electrical conductivity of gases . They used the X-rays discovered a few months earlier to trigger conductivity in the gases. Rutherford developed the experimental techniques to measure the rate of recombination and the velocities of the ions formed under the action of the X-rays . In the following years Rutherford continued these experiments using ultraviolet radiation .\nAfter two years in Cambridge, Rutherford received his \"BA Research Degree\" in 1897. Through Thomson's advocacy, he was awarded the 250 pound per year Coutts Trotter Fellowship of Trinity College in 1898 , which enabled Rutherford to spend another year at Cambridge.\nProfessor in Montreal, Manchester and Cambridge\nIn 1898 he received a call to the McGill University in Montreal ( Canada ), where he worked until 1907. For the research he carried out during this time, he received the Nobel Prize in Chemistry in 1908 .\nIn 1919 he went to Cambridge as a professor , where he was director of the Cavendish Laboratory . 1921 his work appeared Nuclear Constitution of Atoms (dt .: About the core structure of atoms) . From 1925 to 1930 he was President of the Royal Society .\nAwards and recognition\nRutherford is one of the world's most honored scientists. The British crown ennobled him in 1914 as a Knight Bachelor , accepted him in 1925 in the Order of Merit and raised him to hereditary peer in 1931 as Baron Rutherford of Nelson , of Cambridge in the County of Cambridge .\nA 1,906 newly discovered and Willy Marckwald described uranyl -mineral received his honor the name rutherfordine . In addition to the Nobel Prize in Chemistry, which was awarded to him in 1908, Rutherford received numerous scientific and academic prizes and honors. The Royal Society awarded him the Rumford Medal in 1904 and honored Rutherford in 1922 with their highest honor, the Copley Medal in gold. The Accademia delle Scienze di Torino honored him in 1908 with the award of the Bressa Prize (Premio Bressa). The Columbia University gave Rutherford every five years awarded Barnard Medal for the year 1910th\nIn 1906 he was elected to the Academy of Sciences in Göttingen , in 1911 to the National Academy of Sciences and in 1915 to the American Academy of Arts and Sciences . He received the Franklin Medal of the Franklin Institute in Philadelphia in 1924 , the Albert Medal of the Royal Society of Arts in 1928 and the Faraday Medal of the Institution of Electrical Engineers in 1930 . In 1921 he was elected an Honorary Fellow of the Royal Society of Edinburgh . In 1922 he became a corresponding and in 1925 honorary member of the Academy of Sciences of the USSR . In 1932 he was made an honorary member of the German Academy of Natural Scientists Leopoldina .\nIn mid-1946, Edward Condon and Leon Francis Curtiss (1895-1983) proposed by the US National Bureau of Standards to introduce a new physical unit Rutherford with the unit symbol rd for measuring activity , but this did not prevail.\nIn the 2005 New Zealand's Top 100 History Makers series , Rutherford was voted the most influential New Zealander in history. A memorial was built in his place of birth and his former primary school in Foxhill is keeping his memory alive with the Rutherford Memorial Hall.\nA Mars crater was named after him in 1973 and a moon crater in 1976. Rutherford Ridge in Antarctica has been named after him since 2008, and an asteroid has been named after him since 2021: (5311) Rutherford .\nOriginal English editions\n- Radio activity . 1st edition, At the University Press, Cambridge 1904 ( online ); 2nd edition, 1905 ( online ).\n- Radioactive Transformations . Archibald Constable & Co., London 1906 ( online ).\n- Radioactive Substances and Their Radiations . At the University Press, Cambridge 1913 ( online ).\n- Radiations From Radioactive Substances . University Press, Cambridge 1930 (with James Chadwick and Charles Drummond Ellis).\n- Artificial Transmutation of the Elements. Being the Thirty-fifth Robert Boyle Lecture . (= Robert Boyle Lecture, Volume 35), H. Milford, Oxford University Press 1933\n- The Newer Alchemy . University Press, Cambridge 1937.\n- The radioactivity . Authorized edition supplemented with the assistance of the author by Emil Aschkinass, Julius Springer, Berlin 1907 ( online ).\n- Radioactive conversions . Translated by Max Levin, Friedrich Vieweg and Son, Braunschweig 1907 ( online ).\n- Radioactive substances and their radiation . Translated by Erich Marx, Akademische Verlagsgesellschaft, Leipzig 1913.\n- About the core structure of the atoms. Baker lecture . Authorized translation by Else Norst, Hirzel, Leipzig 1921.\n- Uranium Radiation and the Electrical Conduction Produced by It . In: Philosophical Magazine . 5th episode, volume 47, number 284, 1899, pp. 109-163 ( doi: 10.1080 / 14786449908621245 ).\n- A Radio-active Substance emitted from Thorium Compounds . In: Philosophical Magazine . 5th episode, volume 49, number 296, 1900, pp. 1-14 ( doi: 10.1080 / 14786440009463821 ).\n- Radioactivity produced in Substances by the Action of Thorium Compounds . In: Philosophical Magazine . 5th episode, volume 49, number 297, 1900, pp. 161-192 ( doi: 10.1080 / 14786440009463832 ).\n- Comparison of the Radiations from Radioactive Substances . In: Philosophical Magazine . 6th episode, volume 4, number 19, 1902, pp. 1-23 (with Harriet T. Brooks ; doi: 10.1080 / 14786440209462814 ).\n- The Cause and Nature of Radioactivity. - Part I . In: Philosophical Magazine . 6th episode, volume 4, number 21, 1902, pp. 370-396 (with Frederick Soddy ; doi: 10.1080 / 14786440209462856 ).\n- The Cause and Nature of Radioactivity. - Part II . In: Philosophical Magazine . 6th episode, volume 4, number 21, 1902, pp. 569-585 (with Frederick Soddy; doi: 10.1080 / 14786440209462881 ).\n- The Magnetic and Electric Deviation of the Easily Absorbed Rays from Radium . In: Philosophical Magazine . 6th episode, volume 5, number 25, 1903, pp. 177-187 ( doi: 10.1080 / 14786440309462912 ).\n- A Comparative Study of the Radioactivity of Radium and Thorium . In: Philosophical Magazine . 6th episode, volume 5, number 28, 1903, pp. 445-457 (with Frederick Soddy; doi: 10.1080 / 14786440309462943 ).\n- Condensation of the Radioactive Emanations . In: Philosophical Magazine . 6th episode, volume 5, number 29, 1903, pp. 561-576 (with Frederick; doi: 10.1080 / 14786440309462959 ).\n- Bakerian Lecture. Nuclear Constitution of Atoms. In : Proceedings of the Royal Society of London / A. 97, number 686, 1920, pp. 374-400 ( doi: 10.1098 / rspa.1920.0040 ).\n- Edward Andrade: Rutherford and the Nature of the Atom . (= Science Study Series . Number 29). Heinemann, 1964.\n- Edward Andrade: Rutherford and the Atom. The beginning of the new physics . Translated from the American into German by Klaus Prost, Desch, Munich 1965.\n- Lawrence Badash (Ed.): Rutherford and Boltwood. Letters on radioactivity . Yale University Press, New Haven 1969.\n- Lawrence Badash: Rutherford Correspondence Catalog . American Institute of Physics, New York 1974.\n- John Campbell: Rutherford. Scientist Supreme . AAS Publications, Christchurch 1999, ISBN 0-473-05700-X .\n- John Campbell: Rutherford's Ancestors . AAS Publications, Christchurch 1996, ISBN 0-473-03858-7 .\n- James Chadwick (Ed.): The Collected Papers of Lord Rutherford of Nelson . 3 volumes, George Allen and Unwin, London 1962–1965.\n- Arthur Eve: Rutherford . Cambridge University Press, Cambridge 1939.\n- Mark Oliphant: Rutherford. Recollections of the Cambridge Days . Elsevier, Amsterdam 1972, ISBN 0-444-40968-8 .\n- Richard Reeves: Force of Nature: The Frontier Genius of Ernest Rutherford . WW Norton & Company, 2008, ISBN 978-0-393-33369-5 .\n- David Wilson: Rutherford. Simple genius . MIT Press, Cambridge 1983, ISBN 0-262-23115-8 .\n- Literature by and about Ernest Rutherford in the catalog of the German National Library\n- Newspaper article about Ernest Rutherford in the 20th century press kit of the ZBW - Leibniz Information Center for Economics .\n- Information from the Nobel Foundation on the 1908 award ceremony for Ernest Rutherford\n- Entry about Lord Ernest Rutherford of Nelson in the database of the Wilhelm Exner Medal Foundation .\n- Short biography at the Public Broadcasting Service\n- Brian Sweeney, Jacqueline Owens: Ernest Rutherford: Atom Man\n- Nobel Prize for Chemistry 1908, original lecture\n- Thaddeus J. Trenn: Rutherford on the Alpha-Beta-Gamma Classification of Radioactive Rays. Isis Vol. 67 (1976) p. 61ff doi.org/10.1086/351545\n- John Campbell: Rutherford's Ancestors . 1996, p. 12.\n- John Campbell: Rutherford's Ancestors . 1996, p. 20.\n- John Campbell: Rutherford's Ancestors . 1996, p. 39.\n- WJ Gardner: Cook, Charles Henry Herbert . In: Dictionary of New Zealand Biography , accessed March 4, 2013.\n- HN Parton: Bickerton, Alexander William . In: Dictionary of New Zealand Biography , accessed March 4, 2013.\n- Brian R. Davis: Maclaurin, James Scott . In: Dictionary of New Zealand Biography , accessed March 4, 2013.\n- John Campbell: Rutherford. Scientist Supreme . 1999, p. 192.\n- Arthur Eve: Rutherford . 1939, p. 13.\n- Sungook Hong: Wireless: From Marconi's Black Box to the Audion . MIT Press, 2001, ISBN 0-262-08298-5 , pp. 13-16.\n- John Campbell: Rutherford. Scientist Supreme . 1999, p. 246.\n- Johannes-Geert Hagmann: How physics made itself heard - American physicists engaged in \"practical\" research during the First World War . Physik Journal 14 (2015) No. 11, pp. 43–46.\n- History. cara1933.org, archived from the original on May 7, 2015 ; accessed on September 4, 2013 .\n- Rutherford, Oliphant, Paul Harteck: Transmutation effects observed with heavy hydrogen, Proc. Roy. Soc. A, Vol. 144, 1934, pp. 692-703, and under the same title, Nature, Vol. 133, 1934, p. 413\n- The discovery of DD fusion , EuroFusion, 2010\n- PP O'Shea: Ernest Rutherford. His Honors and Distinctions . In: Notes and Records of the Royal Society of London . Volume 27, number 1, 1972, p. 67 ( doi: 10.1098 / rsnr.1972.0009 ).\n- London Gazette . No. 28806, HMSO, London, February 24, 1914, p. 1546 ( PDF , accessed October 1, 2013, English).\n- London Gazette (Supplement). No. 33007, HMSO, London, January 1, 1925, p. 3 ( PDF , accessed October 1, 2013, English).\n- London Gazette (Supplement). No. 33675, HMSO, London, January 1, 1931, p. 1 ( PDF , accessed October 1, 2013, English).\n- Relazione sul XV Premio Bressa . In: Atti della Reale Accademia delle scienze di Torino . Volume 43, 1907-1908, pp. 579-586 ( online ).\n- Report of the Committee on the Bernard Medal . In: Report of the National Academy of Sciences for the Year 1910 . United States Government Printing Office , Washington 1911, pp. 14-15 ( online ).\n- Fellows Directory. Biographical Index: Former RSE Fellows 1783–2002. (PDF file) Royal Society of Edinburgh, accessed April 5, 2020 .\n- Christel Dell, Danny Weber, Thomas Wilde: The Academy Awards . Honorary membership. In: Jörg Hacker (Ed.): German Academy of Natural Scientists Leopoldina . Structure and members. German Academy of Natural Scientists Leopoldina eV, Halle (Saale) 2015, p. 353 ( leopoldina.org [PDF; accessed September 25, 2016]).\n- Edward Uhler Condon, Leon Francis Curtiss: New Units for the Measurement of Radioactivity . In: Physical Review . Volume 69, number 11-12, 1946, pp. 672-673 ( doi: 10.1103 / PhysRev.69.672 )\n- John Barry: Currency trends and developments . In: Reserve Bank of New Zealand Bulletin . Volume 57, number 4, 1994, p. 352 ( PDF ).\n- Helga Neubauer: Foxhill / Brightwater . In: The New Zealand Book . 1st edition. NZ Visitor Publications , Nelson 2003, ISBN 1-877339-00-8 , pp. 1011 f .\n- Ernest Rutherford in the Gazetteer of Planetary Nomenclature of the IAU (WGPSN) / USGS\n- Ernest Rutherford in the Gazetteer of Planetary Nomenclature of the IAU (WGPSN) / USGS\n|ALTERNATIVE NAMES||Rutherford, Ernest, 1st Baron Rutherford of Nelson; Rutherford, Sir Ernest|\n|BRIEF DESCRIPTION||New Zealand nuclear physicist, Nobel laureate in chemistry|\n|BIRTH DATE||August 30, 1871|\n|PLACE OF BIRTH||Spring Grove at Nelson|\n|DATE OF DEATH||October 19, 1937|\n|PLACE OF DEATH||Cambridge|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:b37339be-cc05-4033-88b1-47fca3c50016>","<urn:uuid:0cb6ab44-4e7e-4fc8-9399-3c27eed06dcb>"],"error":null}
{"question":"what is CRNN used for in handwriting recognition?","answer":"CRNN (Convolutional Recurrent Neural Network) combines convolutional and recurrent neural networks for handwriting recognition. The convolutional layers capture hierarchical spatial information from images, while recurrent networks capture temporal data and contextual information within sequences. This architecture processes handwritten text by first extracting features through CNN layers, then analyzing them sequentially through RNN layers, and finally converting them into text through a transcription layer.","context":["Comparing human and machine performances in transcribing 18th century handwritten Venetian script\nAutomatic transcription of handwritten texts has made important progress in the recent years (Sanchez et al., 2014; Sanchez et al., 2015, Sanchez et al., 2016). This increase in performance, essentially due to new architectures combining convolutional neural networks with recurrent neutral networks, opens new avenues for searching in large databases of archival and library records. This paper reports on our recent progress in making million digitized Venetian documents searchable, focusing on a first subset of 18th century fiscal documents from the Venetian State Archives (Condizione di Decima, Quaderni dei Trasporti, Catastici). For this study, about 23'000 image segments containing 55'000 Venetian names of persons and places were manually transcribed by archivists, trained to read such kind of handwritten script, during an annotation phase that lasted 2 years. This annotated dataset was used to train and test a deep learning architecture, with the objective of making the entire set of more than 2 million pages searchable. As described in the following paragraphs, performance levels (about 10% character error rate) are satisfactory for search use cases, which demonstrates that such kinds of approaches are viable at least for this typology of handwritten scripts. This paper compares this level of reading performance with the reading capabilities of Italian-speaking transcribers, preselected with a test based on 100 transcriptions. More than 8500 new human transcriptions were produced, confirming that the amateur transcribers were not as good as the expert. However, on average, the machine outperforms the amateur transcribers in this transcription tasks.\n2. Machine performance\nWe developed a transcription system based on the combination of convolutional and recurrent neural networks as described in (Shi et al., 2017) for handwritten text (Fig.1a) (The code is implemented in python and is available at https://github.com/solivr/tf-crnn). On the one hand, convolutional neural networks (CNN) capture hierarchical spatial information, with the first layers capturing low level features and later ones capturing high level ones. On the other hand, recurrent neural networks (RNN) capture temporal data, with the ability to grab contextual information within a sequence of arbitrary length. Convolutional recurrent neural networks (CRNN) combine the best of both worlds to handle multi-dimensional data as sequences.\nFrom an input image, the convolutional layers extract a sequence of compact representations which corresponds to the columns of the feature map. They are processed from the left to the right of the image to form a sequence of local image descriptors (Fig.1b).\nFig 1 (a) Network architecture. The architecture consists of three parts: 1) convolutional layers, which extract a feature sequence from the input image; 2) recurrent layers, which predict a label distribution for each frame; 3) transcription layer, which translates the per-frame predictions into the final label sequence. (Shi et al., 2017)\n� unable to handle picture here, no embed or link Fig 1 (b) Feature sequence (Shi et al., 2017)\nThe sequence is then input to the recurrent layers which consist of stacked bidirectional long short-term memory (LSTM) cells (Hochreiter et al., 1997). LSTM cells have the ability to capture long-range dependencies but are directional, and thus only use past contexts. Since in image-based sequences context from both directions are useful and complementary, one forward and one backward LSTM cells are combined to form bidirectional LSTMs which are then stacked to have several recurrent layers. The recurrent network outputs per-frame predictions (probabilities) that need to be converted into a label sequence.\nIn the transcription layer, the connectionist temporal classification (CTC) (Graves et al., 2006) is used in order to obtain the “sequence with the highest probability conditioned on the per-frame predictions\". The sequence label is found by taking the most probable label at each time step and mapping the separated labels to the correct sequence label (see (Graves et al., 2006) to have the detailed explanation on how the repeated and 'blanks' labels are dealt with).\nThe CRNN was trained on data coming from various types of Venetian handwritten documents. The dataset is composed of image segments of mainly names and places that have been transcribed by archivists in Venice. Image segments are used in order to reflect only the performance of the transcriber system, without introducing possible errors from the segmentation process. Thus, the segmentation step is not part of the proposed experiment. The set was randomly split into training and testing set and the content of the image segments ranges from one to several words (Tab.1).\n|Set||# images segments||# total words||size of vocabulary|\nTable 1: Datasets used\nWe show in Fig.2a and 3a how words are distributed in the dataset. We de ne the vocabulary to be the set of different words. The impact factor IF is a measure of the words' distribution in the dataset and is defined as IF (i) = c( n i) hist(i; c), with c the vector of counts of each vocabulary word, n the total number of words, hist the histogram operation and hist(i; c) the number of vocabulary words that occur i times. The left part of these plots shows that most of the words do not appear commonly but a few are very present in the dataset as it can be seen on the right of the figures (those are mainly prepositions such as `di', `de', `in', etc). The cumulative sums (Fig.2b and Fig.3b) show that common words have limited impact, but also that the system does not suffer from overfitting to the vocabulary since most of the words used for training are 'rare' in the dataset.\nFigure 2: Word distribution and impact factor in full dataset. We observe that 70% of the dataset is represented by words appearing less than 250 times (out of 54187 words)\nTo evaluate the performance of the system we use the Character Error Rate (CER) measure on the test set defined as CER = (i + s + d)=n with i, s, d, n the number of character insertions, substitutions, deletions and total characters respectively. The numerical results are shown in Tab. 2. Several experiments were performed using different sets of characters (called 'Alphabet' hereafter) and resulted in one model per Alphabet. A few randomly selected examples can be seen in Appendix A.\nOn this dataset, our transcription system is below 10% CER, which is sufficiently good to be able to search for entities in documents using regular expressions and fuzzy matching. Moreover, we believe this performance is better than the human average and in order to verify our hypothesis, we conducted an experiment described in the following section.\nFigure 3: Word distribution and impact factor in the testing dataset\n|Alphabet||Set of characters||# image segments||CER|\nTable 2: The Character Error Rate (CER) for each Alphabet\n3. Human performance\nIn order to quantify the human average error rates on our dataset, we conducted an experiment on Crowdflower's platform, where Italian speaking persons were paid to transcribe image segments of the testing set (see examples in App. A). The contributors had to decipher a few units before being able to start the survey and during the experiment some of their transcriptions were evaluated. There were 103 evaluation questions that allowed to separate low accuracy contributors' answers from reliable ones. Each image segment was transcribed at least three times, and in total 11'727 units were transcribed. Only the answers of contributors maintaining at least 60% accuracy throughout the experiment and who transcribed at least 50 units were taken into account for the analysis. This resulted in a total of 8'674 valid transcriptions to analyze. The number of transcriptions (judgments) per contributor and its location can be seen in Fig.6.\nWe compare the performance of the system and the amateur transcribers in Tab.3 and Fig.4,5 (onesample t-test, p < 0:005). It is clear from the graphs that the CRNN system has a better CER and WER than the human average on this dataset, and only a few contributors have lower or comparable performance to the system but is not yet as good as the expert. It is interesting to notice that the performance of the best amateur transcriber almost doubles when capital letters and punctuation are not considered (case 3) whereas the CRNN makes little improvement. Indeed, although the system has inferred some sort of weak language model, we have seen it producing unlikely transcriptions whereas the best contributor uses its knowledge of Italian proper nouns to deduce the correct transcription when some characters are di cult to read. Thus, the system's CER and WER could be reduced by using a lexicon-based transcription, where the output of the neural network would be compared to a dictionary and the closest element would be chosen.\n|0||: No modifications (Fig.4a)||0.0804||0.1328||-||-|\n|1||: Capital letters replaced by lowercase (Fig.4b)||0.0768||0.1137||-||-|\n|2||: All punctuation removed (Fig.4c, 5a)||0.0766||0.1241||0.2709||0.4318|\n|3||: Combination of Case 1 and Case 2 (Fig.4d, 5b)||0.0718||0.1047||0.2551||0.3507|\nTable 3: Comparison of Character Error Rates (CER) and Word Error Rates (WER) considering different formatting cases of the transcriptions for our system and the mean of the contributors (ground-truth and predictions are formatted in the same way)\nFigure 4: Character Error Rate per contributor for different cases (refer to Tab.3).\nFigure 5: Word Error Rate per contributor for different cases (refer to Tab.3).\nFigure 6: Number of judgements made (image segments transcriptions) by each contributor and its location. The contributors' ordering is the same as Fig.4a (by increasing CER)\nThe developed system shows promising results to make possible the textual search on digitized handwritten documents. These results open up new prospects for massive indexing, analyze and study of historical documents. We showed that the system had lower Character and Word Error Rates than the human average, thus being sufficiently reliable to use for searching purposes. Further work will focus on improving the architecture of the model, especially the CNN. We will also explore the possibility of lexicon- or rule-based transcription to decrease error rates.\nMore generally, it seems that the automatic transcription is currently passing a threshold in terms of performance, now giving better results than good amateur transcribers. Future research will show how far this level of performance depends on the expert initial training set or whether, after some exposition with dozens of different scripts, the automatic transcriber may be able to generalize by itself without further specific training.\n5. Appendix A : Transcription examples\n- A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber (2006) Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks, Proceedings of the 23rd international conference on Machine learning, pp. 369-376, ACM\n- S. Hochreiter and J. Schmidhuber (1997) Long short-term memory, Neural computation, vol. 9, no. 8, pp. 1735-1780, 1997.\n- J. A. Sanchez, V. Romero, A. H. Toselli, and E. Vidal (2104). Icfhr2014 competition on handwritten text recognition on transcriptorium datasets (htrts) , Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on, pp. 785-790, IEEE\n- J. A. Sanchez, A. H. Toselli, V. Romero, and E. Vidal (2015). Icdar 2015 competition htrts: Hand-written text recognition on the transcriptorium dataset, Document Analysis and Recognition (ICDAR), 2015 13th International Conference on, pp. 1166-1170, IEEE\n- J. A. Sanchez, V. Romero, A. H. Toselli, and E. Vidal, (2016). Icfhr2016 competition on handwritten text recognition on the read dataset, Frontiers in Handwriting Recognition (ICFHR), 2016 15th International Conference on, pp. 630-635, IEEE\n- B. Shi, X. Bai, and C. Yao (2017) An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition, IEEE transactions on pattern analysis and machine intelligence, vol. 39, no. 11, pp. 2298-2304"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:8d94bc71-6693-48d4-a5dd-69390c4de61f>"],"error":null}
{"question":"I'm studying best practices in academic publishing. What are the key differences between the formal peer review process for scientific research and alternative publication methods, and what are the quality control challenges each faces?","answer":"The formal scientific peer review process requires manuscripts to be evaluated by at least three anonymous expert referees who assess multiple criteria including relevance, clarity, objectivity, acknowledgment of prior work, theoretical foundation, and experimental procedures. The process helps prevent cronyism, as even Nobel Laureates' papers can be rejected if found wanting. However, it isn't foolproof - some questionable papers get published, particularly in biomedical research funded by pharmaceutical companies. Alternative publication approaches, like those used by think tanks, rely more on internal review processes. For example, think tanks use publication policies that establish standards and templates, with review by internal committees and board members. While this allows faster publication for time-sensitive topics, it may lack the rigorous external validation of scientific peer review. Both systems face challenges - scientific peer review must guard against potential conflicts of interest and ensure adequate scrutiny of industry-funded research, while alternative methods must balance speed with maintaining quality standards and institutional coherence.","context":["The Impact of a Good Publication Policy\nAfter reading Enrique’s post on quality control, I had a discussion with Dolores Arrieta, Coordinator of Communication at CIPPEC, which took us to reflect on the process CIPPEC follows to improve the quality of its production. You can also read the Spanish version.\nOne of the features of this process is that it takes part mainly in-house, but with a significant role by the members of the Board.\nFirstly, and in order to have a unified editorial criteria, CIPPEC has a publication policy that clearly defines the types of publications the think tank produces, sets internal processes that must be followed during the planning, writing and edition phases, and determines template designs for each kind of publication available (books, handbooks, guides, working papers, and policy briefs, among others).\nThrough the years, and based on experience (successes and failures), CIPPEC has built a strong and dynamic publication policy which is updated with new lessons. The importance of a publication policy is based on the belief that it will help the organisation to improve its editorial production by setting high quality and design standards on the content the think tanks publishes. The production of content is one the most important goods a think tank brings to society, and therefore it reflects the image of the institution. A publication policy also helps an organisation to enhance the necessary internal processes so that products may be ready in the least time possible with the best quality.\nCIPPEC’s publication policy rules on very different issues such as the definition and prioritisation of audiences, content and structure, format, templates and dissemination policy, colours and logos, authorial policies, the intervention of the Communication Direction and Board members, and intellectual property.\nPolicy briefs are one of CIPPEC’s most important and visible publication. Therefore, the publication policy focuses on several rules to guarantee a solid quality control process during the edition of policy briefs, a key instrument to address political actors and journalists.\nStep by step: an insight into CIPPEC’s publication process\nEach year, CIPPEC’s Communication Direction organises the think tank’s annual production during its annual planning meetings. During these encounters, each researcher lists the publications that will be produced during the year. Even though many publications can be planned, on many occasions policy briefs are written after an unplanned or an unpredictable situation occurs that offers CIPPEC the opportunity to reflect on a public policy issue. Hence, not all policy briefs can be planned in advance and therefore the publication policy is used to speed up production and correction times in these cases.\n- The process starts when a researcher tells the Communication Direction that he or she would like to write a policy brief, which could focus on a public agenda issue as well as on research findings. The Communication Direction and the researcher then define the convenience and the opportunity of disseminating the policy brief. If necessary, the decision is shared with the Executive Committee, an internal body that defines internal policies, and monitors and evaluates CIPPEC’s production in all areas. The Communication’s Director is also a member of this Committee.\n- With this approval, the researcher goes ahead and writes the policy brief, which should be sent to the Publications Coordinator in the Communication Direction, who edits it and gives feedback on the content.\n- The researcher then incorporates these corrections and feedback and sends a second version to the Publications Coordinator. In the case of books, handbooks and guides, the Communication Direction strongly suggests that the researcher goes through a peer review process.\n- The Publications Coordinator reviews this new version and begins the design process. CIPPEC’s policy briefs are completely designed in-house in Adobe Indesign, in a flexible template tailored to maximise space and present information in a dynamic and attractive way.\n- Once it has been laid-out into the template, the Communications Director sends the policy brief to a specific Committee formed by three members of the Advisory Board (the ‘Policy Brief Committee’), who are usually given 48 hours to give their opinion on the policy brief and advice on potential risks and policy impact of the document.\n- In case the dissemination of the policy brief is not urgent, this Policy Brief Committee has from three to five days to offer their comments. If none of the members provide feedback in that time, the Communications Direction can assume all is O.K. and begin the dissemination process. When feedback is received, the researcher continues to work on the piece in order to reflect such comments and advice, or writes back to clearly justify why he or she rejects such feedback. In certain occasions, when the policy brief is expected to have great impact on the media and on political actors, the Executive Director might read it and offer comments. The Executive Director always receives new policy briefs and is aware of the editorial production, although he is not expected to comment on each piece.\n- Later, a second email is send to the whole Advisory Board, to let them know a new policy brief will be published and in case they want to give feedback.\n- After 48 hours, the Communication Direction begins disseminating the policy brief, by sending it to interested political actors and journalists, and uploading it to CIPPEC’s web site and social networks.\nKey issues to consider while designing a publication policy\n- Minimise mistakes and ensure institutional coherence. While the main aim is to minimise the possibility of mistakes, another important one is to ensure institutional coherence. CIPPEC has nine different Programmes and more than 60 researchers working on different and sometimes overlapping policy areas. Even if researchers have freedom of opinion, organisational consistency is essential.\n- Plan methodically and be flexible. While speed and minimising mistakes are important, timing is crucial. Both the process described and the dissemination of the policy brief need to be planned carefully to maximize the chances that it will have the desired effect. Unpredictable events and sudden political changes can affect the process (making all people involved hasten in order to get the document ready at the right time) and therefore the publication policy should foresee the possibility of working under pressure, and guarantee mechanisms to reduce quality problems.\n- Seek experts’ feedback. The Policy Brief Committee within CIPPEC’s Advisory Board is formed by people with great experience on different areas (communications, the private sector, policymaking, etc) who provide a nuanced analysis on the production and foresee the impact a piece will have on different audiences. Board members in CIPPEC are not paid to do their work.\n- Set clear roles and responsibilities and commit with them. A process is solid when everybody knows what to do. Authors, the Communications team and the advisors all play a different roles in the publication of a piece. The publication policy is needed to establish processes, roles, responsibilities, schedules, etc. Personal commitment is also crucial: those who take on the quality control role must be really committed to doing so properly and fast.\n- Empower the body that will apply a publication policy. The role of CIPPEC’s Communication Direction –responsible for monitoring compliance to the publication policy- is very strong. The team is formed by four people with clear responsibilities, who lead the publication policy, the relationship with the media and political actors, manage the production of virtual content and help organising events, among other tasks. All of them are recognised by the staff and their suggestions and comments are taken into account.\n- Remember to define a learning process. Learning from mistakes and successes has allowed CIPPEC to design a solid but flexible policy. Although the process is well under way (it is institutionalised and respected by all authors and people involved), it is always necessary to keep on working to adjust mistakes and delays occurring on the road.\nSome challenges ahead\n- Incorporate external reviewers into process. While external reviewers are recommended for some types of publications (books, guides or handbooks) it is not expected in the case of policy briefs. This is due to the speed in which, in most of the cases, this type of publication must be ready. In the future it would be important to strengthen this process incorporating the views of the audience CIPPEC is trying to reach with the publication in order to know their thoughts on its opportunity, quality, and usefulness.\n- Have an institutional committee of continuous improvement of production’s quality. Any process can be improved. Having an institutional body which is able to analyse, evaluate results, and make recommendations for improving the publication policy would result in increasing the quality of the process described and their products.","|Triplet Arp 274 [Courtesy NASA]\n||Sistine Chapel #2 [courtesy Wikimedia]\nWhat are the standards for peer-reviewed scholarship?\nDavid H. Bailey\nUpdated 21 December 2018 (c) 2018\nCreationist and intelligent design writers insist that their writings constitute full-fledged scientific research. Creationists, for instance, hold that their notion that the earth and its living things (or even the entire universe) were created out of nothing a few thousand years ago is a scientific theory, every bit as much as evolution is a scientific theory. Similarly, leading spokespersons of the intelligent design movement have asserted that their movement is primarily a scientific movement, not a religious movement, and that \"intelligent design theory\" deserves a place in public school classrooms [Jones2005, pg. 24-35].\nAnd in the same vein, many of the \"new atheist\" writers also consider their work to be important new scholarship that deserves to be taken seriously. See Atheists.\nBut numerous scholars argue otherwise, pointing out that little of this literature, from either camp, has passed peer review. Here is some background.\nPeer review in scientific research\nFor centuries the process of peer review has been recognized as an integral part of both scientific research and the much larger world of scholarly research and communication. In the world of science today, a technical finding is not considered a bona fide scientific result unless and until it has passed peer review -- there is no such thing as non-peer-reviewed science. And many other fields of research hold similar standards -- work must be peer-reviewed before it can be taken seriously in the field in question. Along this line, note that whenever issues are \"debated\" in any other forum -- blogs, discussion forums, news columns, political campaigns, legislative bodies, television and radio, etc. -- such discussions are not to be taken seriously, particularly when the writers or speakers are not highly qualified researchers.\nIt is also keep important to keep in mind a principle popularized by Carl Sagan: extraordinary claims require extraordinary evidence [Sagan1998, pg. 60]. Thus manuscripts that make strong claims, such as that some long-standing theory is fundamentally faulty, or that a long-standing mystery has been resolved, or that long-sought experimental evidence has been found, then such manuscripts are scrutinized particularly carefully, and the authors are expected to provide exceptionally convincing reasoning and documentation.\nThe peer review process\nThe peer review system is remarkably similar across many disciplines, including all disciplines of science. It operates as follows. When a researcher (or team of researchers) completes a research project, the authors document their methodology, results and analysis in a manuscript, which is then submitted to a journal or refereed conference for peer review. The editor (if sent to a journal) or papers chair (if sent to a conference) then privately distributes the manuscript, electronically in most cases, to at least three other persons, chosen because of their knowledge and expertise in the manuscript's particular topic. In selecting referees, editors typically take pains to avoid persons who are in the same organization as one of the authors, or who otherwise might have a significant conflict of interest, professionally or financially, with the manuscript's authors. These anonymous referees rate the paper on criteria such as:\n- Relevance to the journal or conference's charter. If the topic of the manuscript is judged not appropriate for the venue to which it was submitted, it may be rejected without any further review of its contents.\n- Clarity of exposition. Most international scholarly research papers, and virtually 100% of scientific papers, are now written in English, which is the de facto universal language for research. A manuscript submitted to a journal should be written with the highest quality English prose, which often is a challenge, particularly for the growing community of international, non-native-English-speaking scientists. If the manuscript is murkily written, or if it has too many English errors, it may be rejected for this reason alone.\n- Objectivity of style. Research papers must be written in a highly objective, modest, almost self-effacing style, openly acknowledging both sides of underlying issues. If editors or referees of a submitted manuscript find any bluster, hyperbole, chest-thumping, prejudice, unprofessional criticisms of other researchers or their work, or any other indications that the authors are not approaching their material with an entirely objective mindset, the manuscript will be rejected immediately.\n- Acknowledgement of prior work. It is essential that the authors of the manuscript exhibit that they are fully familiar with the state-of-the-art research in the field, and have properly credited related work. Indeed, deficient documentation of prior work on the authors' topic is one of the most common reasons for rejection. Authors may choose to differ with the conclusions of earlier works, but if so they must explain in detail why they believe the earlier works were faulty or incomplete. Sweeping dismissals of previous research, especially in the absence of very compelling data and analysis, are usually seen as evidence that the authors are not really qualified to be addressing the issue in question.\n- Freedom from plagiarism. The usage of other researchers' text or ideas without explicit citation is considered a serious breach of ethics in all fields of research, and, in most cases, is immediate cause for rejection. In sufficiently egregious cases, the authors may be permanently banished from the journal or conference to which their manuscript was submitted. Nowadays leading journals and conferences employ sophisticated plagiarism-detecting software, which often can detect overlap of even a few consecutive words of text with previously published papers. Along this line, it is generally considered inappropriate for authors to include more than a modest amount of text from their own earlier papers, as in introductory material. The submitted paper must include significant new material.\n- Theoretical background. The authors must lay a firm theoretical foundation for their work. What precisely is the theory or principle that is to be tested or analyzed? What is the proposed methodology to test the theory?\n- Experimental procedures and data analysis. In scientific research papers, this portion of the manuscript is typically scrutinized the most carefully of all. If the mathematical arguments and/or experimental procedures are sloppy, poorly documented or insufficiently focused on testing the hypothesis in question, the manuscript may be rejected regardless of its other merits.\n- Statistical methods. Given the importance of quantitative data in modern science, it is essential that the researchers use the most appropriate statistical methods appropriate for their research. For example, most researchers nowadays take rigorous courses in statistical methods as part of their training. Even researchers in nonscientific fields of study such as history, religion, psychology, art or literature, are, increasingly, employing statistical methods in their work.\n- Conclusions. Given all of the above procedures and results, are the conclusions truly justified, firmly based on the results presented? If the authors have read too much into their results, or if there are more prosaic explanations of their results, or if there is any indication that the authors are overstating their results, the manuscript will be rejected.\n- Originality and importance. Even if all of the above items are satisfactory, if the manuscript's results are simply judged as not particularly useful or important to the field, the manuscript may be rejected.\nWhen the editor (or chair) receives these reports, he/she decides whether: (a) to accept the manuscript as-is, (b) to accept the manuscript, provided that some relatively minor items identified by the referees are corrected or revised, (c) not to accept the paper as-is, but to reconsider if some relatively significant items are corrected or revised, or (d) to reject the manuscript outright. For many scientific journals and conferences, fewer than 25% of submissions are initially accepted or accepted pending minor revision. Others are rejected, require major revision (sometimes more than once), are subsequently submitted to another journal or conference, or are never formally published.\nIn spite of these daunting obstacles, hundreds of thousands of peer-reviewed articles are published each year in many thousands of journals and conference proceedings. This massive and rapidly growing body of work constitutes the core of what is properly referred to as modern scientific and secular scholarship.\nNote that the process of anonymous peer review helps to keep the world of research free from \"cronyism\" and related ills. A manuscript submitted by a Nobel Laureate may be reviewed by a lowly postdoctoral researcher, and, if found wanting for one of the reasons listed above, the manuscript may well be rejected. It is important to point out that it is not appropriate for a referee or editor to reject a manuscript merely because he or she does not like the results -- solid, defensible reasons must be provided. And there are safeguards to ensure that there is no \"conspiracy\" in the research specialty to block papers with certain types of results or conclusions (see Conspiracy).\nIs the process of peer review foolproof?\nObviously the process of peer review is not foolproof -- no human endeavor could ever be foolproof. Some highly questionable papers have been published, and some important papers were initially rejected. As a single example, there is growing concern that research funded by pharmaceutical companies is often not fully objective, and, in any case, that the medical research community must be much more careful in peer review of such work [Seife2012].\nIndeed, the need for the biomedical arena in particular to tighten its standards was underscored rather dramatically by a recent Sokal-like hoax, as described in a October 2013 Science article [Bohannon2013]. Science journalist John Bohannon constructed a spoof paper describing the anticancer properties of a substance extracted from lichen. The article had major flaws, from a scientific standpoint -- for example, the effect of the substance was modest and essentially identical over five orders of magnitude of concentration, which to an experienced reviewer would have been immediate grounds to suspect the methodology and conclusions. What's more, there was no indication that the trials had been done with proper experimental safeguards for a human trial. The text, written by Bohannon, had been translated by Google to French, and then back to English, to give it the feel of a third-world author.\nBohannon then sent 304 variations of his paper to open-access journals, where publication costs are to be covered by the authors of accepted papers, but which are otherwise publicly available. By the time his article documenting the spoof was published in October 2013, 157 of the 304 papers had accepted, and only 98 had been rejected. Many of the journals that accepted the papers were based in third-world countries, with possibly less rigorous peer review standards, but several of these journals were affiliated with top-tier publishing houses [Bohannon2013]. This episode has sparked many journals and publishing houses to take a hard look at their peer review procedures. Several marginal journals have already been shut down, and other reforms are in the works.\nHave creationist and/or intelligent design writers subjected their work to peer review?\nWhile creationist and intelligent design writers have published their arguments dissenting from conventional science in various books and online articles, they have not, as far as anyone can determine, even seriously submitted these writings, much less have them published, in any reputable peer-reviewed scientific journal. For additional details, see Creationism and Intelligent-design .\nThis lack of peer-reviewed publications, or even serious attempts at submitting material for peer review, presents a severe obstacle for creationism and intelligent design to be taken seriously in the scientific world. If creationist and intelligent design writers (individually or collectively) believe that any of their technical issues have significant merit on purely scientific grounds, why do they not compose them into well-researched and well-analyzed articles and submit these articles to recognized peer-reviewed scientific journals?\nAfter all, as emphasized in a recent Science letter signed by numerous prominent scientists (after brief mention of the prevailing theories of geology, big bang cosmology and evolution), \"Even as these are overwhelmingly accepted by the scientific community, fame still awaits anyone who could show these theories to be wrong.\" [Gleick2010]. The only reasonable inference from the lack of publications is that these writers themselves recognize that the arguments and data that they have presented to date would not meet the rigorous standards required of serious peer-reviewed scientific literature.\nSome prominent journals in the field of geology and evolution include the Journal of Evolutionary Biology, Evolution, the Journal of Geophysical Research, Science and Nature. Some prominent journals in the field of physics and cosmology include the Journal of Physics A: Mathematical and General, the Astrophysical Journal, the Journal of Astrophysics and Astronomy and the Journal of Cosmology and Astroparticle Physics. Each of these websites includes a facility for submitting papers.\nHave the \"new atheist\" writers subjected their work to peer review?\nBut lest scientists and secular scholars get too smug in their assessment of creationist-intelligent design literature, it must be pointed out that the same criticism can be leveled at the \"new atheist\" literature.\nIt is a fact that in spite of the numerous widely read books and articles by the \"new atheist\" authors, hardly any of this material has been published in professional peer-reviewed academic journals. And, as with the creationist-intelligent design literature, the only reasonable conclusion one can draw is that these writers themselves recognize that their writings would likely not pass peer review. Indeed, this literature has numerous flaws, and published reviews have not been flattering. See Atheists for details.\nJust as with the creationist and intelligent design writers, if any of the current crop of atheist authors believe that they have some new insights or arguments in religious studies, religious history, theology or science and religion, they are invited to submit manuscripts to a leading publication in the appropriate field. Otherwise, their writings will not be taken seriously by professional scholars in these fields.\nSome prominent journals in the field of religious studies and religious history include Religious Studies: An International Journal for the Philosophy of Relgion, Religion and Theology and the Journal of Religious History. Some leading journals in the field of science and religion include Theology and Science and the Journal for Interdisciplinary Research on Religion and Science. Each of these websites includes a facility for submitting papers.\nThe process of peer review has been an essential part of both scientific research and the larger world of scholarly communication for centuries. It requires a huge amount of time and effort on the part of researchers, both to prepare manuscripts for peer review and also to review manuscripts written by other researchers. But the resulting peer-reviewed journals and conference proceedings are of significantly higher quality as a result. While the process is certainly not foolproof, and lapses have occurred of both types (accepting bad papers and rejecting good papers), it has demonstrated itself to be a highly effective means of uncovering truth.\nWith regards to both the creationist-intelligent design literature and also the \"new atheist\" literature, it is significant to note that, as far as anyone can determine, hardly any of these writers have even attempted, much less succeeded, in publishing their work in reputable peer-reviewed academic journals. This literature may be cast in scientific terminology, include references to history or philosophy, and have other trappings of serious scholarship, and may be persuasive to those who lack professional training in the particular fields of science, religious history or philosophy, but it is not real professional scholarship.\nSee also Atheists,\nWhat is science?."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:3875dbe1-7964-447d-aee2-fc81b3afb20c>","<urn:uuid:5e2ecb63-b829-4c10-b3ff-9a672ee99e79>"],"error":null}
{"question":"As a materials engineer studying weld defects: What are the characteristics of crater cracks in welds and how does the preliminary heating process affect the formation of these defects when welding copper to steel?","answer":"Crater cracks form near the end of a weld in high-stress, low-strength areas when the weld pool doesn't have enough volume after cooling to overcome shrinkage stresses. These cracks can travel back through the entire weld centerline and are common in aluminum and some tool steels. When welding copper to steel, preliminary heating helps prevent crater cracks by providing a special thermal cycle that ensures stability of the welding process. The pulse preheating method specifically helps by enabling uniform microstructure formation and proper regulation of temperature distribution, which reduces internal stresses that could lead to crack formation. This is particularly important given copper's high heat conductivity, which requires more heat input than other metals.","context":["The Preliminary Heating at Welding Copper and Steel\nStability of process of welding of copper with copper and copper with steel is provided with a special thermal cycle which includes preliminary heating before welding. Difficulties welding copper and steel are connected with its physical and chemical properties, high affinity of copper to oxygen, the low temperature of melting of copper, considerable absorption by liquid copper of gases, various sizes of coefficients of heat conductivity, linear expansion etc. In view of high heat conductivity of copper the most part of heat entered at welding is removed from heat input zone that leads to necessity of supply to a place of welding considerably a more heat, than at welding other metals. More rational is the local heating. Using impulse heating of welded details at welding copper and steel instead of gas flame lowers carbonization of details’ surfaces. Uniformity of welded details microstructure is provided. Welding defects caused by non-uniform heating are eliminated.\nAt a glance: Figures\nKeywords: impulse source of heating, welding of copper with steel, welded details, microstructure, details fusion line\nAmerican Journal of Materials Engineering and Technology, 2013 1 (3),\nReceived June 29, 2013; Revised August 09, 2013; Accepted September 02, 2013Copyright © 2014 Science and Education Publishing. All Rights Reserved.\nCite this article:\n- Gavrish, P. A.. \"The Preliminary Heating at Welding Copper and Steel.\" American Journal of Materials Engineering and Technology 1.3 (2013): 46-48.\n- Gavrish, P. A. (2013). The Preliminary Heating at Welding Copper and Steel. American Journal of Materials Engineering and Technology, 1(3), 46-48.\n- Gavrish, P. A.. \"The Preliminary Heating at Welding Copper and Steel.\" American Journal of Materials Engineering and Technology 1, no. 3 (2013): 46-48.\n|Import into BibTeX||Import into EndNote||Import into RefMan||Import into RefWorks|\nImprovement of technology of welding and methods directed on increase of reliability of welded compounds of copper with steel, increase of mechanical properties of ready details an important task of welding branch of mechanical engineering. In welding processes, the input parameters have greater influence on the mechanical properties of the weld joints. The heat conductivity of red copper is good, so we should choose welding methods with high heat efficiency and concentrated heat quantity. It is beneficial if the heat efficiency is high and the energy is concentrated. In practice the local heating is produced by the different sources of heat up to heating by gas-oxygen torches, by a welding arc etc. Modern development of the production gas welding equipment more wide use for this aim of the special gas-oxygen burners of type GZU (Russia), working on gas mixture or natural gas. However the way of gas preliminary heating has many shortcomings . Therefore researches of pulse preliminary heating of details before welding are executed.\n2. Experimental Procedure\nTherefore welding of copper with steel is carried out with preliminary and attendant heating . Applying preliminary heating the thermal condition regulation of welded connection is carried out. However, despite of the quite good results received at gas-flame preliminary.\nHeating this way has some lacks:\n• Oxidation of metals superficial layer and additional carbonization of superficial layer of copper and steel.\n• Difficulty of regulation temperature on the side of copper and on the side steel.\n• Complication of regulation temperature on the thickness of metal (At increase of metal thickness various layers of metal heat up differently - closer to a surface metal gets warm up to higher temperature) . With the purpose of removal of the above-mentioned defects applied is the impulsive heating of metal by the currents of rectangular form. In this case the forced heating of the local site to the necessary temperature is provided and the size of thermal influence zone is decreased . The generator of a pulsing current of the rectangular form has been applied for this purpose with power up to 2 kW with adjustable frequency, porosity and current size (Figure 1) .\nThe rectangular form current allows to make the forced heating of a detail with high speed up to temperatures at which there is a qualitative welding process of copper with steel (amount of energy in time unit at the rectangular form pulsing current has 2 times more energy in time unit than the variable sinusoidal form current does). Welding wire with powder obtained according to research .\nResearch microstructures executed by means of microscope. In technology of arc types of welding the weld-fabricated guy-sutures appear with the use of parent metal and flux cored electrode at formed in the narrow zone of crystallization molten metal. The metal of the weld fabricated guy-sutures is estimated on indexes by analogical to the parent metal: durability, plasticity, viscidity and to other indexes. The special value at welding has maintenance of carbon over 0,3%, that results in the loss of plasticity of zone of the weld-fabricated guy-sutures, in addition, maintenance of admixtures in the copper of Bi, Pb, S and O result in formation of eutectic connections also resulting in the loss of plasticity of the weld-fabricated guy-sutures. Samples for metallographic analysis were cut mechanically from sites that cover all areas of the weld: direct suture, heat-affected zone, and exciting area of basic metals steel and copper, and in a direction transverse to the weld and along the weld heat affected zone. Mechanical polishing carried out on the special polishing machine Мonfasupal № 1381550 with a circle with a diameter of 200 mm fitted by felt. Frequency of rotation of a circle from the electric motor is equal 800 rpm. Cloth moistens with polishing liquid which give continuously or periodically. To a rotating circle with cloth press the ground surface a sample and in the course of polishing turn. Polish to a total disappearance рисок and receiving a smooth surface that borrows at well ground surface of 5-10 min.\nThe microstructures of the samples were studied on microscope ERNST LEIST GmbH Dialux.\nFor all options that were investigated were determined conditions providing the formation of high-quality weld metal during welding. Metallographic study of thin sections of welded joints showed that in all cases the structure of the weld metal welding and biphasic. For weld metal compound copper-containing mild steel, iron to 2% of typical coarse cast structure. ε-solid phase solution of iron in copper, with iron concentration less 1,8% fine-grained structure of the seam, chopped inclusions and α-phase. With an iron content of more than 2%, there are round or outlet inclusion α - phase solid solution of copper in iron. Photo weld copper to steel (Figure 2).\nGood formation of a seam and lack of defects – result of pulse preliminary heating.\nMechanical properties of a welded seam are influenced by a microstructure of border of alloy age. Than more dendrites of iron at the alloy age line especially are stronger a welded seam and plasticity of a seam decreases cause such defects cracks on alloy age border. At research of a microstructure of a welded seam of Figure 3 it is visible that the structure as steel and copper at the line of alloy age has uniform character. Dendritnoye a structure of steel prevails the more, than farther from the alloy age line. On an axis of a welded seam copper and steel hashing the uniform.\nThe photo shows a uniform distribution of the dendrites of iron in the fusion line. Mechanical properties of the weld shown good results.\nHowever applying traditional gas heating the alloy age line Figure 4 is more sated with dendrites of iron and has lower mechanical properties.\nTo define plastic properties of a welded seam it is possible if to measure micro-hardness. In Figure 5 the schedule of dependence of micro-hardness from a distance border of a welded seam is shown.\nMicro-hardness of border alloy age from steel has insignificant growth; it is connected with bigger concentration of iron in a welded seam. However on border of alloy age there is a jump micro-hardness the copper parties of border of alloy age above, than for the annealed copper because of existence of dendrites of iron in heat affecter zone.\n1. It is raised a plastic zone of thermal influence, the metal workability after welding is improved.\n2. The findings, thus applying a pulse preheating rid of the defects inherent in gas-fired.\n3. Lack oxidation of metals superficial layer and additional carbonization of superficial layer of copper and steel.\n4. Fine adjustment of regulation temperature on the side of copper and on the side steel with generator of a pulsing current.\n5. Applying preliminary heating it is possible to increase plasticity of a heat affecter zone.\n|||Gavrish, P.A, Prevention of formation of crystallization cracks when welding copper with steel, The 10th International Scientific Conference ″New constructional became both alloys and methods of their processing for increase of reliability and durability of products ″, Zaporizhia National Technical University, Zaporizhia, 2005,77-79.|\n|||Gavrish, P.A, Local thermal processing at welding copper with steel, Proceedings of 7th International Conference ”Equipment and Technologies for Heat Treatment of Metals and Alloys”, 2007, Vol. I, 133-137.|\n|||Gavrish, P.A., Tulupov, V.I, Preliminary heating at welding of copper with steel / The 10th International Conference ″Research and Development in Mechanical Industry″. RaDMI 2010. In Memoriam of Prof.Dr Georgios Petropoulos. Donji Milanovac, Serbia, 16-17, Septemder, 2010, Vol.1, 156-158.|\n|||Tulupov, V.I, Increase of depth of hardening at the electromechanical fair turning, “Reliability of the tool and optimization of technological systems”, HERALD of the Donbass State Engineering Academy, Collection of science papers, 2008, Vol.23, 277-282.|\n|||Chigarev, V.V, Gavrish, P.A, Gribkov E.P, Investigation of the process of drawing flux-cored wire for welding cooper to steel, Taylor & Francis Engineering. Journal Welding international Vol.26, No 9, September, 2012, 718-722 ISSN 0950-7116.|","July 16, 2012\nThe major cause of a crack is when internal stresses exceed the strength of the weld metal, the base metal, or both. Knowing the basics behind why cracks happen, a welder can prevent those cracks from occurring in the first place.\nOne of the primary objectives of any weld fabrication is to prevent weld defects, especially cracks. Cracks are the most severe of all weld defects and are unacceptable in most circumstances. Rework robs the company of precious time and material (that is, money), so prevention is the primary concern.\nCracks don’t always happen immediately after welding, and certain cracks, such as the underbead variety, may not be open to the weld surface. Cracks can develop over time after the weld has been subjected to loads while in service. Tensile and fatigue loads; bending, twisting, or flexing; as well as hot and cold expansion and contraction all can occur long after welding, be it two days, two months, or even two years.\nThe major cause of a crack is when internal stresses exceed the strength of the weld metal, the base metal, or both. And once a focal point for these stresses—that is, a stress riser—develops and accumulates, a crack can propagate.\nA discontinuity is a weld fault that may or may not be serious enough to cause a rejection. Whether or not it violates code specifications will depend on further examination by a competent person against code requirements or in-house quality assurance specifications. If the fault violates either of these two, it becomes a defect. Defects require repair, but discontinuities do not. Violations of customer requirements often fall under the discontinuity rule and the weld will have to be repaired.\nIn short, defects always are discontinuities, but not all discontinuities are defects.\nThe responsibilities of both welder and supervisor affect weld quality. The welder is responsible for the defect when it is due to his or her skill level or weld deposition technique. Weld characteristics like incomplete fusion, excessively concave or convex bead contours, and improper weld size all can come from poor welding technique, improper travel speed, poor electrode manipulation, incorrect weld parameter settings, as well as failure to notify supervision of a problem with the job at hand.\nSupervisors must ensure welders have the tools necessary to do an effective job. They must maintain a shop safety program in compliance with OSHA regulations. They also should, among other things, ensure welders are using the correct base and filler metal; have proper weld procedure testing; work with adequate and functional welding equipment; receive effective and meaningful welder training; and work with properly designed, accessible weld joints.\nResponsibility often goes beyond the welder and supervisor, especially when design-for-manufacturability issues come into play. For instance, joint accessibility has become more of a problem nowadays as many designers are not adequately familiar with the requirements of depositing a serviceable, defect-free weld. Can the welder get the gas metal arc welding gun, shielded metal arc welding electrode, or gas tungsten arc welding torch to the work area and still see the joint—or is he welding blind? Does the welder have enough room to manipulate the electrode at all the required angles to deposit a good weld, and still see the joint?\nIf no design alternative exists, managers must plan for potential weld errors. If an unacceptable weld defect occurs, can a worker get a grinder into the joint to remove the bad weld? If so, how will the weld be repaired? A welder or supervisor can answer all these questions, but the best solution often requires input from customers and product designers.\nThe weld pool has a tremendous amount of built-in stress from weld metal contraction, or shrinkage. Liquid metal is at its maximum expansion, or volume, and so when it cools and solidifies, it has only one direction to go. If the weld pool does not have enough volume after cooling to overcome shrinkage stresses, a crater crack will form, often near the end of a weld, in a high-stress, low-strength area (see Figure 1). It’s the weld’s way of relieving stress.\nThe length of the weld deposit also is highly stressed, so that crater crack can very easily travel back through the entire length of the weld centerline. This is a common problem in aluminum and some tool and die steels. The remedy is simple: Fill the crater to its full cross section (the same as the weld size) before the weld is finished. You can accomplish this with various methods. You may pause for two or three seconds at the end of the weld before stopping the arc; or you may choose to backstep (reverse direction of travel) for about 0.5 inch at the end of the bead.\nAn excessively concave bead profile is a common problem with fillet welds, especially those on stainless steel, INCONEL® alloys, and aluminum, but plain carbon steel isn’t immune. A certain amount of concavity may be acceptable, depending on the welding requirements. But an excessively concave weld bead contour (see Figure 2) is a serious candidate for centerline cracking. It generally occurs immediately when welding aluminum (where it’s often called “hot cracking”), and is slightly delayed in other materials, after the metal cools to about room temperature.\nThe problem with concave welds is very similar to that with crater cracks. Reducing the weld throat reduces its strength dramatically, because there isn’t enough filler metal in the weld cross section to combat shrinkage stresses. This means those stresses are in control, and a crack develops. If the weld has insufficient throat depth, it probably has insufficient strength.\nAs with crater cracks, preventing such centerline cracking isn’t difficult. Two principal culprits are incorrect travel speed and voltage setting. Voltage is a measure of electrical “pressure,” a force pushing down on the face of the liquid weld metal. A small reduction in arc voltage (1 to 1.5 volts) can make a big difference in the weld bead’s contour. Reducing the voltage too much, though, can result in a severely convex weld bead contour. (Note that pulse GMAW brings up further considerations that are beyond the scope of this article.)\nIf you set voltage too high, the weld pool becomes difficult to control, and this may encourage you to increase travel speed. This in turn gives you insufficient weld throat depth and weld strength. Once the pool gets ahead of the arc, it’s over. You probably will get incomplete penetration, lack of fusion, and undercutting—common problems with vertical-down welding. In fact, performing a vertical-down fillet weld with an acceptable weld throat requires masterful weld pool control. To avoid these problems, slow the travel speed and give the weld time to build an acceptable bead contour.\nExcessively convex bead contours—that is, excessive weld reinforcement—isn’t generally associated with weld cracking, though such welds can cause problems. You can waste a lot of time and weld metal depositing an excessively high bead profile. Such a weld is unsightly and almost always unacceptable, mainly because of the weld re-entrant angle to the base metal (see Figure 3).\nSuch bead shapes can have an effect on cracking, especially on cracking that occurs over time. The crack generally is directed down into the base metal, right at the weld toe. If you don’t create a smooth transition of weld metal to base metal, you can disrupt the flow of forces through the weld. Such a high volume of weld metal creates significant shrinkage forces. When these forces exceed the strength of the weld, cracking ensues.\nTo avoid this problem, try increasing travel speed. You can also take a look at your voltage setting. A small increase in voltage increases electrical pressure, forcing the weld contour down to a more acceptable profile.\nUndercut defects (see Figure 4) reduce the base metal thickness where the base metal meets the filler metal. This loss of metal interrupts the transfer of stresses from member to member through the weld. If severe, this creates a stress concentration point and has the potential to accumulate and initiate a crack, rapidly.\nOn high-stress joints, acceptable levels of undercut are near zero. The American Welding Society’s D1.1, D1.2, D1.5, and D1.6 codes have extremely low acceptable limits of undercut, depending on the defect’s orientation in relationship to the applied stress direction and base metal thickness.\nUndercut develops because of improper welding techniques and procedure settings. It usually has no single cause but can come from a range of factors, including incorrect voltage settings, travel speed, and electrode-to-work angle. On fillet welds especially, if voltage (electrical pressure) is too high and the electrode angle favors one member more than another, the arc force will “wash away” the favored member at the weld toe. If the electrode favors one member more and the travel speed is too fast, the arc will naturally melt the member as part of the fusion process, but the high travel speed will not allow the melting electrode to fill in the washed-out area, resulting in an unacceptable weld.\nTo prevent these defects, make every effort to maintain proper voltage levels. For the constant-voltage processes (nonpulsed GMAW and flux-cored arc welding), the voltage stays fairly constant and can be adjusted manually. For constant-current processes, GTAW, and SMAW, voltage varies with the arc length. If you increase the arc length, you increase arc voltage. Be sure to maintain a correct electrode angle, and try decreasing travel speeds to allow weld deposition to do its job.\nOverlap (see Figure 4), or cold lap, is more serious than you might think. If the weld toe remains cold enough that it doesn’t fuse with the base metal, the weld just laps over, or lays over the base metal surface without fusing. This leaves no continuity between the weld metal and base metal, so there isn’t a path for stress to transfer through the weld into the adjoining member. A classic example of a stress riser, such overlap opens the door for cracking if stress accumulates to unacceptable levels.\nAgain, the fix isn’t difficult. If you don’t work the electrode evenly between the two base metals, the weld will favor one member more than the other, and the working parameters (amps and volts) will not liquefy the base metal evenly. Overlap is a common fault when you have to weld blind. Obviously, having to guess where the joint is won’t produce favorable results.\nThe AWS codes call for a “smooth transition” at the toe of the weld. This ensures that the weld stresses flow evenly and, most important, stops those detrimental cracks from forming.\nThe FABRICATOR® is North America's leading magazine for the metal forming and fabricating industry. The magazine delivers the news, technical articles, and case histories that enable fabricators to do their jobs more efficiently. The FABRICATOR has served the industry since 1971. Print subscriptions are free to qualified persons in North America involved in metal forming and fabricating."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:0c218519-a1ad-4007-a95a-511f01778135>","<urn:uuid:d4c5ad2f-f0d0-4b07-98c0-208696d5ac7e>"],"error":null}
{"question":"Can you explain the different types of flatfoot conditions in children versus adults?","answer":"In children, flatfoot can be categorized as either flexible (arch returns when not standing) or rigid (arch always flat). Rigid flatfoot includes conditions like tarsal coalition (abnormal joining of foot bones) and congenital vertical talus (apparent at birth with rocker bottom appearance). Adult flatfoot, on the other hand, progresses through four distinct stages: Stage I shows flatfoot position without deformity with common tendinitis pain, Stage II begins showing foot alignment changes, Stage III presents a fixed deformity with stiff ankle, and Stage IV involves both foot and ankle deformity with possible arthritis. While pediatric flatfoot often responds well to conservative treatment, adult cases frequently require surgical intervention.","context":["Collapsed arches occur in five percent of adults 40 years and older, especially those who are overweight or maintain sedentary lifestyles. At the onset of the condition, adult acquired flatfoot can be controlled with anti-inflammatory medications, physical therapy, taping, bracing, and orthotics. While most cases of adult-onset flatfoot require surgery, congenital flatfoot is an entirely different condition that is best treated with orthotics in children. Ninety percent of children born with flat feet will be fine with conservative treatment.\nOveruse of the posterior tibial tendon is often the cause of PTTD. In fact, the symptoms usually occur after activities that involve the tendon, such as running, walking, hiking, or climbing stairs.\nAs different types of flatfoot have different causes, the associated symptoms can be different for different people. Some generalized symptoms are listed. Pain along the course of the posterior tibial tendon which lies on the inside of the foot and ankle. This can be associated with swelling on the inside of the ankle. Pain that is worse with activity. High intensity or impact activities, such as running and jumping, can be very difficult. Some patients can have difficulty walking or even standing for long periods of time and may experience pain at the inside of the ankle and in the arch of the foot. Feeling like one is ?dragging their foot.? When the foot collapses, the heel bone may shift position and put pressure on the outside ankle bone (fibula). This can cause pain in the bones and tendons in the outside of the ankle joint. Patients with an old injury or arthritis in the middle of the foot can have painful, bony bumps on the top and inside of the foot. These make shoe wear very difficult. Sometimes, the bony spurs are so large that they pinch the nerves which can result in numbness and tingling on the top of the foot and into the toes. Diabetic patients may not experience pain if they have damage to their nerves. They may only notice swelling or a large bump on the bottom of the foot. The large bump can cause skin problems and an ulcer (a sore that does not heal) may develop if proper diabetic shoe wear is not used.\nThere are four stages of adult-acquired flatfoot deformity (AAFD). The severity of the deformity determines your stage. For example, Stage I means there is a flatfoot position but without deformity. Pain and swelling from tendinitis is common in this stage. Stage II there is a change in the foot alignment. This means a deformity is starting to develop. The physician can still move the bones back into place manually (passively). Stage III adult-acquired flatfoot deformity (AAFD) tells us there is a fixed deformity. This means the ankle is stiff or rigid and doesn???t move beyond a neutral (midline) position. Stage IV is characterized by deformity in the foot and the ankle. The deformity may be flexible or fixed. The joints often show signs of degenerative joint disease (arthritis).\nNon surgical Treatment\nConservative treatment also depends on the stage of the disease. Early on, the pain and swelling with no deformity can be treated with rest, ice, compression, elevation and non-steroidal anti-inflammatory medication. Usually OTC orthotic inserts are recommended with stability oriented athletic shoes. If this fails or the condition is more advanced, immobilization in a rigid walking boot is recommended. This rests the tendon and protects it from further irritation, attenuation, or tearing. If symptoms are greatly improved or eliminated then the patient may return to a supportive shoe. To protect the patient from reoccurrence, different types of devices are recommended. The most common device is orthotics. Usually custom-made orthotics are preferable to OTC. They are reserved for early staged PTTD. Advanced stages may require a more aggressive type orthotic or an AFO (ankle-foot orthosis). There are different types of AFO's. One type has a double-upright/stirrup attached to a footplate. Another is a gauntlet-type with a custom plastic interior surrounded be a lace-up leather exterior. Both require the use of a bulky type athletic or orthopedic shoes. Patient compliance is always challenging with these larger braces and shoes.\nIf conservative treatments don?t work, your doctor may recommend surgery. Several procedures can be used to treat posterior tibial tendon dysfunction; often more than one procedure is performed at the same time. Your doctor will recommend a specific course of treatment based on your individual case. Surgical options include. Tenosynovectomy. In this procedure, the surgeon will clean away (debride) and remove (excise) any inflamed tissue surrounding the tendon. Osteotomy. This procedure changes the alignment of the heel bone (calcaneus). The surgeon may sometimes have to remove a portion of the bone. Tendon transfer: This procedure uses some fibers from another tendon (the flexor digitorum longus, which helps bend the toes) to repair the damaged posterior tibial tendon. Lateral column lengthening, In this procedure, the surgeon places a small wedge-shaped piece of bone into the outside of the calcaneus. This helps realign the bones and recreates the arch. Arthrodesis. This procedure welds (fuses) one or more bones together, eliminating movement in the joint. This stabilizes the hindfoot and prevents the condition from progressing further.","What Is Pediatric Flatfoot?\nFlatfoot is common in both children and adults. When this deformity occurs in children, it is referred to as “pediatric flatfoot,” a term that actually includes several types of flatfoot. Although there are differences between the various forms of flatfoot, they all share one characteristic — partial or total collapse of the arch.\nMost children with flatfoot have no symptoms, but some children have one or more symptoms. When symptoms do occur, they vary according to the type of flatfoot. Some signs and symptoms may include:\n- Pain, tenderness, or cramping in the foot, leg, and knee\n- Outward tilting of the heel\n- Awkwardness or changes in walking\n- Difficulty with shoes\n- Reduced energy when participating in physical activities\n- Voluntary withdrawal from physical activities\nFlatfoot can be apparent at birth or it may not show up until years later, depending on the type of flatfoot. Some forms of flatfoot occur in one foot only, while others may affect both feet.\nTypes of Pediatric Flatfoot\nVarious terms are used to describe the different types of flatfoot. For example, flatfoot is either asymptomatic (without symptoms) or symptomatic (with symptoms). As mentioned earlier, the majority of children with flatfoot have an asymptomatic condition.\nSymptomatic flatfoot is further described as being either flexible or rigid. “Flexible” means that the foot is flat when standing (weight-bearing), but the arch returns when not standing. “Rigid” means the arch is always stiff and flat, whether standing on the foot or not.\nSeveral types of flatfoot are categorized as rigid. The most common are:\n- Tarsal coalition. This is a congenital (existing at birth) condition. It involves an abnormal joining of two or more bones in the foot. Tarsal coalition may or may not produce pain. When pain does occur, it usually starts in preadolescence or adolescence.\n- Congenital vertical talus. Because of the foot’s rigid “rocker bottom” appearance that occurs with congenital vertical talus, this condition is apparent in the newborn. Symptoms begin at walking age, since it is difficult for the child to bear weight and wear shoes.\nThere are other types of pediatric flatfoot, such as those caused by injury or some diseases.\nIn diagnosing flatfoot, the foot and ankle surgeon examines the foot and observes how it looks when the child stands and sits. The surgeon also observes how the child walks and evaluates the range of motion of the foot. Because flatfoot is sometimes related to problems in the leg, the surgeon may also examine the knee and hip.\nX-rays are often taken to determine the severity of the deformity. Sometimes an MRI study, CT scan, and blood tests are ordered.\nTreatment: Non-surgical Approaches\nIf a child’s flatfoot is asymptomatic, treatment is often not required. Instead, the condition will be observed and re-evaluated periodically by the foot and ankle surgeon. Custom orthotic devices may be considered for some cases of asymptomatic flatfoot.\nIn symptomatic pediatric flatfoot, treatment is required. The foot and ankle surgeon may select one or more approaches, depending on the child’s particular case. Some examples of non-surgical options include:\n- Activity modifications. The child needs to temporarily decrease activities that bring pain as well as avoid prolonged walking or standing.\n- Orthotic devices. The foot and ankle surgeon can provide custom orthotic devices that fit inside the shoe to support the structure of the foot and improve function.\n- Physical therapy. Stretching exercises, supervised by the foot and ankle surgeon or a physical therapist, provide relief in some cases of flatfoot.\n- Medications. Nonsteroidal anti-inflammatory drugs (NSAIDs), such as ibuprofen, may be recommended to help reduce pain and inflammation.\n- Shoe modifications. The foot and ankle surgeon will advise you on footwear characteristics that are important for the child with flatfoot.\nWhen Is Surgery Needed?\nIn some cases, surgery is necessary to relieve the symptoms and improve foot function. Foot and ankle surgeons perform a variety of techniques to treat the different types of pediatric flatfoot. The surgical procedure or combination of procedures selected for your child will depend on his or her particular type of flatfoot and degree of deformity.\nThis information has been prepared by the Consumer Education Committee of the American College of Foot and Ankle Surgeons, a professional society of 5,700 podiatric foot and ankle surgeons. Members of the College are Doctors of Podiatric Medicine who have received additional training through surgical residency programs. The mission of the College is to promote superior care of foot and ankle surgical patients through education, research and the promotion of the highest professional standards. Copyright © 2004, American College of Foot and Ankle Surgeons, www.acfas.org"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:1b3709ee-3cc5-4f17-ae8b-e9da029a0182>","<urn:uuid:63f2b0c2-0632-48fb-a46d-c0372dd026d3>"],"error":null}
{"question":"How is the ISA/IEC 62443 series being leveraged for cybersecurity in critical infrastructure, and what specific challenges do railway operators face when implementing cybersecurity measures?","answer":"The ISA/IEC 62443 series provides a flexible framework for addressing security vulnerabilities in industrial automation and control systems. It is being leveraged by legislation like New York's proposed cybersecurity bill to protect critical infrastructure including public transportation, utilities, and healthcare facilities. The standards focus on people (training and cybersecurity hygiene), process (risk assessment and management), and technology (compliance certification). In the railway sector specifically, operators face several key challenges: lack of internal expertise, difficulty assessing threat likelihoods, vulnerability of OT systems (which have 30-year lifecycles and legacy components), and complex supply chain risks. Railway OT systems are particularly concerning as they can affect passenger safety and cause accidents if compromised, and they are becoming more vulnerable as they become increasingly interconnected with IT systems.","context":["- September 07, 2021\n- Research Triangle Park, North Carolina\nNew York state legislature is hoping to add additional protections to the state’s critical infrastructure via a newly proposed cybersecurity bill. The bill leverages the industry-adopted ISA/IEC 62443 series of standards to shape metrics and benchmarks for operational technology cybersecurity. If passed, the bill’s measures would be applied to the state’s critical infrastructure facilities, including: public transportation; water and wastewater treatment facilities; public utilities and buildings; hospitals, public health facilities, financial service organizations; and automation and control system components.\n“There have been an increased amount of cyberattacks where hackers are just holding people hostage,” Senator Kevin Thomas, the bill’s sponsor, said. “The bill looks to address this by updating systems to match international standards so that the state’s critical infrastructure is protected as much as possible. There needs to be more vigilance. We need to know whether these critical infrastructure systems can be compromised and how to upgrade them to prevent them from being compromised.”\nThe ISA/IEC 62443 series of standards, developed by the ISA99 committee and adopted by the International Electrotechnical Commission (IEC), provides a flexible framework to address and mitigate current and future security vulnerabilities in industrial automation and control systems (IACSs). The committee draws on the input and knowledge of IACS security experts from across the globe to develop consensus standards that are applicable to all industry sectors and critical infrastructure.\n“The technologies that control and automate the world’s most critical operations, including the facilities where we work and live, are under constant threat and attack,” said ISA Global Cybersecurity Alliance Managing Director Andre Ristaino. “Consistent, global adoption of the ISA/IEC 62443 series of standards will help vendors, third parties, and end users—indeed, the entire digital supply chain—effectively and proactively manage risks to their people, assets, and operations.”\nThe ISA Global Cybersecurity Alliance (ISAGCA), made up of 50+ companies, continues to actively work to confront cybersecurity challenges in multiple ways. In general, a strong cybersecurity posture relies on people, process, and technology:\n- People: Individuals and companies using automation and control systems must be well-trained, and companies must make better cybersecurity hygiene and best practices the fabric of their corporate and facility-level cultures.\n- Process: The ISA/IEC 62443 series of standards, endorsed by the United Nations and backed by hundreds of asset owners in every world region, specifies how to assess and manage cybersecurity risk in OT environments. At the state and federal level, in the United States and around the world, the ISAGCA and its member companies are advocating policies that designate theuse of the ISA/IEC 62443 series of standards as foundational, outcome-focused, technology-neutral documentation. The series of standards articulates roles, responsibilities, and expectations of suppliers, service providers, and asset owners, which can be easily translated to enforceable policy elements.\n- Technology: The ISA Security Compliance Institute offers conformance programs to certify components, devices, systems, and processes that are compliant with the latest ISA/IEC 62443 requirements. ISA advocates common-sense approaches to protecting legacy and next-gen equipment, leveraging the vast knowledge and expertise of our supplier and integrator member companies.\nMany critical infrastructure and industrial manufacturing companies already have orare working diligently to integrate cybersecurity into their risk-management and business continuity plans and strategies. Using the ISA/IEC 62443 series of standards as their foundation, they focus on adopting security as part of the operations lifecycle, ensuring compliance with various aspects of the standards across their supply chains, and including cybersecurity in operational risk-management profiles.\nThe ISA Global Cybersecurity Alliance (ISAGCA) is a collaborative forum of member companies that aim to advance cybersecurity awareness, education, readiness, andknowledge sharing industry-wide, on a global scale. The alliance’s objectives includeexpanding the development and use of the ISA/IEC 62443 series of standards, knowledge-sharing in an open environment, providing best practice tools to help companies secure their infrastructure, creating education and certification programs, and advocating for cybersecurity awareness and sensible approaches with world governments and regulatory bodies.\nAbout ISAGCA Members\nThe ISA Global Cybersecurity Alliance is made up of 50+ member companies, representing more than $1.5 trillion in aggregate revenue across more than 2,400 combined worldwide locations. Automation and cybersecurity provider members serve 31 different industries, underscoring the broad applicability of the ISA/IEC 62443 series of standards. Current members of ISAGCA include 1898 & Co. (Burns McDonnell), ACET Solutions, aeSolutions, Baserock IT Solutions, Bayshore, Carrier Global, Claroty, ConsoleWorks, Coontec, CyberOwl, CyPhy Defense, Deloitte, Digital Immunity, Dragos, Eaton, exida, Ford Motor Company, Fortinet, Honeywell, Idaho National Laboratory, Idaho State University, ISASecure, Johns Manville, Johnson Controls, KPMG, LOGIIC, Mission Secure, MT4 senhasegura, Munio Security, Nova Systems, Nozomi Networks, PAS, PETRONAS, Pfizer, Radiflow, Redacted, Red Trident, Rockwell Automation, Schneider Electric, Surge Engineering, TDI Technologies, Tenable, TI Safe, Tripwire, TXOne Networks, UL, Wallix, WisePlant, Xage Security, and Xylem. For more information about ISAGCA, visit www.isa.org/isagca.","European railway undertakings and infrastructure managers systematically address cyber risks as part of their security risk management processes, especially after the Network and Information Security (NIS) Directive came into force in 2016. Addressing cyber risks in the railway sector can raise entirely new challenges for railway companies who often lack the internal expertise, organisational structure, processes or the resources to effectively assess and mitigate them.\nThe nature of railway operations and the interconnectedness of railway undertakings, infrastructure managers, and the supply chain requires all involved parties to achieve and maintain a baseline level of cybersecurity.\nDirective 2016/1148 (NIS Directive) is the first legislative document focusing on cybersecurity in the EU. It identifies Operators of Essential Services (OES) in the railway sector as:\nInfrastructure managers (IM), as defined in point (2) of Article 3 of Directive 2012/34/EU, include: “any person or firm responsible in particular for establishing, managing and maintaining railway infrastructure, including traffic management and control-command and signalling. The functions of the infrastructure manager on a network or part of a network may be allocated to different bodies or firms”.\nRailway undertakings (RU), as defined in point (1) of Article 3 of Directive 2012/34/EU, include: “any public or private undertaking licensed according to this Directive, the principal business of which is to provide services for the transport of goods and/or passengers by rail with a requirement that the undertaking ensures traction. This also includes undertakings which provide traction only”. This also includes operators of service facilities as defined in point (12) of Article 3 of Directive 2012/34/EU as “any public or private entity responsible for managing one or more service facilities or supplying one or more services to railway undertakings”.\nThe NIS Directive requires IMs and RUs to conduct risk assessments that “cover all operations including the security and resilience of network and information systems”. According to the NIS Directive, these risk assessments, along with the implementation of appropriate mitigation measures, should promote “a culture of risk management” to be developed through “appropriate regulatory requirements and voluntary industry practices”. This need for cyber risk management in the European railway sector was also identified as a key priority by the participants of the ENISA-ERA conference “Cybersecurity in Railways”, which took place online on 16-17 March 2021 and brought together more than 600 experts from railway organisations, policy, industry, research, standardisation, and certification.\nWhile some EU Member States (MS) have issued relevant national guidance to OESs on how to conduct cyber risk assessments, most railway operators choose to adopt one of the different methodologies introduced by industry standards. Indeed, there are currently varying approaches to tackle risk in the railway sector and for now, there is no single approach that covers both information technology (IT) and operational technology (OT) cyber risks.\nIn the railway sector, compromised OT systems can affect passengers’ safety, cause a train accident, or interrupt traffic. OT systems are usually more vulnerable than IT systems, in part due to a lack of cybersecurity awareness in OT personnel, in part because they were not designed with cybersecurity in mind (long lifecycles of 30 years, presence of legacy systems) and because they are less controlled and decentralised compared to IT systems.\nWhile in the past they remained less exposed, often isolated from internet and other IT networks, they are now more and more interconnected with classic IT systems, which makes them even more vulnerable and exposed to cyber threats.\nRUs and IMs need to identify which cyber threats are applicable to their assets and services. One of the common questions is whether threats, such as disasters, physical attacks, or outages, should be included or considered as not being specific to the “cyber” ecosystem. Most stakeholders include them, as they can affect information security.\nIf they are not included, they should be considered in other risk management or business continuity management processes of the company, and this must be agreed on when the threat taxonomy is being developed. Another challenge faced by the railway sector is assessing the likelihood of a threat scenario. One would need to consider the level of capability required for an attack, the level of exposure of the targeted asset, and the intent of an attacker, all of which are information that RUs and IMs may have difficulty in assessing accurately.\nSeveral methods are proposed by the different cyber risk management frameworks. For example, X2Rail-314 proposes to rely on the Common Vulnerability Scoring System (CVSS). They have selected four CVSS Exploitability metrics in CVSS: Attack Vector (System Exposure), Attack Complexity, Privileges Required and User Interaction. Levels for these metrics have been defined, mathematically calculating the resulting likelihood.\nOther methods are less quantitative, but also simpler to apply, such as ISO27005, which combines the likelihood of occurrence of the threat (low, medium, high), the ease of exposure (low, medium, high) and the value of the asset (from 0 to 4) to calculate the likelihood of an incident scenario15. It is also very difficult to maintain this information because it changes through time as the threat landscape evolves.\nFinally, the railway sector faces challenges associated with supply chains. Security risks related to suppliers (e.g., remote access to the railway networks/systems) are less covered because of the heterogeneous and broad nature of the supplier landscape, but also because stakeholders do not have much control over the cybersecurity level of their suppliers and the cyber risks they may introduce. This topic can be reinforced by making an inventory of all the suppliers, categorising them in term of criticality (e.g., do they have access to a critical system, is there a strong interconnection between systems, do they manipulate sensitive data, etc.) and assessing the cybersecurity maturity of the most critical suppliers as a starting point.\nCyber Risk Scenarios\nScenario 1 – Compromising a signalling system or automatic train control system, leading to a train accident\nThis scenario requires high motivation of the attacker and in-depth knowledge of railway systems and networks. It is considered a low likelihood scenario. It has been included as the potential impact can be very high and this is one of the primary concerns of railway stakeholders when considering cyber risks. A similar incident took place in the city of Lodz, Poland in 2008 when an attacker managed to hack into a tram system.\nScenario 2 – Sabotage of the traffic supervising systems, leading to train traffic stop\nThis scenario is a targeted attack using a specific Industrial Control System (ICS) malware to disrupt the traffic supervising systems, thus leading to an urgent stop of train traffic. Such an incident has not yet occurred in the railway sector. This scenario could also be applied to freight docking systems, and thus disturb or interrupt freight activity.\nScenario 3 – Ransomware attack, leading to a disruption of activities\nIn 2021, ransomware attacks are considered the top threat scenario and are targeting the transport sector. In this case, the attacker infiltrates the information system, exploits a vulnerability, and deploys a ransomware on a large amount of assets. A similar incident happened in May 2017 when Germany’s Deutsche Bahn rail infrastructure was infected with WannaCry ransomware22, leading to messages appearing on station information screens.\nScenario 4 – Theft of clients’ personal data from the booking management system\nThis scenario is a targeted attack, where the attacker steals the identity of an administrator and is therefore able to connect to a cloud-based booking management system and exfiltrate customer data. A similar incident happened in November 2017 with Rail Europe North America (RENA) suffering due to a 3-month long data breach23 and in January 2019 when China Railway’s official online booking platform suffered a massive data breach, with information later being sold on the dark web.\nScenario 5 – Leak of sensitive data due to unsecure, exposed database\nThis scenario is also related to data leakage, but the starting point here is a supplier with a low cybersecurity level. The attacker uses this third-party weakness to exfiltrate sensitive data. A similar incident happened in February 2020 with a database of C3UK, which offered Wi-Fi services to passengers in train stations. The database contained 146 million records, including personal contact details and dates of birth, and was exposed online without a password.\nScenario 6 – DDoS attack, blocking travellers from buying tickets\nThis scenario is a targeted attack, where the prerequisite for the attacker is to have created a botnet network (a set of compromised devices controlled by a hacker to perform their attacks). The attacker can then use the botnet to flood devices with requests and make them unavailable. Another possibility to consider for a DDoS scenario is a non-targeted attack, where an Internet Service Provider (ISP) is targeted with this type of attack, thus affecting railway services that use this ISP.\nScenario 7 – Disastrous event destroying the datacentre, leading to disruption of IT services\nThis scenario is the consequence of a disastrous event which leads to disruption of activity. The event (natural disaster, fire, etc.), affects the datacentre and destroys part of it, leading to a physical destruction of IT systems and thus a disruption of activities related to these services. Depending on the redundancy strategy of the company (geo-redundancy, cloud, external back-ups, etc.), the disruption can last more or less time. A similar incident happened in March 2021 when OVH27 had a fire in one of its datacentres, making millions of websites unavailable for days.\nEuropean RUs and IMs use a combination of good practices, approaches, and standards to perform cyber risk management for their organisations. This report gathers insights on these current practices in a single document and can assist railway undertakings and infrastructure managers in their efforts to apply them. It provides examples of reference material, such as available taxonomies of assets and threats, comprehensive threats scenarios, derived from real incidents and cyber risk mitigation measures, derived by guidelines and standards.\nThe report also highlights the challenges faced when applying such approaches. Most importantly, there is a lack of a single cyber risk management approach for railway organisations to cover both IT and OT in a unified manner.\nIT vs OT risk management approaches. The differentiation between IT and OT in the railway sector is increasingly difficult and having discrete approaches and taxonomies for cyber risk management makes the issue more challenging. In many cases, it can be a complex process to identify which approach is better suited, whether a device can be considered IT or OT or which security measures and which standard should be applied. Having a more structured and unified approach with respect to cyber risk management would help the sector to harmonise, thus facilitating risk discussions between the different entities of the railway ecosystem. It can also enable more collaboration with the supply industry of the sector.\nMore harmonization and alignment of good practices. Future work could include further alignment of the sector-specific taxonomies and more guidance on the application of good practices. Wherever possible, further standardisation could be pursued, as this is also a request stemming from the railway supply industry, which advocates for more certification schemes at EU level. Significant sectoral challenges remain, including the cyber risk management of supply chains. This could be remedied with a regulatory approach encompassing the entire railway ecosystem under the same cyber risk management requirements. At present, key elements of the railway supply chain, both IT and OT, do not fall under the same European regulatory framework.\nKeeping railway systems and cyber risk assessments up-to-date. Another significant issue specific to the sector is the plethora of legacy systems which add an additional degree of difficulty when managing cyber risk. At present, it is not possible to provide relevant recommendations to address the cybersecurity of legacy systems in the railway sector. It would be necessary to involve the railway industry in such an exercise. Additionally, even for newly developed systems, there is the need to ensure that the results of risk assessments remain current, that risks are continuously monitored, and that the security level remains adequate. Maintaining an up-to-date threat landscape for the railway sector could be a step towards this direction."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:2554a8c0-dffc-4403-a36e-9d78ae090b93>","<urn:uuid:67451628-c31e-42eb-a257-fe76ddc0cb22>"],"error":null}
{"question":"What's the connection between motor skills and reading development, and how does this affect people with reading disorders? 🧐","answer":"Motor skills and reading development are connected in several ways. For typically developing children, fine motor skills are crucial for reading readiness, particularly in activities like holding books and writing down words. Writing, coloring, and doodling help exercise hand muscles and develop fine motor control, which supports understanding that words represent sounds. For individuals with dyslexia, a different approach involving multi-sensory instruction is needed, incorporating sight, sound, and touch when learning new concepts. This might include activities like drawing and practicing basic letter formation, along with using assistive technologies like screen readers and voice recognition software to support reading and writing development.","context":["July 3, 2018\nTalking, Singing, Reading, Writing, Playing\nWhat’s the best way to get your child ready to read before kindergarten? What does it mean for a child to be ready to read? Librarians all across the nation use Every Child Ready to Read® during storytime and parents can use it too! It can be a useful guide for finding activities to do at home that get kids ready to read.\nReading readiness can be all kinds of things from knowing what a book is to knowing letter names and sounds. It includes things like having a rich vocabulary, liking books and being read to, as well as being able to tell a story and playing with words and sounds. Reading readiness also includes having little hands ready to hold books through fine motor skills!\nEvery Child Ready to Read teaches parents five things they can practice at home. How many of these do you practice with your child?\n1. Talking with your child about everyday things. Asking them about their day can help them understand how language works and can help build up their vocabulary.\n2. Singing songs and doing rhymes with your child. Making musical sounds allows your child to start hearing the small differences in words, therefore this is really important when they start sounding out words for reading.\n3. Reading to your child every day. If they associate reading with something fun and positive, they’ll be less likely to get frustrated if reading is hard for them. Spending time with your little one and a book is one of the best things you can do to help them read on their own.\n4. Writing down words. Your child gets to exercise their hand muscles and work on their fine motor skills. Coloring or doodling also gives them a chance to practice and helps them understand that words stand for sounds.\n5. Playing games. A child’s natural way to learn and grow is playful. Children can learn more words and develop storylines in their heads when they have room to be creative.\nLibrarians use these five practices in their story times and you can use them too! With a little bit of practice, your child will be ready to read when the time comes.\nBuild Your Child’s Brighter Future!\nWant to dive deeper into helping your little one build language skills? Check out our Play and Learning guidance about English and language arts for:\n- Babies — From baby talk to body talk, babies learn so many word skills in their first year.\n- One-year-olds — Chatter, scribbles and tons of books lay a foundation for lifelong language skills!\n- Two-year-olds — So much to say, so little time! Toddler’s communication skills increase at lightning speed.\n- Three-year-olds — New phrases, new words, new solo “reading” — at three, little ones find new ways to play with language every day!\n- Pre-K learners — Getting ready for school success means that pre-K is a year of huge language growth.","WHAT IS DYSLEXIA??\nDyslexia is a reading processing disorder that can hinder reading, writing, spelling and sometimes even speaking. Dyslexia is not a sign of poor intelligence or laziness. It is also not the result of impaired vision. Children and adults with dyslexia simply have a neurological disorder that causes their brains to process and interpret information differently.\n• Dyslexia is often characterized by difficulties with accurate word recognition, decoding and spelling.\n• Dyslexia may cause problems with reading comprehension and slow down vocabulary growth.\n• Dyslexia may result in poor reading fluency and reading out loud.\n• Dyslexia is neurological and often genetic.\n• Dyslexia is not the result of poor instruction.\n• With the proper support, using alternate learning methods, almost all people with dyslexia can become good readers and writers.\nDyslexia occurs among people of all economic and ethnic backgrounds. Often more than one member of a family has dyslexiaMuch of what happens in a classroom is based on reading and writing. So it’s important to identify dyslexia as early as possible.\nWHAT ARE THE WARNING SIGNS OF DYSLEXIA?\nThe following are common signs of dyslexia in people of different ages. If you or someone you know displays these signs, it doesn’t necessarily mean you have a learning disability. But if troubles continue over time, consider testing for dyslexia.\nDYSLEXIA: WARNING SIGNS BY AGE\nHOW IS DYSLEXIA IDENTIFIED?\nTrained professionals can identify dyslexia using a formal evaluation. This looks at a person’s ability to understand and use spoken and written language. It looks at areas of strength and weakness in the skills that are needed for reading. It also takes into account many other factors. These include family history, intellect, educational background, and social environment.\nHOW IS DYSLEXIA TREATED?\nIt helps to identify dyslexia as early in life as possible. Adults with unidentified dyslexia often work in jobs below their intellectual capacity. But with help from a tutor, teacher, or other trained professional, almost all people with dyslexia can become good readers and writers. Use the following strategies to help to make progress with dyslexia.\n• Expose your child to early oral reading, writing, drawing, and practice to encourage development of print knowledge, basic letter formation, recognition skills and linguistic awareness (the relationship between sound and meaning).\n• Have your child practice reading different kinds of texts. This includes books, magazines, ads and comics.\n• Include multi-sensory, structured language instruction. Practice using sight, sound and touch when introducing new ideas.\n• Seek modifications in the classroom. This might include extra time to complete assignments, help with note taking, oral testing and other means of assessment.\n• Use books on tape and assistive technology. Examples are screen readers and voice recognition computer software.\n• Get help with the emotional issues that arise from struggling to overcome academic difficulties.\nReading and writing are key skills for daily living. However, it is important to also emphasize other aspects of learning and expression. Like all people, those with dyslexia enjoy activities that tap into their strengths and interests. For example, people with dyslexia may be attracted to fields that do not emphasize language skills. Examples are design, art, architecture, engineering and surgery.\nSOURCE : NATIONAL CENTRE FOR LEARNING DISABILITIES\nFor more information on Dyslexia in Nigeria, please contact\nC.A.D.E.T Academy, 10b Owena Close, Off Yedseram Street, Maitama Abuja, Nigeria\nPhone: 0705 763 0825"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:758897a1-de2c-4f53-a3c8-840a29cd660f>","<urn:uuid:6624009d-4aaa-40c0-a666-65156a3e7e50>"],"error":null}
{"question":"As a radio enthusiast interested in EME (Earth-Moon-Earth) communications, what are the key grounding requirements for tower installation, and what battery considerations are important for reliable station power?","answer":"For tower grounding, each tower leg must be connected to an 8-foot ground rod using heavy-gauge ground cable and stainless steel clamps. The ground rods should be bonded together with a heavy copper ground cable ring and connected to the house's perimeter grounding system. For power, sealed lead acid batteries are recommended for reliable station operation, providing 13.8V when charged and 10.8V when discharged. They should be charged at 2.4-2.5V per cell for cyclic charging, or 2.3V per cell (13.8V for a 12V battery) for continuous float charging, with charging current limited to 20% of the rated amp-hour capacity.","context":["Tech Night – VHF+ Weak Signal Stations Part 1 – Overview and 6 Meters\nWe recently did a Tech Night on building and operating VHF+ stations as part of the Nashua Area Radio Society’s educational program. I wanted to share the presentation and video from this Tech Night so that our readers might learn a little more about how to get started and build their own VHF+ Weak Signal Station.\nThere is a lot to this topic so we’re going to cover it with two Tech Night presentations. The first one in the series is included here and it provides an Introduction to the VHF+ topic along with details on building and operating a station for the 6 Meter Band.\nJuly 2020 Tech Night Video – VHF+ Weak Signal Stations Part 1 – Introduction and 6 Meters\n6m Yagi and 2m/70cm/23cm Satellite Antennas On A Tower\nWe will be hosting a Tech Night about Building and Operating a VHF+ Weak-Signal Station tonight, July 14th at 7 pm Eastern Time. The live, interactive video of our tech Night will be shared via a Zoom conference and all of our readers are welcome to join. I plan to cover the following topics during our session this evening:\nWhy do weak-signal work on 6 meters and above?\nWhat can you work and what modes are used on these bands\nHow does propagation work at 50 Mhz and above and how can you measure it?\nHow does one operate using SSB, CW, and digital modes on these bands?\nWhat equipment is needed and what are some possible ways that you can put together a VHF+ station?\nSome demonstration of actual contacts\nIn addition to an overview of how to get on all of the bands above 50 MHz, we will focus on the 6 Meter (Magic) band. The session will include demonstrations of FT8 and Meteor Scatter contacts on 6 m. I will also briefly describe the 6 m station here at AB1OC-AB1QB and show how we use it to make contacts. A second Tech Night will cover stations and weak-signal operating on 2 m and above.\nThe Zoom information for our Tech Night Session follows. We suggest that you join early so that you have a chance to make sure that your computer, speakers, microphone, and camera are working.\nNow that spring is here, we’ve continued work on our EME station project. The most recent project was to build complete the ground system for our new EME tower. The proper way to ground a tower is shown above. Each leg of the tower is connected to an 8′ ground rod via a heavy-gauge ground cable. The cable is attached to the tower leg using stainless steel clamps meant for this purpose. The three ground rods associated with the tower legs are then bonded together using a heavy copper ground cable ring.\nThe final step was to connect the bonding run from the tower to the perimeter grounding system around our house. This completed the tower grounding system and enabled us to complete our final permit inspection courtesy of our local building inspector.\nFinished Tower Base\nWith all of this work done and the inspection complete, we added a mulch bed around our new tower to make this area of our lawn easy to maintain.\nThe next step in our project is to begin building the antennas that will go on our EME tower. You can read more about our EME station project via the links that follow:\nSnow is coming to New England this weekend so we wanted to get the control cables run to our new EME Tower before the ground is covered with snow. The project involved installing a Utility Enclosure on our tower and running three control cables to our shack for the following devices:\nAz-El Rotator and Preamp Switching Control Connections\nWe began by install some barrier strips and a copper ground strap in the Utility Enclosure. The copper strap provides a good ground connection to the tower and associated grounding system. The enclosure is clamped to the tower using two stainless steel clamps.\nThe final step was to hook up our rotator cables to a Green Heron RT-21 Az/El Rotator Controller in our shack. We do not yet have our elevation rotator so we tested the M2 Orion 2800 Azimuth Rotator that is installed in our tower. The azimuth rotator is configured so that the rotator’s dead spot faces north. This is a good configuration of our planned EME operation.\nWe continued to test our Portable Satellite Station 4.0 as part of AMSAT’s 50th Anniversary Celebration WAS Activations. You can read about the activations and our station’s performance via the link above. Overall, we were pleased with how the portable setup performed. The weakest link was the downlink performance of our antenna system. We are working on some ideas to improve this element of our setup – more to come on this project…\nThere are many reasons to have an accurate time source in your station. Getting the best performance from WSJT-X modes like FT8 requires your computer clock to be synchronized to within a second for example. You can set your clocks accurately using NTP servers on the Internet. This is the most common way that most stations set their clocks.\nWhat if you are portable and don’t have Internet access or what do you do if your Internet connection goes down? One way to solve these problems is to use a GPS controlled NTP time server in your station. We recently installed one from Leo Bodnar in our station.\nThis device is simple to install. It just requires an Ethernet connection to your network and a GPS antenna. The antenna is included with the unit. The antenna will need to be outdoors with a reasonably clear view of the sky.\nGPS Satellite Lock Screen\nAfter a minute or so after it is installed and powered up, the unit will synchronize to the visible GPS satellites in your location and report its coordinates. This indicates that you have a good GPS system lock and that the clock in the unit is accurate to within a microsecond.\nNTP Summary Screen\nThe unit gets its IP either from DHCP or via a fixed IP address that you can program. Once the unit is set, you use its IP address as the NTP server in your software to set your clocks. You would set you NTP server in a program like Dimension 4 to accurately set your computer’s clock for example. You will want to disable your computer’s normal Internet clock setting function to avoid conflicts with Dimension 4. Once this is set up, your computer clock will be synchronized to the GPS system and will be very accurate and you will get the best performance from WSJT-X.\nThe performance of the 3.1 Station’s antennas is very good but the antenna system is a handful to transport. We are planning to install these antennas on a new tower at our QTH and use our Flex-6700 SDR-based Remote Operating Gateway with some upgrades to create a remotely controlled satellite station that can be operated via the Internet. The main components of the 4.0 Station will include:\nUpgrade plans for our Transportable station include the addition of remote switchable polarity relays and a new Icom IC-9700 Transceiver when it becomes available.\nPolarity Switch Installed in LEO Pack Antennas\nThe polarity switches have been installed on the M2 Antennas 436CP16 and 2MCP8A antennas in our M2 Antennas LEO Pack. We are using a DX Engineering EC-4 console to control LHCP or RHCP polarity selection on the antennas. We have been doing some testing with the upgraded LEO pack which includes the polarity switching capabilities and we are seeing a significant improvement in performance.\nAlfaSpid Az-El Rotator\nWe are also planning to move the upgraded LEO pack antennas to the current 3.1 Tower to take advantage of the AlfaSpid Rotator which is installed there.\nIcom IC-7900 Transceiver\nThe other major upgrade planned for the 2.2 Station is the new Icom IC-9700 Transceiver when it becomes available. This radio will utilize Icom’s SDR platform and includes a Pan Adapter/Waterfall display which will be a very useful addition for operation with Linear Transponder Satellites.\nUpgraded Portable 1.2 Station\nWe really enjoy mountain topping and activating grid squares so we are planning upgrades to our 1.2 Station for this purpose.\nOur 1.2 Portable Satellite Station on Mt. Kearsarge\nThe 1.2 Station utilizes computer control to enable operation with linear transponder satellites and will use solar/battery power along with a 100w/70w Icom IC-910H Satellite Transceiver.\nA pair of 90W foldable solar panels, an MPPT solar charger, and a pair of LiPo 4S4P A123 batteries provide plenty of power to run the IC-910H Transceiver and the associated computer. The portable station also includes a pair of ARR preamps.\nPortable Satellite Antenna System\nThe antenna system we’ll be using is an Elk Portable Log Periodic 2m/70cm yagi on a camera tripod. A combination of a compass and an angle finder gauge helps us to correctly point the antenna.\nAs you can probably tell, all of these upgrades are in progress and are at various stages of completion. We will post updates here on our Blog as we continue to make progress. Here are links to some of these posts:\nQuite a few Nashua Area Radio Society members have been working on a display to get young people and potential new Hams interested in Amateur Radio. Our display will be part of the New England Amateur Radio Convention in Boxboro, MA on September 8th and 9th. We are also planning a similar display for NEAR-Fest at Deerfield Fairgrounds, NH later in the fall. You can see more about our planned display and the associated hands-on activities via the following link.\nI want to share some information about an Amateur Radio event that we will be doing at the Boxboro, MA Ham Radio Convention in September. Our display and hands-on activities provide an introduction to Amateur Radio for young people and include information and a chance to try Amateur Radio activities such as:\nYou can read more about our plans for the event via the link above.\nMorse Trainer Kit\nWe’ve been working with Steve Elliot, K1EL to develop an inexpensive kit building project to include as part of our displays. We will be including a new kit building activity in as part of our display. Builders can purchase the Morse Trainer Kit shown above for $20 and build it at the show. We will provide soldering equipment and kit building mentors to help builders complete their kit. The package includes batteries and a printed manual. We will have these kits available for walk-up purchase at the show on both Saturday and Sunday.\nI am also planning to provide forum presentation on the following topics on Saturday at Boxboro:\nCreating Successful Youth Outreach Projects\nPortable Satellite Station Design, Operation, and Planning for an upcoming ISS Crew Contact\nSTEM Learning for Young People via High Altitude Balloons Carrying Amateur Radio\nThe Amatuer Radio gear on the International Space Station (ISS) supports digital and SSTV modes as well as FM voice communications. The astronauts onboard periodically fire up the SSTV system and transmit images to commemorate milestones in space travel. We recently received a set of 12 images from such an event which commemorated Cosmonautics Day. You can read more about how this is done and view the images via the link below.\nWe installed a 75m loop for SSB operation on our tower when we built it. The loop is full size and is diamond shaped so that our lower SteppIR DB36 yagi can rotate inside of it. The loop is fed at the bottom corner about 20 ft up from the ground. It works great for SSB operation on 75m but we have often wished we could use it across the entire 80m band. This goal led to a project to create a matching system for the antenna. The idea was to use a set of loading coils in series at the feed point create a good match in all segments of the 80m band.\nEZ-NEC Model for 75m Loop\nThe first step in the design of our 80m matching system was to build a model of our current loop using EZ-NEC. The model was then used to determine the correct values of a set of series loading inductors to match different segments of the 80m band.\nMatching System Design Analysis\nWe also considered how likely different segments of the 80m band were to be used by profiling historical spotting data from DXSummit. All of this analysis led to the creation of a final design which is captured in the spreadsheet shown above. The final design requires our current 75m loop to be shortened a bit to work well at the very top of the 80m band.\nModeled Loading Coil Inductance Values\nA set of 5 different inductor pairs can be used in series with the loop’s feed point to create a good match across the entire 80m band. The modeled values for the series matching inductors is shown above.\nMatching System Modeled SWR\nOur microHAM control system can easily implement the switching of the various inductance values based upon the frequency that a radio using the antenna is tuned to. Result modeled SWR for the final 80m loop and match combination is shown above. The design should achieve an SWR < 1.5:1 across the entire 80m band except for the very top where the SWR remains < 2:1. Also, the design optimizes the system’s SWR in the important CW DX, SSB DX, and Digital windows on the 80m band.\nLayout of Components in Enclosure\nWith the design completed, we choose an enclosure and all of the components. Here are the details of what we used:\nThe first step in the construction was to layout all of the components in the enclosure. Attention was paid to keeping the two series inductors at right angles to avoid coupling and to keep RF connections as short as possible. The relays were arranged to keep the leads connecting to the coils of roughly equal length. Finally, the control circuitry was kept as far removed from the RF leads as possible.\nEnclosure Mounting Ears and Clamps\nThe matching system attaches to a tower leg via saddle clamps. We fabricated a set of mounting ears and spacer blocks to position the enclosure far enough away from the tower so that the antenna connections do not interact with the tower.\n80m Matching System Construction\nA summary of the completed matching system construction is shown above.The design uses a set of four double-pole double-throw relays to switch in different coil taps which selects the loading inductance provided by the matching system.\nWe did a set of calculations and found that our relays would be subjected to a worst case peak-peak voltage of about 2.1 KVp-p at the coil tap points.\nThe relays are arranged such that two sets of contacts have to be traversed to select any given coil tap. The relays we are using have a third pole which we are not using. We disassembled each relay and removed the internal contact wiring for the center pole which improves both the coil to contact voltage rating and the isolation values of the relays.\nThese steps combine to improve the voltage rating of the system. This is an important design element given that the match will operate at legal limit power.\nCompleted RF Deck\nThe completed RF deck and control circuitry is shown above. The enclosure we choose came with a removable plastic plate that made mounting and wiring all of the components simple.\nLoading Coil Mounting and Taps\nThe loading inductors are mounted using nylon hardware with the ends connected to the two antenna terminals on the sides of the enclosure. The coils use movable tap clips to allow us to fine-tune the match once the system is installed with the antenna on our tower. The initial clip locations are set to create the inductance values modeled during the design phase.\nRelay Control Circuit Connections\nThe relay control leads use twisted pair wiring to minimize RF pickup. The control leads are routed away from the RF connections to minimize potential RF coupling.\nRelay Control Circuit Details\nThe control circuits for each relay use a combination of a Diode, a Varistor (MOV) and a filter capacitor in parallel to avoid relay coil switching interference and to suppress control line noise.\n1.5 to 1 Matching Balun\nThe matching system is designed to operate at 75-ohms which is pretty close to the resonant impedance of our 75m loop. The current antenna uses a 1.5:1 Balun to match the loop to our 50-ohm coax feedline. We disassembled an identical matching balun (actually a 75-ohm balun plus a 1.5:1 unun) and used it without its enclosure to create a final 50-ohm match.\nMicroHAM Setup to Control 80m Matching System\nThe final step in the construction of our matching system was to program our microHAM antenna switching system to properly configure the relays in our matching system. This was quite simple to do using microHAM’s frequency dependent antenna control capabilities. The microHAM system automatically operates the appropriate relays to create the best possible match as the radio which is using the matching system is tuned across the 80m band.\nUnfortunately, we are in the middle of winter here in New England so I will have to wait for warmer weather to install our new matching system on the tower and make the final adjustments. I am planning another article here when the final integration steps are done to cover the performance of the completed project.","Exploring rechargeable batteries\nRechargeable batteries: they're used\neverywhere, and there's many different brands and types. Almost every amateur\nhas their own opinions on the merits of different types and the best ways to\nlook after them. This month we examine the main types available and their\nsuitability for various equipment amateurs use.\nHow rechargeable batteries work\nBatteries convert stored chemical energy\ninto electrical energy. This is achieved by causing electrons to flow whenever\nthere is a conductive path between the cell's electrodes.\nElectrons flow as a result of a chemical\nreaction between the cell's two electrodes that are separated by an\nelectrolyte. The cell becomes exhausted when the active materials inside the\ncell are depleted and the chemical reactions slow. The voltage provided by a\ncell depends on the electrode material, their surface area and material between\nthe electrodes (electrolyte). Current flow stops when the connection between\nthe electrodes is removed.\nRechargeable cells operate on the same\nprinciple, except that the chemical reaction that occurs is reversed while\ncharging. When connected to an appropriate charger, cells convert electrical\nenergy back into potential chemical energy. The process is repeated every time\nthe cell is discharged and recharged.\nDifferent cells use different electrode\nmaterials and have different voltage outputs (1.2, 1.5, 2 and 3.6 volts for the\ntypes discussed here). Higher voltages are possible by connecting cells in\nseries. A set of several cells connected together is called a battery. However,\nbecause lay people do not distinguish between a 1.5 volt cell and a 9\nvolt battery (which comprises several cells), the term battery is widely\nused for both batteries and cells.\nThe capacity of cells is expressed in\namp-hours (Ah) or milliamp-hours (mAh). The approximate time that a battery\nwill last per charge can be found by dividing the battery pack capacity\n(normally written on the battery pack itself) by the average current\nconsumption of the device. Thus a 600 mAh battery pack can be expected to power\na receiver that takes 60mA for 10 hours.\nCells can be visualised as consisting of a\ncell with a resistor in series. You won't find an actual resistor should you\nsplit open a battery pack, but the effect is the same. Some battery types have\nhigher values of internal resistance than others. High internal resistance\ndoesn't matter if powering items that draw fairly low currents (eg a clock or\nsmall receiver). However, if running something like a 5-watt handheld\ntransceiver, a battery with a high internal resistance will not deliver the\ncurrent asked of it.\nHaving explained some of the characteristics\nimportant to all batteries, we will now look at each cell type in turn.\nNickel-cadmium cells are the most commonly\nused rechargeable batteries in consumer applications. They come in similar\nsizes to non-rechargeable cells, so they can directly replace non-rechargeable\nalkaline or carbon-zinc cells. NiCads have a lower voltage output than\nnon-rechargeable cells (1.2 vs 1.5 volts). This difference is not important in\nNiCad battery packs have voltages of 2.4,\n3.6, 4.8, 6, 7.2, 9, 10.8 volts, etc. This corresponds to 2, 3, 4, 5, 6, 7, 8\nand 9 cells respectively.\nNiCads perform best between 16 and 26\ndegrees Celsius. Their capacity is reduced at higher temperatures. Hydrogen gas\nis created and there is a risk of explosion when cells are used below 0\nNiCad batteries have a low internal\nresistance. This makes them good for equipment that draws large amounts of\ncurrent (eg portable transmitting gear). However low internal resistance means\nthat extremely high currents (as much as 30 amps for a C-sized cell!) will flow\nif cells are short-circuited. Short-circuiting should be avoided as it can\ncause heat build-up and cell damage.\nMost portable transceivers come with NiCad\nbattery packs where the cells are welded to metal connecting straps. There is\ngood reason for this. In high-current applications, the unknown (and varying)\nresistance between cells and battery holder contacts can result in erratic\noperation. This is especially so when the transceiver is used in a salt-laden\nenvironment. An encased battery pack overcomes these difficulties and provides\nmore reliable operation.\nThe normal charging rate is 10 per cent of a\nbattery's capacity for 14 hours. For example, if a battery pack has a 600 mAh\nrating, its correct charging current is 60 mA. Because the charging process is\nnot 100% efficient, the charger needs to be left running for about 14 hours\ninstead of 10 hours. Higher charging currents are possible, but the charging\ntime needs to be proportionally reduced. NiCads can be left on a trickle\ncharger indefinitely if the charging current is reduced to 2% of the battery's\namp-hour rating. Avoid the build up of heat during charging for long battery\nNiCad batteries require a constant current\ncharger; ie one where the current provided to the battery is fixed over the\nentire charging period. Such a charger can be something as simple as an\nunregulated DC power supply with a series resistor to limit the charging\ncurrent into the cells. If the charger's voltage and the battery's desired\ncharging current is known, Ohm's Law can be used to calculate the correct\nseries resistor value. Because NiCads have a low internal resistance, proper\ncharging can occur with several cells in series.\nFor best life, do not discharge NiCads to\nless than 1.0 volt per cell. When charging, NiCads should read 1.45 volts per\ncell. If the cell voltage is higher during charging (eg 1.6 or 1.7 volts), the\ncell is faulty and should be discarded.\nYou'll often hear discussions about the\nso-called 'memory effect' exhibited by NiCad cells. This refers to the claimed\ntendency of cells not to deliver their rated voltage when placed in a charger\nbefore being fully discharged. Belief in the existence of the 'memory effect'\nis widespread amongst users of NiCad batteries. However, textbooks and data\nfrom battery manufacturers make little or no mention of it. Believers say that\nto prevent it batteries must be discharged to 1 volt per cell before charging.\nNon-believers say that this discharging merely reduces cell life.\nEvidence suggests that true 'memory effect'\nis rare. It was first noticed in communications satellites where cells were\ndischarged to precisely the same discharge point every time. In casual amateur\nuse batteries are most unlikely to be discharged to the same point after every\nuse. Much of what is mistaken for the 'memory effect' is voltage depression,\nwhich is caused by long, continuous overcharging, which causes crystals to grow\ninside the cell. Fortunately both the 'memory effect' and voltage depression\ncan be overcome by subjecting the battery to one or more deep charge/discharge\nAnother term you will hear is 'cell\nreversal'. This can occur when a battery of cells is discharged below its safe\n1.0 volt per cell. During this discharge, differences between individual cells\ncan lead to one cell becoming depleted before the rest. When this happens, the\ncurrent generated from the remaining active cells will 'charge' the weakest\ncell, but in reverse polarity. This can lead to the release of gas and\npermanent damage to the battery pack.\nNiCads can short circuit due to the build up\nof crystals inside the battery. The use of a fully-charged electrolytic\ncapacitor placed across the cell can effect a temporary cure. Over-discharging\nof batteries invites short circuiting. Batteries should be stored charged. A\nlifespan of 200 to 800 charges is typical for NiCad batteries.\nNickel metal hydride (NiMH)\nLike NiCads, nickel-metal hydride cells\nprovide 1.2 volts per cell. Battery makers claim that NiMH cells do not suffer\nfrom the 'memory effect' and can be recharged up to 1000 times.\nNiMH cells are not quite as suitable as NiCads for\nextreme current loads, but do offer a greater capacity in the same cell size. A\ntypical AA NiCad may have a 750 mAh, but a NiMH may provide 2400 mAh - three times the capacity.\nIf your style of portable operating involves going out for 3 or 4 hours and running around 5 watts output, NiMH cells are an excellent choice and are lighter than sealed lead acid.\nWhere to get them? 7.2 volt battery packs are often used for models. Two in series gives 14.4 volts, but you'll get over 16 volts immediately after charging. That's above what many commercial rigs are rated so use at your own risk. Still it's much easier to get higher RF power outputs from transistors with 15 - 20 volts than 12 volts, so provided you're happy to use a somewhat non-standard voltage then it may still be sensible to use them for homebrew rigs (provided heatsinking is adequate and transistor ratings are observed). Another source are high-quality battery packs discarded by critical commercial users (such as hospitals). They may still be 80% good and entirely adequate for amateur use. One of my favourite QRP battery packs is a used ex-medical 24 volt NIMH pack bought at a hamfest for a few dollars. I split it in two and made two 12 volt packs. It's small and keeps a five-watt radio going for hours, even with heavy operating.\nNiCad chargers can be used to charge NiMH\nbatteries, but the charging time needs to be lengthened to take NiMH's\ntypically larger capacity into account. The main enemy of rechargeable cells is\nheat. If cells get hot during charging, reduce the charging current to no more\nthan that recommended.\nRechargeable alkaline manganese\nUnlike the preceding two battery types,\nrechargeable alkaline manganese (RAM) cells give a full 1.5 volts each. They\nare therefore suitable for applications where the substitution of 1.2 volt\nNiCads for 1.5 volt dry cells results in degraded equipment performance.\nRAM cells are cheaper to buy than NiCads.\nThey can be recharged between 50 and 750 times. They also have a greater\ncapacity than do NiCads - 1500 mAh is typical for size AA cells. RAM cells are\ngood for use with outdoor and solar equipment as they will work efficiently at\ntemperatures up to and exceeding 60 degrees Celsius.\nRAM cells have a much higher internal\nresistance than NiCads (0.2 ohms vs 0.02 ohms). This means that they cannot\nsupply high peak values of current. For this reason they are unsuitable for use\nwith standard amateur HTs. However, their high capacity and long shelf life (5\nyears) makes them suitable for low powered or emergency-use applications, such\nas clocks and emergency torches.\nChargers intended for NiCad and NiMH cells\nwill not charge rechargeable alkalines. This is because rechargeable alkaline\ncells require a constant voltage source of between 1.62 and 1.68 volts to charge.\nRAM cells should be connected in parallel rather than in series when charging\nseveral cells at a time. Unlike other rechargeable batteries, RAM cells are\npre-charged and do not require charging before first use. I have not had much\nsuccess with rechargeable alkalines and do not recommend them for amateur use.\nLithium ion cells came onto the market in the\n1990s. They offer higher cell\nvoltage (3.6 volts) and greater capacity for a given volume. This makes them\nespecially suitable for handheld equipment where long operating times are\nimportant, such as mobile phones.\nAs an example of what Lithium ion battery\npacks can do, a typical lithium ion battery pack is 55x45x20mm but provides 7.2\nvolts with a 1100 mAh capacity. Lithium ion batteries are more expensive than\nolder battery types and came into amateur use through their inclusion in handheld\ntransceivers such as Yaesu's VX-1R and VX-5R models.\nLithium polymer (LiPo)\nLithium polymer cells are the most recent of the\nbattery types discussed here to come onto the market. They're particularly favoured\nby model aircraft enthusiasts for whom light weight combined with high current capacity\nis essential. Most amateurs don't need such exacting requirements. Overheating and\neven fire are risks with poor handling due to their low internal resistance and potential\nto deliver extremely high currents.\nSealed lead acid\nSealed lead acid batteries (or 'gel cells')\nare less popular than NiCads in handheld equipment, but find widespread use as\nback up batteries in security systems and for amateur portable operation.\nPer-cell voltage is 2.3 volts when charged, and 1.8 volts when discharged. This\nequates to 13.8 and 10.8 volts respectively for a battery of six cells. For\nbest use of the full battery charge, equipment intended to operate with '12\nvolt' sealed lead acid batteries should operate well (if not at full power) at\nvoltages of 10.8 volts or less.\nGel cells are cheap, rugged and reliable and\nshould last several years at least. If you want a battery to run a QRP HF\nstation or a VHF/UHF handheld for several hours, they are the ideal choice.\nThey are also widely used with small solar systems.\nSealed lead acid batteries can either be\nused on a cyclic charge regime (battery connected to charger for a specific\ntime) or continuous float use, where the battery is across the charger any time\nit's not in use. Cyclic chargers should charge at 2.4 or 2.5 volts per cell and\nbe current limited to prevent overcharge. In contrast continuous float charging\n(or trickle charging) requires a charging voltage of only 2.3 volts per cell\n(13.8 volts for a '12 volt' battery). With both types of use the charger\nvoltage is held constant. Connect batteries in parallel if charging two or more\nfrom the one charger.\nChargers for sealed lead acid batteries are\navailable commercially or can be made at home. Special gel cell charger ICs\nexist to provide the necessary voltage and current regulation. Alternatively\nchargers can be made from the more common regulator chips such as the 723 or\nLM317. These chargers can be used to directly trickle charge the smaller '12\nvolt' gel batteries. No damage is done if the charger remains on, even when the\nbattery is fully charged. This is because as the battery voltage approaches 13.8,\nthe charging current will fall to negligible levels.\nSealed lead acid batteries should not be\ncharged at voltages higher than those indicated as safe above. This is because\nhigh charging voltages (eg 2.6 volts per cell) will endanger the battery due to\nthe production of excess gas. At a 13.8 volt charging voltage the production of\ngas is low, and the battery should give years of service. Charging current\nshould not exceed 20 per cent of the rated amp hour capacity of cells. If using\na high current 13.8 volt power supply as a charger, some form of current\nlimiting is desirable to stay within the battery's limits.\nSources of batteries\nThe explosion of portable lightweight battery powered devices has been great news for the amateur\nseeking batteries for their station with a big range now available. Note though that battery quality\nvaries. People have taken apart battery cases only to find a very small low capacity battery delivering much less\nthan the claimed amp hour rating is inside. Be on the lookout for that especially if a battery pack's low price seems too good to be true.\nThe items below could be a starting point when looking for the ideal battery or power accessory for your equipment.\nDisclosure: I receive a small commission from items purchased through links on this site.\nItems were chosen for likely usefulness and a satisfaction rating of 4/5 or better.\nThis article has examined the\ncharacteristics of all major types of rechargeable batteries used by amateurs.\nWe learned that NiCads and Lithium Polymer were best for high current\napplications. Sealed Lead acid and nickel metal hydride were good for casual\namateur portable use. Rechargeable alkaline wasn't recommended for transceivers\nthough may be OK for receivers. The charging of batteries\nvaries too - Rechargeable alkaline and sealed lead acid required a constant\nvoltage, but nickel cadmium and nickel metal hydride cells needed a constant\ncurrent to charge properly. In all cases over-charging, through excessive\nvoltages, currents or charging periods can cause heating, gas build-up and\npossible cell damage. However, if you treat your batteries well, you should\nhave many years of successful operation from them, whichever type you choose.\nI wish to acknowledge the people and\norganisations who have contributed to the writing of this article. These\n* The late Bill Trenwith VK3ATW for suggestions on the manuscript and\nimparting of knowledge gained through many years as a mechanics teacher,\nmodel engineer and radio amateur.\n* Peter Wegner from Coorey & Co, distributors of BIG rechargeable\n* Danielle Cvetkovic from Invensys Energy Systems Pty Ltd for\nmaterial on Hawker sealed lead acid batteries.\n* Adeal Pty Ltd for information on Varta's range of NiCad and NiMH\n1. Hawker P G3VA, Technical Topics Scrapbook\n1990-1994, RSGB, pages 1, 16, 142\n2. ARRL Handbook 1988, ARRL, pages 6-25,\n3. Gruber N WA1SVF, QST November 1994, ARRL,\nAn earlier version of this article appeared in Amateur Radio December 1999 with updates made in 2017."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c0e621dd-ac95-4860-973f-93b61c2c4166>","<urn:uuid:24e1ee2c-d79b-4882-8800-54b8f80b9570>"],"error":null}
{"question":"What were the historical developments in nanopore sequencing technology, and how has microplate reader technology evolved to complement these advances?","answer":"Nanopore sequencing was first conceived in 1989 by David Deamer but took 25 years to master the technical details. The technology evolved from an implausible idea to a practical mobile phone-sized device that can handle long DNA threads and works in zero gravity. Meanwhile, microplate reader technology has evolved toward greater functionality, flexibility, speed, and throughput. Modern systems now offer multiple detection modes including absorbance, fluorescence, and luminescence, with advanced features like monochromator-based technology for flexible wavelength detection and spectrometer-based readers that can capture entire absorbance spectra.","context":["DNA sequencing in zero gravity with handheld nanotech\nA “pocket sequencer” promises to make DNA sequencing cheaper and more accessible, writes Alan FInkel.\nNanotechnology. When American engineer Eric Drexler coined this futuristic term in 1981, he had in mind molecule-sized machines that would do useful tasks. The idea was to copy nature’s own machines – muscle proteins that exert force, for instance, or enzymes that carry out chemical reactions. But engineering at the nanometre scale is tough. We are talking about working with individual atoms. A silicon atom is 0.2 nanometres across. A muscle protein filament is as little as 7 nanometres in diameter.\nFor decades, we let human engineers off the hook, allowing a bevy of prosaic items from paints to plastics to claim the title of nanotechnology. To qualify, these products just had to involve particles smaller than 100 nanometres and display novel properties.\nDelightfully, this nano machine made by Oxford Nanopore Technologies mimics nature to achieve the feat of reading the sequence of the letters of the DNA code, the chemical bases guanine, adenine, thymine and cytosine.\nThe machine is the size of a mobile phone and, unlike traditional sequencers which are desktop-sized and require the DNA to be pre-cut into short segments, it can handle DNA as it comes: double-stranded, long threads.\nThis “pocket sequencer” promises to make DNA sequencing cheaper and more accessible. It has already been used to identify the Ebola virus in a matter of hours and to read the sequence of soil microbes aboard the International Space Station.\nHere’s a nutshell description of how the sequencer works.\nThe machine is a “nanopore”, a large single molecule pierced by a hollow channel a couple of nanometres in diameter. If you embed this nanopore in an ultrathin membrane bathed in an ionic solution and apply a small voltage, a tiny current will flow.\n(As an aside, the reason I am so tickled by this achievement is that our brain cells also communicate via tiny currents flowing through the pores of proteins called ion channels; I spent the major part of my working career designing and manufacturing sensitive amplifiers to measure these currents.)\nThe simple idea behind the nanopore is that as a strand of DNA is threaded through, it partially blocks the current flow. Since the degree of blockage depends on the particular DNA letter, the fluctuations in the current pattern reflect the sequence of letters on the DNA strand as it slithers through the nanopore.\nSounds simple but, as always, the devil is in the detail.\nThe nanopore has two modules. The first grabs double-stranded DNA, cleaves away one of the strands, then ratchets the remaining single strand into and through the hole. It holds each base for a hundred microseconds or more before allowing it to proceed, thereby giving the detection system time to make its measurements.\nThe second component is the pore. Shaped like a thin hourglass, at its narrowest it is a mere 1.2 nanometres in diameter. This narrowing is the sensing region where the electrical resistance changes as each base squeezes through.\nA complication is that neighbouring bases on the DNA strand can partially block the constriction. Accuracy is restored by reading the DNA strand multiple times.\nIngenious, but it took 25 years to master these devilish details. The implausible idea for nanopore sequencing was conceived in 1989 by David Deamer from the University of California at Santa Cruz; but it was way ahead of its time.\nYears later I was delighted to learn that Deamer and his colleagues, in their early experiments to detect the resistance fluctuations, used a “patch clamp” amplifier made by my former company, Axon Instruments.\nHow does Oxford make the nanopores? It programs bacteria to do the work. Now scientists there and elsewhere are trying to develop next-generation nanopores that will be directly fabricated from silicon nitride or graphene molecules. If they succeed, that will truly be Eric Drexler’s dream fulfilled.","This guide provides important information to help you make the right decision. Learn about the different types of readers/detectors; read modes, applications, technologies and other important considerations.\nMicroplate readers are used to detect biological, chemical or physical events in microtiter plates via the measurement of light. Microplate readers/detectors are the key workhorses in many laboratories and are used extensively for many applications across a wide range of disciplines including life sciences, drug discovery, bioassay validation, quality control, drug safety, toxicity testing, clinical diagnostics and biopharmaceutical/pharmaceutical manufacturing processes. Although a well-established product category, microplate readers continue to evolve towards greater functionality, flexibility, speed and throughput. Currently, there are a wide variety of microplate readers available, offering different capabilities and functionalities.\nMicroplate readers differ by the type of detection mode. The most common detection modes are absorbance, fluorescence and luminescence. Additional detection modes include fluorescence resonance energy transfer (FRET), time-resolved fluorescence (TRF), fluorescence polarization, bioluminescence resonance energy transfer (BRET), AlphaScreen and nephelometry.\nMicroplate readers also differ by detector technology. Light detection can be performed using filters, monochromators or spectrophotometry. Monochromator-based technology has the most flexibility in wavelength detection, so is most suitable for labs with variable detection requirements. Newer spectrometer-based readers can capture an entire absorbance spectrum of wavelengths, offering even more flexibility in detection. Filters are for fixed wavelength detection but offer greater sensitivity, so are more appropriate for labs with limited detection needs. Hybrid systems combining these technologies are also available. In addition, lasers rather than flash lamps are better for time-resolved fluorescence and AlphaScreen detection.\nMicroplate readers can be purchased as either single-mode or multimode; the latter combines several read modes in one instrument. Multimode microplate readers enable researchers to perform multiple assay types in one system. Multimode readers are typically more expensive than single-mode readers but purchasing one multimode reader is likely to be more cost-effective than buying several dedicated machines. In addition, they do not require much more bench space than a single reader and are often modular and upgradable, enabling a laboratory to purchase only what they need at the time but allowing for future additions.\nFor example, the CLARIOstar® microplate reader by BMG LABTECH is a multi-mode reader equipped with a patented new, dual monochromator technology, named LVF (linear variable filter) MonochromatorTM technology, offering fluorescence and luminescence measurements. The linear variable filters in the CLARIOstar® have variable coatings along their lengths that can reject or pass certain wavelengths of light. With no stray light and fewer background signals, the LVF Monochromator technology provides a higher-sensitivity detection.\nPlus, for measuring luminescence assays, it is important to ensure that the dynamic range offered by the microplate reader suffices the variety of experiments that will be planned in the future. The CLARIOstar®, for example, offers a nine-log dynamic range, providing you with flexibility in experiments and ensuring no signal goes undetected.\nWhen choosing a microplate reader/detector, the first, and perhaps the most important, consideration is the application requirement. Microplate readers are used across diverse scientific disciplines for varying applications. The most common methods are briefly introduced below. This downloadable compendium of application notes produced by BMG LABTECH covers a wide range of applications in the field of life sciences, spanning from bacterial kinetics, to live-cell assays to cell signaling measurements.\nProtein quantification is a common application where microplate readers are employed. Protein concentration is measured using chromogenic assays by determining absorbance at 280 nm. Microplate readers provide a way to scale up the number of samples processed in protein quantification, while also enabling triplicates in the same experimental conditions.\nThis downloadable application note by BMG LABTECH provides you with predefined protocols in the CLARIOstar® microplate reader that you can use for protein quantification experiments.\nPlus, delve deeper into using microplates in four commonly used colorimetric protein assays and explore a fluorescence-based protein quantification using microplates.\nEnzyme-Linked Immunosorbent Assays (ELISAs)\nELISA is an immunodetection assay that uses labeled secondary antibodies to detect specific antigens. The secondary antibody conjugate, such as horseradish peroxidase (HRP), is detected via an enzyme-mediated chromogenic change. ELISAs are used extensively in the life sciences, pre-clinical research and clinical diagnostics. The majority of plate readers are able to read ELISAs.\nNucleic acid quantification\nDNA and RNA absorb light within the UV range. Absorbance of a sample at 260 nm can be used to calculate the concentration of double-stranded DNA, single-stranded DNA or single-stranded RNA. Protein contamination, common in DNA and RNA preparations, can be determined using the 260/280 nm absorbance ratio. Plate readers with UV bandwidth detection capabilities are suitable for this detection.\nIn recent times, with an increased interest in sample conservation and high-throughput analyses, microplate readers are more preferred over the traditional cuvette-based measurements. This application note details the use of microplates in analyzing smaller volumes, taking a step towards automation.\nReporter genes, such as luciferase or GFP, can also be assessed in microplate readers, enabling in vitro and in vivo determination of gene expression for studies using markers of genetic alteration. Any reader with appropriate wavelength detectors can be used for this detection.\nThis application note by BMG LABTECH outlines how CRISPR/Cas9 genome editing was used to endogenously express nanoBRET donor proteins, enabling measurements of protein interactions and trafficking.\nProtein-protein and protein-ligand interactions\nUsing BRET, it is now possible to measure protein-ligand or protein-protein interactions in a microwell format. The first use of BRET for monitoring ligand binding to GPCRs in live cells is outlined in this useful application note by BMG LABTECH. The CLARIOstar microplate reader enabled calculation of IC50 and KD values from saturation and competition binding assays.\nPlus, learn key elements of measuring biomolecular interactions using Bio-Layer Interferometry in this application note that discusses large molecule kinetic assays on the Octet platform by Pall Life Sciences – ForteBio.\nMeasurement of enzyme activity is a common assay in life sciences and pre-clinical research. The time-resolved enzyme-dependent accumulation of a marker or product can be used to determine enzyme kinetics. A majority of plate readers have this capability.\nMicrobiologist, Dr. Jason Sylvan, Assistant Professor of Oceanography, Texas A&M University, who samples microbes from the depths of ocean, measures microbial enzyme activity using the portable Spark® microplate reader by Tecan that he carries with him on the ship. Read the full article here.\nFluorescent markers such as EGFP, YFP, mCherry and mTomato or fluorescent biosensors can be used to measure a variety of real-time cell-based activities, including, intracellular transport, protein signaling, receptor desensitization, migration, division, apoptosis, metabolism, differentiation, chemotaxis, transcription and translation.\nIn an exclusive interview with SelectScience, the founder of Montana Molecular, Anne Marie Quinn, shares how her team measures, in real time, multiple GPCR signaling pathways inside a living cell using the CLARIOstar microplate reader.\nFluorescent or luminescent markers are also used in pharmacological and drug discovery assays. These are used to investigate ligand binding and protein interactions. TTP Labtech’s ameon® lifetime reader incorporates fluorescence lifetime technology, enabling cost-effective screening of common drug targets as well as challenging epigenetic targets. Read about a novel nanoBRET assay developed in association with BMG LABTECH. In this SelectScience video, Dr. Kevin Pfleger, Associate Professor, University of Western Australia, describes how he uses nanoBRET technology along with the CLARIOstar and PHERAstar FSX microplate readers to develop high-throughput screening assays for developing GPCR-targeted drugs.\nBacterial cell counting can be determined by optical density at 600 nm, which indicates the growth phase for harvesting the cells. To determine confluence of basic cell culture, many modern microplate readers are equipped with a ‘confluence count’ setting. For example, this downloadable method compares the cell counting feature of the Spark multimode microplate reader with a GFP-labeled cell count.\nThe CLARIOstar microplate reader by BMG LABTECH is equipped with an advanced cell layer scanning capability. This enables up to 900 points to be measured in each well, a feature perfect for unevenly distributed cells.\nThere are many different commercially available cell-based assay kits that enable cell viability, proliferation and apoptosis to be measured. These include those for monitoring ATP, measuring caspase activity and detecting bromodeoxyuridine (BrdU). They are also used in toxicity testing in pre-clinical drug development.\nThese assays, applied to live cells, require a microplate reader to maintain appropriate environmental controls. BMG LABTECH’s CLARIOstar® comes with a module – the Atmospheric Control Unit (ACU) – for independent control of oxygen and carbon dioxide.\nRead about the monitoring of apoptosis and necrosis in real time using the CLARIOstar’s ACU feature, that maintains the appropriate O2 and CO2 environment for cells, while the RealTime-GloTM Annexin V Apoptosis and Necrosis Assay measures cell death using luminescence and fluorescent signals.\nIn live, real-time cell-based experiments, it is beneficial to read from the bottom of the microplate, rather than the top. Reading from the bottom offers several advantages for cell-based detection: the light collector can be placed closer to the sample and the cell layer adherent closer to the bottom of the well, which decreases light dissipation. Moreover, the interfering effect of the cell culture medium is significantly reduced. Both factors positively affect sensitivity. In addition, bottom reading allows for a cover or lid to be placed on top of the microplate to prevent cell contamination and liquid evaporation. This is particularly important in time-lapse experiments.\nSeveral readers are optimized for live-cell assays, including BMG LABTECH’s PHERAstar® FSX and CLARIOstar® readers, which use a proprietary Direct Optic Reading system to eliminate the need for fiber optics.\nLive-cell experiments involving translational research concepts, such as testing for ischemia-reperfusion in cells for research on stroke, involve a tight regulation of oxygenation as measurements are made in real time. Read an example of such a study using the ACU to control for oxygenation levels mimicking conditions of a stroke in vitro.\nCell migration in wound healing assays is a popular application using microplate readers. Learn how you can use a microplate reader to automatize the Radius 96-well Cell Migration Assay.\nRecently, there has been an increased drive to develop assays and readers for 3D cell cultures. These spheroids or cell clumps can be grown in multi-well plates and more closely mimic the endogenous environment, which is important in both life sciences and drug discovery applications. Watch the video with Dr. Kirk McManus, Associate Professor, University of Manitoba, and Senior Scientist, CancerCare Manitoba, to learn how temporal resolution of 3D tumor spheroids in colorectal cancer is obtained using the Cytation microplate reader by BioTek.\nIn another method, 3D heart muscle was reconstituted with cardiac myocytes derived from human induced pluripotent stem cells (iPSCs). Learn how calcium flux is measured every 0.01 seconds using the CLARIOstar®.\nPhenotypic screening is used routinely in drug discovery for the identification of substances that alter the phenotype of a cell or an organism. The target cells are screened with compounds or biopharmaceuticals to assess modulation of the activity of interest. In vitro phenotypic screening uses cell lines and cell-based assays and is often performed in high-content microplates and may utilize live cells.\nDeciphering cell signaling, especially for G-protein coupled receptors (GPCRs), forms the basis of discovering therapeutic targets. In fact, a predominant number of current drugs target the GPCR signaling. Upon ligand binding, the Gs-coupled GPCR receptor activates adenylyl cyclase that in turn produces cAMP, governing important cellular responses. Gi-coupled GPCR receptors, however, inhibit adenylyl cyclase and cAMP production. The phosphatidylinositol pathway, on the other hand, is triggered by the Gq-coupled receptors. In Gq-coupled signaling, DAG and intracellular calcium act as second messengers, ultimately influencing cell functions.\nMicroplate readers can be used to detect GPCR signal transduction in living cells. Two methods published by BMG LABTECH describe (1) real-time detection of Gs and Gi signaling and (2) detection of GPCR second messengers in living cells.\nDrug discovery and manufacturing applications often require high-content screening (HCS) and/or high-throughput screening (HTS). Most of the applications above can be conducted in high-content, multi-well plates in microplate readers. High-throughput drug screening for target validation and ADMET (absorption, distribution, metabolism, elimination and toxicity) is a key process in drug discovery. However, phenotypic cell-based assays are now becoming more commonplace. In addition to conventional assays, these readers enable phenotypic screening of cells, either by well or per cell. Multimode microplate readers have been designed specifically for this HTS combined with imaging.\nAmplified Luminescent Proximity Homogeneous Assay Screen (AlphaScreen)\nAlphaScreen is a versatile assay technology developed to measure analytes using a homogenous protocol that enables sensitive and precise interrogation of signaling pathways, receptors, enzymes and kinase targets, in a cell-based format. AlphaScreen has been particularly useful for screening GPCRs, growth factor receptors, intracellular MAPK inhibitors and many other signaling pathways.\nHomogeneous Time-Resolved Fluorescence (HTRF)\nUtilizing rare-earth lanthanides with long emission half-lives as donor fluorophores, HTRF technology combines standard FRET with the time-resolved measurement (TR) of fluorescence. HTRF is commonly used for GPCR and kinase screening, two of the most important target classes investigated within drug discovery. Other HTRF applications include discovery of new biomarkers, studies of protein-protein interactions, epigenetics and an alternative method for bioprocess monitoring.\nUsing the PHERAstar® FSX microplate reader by BMG LABTECH, the HTRF IP-One assay was performed where single clones were functionally screened using stably transfected recombinant CHO-M1 cells. Download the full method here.\nMicroplate readers/detectors are used for numerous applications and, as technologies evolve, even more will emerge. When purchasing a microplate reader/detector, it is important to establish what applications you will be using it for now, and in the future. You will want to ensure that your chosen microplate reader/detector is capable of fulfilling all your requirements but also consider how easily upgradable it is for your future needs.\nRequest a demo: Once you have considered both your application needs and the different technologies that are available, chances are that you will have narrowed down the choice to just a few microplate readers. Before going ahead and purchasing a microplate reader/detector, it is highly recommended that you try it out. Most manufacturers will let you demo the instrument for a trial period. This is a great opportunity to ensure the microplate reader fulfills all your requirements, is easy to use and to make sure you get the right support and training to use the instrument.\nTechnical support and warranty: Consider the extent of the technical support and training that is available from the manufacturer at the point on purchase. Is it included in the purchase cost? It is also recommended that you look into the company’s policy on after-sales service and standard warranty. While everything is operating well, this may be the last thing on your mind, but in the event that after-sales support is required, it is important to know how accessible that is.\nAccessories and add-ons: Not all microplate readers have the same standard features. Be sure to ask what is included with the reader at the time of demonstration. Optional items may be key parts of the kit, including computers to run the instrument, filters or filter cubes, software licenses, software upgrades, FDA-compliant software, environmental controls or injectors.\nSoftware: Microplate readers take hundreds or thousands of measurements, so data acquisition and analysis are key to efficient workflow. Make sure that you know how to use the software for your current needs and how to keep it updated. Find out how well it is supported by the manufacturer.\nCompatibility: A lab can have equipment from different manufacturers. Consider how well your chosen reader will integrate with your existing equipment and how easy it will be to upgrade in the future. Many models have a modular design and add-ons, but their compatibility may be brand limited.\nFuture needs: Taking current and future trends into consideration is essential in maximizing the lifespan of the microplate reader/detector. As systems become more sophisticated, integration, convenience and application specificity will be fundamental. Systems will become faster and more efficient and be able to run a larger variety of different assays. Application possibilities will increase as manufacturers work to provide solutions to individual and industry demands. Manufacturers are taking customer feedback and requirements into consideration, with many developing strategies for even greater specificity assays and technology for the future.\nWhether purchasing a microplate reader/detector for a new application or replacing an existing system, there are a number of factors to consider. You will need to examine your current and future application needs and determine which of the available technologies best suits these applications. Visit the SelectScience Microplate Readers / Detectors product directory for an overview of the latest products from leading manufacturers. Keep up-to-date with the latest techniques and advances in technology by visiting the SelectScience Microplate Readers / Detectors page in our life sciences community for interviews, application notes, videos and the latest news."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:df0b951f-8fff-42a8-a7e7-d840f360f83b>","<urn:uuid:24e83dfb-fb3b-4af0-85f6-1af186ef8a3b>"],"error":null}
{"question":"作为一名私人教练，我想了解Perfect Posture Push-Up和Ball Crunches哪个更适合训练核心肌群？请详细分析两者的目标肌群和训练效果。","answer":"Both exercises effectively target core muscles but in different ways. The Perfect Posture Push-Up trains the chest, shoulders, triceps and core, focusing on systemic strength and core stability. It requires maintaining contact points at the head, shoulder blades and tailbone throughout the movement, ensuring proper form and core engagement. The Ball Crunch specifically targets all three main abdominal groups: rectus abdominis, transverse abdominis, and internal/external obliques. EMG studies show the ball crunch increases abdominal muscle activation by 24-38% and engages additional stabilizing muscles throughout the body. The ball crunch also provides better support for the lower back and helps maintain proper form, making it particularly effective for isolated core training.","context":["In the strength and conditioning hierarchy, exercises like Squats, Deadlifts, Bench Presses and Cleans are considered elite. When performed correctly, they produce demonstrable gains in strength and power. That is indisputable. What is debatable is how suitable they are for weight room beginners.\nBefore embarking on a weight room routine, young athletes need to be prepared for intense demands. You can do this by establishing a certain baseline level of systemic strength and mobility. This is crucial, because improper exercise execution often leads to tight hip flexors and poor ankle mobility.\nYou need to hit the weight room with confidence. Do this by incorporating the following five exercises into your existing routine. Or do them all as a workout. Either way, before long you’re bound to notice major improvements in the way your body moves and feels.\nSupine Barbell Hip Bridge\nMuscles trained: gluteals, hamstrings and core.\nBenefits: improved knee stability, hip strength and mobility.\n- Lie on your back with your knees bent about 90 degrees\n- Place a barbell on the front of your hips, holding it in place with your hands\n- Begin by lightly bracing your core\n- Next, push through your heels until you lift your hips and lower torso off the ground (you can lift the balls of your feet off the ground to emphasize pushing with your glutes and hamstrings)\n- In the top position, your body should form a ramp from your shoulders to your knees\n- Hold for a second, then lower and repeat until you’ve completed 10-12 repetitions\nNote: You may want to place a towel across your hips for comfort.\nOnce you master this exercise, check out the more advanced Single-Leg Hip Thrusters, demonstrated by Tim Tebow.\nMuscles trained: middle and lower trapezuis, deltoids, external rotators and core.\nBenefits: improved thoracic extension, shoulder mobility and core strength, which can help reduce excessive strain on the lower back and rotator cuff.\n- Lie prone over a stability ball with your legs straight and chest up off the ball\n- Begin by bracing your core with your arms positioned outstretched in front of you (pinky side down/ thumbs up)\n- Bring your arms around behind you as far as you can without pain\n- As you do this, try and keep your thumbs pointed up towards the ceiling\n- Once you’ve reached your furthest point, bring your arms back to the starting position and repeat\n- Continue until you’ve preformed 10-12 repetitions\n- Watch a demonstration\nBulgarian Split Squat\nMuscles Trained: glutes, quadriceps, hamstrings and core.\nBenefits: improved hip mobility, lower body strength, balance and coordination.\n- Stand a couple of feet in front of an exercise bench\n- Begin by balancing on one leg as you place your back foot on the bench\n- Keeping your torso as upright as possible, lightly brace your core as you slowly lower your back knee toward the ground while simultaneously allowing your front knee to bend to about a 90-degree angle\n- Pause for a second in the bottom position, then push back up and repeat until you’ve completed 10-12 repetitions per side\n- Check out these pictures and videos of the Bulgarian Split Squat\nPerfect Posture Push-Up\nMuscles Trained: chest, shoulders, triceps and core.\nBenefits: improved systemic strength and core stability.\n- Get down into a Push-Up position. Place your hands just outside shoulder-width apart, “pack” your shoulder blades and brace your core and legs\n- As you hold this upright plank position, have a coach or training partner lie a lacrosse stick, hockey stick, or broomstick lengthwise along your spine. The stick should maintain contact with the back of your head, shoulder blades and tailbone as you execute the drill\n- Slowly lower yourself toward the ground with your elbows at about a 45-degree angle to your torso\n- Once you’re within a few inches of the ground, press back up to the starting position without changing your position or losing contact with the stick at any of the checkpoints\n- Do 10-12 repetitions per set\nMuscles trained: core, chest, shoulders and triceps\nBenefits: increased core stability and upper body strength.\nExecution: (Note: the Pallof Press can be done with either cables or resistance bands)\n- Stand aside a cable resistance machine or sturdy object with a resistance band attached to it\n- If using a band, your distance from the anchoring point will depend on your strength and the tension of the band\n- Once you have the appropriate tension (or weight set), begin by packing your shoulder blades and bracing your core as you press the resistance from directly in front of your chest with your arms bent, to the same point with your arms fully extended\n- The idea is to keep your core and lower body braced\n- Do not allow the resistance to move toward the anchoring point—just straight out and right back toward your chest\n- Perform 10-12 repetitions and then face the opposite way","Ball Crunches | Exercise Ball Crunch\nA common goal among most women is to have super tight, defined abs. But of course, that is no small feat. Who wouldn’t enjoy having flatter, more defined abs? Well, they’re not always easy to get. Getting really tight abs takes great detail to nutrition, a lot of hard work and great form with some of the best ab exercises for women. One of those exercises that I find to be the most effective is a traditional ball crunch. When done on a large stability ball or exercise ball, the abdominal crunch can be an important part of your weekly ab workout!\nUnderstanding Abdominal Anatomy\nFirst it is important to understand the muscles that make up your abdominal section. There are 3 main groups that comprise what is commonly referred to as the abs:\n1. Rectus Abdomens run down the center of the torso from the pubic bone up to the sternum which gives you that chiseled, six pack look when they are well developed and helps with pelvic tilt and spinal support.\n2. Transverse Abdominus which are the deepest layer of muscles in the core and provide stability to your core while they stabilize or hold in your internal organs. The transverse abdominus is often referred to as the corset in pilates and helps pull our stomach up and in.\n3. Internal and External Oblique are located on the sides of the core and help rotate the trunk. These muscles are usually targeted with ab exercises that involve twisting like Russian Twists and Bicycle Crunches\nHow to do Ball Crunches or the Stability Ball Crunch\n– Choose and exercise ball (stability ball) that is approximately the same height as your knees and that is very firm.\n– Sit down on the ball with your knees bent at a 90 degree angle.\n– Walk your feet out as you roll back, allowing your lower back to come to rest on the ball.\n– For a ball crunch, the arms can be crossed over your chest or for a more advanced exercise all crunch, you can extend your arms straight out over head.\n– Begin the movement by first engaging your core and tightening your transverse abdominus muscles. It takes conscious effort to draw those muscles in and to tighten them.\n– Then, using the rectus adbominus activate your abs bringing your shoulders and chest upward toward the ceiling. I like to choose a point in the ceiling to focus on directly over head to be sure that the neck stays in a neutral position.\n– With control, lower the shoulders and chest back down to touch the ball. Abs should remain tight throughout this abdominal exercise\nBut Why Use A Stability Ball for Crunches?\nSure, a crunch can effectively be preformed on the floor so why I do love adding in a stability ball (exercise ball) for this ab exercise for women? Well, the first reason is that a stability ball supports the lower back and maintains proper form throughout the movement while discouraging over extension of the lower back. With the ball there to support these crunches, you can avoid ‘throwing your back into it’ and arching to create momentum which puts your spine in a compromising position.\nSecond, stability and balance are important. By adding in the ball, you force the stabilizing muscles of the core to engage. If you move a traditional ab crunch up to an exercise ball crunch, that uneven surface engages those internal and external oblique muscles to help you stay a top the ball. This in turn gives you a more complete ab workout. Not just that though… with the unstable surface you begin recruiting muscles throughout your whole body including your legs, glutes, and spine.\nAre crunches on a stability ball better?\nBut does this really help you recruit more muscles fibers and improve activation? One study found that doing this exercise on a stability ball boosted activation of the abdominal muscles by between 24 to 38% based on EMG readings. EMG is a method that uses electrodes placed on the muscle to record muscle activation. All of the major ab muscles were activated, including the rectus abdominis, transverse abdominis, and the obliques. But, crunching on a stability ball also brings smaller muscles that stabilize the spine into action.\nOther Ab Exercises & Articles You Might Like\nThe Bottom Line\nBy adding the stability ball to your crunches, you will be able to engage your transverse abdominis and external obliques better than if you do this ab exercise on the floor or with a mat. It also allows for more stabilizing muscles to take part in the movement. You can still do regular or traditional crunches and ab sit ups but with the option of adding in the exercise ball, you can hit your abs in a slightly different and safer way. And while you are there, use that exercise ball for some other awesome ab exercises that really engage your body and help you build lean, tight, defined abs!\n- “Transversus Abdominis Muscle.” NeuroImage, Academic Press, www.sciencedirect.com/topics/veterinary-science-and-veterinary-medicine/transversus-abdominis-muscle.\n- “Functional Anatomy of the Core: the Abdomen.” Human Kinetics, us.humankinetics.com/blogs/excerpt/functional-anatomy-of-the-core-the-abdomen.\n- Exercise and Resting BP, www.unm.edu/~lkravitz/Article folder/abdominal.html."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b732a72d-a697-4762-a70c-20bd9f0abc82>","<urn:uuid:88d59c46-8ee4-4232-abb9-e28112a573e0>"],"error":null}
{"question":"I've been following respiratory disease trends, and I'm wondering about the differences in severity assessment between traditional flu pandemics and COVID-19 pneumonia. How do the classification systems and severity indicators differ between these two respiratory conditions?","answer":"Flu pandemics are assessed using the PSAF (Pandemic Severity Assessment Framework), which evaluates multiple criteria including transmissibility and clinical severity, with historical examples ranging from the devastating 1918 pandemic (2% mortality rate) to the 1957 H2N2 influenza. In contrast, COVID-19 pneumonia severity is classified into two distinct phenotypes: Type 1 (non-ARDS) with good respiratory mechanics but severe hypoxemia, and Type 2 (ARDS) with decreased pulmonary compliance. The severity indicators differ - flu pandemic assessment focuses on population-level impact and case fatality ratios, while COVID-19 severity is determined by individual physiological parameters like respiratory system compliance and response to PEEP.","context":["|Flu Viruses With Pandemic Potential - Credit CDC|\nBetween next Monday's 3 hour Smithsonian Livestream: “The Next Pandemic: Are We Prepared?\", next year's 100th anniversary of the deadliest influenza pandemic on record, and the recent rise in the number of novel flu viruses viewed to have pandemic potential (H7N9, H5N6, H5N8, etc.) - now seems a pretty good time to begin a new series on how we would (or wouldn't) cope during a severe pandemic.\nAdmittedly, not all pandemics are created equal - and as we saw in 2009 - their impact can vary widely. The CDC currently evaluates 14 novel flu viruses for their pandemic potential - ranking two H7N9 viruses (A & B in the chart above) at the top of their severity/impact list.There are other pandemic contenders - including MERS, SARS, Nipah, Pneumonic plague, and even Monkeypox - but novel influenza has the longest, and most worrisome track record.\nThe old Pandemic Severity Index (see graphic below) was adopted in the 2007 Community Strategy for Pandemic Influenza Mitigation plan as a way to quantify the likely impact of any pandemic outbreak. It was based on the initial CFR (Case Fatality Ratio) of the virus, and was modeled in many respects after the 5 category Saffir-Simpson wind scale used for hurricanes.\nLast April, in PSAF Is The New Pandemic Severity Index, we looked at the revised 2017 CDC/HHS Community Pandemic Mitigation Plan, which provided a new gauge of pandemic intensity based on multiple criteria; the PSAF (Pandemic Severity Assessment Framework).\nWhile the 1918 pandemic (upper right) is off by itself in terms of both transmissibility and clinical severity - and resulted in the deaths of between 50 and 100 million people - that isn't necessarily the top of the scale. It is just the worst we've seen in modern times.\nA moderately severe pandemic - like 1957's H2N2 influenza - could still kill hundreds of thousands of Americans, and millions worldwide.Even in 1918, most people who contracted the Spanish flu survived. Estimates are that up to 50% of the world's population fell ill, but in the United States and Europe the mortality rate was on the order of 2%, meaning that about 650,000 Americans (out of 100 million) perished.\nToday, with a population triple that of 1918, that number could approach two million.Reports from other parts of the globe, particularly India and Asia, suggested a much higher (10%-15%) toll in 1918, and low resource countries today would almost certainly suffer greater losses in a severe pandemic (see Are We Prepared to Help Low-Resource Populations Mitigate a Severe Pandemic?).\nWhile modern medicine - including antibiotics, antivirals, and ventilators - will likely have some impact on a severe pandemic, the expectation is that most medical facilities will be quickly overwhelmed, many HCWs will be out sick or taking care of family members, and medical interventions will be in short supply.\nThe supply chain, which keeps hospitals in IV fluids, disposable gowns, gloves, masks and respirators - and everything from aspirin to oxygen - will quickly falter. The same supply line collapse will likely mirror itself to some degree at your local grocery store and pharmacy.\nThe reality is that 90% of flu cases will never see the inside of a hospital. They will have to be treated at home, by family or friends (see Pandemic Solutions: Flu Buddies).While these challenges in our modern society may sound far-fetched, this is precisely the sort of scenario a study - published a couple of years ago in Clinical Infectious Diseases and written by researchers at our own CDC & HHS - envisioned in:\nEstimates of the demand for mechanical ventilation in the United States during an influenza pandemic.\nMeltzer MI1, Patel A2, Ajao A3, Nystrom SV4, Koonin LM5.\nAn outbreak in China in April 2013 of human illnesses due to avian influenza A(H7N9) virus provided reason for US public health officials to revisit existing national pandemic response plans. We built a spreadsheet model to examine the potential demand for invasive mechanical ventilation (excluding \"rescue therapy\" ventilation).\nWe considered scenarios of either 20% or 30% gross influenza clinical attack rate (CAR), with a \"low severity\" scenario with case fatality rates (CFR) of 0.05%-0.1%, or a \"high severity\" scenario (CFR: 0.25%-0.5%).\nWe used rates-of-influenza-related illness to calculate the numbers of potential clinical cases, hospitalizations, admissions to intensive care units, and need for mechanical ventilation. We assumed 10 days ventilator use per ventilated patient, 13% of total ventilator demand will occur at peak, and a 33.7% weighted average mortality risk while on a ventilator.\nAt peak, for a 20% CAR, low severity scenario, an additional 7000 to 11,000 ventilators will be needed, averting a pandemic total of 35,000 to 55,000 deaths.\nA 30% CAR, high severity scenario, will need approximately 35,000 to 60,500 additional ventilators, averting a pandemic total 178,000 to 308,000 deaths.\nEstimates of deaths averted may not be realized because successful ventilation also depends on sufficient numbers of suitably trained staff, needed supplies (eg, drugs, reliable oxygen sources, suction apparatus, circuits, and monitoring equipment) and timely ability to match access to ventilators with critically ill cases. There is a clear challenge to plan and prepare to meet demands for mechanical ventilators for a future severe pandemic.\nPublished by Oxford University Press on behalf of the Infectious Diseases Society of America 2015. This work is written by (a) US Government employee(s) and is in the public domain in the US.Worth noting, the `severe' scenario presented - a 30% attack rate and a CFR of .5% - is nowhere near the severity seen in 1918 (CFR of 2%), and could potentially be dwarfed by an avian flu pandemic with even a higher attack or mortality rate.\nEven so, their scenario finds without the unlikely rapid addition of 35,000 to 60,500 ventilators (and trained staff, oxygen, drugs, and electricity to run them), 200,000 to 300,000 Americans would die for the lack of ventilator resources.And this opens up the thorny problem of who gets a ventilator, and for how long? When do you withdraw ventilator support from one patient in order to give it to another? Who makes these decisions? And how will the public react to the heartbreaking realities of triage during a pandemic?\nThese are topics we visited repeatedly back in 2008 (see Ventilator Triage During A Pandemic , Triage In A Pandemic , Fear And Loathing Of Pandemic Triage, and The Allocation of Medical Resources) and while new guidelines have been published, answers remain murky.\nThe CDC's 2011 document Ethical Considerations for Decision Making Regarding Allocation of Mechanical Ventilators during a Severe Influenza Pandemic or Other Public Health Emergency spends 27 pages looking at a myriad of legal and ethical considerations, but concludes that `policy decisions need to be set and implemented by the responsible public health officials.'In 2015 the State of New York released a 266 page manual called VENTILATOR ALLOCATION GUIDELINES, which is broken up into Adult, Pediatric, and Neonatal sections. While this massive document probes deeply into the complexity of triage, it admits:\nWhile the Adult Guidelines developed by the Task Force and the 2006 and 2009 Adult Clinical Workgroups assist a triage officer/committee as they evaluate potential patients for ventilator therapy, decisions regarding treatment should be made on an individual (patient) basis, and all relevant clinical factors should be considered.\nA triage decision is not performed in a vacuum; instead, it is an adaptive process, based on fluctuating resources and the overall health of a patient. Examining each patient within the context of his/her health status and of available resources provides a more flexible decision-making process, which results in a fair, equitable plan that saves the most lives.Adding to the confusion, standards and guidelines will vary from one state to the next, and none have the benefit of having ever been tested under real pandemic conditions. Not to disparage the work that has gone into them, but the old adage that `No battle plan survives contact with the enemy' comes to mind.\nThese are issues that as a society, we don't tend to think about. We assume there will be a hospital bed, or ventilator, or medicines available to treat us if we need it. But in a severe pandemic we could find our medical infrastructure seriously overwhelmed, and those things we take for granted quickly unavailable.\nOne take away is the very last job you would want to take on in a severe pandemic is that of a hospital's triage officer, or member of a triage committee. And yet, many people will be called upon to make those horrendously difficult decisions.While you may be one of the lucky 50%-70% who are not infected, or among the 90% who only experience mild or moderate illness, a severe pandemic could still compromise your access to medical care. Routine surgeries, cancer treatments, dialysis, regular doctor's office visits and getting routine prescriptions from the pharmacy could all be severely affected.\nIn the months to come we'll look at other possible impacts of a severe pandemic on our society. In the meantime, now would be a very good time to talk with family, friends, and neighbors on how you would deal - and help one another - with a pandemic (or any other disaster) should it come our way.\nFor more on all of this, you may wish to revisit:","- Open Access\nCOVID-19 pneumonia: ARDS or not?\nCritical Care volume 24, Article number: 154 (2020)\nEven though it can meet the ARDS Berlin definition [1, 2], the COVID-19 pneumonia is a specific disease with peculiar phenotypes. Its main characteristic is the dissociation between the severity of the hypoxemia and the maintenance of relatively good respiratory mechanics. Indeed, the median respiratory system compliance is usually around 50 ml/cmH2O. Of note, the patients with respiratory compliance lower or higher than the median value experience hypoxemia of similar severity. We propose the presence of two types of patients (“non-ARDS,” type 1, and ARDS, type 2) with different pathophysiology. When presenting at the hospital, type 1 and type 2 patients are clearly distinguishable by CT scan (Fig. 1). If the CT scan is not available, the respiratory system compliance and possibly the response to PEEP are the only imperfect surrogates we may suggest.\nType 1: Near normal pulmonary compliance with isolated viral pneumonia\nIn these patients, severe hypoxemia is associated with respiratory system compliance > 50 ml/cmH2O. The lung’s gas volume is high, the recruitability is minimal, and the hypoxemia is likely due to the loss of hypoxic pulmonary vasoconstriction and impaired regulation of pulmonary blood flow. Therefore, severe hypoxemia is primarily due to ventilation/perfusion (VA/Q) mismatch. High PEEP and prone positioning do not improve oxygenation through recruitment of collapsed areas, but redistribute pulmonary perfusion, improving the VA/Q relationship. Lung CT scans in those patients confirm that there are no significant areas to recruit, but the right-to-left venous admixture is typically around 50%.\nType 2: Decreased pulmonary compliance\nIn 20–30% of these COVID-19 patients admitted to the intensive care unit (ICU), severe hypoxemia is associated with compliance values < 40 ml/cmH2O, indicating severe ARDS . It is certainly possible that their lower compliance (i.e., lower gas volume and increased recruitability) is due to the natural evolution of the disease, but we cannot exclude the possibility that this severity of damage (increased edema) results in part from the initial respiratory management. Indeed, some of these hypoxemic patients receive CPAP or non-invasive ventilation before ICU admission and present with very high respiratory drives, vigorous inspiratory efforts, and highly negative intrathoracic pressures. Therefore, in addition to viral pneumonia, those patients likely have self-inflicted ventilator-induced lung injury .\nBefore ICU, in non-intubated patients\nCPAP and NIV are the first-line treatment when an overwhelming number of patients come to a hospital. These interventions, often applied outside the ICU in emergency rooms or in other medicine wards, usually improve blood oxygenation. A key aspect of care, however, should be the assessment of respiratory drive and the inspiratory efforts. The ideal indicator would be the measurement of the esophageal pressure swings. If impossible, the clinical signs of inspiratory efforts should be carefully scrutinized. If respiratory distress is present, endotracheal intubation should be strongly considered to avoid/limit the transition from type 1 to type 2 by self-induced lung injury.\nIn ICU, intubated patients\nIn type 2 patients, a lower tidal volume should be applied. However, type 1 patients lack the low compliance/high driving pressure prerequisites of ventilator-induced lung injury, even if treated with volumes higher than 6 ml/kg delivered at respiratory rates of 15–20 breaths/min . More liberal tidal volume (7–8 ml/kg) often attenuates dyspnea and may avoid hypoventilation with possible reabsorption atelectasis and hypercapnia.\nThe type 1 patients lack the prerequisite for higher PEEP to work (recruitability). PEEP levels should be limited at 8–10 cmH2O, since higher levels will decrease pulmonary compliance and can impact right heart function. The type 2 patients are characterized by a reduction of total gas volume and an increase in lung weight and edema. These features may be due to the natural progression of the disease, to bacterial superinfection and/or to self-induced lung injury during the period preceding the intubation. In these patients, a cautious gradual increase of PEEP up to 14–15 cmH2O may be beneficial. A decrease in SvO2 during this phase suggests an inadequate cardiac output so that higher PEEP levels for lung recruitment may no longer be useful. Cardiac ultrasound may also be useful for assessing right heart function when increasing PEEP levels.\nCalculating the shunt fraction is the best tool to assess oxygenation.\nThe etCO2/PaCO2 relationship is a useful tool to quantify efficiency of pulmonary exchange. A ratio < 1 suggests elevated shunt and dead space (areas of lung ventilated and not perfused).\nFor type 2 patients, prone position could be used as a long-term treatment—as in any form of severe ARDS [6, 7]. However, in type 1 patients, prone positioning should be considered more as a rescue maneuver to facilitate the redistribution of pulmonary blood flow, rather than for opening collapsed areas. Long-term prone positioning/supine cycles is of very little benefit in patients with high lung compliance, and it leads to high levels of stress and fatigue in the personnel.\nThe oxygenation response to NO is variable. The COVID-19 pneumonia appears to interfere with the vascular regulation up to complete loss of vascular tone to vasoconstricting or vasodilating agents. We still do not have enough evidence to understand when and on which patients it should be applied. Nitric oxide should not work in fully vasoplegic patients (type 1 in our model) but possibly works in patients in which pulmonary hypertension is more likely (type 2 in our model).\n(Micro)thrombosis and D-dimer levels\nIn this disease, thrombosis and associated ischemic events are very common. A daily check of coagulation parameters, in particular D-dimer levels, should be performed in both the type 1 and the type 2 patients, judiciously anticoagulated when indicated.\nType 1 patients:\nPEEP levels should be kept lower in patients with high pulmonary compliance\nTidal volume thresholds should not be limited at 6 ml/kg\nRespiratory rate should not exceed 20 breaths/min\nPatients should be left “quiet”; avoiding doing too much is of higher benefit than intervening at any cost.\nType 2 patients:\nStandard treatment for severe ARDS should be applied (lower tidal volume, prone positioning, and relatively high PEEP).\nAvailability of data and materials\nForce* TADT: Acute respiratory distress syndrome: the Berlin definition. JAMA. 2012;307(23):2526–33.\nForce ARDSDT, Ranieri VM, Rubenfeld GD, Thompson BT, Ferguson ND, Caldwell E, Fan E, Camporota L, Slutsky AS. Acute respiratory distress syndrome: the Berlin definition. JAMA. 2012.\nMaiolo G, Collino F, Vasques F, Rapetti F, Tonetti T, Romitti F, Cressoni M, Chiumello D, Moerer O, Herrmann P, et al. Reclassifying acute respiratory distress syndrome. Am J Respir Crit Care Med. 2018;197(12):1586–95.\nBrochard L, Slutsky A, Pesenti A. Mechanical ventilation to minimize progression of lung injury in acute respiratory failure. Am J Respir Crit Care Med. 2017;195(4):438–42.\nAcute Respiratory Distress Syndrome N, Brower RG, Matthay MA, Morris A, Schoenfeld D, Thompson BT, Wheeler A. Ventilation with lower tidal volumes as compared with traditional tidal volumes for acute lung injury and the acute respiratory distress syndrome. N Engl J Med. 2000;342(18):1301–8.\nGattinoni L, Taccone P, Carlesso E, Marini JJ. Prone position in acute respiratory distress syndrome. Rationale, indications, and limits. Am J Respir Crit Care Med. 2013;188(11):1286–93.\nGuerin C, Reignier J, Richard JC. Prone positioning in the acute respiratory distress syndrome. N Engl J Med. 2013;369(10):980–1.\nEthics approval and consent to participate\nConsent for publication\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nAbout this article\nCite this article\nGattinoni, L., Chiumello, D. & Rossi, S. COVID-19 pneumonia: ARDS or not?. Crit Care 24, 154 (2020). https://doi.org/10.1186/s13054-020-02880-z\n- Mechanical ventilation"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:bf5cf5f4-e46f-4ca7-ab3f-40b10dedafd9>","<urn:uuid:bdc98c62-7969-40f2-a74d-8547895bd56f>"],"error":null}
{"question":"How do the symptoms and recovery timelines compare between flu and mono?","answer":"The flu and mono have some overlapping initial symptoms but different progression patterns. Flu typically causes fever, muscle aches, runny nose, sore throat, and fatigue, with recovery taking 1-2 weeks with proper rest and hydration. Mono also causes fever and sore throat, but its symptoms are typically more severe and long-lasting. Mono's sore throat is particularly intense in the first 5-7 days, with swollen lymph nodes persisting for about three weeks. While flu symptoms generally resolve within two weeks, mono can cause fatigue that persists for months after the acute infection phase. Additionally, mono often causes specific complications like enlarged spleen and liver inflammation, which are not typical of flu.","context":["Fever, muscle aches, runny nose, sore throat, and fatigue — flu symptoms can leave you feeling so bad that they stop you right in your tracks.\nUnlike the common cold, the flu is a serious respiratory disease that can lead to potentially dangerous complications, according to the Centers for Disease Control and Prevention (CDC). If you do become infected, it’s important to take steps to prevent spreading the disease to others and keep your own symptoms from getting worse.\nWhat Can I Do to Ease Flu Symptoms?\nHow do you treat the flu at home? Since the flu is caused by a virus, taking antibiotics won’t help. It may take a week or two to get back on your feet, advises the Department of Health and Human Services (HHS).\nIn the meantime, there are several things you can do to make yourself more comfortable, says Christopher Ohl, MD, professor of infectious diseases at Wake Forest Baptist Medical Center in Winston-Salem, North Carolina.\n“Most people who get the flu can help ease their symptoms by getting plenty of rest and staying well hydrated with fluids,” Dr. Ohl says. Drinking water, broth, sports drinks, or electrolyte beverages can prevent you from becoming dehydrated. Allowing yourself to rest, especially while you have a fever, also gives your body the strength it needs to fight the virus.\nOhl adds that taking an over-the-counter anti-fever medicine, such as Tylenol (acetaminophen), or a nonsteroidal anti-inflammatory drug (NSAID), such as Advil and Motrin (ibuprofen) or Aleve (naproxen), can also help reduce aches and pains.\nOther nonprescription medication, such as decongestants, cough suppressants, and expectorants, can help treat various flu symptoms. But make sure you’re not using more than one medication with the same active ingredient, to avoid a potentially dangerous overdose, the HHS cautions.\nNever give medication with aspirin to children or teenagers with a virus. Aspirin has been linked to a serious illness known as Reye's syndrome in young people who are recovering from a virus, like the flu or chicken pox, according to the American Academy of Pediatrics.\nIt’s also important to stay home from work or school until you are fever-free for at least 24 hours, to avoid spreading the flu to others, Ohl says.\nThe CDC and the HHS have these additional tips for managing your flu symptoms:\n- Apply a cool, damp washcloth to your forehead, arms, and legs to reduce discomfort associated with fever.\n- Put a humidifier in your room to make breathing easier.\n- Gargle with warm salt water to soothe a sore throat.\n- Stop smoking, which could worsen your flu symptoms, and avoid any secondhand smoke.\n- Avoid drinking alcohol while you are recovering from the flu, to prevent dehydration.\n- If you must go out to visit the doctor or receive medical care, wear a face mask or cover your mouth and nose with a tissue when you cough and sneeze.\n- Wash your hands often to prevent spreading the flu to others.\nCan Antiviral Medication Help Treat the Flu?\nYour doctor may prescribe an antiviral medication to help treat the flu. Antiviral medication can shorten the length of the illness and make symptoms milder. They can also reduce your risk of flu complications, which is why these drugs are often recommended for people who are at high risk, such as children, older adults, and people with certain pre-existing conditions.\nAntivirals come in the form of pills, liquid medication, and inhaled medication. They prevent the virus from spreading in your body.\nThe following antiviral drugs are recommended for treating the flu in the United States:\n- Tamiflu (oseltamivir) is a medication that's approved to prevent and treat certain flu infections in people as young as 2 weeks old.\n- Relenza (zanamivir) is approved to treat certain flu infections in people 7 years and older, and prevent certain flu infections in people 5 and older. This inhaled powder should not be taken by anyone with a respiratory condition, such as asthma or chronic obstructive pulmonary disease (COPD).\n- Rapivab (peramivir) is approved to treat adults with the flu. It's given intravenously for 15 to 30 minutes.\n“Antiviral medication might shorten the duration of symptoms somewhat, but in order for them to be effective, they need to be started in the first two days of illness,” Ohl notes. If you think you’ve been exposed to the flu, contact your doctor as soon as possible after getting sick to find out if antivirals are an option for you. Depending on the flu season, your doctor may recommend one antiviral medication over another.\nThe best flu treatment is prevention. Getting an annual flu shot is the first line of defense against flu viruses. These viruses are constantly changing, so the vaccine is updated based on which flu viruses are making people sick, how they are spreading, and how well the previous year’s vaccine protected against these viruses.\nAdditional reporting by Mary Elizabeth Dallas.","Things to know about mono (infectious mononucleosis)\nAn Epstein-Barr virus is the most common cause of infectious mono although there are other illnesses that produce similar symptoms. This infectious disease can be spread by saliva, and the incubation period for mono is 4-8 weeks.\n- Mono (infectious mononucleosis) is a contagious illness typically caused by the Epstein-Barr virus (EBV).\n- This infectious disease can be spread by saliva, and the incubation period for mono is 4-8 weeks. Using contaminated items, such as drinking glasses or toothbrushes, can spread the infection.\n- Most adults have laboratory evidence (antibodies against the Epstein-Barr virus) indicative of a previous infection with EBV and are immune to further infection.\n- The symptoms (clinical manifestations) of mono include\n- The diagnosis of mono is confirmed by blood tests.\n- Mono can cause liver inflammation (hepatitis) and enlargement of the spleen.\n- Vigorous contact sports should be avoided during the illness and recovery phase to prevent the rupture of the spleen.\n- The long-term prognosis for most people with mono is excellent, and severe complications are rare.\nSore Throat: Is It Mono?\nHaving a sore throat can be a symptom of many conditions, and many people wonder if their own sore throat might be a sign of something more serious than\nthe common cold. Specifically, infectious mononucleosis (\"mono\") and infection\nwith Streptococcus bacteria (\"strep throat\") are two conditions that both\nproduce an extremely painful sore throat.\nWhat is mono (infectious mononucleosis)?\nMono is easily spread through casual contact. Among teens, kissing is the most common means of transmitting mono since the virus is present in saliva.\nInfectious mononucleosis, \"mono,\" \"kissing disease,\" and glandular fever are all terms popularly used for the very common infection typically caused by the Epstein-Barr virus (EBV), but other viruses can also cause the disease. This article focuses specifically on the Epstein-Barr virus as a cause of mono since this is the characteristic virus associated with the condition.\nThe symptoms of Epstein-Barr virus infection include fever, fatigue, malaise, and sore throat. The designation \"mononucleosis\" refers to an increase in a particular type of mononuclear white blood cells (lymphocytes) in the bloodstream relative to the other white blood cells as a result of the viral infection. Scientifically, EBV is classified as a member of the herpes virus family.\nThe disease was first described in 1889 and was referred to as \"Drüsenfieber,\" or glandular fever. The term infectious mononucleosis was first used in 1920 when an increased number of lymphocytes were found in the blood of a group of college students who had fever and symptoms of the condition.\nSymptoms of Mono: Infectious Mononucleosis Treatment\nWhat are the symptoms of mono?\nInitial symptoms can last from 1-3 days before the more intense symptoms of the illness begin.\nThe initial symptoms of mono in adults are\nThese initial symptoms can last from one to three days before the more intense symptoms of the illness begin. The more common intense symptoms include\n- a severe sore throat and\n- fever, which may be persistent.\nIt is typically the severe sore throat that prompts people to contact their doctor.\nWhat are the signs of mono?\nIn addition to a fever from 102 F-104 F, the most common signs of mono are\n- a very reddened throat and tonsils (tonsillitis) and\n- swollen lymph nodes in the neck that typically occur on both sides.\n- The tonsils have a whitish coating in at least one-third of the cases.\n- The spleen (sometimes referred to as the body's biggest lymph node) is an organ found in the left upper abdomen underneath the rib cage, which becomes enlarged or swollen in about half of patients with mono.\n- An enlarged liver and abnormalities in liver function tests (blood tests) may be detected (see Complications, below).\n- Some patients have a splotchy red rash over the body, which has a similar appearance to the rash of measles.\n- Early in the course of the disease (over the first few days of illness), a temporary swelling (edema) of both upper eyelids may appear.\nWhat is the cause of mono?\nThe EBV that causes mono is found throughout the world. By the time most people reach adulthood, antibodies against EBV can be detected in their blood. In the U.S., up to 95% of adults 35-40 years of age have antibodies directed against EBV. This means that most people, sometime in their lives, have been infected with EBV.\n- The body's immune system produces antibodies to attack and help destroy invading viruses and bacteria.\n- These specific EBV antibodies can be detected in the blood of people who have been infected with mono.\nWhen infection occurs in childhood, the virus most often produces no symptoms. It is estimated that only about 10% of children who become infected with EBV develop the illness. Likewise, probably because of immunity from prior infection, adults typically do not develop the illness. Most cases of infectious mononucleosis occur in the 15-24 age group.\nWhile there are other illnesses falling under the broad classification of mononucleosis that can cause similar symptoms (cytomegalovirus [CMV] infection is one example) and an increase in blood lymphocytes, the mononucleosis caused by EBV is by far the most common.\nWhat are the risk factors for mono?\nEBV can infect anyone. As previously discussed, the majority of people have become infected with the virus by the time they reach adulthood, and the majority of these infections produce no symptoms and are not recognized as mono.\n- Mono is most often diagnosed in adolescents and young adults, with a peak incidence at 15-17 years of age.\n- However, it can also be seen in children.\n- Generally, the illness is less severe in young children and may mimic the symptoms of other common childhood illnesses, which may explain why it is less commonly diagnosed or recognized in this younger age group.\nWhat is the contagious period for mono?\nMono is spread by person-to-person contact. Saliva is the primary method of transmitting mono, which leads to the infection of B lymphocytes in the mouth and throat. Infectious mononucleosis developed its common name of \"kissing disease\" from this prevalent form of transmission among teenagers. It typically takes between four to eight weeks for people to become symptomatic after the initial Epstein-Barr virus infection. A person with mono can also pass the disease by coughing or sneezing, causing small droplets of infected saliva and/or mucus to be suspended in the air which can be inhaled by others. Sharing food or beverages from the same container or utensil can also transfer the virus from one person to another since contact with infected saliva may result.\nMost people have been exposed to the virus as children, and as a result of the exposure, they have developed immunity to the virus. It is of note that most people who are exposed to EBV don't ever develop mononucleosis. The incubation period for mono, meaning the time from the initial viral infection until the appearance of clinical symptoms, is between four and eight weeks. During an infection, the contagious period in which a person is likely able to transmit the virus to others lasts for at least a few weeks and possibly longer, even after symptoms have disappeared (see below).\nResearch has shown that depending on the method used to detect the virus, anywhere from 20%-80% of people who have had mononucleosis and have recovered will continue to secrete the EBV in their saliva for years due to periodic \"reactivation\" of the viral infection. Since healthy people without symptoms also secrete the virus during reactivation episodes throughout their lifetime, isolation of people infected with EBV is not necessary. It is currently believed that these healthy people, who nevertheless secrete EBV particles, are the primary reservoir for the transmission of EBV among humans. Therefore, it can be difficult to precisely determine how long the infection may be contagious.\nSee pictures of Bacterial Skin Conditions\nWhat tests do health care professionals use to diagnose infectious mono?\nThe diagnosis of mono is suspected by the doctor based on the above symptoms and signs. Mono is confirmed by blood tests that may also include tests to exclude other possible causes of the symptoms, such as tests to rule out strep throat. Early in the course of the mono, blood tests may show an increase in one type of white blood cell (lymphocyte). Some of these increased lymphocytes have an unusual appearance (known as atypical lymphocytes) when viewed under a microscope, which suggests mono.\nMore specific blood tests, such as the monospot and heterophile antibody tests, can confirm the diagnosis of mono. These tests rely on the body's immune system to make measurable antibodies against EBV. Unfortunately, the antibodies may not become detectable until the second or third weeks of the illness. A blood chemistry test may reveal inflammation and abnormalities in liver function. Diagnostic tests performed in the laboratory may be of value to rule out other causes of sore throat and fever, including cytomegalovirus infection, strep throat, and less common conditions such as acute HIV infection or toxoplasmosis.\nWhat health care specialists treat infectious mono?\nInfectious mono is often managed by primary care specialists, including pediatricians and family medicine specialists. Internal medicine specialists also treat patients with mono. With complications or severe situations, other medical specialists including\n- infectious-disease specialists,\n- gastroenterologist, or\n- neurologists may be consulted.\nWith certain clinical complications such as rupture of the spleen, a surgeon will be involved in the patient's care.\nSubscribe to MedicineNet's General Health Newsletter\nWhat is the usual course and treatment of mono?\nIn most cases of mono, no specific medical treatment is necessary. The illness is usually self-limited and passes much the way other common viral illnesses resolve. Treatment is directed toward the relief of clinical symptoms and signs. Available antiviral drugs have no significant effect on the overall outcome of mono and may actually prolong the course of the illness.\nOccasionally, strep throat occurs in conjunction with mono and is best treated with penicillin or erythromycin (E-Mycin, Eryc, Ery-Tab, PCE, Pediazole, Ilosone).\n- Ampicillin (Omnipen, Polycillin, Principen) and amoxicillin (Amoxil, DisperMox, Trimox) should be avoided if there is a possibility of mono since up to 90% of patients with mono develop a rash when taking these medications. If this happens, the individuals may then be inappropriately thought to have an allergy to penicillin.\nFor the most part, supportive or comfort measures are all that are necessary. Antiviral medications have not been shown to be of benefit.\n- Acetaminophen (Tylenol) or ibuprofen (Advil) can be given for fever and any headache or body aches.\n- A sufficient amount of sleep and rest is important.\n- The throat soreness is worst during the first five to seven days of illness and then subsides over the next seven to 10 days.\n- The swollen, tender lymph nodes generally subside by the third week.\nA feeling of fatigue or tiredness may persist for months following the acute infection phase of the illness. It is recommended that patients with mono avoid participation in any contact sports for three to four weeks after the onset of symptoms to prevent trauma to the enlarged spleen.\n- The enlarged spleen is susceptible to rupture, which can be life-threatening.\n- Cortisone medication is occasionally given for the treatment of severely swollen tonsils or throat tissues that threaten to obstruct breathing.\nPatients can continue to have virus particles present in their saliva for as long as 18 months after the initial infection.\n- When symptoms persist for more than six months, the condition is frequently called \"chronic\" EBV infection or \"chronic mononucleosis.\" However, laboratory tests generally cannot confirm continued active EBV infection in people with chronic EBV symptoms.\nInfectious Disease Resources\nHealth Solutions From Our Sponsors\nWhat are the complications of mono?\nA common, but usually not serious, a complication of mono is a mild inflammation of the liver or hepatitis. This form of hepatitis is rarely serious or requires treatment. It generally resolves on its own as the condition improves.\n- The enlargement of the spleen that occurs with mono makes traumatic rupture of the spleen a possible complication.\n- Swelling of the throat and tonsils can also lead to airway obstruction when severe. Infection in the area of the tonsil can rarely become a serious abscess referred to as a peritonsillar abscess.\nFortunately, the more severe complications of mono are quite rare, and mono is very rarely fatal in healthy people. The rare severe complications include\nMono tends to be more aggressive in patients with abnormal immune systems, such as people with AIDS or those who are taking medications that suppress immune function.\nThe EBV has been associated with some types of cancers, most commonly lymphomas. This occurs most frequently in people whose immune systems have been compromised due to disease or immune-suppressive drugs. EBV infection has also been found to be associated with two types of cancer that are sometimes found in other cultures are:\n- nasopharyngeal carcinoma (cancer of the pharynx and nose) in southern China and\n- Burkitt's lymphoma of the jaw among children in equatorial Africa.\nFurther, numerous studies have also found that EBV infection is associated with the development of at least one subtype of Hodgkin's disease. However, since the vast majority of people have been infected with EBV and never develop these types of tumors, EBV infection cannot be the sole cause of these cancers. The overwhelming majority of people who have had mono recover completely without any serious complications.\nWhat is the prognosis of mono?\nMost people with mono recover completely with no long-term problems. The fatigue associated with the condition may persist for a few months after the fever and other symptoms have resolved. Severe complications as described above are very rare.\nInfectious mononucleosis is usually a self-limited, although sometimes prolonged, and often uncomfortable illness. While specific treatment is rarely necessary, the potential complications make it essential that people with this illness be under the care of a physician.\nIs it possible to prevent mono?\nSince mono is spread from person to person, avoiding close personal contact with infected individuals and practicing excellent hygienic practices can help prevent transmission of the virus. This includes avoiding sharing contaminated utensils like toothbrushes and drinking glasses. However, since periodic reactivation of the virus infection seems to occur in healthy individuals and because many infected people who may transmit the virus to others will not have symptoms of the condition, prevention is extremely difficult. In fact, these individuals without symptoms are believed to be the primary source of transmission of the virus.\nThe fact that up to 95% of adults have antibodies to EBV suggests that prevention of the infection is difficult if not impossible. It is not known why some people develop the symptoms of mono while others appear to acquire the EBV infection without having symptoms. It is possible that many infections occur and produce mild symptoms and are not recognized as mono, while other infections may not produce symptoms at all.\nMedically Reviewed on 4/20/2022\nMacsween, K.F., C.D. Higgins, K.A. McAulay, H. Williams, N. Harrison, A.J. Swerdlow, and D.H. Crawford. \"Infectious Mononucleosis in University Students in the United Kingdom: Evaluation of the Clinical Features and Consequences of the Disease.\" Clin Infect Dis 50.5 (2010): 699.\nShetty, Kartika. \"Epstein-Barr Virus (EBV) Infectious Mononucleosis (Mono).\" Medscape. Apr. 21, 2021. <http://emedicine.medscape.com/article/222040-overview>.\nUnited States. Centers for Disease Control and Prevention. \"Epstein-Barr Virus and Infectious Mononucleosis.\" Sept. 28, 2020. <https://www.cdc.gov/epstein-barr/index.html>."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:c0eca199-d455-4a71-8cc6-21205575b96a>","<urn:uuid:c6f9f8ae-c38f-473b-9a3a-10d466f2a0ee>"],"error":null}
{"question":"How do internal and external factors differ between competitive analysis and SWOT analysis?","answer":"In competitive analysis, internal factors focus on examining topics covered, tone and voice, and messaging strategy, while external factors involve analyzing third-party coverage and target audience. In contrast, SWOT analysis clearly divides factors into internal elements (strengths and weaknesses) that are within company control, such as resources and expertise, and external elements (opportunities and threats) beyond company control, such as market conditions and competition. While competitive analysis emphasizes understanding competitor strategies, SWOT analysis aims to evaluate both internal capabilities and external market factors to develop strategic plans.","context":["A social media competitive analysis is a powerful tool that is commonly used to create a set of benchmarks against competitors within your industry, and by freelancers and agencies to help demonstrate how effective their guidance has made their clients in comparison to the key players in their industry.\nHowever, you don't have to stop there. A competitive analysis of similar social media channels can also teach you a lot about your competitors' strategy, goals, and who exactly they're marketing to.\nOnce you have this deeper understanding of your competitors, you can compare their strategy to your own and examine what they are doing effectively (and not so effectively), why that is, and whether this presents an opportunity for you.\nSo, what should you be doing to take your competitive analysis to the next level so you can gain deeper insights from what your competitors are doing? Just use these 5 questions as a checklist when you're doing your next competitive analysis.\n1. Which topics do they cover?\nWhat are your competitors talking about most? Their wider industry, their own company, common questions from customers, or something more conversational?\nThe key things you want to understand here are:\n- Which topics are working well for them, and are those relevant to your strategy?\n- Why is that content working well for them? Do they seem to be promoting certain content to drive more clicks and engagement? Is there a certain topic that drives more useful interactions than others?\n- Is there something unique about how they're positioning their content in their posts? Are their posts short and to the point, or maybe they start out with an exciting title? Do they use a lot of eye catching images that stand out in the feed on their embedded links?\nPro Tip: Be on the lookout for bitly links. If one isn't in the post text, hover over the embedded link to see if it was posted using a link shortner. Bitly links, along with some other link shortners as well, expose analytics data on how many clicks a link received to the public, just add a \"+\" to the end of the link to see this data.\nThis data is also segmented by how many clicks were driven through all bitly links to that page, and how many were driven from the specific link you found.\n2. What is their tone and voice?\nOne mistake many marketers make when doing a social media competitive analysis is to focus too much on tactics. You also want to take a high level look at the tone and voice your competitors and brands that are similar to yours are using.\nYou want to be on the lookout for the type of language they use:\n- Is it full of jargon and written for someone who already has a lot of knowledge of their space?\n- Or is it written to help someone with almost no knowledge of their company, products, or industry get up to speed quickly on what they're talking about?\n- Is the language they use focused on education, showcasing their company as a great place to work, selling their products and services, or telling the company's story?\nPro Tip: There's a lot more to a brand's voice and tone than what I've covered above. If you'd like the deep dive on what voice and tone are, how to classify them, and why they matter, this explanation of social media voice and tone from Buffer breaks it all down for you.\nThese insights will provide clues to help you figure out the answer to your next question...\n3. Who are they targeting?\nBy examining the topics your rivals are talking about and the tone and voice which they use to approach these subjects you can figure out who they're targeting and also whether a similar strategy makes sense for you.\n- For a B2C brand very simple language with limited jargon could be targeted towards consumers who are unfamiliar with the products and services the company offers.\n- For a B2B brand this type of content could be targeted towards someone in the C-suite who needs a high-level overview of the problem and solutions without getting into too many of the technical details.\n- The topics and how they're covered will give you more clues. If the content they share is at a very high level this suggests they're writing for senior managers or consumers that aren't familiar with their offerings yet, if the content dives deep into the topic, this is another clue that they're targeting consumers who are already well-educated about their products and services (perhaps people who are further along in the buyer's journey?), or for a B2B brand, they could be targeting individual contributors who do the hands on work and have an in-depth knowledge of how the work related to their products and services is done.\nPro Tip: You shouldn't assume that one brand's audience will be the same for every social media platform. The same brand may target people at different levels of experience across platforms or even use two platforms for completely different goals, such as reaching customers on Facebook or LinkedIn and showcasing their unique work culture and courting potential recruits on Twitter or Instagram.\n4. How are they being covered by third parties?\nFor many brands their content strategy extends beyond their own social media channels.\nWhether this is a relationship with smaller bloggers and influencers who are talking about their offerings to their own targeted audiences or coverage in top-tier media sources, it's worth taking a look to understand how your competitors are being positioned by the media or how they're positioning themselves through guest blogging or influencer marketing.\nYou should try to figure out:\n- What are they using third parties to promote? Is it the CEO/owner's vision for the company, a flagship product, a wider discussion of their industry, or something else?\n- How does this fit into their overall strategy as you understand it so far? Does this complement it, fill a gap, or does it overlap in some way? At which stage in the buyer's journey is this content written for? Who is it written for?\n- What type of media outlets are talking about them and how are they getting this coverage? Is it industry bloggers or large media outlets? Do they have a PR team that is collaborating with their social team? Are they doing outreach to influencers via social media?\nPro Tip: Pay attention to the authors of third party content written about a brand. If it isn't apparent who they work for, take a second to look them up on LinkedIn. The author isn't always a third party, it could also be a guest post by someone who works for that company. This is also interesting, but you will want to separate your analysis of guest posts on other blogs and niche sites from how the company is covered by outside sources.\n5. What is their message?\nThe brands with the best content strategies will have a consistent message that fits into an overall narrative about that company, their brand story.\n- This message will often relate to the company's history, for B2B brands it's often how they got started in their industry, what their qualifying background was before the company was launched, and what they offer that makes their team unique and the best choice. It could also extend to the company's culture and highlighting the teams that work within the company, especially if a brand is leveraging social media to showcase their company in hopes of attracting the best and brightest recruits in their industry.\n- For a B2C brand this message may relate more to the product they're selling and the type of lifestyle it provides than the company and the people behind the product.\n- Pay attention to how effectively each brand conveys their messaging and brand story across their website, blog content, and social media properties. While you definitely don't want to duplicate their story and messaging (it just wouldn't be authentic or unique), you may find some inspiration for how you could be better positioning your own brand by looking at competitors and best in class brands that are similar to your own company.\nPro Tip: You may not see the immediate tactical value in this type of analysis, but developing a consistent message and helping your customers identify with who your company is, what they do, and what your team is passionate about are all very important ingredients for building customer loyalty.\nDon't neglect forming your own consistent, unique, and authentic message that your customers will identify with.","Importance of SWOT Analysis in Developing a Marketing StrategyOften viewed as a key step related to planning, SWOT analysis is deceptively simple despite the immense value it delivers. The system combines information from the environmental analysis and separates it into two components: internal issues (strengths and weaknesses) and external issues (opportunities and threats).\nThis level of analysis enables an organization to determine whether there are factors present that will aid in the achievement of specific objectives (due to an existing strength or opportunity) or if there are obstacles that must be overcome before the desired outcome can be realized (due to weaknesses or threats).\nWhat is SWOT analysis?As mentioned above, the process of SWOT analysis evaluates your company's strengths, weaknesses, market opportunities and potential threats to provide competitive insight into the potential and critical issues that impact the overall success of the business. Further, the primary goal of a SWOT analysis is to identify and assign all significant factors that could positively or negatively impact success to one of the four categories, providing an objective and in-depth look at your business.\nHighly useful for developing and confirming your organizational goals, each of the four categories provides specific insights that can be used to cultivate a successful marketing strategy, including:\n- Strengths - Positive attributes internal to your organization and within your control. Strengths often encompass resources, competitive advantages, the positive aspects of those within your workforce and the aspects related to your business that you do particularly well, focusing on all the internal components that add value or offer you a competitive advantage.\n- Weaknesses - Factors that are within your control yet detract from your ability to obtain or maintain a competitive edge such as limited expertise, lack of resources, limited access to skills or technology, substandard services or poor physical location. Weaknesses encapsulate the negative internal aspects to your business that diminish the overall value your products or services provide. This category can be extremely helpful in providing an organizational assessment, provided you focus on an accurate identification of your company's weaknesses.\n- Opportunities - Summary of the external factors that represent the motivation for your business to exist and prosper within the marketplace. These factors include the specific opportunities existing within your market that provide a benefit, including market growth, lifestyle changes, resolution of current problems or the basic ability to offer a higher degree of value in relation to your competitors to promote an increase in demand for your products or services. One element to be aware of is timing. For example, are the opportunities you're catering to ongoing or is there a limited window of opportunity?\n- Threats - External factors beyond the control of your organization that have the potential to place your marketing strategy, or the entire business, at risk. The primary and ever-present threat is competition. However, other threats can include unsustainable price increases by suppliers, increased government regulation, economic downturns, negative press coverage, shifts in consumer behavior or the introduction of \"leap-frog\" technology that leaves your products or services obsolete. Though these forces are external and therefore beyond your control, SWOT analysis may also aid in the creation of a contingency plan that will enable you to quickly and effectively address these issues should they arise.\nTurning SWOT Analysis into a Strategic PlanOnce you've established specific values related to your business offerings within the four quadrants of SWOT analysis, you can develop a strategic plan based on the information you've learned. For example, once you've identified your inherent strengths, you can leverage them to pursue the opportunities best suited to your organization, effectively reducing potential vulnerability related to threats. In the same way, by identifying your organization's weaknesses with regard to external threats, you can devise a plan that will enable you to eliminate or minimize them while improving defensive strategies related to your offerings.\nIt's important to remember that SWOT analysis can be influenced (and often quite strongly) by those who perform the analysis. So it's a good idea to have an outside business consultant review the results to provide the most objective plan."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:70c47eaf-6025-4a8c-8635-09cd2ceff142>","<urn:uuid:381154b7-8779-41cd-b945-151a9403f2be>"],"error":null}
{"question":"Which contains more gluten: active dry yeast bread flour or cake flour?","answer":"Cake flour contains significantly less gluten than bread flour. Bread flour is high-protein and gives a higher-gluten end result, while cake flour is low-protein, containing only 7 to 9 percent gluten by weight.","context":["Bread Baking 101\nBaking bread is a lot easier than it seems; while it's certainly possible to get deep into the scientific reasons for why certain breads end up certain ways, it's also possible to have a delicious, homemade loaf without worrying about the whys.\nBread baking is its own world, different from baking cookies, cakes or pies. Bread's unique combination of ingredients — flour, yeast, water, salt and, most importantly, time — mean that you've got to approach it differently.\nFirst and foremost, bread is alive. Its characteristic texture comes from fermentation, which is the basic activity of yeast, a single-celled fungus (the good kind, not the scary kind). Yeast reacts with the sugars in flour to create carbon dioxide (among other things), which leads to the airy, bubbly texture and taste we associate with bread.\nIf you get really into bread making, you can delve deeply into the art and science of the perfect loaf — there's no limit to the level of obsession passionate bread bakers can have. But in the meantime, here are some frequently asked questions to solve your baking 101 dilemmas.\nFrequently Asked Questions\nWhat yeast should I buy?\nThere are a number of kinds of yeast, all of which work wonderfully for bread. Our recipes call primarily for active dry, as it's readily available and convenient to use. A brief rundown of the types:\nActive dry: You'll usually see this in small packages in the dairy section. It needs to hang out in warm water for a few minutes before you use it — this wakes the yeast up and gets it ready for your recipe. Check expiration dates on these, and keep your packets in a cool, dry spot.\nRapid rise (sometimes called instant): This is a hardy strain of yeast, and does not need to be hydrated before using. While it doesn't actually rise more rapidly than any other yeast, you get to skip the step of hydrating, making the process a couple minutes faster. It is also more concentrated than active dry yeast so you'll get a fuller rise in some recipes than with the same amount of active yeast. We use it in our Challah and potato rolls.\nFresh yeast: This is a favorite of hardcore bakers. As much as we love it, we don't recommend it for our recipes because it's hard to find, and needs to be used up pretty quickly once you buy it. If you do have access to fresh yeast, use 0.6 ounces per packet of active dry in any given recipe.\nSourdough: Sourdough comes from a starter, which is basically an active yeast culture. You can make it on your own (by combining organic flour, water and sugar, and allowing it to ferment) and keep it indefinitely. While it takes longer than yeast, it pays off in flavor.\nDoes the flour I use make a difference?\nYes, it does. Different flours have differing levels of protein, which affects the texture of your finished product. When you bake bread, protein turns into gluten strands, which form a web to hold in the carbon dioxide from the yeast. The more gluten present, the firmer the bread.\nCake flour is low-protein, and gives you a softer texture and a lower-gluten end result.\nBread flour is high-protein, and gives you a firmer texture and a higher-gluten end result.\nWheat flours make for denser bread; potatoes or potato flour make for tender bread. That said, most of our recipes here use all-purpose. You can absolutely make great bread with what's already in your pantry.\nSalt not only sharpens and brightens the flavor in baked goods and helps prevent staleness — it's also invaluable for gluten structure and even browning. But where it's most important is its interaction with yeast. Salt helps slow the rise of yeasted baked goods, leading to an even, stable texture. Be careful not to add salt directly to yeast when you're hydrating it; it'll make for a less-risen bread.\nWhat is proofing the yeast?\nWhen you use active dry yeast, you need to \"wake it up\" by hydrating it. Add it to warm water (about 110 degrees F is ideal) — if it's alive, it'll start to get foamy. Add sugar to that to feed the yeast, and you're proofing it.\nWhy do you knead bread?\nKneading incorporates the flour and liquid ingredients while helping create the gluten structure that establishes the bread's final texture. Great bread can be made either by hand or in a machine. When kneading, don't push so hard that you tear the dough, or knead so long that the dough gets taut. Soft and supple is the way to go.\nWhere should I proof my dough?\nIn a warm, but not a hot, spot about 70 to 80 degrees. Generally, the kitchen counter is fine. If your kitchen is drafty, then inside the oven (turned off) or the microwave works too. If it's too warm in your kitchen, find a cooler room.\nPutting the dough in the fridge slows the rise, which helps develop flavor and makes the dough more manageable (like with our cinnamon buns). If you get called away for some reason when proofing a dough, refrigerate it until you get back.\nWhat is punching down a dough?\nIt's pushing down the dough, with your hand or fist, to help release the air the yeast has produced. You do this after the dough has doubled in size.\nWhen do I know my dough has proofed enough?\nYour dough should look fuller and doubled in size. If it's tight and dense, let it proof longer; if it's airy and about to collapse, then it's gone too far.\nWhen I tried to form the dough into a roll, pizza, or loaf, it sprang right back — what should I do?\nDoughs can get overworked and be tough to form. If the dough is taut, simply cover with a towel and let the dough relax for about 10 minutes.\nWhat are a poolish and sponge?\nBoth poolish and sponge are pre-fermentation methods — they start the leavening process earlier, ending up with a deeper, nuttier flavor and better, more even texture.\nThey're both blends of flour, water and yeast, set aside to ferment in advance — overnight for a poolish, a few hours for a sponge — then mixed in with the remaining ingredients and baked as usual. We use a poolish starter for our multigrain bread because the extended fermentation helps break down the flours, making for a more tender loaf.\nHow do I know when my bread is done?\nBread is done when it's about 190 degrees F inside. If may seem odd to take a loaf's temperature, but it is a sure way to know if it's ready.\nHow should I store my bread?\nAnywhere but the fridge. Well-wrapped in the freezer works, as does a breadbox, or simply a plastic bag at room temperature.","Gluten Percentages in Cake Flour Vs. All-Purpose Flour\nAll-purpose flour is one of the standard pantry staples, a versatile jack-of-all-trades ingredient that can be used for most kinds of baking. Your local supermarket also stocks many other varieties, such as cake flour, that are superior to all-purpose flour for specific purposes. One of the primary differences between all-purpose flour and a specialty product such as cake flour is the percentage of gluten-forming proteins each contains. Cake flour is the lowest in gluten of all the major flour types.\nThe Quick Comparison\nSeveral types of hard and soft wheat are grown in the United States, and they're blended in varying proportions to give flour a suitable set of characteristics. Hard flours are high in gluten, while soft flours are low. Blended from soft wheat varieties, cake flour typically contains only 7 percent to 9 percent gluten by weight. The all-purpose flour sold in most of the United States contains more hard wheat and can range from as low as 8 percent gluten to as high as 11 percent. As a rule, all-purpose flour has less gluten in the South and Pacific Northwest -- 8 percent to 10 percent -- while national brands and brands from other regions contain 10 percent to 11 percent gluten.\nWhy It Matters\nThose varying percentages of gluten have a great deal to do with how the flour performs in your kitchen. Gluten forms long, stretchy chains of protein when the flour is moistened and stirred or kneaded. This gives the dough or batter a chewy, elastic texture, highly desirable in breads and rolls but a flaw in delicate cakes and pastries. Millers attempt to hit a middle ground with all-purpose flour, giving it enough gluten to make decent bread but not so much that cakes or pastries are impossible. The lower gluten in cake flour, and its other special qualities, enables both experienced and unskilled bakers to make better cakes.\nClose Up With Cake Flour\nCake flour is a highly specialized product, and its low gluten level is only part of the reason for its pastry-making prowess. It's milled more finely than other flours, and this tiny particle size helps it mix more readily. Then the flour is subjected to a very strong chlorine bleaching process, which leaves starch granules in the flour porous and absorbent. Both moisture and fats are able to adhere better to cake flour, so the flour itself acts as a mild emulsifier and helps create a smooth, fine-textured batter.\nIf you don't have cake flour at your disposal, a long-standing baker's tactic is to substitute cornstarch for part of the all-purpose flour. Because cornstarch is naturally gluten-free, this effectively lowers the percentage of gluten in your recipe. If you'd like to increase the protein in your diet or lower your carbohydrate consumption, you can achieve a similar effect with other substitutions. For example almond flour is low in carbohydrates and high in protein, and so is coconut flour.\n- North American Millers' Association: Types of Flour\n- On Food and Cooking: The Science and Lore of the Kitchen; Harold McGee\n- Joy of Baking: Baking Ingredient Substitution Table\n- Le Cordon Bleu: Five Alternatives to Wheat Flour"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:e60d46ac-a426-4c54-bb0e-2e07e086f86e>","<urn:uuid:7cd25275-5884-4013-9a85-133223857df4>"],"error":null}
{"question":"How do the challenges faced by indigenous peoples globally compare to those faced by the Kuoy people in Cambodia?","answer":"Both globally and in Cambodia, indigenous peoples face similar challenges related to land dispossession and resource exploitation. Globally, there are approximately 370 million indigenous people who have historically been dispossessed of their lands and struggle for access to valuable resources. Similarly, the Kuoy indigenous people in Cambodia have had their lands cleared and converted into sugarcane plantations by Heng Fu Group Sugar Industry, without proper consultation. In both cases, indigenous peoples face discrimination from authorities - globally, many governments routinely violate indigenous rights, while in Cambodia, local authorities accused the Kuoy of illegal settlement on government land when they protested the land seizures. Both situations also involve the exploitation of natural resources - globally through biopiracy and logging projects, and in Cambodia through economic land concessions that have affected indigenous peoples' farmlands, forests and water resources.","context":["Author and Page information\n- This page: http://www.globalissues.org/article/693/rights-of-indigenous-people.\n- To print all information e.g. expanded side notes, shows alternative links, use the print version:\nThere are approximately 370 million indigenous people spanning 70 countries, worldwide. Historically they have often been dispossessed of their lands, or in the center of conflict for access to valuable resources because of where they live, or, in yet other cases, struggling to live the way they would like. Indeed, indigenous people are often amongst the most disadvantaged people in the world.\nThis web page has the following sub-sections:\n- Who are indigenous people and what makes them different?\n- Conflicting Issues such as Environment, “Biopiracy”\n- Indigenous people have often had many rights denied\n- UN Declaration on Rights of Indigenous Peoples\n- Customary Law—backward or relevant justice systems?\n- Indigenous Peoples’ Struggle Around The World\n- More Information\nWho are indigenous people and what makes them different?\nThere does not seem to be one definitive definition of indigenous people, but generally indigenous people are those that have historically belonged to a particular region or country, before its colonization or transformation into a nation state, and may have different—often unique—cultural, linguistic, traditional, and other characteristics to those of the dominant culture of that region or state. (For more details, see this fact sheet from the United Nations Permanent Forum on Indigenous Issues (UNPFII).)\nIn some parts of the world, they are very few indigenous people, while in other parts, they may number into the hundreds of thousands, even millions. Over the years, many groups of people have been wiped out, either by diseases of colonizing peoples, or through policies of extermination.\nThose indigenous societies that remain today are predominantly subsistence-based (i.e. farming or hunting for food for immediate use), and non-urbanized, sometimes nomadic.\nConflicting Issues such as Environment, “Biopiracy”\nSome people have been critical of indigenous peoples’ treatment of the environment, noting examples such as the deforestation of Easter Island or the disappearance of large animals from parts of America and Australia caused by native people.\nHowever, others have argued that more generally, many indigenous people, for decades—even centuries—have accumulated important knowledge and traditions that allow them to work with nature rather than destroy it, because they are dependent on it and thus have a sense of interdependence. (See for example, works by Indian scientist and activist, Vandana Shiva.)\nIn other parts of the world, such as India, Brazil, Thailand, and Malaysia, multinational companies have been accused of participating in “biopiracy” whereby biological resources used by communities openly for generations (decades, centuries, or even millennia in some cases) have been patented away, leaving the local people unable to use their own local plants and other resources. This is discussed further on this site’s article, Food Patents—Stealing Indigenous Knowledge?. For other indigenous people, logging, dam projects and other activities threaten ways of life, sometimes leading to conflict.\nIndigenous people have often had many rights denied\nAs the UNPFII notes,\nIndigenous peoples around the world have sought recognition of their identities, their ways of life and their right to traditional lands, territories and natural resources; yet throughout history, their rights have been violated. Indigenous peoples are arguably among the most disadvantaged and vulnerable groups of people in the world today. The international community now recognizes that special measures are required to protect the rights of the world’s indigenous peoples.\n— About UNPFII/History, United Nations Permanent Forum on Indigenous Issues (UNFPII), accessed October 16, 2006\nIndigenous people have often found their lands and cultures overridden by more dominant societies. During the era of European colonial expansion and imperialism, it was common for Europeans to think of themselves as more superior over others.\nMany Europeans at that time saw native peoples from regions such as Africa, Asia and the Americas as “primitives,” or “savages” to be dominated. This would help justify settlement and expansion into those lands, and even slavery. Without civilization these people could be regarded as inferior, and if seen as “non-people” then European colonialists would not be impeding on anyone else’s territory. Instead, they would be settling “virgin territory” (sometimes “discovered”) overcoming numerous challenges they would face with much courage.\nOther Europeans saw the same people as perhaps savages, but ones that could be “saved” by being civilized and introduced to Christ. Hence, many European Christian missionaries saw their goal as “civilizing the savages.” (Some of these attitudes still prevail though perhaps not as forthright, or even intentionally, as popular literature of that time that would have depicted non Europeans as inferior or at least to be feared, are still celebrated today. See works by Edward Said for more on this, such as the classic Orientalism (Vintage Books, 1979), and Culture and Imperialism, (Vintage Books, 1993).)\nToday, celebrations of days such as Columbus Day in the US therefore raise bitter feelings for indigenous people. Interestingly, Christopher Columbus never set foot in the United States, though that day is celebrated there. (Democracy Now! radio show discusses explores this issue in more detail looking at the theme of power and dominance ideology that underpins why this day would still be celebrated.) For people of color and especially native American Indians, Columbus Day causes anger as they object to honoring a man who opened the door to European colonization, the exploitation of native peoples and the slave trade.\nMany Europeans and their descendants around the world have tried to look back at history and ask how it was that Europe and the West prospered and rose to such prominence. The late Professor J.M. Blaut accused many historians and others of employing self-congratulation and projecting eurocentric world views, whereby reasons for Europe’s rise were (and still are) attributed to things like favorable conditions for agriculture, for democracy to grow, and for economic superiority to take hold. Race was sometimes claimed to be a factor, too.\nBlaut was critical of these and other underlying assumptions and belief systems that guided this view, showing many assumptions to be false, and suggested instead that colonialism and the “discovery” and exploitation of the Americas, with the plunder of silver, gold and other resources helped fund a European rise.\nBlaut’s work is presented in two books (though a third was never finished for he passed away), part of a volume called The Colonizer’s Model of the World. His two books are Geographical Diffusionism and Eurocentric History (Guilford Press, 1993), and Eight Eurocentric Historians, by J.M. Blaut (Guilford Press, 2000).\nIt was after World War I and II that movements for indigenous rights starting gaining more traction. Witnessing the immense destruction, violence and barbarism of those wars, colonized people began questioning the European claim that their civilizations were superior and peaceful. Weakened European countries could no longer hold on to their colonies, and a wave of anti-colonial and nationalist movements sprung up as people around the world saw their chance to break free. European countries began conceding territories, and for many indigenous groups, accepted that they should have more rights to determine their own destiny.\nUnder international law, tribal people, for example, do have some recognized rights. The two most important laws about tribal peoples are Conventions 107 and 169 under the International Labor Organization (ILO), part of the UN system.\nSurvival International, a prominent organization that presses for the rights of tribal peoples, summarizes that\nThese conventions obliges governments to identify the lands and protect these rights… It ensures recognition of tribal peoples’ cultural and social practices, obliges governments to consult with tribal peoples about laws affecting them, guarantees respect for tribal peoples’ customs, and calls for protection of their natural resources.\n— International Law, Survival International, undated\nThe struggle for such rights is still not over. Many governments routinely violate the rights of indigenous people. A slow process is, however, raising hope for a more comprehensive set of rights, although some major countries are still against some particular aspects.\nUN Declaration on Rights of Indigenous Peoples\nAfter taking more than 20 years to draft and agree, on June 29, 2006, the United Nations Human Rights Council adopted the U.N. Draft Declaration on the Rights of Indigenous Peoples.\nThe Declaration emphasizes the right of indigenous peoples to maintain and strengthen their own institutions, cultures and traditions and to pursue their development in accordance with their aspirations and needs.\nAlthough it would not be legally binding if it were ever adopted by the General Assembly, indigenous communities around the world have pressed hard for this and have felt that the adoption of the declaration will help indigenous people in their efforts against discrimination, racism, oppression, marginalization and exploitation.\nMajor Countries Opposed to Various Rights for Indigenous Peoples\nThe process to draft the aforementioned declaration moved very slowly, not because of some imagined slowness and inefficiencies of an over-sized bureaucracy, but because of concerns expressed by particular countries at some of the core provisions of the draft declaration, especially the right to self-determination of indigenous peoples and the control over natural resources existing on indigenous peoples’ traditional lands.\nSome historically and currently powerful countries have been opposed to various rights and provisions for indigenous peoples, because of the implications to their territory, or because it would tacitly recognize they have been involved in major injustices during periods of colonialism and imperialism. Giving such people’s the ability to regain some lost land, for example, would be politically explosive.\nInter Press Service (IPS) notes, for example, that countries such as the United States, Australia, and New Zealand, have all been opposed to this declaration. These countries have noted in a joint statement that “No government can accept the notion of creating different classes of citizens.”\nFurthermore, as IPS also noted, the delegation claimed that the indigenous land claims ignore current reality “by appearing to require the recognition to lands now lawfully owned by other citizens.”\nThe problem with the delegations’ views are that they ignore historical reality. To say that “creating different classes of citizens” is objectionable does sound fair. However, in this case, different classes were created from the very beginning as indigenous people were cleared off their lands and either treated as second class citizens, or, not even considered to be citizens in the first place. Many of these laws then, were often made by a society that never recognized or accepted that such people had rights, and so the law only applied to the new dominant society, not the original people.\nThere are of course complications to this. For example, there is often a contentious debate about whether some European settlers colonized land that was not inhabited before, or were used by nomadic people, in which case European settlers could argue (from their perspective) that the land was not properly settled. Also, European settlers can also note that sometimes agreements were made with indigenous people to obtain certain lands, but it is also contentious as to whether all these agreements would have been made fairly, as some were made at gun point, while other agreements were achieved through deception and various forms of manipulation.\nSurvival International criticizes Britain and France, of being opposed to some aspects of rights for indigenous peoples, as well as the United States. These two countries, formerly commanding vast empires and colonies have also subjected native peoples to cruel denial of rights and oppression.\nA key part of the declaration has been the “collective” right of indigenous peoples, for they are seen by many indigenous communities as “essential for the integrity, survival and well-being of our distinct nations and communities. They are inseparably linked to our cultures, spirituality and worldviews. They are also critical to the exercise and enjoyment of the rights of indigenous individuals.” (Letter from 40 indigenous peoples’ organizations to Tony Blair, September 2004, quoted by the above-mentioned article from Survival International.)\nA reason such countries may be opposed to collective rights is that it implies land and resource rights, whereas supporting only individual rights would not. Collective rights could therefore threaten access to valuable resources if they cannot be exploited, or if they are used for, and by, the indigenous communities.\nAs Survival International also notes, individual rights is sometimes an alien concept to some societies, and it can be easier to exploit individuals than a collective people:\nFull collective rights over land and resources are essential for the survival of tribal peoples. The Yanomami of Amazonia, for example, live in large communal houses called yanos. The concept of ‘individual ownership’ of such a building is nonsensical. A tribe’s right to decide, for example, whether a mining company should be allowed to operate on its land, also only makes sense as a collective right. The UK claims, however, that these vital collective rights should be individual rights ‘exercised collectively.’ In the USA, the infamous Dawes Act of 1887 demonstrated the danger of this approach. The Act turned communally-held Indian lands into individual plots; 90 million acres of Indian land were removed at a stroke, and the reservations were broken up.\n— UK Government blocks historic UN Declaration, Survival International, February 1, 2005\nWhen interviewed in the above-mentioned IPS article, Stephen Corry, director of Survival International noted,\nThe imperial era was largely based on the dispossession of most of the world’s indigenous people … It cannot be considered over until the world accepts these peoples’ rights.\n— Stephen Corry, Director of Survival International, interviewed by Haider Rizvi, UN Faces Test on Native Rights, Inter Press Service, October 13, 2006\nIndigenous Peoples’ Struggle Around The World\nThe International Work Group for Indigenous Affairs (IWGIA) has for years worked on these issues. Their world reports detail issues and struggles for indigenous people around the world. Their 570-page report for 2006, The Indigenous World 2006 , for example, details the following areas:\n- The Circumpolar North\n- The Arctic Council\n- Sápmi in Norway and Finland\n- Alaska (USA)\n- Arctic Canada\n- North America\n- United States of America\n- Mexico and Central America\n- South America\n- Australia and the Pacific\n- The Islands of the Pacific\n- West Papua\n- East and Southeast Asia\n- South Asia\n- Middle East\n- The Marsh Dwellers of Iraq\n- The Bedouins of Israel\n- North and West Africa\n- The Amazigh people of Morocco\n- The Touareg People\n- The Horn of Africa and East Africa\n- Central Africa, Cameroon and Gabon\n- The Democratic Republic of Congo (DRC)\n- Republic of Congo (Congo Brazaville)\n- Southern Africa\n- South Africa\nThe above only scratches at the surface of the issues. For more detail, consider the following as useful starting points:\n- United Nations Permanent Forum on Indigenous Issues\n- Survival International\n- Inter Press Service’s coverage of indigenous peoples’ issues\n- From OneWorld.net:\n- International Work Group for Indigenous Affairs","“Prame is our home,” 60-year-old Tep Toem told PANAP during a visit to their community. “Before us, it was our parents’, grandparents’, and great grandparents’ home.”\nWhen PAN Asia Pacific (PANAP) first met newly married couple Khum Rany and Hean Jin, the two were busy building their own traditional house on stilt with the help of their family, friends, and neighbors, along the national highway in Chhaeb district in the province of Preah Vihear. They now live a few kilometers from Prame commune in Tbeang Meanchey district, where both were born and raised as members of the Kuoy indigenous community, and where both their parents still live. Just across from their new house is the sugar mill and refinery owned by Chinese firm Heng Fu Group Sugar Industry Co., Ltd. (Heng Fu). It is reportedly one of the largest in Asia and was inaugurated by none other than the Prime Minister of Cambodia in April last year.\nRany and Hean Jin did not randomly select the location of their new house. For the couple, it is a political statement; their “everyday form of resistance” against the company that cleared the five-hectare rice and vegetable farm of Rany’s family and converted it, along with other Kuoy lands, as well as part of their sacred Prey Preah Rokar forest, into a massive sugarcane plantation.\nHeng Fu is a Guandong-based company which mainly produces diversified sugar products. In 2011, it was granted economic land concessions (ELCs) over more than 42,000 hectares of land in Preah Vihear by the Royal Government of Cambodia (RGC). It opened its US$360-million mill and refinery in 2016 with the aim to supply sugar to markets in the European Union (EU), India, and China.\nThe Kuoy people of Prame, as well as the other affected communities in the province, were never consulted on the project. They only learned about the ELCs when the company started clearing their farms and forest in 2012. When they confronted the company, local authorities accused them of illegal settlement on government-owned land.\nEconomic Land Concessions: Cambodia’s current regime of dispossession\nMost Cambodians do not have titles over the land they occupy and cultivate as a legacy of the Khmer Rouge’s policy against private property in the late 1970s. About 75-80 percent of land in the country is currently classified as “state land” and under the management of the RGC. The promulgation of the Land Law in 2001 and of Subdecree No. 146 on Economic Land Concession in 2005 allowed the RGC to reclassify “state land” into “state private land,” and lease them to domestic and foreign individuals and firms for agricultural and industrial exploitation.\nUnder Subdecree No. 146, ELCs can be awarded over up to 10,000 hectares of land for up to 99 years. Heng Fu, however, was able to circumvent the law and acquire more than three times the allowed maximum size of land by registering under five different companies — Heng Nong (Cambodia) International Co., Ltd., Heng Rui, Heng You, Lan Feng, and, Rui Feng.\nHeng Fu was not the first company to go around the law. All over Cambodia, individuals and companies with ties to the national government were able to acquire tens of thousands of hectares of land as ELCs. According to the human rights group Cambodian League for the Promotion and Defense of Human Rights (LICADHO), an estimated two million hectares of land in the country are currently under 274 ELCs — 114 of which were granted to locals and 136 to foreigners (12 are categorized as “others”; 12 as “unknown”). Of the 136, Chinese firms hold 42 ELCs (over 356,560 hectares of land), while Vietnamese and Malaysian firms hold 55 (369,107 hectares) and 12 (90,844 hectares) ELCs, respectively. The number of ELCs has grown unprecedentedly in the last decade that the group declared Cambodia to be “in the grips of a prolonged land grabbing crisis, a slow-motion calamity.”\nELCs are promoted by the RGC as supposed drivers of economic growth and poverty reduction, especially in rural Cambodia. Everywhere in the country, however, ELCs have become synonymous with the physical or socio-economic displacement of people living in the concession areas. They are mostly farmers, fisherfolk, and indigenous peoples who have been occupying ancestral lands for centuries. Aside from not being consulted, affected communities in most cases were inadequately compensated, and, in cases of physical dislocation, not given proper resettlement. Their eviction from their homes and the clearing of their land were usually done with the aid of the local police, the military, and/or private security, and even the slightest resistance was often met with violence.\nThe socio-economic impact of ELCs\nIn Preah Vihear, local advocacy group Ponlok Khmer estimated that the ELCs granted to Heng Fu have affected more than 20,000 people in the agricultural districts of Chhaeb, Chey Sen, and Tbeng Meanchey. The RGC’s so-called “leopard skin” policy is supposedly in effect in the concession area, i.e., existing communities inside the more than 42,000 hectares of land leased to Heng Fu are protected by the state from physical displacement. The company, however, has encroached on the communities’ farmlands, forest, and water resources, at the expense not only of their sources of livelihood, but of their ways of life.\n“Prame is our home,” 60-year-old Tep Toem told PANAP during a visit to their community earlier this month. “Before us, it was our parents’, grandparents’, and great grandparents’ home.”\n“Before the company came to clear our lands, nobody in Prame ever experienced hunger,” said her 45-year-old neighbor Lan Sa Morn. “We have our farms for rice, the forest for crops, and the river and streams for fish. Now our farms and forest have been cleared, our river and streams either poisoned or dried up, our pasture lands barren. Now, many are struggling to provide food for their families.”\nMost of the people from the affected communities in Preah Vihear do not want to work for Heng Fu but some are forced to do so out of the need to feed their families. If given a choice between a job at the company and their land, the people said they would choose to go back to cultivating their farms.\nYoung farmers such as 30-year-old Chhum Sophin of Chhoak Chey district believes that agriculture is a sustainable source of living. “Our family farm’s harvest used to sustain us for one whole year. My parents were able to send us to school with the income they get from rice farming and forest harvesting,” he said. In fact, most of the affected communities in Preah Vihear were quite prosperous before the company came to disrupt their lives.\nThe Preah Vihear people’s fight goes on\nThe Preah Vihear people’s fight is sustained through the strong involvement of and solidarity between the affected communities in their bid to put a stop to Heng Fu’s operations in the province. The people of Chhoak Chey and Breus Ka’ak communities in Tbeng Meanchey, for instance, were at first silent about the injustice committed against them due to their fear of the local authorities who were aiding the company. Support from the people of Prame gave them the courage to actively engage in resistance against Heng Fu.\nThe people of Preah Vihear have applied a variety of tactics to stop the company from taking over their land. They have filed complaints with government officials and agencies including the Senate, the Ministry of Agriculture, Forestry and Fisheries, and the courts, as well as with different local and international human rights institutions. They have also employed direct actions such as blocking and seizing the company’s bulldozers and backhoes, pulling out sugarcane from the plantation, and camping out in the field to prevent Heng Fu from clearing their farms at night. In some instances, the actions have resulted in small victories such as in 2016 when the road blockade participated in by six communities in Chey Sen resulted in a series of negotiations between the communities and local authorities. In others, they resulted in community members having to face civil and criminal charges before the provincial court.\nIt’s been six years but the RGC has yet to fully act on the complaints and demands against Heng Fu. The people of Preah Vihear, however, are not giving up the fight to reclaim their land from the Chinese company. Just this August 30, they travelled to Phnom Penh to file a petition before the Chinese Embassy to investigate the company’s operations in the province, in particular, its compliance with the Chinese government’s environmental and human rights standards.\nAlso, in June, Rany, who confessed that just three years ago she lacked any interest in politics, ran for commune chief under the Cambodia National Rescue Party (CNRP), Cambodia’s main opposition party, during the commune council elections. Prame buzzed with enthusiasm: aside from possibly having the first woman to serve as head of their commune, they believe that Rany’s win could help in reclaiming back the lands taken away from them by Heng Fu. Unfortunately, the 27-year-old encountered a lot of difficulties in her candidacy, including the lack of financial support from her party. She lost the elections to the candidate of the ruling party.\n“I will try again in the next elections,” she declared when PANAP asked if the results discouraged her from participating in politics. Her statement, while pinning hope that an elected position will put everything back to normal for her people in Prame, underlies her lack of faith that the Cambodian political and justice system will ever work on the people’s behalf.\nIn the meantime, Rany, who is expecting her and Jin’s first child early next year, has to take on new responsibilities as a mother.\n“These are all for the future generations. The land, the forest, the streams — we are trying to win them back for our children,” she said. #\nFor more photos please click here.\n#NoLandNoLife Features discuss recent developments, events, and trends on land and resource grabbing and related human rights issues in the region as well as the factors and forces that drive it. Send us your feedback at email@example.com."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:d3a67a42-133a-4a4a-b735-3b3df409c7e9>","<urn:uuid:93d56ffd-40d1-4351-8788-c325fc717260>"],"error":null}
{"question":"How do Auping's employee engagement practices in sustainability compare with ISO 14001:2015's requirements for workforce involvement in environmental management? Please describe their respective approaches.","answer":"Auping implements employee engagement through their 'Voorvliegers' group of 30 managers who evaluate the company's sustainability journey, along with regular employee surveys that allow all staff levels to provide feedback on production processes. They also encourage learning-by-doing and peer training for circular expertise development. ISO 14001:2015, meanwhile, specifically requires boosting leadership involvement and employee engagement as part of its certification requirements. Both systems recognize the importance of workforce participation, but Auping takes a more informal, culture-based approach through its communicative feedback culture, while ISO 14001:2015 provides a more structured framework for employee involvement in environmental management.","context":["Added: Sep 22, 2022\nLast edited: Sep 22, 2022\nRoyal Auping, the largest independent bed manufacturer in the Netherlands, has 303 full time employees and 322 flexible workers from over 40 countries. Auping’s goal is to make high-quality durable beds with a lifelong guarantee, which has remained core to the company’s strategy. Since its founding in 1888, Auping has remained a family business and prioritised its social commitment (both to its workers and the larger community). This is reflected in the company’s social fund scheme, which includes financial emergency support for its employees and their families. Such a social foundation is critical to understanding the company’s DNA of being socially minded and sustainable.\nThe company was initially founded to help hospitals locate suitable mattresses for their patients. So, how has the company, which is now more than 130 years old, continued to remain relevant? By being masterful in innovation! Auping has continuously adapted its products to be more sustainable: it received a B Corporation Certification (marking it as a business that assesses the balance between purpose and profits), which solidified its reputation as a circular economy industry leader in the Netherlands.\nOPPORTUNITY FOR HRM IN AUPING\n1. Auping’s incremental upskilling of employees results in them gaining unique and rich industry expertise. However, establishing a knowledge management system is critical to ensure this valuable knowledge is retained and shared within the company as well as external parties and can serve as in-house training, an extension of its licensing scheme or as an entirely new business proposition for higher education, vocational school or industry professionals.\n2. One of Auping’s main challenges is to ensure the pioneering and enthusiastic spirit is kept alive. Amid the growing global uncertainty and as the company furthers its circular ambitions, ensuring employees’ sense of security and wellbeing becomes a critical opportunity in the successful delivery of its ambitions. HRM has the opportunity to showcase its commitment to the worker’s morale and wellbeing by encouraging honest conversations with employees, assisting and motivating workers who are struggling and ensuring that overall morale at the company is stable, despite the hard times.\nAUPING’S CIRCULAR JOURNEY\nDriven by the 1.5 million incinerated or discarded mattresses a year in the Netherlands alone, and the complex composition of glued materials (which makes it difficult to reuse), Auping’s circular journey was initiated and promoted by Jan-Joost Bosman, its CEO. The company strives to reduce waste—stopping the incineration of used mattresses and the loss of valuable raw materials while developing an easy-to-disassemble and recycle mattress. All of the materials in Auping’s mattresses can be reused to develop new, fresh mattresses. As Auping has a strong social foundation, focused on providing optimal working and social conditions for its employees, the transition to circularity has been a reconciliation of both sustainability and its founding social component.\nWithin Auping, the idea for a circular mattress originated as early as 2010. Auping then developed a long-term vision and strategy to transition toward a Cradle-to-Cradle philosophy to produce the world’s first circular mattress. Developing the strategy was sequential and incremental, including the inclusion of circular metrics in the company’s KPIs. The initial challenge was understanding how to produce the mattress and comprehending the required skills needed to develop the circular mattress. The subsequent step was to implement incremental changes to its production based on three main principles:\n1. Manufacture from 100% reusable materials,\n2. Minimal use of fossil fuels and increased reliance on renewable energy,\n3. Keep employee wellbeing central to operations.\nIn 2020, with technology co-development partner Niaga, Auping designed the world’s first fully circular mattress: the Auping Evolve. Its circular mattress now represents 85% of the company’s total mattress production. And as demand grows, the company has begun expanding its circular mattress range.It aims to be fully circular by 2030 at the latest. In May 2022, Auping launched two premium circular mattresses, the Auping Elysium and Auping Elite. It also trialled a Product-as-a-Service circular business offering:, its Bedzzzy Sleep Subscription service. Due to limited consumer interest this service was recently discontinued.\nKeeping the circular momentum going, summer 2023 will see the Auping Evolve become customizable and its production become automated and robotically produced in a pilot production line in Deventer, the Netherlands. By scaling up its production, it will be able to reduce the mattress's unit cost. Since the costs will still be higher than standard mattress production, Auping has taken the radical step of offering its circular mattress innovation in licence form to its competitors.\n“Financing our circular mattress? The first steps are always more expensive…but that is part of a longer-running business case. And essential! It's not just about making your products sustainable, but also your company! Circular business models make your company future-proof.”\n- Martijn Aalders - Director HR\nHRM SPECIFIC OPPORTUNITIES AND CHALLENGES IN THE CIRCULAR TRANSITION\n1. Auping recognises that the impact they can make independently is small and that it takes collaboration to create systemic change. With this ideology in mind, Auping has embarked on a remarkable and novel strategy: collaborating with its competition. Specifically, a company engineer is assisting a competitor company in the design of its circular mattress. The main goal is to ensure that more players are in the market in order for a circular mattress industry to flourish.\n2. Auping’s sustainability and innovation Director has oversight and is responsible for the execution of Auping’s vision. However, the effort to embed the Cradle-to-Cradle circular approach in all its operations has been a company-wide endeavour. Moreover, the overall strategy development is done with ‘Voorvliegers’, a group of 30 managers within the organisation—including HR—who evaluate and provide feedback on the company’s journey.\n3. The main challenge in Auping’s journey has been defining and finding the right circular expertise. Aside from seeking classical technical expertise, its engineers were encouraged to have a broad mindset, and to be open to learning about circularity and innovation. In 2008, Auping started a lean production process focused on continuous improvement, an approach that has proven fruitful in producing the world’s first circular mattress. Without a formalised upskilling process, its employees gain specific expertise through learning-by-doing and training each other. The continuous nature of Auping’s transition has allowed for older employees to continue their tasks while younger new recruits are trained and learn on the job.\n4. The Human Resources Department has not played an active role in Auping’s transition, as it was initially viewed as a technical endeavour. Yet, the HRM team has been playing a critical role in understanding the company’s existing skills and allocating its workforce according to the challenges that arise. In addition, redeployment within departments is common, thus at Auping, the employees are trained to be flexible and entrepreneurial.\n5. In 2020, Auping received a B Corporation Certification, since then it has required greater HRM involvement in order to formally track the company’s progress towards its goals.\n6. Auping’s reputation as a socially-minded company with a cooperative culture has been nurtured over the years. For instance, Auping has a social financing scheme consisting of a monthly €1.50 contribution per employee to a common account which Auping then matches. This social fund is intended for colleagues and their families who may be facing temporary difficulties. This reputation has allowed the company to successfully attract socially-minded, skilled employees needed to deliver the circular mattress\n7. In delivering a fully circular mattress, the company’s communicative and feedback culture is a key pillar of its success. Its regular employee survey has given formal avenues for managers, engineers, researchers, and production staff to listen to each other, and to learn and improve production processes throughout the company’s journey. Thus, despite management setting the strategic direction, all employees are given as much freedom as possible within their decision-making authority, whereby everyone has their own degree of influence.\n“In terms of hiring? We ensure that the right skills and especially mindset are present within the organisation. We have defined this. We then look at which skills are needed and we draw up a profile. Here, however, we don't look so much at what exactly can someone already do, but where does someone stand? What behaviour/motivation do they have? Anyone can learn, but you have to have the right mindset!”\n- Martijn Aalders - Director HR","ISO 14001:2015 Environmental Management System Certification\nThe current version of ISO 14001 was reviewed and published in 2015. Currently, more than 420,000 organizations around the world are ISO 14001-certified. This number is growing rapidly.\nISO 14001 is an international standard offered by the International Organisation for Standards (ISO). Through the ISO 14001 management system, companies can monitor the impact their products and services have on the environment.\nIt’s primarily for organizations that use natural resources. As a result of their processes that convert natural resources into useful products, the organizations tend to release a lot of wastes that negatively affect the environment. These negative effects need to be recognized and reduced, throughout their lifecycle. Recently, ISO 14001 was updated in 2015, which is why the latest certification in the ISO 14001 family that is available is called ISO 14001:2015.\nISO 14001:2015 - An Introduction\nISO 14001:2015 defines the conditions imposed by a standard Environmental Management System. It includes the guidelines for a company looking to enhance their environmental performance and sustainability policies. Its use is directed towards associations who are looking to manage their liabilities with a planned approach towards sustainability. ISO offers a 35-page implementation guide, curated and overseen by Technical Committee ISO/TC 207/SC 1.\nApplicability of ISO 14001:2015\nAccording to ISO, the Environmental Management System Certification can be applied to any organization. This helps organizations of different sizes, belonging to different industries, get the same advantages of ISO 14001:2015’s application. ISO 14001:2015 eliminates the ‘one size fits all’ approach.\nOne must remember that ISO 14001:2015 is not a scale with which environmental management parameters must be matched to. It’s more like an internationally-authorized guidebook that states how certain operations must be conducted with respect to environmental conservation. The basic fundamentals of ISO 14001:2015 are based on the Plan-Do-Check-Act (PDCA) iterative management method.\nObjectives of ISO 14001:2015\nThe objective of ISO 14001:2015 is to deliver a framework for the applying organization aimed towards the protection of the environment. It consists of policies leveled with socio-economic needs that help create a response plan for dynamic environmental conditions. The certification outlines specific requirements for sustainable development, which include:\n- Mitigation of negative effects caused by an organization on the environment, protecting the environment.\n- Mitigation of possible adverse impacts of the environment on the organization.\n- Enhancing immediate environmental performance.\n- Helping the organization fulfil compliance obligations\n- Demonstration of compliance with the changing regulatory requirements of the certification.\n- Achieving strategic business goals by embedding environmental issues into business management\n- Setting up guidelines for a product’s life cycle, including design, production, curation, distribution, and disposal. This ensures environmental effects are not shifted elsewhere within the product life cycle, unintentionally.\n- Strengthening market position of an organization while achieving operational and financial profits from the implementation of environmentally sound alternatives.\n- Boosting leadership involvement and employee engagement\n- Improvement of confidence in the company and company reputation\n- Clear communication of environmental information to relevant and interested entities.\nAdvantages of ISO 14001:2015 Environmental Management System\nThe advantages an organization taking up ISO 14001:2015 Environmental Management System certification may benefit from are:\n- Boost in customer confidence, recognition for the community, employees, and environmental authorities\n- Improvement of company perception through an internationally-recognized certificate\n- Advantage over competitors, both in business and sustainability\n- Reduced risks of environmental accidents drive down costs of insurance\n- Prevent incidents that may lead to fines and sanctions, catapulted by the lack of environmental protection measures\n- Prevent possible incidents that may lead to sanctions /fines due to lack of environmental protection policies\n- Better alignment to market requirements through sustainable approaches\nSustainable Development Goals\nThrough Intercert’s enriched ISO certifications, organizations will be able to contribute to Sustainable Development Goals (SDG) that the United Nations has prescribed in their ambitious 15-year plan that address crucial issues ailing the world. ISO 14001:2015\nEnvironmental management systems contribute to the following SDG codes:\n- 1: No Poverty\n- 2: Zero Hunger\n- 3: Good Health and Well-being\n- 4: Quality Education\n- 5: Gender Equality\n- 6: Clean Water and Sanitation\n- 7: Affordable and Clean Energy\n- 8: Decent Work and Economic Growth\n- 9: Industry, Innovation, and Infrastructure\n- 12: Responsible Consumption and Production\n- 13: Climate Action\n- 14: Life Below Water\n- 15: Life on Land\nWhy Intercert for ISO 14001:2015 Environmental Management System\nIntertcert serves transparent and impartial services so that your organization realizes every detail advised by ISO 14001:2015 accurately. Our certifications are highly sought-after due to our competitive and cost-effective services. With an experience of over 13 years, we’ve mastered the art of delivering excellence in the form of training and international certifications. We are an accredited management system body with certifications from IAF, IAAC, APAC, and Standard Council of Canada (SCC)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:53ad542b-ec12-4864-b40b-d930da88d188>","<urn:uuid:a65457b0-cba5-480f-b63c-9b932d4a4f43>"],"error":null}
{"question":"I'm researching SSRIs for children - what are their benefits for dysthymia and what are the safety concerns?","answer":"SSRIs are considered the first-choice antidepressants for children with dysthymia as they have proven to be both effective and relatively safe. They work by blocking serotonin reabsorption in brain cells, leading to improved mood. Common SSRIs include Celexa, Lexapro, Prozac, Paxil, and Zoloft. However, there are important safety concerns: antidepressants increase the risk of suicidal thoughts and behaviors in pediatric patients in short-term studies. Therefore, all children taking antidepressants must be closely monitored for clinical worsening and emergence of suicidal thoughts and behaviors.","context":["How is Dysthymia in Children Treated?\nDysthymia is a mood disorder which is less severe than depression. Children diagnosed with dysthymia can be treated using medications, therapy, or both approaches together.\nDysthymia is considered a chronic mood disorder which falls under the category of depression. Like adults, children also suffer from this type of mood disorder. While chronic depression is a very serious condition, dysthymia in children is treatable. Treatments which are considered to be effective include medications and non-medicated therapy such as psychotherapeutic approaches. The main goals for treating this condition include decreasing symptoms of depression, decreasing risk of the development of other mood disorders, and reinforcement of psychosocial functioning.\nAntidepressant medications have been shown to be effective when treating children with dysthymia. Antidepressants fall into different divisions and after careful evaluation of the patient, the mental health professional may choose one to treat their client. These antidepressant medications may include:\n* Selective serotonin reuptake inhibitors (SSRIs): Selective serotonin reuptake inhibitors alleviate indications of depression by blocking serotonin from being reabsorbed by brain cells. High levels of serotonin in the body lead to an improvement in mood. Common SSRIs include Celexa, Lexapro, Prozac, Paxil, and Zoloft.\n* Serotonin norepinephrine reuptake inhibitors: Serotonin norepinephrine reuptake inhibitors are similar to SSRIs as they block serotonin from being absorbed by nerve cells in the brain. This antidepressant also blocks the reabsorption of norepinephrine. The increased levels of norepinephrine and serotonin are theorized to create better moods. Common serotonin norepinephrine reuptake inhibitors include Effexor and Cymbalta.\n* Monoamine oxidase (MAO) inhibitors include Marplan, Nardil, Parnate, and EMSAM. : These types of antidepressants block common monoamine oxidase (MAO)\n* Tricyclic Antidepressants (TCAs): These types of antidepressants block the reabsorption of not only serotonin and norepinephrine but also dopamine.\nSelective serotonin reuptake inhibitors (SSRIs) are the first antidepressants prescribed for children diagnosed with dysthymia as they have shown to be not only effective but safer for children. (Nobile, Cataldo, Marino, & Molteni, 2003).\nCognitive Behavioral Therapy:\nCognitive behavioral therapy integrates cognitive therapy and behavioral therapy. This kind of therapeutic approach aims to change the unhealthy thought patterns (cognitive) which can in turn change the unhealthy actions (behavioral).\nCBT techniques can include:\n* Keeping a journal: The patient will write down his or her thoughts and actions, especially noting when they are feeling depressed. The mental health professional and patient can evaluate the journal entries together to find patterns of why the patient may be feeling sad.\n* Modeling: This technique is a form of role playing. The therapist uses role playing to help the patient understand suitable ways of coping and getting through difficult situations.\n* Systematic Positive Reinforcement: This reward system is used to reinforce positive behaviors. The positive reinforcement promotes positive behavior and positive thinking.\n* Cognitive Rehearsal: The mental health professional asks the patient to remember a challenging situation. The therapist and patient then work together to figure out a solution and better way of coping with the situation if it were to arise again.\nCognitive behavioral therapy is a short term therapy and range in number of sessions when treating dysthymia in children. It all depends on the severity of the dysthymia.\nMayo Clinic. (2008). Selective serotonin reuptake inhibitors (SSRIs). Retrieved from, http://www.mayoclinic.com/health/ssris/MH00066.\nby Jacqueline Chinappi\nMedicine Net. (2010). Chronic depression (Dysthymia). Retrieved from, http://www.medicinenet.com/dysthymia/article.htm.\nNobile, M., Cataldo, G., Marino, C., & Molteni, M. (2003). Diagnosis and treatment of dysthymia in children and adolescents. CNS Drugs, 17 (13); 927-946.\nTell-a-Friend comments powered by Disqus","Antidepressants increased the risk of suicidal thoughts and behaviors in pediatric and young adult patients in short-term studies. Closely monitor all antidepressant-treated patients for clinical worsening, and for emergence of suicidal thoughts and behaviors. Tranylcypromine sulfate is not approved for use in pediatric patients.Excessive consumption of foods or beverages with significant tyramine content or the use of certain drugs with tranylcypromine sulfate or after tranylcypromine sulfate discontinuation can precipitate hypertensive crisis. Monitor blood pressure and allow for medication-free intervals between administration of tranylcypromine sulfate and interacting drugs. Instruct patients to avoid ingestion of foods and beverages with high tyramine content .\nMedically reviewed on June 7, 2018\nCommonly used brand name(s)\nIn the U.S.\nAvailable Dosage Forms:\nTherapeutic Class: Antidepressant\nPharmacologic Class: Monoamine Oxidase Inhibitor, Nonselective\nUses For This Medicine\nTranylcypromine is used to treat certain types of depression. It belongs to the group of medicines called monoamine oxidase inhibitors (MAOI). Tranylcypromine works by blocking the action of a chemical substance known as monoamine oxidase (MAO) in the nervous system.\nAlthough tranylcypromine is very effective for certain patients, it may also cause some unwanted reactions if taken the wrong way. It is very important to avoid certain foods, beverages, and medicines while you are using tranylcypromine. Your doctor may provide a list as a reminder of which products you should avoid.\nTranylcypromine is available only with your doctor's prescription.\nBefore Using This Medicine\nIn deciding to use a medicine, the risks of taking the medicine must be weighed against the good it will do. This is a decision you and your doctor will make. For tranylcypromine, the following should be considered:\nTell your doctor if you have ever had any unusual or allergic reaction to tranylcypromine or any other medicines. Also tell your health care professional if you have any other types of allergies, such as to foods, dyes, preservatives, or animals. For non-prescription products, read the label or package ingredients carefully.\nAppropriate studies have not been performed on the relationship of age to the effects of tranylcypromine in the pediatric population. Safety and efficacy have not been established.\nAppropriate studies performed to date have not demonstrated geriatric-specific problems that would limit the usefulness of tranylcypromine in the elderly. However, elderly patients are more likely to have unwanted effects (eg, low blood pressure) or age-related liver, kidney, or heart problems, which may require caution and an adjustment in the dose for patients receiving tranylcypromine.\nThere are no adequate studies in women for determining infant risk when using this medication during breastfeeding. Weigh the potential benefits against the potential risks before taking this medication while breastfeeding.\nInteractions with Medicines\nAlthough certain medicines should not be used together at all, in other cases two different medicines may be used together even if an interaction might occur. In these cases, your doctor may want to change the dose, or other precautions may be necessary. When you are taking tranylcypromine, it is especially important that your healthcare professional know if you are taking any of the medicines listed below. The following interactions have been selected on the basis of their potential significance and are not necessarily all-inclusive.\nUsing tranylcypromine with any of the following medicines is not recommended. Your doctor may decide not to treat you with this medication or change some of the other medicines you take.\n- Methylene Blue\nUsing tranylcypromine with any of the following medicines is usually not recommended, but may be required in some cases. If both medicines are prescribed together, your doctor may change the dose or how often you use one or both of the medicines.\n- Iobenguane I 123\n- Ma Huang\n- Morphine Sulfate Liposome\n- St John's Wort\nUsing tranylcypromine with any of the following medicines may cause an increased risk of certain side effects, but using both drugs may be the best treatment for you. If both medicines are prescribed together, your doctor may change the dose or how often you use one or both of the medicines.\n- Insulin Aspart, Recombinant\n- Insulin Bovine\n- Insulin Degludec\n- Insulin Detemir\n- Insulin Glargine, Recombinant\n- Insulin Glulisine\n- Insulin Lispro, Recombinant\nInteractions with Food/Tobacco/Alcohol\nCertain medicines should not be used at or around the time of eating food or eating certain types of food since interactions may occur. Using alcohol or tobacco with certain medicines may also cause interactions to occur. The following interactions have been selected on the basis of their potential significance and are not necessarily all-inclusive.\nUsing tranylcypromine with any of the following is not recommended. Your doctor may decide not to treat you with this medication, change some of the other medicines you take, or give you special instructions about the use of food, alcohol, or tobacco.\n- Tyramine Containing Food\nUsing tranylcypromine with any of the following is usually not recommended, but may be unavoidable in some cases. If used together, your doctor may change the dose or how often you use tranylcypromine, or give you special instructions about the use of food, alcohol, or tobacco.\n- Bitter Orange\nOther Medical Problems\nThe presence of other medical problems may affect the use of tranylcypromine. Make sure you tell your doctor if you have any other medical problems, especially:\n- Bipolar disorder (manic-depressive illness), history of or\n- Diabetes or\n- Epilepsy (seizures) or\n- Hyperthyroidism (overactive thyroid) or\n- Hypomania, history of or\n- Hypotension (low blood pressure) or\n- Mania, history of—Use with caution. May make these conditions worse.\n- Headache, history of or\n- Heart or blood vessel disease or\n- Hypertension (high blood pressure) or\n- Liver disease, history of or\n- Paraganglioma (adrenaline-releasing tumor) or\n- Pheochromocytoma (adrenal gland tumor) or\n- Stroke, history of—Should not be used in patients with these conditions.\n- Kidney disease—Use with caution. Effects may be increased because of slower removal of the medicine from the body.\nProper Use of This Medicine\nTake tranylcypromine exactly as directed by your doctor. Do not take more of it, do not take it more often, and do not take it for a longer time than your doctor ordered. To do so may increase the chance of side effects.\nTranylcypromine should come with a Medication Guide. Read and follow these instructions carefully. Ask your doctor if you have any questions.\nThe dose of tranylcypromine will be different for different patients. Follow your doctor's orders or the directions on the label. The following information includes only the average doses of tranylcypromine. If your dose is different, do not change it unless your doctor tells you to do so.\nThe amount of medicine that you take depends on the strength of the medicine. Also, the number of doses you take each day, the time allowed between doses, and the length of time you take the medicine depend on the medical problem for which you are using the medicine.\n- For oral dosage form (tablets):\n- For depression:\n- Adults—At first, 30 milligrams (mg) per day, given in divided doses. Your doctor may adjust your dose as needed. However, the dose is usually not more than 60 mg per day.\n- Children—Use is not recommended.\n- For depression:\nIf you miss a dose of tranylcypromine, take it as soon as possible. However, if it is almost time for your next dose, skip the missed dose and go back to your regular dosing schedule. Do not double doses.\nStore the medicine in a closed container at room temperature, away from heat, moisture, and direct light. Keep from freezing.\nKeep out of the reach of children.\nDo not keep outdated medicine or medicine no longer needed.\nAsk your healthcare professional how you should dispose of any medicine you do not use.\nPrecautions While Using This Medicine\nIt is very important that your doctor check your progress at regular visits to allow for changes in your dose and to check for any unwanted effects.\nYou will also need to have your blood pressure measured before starting treatment with tranylcypromine and while you are using it. If you notice any change to your recommended blood pressure, call your doctor right away. If you have questions about this, talk to your doctor.\nDo not use tranylcypromine if you have used another MAO inhibitor within the past 14 days. Do not take an MAO inhibitor for at least 7 days after you stop tranylcypromine.\nDo not use tranylcypromine if you are taking buspirone (Buspar®), carbamazepine (Tegretol®), cyclobenzaprine (Flexeril®), methyldopa (Aldomet®), milnacipran (Savella®), rasagiline (Azilect®), reserpine (Serpasil®), tapentadol (Nucynta®, Palexia®, Tapal®), or tetrabenazine (Nitoman®, Xenazine®).\nWhen taken with certain foods, drinks, or other medicines, tranylcypromine can cause very dangerous reactions, such as sudden high blood pressure (also called hypertensive crisis). To avoid such reactions, follow these rules of caution:\n- Do not eat foods that have a high tyramine content (most common in foods that are aged or fermented to increase their flavor), such as cheese (especially strong or aged kinds), caviar, sour cream, liver, canned figs, soy sauce, sauerkraut, fava beans, yeasts, and yogurt. Avoid smoked or pickled meat, poultry, or fish, such as sausage, pepperoni, salami, anchovies, or herring. Do not eat dried fruit (such as raisins), bananas, avocados, raspberries, or very ripe fruit.\n- Do not drink alcoholic beverages. This includes Chianti wine, sherry, beer, non-alcohol or low alcohol beer and wine, and liqueurs.\n- Do not eat or drink too much caffeine. Caffeine can be found in coffee, cola, chocolate, tea, and many other foods and drinks. Ask your doctor how much caffeine is safe to use.\nTranylcypromine may cause some people to be agitated, irritable, or display other abnormal behaviors. It may also cause some people to have suicidal thoughts and tendencies or to become more depressed. If you or your caregiver notice any of these adverse effects, tell your doctor right away.\nCall your doctor or hospital emergency room right away if you have a severe headache, stiff or sore neck, chest pains, fast heartbeat, sweating, dizziness, or nausea and vomiting while you are taking tranylcypromine. These may be symptoms of a serious side effect called hypertensive crisis.\nCheck with your doctor right away if you have anxiety, restlessness, a fast heartbeat, fever, sweating, muscle spasms, twitching, nausea, vomiting, diarrhea, or see or hear things that are not there. These may be symptoms of a serious condition called serotonin syndrome. Your risk may be higher if you also take certain other medicines that affect serotonin levels in your body.\nTranylcypromine may cause blurred vision, drowsiness, or make some people less alert than they are normally. Do not drive or do anything else that could be dangerous until you know how tranylcypromine affects you.\nCheck with your doctor right away if you have pain or tenderness in the upper stomach, pale stools, dark urine, loss of appetite, nausea, vomiting, or yellow eyes or skin. These could be symptoms of a serious liver problem.\nTranylcypromine will add to the effects of alcohol and other CNS depressants (medicines that slow down the nervous system, possibly causing drowsiness). Some examples of CNS depressants are antihistamines or medicine for hay fever, allergies, or colds, sedatives, tranquilizers, or sleeping medicine, prescription pain medicine or narcotics, medicine for seizures or barbiturates, muscle relaxants, or anesthetics, including some dental anesthetics. Check with your doctor before taking any of the above while you are using tranylcypromine.\nDizziness, lightheadedness, or fainting may occur, especially when you get up suddenly from a lying or sitting position. Getting up slowly may help. When you get up from lying down, sit on the edge of the bed with your feet dangling for 1 or 2 minutes, then stand up slowly. If the problem continues or gets worse, check with your doctor.\nDo not stop taking tranylcypromine without checking first with your doctor. Your doctor may want you to gradually reduce the amount you are using before stopping it completely.\nBefore having any kind of surgery, dental treatment, or emergency treatment, tell the medical doctor or dentist in charge that you are using tranylcypromine or have used it within the past 10 days. Taking tranylcypromine together with medicines that are used during surgery, dental, or emergency treatments may increase the risk of serious side effects.\nYour doctor may want you to carry an identification card stating that you are using tranylcypromine.\nTranylcypromine may affect blood sugar levels. If you are diabetic, be especially careful in testing for sugar in your blood or urine. If you have any questions about this, check with your doctor.\nAfter you stop using tranylcypromine, you must continue to exercise caution for at least 2 weeks with your foods, drinks, and other medicines, since these items may continue to react with tranylcypromine.\nDo not take other medicines unless they have been discussed with your doctor. This includes prescription or nonprescription (over-the-counter [OTC]) medicines and herbal or vitamin supplements.\nThis Medicine Side Effects\nAlong with its needed effects, a medicine may cause some unwanted effects. Although not all of these side effects may occur, if they do occur they may need medical attention.\nCheck with your doctor immediately if any of the following side effects occur:\nIncidence not known\n- Absence of or decrease in body movement\n- actions that are out of control\n- black, tarry stools\n- bleeding gums\n- blood in the urine or stools\n- burning, crawling, itching, numbness, prickling, \"pins and needles\", or tingling feelings\n- chest pain\n- confusion about identity, place, and time\n- dark urine\n- decrease in frequency of urination\n- decrease in urine volume\n- decreased awareness or responsiveness\n- difficulty in passing urine (dribbling)\n- dry mouth\n- fast, irregular, pounding, or racing heartbeat or pulse\n- fever with or without chills\n- general feeling of tiredness or weakness\n- increased need to urinate\n- light-colored stools\n- longer than usual time to ejaculation of semen\n- loss of bladder control\n- loss of consciousness\n- lower back or side pain\n- muscle twitching\n- painful or difficult urination\n- pale skin\n- passing urine more often\n- pinpoint red spots on the skin\n- rapid weight gain\n- severe sleepiness\n- shakiness and unsteady walk\n- sore throat\n- sores, ulcers, or white spots on the lips or in the mouth\n- sudden jerky movements of the body\n- swelling of the face, ankles, or hands\n- swollen glands\n- talking, feeling, and acting with excitement\n- trouble with sleeping\n- troubled breathing with exertion\n- unsteadiness, trembling, or other problems with muscle control or coordination\n- unusual bleeding or bruising\n- unusual drowsiness, dullness, tiredness, weakness, or feeling of sluggishness\n- upper right abdominal pain\n- yellow eyes and skin\nSome side effects may occur that usually do not need medical attention. These side effects may go away during treatment as your body adjusts to the medicine. Also, your health care professional may be able to tell you about ways to prevent or reduce some of these side effects. Check with your health care professional if any of the following side effects continue or are bothersome or if you have any questions about them:\nIncidence not known\n- Blurred vision\n- continuing ringing or buzzing or other unexplained noise in the ears\n- decreased interest in sexual intercourse\n- dry mouth\n- hair loss or thinning of the hair\n- hearing loss\n- hives or welts, itching, skin rash\n- inability to have or keep an erection\n- loss in sexual ability, desire, drive, or performance\n- loss of appetite\n- memory loss\n- muscle spasm\n- redness of the skin\n- stomach pain\n- unable to sleep\n- weight loss\nOther side effects not listed may also occur in some patients. If you notice any other effects, check with your healthcare professional.\nCall your doctor for medical advice about side effects. You may report side effects to the FDA at 1-800-FDA-1088.\nSee also: Side effects (in more detail)\nAlways consult your healthcare provider to ensure the information displayed on this page applies to your personal circumstances.\nCopyright 2018 Truven Health Analytics, Inc. All Rights Reserved.\nMore about tranylcypromine\n- Tranylcypromine Side Effects\n- During Pregnancy or Breastfeeding\n- Dosage Information\n- Drug Images\n- Drug Interactions\n- Support Group\n- Pricing & Coupons\n- En Español\n- 64 Reviews\n- Drug class: monoamine oxidase inhibitors\nOther brands: Parnate"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:da845fd2-4e18-4f51-9420-849cc163f352>","<urn:uuid:63ac0f2d-996d-4a8c-8a93-4d210bb5bc1f>"],"error":null}
{"question":"What are the current health impacts of climate change, and how are scientists using technology to predict related infectious diseases?","answer":"Climate change is currently causing numerous health impacts, including increased rates of allergies and asthma, food insecurity, and the spread of infectious diseases. It's also leading to migration and conflict, as seen in Syria's civil war. On the disease prediction front, scientists are using advanced satellite technology and predictive models to track and forecast disease outbreaks. These models incorporate satellite data on temperature, precipitation, and vegetation conditions to assess disease risk. For instance, the CHIKRisk model uses data from multiple NASA and NOAA sources to provide monthly outlooks for chikungunya risk worldwide. This satellite-based approach helps track when and where diseases like malaria, dengue, and other vector-borne diseases might emerge, allowing health authorities to prepare and respond effectively.","context":["Harvard Chan School symposium, part of Harvard Climate Action Week, offers tools, ideas from a global perspective\nMay 10, 2023 – The wide-ranging health impacts of climate change, including food insecurity, migration, war, and the spread of infectious diseases—and practical solutions to address these problems—were the focus of a half-day symposium hosted by Harvard T.H. Chan School of Public Health.\nThe May 8 event, titled Climate, Health & Equity: Toward a Sustainable Future, was part of the inaugural Harvard Climate Action Week, sponsored by the new Salata Institute for Climate and Sustainability.\nSpeakers at the symposium, held at Harvard Business School’s Spangler Center, included community activists, state and federal administrators, and scientists.\n“Climate change is not a distant abstract concept or threat,” said Harvard Chan School Dean Michelle Williams in opening remarks. “Climate change is harming our health, right here and right now. Climate change is deepening inequities, right here and right now. And we all know that we have the tools to tackle climate change, right here and right now.”\nThe symposium featured a brief video highlighting Harvard Chan School’s decades-long legacy of tackling the health impacts of climate change, such the landmark Six Cities Study, which led to a tightening of U.S. air pollution standards in the 1990s, and the founding of the Center for Climate, Health, and the Global Environment (Harvard Chan C-CHANGE) in 2018.\nOn the climate change front lines\nKari Nadeau, John Rock Professor of Climate and Population Studies and chair of the Department of Environmental Health, moderated the event, which featured several panels. The first featured three community activists: Carolina Reyes, a community health care worker from Springfield, Mass.; Shweta Narayan, a global climate campaigner with the international nongovernmental organization Health Care Without Harm; and Passy Amayo Ogolla, program manager for the nonprofit Society for International Development.\nNarayan, who lives in a small village in southern India, said she has seen temperature and rainfall changes affect local agriculture and disease spread. Reyes spoke about increasing rates of allergies and asthma in Springfield. And Ogolla described droughts in her community, and elsewhere in Africa, that have led to food insecurity, malnutrition, and people dying from hunger. “The challenges are dire, not just affecting health, but the entire livelihood of the community,” she said.\nOn a personal level, Ogolla has taken action to improve the situation by planting a fruit tree forest in her community to provide both shade and healthy food—and she has encouraged others to plant more trees as well. Reyes has worked to educate doctors, religious leaders, and families about climate change’s health impacts and the importance of keeping the environment safe and healthy for all. “We’re in 2023 and we’re still using gasoline,” she said. “Why are we still using harmful nail polish? Plastic bottles? We have to make it personal and make the government take it personally too.”\nGovernments must be accountable for setting policies aimed at limiting climate change, the activists agreed. Said Ogolla, “I strongly believe that prevention is better than cure. I would strongly call upon policymakers to look into their climate and sustainable energy policies and strategize toward implementing these now so that future generations can have a planet to look forward to. And not just any type of planet—a planet where people are healthy and living sustainably.”\nDocumenting food insecurity\nLouise Ivers, director of the Harvard Global Health Institute and professor of global health and social medicine at Harvard Medical School, moderated a panel on food insecurity, noting that it affects a quarter of a billion people worldwide. Panelists included Harvard Chan School’s Chris Golden, Abrania Marrero, and Jennifer Leaning.\nGolden, assistant professor of nutrition and planetary health, spoke about his research on how climate change destabilizes food systems. Much of his work has focused on Madagascar, where he has assessed how impacts such as sea temperature rise and ocean acidification can lead to coral bleaching, threatening the ability of fisheries to reproduce and to provide seafood for Indigenous cultures and others living in the global South. As for solutions, Golden suggested using environmental data to create an early-warning system for communities and countries to predict where health impacts might occur. He also described his work creating artificial reefs in Madagascar to help rehabilitate fish stocks.\nMarrero, a postdoctoral research fellow, discussed her studies of nutrition, small-scale farming, and climate resilience of island nations, particularly in Puerto Rico. She noted that although Puerto Ricans traditionally grew their own food, the situation changed in recent years to the point where, today, 85%-95% of the food in the country is imported—much of it highly processed and unhealthy. “This radically affects the health of people, of local economies, of livelihoods, of ecosystems,” Marrero said. Research she’s worked on has shown that people who eat locally produced foods have better nutritional status. She also noted that there is much to be learned from small-scale food producers about healthy land stewardship, which can both bolster livelihoods and reduce food insecurity. Studying how small producers operate “can really help us think about what are the practices that we can learn from on a larger scale,” she said.\nLeaning, senior research fellow at the François-Xavier Bagnoud (FXB) Center for Health & Human Rights, talked about how climate change-driven drought led to migration and conflict in Syria, fueling that nation’s ongoing civil war. She also discussed the problem of dangerous heat in India, noting that the government there is working to form heat action plans so that people have adequate shelter and water during life-threatening heatwaves.\nA focus on policy\nState and federal actions to mitigate the health impacts of climate change were the focus of two panels. One featured Melissa Hoffer, who was appointed Massachusetts’s first-ever Climate Chief earlier this year. Having a state climate chief is crucial to ensure that climate considerations are “in the bones” of all the other state agencies, Hoffer said. She has been working on a variety of fronts, such as increasing the use of electric vehicles and promoting environmental justice.\n“We know what we need to do,” said Hoffer. “We have to stop using dirty fossil fuels. We need to stop investing in all the institutions that support the development and production of these fuels. We have to start baking in a social cost of carbon.”\nJanet McCabe, deputy administrator of the Environmental Protection Agency, spoke with Harvard Chan School’s Tamarra James-Todd, Mark and Catherine Winkler Associate Professor of Environmental Reproductive Epidemiology, about federal policies aimed at reining in climate change. “This is an incredible time at the EPA,” said McCabe, citing a new federal rule that will massively reduce air pollutants from heavy-duty trucks, a proposed rule that would institute the toughest-ever emissions caps on all cars and trucks, and an upcoming proposal on limiting fossil fuel-fired power plant emissions. These rules, together with rules being put in place by other federal agencies, put the nation on target to meet President Biden’s commitment to reduce carbon emissions by roughly 50% by 2030, said McCabe.\nClimate change and infectious diseases\nIncreases in malaria and diarrheal diseases, even another pandemic—such are the infectious disease threats posed by climate change, according to three experts from Harvard Chan School who spoke during a final panel.\nAaron Bernstein, C-CHANGE interim director and soon-to-be director of the Centers for Disease Control and Prevention’s National Center for Environmental Health and the Agency for Toxic Substances and Disease Registry, said that as temperatures increase, insects move—along with the diseases they carry. For example, he said, tick-driven Lyme disease is now showing up in Canada for the first time, and avian malaria, transmitted by mosquitoes, is impacting birds at higher altitudes than ever in the mountains of Hawaii. Marcia Castro, Andelot Professor of Demography, noted that deforestation in Brazil puts people in closer contact with wildlife, raising the risks of zoonoses—diseases that jump from animals to humans. And Caroline Buckee, professor of epidemiology, talked about how extreme weather events lead poor people living off the land to become “climate refugees,” moving to cities to find work. “If you were going to design a perfect situation for a global pandemic, you put them in crowded places and connect them by global travel,” she said. “That’s exactly what we have.”\nThe researchers stressed the importance of gathering adequate data to document climate-driven impacts on the spread of disease, and for governments to regulate these impacts. Buckee added that it’s key for researchers in the global North to learn lessons from partners in the global South, who have experience in handling extreme heat and natural disasters. Bangladesh, for example, does a great job of responding to cyclones because they hit the country regularly. Creating meaningful academic partnerships in countries like Bangladesh could help in developing strategies for the future, she said.\nKeynote speaker for the symposium was environmentalist and civil servant Heather McTeer Toney, who previously served as the first woman, the first African American, and the youngest-ever mayor of Greenville, Mississippi, where she fought to clean up the community’s tap water. She went on to serve as regional administrator for the EPA’s Southeast region and as a leader in the Mom’s Clean Air Force. Currently she is vice president for community engagement for the Environmental Defense Fund and executive director of Beyond Petrochemicals, a campaign launched by Michael Bloomberg to stop expansion of petrochemical and plastic pollution in the U.S.\nCommunities facing environmental injustice across the U.S. have been resilient in the face of climate change effects “because it’s what we have to do,” Toney said. In addition to dealing with polluted air, water, and land, these communities also face a host of other issues, such as unfair healthcare and education systems, police brutality, food disparities, and failing infrastructure. Efforts to address the health impacts of climate change, she said, must take all of these factors into account. “We have to connect the dots,” she said.\nFeature photo courtesy Salata Institute for Climate and Sustainability\nFor stories of climate hope and action, subscribe to The Climate Optimist newsletter.","Satellites Tracking Vector-borne Diseases and Forecasting Their Outbreaks\nColumbia, MD– July 27, 2020. Incorporating satellite observations in disease models has become a valuable component in informing public health policy and decisions. Twenty years ago, predicting a disease outbreak seemed like science fiction. But today, researchers can track when and where certain diseases will emerge and spread—from weeks to months beforehand—with the help of Earth observing satellites.\nAedes aegypti mosquitoes transmit the virus for several tropical diseases, including chikungunya, dengue, zika, and yellow fever. They are recognized by white markings on their legs. Image credit: CDC/James Gathany\n“Scientists can determine where disease-favoring conditions in the environment exist, and that can be done only by a global surveillance system that satellites provide,” said Assaf Anyamba, a Universities Space Research Association scientist working in the GESTAR program at NASA GSFC, and the lead scientist on the Rift Valley fever Monitoring project.\nAnyamba’s team is one of several research groups worldwide that is developing predictive outbreak models for some of the world’s most common but not well understood diseases—many of which do not have a vaccine. Rift Valley fever, cholera, chikungunya, and dengue, are generally found in tropical regions around the world with poor water quality and sanitation, and in places with limited access to health care services.\nWith the access to advanced satellite observations, researchers who study these diseases have acquired better tools to observe the temperature, precipitation, and vegetation conditions (i.e. habitats and environmental conditions) that are linked with these vector-borne diseases. They have incorporated these data into models that assess the likelihood of disease outbreaks as a function of these conditions. By anticipating when and where these conditions might become favorable, researchers can help local governments and international health organizations focus their resources to mitigate and/or manage the outbreaks.\n“Disease risk models involve a mixture of understanding of the biology of the organism that spreads the disease and changes in environmental and habitat conditions, plus the numerical tools to be able to put it all together,” said Anyamba, who works with disease experts, biologists, and health officials across organizations and government agencies.\nNASA satellites provide some of the inputs required by models: data on weather and climate, land-surface vegetation conditions, temperature and precipitation especially for geographical regions that lack such measurements on the ground, to monitor and model the conditions that precede an outbreak. But how do researchers know which environmental conditions play a significant part? To answer that question, researchers study past outbreaks and evaluate their models based on such historical events.\nDr. Anyamba (right) working in the field.\nFor example, Anyamba has studied various chikungunya outbreaks alongside his Rift Valley fever research. Chikungunya, a reemerging viral disease and illness is reported to have first appeared in 1952, but recently spread across the world and arrived in the US in 2015. It causes sudden fever, rash, and joint pain that could last for months sometimes causing victims to hunch over due to the aches.\nSince 2016, Anyamba and his team have been working on a chikungunya risk mapping and forecasting system called CHIKRisk. The model incorporates air temperature and rainfall data from NOAA models; land surface temperatures from NASA’s Moderate Resolution Imaging Spectroradiometer (MODIS) instruments; humidity and soil moisture data from NASA’s Global Land Data Assimilation System; human population density data from NASA’s Socioeconomic Data and Applications Center; and chikungunya vector distributions from the Walter Reed Biosystematics Unit’s VectorMap. The model is correlated with ground-based surveillance systems (such as the Program for Monitoring Emerging Diseases (ProMED) that identify disease activity. The project is supported by the U.S. Defense Threat Reduction Agency and the NOAA Climate Program Office - International Research and Applications Project (IRAP)\nThe CHIKRisk model provides monthly outlooks of where chikungunya risk is highest around the world. That information is used by the U.S. Department of Defense’s Global Emerging Infections Surveillance (GEIS) system for situational awareness and protection of health of soldiers stationed overseas, and by the Pan American Health Organization (PAHO) to help control cases in high-risk areas.\nThe same approach can be used for other vector-borne diseases since the outbreak of these diseases is affected by meteorological conditions, which may evolve to anticipated change in climate conditions. Information sharing is key to prevention, mitigation and management of outbreak of such diseases. In 2020, Anyamba’s team used the CHIKRisk model assess the risk for chikungunya in India, Mexico, Indonesia, Malaysia, and Philippines. They found elevated risk for these countries/regions. Anyamba and his team using the same approach and working with researchers sponsored by the NASA’s Health and Air Quality Applications Program, and local health departments in California, New Jersey, Utah, South Dakota, and Louisiana to assess the risks of West Nile and dengue fever for these States.\nAn increased understanding of the link between meteorological conditions as possible precursors of such diseases, and the combined use of satellite data and mathematical models, give us hope that the prediction skill for such diseases outbreak will continue to improve in the future. Such information will be invaluable for health authorities and citizens around the world to manage the risk of these outbreaks.\nFor more information, please see:\nEarth Observatory - Of Mosquitoes and Models: Tracking Disease by Satellite : https://earthobservatory.nasa.gov/features/disease-vector\nFounded in 1969, under the auspices of the National Academy of Sciences at the request of the U.S. Government, the Universities Space Research Association (USRA) is a nonprofit corporation chartered to advance space-related science, technology and engineering. USRA operates scientific institutes and facilities, and conducts other major research and educational programs, under Federal funding. USRA engages the university community and employs in-house scientific leadership, innovative research and development, and project management expertise."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:b85dc689-410e-45a2-bdc0-a3d11c60ec18>","<urn:uuid:dcf299e7-ffdc-4c14-873c-afcc1c1849b6>"],"error":null}
{"question":"How do refined carbohydrates and added sugars affect health, and what are the recommended alternatives?","answer":"Refined carbohydrates like white bread and added sugars can significantly increase triglyceride levels, especially when carbohydrate intake exceeds 60% of daily calories. Instead, choose whole grains, oatmeal, and vegetables for healthier carbohydrates. For sugars, the American Heart Association recommends a daily maximum intake of 25g (6 teaspoons) for women and 36g (9 teaspoons) for men. Healthier alternatives include using natural sweeteners like fruit purées (bananas, dates, figs, or applesauce) which provide sweetness while adding fiber, vitamins, and minerals with fewer calories. You can also use sweet spices like vanilla extract, cinnamon, nutmeg, or cardamom to enhance sweetness without added sugar.","context":["Some healthier alternatives to cook with\nEating healthier may seem overwhelming, but it shouldn’t be! It can be as simple as making a few simple changes to your diet.\nRather than taking an “all or nothing” approach to tame a sweet tooth, cut back on salt, lose a few pounds, or eat more nutrient rich foods, try the “ol switcheroo” instead. Swapping out less healthy ingredients with those that pack a more nutritious punch can make a big difference in your diet that can lead to better health — without skimping on flavor.\nTry these simple swaps to make your next recipe healthier:\nButter and oils\nButter, shortening and tropical oils (coconut, palm oil, and palm kernel oil) are high in saturated fat, which has been linked with heart disease. Substitute with healthier oils like olive, peanut, canola or any non-tropical oil. When cooking, replace all or some of these fats with healthier oils using a 1:1 ratio. In baking, substitute at a ratio of 1:3/4.\nFurthermore, fruit and vegetable purées — such as avocado, pumpkin, applesauce, prune, fig, banana or date — may be substituted for half or more of the fat in recipes.\nConsuming too much added sugar is linked to increased risk of chronic disease, including heart disease. Start by cutting the sugar called for in a recipe by 25% and then look to the most natural substitution: fruit!\nNot only do puréed or mashed bananas, dates, figs or applesauce provide sweetness, they pack nutrients like fiber, vitamins and minerals with far fewer calories than sugar.\nHoney, maple syrup and agave syrup or nectar are a sweeter swap, as they provide a more similar sweetness to sugar. Substitute one cup of sugar in recipes with 2/3 cup of these liquid sweeteners and reduce the liquid in the recipe by ¼ cup.\nAdding to recipes a teaspoon of “sweet” spice — like vanilla extract, cinnamon, nutmeg or cardamom — makes the results seem sweeter too.\nGoing heavy on the salt shaker can lead to high blood pressure, heart disease and stroke. Try seasoning your food with bold flavors so you won’t need salt.\nReplace salt with spices, like fresh or dried herbs. Cinnamon and cumin bring more robust flavors, while basil and thyme are more subtle.\nAvoid the hidden salt in store-bought spice blends by mixing your own. For Italian seasoning, combine basil, oregano, rosemary, parsley, thyme, red chili flakes and garlic powder.\nOther flavor boosters include fresh or powdered onion, garlic and chili peppers, as well as mustards, vinegars, and lemon and other citrus zest and juice.\nWhite flour, white rice and white pasta are made with refined grains, which are grains that have been stripped of their nutrient-rich bran and germ. Refined grains lose three-quarters of their original protein and one-third to half of their nutrients.\nReplacing some or all of the refined grain in a recipe with whole grains, or choosing products made with whole grains, delivers all of the fiber, protein, vitamins and minerals nature intended, along with several health benefits, including lower risk of diabetes, heart disease and high blood pressure.\nInstead of white rice, choose whole grain brown rice or wild rice or riced cauliflower. For ordinary pasta, swap in pasta made with whole grains, or choose legumes, spaghetti squash or vegetable noodles made from zucchini.\nReplace refined white flour with whole grain flours like whole wheat, oat, millet or quinoa, or nut flours, like almond, hazelnut and flaxseed, or cooked black beans. Conversions from white flour to whole grain or nut flours when baking are not always 1:1, so it may take some experimentation to achieve desired results.\nHigh intake of red and processed meats is associated with higher risk of heart disease, cancer, diabetes and premature death. Replace fatty red meats and processed meats with leaner cuts, or substitute poultry, like chicken or turkey, or fish or other seafood.\nWhole food plant-based stand-ins for meat include mushrooms, tofu, tempeh or legume-based veggie burgers.\nThere are also many packaged plant-based meatless products, but they can be highly processed, so be sure to read the label to avoid unwanted ingredients, such as added sugar, sodium and additives.\nReprinted with permission from Environmental Nutrition, a monthly publication of Belvoir Media Group, LLC, 1-800-829-5384, EnvironmentalNutrition.com.\n© 2023 Belvoir Media Group. Distributed by Tribune Content Agency, LLC.","Ways to lower high triglycerides\nThere are many different types of fat, from the polyunsaturated fats in olive oil to the saturated fats in red meat. They all contribute to triglyceride levels in the body but do so in different ways.\nWhen a person eats more calories than their body needs, it stores these extra calories in the form of triglyceride fats. When the body needs more energy at a later stage, it consumes these fats.\nTriglycerides are important for health, but high levels increase the risk of heart disease, which is the leading cause of death in the United States. Lowering triglyceride levels and reducing other risk factors can decrease a person's likelihood of developing heart disease.\nThere are many ways to reduce triglyceride levels safely. The best method may depend on the cause of the high triglyceride levels.\n1. Calories: Reduce intake\nSalads and other dishes that contain vegetables tend to have fewer calories and more fiber, both of which can help reduce triglyceride levels.\nRegularly consuming more calories than the body can burn off will result in an excessive number of triglycerides in the body.\nOne way to lower triglyceride levels in the blood is to consume fewer calories each day.\nAccording to the American Heart Association (AHA), a 5–10 percent weight loss can decrease triglyceride levels by 20 percent.\nThere is a direct correlation between the extent of weight loss and the decrease in triglycerides.\n2. Fats: Choose the right ones\nThe body needs fats to function correctly, but some fats are more healthful than others. Choosing healthful fats may help reduce triglyceride levels.\nSolid fats come from meat, full-fat dairy products, and some tropical oils, such as coconut and palm oil. These foods contain trans fats and saturated fats.\nTrans fats and saturated fats raise triglyceride levels, so people should try to replace them wherever possible.\nUnsaturated fats, especially polyunsaturated fats (PUFAs), can help lower triglyceride levels. Avocados and olive oil contain monounsaturated fats, also a healthful choice.\nOmega-3 fats are present in cod liver oil, flaxseeds, and cold-water fish, such as salmon and sardines. People can add PUFAs to their diet by eating these foods.\nInstead of a steak or hamburger, which are high in saturated fats, people can opt for a fillet of salmon or a tuna sandwich.\nAnimal products, such as lean meats, skinned poultry, fat-free or low-fat dairy, and seafood, are also good options.\nFats should represent between 25 and 35 percent of the diet, according to the AHA.\n3. Carbs: Eat the right sort\nPeople should limit their total carbohydrate intake to less than 60 percent of their recommended daily calorie allowance. If a person eats more carbohydrates than they need, the body will store them as fat.\nA rise in triglyceride levels seems to accompany diets with a carbohydrate intake above 60 percent.\nThere are many ways to avoid carbohydrates, such as wrapping lean burgers in lettuce instead of a high-carb bun.\nSome carbohydrate foods, including certain cereals, can be beneficial in the diet. However, refined carbohydrates, such as white bread, offer little nutrition and add calories to the diet.\nTo get more healthful carbohydrates, choose whole grains, oatmeal, and vegetables, such as carrots.\nFor dessert, opt for fresh or frozen blueberries, blackberries, or raspberries instead of sugary baked goods. These fruits can reduce sugar cravings while also providing healthful carbohydrates.\nUnrefined carbohydrates are not only a source of dietary fiber, but they provide more rapid and prolonged satiety than refined carbohydrates as they release their energy more slowly.\n4. Sugar: Reduce intake\nSugars are a form of carbohydrate, and they are high in calories. Foods that contain a lot of simple sugars, especially refined fructose, can raise triglyceride levels.\nAdded sugar comes in many forms, including:\n- white sugar\n- brown sugar\n- cane juice or cane syrup\n- corn sweetener or corn syrup\n- fruit juice concentrate\n- syrups, such as maple, agave, and molasses\nPeople should avoid added sugars to help reduce their triglyceride levels.\nOpt for fruit-based deserts instead of ice-cream or sticky puddings.\nWhen buying ready-to-eat products, remember that many contain—including some savory items, such as tomato ketchup—contain added sugar.\nTherefore, check the label before buying a product and try to find one with low sugar content.\nEvery 4 grams (g) of sugar is equivalent to 1 teaspoon (tsp). The AHA recommend a daily maximum sugar intake of 25 g (about 6 tsp) for women and 36 g (9 tsp) for men.\n5. Drinks: Choose with care\nDrink water instead of sweetened beverages. Even packet or bottled juices can be high in sugar.\nBeverages often make a significant contribution to overall carbohydrate and sugar intake.\nFruit drinks, soft drinks, and other sugar-sweetened beverages are some of the primary sources of added sugars in the diet.\nAlcohol also has a direct effect on triglyceride levels in some people. People looking to reduce their triglyceride levels may find it beneficial to avoid alcohol.\nTaking steps to avoid beverages containing added sugars can significantly reduce overall calorie intake.\nInstead of beverages that contain high levels of added sugars, people can opt for low-calorie beverages, such as water or tea.\nOn a warm day, instead of reaching for a soft drink, try adding a splash of 100 percent fruit juice to a glass of sparkling water.\nPhysical activity also plays a vital role in reducing triglyceride levels. Burning calories ensures that the body is using up more of its triglycerides.\nAny exercise is beneficial, but the extent of its effects will depend on:\n- the person's initial triglyceride levels\n- the amount of exercise\n- the level of intensity of the exercise\nWalking for 30 minutes each day is a great way to begin, as is engaging in low-stress activities, such as cycling or swimming.\nThe AHA recommend that adults do at least 150 minutes of moderate aerobic activity, 75 minutes of vigorous aerobic activity, or a combination of both each week.\n7. Drugs that can increase triglycerides\nSome drugs contribute to higher levels of triglycerides. According to a 2017 study, these include:\n- oral estrogen\n- retinoic acid derivatives\n- beta-adrenergic blocking agents\n- thiazide diuretics\n- protease inhibitors\n- bile acid sequestrants\n- antipsychotic drugs\n- cyclosporine and tacrolimus\n- interferon alpha-2b\nA person should not stop taking a drug without speaking to a healthcare professional first, as this can be dangerous. Anyone who has concerns about the side effects of a drug that they are using should also get professional medical advice.\n8. Have a health check\nA regular health check can help you keep track of your triglyceride levels.\nThe most common causes of high triglyceride levels relate to diet and metabolism. As well as excess body weight and a high-fat, high-carb diet, several health conditions can increase the risk.\nGenetic factors may also make a person more likely to develop high triglyceride levels under certain circumstances.\nIf tests show that a person's triglyceride levels are high or they have a family history of high triglyceride levels, a doctor might suggest further investigations or monitoring.\nTaking this action can help them find out if there is an underlying health problem or enable them to advise a person how to keep their triglyceride levels low.\nTriglyceride levels can also rise during pregnancy.\n9. Drugs to lower triglycerides\nIf other measures do not work, a doctor may prescribe medications, such as statins, to reduce triglyceride levels.\nSome doctors prescribe fibrates, which are lipid-lowering drugs, for people who cannot tolerate statins.\nHowever, more research is necessary to confirm this benefit and to establish whether these treatments are safe and the best way to use them.\nImportance of triglyceride levels\nIf a person's triglyceride levels are too high, they have a higher risk of certain diseases and disorders.\nHigh triglyceride levels may have this effect because they can cause a buildup of plaque in the arteries. Plaque is a combination of cholesterol, triglyceride fats, calcium, cellular waste, and fibrin, which is the material that the body uses for clotting.\nTriglyceride and cholesterol levels are two of the most important factors to monitor for a healthy heart.\nThere is also a risk of damage to the pancreas if triglyceride levels get too high.\nHealthy triglyceride levels\nAccording to the AHA:\n- At-risk levels range from 150–199 milligrams per deciliter (mg/dl).\n- High triglyceride levels range from 200–499 mg/dl.\n- Very high levels begin at 500 mg/dl.\nThe AHA still consider triglyceride levels of up to 150 mg/dl to be within the normal range, but they recommend keeping levels below 100 mg/dl for optimum health.\nIt is possible for people to lower their triglyceride levels by watching what they eat and adopting a nutrient-rich diet.\nEating plenty of fruits, vegetables, whole grains, legumes, nuts, and seeds can increase nutrient intake while reducing calories.\nA diet that is good for the heart and blood will also be low in sodium, refined grains, added sugars, and solid fats.\nPeople should work directly with a doctor or dietician to make gradual changes to their diet and ensure that any medicines they are taking will not cause complications."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:212699b7-e260-4d35-9885-9120b94fe3c9>","<urn:uuid:8d79fa7b-7dab-4dad-a64e-5d437ff642c8>"],"error":null}
{"question":"Could you compare the first aid training approaches for childcare settings between the Peel Paramedic Association's community programs and the HLTAID012 certification, specifically regarding the scope of emergency response training?","answer":"The HLTAID012 certification specifically focuses on education and care settings, covering comprehensive emergency response training for childcare workers, including specific protocols for infants, children, and adults. It includes training for asthmatic and anaphylactic emergencies, managing acute illnesses, and communicating with parents and emergency services. The Peel Paramedic Association's approach is more broadly community-oriented, offering first aid training to various groups including new parents, but doesn't specify a dedicated childcare-setting curriculum. Their training focuses on general first aid and CPR skills, delivered by experienced paramedics, with an emphasis on making bystander assistance more accessible.","context":["Our Programs & Initiatives\nCPR & FIRST AID TRAINING\nAre you looking to be certified in First Aid and CPR? Does WSIB require your employees have their first aid? Are you a new parent and would like to learn some first aid in case of emergency?\nThe Peel Paramedic Association (PPA) through an initial start up grant from the Ontario Trillium Foundation, has been actively involved in your community for many years offering First Aid and CPR demonstrations and certifications! Our goal is to show how simple bystander CPR is and have all citizens trained in Peel Region! The PPA has participated in many outreach events in collaboration with Peel Regional Paramedic Services, The Mikey Network, St. John Ambulance and Mississauga/Brampton Fire Departments. You can find us at different events throughout the region - school visits (preschool to adult), shopping malls, community centres, regional open houses, private events, neighbourhood parties etc.\nThe Peel Paramedic Association offers training and certification for all levels in First Aid and CPR. Our instructors are experienced paramedics with many years teaching various first aid levels. The first aid and CPR (with AED) skills will be taught in a practical, useful and clear manner leaving you feeling confident with your new knowledge! Not interested in being fully certified? Even without certification, a bystander can still help control a scene and perform \"compression only\" CPR! We offer quick trainings in basic first aid and CPR - no certification needed!\nHelp us take away the fear/stigma related to offering your valuable help in an emergency. Get trained in first aid! If interested in first aid training, having the PPA attend an event or would like to request information please contact email@example.com.\nTOQUES FOR THE HOMELESS\nThe PPA is proud to partner with PRPS Community Relations Officer Brad Bowie and A Platoon Superintendant Jay Szymanski in our first annual Toques for the Homeless campaign. Together we have purchased 300 bright coloured toques. Each response vehicle in the PRPS service will have two toques available to be given to homeless/those in need patients we encounter during our duties. There are no criteria required, other than Paramedics feeling the toques are needed. Replacements will be found at the divisional stations when you run out.\nFor more information on this great initiative, contact Brad Bowie at firstname.lastname@example.org\nANNUAL TOY AND DIAPER DRIVE\nEach year, the Peel Paramedic Association teams up with Peel Regional Paramedic Services. Through Paramedic donations at the stations, money raised at our Annual Golf Tournament, donations by various Peel Regional Departments and community corporate sponsors, the PPA has been able to fill ambulances with toys and diapers for Peel Children's Centre, a local charity supporting Peel's families. Starting in 2015, each year we include toys for hospitalized children in the children's wards at our three local hospitals, as well as Tim Horton's gift cards for the children's families.\nEach year the drive starts in November. Look for more information at the stations, on Pathways and here at www.peelparamedics.ca\nANNUAL FOOD DRIVE - HUNGER DOESN'T TAKE A HOLIDAY\nParamedics and staff of Peel Region Paramedic Services once again supported the Mississauga Food Bank during their 2016 summer food drive. The PPA food drive, \"Hunger Doesn't Take a Holiday\" saw more than 450lbs of non-perishable food collected throughout the month of August.\nThe event, in its second year, saw the Management Group once again claim the PPA cup \"May Our Cup Runneth Over\". Thank you to The Award Store of Mississauga who generously donated the PPA Cup.\nThe PPA would like to thank Superintendents Larry Titus and Lawerence Saindon for their support. We would also like to thank Brad Bowie for his assistance with the pick up and delivery of donations.\nWe were proud to be able to donate the food to an organization that partners with more than 40 community agencies throughout Mississauga. We hope to raise even more donations next year.\nBREAST CANCER AND MOVEMBER FUNDRAISING\nEach year the PPA partners with PRPS in raising funds for both Breast Cancer research (October) and Movember campaign in support of men's health (November). This year we are pleased to offer ribbon pins in support of these great initiatives.","HLTAID012 - Provide First Aid Response In An Education And Care Setting\n*Eftpos payments available on site.\nThis course includes:\nHLTAID009 Provide Cardio Pulmonary Resuscitation (CPR)\nHLTAID010 Provide basic emergency life support\nHLTAID011 Provide First Aid\nCompliant with ACECQA for the purposes of the Education and Care Services National Law as satisfying first aid, asthma and anaphylaxis management requirements for childcare workers. The units teaches the skills and knowledge required to provide a first aid response to infants, children, and adults. The units apply to childcare workers, educators and support staff working within an education and care setting who are required to respond to a first aid emergency, including asthmatic and anaphylactic emergencies.\nIt is delivered in line with the Australian Resuscitation Council guidelines.\nIt teaches how to identify illness or injury, access emergency services, safely provide first aid treatment using action plans, equipment and resources, for CPR, defibrillation, administering an autoinjector for anaphylaxis, administering asthma medication, immobilisation for envenomation, fractures, dislocations, sprains and strains, for bleeding and shock and other conditions.\nYou will learn basic anatomy, physiology and the differences between adults, children and infants relating to CPR, managing an infant or child with an acute illness, or who requires an immediate ambulance response and conveying incident details to parents, caregivers and emergency services.\nThe course covers completing incident documentation and debriefing for improvement of response, recognising psychological impacts, talking with children about their emotions and seeking help as required.\nUnits Being Delivered\nThe following unit(s) will be awarded to successful participants in this course. The certificate will be issued by Allens Training Pty Ltd RTO 90909\nHLTAID012 Provide first aid response in an education and care setting\nHLTAID0011 Provide first aid\nHLTAID010 Provide basic emergency life support\nHLTAID009 Provide cardiopulmonary resuscitation\nThis course will be delivered in the workplace or at an approved third party facility.\nPlease note that enrolment to this course is made with Allens Training Pty Ltd RTO 90909. Refer to the student handbook located on the RTO website\nhttps://allenstraining.com.au/students/student-handbook for all details relating to rights and responsibilities including complaints and appeals.\nParticipants must have the physical capacity to perform 2 minutes of uninterrupted CPR on the floor.\nPre-course study: Students may be required to undertake online pre-course work prior to attending the face to face session depending on the course delivery mode.\nOnline study: Students must have access to a computer, smartphone, tablet or other electronic devices with access to the internet to complete the online/ pre-course studies.\nStudents completing the 100% Online Course will require first aid training equipment for the video conference practical assessment session. Your training provider will arrange this upon your booking.\nCourse durations can vary for multiple reasons, so the durations below are the minimum possible amount.\n100% Online Training - self paced online learning with a scheduled 1.5 hour (minimum) video conference practical assessment with a trainer.\nFace to Face – 9 hours (minimum) full face to face course\nBlended delivery – 7 hours (minimum) face to face course with the addition of 2 hour self-paced learning to be completed prior to attending the face to face course\nRefresher training – 7 hours (minimum) face to face course – conditions apply for this option, for example, you will need to provide previous current certificates.\nOnline Training with Face to Face Assessment - self paced online learning with a scheduled 60 minutes (minimum) face to face practical assessment.\nCertificate Renewal Requirements\nIt is recommended that renewal for this course is undertaken every 3 years, with the additional requirement of updating the CPR component every 12 months.\nCourse Fees and Payments\nCourse fees for this course is $185.\nPayments terms - Payments for individual participants is upfront on the day unless prior agreements have been made.\nIndividuals undertaking this course will be expected to complete both written and practical assessment tasks.\nPerformance tasks and practical scenarios:\nSimulated First Aid Scenario – Perform CPR on an adult (incl. the use of an AED and placing a casualty into the recovery position)\nComplete a first aid incident report form based on the simulated first aid scenario\nPerform CPR on a child\nPerform CPR on an infant\nManage a casualty with anaphylaxis\nManage a casualty with asthma\nManage a choking casualty\nManage a casualty with non-life-threatening bleeding and shock, requiring minor wound cleaning\nManage a casualty with a nosebleed\nManage a casualty with a fracture and dislocation\nManage a casualty with a sprain and strain\nManage a casualty with envenomation (snake/ funnel-web spider bite)\nTheory assessment - A written assessment consisting of multiple-choice questions.\nTheory assessment: A written assessment consisting of multiple-choice questions."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:a9dbf12f-bd02-4cd0-b9a9-3ea4f69f3d95>","<urn:uuid:df1f6a87-09a3-4ef9-a7a0-50ef45af736a>"],"error":null}
{"question":"How can a company effectively implement automated receipt processing to detect anomalies and improve customer rewards distribution? Please explain the process and benefits in detail.","answer":"A company can implement automated receipt processing using Amazon Rekognition Custom Labels through the following process: First, collect and label approximately 200 receipts into valid and anomalous categories. Then, train a model using these labeled receipts through the Rekognition Custom Labels console. After achieving a satisfactory F1 score (around 97%), set up a pilot batch inference pipeline to process hundreds of receipts weekly. Finally, establish an inference endpoint to predict receipt validity in real-time. This implementation can lead to significant improvements, including increasing correct classification of anomalous receipts from 70% to 99%, reducing annual human review costs by $1.5 million, and accelerating customer reward distribution by 5 times. The solution requires minimal code and maintenance while only needing a small dataset to get started.","context":["This publish was co-authored by Arun Gupta, the Director of Enterprise Intelligence at Prodege, LLC.\nProdege is a data-driven advertising and marketing and shopper insights platform comprised of shopper manufacturers—Swagbucks, MyPoints, Tada, ySense, InboxDollars, InboxPounds, DailyRewards, PollFish, and Upromise—together with a complementary suite of enterprise options for entrepreneurs and researchers. Prodege has 120 million customers and has paid $2.1 billion in rewards since 2005. In 2021, Prodege launched Magic Receipts, a brand new means for its customers to earn money again and redeem present playing cards, simply by procuring in-store at their favourite retailers, and importing a receipt.\nRemaining on the chopping fringe of buyer satisfaction requires fixed focus and innovation.\nConstructing an information science crew from scratch is a good funding, however takes time, and sometimes there are alternatives to create fast enterprise influence with AWS AI providers. In accordance with Gartner, by the top of 2024, 75% of enterprises will shift from piloting to operationalizing AI. With the attain of AI and machine studying (ML) rising, groups have to deal with the right way to create a low-cost, high-impact answer that may be simply adopted by a company.\nOn this publish, we share how Prodege improved their buyer expertise by infusing AI and ML into its enterprise. Prodege wished to discover a option to reward its clients sooner after importing their receipts. They didn’t have an automatic option to visually examine the receipts for anomalies earlier than issuing rebates. As a result of the quantity of receipts was within the tens of hundreds per week, the guide means of figuring out anomalies wasn’t scalable.\nUtilizing Amazon Rekognition Customized Labels, Prodege rewarded their clients 5 occasions sooner after importing receipts, elevated the proper classification of anomalous receipts from 70% to 99%, and saved $1.5 million in annual human evaluation prices.\nThe problem: Detecting anomalies in receipts shortly and precisely at scale\nProdege’s dedication to top-tier buyer expertise required a rise within the velocity at which clients obtain rewards for its massively in style Magic Receipts product. To try this, Prodege wanted to detect receipt anomalies sooner. Prodege investigated constructing their very own deep studying fashions utilizing Keras. This answer was promising in the long run, however couldn’t be applied at Prodege’s desired velocity for the next causes:\n- Required a big dataset – Prodege realized the variety of photos they would wish for coaching the mannequin could be within the tens of hundreds, and they’d additionally want heavy compute energy with GPUs to coach the mannequin.\n- Time consuming and expensive – Prodege had lots of of human-labeled legitimate and anomalous receipts, and the anomalies had been all visible. Including extra labeled photos created operational bills and will solely perform throughout regular enterprise hours.\n- Required customized code and excessive upkeep – Prodege must develop customized code to coach and deploy the customized mannequin and preserve its lifecycle.\nOverview of answer: Rekognition Customized Labels\nProdege labored with the AWS account crew to first determine the enterprise use case of having the ability to effectively course of receipts in an automatic means in order that their enterprise was solely issuing rebates to legitimate receipts. The Prodege information science crew wished an answer that required a small dataset to get began, might create fast enterprise influence, and required minimal code and low upkeep.\nPrimarily based on these inputs, the account crew recognized Rekognition Customized Labels as a possible answer to coach a mannequin to determine which receipts are legitimate and which of them have anomalies. Rekognition Customized Labels supplies a pc imaginative and prescient AI functionality with a visible interface to routinely practice and deploy fashions with as few as a few hundred photos of uploaded labeled information.\nStep one was to coach a mannequin utilizing the labeled receipts from Prodege. The receipts had been categorized into two labels: legitimate and anomalous. Roughly 100 receipts of every type had been rigorously chosen by the Prodege enterprise crew, who had data of the anomalies. The important thing to a superb mannequin in Rekognition Customized Labels is having correct coaching information. The subsequent step was to arrange coaching of the mannequin with a number of clicks on the Rekognition Customized Labels console. The F1 rating, which is used to gauge the accuracy and high quality of the mannequin, got here in at 97%. This inspired Prodege to do some extra testing of their sandbox and use the skilled mannequin to deduce if new receipts had been legitimate or had anomalies. Organising inference with Rekognition Customized Labels is a simple one-click course of, and it supplies pattern code to arrange programmatic inference as properly.\nInspired by the accuracy of the mannequin, Prodege arrange a pilot batch inference pipeline. The pipeline would begin the mannequin, run lots of of receipts towards the mannequin, retailer the outcomes, after which shut down the mannequin each week. The compliance crew would then consider the receipts to verify for accuracy. The accuracy remained as excessive for the pilot because it was in the course of the preliminary testing. The Prodege crew additionally arrange a pipeline to coach new receipts with the intention to preserve and enhance the accuracy of the mannequin.\nLastly, the Prodege enterprise intelligence crew labored with the applying crew and help from the AWS account and product crew to arrange an inference endpoint that may work with their utility to foretell the validity of uploaded receipts in actual time and supply its customers a best-in-class shopper rewards expertise. The answer is highlighted within the following determine. Primarily based on the prediction and confidence rating from Rekognition Customized Labels, the Prodege enterprise intelligence crew utilized enterprise logic to both have it processed or undergo extra scrutiny. By introducing a human within the loop, Prodege is ready to monitor the standard of the predictions and retrain the mannequin as wanted.\nWith Rekognition Customized Labels, Prodege elevated the proper classification of anomalous receipts from 70% to 99% and saved $1.5 million in annual human evaluation prices. This allowed Prodege to reward its clients 5 occasions sooner after importing their receipts. The perfect a part of Rekognition Customized Labels was that it was simple to arrange and required solely a small set of pre-classified photos to coach the ML mannequin for prime confidence picture detection (roughly 200 photos vs. 50,000 required to coach a mannequin from scratch). The mannequin’s endpoints could possibly be simply accessed utilizing the API. Rekognition Customized Labels has been a particularly efficient answer for Prodege to allow the sleek functioning of their validated receipt scanning product, and helped Prodege save plenty of time and sources performing guide detection.\nRemaining on the chopping fringe of buyer satisfaction requires fixed focus and innovation, and is a strategic aim for companies right this moment. AWS pc imaginative and prescient providers allowed Prodege to create fast enterprise influence with a low-cost and low-code answer. In partnership with AWS, Prodege continues to innovate and stay on the chopping fringe of buyer satisfaction. You will get began right this moment with Rekognition Customized Labels and enhance what you are promoting outcomes.\nIn regards to the Authors\nArun Gupta is the Director of Enterprise Intelligence at Prodege LLC. He’s obsessed with making use of Machine Studying applied sciences to offer efficient options throughout numerous enterprise issues.\nPrashanth Ganapathy is a Senior Options Architect within the Small Medium Enterprise (SMB) section at AWS. He enjoys studying about AWS AI/ML providers and serving to clients meet their enterprise outcomes by constructing options for them. Outdoors of labor, Prashanth enjoys pictures, journey, and attempting out totally different cuisines.\nAmit Gupta is an AI Companies Options Architect at AWS. He’s obsessed with enabling clients with well-architected machine studying options at scale.\nNick Ramos is a Senior Account Supervisor with AWS. He’s obsessed with serving to clients remedy their most advanced enterprise challenges, infusing AI/ML into clients’ companies, and assist clients develop top-line income."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:69d34f83-3fac-41d1-91dd-74dc47c05740>"],"error":null}
{"question":"What are the differences between the restrictions on freedom of movement for black South Africans under apartheid and Saudi women today?","answer":"During apartheid, black South Africans were prohibited by law from moving freely within their own country and had to use separate transportation systems from white citizens. In Saudi Arabia, while women face significant restrictions on their movement, these mainly revolve around the male guardianship system where women cannot apply for a passport or travel outside the country without their male guardian's approval. Some women need permission to leave their homes, though this varies by family. Unlike apartheid South Africa's racially segregated transportation, Saudi Arabia does not have legally mandated separate transportation systems - the restrictions are based on guardian permission rather than segregated facilities.","context":["Australia/Israel Review, Featured\nEssay: The Apartheid Lie\nMar 30, 2020 | Olga Meshoe Washington\nThe appropriation of South Africa’s history\nImagine having your food served to you on a tin plate and not a normal ceramic one, because of the colour of your skin. Imagine having to use the designated, concealed back entrance of a public hospital to be checked by a doctor in a designated room, out of sight of other patients of a different race. Those are two of the many experiences my parents and millions of other black South Africans experienced during apartheid South Africa.\nTHE APARTHEID LIE\nIn recent years, the term “apartheid” has become so synonymous with the State of Israel that it has almost lost its original meaning: the policy of the South African regime from 1948 to 1994 that segregated and discriminated black South African citizens from white South African citizens. This regime was regulated and institutionalised by a system of over 150 codified laws. By law, black people were dispossessed of their land, homes, and livelihoods, and forcibly relocated to designated, under-developed areas. By law, we black South Africans were prohibited from using the same transportation system, attending the same public schools or enjoying the same public facilities as white South Africans. By law, we could not move freely within our own country and were not allowed to participate formally in the main economy of the country. We were denied the right to vote. We were forbidden from marrying the person we loved if he or she were of a different race group.\nIn addition, black people of different tribes were separated and grouped into mini-homelands to further strip us of our identity as black South Africans. Growing up in apartheid South Africa, I was told by white society that as a black person, I could dream only certain dreams; all others were reserved for white people.\nThe above descriptions of apartheid South African life are the antithesis of contemporary Israeli life.\nIn Israel, by law, Israeli Arabs have the same rights as Israeli Jews. They study in the same school system and are treated as equals to all other Israelis in the same hospitals. Israeli Arabs vote, are elected to the Knesset, and have become Supreme Court justices. Although not required to join the army, some Arab Christian and Muslim citizens of Israel choose to serve in the Israel Defence Forces. Israeli Arabs enjoy the same privilege as other Israelis to study for academic degrees at Israeli universities of their choice.\nPalestinians who live in east Jerusalem also enjoy this privilege, despite not being Israeli citizens. Buses and trains are open to all; they do not have signs saying “Jews only” or “Arabs only” to separate commuters, as was the case in South Africa.\nSome argue that Israel’s Nation-State law (formally titled Basic Law: Israel as the Nation-State of the Jewish People) adopted by the Israeli Knesset on July 18, 2018, which legally enshrines Israel as the nation-state of the Jewish people, confirms Israel’s “apartheid” character.\nHowever, it does not impact or detract from the existing rights to equality and dignity of all Israeli citizens which remain enshrined and protected in Israel’s Basic Law: Human Dignity and Liberty. The Nation-State law merely reaffirms Israel’s Jewish majority character and underlying Zionist founding principles, the very reason for its modern creation as the democratic nation-state of the Jewish people.\nIsrael’s Jewish character was recognised and validated by the League of Nations in 1922, its successor organisation the United Nations in 1945, and again upon Israel’s acceptance as a formal member of the United Nations in 1949.\nJust as Japan is the homeland of Japanese people and France is the homeland of French people, Israel is the homeland of Jewish people. The existence of these sovereign countries as homes for their respective peoples is not discriminatory in nature. Of all these nations, and so many others in the 193 member states of the United Nations, Israel is the only nation accused of being an apartheid state. This malevolent double standard constitutes antisemitism according to the internationally accepted 2016 International Holocaust Remembrance Association working definition of antisemitism.\nDespite being a Jewish State, Israel’s population is comprised of approximately one-quarter Muslims and it is the only state in the Middle East in which other religions, such as Christianity, not only coexist with Judaism but are thriving.\nIsraeli Jews themselves are of more than one colour; more than half of the Israeli Jewish population are descendants of immigrants from North African and Middle Eastern lands. Jews from India, China, and South America also call Israel home. Said differently, the majority of Israel’s population is non-white.\nTHE PEDDLING OF AN ANTISEMITIC LIE\nAnti-Zionists often refer to the late former president of my country, Nelson Mandela, as an authoritative validator of the apartheid lie. One of the most used quotes from Nelson Mandela for this purpose is, “We know too well that our [South Africa’s] freedom is incomplete without the freedom of the Palestinians,” from his speech given on the International Day of Solidarity with the Palestinian People in 1997.\nWhat is not told is that Mandela visited Israel in 1999, something those who perpetuate the apartheid narrative do not want the world to know. On this visit, Mandela said, “I cannot conceive of Israel withdrawing [from territory] if Arab states do not recognise Israel within secure borders.” This is a Zionist statement. While Nelson Mandela was pro-Palestinian, he was not anti-Israel.\nIn addition to dishonestly misrepresenting the positions of authoritative and respected individuals such as Mandela to underpin their deceptive narrative, information sites aimed at journalists are often guilty of furthering the apartheid narrative.\nFor instance, the Institute for Middle East Understanding (IMEU) is a self-described resource hub for journalists seeking information on the socio-economic, political, and cultural aspects of Israel (which it calls ”Palestine”) and Palestinians, for purposes of educating the general public. Despite touting itself as an independent non-profit organisation, IMEU describes Israel as an “occupier” that engages in “ethnic cleansing” in Jerusalem. It also nefariously ascribes Israel’s administration of the West Bank, the result of a bi-lateral diplomatic agreement with the Palestinian Liberation Organisation and which was internationally witnessed and guaranteed by the Oslo Interim Accords in 1995, as the basis for Israel being an apartheid state.\nThe Oslo Interim Accords divided the West Bank into three zones. Depending on the zone, Israel or the Palestinian leadership was assigned all, some, or no civil and security jurisdiction of the zone in question. Not only is it factually incorrect and dishonest to describe Israel’s civil and/or security administration of the zones, per the Oslo Accords, as “apartheid”, it is a deliberate omission and distortion of history.\nIt is true that racism exists in Israel. Racism also exists in mature democracies such as the United States and Great Britain. It also exists in today’s democratic South Africa. If the United States, Great Britain, or South Africa are not described as an apartheid state, it begs the question why Israel is singled out as being an apartheid state because racism can be found within it.\nA COLONIAL-SETTLER STATE?\nWhat of the argument that Israel is a colonial-settler state? That question may be answered by another: can a native become a settler?\nFrom as far back as the second millennium BCE, there has always been a significant Jewish presence in the land, which comprises the modern state of Israel, Gaza, and Judea and Samaria – the disputed territories of the West Bank. Historical data and archaeological artifacts testify to the existence of Jewish culture, politics, and an economy for the past 3,000 years. These demonstrate that Jews are the indigenous people of the land. The immigration of Jews from across the world to the State of Israel does not equate to the increasing occupation of dispossessed land by the dispossessors, but the return of the indigenous people to their homeland. This homeland includes east Jerusalem and west Jerusalem. Indigenous people cannot be settlers. The Jews are not settlers. Israel is not a coloniser.\nCalling Israel a colonial-settler state is an insult to every African nation that was a victim of colonialism. It also dismisses the fact that the economic and political instability that characterises much of Africa today owes most of its existence to Europe’s egregious colonisation of all but two African nations.\nTHE TRUE COST OF THE ISRAEL APARTHEID NARRATIVE\nLies empower evil. Lies about blacks empowered apartheid in South Africa. Lies about Jews made the Holocaust possible.\nWith all its imperfections, Israel is not an apartheid state. This false claim masks the true antisemitic intentions of those who call Israel an apartheid state. It has misled many well-intentioned people around the world into opposing the only true democracy in the Middle East.\nThis apartheid lie continues to embolden antisemitic acts on innocent Jews in the privacy of their homes, during their times of worship, and on university campuses. It odiously characterises non-Jewish Zionists and supporters of Israel as accomplices of Israel’s fictitious crimes against humanity. More importantly, it compromises the chances of peace in the Middle East.\nEqually important but oftentimes forgotten, the apartheid label assigned to Israel redirects focus away from holding the Palestinian Authority and Hamas accountable for their ill-treatment and abuse of the Palestinian people.\nDISHONOURING SOUTH AFRICA AND SOUTH AFRICANS\nCalling Israel an apartheid state trivialises the humiliation and injustices endured by black South Africans who lived through apartheid and who still, together with their children and grandchildren, bear the scars of its legacy. If black South Africans enjoyed the rights enjoyed by Israeli Arabs, there would have been no need for South Africa’s liberation movement. There would not have been a Nelson Mandela, as the world knows him, or other freedom fighters who spent much of their lives incarcerated, and whose families sacrificed much for the democracy South Africa enjoys today.\nIt is thus morally repugnant for any person, any organisation, or any government to incorrectly appropriate South Africa’s apartheid history to apply to Israel. It is also repulsive to rally people across the world on the painful, collective, real experiences of black South Africans for a cause premised on falsehood.\nBlack South Africans must declare that their moral authority on what apartheid is and what it is not cannot be bought; that their history cannot be manipulated to perpetuate a narrative that erases the boundary between legitimate criticism of policies of the Israeli government and antisemitism.\nIt is incumbent upon all persons who genuinely desire to see peace in the Middle East and who have a sincere interest in the liberation of the Palestinian people from their oppressive leaders, to seek the truth and speak the truth against a narrative that is the core of an agenda to delegitimise, demonise, and ultimately destroy the State of Israel and Jewish life in the diaspora.","A 2009 report by the UN questioned whether any worldwide law ratified by the government has ever been applied inside Saudi Arabia. In January 2020, Saudi Arabia hosted the Spanish Super Cup for the first time. The match hosted Barcelona, Valencia, Atlético Madrid and Real Madrid because the 4 participants. The rights group also knowledgeable that the match day marked Loujain’s 600th day in detention. In January 2020, Human Rights Watch among 12 other international human rights organizations wrote a joint letter to Amaury Sport Organisation ahead of Saudi Dakar Rally. The rights group of their statement urged ASO to make use of their determination to denounce persecution of ladies rights within the nation. “The Amaury Sport Organisation and race drivers at the Dakar Rally should speak out in regards to the Saudi government’s mistreatment of ladies’s rights activists for advocating for the proper to drive,” the statement from HRW learn.\nThis Is The Way You Fix Your Broken Women In Saudi Arabia\nMany different conservative Saudi residents argue that cards, which show a girl’s unveiled face, violate purdah and Saudi custom. Nonetheless, ladies’s rights to free movement and to an ID card have progressively been loosened. Women couldn’t vote or run for workplace in the country’s first municipal elections in lots of decades, in 2005, nor in 2011. They campaigned for the proper to do so in the 2011 municipal elections, attempting unsuccessfully to register as voters. In September 2011, King Abdullah introduced that ladies can be allowed to vote and run for office within the 2015 municipal elections. Although King Abdullah was no longer alive on the time of the 2015 municipal elections, ladies had been allowed to vote and stand as candidates for the first time in the country’s historical past. Salma bint Hizab al-Oteibi was the primary girl to declared an elected feminine politician in the nation.\nReaching this level of equality means the relationship between a woman and her father, husband or brother, ought to be based mostly on belief, open communication and a way of accountability. I lived abroad for eight years from 2008 and each time I visited Jeddah to see my family I used to note gradual adjustments. These had been at a societal level somewhat than because of adjustments in rules. Growing up in the ’90s, my generation was surrounded by more taboos than accepted practices. These cultural taboos, backed by an extreme interpretation of religion, have been prevalent in a society segregating women and men.\nThe lack of dependable public polling and free speech makes it tough to gauge how Saudis view women’s altering standing. But one research, from 2018, instructed that fear of social stigma could drive opposition greater than personal resistance. Compared with women they had seen in Jeddah, the Saudi port city where looser social customs allowed girls to go unveiled, wear their all-overlaying robes open over jeans and mingle with men in public, the sisters felt nameless, forgettable. But the modifications driven by Crown Prince Mohammed bin Salman, the dominion’s de facto ruler, have complicated that image over the previous couple of years, codifying for girls the best to drive, attend sporting occasions and journey with no man’s permission, among others. As the social codes that long governed their lives relax their grip, extra women are carrying their hair uncovered and mingling overtly with men — no less than in bigger cities.\nBut he has also cracked down on ladies’s rights activists, putting numerous them on trial in latest months. No country restricts the motion of its female population more than Saudi Arabia. Women can not apply for a passport or travel exterior the nation without their male guardian’s approval, restrictions the Interior Ministry imposes and enforces. In apply, some women are prevented from leaving their properties without their guardian’s permission and guardians can search a court docket order for a girl to return to the family residence. The travel restrictions make it very troublesome for Saudi girls to flee the nation. Many resort to hacking into their male guardian’s telephone to alter their travel permission settings or run away from relations while outside the country. Under the male guardianship system, a person controls a Saudi lady’s life from her delivery till her dying.\nConservative clerics have successfully rebuffed makes an attempt to outlaw baby marriage. Women were not allowed to vote in the nation’s first municipal elections, though Abdullah supported a girl’s right to drive and vote. Norah Al-Faiz, the first feminine cupboard member, will not appear without her veil, seem on television with out permission, or talk to male colleagues except by videoconferencing. The Ulema, Saudi’s non secular authorities, opposed the idea of issuing separate id cards for ladies.\nThis is extra to avoid offending people than truly running into bother with the legislation (though, in some extreme circumstances, the law would be capable of intervene. The arrest of ladies human rights defenders is a part of a broader crackdown in Saudi Arabia on freedom of expression, affiliation, and assembly. Crown Prince Mohammad bin Salman continues to promote his ‘reforms’ to the international public, while silencing anyone at home who dares to query his insurance policies. She has campaigned for civil and political rights, women’s rights, and the rights of the Shi’a minority in the Eastern Province of Saudi Arabia for many years. She stood in municipal elections in 2015, but was banned from taking part. She has additionally campaigned for the proper of girls to drive and for the top of male guardianship system.\nLegally, female voters could register with out the approval of a guardian — but in practice, this was hardly ever the case. Women right here infamously can not drive, and many households insist they go away residence with a male family member. “The first impediment can be mobility — their husbands didn’t allow them to go to register,” says AlMaeena. This kind of public smear marketing campaign performed in opposition to the women human rights defenders in Saudi Arabia is a tactic usually utilized by governments in an effort to discredit activists and silence dissent.\nTherefore, Saudi Arabia presents itself as a particularly difficult political environment for women’s rights activists to demand legal reform. Added to that, civil society organisations, together with girls’s teams, are not allowed in Saudi Arabia.\nSaudi Arabia’s economic system made the biggest progress globally toward gender equality since 2017, according to a World Bank report launched Wednesday. Outside the Kingdom, it helps refurbish his picture tarnished as it’s by the killing of Jamal Khashoggi – at a time when the Saudis are selecting to play an more and more visible function on the world stage. But the main ladies involved in that campaign are actually either detained or overseas.\nIn 2006, U.S. ambassador John Miller, Director of the Office to Monitor and Combat Trafficking in Persons, said the compelled labor of foreign girls home employees was the most typical type of slavery in Saudi Arabia. Miller claimed human trafficking is a problem all over the place, but Saudi Arabia’s many international home staff and loopholes in the system trigger many to fall victim to abuse and torture. If a divorce takes place, women could also be granted custody of their young children until they attain the age of seven. Older children are often awarded to the daddy or the paternal grandparents. Women can not confer citizenship to youngsters born to a non-Saudi Arabian father.\nAs it stands, the lifting of the driving ban does not translate right into a concrete shift within the prevailing legal and cultural mindsets that originally opposed it. Rather, it is an oblique approach to strengthen Saudi’s energy in economic and political terms. Yet, although women in Saudi Arabia might feel uncertain about the authorities’s intentions, time stays to be their best ally. Al-Hathloul’s family stated the prosecutor’s evidence included her contacts with rights group Amnesty International.\n- They criticize the U.S. government for publicizing oppression by enemies such as the Taliban, although its allies, like Saudi Arabia, have comparable policies.\n- In distinction, political commentator Daniel Pipes, sees Saudi gender apartheid as tempered by different practices, such as permitting women to attend college and work.\n- Local and international women’s teams have pushed Saudi governments for reform, profiting from the fact that some rulers are desperate to project a extra progressive picture to the West.\n- An internet radio station that is promoting ladies’s rights from overseas, introduced by way of Twitter that it would broadcast on a weekly basis.\n- The presence of powerful businesswomen—still a rare breed—in some of these teams helped to increase women’s illustration in Saudi Arabian authorities and society.\n- Mary Kaldor views gender apartheid in Saudi Arabia as much like that enforced by the Taliban in Afghanistan.\nThe Insider Secrets of Saudi Arabia Women Found\nTraditionally, ladies have been excluded from studying engineering, pharmacy, architecture, and regulation. Saudi ladies typically specify education as the most important space for women’s rights reform.\nThey gave males the authority to make choices for women of their families. Last week marked another historic day for Saudi ladies, giving them hope for a brighter future for them and their daughters. As Saudi Arabia allows ladies to travel without a man’s consent, Lulwa Shalhoub, a resident of Jeddah, explains how the transfer will affect her life. Ms. Alshaikh, who by no means had to sneak round her parents, sees the place of ladies in another way.\nMany ladies’s schools use distance schooling to compensate for girls’s poor access to transportation. Since there are few feminine lecturers, some universities use videoconferencing to have male professors educate female students with out face-to-face contact. Saudi Arabia is the house of Princess Nora bint Abdul Rahman University, the world’s largest women-solely college saudi arabia beautiful woman. Religious perception about gender roles and the perception that education is more relevant for men has resulted in fewer academic opportunities for girls. The tradition of sex segregation in professional life is used to justify proscribing girls’s fields of examine."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9dd6472c-bd61-402e-bf25-89cb49a2fcb3>","<urn:uuid:dc0cf1fe-f769-4e1c-af60-00c2061f9eaa>"],"error":null}
{"question":"¿Cómo se compara el clima de invierno entre Berlín y Praga?","answer":"Both cities experience cold winters, but with some differences. Berlin has an average daily winter temperature of -1°C to 1°C (30°F to 34°F), with lows dropping to -3°C to -1°C (28°F to 30°F). The city receives 34-51mm of precipitation during winter months, which can fall as snow. Prague also experiences very cold winters with frequent but generally not abundant snowfall. While both cities have continental climates, Berlin benefits from an urban heat island effect that keeps it warmer than surrounding areas, while Prague is described as having a typical European continental climate pattern.","context":["Berlin, Germany: Live Weather\nLive weather in Berlin\nThe latest and today's weather in Berlin , Germany updated regularly\nLoading live weather.\nLatest Berlin Holiday Reviews\nBERLIN IN THE SPRING\nWe travelled in April 09. The average temperature was 26 degrees and very nice! It can be hit and miss...\nHistoric Temperatures for 25th February in Berlin\n|Average High||4°C (39°F)|\n|Record High||13°C (55°F) (2008)|\n|Average Low||-1°C (30°F)|\n|Record Low||-6°C (21°F) (2006)|\nBerlin is the capital city of Germany and enjoys a temperate continental climate. Residents and visitors to this part of Western Europe experience all four seasons in the year, with warm summers and cold winters. The area receives moderate rainfall all year round, but overall the weather is great for visiting any month.\nTypically, the eastern portions of Germany have colder temperatures throughout the year than the western parts. Berlin is an exception, staying warmer than the surrounding countryside and nearby towns like Lubars and Grunheide. The warming is thanks to its urban setting of buildings and roads that create an insulating effect. During the winter months, snow and frost are possible from December through March. In the summer, the humidity increases as the temperatures rise, but fortunately, neither get high enough to be uncomfortable.\nThe city has a storied history, with numerous national governments setting up there throughout the years including the Kingdom of Prussia, the Weimar Republic and Nazi Germany. In World War II, much of the city was devastated and destroyed by air raids and bombings. Most of the other buildings that remained were torn down and rebuilt in the years after the war. Some were rebuilt to honour the city and country's history, while others took on more modern architecture.\nWhen you visit, along with history, you'll be treated to culture, nightlife and sports. The city is home to several music, art and dance communities and it has over 44 theatres and stages. Even if you aren't a partier, you have to check out the incredible nightlife of the city, with a plethora of restaurants, bars, clubs and event venues.\nThe people of Berlin are active and the city hosts a number of major sporting events throughout the year. It hosted the Summer Olympics in 1936, the FIFA World Cup in 2006 and the UEFA Championship Finals in 2015. Other events include the renowned Berlin Marathon, beach volleyball Grand Slam and numerous BMX events.\nNo matter where you choose to stay in the city, you'll be near the attractions and main sights thanks to an excellent public transportation system of buses, trains and trams. The city has hotels to fit every budget, from five-star brand name properties to budget-friendly smaller hotels. Fortunately, nearly any time of the year is a good one to visit Berlin, as the city has so much to offer visitors every month. Even in the dark days of winter, the city has plenty of indoor activities to keep you occupied and entertained.\nSpring Weather in Berlin\nIn the spring in Berlin, you can expect mild temperatures that quickly rise toward the end of the season.\nThe average daily temperature sits between 4°C and 14°C (about 39°F to 57°F). During the warmest parts of the day, the highs get up to 8°C to 19°C (around 46°F to 66°F), great temperatures for getting outside and exploring the city. It can still get pretty cold in the evening and overnight hours, with lows getting down to 0°C to 8°C (about 32°F to 46°F).\nRainfall amounts are moderate as well, with 36mm to 55mm of rain falling on average throughout the season. The months of March, April and May see between 13 and 14 days of rain on average, so it's a good idea to have an umbrella handy. You'll get between five and nine hours of sunshine each day, plenty of time to enjoy the outdoors.\nSummer Weather in Berlin\nWhen the summer rolls around in Berlin, the temperatures continue to rise and the rainfall amounts are all over the board. The average daily temperature is 17°C to 18°C (around 63°F to 64°F). Highs jump up to 22°C to 23°C (about 72°F to 73°F) with some humidity. Even with the more humid conditions, it's still nearly perfect weather for visiting Germany's capital city. July and August are the hottest months, with highs getting up to 23°C (about 73°F). At night, the lows drop down to 12°C to 13°C (around 54°F to 55°F).\nJune sees the most rainfall out of the year, with 71 mm falling across the month and 14 rain days on average. The other months have 45 mm to 62 mm of rainfall with 13 to 14 days of rain each month. Even though the rain amounts are similar to the rest of the year, summer feels wetter than the other months because the rain that falls tends to come all at once in bigger rain and thunder storms.\nThe longest days also come in June, with 11 hours of sunshine each day on average that month and 9 to 10 hours of daylight the other months.\nAutumn Weather in Berlin\nThe seasons continue to change in Berlin, with Autumn ushering in cooling temperatures and lower rain chances from the summer. The average daily temperature hovers anywhere between 5°C and 15°C (about 41°F to 59°F), with the cooler temperatures at the end of the season. The highs still get up to 7°C to 19°C (around 45°F to 66°F), great weather for enjoying the outdoors and sightseeing in the city. Expect lows to get down to 2°C to 10°C (around 36°F to 50°F).\nDuring the months of September, October and November, the average rainfall is 36mm to 48mm. That amount falls over 12 to 15 days on average each month.\nBe prepared for the wetter weather with a good umbrella or poncho. You'll get between three to seven hours of daylight each day to enjoy the outdoors.\nWinter Weather in Berlin\nCold temperatures and the possibility of snow comes in the winter months, with an average daily temperature of -1°C to 1°C (around 30°F to 34°F). In the warmest parts of the day, it only gets up to 2°C to 4°C (about 36°F to 39°F). Lows drop down to -3°C to -1°C (about 28°F to 30°F). If you don't mind the cold, Berlin can be beautiful in the winter, especially around the holidays and when the snow settles on the urban architecture. January is the coldest month, with a low of -3°C (about 27°F).\nDecember, January and February see 34 mm to 51 mm of precipitation each month, with some of that falling as snow. The number of rain days each month is 12 to 16 days, so have rain gear ready just in case.\nThe days are much shorter, with two to three hours of daylight on average each day, giving you a good excuse to head inside and explore the many museums, theatres and other indoor activities of the city.\nFortunately, Berlin and the country of Germany don't experience many weather hazards and extreme weather. Occasionally, large wind storms will sweep across Europe, causing damage. These cyclones can cause land and structure damage, as well as loss of life. Fortunately, they are few and far between and more than likely, you won't face any crazy weather during your stay. Check the local forecast before you go to know exactly what to expect each day.","The best times to visit Prague are the spring and early fall when the weather is mild and there are fewer crowds . Because of the city’s generally chilly climate, the warmer summer months (average high temperatures hover in the low to mid-70s) see the largest influx of tourists – which means higher hotel rates.\nPrague Weather in October : Temperatures noticeably cool in October with autumn now in full swing. As the high average temperature does drop quite a bit, to 13°C this month, exploring outdoor sights is still likely to be pleasant with a warm sweater and perhaps a scarf.\nLocated in the heart of Europe, the Czech Republic sees typical continental weather with long hot summers and very cold winters. The best time to visit is any time between June and September, unless you are planning on heading to the ski slopes, in which case December to March is better.\nThe climate of the Czech Republic can be described as typical European continental influenced climate with warm, dry summers and fairly cold winters. In summer daytime temperatures reach 20-25°C, but sometimes quite higher, 30°C or more.\nTop 10 Things to Avoid in Prague Sightseeing. Wasting Time Waiting for the Cuckoo. Charles Bridge in the Middle of the Day. Getting around. Getting Pickpocketed on the 22 Tram. Getting Ripped off by Taxis. Shopping and money. Tacky Souvenir Shops. Rip-off Exchange Offices. U Fleku’s Pushy Waiters. Wenceslas Square Sausages. Restaurants on Old Town Square.\nPrague can be a very cheap city to visit but it can also be very expensive . It depends where you pull out your wallet. Because there are so many tourists and almost all of them visit the same few sites, it is just good business sense for a shop or restaurant owner to raise their prices and collect as much as they can.\nHere are a few local Prague purchases to give you an idea of local item prices in Prague:\n|THE PRICE OF:||PRICE CZK||$USD|\n|Classic Czech meal at restaurant- meat, sauce, dumplings||140||6.1|\n|Large sausage, bun and mustard at Wenceslas Square||40||1.7|\n|500 ml of beer draft (pint)||35||1.5|\n|750ml bottle of wine good enough to bring to party||160||7.0|\nThe rate of violent crime is low and most areas of Prague are safe to walk around even after dark. Be careful on Wenceslas Square. It is usually packed with tourists and the crowds make things easy for pickpockets. There have also been cases of trusting “love-seekers” being robbed of all their money at night .\nThe currency in Prague is the Czech Crown (CZK). Some hotels, shops and restaurants accept Euros as well, but many only take Czech Crowns.\nAn average tourist will spend around 2500 CZK (100 EUR) per person per day . The lowest daily budget can be as low as 900 CZK if you stay at hostels, eat takeaways and use public transport. If you stay in private accommodation, eat at average restaurants but control your budget you can get by on 2500 CZK a day .\nEnglish in Prague In Prague , a great number of native citizens speak English at least a bit. And at the tourist hotspots, restaurants in the centre, hotels, and gift shops, knowledge of the English language is taken for granted.\nTo really see Prague, it’s best to visit for four to five days . That will allow you to see all the main sites and get a sense of the city’s culture.\nPrague . In Prague , 525 mm (20.5 in) of rain or snow fall per year. Since the amount of precipitation falling during winter is not high, snowfalls are frequent but generally not abundant.\nOVERALL RISK : LOW. The location score shows that Prague is a very safe city, and most visitors to the Czech Republic experience no difficulties. Pickpocketing is an issue in Prague, and not only for tourists. The usual precautions like keeping an eye on your wallet and securing your bags are necessary.\nThe 8 Best Neighborhoods in Prague for Tourists Malá Strana. Historic Malá Strana encompasses the eastern area of the Vltava River and sits just under Prague Castle. Staré Mesto (Old Town) Nové Mesto (New Town) Vinohrady & Vrsovice. Smichov & Vysehrad. Bubenec & Dejvice. Letna & Holecovice. Karlin & Zizkov ."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1b7078ab-7793-4add-a46f-855bb2ecbc10>","<urn:uuid:0550309c-9004-4e54-8ff0-ef88a2602855>"],"error":null}
{"question":"What's the difference in treatment duration between jock itch and sweaty sock syndrome?","answer":"Jock itch typically improves with antifungal treatment within two weeks, and treatment should continue for some time after symptoms subside to prevent recurrence. In contrast, sweaty sock syndrome has no specific treatment duration as it usually resolves on its own before puberty. While jock itch requires specific antifungal medications, sweaty sock syndrome is managed through general measures like using well-fitting shoes and applying petroleum jelly.","context":["Athlete's foot (Tinea) is a fungal infection that causes itchy blisters which usually occur between the toes. Treatment options, such as antifungal creams, can improve symptoms within a few weeks.…\nWhat is jock itch?\nJock itch is an uncomfortable itchy, red rash in the groin area, caused by a fungal skin infection. It is referred to as jock itch, because athletes (colloquially known as 'jocks') commonly get the condition. However, anyone can potentially develop it, particularly individuals who have a fungal skin infection elsewhere on the body.\nJock itch, which is medically known as tinea cruris, is a common fungal skin infection, caused by a group of mold-like fungi called dermatophytes. These fungi also cause other skin infections (collectively known as tinea) such as athlete's foot, when it affects the feet, and ringworm, when it affects other areas of the body. The fungi live off dead skin cells and thrive in warm, moist surfaces, such as those found in the groin when it gets sweaty. You can catch these fungi from close contact with someone who has tinea, or contact with unwashed clothing or towels used by someone with tinea.\nAnyone can develop jock itch, however, athletes more commonly get it due to regular sweating while playing sports, chafing from wearing protective clothing and/or sharing clothes. It is also more common in men. The risk of developing jock itch can be increased by being obese, which results in moisture being trapped in skin folds. Medical conditions, such as diabetes, increase the risk by lowering the body's immune defenses. The severity of jock itch can vary upon the presence of these underlying risk factors.\nSigns and symptoms\nThe common symptoms are itching, chafing and/or burning in the groin, thigh and anal area. There is often a characteristic red, raised rash that commonly has a ring-shape, which spreads down the inner thighs from the groin area (much like ringworm elsewhere on the body).\nMethods for diagnosis\nPotassium hydroxide exam\nA scraping of your skin from the affected area is added to potassium hydroxide (KOH) and examined under a microscope. KOH destroys non-fungal cells, making it easier to see if any fungal cells are present.\nIf fungus is present on the skin, it can be cultured using a special gel plate that selectively grows it.\nA skin biopsy is performed only when other skin infections need to be ruled out.\nTypes of treatment\nGood hygiene practices\nRegularly bathing followed by thorough drying with clean towels, particularly using a separate towel for the affected area, will help to contain the infection. Changing your clothes, especially the underwear or other clothing in direct contact with the affected area, at least daily, will also help.\nJock itch can be treated with topical antifungal creams that are available over the counter, such as ketoconazole and terbinafine, applied directly to the affected area. Oral antifungal medications such as fluconazole are used for infections that don't respond well to topical cream. Treatment for some time after symptoms have subsided is required to prevent it returning. It is also advisable to treat other fungal skin infections, e.g. athlete's foot, to prevent spread throughout the body. If symptoms persist for longer than two weeks, seeing your doctor will help.\nAntifungal side effects\nOral antifungal medications can cause nausea and liver damage if used for a prolonged period of time. Antifungals applied to the skin (topical) can cause itching and a burning sensation.\nJock itch can cause breaks in the skin, which can lead to concurrent bacterial infections that will require separate treatment with antibiotics and may also cause permanent scarring.\nAlthough jock itch can usually be cured, it is common for it to come back.\nMaintaining good hygiene by washing and drying properly, washing linen and towels in hot water and not sharing them with others, can help prevent the spread of jock itch.","Itching feet and toes is a very common occurrence and not always a medical problem. Everybody experiences itchy feet at some time or the other in their lives. There are many causes for itchy feet and toes. Not all of these causes are pathological in nature. In fact, most cases of itchy feet and toes are a normal and temporary sensation caused by a simple irritation of the skin.\nFor example, by walking the whole day while wearing socks and shoes that do not provide adequate ventilation to the feet and promote accumulation of sweat is one common reason for itchy feet. However, the itchy sensation in this instance usually relieves when the socks and shoes are removed, and the sweat evaporates.\nThere are also pathological conditions (such as skin diseases) that can result in itchy feet and toes. In many such conditions, the problem is isolated to the feet and toes like in athlete’s foot while in other cases it may also extend beyond the feet to the ankle area and sometimes even high up to the lower leg. Others may be more widespread skin conditions affecting multiple regions of the body as is seen in psoriasis where the rash and itching may occur anywhere on the body.\nCauses of Itchy Feet and Toes\nTechnically referred to as tinea pedis, athlete’s foot is a fungal infection of the feet. Tinea pedis infection results in reddish and itchy infectious patches on various parts of the foot, such as the skin between the toes, on upper part and sides of the feet, and on the soles. The infectious skin patches may also crack and peel off. As the name suggests, athlete’s foot is a condition that afflicts many athletes. However, anybody who has wet, sweaty feet for long periods of time can get tinea pedis.\nAthlete’s foot is infectious, and one may contract it through sharing of personal items (such as socks, shoes and towels) with an infected person. Treatment of athlete’s foot consists of anti-fungal medications (both oral and topical). Many of the anti-fungal medications are available over-the-counter. The best way to prevent athlete’s foot is to keep the feet (especially the areas between the toes) dry and well-ventilated. Socks should be washed daily.\nRead more on ringworm infection.\nCutaneous larva migrans\nCutaneous larva migrans refers to parasitic immature intestinal worms that are often associated with the feces of animals such as cats and dogs. These parasitic larvae come in contact with the skin while walking barefoot in the moist soil or sand. For this reason, cutaneous larva migrans is also known as creeping eruption, sandworm, or ground itch.\nThese worms are mostly found in the soil of tropical and subtropical locations such as Australia and the Americas. A characteristic feature of infestation with cutaneous larva migrans is the appearance of serpentine, reddish tunnels in the skin of the feet and other parts of the leg. These tunnels are 2-3 millimeters wide and several centimeters in length. They are the paths made by the larvae as they travel beneath the skin.\nIn most cases, the itchy lesions created by the parasitic larvae heal without treatment in about 4-8 weeks. Oral or topical application of anti-parasitic drugs (such as thiabendazole) can prevent itching and kill the parasites. With such treatment, the itching stops within one day, and the lesions may heal within a week. The best way to prevent cutaneous larva migrans is to avoid bare foot contact with moist soil or sand.\nTungiasis is another parasitic infection of the foot that can cause itchy feet. Tungiasis is caused by Tunga penetrans, a flea that burrows in the skin. Also known as the bug of the foot or sandflea, Tunga penetrans lives in moisture-laden soil in parts of India and South America. Walking barefoot on such soil results in the parasitic infection of the foot.\nTungiasis is characterized by a 4-10 millimeters wide white rash with a black center. This rash is both itchy and painful. Left untreated, the rash heals on its own within two weeks. A doctor can also remove the sandflea physically. Anti-parasitic topical applications containing thiabendazole can also be used for the treatment of tungiasis.\nSweaty sock syndrome\nSweaty sock syndrome is characterized by dry, reddish, and shiny skin on the sole of the foot. The affected skin may eventually crack, causing pain and itching. Sweaty sock syndrome is also known as juvenile plantar dermatosis, because it usually affects kids suffering from atopic dermatitis. There is no specific treatment for sweaty sock syndrome because the cause of this condition is unknown.\nHowever, some general measures can promote healing of this condition. Use of well-fitting shoes, and application of petroleum jelly and dimeticone can minimize the friction at the feet. Any fissures caused by this condition can be healed by covering the affected areas with plaster. Sweaty sock syndrome usually resolves on its own before puberty.\nPitted keratolysis is characterized by the appearance of small and shallow cavities in the upper layers of the skin on the soles and the palms. These pits on the skin may sometimes be itchy. Pitted kertolysis commonly affects individuals who suffer from excessive perspiration. Medical treatment may not be necessary. However, over-the-counter antibiotic ointments may help in treating pitted keratolysis. Keeping the feet dry may help in preventing the occurrence of this condition.\nAcroangiodermatitis is characterized by the appearance of ulcers and purple patches on the skin of the feet. This condition usually afflicts young individuals who suffer from various blood vessel disorders. The lesions in acroangiodermatitis are a result of impaired blood flow in the feet, caused by conditions such as venous fistula and venous thrombosis. These lesions are itchy and painful.\nPalmoplantar keratoderma is characterized by thick skin on the soles and the palms. This thickening of the skin occurs in a variety of acquired or inherited skin disorders, and may appear at any age.\nItchy ankles may or may not be associated with skin rashes. Dry skin, diabetes, intestinal parasites, varicose veins, hypothyroidism, heart disease, kidney disease, and psoriasis can result in itchy ankles but no skin rash. Itchy ankles associated with skin rashes may occur due to heat, cholinergic urticaria, allergic reactions, stasis eczema, Crohn’s disease, mites, and bed bugs."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:1debad2f-c2e4-4563-ad3e-d737d9f90a14>","<urn:uuid:5267e18d-78b0-4145-81ba-ccbc5dea055c>"],"error":null}
{"question":"How does goSTAG facilitate biological analysis of gene sets, and what statistical challenges does it address in multiple testing scenarios?","answer":"goSTAG facilitates biological analysis by converting gene lists from genomic analyses into biological themes through enrichment of biological categories and construction of GO subtrees from over-represented terms. It performs over-representation analysis on gene lists, clusters the enriched biological categories, and uses the terms with the most paths to the root in the subtree to represent biological themes. Regarding statistical challenges, when performing multiple tests simultaneously, it must address the multiple comparisons problem where more inferences increase the likelihood of erroneous inferences occurring. Without proper correction, if 100 independent tests are conducted at the 5% level when all null hypotheses are true, the probability of at least one incorrect rejection is 99.4%. This issue requires statistical techniques that enforce stricter significance thresholds for individual comparisons to maintain appropriate overall error rates.","context":["- Open Access\ngoSTAG: gene ontology subtrees to tag and annotate genes within a set\nSource Code for Biology and Medicine volume 12, Article number: 6 (2017)\nOver-representation analysis (ORA) detects enrichment of genes within biological categories. Gene Ontology (GO) domains are commonly used for gene/gene-product annotation. When ORA is employed, often times there are hundreds of statistically significant GO terms per gene set. Comparing enriched categories between a large number of analyses and identifying the term within the GO hierarchy with the most connections is challenging. Furthermore, ascertaining biological themes representative of the samples can be highly subjective from the interpretation of the enriched categories.\nWe developed goSTAG for utilizing GO Subtrees to Tag and Annotate Genes that are part of a set. Given gene lists from microarray, RNA sequencing (RNA-Seq) or other genomic high-throughput technologies, goSTAG performs GO enrichment analysis and clusters the GO terms based on the p-values from the significance tests. GO subtrees are constructed for each cluster, and the term that has the most paths to the root within the subtree is used to tag and annotate the cluster as the biological theme. We tested goSTAG on a microarray gene expression data set of samples acquired from the bone marrow of rats exposed to cancer therapeutic drugs to determine whether the combination or the order of administration influenced bone marrow toxicity at the level of gene expression. Several clusters were labeled with GO biological processes (BPs) from the subtrees that are indicative of some of the prominent pathways modulated in bone marrow from animals treated with an oxaliplatin/topotecan combination. In particular, negative regulation of MAP kinase activity was the biological theme exclusively in the cluster associated with enrichment at 6 h after treatment with oxaliplatin followed by control. However, nucleoside triphosphate catabolic process was the GO BP labeled exclusively at 6 h after treatment with topotecan followed by control.\ngoSTAG converts gene lists from genomic analyses into biological themes by enriching biological categories and constructing GO subtrees from over-represented terms in the clusters. The terms with the most paths to the root in the subtree are used to represent the biological themes. goSTAG is developed in R as a Bioconductor package and is available at https://bioconductor.org/packages/goSTAG\nGene lists derived from the results of genomic analyses are rich in biological information [1, 2]. For instance, differentially expressed genes (DEGs) from a microarray or RNA-Seq analysis are related functionally in terms of their response to a treatment or condition . Gene lists can vary in size, up to several thousand genes, depending on the robustness of the perturbations or how widely different the conditions are biologically . Having a way to associate biological relatedness between hundreds or thousands of genes systematically is impractical by manually curating the annotation and function of each gene.\nOver-representation analysis (ORA) of genes was developed to identify biological themes . Given a Gene Ontology (GO) [6, 7] and an annotation of genes that indicate the categories each one fits into, significance of the over-representation of the genes within the ontological categories is determined by a Fisher’s exact test or modeling according to a hypergeometric distribution . Comparing a small number of enriched biological categories for a few samples is manageable using Venn diagrams or other means of assessing overlaps. However, with hundreds of enriched categories and many samples, the comparisons are laborious. Furthermore, if there are enriched categories that are shared between samples, trying to represent a common theme across them is highly subjective. We developed a tool called goSTAG to use GO Subtrees to Tag and Annotate Genes within a set. goSTAG visualizes the similarities between over-representations by clustering the p-values from the statistical tests and labels clusters with the GO term that has the most paths to the root within the subtree generated from all the GO terms in the cluster.\nThe goSTAG package contains seven functions:\nloadGeneLists: loads sets of gene symbols for ORA that are in gene matrix transposed (GMT) format or text files in a directory\nloadGOTerms: provides the assignment of genes to GO terms\nperformGOEnrichment: performs the ORA of the genes enriched within the GO categories and computes p-values for the significance based on a hypergeometric distribution\nperformHierarchicalClustering: clusters the enrichment matrix\ngroupClusters: partitions clusters of GO terms according to a distance/dissimilarity threshold of where to cut the dendorgram\nannotateClusters: creates subtrees from the GO terms in the clusters and labels the clusters according to the GO terms with the most paths back to the root\nplotHeatmap: generates a figure within the active graphic device illustrating the results of the clustering with the annotated labels and a heat map with colors representative of the extent of enrichment\nSee the goSTAG vignette for details of the functions, arguments, default settings and for optional user-defined analysis parameters.\nThe workflow for goSTAG proceeds as follows: First, gene lists are loaded from analyses performed within or outside of R. For convenience, a function is provided for loading gene lists generated outside of R. Then, GO terms are loaded from the biomRt package. Users can specify a particular species (human, mouse, or rat) and a GO subontology (molecular function [MF], biological process [BP], or cellular component [CC]). GO terms that have less than the predefined number of genes associated with them are removed. Next, GO enrichment is performed and p-values are calculated. Enriched GO terms are filtered by p-value or a method for multiple comparisons such as false discovery rate (FDR) , with only the union of all significant GO terms remaining. An enrichment matrix is assembled from the –log10 p-values for these remaining GO terms. goSTAG performs hierarchical clustering on the matrix using a choice of distance/dissimilarity measures, grouping algorithms and matrix dimension. Based on clusters with a minimum number of GO terms, goSTAG builds a GO subtree for each cluster. The structure of the GO parent/child relationships is obtained from the GO.db package. The GO term with the largest number of paths to the root of the subtree is selected as the representative GO term for that cluster. Finally, goSTAG creates a figure in the active graphic device of R that contains a heatmap representation of the enrichment and the hierarchical clustering dendrogram, with clusters containing at least the predefined number of GO terms labeled with the name of its representative GO term.\ngene_lists < − loadGeneLists (\"gene_lists.gmt\")\ngo_terms < − loadGOTerms ()\nenrichment_matrix < − performGOEnrichment (gene_lists, go_terms)\nhclust_results < − performHierarchicalClustering (enrichment_matrix)\nclusters < − groupClusters (hclust_results)\ncluster_labels < − annotateClusters (clusters)\nplotHeatmap (enrichment_matrix, hclust_results, clusters, cluster_labels)\nTo demonstrate the utility of goSTAG, we analyzed the DEGs from gene expression analysis (Affymetrix GeneChip Rat Genome 230 2.0 arrays) of samples acquired from the bone marrow of rats exposed to cancer therapeutic drugs (topotecan in combination with oxaliplatin) for 1, 6, or 24 h in order to determine whether the combination or the order of administration influenced bone marrow toxicity at the level of gene expression. Details of the analysis are as previously described . The data are available in the Gene Expression Omnibus (GEO) [11, 12] under accession number GSE63902. The DEG lists (Additional file 1), along with the GO terms from Bioconductor GO.db package v3.4.0 and GO gene associations based on biomaRt package v2.31.4, were fed into goSTAG using default parameters except for the rat species, the distance threshold set at < 0.3 and the minimum number of GO terms in a cluster set at > = 15. The defaults include only considering BP GO terms and requiring at least 5 genes within a GO category. There were 762 BPs significant from the union of all the lists. As shown in Fig. 1, the more red the intensity of the heat map, the more significant the enrichment of the GO BPs. Fifteen clusters of GO BPs are labeled with the term with the largest number of paths to the root in each. Negative regulation of MAP kinase activity (GO:0043407) was the GO BP labeled exclusively in the cluster associated with enrichment at 6 h after treatment with oxaliplatin followed by control. However, nucleoside triphosphate catabolic process (GO:0009143) was the GO BP labeled exclusively in the cluster associated with enrichment at 6 h after treatment with topotecan followed by control.\ngoSTAG performs ORA on gene lists from genomic analyses, clusters the enriched biological categories and constructs GO subtrees from over-represented terms in the clusters revealing biological themes representative of the underlying biology. Using goSTAG on microarray gene expression data from the bone marrow of rats exposed to a combination of cancer therapeutics, we were able to elucidate biological themes that were in common or differed according to the treatment conditions. goSTAG is developed in R (open source) as an easy to use Bioconductor package and is publicly available at https://bioconductor.org/packages/goSTAG.\nAvailability and requirements\nProject Name: goSTAG\nProject Home Page: The R Bioconductor package goSTAG is open source and available at https://bioconductor.org/packages/goSTAG\nOperating System: Platform independent\nProgramming Language: R version ≥ 3.4.0\nDifferentially expressed genes\nFalse discovery rate\nGene Expression Omnibus\nGene matrix transposed\nGO subtrees to tag and annotate genes\nEfron B, Tibshirani R. On testing the significance of sets of genes. Ann Appl Stat. 2007;1:107–29.\nSubramanian A, Tamayo P, Mootha VK, Mukherjee S, Ebert BL, Gillette MA, Paulovich A, Pomeroy SL, Golub TR, Lander ES, Mesirov JP. Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proc Natl Acad Sci U S A. 2005;102:15545–50.\nQuackenbush J. Genomics. Microarrays—guilt by association. Science. 2003;302:240–1.\nWang C, Gong B, Bushel PR, Thierry-Mieg J, Thierry-Mieg D, Xu J, Fang H, Hong H, Shen J, Su Z, et al. The concordance between RNA-seq and microarray data depends on chemical treatment and transcript abundance. Nat Biotechnol. 2014;32:926–32.\nHosack DA, Dennis Jr G, Sherman BT, Lane HC, Lempicki RA. Identifying biological themes within lists of genes with EASE. Genome Biol. 2003;4:R70.\nAshburner M, Ball CA, Blake JA, Botstein D, Butler H, Cherry JM, Davis AP, Dolinski K, Dwight SS, Eppig JT, et al. Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. Nat Genet. 2000;25:25–9.\nGene Ontology C. Gene ontology consortium: going forward. Nucleic Acids Res. 2015;43:D1049–56.\nRao PV. Statistical research methods in the life sciences. Pacific Grove: Duxbury Press; 1998.\nBenjamini Y, Hochberg Y. Controlling the False Discovery Rate—a Practical and Powerful Approach to Multiple Testing. J R Stat Soc Ser B Methodol. 1995;57:289–300.\nDavis M, Li J, Knight E, Eldridge SR, Daniels KK, Bushel PR. Toxicogenomics profiling of bone marrow from rats treated with topotecan in combination with oxaliplatin: a mechanistic strategy to inform combination toxicity. Front Genet. 2015;6:14.\nBarrett T, Wilhite SE, Ledoux P, Evangelista C, Kim IF, Tomashevsky M, Marshall KA, Phillippy KH, Sherman PM, Holko M, et al. NCBI GEO: archive for functional genomics data sets—update. Nucleic Acids Res. 2013;41:D991–5.\nEdgar R, Domrachev M, Lash AE. Gene Expression Omnibus: NCBI gene expression and hybridization array data repository. Nucleic Acids Res. 2002;30:207–10.\nThe authors thank Dr. Myrtle Davis and Dr. Elaine Knight for the study design and microarray analyses. We greatly appreciate Dr. Maria Shatz and Dr. Christopher Duncan for their critical review of the manuscript. We thank Drs. Michael Resnick, Thuy-Ai Nguyen, Daniel Menendez, Julie Lowe and Maria Shatz for study designs that motivated the development and application of goSTAG. This research was supported, in part, by the Intramural Research Program of the National Institutes of Health (NIH), National Institute of Environmental Health Sciences (NIEHS).\nThis research was supported [in part] by the Intramural Research Program of the National Institute of Environmental Health Sciences, NIH.\nAvailability of data and materials\nThe microarray gene expression data used as an example for goSTAG is available in GEO under accession number GSE63902.\nPRB conceived the methodology, directed the development of the software and contributed to writing the paper. BDB designed the software, implemented the R code for the software and contributed to writing the paper. Both authors read and approved the final manuscript.\nThe authors declare no competing interest.\nConsent for publication\nCage size and animal care conformed to the guidelines of the Guide for the Care and Use of Laboratory Animals (National Research Council, 2011) and the U.S. Department of Agriculture through the Animal Welfare Act (Public Law 99–198).\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nAbout this article\nCite this article\nBennett, B.D., Bushel, P.R. goSTAG: gene ontology subtrees to tag and annotate genes within a set. Source Code Biol Med 12, 6 (2017) doi:10.1186/s13029-017-0066-1\n- Gene expression\n- Gene Ontology\n- Biological themes\n- Over-representation analysis\n- Functional enrichment\n- Pathway analysis","Multiple comparisons problem(Redirected from Multiple testing correction)\nIn statistics, the multiple comparisons, multiplicity or multiple testing problem occurs when one considers a set of statistical inferences simultaneously or infers a subset of parameters selected based on the observed values. In certain fields it is known as the look-elsewhere effect.\nThe more inferences are made, the more likely erroneous inferences are to occur. Several statistical techniques have been developed to prevent this from happening, allowing significance levels for single and multiple comparisons to be directly compared. These techniques generally require a stricter significance threshold for individual comparisons, so as to compensate for the number of inferences being made.\nThe interest in the problem of multiple comparisons began in the 1950s with the work of Tukey and Scheffé. Other methods, such as the closed testing procedure (Marcus et al., 1976) and the Holm–Bonferroni method (1979), later emerged. In 1995, work on the false discovery rate began. In 1996, the first conference on multiple comparisons took place in Israel. This was followed by conferences around the world, usually taking place about every two years.\nMultiple comparisons arise when a statistical analysis involves multiple simultaneous statistical tests, each of which has a potential to produce a \"discovery.\" A stated confidence level generally applies only to each test considered individually, but often it is desirable to have a confidence level for the whole family of simultaneous tests. Failure to compensate for multiple comparisons can have important real-world consequences, as illustrated by the following examples:\n- Suppose the treatment is a new way of teaching writing to students, and the control is the standard way of teaching writing. Students in the two groups can be compared in terms of grammar, spelling, organization, content, and so on. As more attributes are compared, it becomes increasingly likely that the treatment and control groups will appear to differ on at least one attribute due to random sampling error alone.\n- Suppose we consider the efficacy of a drug in terms of the reduction of any one of a number of disease symptoms. As more symptoms are considered, it becomes increasingly likely that the drug will appear to be an improvement over existing drugs in terms of at least one symptom.\nIn both examples, as the number of comparisons increases, it becomes more likely that the groups being compared will appear to differ in terms of at least one attribute. Our confidence that a result will generalize to independent data should generally be weaker if it is observed as part of an analysis that involves multiple comparisons, rather than an analysis that involves only a single comparison.\nFor example, if one test is performed at the 5% level and the corresponding null hypothesis is true, there is only a 5% chance of incorrectly rejecting the null hypothesis. However, if 100 tests are conducted and all corresponding null hypotheses are true, the expected number of incorrect rejections (also known as false positives or Type I errors) is 5. If the tests are statistically independent from each other, the probability of at least one incorrect rejection is 99.4%.\nThe multiple comparisons problem also applies to confidence intervals. A single confidence interval with a 95% coverage probability level will contain the population parameter in 95% of experiments. However, if one considers 100 confidence intervals simultaneously, each with 95% coverage probability, the expected number of non-covering intervals is 5. If the intervals are statistically independent from each other, the probability that at least one interval does not contain the population parameter is 99.4%.\nTechniques have been developed to prevent the inflation of false positive rates and non-coverage rates that occur with multiple statistical tests.\nClassification of multiple hypothesis testsEdit\nThe following table defines the possible outcomes when testing multiple null hypotheses. Suppose we have a number m of null hypotheses, denoted by: H1, H2, ..., Hm. Using a statistical test, we reject the null hypothesis if the test is declared significant. We do not reject the null hypothesis if the test is non-significant. Summing each type of outcome over all Hi yields the following random variables:\n|Null hypothesis is true (H0)||Alternative hypothesis is true (HA)||Total|\n|Test is declared significant|\n|Test is declared non-significant|\n- is the total number hypotheses tested\n- is the number of true null hypotheses, an unknown parameter\n- is the number of true alternative hypotheses\n- is the number of false positives (Type I error) (also called \"false discoveries\")\n- is the number of true positives (also called \"true discoveries\")\n- is the number of false negatives (Type II error)\n- is the number of true negatives\n- is the number of rejected null hypotheses (also called \"discoveries\", either true or false)\nIn hypothesis tests of which are true null hypotheses, is an observable random variable, and , , , and are unobservable random variables.\nIf m independent comparisons are performed, the family-wise error rate (FWER), is given by\nHence, unless the tests are perfectly positively dependent (i.e., identical), increases as the number of comparisons increases. If we do not assume that the comparisons are independent, then we can still say:\nwhich follows from Boole's inequality. Example:\nThere are different ways to assure that the family-wise error rate is at most . The most conservative method, which is free of dependence and distributional assumptions, is the Bonferroni correction .\nA marginally less conservative correction can be obtained by solving the equation for the family-wise error rate of independent comparisons for . This yields , which is known as the Šidák correction. Another procedure is the Holm–Bonferroni method, which uniformly delivers more power than the simple Bonferroni correction, by testing only the lowest p-value ( ) against the strictest criterion, and the higher p-values ( ) against progressively less strict criteria. .\nThis article may need to be cleaned up. It has been merged from Multiple testing correction.\nMultiple testing correction refers to re-calculating probabilities obtained from a statistical test which was repeated multiple times. In order to retain a prescribed family-wise error rate α in an analysis involving more than one comparison, the error rate for each comparison must be more stringent than α. Boole's inequality implies that if each of m tests is performed to have type I error rate α/m, the total error rate will not exceed α. This is called the Bonferroni correction, and is one of the most commonly used approaches for multiple comparisons.\nIn some situations, the Bonferroni correction is substantially conservative, i.e., the actual family-wise error rate is much less than the prescribed level α. This occurs when the test statistics are highly dependent (in the extreme case where the tests are perfectly dependent, the family-wise error rate with no multiple comparisons adjustment and the per-test error rates are identical). For example, in fMRI analysis, tests are done on over 100,000 voxels in the brain. The Bonferroni method would require p-values to be smaller than .05/100000 to declare significance. Since adjacent voxels tend to be highly correlated, this threshold is generally too stringent.\nBecause simple techniques such as the Bonferroni method can be conservative, there has been a great deal of attention paid to developing better techniques, such that the overall rate of false positives can be maintained without excessively inflating the rate of false negatives. Such methods can be divided into general categories:\n- Methods where total alpha can be proved to never exceed 0.05 (or some other chosen value) under any conditions. These methods provide \"strong\" control against Type I error, in all conditions including a partially correct null hypothesis.\n- Methods where total alpha can be proved not to exceed 0.05 except under certain defined conditions.\n- Methods which rely on an omnibus test before proceeding to multiple comparisons. Typically these methods require a significant ANOVA, MANOVA, or Tukey's range test. These methods generally provide only \"weak\" control of Type I error, except for certain numbers of hypotheses.\n- Empirical methods, which control the proportion of Type I errors adaptively, utilizing correlation and distribution characteristics of the observed data.\nThe advent of computerized resampling methods, such as bootstrapping and Monte Carlo simulations, has given rise to many techniques in the latter category. In some cases where exhaustive permutation resampling is performed, these tests provide exact, strong control of Type I error rates; in other cases, such as bootstrap sampling, they provide only approximate control.\nLarge-scale multiple testingEdit\nTraditional methods for multiple comparisons adjustments focus on correcting for modest numbers of comparisons, often in an analysis of variance. A different set of techniques have been developed for \"large-scale multiple testing\", in which thousands or even greater numbers of tests are performed. For example, in genomics, when using technologies such as microarrays, expression levels of tens of thousands of genes can be measured, and genotypes for millions of genetic markers can be measured. Particularly in the field of genetic association studies, there has been a serious problem with non-replication — a result being strongly statistically significant in one study but failing to be replicated in a follow-up study. Such non-replication can have many causes, but it is widely considered that failure to fully account for the consequences of making multiple comparisons is one of the causes.\nIn different branches of science, multiple testing is handled in different ways. It has been argued that if statistical tests are only performed when there is a strong basis for expecting the result to be true, multiple comparisons adjustments are not necessary. It has also been argued that use of multiple testing corrections is an inefficient way to perform empirical research, since multiple testing adjustments control false positives at the potential expense of many more false negatives. On the other hand, it has been argued that advances in measurement and information technology have made it far easier to generate large datasets for exploratory analysis, often leading to the testing of large numbers of hypotheses with no prior basis for expecting many of the hypotheses to be true. In this situation, very high false positive rates are expected unless multiple comparisons adjustments are made.\nFor large-scale testing problems where the goal is to provide definitive results, the familywise error rate remains the most accepted parameter for ascribing significance levels to statistical tests. Alternatively, if a study is viewed as exploratory, or if significant results can be easily re-tested in an independent study, control of the false discovery rate (FDR) is often preferred. The FDR, loosely defined as the expected proportion of false positives among all significant tests, allows researchers to identify a set of \"candidate positives\" that can be more rigorously evaluated in a follow-up study.\nThe practice of trying many unadjusted comparisons in the hope of finding a significant one is a known problem, whether applied unintentionally or deliberately, is sometimes called \"p-hacking.\"\nAssessing whether any alternative hypotheses are trueEdit\nA basic question faced at the outset of analyzing a large set of testing results is whether there is evidence that any of the alternative hypotheses are true. One simple meta-test that can be applied when it is assumed that the tests are independent of each other is to use the Poisson distribution as a model for the number of significant results at a given level α that would be found when all null hypotheses are true. If the observed number of positives is substantially greater than what should be expected, this suggests that there are likely to be some true positives among the significant results. For example, if 1000 independent tests are performed, each at level α = 0.05, we expect 0.05 × 1000 = 50 significant tests to occur when all null hypotheses are true. Based on the Poisson distribution with mean 50, the probability of observing more than 61 significant tests is less than 0.05, so if more than 61 significant results are observed, it is very likely that some of them correspond to situations where the alternative hypothesis holds. A drawback of this approach is that it over-states the evidence that some of the alternative hypotheses are true when the test statistics are positively correlated, which commonly occurs in practice.. On the other hand, the approach remains valid even in the presence of correlation among the test statistics, as long as the Poisson distribution can be shown to provide a good approximation for the number of significant results. This scenario arises, for instance, when mining significant frequent itemsets from transactional datasets. Furthermore, a careful two stage analysis can bound the FDR at a pre-specified level.\nAnother common approach that can be used in situations where the test statistics can be standardized to Z-scores is to make a normal quantile plot of the test statistics. If the observed quantiles are markedly more dispersed than the normal quantiles, this suggests that some of the significant results may be true positives.\n- Key concepts\n- Familywise error rate\n- False positive rate\n- False discovery rate (FDR)\n- False coverage rate (FCR)\n- Interval estimation\n- Post-hoc analysis\n- Experimentwise error rate\n- General methods of alpha adjustment for multiple comparisons\n- Related concepts\n- Miller, R.G. (1981). Simultaneous Statistical Inference 2nd Ed. Springer Verlag New York. ISBN 0-387-90548-0.\n- Benjamini, Y. (2010). \"Simultaneous and selective inference: Current successes and future challenges\". Biometrical Journal. 52 (6): 708–721. doi:10.1002/bimj.200900299. PMID 21154895.\n- Kutner, Michael; Nachtsheim, Christopher; Neter, John; Li, William (2005). Applied Linear Statistical Models. pp. 744–745.\n- Aickin, M; Gensler, H (May 1996). \"Adjusting for multiple testing when reporting research results: the Bonferroni vs Holm methods\". Am J Public Health. 86 (5): 726–728. doi:10.2105/ajph.86.5.726. PMC . PMID 8629727.\n- Logan, B. R.; Rowe, D. B. (2004). \"An evaluation of thresholding techniques in fMRI analysis\". NeuroImage. 22 (1): 95–108. doi:10.1016/j.neuroimage.2003.12.047. PMID 15110000.\n- Logan, B. R.; Geliazkova, M. P.; Rowe, D. B. (2008). \"An evaluation of spatial thresholding techniques in fMRI analysis\". Human Brain Mapping. 29 (12): 1379–1389. doi:10.1002/hbm.20471. PMID 18064589.\n- Qu, Hui-Qi; Tien, Matthew; Polychronakos, Constantin (2010-10-01). \"Statistical significance in genetic association studies\". Clinical and Investigative Medicine. Medecine Clinique et Experimentale. 33 (5): E266–E270. ISSN 0147-958X. PMC . PMID 20926032.\n- Rothman, Kenneth J. (1990). \"No Adjustments Are Needed for Multiple Comparisons\". Epidemiology. Lippincott Williams & Wilkins. 1 (1): 43–46. doi:10.1097/00001648-199001000-00010. JSTOR 20065622. PMID 2081237.\n- Benjamini, Yoav; Hochberg, Yosef (1995). \"Controlling the false discovery rate: a practical and powerful approach to multiple testing\". Journal of the Royal Statistical Society, Series B. 57 (1): 125–133. JSTOR 2346101.\n- Storey, JD; Tibshirani, Robert (2003). \"Statistical significance for genome-wide studies\". PNAS. 100 (16): 9440–9445. doi:10.1073/pnas.1530509100. JSTOR 3144228. PMC . PMID 12883005.\n- Efron, Bradley; Tibshirani, Robert; Storey, John D.; Tusher, Virginia (2001). \"Empirical Bayes analysis of a microarray experiment\". Journal of the American Statistical Association. 96 (456): 1151–1160. doi:10.1198/016214501753382129. JSTOR 3085878.\n- Noble, William S. (2009-12-01). \"How does multiple testing correction work?\". Nature Biotechnology. 27 (12): 1135–1137. doi:10.1038/nbt1209-1135. ISSN 1087-0156. PMC . PMID 20010596.\n- Young, S. S., Karr, A. (2011). \"Deming, data and observational studies\" (PDF). Significance. 8 (3).\n- Smith, G. D., Shah, E. (2002). \"Data dredging, bias, or confounding\". BMJ. 325 (7378): 1437–1438. doi:10.1136/bmj.325.7378.1437. PMC . PMID 12493654.\n- Kirsch, A; Mitzenmacher, M; Pietracaprina, A; Pucci, G; Upfal, E; Vandin, F (June 2012). \"An Efficient Rigorous Approach for Identifying Statistically Significant Frequent Itemsets\". Journal of the ACM. 59 (3): 12:1–12:22. doi:10.1145/2220357.2220359.\n- F. Betz, T. Hothorn, P. Westfall (2010), Multiple Comparisons Using R, CRC Press\n- S. Dudoit and M. J. van der Laan (2008), Multiple Testing Procedures with Application to Genomics, Springer\n- B. Phipson and G. K. Smyth (2010), Permutation P-values Should Never Be Zero: Calculating Exact P-values when Permutations are Randomly Drawn, Statistical Applications in Genetics and Molecular Biology Vol.. 9 Iss. 1, Article 39, doi:10.2202/1544-6155.1585\n- P. H. Westfall and S. S. Young (1993), Resampling-based Multiple Testing: Examples and Methods for p-Value Adjustment, Wiley\n- P. Westfall, R. Tobias, R. Wolfinger (2011) Multiple comparisons and multiple testing using SAS, 2nd edn, SAS Institute\n- A gallery of examples of implausible correlations sourced by data dredging"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:e0e50672-dddd-45aa-986e-20dd440ef21b>","<urn:uuid:dc4bf75f-bf25-4439-a262-ff06a11c353b>"],"error":null}
{"question":"What's the difference between recording movement in Sapien's recorder vs After Effect's wiggle expression?","answer":"In Sapien, movement recording is done in real-time by taking over a unit and manually performing actions, which are saved as recorded animations that can be played back through script functions. In contrast, After Effects' wiggle expression creates random movement through mathematical parameters - you simply input two numbers to control the frequency (times per second) and amplitude (pixels) of the movement. The After Effects method also offers more control options, such as limiting movement to single dimensions and using slider controls to keyframe the wiggle effect.","context":["This feature does not function in the Gearbox HEK release.\nRecorded animations are a simple way for designers to animate a scene using ingame tools without any need for animating a scene in 3D modeling software. This feature allows you to record your inputs and save them for playback. They can then be invoked through a script to have a unit play out the prerecorded inputs. We will go over how recorded animations in the toolset works.\nBe warned that recordings done after reloading a scenario in Sapien may cause stability issues. Try to keep it to your first scenario load.\nEntering scripted camera mode\nYou will need to set your editor mode so that we can start using recording commands. Find the toolbar in the top left and select the camera icon. This will set your Sapien instance to scripted camera mode.\nSome additional notes for this section. There are some new hotkeys you gain access to in this mode:\n- A: Toggle \"Attach camera to unit\" option.\n- E: Toggle \"Edit camera point\" option.\n- C: Toggle \"Scripted camera control\".\n- Space: Current game view camera position is used to create a new camera point if \"Edit camera point\" is disabled. If \"Edit camera point\" is enabled then it instead moves the \"Active camera point\" to the current position of the game view camera.\nTaking over a unit\nWe will need to pick a unit for us to take over. A unit here is typically a biped or a biped in a vehicle placed in your scene. Look over to your game window and look at a biped you wish to take over. Once you've made your choice go ahead and left click on the biped directly. A sphere should encircle the biped and gradually shrink. This indicates that you have selected the unit.\nDo not attempt to select the unit from the hierarchy list. You must select the unit from the game window screen.\nOnce the unit is selected you can press the following keys to take over the unit:\n- Shift + V: Some additional notes for this section. There are some new hotkeys you gain access to in this mode.\n- Backspace: Toggle \"Camera type\". Pressing this key will cycle between 3 modes for your camera.\n- First Person\n- Third Person\nNow that you've taken over the unit you should see a view that looks pretty similar to a standard ingame session. Press the Caps lock key to start recording.\nPressing the above key will bring up a simple menu. No idea for what or if it affects the recording in anyway. You can just leave it empty or click cancel if you don't want to bother with it. Type in a script function and hit ok if you want to see what it does.\nOnce you've moved on past this menu you should notice some new details in output giving you the current information for your recording. Total frames should starting increasing, indicating that the recording is currently in progress. Take a quick walk around the level and join me at the next section when you're ready.\nSaving your recording\nWhen you have finished performing the actions you want recorded, press Caps lock to save the animation. This will bring up the following menu:\nClicking \"Don't Save\" will throw the recorded animation data away while saving it will place the data under the hierarchy folder named \"Recorded animations\".\nPlaying your recording\nIn order to play the recording on a unit you will have to call it through a script function. The script function we will be using in this example is as follows:\nrecording_play <unit name> <recorded animation name>\nrecording_play elite_1_2 jumping.","Using Wiggle to Create Random Movement in After Effects\nUse an expression to create random movement with the wiggle expression in After Effects. In this post we’ll show you how to create wiggle easily by modifying numbers…not keyframes.\nEditors sometime cringe at the word expressions, but have no fear. When you understand them they really aren’t scary and can end up saying you a lot of time.\nIf you’ve ever seen animations where light is flickering or a camera shakes, it’s likely you’ve seen the wiggle expression in action. Wiggle is one of the most popular After Effects expressions, as it is easy to use and visually interesting. Instead of creating a bunch of keyframes to make a layer randomly move, we can do this with a couple of numbers.\nThe Wiggle Expression in After Effects\nFor this example I created the word ‘wiggle’ from text. I want the position of this text to wiggle, so select the text layer and hit P to open the Position Transform properties.\nOption (Mac) or Alt (PC) click on the stopwatch to create the expression. The text turns Red, telling you there is an expression applied.\nType wiggle(2,50). This expression states that twice a second the text should wiggle 50 pixels on the x & y axis – the first number is how many times a second, the 2nd is the amount of pixel movement.\nIf you want to modify the wiggle simply change the numbers. If we had done this with keyframes, every time the client wanted changes we would have to change the keyframes – not fun!\nWiggling One Dimension\nWhat if you want to wiggle just the x or y position? This expression is a little more complicated, but you don’t have to write it.\nI went to motion-graphics-exchange.com (an online resource for After Effects expressions), and searched for “wiggle one dimension”. This is the expression they have for ‘wiggle in one dimension’:\nwiggle only in x (horizontal):\norg=value; temp=wiggle (5,50); [temp,org];\nwiggle only in y (vertical):\norg=value; temp=wiggle (5,50); [org,temp];\nCopy this text into your After Effects project and you’re done!\nControlling the Wiggle Expression\nWhat if you don’t want the layer wiggling constantly? A common technique is to add an expressions control to a null layer so we can keyframe the wiggle. For AE newbies, a null is an object that doesn’t render that we can use to control other layers.\nWe are going to use the null with the original position wiggle we created on the Wiggle text (above).\nFrom the Layer Menu select New and then Null Object. Rename the Null “wiggle control” and then apply the effect “Sliders Control” (in the Expressions Controls category).\nSelect the wiggle control layer in the timeline, and type E to reveal the Slider Control effect. Click the twirly for Slider Control to see the stopwatch.\nGo to the Wiggle text expression and select just the 2. with the 2 selected, click the pickwip (looks like a curly cue), and drag the pick whip to the Slider stopwatch.\nPick whip is highlighted in Red:\nHere is the larger view:\nWe no longer see a wiggle, as the Slider is now controlling how many times a second the text wiggles. Click the stopwatch to create a keyframe at the beginning, then move the playhead further in time and change the amount for the Slider to increase the wiggle. You can turn on motion blur to enhance the effect.\nMotion Blur Highlighted in RED (click for larger view):\nNow you know how to create a wiggle expression in After Effects and how to control it using a null and an expression slider effect – a huge timesaver.\nWhat After Effects expressions do you rely on?\nShare them with us in the comments below!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:c9c1525a-874a-4a6b-bc61-06335a9458c7>","<urn:uuid:473e9ec1-ccb3-4eb6-96cc-38fc82259e1f>"],"error":null}
{"question":"What is the nature of selfless action according to ancient wisdom and modern research?","answer":"According to ancient wisdom, selfless action involves performing duties without attachment to results, as exemplified in the concept of karma yoga. The Bhagavad Gita teaches that even realized souls must work for world welfare without expectation, distinguishing between actions done with and without ego. A realized person acts because their nature compels them, not from personal volition. Modern research has examined this through empirical studies of ethical decision-making, finding that it can be systematically measured and is influenced by exposure to ethical situations. Studies with graduate students demonstrate that ethical behavior patterns can be evaluated through decision-making measures and correlate with actual ethical conduct and judgment of transgressions.","context":["There is another thing to consider. Illustrious people such as Janaka, who are greater than you have all performed karma much before you.\nkarmaṇaiva hi saṃsiddhim āsthitā janakādayaḥ।\nloka-saṅgraham-evāpi saṃpaśyan kartum-arhasi ॥ BG 3.20\n“Janaka and others attained fulfilment by karma only; You should also perform your duty keeping the welfare of the world in mind”.\nJanaka was a great and elevated soul. Many ṛṣis would send seekers desirous of initiation into brahma-vidyā to him. His story appears in many places – in the Mahābhārata, Bhāgavata, Yoga-vāsiṣṭha, and above all in the Bṛhadāraṇyakopaniṣad where it appears as a conversation between him and Yājñavalkya. Janaka did not give up his kingship. The royal sage that he was, he ruled the kingdom constantly observing how people lived. O Arjuna, when such an exceptional man performed karma, why do you refuse? Just as Janaka was distinguished in his lifetime, so are you now. The ordinary imitate the eminent.\nyadyadācarati śreṣṭhaḥ tattadevetaro janaḥ ।\nsa yat pramāṇaṃ kurute lokastadanuvartate ॥ BG 3.21\n“Whatever a great man does, the other men also do; The world follows whatever he sets up as a standard”.\n“Let us say that Janaka is not in front of us now. Am I not there? You have seen me since childhood. You know my basic nature. On many occasions such as the burning of Khāṇḍava forest, you have seen my prowess. Why am I performing karma in this world? All that anyone might desire, is under my power; what else do I want? However, I still perform karma, for the welfare of the world. If I do not perform karma, anarchy will spread in the world and the society will degenerate. It is not just now, it has been so forever.”\nyadā yadā hi dharmasya glānirbhavati bhārata ।\nabhyutthānam adharmasya tadātmānaṃ sṛjāmyaham ॥ BG 4.7\n\"Whenever there is decline of dharma and rise of adharma, I manifest myself. For the protection of the virtuous, for the destruction of the wicked, and for the establishment of dharma, I am born in every yuga.\"\nparitrāṇāya sādhūnāṃ vināśāya ca duṣkṛtāṃ ।\ndharmasaṃśthāpanārthāya sambhavāmi yuge yuge ॥ BG 4.8\n“Therefore I work out of my own volition for the sake of the world”.\nHow affectionate Bhagavān is towards the universe! We should never forget this. We should diligently and devoutly discharge the duties that have been allotted to us by his divine arrangement. This is the advice most pertinent to us in this age and time. However, the thinkers among us give first preference to the subject of mokṣa (liberation) and second place to our duties in this world.\nIn the story of Kṛṣṇa in the Bhāgavata, the gopikās ask him: “In this world, we see that some people love only those who love them. Some others love those who do not love them in return. Yet others forget those who love them as well as those who do not love them. What is the meaning of this?” This last question was intended to taunt him. Kṛṣṇa said, “Giving love to someone who loves us is a commercial transaction. It is a great thing only when we show kindness towards those who do not like us.”\nBhagavān praises selfless yajña performed just for the benefit of others. Even a jñānī cannot neglect worldly duties.\nsaktāḥ karmaṇyavidvāṃso yathā kurvanti bhārata ।\nkuryādvidvāṃstathā’saktaḥ cikīrṣurlokasaṅgraham ॥ BG 3.25\n“Just as ignorant men perform their duties being attached to them, the wise should perform their duties for the welfare of the world, without attachment”.\nThis is the essence of karma yoga. It has to be always remembered by those who share their ideals with the Gokhale Institute.\ntattvavittu mahābāho guṇakarmavibhāgayoḥ ।\nguṇā guṇeṣu vartanta iti mattvā na sajjate ॥ BG 3.28\n“He who knows the truth about the types of guṇa and karma understands that guṇas as the senses are drawn to guṇas as sense objects, and is not attached.”\nEven though Karma is directly perceived, its root is hidden. When a man’s inner tendencies flow out through his actions, it becomes karma.\nyatpuruṣo manasābhigacchati। tadvācā vadati। tatkarmaṇā karoti। (Taittirīya-āraṇyaka Prapāṭhaka 1, Anuvāka 23, Mantra 1\n“What a man thinks, he speaks and performs as action”.\nTherefore, the place of origin of good and bad results of karma is the manas (mind). The mind is the kartā or doer. Karma follows the doer. If he is good, it is good. If he is bad, it is bad. If his intention is improper, it is not virtuous, even if the endeavour is worthy. If the intention is upright, he is worthy; regardless of the nature of his work, it is virtuous.\nhatvā’pi sa imāṃllokān na hanti na nibadhyate॥ BG 18.17\n“Even after killing these worlds, he does not kill and is not attached”.\nHow does a jñānī differ from an ajñānī? Do not both of them work for the sake of the world? Having said that, there is a characteristic difference. The realised individual selflessly works for the order of the universe. The result of the work of the unrealised man could be either order or disorder. Disorder because ego reigns in him; he is desirous of the rewards of his labour. The worldly man measures everything with a view of what is in it for him. What the ajñānī performs with an eye for profit, the jñānī performs without any expectation. The function is the same. One establishes a satra to provide free food for the needy; the other starts a fancy restaurant. They perform the same karma. The wise man does it without anyone forcing him, with a view that he is one with all beings of the universe; the ignorant does it out of self-interest.\nEven the jñānī might think that he is working for the betterment of the world. Is it not selfishness? Does not the seed of selfishness thrive in him to that extent? Bhagavān’s response is thus: “Even if a tiny bit of egotism is present in one’s mind, he cannot be called a jñānī. In a worldly sense he may be called a jñānī; he just does not have any realisation of the ātmā. However, there is an interesting aspect to consider. Outwardly, one may seem to be egoistic in the matter of karma. Internally he may perform karma not out of his own volition but because his nature compels him. Genuine love for Bhagavān could be the reason for his joy.\nprakṛteḥ kriyamāṇāni guṇaiḥ karmāṇi sarvaśaḥ ।\nahaṅkāravimūḍhātmā kartāhamiti manyate ॥ BG 3.27\n“Actions are being done in every way by the guṇas of prakṛti (nature). However, a man deluded by ego thinks that he is the doer”.\nTo be continued...\nThe present series is a modern English translation of DVG’s Kendra Sahitya Akademi Award-winning work, Bhagavad-gītā-tātparya or Jīvana-dharma-yoga. The translators wish to express their thanks to Śatāvadhāni R Ganesh for his valuable feedback and to Hari Ravikumar for his astute edits.","David Bourget (Western Ontario)\nDavid Chalmers (ANU, NYU)\nRafael De Clercq\nEzio Di Nucci\nJonathan Jenkins Ichikawa\nJack Alan Reynolds\nLearn more about PhilPapers\nMichael D. Mumford, Lynn D. Devenport, Ryan P. Brown, Shane Connelly, Stephen T. Murphy, Jason H. Hill & Alison L. Antes\nEthics and Behavior 16 (4):319 – 345 (2006)\nEthical decision making measures are widely applied as the principal dependent variable used in studies of research integrity. However, evidence bearing on the internal and external validity of these measures is not available. In this study, ethical decision making measures were administered to 102 graduate students in the biological, health, and social sciences, along with measures examining exposure to ethical breaches and the severity of punishments recommended. The ethical decision making measure was found to be related to exposure to ethical events and the severity of punishments awarded. The implications of these findings for the application of ethical decision making measures are discussed.\n|Keywords||No keywords specified (fix it)|\n|Categories||categorize this paper)|\nSetup an account with your affiliations in order to access resources via your University's proxy server\nConfigure custom proxy (use this if your affiliation does not provide a proxy)\n|Through your library|\nReferences found in this work BETA\nDanielle S. Beu, M. Ronald Buckley & Michael G. Harvey (2003). Ethical Decision–Making: A Multidimensional Construct. Business Ethics 12 (1):88–107.\nMichael Kalichman & Sarah Brown (1998). Effects of Training in the Responsible Conduct of Research: A Survey of Graduate Students in Experimental Sciences. [REVIEW] Science and Engineering Ethics 4 (4):487-498.\nScott J. Vitell & Foo Nin Ho (1997). Ethical Decision Making in Marketing: A Synthesis and Evaluation of Scales Measuring the Various Components of Decision Making in Ethical Situations. [REVIEW] Journal of Business Ethics 16 (7):699-717.\nMichael J. Reall, Jeffrey J. Bailey & Sharon K. Stoll (1998). Moral Reasoning \"on Hold\" During a Competitive Game. Journal of Business Ethics 17 (11):1205-1210.\nIrene Roozen, Patrick De Pelsmacker & Frank Bostyn (2001). The Ethical Dimensions of Decision Processes of Employees. Journal of Business Ethics 33 (2):87 - 99.\nCitations of this work BETA\nChase E. Thiel, Shane Connelly, Lauren Harkrider, Lynn D. Devenport, Zhanna Bagdasarov, James F. Johnson & Michael D. Mumford (2013). Case-Based Knowledge and Ethics Education: Improving Learning and Transfer Through Emotionally Rich Cases. Science and Engineering Ethics 19 (1):265-286.\nP. Waples Ethan, L. Antes Alison, T. Murphy Stephen, Connelly Shane & D. Mumford Michael (2009). A Meta-Analytic Investigation of Business Ethics Instruction. Journal of Business Ethics 87 (1):133 - 151.\nChase E. Thiel, Zhanna Bagdasarov, Lauren Harkrider, James F. Johnson & Michael D. Mumford (2012). Leader Ethical Decision-Making in Organizations: Strategies for Sensemaking. [REVIEW] Journal of Business Ethics 107 (1):49-64.\nLauren N. Harkrider, Chase E. Thiel, Zhanna Bagdasarov, Michael D. Mumford, James F. Johnson, Shane Connelly & Lynn D. Devenport (2012). Improving Case-Based Ethics Training with Codes of Conduct and Forecasting Content. Ethics and Behavior 22 (4):258 - 280.\nKligyte Vykinta, T. Marcy Richard, P. Waples Ethan, T. Sevier Sydney, S. Godfrey Elaine, D. Mumford Michael & F. Hougen Dean (2008). Application of a Sensemaking Approach to Ethics Training in the Physical Sciences and Engineering. Science and Engineering Ethics 14 (2):251-278.\nSimilar books and articles\nJoseph G. P. Paolillo & Scott J. Vitell (2002). An Empirical Investigation of the Influence of Selected Personal, Organizational and Moral Intensity Factors on Ethical Decision Making. Journal of Business Ethics 35 (1):65 - 74.\nKevin Morrell & Chanaka Jayawardhena (2010). Fair Trade, Ethical Decision Making and the Narrative of Gender Difference. Business Ethics 19 (4):393-407.\nMarc D. Street, Chris Robertson & Scott W. Geiger (1997). Ethical Decision Making: The Effects of Escalating Commitment. [REVIEW] Journal of Business Ethics 16 (11):1153-1161.\nJeffrey R. Cohen, Laurie W. Pant & David J. Sharp (2001). An Examination of Differences in Ethical Decision-Making Between Canadian Business Students and Accounting Professionals. Journal of Business Ethics 30 (4):319 - 336.\nMichael D. Mumford, Shane Connelly, Ryan P. Brown, Stephen T. Murphy, Jason H. Hill, Alison L. Antes, Ethan P. Waples & Lynn D. Devenport (2008). A Sensemaking Approach to Ethics Training for Scientists: Preliminary Evidence of Training Effectiveness. Ethics and Behavior 18 (4):315 – 339.\nMichael D. Mumford, Stephen T. Murphy, Shane Connelly, Jason H. Hill, Alison L. Antes, Ryan P. Brown & Lynn D. Devenport (2007). Environmental Influences on Ethical Decision Making: Climate and Environmental Predictors of Research Integrity. Ethics and Behavior 17 (4):337 – 366.\nLynn D. Devenport, Ryan P. Brown, Stephen T. Murphy, Alison L. Antes, Ethan P. Waples, Michael D. Mumford & Shane Connelly (2009). Exposure to Unethical Career Events: Effects on Decision Making, Climate, and Socialization. Ethics and Behavior 19 (5):351-378.\nAdded to index2009-01-28\nTotal downloads44 ( #108,761 of 1,913,505 )\nRecent downloads (6 months)4 ( #223,694 of 1,913,505 )\nHow can I increase my downloads?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"detailed"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:44285c0f-c73b-46b4-b3fb-a08abc9c6a3b>","<urn:uuid:da77211e-e42d-41b1-b75e-84133483f1d4>"],"error":null}
{"question":"How does Conplast R affect the setting process of concrete?","answer":"Conplast R disperses the fine particles in the concrete mix, enabling water to perform more effectively. It delays the initial hydration of the cement, resulting in a delayed setting time without negatively impacting subsequent stiffening and strength gain.","context":["Admixtures are materials other than cement, aggregate and water that are added to concrete either before or during its mixing to alter its properties, such as workability, curing temperature range, set time or colour.\nSome admixtures have been in use for a very long time, such as calcium chloride to provide a cold-weather setting concrete. Others are more recent and represent an area of expanding possibilities for increased performance.\nNot all admixtures are economical to employ on a particular project. Also, some characteristics of concrete, such as low absorption, can be achieved simply by consistently adhering to high quality concreting practices.\n|Adfil Construction Fibres||ADFIL has a wide range of specialised fibres for use in most construction applications. Special surfactant coatings enable excellent dispersion of individual filaments, allowing the formation of a homogenous three-dimensional matrix within||ConcreteMortar||See individual data sheets 1 bag per m³|\n|Cebex 112||Improves the workability and trowelling properties of sand/cement mixes. CEBEX 112 improves bond strengths and assists in reducing or eliminating shrinkage cracking caused by evaporation.||Mortar||0.30 - 0.60/100 kgs of cement||Cebex 112 Technical Data\nCebex 112 MSDS\n|Conplast Blocksave SD110||In the production of concrete blocks, CONPLAST BLOCKSAVE will increase strength, improve rheological flow without increased slump, produce a denser block with better finished sides and corners and reduce the unit cost of the block.||Block||0.4 litres/100 kgs of cement||Conplast Blocksave Technical Data|\n|Conplast Blockspeed SD400||When used in concrete block manufacture, BLOCKSPEED allows quicker turnaround, increased strength at an earlier age and increases ultimate strengths without||Block||1.2 litres per 100 kgs of cement||Conplast Blockspeed Technical Data|\n|Conplast P211||To produce increased workability without loss of strength, increased strength without loss of workability, or cement reduction without loss of workability or strength.||Concrete||0.28 - 0.42 litres/100 kgs of cementitious material||Conplast P211 Technical Data|\n|Conplast PA21||CONPLAST PA21 produces air-entrained concrete without the usual loss of strength or additional cement content usually required.||Concrete||0.25 kg -0.60 litres/ 100 kgs of cement||Conplast PA21 Technical Data|\n|Conplast RP264||CONPLAST RP264 produces increased workability concrete, with increased cohesion and set-retardation. Produces increased workability without loss of strength, increased strength without loss of workability, or cement reduction without loss of workability or strength.||Concrete||0.25 - 0.60 litres/ 100 kgs of cementitious material||Conplast RP264 Technical Data|\n|Conplast SP423||To produce highly workable and flowing concrete, with a very high degree of workability retention. Ideal for Readymix operations where slump retention is required.||Concrete||0.35 - 0.42 litres/100 kgs of cementitious material||Conplast SP423 Technical Data|\n|Conplast SP430||Produces a highly fluid, self-leveling concrete with very high early & ultimate strength, reduced permeability and reduced segregation.||Concrete||For following or high workability concrete: 0.70 - 1.30 litres/100 kgs of cementitious material. For strength and high early strength: 0.70 - 2.0 litres/100 kgs of cementitious material.||Conplast SP430 Technical Data|\n|Conplast UW||An admixture to be used in conjunction with CONPLAST 430 to produce a concrete that is flowing, highly workable and cohesive, and is resistant to \"wash-out\" of the fines and cement when placed underwater, in moving or tidal water.||Concrete||0.8 kg - 1.0 kg/100 kgs of cement||Conplast UW Technical Data|\n|Conplast X421||A concrete admixture to produce concrete of increased density and reduced porosity.||Concrete||6.0 litres per cubic meter of concrete||Conplast X421 Technical Data|\n|DYNAcrete PC 5-21||High range water reducing, super-plasticizing admixture.||Concrete||DYNAcrete PC 5-21 Data Sheet|\n|Quikset||An accelerating admixture based on calcium chloride, QUIKSET enhances the early stages of cement hydration producing more rapid stiffening and hardening.||Concrete Mortar||Available in210 litre drums, 18 or 20 litre kegs & 4 litre bottles. Dosage: 1.6 - 3 litres per 100 kgs.|\n|Conplast R||Conplast R disperses the fine particles in the concrete mix, enabling the water content of the concrete to perform more effectively. The initial hydration of the cement is also delayed, resulting in a delay in the setting time of the concrete with no adverse effect on subsequent stiffening and strength gain.||Concrete||10-30 litres per cubic meter of concrete. See data sheet for further information||Conplast R Technical Data|\n|Con- Ad CI||CONAD CI, is a concrete admixture based on Calcium Nitrite, which is proven to inhibit the corrosion of steel in reinforced concrete. CONAD CI contains a minimum of 30% calcium nitrite by mass and also ASTM C494 type C||ConcreteMortar||Con- Ad CI Technical Data|\n|Con- Ad PC 405||Con- Ad PC 405 is a polycarboxylic based, ready to use high range water reducing admixture for concrete, engineered to provide maximum water reduction, slump flow and high strengths in Readymix and Precast concrete applications.||Concrete|\n|Cebex 112||Improves the workability and trowelling properties of sand/cement mixes. CEBEX 112 improves bond strengths and assists in reducing or eliminating shinkage cracking caused by evaporation.||Mortar||0.30 - 0.60/100 kgs of cement|| Technical Data\nCebex 112 MSDS"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:b2088630-5852-470e-8df4-638197b17b81>"],"error":null}
{"question":"What is Samadhi and how is it experienced in practice?","answer":"Samadhi is the eighth and final limb of yoga where the individual self merges with the universal spirit. In practice, it's experienced as a state of concentrated meditation that transcends body and mind, typically achieved in Corpse Pose (Savasana) after meditation. During Samadhi, consciousness becomes detached from the body, leading to a state of transcendent bliss where one is at one with their surroundings.","context":["2) NIYAMA: Internal, ethical observances\n3) ASANA: Poses\n4) PRANAYAMA: Breath control\n5) PRATYAHARA: Sensory control and withdrawal\n6) DHARANA: Concentration\n7) DHYANA: Meditation\n8) SAMADHI: Blissful absorption\n1) YAMA. The yoga journey begins with the five universal moral commandments, or true ethics:\n(i) Ahimsa – not-harming, nonviolence\n(ii) Satya – truthfulness\n(iii) Asteya – non-stealing or misappropriating\n(iv) Brahmacharya – sexual self-control\n(v) Aparigraha – non-covetousness, modesty of life\nWe learn in this way to develop control over our actions in the external world.\n2) NIYAMA. The journey continues with five steps of self-purification:\n(i) Sauca – cleanliness\n(ii) Santosa – contentment\n(iii) Tapas – sustained practice\n(iv) Svadhyaya – self-study\n(v) Ishvara pranidhana – humble surrender to god (or the great spirit which is in everything)\nThese relate to our inner world and senses of perception and help us to develop self-discipline. These ethical precepts are always with us from the beginning to the end of the yoga journey, for the demonstration of one’s spiritual realization lies in none other than how one walks among and interacts with one’s fellow human beings.\n3) ASANA. Maintains the strength and health of the body and opens the whole spectrum of yoga’s possibilities. One of the key purposes of asana is to enable us to develop the ability to comfortably maintain good posture whilst sitting for pranayama etc. Self-cultivation through asana is the broad gateway leading to the inner enclosures. We all possess some awareness of ethical behaviour, but in order to pursue yama and niyama at deeper levels we must cultivate the mind. We need contentment, tranquility, dispassion, and unselfishness, qualities that have to be earned. It is asana that teaches us the physiology of these virtues. When an asana is correctly performed the dualities between body and mind, mind and soul, have to vanish.\n4) PRANAYAMA. Breath is the vehicle of consciousness. By its slow, measured observation and distribution we learn to still the mind and develop awareness.\n5) PRATYAHARA. By drawing our senses of perception inward, we are able to experience the control, silence, and quietness of the mind. This ability to still and gently silence the mind is essential, not only for meditation and the inward journey but also so that the intuitive intelligence can function usefully and in a worthwhile manner in the external world.\n6) DHARANA. True concentration is an unbroken thread of awareness. Yoga is about how the Will, working with intelligence and the self-reflexive consciousness can free us from the inevitability of the wavering mind and outwardly directed senses.\n7) DHYANA. Meditation, in its purest yogic sense, can only be achieved when physical and mental weaknesses have largely been eliminated. Often people think that sitting quietly is meditation. This is a common misunderstanding. Meditation is not something that can be taught, it must be directly experienced in one’s life.\n8) SAMADHI. In samadhi, the individual self, merges with the divine self, with the universal spirit. Yogis realize that the divine is not more heavenward than inward and in this final quest of the soul, seekers become seers. In this way they experience the divine at the core of their being. Samadhi is an opportunity to encounter our imperishable Self before the transient vehicle of body disappears, as in the cycle of nature, it surely must.\nMost of this hand-out is copied from:\nLight on Life\nThe Tree of Yoga\nBoth books by B.K.S Iyengar and highly recommended.","Samadhi is the 8th and final limb of yoga. Samadhi is a state of concentrated meditation that transcends the intellect, mind, and body and complete detachment from the physical world (meaning consciousness becomes detached from the body). This final stage of yoga is also known as enlightenment and can be achieved in Corpse Pose, after meditation involving Dharana and Dhyana. In this state, the yogi can suspend consciousness away from the body, being at one with the environment and surroundings while not being limited to physical restraints of the body. Samadhi represents a state of enlightenment and over time the yogi obtains a ceaseless state of transcendent bliss.\nIn Buddhism, Samadhi is known as the 8th wheel of the eightfold path referring to right concentration. Buddhists believe that this right concentration leads to extraordinary intelligence and even superpowers. But these are simply distractions for the practitioner from the goal of Moksha, or liberation. Samadhi leads to a pleasantness in your current life, knowledge of the divine third eye by concentration on light, clear comprehension of the fluctuations of feelings, perceptions, and thoughts through mindfulness, and the elimination of the 5 Skandha’s (attachments to matter, sensation, perception, mental habits, and discernment). In Buddhism, Samadhi does not refer to enlightenment, rather a state of concentrated meditation that leads to enlightenment. Nirvana is enlightened freedom from attachment and Samsara through Moksha.\nSamadhi is a state of supreme detachment, where consciousness is free to leave the body and can expand beyond the borders of the physical corpse of the consciousness. It is a supreme state of bliss that is experienced in Savasana, or in meditation after a yoga practice is completed. This is why you don’t skip Savasana! Meditate after your yoga practice, it is far more powerful after the body has been tempered. The sensations and insights that flow during these meditation can alter your perspective and even mental processes that can change. It is integral to the yoga practice to rest in Savasana and meditate; they are the most important things you can do to amplify the healing and regenerative qualities of yoga.\nSamadhi is intricately related to consciousness. It can be described as full awareness, perfect concentration, or an altered state of consciousness characterized by ananda and sukha (bliss and joy). Vyasa, one of the authors of the Mahabharata, said ‘yoga is Samadhi’. It is ultimately complete control over the fluctuations of consciousness including distractions and normal functionality of the nervous system and conscious experience.\nPatanjali said that Samadhi has three different aspects: Savikalpa, Asamprajnata, and Nirvikalpa. In Savikalpa the mind is still conscious and the imagination is active and the state can be described as holding onto the imagination with effort. Asamprajnata is a step forward from Savikalpa and is not quite gross awareness, but is a heightened state of conscious awareness. Nirvikalpa is the highest transcendent state of consciousness, the highest of the heights of yoga. It is an engrossing awareness where all things are one and pure unadulterated bliss, wholeness, and perfection are experienced. It is pure joy, freedom, and steady bliss in the knowledge of awareness.\nSamadhi is like balancing blocks on top of one another, where it takes years to learn all of the nuances of each block and how they work together. Simply allowing the body to meditate is not enough; full concentration and focus is required to obtain the state of pure freedom.\nThe final liberation of the yogi comes at the time of death, known as mahasamadhi and is a controlled exit of the consciousness from the body to merge consciousness with the divine. Maha means great.\nI would like to dedicate this post to BKS Iyengar, who died this morning, one of the greatest (yoga) teachers the world has ever known. My hope is that he found mahasamadhi in his last hours and that he has found the freedom and peace beyond. He brought yoga into the west and gave everyone seemingly limitless knowledge on even the most minuscule and minute details. He gave us in the west the opportunity to scale the heights of Raja yoga and changed the world for the better. Thank you."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"concise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:9824ee70-95fd-41a9-bc56-e1b53643a028>","<urn:uuid:c198a1dd-6ba4-49c6-b875-26176848e6be>"],"error":null}
{"question":"How do mobile marketing and email marketing compare in their approach to audience targeting and personalization?","answer":"Both marketing approaches emphasize audience targeting, but implement it differently. Mobile marketing allows for location-based targeting and can tailor ads based on where customers are physically located. Email marketing relies on segmentation based on demographics, purchase history, interests, and engagement levels. Both methods stress personalization - mobile marketing through location-aware strategies and relevant messaging, while email marketing personalizes through name addressing and leveraging customer data for relevant content delivery.","context":["Mobile marketing is one of the newest types of marketing, yet it actually incorporates elements of some of the oldest and well proven methods. Many of the same rules that apply to other kinds of marketing will apply to mobile marketing too. However, there are some noteworthy differences. This article presents some solid advice on getting the best from this brilliant advertising medium.\nFocus first on building a customer base and securing their numbers for your database. Avoiding just adding cell phone numbers to your mobile marketing database. The reason is you must have their permission before you begin doing this. You can get permission through the web or by the reply of a code.\nWhen composing your mobile website copy, be succinct. Mobile device screens are small, so droning on and on about a product just to get more keywords in is annoying to readers. In mobile marketing, your campaigns will need to be kept short and succinct.\nNever randomly message your customers. Always ensure that what you are saying is relevant. Many campaigns have failed because they inundated their customer base with too many meaningless messages. Customers want information, not funny texts that they would normally get from a lose friend.\nYour mission statement will help you design the best campaign possible. Following your principles will enable you to be focused on your target.\nWhen using mobile marketing remember that the goal is to show your customers the benefit of using your goods or services. It’s crucial to know what your customers want. If you are not aware of what your customers want, it is going to be very difficult to sell them anything. Learn as much about them as you possibly can to be most successful.\nWatch the competition to see what techniques they use for their mobile marketing to get an edge on them. You need to stand out in a crowd of competitors.\nThe key to mobile marketing is to keep your message short and to the point. They need to be able to understand and absorb your message in that brief period.\nMobile platforms should drive visitors to your main site. You should want people to visit your webpage or keep them coming back to it. Your business shouldn’t be based solely on a mobile platform.\nYour friends can help you by viewing ads, emails, and your website on their various mobile devices. If you need unbiased opinions, you can hire testers.\nSet the times that your MMS text messages are sent to your customers for normal business hours. You’ll risk annoying customers with this practice, even people who already like your products or services.\nWatch your competition to get ahead in social marketing. The key is to catch the eye better than the others.\nYour mobile platform should have a concrete home base. Being mobile is a way to urge people to check out your home base, and also to stay in contact with those customers who are already avid visitors at your home base. It is a terrible idea to base your business around your mobile platform solely.\nIf you want to succeed in mobile marketing, it is important to remember the limitations of mobile devices when developing your domain and advertisements. Sample many different types of mobile devices to give yourself the best possible idea of things that your customers experience.\nA customer base typically remains the same unless market factors prompt a change. However, this doesn’t always apply to mobile customers. Many dynamics outside the realm of your particular market can cause fluctuations in this customer base. Mobile is still an evolving market, so technology trends can shift the market rather quickly. Know this and account for it.\nIf you are going to be speaking to someone on the phone, treat them with respect and try to keep your sales pitch short and sweet. It’s up to you to behave in a professional manner.\nYou will see a good rise in profits with mobile marketing. Nowadays, many people use their phones for downloading apps or browsing social networking sites. Both of these are good ways to advertise and market your business. You need to bring your marketing to where the customers are.\nAlmost everyone is familiar with the idea of offering free apps, but not everyone understands just how easy it can be to develop one that meets the needs of your business. You can market a special app for your business to boost mobile marketing efforts. Research all the features that are available to you.\nBefore starting any new mobile marketing campaign, be sure your first campaign is successful. Measure your marketing success, not by immediate returns, but by the long-term effectiveness of your campaign. Design new campaigns by using strategies that have been successful in the past.\nMobile marketing should be developed one step after another for best results. You should also do this. Begin with simple texts and calls, then expand your campaign to include things like video content and social networking. Use all of the tools you can.\nRemember that the audience you are targeting are people and they are busy, just like you. Act accordingly.\nThere are many free apps you can give away to customers, but it’s also not that difficult to create your own apps. By creating its own unique, relevant app, a company can double or triple the success of its mobile marketing strategy. Be aware that you have a variety of different options to use here.\nThough many mobile users are connecting to the Internet, as well as using text messaging, that doesn’t mean that all these users understand text message abbreviations. If your customers are unable to make sense of your promotional text message, they will simply disregard it, which equates to lost sales opportunities.\nBy providing a venue for review writing, you will create a closer connection with your customers and build up your credibility. Many people tell you to get away, but you need to survey your clients as much as possible.\nTry A/B testing with your mobile landing page. Mobile pages need to be tested for usability, just as much as any other web page. The better of the two trial pages you create, as deemed by its success, should be your final choice, no matter how emotionally invested you may be in the other. Keep whichever page is more successful.\nMake sure to place links on your site that are associated to social networking sites and are geared towards your business. You can almost guarantee that your customers will not take the time to search for you, but if they know you are on a site, they probably will take the time to check you out.\nWhile mobile marketing can be difficult, it does have the benefit that you can tailor ads based on location. Unlike other marketing forms, mobile marketing has the unique opportunity to know exactly where you are. You could attract your customers to your stores and encourage them to share their location with their friends. Take time to think over the implications this has for your business and ways you can achieve a profitable location-aware strategy.\nIf you make it a simple process to send a mobile marketing ad, customers will! Be sure that your ad is easy to forward and consider offering an incentive to the person that forwards your ad.\nLocation is of the utmost importance when you are engaging in mobile marketing. Mobile marketing lets your customers know where you are and what you are doing. You could attract your customers to your stores and encourage them to share their location with their friends. Consider all the ramifications for your business and how you could create a strategy that is location-aware and effective.\nUse mobile-friendly maps and driving directions on your site. It’s likely your users will be on a mobile device when looking for your location. Create an easy way for your customers to reach you. Your maps should be readable on any mobile device, as well. Also, add a link that can take the viewer to Google Maps if they need turn-by-turn directions.\nMake sure mobile users can see directions to your store on their phone. A lot of people use mobile devices to track down brick and mortar stores in their area. Make yourself accessible. Make sure that all of your maps are clear on mobile devices, along with being found easily in mobile searches. Your link should allow the visitor to see Google Maps and get directions.\nIt is important to do research about your potential audience. Know what your target audience responds to before you start designing a mobile marketing plan. Are they more likely to use a mobile phone than a computer? Do they tend to use one operating system platform more than another? By knowing your audience, you will understand how to communicate with them better.\nYou need to not only focus on attracting new prospects, but keeping your old prospects when it comes to mobile marketing. Customers who are already invested will be much more open to getting text messages with updates about your products. Many times, the types of mobile marketing aimed at new prospects is perceived to be spam.\nDon’t send offers constantly. Marketing data shows that the optimal offer frequency is between once a week and 2-3 times per month. Convince the potential customer that they must act now. If people think that another, possibly better, discount will be arriving soon, they won’t jump on the current one.\nQR codes are a smart way to provide interactive content to your customers. They can be used to share many things including both information about your product and discounts. It is simple to capture these with a phone, and they are not hard to use. QR codes allow you to reach out to your customers in an efficient manner, and give them relevant information in a simple way.\nIncorporate a discount offer or a promo code in any mobile marketing message you send out. A discount offer or coupon will encourage people to visit your site and see what you have available.\nMake yourself pertinent. Remember that your mobile marketing efforts need to be meaningful and have purpose. Be sure that prospective and current customers are getting information that is relevant. It is important to remember that you are sending out information or making information available to a person who is spending time reading it. Always imagine receiving what you are sending out to maintain awareness of the person at the other end of your business marketing.\nClaim your location on all the popular social networking pages. This makes it easier for online users to locate your business. Start with the basics, then your business can branch out to more sites. It’s essential to establish a presence on Facebook and Foursquare, at the very least.\nGet yourself short codes that are dedicated as opposed to sharing them. It will likely cost you a couple thousand dollars to do so, but it will be directly connected to your brand. At some point, your short code will be recognized and people will associate it with your company. This is very little that needs to be paid to avoid legal issues. For example, if a free or discounted code sharing service breaks the rules, your association with them could get you in trouble, too.\nWork your social networks to generate the most interest in your mobile marketing. You need to expose your message to the largest audience possible, so advertise it on social media, your website and even in your store. Let your customers know how to get involved in your offers, you will get the most out of your campaign.\nDo not send emails out too often. You’ll realize higher redemption rates if you don’t send them out too often. Your customer should feel as if they will miss out if they don’t accept the offer immediately. Customers are more likely to ignore a deal if they know another one is going to arrive shortly.\nProvide content that will be considered valuable by the sorts of customer you’re looking for. Provide value to get the attention of people outside of your friend circle. If you are sending messages to high fliers, you want to send them something like a discount coupon for an upscale establishment. If your audience is the middle class family, you would want to send something that is family related.\nMobile marketing can be extremely effective, but if it is used improperly it will simply irritate people. Take a measured approach following some of the tips presented here. Learn all you can, especially if you are a novice. This approach will help you to gain new customers.\nThink about making a mobile application geared to your product. Customers looking for specials, sales and promotions will check your mobile application regularly. It can build up your brand name’s recognition, and it will drive more traffic to your site. You should seek professional help as you develop apps, this can help you in the long run.","Email marketing remains a powerful tool for businesses to engage with their audience, nurture leads, and drive conversions. To run successful email campaigns, it's crucial to have a well-planned strategy in place.\nIn this comprehensive guide, we'll explore the essential tools and resources needed, provide tips for creating compelling email content, and share valuable insights for optimizing your campaigns to maximize results.\nChoosing the Right Email Marketing Platform: To execute effective email campaigns, you'll need a reliable email marketing platform. Popular options include Mailchimp, Constant Contact, ConvertKit, and Drip. Consider factors such as ease of use, automation capabilities, segmentation options, and analytics when selecting the platform that best suits your needs.\nDefining Your Campaign Goals: Before diving into content creation, clearly define your campaign goals. Are you aiming to drive sales, nurture leads, promote a new product, or simply provide valuable information? Having a clear objective in mind will shape your content and ensure your campaigns are focused and effective.\nSegmenting Your Audience: Segmenting your email list allows you to target specific groups with tailored content, resulting in higher engagement and conversions. Consider demographics, purchase history, interests, or engagement levels when dividing your audience into segments. This enables you to deliver personalized content that resonates with each group.\nCrafting Compelling Email Content: Creating engaging email content is key to capturing your audience's attention. Keep these tips in mind:\nWrite attention-grabbing subject lines that entice recipients to open your emails.\nKeep your emails concise and scannable, using subheadings, bullet points, and visuals to enhance readability.\nPersonalize emails by addressing recipients by name and leveraging data to include relevant information.\nIncorporate a clear call-to-action (CTA) that prompts recipients to take the desired action, whether it's making a purchase, registering for an event, or downloading a resource.\nOptimizing Email Campaigns: To maximize the impact of your email campaigns, follow these optimization tips:\nA/B test subject lines, email content, CTAs, and send times to identify the most effective elements.\nEnsure your emails are mobile-friendly, as a significant portion of users access emails on their mobile devices.\nRegularly clean your email list by removing inactive subscribers to improve deliverability rates and engagement metrics.\nMonitor and analyze campaign performance using analytics provided by your email marketing platform. This allows you to track open rates, click-through rates, and conversion rates, providing insights for future optimization.\nCompliance and Data Privacy: Ensure compliance with email marketing regulations by obtaining permission from recipients before adding them to your email list. Familiarize yourself with data privacy laws, such as GDPR or CAN-SPAM, and include an unsubscribe link in every email to give recipients the option to opt out.\nValuable Tools and Resources: In addition to an email marketing platform, consider these tools and resources to enhance your email campaigns:\nEmail design tools like Canva or BEE Free for creating visually appealing email templates.\nEmail automation tools to streamline your campaigns and send timely, targeted emails.\nEmail deliverability tools like SenderScore or GlockApps to monitor and improve your email deliverability rates.\nWith the right tools, compelling content, and optimization strategies, you can create and execute email campaigns that drive engagement, conversions, and foster long-term relationships with your audience. Remember to choose a reliable email marketing platform, segment your audience, craft engaging content, optimize your campaigns, and ensure compliance with data privacy regulations. By following these best practices, you'll be on your way to email marketing success."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"answer_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:40471717-9b8b-4273-a883-7c9a0904fcdc>","<urn:uuid:92972d02-9cdb-40d8-aaa2-959661b173b4>"],"error":null}