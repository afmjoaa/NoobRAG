{"question":"How do dominant and recessive inheritance patterns differ from autosomal dominant inheritance in terms of trait expression?","answer":"Dominant/recessive inheritance and autosomal dominant inheritance differ significantly in how traits are expressed. In dominant/recessive inheritance, traits are only expressed if an individual has two copies of the recessive allele (homozygous), while the dominant allele will be expressed even with just one copy (heterozygous). For example, blue eyes only appear when both alleles are recessive, while brown eyes appear with just one dominant allele. In contrast, with autosomal dominant inheritance, having just one copy of the altered gene is sufficient to develop the trait, with each child having a 50% chance of inheriting it from an affected parent. Some autosomal dominant conditions, like Huntington's disease and Neurofibromatosis type 1, can appear even with a single copy of the altered gene, though in some cases the gene may not be fully penetrant.","context":["Individual differences |\nMethods | Statistics | Clinical | Educational | Industrial | Professional items | World psychology |\nBiological: Behavioural genetics · Evolutionary psychology · Neuroanatomy · Neurochemistry · Neuroendocrinology · Neuroscience · Psychoneuroimmunology · Physiological Psychology · Psychopharmacology (Index, Outline)\n- For other non-genetic uses of the term \"dominance\", see Dominance.\nIn genetics, dominance relationships control whether an offspring will inherit a characteristic from the father, the mother, or some blend of both. More technically, they control the ways genes interact to express themselves as phenotypes in a diploid or polyploid individual.\nThere are three kinds of dominance relationships:\n- Simple dominance\n- Incomplete dominance\nTraits inherited in a dominant-recessive pattern are often said to \"follow Mendelian inheritance\".\nThe dominant/recessive relationship is made possible by the fact that most higher organisms are diploid: that is, most of their cells have two copies of each chromosome -- one copy from each parent. Polyploid organisms have more than two copies of each chromosome, and follow similar rules of dominance, but for simplicity will not be discussed here.\nHumans, a diploid species, typically have 23 pairs of chromosomes, for a total of 46. In regular reproduction, half come from the mother, and half come from the father (see meiosis for further discussion of how this happens, and chromosome for less usual possibilities in humans or in cows).\nRelationship to other genetics conceptsEdit\nAlthough humans have only 46 chromosomes, it is estimated that those 46 contain 20,000-25,000 genes, each of which is related to some biological trait of the organism. Many genes are strung together in a single chromosome. The other chromosome of the pair will have genes for the same functions -- for example, to control height, eye colour, and hair colour.\nHowever, since one chromosome came from each parent, it is quite unlikely that the genes will be identical. The specific variations possible for a single gene are called alleles: for a single eye-colour gene, there may be a blue eye allele, a brown eye allele, a green eye allele, etc. Consequently, a child may inherit a blue eye allele from their mother and a brown eye allele from their father. The dominance relationships between the alleles control which traits are and are not expressed.\nConsider the simple example of the dominant brown eye allele and the recessive blue eye allele. In a given individual, the two corresponding alleles of the chromosome pair fall into one of three patterns:\n- both blue\n- both brown\n- one brown and one blue\nIf the two alleles are the same (homozygous), the trait they represent will be expressed. But if the individual carries one of each allele (heterozygous), only the dominant one will be expressed. The recessive allele will simply be suppressed.\nLatent recessive traits appearing in later generationsEdit\nIt is important to note that an individual showing the dominant trait may have children who display the recessive trait. If a brown-eyed parent is homozygous, they will always pass on the dominant trait, and therefore their children will always have brown eyes, regardless of the contribution of the other parent. However, if that brown-eyed parent is heterozygous (and they typically would have no way of knowing), they will have a 50/50 chance of passing on the suppressed blue-eyed trait to their offspring.\nIt is therefore quite possible for two parents with brown eyes to have a blue-eyed child. In that situation, we can conclude that both parents were heterozygous (carrying the recessive allele).\nHowever, unless there is a spontaneous genetic mutation, it is not possible for two parents with blue eyes to have a brown eyed child. Since blue eyes are recessive, both parents must have only blue-eyed alleles to pass on.\nMain article: Punnett square\nThe genetic combinations possible with simple dominance can be expressed by a diagram called a Punnett square. One parent's alleles are listed across the top and the other parent's alleles are listed down the left side. The interior squares represent possible offspring, in the ratio of their statistical probability. In this example, B represents the dominant brown-eye gene and b the recessive blue-eye gene. If both parents are brown-eyed and heterozygous, it would look like this:\n|B||B B||B b|\n|b||b B||b b|\nIn the BB and Bb cases, the child has brown eyes due to the dominant B. Only in the bb case does the recessive blue-eye trait express itself in the blue-eye phenotype. In this fictional case, the couple's children are three times as likely to have brown eyes as blue.\nTraits governed by simple dominanceEdit\n(not an exhaustive list)\n|Curled Up Nose||Roman Nose|\n|Clockwise Hair Whorl||Counter-clockwise Hair Whorl|\n|Can Roll Tongue||Can't Roll Tongue|\n|Widow's Peak||No Widow's Peak|\n|Facial Dimples||No Facial Dimples|\n|Able to taste PTC||Unable to taste PTC|\n|Earlobe hangs||Earlobe attaches at base|\n|Middigital hair (fingers)||No middigital hair|\n|No hitchhiker's thumb||Hitchhiker's thumb|\n|Tip of pinkie bends in||Pinkie straight|\nSome genetic diseases carried by dominant and recessive allelesEdit\n- Main article: Genetic disorder\n|Some types of Dwarfism||recessive|\nAs can be seen from this, dominant alleles are not necessarily more common or more desirable.\nIn incomplete dominance (sometimes called partial dominance), a heterozygous genotype creates an intermediate phenotype. In this case, both the dominant and recessive gene are expressed, creating a blended or combined phenotype. A cross of two intermediate phenotypes can result in the reappearance of either the parent phenotypes or the blended phenotypes.\nThe classic example of this is the colours of carnations.\nR is the gene for red pigment. R' is the gene for no pigment.\nThus, RR offspring make a lot of red pigment and appear red. R'R' offspring make no red pigment and appear white. RR' and R'R offspring make a little bit of red pigment and therefore appear pink.\nAn example of incomplete dominance in humans is mordan, a trait that is exhibited when eye color alleles from the maternal and paternal chromosomes are blended. This usually occurs when one parent has green eyes and the other parent has brown eyes–the child will have dark blue eyes.\nIn co-dominance, neither phenotype is dominant. Instead, the individual expresses BOTH phenotypes. The most important example is in Landsteiner blood types. The gene for blood types has three alleles: A, B, and i. i causes O type and is recessive to both A and B. When a person has both A and B, they have type AB blood.\nAnother example involves cattle. If a homozygous bull and homozygous cow mate (one being red and the other white), then the calves produced will be roan-colored, with a mix of red and white hairs.\nExample Punnett square for a father with A and i, and a mother with B and i:\nAmongst the very few co-dominant genetic diseases in humans, one relatively common one is A1AD, in which the genotypes Pi00, PiZ0, PiZZ, and PiSZ all have their more-or-less characteristic clinical representations.\nMost molecular markers are considered to be co-dominant.\nIt is important to note that most genetic traits are not simply controlled by a single set of alleles. Often many alleles, each with their own dominance relationships, contribute in varying ways to complex traits.\nThe development of phenotype\n|Key concepts: Genotype-phenotype distinction | Norms of reaction | Gene-environment interaction | Heritability | Quantitative genetics|\n|Genetic architecture: Dominance relationship | Epistasis | Polygenic inheritance | Pleiotropy | Plasticity | Canalisation | Fitness landscape|\n|Non-genetic influences: Epigenetic inheritance | Epigenetics | Maternal effect | dual inheritance theory|\n|Developmental architecture: Segmentation | Modularity|\n|Evolution of genetic systems: Evolvability | Mutational robustness | Evolution of sex|\n|Influential figures: C. H. Waddington | Richard Lewontin|\n|Debates: Nature versus nurture|\n|List of evolutionary biology topics|\nReferences & BibliographyEdit\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|","Chromosomal theory of inheritance\nGenes are located on chromosomes. Chromosomes are in pairs and genes, or their alleles, are located on each of these pairs.\nWhen the cell divides in half, each chromosome ends up in a different cell. This is seen during meiosis in formation of egg cells of sperms. The genes also split into two halves. These are called alleles. Meiotic products have one of each homologous chromosome but not both. Meiosis is a series of cell divisions that creates haploid cells with half of the total number of chromosomes.\nOnce the egg and sperm meet, the pairs are restored but now the genetic combination of the pair is altered. One of the alleles thus comes from the mother and another from the father. This is how a defective gene causing a genetic disorder is also inherited by offspring.\nGene Linkage on Chromosomes\nAs per the Mendelian principles of inheritance genes need to be inherited independently of each other. However, there are far more genes than chromosome pairs. It is found that all of the genes on a chromosome are physically inherited together as a single linked group. Only genes that are located on different chromosomes have independent assortment during meiosis.\nCrossing-Over and Recombination\nAccording to the chromosomal theory 25% would resemble one parent, 25%, the second parent, 25% would have one trait from one parent and one from the other parent – and 25% would have also have the \"other\" traits from each parent.\nSince each chromosome is a diploid, or occurs in pairs, genes on different chromosomes assort independently during sexual reproduction, recombining to form new combinations of genes. Genes on the same chromosome would theoretically never recombine.\nHowever, genes do undergo cross over. During crossover, chromosomes exchange stretches of DNA, effectively shuffling the gene alleles between the chromosomes. This process of chromosomal crossover generally occurs during meiosis.\nThe probability of crossover occurring between two given points on the chromosome is related to the distance between the points. If the distance is long there is a higher chance of a crossover. For genes that are closer together, however, the lower probability of crossover means that the genes demonstrate genetic linkage that means that the alleles for the two genes tend to be inherited together. The amounts of linkage between a series of genes can be combined to form a linear linkage map.\nAutosomal recessive inheritance\nMost genes have a second working copy and one of them may not be actively functional at all. When this is inherited it is known as autosomal recessive inheritance. With recessive genes, it is only if someone inherits two altered copies of the same gene from each of their parents, they are likely to be expressed.\nFor example, inheritance of thalassemia and cystic fibrosis occurs in an autosomal recessive pattern. If both parents carry the faulty gene on one of their alleles they are likely pass on their defective gene in only 25% of their offspring. The baby of this couple has a 75% chance of not developing the condition. There are also other inherited characteristics that are inherited in this way such as blue eyes or red hair.\nFigure 1: Autosomal recessive inheritance. A person who has one altered copy and one working copy of the gene is known as a carrier for that particular altered gene. If two carriers have children together, they have a 25% chance in each pregnancy of their child inheriting two copies of the altered gene and having the condition.\nAutosomal dominant inheritance\nSometimes the inherited gene is a dominant one. This means if a person carries even one copy of that gene, he or she is likely to develop the trait. In some dominant conditions, it is possible to inherit an altered gene without showing any signs of the condition: in other words, the gene is not fully penetrant. Examples of genetic conditions that are inherited in a dominant way are Huntington’s disease and Neurofibromatosis type 1 (NF1).\nIf a parent carries an altered gene for a dominant condition, each of their children has a 50% or 1 in 2 chance of inheriting the altered gene. This is same for all children irrespective of sex of the child.\nFigure 2: Autosomal dominant inheritance - If a person who is a carrier for a dominant altered gene has children, there is a 50% chance of their child inheriting the altered copy of the gene.\nThis is seen when an altered gene is located on the X chromosome (one of the sex chromosomes) rather than on one of the autosomes. Women have two X chromosomes while men have an X and a Y.\nIf this gene is a recessive one then a woman who carries an altered copy will either have no signs of the condition caused by that gene or will have minor signs of the condition. She is said to be a carrier of that X-linked condition. If a man has an altered gene on his X chromosome, then he will have the condition as he has only one X chromosome.\nAccording to inheritance patterns if the woman has a boy there is a 50% chance that her son will have the condition. If she has a girl, there is a 50% (1 in 2) chance that her daughter will inherit the faulty copy and be a carrier like her mother.\nIf the father is has the faulty gene inherited from his mother he is likely to pass it on to all (100%) his daughters and they will be carriers themselves. Because men do not pass on their X chromosome to their sons, none of their sons will have the same X-linked condition as their father (0%).\nHaemophilia and Duchenne muscular dystrophy as well as red-green colour blindness is inherited this way.\nFigure 3: X-linked inheritance - X-linked recessive conditions affect men more often and more severely than women because men only have one X chromosome and women have two X chromosomes so usually have a second working copy of the gene. A woman who is a carrier for an X-linked condition has a 50% chance in each pregnancy of a son having the condition and a 50% chance of a daughter being a carrier for the condition. Men who have an X-linked condition will pass on the altered gene to all their daughters (100%), but none of their sons (0%)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:bf147c71-1485-4868-a06d-294a8a1b5ba6>","<urn:uuid:68caa0ec-1df2-42e6-9774-7eac8db91b7e>"],"error":null}
{"question":"Could you compare how both Beijing and Flowery Branch got their names?","answer":"Beijing got its name in 1403 when the Ming Dynasty emperor Yongle officially changed the city's name to Beijing. In contrast, Flowery Branch originated from the Cherokee word 'Anaguluskee' which means 'flowers on the branch', and this Cherokee trading post later became the town known as Flowery Branch.","context":["Telling stories of Native American history is a core tenant of the History Center’s mission, especially as we are custodians of the White Path Cabin. For November, Native American History Month, we’ve developed and presented two tracks of programming to go along with the Cherokee Court Cases in Georgia exhibit in the rotating gallery.\nOn the evening of November 15th, a very spirited – as a matter of fact fiery! – Museum Theatre performance, A Decade of Removal, presented Choctaw Chief Greenwood Leflore, Cherokee Chief John Ross, and President Andrew Jackson (played by Justin Hoffman, Matt House, and Ken Johnston respectively) taking questions from the audience and calling out each other on the issues with Removal – moderator Glen Kyle had his hands full!\nKen Johnston portraying President Andrew Jackson\nMatt House portraying Choctaw Leader Greenwood LeFlore\nJustin Hoffman portraying Cherokee Leader John Ross\nIn addition, our Director of Education has presented several Webcast performances as Samuel Worcester, of Worcester v Georgia fame, generating some excellent discussions with, of all people, eighth graders! With this combination of traditional exhibits, on-site performances, and digital outreach, the History Center confirms in November’s Native American History Month what it does year round – preserve and share the unique history of our region and all its people.\nStudents meet Samuel Worcester during a live Webcast\nKen Johnston portraying Samuel Worcester during a live Webcast\nThis week the History Center had a group tour with visitors from Germany! They were in town for a choral workshop with Gainesville native Ingrid Arthur. The group enjoyed getting to know Gainesville's history and later performed at Antioch church. Dankeschön!\nThe city of Flowery Branch, Georgia began as a Cherokee trading post called Anaguluskee, a Cherokee word meaning ‘flowers on the branch’ hence the name Flowery Branch. The town was established in 1874 as a railroad stop from Atlanta to Charlotte, North Carolina. The town was the epicenter of cotton farming in Northeast Georgia until the 1920s when the cotton market declined. Wrigley, the company known for its chewing gum, established a factory in Flowery Branch in the 1970s that still continues to provide local jobs. The town is most notably known today as the training grounds for the Atlanta Falcons football team. The $20 million dollar training facility was completed in 2005 before the upcoming training season.\nA view of Flowery Branch in 1899 on Main Street where cotton was sold\nThanks to everyone who attended this week's Forum on the Berlin Wall presented by History Center Board Member Ron Stowe. We have consistently had a full house at our Forums! Thanks to everyone who has joined us for Forums this year. Our next Forum is Films of the Great War on February 11th, 2020.\nThis week From the Archives is a box of Golden Glint Shampoo from 1962. In the early 1900s, the J.W. Kobi Company sold everything from hairpieces to handbags, face creams to dress patterns. The company looked to expand further into the shampoo business and created Golden Glint Shampoo, a scented powder shampoo meant to be an alternative to liquid shampoo; they also had tint in them to “brighten your natural color.” Golden Glint would prove to be their most popular product, selling nationwide in 1921.\nIn our archives we have two boxes of unused Golden Glint Shampoo products. There are many tints including golden blonde, silver, lustre glint, dark copper and titian blonde. The boxes come from Whatley’s Pharmacy, a company in Gainesville that resided in the downtown Gainesville square. The boxes are in amazing condition and are a great piece of history in our archives.\n'Now I am become Death, the destroyer of worlds.' – In August of 1945 atomic bombs were dropped on the Empire of Japan in an attempt to bring about its surrender before an invasion of its Home Islands by the Allies was necessary. But was the bombing itself necessary? Might Ken and Glen have opinions on the subject? Oh, you know they do. Join them as they discuss the development and deployment of the only nuclear weapons ever used, and what dual purpose their use had in 1945.\nSuggest a topic for an episode at email@example.com\nLunch & Learn Thanksgiving: Buckled-Hats and Blunderbuss?\nThursday, November 21st from 12:15 - 1:00 PM\nIncluded in Admission\nFamily Day: Victorian Christmas\nSunday, December 8 from 1-4 PM\nJoin us in December for our most popular event of the year – Victorian Christmas! A great event to attend before the Christmas parade begins (and there's plenty of parking!) With hands-on activities, music, dance, storytelling, and more you’ll be immersed in the holiday traditions of 120 years ago. Family Days are free to the public thanks to the Ada Mae Ivester Education Center\nFor more fascinating photos and information on our region's past, follow our social media!\nPhotograph of Engine 209's last run with its crew in Gainesville in 1959.","Beijing History Facts and Timeline\nSince the first walled city, Ji, rose in the 11th century BC, Beijing has remained at the heart of Chinese civilisation. It was often the capital city for whoever was running the province at the time, and even today has emerged as modern China's administrative capital.\nPacked with culture and history, Beijing offers a tantalising glimpse into the many facets of mysterious China.\nImperial State Capital\nBeijing started out as the capital of the State of Yan, one of the big powers during the Warring States era of 473 to 221 BC. Although the Yan eventually fell, the city's conquerors liked the location sufficiently that it was subsequently made the prefectural capital of the region during the following Qin, Han and Jin dynasties.\nThings really changed for Beijing's history when Mongol forces burned the city of Zhongdu to the ground in 1215. Following this, in 1264, Kublai Khan, a relation of Genghis Khan, built his new territorial capital of Dadu right next to the levelled city, around today's landmark of the Drum Tower, in the northern part of the city. The Mongols continued to rule for the next century, until the first Ming emperor razed the city in 1368.\nBeijing is Born\nThe third emperor of the Ming Dynasty, Yongle, changed the city's name to Beijing in 1403 and gave it co-capital status alongside Nanjing. Yongle was responsible for starting the Forbidden City project, which was completed in 1420 and remains one of the world's most stunning architectural creations. Once the Forbidden Palace was built, the city officially became the capital of the Ming Dynasty. Of interest, 13 of the 16 Ming emperors are buried just outside the city.\nA Global City\nIt is believed that Beijing was the world's biggest city from 1425 to 1650. Most of modern Beijing's shape was created during the early years of the Ming Dynasty, with notable landmarks like Tiananmen Square and the Temple of Heaven being built during this time.\nWhen the Ming era came crashing to an end in 1644 due to a peasant revolution, the Qing quickly stepped in to become China's new rulers. They held on to the city until the Second Opium War (1856 to 1860) saw French-Anglo forces loot and burn the city's two summer palaces in 1860. Some 40 years later, in 1900, Beijing's history was again shaped by the Boxer Rebellion, which heralded the end of dynastic China and the beginning of a new state.\nThe People's Republic\nDuring the Republican era, Beijing went through a rough patch brought about by warring Chinese powers and Japanese occupation of the city in 1937. When Mao Zedong won back control of China in 1949 and instituted the People's Republic of China, Beijing was once again tapped as the national capital of the new country.\nToday, it continues to be on the frontline of China's evolution, with internal struggles like the Tiananmen Square incident of 1989 and more recently, its designation as the host of the 2008 Summer Olympics, marking memorable moments in Chinese history. As China continues to discover its latest role in global geopolitics, the history of Beijing will be sure to evolve in step."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:19a62bdb-e5b8-4980-bd1c-230f07ab05a2>","<urn:uuid:0843dc5a-28dd-4ac1-aa6c-020cbd08ebc1>"],"error":null}
{"question":"What are the essential maintenance considerations for construction measurement tools, and how do these principles apply specifically to continuous water analyzers?","answer":"For construction measurement tools like spirit levels, maintenance involves regularly checking accuracy by testing the level in two directions, ensuring vials aren't damaged, and verifying the bubble remains visible and properly centered. The level should be checked before starting any job, especially for high-accuracy work. For continuous water analyzers, similar attention to maintenance is critical - operators must monitor reagent levels, perform regular visual inspections for leaks and obstructions, maintain proper calibration, and check sensors for fouling or damage. Both types of equipment require creating a systematic maintenance routine, following manufacturer recommendations for component replacement, and keeping detailed records of past issues to prevent future problems.","context":["Getting things level (or vertical) is a critical part of any construction project and for a lot of home improvement ones too. The level is one of the most useful and important tools in any toolbox. The aim of this project is to explain which type of level you should use for different types of jobs and how to use a level effectively.\nWhat is a Level?\nAccording to the Encyclopaedia Britannica a Level is a:\ndevice for establishing a horizontal plane.\nThis is a clear definition as any; succinct and correct. Most levels can also be used to find a vertical plane also, generally in exactly the same way as finding the horizontal plan. There are many different types of levels that are used in different circumstances, of which the spirit level is one. We will cover the main types now.\nThe Different Types of Level Used in Construction and DIY\nBroadly speaking there are 4 types or groups of level that are commonly used in construction projects, although we are only going to focus on the spirit level in this project it is worth quickly mentioning the other types before we start:\nSpirit (or Bubble) Levels\nWe will explain the different types and also how to use a level in greater detail below.\nAs the name suggests this is a level that uses a laser to project the level on to the surface that you need to establish the horizontal plane. The more expensive and sophisticated devices will show the vertical plane also, which can be very helpful.\nWe will not cover these here, as we have an excellent project all about using laser levels that you should read.\nThese are tools that are generally used by surveyors and builders on larger construction sites where a level plane needs to be established over a long distance, such as across the whole building site.\nWhen using an optical level, the level plane is actually derived using a bubble in a vial and then using the lens it is transferred to the site.\nThis is one of the more simple and elegant solutions to transferring levels from one place to another, however they will not establish a level plane. We explain how to do this and even make your own in our project about water levels.\nHow does a Spirit or Bubble Level Work?\nThere is a small glass tube or vial containing alcohol with a little bubble sealed inside. The top of the vial or tube is ever so slightly bowed so that when the flat surface under the vial or level is absolutely horizontal the bubble will come to rest in exactly the middle of the tube or vial. This is usually marked with guide lines to make it easier to see.\nThe accuracy of the level is determined by how curved the top of the vial or glass tube. The “flatter” it is the more sensitive it is and therefore more accurate it will be (in the right hands).\nA tube vial will be able to show the level in one plane but will tends to be easier to use and more accurate for hand use. The different planes will all need to be checked separately; widthways then lengthways and ideally across the diagonal too to get a completely horizontal surface. (A cross check level is often used for this – see types of level below).\nWith a round bubble vial you can get the level in all planes in one go although they can be more fiddly to use without a firm base and a mechanism for fine adjustments, which is why they are most commonly used to level devices that have a tripod stand or similar stand.\nHow to use a Spirit Level\nThis is a quick step by step process for how to use a spirit level:\n- Place the level against the surface where you want to find the level. Ideally this should be smooth and flat so the level can be held securely to it without moving or wobbling\n- Move the level until the bubble in the vial or tube is exactly in the middle of the guide lines. Depending on the level you are trying to achieve this might mean moving the level itself, say against the wall if you are ensuring that pictures or sockets are at the same level, or moving the object the level in on to get it horizontal, such as a paver for a patio or perhaps a shelf\n- Check the bubble again, before marking the level if required\nMeasuring a Perpendicular or Plumb with a Spirit Level\nIn the same way a spirit level can be used to measure true vertical surfaces, or what is called plumb in the trade.\nThe same process is used however the level needs to have a vertical vial which will show the level of the perpendicular.\nMeasuring a Slope with a Spirit Level\nOn many occasions you want the surface to look level, but to actually be on a slight slope. This might be when laying a patio to ensure water runs off, or to have a slight slope in your screed.\nIf this is the case rather than ensuring that the bubble is in the exact centre of the vial, between the guide lines, the bubble should be slightly off centre. Here it helps if you have a vial with multiple guidelines so that you can ensure that the bubble is in the same place at all paced along your (slightly off) horizontal surface.\nOnce you have established the angle that you need the slope to be used can be transferred using the spirit level to ensure that you consistently keep to that slope.\nTIP: An old builders trick to getting the right level is to place the base of the spirit level in with the right level, then you can draw on to the top of the vial with a felt pen where the bubble comes to rest. Now as you work across the surface you can ensure that the level comes to the same point, so the bubble comes to rest at your felt pen mark. Remember to wipe the marks off when you are finished.\nSome spirit levels come with a 45o vial or even an adjustable vial. These can be used in the same way to establish the angle whether it is 45o or another angle set in the adjustable vial; you need to move the spirit level until the bubble is in the centre of the vial. At this point the level is at the required angle.\nHow to Check the Accuracy of a Spirit Level\nWhen you are looking to create a level, be it horizontally or vertically you need to be confident that the spirit level that you are using is accurate. The process for checking a level’s accuracy is very simple; effectively you are checking the level in two directions to see if there is a difference – the size of the difference is a measure of the inaccuracy, as ideally it should read the same in both directions.\nThis process works for all types of spirit levels, but you will need to check each vial or tube individually. Here are the steps for checking the accuracy of your spirit level:\n- Place the spirit level on a flat surface that is as near to level as you can conveniently find\n- Mark on the surface the end of the level and the side of the level\n- Check the position of the bubble – and remember exactly where it is\n- Turn the spirit level around by 180o and place it against the marks you made so it is in exactly the same position but just reversed\n- Check the position of the bubble (of the same vial if your level has more than one) – it should be in exactly the same position as when you first checked it relative to the level. Any difference is the degree of inaccuracy with that vial on the spirit level\nYou can use this same process for the vials used for measuring vertical surfaces in exactly the same way, only by placing the level against a vertical surface.\nIt is worth double checking your measurements twice so that you are sure of the accuracy.\nAs this is a very easy test, we recommend regular checks, certainly do it before you make a purchase, or if you have dropped or think you might have damaged it. We check our level before we start a job, especially if we know it is one where we need to be highly accurate.\nImportant Features of Spirit Levels\nBefore we get into the different types of levels, there are some important features to look out for and are common to all, as essentially the bubble is the part that all these types of level use to do their job.\nIf you are buying a level these are the features you should look out for:\nAll levels should have an accuracy tolerance which will be published for you to check before purchase. It is usually stated as “+ or – 0.Xmm per meter” so you can ensure you get the accuracy that you require\nThe bigger the better when it comes to the bubble as it is easier to see. The issue is that the bubble size is determined by properties of the liquid, which will expand and contract as it get hot and cold which affects the size of the bubble. The bubble should always remain inside the reading lines.\nThe sales patter will be about the liquids expansion confident and the guaranteed working temperatures. The better these are the more accurate but more expensive the level will be.\nMost levels nowadays have the liquid coloured so that it is easier to see the bubble. The colour will fade from the liquid over time and in the sun, but how long this takes is a mark of the quality, but of course you have no way to tell this in advance, except by a guaranty – and may be the price!\nThese are lines inserted into the tube or vial or in some cases they are painted. These need to be clear and easy to see without impacting the movement of the bubble.\nThe vial or tube is actually quite delicate and as such needs to be protected, however the level will be dropped and have a touch life especially if on a building site. The ends should be strong rubber or similar to reduce shock and the construction sturdy, but light weight. Look for shock proof vials and end caps when buying a spirit level.\nDepending on where you will be using your spirit level, you need the most robust body that you can get but will not damage the materials you are working with.\nBuilders will use robust aluminium levels, however if working with more delicate materials such as a work top you might consider a softer body made of something like plastic.\nFor accuracy, some aluminium levels will have one or more milled surface, which is a precision machined surface to ensure that the surface is absolutely flat and horizontal.\nUse and Grip\nYou need to be able to hold the spirit level in place to get an accurate reading so hand holds are helpful, particularly on the longer levels.\nThis is a feature that some levels will have and it is a groove in the surface of the level which is designed to fit against pipes of conduit so that you can ensure that they are fitted horizontally or perpendicular.\nSome levels have magnets within their bodies so that they can be “stick” to metal pipes, scaffold poles or other metallic surfaces. This means that they can be used “hands free”.\nThe Common Types of Spirit Level\nThere are a lot of different types of spirit levels. To make things even more confusing they also have different names often depending on who or what they are being used for or sometimes how they are constructed.\nHere are the most common types of spirit levels that you are likely to come across:\nCarpenters, Builders or Long Levels\n- i-beam levels: the frame of the level is an “i” shape when looking at its end. They are also called Girder Levels (because they look like a girder in cross section). They have similar uses to a box level but tend to be a slightly less sturdy, cost effective alternative. See our range of Girder levels here.\n- Box beam levels; also known as Box Level or Box Section because the frame is a rectangle, box shape. The best ones are not hollow but encase acrylic (or similar) to make them more robust. They are stronger than I-beam levels, but typically heavier. See more Box Levels here.\n- Torpedo levels: These are spirit levels that are ideal for keeping in a toolbox. They are slightly bigger than pocket-sized being 6 to 12 inches (15-30 cm) and typically with 3-vials. They are the generally purpose, all-rounder of the level world designed for use in tighter spaces\n- Scaffold Level: A specialist variation on the torpedo level is the Scaffold Level. As the name implies it is ideal for use when erecting scaffolding, but also used by plumbers and others working with pipes or metal surfaces. They have magnets to “stick” to the pipes. Sometimes called a Boat Level.\n- Pocket levels; these are small spirit levels as the name suggests, designed for quick use and often come with a belt clip mount and can be put in your pocket. Ideally they should have magnets and a v-groove for use with pipes and metallic surfaces.\nSpecialist Spirit Levels\n- Line levels; also called a String Level, these are a single vial with a hook so it can be hung on a string. They are for levelling across long distances, and this was a method that was used before laser levels were invented. Using a taught string tied between two points (so that there is no sag in the line), when the ends of the line are moved so that the bubble is in the middle of the guide lines, then the two ends (and all the line) are at the same level. The Line Level can be hung at either end of the line, whichever is more convenient, so long as the line is tight. They are ideal for getting a level for laying paving stones or for levelling ground\n- Cross check levels; Also known as 2D Levels or Right Angle Levels these are pocket sized levels with two vials at right angles to each other so that you can check the level in two planes at once\n- Circular levels; These are sometimes known as surface levels as they are useful for ensuring that a surface is level. They are most commonly found on devices that need to be levelled, such as a tripod for an instrument or camera\n- Post levels; This is a specialist level that is designed specifically for making sure that posts are plumb in all directions. The wrap around two sides of a post to measure the level in both horizontal planes. Most will have a third vial so that they can be used to ensure that horizontal rails, joists or other timbers, etc will be level.\n- Angle levels; these are levels that are designed to be used as on angles from the horizontal. They have an adjustable vial so that you can set the desired angle\nHome Made Levels\nIf you haven’t got a Spirit Level to hand or fancy making your own it is not actually that difficult. At the simplest you can put a marble or ball bearing on a flat surface and it will roll down any slope; when it does not roll away the surface is level.\nWater always finds a level so you know that the surface of standing water will be level. You can use this fact to find a level using a glass of water.\nSpirit Levels are a vital part of any toolkit but it is important to know how to use them and which type of level you should use. Hopefully this project has given you the confidence to get the right spirit levels for your needs.","Top tips for... analyser maintenance\nKeeping continuous water analysers well maintained is vital to ensure you can rely on the information they provide, writes Jon Farrington\nby Jon Farrington, Technical Manager, ABB\nContinuous water analysers are useful tool to help meet tighter environmental regulations and rising customer expectations, but their performance is only as good as the condition of the analyser itself.\nWith the introduction of ever more stringent standards by the UK Environment Agency, it is particularly important to maintain analysers to the highest possible levels as a matter of best practice. In the event of a water quality incident, operators must demonstrate that their monitoring and measurement equipment was being correctly maintained.\nWith analysers very much on the front line of every water treatment process, the need to ensure they are kept in good working order is critical to avoid any penalties arising from a failure. Following the steps outlined here will provide a useful starting point for helping to design an effective maintenance strategy and prolong the life of continuous water analysers.\n1. Heed the warning signs\nMany of the latest generation of analysers incorporate self-diagnostic features and alarm capabilities to warn when something is wrong. Although these signals are useful in determining when maintenance needs to be performed, it is nevertheless advisable to have a preventive maintenance regime in place to help extend the life of the instrument.\n2. Check reagent levels\nReagents are crucial for producing the chemical reaction needed for accurate measurement to take place. As such, it is important to ensure that reagents are replenished as soon as they are exhausted. Checking reagent consumption regularly will help you predict the best time to carry out a changeover. As reagents have a shelf life, check the expiration date before exchanging the used reagent with the new one. In addition, regular maintenance will also help to show whether the reagent is being consumed at the rate that should be expected for the application. Under or over consumption can be indicative of a problem in the analyser’s fluid handling system, such as a blockage or restriction.\n3. Perform regular visual checks\nAnalyser functions and readings should be thoroughly checked to ensure they work at optimum efficiency. Checking for leaks (pipe connections and tubing and liquid handling components), liquid levels (reagents, calibration and cleaning solution bottles) and potential obstructions to the sample flow should be performed as regularly as possible. It’s worth remembering that erratic readings on the analyser display unit may not be indicative of a fault with the transmitter, but rather external factors such as unexpected changes in water quality. Where doubts arise, it is advisable to check the analyser’s historical log to compare the period during which erratic readings took place against the known operating conditions. In addition, always check the analyser’s high and low measured values. Any discrepancies in the measured values may be symptomatic of a number of potential issues. These could include incorrect calibration or variations in ambient temperature and humidity conditions that could be affecting the sample. If a filter is fitted prior to the analyser, there is also the possibility that some important parameters may be being filtered out.\n4. Check calibration\nEnsuring that your analyser is properly calibrated will have a material impact on its ability to deliver continued measurement accuracy. Errors can arise when changing reagents, with potential failures arising if the reagents are not fitted properly, if tubing is blocked or restricted or if the reagent lines were not properly primed. Failure of a routine calibration can also be symptomatic of issues within the analyser, such as blocked tubes and valves, leaking seals or contamination of the reference standard.\n5. Check the sensor\nIt is worth checking analyser systems that are in direct contact with the process, as sensors can become obstructed or damaged as they often bear the brunt of any adverse conditions. In waste water treatment processes in particular, sensors can quickly become fouled by the build-up of organic matter or subjected to abrasion by sediment. The accuracy of electrochemical sensors used in dissolved oxygen monitoring applications, for example, can quickly become impaired due to fouling of the sensor membrane.\n6. Replace components regularly\nFollow the manufacturer’s recommendations regarding the replacement of any key components. Depending on the type of analyser being used and its application, consider changing parts such as tubing, measurement cells and seals and diaphragms on a yearly or bi-annual basis.\n7. Create a maintenance routine\nGiven the diverse range of parameters that are measured in water and waste treatment processes, it is difficult to set a universal rule when it comes to setting the frequency of maintenance checks. In most cases, it is possible to create a maintenance routine by monitoring the nature of past breakdowns with any issues that have been recorded, such as fouling, abrasion or measurement variations. By using these as a guideline, it will then be possible to create a maintenance routine that can help to safeguard against future occurrences. In addition, information from the device manufacturer should also be taken into account in order to further optimise performance.\n8. Consider the lifecycle\nAnother factor that can affect the way that an analyser is maintained is where it sits in its lifecycle. This should take into account the model, age and make of the analyser. Models that are at the start of their product lifecycle will be well-supported, with ready spares availability. This may differ for models from older product ranges, where spares and the expertise to service them may not be as readily available. Dealing with reputable manufacturers can help to minimise this, as they are more likely to be able to support older products or else to recommend a suitable upgrade path with an alternative replacement.\n-ABB offers comprehensive servicing and support for continuous water analysers. For more information, call 01480 488080 or email firstname.lastname@example.org.\n- Dewatering pump maintenance: Keep moving Maintaining your dewatering pumps goes a long way, writes Joe Moser, pump product manager, Atlas Copco Construction... Read More >\n- Drilling and Tapping: The Mains Man Jason Barratt's skill in drilling and tapping water mains has led to him competing around the world as well as advancing... Read More >\n- Comment: Capital Maintenance comes to the fore The tight cost of capital set by Ofwat for PR19 will mean water companies will need to place the emphasis on maintenance... Read More >\n- Moving towards maintenance 4.0 Water utilities need to embrace smart asset management technologies but that is only part of the solution, writes Chris... Read More >\n- A data-led approach to clearing FOG Water utilities have a major challenge working with local food businesses to prevent fats, oils and grease entering the... Read More >\n- The robots aren't coming... they're already here Stantec lead design engineer Dan Causley looks at advances in technology and their role in boosting productivity in the... Read More >\n- Opinion: A smarter way to read the water network Smart metering technology could be the key to closing water companies' knowledge gaps about usage in their network -... Read More >\n- Smart thinking: The digital asset management revolution With many of the pieces now in place for a digital asset management revolution, AI and machine learning solutions are set... Read More >"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:69d83e66-ca34-45f1-b229-f3b69f35fa31>","<urn:uuid:5473326e-b9a8-4f1b-8b6e-2dcea8e23119>"],"error":null}
{"question":"Compare the body mass relationship with fitness between female mice and the length ranking between the Nile and Amazon rivers - are they both clearly determined or is there controversy?","answer":"While the relationship between body mass and fitness in female mice is clearly positive with larger females producing more offspring, the length ranking between the Nile and Amazon rivers is controversial. The study shows a straightforward positive correlation between female mouse body mass and reproductive fitness in semi-natural enclosures. In contrast, while the Nile is currently considered the world's longest river at 6,650 kilometers compared to the Amazon's 6,400 kilometers, this status has been contested in recent years due to disputes about the true source of the Amazon and therefore its actual length.","context":["|Home | About | Journals | Submit | Contact Us | Français|\nSexual size dimorphism results when female and male body size is influenced differently by natural and sexual selection. Typically, in polygynous species larger male body size is thought to be favored in competition for mates and constraints on maximal body size are due to countervailing natural selection on either sex; however, it has been postulated that sexual selection itself may result in stabilizing selection at an optimal mass. Here we test this hypothesis by retrospectively assessing the influence of body mass, one metric of body size, on the fitness of 113 wild‐derived house mice (Mus musculus) residing within ten replicate semi‐natural enclosures from previous studies conducted by our laboratory. Enclosures possess similar levels of sexual selection, but relaxed natural selection, relative to natural systems. Heavier females produced more offspring, while males of intermediate mass had the highest fitness. Female results suggest that some aspect of natural selection, absent from enclosures, acts to decrease their body mass, while the upper and lower boundaries of male mass are constrained by sexual selection.\nBody size is influenced by natural and sexual selection with both female‐ and male‐biased sexual size dimorphism (SSD), as well as monomorphism, common across vertebrates (Andersson, 1994). Selective forces for increased female size include a positive relationship with fecundity, enhanced resources for parental care, and dominance over resources, while those for decreased female size include increased maturation rate and decreased energy demands; conversely, male‐biased SSD is primarily driven by physical competition for mates with the largest individuals having the highest fitness (Andersson, 1994; Clutton‐Brock, 2009; Cluttonbrock & Parker, 1992; Schulte‐Hostedde, 2007). Taken together, fecundity selection in females and sexual selection in males are largely thought to be the primary selective forces driving larger body size across organisms; however, it has proven more difficult to understand the counteracting selection which constrains body size.\nBlanckenhorn (2000) suggested four costs due to larger body size: (1) viability costs in juveniles due to longer development (or faster growth); (2) viability costs in adults due to predation, parasitism, or starvation; (3) decreased mating success of large males due to lack of agility or high energy costs; and (4) decreased fitness in both sexes due to late reproduction associated with longer development. These four hypotheses include pressures due to both natural (1 and 2) and sexual (3 and 4) selection; however, supporting evidence in vertebrates has been difficult to obtain for the two sexual selection hypotheses. Specifically, within vertebrates, costs associated with relatively large body size, in the context of male sexual selection, have only been demonstrated in the pied flycatcher (Ficedula hypoleuca) and serrate‐legged small treefrogs (Philautus odontarsus) (Alato & Lundberg, 1986; Zhu et al., 2016).\nHouse mouse (Mus musculus) populations inhabiting semi‐natural enclosures are well suited for quantifying selective forces operating on a variety of phenotypes and provide a unique opportunity to assess the natural and sexual selective forces that constrain body size (Carroll & Potts, 2007). Within these enclosures some, but not all, pressures of natural selection (e.g., predation) are absent, and most sexual selection pressures are present (including male–male competition and female choice ((Meagher, Penn, & Potts, 2000; Nelson, Colson, Harmon, & Potts, 2013))). Therefore, by assessing the reproductive success of mice in semi‐natural enclosures, one can evaluate a trait's influence on fitness in the context of moderate levels of natural selection and high levels of sexual selection.\nHere we assess the relationship between body mass (a measure of body size) and fitness in both sexes of house mice. Due to the nature of our study we control for three of the four hypothesized selective pressures on body size (1, 2, and 4 above) allowing us to assess whether male sexual section might act to constrain body size with counteracting pressures on males that are too small as well as those who are too large. We do this by retrospectively analyzing parentage and body mass data from three previous studies using our mouse semi‐natural enclosure system. Each of these studies directly tested outbred control mice in direct competition with experimentally manipulated mice; only control mice are analyzed here. The first study (S1) assessed the fitness consequences of inbreeding; parentage was conducted subsequently to evaluate the deleterious nature of the t‐complex (Carroll, Meagher, Morrison, Penn, & Potts, 2004; Meagher et al., 2000). The second (S2) and third (S3) studies assessed fitness consequences of pharmaceuticals (Gaukler et al., 2015, 2016). Collectively, the fitness and body mass data from these studies provide a unique opportunity to test the selective pressures that may constrain body mass in vertebrates.\nFrom 55 litters, 113 (75 female and 38 male) outbred wild‐derived mice were assessed. Mice from S1 (n = 77) were from the second generation of a colony initially described by Meagher et al. (2000), while those in S2 (n = 24) and S3 (n = 12) were from the twelfth. Mice entered enclosures as sexually mature adults (S1: 23.0 ± 9.5 weeks old, S2: 26.2 ± 7.1, S3: 27.1 ± 2.3, mean ± SD) and were weighed prior to release. Ten populations (S1: n = 7, S2: n = 2, S3: n = 1) were established with 16 females and eight males, half of whom were controls, and seven mice were not weighed. Collectively, these populations represent all published accounts from our laboratory with complete body mass and parentage data. The assessed studies were approved by the Institutional Care and Use Committee at the University of Utah (protocol #s 97‐11011, 07‐8002, and 10‐08002).\nIndoor enclosures are 30–50 m2 and are subdivided to promote territory formation. Subsections have food and water provided ad libitum associated with nest‐boxes in either “optimal” territories (with enclosed nest‐boxes) or “suboptimal” territories (with exposed boxes). Photographs and diagrams of enclosures may be found in the initial studies (Gaukler et al., 2015, 2016; Meagher et al., 2000). Offspring born within S1 populations were removed at ~6.4 weeks of age, while in S2 and S3 all offspring were collected at eight weeks into the study and then again at five‐week intervals; after removal, offspring were euthanized and tissues were harvested. Populations were maintained for 30.0 ± 4.3 weeks.\nFour–17 autosomal microsatellite loci were amplified per offspring. Primers were tagged with CY‐5 or CY‐3 fluorescent dye. DNA samples were PCR‐amplified and run on 6.25% denaturing acrylamide gel at 40 W for 3–7 hours. Gels were imaged on a FluorImager. Additional details on parentage analysis, including loci used, can be found in original reports (Carroll et al., 2004; Gaukler et al., 2015, 2016).\nFor an initial approach, offspring counts of both sexes were first modeled together using a generalized linear mixed model (GLMM) with a Poisson distribution and logarithmic link. We predicted offspring counts of mice across populations by modeling the fixed effects of body mass (at the time of entrance into enclosures), sex, and a sex‐by‐mass interaction, while study, population (nested within study), and litter were included as random effects. This initial model resulted in an unexpected negative correlation between body mass and fitness in males [contrary to published findings (Franks & Lenington, 1986; Krackow, 1993)]. Therefore, we next assessed the presence of a reproductive optimum for the male data alone by performing a GLMM with the same structure (excluding sex and its interaction) above and a generalized nonlinear mixed model (GNLMM) with a second‐order polynomial term for mass and the aforementioned random effects; the GLMM and GNLMM were then compared by Akaike information criterion.\nAs nonlinear models can be sensitive to extreme values we also evaluated the presence of linear versus negative‐quadratic relationships between mass and fitness using a bootstrapping approach. Specifically, separate Poisson generalized linear models (GLMs) (1,000 iterations) were used to assess the influence of body mass (second‐order polynomial), and to calculate an optimum if applicable, for each sex. Assessment between linear and quadratic relationships was performed by evaluating the consistency of positive and negative values (95% CIs) of first‐ and second‐order polynomial terms for mass. The influence of extreme values is mitigated as bootstrapping utilizes random sampling with replacement, which ensures that overall patterns are not driven by individual data points. Importantly, both analysis approaches reached almost identical conclusions. All models were run in R (3.3.1) using lme4 and boot (Bates, Maechler, Bolker, & Walker, 2015; Canty & Ripley, 2016; R Core Team, 2016). Data available from the Dryad Digital Repository: http://dx.doi.org/10.5061/dryad.v3p2g.\nMice weighed 15.2 ± 3.4 g (mean ± SD) range from 7.7 to 26.6 g and were sexually dimorphic with females weighing 14.3 ± 3.0 g and males weighing 17.1 ± 3.4 g (t test; t67 = −4.22, p < .0001). Offspring counts per mouse ranged from 0 to 109 with males producing more (36.1 ± 29.0) pups than females (12.0 ± 11.0; Wilcoxon; W = 661, p < .0001) as expected based on the 2:1 sex ratio.\nFemale fitness increased with increasing body mass (GLMM; Z = 2.44, p = .015; Figure 1a; Table 1A), while this relationship differed in males (GLMM; Z = −6.60, p < .0001). Male body mass had a negative‐quadratic relationship with high fitness possessing an optimal mass, as indicated by the GLMM having essentially no support (ΔAIC = 24.9) relative to the GNLMM (Figure 1b; Table 1B). Moreover, bootstrap models indicate a positive (95% CI: 1.80, 5.09) first‐order polynomial term for mass in females and negative (95% CI: −5.25, −0.21) second‐order mass terms in males (Figure 1c); these bootstrapping results are indicative that overall patterns are not driven by extreme points (e.g., the heaviest mice).\nInfluence of body mass on fitness. (a) For female mice, body mass is positively correlated with fitness. (b) For males, there is a negative‐quadratic relationship with an optimal mass (arrow) for fitness. For (a,b) points represent individuals, ...\nBody mass and fitness model results\nWe demonstrate a positive relationship between body mass and female fitness and a negative‐quadratic relationship in males. The positive relationship in females likely indicates larger females have higher fecundity, a pattern also seen within other rodents such as deer‐mice (Peromyscus sp.) and voles (Arvicolinae) (García‐Navas, Bonnet, Bonal, & Postma, 2016; Myers & Master, 1983), although alternative hypotheses such as differential resource control cannot be eliminated. As sexual selective forces are largely present within enclosures it is likely these forces which influence the observed optimum in male mass; for example, it is possible there is an optimal mass for winning agonistic contests, perhaps balancing strength/agility, or that females prefer to mate with males of intermediate size. These observations suggest that house mouse body size is, at least partially, constrained by male sexual competition and that the simple paradigm of “bigger is better” in regard to sexual selection is not applicable to this species.\nWe are able to assess the possibility of male sexual selection constraining body size because our study design and species selection control for three of the four costs of large body size suggested by Blanckenhorn (2000). By releasing all mice as adults we control for juvenile viability selection and by excluding predators and most parasites, while providing ample access to food/water we greatly reduce the pressure of adult viability selection. Likewise, the proposed cost of “late reproduction” is thought to be of primary importance in species with low encounter rates or with constrained mating periods, neither of which apply to house mice (Singleton & Krebs, 2007). The elimination of three of the four characterized costs of large body size allows us to conclude that stabilizing selection on body size, due to male–male competition, female mate choice, or a combination of the two is sufficient to constrain house mouse body size—an intriguing finding in a polygynous mammal.\nPrevious studies investigating relationships between fitness and body mass of house mice in semi‐natural enclosures have relied on dominance status as a proxy for fitness. A study of 32 mice found a marginally significant trend that “fitness‐rank,” based on social dominance, was positively correlated with male mass, but not female mass (Krackow, 1993), while another larger study found positive relationships for “dominance‐rank” and body mass in both sexes (Franks & Lenington, 1986). Importantly, neither study considered an optimal mass nor directly assessed fitness. One caveat concerning our study is that the analyses were limited to un‐manipulated control mice, who were cohoused with treatment individuals. Although this asymmetry in individual quality could influence the observed relationships, we find this unlikely as control mice were primarily in competition with each other, and gradients of individual quality are the norm in nature. In light of this caveat, it should be acknowledged that a study designed to directly test the influence of body mass on mouse fitness within semi‐natural enclosures considering nonlinear selection would be definitive; however, the results provided here are unique and illuminating on the selective forces shaping the evolution of body size.\nAlthough the notion that a causal relationship between male body size and success in acquiring mates leads to increased body size in both sexes is well supported in vertebrates (Fairbairn, 1997), it may not explain patterns of SSD wherein maximal male size is not optimal. Typically it is assumed that sexual selection for increased size in males is counteracted by natural selection (e.g., predation, interspecific competition); however, the presence of an optimal body mass in semi‐natural enclosures with high levels of sexual selection (and reduced levels of natural selection) indicates house mice may be an exception to this rule. Broadly, our results support that fecundity selection in females may be a primary selective agent for large body size, but question the extent to which larger body size in males is universally beneficial in the context of sexual selection. Moreover, based on the observation herein, that larger females have more offspring when natural selection is relaxed, perhaps instead of asking “why are males relatively large?” we should ask “why are females small?.”\nJ.S.R., J.W.C., A.C.N, and W.K.P., designed the study. S.M.G., S.M., conducted enclosure studies. S.M.G. and L.S.C., assessed parentage. L.C.M. curated data. J.S.R and D.H.C. analyzed data. J.S.R. and W.K.P. wrote the manuscript.\nWe thank Frederick Adler, Christopher Cunningham, and Jon Seger for comments on the manuscript and statistical analyses. This study was conducted, while W.K.P. was supported by National Institutes of Health Grant (R01‐GM109500).\nRuff JS, Cornwall DH, Morrison LC, Cauceglia JW, Nelson AC, Gaukler SM, Meagher S, Carroll LS, Potts WK. Sexual selection constrains the body mass of male but not female mice. Ecol Evol. 2017;7:1271–1275. doi:10.1002/ece3.2753.","Interested to know about fun facts about rivers? Of all the bodies of water in the world, few are as impressive as the great rivers. Crucial to the development of some of the world’s most noteworthy cities, these rivers are the thoroughfares from which much of civilization was built. Here is a list of the ten longest and most important rivers in the world.\nThe Amur River is the tenth longest river in the world. It originates from western Manchuria and flows eastward to form the border between China and Russia. From there it courses to the southwest in a 400 kilometer arc.\nIn terms of water discharged, the Congo is the second largest river in the world. Located in Africa, the Congo River also has the distinction of being the world’s deepest rivers, measuring 220 meters at its deepest point. Its length has been measured at more than 4,700 kilometers.\nLocated in the southern section of Central South America, Parana River runs through Brazil, Paraguay and Argentina. Its length has been measured at over 4,800 km. One of the most important rivers in the region, Parana River is rivaled only by the Amazon River in terms of sheer length.\nOb River is located in the western Siberian region of Russia. Like the Yenisei, it is one of the three major rivers in Siberia and flows into the Arctic. The river is also noteworthy for having the longest estuary in the world as its gulf.\nThe Yellow River is second only to the Yangtze as the longest river in China. it is also the world’s sixth-longest river with a length that has been estimated at more than 5,464 km. The Yellow River starts out at the Bayan Har Mountain range of Qinghai Province in the western section of the country, and it meanders through nine Chinese provinces before emptying into the Bohai Sea.\nOf all the river systems that flow into the Arctic, the Yenisei River has the distinction of being the largest. One of the three Siberian Rivers that wind into the Arctic Ocean, the Yenisei River starts out in Mongolia and courses throughout much of the central portion of Siberia.\nThe Mississippi is the main river of North America’s largest river system. Its entire length is located in the United States, beginning in the northern section of Minnesota and winding south to the Mississippi River Delta. Its length has been measured at more than 6,275 kilometers.\nThe Yangtze River has the distinction of being Asia’s longest river. It is also the world’s third longest river, with an impressive length of more than 6,300 kilometers. The Yangtze starts out from the Qinghai-Tiber glaciers and empties out into the East China Sea at Shanghai province.\nThe largest river in the world in terms of volume of water discharged is the Amazon. It is also the world’s second longest river at 6,400 kilometers, and its drainage basin spanning a vast expanse of 7,050,000 square kilometers is the largest in the world.\nThe Nile is the longest river in the world, with a length of about 6,650 kilometers. In recent years, the status of the Nile as the world’s longest river has been the subject of contention, due to issues with regard to the true source of the Amazon and therefore, its actual length. At present however, the Nile is still widely considered to be the longest river in the world."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:4590be36-c3f7-43da-86d2-1c6d7fdb65c7>","<urn:uuid:b1a574e8-ffd0-4d0b-b0b4-90e49dfd1a0f>"],"error":null}
{"question":"How does the overload principle work in sports training, and what's the safe percentage for increasing training intensity? 🤔","answer":"The overload principle means that a stimulus has to exceed current capacity to generate new adaptations. For safe training, overloads should only be increased by 5-10% of the current capacity. Increasing beyond that raises injury risk exponentially. For example, someone returning to weight lifting after 15 years shouldn't attempt 4-hour sessions or 500-pound lifts on day one, as this could result in injury or excessive fatigue.","context":["Pablo Toledo is the Coach Development Department Leader. He holds multiple coaching licenses as well a Ms. In Sports Training, specializing in Soccer & Futsal Conditioning.\nI remember taking my first couple of coaching courses in the USA and thinking “How am I supposed to organize a preseason based on this?”. To the date, I still think we do a poor job in teaching physical education to our coaches. Realistically, this is non-existent until you reach your C license, in which you’re only requested to watch a 1 hour webinar on periodization.\nThat means to me that most coaches run years of sessions and conditioning periods not knowing at all what they’re doing, and that’s a problem, especially because many have what I call “The Rocky Balboa Syndrome” and believe that conditioning is about destroying the players for three hours, sending them home devastated and calling them to practice the following day to train even harder, while they see them show up almost limping, and maybe even feeling proud because a couple parents love the idea that “the coach is tough”, until the inevitable happens…\nYou know who the slowest player on a field is?\nThe one that’s injured.\nThis is why, starting today, I’ll be posting a series of conditioning related posts, and I really hope these posts stick, because it will help many players and avoid many injuries.\nLet’s start from the very beginning.\nThe first thing that a coach must understand is that there are 5 principles that any and every conditioning school validates as truth, that are:\n- Adaptability / Overcompensation: This is the capacity of our bodies to adapt, through the right stimulus, to a physical demand. This is why your biceps muscle get bigger and stronger if you do curls every day. Training (curls in this case) is the new / more demanding stimulus that your biceps progressively adapt to in order to perform the action more efficiently.\n- Overload: Overload means that a stimulus has to represent an overload of the current capacity to generate new adaptations. Imagine that you run marathons frequently. Now let’s suppose that you go on a 2 minutes slow run. That’s not an overload, as your body is already capable of doing that very efficiently (this doesn’t mean you shouldn’t do it for other purposes, it only means is not an overload). Now, here comes something extremely important to understand: new overloads have to be increased in 5 – 10% of the current capacity. More than that increases the risk of injury exponentially. Example: imagine that you’re not in good shape and decide to start lifting weights at the gym after 15 years of sedentary lifestyle. Would you start by going to the gym for 4 hours and trying to lift 500 pounds on your first day? No, because you would hurt yourself, or be out for excessive fatigue and pain for a month before returning to the gym. That would be a horrible training plan, right?. Then, why do we try to do that to our players? They come back after the off season, sometimes out of shape, and our best idea is to have them train like harder than ever on the first day like we need to break them. What’s the benefit of that? None.\n- Specificity: The word says it all. If you want to be a soccer player, you have to train soccer specific movements and capacities. Soccer professionals sometimes we do weird things. We see players sent to train cross country in the mountain and a very sophisticated coach proudly preaching that “well it’s hard training, and they increase their VO2 max by it”. Interesting, because I’ve never seen a cross country professional train for the Olympics playing soccer, nor a swimmer do it boxing. If you want to perform as a soccer player, you have to perform overloads of soccer specific movements, as your body will adapt to them specifically as well. Careful, that doesn’t mean this is all you should do. Other disciplines can be useful for cross training, for example, but make sure you understand what the core of your preparation needs to be, and this last means you need to understand in depth what the physical demands of the sport are! (I’ll explain this in further detail on a second post).\n- Individuality: This principle means that you and I and the guy next to us we are different and therefore we have different capacities, and we also might be (most likely) starting from different baselines. Imagine coach says “Let’s run 3 miles in 30 minutes” or “let’s do 4 series of 20 push ups with 1 minute rest periods”, is that appropriate for the player in the best shape? Is it creating the right overload? Maybe that’s too much, or maybe that’s too little. And what about the player that’s in the worst shape? Are you sure that’s not over the 10% overload recommended and you’re not risking a player getting injured?. I know you’re thinking “ok, but how do you want me to calculate the right overload for my 25 players when I’m the only one coaching?” How to do this is one of greatest secrets of conditioning that we’ll learn through this program.\n- Reversible: This means that “if you don’t use it, you lose it!”. Reversible means that if the physical demand is discontinued, your body also readjusts to the lower levels. That’s why, when you stop going to the gym you get skinny again over time.\nLet’s stop here for now. First checkpoint and let’s make sure we understand this well.\nIn the next couple of posts we are going to cover:\n- Soccer Specific Physical Demands & How To Train Them\n- How To Calculate Loads Based On An Individual Optimization Model\n- What’s Periodization And How To Apply it?\n- Injury Prevention Fundamentals, Protocols, and How To Apply them?\n- Warm Up & Stretching Principles & Applications.\n- Recovery Therapies\n- Nutritional Tips\n- How To Design a Pre Season For Soccer!\nThanks for tuning in, coach!"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:6bf9c186-312d-46f9-8abc-14e075fddd7e>"],"error":null}
{"question":"How successful was the early warning system for the 2006-2007 Rift Valley fever outbreak vs recent chikungunya predictions? 🤔","answer":"The early warning system for the 2006-2007 Rift Valley fever outbreak was quite successful, with 64% of the 1,088 reported cases falling within predicted risk areas, and the remaining cases occurring within 30 miles of risk zones. The system provided 2-6 weeks of advance warning, allowing Kenya's government to implement preventive measures. For chikungunya, while the CHIKRisk model has been implemented and is being used by organizations like the U.S. Department of Defense and Pan American Health Organization, the documents do not provide specific success rates for recent predictions, though they mention elevated risk assessments for India, Mexico, Indonesia, Malaysia, and Philippines in 2020.","context":["Satellites Tracking Vector-borne Diseases and Forecasting Their Outbreaks\nColumbia, MD– July 27, 2020. Incorporating satellite observations in disease models has become a valuable component in informing public health policy and decisions. Twenty years ago, predicting a disease outbreak seemed like science fiction. But today, researchers can track when and where certain diseases will emerge and spread—from weeks to months beforehand—with the help of Earth observing satellites.\nAedes aegypti mosquitoes transmit the virus for several tropical diseases, including chikungunya, dengue, zika, and yellow fever. They are recognized by white markings on their legs. Image credit: CDC/James Gathany\n“Scientists can determine where disease-favoring conditions in the environment exist, and that can be done only by a global surveillance system that satellites provide,” said Assaf Anyamba, a Universities Space Research Association scientist working in the GESTAR program at NASA GSFC, and the lead scientist on the Rift Valley fever Monitoring project.\nAnyamba’s team is one of several research groups worldwide that is developing predictive outbreak models for some of the world’s most common but not well understood diseases—many of which do not have a vaccine. Rift Valley fever, cholera, chikungunya, and dengue, are generally found in tropical regions around the world with poor water quality and sanitation, and in places with limited access to health care services.\nWith the access to advanced satellite observations, researchers who study these diseases have acquired better tools to observe the temperature, precipitation, and vegetation conditions (i.e. habitats and environmental conditions) that are linked with these vector-borne diseases. They have incorporated these data into models that assess the likelihood of disease outbreaks as a function of these conditions. By anticipating when and where these conditions might become favorable, researchers can help local governments and international health organizations focus their resources to mitigate and/or manage the outbreaks.\n“Disease risk models involve a mixture of understanding of the biology of the organism that spreads the disease and changes in environmental and habitat conditions, plus the numerical tools to be able to put it all together,” said Anyamba, who works with disease experts, biologists, and health officials across organizations and government agencies.\nNASA satellites provide some of the inputs required by models: data on weather and climate, land-surface vegetation conditions, temperature and precipitation especially for geographical regions that lack such measurements on the ground, to monitor and model the conditions that precede an outbreak. But how do researchers know which environmental conditions play a significant part? To answer that question, researchers study past outbreaks and evaluate their models based on such historical events.\nDr. Anyamba (right) working in the field.\nFor example, Anyamba has studied various chikungunya outbreaks alongside his Rift Valley fever research. Chikungunya, a reemerging viral disease and illness is reported to have first appeared in 1952, but recently spread across the world and arrived in the US in 2015. It causes sudden fever, rash, and joint pain that could last for months sometimes causing victims to hunch over due to the aches.\nSince 2016, Anyamba and his team have been working on a chikungunya risk mapping and forecasting system called CHIKRisk. The model incorporates air temperature and rainfall data from NOAA models; land surface temperatures from NASA’s Moderate Resolution Imaging Spectroradiometer (MODIS) instruments; humidity and soil moisture data from NASA’s Global Land Data Assimilation System; human population density data from NASA’s Socioeconomic Data and Applications Center; and chikungunya vector distributions from the Walter Reed Biosystematics Unit’s VectorMap. The model is correlated with ground-based surveillance systems (such as the Program for Monitoring Emerging Diseases (ProMED) that identify disease activity. The project is supported by the U.S. Defense Threat Reduction Agency and the NOAA Climate Program Office - International Research and Applications Project (IRAP)\nThe CHIKRisk model provides monthly outlooks of where chikungunya risk is highest around the world. That information is used by the U.S. Department of Defense’s Global Emerging Infections Surveillance (GEIS) system for situational awareness and protection of health of soldiers stationed overseas, and by the Pan American Health Organization (PAHO) to help control cases in high-risk areas.\nThe same approach can be used for other vector-borne diseases since the outbreak of these diseases is affected by meteorological conditions, which may evolve to anticipated change in climate conditions. Information sharing is key to prevention, mitigation and management of outbreak of such diseases. In 2020, Anyamba’s team used the CHIKRisk model assess the risk for chikungunya in India, Mexico, Indonesia, Malaysia, and Philippines. They found elevated risk for these countries/regions. Anyamba and his team using the same approach and working with researchers sponsored by the NASA’s Health and Air Quality Applications Program, and local health departments in California, New Jersey, Utah, South Dakota, and Louisiana to assess the risks of West Nile and dengue fever for these States.\nAn increased understanding of the link between meteorological conditions as possible precursors of such diseases, and the combined use of satellite data and mathematical models, give us hope that the prediction skill for such diseases outbreak will continue to improve in the future. Such information will be invaluable for health authorities and citizens around the world to manage the risk of these outbreaks.\nFor more information, please see:\nEarth Observatory - Of Mosquitoes and Models: Tracking Disease by Satellite : https://earthobservatory.nasa.gov/features/disease-vector\nFounded in 1969, under the auspices of the National Academy of Sciences at the request of the U.S. Government, the Universities Space Research Association (USRA) is a nonprofit corporation chartered to advance space-related science, technology and engineering. USRA operates scientific institutes and facilities, and conducts other major research and educational programs, under Federal funding. USRA engages the university community and employs in-house scientific leadership, innovative research and development, and project management expertise.","Responding to a deadly 1997 outbreak of the mosquito-borne disease Rift Valley fever, researchers had developed a “risk map,” pictured above, using NASA and National Oceanic and Atmospheric Administration measurements of sea surface temperatures, precipitation, and vegetation cover. As reported in a recent NASA-led study, the map gave public health officials in East Africa up to six weeks of warning for the 2006-2007 outbreak of the deadly Rift Valley fever in northeast Africa — enough time to lessen human impact.\nOn the map above, pink areas depict increased disease risk, while pale green areas reflect normal risk. Yellow dots represent reported Rift Valley fever cases in high-risk areas, while blue dots represent occurrences in non-risk areas. The researchers have detailed the map’s effectiveness in the Proceedings of the National Academy of Sciences.\nDuring an intense El Niño event in 1997, the largest known outbreak of Rift Valley fever spread across the Horn of Africa. About 90,000 people were infected with the virus, which is carried by mosquitoes and transmitted to humans by mosquito bites or through contact with infected livestock. That outbreak prompted the formation of a working group — funded by the U.S. Department of Defense Global Emerging Infections Surveillance and Response System — to try to predict future outbreaks.\nThe working group didn’t start from scratch. The link between the mosquito life cycle and vegetation growth was first described in a 1987 Science paper by co-authors Kenneth Linthicum of the U.S. Department of Agriculture and Compton Tucker of NASA’s Goddard Space Flight Center. Later, a 1999 Science paper described a link between Rift Valley fever and the El Niño-Southern Oscillation, a cyclical, global phenomenon of sea surface temperature changes that can contribute to extreme climate events around the world.\nBuilding on that research, Assaf Anyamba of NASA Goddard and the University of Maryland, and his colleagues, set out to predict when conditions were ripe for excessive rainfall — and thus an outbreak. They started by examining satellite measurements of sea surface temperatures. One of the first indicators that El Niño will boost rainfall is a rise in the surface temperature of the eastern equatorial Pacific Ocean and the western equatorial Indian Ocean. Perhaps the most telling clue is a measure of the mosquito habitat itself. The researchers used a satellite-derived vegetation data set that measures the landscape’s “greenness.” Greener regions have more than the average amount of vegetation, which means more water and more potential habitat for infected mosquitoes. The resulting risk map for Rift Valley fever, showing areas of anomalous rainfall and vegetation growth over a three-month period, is updated and issued monthly as a means to guide ground-based mosquito and virus surveillance.\nAs early as September 2006, the monthly advisory from Anyamba and colleagues indicated an elevated risk of Rift Valley fever activity in East Africa. By November, Kenya’s government had begun collaborating with non-governmental organizations to implement disease mitigation measures—restricting animal movement, distributing mosquito bed nets, informing the public, and enacting programs to control mosquitoes and vaccinate animals. Between two and six weeks later—depending on the location—the disease was detected in humans.\nAfter the 2006-2007 outbreak, Anyamba and colleagues assessed the effectiveness of the warning maps. They compared locations that had been identified as “at risk” with the locations where Rift Valley fever was reported. Of the 1,088 cases reported in Kenya, Somalia, and Tanzania, 64 percent fell within areas delineated on the risk map. The other 36 percent of cases did not occur within “at risk” areas, but none were more than 30 miles away, leading the researchers believe that they had identified most of the initial infection sites.\nThe potential for mapping the risk of disease outbreaks is not limited to Africa. Previous research has shown that risk maps are possible whenever the abundance of a virus can be linked to extremes in climate conditions. Chikungunya in east Africa and Hantavirus and West Nile virus in the United States, for example, have been linked to conditions of rainfall extremes.\n“We are coming up on almost 30 years of vegetation data from satellites, which provides us with a good basis for predicting,” said Linthicum, co-author on the 1987 paper, upon his return from a Rift Valley fever workshop in Cairo, Egypt last month. “At this meeting, it was clear that using this tool as a basis for predictions has become accepted as the norm.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:dcf299e7-ffdc-4c14-873c-afcc1c1849b6>","<urn:uuid:92aca240-511f-405c-9d7d-ec0c36667825>"],"error":null}
{"question":"What are the key safety requirements for underground transit stations, and how can specialized monitoring equipment help detect potential power supply problems in rail systems?","answer":"Underground transit stations must meet strict safety requirements, particularly around evacuation and ventilation. NFPA 130 mandates that riders must be able to evacuate platforms within 4 minutes and reach safety within 6 minutes. The code also requires mechanical ventilation systems to become operational within 180 seconds and maintain airflow for at least an hour. For power supply monitoring, specialized equipment like CompactRIO systems can be installed to detect issues. These systems can continuously sample voltage at rates over 10,000 Hz, synchronize multiple data streams (including voltage readings, GPS data, and video), and implement real-time calculations to identify problematic events like damaging high-voltage transients. This comprehensive monitoring helps identify whether power problems originate from the external power network or from train subsystems.","context":["Technological advances have improved the ability to monitor, control and manage operational and safety performance of transit systems. However, they have significantly added to the complexity of projects, particularly tunneling projects. For example, a transit line project can have thousands of unique communications points that are transmitted and report in some manner to a remote location, such as a control center. These include train tracking, signaling, emergency communications devices, intrusion alarms, gas monitors, failure monitors on myriad types of equipment, ventilation control and monitoring, fire alarms, and CCTV.\nWhile not expensive as stand-alone elements, their installation, integration, and testing can add significant time to rail projects. Many of these subsystems, like fire alarms, must be connected in order to work. Activation of a fire alarm in a station affects operational functions of elevators, escalators, messaging systems, station ventilation and alarm reporting. Each interface must be tested for each alarm, of which an underground station can dozens.\nFurther, given the long time to complete rail projects, specifications for advanced communication systems are often obsolete by the time they are ready to be installed toward the end of a project. This can result in change orders with schedule impacts if upgrading to a more modern standard. Many of these technological requirements are driven by fire and safety codes that are unique to rail projects, discussed in detail below.\nFire Safety Standards\nRail transit stations, particularly below ground, are also subject to safety regulations. The U.S.-based National Fire Protection Association (NFPA), an independent global trade group, publishes safety and fire codes for a range of facilities, including rail transit systems under NFPA 130 Standard for Fixed Guideway Transit and Passenger Rail Systems.258 NFPA 130 is not federal law, but it has been formally adopted by many jurisdictions and agencies as part of their fire safety codes for rail transit construction. While some countries like Spain, France, Japan, Italy, Germany and Austria have their own fire safety standards for transit, most agencies around the world follow NFPA 130.259\nNFPA 130 largely consists of performance-based criteria for ventilation, fire endurance and spread, and evacuation, but also include specific provisions for materials, distances between exits, spacing of stations and cross-passageways, and doors, among others. For example, one part of the code that has direct implications for the scope of subway stations, and thus costs, is riders standing on a platform must be able to evacuate the station within four minutes and reach a safe location within six minutes.260\nThe code also sets parameters for modeling evacuation scenarios. These evacuation times are based on peak service, with trains one headway behind schedule, resulting in twice the normal passenger load on vehicles and twice as many passengers on a platform.261 Additionally, evacuation scenarios assume that one escalator on each station level is out of service, and that the escalator chosen must be the one that would most negatively impact passenger exit capacity.262 Escalators generally cannot make up more than half of a station’s egress capacity on each level.263 This is intended to ensure that evacuation can be completed even in a worst case scenario.\nOne of the more significant determinants of station platform size are NFPA 130 requirements on the number and width of stairs, as well as the maximum permissible distance from the most remote points of the platform to the nearest exit.264 As a result, station and platform sizes often comfortably exceed the levels that would be necessary to handle normal passenger flow rates. While intended to ensure space for evacuation, meeting these strict standards can lead to a more comfortable passenger experience.265\nOther standards that may impact station costs or elements include provisions for the inclusions of cross-passages to allow for passengers to move between tunnels in case of emergency and, for example, if one tunnel has smoke. According to NFPA 130, if the distance between two stations is greater than 2500 feet, cross passages must be built between the tunnels at 800-foot intervals if there are no intermediate shafts to the surface.266 According to one analysis, cross passages are rare in Europe as well as in Japan.267 This is likely in part due to the relatively close spacing an d travel time between stations that may allow passenger to walk a short distance to evacuate, and reducing the likelihood that a train would get caught in the middle of a tunnel and unable to drive to the next station.268 Constructing cross-passages can require additional excavation and complexity that may affect construction costs.\nVentilation systems that can bring fresh air to underground passengers during a safety incident is also a major element of underground metro systems. NFPA 130 requires mechanical and passive ventilation systems to become fully operational within 180 seconds, and maintain airflow rates for at least one hour to allow for evacuation of vehicles.269 Design of ventilation systems also accommodate the maximum number of trains possible between ventilation shafts during an emergency.270\nTransit systems in earthquake prone areas also must comply with seismic safety guidelines. At and above ground systems are particularly vulnerable to ground movement from earthquakes while underground transit systems largely move with soil in the event of an earthquake and are generally safer.271\nSeismic codes for transit are largely handled at the local or a gency level, though there are certain statewide and federal guidelines that agencies may incorporate into their design standards.272 For example, Seattle’s Sound Transit adopted agency-wide seismic standards that take a hazard-based approach to earthquake resilience. These approaches include planning for an Operating Design Earthquake (ODE) this strength over a facility’s 100 year design life. The other is a Maximum Design Earthquake (MDE), which would be expected to occur once every 2500 ye ars, with a 4 percent chance of an earthquake exceeding this level during a facility’s design life. Sound Transit’s guidelines require light rail facilities to withstand ODE’s and resume operations in a “reasonable amount of time,” and withstand a MDE without collapsing or risking lives.273\nMeeting such standards can vary depending on the seismic profile of varying regions. For example, San Francisco’s Bay Area Rapid Transit (BART) strengthened its standards over the past decades and are undertaking vulnerability analyses and retrofitting key facilities to enhance their earthquake resilience. These measures include enlarging tunnels that cross through faults to account for potential displacement and incorporating concrete-encased steel ribs.274 Aerial structures are reinforced with stronger foundations or columns to withstand collapse or poor soil is replaced with non-liquifiable soil to prevent collapse or damage.275\nTransit stations are also subject to accessibility requirements under the Americans with Disabilities Act of 1990 (ADA). Design specifications for accessibility are outlined under Title II and III of the ADA, also known as ADA Accessibility Guidelines. Enforced by both the federal departments of Justice and Transportation, these guidelines cover vehicles, buildings, transportation facilities, and many other types of facilities. The U.S. Access Board, a federal government agency, writes all code/guidance and has issued supplements to cover different facilities. The ADA guidelines were last updated in 2004 to address usability and format issues, as well as cover new types of facilities. The U.S. DOT formally adopted these new standards in 2006.\nAmong the DOT-specific guidelines for transit include locating accessible routes in the same area as general circulation paths, including detectable warnings on curb ramps and along platforms that do not have screen doors or platform guards, minimum platform heights, and maximum rail platform slopes.276 DOT has added to these standards over time. For example, in September 2011, DOT added a provision mandating that individuals with disabilities, including wheelchair users, “must have access to all accessible cars available to passengers without disabilities in each train using the station”, to prevent segregating disabled riders in separate vehicles.277 These standards apply to all new construction, as well as alterations to existing facilities.\nThe ADA requires that any alterations to existing facilities make them fully ADA compliant, or to the maximum extent feasible in cases where full accessibility is not possible. If making a facility fully accessible would exceed 20 percent of the alteration cost, agencies are only required to incorporate accessibility elements that would not result in a disproportionate cost (under 20 percent).278\nA U.S. DOT 2016 ruling clarified that any alterations to existing transportation facilities that can impact their usability must incorporate accessibility, including for wheelchair users.279 The ruling also clarifies that the ADA requirement to incorporate accessibility to the maximum extent possible is primarily intended for rare cases where it is impossible to make an existing facility fully ADA compliant. In these cases, agencies cannot cite disproportionate cost as a limiting factor preventing incorporation of accessibility. The disproportional cost provision applies only in instances where a primary function area of a station (such as a platform) is being renovated.\nCoverage of the impact of ADA compliance on construction costs has largely revolved around elevator retrofits on older subway systems. The cost of retrofitting elevators has gained particular attention in New York City. Only 23 percent of New York MTA’s subway stations are accessible, and the agency has retrofitted several stations without installing elevators or ramps.280 A 2019 lawsuit ruled that the agency violated the ADA by not installing elevators as part of a 2013 subway station renovation in the Bronx, and must make stations accessible when renovating future stations.281 The agency announced a $5.5 billion capital program in 2019 to install elevators in 70 stations in five years.282 The plan received increased scrutiny for its cost—nearly $78 million per elevator, in contrast to examples from European cities, where station upgrade costs per elevator are as low as $22 million.283 These costs are also lower in other North American cities like Boston, where the MBTA installed three new elevators and two escalators at a Red Line station for $36 million, and Chicago, where a new station with four elevators cost $75 million ($19 million per elevator).284\nAccessibility regulations abroad are largely handled at the country level, but generally all stations built in recent decades are designed to be accessible. Transportation systems in Canada are governed by the newly enacted Accessible Transportation for Persons with Disabilities Regulations (ATPDR), as well as the 2018 Accessible Canada Act, which is the first nationwide accessibility act.285 Provinces also have their own accessibility regulations that apply to public entities, like the Accessibility for Ontarians with Disabilities Act.286 Public transportation in Australia is similarly governed by the national Disability Discrimination Act of 1992, which includes design and service standards for public transport similar to the ADA.287\nThere are no European Union-wide accessibility standards comparable to the ADA, but rather individual member state regulations. The European Accessibility Act, passed by the European Parliament in 2019, largely focuses on fare payment systems and does not explicitly address system design.288 Accessibility on European transit systems can vary significantly. In Barcelona, 143 out of 158 metro stations (81 percent) are accessible, while just under 20 percent of stations on the London Underground are accessible.289 Just three percent of stations on the Paris Metro, for example, are accessible to passengers with disabilities, while the much newer tram system is fully accessible.290 While France passed a law in 2005 to improve accessibility in public spaces, Paris’ Metro was exempt, and its operator has argued that the system’s age would make retrofitting stations extremely costly.\nDesign and architecture can be significant cost drivers for transit projects in three ways: poor management of the design processes, project design itself, and design standards. Lack of oversight of the design process can result in accepting inadequate or faulty designs that result in issues during construction and require change orders. The design of transit projects themselves, particularly on underground stations, can also raise construction costs. Deep, extravagant stations and the use of bespoke materials have been cited as major cost drivers in cities like New York and Toronto. Lastly, select safety standards can require more complex system design to make a project resistant to natural disasters like earthquakes. Stringent evacuation standards in fire safety codes like NFPA 130 can also result in large subway s tations, while the need to install cross-passages and ventilation systems can be an additional source of costs. Accessibility standards, on the other hand, do not appear to be a particularly significant cost driver for new construction, though accessibility retrofits of older station in New York City have received scrutiny for th e high costs of elevator installations compared to other cities.","Siemens Uses CompactRIO, LabVIEW, and DIAdem to Determine the Root Cause of Damaging High-Voltage Transients\n\"Perhaps the greatest benefit of CompactRIO is the FPGA/processor combination. The reconfigurable FPGA hardware achieves scope-like accuracy and rates while the processor manages extended and remote operation.\"\n- Ryan Parkinson,\nDetermining the source of electrical high-voltage transients to prevent light-rail car failure.\nCombining the benefits of the field-programmable gate array (FPGA) and processor in NI CompactRIO hardware to create a rugged, semipermanent monitoring system that records multiple data formats and rates, synchronizes the data, and performs real-time analysis to remotely monitor sensors in an industrial environment for extended durations.\nRyan Parkinson - Siemens\nJacob Cassinat - Siemens\nGoverning subsystem interactions is a fundamental challenge for system integrators. Despite defining exhaustive I/O limits, sometimes failures occur and it is not clear which subsystem interaction generated the destructive element. It is difficult to request subsystem vendor's resources to troubleshoot a problem that did not clearly originate from their equipment, and testing each system in isolation may not account for all interactions. In these cases, the system integrator may be best positioned to monitor the relevant parts of the overall system, isolate the problem source, and assign the appropriate resources to resolve the issue. The Siemens Rail Systems Division recently successfully performed this system integrator's task.\nOver the past three years, one of our customers faced a recurring issue with our SD160 light-rail transit vehicles. Denver RTD, a bus and light-rail service operating in Denver, Colorado, has 170 Siemens vehicles in operation. These vehicles receive power from an overhead catenary system (OCS), which in turn receives power from RTD's power distribution network. The auxiliary power supplies (APS) on board each vehicle receive power from the OCS and condition it for use by most of the other onboard vehicle subsystems. This APS had a high failure rate, which caused a critical failure for the vehicle. The failure log reported a high-voltage transient on the power input to the APS, which led the vendor to believe that either Denver RTD or the onboard propulsion subsystem (which provides power to the APS during electro-dynamic braking) were providing power outside the acceptable transient limit. However, both RTD and the propulsion unit supplier confirmed that their systems should not generate such a transient. Each light-rail vehicle failure was extremely expensive and time-consuming for Siemens and our supplier, and the failures caused operating delays for our customer. We needed to monitor the situation, establish the root cause, and find a solution as quickly as possible.\nPreliminary Diagnostic Efforts\nInitially, engineers at RTD verified OCS voltage levels met specifications. Subsequently, engineers from the APS vendor confirmed voltage transients that could contribute to the equipment failures, although when inducing these transients through various test routines, the APS always performed as designed. This testing required removing the vehicle from passenger service so personnel could monitor portable scopes. This method was inconclusive because high-voltage transients don’t occur very frequently and it is unlikely that a rare, damaging transient would occur during a short test period. It became clear that more comprehensive testing on vehicles in transit was needed to accurately characterize actual operating conditions.\nThe APS vendor built its own remote data logger to permanently install on an SD160 vehicle. It could obtain snapshots of system-level voltage data, but the data was insufficient to understand the surrounding environment and what was causing the transients. These approaches helped us realize that we needed to see the complete picture to diagnose the issue. We decided to build on these earlier efforts and design a rugged, remote system to monitor the trains for long periods of time to find and correct the problem.\nWe needed a highly flexible, yet powerful monitoring system to accommodate the variety of sensors and communication protocols from the different subsystems. We defined the following requirements:\n- Continuous multichannel voltage sampling at >10,000 Hz to monitor six inputs for high-voltage transients\n- At least three different configurable sampling rates to optimize each signal class data rate and minimize storage requirements\n- A serial input using standard protocols to interface with the GPS antenna and provide location information for events\n- Real-time calculations to provide output responses to interact with the sensors\n- Pre- and posttrigger (event) data recording without saving nontrigger data to optimize analysis and minimize storage needs\n- Large storage capacity\n- Video management\n- Automatically synchronize all inputs regardless of data rate or format\n- Automatic downloads for extended operation with minimal personnel interactions\n- Vibration and temperature operating ranges acceptable for installation on a rail vehicle\n- Small footprint for installation in an electrical compartment\nWe decided to instrument two light-rail vehicles with CompactRIO modules, which we were surprised to learn fulfilled all of our system requirements. We used two different vehicles to compare the collected data and monitor how the vehicles interacted. We installed high-voltage transducers and connected them to several train components, focusing on areas that before and after the power input filters. This helped us determine if the transients were being generated from RTD's power network (prefilter), or if a subsystem of the train (postfilter) was generating the transient.\nFigure 1: Semipermanent CompactRIO Installation in Denver Car 321 Propulsion/APS Compartment\nFigure 2: High-Voltage Transducers and Fuse Installation in Denver Car 321\nProgramming With LabVIEW\nWe programmed our system exclusively with NI LabVIEW system design software, using the LabVIEW Real-Time and LabVIEW FPGA modules. We programmed the FPGA to acquire high voltages, currents, and vehicle diagnostics. We programmed the processor to acquire GPS locations and vehicle speeds, to perform daily housekeeping, and to perform postprocessing which allowed us to erase nontrigger data and minimize storage requirements since we were recording about 1 GB of data every 30 minutes. With automated postprocessing, we stored only about 5 GB per day. NI has a great database of prewritten code. Plugging in GPS software modules and general templates for the FPGA and processor software layout saved us a significant amount of time. After attending LabVIEW Core 1 and Core 2 classes in San Diego, we progressed from first-time users to advanced programmers in only a few months. Due to the intuitive nature of LabVIEW and previous programming experience, we completed and tested the software in less than six months.\nBenefits of CompactRIO\nFPGA and Processor\nPerhaps the greatest benefit of CompactRIO is the FPGA/processor combination. Because the FPGA is reconfigurable, the achievable data rates and sampling accuracy are comparable to most state-of-the-art scopes. We can perform real-time calculations and outputs with no processor delays. Once the data is timestamped and buffered, the processor advantages come into play. Software engineers can take advantage of the full breadth of the processor’s flexibility to achieve extended and remote FPGA operation and manage large data sets. The buffered data can be retrieved and written to a USB drive, making its storage capabilities comparable to a laptop. The GPS signal is monitored and recorded. Scripts are run to postprocess, erase nontrigger data, and prepare the data for analysis. Daily tasks are performed and automated FTP uploads to a server can be executed each evening.\nRugged and Reconfigurable\nThe CompactRIO exceeded all our environmental requirements. It handles a temperature range of -40 °C to 80 °C, so we mounted the unit externally in an electrical compartment. Its small footprint and excellent vibration/shock resistance allowed for easy, semipermanent installation.\nCompactRIO is highly customizable. We knew we needed to conduct multiple phases of investigation, and the ability to reconfigure the system to hone in on potential problem areas was a significant benefit. After performing preliminary voltage analysis, we discovered that it would be beneficial to monitor two current signals. Adding these two signals was a very simple task. Using a CompactRIO with swappable input modules, we could monitor almost any conceivable input.\nCompactRIO offers multiple data access options. We could use a router to access the data over a network via wireless downloads or manually access the data via a removable USB drive. To make project development manageable, we did not implement wireless downloads at first. We recorded data to the flash drive and personnel accessed it once a week by shutting off the program, removing the USB drive, and swapping it with a new one. We are currently working with RTD to implement daily, automated FTP transfers to a server, which will save time and make the data more quickly accessible.\nTo effectively measure and record the transients, we needed to implement a sample rate >10,000 Hz. Vehicle diagnostic data is only available every millisecond, so a 1,000 Hz rate is sufficient. A third sampling rate of 50 Hz can be added if temperature readings are necessary. The FPGA easily accommodates these different rates and writes the data to a buffer. A fourth rate of 1 Hz was needed for the GPS input, and was handled in the processor.\nAnother benefit of CompactRIO is the video processing. We originally planned to handle and record the video in the CompactRIO FPGA. However, as the project progressed, we realized the CompactRIO could not handle the buffering we required at the specified resolution. We bought a digital video recorder (DVR), but then ran into difficulty synchronizing the video in the DVR with the CompactRIO data. They had different internal clocks, and due to natural clock drift, we could not rely on them for our required synchronization level. The solution here was simple. We bought an NI 9401 bidirectional digital input module to plug into the CompactRIO chassis. As the system records the voltages, the FPGA runs a simple algorithm to determine if a voltage transient is occurring. If it finds one, it sends out a binary signal to the DVR via the NI 9401. This occurs in real time with no processor delays. The DVR records the binary signal as a trigger with the video file, so we only have to download the video files that show active triggers, which reduces video storage and download times. This also helps us perfectly synchronize the video data with the CompactRIO voltage and GPS data by aligning the triggers.\nAnalyzing Data With DIAdem\nRecording data with various rates and formats is only half the challenge; making sense of the data and effectively analyzing it is the other half. NI DIAdem data analysis software provided the ideal platform for the analysis task.\nFigure 3: DIAdem Analysis Page\nFigure 4: Frames from recorded video files. The frame on the left precedes the frame on the right and depicts an arcing event similar to the three shown on the top chart of Figure 3. For perspective, the arm (pantograph) that is sliding on the overhead catenary wire is 6.5 feet wide.\nDIAdem supports the Technical Data Management Streaming (TDMS) file structure, which provides binary storage advantages. DIAdem uses automatically stored metadata to open, navigate, zoom, and perform computations on extremely large files very quickly. Figure 3 shows high-voltage channels in the top chart (three transient events occurred); elevation in the middle chart; and GPS, vehicle speed, and diagnostic data in the bottom charts.\nDIAdem also supports scripting. Because we ran our monitoring system for more than three months, we generated hundreds of gigabytes of data and it was not feasible to open each file and manually analyze it. After determining the critical data needed to establish the root cause of the transients, we wrote a script that opened each file, looked for critical events, and summarized the findings (returned maximum transient events, durations, APS responses, trended location data, and so on).\nAt the conclusion of our study, we determined that transients exist in our customer's power distribution network and identified their causes, but we established that these transients lie within the required limits for magnitude and duration. The damaging transients were found to be generated in the APS. With this information, our vendor immediately analyzed its system further, believes it has identified the root cause and is correcting the issue.\nCurrently, we are working on a new project in Portland, Oregon where we are monitoring brake disc temperatures and hydraulic fluid line pressures to study disc wear. We can use the same program layout we wrote for the RTD voltage transient project with only simple changes to the sample rate, the input cards, the input scaling, and the trigger to record data. In addition, we are implementing some lessons learned from our Denver testing. After our initial investment for the Denver project, our cost and timeline to reconfigure and redeploy in Portland are significantly lower.\nBookmark and Share\nExplore the NI Developer Community\nDiscover and collaborate on the latest example code and tutorials with a worldwide community of engineers and scientists.\nWho is National Instruments?\nNational Instruments provides a graphical system design platform for test, control, and embedded design applications that is transforming the way engineers and scientists design, prototype, and deploy systems."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:26460705-36ea-4986-9de6-6b55a86b4778>","<urn:uuid:9de125bb-6664-475e-bde3-fa5d324f0057>"],"error":null}
{"question":"How do marketing costs compare between starting a radio taxi business like Ola/Uber vs opening a new restaurant? Can someone explain the difference?!","answer":"For restaurants, marketing costs typically amount to 3-6% of sales and may require professional agency services, especially for fine dining establishments, along with social media marketing and loyalty programs. Radio taxi businesses like Ola/Uber handle most marketing through their mobile applications and GPS systems, with drivers primarily paying a percentage of their ride fare to the company. The restaurant's marketing costs are generally higher as they need continuous promotion to maintain business, while radio taxi services benefit from the established platform's marketing infrastructure.","context":["How to Start a Taxi Business in India\nWith the emergence of companies, such as Ola and Uber, every individual owning a smartphone can avail a cab from the convenience of their home.\nSuch rising demand makes investing in the taxi business a profitable option for many. If you wish to benefit from the same, here is a detailed overview of how to start a taxi business.\nWithout any further delay, let's begin!\nHow to Start a Taxi Business in India?\nBefore starting a taxi business in India, one must have a proper business plan. An appropriate business plan is crucial as it affects an individual's liability, business taxation, paperwork, etc.\nAdditionally, one must also factor in the importance of obtaining a driver's license as per the Motor Vehicle Act's mandates.\nNow, let us look at the basic business structures you should consider when chalking out how to start a cab business.\nHow Many Types of Taxi Businesses Are There in India?\nTypically, there are three types of taxi businesses in India. Refer to the following section to understand the types in detail -\nTraditional Taxi Business\nThis type of business may include single taxis or certain organisations owning numerous cars and running a business. However, this type of business is losing its hold in India due to its poor accessibility.\nThis brings us to the next type of taxi business.\nRadio Taxi Business\nCabs operating under Ola or Uber are radio taxis that have gained popularity due to their easy accessibility. These taxis have a GPS installed in them that helps in tracing driving routes. Further, one can easily book a cab via the respective company’s mobile-based application. Unlike the traditional type, cab drivers own these radio taxis.\nFurther, drivers need to pay a percentage of their scheduled ride fare to the company in case of radio taxis.\nTaxi Fleet Business\nOne can also start a taxi business with a small fleet. Considering the demographics and location, this business can become profitable in a short span.\nFor instance, in tier 1 cities, maximum demand comes from office goers who use taxis for daily commute. Alternatively, for tier 2 or 3 cities, families or students requiring an outstation trip can benefit from this service.\nFurther, an individual owning a fleet can further streamline his business by including administrators, managers, cashiers and drivers.\nTherefore, factoring in the aforementioned points is essential whether an individual wishes to start a taxi business with one car or multiple cars.\nAlong with the above factors and types, one must also be aware of the documents required to start a taxi business.\nWhat Are the Documents Required to Start a Taxi Business in India?\nHere is a list of documents necessary to start a cab business in India:\n- Registration certificate.\n- Vehicle insurance.\n- Environment clearance certificates from the State authorities.\nApart from this, individuals must present the following documents to prove drive authority -\n- Identity proof.\n- Proof of address.\n- Passport size photos.\n- Application fee.\nOther than this, there are some prerequisites to obtain a driver's license. These are as follows -\n- At least 18 years of age.\n- Education up to at least class 8.\n- Must possess a learner's license.\n- Proper training from any Government Motor School.\n- License application, made through online and offline modes.\n- Must take a driving test to prove driving skills.\n- Should have a medical fitness certificate.\nNow, let's look at the types of permits one must obtain when wondering how to set up a taxi business in India.\nHow to Obtain Permit as Per the Provisions of the Motor Vehicle Act?\nDepending on the class of vehicles, there are several permits one needs to obtain to open a taxi business and drive on Indian roads legally.\nHere is a brief overview!\nIt is vital to note that not all vehicles are suitable for a taxi business. Some vehicles are only suitable to transport goods across places. Therefore, to transport people, one must obtain vehicle permits from the Regional Transport Office or the State Transport Office.\nPassenger Carriage Contract\nA driver carries a passenger from one place to another for a particular fare under a contract. With respect to this, one must file an application detailing the type and seating capacity of a taxi and the area where it will be operational as specified by the State Authorities.\nTo know how to start a taxi business, one must have prior knowledge regarding a fitness certificate's importance as per the Motor Vehicles Act. A fitness certificate decides whether a vehicle is in proper driving condition. For this purpose, regular checks and thorough maintenance is necessary.\nFurther, a specific speed limit is set for taxis in each state according to the certificate. One can obtain a fitness certificate from an Authorised Testing Station.\nAll India Tourist Permit\nDrivers must obtain an All India Tourist Permit for inter-state transportation. To obtain this permit, one needs to make an application to the Transport Authority. Note that these taxis cannot be used for local transportation.\nCommercial Vehicle Driving License\nThe Regional Transport Office of respective States grants this upon fulfilling the conditions as mentioned already.\nNow let us focus on the probable ROI of the taxi business in India.\nHow Much ROI Can You Expect?\nThe return on investment (ROI) of a cab business will be higher in the long run as the set-up's initial costs are high. A person learning how to start a taxi business must know that he needs to invest a significant amount in spacing. Therefore, it might be hefty for an individual to bear all those costs.\nHowever, offering the drivers a percentage-based compensation is an ideal way to tackle this situation. Although the profits might not flood in immediately, they will increase in the long run.\nTherefore, by following the aforementioned pointers, one can effortlessly learn how to start a taxi business and enjoy its long-term benefits. A novice and an expert can equally get valuable inputs from this article before starting a cab business.","Opening a restaurant and becoming a restaurant owner is part of many people’s dreams. Turning that dream into a reality can be challenging, but also rewarding. It’s a big decision and with every big decision comes the question of, “How much does it cost to open a restaurant?”\nThe startup cost to open a restaurant is significant. There are many factors to consider which can make it a tough question to answer. However, this article will provide you with the information you need to start budgeting for your restaurant.\nSo, how much will it cost to start a restaurant? Let's find out!\nAverage Cost to Open a Restaurant\nThe average restaurant opening cost can be hard to come by. The answer will vary based on the type of restaurant you want to open, the location, and the size of the business. Other factors include whether you’ll want to offer a dine-in experience or operate a ghost or commissary kitchen and offer takeout and delivery options.\nOnce you narrow down these factors, you’ll get a better idea of the normal expenses of running a restaurant. This can also help you understand how to increase restaurant sales.\nRestaurant costs can vary, but they range from $170,000 to $750,000. However, these numbers shouldn’t discourage you from bringing your dream restaurant to reality. There are several ways to reduce restaurant expenses and plan ahead.\nTypical Restaurant Startup Costs\nNot all restaurants will have the same startup costs. However, you should consider the different areas where you plan on spending your money in order to get clarity on your final expected costs and what to expect to pay.\nProperly planning out your restaurant business can help you avoid losing thousands in additional labor cost and potential customers. Poor restaurant equipment choices and construction can end up costing you twice as much as time goes on. Below are some essential startup costs and expenses for opening a restaurant.\nBreakdown of Restaurant Startup Costs\nThe following restaurant opening costs can help you budget for your potential new business project:\nEven though your restaurant won’t be fully operational during the building stage, you’ll still have utility expenses. These include water, gas, and electricity.\nIt’s common for these expenses to add up quickly. Average costs can range from $1,000 to $1,200 per month depending on the size of the property.\nPicking a location for your restaurant is one of the most crucial decisions. It’s beneficial to pick a location that has good foot traffic, but these spots can be pricey. When choosing the location, you’ll have to consider the following options:\n- Investing in a new construction property\n- Converting an existing commercial property or space per sqaure-foot into a restaurant\n- Purchasing an existing and/or operating restaurant\nEach of the options above has pros and cons. Choosing the right option will require thorough research.\nFinishing Touches and Equipment\nIt’s true that the little expenses start to add up. These include interior finishing touches and your kitchen equipment. From kitchen tools to restaurant inventory software, it’s all something to consider. The back of house equipment like the lighting, signage, phone systems, and music systems are also crucial.\nFinishing touches like decor and furniture are what help bring life to your restaurant and make the space appealing to guests. Don’t forget that everything needs to be ready in terms of restaurant inventory such as food, plates, cups, utensils, and cookware. All in all, these costs can range from $20,000 to $400,000.\nMarketing is a restaurant opening cost that can be minimal or expensive, depending on your needs. Franchise locations might have the advertising part taken care of. However, new restaurants might need to be advertised more, especially in a saturated market.\nThe costs for this can vary based on the channel or niche market you’re targeting. A new brick and mortar fine dining restaurant might benefit from professional marketing from an agency. Smaller establishments might only require some social media marketing efforts to achieve O2O success.\nMarketing and PR costs can depend on your competition and restaurant concept. In most cases, similar costs will equal about 3-6% of your sales. Continuous marketing is necessary in order to keep business moving. You may also want to consider incorporating a loyalty program.\nDevelopment and Organizational Costs\nObtaining the necessary licenses and permits is one aspect of opening a restaurant that people dread. However, it’s essential.\nYou’ll have to file and pay your restaurant licenses and permits as well as pay for all utilities. The average licensing cost of this can range from $2,500 to $200,000.\nRestaurant technology will help your business run smoothly. You’ll need to have a point of sale system, like a bar POS system for your restaurant, and you’ll likely want to incorporate restaurant tech. These include restaurant QR code menus, ERP accounting system (see ERP meaning), and restaurant inventory management software. It's also crucial for restaurants to understand the benefits of ERP in this case.\nNo restaurant is complete without food. A successful restaurant opening will call for fully stocked pantries, refrigerators, and freezers. If you run out of food items, especially at the beginning of your business venture, it can give guests the wrong impression.\nKeeping your inventory stocked can guarantee that you have all you need to provide seamless customer service. Some restaurants consider having safety stock on hand. Your average food cost per month will likely vary based on the menu offerings and the type of cuisine you serve.\nCommon Restaurant Startup Mistakes\nBeing aware of potential mistakes or problems before they arise can help you better plan and prepare to avoid them. Even some of the best business owners and best coffee roasters can make mistakes. Below are three common mistakes that can occur when opening a restaurant:\nNot Having a Solid Business Plan\nA solid business plan is essential for a restaurant startup. Anyone can come up with a good idea, but you should make sure that your idea and plan are feasible. A business plan can help you outline your business and understand how it will operate. It's also easy to seek out investors or eCommerce banks for small business loans with a business plan. This will also help you avoid sticky situations in the future.\nNot Focusing On the Location\nLocation is a crucial factor when opening a hospitality business. You should always choose the location that is first on your list, given that it’s within your budget to do so.\nBe sure to consider your target market, where they tend to spend time, and a location with decent foot traffic. If you find a location that meets all three characteristics, strongly consider it.\nNo Financial Planning\nIf you don’t plan out your finances, you might underestimate the amount of money that is required. This can result in a lot of problems down the road. Unexpected expenses can come up, and it’s good to be prepared for this by having money set aside.\nFrequently Asked Questions About the Cost of Opening a Restaurant\nWhat Is the Average Cost of Opening a Restaurant?\nThe average cost of opening a restaurant can vary depending on numerous factors, but it tends to range from $170,000 to $750,000. These factors can include the type of restaurant you open, the location, and the size of the restaurant.\nWhat Costs Should Be Considered Before Opening a Restaurant?\nThe costs that you should consider before opening a restaurant include:\n- Finishing touches and equipment\n- Development and organization\n- Food costs\nWhat Are Common Restaurant Startup Mistakes?\nThe common restaurant startup mistakes that people run into include:\n- Not having a solid business plan\n- Not focusing on the location of the restaurant\n- No financial planning"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:cf151064-cb40-4f3b-8ec1-4ca0272b468d>","<urn:uuid:3658a8c6-8e02-4dff-899a-d50af0df1a8d>"],"error":null}
{"question":"How do carbs and fats compare in terms of their caloric content per gram? What's the deal with their role in weight loss?","answer":"Carbohydrates provide 4 calories per gram, while fats provide 9 calories per gram, making fats more than twice as calorie-dense. However, their roles in weight loss are complex. Carbohydrates should contribute about 50-55% of energy requirements and are important for sustained energy throughout the day, especially high GI (Glycaemic Index) carbs. Fats, while more caloric, should make up 20-35% of daily calories and are essential for storing vitamins A, D, E, and K. Despite their higher caloric content, fats can actually help with weight loss by making you feel more satiated, potentially leading to reduced overall food intake.","context":["Calories in: having lunch in the garden\nLosing weight is, in theory, a very simple process:\n- If the calories burned off from exercise exceed the calories consumed from food , you will lose weight.\n- If the calories consumed from food exceed the calories burned off from exercise, you will gain weight.\nSo all you have to do is exercise more and eat less.\nSounds easy, doesn't it? So why do so many people have problems losing weight? It's because it means breaking the habits of a lifetime.\nGet Slim Quick: The Failure of Crash Diets\nOne of our worst habits is to fill our lives with all sorts of activities, some of which are trivial, and then when something happens that really matters, we go into crisis management and respond to it in the expectation that we have to get quick results. Such is the case of many people, especially women, who meet the partner of their dreams and decide they are more likely to be successful if they lose some weight, or they have made their catch and must lose some weight in the next few weeks to get into a reasonable sized wedding dress. So they go on a crash diet with the following consequences:\n- The body uses up its reserves of fat as an energy source, but the metabolism slows down to make it last as long as possible. This is because we are designed to store fat when food is plentiful and use it up gradually when food is scarce. When you stop eating, the body thinks that food is scarce and goes into conservation mode. You feel weak and lethargic, and eventually become demotivated and start eating again, putting on the small amount of weight that you have lost.\n- If you try to do some exercise while dieting, you will lose weight more quickly but the weight loss will consist of both fat and muscle, because you are not getting the nutrients required to maintain and build muscle mass. You will end up weak and feeble and will eventually become demotivated and start putting on weight.\nRealism: Plan For The Next Year Ahead\nTo successfully achieve weight loss, you have go for moderate short-term results, followed by something more significant in about six months or a year.\nIf you live a sedentary lifestlye, sitting at a desk all day and doing no exercise, and then you start to do some exercise (for example going to a gym), you will start to lose some weight and will notice a difference in just a few weeks. This is because the energy balance (calories in minus calories out) has changed, but you won't continue losing weight indefinitely because the energy balance stablises at the new level. To lose more weight you have to look at your diet and try to achieve an energy deficit of about 500Kcal/day, which should cause you to continue losing weight at about 1 pound per week. However, it's a bit more complicated because losing weight is not the same as losing fat. When you do intensive or weight-bearing exercise, you can expect to build muscle, which is heavier than fat, so instead of losing weight you might gain some weight. So you can't measure your progress just by stepping on the scales, and in addition you have to take circumference or skinfold measurements to obtain your body composition. Normally you should aim for an increase in muscle mass and a loss of fat, and if the fat loss exceeds the muscle gain, there will be an overall loss of weight. In the end, you might decide that you have achieved your objectives when you look in the mirror and you like what you see, or you can get into some different type of clothes.\nBalance of Nutrients\nWhatever your objectives might be, you have to make sure the balance of nutrients is right, as follows:\n- Carbohydrate should be your main source of energy, contributing about 50-55% of energy requirements, and is found in abundance in starchy foods such as potatoes, rice, pasta and bread. Carbohydrates are complex sugars that are gradually broken down into simple sugars, available for conversion to energy. They are classified according to their Glycaemic Index (GI) which is a measure of their complexity. The most complex carbohydrates have a high GI and the simplest sugars have a low GI. Carbohydrates with high GI are preferred for normal daily consumption because they break down gradually and keep us going throughout the day. Carbohydrates with low GI (sweets, chocolates, cakes etc.) tend to raise the blood sugar too quickly and contribute to fat storage, but they are preferred during or after prolonged exercise when the blood sugar is low and you need a boost. The calorific value of carbohydrate is 4Kcal/gm.\n- Protein is essential for building and repairing muscles and other body tissues, but it is also an energy source and our total consumption of protein should be equivalent to about 10-15% of our energy requirements. Protein is available in meat, fish, eggs, dairy products, nuts, pulses and beans. It consists of long chains of amino acids, arranged according to the DNA of the plant or animal from which it came. When digested, it gets broken down into component amino acids and re-arranged according to our own DNA. Aren't we clever? We are digitally programmed to disassemble and re-assemble proteins according to our requirements. When we do intensive or load-bearing exercise, we cause small micro-tears in our muscles that get filled in with re-assembled protein from our diet, and that's how we build muscle, and that's why it's important to have some protein in our diet, especially if we are doing exercise. However, it's important not to have too much protein, because the excess is burnt off as energy and causes the blood to become acidic, dissolving calcium from our bones. The calorific value of protein is 4Kcal/gm, same as carbohydrate.\n- Fat is essential for storing certain vitamins, namely A, D, E and K. It is present in a wide variety of foods, but especially fatty cuts of meat, full cream milk, butter, margarine, animal and vegetable oils, cakes and biscuits. Normally it contributes about 30-35% of our energy requirement, but if you are trying to lose weight you should try and reduce it to about 20%. Fat is made of triglycerides, consisting of three fatty acids combined with glycerol, an alcohol that has three hydroxyl groups. Saturated fats are those made up of fatty acids which have no double bonds in their carbon chains. They are solid at room temperature (for example lard) and they clog up the arteries and increase the risk of heart attacks and strokes. Unsaturated fats are those made up of fatty acids which have double bonds. They are liquid at room temperature (for example vegetable oil) and they keep the arteries clear. So it's good to have moderate amounts of unsaturated fat, while trying as much as possible to avoid saturated fats. The calorific value of fat is 9Kcal/gm, more than twice as much as carbohydrate and protein, so it's important not to consume too much. Also we have to take into account that fat goes easily into storage if it is not burnt off as energy.\n- Alcohol has no nutritional value, but it has a calorific value of 7Kcal/gm. If you are feeling hungry and you drink some alcohol, you will still be hungry and you will need to eat something, so you get your calories twice over. That's why people who drink too much tend to get fat, and in men who get fat around the abdomen, it's known as the \"beer gut\". Alcohol also has the property of absorbing water so it causes dehydration. If you are already dehydrated from exercise and you drink alcohol, it will make the problem worse. It's a common practice for sports people to meet at the pub after a competition, but they always like to rehydrate with water and fruit juices first. By all means have some alcohol if you enjoy it, and it can help you to relax, but keep it in moderate quantities.\n- Vitamins and minerals are known as \"micronutrients\" and serve a wide range of purposes. There are recommended levels of each of them, but it's quite complicated, trying to work out how to include them in your diet. Instead, the Food Standards Agency has categorised food types according to the Eatwell Plate Model, and if you stick to this you will get all the vitamins and minerals that you need.\nPersonal Training Programme\nWhen you sign up for a personal training programme, for which nutrition is an essential component, I will ask you to make up a diet diary and an activity diary so I can see what you are eating and how you are burning it off as exercise. Then I will work out the energy balance and recommend changes that will result in a manageable energy deficit. At the same time I will try to identify any issues with the balance of nutrients and deal with them in order of priority, the most significant ones first. It takes time to change your lifestyle and eating habits, so we won't try to change everything at once.\nIdeally, we should eat when we feel hungry and then stop, but if we like the food we tend to continue eating until we can't eat any more. We also eat for social and cultural reasons, for example people having birthdays and handing out cakes and biscuits. We tend to habitually indulge ourselves in festivities, against our better judgement, and it takes time to find out how to keep it in moderation and still enjoy ourselves. The social and cultural issues, and the lifestyle and habits are probably more complex than anything I have said about the science of physiology and nutrition, but if you are really serious about changing your life, you can sign up for the personal training programme and I'm sure I will be able to help you.\nCalories out: London Marathon 2010,\ncrossing Tower Bridge","Are you tracking your macros? Or on a diet? Or just curious how much fat you’re eating in a day? You’ll need to know how many calories are in a gram of fat! This quick guide breaks down everything you know about calculating your fat intake.\nToday I’m putting on my nutritionist hat to talk all things fat! There is a growing trend to measure macros (short for macronutrients) instead of calories. Macros simply refers to the grams of protein, carbohydrates, and fat one consumes.\nThe advantage to this technique is that it is not only an effective means of tracking energy consumption but also a way of ensuring you balance your daily intake of each of these three macros (protein, carbs, and fats) to ensure a proper balance.\nmany calories are in a gram?\nSo how many calories are in a gram of each macronutrient?\n- 1 gram of fat provides 9 calories\n- 1 gram of protein provides 4 calories\n- 1 gram of carbohydrates provides 4 calories\nIs Fat healthy?\nLet’s start by opening up a whole can of worms (which are surprisingly low-fat by the way). The conventional wisdom on whether or not fat is healthy has changed over the years and has been sadly affected by the agendas of various food industry groups. Sound advice, however, is provided by Harvard Health’s recommendations:\n- Avoid trans fat\n- Limit saturated fats\n- Focus on eating healthy polyunsaturated and monounsaturated fats\nFat is a necessary part of a healthy diet, and has many functions in your body. Fat is needed to build cell membranes, the vital exterior of each cell, and the sheaths surrounding nerves. It is essential for blood clotting, muscle movement, and preventing inflammation. Fats are also used as an energy source, protect your organs, support cell growth, keep blood pressure under control, and help your body absorb vital nutrients such as fat-soluble vitamins A, D, E, and K. That is some serious heavy lifting for a macronutrient that had a bad rap for so long!\nWill Eating Fat make me Fat?\nNo, eating fat will not directly translate to fat on your body. Fat grams do have more calories than either protein or carb grams (9 calories per gram of fat vs. 4 calories per gram of protein or carbs), but there is a lot of evidence that consuming fat makes you feel more satiated (fat satisfies your hunger) and can thus trigger you to eat less.\nWhat are the bad fats?\nThere are two fats to look out for and minimize in your diet as much as possible: trans fats and saturated fats.\nThe worst type of dietary fat is the kind known as trans fat. It is a byproduct of a process called hydrogenation that is used to turn healthy liquid fats into solids and to prevent them from becoming rancid. Basically, it was long used in making many processed foods. Trans fats have no known health benefits and there is no safe level of consumption.\nUnlike trans fats, saturated fats are naturally occurring. A saturated fat molecule is “saturated” with many hydrogen atoms. Okay, that wasn’t really very helpful, so we’ll get away from the technical stuff. Saturated fat is primarily found in animal products like beef, pork, butter, whole milk, cream, and cheese. High amounts of saturated fat are often found in processed and “fast” foods like pizza, hamburgers, cookies, chips, etc. A diet heavy in saturated fats can drive up your LDL cholesterol and prompt blockages to form in arteries. For this reason many nutrition experts recommend limiting saturated fat to under 10% of daily calories.\nHow much fat do you need?\nThe Cleveland Clinic recommends fat intake of 20-35% of daily calories.\n- If you need 1500 calories then eat 33-58 grams of fat per day\n- If you need 2000 calories then eat 44-78 grams of fat per day\n- If you need 2500 calories then eat 56-97 grams of fat per day\nWhat do “Fat Free” and “Low Fat” really mean?\nDid you know that labels on food packages that have claims about fat content are actually regulated? Here’s what they mean!\n- Fat-Free = less than 0.5 gram of fat per serving\n- Low-Fat = 3 grams or less of fat per serving\n- Reduced-Fat = 25% less fat than regular versions\n- Low Saturated Fat = less than 1 gram of fat per serving and have no more than 15% of total calories derived from saturated fat.\nDeceptive marketing is still possible with fat content. Take, for example, beef hamburger sold as 85% fat-free. With 15.5 grams of fat per serving fat makes up 62% of the calories in a 226 calorie serving. That certainly doesn’t sound like 85% fat-free. The 85% fat-free claim simply means that fat constitutes only 15% of the serving by weight. Since water makes up much of the weight in many foods this type of marketing can be deceptive in understanding total fat as a proportion of macronutrients."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:c1add32f-22d8-431b-bbb2-c766148e023e>","<urn:uuid:2d5c135c-d7bc-48df-9cb3-e075fbc2914c>"],"error":null}
{"question":"What's the difference between accessibility documentation required for digital materials vs physical facilities in federal programs?","answer":"For digital materials, documentation requires an up-to-date Accessibility Conformance Report (ACR) based on the VPAT®, which must detail how each accessibility criterion is supported and tested, including manual testing verification. For physical facilities, compliance is documented through adherence to specific technical standards outlined in various CFR parts (such as 28 CFR Part 36 and 43 CFR Part 17) and design manuals. Both require ongoing compliance assurance, but digital materials specifically need annual updates or documentation updates with significant product changes, while physical facility requirements focus on initial design and construction standards.","context":["- The Rehabilitation Act - The Rehabilitation Act of 1973 ensures access to programs and activities that are federally funded. It also protects the rights of Federal employees with disabilities. Federal agencies are responsible for enforcing requirements as they apply to their own programs, services, and employment practices. The law also requires electronic and information technology procured by Federal agencies to be accessible according to standards issued by the Access Board.\nCode of Federal Regulations (CFR)\n- 28 CFR Part 36 - Nondiscrimination on the Basis of Disability by Public Accommodations and in Commercial Facilities\n- 43 CFR Part 17 - Nondiscrimination in Federally Assisted Programs of the Department of the Interior - Subpart B - Nondiscrimination on the Basis of Handicap\n- Electronic and Information Technology Accessibility Standards (Section 508) (2000)\n- Section 504 - Subpart E - Enforcement of Nondiscrimination on the Basis of Handicap in Programs or Activities Conducted by the Department of Interior (pdf)\n- Telecommunications Act Accessibility Guidelines (1998)\n- Section 504 - Subpart E - Enforcement of Nondiscrimination on the Basis of Handicap in Programs or Activities Conducted by Dept of Interior (pdf)\n- Fair Housing Act Design Manual - A Manual to Assist Designers and Builders in Meeting the Accessibility Requirements of The Fair Housing Act\nManagement Policies 2006 - The Guide to Managing the National Park System\n- Chapter 1 The Foundation\n- Chapter 5 Cultural Resource Management\n- Chapter 6 Wilderness Preservation and Management\n- Chapter 7 Interpretation and Education\n- Chapter 8 Use of the Parks\n- Chapter 9 Park Facilities\n- 9.1.1 Facility Planning and Design\n- 9.1.2 Accessibility for Persons with Disabilities\n- 9.2 Transportation Systems and Alternative Transportation\n- 9.2.2 Trails and Walks\n- 188.8.131.52 Equestrian Trails\n- 9.3 Visitor Facilities\n- 9.3.1 Informational and Interpretive Facilities\n- 184.108.40.206 Visitor Centers\n- 9.3.2 Overnight Accommodations and Food Services\n- 9.3.3 Comfort Stations\n- 9.4 Management Facilities\n- Chapter 10 Commercial Visitor Services\nDirector's Orders (DOs)\n- DO 2: Park Planning, Chapter 2 of Management Policies, Park System Planning, and Planner's Source Book\n- DO 16A: Reasonable Accommodation for Applicants and Employees with Disabilities\n- DO 42: Accessibility for Park Visitors\nThe 7 Principles of Universal Design\n- Equitable Use - The design is useful and marketable to people with diverse abilities.\n- Provide the same means of use for all users: identical whenever possible; equivalent when not.\n- Avoid segregating or stigmatizing any users.\n- Provisions for privacy, security, and safety should be equally available to all users.\n- Make the design appealing to all users.\n- Flexibility in Use - The design accommodates a wide range of individual preferences and abilities.\n- Provide choice in methods of use.\n- Accommodate right- or left-handed access and use.\n- Facilitate the user's accuracy and precision.\n- Provide adaptability to the user's pace.\n- Simple and Intuitive Use - Use of the design is easy to understand, regardless of the user's experience, knowledge, language skills, or current concentration level.\n- Eliminate unnecessary complexity.\n- Be consistent with user expectations and intuition.\n- Accommodate a wide range of literacy and language skills.\n- Arrange information consistent with its importance.\n- Provide effective prompting and feedback during and after task completion.\n- Perceptible Information - The design communicates necessary information effectively to the user, regardless of ambient conditions or the user's sensory abilities.\n- Use different modes (pictorial, verbal, tactile) for redundant presentation of essential information.\n- Provide adequate contrast between essential information and its surroundings.\n- Maximize \"legibility\" of essential information\n- Differentiate elements in ways that can be described (i.e., make it easy to give instructions or directions).\n- Provide compatibility with a variety of techniques or devices used by people with sensory limitations.\n- Tolerance for Error - The design minimizes hazards and the adverse consequences of accidental or unintended actions.\n- Arrange elements to minimize hazards and errors: most used elements, most accessible; hazardous elements eliminated, isolated, or shielded.\n- Provide warnings of hazards and errors.\n- Provide fail safe features.\n- Discourage unconscious action in tasks that require vigilance.\n- Low Physical Effort - The design can be used efficiently and comfortably and with a minimum of fatigue.\n- Allow user to maintain a neutral body position.\n- Use reasonable operating forces.\n- Minimize repetitive actions.\n- Minimize sustained physical effort.\n- Size and Space for Approach and Use - Appropriate size and space is provided for approach, reach, manipulation, and use regardless of user's body size, posture or mobility.\n- Provide a clear line of sight to important elements for any seated or standing user.\n- Make reach to all components comfortable for any seated or standing user.\n- Accommodate variations in hand and grip size.\n- Provide adequate space for the use of assistive devices or personal assistance.\nSpecific Universal Design requirements for all projects:\n- Pedestrian Routes - Project is designed so that all users follow the same routes through the facility and site.\n- Entrances to Newly Constructed Facilities - Project is designed so the primary entrances (visitor and employee) of newly constructed facilities do not have steps. Staired secondary routes are included on sloped sites.\n- Entrances to Facilities\n- Project is designed to provide a covered entry and roof drains away from entry walk and entrance.\n- Distance from drop-off and closest accessible parking space (car and RV/Bus) to accessible entrance of all accessible buildings is 200' or less.\n- Doors - Visitor use buildings provide power assist door openers on main accessible entrances. Doors on accessible routes use lever or push hardware.\n- Accessible Route Design Standards\n- NPS Universal Design and Accessibility Scoping Form for ABAAS Facilities\n- NPS Universal Design and Accessibility Scoping Form for ABAAS Outdoor Recreation Facilities\nHarper's Ferry Center (HFC) Programmatic Accessibility Requirements\nThese guidelines are required to be followed for interpretive media and exhibits included in DSC projects.\n- Programmatic Accessibility Guidelines for National Park Service Interpretive Media (pdf)\n- Media Accessibility Information\nCenter for Universal Design Technical Guidance\n- Section Through Pipe Protection Panel (34\" Counter Height) (Tech Sheet 1.04.1) (pdf)\n- Section Through Pipe Protection Panel (32\" Counter Height) (Tech Sheet 1.04.2) (pdf)\n- Threshold: Recessed Into Subfloor (Tech Sheet 2.09.1) (pdf)\n- Roll-In Shower: Wood Frame Construction (Tech Sheet 1.03.2) (pdf)\n- Roll-In Shower: Slab on Grade Construction (Tech Sheet 1.03.3) (pdf)\n- Curbless Showers: An Installation Guide (pdf)\n- Wood Ramp Design: How to Add a Ramp (pdf)\n- QAP Tech Sheet: Bathrooms for Multifamily Housing (pdf)\n- Affordable and Universal Homes: A Plan Book (pdf)\n- Workspace layout\n- Proper Grips\n- Principles Of Universal Design Poster (pdf)\n- Principles of Universal Design Quick-Reference\nUniversal Design Research","Communicating Digital Accessibility Requirements\nWhen procuring digital materials and technologies from a publisher or vendor, it's essential that you and your team communicate accessibility requirements to ensure federal statutes and regulations are met. Your team may also choose to provide accessibility guidance to supplement the accessibility requirement language. This guidance includes a rationale for the requirements, best practices for manual accessibility testing of products, and information on how the vendor can optimize and communicate their product's accessibility in order to assist you in making purchasing decisions.\nPlease use and adapt this sample language in RFPs, Instructional Materials Adoption, and in contracts. If there is a requirement to also submit files to the National Instructional Materials Access Center (NIMAC), see NIMAS in Purchase Orders and Contracts.\nSample Language for Accessibility Requirements\n[Agency name] requires digital materials and technologies to be accessible to students, employees, and community members with disabilities. Digital materials and technologies should conform to the standards for accessibility set forth in Section 508 of the Rehabilitation Act of 1973, as amended (29 U.S.C. § 794d), and its implementing regulations (36 C.F.R. § 1194). The Revised Section 508 incorporates the Web Content Accessibility Guidelines (WCAG) by reference. Web and non-web content (including websites and documents) is required to conform to the most current version of WCAG at level AA in order to meet Section 508 requirements. Beyond Section 508, additional specifications are defined according to the type of material or the delivery format. Please refer to the attached guidance for additional information on those specifications.\nConformance to the specified standards can be documented through the submission of an up-to-date, complete, and accurate Accessibility Conformance Report (ACR). The ACR should be based on the latest version of the Voluntary Product Accessibility Template (VPAT®), which can be obtained from the Information Technology Industry Council (ITI) website.\nThe ACR’s Remarks and Explanations for each criterion should include an explanation of not only how the criterion is supported, but also how that support was validated and tested. For any criterion that is not fully supported, an explanation of the barriers created by the criterion not being supported should also be included. If the criterion does not apply, an explanation should be provided.\nIn addition to a current and accurate ACR, priority will be given to product submissions whose conformance is documented through the completion of an independent, third-party audit that does not rely solely on an automated scan but also includes manual testing. If the ACR is completed by a third party and includes verification of manual testing, that ACR can serve as the independent audit.\nUpdated documentation, including an assurance of continued compliance, should be provided on an annual basis, or whenever a significant product update takes place. Any questions about documentation requirements should be directed to [accessibility team] at [email address and telephone number].\nReferenced Standards and Specifications\nSpecifications by Material Type or Delivery Format\nThese additional specifications are defined according to the type of material or the delivery format. They should be included with the sample language to provide helpful guidance to vendors based on their products.\nWeb-based materials should conform to the most current applicable versions of the following standards:\n- The Web Content Accessibility Guidelines (WCAG) at level AA\n- The Web Accessibility Initiative (WAI) Accessible Rich Internet Applications (ARIA) specification\n- The MathML specification for digital mathematical notation\nPublications and Documents\nPortable Document Format (PDF) documents should be tagged and conform to PDF/UA (PDF/Universal Accessibility).\nEPUB publications should conform to the most current version of the EPUB specification. They should also conform to the EPUB Accessibility specification. A third-party certification can help vendors confirm that their publications meet these specifications. This certification should be noted in the publication’s metadata along with other required metadata needed to determine how the publication meets specific learner needs. EPUB publications should also conform to the most current ARIA specification and use MathML for digital mathematical notation.\nSoftware and Apps\nFor optimal interoperability, it is recommended that software and mobile applications (apps) conform to the latest version of the User Agent Accessibility Guidelines (UAAG). If the software or app can be used to author content, the Authoring Tool Accessibility Guidelines (ATAG) may also apply.\nThis section includes information to help publishers and vendors better understand the requirements in the previous section and communicate how they are meeting those requirements in their documentation submitted to the purchasing agency.\nRationale for Accessibility Requirements\nThe Office for Civil Rights at the U.S. Department of Education defines “accessible” to mean that a person with a disability is afforded the opportunity to acquire the same information, engage in the same interactions, and enjoy the same services in an equally effective, equally integrated manner, and with substantially equivalent ease of use as a person without a disability 1.\nAccessibility applies to both materials (the content or information to be learned) and technology (the hardware or software that delivers material). Accessible materials are designed or enhanced in a way that makes them usable by the widest possible range of learner variability, regardless of format (print, digital, graphical, audio, video) 2. Accessible technologies are usable by people with a wide range of abilities and disabilities and are directly usable without assistive technology (AT) or usable with it 3. Individuals with disabilities use a range of AT for perceiving and physically interacting with technologies.\nThe use of accessible educational materials and accessible technologies strengthens opportunities for learners to experience independence, participation, and progress. When learners have difficulty using educational materials and technologies due to a lack of accessibility, they are at risk of falling behind their peers. Timely access to accessible materials and technologies for learners with disabilities results in the same opportunities to fully and independently participate and make progress in the curriculum.\nBest Practices for Manual Testing\nA robust process for determining the accessibility of digital materials and technologies should include the following manual testing:\n- Manual checks of a representative sample of pages to determine that alternative text on images and graphs are appropriate for the instructional context in which the materials will be used.\n- Manual checks of a representative sample of pages with tables, forms, dynamic content and other applications that are known to present accessibility challenges.\n- Manual checks of any video content to ensure the inclusion of high-quality closed captions.\n- Testing to determine whether page content and controls can be accessed, operated, and reset when necessary using only a keyboard.\n- Testing with screen-reader software.\n- Documentation of the experience of users with disabilities, including basic information about the assistive technology used.\nOptimizing & Communicating Product Accessibility\nThere are certain actions vendors can take to optimize and communicate product accessibility. They include:\n- Complete the free training, Section 508: What Is It and Why Is It Important to You?, available at dhs.gov.\n- Review and understand the latest version of the Web Content Accessibility Guidelines (WCAG), available from the World Wide Web Consortium (W3C).\n- Ensure staff tasked with completing the Accessibility Conformance Report are not only familiar with the product’s key features, but are also trained in accessibility best practices.\n- Identify product accessibility requirements at the beginning stage of design and integrate those requirements throughout the development cycle.\n- Include iterations of accessibility testing throughout the development workflow in order to identify barriers as early as possible.\n- Enlist individuals who rely on assistive technologies in their daily lives to participate in product testing and provide authentic feedback about accessibility support.\n- Prepare a product accessibility statement that clearly states the standards addressed and the level of conformance for each, along with information about supported accessibility features. This product accessibility statement should also discuss where and how accessibility is addressed in the product development process.\n- Provide a single point of contact for addressing accessibility questions in the accessibility statement, and make sure those experiencing accessibility challenges have a variety of ways to contact the product’s accessibility team.\n- Obtain certification under the Department of Homeland Security Trusted Tester Process and Certification Program and include notice of that certification in the product accessibility statement.\n1Joint “Dear Colleague” Letter: Electronic Book Readers (June 29, 2010)\n2As used in the 84.327Z priority, ‘‘accessible educational materials’’ means print- and technology-based educational materials, including printed and electronic textbooks and related core materials that are required by SEAs and LEAs for use by all students, produced or rendered in accessible media, written and published primarily for use in early learning programs, elementary, or secondary schools to support teaching and learning.\n3As used in the 84.327Z priority, ‘‘technology’’ means any equipment or interconnected system or subsystem for which the principal function is the creation, conversion, duplication, movement, control, display, switching, interchange, transmission, reception, or broadcast of data or information. It includes, but is not limited to, electronic content; telecommunication products; computers and ancillary equipment; software; information kiosks; transaction machines; videos; information technology services; and multifunction office machines that copy, scan, and fax documents.\nLearn how to use a Voluntary Product Accessibility Template (VPAT®) to make procurement decisions.\nFind sample language for NIMAS to include in purchase orders or contracts with publishers and vendors.\nLearn how to select digital materials that are accessible for everyone, including individuals with disabilities."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:7bcf7a36-4853-4305-8c98-5c04c4039e6a>","<urn:uuid:b064601c-3972-4c0a-a856-6749dbb22f3d>"],"error":null}