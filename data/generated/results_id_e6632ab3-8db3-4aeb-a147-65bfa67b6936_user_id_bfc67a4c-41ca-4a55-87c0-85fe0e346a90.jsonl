{"question":"How do the Malnutrition Quality Improvement Initiative (MQii) and Lean Six Sigma approaches compare in their methods for measuring and reducing process errors?","answer":"The MQii and Lean Six Sigma use different but complementary approaches to measure and reduce process errors. MQii uses electronic clinical quality measures (eCQMs) to measure and quantify healthcare quality, specifically focusing on malnutrition care metrics like screening, assessment, care plan development, and diagnosis rates. Meanwhile, Lean Six Sigma uses statistical measurements called Sigma levels, where Six Sigma quality means 3.4 defects per million opportunities. The MQii focuses on implementing best practices through a toolkit and measuring improvement through EHR data, while Lean Six Sigma uses the DMAIC (Define, Measure, Analyze, Improve, Control) methodology and focuses on reducing process variations and waste. Both approaches emphasize data collection to establish baseline performance and track improvements over time, though they measure success differently based on their specific focus areas.","context":["FOOD INSECURITY RESOURCES\nWe have compiled resources to support you and your colleagues in integrating food insecurity screening and interventions into your nutrition care process.\nThe Malnutrition Quality Improvement Initiative (MQii) aims to advance evidence-based, high-quality malnutrition care for patients who are malnourished or at-risk for malnutrition. The MQii is a project of the Academy of Nutrition and Dietetics, Avalere Health, and other stakeholders who provided expert input through a collaborative partnership to advance malnutrition care across the nation.\nThe MQii offers a Toolkit which provides tools and resources to help your organization understand malnutrition care best practices, as well as guidance to help your facility identify quality improvement opportunities and critical gaps. The Toolkit can assist you in generating leadership support for a quality improvement project, building an interdisciplinary Project Team, and implementing the changes needed to advance malnutrition care in your hospital. The MQii also offers a set of de novo malnutrition electronic clinical quality measures (eCQMs) to assess the current quality of care at your institution and monitor changes in the quality of care delivered over time. The Toolkit and eCQMs are intended to mutually support your facility as you implement a quality improvement project and assess the impact on your care.\nThe MQii provides a framework for you to identify malnutrition care gaps at your hospital, introduce a QI project to improve care, and assess progress on the changes implemented. The tools provided through the MQii offer timely, structured, efficient, and evidence-based guidance to inform your QI strategy.\nHospitals across the United States have had success advancing the quality of malnutrition care their patients experience through their participation in the MQii, with the overarching goal of improving patient outcomes and reducing burden to the system.\nIt is very important to seek leadership support and approval in order to begin an MQii project. The MQii Team is happy to provide you with a “How to Get Started Checklist” that outlines a high-level step-by-step guide to help you undertake a project at your facility. The Value of Quality Malnutrition Care Briefing provides the business case for your leadership and will be a crucial part of gaining their support.\nYes, anyone can undertake a malnutrition QI project—even with little or no experience in quality improvement efforts. Many facilities that have engaged in MQii projects to date have implemented changes with little experience and have done so in the midst of IT upgrades, leadership changes, and staffing shortages. Sites, particularly those dealing with other competing projects or internal challenges, typically have the greatest success starting with a small, manageable project (e.g., within one hospital unit or primarily engaging with one clinician group) and building out from there.\nShare the business case and the evidence included in the Value of Quality Malnutrition Care Briefing with your hospital or health system’s leadership to showcase the benefits of participating in the MQii and addressing malnutrition care gaps. Give each of them a copy and tell them you want to start a malnutrition quality improvement project at your hospital. You can also use the “MQii Overview” PowerPoint presentation to help convey the burden of malnutrition on the hospital system and why implementing a malnutrition QI project is an important step to potentially improve patient outcomes such as rates of readmissions, pressure ulcers, and falls, among others. Schedule a meeting with your Quality and Executive teams and discuss these resources with them and make the case for this project.\nElectronic clinical quality measures use data from electronic health records (EHRs) and/or health information technology systems to measure and quantify healthcare quality. The Centers for Medicare & Medicaid Services (CMS) use eCQMs in a variety of quality incentive programs and to publicly report data about quality. Measuring and reporting eCQMs helps ensure that care is delivered in a safe, effective, equitable, and timely manner.\nThe MQii Toolkit is an evidence-based, patient-centered set of resources, tools, and information to help hospitals support malnutrition quality improvement (QI) and implement malnutrition care best practices. The Toolkit provides resources for all members of the care team (e.g., dietitians, nurses, physicians, patients, and caregivers) who engage in care for patients who are malnourished or at risk of malnutrition. It spans all steps in the malnutrition clinical workflow, from screening through assessment, care plan development, diagnosis, monitoring and evaluation, and discharge planning. The Toolkit was built to support malnutrition QI for hospitalized older adults (ages 65 and older), but the majority of tools and resources are relevant and applicable to any adult hospitalized patient and may have implications for post institutional care as well. By supporting malnutrition QI using guidance from this Toolkit, hospitals may be able to:\nThe MQii is based on the nutrition care process. To start, use the sample flow chart for recommended malnutrition care and the flowchart template, and/or answer the questions in the Malnutrition Care Assessment and Decision Tool to evaluate your current care delivery and determine on which aspect(s) of the workflow your institution will focus its quality improvement efforts. These tools can help your facility determine how your current workflow process compares to the recommended workflow, identify where gaps exist in your current process, and select the best areas in your workflow to target for improvement. The number of aspects of the recommended clinical workflow your organization tackles is up to your Project Team.\nThe number of components of the recommended clinical workflow your organization tackles is up to your Project Team. Using the resources to support with project prioritization will allow you to identify those target areas where you can direct resources and efforts towards.\nYou will need to work with your facility’s IT staff to verify that the data elements required to calculate the MQii eCQMs currently exist in your EHR. For sites with Epic, Cerner, or AllScripts installations, most of these EHR versions can collect the necessary patient data to report on the eCQMs, and many organizations are already collecting much of the required data. Therefore, if your site employs a recent version of Cerner, Epic, or AllScripts EHR platforms, it is very likely that your existing system has some capability to collect on the measures’ required data elements.\nShould you choose to collect information on one or more of the eCQMs, you will need to involve your IT staff and/or EHR maintenance team to input the specifications into your facility’s EHR platform. In the event that the data elements exist but are not coded to the appropriate specifications, you will need to re- label the applicable data elements to the structured code sets outlined in the measure specifications.\nThe MQii also provides a data report template to enable you to assess your facility’s ability to collect necessary data elements, and a performance calculator to help you rapidly aggregate and analyze your data to determine your performance.\nThe MQii resources are intended to be used by all clinicians that play a role in addressing the nutritional needs of hospitalized patients. As such, you are encouraged to identify an interdisciplinary MQii Project Team to facilitate your malnutrition QI project. The Project Team should have a project manager, as well as project champions from dietitian, physician, and nurse clinician groups at a minimum. These clinician leads will assist with the education and training of care team members who will be implementing the changes and will liaise with other clinicians to address any questions.\nIf you will be collecting data to evaluate your performance, you will also want to include an IT resource on the Team. The IT resource will be expected to pull relevant data elements from the facility’s EHR and ideally be able to manipulate the EHR system to program the specifications of the eCQMs, or make changes to accommodate new data elements (if necessary). Finally, for the greatest likelihood of success, you are strongly recommended to engage a representative from your hospital’s Quality Department and a senior or executive leader with decision-making authority to facilitate necessary internal stakeholder engagement. Ideally, the Project Team members will be familiar with and have experience implementing quality improvement (e.g., using Lean, Six Sigma, Plan-Do-Study-Act models), but this is not a requirement.\nThe time commitment for your hospital and the Project Team will vary depending on the scope of the project and each individual’s role in the project. It is likely you will need to commit more time as you begin your project to support training and education on your desired change and establish any data collection processes. The time commitment typically decreases following the start of implementation as the project infrastructure is already in place; tasks will become focused on addressing follow-up questions or training needs from clinicians and performing periodic data pulls. The time commitment will also vary depending on how many QI interventions you plan to implement and how many clinicians or units within the hospital you plan to involve.\nIt is important to note that the time commitment for each Project Team member will vary based on his or her role. For instance, the project manager may spend a few hours per week training staff on best practices in malnutrition care, tracking and monitoring changes in clinical practice, and supporting the Project and care teams. Following implementation, he or she may also spend time analyzing the data, reviewing feedback reports, assessing progress, and advising on what to do next. By contrast, the time commitment for a care team member (e.g., dietitian, nurse, physician) not in a leadership role on the team will be minimal, as they will primarily be spending time receiving initial training and familiarizing themselves with the best practices and how to implement them.\nThere are no fees associated with participating in the MQii to implement a malnutrition quality improvement, or to access the MQii tools and resources.\nA central aspect of quality improvement is to collect data to identify your facility’s baseline performance, assess changes over time, and inform aspects of care that might require further training or education for clinicians. You can use the four MQii eCQMs to collect data regarding rates of malnutrition screening, assessment, care plan development, and diagnosis at your facility and assess changes over time. This may be done through direct extraction of the data from the EHR using the eCQM technical specifications.\nThe eCQMs were developed with significant multi-stakeholder input, including from advisors in malnutrition care and quality improvement and a Technical Expert Panel. After development, the measures were tested for feasibility, validity, and reliability throughout 2016 in accordance with testing processes recommended by the National Quality Forum (NQF). Feasibility testing took place in three health systems and three different EHR vendors. Subsequently, validity and reliability testing took place within two health systems.\nA manuscript was published in a peer-reviewed journal describing the testing approach and results. In addition, a separate manuscript was published demonstrating the use of the measures in a subset of hospitals in the MQii Learning Collaborative implementing quality improvement guided by the MQii Toolkit.\nThe MQii Toolkit was tested over a three-month implementation period in 2016 through a multi-site Demonstration and Learning Collaborative. The Demonstration took pace at a single hospital that received hands-on training and support for the project, and collected data to assess the impact of using the Toolkit. A paper was published in 2018 outlining the approach and results of the testing efforts. By contrast, a five-hospital Learning Collaborative implemented use of the Toolkit and tracked results with limited support, in order to understand how the Toolkit is adopted and used under real-world circumstances. Findings from this initiative informed revisions to increase the Toolkit’s content, adaptability, usability, and functionality. The revised Toolkit was then tested again by a 50-hospital Learning Collaborative in 2017.\nThe Toolkit’s use demonstrated that the introduction of recommended malnutrition quality improvement actions helps hospitals achieve performance goals in nutrition care.\nCurrent evidence on malnutrition demonstrates a higher risk, prevalence, and burden of malnutrition among the older adult population (individuals ages 65 years and older). Accordingly, the tools and resources were developed with the 65+ population in mind. Yet, these tools are relevant and applicable for all hospitalized adults (aged 18 years and older) and may also have implications for post-institutional care. In 2020, a paper was published outlining the utility of these measures to understand differences in nutrition care and prevalence of malnutrition across hospital services.\nThe screening eCQM is specified for all hospitalized adults aged 18+, while the nutrition assessment, care plan development, and diagnosis eCQMs are specified for hospitalized patients aged 65+. However, these three eCQMs can be modified and used to evaluate the quality of malnutrition care among this broader patient population of hospitalized individuals aged 18+ if desired.\nWeb links to useful tools and resources that may help your implementation efforts can be found throughout the Toolkit. Resources addressing various aspects of malnutrition care — from education about malnutrition’s prevalence and economic impact, to daily quality improvement implementation, to clinical guidelines — can be accessed in the Resource Repository section of the MQii website. Resources include tools for different provider types (e.g., physicians, nurses, dietitians), and address the full spectrum of patient care from admission to discharge. Materials include handouts, presentations, and informational videos.\nSome examples of additional resources available include:\nTypically, if senior leadership approves the project, then IT participation will be easy to facilitate. You should work with senior leadership to contact the director of IT or Clinical Informatics. Alternatively, if you already have a relationship with IT, you can reach out to them directly. You should inform them of the information you will need, and request an IT team member to serve as your point of contact. Some health systems also have a Performance Improvement (or Quality) Department that may be able to establish a performance data collection and analysis process for the eCQMs. This could serve as an alternate approach if it is difficult to gain assistance from IT to collect the eCQM performance data and assess your quality of care.\nThis varies based on the skill set and knowledge of the IT personnel and their familiarity with the nutrition templates and components in the EHR. However, in most scenarios the report can be built and tested in about 36-48 hours of staff time that typically stretches across 1-2 weeks overall depending on project prioritization and existing staffing resources. In some situations, you may uncover more testing and refinement is needed, which could take up to an additional week’s worth of time.\nThe 2022 Learning Collaborative brings together leading hospitals and health systems across the U.S. to support acceleration and dissemination of malnutrition best practices for hospitalized patients. Learn more.\nResources and events to support you and your colleagues in caring for patients experiencing or at risk for malnutrition throughout the COVID-19 pandemic. Learn more.\n|cookielawinfo-checbox-analytics||11 months||This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category \"Analytics\".|\n|cookielawinfo-checbox-functional||11 months||The cookie is set by GDPR cookie consent to record the user consent for the cookies in the category \"Functional\".|\n|cookielawinfo-checbox-others||11 months||This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category \"Other.|\n|cookielawinfo-checkbox-necessary||11 months||This cookie is set by GDPR Cookie Consent plugin. The cookies is used to store the user consent for the cookies in the category \"Necessary\".|\n|cookielawinfo-checkbox-performance||11 months||This cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category \"Performance\".|","Lean Six Sigma and a sample application\nLean Six Sigma is a concept that aims to improve process performance by minimizing waste and reducing variations. It is a method that combines Lean Manufacturing, Lean Enterprise and Six Sigma principles to eliminate waste and improve quality.\nThe origins of Lean Six Sigma can be traced back to 1986 when Motorola came up with strategies to compete with higher quality Japanese products. Japan used the Kaizen approach (continuous improvement) in product development to produce world-class products of high quality.\nIn the 1990’s, an American businessman called Larry Bossidy introduced Six Sigma in Manufacturing and soon after he was engaged to introduce the concept in GE.\nIn early 2000’s the two concepts of Lean Manufacturing (Reduction of waste) and Six Sigma (higher process quality leading to reduced variability) came together as a single concept called Lean Six Sigma. The concept then found acceptance in other industries such as Healthcare, Finance, Retail and Supply chain etc.\nLean focusses on eight kinds of waste (Muda is Japanese word for waste) inherent in processes;\nSix Sigma focuses on improving the quality of process outputs by identifying and removing the causes of defects and minimising variability in processes.\nLean Six Sigma aims to achieve continuous flow of quality outcomes, by exposing constraints between process steps and reducing variability between and within the process steps through a cycle of iterative improvements. Lean Six Sigma uses the DMAIC (Define, Measure, Analyse, Improve and Control) phases similar to Six Sigma.\nBasic Concepts of Six Sigma\nSix Sigma quality is a statistical term used to indicate how well a process is controlled in terms of its variability from the mean. It is a fundamental nature of any process that over time and scale, variations will creep in due to a variety of reasons or factors. The aim of Six Sigma is to keep the process running within acceptable limits from a mean (or arithmetic average of a process data set).\nThe word Sigma ( σ ) is the standard deviation or the spread around the mean or central tendency. In simple terms, Six Sigma quality performance means 3.4 defects per million opportunities. It is important to note that not all processes, products or systems need to function at Six Sigma quality level. Other than for critical processes involving high safety requirements, such as healthcare, pharmaceuticals, airplanes, manufacturing, etc. it is enough for most processes to function at 3 Sigma or 4 Sigma. The trade-off between achieving Six Sigma or lower levels of Sigma is simply cost and often it is not practical or cost-effective to aim for a high level of Sigma. The table below illustrates the number of defects per million opportunities (DMPO) at various levels of Sigma. It is easily evident as to how efficient processes need to be at Six Sigma level.\nSigma Level DMPO\n2 σ 308,537\n3 σ 66,807\n4 σ 6,210\n5 σ 233\n6 σ 3.4\nLean Six Sigma Case Study\nThe objective of this case study is to illustrate how to apply Six Sigma thinking and concepts to organizational problems and processes.\nImagine a retail organization that uses disparate core systems such as CRM (Customer relationship management), ERP (Enterprise Resource Planning), Analytics and Financial Accounting to run its business. This organization has 100,000 unique Customer master records that are regularly referenced in sales, order management, delivery, invoicing and accounts receivable transactions. Each Customer master record has 10 attributes associated with it as shown below;\nCustomer Master Records (100,000 unique records)\nCustomer Pricing Code\nThis structure implies that there are 1 million elements associated with Customer master data.\nThese Customer master records are created, referenced and updated separately by different individuals, in different departments and business units, depending on their role and function. For example, the Finance Department may manage elements of Customer master data relating to invoicing and accounts receivable. The Sales team may manage elements relating to Customer orders. As is typical in many organizations and situations, disparate systems and independent work functions cause the following issues with Master data;\nDuplicated master data across systems that are out of sync\nWasted effort in data maintenance\nErrors that get accumulated over time because of data changes made in multiple systems\nBusiness risk arising from poor governance, etc.\nDMAIC Approach to improve the process of data management\nThe DMAIC (Define, Measure, Analyse, Improve and Control) concept can be applied to the above case problem, as follows;\nDefine (The Problem)\nMaster Data, maintained separately in multiple business systems, has been observed to contain an unacceptable level of errors (defects) causing unnecessary manual intervention (extra processing) that is costing the organization money, business responsiveness and customer satisfaction.\nMeasure (The Process parameters and Sigma)\nAn important step in Six Sigma Analysis is to measure the key operating parameters of the process in consideration to understand current levels of Sigma (Standard Deviation from mean). The table below shows the impact of errors (Defects per million elements) in terms of cost and time. An error can be broadly defined as any situation relating to any Customer master data element that requires changes to data arising from any non-business driven reason. Sigma (standard deviation) can be determined by sampling in a specific section or department or the entire organization at data element levels or entire Customer Master record levels.\nIt has been assumed that it takes an average of 10 minutes to fix an error at a cost of $50/hour. These assumptions can be validated in the Analysis stage by end users or by Six Sigma experts.\nLet us assume that sampling shows there are likely to be 200,000 errors (20% of the total volume of Customer Master data elements). From the table, we can see that the current process of managing customer master data reveals a Sigma between 2 and 3, implying it is costing the organization at least $556,725 in remediating these process errors. It is useful to note that data volume may also grow at 20% year on year and with growth in master data volume there is also an increase in DMPO.\nAnalysis (Why are there errors in the process?)\nThe errors in the process may be occurring due to one or a combination of the following reasons;\nAsynchronous editing of Customer master data elements in different systems\nLack of a concept of Master data and downstream systems for data leading to uncontrolled changes\nLack of formal data governance policies and procedures defining how Master data is created and edited\nImprove (How can the process be improved?)\nThe process for managing Customer master data and its elements can be improved through several options;\nNominating one of the systems (CRM or ERP or Financial or Analytics systems) as the Master system for Customer data and setting up integrations between that designated Master system to propagate changes\nImplementing a centralized, organization-wide Master Data Management (MDM) system to manage and control all Customer master data. This MDM system will then propagate all changes to master data to all other systems referencing that data through integrations (This may be a costlier option but improve process accuracy the most and help achieve 4 σ or 5 σ efficiencies)\nSetting up a manual governance process to manage changes to Master data in all systems in a coordinated way (This may be the cheapest option but may not improve process accuracy significantly and in a sustained manner)\nControl (How can the process be controlled?)\nRegardless of which option is chosen to improve the process efficiencies, it is important to ensure that process parameters are regularly measured and action taken to remediate. This is done by implementing good data governance initiatives.\nLean Six Sigma is a concept that aims to improve process performance and efficiencies by reducing waste and eliminating errors. It relies on collaboration amongst team members to achieve that. Lean Six Sigma uses the DMAIC (Define, Measure, Analyse, Improve and Control) phases to reduce errors. As such it is a evidence-based, data driven approach to improvement that focuses on defect prevention. Lean Six Sigma initiatives improve customer satisfaction and profitability by reducing variation, waste and cycle time while promoting work standardisation and flow.\nIt is recommended that organizations new to the Lean Six Sigma begin by implementing the Lean approach to make the workplace as effective and efficient as possible, reducing waste and using value stream mapping to improve throughput. When process problems persist, the more technical Six Sigma statistical tools can be applied by a team of specialists in various areas of the overall process."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:6d060c2a-9f56-4935-81f0-be58fdd0ff86>","<urn:uuid:02bb5903-0b04-4b5d-bf7f-7f6c52131372>"],"error":null}
{"question":"What are the key differences between emergency work authorization for fire damage versus submitting a claim for home vandalism after a fire?","answer":"For fire damage emergency work, you must get authorization before proceeding with any work and should verify coverage with your insurer first. The insurer can suggest or dispatch a disaster restoration professional, but you need to understand the scope and costs before authorizing. In contrast, for vandalism after a fire, the damage is automatically covered under homeowners' insurance policies that cover theft and vandalism, though you should not dispose of damaged items until an insurance representative has assessed the damage and made a claim report. In both cases, documentation of damages is important, but the authorization process differs.","context":["|Step by step: what to do when a loss occurs||Step by step: what to do when a loss occurs||http://chad.ca/en/consumers/making-a-claim/the-claimants-handbook/382/step-by-step-what-to-do-when-a-loss-occurs||\nTHE FIRST HOURS\nNotify your insurer\nContact your broker or insurer and describe to him to the\nbest of your ability the circumstances surrounding the loss.\nLimit the damage\nIt is your responsibility to take steps to limit the damage\nor stop the situation from deteriorating, but do not make\nany permanent repairs before speaking to your insurer. If\nyou are able to do so, document the loss (with pictures\nor videos). If water damage has occurred, dry everything\nas quickly as possible to avoid mold developing. If you\nthrow out water soaked items, make sure to take a picture\nand describe them for the purposes of settling the claim.\nIn case of fire, do not enter the building before receiving\npermission from the competent authorities.\nAuthorize emergency work\nIn order to avoid further damage occurring, emergency\nwork may be necessary. You can retain the services of\na disaster restoration professional or your insurer can\nsuggest one and even dispatch the company to the site,\nbut make sure you properly understand what they are\ndoing and how much it will cost before giving them\nauthorization to proceed.\nMeet with the claims adjuster in charge of your file\nThe insurer will designate a claims adjuster to settle your\nclaim. He will guide you through the claims process.\nHe will investigate the cause of the loss, estimate the\ndamages and negotiate the settlement with you.\nHandy tips and advice\nCheck with your insurer to make sure the emergency\nmeasures taken are covered by your contract.\nAnalyse the situation before authorizing the\ndemolition or replacement of certain materials, and\nkeep a sample of what is to be replaced.\nRefuse to have any of your property moved before\ntaking an inventory and documenting the state of the\nitems (salvageable or a total loss).\nReview your options before signing an assignment\nof claim. Read our advice on the text box below \" Should I sign the\nassignment of claim or not? \".\nMonitor the work done by the disaster restoration\nprofessionals: for example, after a fire, the service\nprovider might take away all your clothing for\ncleaning even though it may be more advantageous\nand less expensive to replace certain outdated\npieces of clothing or clean them yourself. If your\ncontract includes certain coverage limits (for\ninstance, a $15,000 maximum for water backup\nin the basement), you might have to make\ncertain decisions on how you use the insurance\ncompensation to which you are entitled. Get a\nquote before the disaster restoration professionals\ndo their work, and tell the claims adjuster what\nyour plans are in order to avoid having a large chunk\nof your compensation used up on cleaning old\nclothes or restoring furniture of little value.\nWrite on your checklist the name of the team leader\nand the number of workers on-site, how many hours\nthey worked and what kind of emergency work was\nMonitor the work. After all, it’s your home.\nVerify how much coverage you have with the help of the claims adjuster\nThe claims adjuster will explain to you the coverage your\ninsurance contract provides and, if necessary, how to go\nabout obtaining additional living expenses (for example,\nif you have to leave your home). Keep all your receipts for\nexpenses incurred in the wake of the loss since they may\nFurthermore, if you need to have your furniture stored,\ncarefully choose the personal effects that you want to keep\nwith you, as it may be difficult to access the storage space\nor find a specific item amongst all the boxes.\nTHE CLAIMS SETTLEMENT PROCESS\nIdentifying the cause of the loss\nThis is the responsibility of the claims adjuster. He will\nconfirm the damage, verify your initial statement and,\nif necessary, consult with specialists. At his request, you will\nhave to provide him with certain information or documents\nto help him identify the cause of the loss. Depending on\nthe results, he will confirm whether your claim is admissible\nor not under the terms of your insurance contract.\nEstimating the damages\nThe claims adjuster must also estimate the amount of\ndamages and decide on how much compensation the\ninsurer will offer to pay you. He will sometimes be assisted\nby an appraiser or other specialists. However, it is your\nresponsibility to properly document and justify your\nclaim, and, in particular, to prepare an inventory of\ndamaged, destroyed or stolen property with as much\nproof of ownership as possible (invoices, pictures, etc.).\nFor each item, indicate whether you wish to salvage it by\nhaving it cleaned or whether you feel it is unsalvageable.\nUltimately though, it is the claims adjuster who is\nresponsible for declaring an item a total loss.\nThe faster you submit your inventory, the faster your claim\nwill be processed. Be aware that the insurer could refuse to\npay if you make a claim for more than you actually own or\noverestimate the value of your property. Use the personal\nproperty inventory form available at chad.ca.\nChoosing the contractor\nYou are free to choose the contractor who will do the repairs\nor you can do them yourself. Insurers can also suggest service\nproviders with whom they already have a business relationship.\nNo matter what, the decision is always up to you. Discuss your\ndecision with the claims adjuster in charge of your file and come\nto an agreement on the terms and conditions of the settlement,\nsince they may differ if you do the work yourself.\nAuthorizing the work\nReview the quotes for the work, taking care to verify the\nproposed costs for each step of the process—for instance,\nrebuilding and storage; or cleaning, repairing and replacing\npersonal property—as well as timelines. Confirm it all\nwith the claims adjuster and verify whether your insurance\ncontract provides lump sum compensation or breaks down\nthe compensation into fixed “line items.” If there is a limit\non the amount of insurance, you may have to prioritize\nbetween repairs to your home and replacing damaged\nShould I sign the assignment of claim or not?\nThe disaster restoration professional, the contractor\nor the service provider may ask you to sign an\nassignment of claim that will allow the insurer to pay\nhim directly for the work he does at your home. You\nhave no obligation to sign this document. Be aware\nthat although this procedure simplifies payment\nto service providers, it may make it hard to control\nIn order to maintain some control over your total\ncompensation, you can ask that the maximum cost\nof the work be specified in the assignment of claim;\nrequire a signed letter of satisfaction before the final\ncheque is sent; or ask the insurer to make out the\ncontractor’s final cheque in both your names. These\nactions enable you to avoid “signing a blank cheque”\nwhen you make an assignment of claim, and also allow\nyou to confirm that the work was done to your full\nsatisfaction before making the final payment.\nNegotiating the settlement\nOnce the claims adjuster has received the inventory of\ndamaged personal property (with the items’ origin and\nreplacement value as of today’s date), he can calculate\nthe settlement offer using the information in your contract\nconcerning coverage, limits and exclusions, and, in\nparticular, replacement cost or depreciated value.\nIf you have replacement cost insurance, your property\nwill be repaired or replaced by new items of the same\nnature and quality, even if replacing them costs more than\nwhat was originally paid. If you decide, however, not to\nhave certain items repaired or replaced, the insurer will\ncompensate you for the value of the damaged goods\non the day the loss occurred, in other words, at their\nBefore any of your property is repaired, cleaned or\nreplaced, make sure to carefully review the settlement\noffer and confirm that it is acceptable. If not, there is\nalways room to negotiate.\nPaying compensation and the deductible\nDepending on the extent of the damages, payments are\nmade throughout the settlement process, as the service\nproviders submit their invoices. Make sure you have told\nyour insurer whether or not you are satisfied with the\nwork that was done before the insurer pays the service\nprovider. If damages do not exceed the limit of insurance,\nyour deductible will be deducted from the compensation\nor the insurer will ask you to pay this amount directly to\nthe service provider.\nClaiming the amount of the deductible from a liable third party\nIf a third party is liable for the damages you suffered,\nyou can claim reimbursement from this party for both your\ndeductible and damages that your insurer did not\nreimburse. Do not forget that there are limitation periods\nfor taking legal action. Be careful! Ask your claims adjuster\nfor more information and consult a lawyer, if necessary.\nA model letter is available at chad.ca.\nDid you know?\nThe insurer not only has the obligation to return your\nproperty to the same condition it was in before the\nloss, but it is also obliged to compensate you within\n60 days of having received your notice of claim,\nor the relevant information and additional supporting\ndocuments that it requested you provide.\nBack to the main menu\n|5/27/2015 4:40:21 PM|","The Fire Insurance Claim Process\nFire victims dealing with filing a claim with their insurance company are often faced with many questions. The following is a tip sheet offered by the Insurance Information Institute, an industry trade group, on what you need to know if your house has been damaged by fire:\nIf my house burns down, will my insurance company pay to have it rebuilt?\nThe typical homeowner’s policy covers damage due to wind, fire and lightning. So if your home has been completely destroyed by a fire or if the roof has been burned, your insurance company will pay to have your home rebuilt or to have the roof replaced. It will also pay if flames and smoke have damaged any other part of your home.\nWhat about the contents of my house?\nIn addition to paying for damage to the dwelling, homeowners’ policies cover other structures on the premises, such as a garage or tool shed, as well as damage to your furniture, clothes, appliances and other personal possessions up to the limits of your policy.\nWhat information is needed to report a claim?\nEach claim is different, but information your insurance company will likely need include:\n- Date of loss\n- Type of loss or damage\n- Location of damage\n- Any related injuries\n- Others involved\n- Condition of the home\n- Description of damaged contents\n- Whether or not temporary repairs are necessary\n- A police report\nWho will pay for temporary housing?\nYour insurance company might cover your housing expenses, depending on your plan. These “additional living expense coverage” or “loss of use coverage” options will pay for similar housing while repairs are being made to your home or if you permanently relocate. Typically, you need to seek reimbursement for expenses incurred. Also keep in mind that payments do no cover lost wages or earnings.\nShould I file a police report?\nYes. In many cases, a police report is required when reporting a claim.\nWhat about receipts?\nThe more documentation you have, the better. Assuming such documents survived the fire, receipts, owner’s manuals, warranty cards, appraisals, photographs or the original boxes that the items came in will all help.\nWhat other documents should I compile?\nKeep an accurate record of all temporary repair expenses such as bills or material receipts so that you can add the amount to your claim. Also, keep an accurate record of any and all expenses incurred to be considered for possible reimbursement. Do not make any permanent repairs until the insurance adjuster has had a chance to review the damage.\nWhat is a deductible?\nA deductible is the amount you agree to pay as your part of the loss. The insurance company will pay for the amount over the deductible if it is a covered loss. For example, if the covered claim is $2,000 and your deductible is $500, you pay $500 and your insurance company pays the $1,500 balance.\nHow do I figure out what I’ve lost?\nMake a written list of what was damaged. To be as accurate as possible, include the manufacturer, brand name and the place and date of purchase. A good way to start this process is to divide your list into broad categories, such as location: living room, master bedroom, kitchen, etc. If available, photographs, videotapes or personal property inventories are valuable resources during the itemization process.\nMuch of my furniture and possessions were badly damaged — can I get rid of them if I have a home inventory?\nA homeowner should not throw things away until an insurance company representative has been able to assess the damage and make a claim report.\nMy home was vandalized after the fire and my new television was stolen. Am I covered?\nHomeowners’ insurance policies cover theft and vandalism, so any losses due to looting in the wake of the fire should be covered.\nShould I make temporary repairs to my house?\nIt is important to take immediate steps to prevent further damage to your home. Some policies may cover such repairs, others might not. Some policies will let you hire a contractor to do the work. Whether hiring somebody or doing it yourself, document all your expenses.\nDoes my insurance pay for the loss of any trees, shrubs or other plants I lost in the fire?\nThe typical homeowner’s policy covers trees, shrubs, plants or lawns on the residence for loss caused by fire. Usually insurers will pay up to 5 percent of the limit of liability that applies to the dwelling for all trees, shrubs, plants or lawns. No more than $500 will be paid for any one tree, shrub or plant. Insurance, however, does not cover property grown for business purposes.\nIf I have questions about my homeowner’s policy, where can I get help?\nYou can call your insurance agent, broker or company representative or the National Insurance Consumer Helpline: Call 1-800-942-4242 and ask for the free brochure, How to File an Insurance Claim."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:8e7cd652-667b-4db0-937c-862cc07ec75c>","<urn:uuid:273a488e-5338-4e00-8dd1-08416ac09fa5>"],"error":null}
{"question":"What is the relationship between sigma and pi bonds in ethene (C2H4) compared to other types of molecular bonds?","answer":"In ethene (C2H4), there is both a sigma bond and a pi bond between the carbon atoms. The sigma bond is formed through the overlap of sp2 hybridized orbitals, while the pi bond is formed by the sideways overlap of unhybridized p orbitals. For the pi bond to form successfully, the CH2 groups must be coplanar, making C2H4 a planar molecule with approximately 120-degree bond angles. In general, multiple bonds consist of one sigma bond and one or more pi bonds, with double bonds having one of each.","context":["AP Chapter 9 Molecular Geometry and Bonding Theories Molecular Shapes • The 3-dimensional shapes and sizes of molecules are determined by their bond angles and bond lengths. • Molecules with a central atom surrounded by n atoms B, denoted ABn, adopt many different geometric shapes, depending on the value of n, and on the atoms involved. • The majority have one of 5 basic shapes: linear, trigonal pyramidal, tetrahedral, trigonal bipyramidal and octahedryl. The 5 basic shapes for ABn molecules Additional molecules can be made by removing corner atoms from the 5 basic shapes. This one starts with a tetrahedral. The VSEPR Model • VSEPR – valence shell electron-pair repulsion model. • Molecular geometries are based on the repulsions between electron domains. (Like charges repel.) • Shared pairs (bonded pairs) and unshared pairs (nonbonding pairs) create electron domains around the central atom. VSEPR • Electron domains remain as far apart as possible. • Nonbonding pairs of electrons (unshared pairs) exert more repulsions than bonding pairs (shared pairs.) • Electron domains from multiple bonds exert slightly more repulsions than those from single bonds. VSEPR • The arrangement of electrons around a central atom is called electron-domain geometry. • The arrangement of atoms is called the molecular geometry. In general, each non-bonding pair, single bond or multiple bond produces an electron domain around the central atom. Balloons tied together at their ends naturally adopt their lowest-energy arrangement. Electron Domains • The best arrangement of a given number of electron domains is one that minimizes the repulsions among them. This is why the balloon analogy – they have similar preferred geometries. Predicting Shapes 1. Draw the Lewis structure of the molecule or ion and count the total number of electron domains around the central atom. 2. Determine the electron-domain geometry by arranging the electron domains around the central atom so repulsions are minimized. (Table 9.1) 3. Use the arrangement of the bonded atoms to determine the molecular geometry. The Effect of Nonbonding Electrons and Multiple Bonds on Bond Angles The bond angles decrease as the number of nonbonding electron pairs increase. Nonbonding pairs of electrons • Nonbonding pairs of electrons take up more space than bonding pairs. • The electron domains for nonbonding electron pairs exert greater repulsive forces on adjacent electron domains and tend to compress the bond angles. Electron domains for multiple bonds exert greater repulsive forces on adjacent electron domains than single bonds. Molecular Shape and Molecular Polarity Covalent Bonding and Orbital Overlap • Valence bond theory states that covalent bonds are formed when atomic orbitals on neighboring atoms overlap one another. Hybrid Orbitals • This involves mixing s, p and sometimes d orbitals. • Linear = sp • Trigonal planar = sp2 • Tetrahedral = sp3 • Trigonal bipyramidal = sp3d • Octahedral = sp3d2 Multiple Bonds • Sigma bonds (σ) – the overlap of 2 s orbitals • Pi bonds (π) – the sideways overlap of p orbitals. π bonds in Benzene rings and resonance structures Resonance structures Hybridization Involving Triple Bonds Sigma and Pi Bonds • Double bonds consist of one δ bond and one π bond • A triple bond consists of one δ bond and two π bonds • π bonds must lie in the same plane, therefore, the presence of π bonds makes the molecule slightly rigid. Molecular Orbitals • In the Molecular Orbital theory, electrons exist in allowed energy states called molecular orbitals (MO). • They can be spread among all the atoms in a molecule, they have a specific amount of energy and can hold a maximum number of 2 electrons. Molecular Orbitals • The combination of two atomic orbitals leads to 2 MOs, one with low energy and one with higher energy. • The lower energy MO concentrates the charge density in the region between the nuclei and is called a bonding molecular orbital. • The higher energy MO excludes electrons from the region between the nuclei and is called antibonding molecular orbitals. Bond orders • When the appropriate number or electrons are placed into the MOs, bond orders can be calculated, which is half the distance between the number of electrons in bonding MOs and the number of electrons in the antibonding MOs. • A bond order of 1 corresponds to a single bond and so on. They can be fractions! Paramagnetism and Diamagnetism • Paramagnetism – the attraction of a molecule by a magnetic field due to unpaired electrons. • Diamagnetism – in molecules where all the electrons are paired, the molecules exhibit a weak repulsion from a magnetic field.","If you like us, please share us on social media.\nThe latest UCD Hyperlibrary newsletter is now complete, check it out.\nC2H4, also known as ethylene or ethene, is a gaseous material created synthetically through steam cracking. In nature, it is released in trace amounts by plants to signal their fruits to ripen. Ethene consists of two sp2-hybridized carbon atoms, which are sigma bonded to each other and to two hydrogen atoms each. The remaining unhybridized p orbitals on the carbon form a pi bond, which gives ethene its reactivity.\nIn 1916 G. N. Lewis proposed that a chemical bond is formed by the valence electrons of atoms which would prove fundamental in the valence bond theory. We all remember our first alkene discussed in Organic Chemistry and it was usually C2H4. This is becuase ethene is the smallest alkene and how it is also planar. The Valence Bond theory will show us how the bonds in C2H4 come together. Two things that will help us utilize the Valence Bond Theory effectively are familiarity with Lewis Structures and VSEPR theory.\nA key component of using Valence Bond Theory correctly is being able to use the Lewis dot diagram correctly. Ethene has a double bond between the carbons and single bonds between each hydrogen and carbon: each bond is represented by a pair of dots, which represent electrons. Each carbon requires a full octet and each hydrogen requires a pair of electrons. The correct Lewis structure for ethene is shown below:\nFor more information on how to use Lewis Dot Structures refer to http://chemwiki.ucdavis.edu/Wikitexts/UCD_Chem_124A%3a_Kauzlarich/ChemWiki_Module_Topics/Lewis_Structures.\nValence Shell Electron Pair Repulsion (VSEPR) Theory is used to predict the bond angles and spatial positions of the carbon and hydrogen atoms of ethene and to determine the bond order of the carbon atoms (the number of bonds formed between them). Each carbon atom is of the general arrangement AX3, where A is the central atom surrounded by three other atoms (denoted by X); compounds of this form adopt trigonal planar geometry, forming 120 degree bond angles. In order for the unhybridized p orbitals to successfully overlap, the CH2 must be coplanar: therefore, C2H4 is a planar molecule and each bond angle is about 120 degrees. The diagram below shows the bond lengths and hydrogen-carbon-carbon bond angles of ethene:\nAccording to valence bond theory, two atoms form a covalent bond through the overlap of individual half-filled valence atomic orbitals, each containing one unpaired electron. In ethene, each hydrogen atom has one unpaired electron and each carbon is sp2 hybridized with one electron each sp2 orbital. The fourth electron is in the p orbital that will form the pi bond. The bond order for ethene is simply the number of bonds between each atom: the carbon-carbon bond has a bond order of two, and each carbon-hydrogen bond has a bond order of one. For more information see http://chemwiki.ucdavis.edu/Wikitexts/UCD_Chem_124A%3a_Kauzlarich/ChemWiki_Module_Topics/VSEPR\n1) What is the hybridization of carbon in C2H4?\n2) What is the hybridization of hydrogen in C2H4?\n3) What is the bond order of the C to C bond in C2H4?\nAn NSF funded Project"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:a7f6fb45-cd5e-4449-b2ca-70c2f15acb73>","<urn:uuid:f55d199e-16f0-4f8c-ab1f-131e3981adb1>"],"error":null}
{"question":"How do Galerie Maeght and MoMA differ in their approach to international art accessibility?","answer":"Galerie Maeght focuses on shipping art internationally and maintaining galleries in Paris and San Francisco, dealing with challenges like currency exchange, shipping costs, and cross-border legal issues. They primarily work with Paris-based artists but maintain an international presence. In contrast, MoMA centralizes its vast collection of 150,000 art pieces in New York City, making it accessible to millions of visitors who come to view works like Van Gogh's Starry Night and Monet's Water Lilies at their physical location.","context":["When thinking about international business in reference to Galerie Maeght, the first thing that comes to my mind is bubble wrap. Lots and lots of bubble wrap. It is kept shoved into a drawer near the framing table, and the rest of the stores of it are kept in the basement, rolled into huge cylinders. One almost crushed me–how sad would it have been if I was hurt by bubble wrap?\nIt’s used when shipping pieces as small a 10 inch by 12 inch piece of art to a life-sized statue of one of Malevich’s sportifs. A crucial part of protecting the beautiful works that we so value, this simple material is one of the most visible signifiers of Galerie Maeght’s position in the global theatre.\nThe gallery is deeply intertwined into the history of cubism, abstract, modern and contemporary art. It holds extensive collections by Baya, Miró, Calder, Kelly, and Giacometti, to name a few. These works are in high demand by museums around the world, and many people come to our gallery to see the wealth of art that the gallery has to offer. Galerie Maeght usually works with artists who live and work in Paris, as it is easier to provide materials, ship and frame works, and stay in contact, but the artists who work with the gallery are often still international. In addition to all that, there is the Jules Maeght Gallery in San Francisco (pictured above), all the way across the Atlantic and the continental United States. Galerie Maeght is, deeply, a global organization, and that creates interesting issues as well as opportunities.\nFirst, our clientele come from around the world. This creates a host of new problems that must be navigated to properly receive these customers and to (hopefully) make a sale.\nDifferent cultures have different ideas of how customer service should work in a retail-based environment. I often come back from my lunch hour only to hear my coworkers complain to me about the americans who came in and told them about their entire day. “I don’t want to know!” they exclaim. And while that’s a more amusing example, at times it can mean a lost sale if a customer feels they are receiving too much or too little attention. There are also language barriers, which can be especially frustrating if a customer is looking for something specific but cannot articulate what it is in a second language.\nEven if a product is chosen, we much concern ourselves with its transport. If a customer buys a fairly cheap poster, we often just roll it into a plastic bag. But if they are traveling by plane, the piece must be properly protected, and that requires a cardboard tube. Each tube costs us about 3€. This isn’t very steep when compared to a 600€ signed print, but with a 15€ poster, this begins to add up. Should the customer choose to ship, that adds an even higher cost. This also means that, effectively, we cannot frame their piece for them. Shipping a rolled-up print is much, much cheaper than shipping a whole frame. And while this is better for the customer, it means that we both lose the money that we would have made off of framing and are required to entrust that another, unknown company will properly frame and display our product.\nPayment is also complicated. Of course, our sales to international customers are affected by currency exchange rates, but it’s even more complex than that. It often takes several days for a high-value purchase to be processed. While this is not much of an issue for our France-based customers, our international customers may not be here long enough for us to wait for the payment to go through before giving them their purchase. This can cause aggravation and impatience.\nUnder the same umbrella of currency, having the Jules Maeght Gallery in San Francisco means that Galerie Maeght must continually monitor the exchange rates between the euro and the U.S. dollar. This country-based difference also presents issues in terms of following legal rules of protecting intellectual property. We often sell prints from artist exhibitions from the San Francisco location, and vice versa. Laws regarding intellectual property and art ownership are much stricter in France than in the U.S.; what happens when that piece of artwork crosses that border? I hope to find out more about this. The two galleries also cross-advertise on their social media platforms, which I have and dealt with when writing posts for the company’s Instagram page. This means that the Paris gallery is still looking to appeal to customers in San Francisco. With nine hours of time difference between the locations, when does the company post? How does it mark its location?\nBut, as is often the case with global business, the benefits far outweigh the risks.\nInternational customers mean that word of our gallery spreads far and wide. Many people come to the gallery after having visited 15 or 20 years earlier, and they usually bring friends. People come because “my mom bought a Miró here back in the 1980s” or “my aunt gave me a gift from this gallery and I absolutely loved it.” Despite strange timing and location-tagging issues with social media, the gallery’s social media presence has still alerted many people its location and products, and brings in more foot traffic.\nThe San Francisco gallery gives Galerie Maeght an invaluable eye into the industry of modern and contemporary art in the U.S. This collaboration also diversifies the artwork that can be offered by the gallery in Paris, and thereby provide more options and satisfy more customers. This close connection is a valuable competitive edge over purely Paris-based galleries, especially when serving american clientele, who are popular in the prestigious 7th arrondissement where the gallery is located. This also provides extra value to the artists who exhibit both in the Paris and San Francisco locations, as they can know that their artwork and ideas will have an even greater reach. This is yet another incentive for artists to partner with Galerie Maeght.\nArt is, at its core, communication of emotion. We create visual art as a way to express what words and music fail to convey. An understanding of the artist’s native tongue or culture is not forcibly required to understand what they are trying to say, and in this way, art is fundamentally global. It only makes sense that Galerie Maeght, as a purveyor of these cultural gems, should be global too.","Museum of Modern Art11 West 53 Street, New York, NY 10019\nMuseum of Modern Art website >\nHOURS: Saturday - Monday 10:30 a.m. to 5:30 p.m.\nWednesday - Thursday 10:30 a.m. to 5:30 p.m.\nFriday 10:30 a.m. to 8 p.m.\nNEIGHBORHOOD: Midtown, Manhattan\nCATEGORIES: art, architecture, design\nThe Museum of Modern Art is the central institution of New York City’s art world. The energy of the museum is captured in the millions of visitors, just as much as it is in its vast and awe-inspiring collection. A leader in developing and collecting modernist art, it’s often pointed to as the single most influential museum in the modern art world. MoMA’s holdings include more than 150,000 pieces of art, in addition to 22,000 films and 4 million film stills. Some of its most famous works include The Starry Night by Van Gogh, Water Lilies by Monet, and Number 31 by Jackson Pollock. The museum also boasts the world’s first curatorial department dedicate to architecture and design. The collection consists of 28,000 works, ranging from blueprints and works on paper to large scale models, appliances, and sports cars.\nIn the early 2000s, the museum underwent an extensive renovation, designed by Japaneses architect Yoshio Taniguchi. The remodeling doubled the space for the museum’s exhibitions and programs, optimized the building’s use of natural light, and expanded its beloved sculpture garden.\nAdmission is free for children under 16, $12 for students with ID, $16 for seniors, and $20 for adults. Members are free, guests of members cost $5.\nREADER REVIEWS (3)\nMoMA’s permanent collection is great, as is their film program!\nIf I could, I would visit MoMA every day. They house so many of my favorite works of art, and it’s a complete thrill every time I visit. It’s incredible the exhibits that they can mount just within their own permanent collection.\nI once interned at MoMA, so I am partial to their collections. I especially love the sculptures in the outdoor courtyard! The woman almost falling into the garden pool is one of my favorites. Their permanent installations boast some of the greatest, most recognizable pieces of modern art.\nMOST POPULAR RESTAURANTS\nBrowse by Keyword\n- The Opening of the Highline Urban Park Video The Opening of the HIghline Urban Park\n- Inhabitat Green Tour of BKLYN Designs Video Inhabitat Green Tour of BKLYN Designs\n- Project Earth Day Eco Fashion Show Video Project Earth Day Eco Fashion Show\n- Project Earth Day Fashion Competition Video Project Earth Day Fashion Competition\n- Portable Green Energy Video Portable Green Energy\n- Mary Lou Jepsen on One Laptop Per Child Video Mary Lou Jepsen on One Laptop Per Child\n- Enviro-artist Chris Jordan Video Enviro-artist Chris Jordan\n- Chris Jordan Interview Video Chris Jordan Interview\n- Greener Gadgets Design Competition Winners Video GGC Design Competition\n- Greener Gadgets Introduction Video Greener Gadgets Intro\n- Grow A Living Treehouse Video Grow A Living Treehouse\n- New Mklotus Prefab Video New Mklotus Prefab\n- Sustainable Design Panel Video Sustainable Design Panel\n- Green Building & Remodeling For Dummies\n- Design Like You Give a Damn: Architectural Responses to Humanitarian Crises\n- The Green House: New Directions in Sustainable Architecture\n- Cradle to Cradle: Remaking the Way We Make Things\n- Green Roofs: Ecological Design And Construction\n- ecoDesign: The Sourcebook\n- Prefab Prototypes: Site-Specific Design for Offsite Construction\n- Planet Earth: As You've Never Seen It Before"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:25c08099-90ec-413e-8a52-c94cf008b7d4>","<urn:uuid:daef7c96-7abf-4551-9288-8e4217d125f4>"],"error":null}
{"question":"What are the main components of an emergency power system, and what maintenance procedures are essential for their reliability?","answer":"The two main components of an emergency power system are power generation (via auxiliary/emergency power system like diesel generators) and protection (via uninterruptible power supply/UPS). For maintenance, NFPA 110 requires following manufacturer recommendations and performing regular testing. Essential procedures include weekly inspections of all components, monthly exercising of diesel generators for minimum 30 minutes at sufficient load to maintain recommended exhaust temperatures, monthly testing of transfer switches, and annual fuel quality testing. Storage batteries must be inspected weekly, with monthly testing of electrolyte specific gravity for lead-acid batteries. The system requires qualified personnel for maintenance and detailed records of all inspections, tests, repairs, and modifications.","context":["Power failures are infrequent, but nevertheless they do still occur, thus every business should review its contingency and emergency power plans.\nBy submitting your personal information, you agree that TechTarget and its partners may contact you regarding relevant content, products and special offers.\nThere are two main components in a complete emergency power system -- power generation and protection -- and these are provided by:\n- an auxiliary or emergency power system\n- an uninterruptible power supply (UPS)\nAn auxiliary or emergency power system provides generated power, usually via a diesel generator, but gas, petrol or hydrogen in the case of fuel cells may also be used. However, there's a delay before a conventional generator reaches operating power and can supply electricity. This gap in supply can cause problems for computers and the applications running on them, so a generator isn't enough to provide a reliable power supply.\nThis is where the uninterruptible power supply comes in. An uninterruptible power supply provides stored power, usually from lead-acid batteries, but it can also come from NiCad batteries and flywheel systems. Its primary roles are to provide near instantaneous protection from input power interruptions and short-term power in the event of a power failure.\nUninterruptible power supplies correct a variety of poor power quality problems. A sag in power can occur when an air conditioning unit turns on, while power spikes -- brief, high-energy bursts -- are typically caused by lightning or malfunctions in the power supply. Line noise -- usually caused by nearby equipment or frequency variations -- and brownouts -- extended periods of under voltage -- are all smoothed out and corrected by an uninterruptible power supply.\nThe short-term power supplied by an uninterruptible power supply presents enough time in which to shut down protected equipment safely, or to bring an auxiliary power source on-line, which can deliver more sustainable power. For many smaller organisations, a generator is not a viable option. Cost and maintenance are obvious issues, but there can be problems such as location, planning permission, size, noise, heat and exhaust fumes. In such a situation, choosing the best uninterruptible power supply is even more important.\nDesktop computers should be supported by an off-line uninterruptible power supply with an Automatic Voltage Regulation (AVR) transformer -- often described as line interactive -- or, if the organisation can afford it, a slightly more expensive on-line uninterruptible power supply . The main advantage of an on-line UPS is its ability to provide an electrical firewall between the incoming power and the equipment connected to it. An on-line uninterruptible power supply may be necessary when the power environment is noisy, such as in industrial settings, for larger equipment loads like data centres, or when operation from an extended-run backup generator is necessary, because electrical interference from other machines -- even a vacuum cleaner -- can disturb power waves. Critical servers should be protected with an on-line double conversion UPS, filtering the input power and controlling output voltage and frequency.\nWherever possible, choose an uninterruptible power supply that has the ability to initiate a controlled shut down of the equipment it's protecting, and check that it can support the equipment's load and sensitivity. The battery runtime depends on the type and size of batteries and rate of discharge -- which, in turn, is based on the type of equipment connected to it -- and for most uninterruptible power supplies is relatively short, five to 15 minutes being typical. In order to bridge longer supply interruptions, you will need extended-runtime UPSes with extra batteries.\nIn small offices, a cheaper option is to use laptops instead of desktops that are connected to the mains power via power strips, which include surge protection. This setup only requires the investment of an uninterruptible power supply for the office server. However, it is best to avoid relying on a single uninterruptible power supply, as it can be a single point of failure. It is better to use multiple smaller uninterruptible power supplies to provide greater redundant power protection.\nMany servers and more powerful desktops can accept more than one power supply, so. in the event of one power supply failing, one or more other power supplies are able to power the load. Redundancy can be further increased by connecting each power supply to its own uninterruptible power supply. This provides double protection from both a power supply failure and a UPS failure. This configuration is referred to as 2N redundancy. If the budget does not allow for two uninterruptible power supply units, then plug one power supply into the mains power via a power strip and the other into the UPS, making sure each power supply can power the entire server by itself.\nIf your operations require constant power with no breaks and high reliability, then you will need a generator or generators in addition to uninterruptible power supplies with switchgear, so the generator can start automatically on a mains failure. In such situations, seek professional advice from a company specialising in power protection.\nAlso, power supply equipment needs to be maintained. A UPS manufacturer's standard warranty is normally only two years, and UPS batteries do decline in efficiency. Regular controlled tests of the system need to be carried out to ensure power can be restored within an acceptable timeframe.\nInstalling a suitable, uninterruptible power supply is a proactive and cost-effective decision. It helps ensure profitability and employee productivity won't suffer during any type of power disruption, as you can keep your electronic systems running. Doing so also avoids lengthy recovery periods and the need to re-input data. Even if your equipment is insured, the insurance won't cover any loss of goodwill or reputation if your level of customer service is affected. Lightning rarely strikes twice, but make sure you're ready, and install either an emergency power source, or test that the one you have is up to the job.\nAbout the author:\nMichael Cobb, CISSP-ISSAP, CLAS is a renowned security author with more than 15 years of experience in the IT industry. He is the founder and managing director of Cobweb Applications, a consultancy that provides data security services delivering ISO 27001 solutions.","Chapter 8 of NFPA 110 contains requirements for performing maintenance on Emergency Power Supply Systems (EPSSs). Planned maintenance is a critical part of any backup power system. Regular verification that an installed system is functional is necessary to ensure that standby power will be available when called upon at a loss of primary power.\nAll components of the system, from fuel quality up to the operation of the transfer switch, are addressed in this chapter because each relies on the other. These operational testing requirements are the minimum benchmarks for proper operation of the EPSS.\nNFPA 110 requires that the manufacturer’s maintenance recommendations be followed along with the instruction manuals, the minimum requirements of this chapter, and any additional requirements of the authority having jurisdiction (AHJ). (8.1.1) Random inspection and testing are not a basis for maintaining the dependability of an EPSS. The continued reliability of the EPSS is dependent on an established program of routine maintenance and operational testing.\nIt also requires that consideration be given to temporarily providing a portable generator or alternate source of emergency power whenever the emergency generator is out of service or during routine testing. (8.1.2).\nManuals, Special Tools and Spare Parts\nAt least two sets of instruction manuals for all major components of the EPSS shall be supplied by the manufacturer. These manuals should contain detailed explanations of the operations and maintenance tasks, an illustrated parts list with part numbers, and schematic diagrams of electrical wiring systems (one-line drawings), including operating and safety devices, control panels, instrumentation, and annunciators. (8.2.1)\nFor Level 1 systems, instruction manuals shall be kept in a secure, convenient location, one set near the equipment, and the other set in a separate location. (8.2.2)\nSpecial tools and testing devices necessary for routine maintenance shall be available when needed. (8.2.3)\nSpare parts as recommended by the manufacturer or by experience should be stocked and maintained in a secure location on the premises. (8.2.4)\nIt is recommended that the instruction manuals, special tools, and spare parts be stored in a metal cabinet in the generator room. The instruction manual should be stored on the inside of the cabinet door.\nMaintenance and Operational Testing\nThe EPSS shall be maintained to ensure to a reasonable degree that the system is capable of supplying service within the time specified for the type and for the time duration specified for the class. (8.3.1). It is suggested that the manufacturer’s maintenance recommendations, procedures, and frequency be followed. However, NFPA 110 outlines an alternate maintenance program if no manufacturer guidelines are available. Figure A.8.3.1(a) and Figure A8.3.1(b) provide alternate suggested procedures.\nA routine maintenance and operational testing program shall be initiated immediately after the EPSS has passed acceptance tests or after completion of repairs that impact the operational reliability of the system. (8.3.2). A written schedule for routine maintenance and operational testing of the EPSS shall be established. (8.3.3)\nWhen components of an EPSS are repaired or replaced, an operational test is necessary to verify the proper operation of the system.\nThe operational test shall be initiated at an ATS and shall include testing of each EPSS component on which maintenance or repair has been performed, including the transfer of each automatic and manual transfer switch to the alternate power source, for a period of not less than 30 minutes under operating temperature. (126.96.36.199)\n8.3.4 - Transfer switches shall be subjected to a maintenance and testing program that includes all of the following operations:\n- Checking of connections\n- Inspection or testing for evidence of overheating and excessive contact erosion\n- Removal of dust and dirt\n- Replacement of contacts when required\n8.3.5 - Paralleling gear shall be subject to an inspection, testing, and maintenance program that includes all of the following operations:\n- Checking connections\n- Inspecting or testing for evidence of overheating and excessive contact erosion\n- Removing dust and dirt\n- Replacing contacts when required\n- Verifying that the system controls will operate as intended\n8.3.6 - Storage batteries, including electrolyte levels or battery voltage, used in connection with systems shall be inspected weekly and maintained in full compliance with manufacturer's specifications. Note: A major cause of problems when starting generator sets is the lack of battery maintenance.\n- Maintenance of lead-acid batteries shall include the monthly testing and recording of electrolyte specific gravity.\n- Battery conductance testing shall be permitted in lieu of the testing of specific gravity when applicable or warranted.\n- Defective batteries shall be replaced immediately upon discovery of defects.\n8.3.7 - A fuel quality test shall be performed at least annually using appropriate ASTM standards. Note: Fuel quality and fuel management are concerns since many generator failures are attributed to poor fuel quality, contamination, and other fuel system problems. While NFPA 110 does not specify a specific ASTM standard for the fuel quality test, ASTM D 975 - Standard Specification for Diesel Fuel Oils, contains test methods for existing diesel fuel. Your fuel supplier or generator service provider can also provide test kits or fuel sampling to comply with this requirement.\nOperational Inspection and Testing\nNFPA 110 requires that the EPSS, including all appurtenant components (i.e. generator sets, transfer switches, circuit breakers, fuel systems, and other equipment) be inspected weekly and exercised under load at least monthly. (8.4.1)\nA weekly inspection procedure might be provided for a specific generator. In lieu of that, there are weekly inspection items for EPSS installations identified in Figure A.8.3.1(a). There are also inspection items for monthly, quarterly, semi-annual, or annual intervals.\nDiesel generator sets in service shall be exercised at least once monthly, for a minimum of 30 minutes, using one of the following methods:\n- Loading that maintains the minimum exhaust gas temperatures as recommended by the manufacturer Note: there is no load percentage required for this test. Rather, the load needs to be sufficient to achieve the exhaust temperature recommended by the manufacturer.\n- Under operating temperature conditions and at not less than 30 percent of the EPS standby nameplate kW rating. Note: The minimum load is intended to limit the possibility of wet-stacking (incomplete combustion), which may occur when a diesel generator is lightly loaded.\nDiesel-powered EPS installations that do not meet the minimum requirements listed above shall be exercised monthly with the available EPSS load and shall be exercised annually with supplemental loads (load bank testing) at not less than 50 percent of the EPS nameplate kW rating for 30 continuous minutes, and at not less than 75 percent of the EPS nameplate kW rating for 1 continuous hour for a total test duration of not less than 1.5 continuous hours.\nSpark-ignited (gaseous fueled) generator sets shall be exercised at least once a month with the available EPSS load for 30 minutes or until the water temperature and the oil pressure have stabilized. Note: Spark-ignited (e.g. natural gas or propane) generators are not subject to the minimum load requirements of diesel generators and can be tested with available loads for 30 minutes or until the water temperature and oil pressure have stabilized.\nThe EPS test shall be initiated by simulating a power outage using the test switch(es) on the ATSs or by opening a normal breaker. Opening a normal breaker shall not be required. (8.4.3) Where multiple ATSs are used as part of an EPSS, the monthly test initiating ATSs shall be rotated to verify the starting function on each ATS. Note: Rotating the ATS that initiates the cold engine start of the monthly test will verify the operation of each transfer switch. Consideration should be given to ATS criticality.\nLoad tests of generator sets shall include complete cold starts. (8.4.4) Time delays shall be set as follows: (8.4.5)\n- Time delay on start:\n- 1 second minimum\n- 0.5 second minimum for gas turbine units\n- Time delay on transfer to emergency: no minimum required\n- Time delay on restoration to normal: 5 minutes minimum\n- Time delay on shutdown: 5 minutes minimum\nTransfer switches shall be operated monthly. (8.4.6) The monthly test of a transfer switch shall consist of electrically operating the transfer switch from the primary position to the alternate position and then a return to the primary position. Note: Although it is not required to verify the type rating of the system during the monthly test, it is required to annually confirm that the system responds within the time dictated by its rating.\nEPSS circuit breakers for Level 1 system usage, including main and feed breakers between the EPS and the transfer switch load terminals, shall be exercised annually with the EPS in the “off” position. (8.4.7) Circuit breakers rated in excess of 600 volts for Level 1 system usage shall be exercised every 6 months and shall be tested under simulated overload conditions every 2 years. Note: It is not intended that the circuit breakers be exercised under load.\nEPSS components shall be maintained and tested by qualified person(s). (8.4.8)\n36 Month Testing\nLevel 1 EPSS shall be tested at least once within every 36 months. (8.4.9) This requirement applies to all generator types if they supply a Level 1 EPSS. The intent is to ensure that the EPSS with all its auxiliary systems is capable of running with load for the duration of its assigned class. It is not intended that a full facility power outage be conducted as part of these tests. This 36-month test is not a requirement for a Level 2 EPSS.\nLevel 1 EPSSs are tested for the duration of their assigned Class if less than 4 hours. Where the assigned Class is greater than 4hours, it shall be permitted to terminate the test after 4 continuous hours. (188.8.131.52)\nThe test shall be initiated by operating at least one transfer switch test function and then by operating the test function of all remaining ATSs, or initiated by opening all switches or breakers supplying normal power to all ATSs that are part of the EPSS being tested. (184.108.40.206) A power interruption to non-EPSS loads shall not be required. (220.127.116.11)\nThe minimum load for this test shall be as follows: (18.104.22.168)\n- For a diesel-powered EPS, loading shall be not less than 30 percent of the nameplate kW rating of the EPS or that which maintains the minimum exhaust gas temperatures as recommended by the manufacturer. A supplemental load bank shall be permitted to be used to meet or exceed the 30 percent requirement.\n- For spark-ignited EPSs, loading shall be the available EPSS load.\nThe 36 Month Test shall be permitted to be combined with one of the Monthly Tests and one of the Annual Tests as a single test. (22.214.171.124) Where the test is combined with the annual load bank test, the first 3 hours shall be at not less than the minimum loading required by 126.96.36.199 (30 percent for diesel EPSs) and the remaining hour shall be at not less than 75 percent of the nameplate kW rating of the EPS.\nRecords shall be created and maintained for all EPSS inspections, operational tests, exercising, repairs, and modifications. (8.5.1)\nThe record shall include the following: (8.5.3)\n- The date of the maintenance report\n- Identification of the servicing personnel\n- Notation of any unsatisfactory condition and the corrective action taken, including parts replaced\n- Testing of any repair in the time recommended by the manufacturer\nThis new section was created in the 2016 edition to consolidate record. No retention time is specified to reflect actual operating conditions. However, the retention period may be defined by the facility management or the authority having jurisdiction (AHJ) and must be made available to the AHJ on request.\nWe have created a collection of maintenance and testing schedules, forms and logs to help keep you in compliance with NFPA 110 recordkeeping rules. You can use these forms to develop your maintenance and testing plan, keep a log of your maintenance history, and record results of your weekly inspections and monthly operational tests."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:ab4b0180-eec0-46e0-920d-3be9839329e7>","<urn:uuid:9905f4f7-7df2-4961-8079-5271fc611ccb>"],"error":null}
{"question":"How do moisture testing methods differ between coffee analysis and grain storage preparation, particularly regarding the time required and technology used?","answer":"Coffee moisture testing can be done through several methods: the official oven method takes 24 hours at 105°C, while faster alternatives include thermo-gravimetric analysis (10-15 minutes) and capacitance testing (1-10 seconds) using RF signals. For grains like maize, moisture testing can be done using handheld grain moisture or digital testers to verify proper drying, which typically takes about 3 days when sun-drying, or can be done using commercial dryers with heated air or grain cooling systems.","context":["In the import and export arena of the coffee sector, moisture levels in\ncoffee are important, not only because they have an impact on storage considerations, but becuase coffee is most often bought by the pound. A pound of coffee at 14% moisture may not be as good a value as the same coffee at 10% moisture, especially in large quantities.\nRoasters also need to know the moisture level in green coffee, prior to roasting. Roast profile software is now standard with most commercial roasters, but if you apply the same program to samples with different moisture levels, the resulting product will be different. To attain true repeatiblility, all variables need to be eliminated.\nThe official method for determining moisture levels in green coffee, is carried out by heating a sample of whole beans in an oven, until the beans are dehydrated. The test takes up to 24 hours to complete, because the oven must be kept at a low temperature (105 c) , in order to help prevent the loss of volatiles (oils and other organic compounds). Some volatiles will inadvertently get burned off however, which is why the method is called loss of mass, as opposed to loss of moisture.\nSince the Oven Method is too cumbersome to be practical, for routine testing of moisture, other methods are generally used.\nThe most versatile oven alternative method (CAN BE USED FOR ROASTED AND INSTANT COFFEE TOO), is thermo-gravimetrics. Here the sample is heated, and weighed simultaneously, in a moisture balance.\nThere are different heat sources, the most common being halogen. This inexpensive PMC 50 Moisture Balance, will accept about 6-7 grams of sample. Since the sample is ground for analysis, when using a moisture balance, any thing more than 6-7 grams, gets piled too high for effective drying, on a 3 inch diameter pan.\nMOISTURE DETERMINATION IN COFFEE\nLow cost field analyzers like this COFFEE PRO MOISTURE MAC are an inexpensive, but effective way, to ensure coffee moisture levels are adequate for shipping Moisture levels of between 8 and 13% are recommended for safe transportation and storage of coffee. (Clarke, 1985; Reh, Gerber, Prodolliet, & & Vuatez, 2006)\nTHE SINAR SPEAR MOISTURE ANALYZER, TRANSMITS A SMALL RF FIELD THE SIZE OF A SOCCER BALL, AND USES THAT AS THE ASSUMPTION OF THE SAMPLES MASS. THE SAME TECHNOLOGY IS USED FOR THE SINAR DRYPRO INLINE MOISTURE ANALYZER\nBOTH ARE CAPABLE OF TRANSMITTING CONTINUOUS MOISTURE MEASUREMENT\nALL RIGHTS RESERVED\n589 Rappahannock Drive WhiteStone Va 22578\nTel (804) 435-5522 Toll Free (866) 244-1578 Fax (703) 991-7133\nBY THE OPERATOR. DEPENDING ON THE ACCURACY REQUIREMENTS OF THE TEST, AN ANALYSIS CAN USUALLY BE COMPLETED IN 10-15 MINUTES. HIGHER TEMPERATURES CAN BE USED, IF GLASS FIBER PADS ARE EMPLOYED TO HELP PROTECT THE SAMPLE FROM SURFACE BURN.\nBECAUSE ROASTED COFFEE IS DARK,\nAND ALL COFFEE CONTAINS VOLATILES,\nCARE HAS TO BE TAKEN NOT TO BURN\nTHE SAMPLE, WHICH RESULTS IN FALSE RESULTS.\nTHESE GLASS FIBER PADS ALLOW THE\nOPERATOR TO USE HIGHER TEMPERATURES, THEREBY REDUCING THE TIME REQUIRED FOR A TEST.\nLOSS ON DRYING\nThe most popular method for routine testing of moisture in agricultural commodities, takes advantage of the electrical properties of water, to provide a usably accurate reading, with tolerances ranging between.1%, and .5%.\nHere, a small rf signal is transmitted in to the sample, and the amount of energy retained in the sample (CAPACITANCE) is measured, which directly correlates to the amount of present moisture.\nBesides speed, (1-10 Seconds per test) one of the primary benefits in using the capacitance method to measure coffee moisture, is that the sample generally requires no preparation, which saves time and money.\nUNIFORM PACKING DENSITY IS A REQUIREMENT FOR USING CAPACITANCE TO SUCCESSFULLY MEASURE A COMMODITIES MOISTURE. THIS COFFEE PRO MOISTURE MAC, USES A FIXED VOLUME SAMPLE CELL TO DETERMINE THE SAMPLES MASS, AND A COMPRESSION CAP TO FACILITATE UNIFORM DENSITY. BECAUSE NOT ALL COFFEE BEANS ARE THE SAME SIZE, AND THEREFORE HAVE SLIGHTLY DIFFERENT PACKING DENSITIES, THE PUBLISHED TOLERANCES FOR THIS TYPE OF COFFEE MOISTURE ANALYZER,, IS +/- .5%\nTHE SINAR AP 6060 COFFEE MOISTURE ANALYZER, USES A BALANCE TO DETERMINE MASS.\nBECAUSE IT DOES NOT USE A COMPRESSION CAP,\nSAMPLES LIKE ROASTED WHOLE BEAN COFFEE, AND ROASTED GROUND COFFEE, CAN BE ANALYZED.\nGROUND COFFEE IS SEPARATED IN TO COARSE, MEDIUM, AND FINE, TO ALLOW FOR DIFFERENT DENSITIES.\nMOISTURE, PARTICLE CHARACTERIZATION, COLOR, AND SAMPLING","How to store maize grains for long without toxic preservatives.\nPoorly stored food is vulnerable to moisture and storage pests such as weevils, grain borers and rodents.\nHow can farmers and traders store their dry cereals like maize or beans cheaply and guarantee its safety? In this article, we will focus on three solutions for storing grains for long without preservatives. You can use hermetic bags, air tight containers or mini silos.\nHermetic storage bags\nUse of hermetic storage bags is an emerging trends in food waste management. It can preserve dried maize, beans, cowpeas, green grams, sorghum, millet and other cereals without the use of insecticides. This cheap technology is ideal for farmers and traders in Kenya who produce or purchase and store maize for later use in the season.\nThe bag has two layers for better resistance against moisture, air and pests. Pack your grains into the innermost air-tight plastic bag. Tie it tight with an unbreakable string or wire to make it air or moisture proof. The sealed bag can reduce oxygen levels from 21% to 5%. Lack of oxygen can reduce live insects like weevils to less than 1 insect/kg of grain. It also and creates unfavorable conditions for pest multiplication.\nWhere to buy hermetic bags in Kenya\nYou can buy hermetic bags from local shops or import them by ordering online.\nThe following brands are available for sale in all leading Kenya towns like Mombasa, Nairobi, Nakuru and Eldoret.\n- Purdue Improved crop storage (PICS),\n- Grain Pro\n- Super Grain\n- Zero fly\n- Agro Z\n- Elite bag\nConsumers can get the Kraft Paper Flat Base Hermetic Bag- Zipper Pouches for packing powder like flour and other grains. If you prefer something you can re-use, go for the multifunctional Reusable Silicone Food Storage Bags, they are leak proof, extra thick Hermetic Reusable Bags.\nAgcenture does not approve any brand superior to others. The efficiency of the bag will depend on you following the supplier instructions.\nPros of using hermetic technology\n- Store grains for up to 24 months (2 years) while keeping its qualities such as flavor and moisture loss.\n- The bags are reusable.\n- Eliminates need for pesticides or fumigants to treat the stored rice, corn, beans, paddy, milled rice, and other cereal crops. making it fit human consumption.\n- Cheaper compared to metallic or plastic silos. The price of a 50-100 kg hermetic bag retails at around Ksh 200 in Kenya.\n- It increases your farming income as you You can sell grains when prices are high during the shortage season.\n- Seed viability is greatly extended with hermetic storage.\nMini silos are air tight storage canisters. They are ideal for storing cereals for domestic use. They are made of plastic, wood, glass or stainless steel. They are available in different capacities like 1500kg, 1200kg, 1000kg, 700kg, 500kg, 350 kg, 250 kg and 100 kg. You can buy one that suits your needs.\nThey can keep your cereals safe from leaking roofs, a rodent’s invasion and torn packaging bags. In this article we look at steel and plastic ones.\nAir tight steel Canisters\nThe metallic ones are made of galvanized steel sheet, 0.5 mm thick and soldered with tin to be purely hermetic. A silo has two openings; on top to fill the grain and an outlet on the side. The silos’ openings are closed with a lid which has rubber band seals.\nAlways place the silos in a protected area from rain and sun in a granary, storage room or a room inside the house.\nDo not place them directly on the floor but place it on a wooden platform at least 15 cm above the ground level.\nPros of using metallic mini-silos.\n- Long lifespan of up to 20 years.\n- They have variety and are affordable to various people as it comes in different capacities\n- You can store grains for over one (1) year ensuring food availability throughout the year.\n- Reduces your food losses by up to 15% rate and you can enjoy a stored market gain of around 50%.\n- It increases your farming income as you can store and sell products when prices are high during the lean season.\nStore shelled, clean and sun-dried grains. Treat the grains with a fumigant before putting it in the silo. You should check the grain condition from time to time.\nAirtight Grain containers\nAre appropriate for consumers at home for storing grains (rice, grams, and beans), flour and other dry food components at home.\nThey cut down on food loss and wastage by keeping it dry and safe. Plastic containers are available in various shapes, makes and sizes.\nContainers are a great addition in your food storage, deco and kitchen arrangement. Some are transparent, allowing you to check your food’s condition without opening. You can get them in sets of 3, 4, 6 or 10 containers.\nWhere to buy the best airtight grain containers\nYou can find airtight food containers for sale in your local stores. If you cannot find a desirable make or size there, look for one in a leading online store like Amazon or eBay.\nThese are ideal for bulk grain handlers like maize wholesalers. They can hold 5 to 300 metric tons of produce. You will use them together with gunny bags or the common sacks. The one shown above, consists of two plastic halves. They are joined by an air-tight zipper after the cocoon is loaded with sacks of the commodity to be stored.\nCocoons work like hermetic bags by limiting aeration. Besides, they can also keep grains safe from floods if water does not reach the zipper line.\nHow to dry your Maize grains adequately before storage\nMoisture causes cereals and pulses like maize and beans to decay, get decolorized or contaminated by the deadly poison like aflatoxin . It can happen if it was poorly dried before storage or exposed while on the farm, during transit or storage.\nBefore storing grains in a hermetic bag, metallic silos or airtight containers ensure you have achieved the right moisture content of 12 to 14 percent for most grains.\nGrains can be hygienically sun-dried for around 3 days. You can also use a commercial dryer (heater air, in store drying or grain cooling) either on-farm or offsite. While sun drying, ensure you dry seeds on top of waterproof canvases.\nDo not mix grains maintained at different MCs to avoid seed cracking. To check on the grain’s moisture content (MC). Use a handheld grain moisture or digital testers to avoid over drying and decrease your yield weight."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:b3862f51-2177-4592-ab33-1c77afc143a9>","<urn:uuid:f55f8a3d-c045-40bb-aee4-f958d61b3496>"],"error":null}
{"question":"What are the nutritional requirements for school meals in terms of meeting students' growth needs, and how does poor nutrition impact learning outcomes?","answer":"School meals must meet specific nutritional requirements including proper amounts of calories, fat, saturated fat, trans fat, and fiber as per USDA guidelines. Students must receive adequate vitamin and mineral intake through a variety of fresh fruits and vegetables. A balanced meal should include fruits, grains, vegetables, protein, and dairy to meet adolescents' growth needs during development stages. As for learning impact, poor nutrition leads to decreased energy and reduced interest in learning. Research shows that proper nutrition, including protein, unsaturated fats, complex carbohydrates, and essential minerals like iron and potassium, is crucial for optimal brain function, focus, memory, and emotion regulation. Providing nutritious meals, particularly breakfast, has been associated with lower tardiness rates, fewer disciplinary referrals, improved attendance, and better math and reading test scores.","context":["Welcome to Child Nutrition and Wellness for Ohio County Schools!\nOur department has over forty employees that work hard to ensure the children in our schools are fed nutritious meals each day. Ohio County Schools take part in the National School Breakfast and the National School Lunch Program which promote healthy food choices. Students are served the proper amount of calories for their age and the amounts of fat, saturated fat, trans fat, and fiber are kept within guidelines set forth from USDA. A wide variety of fresh fruits and vegetables are also offered on a regular basis to ensure adequate vitamin and mineral intake as well.\nStudents in all 13 of our schools are participating in the Community Eligibility Provision Program of USDA. This program allows every student to receive one free breakfast meal and one free lunch meal each day in our schools free of charge. There is no need to complete free/reduced meal applications to receive the free meals.\nPlease contact the Child Nutrition Office with any questions:\nRenee Griffin, M.S., R.D., L.D., Director 304-243-0477\nLisa Mamakos, Child Nutrition Secretary 304-243-0486\nBreakfast and Lunch Information\nAll Ohio County Schools will provide breakfast and lunch daily. Every student may get ONE free breakfast meal and ONE free lunch meal each day. Only high school students may receive a second breakfast or a second lunch meal by paying cash (second meals CANNOT be charged). Extra milk or milk only may be purchased at all schools for a price of 40 cents per eight ounce bottle. Milk sales CANNOT be charged. Staff and adult visitors may purchase a breakfast meal for $3.00 or a lunch meal for $4.00.\nMany of our schools offer a Second Chance Breakfast, a Grab N Go Breakfast, or a Breakfast After the Bell in order to comply with the WV Feed to Achieve Act making breakfast part of the school day, in addition to a before school breakfast. Please check with your school for the specific times breakfast is offered.\nFor lunch, elementary schools offer an entrée daily along with other sides including fruits and vegetables and a variety of low fat milk selections and water. Additional fresh fruits and/or vegetables are also available in elementary school cafeterias. Middle Schools offer a hot entrée daily along with sides including fruits and vegetables in addition to a Garden Bar and a cold sandwich offering. A variety of low fat milk options and water are available daily in middle school cafeterias. The high school offers one or two hot entrée choices daily along with a wide variety of entrée salads, wrap meals, and a plentiful selection of fresh fruits and vegetables each day. Vegetarian options such as hummus and cottage cheese are also available daily.\nPlease note that in all schools for breakfast, students MUST select at least one serving of fruit (or vegetable if offered) with the breakfast meal. In the middle and high school cafeterias, students MUST take at least one serving of fruit or vegetable of their choice on the tray.\nInformation and Resources\nCivil Rights Information\n- Policy 2025: Ohio County Schools Rights Policy\n- Policy 2025.01: Ohio County Schools Civil Right Complaint Procedure\n- USDA Non-Discrimination Statement\n- USDA Program Discrimination Complaint Form\nNutrition Newsletters and Resources\nSchool Nutrition Standards\nWVDE Child Nutrition homepage: http://wvde.state.wv.us/child-nutrition/\nFoods sold, served or distributed outside of the school cafeteria must meet the nutrient requirements for West Virginia schools. This includes foods sold in vending machines and school stores, served during classroom parties, and provided by parents for snacks. The West Virginia Board of Education enacted rules for these \"other foods\" for three reasons:\n- Snacks can help your child's meet their nutrition needs instead of adding unnecessary (and unhealthy) fat, sugar and salt;\n- Food safety concerns; and\n- Allergies and/or special diets required by some children.\nLink to the Nutrition Calculator: http://wvde.state.wv.us/child-nutrition/tools/calculator.html\nClick here for more OCS Wellness Information.\nSports Nutrition Resources\nSchool Party Ideas -- Let's Party Book","By: Sarah Yi — Editor-in-Chief\nNon-nutritious and prepackaged school lunches have large effects on child and adolescent development such as obesity and less energy and interest in learning. It is crucial that children are provided with healthy and nutritious school lunches as it leads to better mental, physical, and emotional health both in and out of the classroom.\nNon-nutritious pre-packaged school lunches lead to a high obesity rate in children. According to the Centers for Disease Control and Prevention (CDC), “obesity prevalence was 13.4% among 2- to 5-year-olds, 20.3% among 6- to 11-year-olds, and 21.2% among 12- to 19-year-olds. Childhood obesity is also more common among certain populations.” Obesity rates have increased significantly through the years therefore if healthier school lunches are served, these statistics could potentially be reduced. A student’s school lunch is one out of the three important meals of the day; therefore it should be full of nutrients and should be filling as a childs’ growth spurt usually happens during these stages of development. According to Johns Hopkins Medicine, many adolescents experience a growth spurt and an increase in appetite and need healthy foods to meet their growth needs. Meeting daily requirements of balanced meals including fruits, grains, vegetables, protein, and dairy allows the body to grow and build strong. Factory packaged pancakes and muffins are full of sugar and do not meet an adolescent’s growth needs.\nUnhealthy foods provided by the school lead to less energy and less interest in learning. According to the Food Research and Action Center, providing students with breakfast in the classroom is associated with lower rates of tardiness, fewer disciplinary office referrals, improved attendance rates, and improved math and reading achievement test scores. Keeping the stomach full increases attentiveness among students. These nutrients send signals to the body and affect the way students learn and their attention span in the classroom. According to the National Institute for Student-Centered Education, the brain needs a variety of nutrients to be able to function optimally. To focus, remember and regulate our emotions we need protein, unsaturated fats, complex carbohydrates and sugars (in grains, fruits, and vegetables), as well as a host of trace elements such as iron, potassium, and selenium. With a good mix of protein, unsaturated fats, complex carbohydrates, sugars, including many vitamins and minerals, the brain will function to its full potential during the school day. With that said, school lunches must meet all requirements because as it lacks, so does the students’ brain function.\nAlthough healthy lunches are usually not as appealing as frozen cheeseburgers and greasy pizza, they are beneficial overall. Most healthy lunches are not as good tasting and often the textures are usually off. This leads to an increase in food waste. When students realize it doesn’t taste as good as unhealthy packaged food, the high quality, and more expensive healthy food is wasted and thrown away. According to Christopher Wanjek, author of Food at Work and Bad Medicine, recent studies have shown that claims of food waste may be inflated. At a young age, many may not think about what they are supposed to eat versus what they aren’t. If they find something not tasting as good as an unhealthier version, they are pickier and more likely to throw it out, wasting a perfectly edible and healthy meal.\nA solution we must consider should be making healthy options that are also appealing to students. Kids voted on healthy school lunches that are appealing in Fitchburg, Massachusetts. According to the Commonwealth of Massachusetts, Department of Public Health, winning items added to the lunch menu include a banana split (banana cut length-wise and topped with cut fresh fruit), veggie kabob, whole wheat pita pizza, and yogurt parfaits. These items were chosen by the students and were added to the menu because they were appealing. Having students try which healthy items they prefer will lead to less food waste and healthier options. Allowing students to voice their opinions about new options ensures that the picky eaters and fast food lovers will choose these items as well. If we don’t fix this problem, the growth and development of our future generation, and learning, will be affected negatively."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:daf10fdc-8fa2-4031-b3be-154068603e04>","<urn:uuid:6ea46ec3-e40a-4fee-a409-a275c51f3bf0>"],"error":null}
{"question":"How does the Fukushima disaster's impact manifest differently in poetry and scientific research?","answer":"The Fukushima disaster's impact is reflected differently in poetry and scientific studies. In poetry, it appears in the award-winning haiku that speaks of 'lingering grief' and traces of Fukushima in salmon, which juror Terry Ann Carter praised for expressing 'contemporary sadness' about continuing pollution damage. In scientific research, the impact is documented through concrete evidence: researchers found that pale blue grass butterflies fed with leaves from areas around Fukushima showed measurable negative effects, including shorter lifespans and morphological abnormalities. The scientific study also revealed that while these effects could be passed down to offspring fed contaminated food, they could be minimized if the next generation was given uncontaminated food.","context":["Congratulations to Jacquie Pearce, the winner of our inaugural National Haiku Contest! Our guest jurors Terry Ann Carter and Naomi Beth Wakan selected two haiku by Jacquie Pearce as co-winning poems of the inaugural contest — the most delightful part of this? All 500+ haiku we received were judged blind by a first reader, and selected blind by our two jurors. And still, the two winning haiku are by the same amazing poet!\nThank you to all the poets who sent us their beautiful tiny poems. We were truly overwhelmed with the enthusiasm for this contest, and we can’t wait to run it again for National Haiku Writing Month 2019.\nWithout further ado, we are thrilled to present you with Jacquie Pearce’s two winning poems, as well as a few honorable mentions selected by our generous jurors:\nafter the rain\nmy daughter jumps into\neach piece of the sky\n“[This] is a haiku that beautifully combines the ‘small picture’ (the jumping daughter) and the ‘cosmic picture’ (the sky reflected in the puddles, which ends up in pieces because of the disturbance of the water),” says juror Terry Ann Carter. “The puddle is never actually mentioned, but we understand this, through the first line. There is a joyful hopefulness emanating from this haiku; a child who is allowed to express herself by jumping in puddles, will one day solve some of the world’s problems. An intimacy between a mother who is observing and a daughter who is playing.”\nThe poet herself also shares the beauitful moments that inspired her to create this winning haiku:\n“When my daughter was little, she loved rain and puddles. One day, when she was in kindergarten or grade one, it was pouring rain while I walked to the school to pick her up. The whole way there, I complained to myself, thinking what a lousy, wet day it was. Then, my daughter stepped out of the school and exclaimed with huge delight, ‘It’s raining!’ She felt such joy in response to the rain, that it turned my own thinking on its head, and helped me to see the world with a renewed sense of wonder. That’s what haiku does as well. Approaching life with a ‘haiku mind’ helps me notice little things around me and see ordinary moments with a sense of newness and significance.”\nlingering grief . . .\na trace of Fukushima\nin the salmon\nJuror Terry Ann Carter praised this poem for how it “expresses a contemporary sadness by pointing to the continuing pollution damage from the fall out of the recent Fukishima nuclear accident. It is a global concern. Haiku are usually not known for their ‘messages’ but this poem is simply (and profoundly) stating a truth; an understanding in the moment, that deeply affects us all. Michael Dylan Welch, respected haiku poet, scholar and activist, speaks of the continuing moment, which is applied here with the use of ellipsis. Darkly poignant. Of our time.“\nNaomi Beth Wakan also weighed in on their final two selections: “It was hard to choose between the two winning haiku because they were both so excellent in their own way. The one describing the child jumping in puddles, and the mother so simply describing the sky reflected in the puddles was such a joyful one reflecting the promise of youth. Yet the one reminding us that in all our actions, even that of eating a piece of salmon, are moral and political issues that must be considered, presented a bitter truth. So bitter and sweet, I felt, were both necessary to be present in my choice. Each haiku presented a ‘naked’ image which gave rise to so many ideas and feelings, and that, I feel, is the essence of good haiku writing.”\nJacquie Pearce grew up on Vancouver Island. She has published poetry, short non-fiction and several novels for children. Her haiku have won awards and appeared in a variety of publications, including the Haiku Canada Review, Frogpond, The Red Moon Anthology and Of Skin on Skin, an anthology of erotic haiku. (Blog: www.wildink.wordpress.com; website: www.jacquelinepearce.ca\ntwo men talk\nBy Dorothy Mahoney | Dorothy Mahoney’s book, Off-Leash (Palimpsest Press, 2016)) contains several haibun. Palimpsest will publish her next book in 2020.\nabandoned bird nest\nour empty spaces\nBy Marianne Paul | Marianne Paul’s haiku have appeared in various publications, including (among others) the Literary Review of Canada. She has won the Vancouver Cherry Blossom Festival haiku invitational contest (Canada division) and the Jane Reichhold Memorial Haiga Competition (mixed media category). She is the author of the novel Tending Memory and the poetry collection Above and Below the Waterline, published by BookLand Press. Marianne is a member of Haiku Canada.\na student globe\nin the attic\nthe world I once knew\nBy Beth Skala | Beth Skala is a founding member of the writers’ group Pens Ultimate Nanaimo. She is also a member of the Federation of BC Writers and Haiku Canada. Her poetry has been published in anthologies across Canada, and she has self-published two chapbooks on the poetry of ageing: Not For Sissies and Puttin’ on the Glam. Her memoir unSUPERvised: growing up in the 1950s is a collection of haibun (prose plus haiku). Beth is the only haiku poet to have won Best BC Haiku three times in the annual Vancouver Cherry Blossom Festival Haiku Invitational (2011, 2016, 2017).","Food affected by Fukushima disaster harms animals, even at low-levels of radiation\nButterflies eating food collected from cities around the Fukushima nuclear meltdown site showed higher rates of death and disease, according to a study published in the open access journal BMC Evolutionary Biology.\nResearchers fed groups of pale blue grass butterflies (Zizeeria maha) leaves from six different areas at varying distance from the disaster site, and then investigated the effects on the next generation. Feeding offspring the same contaminated leaves as their parents magnified the effects of the radiation. But offspring fed uncontaminated leaves were mostly like normal butterflies, and the authors say this shows that decontaminating the food source can save the next generation.\nThe 2011 meltdown at the Fukushima Dai-ichi nuclear power plant released substantial amounts of radiation into the surrounding area. Humans were evacuated, and no significant health effects have been reported, but the scientists from the University of the Rukyus, Okinawa, Japan, are studying the impact on the area’s wildlife.\nIn a previous study, the group suggested that eating leaves with high levels of radiation seriously affected the pale grass blue butterfly. Their new study investigated the effect of eating leaves with much lower levels of radiation, which had been collected in 2012, a year after the disaster, from six areas that were 59km to 1760km from the site.\nTheir study showed that even in these comparatively low levels of radiation, there was an observable difference in the butterflies’ lifespan, depending on the dose of caesium radiation in their food, which ranged from 0.2 to 161bq/kg. For comparison, leaves collected in the months after the disaster around 20km from the site had radiation in the thousands of Bq/kg. Butterflies fed leaves with higher caesium radiation doses were also smaller and some had morphological abnormalities such as unusually shaped wings.\nProfessor Joji Otaki, University of Rukyus, says: “Wildlife has probably been damaged even at relatively low doses of radiation, and our research showed that sensitivity varies among individuals within a species.”\nIn the second part of the experiment, the researchers looked at the next generation of butterflies. These were split into groups fed an uncontaminated diet, and those fed the same diets as their parents.\nThe offspring fed an uncontaminated diet had a similar lifespan, irrespective of the amount of radiation their parents had been exposed to. The only effect seemed to be that those whose parents had been exposed to higher caesium diets had smaller forewings. But those fed the same contaminated diet as their parents showed magnified effects.\nThe authors say that this shows that the effects of eating contaminated food can be significant, and that they can be passed on, but are minimized if the next generation have an unaffected diet.\nProfessor Otaki says: “Our study demonstrated that eating contaminated foods could cause serious negative effects on organisms. Such negative effects may be passed down the generations. On the bright side, eating non-contaminated food improves the negative effects, even in the next generation.”\n- ENDS -\nT: +44 (0)20 3192 2054\nNotes to Editor\nIngestion of radioactively contaminated diets for two generations in the pale grass blue butterfly\nChiyo Nohara, Wataru Taira, Atsuki Hiyama, Akira Tanahara, Toshihiro Takatsuji and Joji M. Otaki\nBMC Evolutionary Biology 2014, 14: 193\nAfter embargo, article available at journal website here:\nPlease name the journal in any story you write. If you are writing for the web, please link to the article. All articles are available free of charge, according to BioMed Central's open access policies\nBMC Evolutionary Biology is an open access, peer-reviewed journal that considers articles on all aspects of molecular and non-molecular evolution of all organisms, as well as phylogenetics and palaeontology.\nBioMed Central (http://www.biomedcentral.com/) is an STM (Science, Technology and Medicine) publisher which has pioneered the open access publishing model. All peer-reviewed research articles published by BioMed Central are made immediately and freely accessible online, and are licensed to allow redistribution and reuse. BioMed Central is part of Springer Science+Business Media, a leading global publisher in the STM sector."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:2e1dad2c-18e3-4518-b334-53b03466017a>","<urn:uuid:f7a09878-c90a-4cd7-a38f-e6c60a29ac37>"],"error":null}
{"question":"As a researcher studying global health security, I'm interested in understanding how infectious disease outbreaks intersect with medical waste management. What are the current approaches to preparing for vector-borne disease outbreaks in the Netherlands, and what environmental challenges do medical waste disposal methods pose in managing these outbreaks?","answer":"The Netherlands is taking a comprehensive approach to preparing for vector-borne disease outbreaks through a 10 million euro research initiative. The research involves 25 PhD researchers focusing on four key themes: climate changes, water management, agricultural methods, and international travel/import risks. They aim to develop a 'weather forecast' system for outbreak risks, particularly focusing on diseases transmitted by mosquitoes. The project involves citizen science and collaboration with multiple organizations. However, the management of medical waste from such outbreaks poses significant environmental challenges. Medical waste disposal methods can create environmental issues through airborne pollutants, radioactive contamination, groundwater pollution, and wildlife hazards. Even after proper disposal, methods like incineration and autoclaving produce CO2 emissions. The disposal of potentially infectious medical waste requires careful handling, with strict requirements for storage, transport, and processing to prevent environmental contamination.","context":["10 million for One Health research in the Netherlands\nRotterdam, 12 June 2019 – Coming five years, Erasmus MC, together with partners from the Netherlands Centre for One Health, will investigate how the Netherlands can be better prepared for infectious diseases transmitted by mosquitoes. The Dutch Research Council (NWO) has awarded almost 9 million euros for this research. This amount has been supplemented to 10 million euros in contributions from seven collaborating public organisations. The multidisciplinary character of the collaboration, in which citizen science will play a role, is unique.\nOutbreaks of (new) infectious diseases in humans and animals are becoming more prevalent worldwide. That is due to various factors, such as population growth, international trade, international travel and climate change. In the Netherlands, a relatively large number of people, livestock and animals live near each other. In combination with our water-rich landscape and busy international trade and travel, it makes us vulnerable to outbreaks of infectious diseases.\nProfessor Marion Koopmans, virologist at Erasmus MC and one of the Scientific Directors of the Netherlands Centre for One Health: ‘Large disease outbreaks are thankfully quite rare. However, if such an outbreak occurs, we only investigate it from that moment onwards, which means we are always chasing after the facts. However, given our changing world, we need to be ready for more frequent infectious disease outbreaks, also in Europe. As the health of people, animals and the environment is interrelated, the most effective approach is to consolidate our strengths by collaborating with partners from different disciplines. By working together, we will be better prepared for the future.’\nFigure: various global changes affect the human, animal, and environment ecosystem making new outbreaks of existing and new virus diseases possible. © Marion Koopmans / Frank Deege\nThe team will mainly focus on vector-borne diseases: infectious diseases transmitted by insects such as mosquitoes. As a result of climate change, exotic mosquito species are becoming more common in the Netherlands. But under the right conditions, mosquito species native to the Netherlands can transmit (tropical) viruses too. The recent outbreak of the usutu virus (the “blackbird disease”) among birds demonstrates the importance of early preparedness for such diseases. That applies not only to the Netherlands but also the Dutch Caribbean and the rest of Europe.\nOutbreaks can arise due to a combination of factors. Over the next five years, 25 PhD researchers will focus on four themes that influence the development of outbreaks:\n- changes in the climate,\n- changes in water management,\n- changes in agricultural methods, and\n- changes concerning international travel and import risks\nThey will investigate the impact of changes in the climate, water management, agricultural methods and import risks on the probability of a vector-borne virus outbreak in the Netherlands. Through collaboration with researchers from the National Institute for Public Health and the Environment (RIVM), the Netherlands Food and Consumer Product Safety Authority (NVWA), and the blood banks in the Netherlands and the Dutch Caribbean, the outcomes will be translated into measures to ensure we are better prepared for a possible disease outbreak. ‘Ultimately we want to develop a sort of “weather forecast” for the risk of outbreaks’, says Marion Koopmans.\nDuring the research, the team will collaborate with various scientific bodies and make use of research results from other projects. These will include citizen science projects: initiatives in which citizens and high school pupils are involved. For example, they will provide research data about birds, mosquitoes and water or use travel apps such as the Municipal Health Services’ “GGD reist mee” and the “ZIeKA-monitor”.\nCoordination: Erasmus MC\nCollaborative partners: Avans University of Applied Sciences, Leiden University Medical Center, Leiden University/Naturalis, Netherlands Institute of Ecology (NIOO-KNAW), Radboud University Medical Centre, University Medical Center Utrecht, Utrecht University, Wageningen University & Research\nCo-funding partners: Deltares, National Institute for Public Health and the Environment (RIVM), Royal Netherlands Meteorological Institute (KNMI), Red Cross Blood Bank Foundation Curacao, Sanquin, Technasium Foundation, Centre for Monitoring of Vectors (CMV)\nInternational collaborating partner CEAB-CSIC: Centre for Advanced Studies of Blanes (CEAB) a research institute within the Superior Council of Scientific Investigations (CSIC)\nNetherlands Centre for One Health\nNCOH is a collaboration of 9 Dutch academic research institutes focussing on One Health research. It aims to realise an integrated approach to the global risks of infectious diseases and to create sustainable solutions for major societal challenges in the areas of human, animal, and ecosystems health.\nMore information: ncoh.nl/research","Medical Waste Disposal Questions & Answers\nWhat is considered medical waste?\nAny kind of waste that contains pathogens that can cause disease. It is inclusive of waste produced by hospitals, surgi-centers, medical and dental offices, labs dialysis centers, plasma centers and veterinarian practices.\nWhat are the most common methods of disposing of medical waste?\nThere are four major categories for medical waste disposal. All medical waste must be regulated and be in compliance with Federal and State laws.\n1. General Medical Waste produced in medical offices such as paper, plastics and regular office waste all of which may be disposed in regular waste containers.\n2. Infectious Medical Waste, this type of waste may contain or propose infection that would be harmful to humans, animals and the environment. Infectious waste has strict requirements for storage, transport, disposal, licensing, and processing.\n3. Hazardous Medical Waste poses threat of infection inclusive of sharps that have not been or have been used, because they have the ability to puncture or harm the user. Chemotherapy agents, as well as chemicals, such as solvents, mercury in thermometers, and lead in paint.\n4. Radioactive Medical waste inclusive of all sharps that have not been used, because they have the ability to puncture or harm the user. Chemotherapy agents fall into this category, as well as chemicals, such as solvents, mercury in thermometers, and lead in paint.\nWho is responsible for the disposal of the medical waste after it has been taken away from the producers of the waste?\nIn 1988, Congress enacted the Medical Waste Tracking Act of 1988, a United States federal law that addressed the handling and disposal of medical waste in coastal areas. It was designed primarily to monitor the treatment of medical wastes through their creation, transportation and destruction, i.e. from \"cradle-to-grave.\" Four requirements were primarily identified; first, to provide a means of monitoring \"the transportation of waste from the generator to the disposal facility\" unless said waste had previously been incinerated. Secondly, to be able to ensure the \"generator of the waste\" that the waste had been \"received by the disposal facility.\" Next to develop a uniform form for the tracking of materials across states and finally to develop a means to label and contain the wastes for the safety of the handlers.\nHow does medical waste effect the environment?\nMedical waste airborne pollutants can effect our environment; radioactive pollutants, ground water pollutants and wildlife hazards posed by pharmaceutical ingestion and pricks from sharps (needles) if not disposed of properly also pose a huge problem. Most methods of disposal that include incineration and autoclaving produce c02 emissions even after proper disposal.\nCan medical waste be recycled?\nUsually medical waste is sterilized and then disposed in a sanitary landfill. There are other items that can easily be recycled including plastic items, such as IV bags, syringes, dialysis tubes and plasma tubing, which can be treated, either by melting them down or by chemical sanitation, therefore sterilized, and now ready to be made anew.\nWhat do we do with 2 billion pounds of potentially infectious medical waste?\nThere are three primary treatment methods used to dispose of medical waste, each uses either heat, chemical reactions, or a combination of both to decontaminate bio hazardous wastes. All ending up in our landfills. In addition to disinfection, some states and landfills require that medical waste be shredded to make the contents unrecognizable.\nHow long can biomedical waste be stored?\nStorage of biomedical waste in a place other than at the generating facility shall not exceed 30 days. The 30-day storage period shall begin on the day the waste is collected from the generator.\nHow is medical waste collected?\nDue to recent regulation, biohazard waste disposal companies must come to pick up the waste and take it to a treatment facility to be rendered safe and non-hazardous.\nWhat is the importance of environmentally friendly medical waste disposal?\nEnvironmentally safe waste treatment centers not only use materials and processes that are eco-friendly, but they also help to significantly decrease the amount of toxic waste that leaks into the environment helping to preserve our earth.\nHow does segregation of medical waste help the environment?\nSegregation reduces the amount of waste that needs special handling and treatment and prevents the mixture of medical waste like sharps with the general waste. It also provides the opportunity of recycling, and reduces the cost of disposal and treatment.\nAre there ecofriendly ways to reduce medical waste?\nStart by proper segregation of the waste. Reduce the packaging size as warranted and reuse single use devises as long as it is in compliance and an option of your state and federal law. Have a medical waste management plan and last but not least, be sure to use a disposal management company that understands and is compliant with medical waste disposal laws."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:20b30eed-f44f-4455-be6b-c11aee9f72e3>","<urn:uuid:0fb89704-868b-4f0b-b86a-190af62e2ac4>"],"error":null}
{"question":"How do modern funding systems in education and EV charging infrastructure deployment both reveal systemic inequities, and what metrics are used to measure these disparities?","answer":"In traditional school funding systems, uneven per-pupil spending occurs across schools due to staff-based allocation rather than dollar-based allocation, leading to some schools receiving more resources than others. Similarly, in EV charging infrastructure, clear disparities exist based on multiple metrics: racial composition (majority-White areas are 2 times more likely to have chargers), education levels (areas above US median for bachelor's degree attainment are 3.55 times more likely to have chargers), and income levels (areas with above-median household income are 2.24 times more likely to have chargers). Both systems demonstrate how traditional allocation methods can perpetuate resource inequities, with measurable impacts on different communities' access to vital services.","context":["As much debate as discussions about weighted student funding generate, the basic concept is straightforward: Instead of allocating staff to schools and paying their costs, a district that uses a student-based funding model would simply allocate a set dollar amount to each school based on its actual mix of students. Each year, as the mix and number of students at each site changes, so would the allocation.\nSuch a system takes the mystery out of how districts allocate funds to each school and clarifies how funds move when enrollments shift. As students transfer from one setting to another, the funds designated for that student transfer as well.\nCompare & contrast: Traditional funding v. student-based funding\nLet’s compare a traditional funding model (based on total enrollment) with a student-based allocation model (based on actual number of students and their specific needs.)\nThe traditional staffing-based allocation system\nThe illustration below shows the budget of an actual Chicago school under a traditional staffing allocation system.\nFigure 1: Traditional budget allocation to one Chicago School. From John Myers, “CPS Budget Analysis: CPS Eyes Equity,” Catalyst Chicago, February 2005.\nIn this model, school districts (rather than school leaders) hold the purse strings, deciding what gets purchased for each school (i.e., a principal, some number of teachers, a counselor, etc.) Larger districts use staffing formulas to determine how many of each type of staff a school gets, often allocating a vice principal for schools with more than 400 students, and so on. What’s then spent at each school is the actual salaries of the staff assigned to each school. The district spends more on schools with more staff per pupil or higher salaried staff.\nNot surprisingly, this kind of allocation scheme – which allocates staff rather than dollars – tends to yield uneven per-pupil spending across schools.\nThe student-based allocation model\nNow let’s look at a model budget for the same school, recalibrated under a student-based allocation model:\nFigure 2: Student-based allocation based on actual student enrollment at the same Chicago school. From John Myers, “CPS Budget Analysis: CPS Eyes Equity,” Catalyst Chicago, February 2005.\nUnder the student-based funding model, dollars are allocated on the basis of enrollments for different types of students. This particular school would get an additional $3.5 million and change. (In the level funding environment created by this new model, some schools would also *lose* funds – a sticking point that often drives controversy.) In addition to the more equitable dollar allocations, school leaders also have greater latitude to ensure that staff and services address the real needs of the school’s individual students.\nReal world benefits of student-based funding\nFrom a practical standpoint, why would a superintendent want to consider student-based allocation?\n- Equitable access: Students and families rightly have an expectation of equitable access to district resources. A system that routinely directs more funds to some schools (often smaller or wealthier sites) flies in the face of this basic principle.\n- Portability: For school districts with choice-based assignment plans, funds are portable and follow individual children as they move to their preferred school.\n- Flexibility: Part and parcel of the student-based funding model is that principals are no longer bound to adhere to district staffing models. Building leaders can apply their dollars in ways that best meet the needs of their particular mix of students – setting the stage for innovation. For instance, under student-based funding, school leaders have the opportunity to implement models such as blended learning – which depend on the ability to invest in technology, lab monitors, and a host of other nontraditional expenditures – to drive better outcomes for their students.\n- Accountability: Districts with prescriptive staffing formulas can’t hold schools or principals accountable for the performance at their school, given that most of the resource decisions have been decided centrally. Furthermore, as schools use their flexibility to innovate and alter their delivery models, districts can ensure the financial viability of different school models by keeping per student (or per student type) costs comparable across all schools. Given the comparable funding, more successful models can be scaled across additional sites without draining undue funds from the district.\nIn the end, student-based allocation facilitates the goal of districtwide improvement: Funds follow students, thereby creating incentives for schools to attract students, keep full enrollment, and demonstrate excellent student performance. As families move their students to higher performing schools, the system begins to align to a continuous improvement cycle that drives ongoing performance gains at all schools.\nA transition to weighted student funding (a.k.a. student-based allocation) is one of the key actions that defines portfolio school districts. Read our occasional series on portfolio districts for more detail.\nMarguerite Roza, Ph.D., is Director of the Edunomics Lab at Georgetown University and Senior Research Affiliate at the Center on Reinventing Public Education. Dr. Roza’s research focuses on quantitative policy analysis, particularly in the area of education finance.","As electric vehicles (EVs) roll onto the roads in large volumes across the U.S, there has been a corresponding demand for more robust charging infrastructure. Despite the appealing environmental and economic benefits of EVs, the convenience of charging stations heavily influences adoption. And there begins the problem. While electric vehicle use is growing rapidly in well-to-do, predominantly White communities, minority neighborhoods have largely been left behind to date.\nThe U.S. government has emphasized the importance of equity when planning infrastructure investments in bills like the Build Back Better plan, and has incentivized a large portion of EV infrastructure funding in programs like NEVI and policies that seek to ensure EV charging infrastructure is deployed equitably.\nBut how well are these policies performing?\nTaking a granular approach to assess the current state of EV infrastructure\nIn an effort to understand the current state of equity in charger deployment, identify gaps that may exist, and add to the body of knowledge surrounding EV infrastructure deployment – we decided to leverage our vast data core, analytical expertise, and powerful software platform to conduct an analysis of Public EV Level 2 chargers in Columbus, Ohio.\nIn the sections below, we explore where access to EV chargers is most prevalent based on variables like population density, various socio-demographic statistics, and with different definitions of what constitutes a charging gap.\nEV Charging Infrastructure Basics\nFor EVs to achieve broad adoption and utilization, drivers need easy access to charging infrastructure. While many EV owners charge at home, people with longer commutes or irregular driving habits are unlikely to see themselves in EV ownership without excellent access to public charging stations.\nThere are three levels of charging equipment, determined by charging speed.\n- Level 1 (L1) – less than 2% of public EV chargers in the U.S. are L1.\n- Level 2 (L2) – the most common type of public EV chargers, accounting for more than 80% of public EV chargers in the U.S.\n- Level 3 (L3) – more than 15% of public EV chargers in the U.S. are L3.\nDue to the overwhelming preference for and majority of public charging stations being Level 2, we focused our analysis on L2 chargers.\nThe Landscape of EV Charging Infrastructure in Columbus\nAt first glance, you see that Columbus has Level 2 EV chargers spread across the city. A high concentration of chargers are located in the downtown area. So during our study, we took population density into consideration when drawing any conclusions regarding charger placement.\nRacial Factors and public EV Charger Locations\nCloser inspection of the distribution of public chargers in Columbus reveals disparities when comparing majority White areas to majority non-White areas. This is in line with other recent studies on public EV charging distribution. For example, Axios did an analysis of 35 U.S. cities and found that majority-White census tracts are 1.4 times as likely to have a charger.\nGiven UrbanFootprint’s unique ability to aggregate data across all census resolutions, we looked a level of granularity deeper, analyzing census block groups in Columbus using our Analyst application. It revealed that the EV charging locations in our study area are even more heavily skewed towards majority-White areas than what Axios had found in other cities around the country.\nIn Columbus, majority-White block groups are 2 times more likely to have a charger, and 2.3 times more likely to have at least three chargers.\nBut we knew there were likely other factors that may be more strongly correlated with the prevalence of charging stations than race and ethnicity.\nRelationship between EV Charger Presence and Educational Level\nOur study found an even stronger correlation between high education levels and the presence of EV chargers. It was immediately apparent from looking at the map that the median US educational attainment level (36% with at least a bachelor’s degree) was a tipping point for whether a block group would likely contain an L2 charger.\nTo be specific, around 80% of L2 chargers in our study were located in block groups with above the US median for bachelor’s degree attainment – and those block groups were 3.55 times as likely to contain a charger than those below the median.\nThe really shocking piece of this statistic is that these same block groups accounted for only 51% of population and only 40% of area.\nMedian Income and its Impact on EV Charger Distribution\nMedian income also appears to play a significant role in charger presence. Within our study area, block groups with chargers had median incomes 1.1 times higher than those without. Moreover, block groups with average incomes above the US median for household income ($68,703) were 2.24 times as likely to have a charger.\nThis finding supports the argument that historically, charger placement has favored higher income areas, and raises concerns for how access to EV infrastructure will lead to income-based disparities in future EV usage.\nEquitable Access to EV Chargers\nThe end goal of public EV charging infrastructure is to serve the public. That is, having enough chargers in a given block group to satisfy demand. We wanted to get an overall picture of who is being “served” versus “unserved” in Columbus. We chose 4 L2 chargers as a threshold for which to consider a block group “served.”\nWhen we looked at the data through this lens, all three ‘metrics’ (education level, median income, and racial composition) showed equity-related differences.\nConclusions and Recommendations\nOverall, our findings indicate that education level and median income are the most closely related metrics to the distribution of EV chargers in Columbus, Ohio. While racial factors are not quite as strong, there is still a trend for chargers to be more present in predominantly White block groups – in line with studies of other major cities across the country.\nAdditionally, the results were progressively more compelling as we peeled back all of the layers of data. For example, when limiting the analysis to the areas of the city with the highest population density, and increasing the threshold of what counts as a charging gap, the trends are magnified significantly.\nPolicymakers, local governments, utilities, and private companies should consider these findings when incentivizing, funding, planning, and placing future EV infrastructure. By focusing on ensuring equitable access to chargers, they can support the wider adoption of EVs. This includes prioritizing charger installation in diverse neighborhoods and areas with lower educational attainment and income levels.\nBy leveraging the right data at the intersections of climate, community, and the built environment, we can surface the actionable insights that will ensure a more equitable distribution of EV chargers – that will ultimately contribute significantly to higher EV adoption rates, pushing us closer towards a more resilient, sustainable, and inclusive future. Resilience Insights, when paired with our comprehensive Analyst application for data visualization, provide answers to many complex questions related to Infrastructure & Mobility for any location in the United States. Contact us if you want to learn more!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:04887f4b-0924-48fa-b974-4186538a63fd>","<urn:uuid:5fbf5a0f-b48f-437c-a774-25770944f520>"],"error":null}
{"question":"What are the essential tools for measuring conversion optimization, and what methodological precautions should be taken when testing website changes?","answer":"Essential conversion optimization tools fall into three categories: quantitative tools (like analytics and heatmaps) to track what is happening, qualitative tools (such as feedback tools and session recordings) to understand why things happen, and testing tools to measure improvements. When testing website changes, key methodological precautions include: ensuring sufficient sample sizes to avoid random fluctuation, examining all available data variables rather than just conversion rates, and testing a limited set of significant differences rather than too many variables at once. Additionally, when comparing versions, it's important to control for factors like device types and ensure even distribution of user segments across test variations.","context":["Last updated Sep 18 2020\nThe user-centric approach to Conversion Rate Optimization (CRO)\nForget about best practices and quick hacks: to increase website conversion rate, your best move is to understand your visitors, users, and customers, and give them what they need.\nIn this guide, we dispel a few myths about conversion optimization, talk to 20+ conversion experts, and give you a free, downloadable 3-step CRO plan to help you gather the data you need to improve your conversion rates over and over again.\nWhat is conversion rate optimization (CRO)?\nConversion rate optimization (CRO) is the practice of increasing the percentage of users who perform a desired action on a website. Desired actions can include purchasing a product, clicking ‘add to cart’, signing up for a service, filling out a form, or clicking on a link.\nA more user-centric definition of CRO\nStandard definitions of CRO, like the one we just wrote above, place their focus on conversion percentages, averages, and benchmarks. This emphasis on a numerical approach comes with a downside—the more you look at spreadsheets full of conversion data points and actions, the less you think of the individuals behind them.\nHere is an alternative, more holistic and user-centric way of defining CRO: think of it as the process of focusing on understanding what drives, stops, and persuades your users, so you can give them the best user experience possible—and that, in turn, is what makes them convert and ultimately improves your conversion rate.\nWhy do we think this second definition works better? Because focusing on the final action—the conversion—is obviously important, but in reality, a lot happens before that point:\n- Specific DRIVERS bring people to your website\n- Specific BARRIERS make them leave\n- Specific HOOKS persuade them to convert\nWhen you’re working to improve conversions, not every problem is quantifiable, backed by hard numbers, and with a clear-cut answer. Yes: sometimes, an obvious bug is blocking 80% of your users from doing something, and fixing that one bug will save your entire business; other times, your website functions perfectly and yet people still are not converting. When this happens, you’ll need to dig deeper to understand the why beyond the data you have—you’ll need, in other words, to focus on your users first. And that’s what we at Hotjar think CRO is actually about.\nWhether you own an ecommerce site or manage online marketing or SEO (search engine optimization), CRO will constantly be a top-of-mind topic to help your organization grow.\nHow to calculate conversion rate\nConversion rate is calculated by dividing the number of conversions (desired actions taken) by the total number of visitors and multiplying the result by 100 to get a percentage.\nFor example, if your web page had 18 sales and 450 visitors last month, your conversion rate is 18 divided by 450 (0.04), multiplied by 100 = 4%.\nWhat is the average conversion rate?\nDepending on what you read, the average conversion rate is anywhere between 1% and 4%.\nBut let us come out and say it: this figure is sort of meaningless, since:\n- Conversion rates differ wildly depending on the conversion goal (ad clicks, checkout completions, newsletter signups, etc.)\n- Every website, page, and audience is different\n- Most people don't share their conversion data publicly anyway\nAverages may be useful as starting points for benchmarking, but what do they really have to do with YOUR website?\nThere is no actual, ultimate industry figure you can rely on or compare yourself against with 100% confidence. Obsessing over an average percentage figure, and trying to squeeze as many conversions as possible just to stay in line with it, is not the best way to think about conversion rate optimization. Once again, you’re better off focusing on developing an in-depth understanding of what actually matters to your users, so you can give it to them—and then, conversions will naturally follow.\nConversion optimization best practices—and why they’re dangerous\nIn the world of digital marketing, a CRO best practice is a commonly-held belief that a particular optimization action will guarantee an increase in conversion rate, for example:\n- Use a strong color for all CTA (call-to-action) buttons\n- Place CTAs above the fold\n- Use urgency (e.g., time-limited offers) to drive sales\n- Always display testimonials\n- Use fewer form fields on your forms\nAre these best practices good for improving YOUR conversion rate? Debatable.\nFirst of all, best practice is—by definition—past practice: it's something that worked in the past for someone else. You can’t guarantee it’s going to work today.\nSecond, just because it worked for someone else, doesn’t mean it’s going to work for you.\nBlindly applying existing best practices puts companies in a perpetual state of playing catch-up—while more progressive and experimental businesses are busy improving and making the changes that will be recognized as 'best practices' in a few years.\n📚Useful extra reading: here is our philosophy about best practices, and how following them blindly can end up being counterproductive and hurt business.\nThere is, however, one core principle we can recommend as always valid: spend time understanding your users and customers—or, as we like to say around here at Hotjar, build a customer-centric culture by obsessing over your users and customers. They are the people who matter to your business and have the answers you need to improve it. Fixate on their needs and desired outcomes, learn as much as you can about their concerns and hesitations, and then deliver solutions that address them.\nIn the long term, what leads to growth is not blindly applying best practices that you see on other blogs or hear from your boss(es). The winning approach is investing in understanding and learning from your users and using the insight to build an optimization strategy that continuously improves your business.\nHow to do this in practice is covered in the 3-step CRO program chapter of this guide; before then, let’s take a rapid look at the tools that will get you there.\nThe best conversion rate optimization tools\nIt may sound weird coming from a company that sells a tool that helps people optimize websites—but, as a principle, we believe that the best optimization tools are free.\nYour brain, ears, eyes, and mouth are the primary tools you need to understand your customers, empathize with their experience, draw conclusions based on the data, and ultimately make the changes that improve your conversion rates.\n(Sidenote: here is a great example of what can happen when you don’t use these tools, make assumptions about what people need, and build something that nobody uses.)\nHow do you use these free tools?\n- Listen to what your users have to say about your website\n- Watch how people use your website\n- Immerse yourself in the market\n- Talk to whoever designed and built your site (and your product/service)\n- Speak to the staff that sell and support your product/service\n- Draw connections between different sources of feedback\nAll the other, traditional optimization tools are simply the means that help you do it. And they help in three ways:\n1. Quantitative tools to uncover what is happening\nQuantitative tools allow you to collect quantitative (numerical) data to track what is happening on your website. They include:\n- General analytics tools that track website traffic (e.g., Google Analytics)\n- Website heat map tools that aggregate the number of clicks, scrolls, and movement on a page\n- Funnel tools that measure when visitors drop off from a sales funnel\n- Form analysis tools that track form submissions\n- Customer Satisfaction (CSAT) tools that measure customer satisfaction on a scale from 1 to 10\n- Tools that use the Net Promoter System to measure the likelihood of people recommending your website/product to someone else on a scale from 0 to 10\n2. Qualitative tools to uncover why things happen\nQualitative tools help you collect qualitative (non-numerical) data to learn why your website visitors behave in a certain way. They include:\n- Website feedback tools (on-page polls, pop-ups, surveys) where visitors are asked questions about their experience\n- Website session recording/replay tools that show how individual users navigate through your website\n- Usability testing tools where a panel of potential or current customers can voice their thoughts and opinions on your website\n- Online reviews where you can read more about people’s experience of your brand and product\n3. Tools to test changes and measure improvements\nAfter you’ve collected quantitative and qualitative feedback and developed a clear sense of what's happening on your website, testing tools allow you to make changes and/or report on them to see if your conversion optimization efforts are going in the right direction. They include:\n- A/B testing tools that help you test different variations of a page to find the best performer (recommended for high-traffic sites, so you can be certain your results are statistically valid)\n- Website heat map + session recording tools that allow you to compare different variations of a page and the behavior on it\n- Conversion-tracking analytics tools that track and monitor conversions\n- Website feedback tools (like visual feedback widgets or NPS dashboards) that help you collect qualitative feedback and quantify it, so you can compare the before/after response to any change you made.\nThe remaining chapters of this guide cover:\n- Our 3-step CRO program\nA free, downloadable 3-step plan that helps you understand the drivers that bring people to your site, the barriers that stop them from converting, and the hooks that persuade them to stay.\n- Landing page CRO: a case study\nA practical case study on how to use the 3-step CRO plan to optimize a landing page and drive conversions effectively.\n- 20+ expert CRO tips\n20+ expert CRO practitioners answer the question: What's the one thing you recommend to people working on improving their conversion rate?\n- CRO glossary\nAn explanation of the most commonly used terms in CRO\nCommon Conversion Rate Optimization FAQs\nConversion rate optimization can live throughout your website. You can optimize your homepage, product pages, checkout process, and anything else on your website. When making CRO improvements, it's always important to measure, test, and adjust so you can continuously improve over time.\n1. How do you improve my conversion rate?\nIf your conversion rate is low (or isn’t as high as you’d like), you should analyze the pages that are underperforming to see what could be stopping customers from checking out. You should also analyze your highest converting pages to see what exactly is pulling customers to the finish line.\n2. How do you measure conversion rate optimization?\nTo measure and calculate your conversion rate, divide the number of conversions by the total number of visitors, multiply that times 100 and you have your conversion rate percentage. You should measure your conversion rate fairly often, so you can see if your CRO actions have resulted in true improvement.\n3. What can help you analyze conversion rate optimization?\nTools such as Hotjar enable you to analyze your conversion rate and optimization efforts. With applications such as heatmaps, scrollmaps, and multivariate or A/B testing, you can improve your conversion rate easier than ever.","Testing is fundamental to proper conversion rate optimization. Done well, an A/B test can help ecommerce marketers identify the best landing page design, which form of navigation works best, or even how and where to position page elements.\nUnfortunately, there are at least three relatively common A/B test mistakes that will lead to inaccurate results and, perhaps, poor decisions. Avoid these mistakes and your A/B tests will work better.\n1. Random Fluctuation\nStatistical fluctuation, which is sometimes called random fluctuation, describes random changes in a set of data that are not related to the measured stimulus.\nThe classic example of random fluctuation is a coin toss. Many large studies (and common sense) show that in a fair coin toss or series of fair coin tosses, heads should come up about half of the time. Or, more precisely, heads will appear at a ratio of 1/2. Similarly, tails will land up about half of the time.\nIf you don’t think this is true, try flipping a fair coin about 100,000 times, recording the result. You will find that about 50,000 will be heads.\nBut in smaller tests, one can get dramatically different results. As an example, consider the result of 10 right-handed coin tosses versus 10 left-handed coin tosses done specifically for this article. In both tests heads actually came up 60 percent of the time. So if these were the only tests one conducted, the data might lead you to say that heads will come up 60 percent of the time, which is false.\nThis can happen in A/B testing too. Imagine a marketer who A/B tests a change to a landing page. He gets a sample of 500 people and determines that the change improves conversions 20 percent. Would that be correct? Could you get a similar result even if there was no difference in the real difference on the things being compared? The answer is probably yes, you could get a 20 percent random fluctuation.\nSo how do you avoid making a mistake like this one? While there are a couple of ways to go about it, you might want to simply try increasing the sample size. Run the A/B test until an appropriately significant number of folks have participated in each branch.\nEvan Miller, who wrote about this particular problem with A/B testing back in 2010, has a helpful tool for estimating the sample size needed for each branch of an A/B test in order to avoid simply measuring a random fluctuation\n2. Not Examining the Data for Unexpected Differences\nWhen a marketer uses A/B testing for conversion rate optimization, often the only thing that is examined is, well, the conversion rate. But this too can lead to poor results.\nImagine an online retailer who has just spent a lot of money to create a mobile-optimized version of its ecommerce site. Before completely launching the mobile version, the retailer conducts an A/B test, ensuring that each branch of the test (A and B) will have a sufficient number of data points (users). Alternating site visitors are shown either the old site or the new mobile-optimized site.\nWhen the test is complete, the data shows that the mobile-optimized version actually reduced conversions by six percent. If the retailer stopped here, it might never launch the mobile-optimized version of its site, concerned that its sales would drop significantly.\nBut upon further review, there was a fatal flaw in the test. There was no consideration given to the type of device site visitors were using. So as a result, it is possible that the majority of mobile visitors, who would have actually benefited from the mobile-optimization were shown the old version of the site. A better setup would have been to evenly distribute desktop and mobile uses so that about half of the mobile users saw version A and about half saw version B.\nTo avoid this sort of error, be careful to examine all of the available data and not simply the conversion rate.\n3. Testing Too Many Variables\nOne of the most common A/B tests an ecommerce business might conduct will have to do with a site redesign. A merchant could seek to determine is a completely new and improved checkout works better than the old (and completely different) checkout.\nThis sort of test, conducted well, can give some good insights. But if the pages being compared are radically different, the test might be trying to measure too many variables.\nTry to set up A/B tests so that they measure or test a small set of significant differences in two or more pages. For example, given the same graphic design, does promotional text A convert better than promotional text B? Or given identical promotional copy and layout, does graphic design A convert better than graphic design B?\nBe careful not to take this one too far, or you can waste a lot of time testing insignificant things. It probably doesn’t matter if the headline font is 16 points or 18."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:c6757460-d5f3-44af-88c5-db0e5a9e29fe>","<urn:uuid:68aa0239-819d-4cb5-956b-8b923938aa19>"],"error":null}
{"question":"How does the preparation method of Gordon Ramsay's beer bread compare to the traditional rosemary beer bread recipe?","answer":"Gordon Ramsay's beer bread recipe requires mixing self-rising flour and salt with beer, filling mini loaf tins three-quarters full, baking for 30 minutes at 180°C, then brushing with milk and dusting with wholemeal flour for an additional 5 minutes. The traditional rosemary beer bread recipe involves combining self-rising flour, sugar, rosemary, and beer, pouring into a loaf pan, baking at 190°C for 55 minutes, and brushing with melted butter 3 minutes before completion.","context":["No beer in the feast! Sounds as flat as bread without yeast. But how about combining the two? Well!! It’s Gordon who can make the bread extra fluffy without using yeast. These mini bread loaves are soft and delicious. Read this article and you’ll be surprised! This recipe is as simple as a child’s play.\nGordon Ramsay’s beer bread needs lesser ingredients. The beer supplemented gives some extra lift to the bread. The dough might look too runny for the bread. But trust me! The result will be amazing and delicious. You can pair it with Ploughman’s lunch, a salad, eggs, or any combination of your choice.\nBread in a beer!! Wanna try it?? Check out the list of kitchen equipment and the handy ingredients needed for Gordon Ramsay’s beer bread. Make sure your bread looks and tastes the same as mentioned below. But first, check out some other recipes from the Chef’s delight.\n1. Gordon Ramsay’s Beef Lettuce Wraps– Lettuce wrap is the new hip thing to try. Give a shot to the beef lettuce wraps. It is the perfect finger food for a Sunday brunch.\n2. Gordon Ramsay’s Baked Chicken– Looking for a healthy alternative to cook chicken? Try the baked chicken recipe from Gordon Ramsay. You will never go back to the conventional chicken cooking methods\n3. Gordon Ramsay’s Omelette– If you are looking for a foolproof omelette recipe that will help you to kickstart your day, then I strongly advocated that you try Gordon Ramsay’s rendition.\n4. Gordon Ramsay’s Pork Tenderloin-The crispy pork will make you fall in love with this recipe. It is quite an elegant dish you can cook to impress someone special.\nWhat Equipment Will You Need For Gordon Ramsay’s Beer Bread?\nCheck out the kitchen equipment listed below to make delicious Gordon Ramsay’s beer bread.\n- Oven- You can’t bake the bread without the oven. So, the oven is the major equipment in this recipe.\n- Mixing Bowl- You’ll need a mixing bowl to prepare the dough for the bread. Make sure you use a large mixing bowl or use the bowl according to the number of ingredients used.\n- Mini Loaf Tins- Use a set of 8 mini loaf tine with dimensions (4.5 x 7.5 x 3 cm). You’ll get perfect fluffy mini loaves.\n- Wire Rack- After you remove the loaves from the tin, let them cool and rest on the wire rack.\n- Spoon- Add the ingredients to the flour in the mixing bowl using a spoon.\nHow Much Time Do You Need To Make Gordon Ramsay’s Beer Bread?\n|Preparation Time||Cooking Time||Total Time|\n|20 Minutes||35 Minutes||55 Minutes|\nWhat Ingredients Will You Need For Gordon Ramsay’s Beer Bread?\nHave a look at the list of ingredients below. Gather all these ingredients and get ready to make Gordon Ramsay’s beer bread.\n- Self-rising Flour and Wholemeal Self-raising Flour- Use self-rising flour for extra fluffy bread.\n- Butter- Use butter to grease the min loaf tins.\n- Milk- Milk helps to enhance the color of the bread. The sugars present in the help to brown the bread.\n- Beer- Beer is the main ingredient in the recipe. The yeast present in the beer helps in the baking process and adds starch to the flour.\n- Flaked Sea Salt- Flaked sea salt has larger flakes and is less processed than table salt. It also adds a stronger salty taste.\nSteps To Make Gordon Ramsay’s Beer Bread Recipe\n1. Grease the loaf pan\nGrease your mini loaf tins with butter and dust the flour in them. Shake them off to remove the excess flour and leave them aside.\n2. Prepare the dough\nIn a large mixing bowl, add self-rising flour and salt. Mix them and add beer to the bowl. Now, mix them to form a dough. Add equal portions of dough to the loaf tins. Fill them until three-quarters full.\nPlace the loaf tins on the baking sheet in the preheated oven (180 C) and bake for at least 30 minutes. Then, brush the top of the dough in each loaf tin with milk, dust some wholemeal self-rising flour on them, and bake again for 5 minutes.\nTake these loaves out from the tin and let them cool on the wire rack. Serve them with your favorite salad with any combination you like.\n|Saturated Fat||3 g|\n|Polyunsaturated Fat||6 g|\n|Monounsaturated Fat||1 g|\n|Vitamin A||65 IU|\nHow Will Gordon Ramsay’s Beer Bread Look and Taste?\nSo, Gordon Ramsay’s beer bread is ready. It’s super fluffy and delicious. It’s slightly chewy and buttery, it tastes mildly like beer. You can have this bread with Ploughman’s lunch, omelet, eggs, or with any other meal of your choice. You can refrigerate in an air-tight seal and enjoy it for almost 3 days.\nGordon Ramsay’s Beer Bread Recipe\n- Mixing Bowl\n- Mini-Loaf Tins (4.5x 7.5x 3cm)\n- Wire Rack\n- 1.6 pounds Self-raising Flour (plus extra for dusting)\n- 2.6 ounces Wholemeal Self-raising Flour\n- Butter (for greasing)\n- 2 tablespoon Milk\n- 8.4 ounces Beer (or larger)\n- 1/2 tablespoon Flaked Sea Salt (eg. Maldon)\n- Grease the inside of a loaf tin with butter, dust some flour in the tin and shake off the excess flour out from the tin.\n- Add the flour to the mixing bowl and add salt. Mix them and add beer to the bowl. Mix them well until the mixture is free from lumps. This batter might appear too runny for the bread mix.\n- Pour the batter inside the tin until they're three-quarters full. Place these on a baking sheet and bake them for at least 30 minutes. Now, brush the top with milk and dust some flour on them. Bake again for 5 more minutes.\n- Remove them from the oven. Now, let the loaves cool and for some time on a wire rack after you remove them from the tin.","Cooking with beer is as old as drinking it, a practice enjoyed by many over the thousands of years of beer-making. In ancient Egypt, Sumerians believed that cooking with beer was a way to bring “healthy food” to the people. Today, beer lovers continue to infuse recipes with this intoxicating concoction of hops and barley in an effort to enhance and enrich everyday food.\nMethod 1 of 4: Cooking with Beer\n1Know your beer. There are three main types: these are ales, stouts and lagers. Ales and lagers tend to be best for using in cooking, although stout has its place too, such as in some versions of Christmas pudding.Ad\n2Select a beer that complements and enhances the food. The right type of beer should be considered before you cook. One rule of thumb to follow is like wine––use dark beer for robust dishes and light beer for lighter fare.\n- Generally, pale ale beer complements nearly all recipes. Beer intensifies during cooking, so a lighter tasting beer may lend more of a blended flavor than a darker beer.\n- Nut brown beer is ideal for rich dishes such as stews or cheese dishes.\n- Strong Belgian ales can complement meat dishes. Most meat, especially red meat, dishes will require the use of dark brown ale rather than the lighter version.\n- Fruity beers work well with desserts.\n- Wheat ales can enhance seafood and poultry dishes.\n- Lager beer is works well for baking breads because it adds levity to the dough. Beer can be used instead of yeast in pancakes and some breads.\n- Adding beer to batters produces a light and crispy texture.\n3Evaluate the malt and hop levels. Malt and hops are the flavoring agents in beer. If the levels are higher, more beer flavor will come out in your dish.\n4Determine the role that the beer will play in your cooking. Enhancing the flavor of the food is not the only role beer can have in cooking. It is also a natural meat tenderizer, yeast enhancer that complements bread and pancakes. It is also a deglazing agent that can re-infuse a dish with cooked ingredients.\n5Choose a reputable brand. Never cook or use beer that you wouldn’t enjoy drinking. Remember that price doesn’t always dictate flavor, so if you're not already sure, sample the beer before adding it to your dish.\n- Don't be afraid of using stale beer though. Last night's unfinished beer, provided it was refrigerated, can be used to cook with. After all, it's probably flat and no good for drinking now!\n6Follow the directions in the recipe. Beer is meant to enhance and enliven the flavors of your food, so add this ingredient specifically according to directions. Overdoing the addition of beer could overwhelm the flavor of the food and cause it to be unpalatable.\n7Bring the beer to room temperature before cooking. Beer that is too cold or hot may compromise other ingredients in your dish. If your brew has been in the refrigerator, make sure you take it out and give it time to get to room temperature before adding it to your dish. Only do otherwise if a recipe overrides this specifically.\n8Remember to use a measuring device created for liquids. Some measuring cups are actually made for dry ingredients such as flour or sugar. Purchase a measuring cup system from the local home store that is designed specifically for liquids to obtain an accurate reading.\n9Start cooking. Below are some typical dishes made using beer. Experiment with a few to see what your favorites will be.Ad\nMethod 2 of 4: Beer soup\n1Assemble the following ingredients:\n- 2 liters (0.5 US gal), 3½ pints, 9 cups chicken or vegetable stock\n- 300ml, ½ pint, 1 1/4 cups German beer\n- 250g, 9 oz stale bread (crusts removed)\n- Salt and pepper to season\n- Freshly grated nutmeg to season\n- 100ml, 4 fl oz, 7 tablespoons single (light) cream.\n2Pour the stock into a saucepan.\n3Add the German beer and stale bread.\n4Season to taste with salt and pepper.\n5Cover the saucepan. Cook over a very gentle heat for half an hour.\n6Remove from the heat. Allow to cool enough to blend.\n7Puree in the blender. Add a little grated nutmeg and the cream. Check the flavor.\n8Reheat. Serve scalding hot.Ad\nMethod 3 of 4: Beer pancakes\nThe beer will provide the rising action needed to make the pancakes work.\n1Assemble the following ingredients:\n- 2 cups all-purpose/ plain flour\n- 2 cups beer\n- 2 eggs, slightly beaten\n- 2 tbsp honey or maple syrup\n- Some butter.\n2Pour the beer, eggs and syrup or honey into a mixing bowl. Mix well.\n3Sift the flour into the wet ingredients. Mix well with a whisk. Aim for a thin and lumpy mix.\n4Heat a little butter in a frying pan. Spoon in the batter and cook the pancake. Use medium heat.\n- Flip the pancake over when the bubbles appear on the surface and the pancake edge is firm.\n- For fluffier, thicker pancakes, use a pancake ring.\n- Add things such as chocolate chips, berries or chopped bananas as wished.\n- A half cup of wholemeal flour can be substituted for a half cup of plain flour if desired.\nMethod 4 of 4: Rosemary beer bread\n1Preheat oven to 375ºF/190ºC.\n2Gather the ingredients. You will need:\n- 3 cups self-rising flour (self-raising flour)\n- ½ cup sugar\n- 12 ounces of pale or light beer\n- 1/3 cup fresh chopped rosemary\n- 2 tablespoons of melted butter\n3Coat a loaf pan with butter and set aside.\n4Combine the flour, sugar, rosemary and beer in a large bowl and mix well.\n5Pour the mixture into a loaf pan.\n6Bake for about 55 minutes, or until the bread is risen and a knife inserted into the center comes out clean.\n7About three minutes before the bread is finished, brush melted butter over the top.\nGive us 3 minutes of knowledge!\nSome other delicious beer recipes\n- Tell diners or guests beforehand that you included beer as an ingredient to the dish. Some people have allergies to wheat or hops.\n- Consider serving a different type of beer with your beer-cooked dish to complement different flavors.\n- Try different beers to create an appreciation of different flavors and tastes\n- Although the alcohol level cooks down in most recipes, tell anyone who is avoiding alcohol that the dish contains beer.\n- Avoid using old or outdated beer in your recipes––like any ingredient, throw it away once it has expired.\n- Not all beer is vegetarian or vegan; be aware if you're following such a diet and using beer for cooking.\nSources and Citations\n- http://www.npr.org/templates/story/story.php?storyId=113747902 – research source\n- http://www.globalgourmet.com/food/egg/egg0397 – research source/beertips.html#axzz1ZkTfVTC3\n- http://www.foodnetwork.com/recipes/calling-all-cooks/beer-bread-recipe/index.html – research source\n- Beer soup recipe adapted from Larousse Gastronomique, Beer soup, p. 89, (2009), ISBN 978-0-600-62042-6.\n- Beer pancake recipe adapted from http://captainjaxestreasureisland.blogspot.com/2008/08/beer-pancakes.html.\nIn other languages:\nThanks to all authors for creating a page that has been read 54,972 times."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:76fee4b3-1941-4680-a1d4-e1038b82a404>","<urn:uuid:8e7dbc46-dcc2-4f06-a882-efc9e16cbe9e>"],"error":null}
{"question":"What is the relationship between orchestration and composition in musical scoring, and how have digital technologies transformed the process of creating and previewing theatrical projections?","answer":"Orchestration is considered an inseparable part of the composition process, though composers may sometimes hire orchestrators for larger ensembles to provide input on making arrangements sound their best or to help with technical details like harp pedalling or breathing points for brass players. For digital projections, technological advances have revolutionized the creative process - powerful media servers now enable real-time previewing and simultaneous video editing, while 3D previewing allows production teams to build and review their work before entering the theater. This is particularly valuable given limited rehearsal periods, as opera productions may only have 10-15 hours onstage to coordinate design elements with cast and music. The shift from analog to digital has dramatically improved both creativity in design and response time to production needs.","context":["You have worked in feature film, documentaries and written for the theatre, what would you say are the main differences if any between the three mediums?\nMusically there isn’t necessarily a big difference. The direction of the music is determined by the vision of the director in collaboration with the composer, in both film and theatre productions. But the working methods are different. In a theatre performance, you don’t have a “locked” timeline like in a movie, so you don’t normally use the same amount of macro-timing like you might do when scoring a complex film sequence with a lot of different cue points. Since theatre is a living and breathing medium, the timing of the actors also might vary a bit from performance to performance. So the structure of the music needs to be a bit more “flexible” since the scenes might play out a bit different from day to day. That said, for theatre productions I often like to work around this, by splitting the musical cues into different parts that can indeed be triggered by certain dialogue lines or other cue points – so that you get a seamless effect of a wholly composed thing that just magically fits together! It’s a semi-interactive approach to the music, a bit like in modern video games when the music continues to play in the background, morphing to a different part of the music when something specific happens in the action.\nWhat is clearly also different in the theatre, is that the actors are often already working with the music while they are rehearsing the play. So there might be more cases of the music actively influencing the stage direction and the rhythm of certain sequences. That rarely happens with music for film, unless you are filming musical numbers which of course requires that music to be ready before filming. But in general the composer comes in a little bit later in the filmmaking process, often after filming has wrapped. So in films, the rhythm and structure of the music cues are more often determined by the editing, than vice versa.\nThere is approx. 30 mins of music in Mio, min Mio, when you have written the score is the music played live with the performance or is it recorded and played when the performance is on stage?\nOn both “Mio, min Mio” and “Ingenting” (both directed by the amazing Hilde Brinchmann) the music was pre-recorded and played back by the stage sound engineer through a piece of software called QLab. This way the musical cues can be split into different parts like I mentioned before, and glued together on-stage during the final weeks of rehearsals, fine-tuning the whole performance and the structure of the music on location. It is a very creative and fun process between director, composer and sound engineer.\nHowever, on another recent project, a modern adaptation of Henrik Ibsen’s “Peer Gynt” which was staged the last three summer seasons as an outdoor play in Oslo, we needed to do the same kinds of seamless live transitions between cues, but with a 40 piece live orchestra (actually the Staff Band of the Norwegian Armed Forces) instead of pre-recorded files. Which means that the conductor (the magnificent Bjarte Engeset) had to keep track of a very long list of micro-cues throughout, carefully listening to every line of dialogue by the actors, at the same time as conducting the orchestra, being prepared to skip a certain amount of bars or even skipping to a different piece of music at any point in time! At certain points we had different players in the orchestra purposely playing two different pieces of music simultaneously, to create optimal “crossfading” effects! Total madness, but when you work with amazing people, it can be done!\nWhat is your principal instrument when you are writing a score for a play or film?\nThe piano is my composing tool, for getting the ideas down (although the drums are what I regard as my main instrument). I never get my best ideas when I am in the studio, surrounded by all the software synths and sounds with their limitless possibilities, like they are almost a distraction from the ideas themselves. I find that an acoustic piano delivers another kind of feedback, it’s almost like it’s talking back to you instead of just making the sounds. So I might have working days when I purposely stay away from the studio, to have more peace for just getting the ideas down first, recording rough piano sketches into my phone or iPad. Or just taking a walk in the forest while having the phone ready to record me singing, when an idea appears. I think I read somewhere that the brain is more efficient at producing new ideas when you’re walking than when you’re sitting still – it certainly seems that way to me! As soon as I have the idea for a track, I more or less know where I want to go with it, in terms of instrumentation and arrangement. That’s when I go back to my studio and get to work.\nWhat percentage of the instrumentation was symphonic on Mio, min Mio?\nI would say 1%! The only live element are some of the drums (which I played myself). The rest is all sample libraries, played on keyboards and edited and mixed in my sequencer. The budget is really what determines what you can do. For good and bad, sample libraries are sounding better and better, and are making it easier than ever to produce decent sounding results in quite a short amount of time – but they can’t in any way replace the real thing, and they shouldn’t. It’s a difficult balance, because I always want to use as many live musicians as possible, and I also want to support and give jobs to all the great musicians around! But the way things are now, only the big budget films in Norway makes it possible to hire a full orchestra. And the way you might utilize a blend of real players and sample libraries are also determined by what kind of sound you are attempting to create. On my first two feature films “Rafiki” (2009) and “The Tough Guys” (2013) I used studio musicians to cover up the shortcomings of the symphonic sample libraries and to make it breathe. On later projects I have sometimes opted for more of a chamber music sound rather than a full symphonic one, like in “The Brothers Lionheart” (2014) which is largely based on a string quintet with added percussion and keyboards, and “Los Bando” (2018) which is more of an indie pop score with strings and woodwinds, drums and keyboards. Going the “chamber” route gives you the privilege of basing the whole thing on a group of 5 or 6 real musicians and not just use them to “cover up” the samples. “Mio” was a more minimalistic production with a tighter schedule, but still needed this big symphonic kind of sound, which led me to just embracing the sample libraries fully this time – but I would love to rework some of the music for a symphonic suite or something at a later date. On “Ingenting” this was a much easier creative choice since the music isn’t even supposed to sound symphonic – it’s all based on retro synth sounds, inspired by the likes of Wendy Carlos. And “Secret of the Catacomb” is sort of a blend between the symphonic and synthetic aesthetics and had to be composed and produced in a very short time, so that’s all sample based too.\nYou have not released your latest three scores on CD, will they get a compact disc release do you think?\nIn Norway, music streaming has completely taken over the market. CDs are given away for free by the bucket loads, people just want to get rid of them here. Not me though – I love CDs! Ideally, I would like to release every album on CD, so it’s really just a question of time and money. But I have noted that the soundtrack fan community seems to still hold the CD in high regard, like I do. That’s awesome! And I think there’s a fair chance we will see a CD version of “Mio” sometime in 2020.\nWhat would you say are your musical influences, or which composers and artists have inspired you?\nThat’s a big one! Everything I have ever listened to and loved, I guess! When I was young, I listened to a lot of Genesis, Phil Collins and Peter Gabriel (still do), and in my teenage years I loved Norwegian alternative 90s rock bands Motorpsycho and Seigmen. I played a lot of computer games and totally absorbed the music of all those classic Lucas-Arts games of the 90s, with composers like Michael Z. Land, Peter McConnell and Clint Bajakian. I also discovered John Williams, listening to those Star Wars Special Edition double CD albums and reading the analytic liner notes with great care. That was a big eye-opener into the world of film music, I guess I was 14 when those albums came out. Today, Williams’ more modern work with those beautiful and more experimental scores like “A.I. Artificial Intelligence” has a special place in my heart. I also identify greatly with Danny Elfman in terms of his journey from rock musician to reluctant film composer and have a soft spot for some of his more mature and minimalist work like “The Unknown Known”. In the pop/rock realm I am a big fan of Ben Folds, every one of his albums and all of his crazy ideas. Other piano-oriented songwriters like Tori Amos and Fiona Apple and genre-defining, ground-breaking artists like Björk and Radiohead were also a big part of my musical coming-of-age period. Last but not least: My three all-time favourite classical composers are probably Edvard Grieg, Antonin Dvorak and Maurice Ravel. I go absolutely nuts over Grieg – and that’s not just because I am a Norwegian! His catalogue is simply an endless treasure trove of beautiful music (not just the “hits!).\nSECRET OF THE CATACOMB I think is a very atmospheric score, the music is very expressive and dark, how much time did you have to write the music for this radio production, and at what stage of production do you become involved and is it harder to write for a radio production as opposed to a feature film?\nGlad you enjoyed it! I think I used just over six weeks of composing and sequencing/recording and mixing simultaneously. Working with radio dramas is a lot of fun, and another quite different process from both film and theatre! Since you don’t have pictures, both the sound design and the music can get quite “descriptive”. That’s why you tend to go for those gut instinct things like dark Gregorian drones, to make the listener immediately aware that we are inside a dark cavern, and a scary monk-type creature is staring at us!\nThe series consists of eight episodes, each clocking at around 30 minutes. Obviously, scoring four hours of content in six weeks is… not recommended, to say the least. So this again is a creative process between me, director Guri Skeie and sound designer Hilde Rolfsnes. The first step is reading the script and partaking in an early script reading with the cast and crew. Then Guri and Hilde will give me a “wishlist” of different themes and cues that they need for placing around the episodes – they have such a detailed vision for the project and know exactly what they are after. So my job then is to compose the desired “package” of music cues, and record and produce it all in parallel with them recording the episodes with the actors. After that it’s largely sound designer Hilde’s job to glue all the music cues together with the episodes, at the same time as she’s creating all the sound design and mixing everything. There’s not much time for second-guessing or delivering something that doesn’t work, for any of us. It’s quite fast paced! But I don’t think it’s harder than working on a feature film. It’s different, since you’re not so much scoring scene for scene, but doing a more flexible package of cues that can work well for different scenes and situations. It’s a fun challenge!\nWhat musical education did you receive, and was writing music for film and theatre something you had always wanted to do?\nIn my teenage years I did spend a lot of time scoring the amateur short films and computer games me and my friends were having fun creating at the time. So, I think I knew deep down that I this was what I was going to be doing.\nI later studied musicology and music technology. There was no film music related course here back in the early 2000s and studying abroad was not even an option for me – I’m way too homesick. The national composition studies I knew of felt geared towards a certain type of contemporary classical music that I have never been that interested in. In that period, I spent most of my time playing in bands anyway, primarily as a drummer. In an act of desperation (or sudden inspiration?) I packed all my things and moved to the big city (Oslo) while I was supposed to be finishing my master’s degree in another town 500 kilometres away (Trondheim). Half a year later I got my first gig at the Norwegian Broadcasting Company with a children’s TV series. This was 2006, and I’ve been at it since then! I was a bit of a lousy student, to be honest. What I liked about musicology was the more playful aspects of it; like the subjects in composition and improvisation. I made some lifelong friendships, and my mind was opened to music history and learning more about certain composers and periods of music, of which I am grateful. But I didn’t speak the academic language. My mind was solely on making music, not analysing it.\nWhat are your earliest memories of any kind of music?\nMy dad has always been a big music fan. LPs by the likes of Frank Zappa were always in heavy rotation in our home – and as you may know, Zappa represents pretty much every music genre there is! I also enjoyed stuff like Santana’s debut album and The Beatles’ “Sgt. Pepper’s Lonely-Hearts Club Band” (I have since grown to be a huge Beatles fanatic). I vaguely remember the sound of a-ha coming out of the kitchen radio. I was three when they had their big breakthrough with “Take On Me” in 1985. But I was probably drumming on kitchen utensils and singing improvised nonsense lyrics before that.\nDo you orchestrate all of your music for film and theatre and do you think that orchestration is an extension of the composing process?\nI always do the arrangements myself, and indeed see them as an inseparable part of the composition. I have sometimes hired an orchestrator when working with larger ensembles, if I want some kind of proof-reading or even creative input on how to make the arrangements sound their best. As my university drop-out tactic might suggest, music notation is not exactly my strongest side – I enjoy doing it, but it’s not what I do best. So, it’s sometimes good to have a second opinion when dealing with the dirty details like harp pedalling or how often the tuba player needs to breathe. As a general rule, I only write down the parts that are actually going to be played by real musicians. If I am only going to use sample libraries, I skip the written score entirely, and only work directly in the sequencer. But on most of my projects, it’s a mix of the two. When you’re dealing with smaller ensembles and soloists, it’s always great to just talk to the musicians directly and communicate how you want the parts to be played. But of course, the closer the notation is to already be conveying those intentions in easily readable form, the more of a head-start you have in the recording session.\nMany Thanks to Eirik for taking the time to answer my questions.","Revolutions in Tech\n50 years of advances in technology and production\nThe stunning pace of technological development since 1970, the year of OPERA America’s founding, has radically altered the process of putting opera onstage and continually expanded the field’s range of production possibilities. In three primary areas especially — lighting, video projections, and sound — the changes have been so revolutionary that they would have been all but unimaginable 50 years ago.\nThe decade of the 1970s was a period of expansion of the role of lighting, as part of the increased emphasis being placed on opera’s dramatic values. “Lighting became more naturalistic and dynamic,” says lighting designer Rick Fisher. “The new theatricality demanded color and nuance to support the drama.” In the late 1970s and early 1980s, computer control boards started making their way into opera houses, their more elaborate cueing giving designers a tool to cultivate more sophisticated lighting plans.\nThe era saw much experimentation with individual light sources, as manufacturers worked to develop fixtures that emitted more light while giving off less heat. ETC’s Source Four fixtures and dimming systems, introduced in 1992, used innovative lamps, along with a reflector, to push more light forward while letting heat pass through the back. It was the beginning of a revolution in lighting sources, moving toward more versatile, powerful equipment.\nThe revolution continued with the rollout of LED lighting fixtures, starting in 2006. In their early years, these went through growing pains: consistency problems in color mixing and range, limited ability to blend with standard fixtures. But in the last decade, their role has matured. Their color-mixing capabilities provide a more responsive palette, and their compact profile allows installation in restricted spaces. Designer York Kennedy notes that LED fixtures are especially effective in bringing out accents in scenery architecture and in throwing even light onto cycloramas and drops. “The new LED fixtures are a great tool for the subtle and carefully timed gestures that opera demands,” Kennedy says.\nLED technology has had a decisive impact on the rig for a production. One fixture — even one in a fixed position — can now do many things. If moving lights can be incorporated into the space and budget, they can expand possibilities for color, beam shape, patterns, and movement. The switch to LED isn’t easy: It means rethinking the infrastructure of opera-house lighting arrangements. The fixtures themselves carry onboard controllers for color, brightness and, in some cases, movement, relegating standard dimmer controlling to the past. “You need more data cable for control and less dimmer room capacity,” notes designer Duane Schuler. It also requires a new level of skill and training for lighting professionals, who must have a deep knowledge of computer controls and complex lighting-control computer networks.\nIn the past half-century, projection technology has become integrated into opera production in ways previously unimagined. All one needs to do is look back at recent examples such as The Dallas Opera’s Moby Dick and Everest, or Santa Fe Opera’s The (R)evolution of Steve Jobs and The Golden Cockerel. Washington National Opera’s recent mountings of Don Giovanni and Samson and Delilah both used the same modular set, but with projections to give the two shows very different looks. In cases like these, projected imagery has become an integral part of production design; indeed, another character within the piece.\nThe burgeoning use of projection stems from breakthroughs in the technology involved. Early projectors were bulky and noisy. It was difficult to place the equipment in a spot where it would be hidden and where the sound could be baffled, while still keeping the lamps cooled. Over the past decade, LED projection technology has changed all that. Current projectors are brighter and more compact. They are also quieter, which makes them easier to hang in locations closer to the audience, such as the front rail of the balcony: the optimum spot for front projections.\nVideo design used to be an analog process. In order for an image to be reworked, using the Pani and Pigi projectors prevalent from the 1970s to the early 2000s, film had to be perforated and spliced: a labor-intensive technique. “The reworking of an image often involved the use of complicated hand-cut masks and the reshooting of film,” says projection designer Elaine J. McCarthy. “You needed a three-day turnaround to see if the changes were correct. Each edit had to be carefully weighed in terms of the time available and the expense of the new slide.” Digital processes, McCarthy notes, have eased turnaround time, and correspondingly lessened the impact on the bottom line.\nPowerful media servers, emerging in the early years of this century, allow for real-time previewing and simultaneous video editing. Designer Driscoll Otto notes that these servers allow production teams to build and preview their work in 3D before they ever enter the theater. This is an especially welcome development in light of the need to share limited rehearsal periods with other production personnel; if the projections are ill-prepared, it steals time from directors and other designers. “While a Broadway production typically has three to four weeks before opening, an opera may only have 10 to 15 hours onstage to coordinate the design elements with the cast and the music,” notes Rick Fisher.\n“As the imagery moved from tape to the digital world, the creation of imagery has moved forward in leaps and bounds,” says designer Ben Pearcy. “Computers and software have allowed much more creativity in design, as well as a much faster response to production needs.”\nOnce only discussed behind closed doors, audio amplification has become in some cases an overt element in opera performance. The modern era of miking in opera began with the 1987 premiere Nixon in China, in which John Adams called for amplification in his scoring. Since then, composers like Adams, David T. Little, and Mason Bates have written works with complex, layered orchestrations that demand voice enhancement. Meanwhile, opera is now being staged in sonically challenging venues: chamber works in black box theaters not designed for music; site-specific productions in spaces chosen for their dramatic appropriateness rather than their acoustic properties. Often, their acoustics demand technological intervention.\nTechnological advances have helped integrate amplification into the mainstream. Clunky, highly visible sound equipment is a thing of the past. Line array loudspeaker systems allow designers to sculpt sound coverage, balance frequency response, and send sound to all areas of the theater, so that the amplification truly augments natural voices rather than just making them louder. Wireless mics are easier to conceal than in the past — an especially important factor in the era of HD filming — and they’re also more reliable and responsive to the broad dynamics of the human voice, resulting in a more natural sound.\nToday’s digital mixing consoles allow for efficient setups, and their recall abilities promote consistency from performance to performance. New digital processing equipment lets designers adjust sound system parameters from anywhere in the theater. “These systems help me quickly address the concerns of the performers, as well as any problems that might be audible to the audience,” says designer Kai Harada.\nKarl Kern, Santa Fe Opera’s A/V director, notes that today’s programmable boards and speaker systems not only enhance the quality of performances, but also keep his budget under control: The process of moving systems in and out of repertory is much less labor-intensive than in the past. Moreover, as with lighting and video, today’s sound design tools allow for comprehensive preplanning and for previewing sounds without chewing up valuable staging time. “The equipment helps us meet deadlines and minimize errors in operation,” Kern says.\nIt isn’t just the instruments that have improved; sound designers now have a more sophisticated understanding of the art and science of amplification. “Savvy and sensitive designers have emerged who use their backgrounds in music, along with the new audio technologies, to build appropriate sound environments and enhance the audience experience,” says sound designer Mark Grey.\nFor all the innovations in sound, lighting, and projection, the technologies that have had the greatest impact on opera production are those that have transformed every aspect of our lives. Consider that in the 1970s, the fax machine represented the cutting edge in professional communication, allowing nearly immediate sharing of contract drafts, measurement charts, and show inventories. To relay information, technical directors put together fax trees, with each recipient forwarding the fax on to a number of others, with the hope that the messages would get through the complicated routing and result in answers.\nIn the age of the computer and the smartphone, faxing seems all but prehistoric. The internet has also made access to advice from other shops and vendors nationally and internationally much simpler. Videoconferencing lets design departments share imagery remotely. File-sharing platforms like Dropbox have streamlined the process of vetting materials and sharing inventories for co-productions and rentals. “It saves a lot of time over sending out slides and discs,” says Corinna Bakken, costume designer at Minnesota Opera. All of this represents a quantum leap in efficiency, as well as a significant cost savings compared to the days of long-distance bills and FedEx charges.\nYork Kennedy relates an instance when he had left a production, and last-minute changes to the show’s blocking necessitated a quick alteration of lighting cues. The theater was able to set up a Facetime stream that allowed him to watch the stage and then work with the programmer and stage manager to adjust the timings to the new staging. “Not ideal by any stretch, but we were able to make it work!” he says.\nTrue, communication advances have not entirely eliminated the need for shipping or for direct, face-to-face contact, but projects can now move along at a much faster clip. Facebook costume groups and OPERA America’s listservs abet the process. “If we are looking for new construction techniques or specialty resources, the world is at our fingertips,” says Marsha LeBoeuf, costume designer at Washington National Opera.\nPhones allow shoppers in the costume and props shops immediately to share images of potential purchases with designers. Another change in the costume-design process has been dictated by the market. Fabric stores have been cutting back on their stock; quite a few stores have disappeared altogether. Vendors that do custom fabric printing are to some extent filling in the gap, and current technology lets shops provide their own artwork. “This keeps costs down and gets us the imagery that the specific designs require,” says Daniele McCartan, costume director at San Francisco Opera.\nFrom an audience standpoint, the most visible technological innovation of recent decades has been the introduction of supertitles, beginning with a Canadian Opera Company Elektra in 1983. In the years since, the creation and use of subtitles have evolved from a time-intensive process, involving making, editing, reprinting, and reordering stacks of projector slides, to an easy-to-manage system of digital projectors and software programs. In 1995, the Met introduced the first seat-back titles, followed soon after by Santa Fe Opera. Titles — both back-of-the-seat and overhead — have been responsible for bringing a new level of accessibility to the art form and arguably for expanding the audience for opera.\nA technician is no longer somebody who carries a wrench or a hammer or a pair of scissors. Today’s production personnel need to have mastery of a vast toolkit of technological resources. Whether it is drafting a show for the scene shop, preparing a high-resolution image for printing a backdrop or run of fabric, or coordinating production elements during a tech rehearsal, they need a sophisticated understanding of computer technology. Fifty years ago, at the birth of OPERA America, this might have seemed pure science fiction. Now it’s a reality."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:d0173b9e-46fc-402d-b1be-d2149ca9a663>","<urn:uuid:9353321b-6a01-49ea-be1f-b9ac5d2089a4>"],"error":null}
{"question":"As an architecture enthusiast who often visits New York, I'm curious: What's the core difference between Heermance Farm's greenhouses and the Whitney Museum in terms of their architectural purposes?","answer":"Heermance Farm's greenhouses are specifically designed for year-round cultivation of specialty crops, with one greenhouse dedicated to hydroponic farming where they can control plant nutrition levels. In contrast, the Whitney Museum building, designed by RPBW, was built to house an expanding art collection of 19,000 pieces with approximately 4,650 square meters of exhibition space spread across multiple floors, including the city's largest space for 20th century American art on its fifth floor.","context":["In the Hudson River Valley, there lives a farm loved by New York City chefs, growing produce not typically considered local. Heermance Farm cultivates crops like galangal, Thai ginger, Tulsi basil and salsify, working with chefs to continually add new produce to their repertoire.\n“Because we grow for chefs, we’re able to sort of be daring, and together we wanted to challenge ourselves to discover what was being overlooked,” says Farm Manager Kevin Ferry. “We’re very good at coloring outside of the lines, so we invite chef collaborators to the canvas. Every year or season these chefs are defining new trends in culinary expression, and we are always willing to explore.”\nEdible Manhattan caught up with Ferry to discuss what’s they’re currently cultivating, hydroponic farming and where you can try Heermance’s produce in NYC.\nWhat specialty vegetables are you growing right now?\nRight now, our greenhouses are full of specialty crops, from Malabar spinach and galangal to turmeric and lemongrass. But I’m really excited this year about a new set of epicurean rarities we’re growing that fell out of standard growing practices over many generations—crops like crosnes (pronounced croans), skirret and salsify, which were once medieval staples or even traveled on the old exploring trade routes from east Asia and adopted into French country gardens. We’re planning to save those seeds and propagate them, too.\nWe’re on the heels of the warmer summer months, so while we’re distributing the last of our winter storage crops, we’re seeding constantly. We’re really looking forward to seeing the results of this one squash trial seed we ordered from Dan Barber’s seed company Row 7. His seeds have been a real pleasure to grow because, as a chef, he’s looking for a squash that can be consumed at different stages of its growth, prepared with skin on, etc.\nHas Heermance always supplied produce to NYC restaurants? Did you notice more demand when you began offering specialty produce?\nIt’s sort of the other way around; we’ve always considered the specialty crops to be our main focus. The nice thing is that chefs are shopping for their staples, too, so they might come to Heermance for edible flowers or shiso, but they also wind up wanting our fingerling potatoes and other essentials.\nWhat are the top veggies Heermance sells to NYC chefs?\nIt depends on the time of year. Heirloom tomatoes are the queens of summer. But in the spring, everyone loves carrots‑little, tender baby carrots are vibrant and versatile. Also, broccoli and an array of Asian cabbages and greens. We actually grow all year long in our greenhouses, so in February, chefs want anything green because it’s hard to come by anything green locally in this region during those frigid weeks. This past winter the demand for fresh baby lettuce heads was higher than we’ve ever seen.\nIn terms of flavor or the quality of restaurant dishes, how does buying specialty vegetables locally impact dishes versus getting that produce imported from their countries of origin?\nThere’s no comparison. All it takes is one bite into a flavorless, watery beefsteak tomato that lost half its vitamins traveling across the country to know the value in ripe, local produce. Of course, there are things chefs can do to improve the taste, like roasting tomatoes to bring out the sugar, but it’s not the same.\nI would say the difference starts with the smell and then the feel. When you walk into our greenhouses and really take a full deep breath and pick fresh, aromatic Tulsi basil in March, it’s like seeing in color for the first time, like that scene in the Wizard of Oz. It’s like entering paradise, and we’re able to send part of that special place to our restaurants. I love it.\nHow many pounds of veggies do you guys sell to NYC chefs on a weekly basis?\nThat changes per season. In the winter, we’re selling a lot of kale, winter squash, heirloom potatoes, bok choy, lettuce and things that are just lighter. Most of our produce are in fresh pick units like pieces, cases or bunches or even edible flowers and microgreens in small boxes.\nWhat about the Hudson River Valley is good for growing these exotic vegetables?\nGrowing vegetables in general is something that the beautifully rich soils of the Hudson River Valley accommodate so well. But when I think of exotic vegetables, I think of ones that are grown in warmer climates. We’re USDA Zone 5b, and that’s only going to get warmer, but our outdoor growing season isn’t long enough to support something like a field of artichokes (although we do grow a few every year). So we grow our “exotic” vegetables in our greenhouses all year long, but our fields are always full of heirloom vegetables during the warmer months.\nHeermance Farm practices hydroponic farming. What does that mean, and what effect does it have on your produce?\nOne of our greenhouses is hydroponic, and a lot of people have this idea that hydroponics occurs in a sci-fi lab, but it’s actually a very natural environment full of life, the roots just don’t touch soil. We blend nutritious tea out of compost where our wriggler worms are thriving, we’ve used seaweed and molasses, which are rich in trace minerals and macro nutrients, respectively. We take hydroponics at its literal Greek translation which is “working water.” Based on the cultivar, we add a specific mixture of fertilizer nutrients.\nRight now, we’re harvesting hydroponic zucchini, broccoli, Romanesco, bok choy and lots of other things. We’re able to control the levels of nutrition the plants are receiving, so instead of undergoing a rainy period in the field where the plant is taking up a lot of water and not necessarily a ton of nutrients because they’re being lost in the runoff, we’re able to maintain a steady supply of nutrition that creates a more consistently nourished, and therefore more flavorful, vegetable.\nIn what restaurants in NYC can our readers taste your produce?\nWe’ve been really lucky to have developed a nice relationship with Daniel Boulud’s Dinex Restaurant Group, so you can find our produce at Daniel Restaurant, as well as Le Pavillon near Grand Central Station. We recently started selling to the Russian Tea Room, which is such an iconic place. We also sell frequently to Gotham on 12th Street and Frevo in Greenwich Village.\nFeature photo courtesy of Heermance Farms.","The Whitney Museum of American Art opened in its new location designed by Renzo Piano’s studio RPBW on New York’s Gansevoort Street on May 1 2015. The first privately owned institute of American art, the Whitney Museum of American Art was founded in 1930 (MOMA was founded in 1929) by the forward-looking Gertrude Vanderbilt Whitney. It had been located at the corner of Madison Avenue and 75th Street, in a building designed by Marcel Breuer in 1966 which attracted a lot of controversy among critics and New Yorkers as soon as it opened. Fifty years later, Renzo Piano was appointed to build a new building that would inherit the same expressive power and be capable of housing a collection which has increased nearly tenfold, from 2000 to 19,000 pieces. This is in addition to new programmatic and spatial requirements, such as an exhibition space suitable to house a collection that includes very large works, a centre for education and instruction, conservation labs, a library with reading rooms, a 170 seat theatre as well as a coffee shop on the top floor and a museum shop on the ground floor.\nThe lot Renzo Piano worked on is a very important one: located in the Meatpacking District, a bustling neighbourhood in southwest Manhattan, it is squeezed between the highway along the Hudson River and the start of the High Line, a new city park created by Diller Scofidio + Renfo through redevelopment of an unused branch of the New York Central Railroad. In addition to these elements, the urban landscape is made up of factories and butchers’ shops both old and new that still preserve their original appearance and still sell meat wholesale, buildings from different ages packed together in a creative disorder that only makes it more interesting today. Taking advantage of the vicinity of the High Line, Renzo Piano connects the museum entrance and lobby with the southern exit of the overhead park, opening up a 600 sqm space, then designing an eastern wall made of glass with big patios extending out of the exhibition spaces on the upper levels. The urban landscape project saw the involvement of Piet Oudolf, the Dutch landscape designer who worked on the High Line project.\nIn this three-dimensional vision of architecture, we may see a focus on the multi-faceted architectural panorama typical of a district with an industrial heart. Its cladding of bluish-grey steel panels does not conceal the highly asymmetric nature of the façades: the one facing the Hudson River, with its two glass walls, has a different purpose and perspective from the one facing Gansevoort, which appears to be suspended over the street, with an oblique cut into it paying homage to the forms of Breuer’s old modernist Whitney. Inside, approximately 4650 sqm of exhibition space is distributed over the fifth to eighth floors; the fifth floor is a 1670 sqm open space, the biggest space the city of New York has to offer to the history of twentieth century American art.\nArchitects: Renzo Piano Building Workshop (RPBW)\nExecutive Architects: Cooper Robertson\nProject Team Owner’s Rep: Gardiner & Theobald, Inc.\nMEP Engineer: Jaros, Baum & Bolles\nLighting/Daylighting Engineer: Ove Arup & Partners\nStructural Engineer: Robert Silman Associates\nConstruction Manager: Turner Construction, LLC\nRestaurateur Union Square Hospitality Group / Danny Meyer\nClient: Whitney Museum of American Art\nLocation: 99 Gansevoort Street, New York City (USA)\nKey Dates Design Unveiling: April 30, 2008\nGroundbreaking: May 24, 2011\nTopping Out (Structural Steel): December 17, 2012\nPublic Opening: May 1, 2015\nSize Stories 9\nTotal building size: 20,500 mq\nInterior exhibition space: 4600 mq\nOutdoor galleries & terrace: 1200 mq\nDesign team: M.Carroll and E.Trezzani (partners in charge) with K.Schorn, T.Stewart, S.Ishida (partner), A.Garritano, F.Giacobello, I.Guzman, G.Melinotov, L. Priano, L.Stuart and C. Chabaud, J.Jones, G.Fanara, M.Fleming, D.Piano, J.Pejkovic; M.Ottonello (CAD operator); F.Cappellini, F.Terranova, I.Corsaro (models)\nConsultants: Robert Silman Associates (structure); Jaros, Baum & Bolles (MEP, fire prevention); Arup (lighting); Heintges & Associates (facade engineering); Phillip Habib & Associates (civil engineering); Theatre Projects (theatre equipment); Cerami & Associates (audiovisual equipment, acoustics); Piet Oudolf with Mathews Nielson (landscaping); Viridian Energy Environmental (LEED consultant)\nEnvironmental LEED (Leadership in Energy & Environmental Design) Gold rating expected from the US Green Building Council would make the Whitney New York City’s first certified LEED-Gold art museum\nPrincipal Materials Concrete, steel, stone, reclaimed wide-plank pine floors, and low-iron glass\nPhotographs by: © RPBW - Stefano Goldberg - Publifoto, © Nic Lehoux, © Timothy Schenck/ Whitney Museum of American Art"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:0aa6fae2-8315-4344-88a2-77de9e8bccb7>","<urn:uuid:fa8e1d3c-1ab2-4702-ab51-fa4dd98d8a32>"],"error":null}
{"question":"Which searching technique is most similar to the XOR operator's behavior in programming?","answer":"Boolean operators in search strategies and XOR in programming have similar principles of exclusivity. While Boolean operators like AND and OR in searching combine terms to include or exclude results, XOR in programming returns 1 only when exactly one of its operands is 1, making it unique like using specific search terms to find results that contain one term but not both terms simultaneously.","context":["Reference materials are a valuable resource when doing research.\nUse dictionaries to help with terminology. Encyclopedias provide background information, an overview of topics and issues and often lead you to further readings.\n- Use a dictionary or encyclopedia to find definitions and explanations of terms and concepts.\nOnline resources like Google and Wikipedia, while not always accurate, are a great way to orient yourself in a topic, since they usually give a basic overview with a brief history and any key points. Reminder: you cannot use Wikipedia as a source in your bibliography!\nDo an initial search for academic sources:\nHere is the African Studies Subject Guide to help you pick the right databases, search-engines and sources for your assignment.\nDevelop a search strategy to help you find books and journal articles:\nStep 1: Write your topic out in sentence or question form\n- How is African slavery portrayed in African writing?\nStep 2: Break your topic sentence up into main ideas or keywords\n- Africa, slavery, writing\nStep 3: Think of synonyms or alternate words to describe each concept\n- writing - literature\nTip: Use a thesaurus, dictionary, or encyclopedias to find alternate words.\nStep 4: Add \"Boolean operators\" (AND, OR) to make a complete search statement\n- Use AND to limit or narrow your search to results that mention all of your keywords.\n- Use OR to broaden your search to include synonyms.\n- Africa AND slavery AND writing\n- Africa AND slavery AND (writing OR literature) - Note: OR terms must be bracketed.\nStep 5: Add wildcards to search for all possible word endings\nA wildcard is usually represented by a *. This is also called truncation.\n- (writ* OR literature) AND Africa* AND slave*\nStep 6: Consider Key Phrase searching\nSome databases search each word separately. To ensure that your words are evaluated as a key phrase, enclose them in double quotation marks.\n- \"human rights\"\nStep 7: Evaluate your results\nIf you are finding too many or too few results, try these tricks:\nTo broaden your search (find more):\n- Find synonym for each keyword.\n- Search for a broader concept ('dog' instead of 'poodle').\n- Use wildcards/truncation.\nTo narrow your search (find fewer):\n- Add another concept or idea to your search with AND\n- Use more specific words ('poodle' instead of 'dog').\nThe library's search tool Omni is on the library homepage and lets you do just one search to find books, newspaper articles, journal articles, and other types of resources.\nUse the Step-by-Step Guide to Building your Search above to create your search string.\nOnce you have your results:\n- To see only BOOKS click on Book\n- To see only JOURNAL ARTICLES click on Articles and click on Peer-Reviewed Journals.\nBecause there will be many results, it is important to narrow your search.\n- Resource Type\n- Publication Date\n- Communication and Mass Media Complete\n- Columbia International Affairs Online\n- International Political Science Abstracts\n- PAIS Index (Political Science / International Affairs)\n- Worldwide Political Science Abstracts\n- GEOBASE (for Urban Studies)\n- Africa Development Indicators\n- Africa Knowledge Project\n- Historical Abstracts\n- JSTOR (multidisciplinary)\n- Google Scholar\n- Berg Fashion Library\n- Gale Literature\n- Literature Resource Center\n- MLA International Bibliography (literature)\n- Music Index Online\n- RILM Abstracts of Music Literature\n- FIAF International Index to Film Periodicals Database\n- Film & Television Literature Index with Full Text\n- Sociological Abstracts\nTIP: Click on \"Scholarly (Peer Reviewed) Journals\" box if the option appears. This way you will only see those articles are that appropriate for your research.\nALSO NOTE: Do not limit yourself to these databases. See full list of Databases By Subject.\nYour annotation should include:\n- A brief summary of the source\n- The source’s strengths and weaknesses\n- Its conclusions\n- Why the source is relevant in your field of study\n- Its relationships to other studies in the field\n- An evaluation of the research methodology (if applicable)\n- Information about the author’s background\n- Your personal conclusions about the source\nResources that may help you\nConsult the help guide on writing an annotated bibliography\n- Writing an Annotated Bibliography (University of Toronto)\n- How to Write Annotated Bibliographies (Memorial University)\n- Annotated Bibliography Purdue Online Writing Lab\nCiting your sources\nReferencing your sources is an important part of academic writing. Why?\n- it lets you acknowledge the ideas or words of others if you use them in your work\n- it helps you to avoid plagiarism\n- it demonstrates that you are using the scholarly record and that you can provide authority for statements you make in your term paper\n- it enables readers to find the source information\n- you don't have to cite common knowledge\nTips to avoid plagiarism\n- Quoting, Paraphrasing and Summarizing.\n- This is an excellent video on paraphrasing which can be a little tricky.\nTAKE NOTES: writing down page numbers and references throughout your research is a good way to save time when you need to quote and cite sources\nNEVER copy and paste material unless you cite it properly.\nAt the end of each paper/report you must CITE ALL SOURCES you have used, whether you quote them directly or paraphrase the ideas.\nWhen in doubt, ask for help!","There are three major bitwise operators that can be used to combine two numbers: AND, OR, and XOR. When I say that an operator is bitwise, it means that the operation is actually applied separately to each bit of the two values being combined. For example, if x = y AND z, that means that bit 0 of x is actually bit 0 of y ANDed with bit 0 of z. Make sense? Let’s look at each operator in turn and see how they work. Once we’ve been through all of that, we’ll be in a position to come up with some practical applications of each.\nThe AND Operator\nThere are two kinds of AND operators in the C language: the logical AND, and the bitwise AND. The former is represented by the && operator, and the latter uses the & operator. You’ve probably seen the first on e numerous times, as it’s often used in if statements. Here’s an example of the logical AND in action:\nif ((x == 5) && (y == 7)) DoSomething();\nin this case, you would expect that the function will only be called if x is 5 and y is 7. The bitwise AND works very much the same way, except that it works with two bits instead of two expressions. The bitwise AND operation returns 1 if and only if both of its operands are equal to 1. In other words, we have the following truth table for the bitwise AND:\nThe first column shows the result of a bitwise AND when combining explicitly defined bits. But it’s the second column that’s interesting. It says that if you AND any bit with 0, the result is 0; and if you AND any bit with 1, the result is the original bit. This little piece of information will be the key to making the bitwise AND work for us later, so keep it in mind. To finish up, let’s see an example of combining two words using the bitwise AND operator:\n0110 1011 1000 0101\n& 0001 1111 1011 1001\n0000 1011 1000 0001\nDo you see why we get the result we do? There is a 1 in the result only in the positions where the corresponding bits in both operands are also equal to 1. That’s all there is to the bitwise AND operation. Let’s move on.\nThe OR Operator\nJust as with the AND operation, there are two different types of OR in the C language. The logical OR uses the || operator, and the bitwise OR uses the | operator. A use of the logical OR might look something like this:\nif ((x == 5) || (y == 7)) DoSomething();\nIn this example, the function will be called if x is 5, if y is 7, or both.The only way the function is not called is if both of the conditions are false. The bitwise OR is very similar, in that it returns 0 if and only if both of its operands are 0. To illustrate this, we have the following truth table:\nOnce again, the second column is the interesting one.\nNote that whenever you OR a bit with 0, the result is the original bit, and whenever you OR a bit with 1, the result will always be 1. This will be the key to using OR effectively a little later on. For now, let’s just look at an example of using the bitwise OR operation on two words:\n0110 1011 1000 0101\n| 0001 1111 1011 1001\n0111 1111 1011 1101\nHere you can see that the result contains a 0 only when the corresponding bits in both of the operands are also 0. Now we’ve got just one more combinational operator to look at, and that’s XOR.\nThe XOR Operator\nThe XOR is a little strange because there is no logical equivalent for it in C, even though many languages include one. The XOR operation is symbolized by the ^ character in C. The term XOR stands for “exclusive OR,” and means “one or the other, but not both.” In other words, XOR returns 1 if and only if exactly one of its operands is 1. If both operands are 0, or both are 1, then XOR returns 0. To see this, take a look at the truth table for XOR:\nThe truth table here is interesting. Note from the first column that anything XORed with itself returns 0. This fact will lead to an interesting application of XOR later on. In the second column, we see that any bit XORed with 0 yields the original bit, and any bit XORed with 1 yields the complement of the original bit. This is something you may not have seen before: the bitwise NOT. It is a unary operator, meaning that it only takes one operand, like a negative sign. A bitwise NOT simply inverts all the bits in its operand, meaning that all 0s are changed to 1s, and vice versa. Now, let’s take a look at an example of using XOR on two words:\n0110 1011 1000 0101\n^ 0001 1111 1011 1001\n0111 0100 0011 1100\nSo there you have it, the last of the combinational operators, plus the only unary bitwise operator, the NOT. Before we can look at any applications of these, there is one other class of bitwise operator I need to show you, called shifts. Don’t worry, this will go pretty quickly, and then we can get on to the interesting stuff.\nThere are two bitwise shift operators, namely shift left and shift right. In C, they are represented by the << and >> operators, respectively. These operations are very simple, and do exactly what they say: shift bits to the left or to the right. The syntax for a shift operation is as follows:\n[integer] [operator] [number of places];\nA statement of this form shifts the bits in [integer] by the number of places indicated, in the direction specified by the operator. Probably the best way to visualize this is with an example. Take a look at the following code, which demonstrates a shift left.\n// Precondition: x == 0000 0110 1001 0011 // Postcondition: y == 0000 1101 0010 0110 x = x << 1;\nFrom this example, you should be able to see what’s going on. Every bit in x is shifted to the left by one place. When you do this, the MSB (most significant bit, remember?) of x is lost, because there isn’t another place to shift it to. Similarly, after a shift left, the LSB of x will always be 0. There is no position to the right of the LSB, and so there’s nothing to shift into the LSB, so it’s assigned a value of 0. Just to make sure you’ve got the idea of this, let’s take a look at a shift right:\n// Precondition: x == 0110 1111 1001 0001 // Postcondition: y == 0000 0110 1111 1001 x = x >> 4;\nHere, the bits are being shifted right by four places. Got it? Good. That finishes out the set of bitwise operators, so now we can finally get around to seeing what they’re good for."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:03ab2dbf-05fd-41f3-ad7b-c209db3232be>","<urn:uuid:64fee81c-bae5-43c8-9f2a-08a704e06e3c>"],"error":null}
{"question":"How does TEACCH's approach to autism intervention incorporate parental involvement, and what distinguishes its family support model from programs like NAS Earlybird and the Son-Rise programme?","answer":"TEACCH works in collaboration with parents as 'cross-cultural interpreters' between the neurotypical world and individuals with autism, providing parent training and support groups through seven centers in North Carolina. In comparison, NAS Earlybird is specifically designed for families of preschool children under 5, offering group training sessions and home visits that actually incorporate TEACCH techniques along with PECS and SPELL. The Son-Rise programme takes a different approach, positioning parents as the primary therapists who lead all teaching at home, focusing on 'joining' their child's world by copying behaviors to build trust before teaching new skills. While all three programs involve parents, they differ in their implementation - TEACCH provides comprehensive services including employment and residential programs, Earlybird focuses on early intervention, and Son-Rise emphasizes exclusive parent-led home-based intervention.","context":["In the field below enter the email address where you received the invitation letter.\nPlease enter a valid email addressSet & Continue\nTEACCH began at the University of North Carolina at Chapel Hill in 1972 under the direction of the late psychologist, Dr. Eric Schopler. TEACCH now includes seven centers in North Carolina that provide diagnostic evaluations, parent training, and social and support groups. TEACCH also operates model early intervention and supported employment programs and a residential/vocational program, as well as providing individual consultations and professional training workshops nationally and internationally.\nTEACCH developed the concept of the “Culture of Autism” to convey the idea that like cultures, autism spectrum disorders yield characteristic patterns of learning, thinking, communicating, and behaving. TEACCH proposes that teachers, parents, and therapists can function as “cross-cultural interpreters” who use specialized strategies to work back and forth between the expectations of the neuro-typical world and the ways individuals with autism understand and communicate.\nFeatures of the Culture of Autism include:\nThe overall TEACCH approach is called “Structured TEACCHing.” The fundamental principles of Structured TEACCHing are:\nStructured TEACCHing suggests that for teaching new skills or to minimize behavioral difficulties in situations, visual answers should be provided for the following questions:\nFor young and beginning learners, typical techniques for answering ‘where’ include defining learning spaces through furniture placement and blocking out sources of distraction. For older and more abstract thinking individuals, it is often useful to provide visual guidance (or options) about where to sit, where to put possessions, and how to get from place to place in the school or work setting. Techniques for showing ‘what’ involve visual or written directions (often accompanied with brief verbal directions) about the steps in an activity and the sequence of activities to be completed. Possible techniques for showing ‘how much’ or ‘how long’ include 1) having materials organized into containers that become emptier as the activity moves toward completion; 2) having a list of activities that are checked off or removed as they are completed; 3) using a visual timer that provides information about the passage of time and arrival of the end of the activity. Consistent with these techniques, ‘finished’ is shown by empty containers of materials, a completed checklist, or a visual and/or auditory cue from a timer or clock. When activities are completed, there is typically a visual cue for guiding the individual to look back to the schedule to find the next activity.\nStructured TEACCHing is appropriate for individuals with ASD at all ages and functioning levels, because the general principles are applied flexibly for each individual. That is, visual answers to these questions for a young or developmentally delayed child would look very different from those for a high school student or adult with average intelligence, but the general principles of using visual or written supports to organize time and space would still apply.\nStructured TEACCHing can help individuals with ASD learn new skills or participate appropriately in any setting, including but not limited to\nEvidence-Based research on TEACCH:\nComparative efficacy of LEAP, TEACCH and non-model-specific special education programs for preschoolers with autism spectrum disorders. Journal of autism and developmental disorders.1 This study compares three programs for children with ASD under age five. The first approach, TEACCH, is based on the “culture of autism” and cognitive-social learning theory. TEACCH involves changes to the environment such as visual schedules, work areas, and organizational systems to promote learning. The second approach, LEAP (Learning Experiences and Alternative Program for Preschoolers and their Parents) is based on a combination of applied behavior analysis (ABA) and common aspects of early childhood education, with a goal of reducing symptoms of autism that interfere with learning. One important difference is that TEACCH often educates children with ASD in a separate classroom away from typically developing peers, while LEAP uses an inclusive educational approach. These two approaches were compared with the third approach of non-specific special education. According to the researchers, the third approach “did not use practices aligned with any particular theoretical or conceptual model.”The study took place in public schools in four U.S. states. One hundred ninety eight children between age three and five years were randomized to one of the three programs. Communication skills, sensory and repetitive behaviors, social interaction and fine motor skills were measured at baseline and at the end of a year. All three programs were found to produce statistically improvements in child outcomes, but no important differences in outcomes by program.\n1 Boyd BA, Hume K, McBee MT, et al. Comparative efficacy of LEAP, TEACCH and non-model-specific special education programs for preschoolers with autism spectrum disorders. Journal of autism and developmental disorders. Feb 2014;44(2):366-380.","From ABA to PBS, methods for working with autistic children can be a confusing soup of acronyms. It's common for schools to cherry pick from each of these strategies and use a little of each. Others however will follow one specific model, such as ABA, so if you are considering these schools, it's important to be sure that this approach is right for your child and his or her developmental stage. Some commonly used approaches are:\nABA (applied behaviour analysis)\nWhat is it? ABA looks at the causes and the consequences of behaviour. It then makes changes to what happens before the behaviour occurs (the antecedent) and what happens after the behaviour occurs (the consequence). An antecedent may be a change to the environment, or it could involve a demonstration, for example how to interact with other children. Prompts (visual or verbal or both) may be used to indicate what should happen next. Any task, even things like ‘brush your teeth’ will be broken down into lots of smaller manageable steps. Over time the prompts and small steps will gradually disappear. The consequence is then a reward or positive reinforcement that helps the child to think, ‘When I do this, that enjoyable thing happens, so I’ll keep doing it.’\nHow can it help? ABA aims to develop and increase the good behaviours, and to minimise the behaviours that interfere with learning or relationships, or those that could be harmful. It can also help to teach basic skills.\nWho can it help? ABA is designed to work with all age groups. But it is controversial – while some parents are firm fans, others in the autism community are critical.\nPBS (positive behaviour support)\nWhat is it? PBS seeks to understand the reason for challenging behaviour. By assessing a person’s life history, physical health and emotional needs, it looks to minimise the triggers for this by making changes to the environment or to routines. So proactive strategies might be dimming the lights, tying hair back to stop it being pulled, giving rewards, or using a sign to confirm a task is finished. Reactive strategies include not responding to behaviour, distracting the child, giving reminders, or leaving the room. A good PBS plan has more proactive than reactive strategies.\nHow can it help? PBS aims to reduce anxiety or confusion that can lead to challenging behaviour. Additionally, PBS teaches a child more appropriate ways of communicating and getting what they want - using words or signs for example.\nWho can it help? Anyone with behavioural difficulties, including children with ASD. PBS can also be used with people with learning, developmental and social difficulties.\nPECS (Picture Exchange Communication System)\nWhat is it? PECS is a systematic way to teach a child to communicate using pictures. It provides an alternative means of communication allowing children who cannot talk or write to share their thoughts and opinions. For example, in simple terms, if a child wants a drink, he will give a picture of 'drink' to the adult who will then give him a drink.\nHow can it help? Originally, some believed PECS could hinder any progress with speech development. However, in practice even those previously uninterested become more open to trying other forms of communication, like speech. It also helps to alleviate frustration, minimising tantrums and challenging behaviour. It can be especially helpful to encourage children to interact and socialise.\nWho can it help? PECS was originally designed for children with autism spectrum disorder and related developmental disabilities, but can be used for other non-verbal children as well as adolescents who have a wide range of communicative, cognitive and physical difficulties.\nSCERTS (Social-Communication, Emotional Regulation and Transactional Support)\nWhat is it? SCERTS is an educational model which aims to teach children with autism the core skills that are needed in order to have the best outcomes in later life, and to be able to apply functional skills in different settings. It focuses on social communication (functional communication and emotional expression) and emotional regulation (coping with everyday stress). It may involve modifying the environment or providing learning tools like picture communications, written schedules, or sensory supports.\nHow can it help? SCERTS provides a framework so that families, teachers, and therapists can all work on a child’s individual plan, which will cover goals and objectives for learning both at home and at school. So for example they may work in tandem on getting a child to communicate food choices at mealtime through pictures, words or gestures; or to respond in the same way when a child shows sign of emotional overload, whether that might be through offering deep pressure, or clarifying tasks with visual prompts.\nWho can it help? Mainly children with ASD and their families, but the SCERTS model can also be used across a range of developmental abilities.\nSPELL (Structure, Positive approaches and expectations, Empathy, Low arousal, Links)\nWhat is it? A framework for understanding and responding to the needs of children and adults on the autism spectrum. The aim is to provide a basis for communication, and reduce the disabling effects of autism. Developed by the National Autistic Society.\nHow can it help? SPELL looks at ways in which we can change or structure the environment to make an individual feel safe and reduce their anxiety. It helps us to see the world through their eyes and understand which noises or smells or environments can be overwhelming.\nWho can it help? Any child on the autism spectrum. It works well with other interventions, especially TEACCH.\nTEACCH (Treatment and Education of Autistic and Communication-Handicapped Children)\nWhat is it? In collaboration with parents, TEACCH looks to understand the whole child or adult, focusing on their skills, interests and needs. Individual plans are designed to make the most of their strengths within a structured environment. This is sometimes called ‘structured teaching’ and involves looking at the physical structure or the organisation of the room, visual schedules (where/when/what the activity will be), visual information (what can we do in this work or play area), and task information (visually clear information on what the task is about).\nHow can it help? The TEACCH programme helps to prepare people with autism to live or work more effectively at home, at school and in the community by focusing on communication and social skills, independence, coping skills and skills for daily life. It provides a wide range of services including educational services, supported employment programmes, parent training and counselling and individualised treatment programmes.\nWho can it help? Any child or adult with autism. Some people may need some form of TEACCH in place throughout their whole life. TEACCH is a key element of the SPELL approach and the National Autistic Society’s Earlybird programme.\nNAS Earlybird (National Autistic Society Earlybird)\nWhat is it? A support programme aimed at families of preschool children. Parents sign up for group training sessions and home visits, which help them to understand autism, communicate with their child, and manage behaviour.\nHow can it help? It helps parents to understand their children and establish good practice at an early age by using techniques from TEACCH, PECS and SPELL.\nWho can it help? It supports parents of children under 5 years old. The NAS EarlyBird Plus Programme is for families whose child is 4-8 years old.\nWhat is it? Portage is a home-visiting educational service for pre-school children with special needs. It focuses on three main elements; child-led play, structured teaching and family focus. Portage teachers show parents how to take a ‘small steps’ approach to learning, breaking down long-term goals into achievable targets.\nHow can it help? Through playing and learning together at home, home life for the whole family can be improved. Plus, families can work together on the skills needed to interact and socialise outside of home.\nWho can it help? Children with special needs and their families.\nThe Son-Rise programme\nWhat is it? Sometimes also known as the Options Method, it's an alternative autism treatment where parents lead the teaching and therapy at home, where the child feels safest. Parents learn how to ‘join’ their child in their world so they can build a relationship before trying to teach any new skills. If the child starts flapping his hands or repeating an action, parents must copy. Parents learn to relate to the child in a way that the child can understand and a trust is built between them.\nHow can it help? By establishing trust, parents can then begin to teach through play, helping to develop communication and social skills.\nWho can it help? Children on the autism spectrum as well as children with other disabilities.\nAuditory Integration Training (AIT)\nWhat is it? AIT is an educational music programme based on the idea that some people are hypersensitive (over-sensitive) or hyposensitive (under-sensitive) to certain frequencies of sound. This can cause issues with concentration, understanding or communication, and may lead to irritability, tantrums, slow responses and tiredness.\nHow can it help? AIT involves a person listening to a selection of music that has been electronically modified so the frequencies have been changed. The aim is to reduce ‘distorted’ hearing and hypersensitivity of specific frequencies by ‘re-educating’ the brain, so that all frequencies can be heard equally well.\nWho can it help? People who have ADHD, ADD, dyslexia, hearing sensitivities, autism, developmental delays, poor concentration, speech and language problems as well as a variety of other special needs.\nSensory Integrative Therapy/ Sensory integration training\nWhat is it? A therapy to help people cope with sensory difficulties by exposing them to sensory stimulation in a structured, repetitive way. Sensory difficulties could be noise or taste-related, or they could be tactile or visual.\nTreatments could involve wearing a weighted vest, being brushed or rubbed with various instruments, riding a scooter board, or sitting on a bouncy ball.\nHow can it help? The idea is that through repetition, the brain will adapt and respond in a more ‘organised’ way to sensations and movement. Sensory difficulties can make everyday situations unbearable, but therapy can change the lives of whole families, enabling them to do everyday things like taking a trip to the supermarket together.\nWho can it help? Any child or adolescent, particularly with ASD, who struggles to process sensory information such as textures, sounds, smells, tastes, brightness and movement."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:f9d61956-f706-46a9-85f4-1015ccbae6a5>","<urn:uuid:5ee6a3cf-358c-44de-9c03-9345cbccbc63>"],"error":null}
{"question":"How do completion tools handle different types of challenging environments?","answer":"Completion tools and designs are tailored for each specific situation across diverse applications including deep water, unconventional, carbonate, sandstone reservoir, and HPHT environments. The design process combines experience and creativity to ensure optimal solutions that satisfy production criteria while maintaining safety. In risky environments, special attention is given to ensure optimal longevity and integrity of the completion tools.","context":["Session Chairs: Christopher Fredd, Schlumberger; Mark Rivenbark, Packers Plus\nThe industry has been applying new completion and stimulation approaches to address a wider range of challenges such as low permeability reservoirs, emphasis on efficient operations, the need for increased reliability, the need to increase production with reduced surface footprint, and reducing constraints on resources. Different approaches have included extended reach wells approaching 40,000 ft. in length, multi-laterals with instrumentation and hardware to enable monitoring, control, and access, and long horizontal wells with as many as 60 transverse hydraulic fractures. As the industry moves along the learning curve, these new innovations are providing a new perspective on completion and stimulation strategies that are bringing value to a wide range of reservoir conditions.\nThis panel session will provide an overview of different approaches to maximum reservoir contact and complex wells, and illustrate how innovation has turned challenges into success around the world.\nSession Chairs: Franck Marine, Schlumberger; Curtis W. Kofoed, ZADCO\nComplexity of reservoirs, increasing the production, optimising fluids recovery, keeping maximum of safety, are challenges that require more and more sophisticated and efficient completion tools and completion design. Some risky environments also require optimal longevity and integrity of the completion tools. The completion design has also some impacts on wellbore conditions. Diversity of applications (deep water, unconventional, carbonate, sandstone reservoir, HPHT, etc) also require different completions methods that must be considered case to case and tailored for each situation. The design then becomes an exciting challenge that requires combination of experience, and creativity to insure perfect solution that will satisfy and optimise the production criteria. Innovation is an absolute necessity to get safe, economical, and durable installation. During this workshop, some innovative and efficient solutions that permit to achieve these goals will be discussed.\nSession Chairs: Ahmed Al Shueili, BP Oman; Daniel Beaman, ZADCO\nStimulation of MRC or complex wells is sometimes compromised by the logistical and operational challenges involved in optimally performing and placing the stimulation treatment. The effect of this compromised design and execution is that sub-optimal stimulation treatments are pumped and placed, and provide little benefit to the reservoir. In turn, the well or development fails to deliver on its potential. The ability to successfully stimulate and subsequently evaluate the treatment is of the upmost importance and gives the ability of the well or development to deliver upon its potential. This can be the key to the success and future investment of these high value development projects.\nIn this session case studies will be presented that show how the stimulation treatment was designed and executed with a reservoir centric approach, and not compromised, to deliver the best treatment for the reservoir and allow it to deliver upon its potential.\nSession Chairs: Alfred Jackson, ZADCO; Ali Salem Al-Reyami, ADMA-OPCO\nThe growth in the number of intelligent completions installations along with longer and more productive maximum reservoir contact completions has created challenges and technology development needs. Many access and control challenges are present in these wells including, long reservoir intervals for logging and production surveillance, water or gas profile control over the life of the well, effective placement of stimulations and other treatments. This panel session will discuss the challenges for applying new technology in the region, present recent example of new technology to be applied in extended reach, maximum reservoir contact wells, and how technology can extend the life cycle of the well. This panel will also review the challenges, gaps, and limiters from sessions 2 and 3 as well as discuss various ways to find answers."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:876e8d1a-6b98-432e-b235-d3249445e280>"],"error":null}
{"question":"What are the potential health benefits of moderate alcohol consumption, and what are the specific cognitive impairments associated with alcohol-related dementia?","answer":"Moderate alcohol consumption (maximum 1 drink for women and 2 for men) has been linked to several potential health benefits, including lower risk of cardiovascular disease and mortality, reduced risk of type II diabetes, decreased cognitive deterioration, and increased HDL cholesterol levels. However, alcohol-related dementia presents numerous cognitive impairments, including memory loss, difficulty processing new information, trouble with familiar tasks, problems remembering words, poor decision-making, and difficulty concentrating. People with alcohol-related dementia may struggle to understand new information, have trouble recalling events, and experience balance issues even when sober. They may also exhibit mood swings, depression, irritability, and in some cases, paranoia and hallucinations.","context":["Alcohol consumption today involves two contradictory facets. While the advantages of drinking alcohol are seen in terms of its consumption in moderation, its abuse and excessive consumption is currently a major public health problem, resulting in one of the leading causes of death and disability worldwide.\nAlcohol dependence after prolonged consumption\nSeveral studies have shown that moderate alcohol consumption may offer potential health benefits, although at no time is its intake without risk. Moderate consumption is considered to be a maximum intake of 1 drink (35cl 5% beer or 15cl 12% alcoholic strength glass of wine) for women and 2 for men. The intake of these amounts of alcohol has been related to a lower risk of disease and mortality from cardiovascular accidents and with other possible benefits such as reducing the risk of type II diabetes or cognitive deterioration, or raising HDL cholesterol levels. However, it is worth remembering that maintaining a healthy diet and being physically active have many more health benefits and are measures that have been widely studied and recognized as beneficial in this regard.\nThe most relevant aspect of alcohol consumption, however, is related to its negative impact. It is currently a major public health problem that is responsible for 2.2% and 6.8% of age-standardized deaths in women and men, respectively, in addition to a significant contribution to disability and limitation of quality of life.\nIt is defined as more than 3 drinks per day or more than 7 drinks per week for women and men over 65 years of age, and more than 4 drinks per day or 14 drinks per week for men under 65 years of age. In addition, binge drinking is referred to as 4 or more drinks in two hours for women and 5 or more for men. It also includes any consumption by pregnant women or persons under 21 years of age. Most people who binge drink will not necessarily be alcoholics or alcohol dependent.\nHeavy drinking can increase the risk of serious health problems. In the short term it is associated with injuries, traffic accidents, falls, drowning and burns, violent behavior or alcohol intoxication among others. Among pregnant women it can trigger miscarriages or fetal alcohol spectrum disorders (FASD).\nAdditionally, over time, long-term excessive alcohol consumption can promote:\n- Chronic diseases such as high blood pressure, heart disease, stroke, liver disease and digestive problems, as well as deficiencies of essential vitamins (Wernicke-Korsakoff Syndrome due to Vitamin B1 deficiency).\n- Increase the risk of cancer of the breast, mouth, throat, esophagus, larynx, liver, colon and rectum.\n- Promotes the weakening of the immune system.\n- Learning and memory problems, including dementia and poor school performance.\n- Mental health problems, such as depression and anxiety.\n- Social problems, such as family problems, work problems and unemployment.\n- Alcohol use disorders, or alcohol dependence.\n13.5 million variants\nCenters for Disease Control and Prevention. Alcohol and Public Health [May 2022].\nNational Institute on Alcohol Abuse and Alcoholism. Alcohol Consumption [May 2022]\nMayo Clinic. Nutrition and healthy eating. Alcohol use: Weighing risks and benefits. [May 2022]\nZhou H et al. Genome-wide meta-analysis of problematic alcohol use in 435,563 individuals yields insights into biology and relationships with other traits. Nature Neuroscience, 25; 2020, 23(7):809-818.","Alcohol is one of the most misused substances in the United States. According to the 2019 NSDUH, almost 15 million Americans over 12 struggle with alcohol use disorder. Unfortunately, alcoholism leads to various health problems. But, can alcoholism cause dementia?\nWhat is Dementia?\nIndividuals often confuse dementia and Alzheimer’s. While Alzheimer’s is a common cause of dementia, dementia is a general term for memory loss and cognitive impairments caused by aging.\nDementia is the result of abnormal brain changes. The disorders grouped under the term dementia trigger a decline in thinking skills. Dementia can severely impact a person’s independent function and daily life as well as behaviors, feelings, and relationships.\nHow is Dementia and Alcoholism Linked?\nAlcohol directly affects the brain’s functions. Alcoholism or alcohol use disorder damages brain cells and interferes with using good judgment and decision making.\nAdditionally, many alcoholics develop nutrition issues because of drinking and poor eating habits. All these factors contribute to developing alcohol-related dementia.\nWhat are the Signs and Symptoms of Alcohol-Related Dementia?\nCommon signs and symptoms of dementia and alcoholism include:\n- Memory loss\n- Difficulty with familiar tasks\n- Trouble processing new information\n- Depression and irritability\n- Poor judgment and loss of inhibition\n- Problems remembering words\n- Erratic behaviors\n- Personality changes\n- Difficulty concentrating\n- Poor decision-making\n- Paranoia and hallucinations\n- Damage to liver, pancreas, or stomach\n- Numbness in legs and arms\n- Slow, wide, stumbling gait\n- Poor temperature control\n- Sleep pattern disturbances\nMemory Problems Due to Dementia and Alcoholism\nIndividuals with alcohol-related dementia generally have issues with their memory. For example, they may struggle to understand new information or details of a conversation. They may also have trouble recalling events such as where they went on vacation or where they lived before.\nDementia, Alcoholism, and Balance Issues\nAlcohol-related dementia often causes balance issues even when individuals are sober. They may fall over because alcohol damages the part of the brain that controls coordination and balance.\nCan Alcoholism Cause Dementia Mood Swings?\nBith dementia and alcoholism can cause mood swings. People may struggle with apathy, irritability, and depression. Mood swings make it harder for a person to stop drinking and even harder for loved ones to help.\nWho Can Develop Alcohol-Related Dementia?\nAny person who drinks alcohol heavily over many years can develop alcohol-related dementia. It is unknown why some heavy drinkers develop dementia, and others don’t. It may be a difference in diet and other lifestyle factors.\nTypically, alcohol-related dementia affects men over 45 struggling with chronic alcoholism. However, women may also develop this disease since alcohol affects them more substantially than men. To reduce the risk of dementia and alcoholism, experts recommend drinking no more than two drinks per day.\nDoes Alcohol Speed Up Early-Onset Dementia?\nAccording to Medical News Today, a study out of France found that alcohol use disorder is a major factor in all types of dementia. Researchers studied one million people discharged with alcohol-related brain damage between 2008 and 2013.\nSimultaneously, researchers also studied one million people diagnosed with alcohol use disorder during that time. Nearly 40 percent of early-onset dementia cases were attributed to alcohol-related brain damage. At the same time, 18 percent had other alcohol use disorders.\nCan Alcohol-Induced Dementia be Reversed?\nUnfortunately, dementia is an irreversible disease. However, in some cases, alcohol-related dementia is reversible. Individuals who seek early treatment for dementia and alcoholism often see improvements.\nSince alcoholism can cause dementia, individuals have to be willing to help themselves to reverse the damage. Professional help is often needed when individuals struggle with dementia and alcoholism.\nCan Alcohol Permanently Damage your Brain?\nMost people know alcohol has short-term and long-term effects. But, very few people realize that chronic long-term alcohol abuse does permanent damage to your brain. This damage includes:\n- Withdrawal symptoms are often severe and damage brain cells. The most dangerous symptoms include seizures and hallucinations. For instance, 5 percent of alcoholics in withdrawal experience delirium tremens (DTs).\n- Neurotransmitter damage slows down communication between areas of the brain\n- Brain shrinkage results from gray matter loss: gray matter contains cell bodies and white matter, which controls pathways.\n- Cognitive Impairment affects verbal skills, mental processing, memory, learning, and impulse control. The areas of the brain related to problem-solving and impulse control are often damaged the most. This damage typically results in alcoholism and dementia.\nWhen Dementia and Alcoholism Leads to Wernicke-Korsakoff Syndrome\nOne syndrome of dementia and alcoholism is called Wernicke-Korsakoff syndrome or WKS. This syndrome is really two disorders that occur both independently and together. The two disorders are Wernicke’s encephalopathy and Korsakoff syndrome or Korsakoff psychosis.\nWernicke’s encephalopathy involves abnormal eye movements, unsteady gait, and confusion. At the same time, alcohol is not a direct cause of this syndrome as much as brain cell damage. Thiamine deficiency or Vitamin B1 deficiency is common with dementia and alcoholism due to a poor diet.\nWhat are the Symptoms of Wernicke-Korsakoff Syndrome?\nThe symptoms of WKS include:\n- Retelling the same stories\n- Asking the same questions over and over even though they have been asked and answered\n- Repeating the same information in a conversation\n- Unaware these symptoms are happening\nSimultaneously, individuals with WKS reason well, make accurate deductions, are witty, and complete mental games such as chess.\nWhat is Korsakoff Psychosis?\nWhen Wernicke’s syndrome is not treated correctly, the result is Korsakoff psychosis. Korsakoff impairs memory and other cognitive functions. Fabrication is the most common symptom of Korsakoff.\nFabrication or confabulation is making up detailed and believable stories to cover the gaps in memory. People with this form of dementia struggle to learn new things while still functioning in other mental abilities.\nWhat are Early Warning Signs of Alcohol-Related Dementia?\nIf you or a loved one has continually refused help for their alcohol use disorder, it is crucial to watch for early signs of dementia. Since many people with alcoholism don’t understand the damage alcohol does to the brain, signs of dementia may push them to seek help.\nEarly warning signs include:\n- Unexplained personality changes\n- Trouble solving complicated problems\n- Getting lost in a familiar area\n- Short-term memory loss\n- Cognitive issues\n- Poor decision-making\n- Confusion about time and place\n- Trouble communicating\nIs there Testing for Alcohol-Related Dementia?\nSeveral medical exams are performed to diagnose alcohol-related dementia. Some doctors may insist a person stop drinking before doing these exams, but most doctors do not.\nDoctors will examine the nervous and muscular systems. They look for abnormal eye movement, increased pulse, and muscle weakness. Blood work is also typically done to check nutrition levels.\nTreating Dementia and Alcoholism\nChronic alcohol abuse is the cause of alcohol-related dementia. So, the first step in treating dementia and alcoholism is to stop drinking alcohol. When caught early, individuals with alcohol-related dementia are likely to see improvement with diet changes and no alcohol.\nAdding vitamin B1 to your diet may help minimize the nutritional deficiency damage of alcohol use disorder. For people with Wernicke encephalopathy, vitamin B1 may prevent or reduce the risk of Wernicke-Korsakoff syndrome. However, once Korsakoff psychosis develops, improving memory loss is nearly impossible.\nTherapies for Dementia and Alcoholism\nJust like alcoholism, most people with dementia have irregular behavioral patterns. In addition, dementia causes psychological distress resulting in a decline in quality of life.\nAlthough there are medications such as sedatives, antipsychotics, and antidepressants, the side effects cause most people to stop taking them. Various types of therapies are useful in treating the psychological and behavioral symptoms of dementia and alcoholism.\n- Occupational therapy – helps overcome functional impairments of dementia that affect daily life\n- Physical therapy – exercise can help with coordination and overall well-being by reducing falls, improving sleep, and treating depression\n- Holistic therapy – yoga and mindful meditation can induce calm and stillness\n- Music and art therapy – music therapy improves memory and well-being while art engages attention and meaningful stimulation\nHow Can Family Support Someone with Dementia and Alcoholism?\nNo matter what type of alcohol-related dementia your loved one struggles with, family support is crucial in their daily life. Quitting alcohol by yourself is very challenging. With family support, a person is more likely to have a lasting recovery.\nAdding the struggles of alcohol-related dementia means family support is even more important. Family members can help in the following ways.\n- Support their recovery – challenges of recovery can change daily\n- Assist them in improving their skills – try not to do things for them but with them\n- Ask professionals for advice\n- Encourage daily journaling\n- Break down large tasks into smaller steps\n- Be patient\n- Use short sentences and give them time to respond\n- Label cupboards and arrange rooms making it easy to find things\n- Encourage a healthy diet\n- Provide them transportation to support groups and therapy for alcoholism\nSeek Help for Dementia and Alcoholism at North Jersey Recovery\nThe best way to prevent alcohol-related dementia is to stop drinking alcohol. For many, this is easier said than done. But, at North Jersey Recovery, we design personalized treatment plans to give you the best chance of recovery. Contact us today and find out how we can help you."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:511d94e2-298f-469a-9f2a-9c3c77952c7d>","<urn:uuid:08a2bffa-2836-4e56-9ff9-7974c1422042>"],"error":null}
{"question":"As someone interested in data privacy regulations, what are the main differences between how TradeLens handles personal data protection under GDPR and how healthcare providers manage patient data under HIPAA?","answer":"Under GDPR compliance, TradeLens operates with IBM as a 'processor' of personal data, while participants are 'controllers' responsible for obtaining consent. IBM's Data Processing Addendum specifies the types of data processed, security measures, and processing locations. In contrast, under HIPAA, healthcare providers (as covered entities) must follow strict privacy rules for Protected Health Information (PHI), including providing Notices of Privacy Practices to patients, obtaining specific authorizations for data sharing, and fulfilling patient requests to access their data within 30 days. While both frameworks protect personal data, HIPAA is specifically focused on healthcare information with detailed requirements for patient rights and access, while TradeLens follows broader GDPR requirements for general personal data protection.","context":["Supply-chain companies that are digitizing and leveraging data and document sharing will have the mostto gain in the new shipping economy.\nBut there remains a concern: Companies worry that moving their own and their customers’ proprietary information to digital platforms exposes them to a sinister and serious list of security risks.\nIn many ways, the concerns are justified; there is a lot of nasty stuff out there. Here are some most people and businesses should be aware of:\nSo, what should smart businesses do if they want to take advantage of the opportunities of the digital revolution? Hesitating,and missing out on the latest technological advances might mean continuing practices that are just as risky if not more so.\nTake documentation for example:Submitting forms via email comes with the risk of your data and document sending up in the wrong hands if they get forwarded or posted in an insecure way. Even sitting in your network behind a firewall, your documents and data can be exposed to a world of malware that's sometimes introduced to your system by trusted partners without their knowledge.\nBut security and creativity need not be in a tug of war.\nHere are five ways that TradeLens offers the benefits of digitization while also ensuring business and personal information is kept secure.\nTradeLens uses Hyperledger Fabric, a permissioned blockchain that ensures the immutability and traceability of shipping documents while protecting data using the world's most advanced encryption technology. The IBM Blockchain platform is built on Z/System and Linux ONE security to prevent ransomware from locking data. Plus, permissioned blockchain protects against spoofing because every organization and user have cryptographic certificates.\nPermissioned is an important differentiator. It's very different from non-permissioned, public blockchains that are vulnerable to a “51% attack.” With permissioned blockchains, networks are able to choose their members, specify their level of access and determine their level of control. Tampering and repudiation are discouraged because the IBM Blockchain Platform records who did what. TradeLens members are also protected against information disclosure by data segregation with channels.\nWe understand that, in an industry built on trust, no one can afford to jeopardize the security of their or their customers’ private data.\nUnsurpassed security standards and unparalleled processes ensure every entity and user in the TradeLens ecosystem belongs in the ecosystem. It's an invite-only network: organizations are on-boarded after thorough checks. The process of adding users includes accurate management and monitoring of who has access to what using the Permission Matrix.\nAccess is secured through the use of user IDs and passwords managed by IBM ID. And authentication can be delegated through the use of OpenIDConnect Federated Authentication and OAuth2.\nBusinesses maintain complete control of their documents, including organization onboarding, certificates creation, etc. through the Document Store — on a segregated and encrypted blockchain node. To use blockchain-enabled document sharing, participants post trade documents to the IBM Blockchain Document Store, housed on a blockchain node. Any documents stored in nodes managed by IBM that contain personal data are considered to be \"processed\" by IBM. IBM will process such personal data only within the limits of the Data Processing Addendum (DPA) with the participant.\nTradeLens uses the same comprehensive security embedded in mission-critical platforms that IBM manages for Fortune500 companies. All TradeLens users benefit from the same standards of security,robustness, and scalability that underpin platforms used by some of the world's largest corporations.\nFrom the earliest development, through implementation and its continuous evolution and support, TradeLens relies on IBM secure development processes that include source-code reviews, industry-standard encryption algorithms, and vulnerability management. Never ones to take security for granted, we use third-party specialists for penetration testing and ISO 27K compliance.\nEvery communication and all data exchanged to and from TradeLens is secured to the highest level in the world on HTTPS over TLSv1.2. This guarantees all our APIs are secured, and an imposing Cloudflare Firewall protects the entire solution from a multitude of threats,including Denial of Service.\nAll data handling processes on TradeLens meet stringent GDPR requirements. IBM is an authority in General Data Protection Regulation (GDPR) which governs the use of personal data of EU citizens by third parties.\nIBM's rigorous compliance with GDPR ensures that the TradeLens platform and its members comply. IBM is a “processor” of personal data provided to the TradeLens platform; participants are considered “controllers” of that data, obligated to obtain or verify consent from those customers whose data will be processed by IBM.\nIn addition, IBM's DPA specifies, a) the types of personal data that IBM will process in offering the TradeLens solution, b) the types of processing activities that IBM may undertake with personal data, c) the security measures in place to protect personal data, d) the location(s) where the processing activities will take place, and e) the procedure for requests for access and/or deletion of personal data contained on IBM's systems.\nWe understand that, in an industry built on trust, no one can afford to jeopardize the security of their or their customers’ private data. TradeLens is sworn to protect ecosystem information, ensuring appropriately permissioned organizations can only access the information they are permitted to see. That translates to higher standards than mandated by GDPR or any other authority.\nTradeLens standards are anchored by the practice of using blockchain technology to implicitly ensure people only have access to data and documents appropriate to their business and role.\nTradeLens recently obtained ISO27K certification, which includes:\nThe processes supporting these certifications include P-D-C-A cycle (Plan, Do, Check, Act) and continuously test and improve our solution security capabilities.\nConnecting supply chain partners so they can share information without hesitation or reservation is an endeavor that hinges entirely on trust. It’s because of that that we focus with equal intensity on both innovation and security.","Healthcare facilities gather and manage volumes of critical patient information that, if lost or stolen, could result in patient identity theft and delayed care. In 1996, the Health Insurance Portability and Accountability Act, or HIPAA, prompted lawmakers to build a set of privacy laws governing the management and security of patient information.\nUsing this HIPAA security rule checklist, you can see how these standards apply to your organization and take steps to obtain compliance.\nWhat is the HIPAA Privacy Rule?\nThe Department of Health and Human Services issued a set of orders that standardized privacy law for all individuals and organizations that would manage patient health data. These accountable organizations are known as covered entities and are liable for all mandates expressed in the Standards for Privacy of Individually Identifiable Health Information, also known as the HIPAA Privacy Rule.\n“A major goal of the Privacy Rule is to assure that individuals’ health information is\nproperly protected while allowing the flow of health information needed to provide\nand promote high quality health care and to protect the public’s health and well being.” – United States Department of Health and Human Services\nThese privacy standards arrived as medical professionals started to digitize medical records. Taking advantage of digital documentation allows all healthcare-related organizations to better serve patients, since managing digital records is far more efficient than managing hard copies of medical records.\nTo Whom Does the HIPAA Privacy Rule Apply?\nThe HIPAA Privacy Rule applies to what are referred to as covered entities. These agencies assist in the administration of healthcare services, to include treatments, insurance payments, and more.\nWhat are Covered Entities?\nA covered entity includes private medical practices, hospitals, and any auxiliary organization that must access protected health information to operate. Often, there are several healthcare-related agencies working together to assist a patient in receiving the medical care that they require.\nThe Privacy Rule identifies a covered entity as one of the following:\n- Health Plans\n- Insurance providers\n- Medicare/Medicaid insurers or supplemental insurers\n- Employer-sponsored plans\n- Government-sponsored plans\n- Church-sponsored plans\n- Coop plans\n- Healthcare Providers\n- Healthcare Clearinghouses\nThe Privacy Rule also applies to non-covered entities that serve as third-party vendors or business associates to a covered entity.\nWhat is Protected Health Information (PHI)?\nProtected Health Information, or PHI, is the formal term for “individually identifiable health information.” Covered entities manage PHI in accordance with their duties and are under scrutiny to protect patient identities and privacy by abiding by all HIPAA compliance standards pertaining to lawful use of PHI.\nDownload Our HIPAA Compliance Checklist\nWhat are HIPAA Authorizations?\nIn the event that a covered entity needs to share PHI with another individual or agency, but that individual or agency is not otherwise permitted access to a patient’s PHI under HIPAA Privacy law, covered entities may seek a patient authorization.\nHIPAA authorizations must be signed by the patient and lay out clearly who the authorization is for, the purpose of the authorization, and when the authorization expires. The covered entity should also define any contingencies or parameters laid out by the patient to meet the authorization’s purpose.\nAn example of a HIPAA authorization could be a mental health patient that agrees to share his/her therapy notes in a full psychological evaluation. This is a common scenario for veterans providing medical evidence during a PTSD disability claim. Even though filing a disability claim involves due process and legal discovery, investigators may not access those medical records without a signed patient authorization.\nHIPAA Protected Health Information Uses and Disclosures\nWhat is a Notice of Privacy Practices (NPP)?\nAll covered entities must disclose a notice of privacy practices, or NPP, that outlines the patient’s rights according to HIPAA privacy law and PHI. The NPP should also explain how a patient may file a complaint against a covered entity that they feel violated their rights under HIPAA privacy law.\nNPPs are items commonly found in registration paperwork when a patient sees a medical professional for the first time. The documentation explains how a covered entity may use the patient’s PHI within the bounds of HIPAA compliance.\nYour HIPAA Security Checklist:\nA HIPAA security checklist can help you identify where your business operations fail to meet HIPAA privacy requirements. You can use the checklist below to perform an internal audit. Or you can use the checklist as a way to gauge how seriously your organization takes HIPAA compliance.\nPatient Access and Consent\n- Have you established a process to help patients access their PHI? In this day and age, many covered entities make sure that patients can access their PHI safely online, even if another covered entity maintains the PHI database. Regardless, your organization should have clear policies and procedures to help patients view their PHI.\n- Do you have a process for accepting and fulfilling PHI copy requests from patients? When a patient requests copies of their PHI, HIPAA compliance dictates that you give the patient a copy in the requested format (hard copy or digital) within 30 days of their request.\n- If your firm decides to charge patients a fee for copies of their PHI, do you make those fees accessible? HIPAA compliance requires covered entities to fulfill PHI copy requests to patients at a reasonable cost. Prohibitive costs do not properly reflect the amount of labor and expenses required to fulfill PHI requests. Agencies that charge too much may be doing so intentionally to keep from having to be HIPAA compliant.\n- Are your authorizations specific, to include uses, recipients, disclosures, and expiration dates? Vague HIPAA authorizations do not protect your organization or the patient. All critical details of the authorization should be clearly spelled out according to the patient’s expectations.\n- Do your authorizations use “plain English,” as opposed to medical jargon and elusive clinical terms that the patient will not understand? If it appears that the patient had no clue of what they were signing because of convoluted words and phrases, your authorization could be in violation of HIPAA privacy law. It’s critical that your authorizations use language that is understandable to the average patient.\n- Do you secure the patient’s signature and date on every authorization? HIPAA authorizations are invalid unless they have the patient’s signature, as well as the date on which it was signed.\n- Do you store your HIPAA authorizations in a secure location and properly dispose of them once they are no longer needed? Losing a HIPAA authorization could open your organization up to legal action from the patient. It’s critical that you properly store and share authorizations in accordance with HIPAA privacy law.\nNotice of Privacy Practices (NPP)\n- Do you have an NPP included in your new patient paperwork? To be HIPAA compliant, you should onboard every new patient or client with an NPP so that those individuals understand their PHI rights from the start of your services.\n- Do you have your patients or clients confirm that you informed them of their rights according to HIPAA privacy law? Having your patients or clients confirm in writing and with a signature that they have read and understood your NPP protects you as a covered entity.\n- Do you prominently display your NPP on the premises and/or clearly on your website? Demonstrating that you publicize your NPP for all to see further protects you against patients claiming that you did not inform them of their rights under HIPAA privacy law.\n- Do you have policies and procedures in place to manage patients with concerns that you’re not complying with your NPP? It is possible that some patients may accuse you of not advising them of their rights per HIPAA privacy law. More importantly, a patient or client may fall through the cracks. Either way, you should have a clear process on how to manage those complaints and rectify them immediately.\n- Do your day-to-day operations align with your NPP and HIPAA Privacy Law? You should perform routine audits of your business operations to ensure that you’re not merely paying lip service to HIPAA privacy law.\nEmployees and Business Associates\n- Do all of your staff members understand HIPAA privacy law, as well as workplace policies and procedures relevant to PHI? Much of your HIPAA compliance pertains to consistent adherence by your staff. As a covered entity, it is your responsibility to ensure that every employee understands HIPAA privacy law and how they must manage PHI in their current role.\n- Have you trained your employees and collected proof (such as signed documentation) that they received the proper HIPAA compliance training? Similar to how you have patients sign and confirm that they had read and understood your NPP, you should include attestation documentation at the conclusion of HIPAA compliance training for your staff.\n- Do you have a process in place for employees to report HIPAA non-compliance without fear of reprisal? Ideally, you should create a way for employees to report non-compliance anonymously. This approach ensures that managers and lower-level employees alike are held accountable in accordance with HIPAA privacy law.\n- Do you collect confidentiality agreements from your employees and independent contractors? Employees and independent contractors of covered entities are known as non-business associates. Since it is likely that these people will come into contact with or manage PHI as part of their job description, it’s important that you collect confidentiality agreements from each of them.\n- Do you choose your business associates carefully, to include carrying out due diligence on that organization’s privacy policies and procedures? You could be held liable if one of your vendors mismanages PHI that your business maintains. Part of being HIPAA-compliant is ensuring that you only work with vendors that also understand HIPAA privacy law.\n- Do you maintain a list of all your business associates and third-party vendors? If your organization manages PHI, it’s very likely that most or all of your business associates and third-party vendors may come into contact with that PHI. It’s critical that you maintain an up-to-date record of all external parties with whom you do business.\n- Have you established the proper contracts (business associate agreements) with your business associates and third-party vendors that contain HIPAA-compliant directives on all matters pertaining to PHI? You should disclose to your business associates that managing PHI is part of your business operations. This informs your vendors that they must maintain HIPAA compliance, especially if their services also involve the use of PHI.\n- Do you reexamine your business associate agreements every year, to include updating your list of business associates? The nature of your relationship with third-party vendors and business associates can change year-over-year as you and the other party scale your respective operations. As such, you may need to update portions of your business associate agreements to remain HIPAA-compliant.\n- Do you have an up-to-date network diagram? Network diagrams show you all possible attack vectors from which hackers and malware might enter and try to steal or destroy PHI.\n- Do you have basic cybersecurity protocols in place? Due to the sensitivity of PHI, it is critical that you maintain all the necessary firewalls, malware protection, and monitoring to keep your and your patient’s information secure.\n- Do you have a plan to respond to an incident or breach? The initial moments after a security breach are often the most critical. Having a plan in place to quarantine the incident, diagnose the root cause, patch the intrusion, and report any damage will protect PHI under your organization’s care. Also, it will help your cybersecurity team update its tools, policies, and procedures to deal with similar intrusions more efficiently.\n- Has your staff received training on phishing attacks and how to prevent them? Sometimes the biggest threat to your organization is an employee clicking on an unknown link and releasing malware onto the company network. Making sure that your employees know how to safely deal with phishing attacks can drastically reduce your cyber risk.\nKey Takeaways: HIPAA Security Rule Compliance Checklist\nUsing the checklist above, you can take initial steps to become HIPAA-compliant in accordance with privacy laws pertaining to PHI. Failing to comply with HIPAA Privacy Law can result in financial penalties and patient lawsuits.\nRSI Security is an agency dedicated to assisting covered entities in their quest to acquire and maintain HIPAA security compliance. Our team of cybersecurity specialists can help you create a personalized HIPAA security rule compliance checklist and establish the necessary safeguards to protect your PHI against negligence or abuse.\nDownload Our Complete Guide to Navigating Healthcare Compliance Whitepaper\nNot sure if your HIPAA or healthcare compliance efforts are up to snuff? Unsure about where to even start? Download RSI Security’s comprehensive guide to navigating the HIPAA and healthcare compliance labyrinth. Upon filling out this brief form you will receive the whitepaper via email."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:6587ddd4-0fa5-47ac-8189-e2e19c60f17d>","<urn:uuid:e152642d-e4fa-4a0b-9ec7-584621db2de8>"],"error":null}
{"question":"I'm looking to enhance my beauty routine - what are the differences in long-term safety considerations between edge control styling with hairspray and semi-permanent BB cream treatments?","answer":"Edge control styling with hairspray is generally safer, with minimal long-term risks. While hairspray may dry out the hair if used too frequently, this can be mitigated by using a silk scarf at night to prevent dryness and breakage. In contrast, semi-permanent BB cream treatments pose significant safety concerns including potential allergic reactions, infection, granulomas, scarring, organ toxicity, and cell damage. The BB cream pigments often contain titanium dioxide, which can accumulate in the skin over time, potentially triggering immune responses and creating long-term uncertainty. Additionally, these treatments are not FDA-approved, and the pigments may cause permanent discoloration if exposed to laser treatments.","context":["Baby Hairs: Styling Tips for Your EdgesSeptember 24, 2020\nWhether you want to keep your baby hairs pressed down or swoop and swirl ’em, taming your edges is a great way to polish off your look. If you’re not sure how to style those flyaways by your forehead, though, fear not. We tapped Robin Groover, African Pride Brand Educator and Founder of Too Groovy Salon, for her expert tips. So grab your boar bristle brush, some edge control gel and keep reading, because she’s breaking down the basics of edge styling, below.\nSTEP 1: Gather Your Tools\nBefore styling, make sure you have all the tools necessary to create your look. “You’ll want a small bristle brush, small rat tail comb or an edge comb and a good edge control,” says Groover. “The edge control products range from matte pomades for a straight, invisible, soft hold to ringing gels with maximum hold.” She explains that your choice of product should align with the look you’re trying to achieve. Groover’s personal favorite? The African Pride Olive Miracle Super Hold & Smooth Edges, which doesn’t flake, stick or feel greasy.\nSTEP 2: Prep the Hair\nNow that you have your tools, Groover suggests pulling your hair away from your face, securing it with a headband or scrunchie and figuring out how much hair you want to work with for your style. “Choose baby hairs from the natural hairline if you want a more natural look, but for a more designer look, pull some baby hairs from past the natural hairline,” she says. “Once the look is identified, brush the hair forward in a natural fall toward the front — similar to how you would brush down bangs.” You can opt to wet the hair in this step, too. It will make the hair easier to work with.\nSTEP 3: Apply Your Edge Control Product\nTake a small amount of your edge control product into your fingers and smooth it onto your baby hairs. The more product you use, the more definition your style will have. Once the hair is pressed down, grab your boar bristle or edge brush. “Brush the baby hairs flat and outline the desired look with the pointed end of a comb,” says Groover. “Using a tracing motion to outline the edges will give you artistic swirls and waves and help you frame the face.”\nShe explains that when doing spirals, you want the look to flow with the natural swirls of your hair. “I always make sure to use an edge control with slip to help boost the curls and swirls with ease of motion when arranging the design.” For a clear gel, we recommend the Carol’s Daughter Black Vanilla Edge Control Smoother.\nSTEP 4: Mist on Hairspray\nWhile your edge control product will have enough hold to keep your hair in place throughout the day, you may want to consider sealing in your style with hairspray if you plan on working out, doing an activity or wearing a wig. “Using hairspray may dry the hair out if used too often,” Groover cautions. “But the technique is always useful when applying lace front wigs.”\nSTEP 5: Sleep in a Silk Scarf\nWant to keep your style intact overnight? Try using a silk scarf, like the Kitsch Satin Sleep Scarf, to protect your hair. “Using a silk scarf at night will prevent dryness, frizz and breakage,” says Groover.","What You Need To Know About The Permanent BB Cream Trend\nImagine waking up every morning with a perfectly even complexion — the kind that looks like you’ve had a team of pros airbrushing your face overnight. That’s the promise of semi-permanent BB cream (a.k.a. ‘BB Glow’). But is it too good to be true? The AEDITION asks the experts.\nWhat Is Semi-Permanent BB Cream?\nBB Glow is essentially a form of microneedling that has made its way stateside from South Korea. Like typical microneedling, an aesthetician first applies numbing cream to the face. Unlike the traditional treatment, semi-permanent pigments are added to the mix to tint the complexion for about six months to one year. “This procedure is meant to improve the overall appearance of the skin including uneven skin tone, fine lines, wrinkles, hyperpigmentation, age spots, post-acne pigmentation, and large pores,” explains Sejal Shah, MD, a board certified dermatologist in New York City.\nSo, what makes semi-permanent BB cream different from a traditional microneedling procedure? “With anti-aging microneedling formulas, the ingredients absorbed through the channels are mostly skin-centric and nutritional raw ingredients,” says permanent makeup pioneer Dominique Bossavy. As she explains it, they are usually biodegradable and either work with the lipids or water particles in the cellular makeup of the skin. “They are easily absorbed or carried away and expelled, so their negative effects are either limited and short or there are none,” she continues.\nThe same cannot necessarily be said of BB Glow pigments. The treatment is not approved or regulated by the United States Food and Drug Administration (FDA). As the FDA notes in its Tattoos & Permanent Makeup Fact Sheet: “Although a number of color additives are approved for use in cosmetics, none is approved for injection into the skin.” The agency continues to investigate the safety of permanent makeup ink with periodic voluntary recalls, including one in 2017 involving inks that were microbially contaminated. While there are reputable providers using quality pigments, Bossavy warns that the lack of regulation makes BB Glow a “buyer beware” procedure.\nHow Semi-Permanent BB Cream Differs from Other Permanent Makeup\nOur experts say that the process of getting semi-permanent blush or lip color is essentially the same as getting a semi-permanent foundation procedure. The difference lies in the amount of surface area being treated. The more skin being injected with pigmented serum, the more potential for complications and side effects.\nLike all semi-permanent makeup, there’s really no way to predict exactly how the pigment will hold up over time. “When the pigment starts to fade, it may be more noticeable on the entire face because it is a relatively larger area,” Dr. Shah warns. From microblading mishaps to blundered lip blush, she has treated patients looking to remove their semi-permanent makeup — something that’s harder to do with an all-over treatment like BB cream.\nIt’s no secret that tattoos can be notoriously difficult to remove, and BB Glow pigments often contain titanium dioxide, which is blended with brown and red pigments to mimic skin tone. It is known to turn blue or black when exposed to lasers, potentially leading to permanent discoloration.\nBut that’s not the only concern associated with titanium dioxide. “[It’s] one of the largest, most opaque and lasting pigment particles that can remain a lifetime in the skin without fading,” Bossavy explains. “The quantity of non-degradable material deposited over an entire face every few months … can accumulate over time and trigger an immune response that can last longer — creating much uncertainty for the long term.”\nBB Glow side effects can range from allergic reactions, contact dermatitis, broken capillaries, infection, and postinflammatory hyperpigmentation (PIH) to granulomas (i.e. the hardening of tissue surrounding the pigment particles), scarring, organ toxicity, and cell damage. Additionally, there is still much research to be done about semi-permanent makeup more generally. For example, some people with permanent eyeliner have reported experiencing burning and swelling from magnetic resonance imaging (MRI) procedures. Though it may be too early to say with certainty, pigment covering the entire face could prove similarly problematic.\nHow to Get Glowing Skin Without BB Glow\nGiven the safety concerns of semi-permanent BB cream treatments, what can mimic the, admittedly, alluring effects of a ‘I woke up like this’ flawless face? “There are a number of topical treatments available, as well as chemical peels, lasers, radiofrequency microneedling, and microneedling without pigments,” Dr. Shah says. On the skincare front, she recommends vitamin C, kojic acid, hydroquinone, arbutin, and retinoids for lightening and brightening the skin. Fractional lasers (Fraxel®, CO2, Pico), microneedling, and chemical peels, meanwhile, can “address texture and tone,” she adds.\nUntil more research can be done into the long term safety and efficacy, Bossavy suggests steering clear of the ‘beauty’ treatment. “Many things are possible, but that doesn't mean they are safe,” she cautions. “Too often, when beauty is at stake, we are quick to recklessly sacrifice safety for instant beauty gratification. It may look good and feel good now, but we will pay a high price later.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:1f9b3da7-af95-4494-8004-e5edfd00a44d>","<urn:uuid:4d669b92-ef53-49d7-a99e-1cd0c717c50e>"],"error":null}
{"question":"What role does proper posture play in rhythm execution for both traditional music reading and guitar playing, and how do these approaches complement each other?","answer":"In guitar playing, proper posture is crucial and involves sitting straight up with the guitar-holding thigh parallel to the floor, the guitar's body resting at a 20-degree angle against the chest, and maintaining straight wrists to prevent injury. The elbow serves as a pivot point for strumming, and the guitar should balance between the leg and chest rather than being held by the arms. In music reading and execution, proper posture facilitates the ability to keep eyes ahead of the hands while playing, ensuring smooth rhythm execution. Both approaches emphasize the importance of taking things slowly at first, maintaining consistent timing, and avoiding tension in the body. Regular practice sessions of reasonable duration (around 30 minutes) are recommended to prevent physical and mental exhaustion.","context":["READING MUSIC WITH WEBRHYTHMS\nWelcome to WebRhythms - an easy step-by-step method for learning to read rhythm, created by Vic Firth artist and educator Norm Weinberg. Starting at the very beginning, you'll progress through 20 lessons, where each introduces a new topic. By the end of the series, you'll be a master at reading rhythm!\nIn this WebRhythms lesson, you'll learn to read 32nd notes. The exercise you'll be working on in this lesson will include audio play-along tracks in five different levels that you can use to track your progress!\nLook out! Here come those notes with lots of beams. Okay, okay, I know that for some players, looking at a page full of beams can be a pretty scary experience. But relax, additional beams don’t necessarily mean you’re going to have to play so fast that your sticks turn into kindling. More often than not, music that makes use of thirty-second notes moves at a slower pace than music using longer note values.\nWe’ll start with the basics. First, let’s talk about what thirty-second notes look like and what they do. Thirty-second notes have three flags whenever they aren’t connected to any other notes. Most often though, you’ll see thirty-seconds that have three beams instead of three flags. Free-standing thirty-second notes are rare. Similar to the way that sixteenths are grouped, thirty-seconds are beamed together into groupings that visually indicate the counts.\nWhat do thirty-second notes do? Just as eighth notes divide quarters into two equal parts and sixteenth notes divide eighths into two equal parts, thirty-seconds divide each sixteenth into two equal parts. To look at this in another way, remember that in common time there are four sixteenth notes to each quarter. And since there are two thirty-seconds to each sixteenth, there will be eight thirty-seconds in a single count.\nPerforming a full count’s worth of thirty-second notes is truly easy. Just count a group of four sixteenth notes (1 e + a) and play those syllables with your right hand. Now, place a left hand stroke in between each one of those syllables. If you did this correctly, you’ve just played eight notes between counts one and two, and you’ve performed a set of thirty-second notes. Congratulations! See, that wasn’t so hard.\nIf you played the last WebRhythm exercise in cut time, you’ve actually got a jump on understanding thirty-seconds. In cut time, eighth notes get the syllables of “1 e + a” as they divide each of the two main counts into four equal parts. The sixteenth notes that were in the exercise forced you to play two strokes for each of those syllables. In effect, you were playing the same rhythm of thirty-seconds (eight equal divisions of each count) even though the notes themselves were written as sixteenths.\nTake a look at example 1 and you’ll see how thirty-second notes (the stems-up notes with three beams) relate to the other types of values that you already know. Notice that there are eight thirty-seconds to each quarter, four thirty-seconds to each eighth, and two thirty-seconds to each sixteenth note.\nNow that you know how thirty-seconds operate, let’s discuss a counting system for them. Example 2 shows two different ways to approach the counting of thirty-seconds. In the first measure, you’ll see that the thirty-second notes aren’t really counted at all. Instead, you simply use your common sense to tell you how to place two notes on each of the same syllables that are used for sixteenths.\nDon’t forget that any note, including thirty-seconds, can be replaced with the same value of rest. In example 4, you’ll find some additional figures that include sixteenth and thirty-second rests. For this example, there are no counts written below the notes. To “solve” these measures, think about the value and the length of each note. Rhythms of this complexity usually move very slowly, so don’t try playing this too fast.\nOne last type of figure is presented in example 5. Here, four thirty-seconds are connected to a set of three sixteenth note triplets. In the first figure, the thirty-seconds begin on the first half of the count and the triplet begins on the “and” syllable. The second figure is just the reverse, with the triplet starting on the “number” syllable and the thirty-seconds starting on the “and”.\nJust as an aside – since things are starting to get a bit more complicated, it might be a good time to review some of the practice suggestions presented in earlier lessons.\nBe certain that the speed of your counts is steady and consistent. There should be no pause between figures, measures or between lines. Take the exercise very slowly at first and have a successful experience. If you’re having trouble, SLOW DOWN! Set yourself some realistic goals. Then pat yourself on the back and play it again. Play it again, and again, and again. If you play this exercise fifty times in a row without errors, then you can be very confident that the fifty-first time will also be perfect.\nCounting out loud will serve as additional feedback to your ears and your eyes that everything is going along okay. As you count, keep your verbal syllables short and crisp so you can hear where the sound is supposed to occur. If you draw out the verbal count, it becomes harder to synchronize the sound of the drum to the sound of the count. Always try to keep your eyes in front of your hands. This means that while your hands are playing count one, your eyes need to be looking at count two (at least).","On the surface it may seem that strumming your guitar is simply moving your pick up and down the strings to make sound. However, the techniques required to strum a guitar correctly are a turning point. This is where someone tinkering with the guitar can transition his/herself into a practicing musician. It only takes some attention to details like posture and wrist position and then practice with basic guitar strumming patterns to master the concept of rhythm.\nOur assumption is that you have already learned how to tune your guitar and play a few basic chords. You may have even picked out the notes of your favorite song and strummed a bit without knowing there were finer details. But now let's get into those details, as they make you a more efficient player, a more competent musician, and start to make playing truly fun.\nFirst, let's get you seated in a posture that will make it easier to hold your guitar and focus on guitar strumming patterns for extended periods. This lets you focus on playing rather than keeping the guitar from running away on you. For beginners, we are focusing only playing while seated. Standing positions come later. We are also focused on learning the basics with a pick, as this is simpler than finger strumming at the start.\nYou want an armless chair that allows you to sit straight up (no leaning back) with your guitar-holding thigh parallel to the floor. This keeps your guitar from sliding down your leg as you play. Right-handed players use their right thigh. Lefties do this is the mirror position with their left thigh. Some chairs will be perfectly proportioned for this. Occasionally, chair's cross-bar works if you can hook your heel on it. Most times you will need a small step or some phone books to get that leg parallel.\nThe bottom curve of your guitar's body should rest comfortably on your thigh. Lean the upper part of your guitar's body against your chest at a 20-degree angle. Position the guitar's neck at a slight incline toward the head. Your guitar should be able to rest on your leg and chest without any help from your arms or hands; they have other work to do.\nTo hold a guitar pick, make a hitchhiking gesture with your strumming hand. Lay the pick on top of the first knuckle of your index finger with the pointy end sticking out like it is a continuation of that knuckle. Put your thumb down on the pick to hold it in place. Practicing guitar strumming patterns will help you discover the ideal pick-holding pressure. It is an ephemeral balance somewhere between relaxed and clamping down.\nRemember, your leg and your chest balance your guitar rather than your arm or your hands. Think of the elbow of your strumming arm as the pivot point of a lever that runs from that elbow to your pick. This lever needs room to pivot, so you need to position your elbow on the top edge of your guitar's face with your inner bicep coming in contact with (but not resting on) the top of the guitar body. It will be a slightly different position for different bodies (yours) and different guitar bodies. From the pivot position, your pick should be able to move freely across the strings directly in front of the guitar's sound hole.\nAs part of that elbow-to-pick lever, your wrist should be straight in line with your arm and not bent in what's called a “gooseneck”. Bending your wrist during guitar strumming patterns can lead to carpal tunnel and related injuries. Your arm strums the guitar from your elbow, not your wrist.\nThe same rule applies to your fingering arm. Do not bend your wrist. The final positioning angle for your guitar is to move the neck to a 45-degree angle out from your body so that your fingering wrist stays straight while your fingers can reach the fretboard. The ideal position allows you to relax tension from your shoulders and maintain straight wrists. From here you should be able to strum with your pick and reach the fretboard with your fingers.\nThough it might seem obvious, there are two strumming directions: up and down. However, up is not a reverse down. Our guitar strumming patterns require that we think of up and down strumming differently.\nPivoting from the elbow, use your elbow-to-pick lever as a single unit to push the pick down through the strings in a single movement. Try it a few times. You might even finger a few simple chords. This down strum establishes your guitar strumming patterns momentum.\nIn contrast to the momentum of the down strum, the up strum is about adding lightness and character to your rhythm. Instead of pulling back through all the strings, just graze the top three strings on your way up. This variation between your up and down strumming will help give liveliness to your sound.\nExperiment with this to get a sense of the difference. Try a few down and up strums where you push through all the strings in both directions. Then try just the top three strings on the up stroke. Try it again with some basic chords. Do you hear the difference?\nRhythm is being able to play notes in sync with a song's beat. One of the simplest beats to begin learning is the 4/4 beat. This is a cycle of four beats that repeats itself over and over for the whole song. You've likely heard a popular rock or country song where someone shouts, “One, two, three, four!” to start the song's beat. You might even be familiar with the tick-tock of a metronome used by piano students to keep time. Time is the beat. The metronome's tick-tock keeps the beat consistent.\nThe beat is the framework for our guitar strumming patterns. Most guitar students can establish their own beat by lightly tapping their foot to the same one-two-three-four pattern. Try it first with just your foot. Count out loud with each tap. After the fourth tap, start again at one:\n“One, two, three, four, one, two, three, four, one, two, three, four…”\nTry to keep the counting (the beat) as consistent as possible. You are your own metronome.\nFinger your favorite chord and hold the fingering. Now continue your beat counting. Each time you count “one”, strum down on the count. Let the chord ring through “...two, three, four…”, then down-strum on the next “one”.\nCount to yourself: “STRUM, two, three, four, STRUM, two, three, four, STRUM, two, three, four…”, and keep repeating until it feels natural. This can take some time. You're doing a lot of new things at once.\nThis is just like the first of our guitar strumming patterns, but you add another down-strum on the “three” count.\nCount: “STRUM, two, STRUM, four, STRUM, two, STRUM, four, STRUM, two, STRUM, four…”, and repeat until this feels natural. Try different chords for variety but try to save the up-strum for the next exercise.\nSame four-count beat, but this time we play on all four beats, using the up-strum on beats two and four.\nCount: “DOWN, UP, DOWN, UP…”, but keep in mind that there are two things to note.\nFirst, these exercises seem simple on the page, but you should be realizing that rhythm takes practice (for most of us, at least). It is very much like trying to walk and chew gum at the same time. Each part (counting to four and strumming) seems simple. However, doing both together is deceptively difficult. Know that all musicians start this way and that practice will get you through it.\nSecond, practicing guitar strumming patterns is mentally exhausting. Practice for 30 minutes and try again tomorrow. Otherwise, you will wear yourself out both physically and mentally. Stopping after 30 minutes will keep you from getting frustrated.\nIf you listen to most classic rock songs (think Elvis, The Beatles, and The Rolling Stones), they are playing a 4/4 beat and the drummer plays on the “two” and the “four”. Once you understand this, you will notice it in all kinds of popular songs. Not all, though, as there are other beat counts that you will learn later. For example, Led Zeppelin was notorious for unusual beat counts.\nCount: “One, STRUM, three, STRUM, one, STRUM, three, STRUM…”; this will be a particular challenge, as you previously were strumming on the one and three beats. Keep tapping your foot as you play, as this will help you keep your rhythm consistent.\nPracticing the four guitar strumming patterns presented here provides a solid introduction to rhythm's function in music. Combined with your efforts to maintain proper posture and form, you are already improving as a musician. Know that these four patterns are very basic. There are counts between the beats and counts between those counts. However, these basic patterns will serve as the foundation for more complex rhythm lessons, truly understanding the guitar, and learning to play along with other musicians."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:790d51bc-79e8-49c8-81be-5254251a468e>","<urn:uuid:d9a98600-591c-461b-8536-b0251f8f91e7>"],"error":null}
{"question":"What are the soil requirements for growing carrots in raised beds, and what benefits do raised beds offer for carrot cultivation?","answer":"Carrots grow best in light sandy soil, and if your soil is heavy or clay-based, you should dig in plenty of fine builder's sand. All Purpose compost is suitable to use, either alone or mixed with soil and sand. Sharp sand is also excellent. The soil should not have recently composted material, as this can cause forking of the roots. Raised beds provide several benefits for growing carrots: they warm up quickly, extend the growing season, make watering more efficient, and allow gardeners to create custom soil blends. You can fill a raised bed with sandy humus that is ideal for root vegetables like carrots. Additionally, raised beds make planting, weeding, and harvesting more convenient, especially for those with mobility challenges.","context":["Growing carrots from seed\nBefore we go any further though, please look at the photos which both show carrots being grown well above ground. This is to deter the carrot fly pest which only flies at just a few inches above ground.\n1) Carrots grow best in soil which has not been recently composted, or if compost has been used it should be very well rotted otherwise the end result will look like something which has been spawned from a nuclear reactor site with lots of roots forking from the main one.\n2) Carrots are grown best in a light sandy soil, so if your soil is heavy or clay based then dig in plenty of fine builder's sand, the light coloured sand is best we find. All Purpose compose which you can buy from any garden centre is perfectly OK to use, either on it's own or mixed with soil and sand. Sharp sand is excellent too.\n3) You will find that some of your crop will show above ground and that part may turn green. Greening of the top of the carrot is caused by sunlight. Heavy rain can wash away the soil from carrot tops exposing them to the sun. The green colour is the chlorophyll pigment. Mound up the soil around the shoulders of the carrots to prevent exposure to the sun.\nSowing carrot seeds:\nA typical packet of carrot seeds contains in the region of 2000 seeds and these can be sown in your garden in one of several ways.\n1) By broadcasting: basically this means you get a handful of seed and scatter them onto your already prepared and pre-watered patch of soil. Cover them with a light coating of new soil and leave them to grow. This method means that you will have a great many shoots appearing very close together and they need to be thinned out to leave the strongest growing on to become part of your salad.\n2) The more time consuming method is to dib 1 CM deep holes in the soil and drop in an individual seed. Cover and leave to grow as normal. You will still have to do some thinning out because it is a virtual impossibility to drop just 1 seed into every hole. But at least with this method most of your seeds will grow at predetermined distances away from each other.\nWell, all the above should get you a good crop but there are other little things such as carrot fly which can ruin a whole crop, but we tackle that problem on one of our pages in the pests/diseases section. However, if you follow the tips below you should avoid the problem\n1) Carrot flies fly close to the ground and lay their eggs near to the top of you carrots. To overcome this you need to buy some very fine insect netting which will keep them away.\n2) Make a raised bed as high as you can and plant your seeds in there. As the pests only fly low they will not be able to infect your crop.\n3) When you plant your carrot seeds then also plant plenty of spring onion seeds around the edges of your plot and these will help to deter the pests.","Would you like to have more garden space with less stooping? How about spending less time doing soil preparation? Or maybe you would just like to have a little hardscaping for visual interest but you don’t want to invest a lot of time and money into construction. Raised bed gardening may be just the thing for you.\nRaised bed gardening involves building frames that can range in height from six inches to 18 inches and filling the frame with good garden soil. Frames can be any length, but most people prefer beds that are four to six feet long, or the length of a standard piece of lumber available at the hardware store.\nBenefits Of Raised Garden Beds\nRaised beds provide a number of benefits. Building a raised bed “pyramid” creates visual interest and increases the square footage available for planting. Second, raised beds can extend the growing season. Soil in raised beds warms quickly, and the bed can be covered with a hoop or tunnel to protect plants when frost threatens in early spring or late fall. Third, raising the level of the soil minimizes the need for bending or kneeling.\nThis makes planting, weeding and harvesting more convenient for individuals with mobility challenges. Fourth, watering and irrigating raised beds is easier and less wasteful than attempting to water the average garden area. Some raised bed kits come with a drip irrigation system or a sprinkler head fitted to the bed.\nFinally, the soil in raised beds can be blended to a gardener’s specifications. One raised bed can be filled with compost-rich humus for squash, melons or salad greens, while another can be filled with sandy humus that is ideal for carrots and other root vegetables. Raised beds increase possibilities for gardeners.\nBuilding Raised Garden Beds\nThe big question, of course, is how difficult is it to build one – or several – raised beds? The answer is that raised beds are very easy to construct, even for someone for whom DIY is a four-letter word.\nDie-hard do it yourselfers can build their raised beds with paver bricks, flat stones or standard 2 x 4s in whatever lengths are desired. The easiest to assemble is a basic “box” made of lumber that would be appropriate for decking; don’t use treated wood, however, because the chemicals used to treat the wood can leach into soil and contaminate edible crops.\nThe depth of the box should be a minimum of 6 inches. Lumber can be stained and sealed or left unfinished. Staining and sealing will extend the life of the fixtures and help keep them weatherproof.\nThese structures will be permanent fixtures in the landscape, so take care that the construction be aesthetically pleasing as well as functional.\nA much easier – and temporary – solution is to purchase a raised bed kit. These kits generally contain artificial lumber boards, corner brackets and anchors for the larger sizes. The brackets are on a stake with a pointed end that slides into the ground and openings that hold the lumber.\nMany of these kits are stackable, creating a significant amount of growing space but using a minimal amount of square footage. These beds assemble and disassemble quickly. Most people break these beds down at the end of every season and store the pieces in a shed or garage.\nHowever the beds are constructed, they need to be filled with soil. This is where the fun begins, because a gardener can create growing conditions in raised beds that would not normally be found in their location. For example, a bed in which herbs are to grow would require a sandy loam that drains easily.\nMidwesterners can grow hardy cacti in a soil mix that is primarily sharp sand. A standard kitchen garden or vegetable bed would need a rich loam; squash, cucumbers and pumpkins need extra manure or mushroom compost added to the soil mix for optimum fertility.\nOnce the soil has been installed, the beds can either be direct seeded or starter plants can be placed into them. Once the plants are placed or the seedlings have emerged, then it’s time to mulch. Raised beds drain more quickly than flat beds, so mulch helps to keep the soil evenly moist during the growing season and it also helps keep weeds to a minimum.\nThe kind of mulch you use depends on the plants in the bed. Tomatoes, for example, benefit from either black or red plastic mulch. Leafy greens or herbs benefit from wood chips or shredded bark, which keep soil cool during hot summers and also keep soil from splashing up onto leaves that will be going into the evening’s salad.\nBed maintenance consists of ensuring the bed is adequately watered, and applying time released fertilizer around plants at the beginning of the growing season, or applying a water-soluble fertilizer once a week. The water-soluble fertilizer can be applied with a watering can, sprayer, with a sprinkler that is focused strictly on the bed or through drip irrigation. \\\nBecause the garden area is contained, less fertilizer can be used throughout the season, as there will be less runoff and waste. Near the end of the season, the beds can be covered with clear plastic tunnels in order to stave off frost damage and extend the growing season. Gardens built in areas of extreme heat (Texas and Arizona come to mind) can be covered with shade cloth during the warmest time of the day to prevent sun scald.\nRaised bed gardening affords greater accessibility for motion-challenged individuals, creates opportunities for more plant diversity, and helps extend the growing season. Raise the standard on your garden this year and try a few raised beds."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:e5790b0d-9115-4cd8-8efb-0995b2855b32>","<urn:uuid:911c5ac9-d268-4fd3-a607-7edc0f5cf5c1>"],"error":null}
{"question":"What are the key components of PCP surface drive systems, and what are the different braking control methods available for managing stored energy in mechanical systems?","answer":"PCP surface drive systems typically include several key components: a thrust bearing to support the rod string, a stuffing box to seal around the polished rod, a power train with sheaves/belts/gears/hydraulic motors, a prime mover power source (electric motor or combustion engine), and a braking system. As for braking control methods, there are five main types: 1) DC injection braking, which regulates DC current in motor windings, 2) Quick stop braking, which reduces drive output frequency, 3) External dynamic brakes and choppers that use resistor banks, 4) Flux braking, which increases flux-producing motor current, and 5) Line regeneration braking, which feeds energy back to the AC supply. These methods help manage different forms of stored energy, including torsional energy from rod string winding and fluid potential energy in well systems.","context":["A Group of manufacturers of surface drives for progressing cavity pumping (PCP) systems has supported the development of an industry standard for such equipment. The purpose of the standard is to provide guidelines for designing, specifying and using surface drives and to ensure acceptable levels of safety in the operation of this equipment. This paper describes the safety issues associated with this equipment that are addressed by the standard.\nProgressing cavity (PC) pumping technology has grown dramatically from the first downhole pumps that produced less than 10 m 3/fluid per day to the newest pumps being used in Venezuela that are producing over 300 m 3/day. This growth in pumping capacity has also been accompanied by the technology being used in more diverse applications that range from primary heavy oil to dewatering in coalbed methane operations.\nThis proliferation of the technology prompted the development of a new ISO Standard (ISO 15136-1) to address the selection, manufacture, testing and use of progressing cavity pumps. This ISO Standard, however, does not address surface drive units that are required to drive these pumps. To close this gap, a group of manufacturers of surface drives for PCP systems took the initiative to support the development of a standard for PCP surface drives.\nThe purpose of the Standard is to provide guidelines for the design, specification and use of surface drive equipment, to ensure the safe operation of this equipment. The common format for drive specification defined in the Standard also provides end users with a simple means to compare operating limits for different models and manufacturers.\nSurface Drive System Components\nCurrently, there are at least 10 manufacturers making more than 200 models of surface drive systems. These systems usually include the following components, arranged in different configurations:\n- thrust bearing to support the rod string\n- stuffing box to seal around the polished rod against tubing head pressure\n- power train consisting of combinations of sheaves, belts, gears, hydraulic motors/pumps\n- prime mover power source consisting of an electric motor or internal combustion engine\n- braking system to manage the release of energy stored in the production system when the surface drive is shut down\nMost of these components are typically incorporated into a single device that is mounted on the wellhead and is termed the surface drive, wellhead drive or drivehead. In this paper the term surface drive is used. The operating limits for surface drives are usually defined by maximum operating speed, torque and axial load. However, it must be recognized it is not possible to fully define the operational limits of the braking system by these parameters only.\nImportance of the Braking System\nDuring operation, energy is stored in the production system in two ways:\n- torsional energy, in terms of elastic “winding ” of the rod string (similar to winding up a spring)\n- fluid potential energy determined by the fluid level in the well annulus\nFollowing a shut-down, this energy is released in different ways, depending on the conditions in the well at this time.\nAuthor: Wagg, B. T.\nPublisher: Canadian International Petroleum Conference, 11-13 June, Calgary, Alberta\nPurchase URL: Purchase this paper","FIVE SOLUTIONS TO BRAKING CONTROL\nAs we try to improve the performance and productivity in industrial process where stopping or slowing down an overhauling load is important, the capability to manage the load is essential. A few common applications include dynamometers, test stands, and punch presses.\nThere are many solutions to incorporate braking control into an application. Depending on the desired level of performance and control, the technology being used in the drive, the physical demand and characteristics of the application, this article will help you determine the appropriate solution for the application.\nWhat is braking control?\nBraking is essentially the removal of stored motion (kinetic energy) from a mechanical system. When the motor and attached system are brought up to speed, electricity is added and converted in to the motion of the system. To stop or slow down the system, the kinetic energy (that has previously been stored) in the mechanical system must be removed. There are four reasons and examples of applications that may need the addition of braking capability:\n- To slow or stop the application e.g., Conveyors\n- To reverse the direction of the application e.g., Fans\n- To hold the application in a fixed position e.g., Hoist and Cranes\n- To provide a load or hold back an overhauling load e.g., Dynamometers\nWhat happens to the stored energy?\nIt is important to understand how the energy is removed and what happens to it.\nWhen braking is applied to an application, the kinetic energy is reduced or removed. In a lot of applications, the braking methods that are applied converts the energy into heat. These methods can be wasteful. Other solutions include the use of internal or external methods, and at the point of the motor with a mechanical brake.\nWith a mechanical brake, the energy is removed directly from the system by the brake shoes or and brake disc that converts the motion in to heat. Alternatively, if a drive is being used, the energy can be removed in the form of electrical energy. Here are some key factors to consider in determining the best solution.\n- How much braking is needed? Is full braking torque needed or a slow down?\n- How controlled does the stop or slowdown need to be? Tightly controlled stops, or some pre-set level?\n- How much shock can the mechanical system take?\n- Is there a need to brake continuously, or intermittently?\n- How quickly does the brake need to respond?\n- How much am I willing to pay to add braking capability to my system? What is the price/performance tradeoff?\nFive types of braking control\nIn order to enhance the performance or if it is necessary in a application to have a\nform of braking control, then one of five basic types of electrical braking can be\nDC injection braking, quick stop, dynamic brakes/choppers, flux braking, or line regeneration.\nOne - DC Injection Braking\nIn DC injection braking is the most basic form of braking, the AC drive regulates a pre-set DC current in the motor windings, making a fixed magnetic field in the motor. The heat is created as the energy in the system is converted in the motor. The DC current also can be maintained after the motor stops to hold it in position. DC injection is a standard feature on most drives today. This form of braking is normally used under light loads and that prolonged usage may cause damage to the motor.\nTwo - Quick Stop Braking\nQuick stop braking provides a performance improvement over DC injection braking, and used with AC Volts/Hertz drives. The quick stop method reduces the drive output frequency and regulates the output current until the motor stops. Since no additional hardware is required, quick stop is easy to implement. You must remember that a \"quick stop\" converts the systems energy into heat at the motor and may damage the motor. This type should be used intermittently.\nWhile these braking methods convert the system motion into heat at the motor, other forms of technology extract the energy through the motor leads and bring it into the drive.\nThree - External dynamic brakes and choppers\nExternal dynamic brakes and choppers are utilized to remove this energy from the capacitor bank of the drive. The dynamic brake or chopper consists of a power-switching device and a resistor bank. The IGBT or SCR device removes the energy by regulating the current in the resistor bank, where the energy is converted into heat. The dynamic brakes or chopper units can be sized or paralleled to get as much braking as necessary, with the dynamic response of the system limited only by drive performance. One drawback of dynamic braking is that it is inefficient for continuous or high duty cycle operation because it produces excessive heat. Also a large resistor bank is needed for high-duty cycle operation which can become costly.\nFour - Flux Braking\nFlux braking is a method that can be implemented in drives with field-oriented control. The flux braking method dramatically increases the flux-producing component of the motor current, resulting in increased motor losses. As with several of the above methods, flux braking converts the system energy into heat at the motor, and should be used intermittently to avoid motor damage.\nFive - Line Regeneration Braking\nLine regeneration is another method of removing energy from the mechanical system. Line regeneration systems feed the energy from the system back onto the AC supply control, and is the most effective method for continuous braking applications such test stands. Depending on the design and application of line regeneration systems, additional benefits can be realized such as power factor correction and line current harmonic reduction. The biggest drawback of line regeneration is that it is the most expensive hardware solution available.\nFor further assistance, call a Galco Sales Representative today at\nFeatured Products || Drives, Motors, & Accessories\nFeatured Products || Industrial Controls"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:09b77775-739b-446e-8d7b-b81fc6a9b77f>","<urn:uuid:2719fa90-1f51-4049-b5ba-dbaf7bfa4e0f>"],"error":null}
{"question":"What are the similarities and differences between the New Year's Eve celebrations in the Philippines and Chinese traditions?","answer":"Both cultures have significant New Year's Eve celebrations with some similarities and differences. In the Philippines, the main traditions include using firecrackers and making loud noises with drums and hornpipes, as Filipinos believe these keep bad spirits away. They also prepare sumptuous meals to welcome the year. In Chinese culture, New Year's Eve traditions are more elaborate, including having a big family reunion dinner, cleaning and decorating houses before the celebration, worshiping ancestors, watching the CCTV New Year's Gala at 8:00 PM, setting off fireworks (though now restricted in many cities), giving red envelopes, and staying up late. While both cultures emphasize family gatherings and special meals, Chinese celebrations include more specific customs and ritualistic elements.","context":["|1-Jan||New Year’s Day|\n|16-Feb||Chinese Lunar New Year’s Day|\n|25-Feb||People Power Anniversary|\n|9-Apr||The Day of Valor|\n|13-Apr||Lailatul Isra Wal Mi Raj|\n|21-Aug||Ninoy Aquino Day|\n|22-Aug||Eid al-Adha (Feast of the Sacrifice)|\n|23-Aug||Eid al-Adha Day 2|\n|27-Aug||National Heroes Day holiday|\n|1-Nov||All Saints’ Day|\n|2-Nov||All Souls’ Day|\n|8-Dec||Feast of the Immaculate Conception|\n|31-Dec||New Year’s Eve|\nExplanation of Each Filipino Holiday\nNew Year’s Day: This is one of the grandest holidays in the Philippines. The use of firecrackers and anything that makes loud noise such as drums and hornpipes are the most popular and oldest tradition to celebrate New Year. Filipinos believe that loud noises keep the bad spirits away. On New Year’s day, a sumptuous dishes are prepared to gladly “welcome” the year.\nChinese New Year: Chinese comprise a substantial minority of the Philippine population, and China has culturally influenced Filipinos, thus the Philippines celebrate the Chinese New Year with China on the same day, which is February 16 this year.\nPeople Power Anniversary: February 25, 1986, is an important day to remember in Philippine history wherein about 2 million Filipinos filled the streets to cry for freedom from the tyrannical regime of former President Ferdinand Marcos.\nHoly Week: The majority of the Philippine population is Catholic. Filipinos celebrate Holy Week at the same time as Vatican City, on March 29 – April 1, 2018. It’s a weeklong national solem holiday to pray and reflect on Christ’s death. Various traditions are observed such as “pabasa,” where devotees sing/chant the story of Christ’s life, death and resurrection.\nThe Day of Valor: It is also known as Araw ng Kagitingan, a national holiday to commemorate the fall of Bataan during World War II. It was April 9, 1942, when more than 76,000 soldiers surrendered to Japanese troops.\nLailatul Isra Wal Mi Raj: This Muslim holiday that is also known as Al Isra wal Miraj and falls on April 13, 2018. About 10 million Filipinos are Muslim, hence this holiday is observed in the Philippines. This event marks the night that Allah took Mohammad on a journey from Mecca to Jerusalem and then to heaven (source).\nLabor Day: International Labor Day is observed in almost every country in the world. Filipinos celebrate this day every May 1 to pay tribute to workers contributing to the country’s economy.\nIndependence Day: A national holiday in the Philippines to commemorate the declaration of the country’s independence that took place on June 12, 1898. Some Filipinos celebrate this day with fireworks, a parade and flag raising at the Aguinaldo Shrine. Some just spend the day with their families in either outdoor and indoor activities.\nEid al-Fitar: Another religious holiday celebrated by Muslims that marks the end of Ramadan on June 16, 2018. It is observed in the Philippines as a non-working holiday.\nNinoy Aquino Day: A national non-working holiday on August 21 in the Philippines to commemorate the death of the former Senator Benigno “Ninoy” Aquino, Jr. His death led Filipinos to cry for freedom from the tyrannical regime of Ferdinand Marcos. Aquino was a well known critic of Marcos. In 1980, he and his family moved to the US, and in 1983, Aquino decided to return home and continue to fight for democracy. Upon his arrival at Manila International Airport, he was assassinated. This event moved more Filipinos to fight for democracy, hence the People’s Revolution took place and successfully led to the downfall of Ferdinand Marcos.\nEId al Adha: A Muslim religious holiday also known as the Sacrifice Feast is celebrated on August 22, 2018. It’s a holiday to honor Abraham’s willingness to sacrifice his son to Allah.\nNational Heroes Day: A national public holiday on August 27 in the Philippines to pay tribute to the country’s heroes.\nAll Saints Day: A day to commemorate deceased relatives and friends. Filipinos come home to spend the day together with their families and visit tombs of their loved ones. Lighting candles and offering of flowers are one of the Filipino customs during this holiday on November 1. Originally, All Saint’s Day was a solemn holy day for Catholics in honor of all saints. However, in the Philippines, it has been traditionally spent paying respects to relatives and friends.\nAll Souls’ Day: Another national holiday to extend All Saint’s Day on November 2.\nBonifacio Day: Andres Bonifacio is one the greatest Filipino heroes. He was one of the Filipino leaders that triggered the Philippine revolution against the Spanish Empire in 1896. November 30 is the day Bonifacio was born, hence the holiday.\nFeast of the Immaculate Concepcion: This is a holiday observed by Christians, particularly Catholics, to celebrate God’s gift to humanity in Mary. The holiday is celebrated on December 8.\nChristmas Eve: Christmas Eve is one of the grandest holidays for Filipinos. It’s all about families getting together to enjoy a sumptuous meal, to pray and reflect on the birth of Jesus Christ.\nChristmas Day: The grandest holiday celebrated by Filipinos. Filipinos celebrate it with traditions such as gifts, preparation of sumptuous meals and by giving money to children.\nRizal Day: This national holiday commemorates the life and work of Jose Rizal, the national hero of the Philippines who also triggered the Philippine revolution against the Spanish Empire. On December 30, Rizal was executed because of his rebellion, sedition and of forming illegal association against the foreign invaders.\nNew Year’s Eve: Filipinos celebrate New Year’s Eve with almost every country in the world. Firecrackers, loud sounds and sumptuous meals are prepared to welcome the year.","Top Chinese New Year's Eve Traditions\nChinese New Year's eve is the big day for ringing out the old and ringing in the new. With the development of the economy, some traditional customs have gradually disappeared, some new ways of celebration appeared. Here are the top 8 traditions of Chinese New Year's eve.\n1. Having a big dinner with Family\nChinese New Year's eve is the reunion day for every Chinese family. With a 7-day public holiday, all Chinese people would try their best to go back home no matter how far they live.\nAll family members will gather together, enjoying a grand family reunion dinner, chatting, singing, and laughing to celebrate this special day.\n2. Cleaning Houses\nChinese families will clean their houses: sweeping the floor, washing clothes, cleaning spiders' webs, and dredging ditches on Chinese New Year's Eve.\nIt is traditionally believed that dust represents \"old\" things, so cleaning houses means doing away with the \"old\" and preparing for the \"new\"; with the intention of sweeping all the rotten luck from the previous year out the door.\nInterestingly, one of the taboos of Chinese New Year is avoiding cleaning houses on Chinese New Year Day. People believe that if they do sweep or dust on New Year's Day, their good fortune will be swept away.\n3. Decorating House\nApart from cleaning houses, Chinese people will also decorate their houses on Chinese New Year's eve.\nPeople believe that auspicious decorations such as Spring Couplets, Door-God pictures, New Year's paintings, paper-cuts, and lanterns can drive away evil spirits and bring good luck.\n4. Worshiping Ancestors\nWorshiping ancestors is an old custom dating back thousands of years. People believe that the deceased family members have a continued existence in heaven, their spirits will look after the family and have the ability to influence the fortune of the living. So people offer sacrifices to ancestors at traditional festivals and hope the deceased ancestors would bless the whole family.\nOn Chinese New Year's eve, people will offer various dishes, fruits, and burn incense to worship ancestors.\n5. Watching CCTV New Year's Gala\nCCTV's New Year Gala, also known as the Spring Festival Gala, is called Chunwan ( chūn wǎn) in Chinese. The TV show is broadcast annually on Chinese New Year's eve at 8:00 PM.\nSitting together and watching the Spring Festival Gala has become a popular way to celebrate Chinese New Year's eve since the 1980s.\nThe show includes many different kinds of performances such as singing, dancing, comic dialogue, sketch comedy, magic, and acrobatics.\n6. Setting off Fireworks\nAncient Chinese people believed that fire could dispel bad luck, sparks could bring good luck, loud noise could scare away evil spirits and smoke made Yang energy (a kind of positive life-energy) rise. Fireworks produce such effects as fire, sparks, sound, and smoke when they are set off. It naturally combines with people's good wishes and becomes an ideal product for celebrations.\nIn ancient times, people would set off fireworks\nat 12:00 PM on Chinese New Year's Eve to celebrate the coming of the new year. Nowadays, many cities across China have imposed bans or restrictions on the use of fireworks for the sake of safety and environmental protection.\n7. Giving Red Envelopes\nThe red envelope or red packet is used as a monetary gift from the older generation to the younger generation during festivals or special occasions in China. Through giving red envelopes, parents send their good wishes to the children.\nCurrently, giving digital red envelopes via Wechat or Alipay app becomes a trend among young people.\n8. Staying up Late\nStaying up late or all night on Chinese New Year's Eve. After the grand reunion dinner, all family members will sit together, chatting, playing cards or mahjong, watch the gala to welcome the arrival of the New Year.\nPeople believe that staying up late can delay the aging process of the elder family member and prolong their life. The longer children stay awake, the longer their parents will live. You will find many families will keep their light on all night on Chinese New Year's eve.\nTop Traditional Dishes to Eat on New Year's Eve\nThe family reunion dinner held on the evening of Chinese New Year's eve is one of the major events of the Spring Festival. In ancient times, it was the biggest meal of the whole year. People could eat a lot of food that they wouldn't normally eat.\nAlthough there are many choices for food in modern times, traditional Chinese New Year's Eve dinner is also a very important meal for every Chinese family. Every dish has a different meaning.\n1. Fish: Always Have More Than You Wish For\nThe Chinese word for \"fish (鱼 yú)\" has the same pronunciation as the word for \"surplus (余yú)\". Chinese people always like to have a surplus at the end of the year, people think that saving things helps to make more in the next year. Eating fish stands for \"always have more than you wish for\".\nWhat's interesting is that people will not finish all the fish as leaving some on the table signifies having everything in abundance for the coming year.\n2. Chicken: Good Luck!\nThe Chinese word \"Chicken (鸡 jī)\" sounds like the word \"吉jí\", which means good luck and auspice in Chinese culture. In southern China, people will serve a whole chicken on the dinner table to show happiness and reunion.\n3. Nian Gao: Wish You a Prosperous Year\nChinese New Year Cake in South China\nNian gao (年糕 nián gāo), also known as \"rice cake\" or \"Chinese New Year cake\", is a traditional food made from glutinous rice flour. People believed nian gao carries auspicious meaning.\nFor old people, nian gao expresses the wish for longevity. For young people, it expresses the wish for\npromotion and high income. For kids, it expresses the wish to\n4. Dumplings: Change from the Old and to the New\nDumplings, in Chinese, sound like the Chinese expressions of changing from the old to the new.\nThe shape of a Chinese dumpling is similar to a gold ingot, people believe that eating dumplings on Chinese New Year's Eve means \"ushering in wealth and prosperity\".\nWhat's interesting is that people who eat the dumplings occasionally stuffed with special materials like a coin, a candy, a peanut, or a red date, will have the best luck in the new year.\nClick Chinese New Year Food to find out more lucky food eaten during Chinese New Year."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:7eeb4601-9e17-4437-9a59-9ce4edc58f62>","<urn:uuid:4b0fac7f-102a-48ee-b41e-e95518ebc889>"],"error":null}
{"question":"I'm trying to take better holiday photos - what's the main difference between using auto white balance versus custom white balance settings for holiday lights?","answer":"Auto white balance and custom white balance settings have distinct impacts on holiday light photography. Auto white balance automatically neutralizes extreme color casts, which can actually diminish the rich, saturated colors of holiday lights. In contrast, custom white balance allows you to manually tell the camera what is white in your specific lighting situation, leading to more accurate color representation. This is particularly useful in mixed lighting situations, like when you have both window light and overhead lights. For holiday lights specifically, it's recommended to turn off auto white balance to capture the exaggerated colors the holidays have to offer.","context":["At this time of year, many of the world's cultures and religions celebrate holidays that involve lights.\nYears ago, taking great photographs of holiday lights was difficult because the films of yesteryear weren't very light sensitive.\nThey had difficulty recording an image in the low-light of a candle, for example. This is no longer the case as technology has solved these problems. Many DSLRs can be set to ISO 6400, 12,800 and even higher settings with little noise.\nIn addition, most photographers today rely on auto-exposure with their point-and-shoots or DSLRs. Unlike the light meters of old, which were often \"fooled\" by low-light situations, today's meters in auto-exposure cameras are able to give good readings even in low light.\nThis is an important point because holiday lights usually look their best when shot without added light. In fact, this is Rule One when it comes to getting good pictures of lights: Turn off your flash. Let's repeat that: For most pictures of holiday lights, turn off your flash!\nNote that we said \"most.\" There are a few occasions when you will want to add light, but usually you won't.\nNow, let's remember one important point if you're taking a picture without flash: You're probably going to need a slow shutter speed. This means you may need to mount your camera on a solid unmoving surface to avoid camera-shake. A tripod is best.\nWhen else might you want to use your flash?\nLet's say the subject of your picture is your kids under the tree. How are you going to light their faces? On the one hand, you may find that the Christmas-tree lights are sufficient and give a very soft glow to their cherubic expressions. Or maybe it is Christmas morning, and they are lighted by window-light that is streaming into the room. In these cases, you don't need your flash. But, on the other hand, maybe you don't have enough light to really see their faces. Then you may have to use your flash. How do you know which way to go?\nOne approach is to shoot both ways, then select the better image. We think a better way is to plan ahead and meter your subject. Remember that Guideline One of the Three NYIP Guidelines for Great Pictures is to decide on your subject before you do anything else. In this case, you've decided that the subject is the faces of the kids. Guideline Two is to draw attention to your subject. One method of drawing attention is to make sure your subject is well-exposed. So meter the light that falls on their faces from the lighted tree. Get in close and meter just the faces! If there's enough available light for a well-exposed picture, shoot it. If not, use your flash.\nNow let's move outdoors.\nHere we see elaborate lighting on apartments, stores, and the street. Again, if you want to capture the lights themselves, don't use your flash.\nOne other tip for outdoor lights — you'll get the best results when you shoot at twilight. That way, you'll capture some color in the sky, rather than the pitch-black tone that will be recorded on film later at night.\nBut what if you want to take a picture of your friend in front of a brightly lit display?\nYou want to capture both the bright lights and your friend. If you use flash, you get your friend, but you're in danger of minimizing the bright lights behind. On the other hand, if you don't use flash, you get better detail of the lights but your friend is reduced to a silhouette.\nThere's an answer. Many of today's point-and-shoot cameras have a Night Portrait Mode. This setting tells the camera that you want the flash to fire (which will light your friend in the foreground); but that you also want the lens to stay open long enough to record the lights in the background. Your solution to getting light on your friend's face and capturing the light display is to use this setting. The flash exposes the face. The long exposure captures the lights.\nBut, again, watch out here. The long exposure — typically, one-quarter of a second long — requires that you steady your camera to avoid camera shake. Once again, we advise you to use a tripod.\nTo take great holiday photos in this season of lights, we offer you these 4 tips:\n- Turn off your flash unless you have a very good reason to use it.\n- Use a fast ISO — we suggest ISO 800, 1600 or above.\n- Avoid camera shake.\n- Use a tripod...or, at least, brace the camera. Trust your camera's built-in meter.\nSome Special Considerations\nAmplifying a digital signal is like turning the volume up on your radio as loud as it will go. At the maximum volume every hiss, pop, and scratch is heard and, depending on the quality of the equipment, quality is diminished. The same thing happens in a digital camera. When the ISO setting is increased, every image artifact and defect is magnified.\nTo achieve the best image quality, you might try working with a slower ISO setting to start. If you are having trouble getting a good exposure, increase the ISO as needed. You might even try using the Auto ISO setting and see how the camera chooses to handle exposure.\nRegardless of the ISO setting chosen, most inexpensive digital cameras produce \"noise\" during long exposures. Noise is caused by the small electrical disturbances that are present in every electrical system. In order to capture a weak light signal, such as a subject in low-light, longer exposures are usually needed. The longer a digital camera shutter is open, the more electrical noise is recorded as well.\nSo, it seems we have a double-edged sword: Increase the ISO to achieve faster shutter speeds and you will amplify noise and other image problems. Reduce the ISO and shutter speeds are slower. As a result, you will record inherent noise that might not be seen in a \"normal\" exposure.\nLimited Dynamic Range\nTo make things worse, digital cameras have a limited dynamic range. Image sensors are only sensitive to a specific range of brightness. Anything outside of that range is recorded as pure white or pure black. This can result in an image without shadow or highlight detail.\nHere are a few ways to solve these problems. Noise can be reduced with software. In fact some cameras offer in-camera noise reduction features. Proprietary software is used exclusively, yielding uneven results. Test your camera's capabilities before committing to this feature. There are many noise reduction software products on the market today, some as stand-alone applications and others which are plug-ins that work in conjunction with your favorite image editor. This means you can select a camera with noise reduction or address any problems later in the digital \"darkroom.\"\nTiming is Everything\nAs we noted earlier in this article, when shooting holiday lights outside, I find that the best exposures can be made at twilight. Twilight is after the sun has set but before the dark of night. This fleeting balance of light and shadow will yield the brilliance of the lights while maintaining details in the shadow. Don't underestimate shadow detail to help establish your composition. Consult your camera's manual for details on your white balance options and how to adjust them. In the finished photo the viewer will perceive the twilight photo to be taken at night.\nWhen shooting holiday lights inside, try turning on lights in the room to increase the ambient light, rather than using a flash. Flash can produce a harsh, high-contrast quality that obliterates the brilliance of the light. A carefully positioned incandescent light can work increase the ambient light without overpowering your holiday lights.\nShoot Two Exposures\nOne way to extend the tonal range of a digital image is by making two exposures of a scene. Shooting in Manual mode, make one exposure configured to capture the best highlight detail. Make a second exposure to capture the best shadow detail. Then combine the two exposures in Photoshop as separate Layers. Using the Eraser tool remove poorly exposed areas to reveal detail and take advantage of the best parts of each Layer/exposure. Using this technique you could extend the tonal range well beyond the possibility of any single exposure made with the same camera. Of course this requires a strong tripod to ensure both compositions match perfectly. Consider using a remote control to reduce the possibility of camera movement.\nAccomplished photographers may also create two separate images‚— one favoring highlights, the other shadows‚ — from a single RAW file.\nTurn Off Automatic White Balance\nIn many photographic situations white balance is a godsend. By automatically neutralizing extreme color casts, believable digital color is rendered without breaking a sweat. It is important to remember, not all photos require white balance. Tone down the rich, saturated colors of a sunset and you're left with nothing. Attempt to white balance a fireworks display and you end up with dull lifeless, de-saturated bursts and streaks of light. Holiday lights should be treated similarly. By turning off the auto white balance feature you are sure to capture the exaggerated colors the holidays have to offer.\nYou could try turning off white balance altogether or even experiment with any of the other manual settings to find a color balance that suits your visual needs. Either way is a better bet than giving the decision to the camera.\nTest, Test, Test\nThe immediate feedback of digital photography begs you to test your exposures to determine what works best. Take advantage of the metadata that most digital cameras embed inside every digital picture you make. Metadata can include camera make and model, exposure, flash, white balance and other important information that can help you to determine what works and what doesn't work. This means you don't even have to take notes! The Camera Data screen reveals shutter speed, aperture, ISO settings, lens focal length, flash settings and even the metering modes.\nHoliday lights are usually around for more than a couple of days each year, take advantage of this by shooting early in the season and then re-shooting if you have to.\nFor the past 100 years The New York Institute of Photography has been educating photographers of all backgrounds. We offer diverse photography classes, starting with our most comprehensive course: The Complete Course in Professional Photography. Study with us online, at your own pace. Learn more today!","On DSLR cameras, there is a feature called white balance. The standard setting is automatic, but learning how to create your own custom white balance in your photography can make the difference of turning a blue-toned photograph into one that reads the color correctly. Because no one really wants an orange face, except maybe Willy Wonka.\nWhat is white balance in photography?\nDifferent light sources have different temperatures. The sun casts a different light than shade. Fluorescent lights read differently than tungsten, etc. White balance is used to help your camera determine what is actually white so then the photo reads all other cameras more accurately.\nCamera presets for white balance\nThe camera does its best to read color accurately. Most of the time the camera does a pretty good job, but there are times when it gets very confused. Those are the times when a custom white balance comes in handy. However, when you don’t have time to create a custom white balance – during a wedding ceremony, for example – auto works well.\nUse this when the sun shines bright. Since I live in Seattle, I rarely use this setting.\nAside from auto or a custom white balance, this is the setting I use most often. It’s for when the clouds mostly cover the sky, and the light is pretty cool. It warms up the photo a bit.\nUse this when you are shooting in the shade or the light is very cool (blue tones).\nThese are lights that are most often found in homes. The light that comes off those bulbs is very warm (yellow and orange), so this setting adds blue/cool tones to the image.\nThese lights often emit a cool tone. Accordingly, this setting adds warmth to the image.\nThe light coming off the flash is quite cool. The camera will adjust by warming up the tones.\nThis is where you get to tell your camera what is white in the lighting setting that you are in. You get to override what the camera thinks, determining what white is white in the image. This setting will then adjust all the colors to read accurately.\nTungsten (will correct for orange overhead light) Daylight setting\nWhy bother with a custom white balance?\nWell, for one thing we want to avoid skin tones that read inaccurately. It’s true that sometimes an “error” in color can create a more compelling image. For example, in the winter I like to have more blue tones throughout the image. But, beyond that, I generally want the colors to read true.\nMost often the auto or cloudy settings work just fine for where I live, yet there are times when my camera is confused by the tones. It’s not until I tell my camera what is white that it will adjust to the other colors.\nI will often create a custom white balance when I have light coming from two sources, a window and overhead light for example. If I can’t turn off the overhead lights, I’ll create a custom white balance as my camera has a hard time reading white when there is mixed lighting.\nSame image shot with a custom white balance\nHow to make a custom white balance?\nFollow this steps for making a custom white balance for any image.\nFind something white in the same light where you are photographing and take a photo of it. The image doesn’t have to be in focus, but you want it to mostly fill the frame and not be overexposed.\nGo into your MENU and select Custom WB.\nSelect the image you just created by pressing the SET button. It’s the button in the middle of the big dial by your right thumb.\nNow set your Camera WB setting to the Custom Icon (see above)\nNow you’re all set! Your new custom WB should get you great result. Remember to update your WB settings if you move into a different lighting situation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:7820c7ff-f857-4178-b5c5-7dd0a51ad6a0>","<urn:uuid:e6186a91-c0f0-478c-99be-319090dec3d3>"],"error":null}
{"question":"What are the distinctive characteristics of current climate change compared to past changes, and how do ocean acidification patterns reflect these unprecedented changes?","answer":"Current climate change is unique in several key aspects compared to past changes. First, while past climate changes were natural in origin, most warming in the past 50 years is attributable to human activities. Second, the current CO2 concentration has reached record highs at an exceptionally fast rate compared to the past half-million years. This rapid change is mirrored in ocean chemistry, where acidification is occurring 30-100 times faster than any time in the last several million years. The ocean's absorption of CO2 has caused pH to decrease by 0.1 units since industrialization, representing a 26% increase in acidity - a rate that is 100 times faster than anything experienced in tens of millions of years. While there were past episodes of massive CO2 release, such as during the Paleocene-Eocene boundary 55 million years ago, these occurred over several thousand years, allowing time for oceanic buffering. In contrast, today's rapid changes are giving marine organisms little time to adapt, potentially leading to significant impacts on marine ecosystems.","context":["Frequently Asked Question 6.2\nIs the Current Climate Change Unusual Compared to Earlier Changes in Earth’s History?\nClimate has changed on all time scales throughout Earth’s history. Some aspects of the current climate change are not unusual, but others are. The concentration of CO2 in the atmosphere has reached a record high relative to more than the past half-million years, and has done so at an exceptionally fast rate. Current global temperatures are warmer than they have ever been during at least the past five centuries, probably even for more than a millennium. If warming continues unabated, the resulting climate change within this century would be extremely unusual in geological terms. Another unusual aspect of recent climate change is its cause: past climate changes were natural in origin (see FAQ 6.1), whereas most of the warming of the past 50 years is attributable to human activities.\nWhen comparing the current climate change to earlier, natural ones, three distinctions must be made. First, it must be clear which variable is being compared: is it greenhouse gas concentration or temperature (or some other climate parameter), and is it their absolute value or their rate of change? Second, local changes must not be confused with global changes. Local climate changes are often much larger than global ones, since local factors (e.g., changes in oceanic or atmospheric circulation) can shift the delivery of heat or moisture from one place to another and local feedbacks operate (e.g., sea ice feedback). Large changes in global mean temperature, in contrast, require some global forcing (such as a change in greenhouse gas concentration or solar activity). Third, it is necessary to distinguish between time scales. Climate changes over millions of years can be much larger and have different causes (e.g., continental drift) compared to climate changes on a centennial time scale.\nThe main reason for the current concern about climate change is the rise in atmospheric carbon dioxide (CO2) concentration (and some other greenhouse gases), which is very unusual for the Quaternary (about the last two million years). The concentration of CO2 is now known accurately for the past 650,000 years from antarctic ice cores. During this time, CO2 concentration varied between a low of 180 ppm during cold glacial times and a high of 300 ppm during warm interglacials. Over the past century, it rapidly increased well out of this range, and is now 379 ppm (see Chapter 2). For comparison, the approximately 80-ppm rise in CO2 concentration at the end of the past ice ages generally took over 5,000 years. Higher values than at present have only occurred many millions of years ago (see FAQ 6.1).\nTemperature is a more difficult variable to reconstruct than CO2 (a globally well-mixed gas), as it does not have the same value all over the globe, so that a single record (e.g., an ice core) is only of limited value. Local temperature fluctuations, even those over just a few decades, can be several degrees celsius, which is larger than the global warming signal of the past century of about 0.7°C.\nMore meaningful for global changes is an analysis of large-scale (global or hemispheric) averages, where much of the local variation averages out and variability is smaller. Sufficient coverage of instrumental records goes back only about 150 years. Further back in time, compilations of proxy data from tree rings, ice cores, etc., go back more than a thousand years with decreasing spatial coverage for earlier periods (see Section 6.5). While there are differences among those reconstructions and significant uncertainties remain, all published reconstructions find that temperatures were warm during medieval times, cooled to low values in the 17th, 18th and 19th centuries, and warmed rapidly after that. The medieval level of warmth is uncertain, but may have been reached again in the mid-20th century, only to have likely been exceeded since then. These conclusions are supported by climate modelling as well. Before 2,000 years ago, temperature variations have not been systematically compiled into large-scale averages, but they do not provide evidence for warmer-than-present global annual mean temperatures going back through the Holocene (the last 11,600 years; see Section 6.4). There are strong indications that a warmer climate, with greatly reduced global ice cover and higher sea level, prevailed until around 3 million years ago. Hence, current warmth appears unusual in the context of the past millennia, but not unusual on longer time scales for which changes in tectonic activity (which can drive natural, slow variations in greenhouse gas concentration) become relevant (see Box 6.1).\nA different matter is the current rate of warming. Are more rapid global climate changes recorded in proxy data? The largest temperature changes of the past million years are the glacial cycles, during which the global mean temperature changed by 4°C to 7°C between ice ages and warm interglacial periods (local changes were much larger, for example near the continental ice sheets). However, the data indicate that the global warming at the end of an ice age was a gradual process taking about 5,000 years (see Section 6.3). It is thus clear that the current rate of global climate change is much more rapid and very unusual in the context of past changes. The much-discussed abrupt climate shifts during glacial times (see Section 6.3) are not counter-examples, since they were probably due to changes in ocean heat transport, which would be unlikely to affect the global mean temperature.\nFurther back in time, beyond ice core data, the time resolution of sediment cores and other archives does not resolve changes as rapid as the present warming. Hence, although large climate changes have occurred in the past, there is no evidence that these took place at a faster rate than present warming. If projections of approximately 5°C warming in this century (the upper end of the range) are realised, then the Earth will have experienced about the same amount of global mean warming as it did at the end of the last ice age; there is no evidence that this rate of possible future global change was matched by any comparable global temperature increase of the last 50 million years.","What is Ocean Acidification?\nSince the beginning of the Industrial Revolution, when humans began burning coal in large quantities, the world’s ocean water has gradually become more acidic. Like global warming, this phenomenon, which is known as ocean acidification, is a direct consequence of increasing levels of carbon dioxide (CO2) in Earth’s atmosphere.\nPrior to industrialization, the concentration of carbon dioxide in the atmosphere was 280 parts per million (ppm). With increased use of fossil fuels, that number is now approaching 400 ppm and the growth rate is accelerating. Scientists calculate that the ocean is currently absorbing about one quarter of the carbon dioxide that humans are emitting. When carbon dioxide combines with seawater, chemical reactions occur that reduce the seawater pH, hence the term ocean acidification.\nCurrently, about half of the anthropogenic (human-caused) carbon dioxide in the ocean is found in the upper 400 meters (1,200 feet) of the water column, while the other half has penetrated into the lower thermocline and deep ocean. Density- and wind-driven circulation help mix the surface and deep waters in some high latitude and coastal regions, but for much of the open ocean, deep pH changes are expected to lag surface pH changes by a few centuries.\nOcean acidification and global warming are different problems, but are closely linked because they share the same root cause—human emissions of carbon dioxide. The atmospheric concentration of carbon dioxide is now higher than it has been for the last 800,000 years and possibly higher than any time in the last 20 million years. Humans have thus far benefited from the ocean’s capacity to hold enormous amounts of carbon, including a large portion of this excess carbon dioxide. Had the ocean not absorbed such vast quantities of carbon dioxide, the atmospheric concentration would be even higher, and the environmental consequences of global warming (sea level rise, shifting weather patterns, more extreme weather events, etc.) and their associated socioeconomic impacts would likely be even more pronounced. However, the oceans cannot continue to absorb carbon dioxide at the current rate without undergoing significant changes in chemistry, biology, and ecosystem structure.\nMeasuring ocean acidification: Past and present\nScientists know that the oceans are absorbing carbon dioxide and subsequently becoming more acidic from measurements made on seawater collected during research cruises, which provide wide spatial coverage over a short time period, and from automated ocean carbon measurements on stationary moorings, which provide long-term, high-resolution data from a single location.\nThese records can be extended back through time using what are known as chemical proxies to provide an indirect measurement of seawater carbonate chemistry. A proxy is a measurement from a natural archive (ice cores, corals, tree rings, marine sediments, etc.) that is used to infer past environmental conditions. For example, by analyzing the chemical composition of tiny fossil shells found in deep ocean sediments, scientists have developed ocean pH records from ancient times when there were no pH meters. Furthermore, because the ocean surface water is in approximate chemical balance, or equilibrium, with the atmosphere above it, a record of historical ocean pH can be inferred from atmospheric carbon dioxide records derived from Greenland and Antarctic ice cores, which contain air bubbles from the ancient atmosphere. Such evidence indicates that current atmospheric carbon dioxideconcentrations and ocean pH levels are at unprecedented for at least the last 800,000 years.\nGoing back deeper in Earth history to the Paleocene-Eocene boundary about 55 million years ago, scientists have found geochemical evidence of a massive release of carbon dioxide accompanied by substantial warming and dissolution of shallow carbonate sediments in the ocean. Although somewhat analogous to what we are observing today, this carbon dioxide release occurred over several thousand years, much more slowly than what we are witnessing today, thus providing time for the oceans partially to buffer the change. In the geologic record, during periods of rapid environmental change, species have acclimated, adapted or gone extinct. Corals have undergone large extinction events in the past (such the Permian extinction 250 million years ago), and new coral species evolved to take their place, but it took millions of years to recover previous levels of biodiversity.\nHow is ocean acidification affecting ocean chemistry?\nSeawater has a pH of 8.2 on average because it contains naturally occurring alkaline ions that come primarily from weathering of continental rocks. When seawater absorbs carbon dioxide from the atmosphere, carbonic acid is produced (see Box 1), reducing the water’s pH. Since the dawn of industrialization, average surface ocean pH has decreased to about 8.1.\nBecause the pH scale is logarithmic (a change of 1 pH unit represents a tenfold change in acidity), this change represents a 26 percent increase in acidity over roughly 250 years, a rate that is 100 times faster than anything the ocean and its inhabitants have experienced in tens of millions of years.\nAcidification can affect many marine organisms, but especially those that build their shells and skeletons from calcium carbonate, such as corals, oysters, clams, mussels, snails, and phytoplankton and zooplankton, the tiny plants and animals that form the base of the marine food web.\nThese “marine calcifiers” face two potential threats associated with ocean acidification: 1) Their shells and skeletons may dissolve more readily as ocean pH decreases and seawater becomes more corrosive; and 2) When CO2 dissolves in seawater, the water chemistry changes such that fewer carbonate ions, the primary building blocks for shells and skeletons, are available for uptake by marine organisms. Marine organisms that build shells or skeletons usually do so through an internal chemical process that converts bicarbonate to carbonate in order to form calcium carbonate.\nExactly how ocean acidification slows calcification rates, or shell formation, is not yet fully understood, but several mechanisms are being studied. Most hypotheses focus on the additional energy an organism must expend to build and maintain its calcium carbonate shells and skeletons in an increasingly corrosive environment. In the face of this extra energy expenditure, exposure to additional environmental stressors (increasing ocean temperatures, decreasing oxygen availability, disease, loss of habitat, etc.) will likely compound the problem.\nThese effects are already being documented in many marine organisms, particularly in tropical and deep-sea corals, which exhibit slower calcification rates under more acidic conditions. The impact on corals is of great concern because they produce massive calcium carbonate structures called reefs that provide habitat for many marine animals, including commercially important fish and shellfish species that use the reefs as nursery grounds. Coral reefs are vital to humans as sources of food and medicine, protection from storms, and the focus of eco-tourism. In addition to corals, studies have shown that acidification impairs the ability of some calcifying plankton, tiny floating plants and animals at the base of the food web, to build and maintain their shells. Scientists have also observed increased larval mortality rates of several commercially important fish and shellfish.\nWhat can we expect in the future?\nOcean acidification is occurring at a rate 30 to100 times faster than at any time during the last several million years driven by the rapid growth rate atmospheric CO2 that is almost unprecedented over geologic history. According to the Intergovernmental Panel on Climate Change (IPCC), economic and population scenarios predict that atmospheric CO2 levels could reach 500 ppm by 2050 and 800 ppm or more by the end of the century. This will not only lead to significant temperature increases in the atmosphere and ocean, but will further acidify ocean water, reducing the pH an estimated 0.3 to 0.4 units by 2100, a 150 percent increase in acidity over preindustrial times. Assuming a “business-as-usual” IPCC CO2 emission scenario, predictive models of ocean biogeochemistry project that surface waters of the Arctic and Southern Oceans will become undersaturated with aragonite (a more soluble form of calcium carbonate) within a few decades, meaning that these waters will become highly corrosive to the shells and skeletons of aragonite-producing marine calcifiers like planktonic marine snails known as pteropods.\nAlthough ocean acidification has only recently emerged as a scientific issue, it has quickly raised serious concerns about the short-term impacts on marine organisms and the long-term health of the ocean. Scientists estimate that over the next few thousand years, 90 percent of anthropogenic CO2 emissions will be absorbed by the ocean. This may potentially affect biological and geochemical processes such as photosynthesis and nutrient cycling that are vital to marine ecosystems on which human society and many natural systems rely. At the same time, marine organisms will face the enormous challenge of adapting to ocean acidification, warming water, and declining subsurface-ocean oxygen concentrations.\nNews & Insights\nWHOI working to address ocean acidification; protect region’s vital shellfish industry\nA new report addresses the impacts of ocean acidification in Massachusetts and New England coastal waters on the region’s vital seafood industry.\nOcean acidification gets a watchful eye in New England aquaculture ‘hot spot’\nShellfish aquaculture is thriving in New England, but future growth in the industry could be stunted as coastal waters in the region become more acidic. Researchers at WHOI have developed…\nOcean acidification causing coral ‘osteoporosis’ on iconic reefs\nScientists Pinpoint How Ocean Acidification Weakens Coral Skeletons\nClimate Change Will Irreversibly Force Key Ocean Bacteria into Overdrive\n[ ALL ]\nWHOI in the News\nThe Top Eight Ocean Stories of 2022\nThe $500 Billion Question: What’s the Value of Studying the Ocean’s Biological Carbon Pump?\nEcology Research: Ocean acidification causing coral ‘osteoporosis’ on iconic reefs\nDisentangling influences on coral health\n[ ALL ]\nFrom Oceanus Magazine\nOcean acidification is no big deal, right?\nWHOI’s Jennie Rheuban discusses the very real phenomenon of an increasingly acidic ocean and the toll it’s taking on marine life.\nTo Tag a Squid\nHow do you design a tag that can attach to a soft-bodied swimming animal and track its movements? Very thoughtfully.\nHow Do Corals Build Their Skeletons?\nWHOI scientists discovered precisely how ocean acidification affects coral skeletons’ a factor that will help scientists predict how corals throughout the world will fare as the oceans become more acidic.\nSearching for ‘Super Reefs’\nSome corals are less vulnerable to ocean acidification. Can the offspring from these more resilient corals travel to other reefs to help sustain more vulnerable coral populations there?\nGraduate student Hannah Barkley is on a mission to investigate how warming ocean temperatures, ocean acidification, and other impacts of climate change are affecting corals in an effort to find…"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:3504e0be-1f20-4313-b40b-aa594b9e43e5>","<urn:uuid:51ca6f21-f0f4-4343-8b14-a7f059f0fe69>"],"error":null}
{"question":"What role do technological innovations play in sustainable food production, and how do geographical factors influence their implementation?","answer":"Technological innovations like digital fish auctions and vertical farming are transforming sustainable food production. Digital auctions have enhanced fisheries sustainability through improved monitoring and transparent catch reporting, while helping stabilize market prices and ensure fair competition. For vertical farming, geographical considerations are crucial for sustainability - these systems can reduce water usage by 95% compared to conventional agriculture and are particularly valuable in water-scarce regions like Sub-Saharan Africa and the Middle East. The technology is also especially beneficial in extreme climates like Scandinavian countries where traditional agriculture faces challenges. Both technologies' effectiveness depends heavily on regional factors - vertical farms are most successful in areas with water scarcity, extreme climates, or limited agricultural land, while digital fish auctions benefit regions with established fishing industries and internet infrastructure.","context":["Pefa: using technology to increase the sustainability, transparency and profitability of fisheries\nPefa Auction Clock, a service of Pefa and the first auction system for fresh fish to use the Internet, revolutionized Europe’s seafood industry in terms of its transparency, fisheries monitoring and sustainability. It has helped governments to better monitor the status of their fisheries as well as helping to stabilize the seafood market to the benefit of fisher-folk and buyers alike. Adeyemi Ademiluyi (Seafood Trade Intelligence Portal (STIP)) talked with Gijsbert Spek, managing director of Pefa, about the influence of Pefa Auction Clock in the market.\n“At the start of digital auctioning most of the buyers were still local and on-site despite the technical capabilities of the systems, even the remote buyers were from the area. It took a while for the international purchases to take hold. People, stuck in their old ways, did not believe that it would be a fair system; they thought that your chances of getting a good deal were higher in the auction house. As people got more used to the Internet and buying on the Internet, and with the new generations coming into the companies, these fears started to subside.”\nIncreasing transparency and stabilizing prices\n“I can’t prove it but I think digital auctions have helped to stabilize price developments and make it fairer for everyone, which is good for the market. You can no longer see who is buying- back in the day, when you were in the auction room, you saw everybody. You knew that every week Bob bought 10 tons of salmon, and you saw that he had already bought his 10 for the week. You knew that he was complete, so you knew the playing field and could bid accordingly. Now, you can still see who has bought what, but you don’t know who is looking and who might have a big client for a certain product; it makes it harder to underbid.”\n“Digital auctions opened up competition, on both the side of the buyer and the seller, ensuring fairer prices that are closer to market value. There is no monopoly as there are always other buyers and other sellers. For the fisherman, it gives them a big market reach and access to a lot of buyers both local and remote. It also creates a lot of competition for them, with buyers being able to source products from several different auctions in different countries simultaneously.”\n“Digital auctions also made fish sales more transparent from day one. Everything is monitored, recorded and out in the open. Who is catching what kind of fish, where they are catching it, how much they are catching, what prices it is going for and what method they are using? If you have a fish company in Italy, and you are buying from the Netherlands, you know that they buy their fish from auction; it’s the only place that they can buy a fish. If you have access to the secured part of the Pefa website, where you can see the prices that your supplier is paying for the fish, it changes things.”\nFisheries management policies and sustainability\n“It also helps governments manage fisheries and fishing quotas. Auctions are obliged to disclose catches, as are the fishermen. Before, auctions would send a physical disk every week. Then it changed to email. Now governments have access to landing reports in real time. The level of detail and the reliability of the data have also increased and the data gets used in monitoring, policy development and to write reports and publications.”\n“Back in the days when I began in the world of seafood auctions we didn’t talk about sustainability or traceability; it was not an acknowledged issue. What we saw in the use of the Internet was the potential of new buyers from abroad having access to our fish auctions. That has changed over the years; sustainability has become more and more important in the fish industry. Transparency as well, but transparency has always been built into our way of working in the fish business- everything is registered. We have always had the vision that everything should be as transparent as possible; that transparency is the best solution for the market.”\nThis is an edited version. Please go to Undercurrent News to read more.","Vertical Farming: Location a Key Factor to Success, Says IDTechEx\nVertical farming, the practice of growing crops indoors on vertically stacked layers, has received no small amount of interest over the last few years. Vertical farms commonly tout impressive numbers, such as using 95% less water and providing crop yields 20-30 times that of conventional agriculture. These claims, among many others, have seen many vertical farming start-ups being founded alongside large amounts of industry funding; funding for the industry reached a record high in 2021, with over US$1 billion being raised across the entire industry. The recent IDTechEx report, \"Vertical Farming 2022-2032\", details the economic and technological factors shaping this rapidly growing industry.\nWith crops being grown indoors under controlled environments, a selling point used by multiple vertical farms is that they can grow crops anywhere – even in the heart of a city. This has led to proponents of the industry envisioning \"smart cities\", where vertical farms in city skyscrapers help feed the urban population. While this is achievable in principle, the truth is that the choice of location for vertical farming is much more involved and intricate than it may appear from these claims alone. Choosing an ideal location can be one of the most important factors in determining the success of a vertical farm.\nSome vertical farms may choose to set up their facilities in pre-existing facilities, such as abandoned warehouses. In these cases, identifying the suitability of the venue is the first point of consideration: vertical farms are very energy intensive, and it is important to ensure the facilities chosen can support these energy loads. In addition, the ergonomics of the facility is also important; should the layout not be given proper consideration, this can impede workers and decrease worker efficiency. As labor costs are typically among the largest sources of expenditure for a vertical farm, improving labor efficiency to reduce these costs is of paramount importance.\nWhile growing crops in the center of a city may seem ideal, the reality is that this may be counterproductive. Obtaining and maintaining such a location is expensive and can contribute significantly to the operating expenditure of a vertical farm while presenting logistical challenges in distributing produce; the \"last mile\" of food distribution is often the hardest. Having a farm right next to the consumers themselves may also be less ideal than instead choosing a location near food distribution centers, as this allows for more efficient delivery of produce. As distribution centers are typically located on the outskirts of cities, the cost of land is also much cheaper. This is the approach chosen by UK-based Jones Food Company, which chose Scunthorpe as a location for its vertical farm – this is a relatively low-cost location located near food distribution centers and a network of motorways that could still reach many consumers in a day, even if it isn't right in the middle of the capital city. Vertical farms should carefully consider their place in the supply chain before establishing a base.\nOn a larger scale, vertical farms may prove more profitable in different geographical regions. Vertical farms can reduce water usage significantly over conventional agriculture, and the high degree of control over the growing environment allows them to grow crops in extreme climates – where such crops may not otherwise be able to grow. In return, vertical farms demand more energy to carry out growing operations. To maximize their potential, vertical farms would ideally be located in regions of water scarcity, such as Sub-Saharan Africa and the Middle East, or in areas with extreme climates, such as in Scandinavian countries, where the low amounts of sunlight and high costs of regulating greenhouse environments single out vertical farms as an optimal solution. The amount of agricultural land available is also an important factor – regions looking to increase food security and reduce reliance on imports while facing challenges in acquiring sufficient agricultural land would find vertical farms to be ideal. A particularly prominent example of such a country is Singapore, which has demonstrated much interest in vertical farming over the last few years.\nBeyond the considerations of water scarcity and temperature, the general availability of fresh produce and the distribution networks of given countries should also be considered. Vertical farms use the added freshness and higher quality of their crops as a primary selling point, but these are typically offset by higher prices. Should there already be a large supply of high-quality produce made available at lower costs, vertical farms will find it hard to distinguish their own produce and may struggle to establish a significant market share. The converse would also be true; should a country lack easy access to fresh produce, vertical farms are expected to see much demand for their produce. An example of such a region would be the Middle East: leafy greens typically travel several thousand miles to reach stores, resulting in consumers facing high prices and low-quality products. The high price of conventionally farmed leafy greens, alongside government subsidies, makes it easier for vertically farmed produce to approach price parity while providing much fresher, higher-quality products.\nWhile the choice of location is an important consideration, it is only one of many others that must be given proper thought. Only through proper optimization of growing operations to improve efficiency and reduce costs can vertical farms reach their true potential. In the IDTechEx report, \"Vertical Farming 2022-2032\", many further important factors for consideration are discussed in detail, and the future of vertical farming is evaluated through 10-year market forecasts.\nIDTechEx guides your strategic business decisions through its Research, Subscription and Consultancy products, helping you profit from emerging technologies. For more information, contact research@IDTechEx.com or visit www.IDTechEx.com.\nThis post does not have any comments. Be the first to leave a comment below.\nPost A Comment\nYou must be logged in before you can post a comment. Login now."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:eda18380-da7d-4d66-b089-0954f7195c46>","<urn:uuid:98c0eed7-8d91-42cd-b0d1-cc7c105f9a4d>"],"error":null}
{"question":"How does PACS integrate with other hospital information systems, and what are the recommended security measures for protecting patient data in these integrated environments?","answer":"PACS integrates with multiple hospital systems through standardized protocols. It works with the Hospital Information System (HIS), Radiology Information System (RIS), and uses the DICOM standard and HL7 protocol for communication. When a patient gets an x-ray, for example, the RIS handles scheduling and registration, then interfaces with PACS to provide patient demographics and exam information. To protect this integrated environment, several security measures are recommended: implementing strong encryption of data, requiring strong authentication, maintaining strict access control, regular monitoring of searches and downloads, and training staff about cybersecurity risks. Additionally, organizations should develop disaster recovery plans and consider cyber insurance to protect against data breaches.","context":["Send the link below via email or IMCopy\nPresent to your audienceStart remote presentation\n- Invited audience members will follow you as you navigate and present\n- People invited to a presentation do not need a Prezi account\n- This link expires 10 minutes after you close the presentation\n- A maximum of 30 users can follow your presentation\n- Learn more about this feature in the manual\nDo you really want to delete this prezi?\nNeither you, nor the coeditors you shared it with will be able to recover it again.\nMake your likes visible on Facebook?\nConnect your Facebook account to Prezi and let your likes appear on your timeline.\nYou can change this under Settings & Account at any time.\nTranscript of Untitled Prezi\nHistory of PACS\nPurpose of PACS\nElectronic Communication System components\nHow PACS works\nReferences Introduction Picture Archive and Communication Systems (PACS) is a medical imaging technology Have comprehensive networks of digital devices designed for acquisition, transmission, storage, display, and management of diagnostic imaging studies Computer Networks Dr.Homam Shraif Done By: Aiysha AlMuhaish\nShatha Alotiabi History The concept of a digital image communication\nand display system was devised 1970s 1982 principles of PACS were first discussed at meetings of radiologists 1992 first large-scale PACS in the United States by The U.S. Army Medical Research and Materiel Command Since that time, PACS technology has grown through the support of InfoRAD.\nJournal of Digital Imaging, devoted to research in digital technology.\nPACS have become widely used by hospitals.\ninﬂuenced by new technologies, faster network connections and other technical improvements To replace hard film copies with digital images.\nLong-term cost savings in radiology services.\nAvailable at all stations and on any time .\nMuch more efficient and greatly reducing the turn around time for report dictation. Why PACS ? Component of Electronic Communication System Picture Archiving and Communication system (PACS) Digital Imaging and Communications in Medicine (DICOM) Hospital Information System (HIS) Radiology Information System (RIS) Health Level Seven (HL7) DICOM A standard for the facilitation of electronic medical imaging, consisting of a standardized image(American College of Radiology, 2001). Developed by the National Electrical Manufacturers Association (NEMA).\nThe backbone of modern image display, It includes a file format definition and a network communications protocol.\nUse TCP/IP to communicate between systems.\nDICOM enables the integration of scanners, servers, workstations, printers, and network hardware from multiple manufacturers into a picture archiving and communication system(PACS). DICOM plays an integral role in the digital medicine by providing A universal standard of digital medicine\nExcellent image quality\nFull support for numerous image-acquisition parameters and different data types\nClarity in describing digital imaging devices and their functionality HIS An integrated system of both hardware and software that is used by a health care provider to support and conduct all information aspects of providing quality patient care and the business of health care (Englebardt & Nelson, 2002). Support of Clinical and Medical Patient Care Activities in the Hospital\nIncludes many applications addressing the needs of various departments in a hospital.\nModern hospital information systems typically use fast computers connected to one another through an optimized network.\nProduce relevant and quality information to support decision making RIS An electronic system that is used by radiology departments to manage medical imaging information such as diagnostic imaging orders, scheduling, and diagnostic imaging interpretations/ reports and to prepare billing information . RIS is usually based on the HL7 standard Hint While a PACS is managing the imaging data, the RIS is managing administrative tasks HL7 An electronic communication standard for health care applications that facilitates clear communication in the health care community through an agreed upon format or protocol for exchange. , integration, sharing, and retrieval of electronic health information The name \"Health Level-7\" is a reference to the seventh layer of the OSI Reference model also known as the application layer. The name indicates that HL7 focuses on application layer protocols for the health care domain. HL7 effectively considers all lower layers merely as tools. Imaging Modality Computed Tomography (CT) Magnetic Resonance Imaging (MRI) Ultrasound (US) Nuclear Medicine (NM) Computed Radiography (CR) One or many imaging modalities can be connected on the network.\nAll these modalities should generate images in DICOM format and be able to send images to a remote server using the standard DICOM format Digital Radiography (DX) How PACS Works With Other Systems The following example of an average chest x-ray illustrates how PACS, RIS, and other systems might operate together in a fully interfaced environment. 1) The patient’s doctor requests a chest x-ray, and it is scheduled in the RIS.\n2) The patient arrives at Imaging Department reception desk and is registered, and the chest x-ray is ordered in the RIS, creating a unique exam number. 3) The RIS system sends an interface message to the PACS system providing demographic information about the patient and the exam requested.\n4) The technologist,receives patient and exam demographics via the RIS/PACS interface. 5) The x-ray is performed and the digital image is transferred to the PACS server, where the RIS/PACS interface provides verification that the patient and exam information is valid.\n6) Digital copies of the x-ray are queued for long-term storage. 7) The radiologist reports the x-ray using a diagnostic-quality PACS workstation and dictates the result into the RIS system .\n8) The text result is stored in the RIS; once the radiologist signs the report, it is sent to the requesting physician and passed to the RIS/PACS interface, and the exam and report can then be viewed by authorized clinicians. The “Before and After” of a PACS Implementation PACS Components To understand the benefits of PACS, one must first understand the fundamentals of how this technology works together and components of PACS. Network Workstations Image Acquisition Component The images of a PACS are produced by several radiologic imaging modalities. The images can be transmitted from the modalities using a specied interface. A DICOM interface is the most frequently used standard. Due to the fact that many imaging equipments are not supporting industrial standards, like the DICOM standard, acquisition computers (also called acquisition gateways) are needed to enable the digital exchange of the images. So, computer is placed between the modalities and the PACS network. The computer receives the picture from the imaging modality through its speciﬁed interface, preprocessing it and converting it to a standard, which is supported by the PACS (i.e. the DICOM standard Figure 3. shows a simple model of the image acquisition process. PACS Controller The main engine of the PACS, It controls all transactions in the system between components and its database server and an archive system. The images and the patient information are transmitted from the imaging modalities or an acquisition computer, the radiology information systems (RIS) and the hospital information system (HIS) to the PACS controller. After receiving the data the controller continuous the processing of the data, consisting of the following tasks: Extracting text information describing the received studies\nDetermining the workstations to which the newly generated studies have to be forwarded\nAutomatically optimization of the pictures (optimal contrast, brightness and correct orientation)\nPerforming image data compression\nArchiving the picture and deleting archived pictures from the acquisition computers\nServing archive requests from workstation or other controllers The most important property of the PACS controller in cooperation with its archiving system is to fulﬁll data integrity and system efficiency.\nIt must ensure that no data is lost after receiving it from the imaging modalities. As long as an image is not archived in the long-term archive, PACS always keeps two copies of it in different storages . Data Base Server and Archiving System The PACS database ensure that all images are automatically grouped into the correct examination , are chronologically order , correctly orientated and labeled , and can be easily retrieve [strickland 2000] The PACS database server should consist of redundant databases with identical reliable commercial database software (e.g Oracle , MySQL). Is part of PACs controller. PACS database system should be interfaced to the radiology information system and HIS. The hardware of database system should use an fast multiple central processing unit and performance interfaces, like SCSI and S-ATA[Huang 1996]. Storage component separated in fast short term and slower long term. Workstations are the human interfaces of PACS. Kim et al [Kim 1991] wrote the workstations are the point of contact of the radiologist and referring physicians. The workstation computers are running software for communication, database access, displaying the images, resource management and for processing. With this software the following fundamental operations are performed on a PACS workstation (see [Huang 1996]): Case preparation (Accumulation of all relevant images and information belonging to a patient examination).\nImage arrangement (Selection of cases for a given subpopulation)\nTools for arranging (Tools for arranging and grouping images for easy review)\nInterpretation (Measurement tools for facilitating the diagnosis)\nDocumentation (Tools for image annotation, text, and voice reports)\nCase presentation (Tools for a comprehensive case presentation.) Heitmann [Heitmann 2006] distinguishes between four different types of workstation: Workstations for primary diagnosis with high resolution monitors (at least 2500x2000 pixel)\nWorkstations for writing reports with a lower resolution (at least 1000x1000 pixel),\nWorkstations for detailed evaluation with high resolution and faster graphic acceleration\nand a Workstation for digitalizing and printing, including a laser printer and a laser-film scanner Many of such network are connected by a wide area network (WAN), provided by an regional Internet service provider. In the future increasing WAN speed will allow hospitals to work together over larger distances sharing one PACS system. [Knig and Klose 1999]. For keeping a network simple and scalable for standard UNIX and Microsoft Windows based computers, the TCP/IP protocol is recommendable. As technologies for the local area network, the Ethernet protocol following the IEEE 802.x standard or asynchronous. Transfer mode (ATM) can be used; ATM allows priorisation of certain data, but is harder to implement as the technologies differ from vendor to vendor.\nEthernet provides higher network speed with up to 10 Gigabit/s, while ATM runs with up to 2.2 Gigabit/s. Video: PACS at BRCH- Before and After Thank You ... Any Question Summary The benefits of PACS system are clear. Within seconds after an image is acquired, it can be viewed by the radiologist and any number of referring and treating physicians simultaneously. There is no film to be lost or stolen.\nVirtual private networks can transmit whole examinations across the globe within seconds for remote consultations.\nToday’s archives can keep decades of images online in a cost-effective manner and in a much more organized and accessible manner DICOM is an object oriented principle which is using data and services in medical field. The Data is divided into two parts; text data and pixel data. These data is creating DCM files The services in DICOM principles are related to handling DCM files in the networks. The two most important principles in service are called SCP and SCU. SCP means the device which is listening or receiving. SCU is the device which is speaking or transmitting in the network. a DICOM session is created by commands like Get, Store, Find and Move References Maureen N. Hood, MS, RN; and Hugh Scott, MS. Introduction to Picture Archive and Communication Systems. 2006 Jun; 25(3). doi: 10.1016/j.jradnu.2006.06.003\nIAN D. ROBERTSON, TRAVIS SAVERAID .Hospital, radiology, and picture archiving and communication systems.2008 Jan; 49(1). doi: 10.1111/j.1740-8261.2007.00329.x\nDr Strickland. PACS (picture archiving and communication systems): filmless radiology.2000 Jan; 83(1):82-86. doi:10.1136/adc.83.1.82\nDr. Samir H. Abdul-Jauwad, Senior Member, IEEE, and Eng. Mansour I. AlJaroudi. Developing a Wireless DICOM for Osirix.2012 Jan.89-92.doi: 10.1109/BHI.2012.6211514\nMximilian Hecht. PACS - Picture Archiving and Communication System [Internet].2013.march.27 < http://www.cg.tuwien.ac.at/courses/Seminar/WS2009/PACS.pdf> Margaret Rouse, picture archiving and communication system (PACS) [Internet].2010[updated June 2010] < http://searchhealthit.techtarget.com/definition/picture-archiving-and-communication-system-PACS>\nPACS - Picture Archiving and Communication System [Internet].2013.march.25 < http://www.aheconline.com/mktg/Course_Materials/Dig_Facts_Fiction/6-Dig_Facts_Fiction.pdf>\nPACS Fundamentals chapter 1 [Internet].2013.march.25\nHow does PACS technology affect health care IT [Internet].2013.march.27 <http://searchhealthit.techtarget.com/tutorial/FAQ-How-does-PACS-technology-affect-health-care-IT>\nPicture archiving and communication system [Internet].2013.march.27\nPACS - Picture Archiving and Communication System [Internet].2013.march.27\n< http://www.cg.tuwien.ac.at/courses/Seminar/WS2009/PACS.pdf >\nSNWombat. PACS at BRCH- Before and After[Youtube Video]. South Florida:Boca Raton Community Hospital; 2001.1 video: 4:37 min., sound, colour, 4 ¾ in. Figure 1. DICOM Commands. Figure 2. Flow of work Figure 4. Network Model of PACS.","Healthcare organizations must modernize their cybersecurity systems or risk falling behind the times compared to other industries that have made significant investments in safeguarding their IT assets. Reports have predicted that the healthcare sector will face more cyberattacks than any other industry, as hackers exploit system weaknesses. Find out need-to-know information regarding healthcare cybersecurity and get the wake-up call you need to protect your healthcare organization.\nThe Rising Cost of Healthcare Industry Data Breaches\nAccording to the Identity Theft Resource Center, over one-quarter of all data breaches target the healthcare industry, resulting in estimated losses totaling $5.6 billion. Accenture, a consulting firm, believes that healthcare industry breaches will cost over $300 billion in cumulative lifetime patient revenue over the next five years.\nIn addition to the financial cost, businesses face significant reputational damage after a cyberattack. In 2015, 113 million health records were exposed in the U.S. An additional 16 million American health records were exposed in 2016, per the U.S. Department of Health and Human Services.\nWhy Cyberattackers Target the Healthcare Industry\nThe numbers show the devastating impact of cyberattacks in the healthcare industry. It’s worth pointing out that the recent rise in healthcare data breaches comes as other industries have increased their investment in cybersecurity systems that detect, block, and mitigate attacks. Hackers searching out new targets turned to industries they previously ignored, such as healthcare. It’s much easier to apply the same strategies to a new industry than invest time in developing new hacks for the latest cyber security, after all.\nOnce cyber thieves get healthcare data, what do they do with it? The black market has many uses for medical records. Cyber criminals can harvest the data to conduct “medical identity theft.” From the harvested data, they can create false or synthetic identities.\nThey can glean enough raw data from an individual’s medical record to perform traditional identity theft. From a medical record, they may be able to open a bank account or credit card, take out a business loan, and more.\nMedical identity theft is more complex than credit card theft. While a consumer can detect and dispute a credit card charge fairly quickly, it is more difficult to resolve medical identity theft.\nCyber criminals have begun using ransomware to extract money from healthcare organizations. Criminals steal the data then promise to return access to the data or systems if a ransom is paid.\nSince healthcare organizations have been slow to adopt cybersecurity best practices, medical staff may be unaware of the risks to patient data. The average cybersecurity budget for a healthcare organization is a fraction of the budget for a financial firm.\nRecommendations for Cybersecurity\nBoth provincial and federal governments set laws regarding privacy of data. These laws indicate who can access medical information, how data must be stored and managed, and what protocol to follow if there is a breach. If you’re not sure what the privacy laws are, or how they apply to healthcare data security, this is the place to start. If it’s been a while since you’ve reviewed the privacy laws, look for recent, relevant findings and rulings that could apply to your business.\nSince many healthcare workers do not know the risks of cyberattack, it’s important to educate staff. Not only should IT workers understand cybersecurity threats and protection, but medical workers should know how their roles pertain to data management. In particular, staff should understand how to properly handle confidential information including patient data. Staff should know how to recognize common types of cyberattacks, such as phishing emails. By training your employees on cybersecurity, you can cut the risk of an attack from 70 percent to 45 percent.\nIndustry Best Practices\nBest practices that combine the particular needs of the healthcare sector with practical cybersecurity solutions do exist. They include strong encryption of data, strong authentication requirements for data, strict access control of data, and regular monitoring of searches and downloads. If a large batch of sensitive data is downloaded or transferred, it should trigger an alarm. Along with implementing best practices, healthcare businesses should research newer technologies that may offer additional levels of protection. These include biometric security, tokenization, and blockchains to record transactions.\nDisaster recovery planning is essential for the healthcare businesses. Comprehensive planning includes defining what needs to be protected, implementing a DR plan, and testing the plan. The growth of DRaaS or disaster recovery as a service allows a healthcare businesses to invest in the disaster recovery they need while keeping costs affordable.\nCyber Insurance A cyber insurance policy can help protect your business from a data breach and many other IT disasters. A comprehensive insurance policy will cover claims made against your business by victims of the breach as well as direct losses you experience.\nIT Service Providers\nIf all of this sounds like a lot to handle on your own, consider turning to the cloud for help. Healthcare organizations are increasingly using the cloud to store data and share data while controlling cost. Cloud service providers offer scalable solutions for healthcare, so your business can access the protections you need now and change your plan as you grow. Many service providers understand the regulations, risks, and needs of the healthcare industry and act as supportive partners.\nGiven the gap between healthcare cybersecurity and the security of other industries, it is critical that healthcare organizations make it a priority to protect their data. As long as the healthcare sectors uses vulnerable hardware and software for transactions and does not move toward stronger cybersecurity practices, organizations will continue to face staggering risk.\nProtecting health data will take a coordinated effort among healthcare companies, care providers, insurance companies, and IT partners who can provide solutions and experience. By adopting the lessons learned by other industries — namely financial and legal — healthcare companies can reduce their risk and protect their patient data before “the worst” happens."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:350d1d0b-7700-4011-a99b-3c04f3806b3a>","<urn:uuid:24b7fcad-d28b-4d04-850a-afb34ef33bb0>"],"error":null}
{"question":"What's the relationship between compositional space requirements for portraits and technical camera settings for portrait photography?","answer":"Portrait photography involves both compositional and technical considerations. From a composition perspective, when a subject isn't looking directly at the camera, their head should be placed slightly off-center in the opposite direction of their gaze to provide 'space to look into,' and subjects not in the center should generally look toward the center of the frame. On the technical side, portraits typically require fast shutter speeds (faster than 1/60th of a second) to freeze motion and capture sharp images. This technical requirement may need to be balanced with additional lighting, larger aperture, or higher ISO to compensate for the reduced light from using fast shutter speeds.","context":["Using Focal Points in Photography: movement\nIf you prefer your rules of photo composition based on clear-cut mathematical formulae, you’re going to find this one uncomfortably vague.\nIt’s all to do with implied movement – a very important concept, even though photographs might appear to be completely static snapshots in time.\nMany photographs contain some kind of implied movement. The objects obviously aren’t moving as you look at them in the photograph, but it’s clear that a pedestrian was walking, for example, at the time you took the picture; that an athlete caught mid-hurdle will hit the ground with their feet a moment later; or that a speeding Formula One car that’s been frozen by the camera is nevertheless hurtling towards the next corner at 200 miles per hour.\nWhere there is this kind of implied movement, the composition may well look uncomfortable if there’s not a little space for the object to move into.\nWe’re happier if we can see the track where the hurdler’s foot is about to land; if a pedestrian’s about to walk into a doorway, we prefer to see the door.\nAnd it’s better to have more space in front of the Formula One car than there is behind it. Our minds like to mentally extrapolate the movement implied in our photographs.\nThere is also another kind of implied movement that’s rather less literal, and it’s the direction of gaze of any people in the photograph.\nIf they’re looking at something, we’re kind of interested in what it is they’re looking at.\nQuite apart from that, though, there needs to be space within the composition for their gaze to move into. This is another kind of movement we need to allow for when framing the photograph.\nIt’s very difficult to offer any precise rules for this – it’s rather like balancing shapes and tones in a photograph, in that you really have to do it by eye and instinct.\nThere are one or two generalisations you can make in portraiture, though.\nFirst, if the subject isn’t looking directly at the camera but slightly to one side, you’ll probably need to place their head slightly off-centre in the opposite direction to their gaze – they need space to look into.\nThe other is that subjects not in the centre of the frame should nevertheless be looking towards the centre. But like all rules, this one’s just begging to be broken.\nPAGE 1 – Using Focal Points in Photography: focal position\nPAGE 2 – Using Focal Points in Photography: movement\nPAGE 3 – Using Focal Points in Photography: breaking the rules\nPAGE 4 – Using Focal Points in Photography: leading the eye\nPAGE 5 – Using Focal Points in Photography: negative space\nPAGE 6 – Using Focal Points in Photography: exposure issues\nBanish Bad Pictures: 9 quick fixes for common camera complaints\n10 reasons your photos aren’t sharp (and how to fix them)\nNew camera anatomy: 12 key camera settings to get you started right\n24 camera features every beginner must memorize","Shutter Speed of a camera lets you control how you “Capture Motion” in your images. It is also one of the 3 elements of the exposure triangle and needs to be adjusted to get the correct exposure. If you are a beginner and just moving out of the Automatic mode of your camera, understanding the shutter speed can be a bit tricky at times but is totally worth the effort.\nBefore we begin to understand what exactly Shutter Speed is, let us look at a few images.\nThe key difference of technique used for capturing the above images is the variation in shutter speed. Let us dive into what Shutter Speed actually is and how it affects images.\nWhat is Shutter Speed?\nShutter Speed of a camera is the duration for which the camera shutter is raised (or open) from its position, allowing light to fall on the camera sensor.\nMeasured in seconds, shutter speed can vary from a tiny fraction of a second to a few minutes or even more. Selection of shutter speed depends on how you want to show your subject and the amount of light available.\nTypical shutter speeds available on cameras are 4seconds, 2 seconds, 1 second, ½ seconds, ¼th of a second and so on up to 1/4000 th of a second along with a bulb mode.\nSlow Shutter Speeds\nA slower shutter speed (anything less than 1/60th of a second) will bring in some motion of the subject in the image as the sensor is exposed to incoming light for a considerable amount of time.\nWhen using a slower shutter speed, removing camera shake is very important to maintain high image quality. Using tripods and/or stabilizers makes it possible to create high-quality images while using lower shutter speeds.\nHandheld photography is mostly done above shutter speeds of 1/60th of a second, as anything less induces camera shake. Camera shake is the motion of the camera during the interval for which the shutter is open, resulting in blur in images.\nFast/High Shutter Speed\nHigh shutter speeds (anything faster than 1/60th of a second to 1/8000th of a second, or even faster) are used to freeze motion. Action photography, most of Portraiture and many other genres demand high shutter speeds be used. The problem with high shutter speed is that the sensor is exposed for a very small duration which in turn means that very little light is available to create the image. Additional lighting, a large aperture, and even high iso can be used to compensate for high shutter speed.\nThere is no better or worse choice of shutter speed, there is only the best choice for the type of pictures you want to click under the conditions that are present.\nHow Shutter Speed affects your Pictures.\nSome examples of different shutter speeds and their results are shown below.\nSlow shutter speeds can be very effective to show motion. As the shutter is open for a longer duration, any relative motion between the camera and the subject is recorded by the sensor. Using tripods becomes a must to get proper long exposure shots.\nVariation from slow to fast shutter speed for the same moving subject. Showing motion in the subject can bring life to the image and make is a lot more meaningful.\nHow does Shutter Speed affect Exposure?\nAs we already know by now, by controlling the shutter speed, we control the duration for which our camera sensor is exposed to incoming light. Shutter Speed can also be used to control the exposure for an image.\nThe image below shows the variation in Exposure for an image with changing Shutter Speed when all other parameters are kept constant.\nTo summarize Shutter Speed, it controls how “frozen” the subject in our image is, at the cost of the exposure.\n“Slower shutter speed equals more motion with more light”\n“Faster shutter speed equals frozen subject with less light”\nAs a photographer, this is one of the decisions that one needs to make all the time.\n“Do you let in more light by dialing the shutter speed down a couple of stops?”\n“Is freezing motion more important than avoiding grain in the image?”\n“Is a particular shutter speed fast enough to allow you to shoot handheld?”\nHopefully, with the understanding of shutter speed, it will be a lot easier to control the way your images look. Do check our article on how to nail the exposure of your images here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:a08fe9d8-cb4f-4b97-a0ff-7b288f5dab0e>","<urn:uuid:5995f387-7673-4da1-99b9-a07123edb832>"],"error":null}
{"question":"What therapeutic services are offered for children dealing with family separation, and what ethical boundaries must counselors maintain in these interventions?","answer":"For children dealing with family separation, play therapy services are offered using tools like dolls, toy figures, modelling clay, sandbox, and paints to help them express thoughts and anxieties they find difficult to talk about. Additionally, counseling services for ages 4-18 provide a safe space to discuss feelings about parental separation and build confidence and resilience. Regarding ethical boundaries, counselors must maintain professional distance by avoiding dual relationships with students/families, not using personal social media for communication, and documenting any out-of-office interactions. They must handle information confidentially and act in the best interest of students without malicious thoughts.","context":["Young person's services\nBright Light offers alternative ways for younger children to communicate with us and others including play therapy. Trained therapists specialising in work with young children help them explore their feelings creatively. Play can encourage children to express their thoughts and or anxieties for example and communicate extreme experiences which they might find it difficult to talk about. Therapeutic play using dolls and toy figures, modelling clay, a sand box and paints - just some of the things we have in the Bright Light toolbox, can help your child to communicate their concerns.\nYoung children are naturally upset when their parents are separating and often benefit from our play therapy service. Our specialist practitioners also work with children traumatised because of domestic abuse or when they have lost someone they loved and are able to use this form of therapy to express what they are feeling.\nYoung person's counselling\nOur 4-18@BL counselling service will help you if you are a young person worried about your parents’ divorce, separation or conflict and/or maybe having difficulties with a step-parent or other carer.\nWe know at Bright Light that you can often feel at risk when your parents are on the point of separating - it's a major traumatic change in your life and for those around you. As well as often feeling very unhappy, angry, or confused, you may also blame yourself for the break-up.\nStrong feelings can be stirred up within you and your family when you find yourself pulled between your loved ones and others expectations of where your loyalty should be when your parents/families split. We will help you to understand what is happening, and why, and we will support you to make sure this major transition is as positive as possible for you.\nWe work with you and whoever else you might want to be part of your journey through a major change in your life. We can help you to build your confidence and resilience so that you are able to cope better now and in the future with key life changes/challenges and will be able to develop your own positive relationships as an adult in the future.\nOur 4-18@BL counselling sessions will provide you with a safe confidential place to talk to someone about lots of things including:\nThe breakdown in your parents’ relationship, and how you feel,\nyou being unhappy or confused about a family relationship,\nany problems you may have living in a new stepfamily/cared for environment,\nissues out-with your family (bullying, gender identity),\nworries about your mental health (depression, anxiety, feel suicidal),\nloss of a loved one,\naddiction (gambling, sexual, substance, social media),\nissues around domestic abuse.\nWhere children and young people are exposed to domestic abuse we provide a confidential specialist service in a safe space in some Edinburgh schools and also from premises at 9A Dundas Street, Edinburgh. We promote each young person's safety and resilience in any abusive circumstance.\nPlease note that where children and young people are deemed to be at risk, normal protocols around confidentiality cannot apply and Bright Light is bound to report disclosures to social services or the police. Please be aware that we follow Getting It Right for Every Child (GIRFEC) protocols.\nReferring young people\nWe welcome young people between the ages of 4 and 18 being referred to us from parents, teachers, health professionals, social workers or family lawyers, for example, or children and young people may refer themselves.","Ethics Issue in Elementary School Counseling: Keep a Professional Distance\nPeople are guided and governed by rules and regulations that are on paper. There are, however, some that are not written down, and one is expected to follow; they are instincts and could also be referred to as ethics. Ethical issues are one of the most important aspects that one has to put in mind and observe to the latter while at the workplace or while carrying out various activities in their day to day lives. To one person, an action could be ethically right while the same action would appear wrong to a different individual. What this means therefore is that ethical issues are perceived differently by various people. The term ethics refers to the moral principles that dictate or determine a person’s character and differentiates between that what is good and bad (Alzheimer Europe, 2010). The aim of this paper is to undertake research on keeping a professional distance in Elementary School Counseling. The various components that involve keeping professional distance such as the relationship between the counselor and the child shall also be discussed. The research also aims at discussing the various ways of maintaining the professional distance which is an ethics issue. There are different types of school counselors, and this paper involves Elementary School Counselors whose duties will also be evaluated to understand better the aspect of keeping a professional distance. At Elementary school, the counselors are involved with young children and therefore should take the full burden of determining the extent to which they relate with the young. The above decisions are made concerning the ethical standards the counselors apply or believe.\nElementary School Counselors\nThe most important aspect while trying to understand one’s character is to get to know the people, things, situations, and the environment in which that person operates. The same case applies to this research’s main theme, which is how elementary school counselors should keep a professional distance. As seen earlier, elementary school counselors deal with children who are at a crucial stage of mental, emotional and intellectual development (Our Lady of the Lake University, n.d.). The children are at the early age of social development and are attaining attitudes towards their teachers and fellow students. They, therefore, require specialized attention from the counselors but it also means that the counselors should maintain a professional distance.\nThe counselors are tasked with the responsibility of building and maintaining professional relationships with both the children and their parents/guardians (Ohio Department of Education, 2015). The latter is a healthy practice as it creates confidence in the parents in that their children are in good hands and hence they can expect their children to receive the best care. In Ohio, for example, school counselors undertake a significant role in helping students in achieving success in education, living their dreams and in realizing their careers (Ohio Department of Education, 2015). The counselors have to achieve the above, and this could only be done if they keep a professional distance. A student will not be successful in education if he starts getting romantically involved with the counselor.\nFactors That Affect the ‘Keep a Professional Distance’ Rule and Ways in Which Elementary School Counselors Solve Them\nVarious associations support and set the rules and regulations that govern school counselors. The American School Counselor Association is one such body, and it helps all the stakeholders in the counseling department, be it counseling students, supervisors, and professionals (American School Counselor Association, 2016). The professional organization identifies the duties of a school counselor as follows; advocating, leadership, collaboration, and provision of consultation services within the field of counseling (American School Counselor Association, 2016). For one to provide the services mentioned above, he must act according to his professional code of ethics. The American School Counselor Association (2015) expects its members to handle students with dignity and respect. Dignity may involve a lot of things, but in this case, it applies to the relationship between the child and the counselor. If the counselor is, for example involved romantically with his/her students, then there is no dignity or respect for their professional boundaries. Romantic relationships between counselors and the students or even their guardians are often in breach of the respect for both parties. For this reason, counselors are called upon to ensure that they keep a professional distance in their practice.\nElementary school counselors and other school level counselors are expected to manage boundaries and dual relationships (American School Counselor Association, 2016). It is the counselor’s duty to ensure that he/she does not exceed certain boundaries during their interactions with the students and their parents. Counselors should be in a position to explain their relationships with students and the extent to which they go, to their patients (American School Counselor Association, 2016). A counselor may decide to attend a student’s distant event, but in doing so, they are advised to take the appropriate professional measures that would not put them in a compromising situation. In accordance with the American School Counselor Association (2016), counselors extend boundaries such as the ones mentioned earlier, and are hence expected to document the type of interaction and state the possible benefits, positive and negative outcomes for both the counselor and the student. Counselors are therefore in a position to keep a professional distance as bound by their professional rules and regulations.\nThe work of an elementary school counselor is to provide social, emotional and educational support to the students, among other services. These services are however monitored by a school administrator, and the counselors should refrain from offering such support if the administrator is absent (American School Counselor Association, 2016). The result for acting against the above is dual relationships with parents/guardians, school staffs and students which will later affect the professional distance that needs always to be maintained. School counselors should not have dual relationships that go beyond professional standards with school personnel and parents, as it may affect the integrity of the relationship between the students and the counselors (American School Counselor Association, 2016). Dual relationships are as a result of conducting other services other than the ones the counselor is commissioned with; such as taking part in grading the students in absentia of the administrator and installing direct discipline (American School Counselor Association, 2016). The latter practices adversely affect the professional distance that the counselors are required to keep, and to overcome such temptations, the counselors should stick to their specified duties.\nKeeping a professional distance while at the workplace involves more than just romantic interactions with fellow workers and the clients; in this case the students. According to the American School Counselor Association, a counselor should not communicate with the students and parents/guardians using their personal social media platforms such as e-mails, texts, and Facebook unless they are given such directives by the school district (2016). If a counselor encourages such kind of interaction, there is the loss of respect, and they eventually are taken for granted by the guardians/parents and the children. The professional distance that should be maintained is otherwise breached. To ensure that technology does not in any way interfere with the professional distance, the professional organizations provide the endorsement for the social sites to be used for communication between the counselors and the students, parents/guardians (American School Counselor Association, 2016). These sites are monitored by the organization and the school districts and get used whenever necessary. Technology is one of the major aspects that breach the code of ethics for maintaining a professional distance while at the workplace. Dealing with the latter issue aids at ensuring that elementary school counselors and other school levels counselors comfortably observe the Professional Distance rule.\nAt the elementary level, counselors are tasked the responsibility of enhancing academic, personal and social skills of the students that are important for their academic success. It is also at this stage that the students are taught on decision-making processes, self-understanding and coping strategies (South Carolina Guidance and Counseling Writing Team, 2008). Counselors at the Elementary level are therefore expected to act as role models to the students, and this could be achieved by keeping a professional distance in their jurisdiction. If a counselor starts to make advances for the young child, they become bad examples to the child and the society as a whole. There are various benefits associated with a student having to go through the guidance and counseling program, and they are attributed to a good counselor (South Carolina Guidance and Counseling Writing Team, 2008). Such benefits include improved academic performance, career exploration, developing problem-solving skills among others. These benefits and many others are attributed to a counselor who can maintain a professional distance. The students may not benefit if the counselors were to act against the code of ethics.\nParents/guardians are also beneficiaries of the guidance and counseling programs as they can indirectly support their children and enhance the parent-school interaction (South Carolina Guidance and Counseling Writing Team, 2008). The parents/guardians are also more knowledgeable on the assistance that they can receive from the school and the counselors. Guidance and counseling programs assist the parents in championing for their children’s academic and career development (South Carolina Guidance and Counseling Writing Team, 2008).All these benefits are experienced if the counselors act in good faith and give the required services. The counselors shall, therefore, maintain a professional relationship with the guardians/parents, which in turn facilitates respect between the two parties. A parent will not allow his child to be counseled by a person who is not of good moral values or one who goes about making advances at the same parent. Keeping a professional distance is, therefore, mandatory for any elementary school counselor.\nSchool counselors are expected to perform their duties in the best interest of the students and act in the absence of malicious thoughts (American School Counselor Association, 2017). Malicious thoughts range from murder, abduction, rape to bad judgment. Counselors should always keep a professional distance to prevent malicious thoughts from occupying their minds. If counselors romantically involve themselves with the students, then they are not acting in the best interest of the child. Such thoughts could lead to rape which is against the code of ethics guiding counselors. It is advisable for the counselors to keep looking back at the rules and regulations and always remind themselves of the sole reason they are in the counseling industry. Elementary school counselors should inform the students and their parents/guardians on the extent to which they go about their relationships before the counseling sessions (American School Counselor Association, 2017). The above action is one of the many ways that help counselors to maintain the professional distance as it could be observed that the advances may originate from either the student or the parent/guardian. If the counselor happens to notice romantic advances from the students or their parents, they should prevent the consequences as early as possible and most probably report to the administrator for him/her to take action.\nPersonal values and attitudes go a long way in assisting a counselor in keeping a professional distance. According to the American School Counselor Association, it is ethical for an elementary school counselor to improve awareness of ‘personal values, attitudes, and beliefs’; acknowledge when the personal characteristics interfere with effective performance (np, 2017). In this research, personal characteristics could be taken to mean having feelings for the minors and the urge to sexually get associated with them. If a counselor experiences such instances they should seek help; which in turn helps in maintaining professionalism. It is always better to solve a problem once the signs start to show rather than waiting to later deal with adverse consequences.\nThere are written job descriptions that ought to be followed by elementary school counselors and having sexual relationships with students is not one of them. Counselors should always be accountable for their actions which should follow the rules given and be as appropriate as possible (American School Counselor Association, 2017). When dealing with minors, some areas might be controversial, and counselors are advised to involve family members (American School Counselor Association, 2017). Such an action could, however, affects the professional distance as the family members may be tempted to get too close with the counselors. In such a case, it is upon the counselor to act according to the rules and show that they understand the code of ethics by discouraging such relationships.\nElementary school counselors may not be faced with the challenge of keeping a professional distance as other counselors in various levels because they are involved with young children. However, they face the challenge of having to deal with the parents/guardians who may have other intentions than for the child’s welfare. According to the American Counseling Association, some counselors may receive gifts from their clients, and they are expected to understand and overcome the challenges that come with such acts (2014). Some gifts are given out of good faith and are meant to show genuine gratitude while others come as a way of attracting one party to the other. If the counselor is not careful in discerning the aim behind the giving, they may fall into temptation. The counselor is, therefore, required to take into account the relationship between them and the client, the value of the gift and the reason which drives the client into giving (American Counseling Association, 2014). If the counselor is in a position to get answers for the latter situation, then they can comfortably maintain the professional distance as stated by the rules.\nCounselors could attend to a child of a person with whom they have had a previous relationship either at a former workplace or in life (American Counseling Association, 2014). The counselor must be very observant to prevent crossing the professional line and get involved with such a person. Counselors have to undertake ‘professional precautions such as informed consent, consultation, supervision, and documentation’ so as to ensure that exploitation does not occur and fair judgment is served (American Counseling Association, 5, 2014). Some past partners are malicious enough to ruin one’s career, and it is, therefore, advisable for a counselor to take the right measures if faced with such a situation. Other people are vulnerable enough to fall in the trap, and in this case, the counselor is sure to tamper with the professional distance.\nProfessional distance could be interfered with if counselors extend the counseling boundaries. Such extents could include visiting the student’s family member in the hospital; an act which could lead to impaired judgment (American Counseling Association, 2014). The counselor could be thought to develop some romantic relationship with either the family member or the child, and such a thought is deluded if there is proper documentation. The counselor is required to state the reasons for such a visit or any other out of office activity involving the students and their families (American Counseling Association, 2014).\nConfidentiality goes hand in hand with keeping a professional distance. Counselors are supposed to handle the students’ information with confidentiality and should not let anyone get access to the same unless guided by the administrator (Black, Riechel, and Shillingford, nd). It, however, becomes difficult for the counselors in trying to tell what information should be given to the school authority and that which should remain private. It is at such a point that keeping a professional distance becomes hard. The counselors should, however, follow the code of ethics on keeping a professional distance.\nThere are ethical rules and moral values that govern or guide people in their daily undertakings. Counselors have rules that govern how they go about providing their services at various school levels. Elementary school counselors deal with minors who are at the critical age of social and moral development. These children are at the stage where they seek academic success and development of future careers. It is the counselor’s duty to ensure that the students together with their parents/guardians reap maximum benefits. The counselors are faced with the challenge of maintaining the professional distance as they are expected to be close to the children but at the same time desist from falling into temptations. There are, however, rules that govern the counselors and solutions for the same are also provided. Elementary school counselors should always keep the professional distance and at the same time ensure that they act in the best interest of the students. The rules and regulations are set by organizations such as the American School Counselor Association (ASCA) and the American Counseling Association (ACS).\nAlzheimer Europe. 2010. What is meant by the term ethics? Retrieved from: http://www.alzheimer-europe.org/Ethics/Definitions-and-approaches/What-is-meant-by-the-term-ethics\nAmerican Counseling Association. 2014. 2014 ACA Code of Ethics. Retrieved from: https://www.counseling.org/resources/aca-code-of-ethics.pdf\nAmerican School Counselor Association. 2016. ASCA Ethical Standards for School Counselors. ASCA. Retrieved from: https://www.schoolcounselor.org/asca/media/asca/Ethics/EthicalStandards2016.pdf\nAmerican School Counselor Association. 2017. Ethical Tips for School Counselors. ASCA. Retrieved from: https://www.schoolcounselor.org/school-counselors-members/legal-ethical/ethical-tips-for-school-counselors\nBlack T S, Riechel K E M, and Shillingford A M. nd. School Counselors’ Constructions of Student Confidentiality. Retrieved from: http://files.eric.ed.gov/fulltext/EJ1034752.pdf\nOhio Department of Education. 2015. Ohio Standards for School Counselors. Retrieved from: https://education.ohio.gov/getattachment/Topics/Career-Tech/Career-Connections/School-Counselors/School-Counselor-Standards-and-Evaluation/SchoolCounselorStandards.pdf.aspx\nOur Lady of the Lake University. nd. A look at ethical issues in school counseling. Ollusa. Retrieved from: http://onlineprograms.ollusa.edu/med-school-counseling/resources/ethical-issues-in-school-counseling\nOur Lady of the Lake University. nd. Comparing the roles of different types of school counselors. Ollusa. Retrieved from: http://onlineprograms.ollusa.edu/med-school-counseling/resources/types-of-school-counselors\nSouth Carolina Guidance and Counseling Writing Team. 2008. The South Carolina Comprehensive Developmental Guidance and Counseling Program Model. Retrieved from: https://ed.sc.gov/scdoe/assets/File/agency/ccr/Career-and-Technology-Education/documents/Ann4SCCDGCPM062308Finalpostedaug2011.pdf"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:225931af-8bde-4566-87fd-ead2724ab99c>","<urn:uuid:e6987cfe-b0c0-459b-b2bc-230d610139f8>"],"error":null}
{"question":"As a math teacher, I'm curious how the teaching of rational numbers evolves from grade 5 to grade 7. What are the key differences in how fractions and decimals are approached?","answer":"In Grade 5, students focus on basic operations with fractions and decimals, developing fluency with addition and subtraction of fractions with unlike denominators, and working with decimals to hundredths. They learn limited cases of fraction division (unit fractions divided by whole numbers and whole numbers divided by unit fractions). In Grade 7, the approach becomes more comprehensive - students develop a unified understanding of rational numbers, recognizing fractions, decimals, and percents as different representations of the same concept. They extend all four operations to all rational numbers, including negative numbers, and learn to convert between forms. Grade 7 students also work with more complex concepts like repeating decimals and complex fractions.","context":["In Grade 5, instructional time should focus on three critical areas: (1) developing fluency with addition and subtraction of fractions, and developing understanding of the multiplication of fractions and of division of fractions in limited cases (unit fractions divided by whole numbers and whole numbers divided by unit fractions); (2) extending division to 2-digit divisors, integrating decimal fractions into the place value system and developing understanding of operations with decimals to hundredths, and developing fluency with whole number and decimal operations; and (3) developing understanding of volume.\n- Students apply their understanding of fractions and fraction models to represent the addition and subtraction of fractions with unlike denominators as equivalent calculations with like denominators. They develop fluency in calculating sums and differences of fractions, and make reasonable estimates of them. Students also use the meaning of fractions, of multiplication and division, and the relationship between multiplication and division to understand and explain why the procedures for multiplying and dividing fractions make sense. (Note: this is limited to the case of dividing unit fractions by whole numbers and whole numbers by unit fractions.)\n- Students develop understanding of why division procedures work based on the meaning of base-ten numerals and properties of operations. They finalize fluency with multi-digit addition, subtraction, multiplication, and division. They apply their understandings of models for decimals, decimal notation, and properties of operations to add and subtract decimals to hundredths. They develop fluency in these computations, and make reasonable estimates of their results. Students use the relationship between decimals and fractions, as well as the relationship between finite decimals and whole numbers (i.e., a finite decimal multiplied by an appropriate power of 10 is a whole number), to understand and explain why the procedures for multiplying and dividing finite decimals make sense. They compute products and quotients of decimals to hundredths efficiently and accurately.\n- Students recognize volume as an attribute of three-dimensional space. They understand that volume can be measured by finding the total number of same-size units of volume required to fill the space without gaps or overlaps. They understand that a 1-unit by 1-unit by 1-unit cube is the standard unit for measuring volume. They select appropriate units, strategies, and tools for solving problems that involve estimating and measuring volume. They decompose three-dimensional shapes and find volumes of right rectangular prisms by viewing them as decomposed into layers of arrays of cubes. They measure necessary attributes of shapes in order to determine volumes to solve real world and mathematical problems.\nGrade 5 Overview\nOperations and Algebraic Thinking\n- Write and interpret numerical expressions.\n- Analyze patterns and relationships.\nNumber and Operations in Base Ten\n- Understand the place value system.\n- Perform operations with multi-digit whole numbers and with decimals to hundredths.\nNumber and Operations—Fractions\n- Use equivalent fractions as a strategy to add and subtract fractions.\n- Apply and extend previous understandings of multiplication and division to multiply and divide fractions.\nMeasurement and Data\n- Convert like measurement units within a given measurement system.\n- Represent and interpret data.\n- Geometric measurement: understand concepts of volume and relate volume to multiplication and to addition.\n- Graph points on the coordinate plane to solve real-world and mathematical problems.\n- Classify two-dimensional figures into categories based on their properties.\n- Make sense of problems and persevere in solving them.\n- Reason abstractly and quantitatively.\n- Construct viable arguments and critique the reasoning of others.\n- Model with mathematics.\n- Use appropriate tools strategically.\n- Attend to precision.\n- Look for and make use of structure.\n- Look for and express regularity in repeated reasoning.","Mathematics Grade 7\n(1) Students extend their understanding of ratios and develop understanding of proportionality to solve single- and multi-step problems. Students use their understanding of ratios and proportionality to solve a wide variety of percent problems, including those involving discounts, interest, taxes, tips, and percent increase or decrease. Students solve problems about scale drawings by relating corresponding lengths between the objects or by using the fact that relationships of lengths within an object are preserved in similar objects. Students graph proportional relationships and understand the unit rate informally as a measure of the steepness of the related line, called the slope. They distinguish proportional relationships from other relationships.\n(2) Students develop a unified understanding of number, recognizing fractions, decimals (that have a finite or a repeating decimal representation), and percents as different representations of rational numbers. Students extend addition, subtraction, multiplication, and division to all rational numbers, maintaining the properties of operations and the relationships between addition and subtraction, and multiplication and division. By applying these properties, and by viewing negative numbers in terms of everyday contexts (e.g., amounts owed or temperatures below zero), students explain and interpret the rules for adding, subtracting, multiplying, and dividing with negative numbers. They use the arithmetic of rational numbers as they formulate expressions and equations in one variable and use these equations to solve problems.\n(3) Students continue their work with area from Grade 6, solving problems involving the area and circumference of a circle and surface area of three-dimensional objects. In preparation for work on congruence and similarity in Grade 8 they reason about relationships among two-dimensional figures using scale drawings and informal geometric constructions, and they gain familiarity with the relationships between angles formed by intersecting lines. Students work with three-dimensional figures, relating them to two-dimensional figures by examining cross-sections. They solve real-world and mathematical problems involving area, surface area, and volume of two- and three-dimensional objects composed of triangles, quadrilaterals, polygons, cubes and right prisms.\n(4) Students build on their previous work with single data distributions to compare two data distributions and address questions about differences between populations. They begin informal work with random sampling to generate data sets and learn about the importance of representative samples for drawing inferences.\nCore Standards of the Course\nStrand: MATHEMATICAL PRACTICES (7.MP)\nThe Standards for Mathematical Practice in Seventh Grade describe mathematical habits of mind that teachers should seek to develop in their students. Students become mathematically proficient in engaging with mathematical content and concepts as they learn, experience, and apply these skills and attitudes (Standards 7.MP.1–8).\nMake sense of problems and persevere in solving them. Explain the meaning of a problem and look for entry points to its solution. Analyze givens, constraints, relationships, and goals. Make conjectures about the form and meaning of the solution, plan a solution pathway, and continually monitor progress asking, \"Does this make sense?\" Consider analogous problems, make connections between multiple representations, identify the correspondence between different approaches, look for trends, and transform algebraic expressions to highlight meaningful mathematics. Check answers to problems using a different method.\nReason abstractly and quantitatively. Make sense of the quantities and their relationships in problem situations. Translate between context and algebraic representations by contextualizing and decontextualizing quantitative relationships. This includes the ability to decontextualize a given situation, representing it algebraically and manipulating symbols fluently as well as the ability to contextualize algebraic representations to make sense of the problem.\nConstruct viable arguments and critique the reasoning of others. Understand and use stated assumptions, definitions, and previously established results in constructing arguments. Make conjectures and build a logical progression of statements to explore the truth of their conjectures. Justify conclusions and communicate them to others. Respond to the arguments of others by listening, asking clarifying questions, and critiquing the reasoning of others.\nModel with mathematics. Apply mathematics to solve problems arising in everyday life, society, and the workplace. Make assumptions and approximations, identifying important quantities to construct a mathematical model. Routinely interpret mathematical results in the context of the situation and reflect on whether the results make sense, possibly improving the model if it has not served its purpose.\nUse appropriate tools strategically. Consider the available tools and be sufficiently familiar with them to make sound decisions about when each tool might be helpful, recognizing both the insight to be gained as well as the limitations. Identify relevant external mathematical resources and use them to pose or solve problems. Use tools to explore and deepen their understanding of concepts.\nAttend to precision. Communicate precisely to others. Use explicit definitions in discussion with others and in their own reasoning. They state the meaning of the symbols they choose. Specify units of measure and label axes to clarify the correspondence with quantities in a problem. Calculate accurately and efficiently, express numerical answers with a degree of precision appropriate for the problem context.\nLook for and make use of structure. Look closely at mathematical relationships to identify the underlying structure by recognizing a simple structure within a more complicated structure. See complicated things, such as some algebraic expressions, as single objects or as being composed of several objects. For example, see 5 – 3(x – y)2 as 5 minus a positive number times a square and use that to realize that its value cannot be more than 5 for any real numbers x and y.\nLook for and express regularity in repeated reasoning. Notice if reasoning is repeated, and look for both generalizations and shortcuts. Evaluate the reasonableness of intermediate results by maintaining oversight of the process while attending to the details.\nCompute unit rates associated with ratios of fractions, including ratios of lengths, areas and other quantities measured in like or different units. For example, if a person walks 1/2 mile in each 1/4 hour, compute the unit rate as the complex fraction 1/2/1/4 miles per hour, equivalently 2 miles per hour.\n- Decide whether two quantities are in a proportional relationship, e.g., by testing for equivalent ratios in a table or graphing on a coordinate plane and observing whether the graph is a straight line through the origin.\n- Identify the constant of proportionality (unit rate) in tables, graphs, equations, diagrams, and verbal descriptions of proportional relationships.\n- Represent proportional relationships by equations. For example, if total cost t is proportional to the number n of items purchased at a constant price p, the relationship between the total cost and the number of items can be expressed as t = pn.\n- Explain what a point (x, y) on the graph of a proportional relationship means in terms of the situation, with special attention to the points (0, 0) and (1, r) where r is the unit rate.\nUse proportional relationships to solve multistep ratio and percent problems. Examples: simple interest, tax, markups and markdowns, gratuities and commissions, fees, percent increase and decrease, percent error.\nApply and extend previous understandings of addition and subtraction to add and subtract rational numbers; represent addition and subtraction on a horizontal or vertical number line diagram.\n- Describe situations in which opposite quantities combine to make 0. For example, a hydrogen atom has 0 charge because its two constituents are oppositely charged.\n- Understand p + q as the number located a distance |q | from p, in the positive or negative direction depending on whether q is positive or negative. Show that a number and its opposite have a sum of 0 (are additive inverses). Interpret sums of rational numbers by describing real-world contexts.\n- Understand subtraction of rational numbers as adding the additive inverse, p – q = p + (–q). Show that the distance between two rational numbers on the number line is the absolute value of their difference, and apply this principle in real-world contexts.\n- Apply properties of operations as strategies to add and subtract rational numbers.\n- Understand that multiplication is extended from fractions to rational numbers by requiring that operations continue to satisfy the properties of operations, particularly the distributive property, leading to products such as (–1)(–1) = 1 and the rules for multiplying signed numbers. Interpret products of rational numbers by describing real-world contexts.\n- Understand that integers can be divided, provided that the divisor is not zero, and every quotient of integers (with non-zero divisor) is a rational number. If p and q are integers, then –(p/q) = (–p)/q = p/(–q). Interpret quotients of rational numbers by describing real-world contexts.\n- Apply properties of operations as strategies to multiply and divide rational numbers.\n- Convert a rational number to a decimal using long division; know that the decimal form of a rational number terminates in 0s or eventually repeats.\nSolve real-world and mathematical problems involving the four operations with rational numbers. Computations with rational numbers extend the rules for manipulating fractions to complex fractions.\nStrand: EXPRESSIONS AND EQUATIONS (7.EE)\nUse properties of operations to generate equivalent expressions (Standards 7.EE.1–2). Solve real-life and mathematical problems using numerical and algebraic expressions and equations (Standards 7.EE.3–4).\nUnderstand that rewriting an expression in different forms in a problem context can shed light on the problem and how the quantities in it are related. For example, a + 0.05a = 1.05a means that “increase by 5%” is the same as “multiply by 1.05.”\nSolve multi-step real-life and mathematical problems posed with positive and negative rational numbers in any form (whole numbers, fractions, and decimals), using tools strategically. Apply properties of operations to calculate with numbers in any form; convert between forms as appropriate; and assess the reasonableness of answers using mental computation and estimation strategies. For example: If a woman making $25 an hour gets a 10% raise, she will make an additional 1/10 of her salary an hour, or $2.50, for a new salary of $27.50. If you want to place a towel bar 9 3/4 inches long in the center of a door that is 27 1/2 inches wide, you will need to place the bar about 9 inches from each edge; this estimate can be used as a check on the exact computation.\n- Solve word problems leading to equations of the form px + q = r and p(x + q) = r, where p, q, and r are specific rational numbers. Solve equations of these forms fluently. Compare an algebraic solution to an arithmetic solution, identifying the sequence of the operations used in each approach. For example, the perimeter of a rectangle is 54 cm. Its length is 6 cm. What is its width?\n- Solve word problems leading to inequalities of the form px + q > r or px + q < r, where p, q, and r are specific rational numbers. Graph the solution set of the inequality and interpret it in the context of the problem. For example: As a salesperson, you are paid $50 per week plus $3 per sale. This week you want your pay to be at least $100. Write an inequality for the number of sales you need to make, and describe the solutions.\nStrand: GEOMETRY (7.G)\nDraw, construct, and describe geometrical figures, and describe the relationships between them (Standards 7.G.1–3). Solve real-life and mathematical problems involving angle measure, area, surface area, and volume (Standards 7.G.4–6).\nDraw (freehand, with ruler and protractor, and with technology) geometric shapes with given conditions. Focus on constructing triangles from three measures of angles or sides, noticing when the conditions determine a unique triangle, more than one triangle, or no triangle.\nKnow the formulas for the area and circumference of a circle and use them to solve problems; give an informal derivation of the relationship between the circumference and area of a circle.\nSolve real-world and mathematical problems involving area, volume and surface area of two- and three-dimensional objects composed of triangles, quadrilaterals, polygons, cubes, and right prisms.\nStrand: STATISTICS AND PROBABILITY (7.SP)\nUse random sampling to draw inferences about a population (Standards 7.SP.1–2). Draw informal comparative inferences about two populations (Standards 7.SP.3–4). Investigate chance processes and develop, use, and evaluate probability models (Standards 7.SP.5–8).\nUnderstand that statistics can be used to gain information about a population by examining a sample of the population; generalizations about a population from a sample are valid only if the sample is representative of that population. Understand that random sampling is more likely to produce representative samples and support valid inferences.\nUse data from a random sample to draw inferences about a population with an unknown characteristic of interest. Generate multiple samples (or simulated samples) of the same size to gauge the variation in estimates or predictions. For example, estimate the mean word length in a book by randomly sampling words from the book; predict the winner of a school election based on randomly sampled survey data. Gauge how far off the estimate or prediction might be.\nInformally assess the degree of visual overlap of two numerical data distributions with similar variabilities, measuring the difference between the centers by expressing it as a multiple of a measure of variability. For example, the mean height of players on the basketball team is 10 cm greater than the mean height of players on the soccer team, approximately twice the variability (mean absolute deviation) on either team; on a dot plot, the separation between the two distributions of heights is noticeable.\nUse measures of center and measures of variability for numerical data from random samples to draw informal comparative inferences about two populations. For example, decide whether the words in a chapter of a seventh-grade science book are generally longer than the words in a chapter of a fourth-grade science book.\nUnderstand that the probability of a chance event is a number between 0 and 1 that expresses the likelihood of the event occurring. Larger numbers indicate greater likelihood. A probability near 0 indicates an unlikely event, a probability around 1/2 indicates an event that is neither unlikely nor likely, and a probability near 1 indicates a likely event.\nApproximate the probability of a chance event by collecting data on the chance process that produces it and observing its long-run relative frequency, and predict the approximate relative frequency given the probability. For example, when rolling a number cube 600 times, predict that a 3 or 6 would be rolled roughly 200 times, but probably not exactly 200 times.\nDevelop a probability model and use it to find probabilities of events. Compare probabilities from a model to observed frequencies; if the agreement is not good, explain possible sources of the discrepancy.\n- Develop a uniform probability model by assigning equal probability to all outcomes, and use the model to determine probabilities of events. For example, if a student is selected at random from a class, find the probability that Jane will be selected and the probability that a girl will be selected.\n- Develop a probability model (which may not be uniform) by observing frequencies in data generated from a chance process. For example, find the approximate probability that a spinning penny will land heads up or that a tossed paper cup will land open-end down. Do the outcomes for the spinning penny appear to be equally likely based on the observed frequencies?\n- Understand that, just as with simple events, the probability of a compound event is the fraction of outcomes in the sample space for which the compound event occurs.\n- Represent sample spaces for compound events using methods such as organized lists, tables and tree diagrams. For an event described in everyday language (e.g., “rolling double sixes”), identify the outcomes in the sample space which compose the event.\n- Design and use a simulation to generate frequencies for compound events. For example, use random digits as a simulation tool to approximate the answer to the question: If 40% of donors have type A blood, what is the probability that it will take at least 4 donors to find one with type A blood?\nhttp://www.uen.org - in partnership with Utah State Board of Education (USBE) and Utah System of Higher Education (USHE). Send questions or comments to USBE Specialist - Lindsey Henderson and see the Mathematics - Secondary website. For general questions about Utah's Core Standards contact the Director - Jennifer Throndsen. These materials have been produced by and for the teachers of the State of Utah. Copies of these materials may be freely reproduced for teacher and classroom use. When distributing these materials, credit should be given to Utah State Board of Education. These materials may not be published, in whole or part, or in any other format, without the written permission of the Utah State Board of Education, 250 East 500 South, PO Box 144200, Salt Lake City, Utah 84114-4200."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:fc753e7b-1c0a-49d5-a5f1-1914066dd997>","<urn:uuid:10b27653-c303-47f9-acfd-5fbef73d63f2>"],"error":null}
{"question":"As a trade analyst, I need to understand the intersection between African trade policies and international transport corridors. How do the objectives of the African Union's Boosting Intra-African Trade initiative compare with the Ashgabat Agreement's trade facilitation goals?","answer":"Both initiatives share similar objectives but operate in different regions. The Boosting Intra-African Trade (BIAT) initiative focuses on enhancing trade within Africa through seven clusters: Trade Facilitation, Trade Policy, Productive capacities, Trade related Infrastructure, Trade Finance, Trade Information and Factor Market integration. It aims to boost trade volume between African countries and deepen regional market integration. The Ashgabat Agreement, on the other hand, establishes a multimodal transportation corridor linking the Persian Gulf and Central Asia, aiming to create the quickest commercial route between Central Asian countries and Iranian/Omani ports. While BIAT focuses on comprehensive trade development within Africa, the Ashgabat Agreement specifically emphasizes transportation infrastructure and transit procedures to facilitate trade between Central Asia and the Persian Gulf region.","context":["African Union Agenda 2063\nAgenda 2063 is a strategic framework for the socio-economic transformation of the continent within a 50 year period, from 2013 to 2063. It is Africa’s blueprint and master plan for transforming Africa into the global powerhouse of the future. The genesis of Agenda 2063 was the realisation by African leaders that there was a need to refocus and reprioritise Africa’s agenda from the struggle against apartheid and the attainment of political independence for the continent which had been the focus of The Organisation of African Unity (OAU), the precursor of the African Union; and instead to prioritise inclusive social and economic development, continental and regional integration, democratic governance and peace and security amongst other issues aimed at repositioning Africa to becoming a dominant player in the global arena. Visit the AU website to find out more.\nThe 50th Anniversary Solemn Declaration signed in May 2013 marked the re-dedication of Africa towards the attainment of the Pan African Vision of An integrated, prosperous and peaceful Africa, driven by its own citizens, representing a dynamic force in the international arena and Agenda 2063 is the concrete manifestation of how the continent intends to achieve this vision. The Africa of the future was captured in a letter presented by the former Chairperson of the African Union Commission, Dr. Nkosazana Dlamini-Zuma.\nAgenda 2063 builds on, and seeks to accelerate the implementation of, past and existing continental initiatives for growth and sustainable development, including the Lagos Plan of Action, The Abuja Treaty, the Minimum Integration Programme, PIDA, CAADP, the New partnership for Africa’s Development (NEPAD), Regional Plans and Programmes and National Plans. Agenda 2063 encapsulates not only Africa’s Aspirations for the Future but also identifies key Flagship Programmes which can boost Africa’s economic growth and development and lead to the rapid transformation of the continent.\n- Agenda 2063: Popular version - Final edition, April 2015\n- Framework Document - September 2015\n- Background Note - 2015\n- Popular version - Second edition, August 2014\n- Agenda 2063 Progress Report - November 2019\nThe First Ten-Year Implementation Plan for Agenda 2063 builds upon the Agenda 2063 Framework Document adopted in January 2015, and seeks to accelerate Africa’s political, social, economic and technological transformation while continuing the Pan African drive for self-determination, freedom, progress and collective prosperity. The First Continental Report on the Implementation of Agenda 2063 is a consolidation and evidence-based assessment of country and regional-level progress reports on Agenda 2063, complemented with interventions and results achieved at the regional and continental level.\n- First Continental Report on the Implementation of Agenda 2063, February 2020\n- Agenda 2063 First Ten-Year Implementation Plan (FTYIP) 2014-2023 (2.04 MB)\n- FTYIP: Core indicators Profile Handbook for Member States, March 2017 (1.64 MB)\nExpert Meeting on Agenda 2063 Financing, Domestic Resource Mobilsation and Partnership Strategy: Meeting report and Summary of recommendations, December 2016 (1.00 MB)\nAgenda 2063 places great import in the role that trade plays in developing economies and recognises that trade is a powerful engine for economic growth and development. Within this framework, the AU has developed continental frameworks and flagship projects aimed at adding value to Africa’s commodities, reducing and removing barriers to intra-African trade and creating a larger market for African goods and services amongst its population of over one billion people.\nSeveral continental frameworks have been developed to address the development of key sectors such as Agriculture, trade, transport, energy and mining. These sectors are seen as key in enabling Member States of the Union to achieve their development goals. To ensure coherence and convergence, these frameworks have been captured in the priority areas of the First Ten Year Implementation Plan.\nThe continental frameworks include the following:\nBoosting Intra-African Trade (BIAT)\nBoosting the volume of trade that African countries undertake among themselves and deepening regional market integration constitute a necessary response to the challenges facing Africa in the multilateral trading system and the global economy. In addition, fostering competition among African countries will assist in enhancing their capacity and prepare them to compete more effectively on the global market. To effectively achieve this, the BIAT Action Plan is divided into seven clusters: Trade Facilitation, Trade Policy, Productive capacities, Trade related Infrastructure, Trade Finance, Trade Information and Factor Market integration.\n- Decision on Boosting Intra-African Trade and Fast Tracking the Continental Free Trade Area (34 KB)\n- Declaration on Boosting Intra-African Trade and The Establishment of a Continental Free Trade Area (CFTA) (91 KB)\n- BIAT: Issues Affecting Intra-African Trade, Proposed Action Plan for boosting intra-African Trade and Framework for the fast-tracking of a AfCFTA\nSynthesis Paper on Boosting Intra-African Trade and Fast Tracking the Continental Free Trade Area\nAccelerated Industrial Development for Africa (AIDA)\nThe leaders of Africa have in recent years shown commitment to the industrialization of the continent in both the short and long-term, and have taken a number of major initiatives to meet the challenges of development. During the January 2008 AU Summit on “the industrialization of Africa”, the Heads of State and Government endorsed and adopted the Plan of Action for the accelerated industrial development for Africa and directed the Commission of the African Union to speedily operationalize it in collaboration with development partners.\nAction Plan for the Accelerated Industrial Development of Africa, September 2007\nProgramme for Infrastructure Development in Africa (PIDA)\nPIDA is a multi-sector programme covering Transport, Energy, Transboundary water and ICT dedicated to facilitating continental integration in Africa through improved regional infrastructure and is designed to support implementation of the Abuja Treaty and the creation of the African economic Community. PIDA is a joint initiative of the African Union Commission, the NEPAD Planning and Coordination Agency (NPCA), and the African Development Bank (AfDB) launched in January 2012. PIDA promotes regional economic integration by building mutually beneficial infrastructure and strengthening the ability of countries to trade and establish regional value chains for increased competitiveness.\n- Presidential Infrastructure Champion Initiative Report - February 2020\n- Infrastructure Outlook 2040\n- AU-NEPAD Continental Business Network 5% Agenda Report 2017\n- NEPAD-CBN Report on De-Risking Infrastructure and PIDA Projects in Africa - June 2016\n- Financing Africa’s Infrastructure Development: Leveraging Public-Private Partnerships for Regional Infrastructure Transformation - NEPAD Agency, 2014\n- Programme for Infrastructure Development in Africa (PIDA) - 2012\nPIDA Executive Summary, November 2011\nThe PIDA Priority Action Plan (PIDA-PAP), which extends to 2020, comprises 51 programmes and projects divided into 433 projects covering transport, energy, ICT and trans-boundary water sectors. PIDA will allow countries to meet forecast demand for infrastructure services and boost competitiveness by: (i) Increasing efficiencies; (ii) Accelerating growth; (iii) Facilitating integration in the world economy; (iv) Improving living standards and; (v) Unleashing intra-African trade.\n- PIDA Progress Report 2019\n- 2018 PIDA Progress Report: Summary Update\n- PIDA Priority Action Plan Progress Report, February 2017\n- PIDA Progress Report 2017\nPIDA Progress Report 2016\nComprehensive African Agricultural Development Programme (CAADP)\nCAADP is Africa’s policy framework for agricultural transformation, wealth creation, food security and nutrition, economic growth and prosperity for all. In Maputo, Mozambique in July 2003, the AU Summit made the first declaration on CAADP as an integral part of NEPAD. CAADP aims to stimulate and facilitate increased agricultural performance through improvements in policy and institutional environments, access to improved technologies and information, and increased investment financing.\n- Revised Follow up Mechanism for Plan of Action for the Promotion of Employment and Poverty Alleviation, September 2004\n- Comprehensive Africa Agriculture Development Programme, July 2003\n- Introducing the Comprehensive Africa Agricultural Development Programme\n- Country CAADP Implementation Guidelines under the Malabo Declaration, 2016\n- Implementation Strategy and Roadmap to Achieve the 2025 Vision on CAADP\n- Declaration on Renewed Partnership for a Unified Approach to End Hunger in Africa by 2025 under the CAADP Framework, July 2013\nMutual Accountability Framework for the CAADP, 2011\nAfrica Mining Vision (AMV)\nThe Africa Mining Vision, adopted by Heads of State at the February 2009 AU Summit, defines an aspiration for mining to catalyse broad, transparent, equitable and sustainable growth and socio-economic development. It is Africa’s own response to tackling the paradox of great mineral wealth existing side by side with pervasive poverty through the integration of mining into development policies at local, national and regional levels. If the AMV is to effectively support the contribution of mining to growth and development, it needs to be implemented at the country level. And at regional level, it means integrating mining into industrial and trade policy.\n- Africa Mining Vision, February 2009\n- Addis Ababa Declaration on Building a Sustainable Future for Africa’s Extractive Industry – From Vision to Action, December 2011\n- The Country Mining Vision (CMV), October 2013\nThe Agenda 2063 flagship initiative the African Continental Free Trade Area (AfCFTA) refers to a continental geographic zone where goods and services move among member states of the AU with no restrictions. The AfCFTA aims to boost intra-African trade by providing a comprehensive and mutually beneficial trade agreement among the member states, covering trade in goods and services, investment, intellectual property rights and competition policy. Read more...\nAfrican Commodities Strategy\nTo ensure better management of its commodities and natural resources for the benefit of Africa’s people and to drive economic growth, the Agenda 2063 continental Commodities Strategy aims to identify, formulate and drive the implementation of policies and programmes that will enable African countries to add value, extract higher rents from their commodities, integrate into global value chains and promote vertical and horizontal diversification anchored in value addition and local content development. This Strategy aims at fulfilling the aforementioned objectives by reviewing the state of play for high-priority commodities sectors in Africa, namely agriculture, mining, and energy. It considers current trends and outlooks, as well as sector-specific opportunities and challenges.\nThe strategy envisions a commodity-led industrialisation which focuses on developing Africa’s commodities as a driver for achieving the structural, social and economic transformation of the continent. Commodity based industrialisation can serve as a launching pad for long term diversification in new non commodity sectors, if well supported by robust industrial policies.\nA draft strategy was developed in 2016, which focuses on energy, agriculture and crosscutting strategies dealing with mining and industrialisation. The updated draft African Union Commodity Strategy was presented to the 2nd Meeting of the STC on Trade Industry and Mining in January 2019 and will be presented to an extraordinary Specialised Technical Committee (STC) of Ministers of Trade, Industry and Minerals for adoption.\nA brief on the African Union Commodities Strategy and Industrialisation - March 2017\nAfrican Passport and Free movement\nThe objective of the African Passport is to remove restrictions on Africans ability to travel, work and live within their own continent. The initiative aims at transforming Africa’s laws, which remain generally restrictive on movement of people despite political commitments to bring down borders with the view to promoting the issuance of visas by Member States to enhance free movement of all African citizens in all African countries.\n- Migration Policy Framework for Africa - June 2006\n- The Migration Policy Framework for Africa (MPFA) 2018-2020\n- Revised Migration Policy Framework for Africa and Plan of Action 2018-2027\n- Final Report of the 4th Pan African Forum on Migration - November 2018\n- Decision on Free Movement of Persons and the African Passport, July 2016 (293 KB)\n- Protocol to the Abuja Treaty relating to the Free Movement of Persons, Right of Residence and Right of Establishment, adopted 29 January 2018 (3.80 MB)\n- Protocol to the Abuja Treaty relating to the Free Movement of Persons, Right of Residence and Right of Establishment (French) (4.26 MB)\nStatus of signature and ratification as at 16 July 2019 (12 KB)\nThe SAATM aims to ensure intra-regional connectivity between the capital cities of Africa and create a single unified air transport market in Africa, as an impetus to the continent’s economic integration and growth agenda. SAATM provides for the full liberalisation of intra-African air transport services in terms of market access, traffic rights for scheduled and freight air services by eligible airlines thereby improving air services connectivity and air carrier efficiencies. It removes restrictions on ownership and provides for the full liberalisation of frequencies, tariffs and capacity. It also provides eligibility criteria for African community carriers, safety and security standards, mechanisms for fair competition and dispute settlement as well as consumer protection.\n- Launching of the Single African Air Transport Market: Concept note, January 2018 (330 KB)\n- The Single African Air Transport Market: An Agenda 2063 Flagship Project (1.20 MB)\n- The Single African Air Transport Market: PIDA Brochure, 2017 (1.95 MB)\n- Establishment of SAATM: Updated Activity Roadmap 2016-2017 (319 KB)\n- Meeting of Experts of the Ministerial Working Group on Establishment of a SAATM: Concept note, October 2017 (313 KB)\n- Report of the Second Ministerial Working Group on SAATM, October 2016 (624 KB)\n- Ministerial Working Group on Implementation of the Yamoussoukro Decision & Establishment of SAATM: Final Communiqué, October 2016 (56 KB)\n- Ministerial Conference on Aviation Security and Facilitation in Africa: Report, April 2016 (784 KB)\nScience, Technology and Innovation Strategy for Africa 2024\nStrengthening capacity in science, technology and innovation for sustainable development, and harnessing the outcome discoveries, can only be achieved within a comprehensive framework of science, technology and innovation, where we respond at the national, regional and continental levels, with policies, programmes, institutions and partnerships, which maximize economic opportunities and societal benefits. In this regard, the African Heads of State and Government in June 2014 adopted the Science, Technology and Innovation Strategy for Africa 2024 (STISA-2024) as the continental framework for accelerating Africa's transition to an innovation-led, knowledge-based economy within the overall framework of the AU Agenda 2063.\nSTISA-2024 outlines the key socio-economic priority areas that Africa has to collectively address through scientific research and development, and innovation. STISA-2024 is the first of ten-year strategies, that will be developed in the framework of the long-term journey of Agenda 2063 and deployed across various socio-economic and development sectors.\nScience, Technology and Innovation Strategy for Africa 2024\n- Five Year Science, Technology and Innovation Plan of Action (2019-2024)\nThe decision to adopt Cyber Security as a flagship programme of Agenda 2063 is a clear indication that Africa needs to not only incorporate in its development plans the rapid changes brought about by emerging technologies, but also to ensure that these technologies are used for the benefit of African individuals, institutions or nation states by ensuring data protection and safety online.\nAU Convention on Cyber Security and Personal Data Protection, June 2014\n- Status of signature and ratification, June 2016\nThis aims to put in place policies and strategies that will lead to transformative e-applications and services in Africa; especially the intra-African broad band terrestrial infrastructure; and cyber security, making the information revolution the basis for service delivery in the bio and nanotechnology industries and ultimately transform Africa into an e-Society.\nAfrican Financial Institutions\nThe creation of African Continental Financial Institutions aims at accelerating integration and socio-economic development of the continent through the establishment of organisations which will play a pivotal role in the mobilization of resources and management of the African financial sector. The financial institutions envisaged to promote economic integration are the African Investment Bank and Pan African Stock Exchange; the African Monetary Fund and the African Central Bank.\n- Protocol on the Establishment of the African Monetary Fund - 27 June 2014\n- Protocol on the African Investment Bank - 30 June 2009\n- Comments of the RECs on the timelines for the Establishment of the African Central Bank and AMCP Macroeconomic Convergence Criteria: Report - January 2020","The Ashgabat Agreement was signed in 2011, establishing an international transit and transport corridor between the countries involved. Its goals are to improve connectivity between these regions and facilitation of transit, trade, and cargo. India has recently become a signatory to the Ashgabat Agreement.\nIn this article, we will discuss the importance of the Ashgabat Agreement, as well as its key provisions.\n- What is Ashgabat Agreement?\n- Members of the Ashgabat Agreement\n- Objectives of the Ashgabat Agreement\n- Importance of this Agreement for India\n- Ashgabat Agreement Connectivity\n- International North-South Transit Corridor\n- Relevance of INSTC to India (Economic and Strategic Dimensions)\n- International North-South Transport Corridor (INSTC) – Role of India\n- Eurasian Railway Connectivity\nWhat is Ashgabat Agreement?\nThe Ashgabat Agreement can be defined as a multimodal transportation agreement that takes into account the creation of an international transport and transit corridor linking the Persian Gulf and Central Asia. Road, rail, and maritime transportation will all be included in the multimodal corridor.\nThis agreement aims to create the quickest commercial route possible between the countries of Central Asia and the Iranian and Omani ports.\nThe trade agreement specifies a transit route through Iran, Turkmenistan, Kazakhstan, and Uzbekistan on the way to Oman and the Persian Gulf.\nThis would connect parts of the Middle East with Central Asia.\n- It is a multimodal transport agreement involving the Persian Gulf and Central Asia.\n- The Ashgabat Agreement has four original signatories. They are Iran, Turkmenistan, Oman, and Uzbekistan.\n- The goal of this agreement is to establish the quickest trade path feasible between the nations of Central Asia and the ports of Iran and the Sultanate of Oman.\n- Kazakhstan became a signatory in February 2018, and this agreement is expected to continue to be revised as needed.\n- In addition to easing trade barriers, the Ashgabat Agreement will simplify procedures for the movement of goods between signatory nations. Ashgabat Agreement’s Contribution to the Development of Transportation:\n- A rail link connecting Turkmenistan, Afghanistan, and Tajikistan was built in 2013.\n- A transportation route connecting Azerbaijan, Turkmenistan, Turkey, Afghanistan, and Georgia was built in 2014.\n- The Iran-Turkmenistan-Kazakhstan (ITK) rail project was started.\nMembers of the Ashgabat Agreement\nThe four founding members of the agreement—Iran, Oman, Turkmenistan, Qatar, and Uzbekistan—signed the original agreement on April 25, 2011, with the intention of creating a trade and transportation corridor between them.\nLater, in 2013, Qatar retracted the contract. That same year, Kazakhstan filed a membership application, which was later approved in 2015.\nIn addition to the Lapis Lazuli corridor, Pakistan officially joined the Ashgabat Agreement in October 2016. Kazakhstan is also included in the agreement.\nIndia joined this agreement under the PM of India, Narendra Modi, on February 1, 2018, which calls for the construction of a global transportation and transit route or corridor connecting Central Asia with the Persian Gulf in order to considerably increase trade and investment.\nObjectives of the Ashgabat Agreement\n- The Ashgabat Agreement aims to create an integrated transport and trade corridor, thereby increasing trade and lowering barriers to travel.\n- It will also facilitate cargo transit, trade, and communication between the signatory countries. This will also help to synchronize the International North-South Transport Corridor.\n- The only goal of this contract is to create a corridor for trade and transport by creating the shortest possible commercial interaction between the Central Asian Republics or Central Asian Countries and the Persian Gulf\nImportance of this Agreement for India\n- In 2015, the Indian government announced the National Policy on Integrated Transport to strengthen connections on the International North-South Transport Corridor.\n- It will enable India to improve trade and commerce with the Eurasian region. This can be done by utilizing an existing transportation and transit corridor.\n- Without India’s participation in a Central Asian-led transport hub for north-south connectivity projects, it would not have been possible for India to establish the 610 km north-south railway link from Chabahar to Zahedan or operate a multi-purpose terminal at Chabahar.\n- The relationship between India and Turkmenistan is crucial to India’s ‘Connect Central Asia’ project.\n- India will have the option to divert cargo from the conventional marine channels to the transcontinental land transport element.\nAshgabat Agreement Connectivity\nThe Ashgabat Agreement is a significant step toward increased interconnection between nations, which will ultimately be advantageous to all participating nations.\nThe Ashgabat Agreement also provides for the free movement of people and goods across borders, allowing for increased trade and commerce. This is a win-win situation for the countries involved. This is particularly true for the nations that lack connectivity among themselves.\nIndia can expand its commercial and economic relations with these countries through Ashgabat connectivity options and thereby produce a positive influence. It would also be beneficial for regional and international security.\nThe Ashgabat Agreement is a landmark step toward making connectivity easier for Indian businesses.\nInternational North-South Transit Corridor\nThe Ashgabat Agreement will link the International North-South Transport Corridor (INSTC), allowing simple freight transportation between Russia, India, Europe, Iran, and Central Asia. This is accomplished by combining ship, rail, and road connections.\nThe main route comprises moving commodities mostly from Russia, India, Iran, and Azerbaijan by boat, train, and road.\nThe INSTC envisions goods being transported by sea from Mumbai (India) to Bandar\nAbbas (Iran), by road from Bandar Abbas to Bandar-e-Anzali (an Iranian port on the Caspian Sea), by ship from Bandar-e-Anzali to Astrakhan (a Caspian port in the Russian Federation), and by rail to areas of the Russian Federation and farther into Europe from Astrakhan.\nRelevance of INSTC to India (Economic and Strategic Dimensions)\n- To carry commodities to Russia, India must currently rely on the sea route via Rotterdam to St. Petersburg, as well as increasingly through China and then inland.\n- With the INSTC, shipping goods from India to Eurasia and the surrounding areas would be substantially less expensive.\n- According to certain assessments, the INSTC will reduce transit time and cost, allowing for the seamless movement of commodities from India to Russia and neighboring nations.\n- Estimates indicate that compared to the current route from St. Petersburg to Moscow, the corridor would be 30% less expensive and 40% shorter.\n- The region has access to potash. Other fertilizer inputs are also available.\n- In particular, these nations stand to gain significantly from India’s expertise in digital technologies and IT-enabled services.\n- Being one of the world’s largest consumers of energy, India’s energy needs could be satisfied by the abundance of natural resources in Central Asia, including uranium, natural gas, and petroleum.\n- In line with the Connect Central Asia Policy, the INSTC will deepen our ties with Central Asia.\n- Along with the INSTC’s beginning, the Belt and Road Initiative (BRI), a massive project involving 68 countries in Asia, Europe, and Africa, is reshaping international trade norms in China’s favor.\nInternational North-South Transport Corridor (INSTC) – Role of India\n- The enormous extent of India’s customs cooperation agreement with Iran needs to be expanded. This would enable transit to the Caspian Sea and other Central Asian nations via Iran.\n- The easing of sanctions on Iran has given investors several opportunities to complete the INSTC’s gaps. This was previously impractical.\n- The objective now should be to ensure that this development is completed as soon as possible. India has earlier pledged a $100 million investment for the building of a free trade zone in Chabahar, Iran.\n- There is a compelling political and economic argument for India to devote greater political resources to improving access to the INSTC and establishing a link with Iran.\n- India may create a forum for Central Asia similar to the one it has with Africa.\n- The Shanghai Cooperation Organization (SCO) has developed into a potent regional alliance in Eurasia. India should take the initiative to engage with it.\nEurasian Railway Connectivity\nThis agreement will make use of the Turkmenistan-Afghanistan-Tajikistan (TAT) rail line from 2013, the Afghanistan-Turkmenistan-Azerbaijan-Georgia-Turkey transportation corridor in 2014, the Iran-Turkmenistan-Kazakhstan railroad, as well as TRACECA (Transport Corridor Europe Caucasus Asia), which links the EU with 14 Eastern European, South Caucasian, and Central Asian nations.\nIndia finalized its plan to construct a 900-km railway connecting Afghanistan’s Hajigak area with Iran’s Chabahar Port at Shahid Beheshti terminal earlier in 2011. The agreement was first signed in 2015 between the three countries. By removing the necessity for both nations to pass via Pakistani territory in 2017, the Chabahar Port of Iran facilitated trade between India and Afghanistan.\nDuration and Termination of this Agreement\nThe Ashgabat Agreement will be in existence for ten years, after which it will be automatically extended. The agreement will, however, become invalid after a year if any participant sends the others a written notice of their intention to cancel it.\nEnhanced connectivity is essential for trade and transportation, and this agreement is a silver lining for all countries in the region. It has opened up the sea, air, and road route to facilitate trade, especially for nations that had been unable to access Eurasia directly.\nFAQs on Ashgabat Agreement\nWhat is the capital of Turkmenistan?\nAshgabat is the capital of Turkmenistan.\nWhich countries are members of the Ashgabat Agreement?\nIndia, Iran, Kazakhstan, Oman, Pakistan, Turkmenistan, and Uzbekistan are members of the Ashgabat Agreement.\nWhich nations comprise the founding member states of INSTC?\nThese are India, Iran, and Russia being the huge network of north-south connectivity projects."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:312d1635-2b74-49b3-b771-80db78a6e4e3>","<urn:uuid:4a5c8e4f-fc47-45cc-a9af-05b3f05e417d>"],"error":null}
{"question":"How do the early warning signs of schizophrenia compare with the symptoms of Generalized Anxiety Disorder in terms of cognitive and behavioral manifestations?","answer":"Schizophrenia and GAD show distinct cognitive and behavioral manifestations. Early signs of schizophrenia include deficits in working memory, impaired motor skills, peculiar use of words, inappropriate laughter, bizarre behavior, and conversation that seems deep but is not logical. In contrast, GAD manifests primarily through excessive and uncontrollable worry about everyday activities, accompanied by restlessness, difficulty concentrating, mind going blank, and sleep disturbances. While schizophrenia may involve social withdrawal and deterioration of relationships, GAD is characterized by persistent anxiety and tension about multiple events without psychotic symptoms.","context":["Disorders and Treatment\n- Mental Illness\n- Bipolar Disorder\n- Mood Disorders\n- Borderline Personality\n- Mental Health Diagnosis\n- Mental Health Treatments\n- Alternative Meds\n- Case Studies\nSigns of Mental Illness\nSigns of Mental Illness / Disorders\nDepressed or irritable mood most of the day-nearly every day.\nLoss of interest or pleasure in activities ( such as hobbies, work, sex, or being with friends ) most of the day-nearly every day.\nA sudden change in weight or appetite.\nInability to sleep or sleeping too much.\nAgitation or restlessness ( observed by others. )\nConstant fatigue or loss of energy.\nFrequent feelings of worthlessness or guilt.\nDifficulty concentrating or making decisions.\nFrequent thoughts of death or suicide.\nPersistent sad, anxious, or \"empty\" mood.\nFeelings of hopelessness, pessimism.\nFeelings of guilt, worthlessness, helplessness.\nLoss of interest or pleasure in hobbies and activities that were once enjoyed, including sex.\nDecreased energy, fatigue, being \"slowed down\".\nDifficulty concentrating, remembering, making decisions.\nInsomnia, early-morning awakening, or oversleeping.\nAppetite and/or weight loss or overeating and weight gain.\nThoughts of death or suicide; suicide attempts.\nPersistent physical symptoms that do not respond to treatment, such as headaches, digestive disorders, and chronic pain.\n- Abnormal or excessive elation\n- Unusual irritability\n- Decreased need for sleep\n- Grandiose notions\n- Increased talking\n- Racing thoughts\n- Increased sexual desire\n- Markedly increased energy\n- Poor judgment\n- Inappropriate social behavior\nRestlessness or feeling keyed up or on edge.\nDifficulty concentrating or mind going blank.\nDifficulty falling or staying asleep, or restless unsatisfying sleep.\n- Endless checking or rechecking actions.\n- A constant and unrealistic worry about everyday occurrences and activities.\n- Fear and anxiety that appear for no apparent reason.\n- Panic Disorder: a sudden, uncontrollable attack of terror that can manifest itself with heart palpitations, dizziness, shortness of breath, and an out-of-control or terribly frightening feeling;\n- Generalized Anxiety Disorder: excessive anxiety and worry that last for at least six months accompanied by other physical and behavioral problems;\n- Social Phobia: a persistent fear of one or more situations in which the person is exposed to possible scrutiny of others;\n- Obsessive-Compulsive Disorder: repeated, intrusive and unwanted thoughts that cause anxiety, often accompanied by ritualized behavior that relieve this anxiety;\n- Post-Traumatic Stress Disorder: caused when someone experiences a severely distressing or traumatic event. Recurring nightmares and/or flashbacks and unprovoked anger are common symptoms.\n- Excessive fatigue and sleepiness or an inability to sleep.\n- Social withdrawal and isolation.\n- Deterioration of social relationships.\n- Inability to concentrate or cope with minor problems.\n- Apparent indifference, even in highly important situations.\n- Dropping out of activities.\n- Decline in academic and athletic performance.\n- Deterioration of personal hygiene; eccentric dress.\n- Frequent moves or trips or long walks leading nowhere.\n- Drug or alcohol abuse.\n- Undue preoccupation with spiritual or religious matters.\n- Bizarre behavior.\n- Inappropriate laughter.\n- Strange posturing.\n- Low tolerance to irritation.\n- Excessive writing without apparent meaning.\n- Inability to express emotion.\n- Irrational statements.\n- Peculiar use of words or language structure.\n- Conversation that seems deep but is not logical or coherent\n- Staring; vagueness.\n- Unusual sensitivity to stimuli ( noise, light. )\nSigns of Schizophrenia in Childhood\n- Deficits in working (short-term) and verbal memory.\n- Impairments in gross motor skills (the child's ability to control different parts of the body).\n- Attention deficits.\n- Mixed-handedness (the use of different hands for different tasks), particularly in females.\n- Eye tracking dysfunction. This genetic trait is strongly associated with schizophrenia and may reflect abnormalities in the frontal regions of the brain. (Some experts believe that this is such a powerful marker in patients with close relatives with schizophrenia that it can be used as a predictor. This trait can only be detected by a health professional using special equipment.)\n- Hallucinations or delusions. (This does not include normal childhood fantasies and stories, in which the child is aware that they are inventions.)\n- A decline in verbal memory, IQ, and other mental functions.\nThe information provided on the PsyWeb.com is designed to support, not replace, the relationship that exists between a patient/site visitor and his/her health professional. This information is solely for informational and educational purposes. The publication of this information does not constitute the practice of medicine, and this information does not replace the advice of your physician or other health care provider. Neither the owners or employees of PsyWeb.com nor the author(s) of site content take responsibility for any possible consequences from any treatment, procedure, exercise, dietary modification, action or application of medication which results from reading this site. Always speak with your primary health care provider before engaging in any form of self treatment. Please see our Legal Statement for further information.","Generalized anxiety disorder (gad) is a type of anxiety disorder in which a person feels persistent, excessive and unrealistic worry about everyday occurrences there is no specific event or. Generalized anxiety disorder is the worry disease learn how to put chronic worry in its place. Try online counseling: if you are struggling, consider an online counseling session with a licensed professional at be. Generalized anxiety disorder (gad) is characterized by six months or more of chronic, exaggerated worry and tension that is unfounded or much more severe than the normal anxiety most people. Generalized anxiety disorder, or gad, is a type of anxiety disorder that causes excessive worry and anxiety about many different things some anxiety is normal, but with gad it is difficult. Generalized anxiety disorder screener (gad-7) over the last 2 weeks, how often have you been bothered by the following problems not at all several.\nOmhs internet page for information on generalized anxiety disorder, including information on screening. The hallmark of generalized anxiety disorder (gad)—the broadest type of anxiety—is worrying too much about everyday things, large and small. Generalized anxiety disorder (gad) is characterized by chronic, unrealistic, and exaggerated feelings of worry and tension. People with generalized anxiety disorder, or gad, worry about everyday things even when there's no apparent cause for concern, and live their daily lives in a state of dread. Generalized anxiety disorder (gad) is ongoing anxiety that can disrupt your day-to-day life with treatment, people with gad can get better.\nGeneralized anxiety disorder (gad) is a common condition characterized by constant worry and tension that persists for several months, even when there is little or no cause. What is generalized anxiety disorder (gad) mitchell is 17 and in his final year of high school and plans to attend university next year.\nGeneralized anxiety disorder 30002 (f411) reprinted with permission from the diagnostic and statistical manual of mental disorders, fifth edition. Key clinical pointsgeneralized anxiety disorder generalized anxiety disorder is characterized by persistent anxiety and uncontrollable worry that occurs consistently for at least 6 months.\nGeneralized anxiety disorder (gad) people with gad feel continually worried or anxious about a range of events or activities in their daily lives and have difficulty controlling or stopping.\nSynopsis generalized anxiety disorder f411 - icd10 description, world health organization anxiety that is generalized and persistent but not restricted to, or even strongly predominating. Generalized anxiety disorder is common among patients in primary care affected patients experience excessive chronic anxiety and worry about events and activities, such as their health. People who experience generalized anxiety disorder (gad) exhibit excessive anxiety and worry about multiple events or activities most days of the week. Generalized anxiety disorder: abbreviated gad a condition characterized by 6 months or more of chronic, exaggerated worry and tension that is unfounded or much more. Criteria for diagnosis of generalized anxiety disorder (gad) include: the experience of excessive anxiety and/or worry which occurs more days than not for at least 6 months the anxiety is. When does worry become generalized anxiety disorder see an overview of the dsm-5 symptoms and assessment for gad and how they are used for diagnosis.\nGeneralized anxiety disorder is more than the normal anxiety people experience day to day it's chronic worry and tension, even though nothing seems to provoke it. Generalized anxiety disorder causes people to feel constant, severe anxiety, often without a reason this emedtv web page offers more facts on the condition, including the symptoms, causes. Read our article and learn more on medlineplus: generalized anxiety disorder. Generalized anxiety disorder (gad) is characterized by persistent and excessive worry about a number of different things people with gad may anticipate disaster and may be overly concerned. You don't have to feel consumed by your generalized anxiety disorder learn about causes, medications, and therapies that will help you take back control. Webmd explains general anxiety disorder (gad), including causes, symptoms, diagnosis, treatment, and prevention. Generalized anxiety disorder (gad) is a psychiatric disorder characterized by a constant sense of worry and fear that interferes with daily life."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:be82377f-631b-4d5a-b170-d5525d1a7f50>","<urn:uuid:3aa91436-8330-45b3-a7d6-f89d419ee00e>"],"error":null}
{"question":"Looking at guitar specs, what's the key difference between fingerboard maintenance in Petillo's refretting process versus the Fender Pawn Shop Super-Sonic's fingerboard construction?","answer":"Petillo's refretting process involves extensive fingerboard preparation, including planing, scraping, and sanding to a new radius if necessary, while preserving inlays intact. The fingerboard is then buffed with a special Petillo Fingerboard Dressing to seal the wood from oils, perspiration, and moisture. The Fender Pawn Shop Super-Sonic, on the other hand, features a more basic rosewood fingerboard with clay-like position dots and side markers along the fingerboard join, with no mention of special maintenance processes or protective treatments.","context":["Petillo Precision Fret™\nU.S. Patent No. 4,064,779\nWhy a Petillo Refretting?\nTo answer this question,\nwe must first examine a conventional refretting. We ask\nsome very important questions. Remember that the heart\nof the instrument is the frets and the fingerboard, and\nif the positioning of the slots is not true to the fingerboard\nscale then the instrument will play out of tune.\n- Why should there be a refretting?\n- What method is used to remove the old frets?\n- Is the fingerboard damaged when frets are removed, including\n- Are the fingerboard fret slots damaged or have they\nbecome too wide to hold the frets into the fingerboard?\n- What process is done to remove twists, humps, or wear\nthroughout the fingerboard?\n- Are the inlays damaged, destroyed, removed, etc?\n- How are the frets finished off?\n- Is the action low without buzzes or dead spots?\n- Do the installers guarantee that your instrument will\nplay in perfect tune?\n- Will every note be in tune with each corresponding note\nanywhere on the fingerboard?\n- What experience do the people have who repair your instrument?\n- Are they competent?\n- Is the work done in a professional manner in a reasonable\nlength of time?\nThese are just a few questions you should ask before considering\na refretting on any of your instruments. In\nmy 39 years of making, repairing and altering stringed\ninstruments. I have seen thousands of instruments that\nwere altered so badly, without skill or consideration,\nthat they cost 3 to 4 times more than the original price\nto restore the instrument back into working condition.\nA conventional refretting is removing the old frets and\nhammering or pressing in new frets. The problem with this\nmethod is the frets are held in place by the friction\nof the wood in the fingerboard alone. Due to climatic\nchanges, improper storage, perspiration from playing that\ncorrodes the fret alloy making it weaker, cleaning oils\nthat soften the wood, and poor removal of the old frets\n(which widen the fingerboard slots affects the friction\njoin), many problems result: high frets, buzzes, dead\nspots, intonation difficulties, and the edges of the frets\nrising off the fingerboard (sometimes causing the high\n\"E\" string to get caught underneath the frets\nafter bending or strumming).\nThe Petillo Precision Fret™ (Fig. 1, left) allows\nthe strings to rest in the center of the fingerboard slot;\nin this position the strings are perfectly centered in\nthe middle of the fret, resulting in perfect intonation.\nThe conventional fret (Fig. 1, right) has a flat\ntop which causes the strings to rest in a wide flat area\nwith no accurate string placement, thus causing intonation\ndifficulties. The four categories below - Fret Repositioning,\nFingerboard Radius, Fret Alloy, and Micro polishing, describe\nthe preparation and process of a Petillo refretting.\nIn many cases the fret slots\nwhich hold the frets into the fingerboard are sawn in\nthe wrong places. Before sawing new slots we make sure\nthat the fingerboard slot positioning is located properly\nwithin the particular fingerboard scale. If repositioning\nis needed new slots are plugged with the same wood species\nof the fingerboard, then resawn to tolerance of .002 of\nan inch. Once properly resawn to the correct slot positioning\nof fingerboard scale then the Petillo frets are ready\nto be installed. A lot of intonation problems come not\nonly from the flattened-down tops of a conventional fret,\nbut also from inaccurate division of the fingerboard scale.\n||Petillo Precision Fret™\nSometimes if necessary, the radius of the fingerboard\nis altered to fit the players hand and style, or to remove\ntwists, humps, and wear in the fingerboard. The flatter\nthe radius of the fingerboard the lower the action can\nbe. To accomplish this the fingerboard is planed, scraped,\nand sanded to a new radius. The inlays will still be intact.\nChanging the fingerboard radius is optional. If the musician\ndesires to keep the original radius then the fingerboard\nis just clean sanded true. Many players find that by changing\nthe radius, they discover an optional lower action, and\ngreater ease in bending strings, playing chords, and overall\nMost frets used in manufacturing and sold from repair\ncatalogs are made from a soft nickel tin alloy with flattened-down\ntops. Because the alloy is made of a soft metal, its life\nspan is short with expected wear. The main concern with\nchoosing the right material is not only to make the life\nspan of the frets endure years of playing, but also to\nimprove tone, volume, and sustain with perfect intonation\nthroughout the fingerboard. Over the years, I have experimented\nwith different exotic metals for frets, such as Titanium,\nStainless Steel, Hestelloy C-22, Inconel 718, Ceramic\ncoated material, Zirconium Tungsten Coatings, and Cobalt\nBased Super-Alloys. In the end, the best of these materials\nproved to be a Stainless-Steel alloy. This material proved\nto be the most effective in achieving my objectives for\nsuperior fret performance.\n|Modified Gibson Les Paul,\nModifications: Nigerian Ebony fingerboard with Abolone\nblock inlays, outlined with Rosewood binding. Micro Polished\nPetillo Precision Frets™ and fingerboard with\nPetillo size .009 strings.\nTo micro polish is to produce a finish on a surface of\nan object, making it as smooth as possible, so that there\nare no scratches or imperfections, only a smooth high-gloss\nsurface. The molecular structure of the Petillo fret alloy\nis changed after micro polishing with a specialized coating\nprocess which makes the surface area of the fret harder.\nEach fret is done individually. What value does this have\nfor fretted instruments? How many times have you played\na guitar, bass, or any fretted instrument where the strings\nseem to be restricted by rough frets, or when bending\nthe strings makes a rasping noise? Once the micro polishing\nof the frets is completed, effortless string bending and\nfinger positioning, which give longer life to the strings\nand fret alloy, results. To assure the longevity of the\nfrets, the fingerboard is then buffed with Petillo Fingerboard\nDressing, sealing the wood from finger oils, perspiration,\nand moisture. This compound is specially designed for\ndark unfinished woods such as Ebony, Rosewood, Walnut,\nTulip Wood, etc.\nThis fret micro polishing and fingerboard buffing process\ncan be applied to conventional frets, making a vast improvement\nin the performance of your instrument. However, it will\nnot match the playability achieved with The Petillo Precision\nback to top","Fender Pawn Shop Super-Sonic\nThe Fender Pawn Shop Super Sonic actually resurrects a short-lived model first seen in 1997 bearing the Squier brand name. Back then, it formed part of the new Vista series and originated from Japan; second time around, it's a fully fledged Fender that's made in Mexico.\nUnlike most other Pawn Shops, this model stays pretty true to its roots, so presumably the original design was deemed adventurous enough, and there can be few arguments on that score.\nAs on the earlier Super-Sonic, the lefty-look, large headstock comes complete with a protruding 'bullet'-style truss rod adjuster, but now only one string guide is considered sufficient. The vintage-style Kluson copy tuners differ to those used on Pawn Shop stablemates such as the Jaguarillo and Offset Special, because the post tops have deeper slots, making restringing an easier job.\nThe nut slots are consistently well cut depth-wise, but uneven spacing puts the strings slightly off-centre down the lightly radius'd rosewood fingerboard. The latter borrows the Jaguar's string-bend-friendly shorter scale length (610mm/24-inch) and a 22-strong fret count. The easy feeling fret ends offer a worn-in feel that's maintained by a C profile maple neck, which is gloss finished in vintage fashion.\n\"High-end access to the fingerboard is good, because most of the neck is clear of the body\"\nClay-like position dots are equally old-time, replacing the earlier version's pearloid markers, while side markers are sited along the fingerboard join in time-honoured manner. As usual, this makes them difficult to see - black dots set in the maple neck would be more practical, echoing some 70s Fenders.\nThe original Super-Sonic employed a basswood body, but these days, Fender seems quite coy about stating specific body timber details. Presumably, this is now alder, as used on the other Pawn Shop models, but it's hidden under a sparkly Sunfire Orange Flake finish.\nMatching the neck, this model's body styling is lifted from the Jaguar and similarly reversed to create the left-handed look, but the overall decreased dimensions contribute to a more compact shape. Unlike some other 'upside- down' designs, high-end access to the fingerboard is good, because most of the neck is clear of the body. As before, a curved heel block serves to accommodate a small, angular neckplate that actually still sports the Squier 'S' logo.\nThe neck extends under the scratchplate to ensure secure anchorage, while one of the four fixing screws doubles as the upper strap-button holder.\nThe off-white scratchplate carries a three-way toggle pickup selector switch plus two Fender Atomic humbuckers, which provide the biggest visual difference between old and new Super-Sonics. Here, both pickups are mounted at a backward angle, not just the bridge pickup as on the original version. In addition, height adjustment is now via three screws rather than two, while the exposed bobbins are black and cream, instead of all black. Controls still sit on a separate metal plate and comprise two volume pots topped with Jaguar-style knobs.\nAdding a Strat ingredient to the mix, the vintage-style vibrato unit is another constant factor, featuring bent steel saddles and a screw-in arm. The feel is responsive, but tuning drift is very apparent after a quick waggle or two with string-bending. A few tweaks will be required here.\n\"This one is undoubtedly intended to be up front and dirt-friendly\"\nThe Atomic humbuckers are very potent components, staying true to their title output-wise, while the tonal character borders on brash and brutal, with an aggressive edge guaranteed to slice through the thickest mix. This comes across very strongly in all three pickup positions, although the both-on option adds some unusual honky overtones.\nThe previous Super-Sonic could be a quite subtle performer, but this one is undoubtedly intended to be up front and dirt-friendly, maintaining impressive definition even under heavy distortion conditions.\nThe volume controls clean up the sound as they should, but a tone control would be handy to tame the rather unremitting attack. Like the original Super- Sonic, the layout reverses normal logic, since the nearest knob governs the bridge humbucker. The pickup selector is equally 'upside down' in operation, and when set to the centre position turning either volume off eliminates output entirely, so it can't function as a kill switch.\nAttention-grabbing appearance. Easy feel.\nPitch return problems. Circuitry quirks.\nThis revives a decidedly original Squier design and adds a few updates to add appeal for the modern market.\nScale Length (Inches)\nNo. of Frets\nVintage-type vibrato, vintage-style tuners\nCountry of Origin\nString Spacing (In)\n2x volume, 3-way selector\nScale Length (mm)\n2x Fender Atomic humbuckers\nReverse offset double-cutaway solidbody electric\nSunfire Orange Flake, Apple Red Flake, Dark Gunmetal Flake"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:b8319dac-827f-4e4d-b5b7-ebfa46ec9be3>","<urn:uuid:2bcb5ca8-1b40-4a4d-bb3f-5d257f3379aa>"],"error":null}
{"question":"What was the survival rate of TV programmes from the 1960s?","answer":"The survival rate for programmes from the 1960s was only about 8 to 10% of the total programme output.","context":["How many old programmes survive?\nAs a general rule, after 1978, most Television programmes survive. Prior to this time, a representative sample of programmes exist from both the BBC and ITV companies. The first decade to fully benefit from the introduction of recording techniques was the 1960s and the survival rate for programmes from that decade is not much more than 8 to 10% of the total programme output. The reasons for this are varied and there is no simple, single answer. The expansive use of video tape recording is certainly a major factor. Until autumn 1958, there was no videotape recording available to the BBC or ITV. Programmes were mostly broadcast live. Prior to 1958, the only way a programme could be recorded was by using film. It was known that only 35mm-cinema film had the required resolution and quality to record the British 405-line standard. Unfortunately the cost of recording Television pictures on 35mm film was prohibitive and only the BBC and two of the early ITV companies used this recording method. 16mm film was eventually used as an acceptable standard, but mostly for making copies of programmes for overseas sales or internal use.\nVideotape brought a revolution in that the physical tape could be re-used. This meant that the cost of a single tape (which in the early 1960s was approximately £200) could be subdivided by being re-used. It was estimated that a typical tape could last up to ten re-recordings (in practice this was often much less). At that time, the cost of making a 35mm film recording negative and one print was roughly the same price as videotape. Simple economics meant that there was no impetus to keep programmes recorded on tape. If a programme was kept for posterity, it was thought that the film recording was superior to videotape. In practice, this meant that film was more durable. For this reason, the majority of deliberately archived programmes exist as film recordings. 35mm film prints were therefore a common standard for archival storage.\nSurely an effort could have been made to keep reference copies of programmes?\nMost Television companies had no further use for old programme material. Primarily, agreements with the actor’s union - Equity, restricted the number of repeats. This effectively meant that the commercial value of a recording was minimal. Other programmes had agreements written into their contracts which ordered the wiping of recordings after a certain date. Despite this ruling, some programmes in this category have survived (for example, both the BBC’s ‘Maigret’ and ‘Sherlock Holmes’ 1960’s series originally had this form of contract). The introduction of Colour Television in 1967 led the way for a gradual purge of black and white programme material. Both the BBC and ITV companies were faced with the likelihood of a back catalogue of programmes that were unlikely to be repeated. They mostly decided to destroy out of date material, keeping key recordings for posterity, rather than for commercial needs. The old videotapes with 405 line recordings were either junked or re-recorded with the new 625-line standard. Film recordings were easier to replay and a representative number of these survive to fill in the gaps left by the junked videotapes. Added to all these factors was the thought that most Television was ephemeral and therefore unworthy of preservation.\nWhat about the National Film Archive, didn’t they save programmes?\nBudget restrictions prohibited the number of programmes required. Ironically, when money was available, it was often too late. One example was the difficulty in obtaining copies of BBC videotapes. This problem was only addressed in 1972, by which point a good many programmes had been lost forever. It is worth noting that most copies that did exist of BBC and ITV programmes were either single prints or a single videotape. Therefore, the cost of copying for another archive was an added difficulty and expense.\nWhen we watch old programmes, do they have the same quality as seen on their original broadcast?\nUnfortunately in most cases the quality is poor. Because of the current 625 line picture standard the remaining 405 line videotapes (of which roughly only under 1000 survive) have to be converted. Film recordings only record part of the original picture information and effectively loose the live feel of the original broadcast. Recently some promising work has been achieved with a view to restoring this live ‘look’.\nProgrammes still seem to ‘turn up’. Where do they come from?\nThe most obvious source is from the broadcasting companies. Some programmes have been mislaid rather than truly lost. Other sources include Production staff who kept examples of their own work or took home prints marked for destruction. This has led to a fruitful source of material. It has been possible for dedicated enthusiasts to locate prints, which were usually made for export on 16mm. A good many programmes have been saved simply because these export prints have found their way into private film collections. A good many of these prints had their main or end titles removed to make them commercially worthless. Luckily for us, they survive to tell the 21st Century viewer what Television was like in its infancy."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:3ac9b3bb-e0b2-4360-a9dd-001a0c644c93>"],"error":null}
{"question":"How do the filter maintenance approaches compare between a Hayward Perflex D.E. filter system and a kidney loop filtration system in terms of operational impact?","answer":"With a Hayward Perflex D.E. filter system, maintenance requires shutting down the entire filter system to perform backwashing or regeneration when pressure increases. In contrast, a kidney loop filtration system can be shut down for filter maintenance with no impact on system operation since it operates independently from the main system. The kidney loop approach also allows for less expensive filters since it doesn't need to operate at high pressure, and can achieve efficient laminar flow while maintaining specified fluid cleanliness levels using 25-micron elements on the pressure side and 10-micron elements on the return side.","context":["Hayward Perflex D.E. Filters: Bumping Vs Back Washing\nThe Hayward Perflex D.E. Pool Filter operates in a different way then most D.E. pool filters. This type of filter cannot only be back washed but the filter also has an option called “regeneration” which allows you to get the most from your D.E. powder. The Perflex filter also has something called a “bump handle” that most D.E. pool filters do not have. This bump handle allows you to shake the D.E. from the pool filter fingers that are inside of the tank.\nThe Hayward Perflex filter has man options that may seem confusing at first, but once you read and learn how to use this type of pool filter you will have no problem keeping your swimming pool clear and blue. You will need a bag of D.E. powder before you can operate this type of pool filter, So if this is your first time working with this type of swimming pool filter, then the first thing you are going to need is a bag of D.E. powder that can be bought online or locally at any pool supply store.\nWhat Does Bumping The Handle Do?\nBumping is when you lift the bump handle on the top of the Perflex filter up and down. When you bump the handle you move the tube sheets inside of the filter up and down which causes the D.E. to loosen up and move around inside of the filter tank.\nWhat Is Back Washing The Filter Mean?\nBack Washing is the process of removing dirt, debris and also D.E. powder from the pool filters tank. This is done using the swimming pool filter systems pump. You first have to bump the handle on top of the filter a few times and then open a valve to back wash the filter. See below for a full lesson on how to properly back wash your Hayward Perflex D.E. Pool Filter.\nHow Does The Hayward Perflex D.E. Pool Filter Work?\nThere are a few main parts that you will always be working with when you operate your Hayward Perflex D.E. swimming pool filter. The main parts are the filter gauge, the bump handle, the back wash valve and you will also need to have a supply of D.E. filter powder. When a filter is brand new or just has been back washed there is no D.E. powder inside of the filter. Inside of the filter is something called “fingers”. These filter fingers are plastic tubes that are covered by mesh that allow the water to flow through them but not the D.E powder.\nWhen you add D.E. powder through the skimmers, the D.E. powder travels through the swimming pool plumbing, then enters the filter where the D.E. powder then coats the filter fingers. Once the filter fingers are coated with D.E. powder the pool water flows through the fingers and then the D.E. powder catches all the debris. Once all of the D.E. inside of the filter gets covered with dirt and debris, it will be time to regenerate or back wash the filter.\nWhat Is D.E. Filter “Regeneration”?\nRegeneration is a bit different than actually back washing the pool filter. When you regenerate you are basically “flipping” & “re-using” the existing D.E. inside of the pool filter. When you back wash you are removing the old D.E. powder from the pool filter and then replacing it with new. Regeneration does not require you to replace the D.E. powder because you are just “bumping” it around and its sticking back to the filter fingers inside of the tank.\nHow To Regenerate The D.E. Inside The Filter\nRegenerating your D.E filter will allow you to get a bit more filtering action from the D.E. inside of the filter tank. The first thing you will want to do is shut the filter system down. Once you have shut the filter system down you will want to bump the handle up and about 10 times and then you will want to turn the filter back on again. What the bump does is loosen all of the D.E inside of the filter, turning it over and then once you turn the filter back on the D.E. re sticks to the filter fingers. You will want to regenerate if you see the flow of your filter start to slow down. If you have regenerated your filter but you still have poor flow, then your best bet would be to just back wash the filter which removes all of the D.E. from the filter.\nHow Often Do I Back Wash My Perflex Filter?\nOn the side of your filter you will have something called a pressure gauge. This pressure gauge measure the amount of pressure that is inside of your pool filter tank. When your filter has just been back washed you should take a look at the number on that gauge and see what your “normal operating pressure” is. When the pressure on the gauge raises 5 – 10 LBS above the normal operating pressure the filter will have to be back washed.\nHow To Back Wash The D.E. Out Of The Hayward Perflex Filter\nWhen the pressure inside of your filter tank rises you will have to back wash the pool filter. To do so, the first step is to shut down the filter system. Once you have shut down the filter system you will want to bump the bump handle up and down slowly at first but in the end you will want to bump that handle 15 or so times. Once you have bumped the handle you will want to open up the valve that is located on the bottom of the filter tank. This is called the back wash valve and when you open it pool water and D.E. will start coming out. Once you have opened the valve you will want to turn the filter system back on for about two minutes to forcefully flush all of the D.E. out of the filter.\nNow that all of the D.E. powder has been flushed from inside of the filter tank, you are going to need to add more D.E. powder to the filter. You can do so by making sure that the filter system is turned on and running and then you will want to walk over to the pool skimmer and add the required amount of D.E. to the pool filter. You can do so by scooping the D.E. powder into the filter slowly and 1 LB at a time.\nWhen & How Do I Add More D.E. Powder?\nEvery time you back wash the Hayward Perflex filter you will need to re-add some D.E. powder to the filter. Once you have finished back washing you will want to turn your filter system back on. Once the filter system is back on you will want to look on the side of the filter tank and see how much D.E. you are supposed to add to the filter. Once you find out how much D.E. you need to add to the filter you will want to walk over to the skimmer and add that amount slowly inside the skimmer until it all has worked its way to the filter.\nCleaning, Storing & Maintaining Your Hayward Perflex Pool Filter\nThe best way to get the longest life from your pool filter is to properly take care of it. You should take your filter apart at least once in the beginning of the season and once at the end of the pool season. This way you can clean the filter fingers, inspect all of the o-rings, inspect the tube sheets and keep the inside of the filter is perfect shape.\nIf you have any questions, please feel free to ask them below.","Filtration 200: Beyond the Basics\nBy Justin Bitner, Product Sales Manager at Eaton Filtration\nIf working with hydraulics is part of your life, you’ve almost certainly been reading “Filtration 101” articles since day one. You know, the articles that say “Keep the fluid clean” and “Change the filters regularly” and “Put them here or there”—in short, the common-sense things that any hydraulic professional worth his or her salt already knows and does.\nThere is no denying that those things are important. In fact, they’re so important that I’m going to briefly reiterate them here before moving beyond the basics.\n- Filter the fluid before it goes into the reservoir. It doesn’t come from the supplier clean enough to put in your system. Period.\n- Monitor fluid condition regularly. That means laboratory-grade analysis on a schedule determined by your operating conditions, not some arbitrary timetable.\n- Always flush the system to manufacturer specs or ISO standards every time components are replaced or repairs are performed.\n- Monitor the filters and change them before they go into bypass mode. Here again, that requires a schedule based on your operating conditions, not some arbitrary timetable.\n- Make sure the replacement filter elements meet the original manufacturer’s specifications. “Will fit” and “as good as” promises are not acceptable substitutes for verified laboratory testing and qualification.\n- Don’t forget that breathers are filters, too. Unless your system is installed in a clean room, airborne contamination can be a major issue.\n- In almost all cases, filters should not be located at pump inlets. Cavitation is a much greater threat to your pump than contamination—which should not be in your fluid in the first place.\n- Filters should not be placed on the drain lines of piston pumps and motors. Those lines need to be free flowing or you risk the opposite of cavitation, which is equally destructive.\n- The best place for fine filtration is generally on the return side of your system. If you start with clean fluid, any contamination will be generated within the system and caught by a return line filter.\nIf that checklist sounds a lot like your filtration protocol, chances are your hydraulic systems are performing well because you have the basics covered. But in today’s competitive environment, covering the basics is only the beginning. There is a lot more to be considered and implemented if you’re going to maximize the return on your hydraulic investment.\nSo let’s go through that list again and move beyond the basics.\n- If your operating pressures are going up, so should your cleanliness levels. Where 19/17/15 may have been acceptable a few years ago, many of today’s higher-performance systems need 15/13/10 or even better cleanliness levels. Maintaining those levels will probably require more than periodic laboratory analysis of your fluids.\n- One solution is to use a commercially available “laboratory in a suitcase” and do your own testing in-plant. Eaton, and other filter manufacturers, offer highly capable units that will allow you to rest on an accelerated schedule without incurring the costs associated with an outside laboratory.\n- A more advanced solution is to install an Inline Particle Counter (IPC) to provide real-time monitoring. These devices use a variety of technologies to monitor the fluid condition and generate an alarm when anomalies are detected.\n- Inline Viscosity Sensors (IVS) and Inline Water Sensors (IWS) are also available. A comprehensive monitoring solution will include all three types.\n- Even more sophisticated units use a sensor that measures changes in the dielectric characteristics of the fluid. Those changes may be caused by particle contamination, water, or even depleted additives. These systems continuously monitor fluid condition and alert you to any changes in that condition. It’s a different, but very effective, approach.\n- Being proactive during system design can eliminate many common challenges encountered during system operation. For example, mounting any required suction filters outside the reservoir will greatly simplify maintenance. The truth is that suction strainers and filters mounted inside the reservoir are seldom, if ever, serviced.\n- Utilizing manifolds and tank-top filters is another design element that can reduce maintenance challenges. It also eliminates many separate contaminant ingress points, making the whole system more robust.\n- Specifying filters based on maximum published flow rates is another mistake that can be avoided during the design process. Maximum flow rates are exactly that—the maximum the filter can handle without damage to the element. They are most definitely not the flow rates at which the filter can be expected to deliver maximum cost-effective performance.\n- Other factors, such as viscosity changes and cold-start conditions, also impact filter performance and life. As a rule of thumb, sizing the filter for twice the desired maximum flow rate will optimize both filtration performance and cost-effectiveness over the long run.\n- Another good practice is to specify filters equipped with differential pressure gauges or switches. Comparing the inlet pressure to the outlet pressure gives a very good indication of filter element condition. The switch-type units can be hardwired to an annunciator light or another device to notify maintenance personnel of an approaching end-of-life condition before the filter goes into bypass mode.\n- Breathers already have been mentioned, but they are worth a second look. A typical low-cost breather uses a non-replaceable, 10-micron paper element, which means there will be a lot of 10-micron particles allowed into the reservoir.\n- Experience says that if one 10-micron particle goes into the pump, two will come out. Yes, the return line filter will capture them both, but the damage has been done at that point, and the pump’s useful life has been shortened. Spending a few dollars for an effective breather, and then maintaining it like every other filter in the system, will prevent the damage in the first place.\nAs mentioned above, the best location for filters is generally on the return side of the system. In most cases, a finer filter can be installed there than is possible on the supply side because any flow restriction is less likely to impact pumps, motors, and valves.\nFor many systems, an even more cost-effective approach is to take the bulk of the filtration out of the active loop entirely by using off-line filtration in a so-called kidney loop. That is a stationary system that continuously filters the fluid in the reservoir.\nThis approach has several advantages:\n- A kidney loop does not need to operate at high pressure and can, therefore, use less expensive filters.\n- A kidney loop can be shut down for filter maintenance with no impact on system operation.\n- A kidney loop usually can achieve efficient laminar flow to maximize filter performance and deliver dollar/grams of dirt-holding capacity than a filter rated for pressure/return service.\n- A kidney loop can easily incorporate a low-cost, low-pressure cooler to moderate fluid temperature.\n- A kidney loop does not require exceptionally fine filtration since it continuously cleans the fluid. Typical systems use a 25-micron element on the pressure side and a 10-micron element on the return while maintaining specified fluid cleanliness levels.\nDirt destroys more hydraulic systems than any other cause. Effective filtration is your first line of defense. Moving beyond the basics will pay dividends in terms of optimized system performance, minimized operating cost, and eliminated maintenance headaches. It’s definitely worth the effort.\nTagged basics, filtration, maintenance"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:b4571dde-782c-4a58-912a-d35a4e021632>","<urn:uuid:fa2878b9-b46b-430e-86de-c0016b869a12>"],"error":null}
{"question":"What are the key visual indicators for identifying adult elm leaf beetles versus adult emerald ash borers?","answer":"Adult elm leaf beetles and emerald ash borers have distinct appearances. Elm leaf beetles are about 1/4 inch long and yellowish to olive green, with a black stripe along each side of the wing covers and four black spots on the thorax. In contrast, adult emerald ash borers have a metallic green body with purple areas beneath their wings, are about a half-inch long, and have a flattened back.","context":["Elm Leaf Beetle\nRevised by Bruce A. Barrett\nDivision of Plant Sciences\nThe elm leaf beetle attacks all species of elm, especially Siberian elm (commonly called Chinese elm) and American elm. The insect feeds on the leaves, causing the leaves to dry up and die. This foliage feeding by itself generally will not kill the tree. But it will weaken the tree and make it more susceptible to branch dieback and attack from other insects and diseases, such as Dutch elm disease. This serious disease of American elm is not transmitted by the elm leaf beetle but is carried by the elm bark beetle, which does attack weakened trees. The elm leaf beetle also may become a household nuisance by migrating into homes and buildings during the fall months.\nEggs are laid in groups of 10 to 30 on the underside of leaves. They are pointed, orange-yellow and about 1/16 inch long.\nThe larva is the elm leaf beetle’s most destructive life stage. When full grown, it is about ½ inch long and is dull yellow with two black stripes.\nThe pupa — the stage in which the larva changes to an adult — usually is found around the base of elm trees. The pupae are about 1/4 inch long and bright orange-yellow.\nThe adult beetle is about 1/4 inch long and yellowish to olive green, with a black stripe along each side of the wing covers (Figure 1). The stripes are sometimes indistinct in the dark olive forms. The adult has black eyes and has four black spots on its thorax.\nThe elm leaf beetle passes the winter as an adult under the bark of trees or in sheds, barns or houses. It is not harmful to humans, other animals, dwellings or household contents, but it is considered a nuisance inside the home.\nAs the days cool in the early fall, the adult beetles seek shelter. They crawl into homes and other buildings through cracks and holes in and around screens, windows and door facings.\nWhen the days warm up in the spring and the trees begin to leaf out, the adult beetles begin their annoying movement about and leave the house. They fly to the nearest elm tree and begin feeding on the new leaves. At this time, they lay eggs that hatch in about a week. The larvae feed for two or three weeks (Figure 2), then drop or crawl to the base of the tree, where they pupate. After about 10 days, the adults emerge.\nSuccessive generations continue through the summer and into the fall, when the adults, seeking shelter for hibernation, again invade the home. The northern half of Missouri usually sees three generations each year, and the southern half, four.\nAdult elm leaf beetle.\nLee Jenkins Slide Collection\nLarvae on elm leaf.\nLee Jenkins Slide Collection\nThe first step in preventing entry into the home is to control these beetles at their source — elm trees.\nThorough coverage of all foliage is necessary. So, use spray equipment that is of sufficient size and capacity to enable coverage of the tallest trees. A good time to treat is when the larvae are near the base of the tree and prior to pupation. At this time, only the trunk of the tree needs to be sprayed.\nRead the label on the insecticide container, and use only those formulations that list the pest you wish to control and the site where you wish to apply the insecticide. Certain formulations of the following insecticides applied as sprays may be used.\n|Carbaryl||Ferti-lome Liquid Carbaryl Garden Spray|\n|Cyfluthrin||Bayer Advanced Multi-insect Killer|\n|Imidacloprid||Bayer Advanced Tree and Shrub Insect Control|\n|Lambda-cyhalothrin||Spectracide Triazicide Lawn and Garden Insect Killer|\n|Neem||Ferti-lome Triple Action Plus|\n|Pyrethrins||Spectracide Garden Insect Killer|\nA community-wide control program is more effective when sprays are applied to protect the trees from the first-generation larvae. When only a few trees are sprayed, later generations of beetles moving from unsprayed trees frequently will reinfest sprayed trees, thus making additional spray applications necessary.\nIn the house\nMake the house as tight as possible in an effort to prevent entry. Stuff cotton in sash cord channels, and use caulking compound to fill cracks around doors and windows and all other openings large enough for insects to go through.\nTo control beetles crawling on and into buildings, spray the exterior foundations, outside walls and around window wells and other exterior points of entry with appropriately labeled insecticides.\nUse household sprays or aerosols containing pyrethrins to kill beetles that get into the home. Repeated applications will be necessary. Beetles may be captured or collected for disposal with a vacuum cleaner.\nInsecticides are poisons and should be handled with care. Read and follow label recommendations and directions. Store pesticides safely and out of reach of small children. Carefully, promptly and properly dispose of unused portions of diluted sprays and empty containers.","Emerald ash borer beetles (EAB) are costing cities, companies and property owners millions of dollars because of damage caused by their invasion of ash trees. Unless these infestations are stopped, the economic impact will continue to worsen for industries that rely on this type of lumber. Because of the continual spread of EAB infestations, learning to identify EABs and the signs of infestation are critical so they can be stopped.\nIdentifying the Emerald Ash Borer\nIt can be challenging identifying the emerald ash borer because there are other types of beetles that look very similar to it. Plus, during the pre-adult stages of the EAB lifecycle, you must rip the bark from a tree in order to spot them because these beetles live in small chambers they've built beneath the bark.\n- Adult: An adult emerald ash borer has a metallic green body with purple areas beneath its wings. Its entire body is about a half-inch long and it has a flattened back.\n- Pupae: After one to two years of feeding inside the bark of an ash tree, the EAB larvae build small tunnels and chambers for themselves in the layers below the bark, known as the sapwood. In the spring, the EAB pupae emerge from the tree as adults. This is the lifecycle stage between larvae and adulthood.\n- Larvae: Immature emerald ash borers, or larvae, have lightly flattened bodies that are white with two brown pinchers on the very end. They vary in size, depending on how much they feed and grow beneath an ash tree's bark.\n- Egg: The eggs of an EAB are only about one millimeter long, making them very difficult to spot. After the adult female deposits them into crevices within the tree bark, they hatch from the eggs and immediately begin boring their way deeper into the tree.\nHow EAB Infestation Spreads\nEmerald ash borers typically only fly a distance of about a half-mile, so infestation is spread by humans who inadvertently transport firewood, logs, branches, chips and other raw ash wood products infested with larvae.\nThis destructive beetle was first discovered in North America in 2002, just outside Detroit. Since then, EAB infestation has spread to Canada and at least 22 other states. Since emerald ash borers originate in Asia, infestations in the U.S. and Canada also spread from larvae-infested wood products imported from the Far East.\nSigns of EAB Infestation\nEmerald ash borer beetles are responsible for killing tens of millions of ash trees throughout North America, so it's crucial to identify the signs of infestation as soon as possible. An EAB-infested tree can die within two to four years because EAB larvae feed on the sapwood and inner bark, causing the tree to starve as it loses its ability to transport nutrients and water from its roots to the rest of the tree.\n- Dieback of the Tree's Canopy: The upper and outer canopy of a tree begins to die after years of EAB infestation. Dead branches appear throughout the canopy, beginning near the top.\n- Epicormic Sprouts: As an EAB-infested tree becomes sicker, it may try to grow new branches and leaves from the area of the trunk located below the larvae infestation. This usually occurs near the base of the tree.\n- Splitting Bark: An infested tree often has long, vertical splits in the bark caused by larval boring and feeding.\n- Increased Woodpecker Interest: Woodpeckers like to eat EAB larvae beneath the bark. When woodpeckers forage for these beetles, they create holes in the tree. In a highly infested tree, woodpecker damage may appear as long strips of missing bark.\n- D-Shaped Holes: An abundance of tiny holes shaped like the letter \"D\" in the tree's outer bark is a sign of EAB damage because they use these holes to emerge from the tree.\n- S-Shaped Patterns: As larvae feed on the sapwood beneath the bark, they wind back and forth, creating long S-shaped patterns. These patterns are not visible unless the outer layer of bark has been removed.\nOne of the most effective ways to stop the spread of EAB infestation is to avoid moving ash firewood across state lines. If you do notice any signs of emerald ash borer beetles, contact your local town office for more information."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:265c142b-4399-4211-ac66-29cf73df21bc>","<urn:uuid:dce30520-0d10-408f-ae4f-d1bbbc6f6680>"],"error":null}
{"question":"How do beneficial and pest nematodes differ in their effects on plants and soil health, particularly in the context of climate change?","answer":"Beneficial and pest nematodes have drastically different effects on plants and soil health. Beneficial nematodes help convert organic matter into plant nutrients and prey on soil-dwelling plant pests like white grubs and root maggots. They can even attack and kill harmful insect pests, converting their remains into nutrients, especially nitrogen, that plants can use. In contrast, pest nematodes damage plants by feeding on their roots, causing symptoms like root knots, galls, injured root tips, and excessive root branching, which can lead to stunted growth and plant death. With climate change, the threat from pest nematodes is increasing, as their reproduction rate and spread has a direct relation with temperature. As global temperatures rise, there is a growing threat of plant diseases spread by nematodes, particularly affecting crops like citrus trees.","context":["Environment plays an important role in development of plant diseases. It may affect succulence, any plant growth level, presence or even genetics of the plant.\nSimilarly, there is an absolute involvement of climatic factors to effect spore formation, multiplication, growth, survival, spread and spore germination as well.\nMoreover, activity and production rate of the pathogen vectors may also be affected. A great climate change occurred in recent few years due to a lot of reasons. Global warming is the major cause of this climatic change.\nUnexpected climate change is greatly affecting plant diseases directly and indirectly by promoting disease causing agents and insects population as well. Early changes in temperature and other factors favor the disease pathogens and their vectors to survive and infect the plants at initial stage when they are more susceptible for the attack.\nPlant resistance mechanism is also going to affect drastically leading to promote the pathogens to spread diseases. In this way resistant plants are going to be susceptible for the pathogen attack.\nIn results, crop production is going to decline badly and there is massive reduction in market value of the fruits vegetables and other agriculture commodities. There are some biotic and abiotic factors favoured by the climate change.\n- FUNGAL DISEASES\nGradual increase in CO₂ amount may increase or decrease in disease development ratio. Its increased amount positively affects plant canopy and density which favors different plant diseases. Also, it affects fungal activity in two ways.\nFirstly, its varying amount has effect on stomatal opening and plants’ defence system which makes plants sensitive for the fungal attack. In other way, with the increased in CO₂, fungal activity and life cycle becomes rapid as in case of Erysiphi graminis the rate of attack becomes double in plants.\nIncreased temperature in environment causes plant injuries, which invite fungus to get entry in plant cells and cause infection e.g increase in plant blight. Storms and rains due to environmental changes affect plant structure system which makes faster growth in pathogens’ life cycle.\n- VIRAL DISEASES\nFungi is the most studied pathogen influencing by the climate change. Viral infections have been ignored in aspect of climatic change, but it is greatly influenced and causes more disease spread comparatively. Increasing rate of CO₂ helps in spread of virus.\nViral transmission in plants is usually done by the insects e.g cotton leaf curl in cotton plant by whitefly. With the climate change insect reproduction rate is favored and rapid spread of viral diseases occurs. Other environmental factors e.g storms, hailing, high and low temperature etc cause plant injuries which through which virus takes entry into the plant system causing infection.\n- BACTERIAL DISEASE\nBacterial activity is also affected positively by the climate change. These germs complete their life cycle in dead plant matter and gain entry in plants through insects or through stomatal openings causing different plant disease.\nThey need moisture for their best survival and infection. On increase in temperature their population decline is expected. However, bacteria mostly enter in the plant body through cuts and injuries. In summer storms there are more chances of plant injuries and direct effect on plant diseases by the increasing rate of bacterial infections.\n- NEMATODAL DISEASES\nNematodes are soil living organisms. They spend more of their life period in damped shady soil. They basically attack on plants’ root portion and ultimately weakens the plant by nutrition deficiency as they feed and destruct the root structure e.g citrus trees.\nTheir reproduction rate and spread has direct relation with temperature. As temperature is increasing day by day so there is great threat to spread plant diseases by the nematodes.\n- ABIOTIC FACTORS\nAbiotic factors (temperature, humidity, storms, rain etc) affect plants in both direct and indirect way. Increased temperature causes injuries, plant burnings, wilting, blistering, discoloration and desiccation of the plant tissues etc while sudden decrease in temperature cause frost killing of plant parts e.g in cherry and peach. Low temperature also has negative effect on indoor plants.\nHowever, they indirectly promote plant disease by favoring pathogens to reproduce rapidly and giving best suited environment for their survival and spread. Inadequate oxygen availability also affects plant growth by causing desiccation of different plant roots in specific soils. light, air pollution, acid rain and hailing also going to promote in these drastic conditions which also participate in plant destruction.\nActivity of biological control agents is seriously affected by the change in climatic factors. Because change in temperature affects hibernating population in their off season and their activity badly affected due to unfavorable conditions in their active season.\nTemperature changes and rains affects pesticidal activity. Due to which there is improper pest management. This gap also helps to produce new races of the pest population which are more dangerous and resistant against chemical control.\nHuman activities are the main reason for these circumstances. Fossil fuels burning is participating in increasing level of CO₂ in atmosphere. Aerosols and greenhouse gases are leading to global warming. Urbanization and industrialization is also the basic reason for such environmental changes.\nThese swear changes in environment also have negative impact on human health as well. The Agriculture economy of the country may be affected badly in future if such climatic changes neglected.\nSo, it is strictly needed that avoid all those activities on earth which participate in such climatic changes. Otherwise changes in temperature, rain water evaporation (increase in moisture level) and extreme conditions are expected to enhance plant disease epidemics in future.\nAuthors: Aamer Sohail1, Muhammad Waqas1, Muhammad Sufian1, Adeel Arshad2\n1.Department of Entomology, University of Agriculture Faisalabad\n2.Institute of Animal and dairy Sciences, University of Agriculture Faisalabad","Nematodes are microscopic soil-dwelling worms, many less than 1/16-inch long.\nThere are beneficial nematodes and pest nematodes.\nBeneficial nematodes help turn organic matter into plant nutrients. They also prey on soil-dwelling plant pests such as white grubs and root maggots.\nPest nematodes feed on plant roots, stunting and sometimes killing plants including many vegetables.\nNematodes are slender, translucent, unsegmented worms. Pest nematodes can be as small as 1/50-inch long. Beneficial nematodes that parasitize pest insects are larger, 1/25-inch long to several inches long.\nNematodes live in the film of water that coats soil particles; they thrive where the soil is rich, moist and warm. Nematodes can’t move more than about 3 feet on their own in the course of their lives, but they often travel around the garden in water, in shifted soil, in soil surrounding transplants, and on garden tools and even ants.\nPredatory nematodes either have teeth or long spear-like structures which they use to stab and suck the juices out of plants or their insect prey.\nNematode reproduction is usually sexual, though some individuals are capable of self-fertilization. Eggs are laid in a gelatinous mass. From hatching to full-formed adulthood, nematode development usually consists of four molts that take about a month; the life cycle of some pest species is only 3 or 4 weeks. There are many generations over the course of a year. When the soil grows cold, nematodes overwinter as eggs or adults. Where winters are not cold, nematodes are active year round.\nNematodes are found throughout North America.\nSome pest nematodes attack the roots of tomatoes, potatoes, peppers, lettuce, corn, and other vegetables. Other pest nematodes attack the stems and leaves of onions, rye, and alfalfa.\nPest nematodes include root-knot nematodes, lesion nematodes, ring nematodes, and sting nematodes.\nDamage inflicted by these pests includes root knots or galls, injured root tips, excessive root branching, leaf galls, lesions on drying tissue, and twisted, distorted leaves. This damage can result in stunted growth and sometimes plant death.\nFeeding Habits and Damage: Nematode damage often looks like a plant disease: leaves may turn yellow and become wilted and stunted. Roots of plants pulled from the ground often look lumpy and stunted.\nThe root knot nematode is perhaps the most destructive of the soil-dwelling pest nematodes in vegetable gardens. It moves in soil water to find the roots of susceptible crops. When it enters a root, it begins to feed by siphoning water and nutrients away from the plant. It secretes a substance that causes root cells to weaken and enlarge into firmly attached, knotty galls. After feeding the nematode lays a mass of eggs, new larvae hatch. The infested root grows large until it splits open and releases a new population of nematodes into the soil. The split root often becomes infected with fungi and bacteria and rots. The plant is left weakened, often near death.\nPest Nematode Controls: There are no organic methods to permanently eliminate pest nematodes but the population can be reduced. Here are some control strategies:\n- Remove and destroy infected plants along with their roots from the garden. Place plants and roots in a clear plastic bag and place it in the sun for a week to kill the pests. Then dispose of the bag in the trash, not in the compost pile.\n- Plant nematode resistant varieties: There are tomato and other crop varieties that have been developed to minimize nematode damage; damage will still occur but be less severe. Planting resistant varieties will indirectly reduce the pest nematode population over the course of a few years as there is less for the pests to feed on.\n- Plant crops that are not injured by pest nematodes. Where pest nematodes are present, plant crops that are not harmed by nematodes; this also will gradually reduce the number of pests in the soil. Crops that can survive in nematode infested soil include broccoli, Brussels sprouts, mustard, onions, leeks, garlic, rutabagas, and glob artichokes.\n- Confine pest nematodes to specific problem spots. Don’t allow nematodes to move around the garden. Nematodes cannot move more than about 3 feet on their own. Clean tools used in infested areas; wash the tools over the problem spot. Don’t carry pest nematodes around the garden.\n- Add organic compost to garden beds. Beneficial fungi and bacteria that attack pest nematodes thrive in organically rich soil.\n- Plant French marigolds (Tagetes patula) or African marigolds (Tagetes erecta) in the garden as a cover crop; grow only these plants for two months across vegetable planting beds. After two months, cut them down, let them dry in place, then turn them into the soil. Chemicals in these plants repels nematodes. You will likely have to repeat this treatment in two years.\n- Solarize the soil. Cover the soil with clear plastic during the hottest part of the summer to kill both pest nematodes; this will rid the soil of pest nematodes for a year or two. However, beneficial organisms are also harmed or destroyed by solarization of soil.\n- Add chitin to the soil. Chitin is a natural component of nematodes bodies. Fungi attack nematodes by breaking down the chitin in the body. Adding chitin to the soil will stimulate fungi to attack nematodes.\n- Add ground sesame stalks to planting beds. Sesame will suppress nematodes.\n- Leave the garden fallow for a year. The nematode population will decline if denied food for a year.\n- Till the soil in winter to expose nematodes to killing sunlight and dryness.\nNot all nematodes are pests; some are beneficial to soil and plants. These nematodes eat organic matter in the soil helping to decompose it and turn it into nutrients for plants. They also attack and kill harmful insect pest, ingest the remains, and turn it into nutrients—especially nitroten–plants can take up.\nBeneficial nematodes that are insect parasites are harmless to humans, wildlife, bees, and earthworms. They attack cutworms, root weevils, corn and stem borers, and squash vine borers and some pest root nematodes.\nThe beneficial nematodes attack soil-dwelling insects through their natural body openings. Once inside, these beneficial nematodes release a bacterium that paralyzes and kills the insect. The nematodes then feed on the tissue of the insect carcass and also eat the bacteria. They reproduce inside the carcass and then move on to a new host.\nThree beneficial nematodes are: Steinernema carpocapsae attacks cutworms, armyworms, corn rootworms, and fire ants; Steinernema fetiae attacks root knot nematodes, ring nematodes, and string nematodes; Heterorhabditis bacteriophora attacks cabbage root maggots, Colorado potato beetle larvae, white grubs, and root weevils.\nBeneficial nematodes can be purchased for use in the garden. Beneficial nematodes come packaged in a gel, in a power, or mixed with peat and vermiculite. They are commonly mixed with water before being applied with a watering can or sprayer to the soil or plants, or injected into plant stems by syringe. It is important to follow label instructions when applying nematodes to the soil or plants. Commonly applications are repeated every two to three weeks if needed.\nThe efficacy of nematode application can be affected by hot weather, cold soil, and heavy rain. Nematodes are most active in temperatures between 72°F and 82°F."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:80b88c73-e007-4bc1-95fc-3e7c2940b63f>","<urn:uuid:16afa57a-6095-47ec-87b8-6f4737daed93>"],"error":null}
{"question":"How do the preventive measures and transmission patterns differ between impetigo and bacterial pharyngitis?","answer":"Impetigo and bacterial pharyngitis have different transmission patterns and preventive measures. Impetigo requires direct contact with infected sores for transmission, with bacteria commonly present under fingernails and on clothing. It is not transmitted through saliva, coughing, or bowel actions. Prevention involves washing clothes and bedding in hot water, regular handwashing, keeping fingernails trimmed, and showering with antibacterial soap. For bacterial pharyngitis, prevention focuses on avoiding sharing food, drinks, and utensils with infected individuals, maintaining appropriate hygiene, washing hands especially before eating and after coughing or sneezing, using hand sanitizers, and avoiding exposure to smoke. Unlike impetigo, pharyngitis transmission can occur through closer contact with infected individuals.","context":["What is it ?\nThis is a bacteria that lives on the skin, in nasal hairs, underarms and in groins. If the skin is traumatised, due to scratching, insect bites, scrapes and cuts, it may infect resulting in the characterstic school sore appearance. This sore will be heavily infected and scratching will transfer this infection to other parts of the body. Sometimes a similar bacteria called streptococcus pyogenes is the culprit, though the treatment is the same. The sores may start off with a blister, called bullous impetigo, or they may spread from an existing skin problem such as an insect bite.\nWhat is the treatment ?\nTreating the infection requires eradicating the bacteria from the sores using antibiotics. If there are numerous sores then oral antibiotics and an antibacterial ointment called bactroban® needs to be prescribed. If there are only one or two spots then the ointment is enough. In both cases it is important to treat areas such as under the fingernails and nasal passages with the bactroban a few times per day. Cover with a non adherent dressing if possible over the top of the sore. Note that topical disinfectants do not treat impetigo.\nIs it contagious ?\nTo ‘catch’ impetigo there needs to be direct contact with the infected sores. The child with impetigo will also commonly have the bacteria under the finger nails and on clothing. It is not transmitted in saliva or coughing or in bowel actions. Once treated for 24 hours, and as long as obvious sores are covered a child can return to daycare or school.\n- Separately wash clothes and bedding in hot water\n- Regular handwashing and keeping fingernails trimmed\n- Regular showering with an antibacterial soap such as phisohex®\nWhat about complications ?\nIf treated properly complications are rare.\n- Occasionally if not treated the infection can spread to the deeper tissues and cause an abscess, or more serious infection. This may require hospitalisation for more aggressive treatment\n- A rare complication following infection with the bacteria streptococcus is a kidney disease called glomerulonephritis. This is rare in urban Australia but is seen commonly in indigenous populations.\nWhat about recurrent infections ?\nIn some instances, people will have multiple recurrent staphylococcus infections. This is usually due to long term carriage of the organism in places that the antibiotics cannot reach. This includes the nasal passage, the fingernails and under the arms. Treating these areas is important, and sometimes it is necessary to treat the whole family. This can be particularly difficult problem, especially if the child has eczema.\n- For the nose apply bactroban 3 times per day for 5 days, then 2-3 times per month place some antibiotic ointment across the entrance of the nasal passage using a cotton bud.\n- Wash or bath using an antibacterial called triclosan, some examples include phisohex®, oilatum plus®, or QV flare up®. It is a good idea to also wash the hair with the triclosa, and leave for three minutes.\n- Treat all family members simultaneously\n- Do not share towels. Wash sheets and towels in hot water to prevent contamination\n- Discard antiperspirant rollers, or jars of moisturiser that may have been contaminated by a ‘finger’. Same goes for toothbrushes\n- Place combs brushes hair ties in the dishwasher, or get new ones and do not share these.\n- Remove all piercings whilst performing the above – for at least 5 days.\nThere is little evidence that recurrent infections are due to a weakness in the immune system or some lack in vitamins or minerals.","Throat Pain Due to Bacterial Pharyngitis: Symptoms, Treatment- Antibiotics\nWhat is Pharyngitis?\nInflammation of pharynx is termed as pharyngitis. Pharynx is located at the back of the throat and is the most common cause of throat pain or sore throat. There are two ways by which one can have pharyngitis i.e., viral and bacterial. In this article, we will focus on bacterial causes of pharyngitis, its causes, symptoms, prevention, and treatment.\nWhat Is Bacterial Pharyngitis?\nGenerally, pharyngitis is caused by viral infections. Less common but nevertheless pharyngitis is also caused by bacterial infections requiring antibiotic treatment. The most common bacterial infection of throat is Strep Throat, which is caused by Streptococcus-A bacterium. Some of the very rare causes of bacterial pharyngitis are gonorrhea, chlamydia, and corynebacterium.\nWhat Are The Forms Of Bacteria Responsible For Bacterial Pharyngitis Causing Throat Pain?\nThe most common form of bacteria responsible for causing pharyngitis is Streptococcus pyogenes. If there is a suspicion of exposure to such bacteria, it can be confirmed with routine lab tests. Such bacterial infections need to be treated with a course of antibiotics. If not treated appropriately, Streptococcus pyogenes pharyngitis can result in some sort of complications. In rare cases, bacteria apart from Streptococcus pyogenes are known to cause pharyngitis.\nSymptoms of Throat Pain Due to Bacterial Pharyngitis\nThe Common Symptoms Which Accompany Bacterial Pharyngitis Are As Follows-\n- White filming in the throat\n- Swelling of lymph nodes present in the neck\n- Appetite loss along with nausea\n- Loss of taste\n- General malaise\n- In some cases, there may be development of rashes, especially with Infectious Mononucleosis.\nInvestigations To Confirm Throat Pain Due to Bacterial Pharyngitis\nThe following are the investigations done generally if there is a suspicion of bacterial Pharyngitis-\nPhysical Exam for Throat Pain Due to Bacterial Pharyngitis\nIf an individual is having symptoms of pharyngitis, a physician does a careful inspection of the throat to look for white filming in the throat, redness, or swelling. The physician may also look for swollen lymph nodes in the neck area.\nThroat Culture for Throat Pain Due to Bacterial Pharyngitis\nThe next form of investigation is a throat culture where the physician may use a swab to take a sample of secretions and send it for culture in the lab. A physician can also do a rapid strep test which helps to identify, if the test is positive, for Streptococcus bacteria.\nBlood Tests for Throat Pain Due to Bacterial Pharyngitis\nApart from the above tests or if the above tests come back negative and the physician suspects some other cause for sore throat or throat pain then the physician may order a blood test. A blood test may identify whether you have mononucleosis. It can also be done to rule out throat infection due to other potential causes like measles, croup, whooping cough etc.\nHow Can I Prevent Contracting Bacterial Pharyngitis Resulting In Throat Pain?\nIf one follows the following steps then one can avoid getting bacterial pharyngitis.\n- Maintaining appropriate hygiene prevents pharyngitis in a big way.\n- Avoid sharing of food, drinks, and utensils from an infected individual.\n- Avoid coming in contact with infected individuals.\n- Wash hands especially before eating and after coughing or sneezing.\n- Using hand sanitizers.\n- Avoidance of smoking is essential or getting exposed to second-hand smoke.\nTreatment for Throat Pain Due To Bacterial Pharyngitis\nAntibiotics in the form of penicillin or amoxicillin are given for treatment of bacterial pharyngitis. These antibiotics are generally used to eliminate the bacteria and cut down the time when an individual is prone to spread the disease. Antibiotics also help prevent spread of bacteria to other parts of body. Antibiotics also prevent the immune system from being compromised due to bacteria.\nPrognosis and Possible Complications of Throat Pain Due to Bacterial Pharyngitis\nThe prognosis is usually good and the symptoms go away within a week to fortnight with use of appropriate antibiotics.\nWhen it comes to complications, they are rare in cases of bacterial pharyngitis, but still there are chances of some possible complications. If not treated appropriately with antibiotic medications, complications can result, especially if the infection starts spreading to other parts of the body. For e.g., the infection can spread to ears and sinus. It can also result in a condition called peritonsillar abscess. It is also noted to have compromised the immune system resulting in a potentially serious condition like rheumatic fever. Therefore, it is of utmost importance to treat bacterial pharyngitis with appropriate antibiotics.\nWhen Should I Seek Professional Medical Advice?\nMost cases of bacterial pharyngitis can be successfully treated with antibiotics, but one needs to seek medical advice if the following take place:\n- You have had sore throat with throat pain for more than a week.\n- You have high fever.\n- You have swelling of lymph nodes.\n- There is a development of rash."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:00e45d7c-4977-415b-83a5-cb4b66f3acb7>","<urn:uuid:d200bc53-d391-4f2f-b174-6faff126a526>"],"error":null}
{"question":"What are the potential health risks and environmental impacts of chlordane, and why was it banned from use in the United States?","answer":"Chlordane was banned in the US in 1988 due to several concerns. In laboratory studies, mice fed chlordane had higher rates of liver cancer than untreated mice. The chemical was found to be highly persistent in the environment, staying in some soils for over 20 years, and could build up in animal and fish fat. There were concerns about human exposure through contaminated food, including fish, shellfish, dairy, meat and poultry products. Chlordane can remain in the air of treated homes for up to 15 years after application and can contaminate water systems through runoff from urban areas where it was used for termite control.","context":["What is chlordane?\nChlordane is a man-made mixture of chemicals that was widely used as an insecticide in the United States (U.S.). Although no longer used, chlordane is very persistent and can still be found in some soils. Chlordane contains heptachlor, another persistent insecticide (see BCERF Fact Sheet #12, Pesticides and Breast Cancer Risk, An Evaluation of Heptachlor). The most common trade names for chlordane sold in the U.S. were Octachlor and Velsicol 1068.\nWhat is the history of chlordane's use?\nChlordane was used extensively as an insecticide in the U.S., from its introduction in 1947, through the 1980s. The most common use of chlordane was for termite control. It was poured or injected around foundations to protect homes and buildings from termite damage. Its use was especially high in areas where termites caused structural damage, such as the southern U.S. It was also used to kill insects in the soil, to prevent them from damaging food crops, gardens and turf, and was used as an herbicide to control weeds in turf. Another use was to prevent fire ants from building nests in power transformers. Chlordane's use on food crops was canceled in 1978 by the U.S. Environmental Protection Agency (EPA). Its use for protection of buildings and power transformers continued for another 10 years. In 1988, all commercial and domestic use of chlordane in the U.S. was banned by the EPA.\nWhy was chlordane use banned?\nLaboratory mice that were fed chlordane over long periods of time had a higher incidence of liver cancer than untreated mice. These results raised concerns about chlordane's ability to cause cancer in humans. Chlordane was also found to stay in the environment and build up in animal and fish fat. There was a concern that people may be exposed to this insecticide by eating food contaminated with chlordane, including fish, shell-fish, dairy, meat and poultry products. Its use was subsequently banned due to these concerns.\nIs chlordane still made commercially?\nChlordane is still made in the U.S. for export. Formulations containing chlordane are available internationally for termite control and wood treatment.\nHow do federal agencies regulate chlordane to protect the consumer?\nAll chlordane sales and use in the U.S. were canceled by the EPA in 1988. Since chlordane is still made in the U.S., the Occupational Safety and Health Administration (OSHA) regulates chlordane levels in the workplace. The EPA limits the amount of chlordane that can be released from any industrial source into waste waters. The EPA also sets the maximum level of chlordane allowed in drinking water. This \"maximum contaminant level\" for chlordane has been set at no more than 2 micrograms of chlordane per liter of drinking water (one liter is approximately one quart). The Food and Drug Administration (FDA) and the U.S. Department of Agriculture (USDA) monitor the levels of chlordane and its breakdown products in domestic and imported foods.\nWho might have been exposed to chlordane?\nPeople most likely to have been exposed to this chemical in the past are:\nHow can we be exposed to chlordane today?\nChlordane is very stable in the environment. It can remain in some soils for over 20 years. People digging around the foundations of buildings treated with chlordane in the past could still expose themselves to this highly persistent chemical. Children may be exposed by playing with chlordane-contaminated soil found near the foundations of chlordane-treated buildings and homes. Chlordane can be found in the air of some homes 15 years after its use. Because of extensive use of chlordane for termite control in urban areas, small amounts of chlordane in soil can be carried into water run-off, and contaminate river and lake beds where fish feed. Chlordane can build up and accumulate in the fat tissue of fish and shell fish that have lived in contaminated bodies of water. Those working in factories that make chlordane should follow current OSHA workplace protection guidelines to minimize their exposure to chlordane.\nDoes chlordane cause cancer in experimental animals?\nChlordane caused an increase in the incidence of liver cancer in male and female mice and thyroid cancer in female rats when it was fed to these laboratory animals over long periods of time. Chlordane may also act with other carcinogens to \"promote\" liver tumors in male mice. Diethylnitrosamine (DEN) is a known cancer-causing substance (carcinogen). Male mice that were given DEN in drinking water and then fed a diet containing chlordane, were twice as likely to develop liver tumors than when given DEN alone.\nDoes chlordane cause cancer in humans?\nThere is inadequate evidence to show that chlordane causes cancer in humans. In a few reports, chlordane exposures have been linked with cancer. But since most of the people who have been exposed to chlordane have also been exposed to other pesticides, establishing a clear link between chlordane exposure and cancer is difficult. In one study, an increase in deaths due to lung cancer was observed among pesticide applicators. However, there was no significant increase in deaths from any cancers among men who worked in chlordane-manufacturing plants. In another study, agricultural workers who handled chlordane and other pesticides were observed to have a higher risk of developing a type of cancer called non-Hodgkin's lymphoma. But other studies have not observed an increase in the risk for this cancer among chlordane-exposed agricultural workers. Unfortunately, similar studies on women exposed to chlordane through their work in agriculture or at chlordane manufacturing plants have not been done.\nDoes chlordane cause breast cancer?\nIn studies conducted so far, chlordane has not been directly linked with causing breast cancer in either animals or in humans. Since chlordane can build up in breast fat, three studies have looked at the levels of a chemical contained in chlordane mixtures (trans-nonachlor) and chlordane break-down products (oxychlordane) in the breast fat of women with and without breast cancer. The results of these studies have not been consistent. Two of these studies did not find significantly higher levels of chlordane or its breakdown products in the breast fat of women with breast cancer compared to women without the disease. Both of these studies tested small groups of women (20 or less). The one study that did find elevated levels of oxychlordane in women with breast cancer compared to women without breast cancer is of very limited value because of the very small number of women studied (only 5 in each group) and other problems in the design of the study.\nIt is difficult to make any definite conclusions from the very few small studies that have been conducted to date. Larger, more carefully designed studies are needed to further investigate whether higher body levels of chlordane and its breakdown products are associated with an increased risk of developing breast cancer. Several studies involving larger groups of women are in progress.\nHow may chlordane affect breast cancer risk?\nA woman's lifetime exposure to estrogen has been linked to increased breast cancer risk. Estrogen is a female hormone that helps control the reproductive cycles and breast growth. There is a concern that synthetic chemicals that act like estrogen or cause other chemicals to act like estrogen may increase a woman's risk of developing breast cancer. In one laboratory experiment designed to test for a chemical's ability to act like estrogen, chlordane did not act like estrogen when it was tested alone. There is currently no evidence that chlordane can enhance the estrogen-like effects of other environmental pollutants. The one study that had suggested that chlordane can enhance the estrogen-like effect of other pesticides is considered invalid because the results of this study could not be reproduced.\nAnother way a chemical may affect breast cancer risk is if it \"disrupts\" the way the body makes or breaks down estrogen. Estrogen can be broken down in the liver by several routes. One route yields a very weak form of estrogen that is excreted from the body. Other routes yield forms of estrogen that may be cancer promoting. Chlordane increases the rate of estrogen breakdown in the liver. However, scientists have not determined if chlordane causes breakdown of estrogen into a more or less cancer-promoting form.\nThe immune system of the body plays an important role in the body's defense against cancer. There is concern that chemicals that damage the immune system may affect cancer risk. The development of one part of the immune system has been shown to be adversely affected in young experimental animals that were exposed to chlordane before birth. However, these studies did not determine if the chlordane-exposed animals were more prone to develop breast cancer as adults. Therefore, more animal studies are needed to determine if chlordane-induced changes in the immune system can affect breast cancer risk.\nIs chlordane present in breast milk?\nSince chlordane can build up and be stored in breast fat, human milk can carry this chemical from a mother to a breast-fed infant. At the levels of contamination found in breast milk samples in the U.S., researchers have concluded that the estimated risk of cancer is far outweighed by the beneficial effects of breast feeding an infant. The amount of chlordane that an infant in the U.S. may receive from breast milk has been estimated to be well below the \"Allowable Daily Intake\" set by the World Health Organization.\nThere is not enough evidence to show that chlordane directly causes breast cancer in humans or laboratory animals. However, there is limited evidence that chlordane has the potential to affect breast cancer risk: it may affect estrogen levels in animals, compromise the animal's immune system and act with other carcinogens to \"promote\" liver tumors. Further studies are needed to determine if chlordane affects breast cancer risk through these mechanisms.\nWhere is more research needed?\nIs more research being done?\nThe National Institutes of Health (NIH) has recently funded two large new studies to determine any possible association between higher body levels of pesticides such as chlordane and breast cancer risk. One study based in California is looking at blood levels of a variety of persistent pesticides, including chlordane and its breakdown products, in African-American women. Another large scale study is being conducted on women residing in Long Island, New York to determine if chlordane exposure is associated with increased breast cancer risk. When the results of these studies are available, they will be included in any updated versions of this fact sheet.\nHow can I minimize exposure to chlordane that may still be in the environment?\nPrepared by Renu Gandhi, Ph.D., BCERF Research Associate\nand Suzanne M. Snedeker, Ph.D., Research Project Leader, BCERF\nWhen reproducing this material, credit the authors and the Program on Breast Cancer and\nEnvironmental Risk Factors in New York State.\nFunding for this fact sheet was made possible by the New York State Department of Health and the U.S. Department of Agriculture/Cooperative State Research, Education and Extension Service."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:119b8afc-f38f-41f8-97de-c799249f29c9>"],"error":null}
{"question":"What is more commonly documented in medical records: medication changes or cognitive impairment?","answer":"Medication changes are more commonly documented in medical records compared to cognitive impairment. While medication reconciliation is a standard component of care transitions and is specifically reviewed during post-discharge follow-up, cognitive impairment is frequently undocumented, with less than 9% of cases having documentation of cognitive impairment in the medical record, even when present.","context":["Transitions of Care: Optimizing the Handoff from Hospital-Based Teams to Primary Care Physicians\nAm Fam Physician. 2014 May 1;89(9):706-707.\nIn a recent study of Medicare beneficiaries who had been hospitalized, nearly one in five was readmitted within 30 days of discharge, and those with conditions such as congestive heart failure or chronic obstructive pulmonary disease had even higher rates of readmission.1 This results in an estimated cost of more than $17 billion to the federal government.1 A systematic review of interventions to improve the handover of patient care from hospital-based teams to primary care physicians found that multicomponent interventions that include medication reconciliation, use of electronic tools to facilitate communication, and shared involvement in coordinating follow-up care reduce rehospitalizations and improve patient satisfaction.2 Because the average length of hospitalization is four days for most general medicine patients, primary care physicians and hospital-based teams must partner from the moment of hospitalization to optimize the transition plan.\nThe effectiveness of hospital-based care transition programs is unclear. Although some programs reduced 30-day rehospitalization rates,3,4 a systematic review found that no single intervention is reliably helpful, and successful readmission reduction programs generally occur only in single institutions.5 However, it seems that programs that focus on the whole patient rather than a specific diagnosis are more successful in reducing readmissions.6 This concept is in keeping with the focus of primary care physicians. To solve the challenge of care transitions, the primary care physician should have a prominent role at three times: at admission, immediately after discharge, and at the postdischarge follow-up visit. For physicians who manage or participate in the hospital care of their patients, this task occurs naturally. This commentary addresses the role of primary care physicians who are not directly involved in the hospital care of their patients.\nAt the time of hospitalization, the primary care physician should contact and maintain communication with hospital-based clinicians. Ideally, an automated admission-discharge-transfer notification would be sent to the primary care physician via an electronic medical record system; however, other modes of communication from hospital-based clinician to primary care physician (e.g., telephone calls, e-mails, text messages, facsimile) may help start the dialogue. The primary care physician should discuss any concerns about the care plan, medications, medical history, and any social or family dynamics that may affect care, and should obtain an estimated date and destination of discharge.\nAlthough connecting with hospital-based clinicians at the time of admission is important, primary care physicians can have the most impact after the patient has been discharged, when postdischarge problems may be compounded without appropriate care coordination. To bridge the gap between discharge and follow-up, the primary care physician or a staff member with clinical knowledge can contact the patient 24 to 72 hours after discharge. This reconnects the patient and physician, and lets the patient know that his or her primary care physician has resumed care. The following items should be addressed:\nAny ongoing symptoms\nSelf-care plans developed by the hospital team\nMedications, particularly any that are new, or that have been changed or stopped\nArrangements for aftercare services (e.g., visiting nurse, home physical therapy)\nScheduling of and transportation to follow-up visits\nDuring the follow-up visit, the physician should focus on issues exclusive to the hospitalization. For example, the face-to-face encounter can be used to assess how the patient is recovering and to review the postdischarge care plan. Using patient engagement techniques such as “teach back” can ensure the patient's understanding of self-care plans.7 The physician should also conduct a thorough medication review and ensure that the patient understands any changes to the medication regimen. In addition, there may be outstanding issues to be addressed, such as test results that were pending at the time of discharge 8 or follow-up tests that were recommended during the hospitalization.9\nCaring for hospitalized patients and arranging their discharge plans have become increasingly complex, and patients may wonder what will happen to them next. Evidence-based information on understanding and improving care transition gaps may help health care organizations establish bundled programs for transition of care.10 Primary care physicians should have a prominent role in the transition of their hospitalized patients. Although these steps may require additional time and resources in an already busy practice, they are critical to keeping patients safe and healthy at home after discharge. Recent additions to Current Procedural Terminology codes 99495 and 99496 for transitional care management may support the primary care physician's care transitions efforts, and participation in alternative payment models (e.g., bundled payments, accountable care organizations) may provide the infrastructure for a successful office-based care transition program.\nREFERENCESshow all references\n1. Jencks SF, Williams MV, Coleman EA. Rehospitalizations among patients in the Medicare fee-for-service program [published correction appears in N Engl J Med. 2011;364(16):1582]. N Engl J Med. 2009;360(14):1418–1428....\n2. Hesselink G, Schoonhoven L, Barach P, et al. Improving patient handovers from hospital to primary care: a systematic review. Ann Intern Med. 2012;157(6):417–428.\n3. Hansen LO, Greenwald JL, Budnitz T, et al. Project BOOST: effectiveness of a multihospital effort to reduce rehospitalization. J Hosp Med. 2013;8(8):421–427.\n4. Jack BW, Chetty VK, Anthony D, et al. A reengineered hospital discharge program to decrease rehospitalization: a randomized trial. Ann Intern Med. 2009;150(3):178–187.\n5. Hansen LO, Young RS, Hinami K, Leung A, Williams MV. Interventions to reduce 30-day rehospitalization: a systematic review. Ann Intern Med. 2011;155(8):520–528.\n6. Dharmarajan K, Hsieh AF, Lin Z, et al. Hospital readmission performance and patterns of readmission: retrospective cohort study of Medicare admissions. BMJ. 2013;347:f6571.\n7. Schillinger D, Piette J, Grumbach K, et al. Closing the loop: physician communication with diabetic patients who have low health literacy. Arch Intern Med. 2003;163(1):83–90.\n8. Roy CL, Poon EG, Karson AS, et al. Patient safety concerns arising from test results that return after hospital discharge. Ann Intern Med. 2005;143(2):121–128.\n9. Moore C, McGinn T, Halm E. Tying up loose ends: discharging patients with unresolved medical issues. Arch Intern Med. 2007;167(12):1305–1311.\n10. Kim CS, Flanders SA. In the clinic. Transitions of care. Ann Intern Med. 2013;158(5 pt 1):ITC3-1.\nCopyright © 2014 by the American Academy of Family Physicians.\nThis content is owned by the AAFP. A person viewing it online may make one printout of the material and may use that printout only for his or her personal, non-commercial reference. This material may not otherwise be downloaded, copied, printed, stored, transmitted or reproduced in any medium, whether now known or later invented, except as authorized in writing by the AAFP. Contact firstname.lastname@example.org for copyright questions and/or permission requests.\nWant to use this article elsewhere? Get Permissions","Unrecognized Cognitive Impairment and Its Effect on Heart Failure Readmissions of Elderly Adults\nEditor's Note: Summary based on Agarwal KS, Kazim R, Xu J, Borson S, Taffet GE. Unrecognized cognitive impairment and its effect on heart failure readmissions of elderly adults. J Am Geriatr Soc 2016;64:2296-301.\nRationale for Study/Background: To determine whether 30-day readmissions were associated with presence of cognitive impairment (CI) more in elderly adults with heart failure (HF) than in those with other diagnoses and whether medical teams recognized CI.\nDesign: One-year prospective cohort quality improvement program of cognitive screening and retrospective chart review of documentation and outcomes. CI testing with the Mini-Cog test was completed within 48 hours before planned discharge.\nInclusion Criteria: Individuals aged 70 and older who were discharged home from the unit between January 27 and December 18, 2014, were eligible for the project. Individuals with any primary diagnosis of HF, independent of etiology, were included in the HF group, as were those with preserved or reduced ejection fraction.\nExclusion Criteria: Individuals were ineligible if they were being evaluated for or given a cardiac transplant or ventricular assist device; had a primary oncology diagnosis; had end-stage renal disease undergoing dialysis; or were scheduled to be discharged to hospice, skilled nursing facility (SNF), long-term acute care hospital, or nursing home.\nPrimary outcome(s): Investigators reviewed the EMR for hospital readmissions within 30 days of discharge.\nStatistical Analysis: Data were presented as means and standard deviations for continuous variables and numbers and percentages for categorical variables.\nResults: Mini-Cog scores were less than 4 (indicating CI) in 157 encounters (82 [67.7%] with HF, 75 [62.5%] without). Mini-Cog scores were similar in rate and distribution between groups. Individuals with HF and CI had a significantly higher 30-day readmission rate than did the other groups (26.8% vs. 13.2%; P = .01; HF, no CI, 12.8%; no HF, no CI, 13.3%; CI, no HF, 13.3%). In individuals with HF and CI, those with documented caregiver education had lower readmission rates than those without (14.3% vs. 36.2%; P = .03). Fewer than 9% had documentation of CI in the medical record.\nConclusion: Cognitive impairment (CI), which is frequently undocumented, may indicate greater risk of readmission for individuals with HF than those without. Screening for CI, adapting discharge for it, and involving family and caregivers in discharge education may help reduce readmissions.\nLimitations of study: These include inability to follow people for mortality after discharge, loss of 77 participants to the study because testing was missed or refused, and the retrospective collection of some data. In a prospective cohort study, it is difficult to sort out cause and effect.\nGeriatric perspective for the cardiovascular clinician: Even though cognitive impairment (CI) is a marker of severity of illness, this study importantly documents that substantial CI often goes undetected in the chart, and clinicians who focus on the care of older persons note that such lack of documentation usually means lack of detection. Importantly, the study also documents that CI in HF patients is associated with higher readmissions. The authors correctly point out that better detection can lead to better discharge planning and post discharge management. It is likely that coordinated detection and management of such patients could help reduce repeat hospitalization.\nClinical Topics: Cardiac Surgery, Geriatric Cardiology, Heart Failure and Cardiomyopathies, Invasive Cardiovascular Angiography and Intervention, Cardiac Surgery and Heart Failure, Acute Heart Failure, Heart Transplant, Mechanical Circulatory Support\nKeywords: Aged, Caregivers, Cognition, Disease Management, Heart Failure, Heart Transplantation, Heart-Assist Devices, Renal Dialysis, Geriatrics\n< Back to Listings"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:5cee4f34-d73b-4b2c-8d44-d64868135b6f>","<urn:uuid:12456685-138b-4b25-9fed-1495566efd8e>"],"error":null}
{"question":"How do the time limitations for filing discrimination claims compare between discrimination cases and unfair dismissal cases, and what are the qualifying criteria for each type of claim?","answer":"For discrimination cases, employees must file a charge with the EEOC or NERC within 300 days of when the discrimination occurred, and then have 90 days to file a lawsuit after receiving a Notice of Right-to-Sue. For unfair dismissal claims, employees must have completed one year of continuous employment in Northern Ireland and must commence their claim within three calendar months of dismissal. However, in cases of automatically unfair dismissal, such as discrimination, there is no qualifying period of continuous employment required. Additionally, for discrimination cases, private employers must typically have 15 or more employees to be subject to anti-discrimination laws, with some variations depending on the type of discrimination.","context":["Decoding the Complexities of DismissalPosted in : HR Updates on 29 March 2018\nDismissal is a taboo topic in the employment realm and has been since the emergence of employment legislation. As an HR Consultancy, the topic of dismissal is never too far from the workload at Think People. We have an endless stream of clients who have reasons to justify the dismissal of certain employees, however despite following a compliant process, they feel it necessary to seek reassurance from our HR Consultants to ensure that they are not breaching employment legislation or best practice procedure. The need to seek reassurance arises due to the serious implications of a breach at an Employment Tribunal.\nAside from the stigma around dismissal in general, are the complexities which surround the different types of dismissal. We often find that our clients tend to get confused with the categorisation of dismissals, which subsequently affects the choice of procedure they choose to deal with the situation. This article will provide a brief overview of the types of dismissal which are recognised within current employment practices.\nAn important starting point for anyone dealing with a potential dismissal is a knowledge of what constitutes a fair dismissal. The Employment Rights Act 1996 outlines a number of potentially fair reasons whereby an employer can justify dismissal. These fair reasons include:\n- Statutory illegality (this can include, for example, a truck driver who loses their driving license and thus cannot fulfil their duties)\n- Some other substantial reason (this can include the end of a fixed term contract where an individual is employed to cover a period of maternity absence and as such the temporary arrangement has come to an end)\nHowever, any dismissal will only be seen as fair if the employer acted reasonably throughout the dismissal process. Also in any dismissal case, an appropriate procedure must be followed by the employer prior to reaching an outcome of dismissal. Employers should take good cognisance of the LRA codes of practice. This means that the key things to be mindful about when contemplating dismissing an employee are\n- the reason you are opting to dismiss; and\n- the process you follow during the dismissal process.\nCompany policies and procedures are often put on a back burner whilst the day-to-day employment activities take priority. However, a robust disciplinary procedure informs all staff what is expected of them during their employment and how they will be dealt with should they decide to engage in misconduct. Thus, it is so important that all employers have clear and compliant disciplinary and dismissal procedures in place. All employees should be fully trained and be aware of the policies and procedures. Managers and HR should be knowledgeable on how to implement them.\nIt is also important that employers attempt to resolve situations prior to reaching the dismissal stage. Resolution can be reached by engaging in more positive approaches like further training or coaching, performance improvement plans and counselling. These are a few examples of the wide range of options which are available within an employer’s arsenal. A dismissal should ideally be the last resort and an employer should be able to demonstrate that they have exhausted all avenues with the employee.\nAs with outlining what counts as a fair dismissal, the Employment Rights Act 1996 also cements the concept of unfair dismissal. There are two specific conditions which must be met in order for a claim to qualify as unfair dismissal. A dismissal can be deemed unfair if the employer does not have an appropriate or justified reason for dismissal (like those highlighted above) and if the employer does not follow appropriate, compliant policies and procedures prior to the dismissal. The latter is also a key indicator of the importance of having robust and compliant policies in place, regardless of the size of the organisation and industry.\nThere are certain criteria that must be met in order to qualify for a claim of unfair dismissal, these include: the individual must be an employee, they must have completed the qualifying period of continuous employment (which is one year in Northern Ireland) and they must commence their claim within three calendar months of dismissal.\nThere is, however, an exception to the criteria which circumvents the need for an employee to have completed the qualifying period of continuous employment, and this is when the dismissal is automatically unfair. Automatically unfair cases cover a range of criteria but cases of discrimination could arguably be the most common. It is important to note that in cases of automatically unfair dismissal, such as discrimination, there is no qualifying period of continuous employment.\nA key case in the area of unfair dismissal cases is the case of Polkey v AE Dayton Services Ltd 1987. This case has brought about the 'Polkey Deduction', a rule followed by the employment tribunals.\nA ‘’Polkey Deduction’’ occurs when an employer has been found to have acted unfairly in dismissing an employee by failing to follow correct procedure. A tribunal will have to decide whether the award should be reduced based on the probability that a dismissal would have occurred anyway.\nIn the case of Autism Sussex Ltd. V Angel 2014 the employee was dismissed for falsifying her time sheets. The tribunal placed emphasis on the idea of fairness for a claim of unfair dismissal; fairness on the reason for dismissal and also fairness in the procedure which was followed. The employee won the claim against the employer due to the fact that the employer failed to carry out a reasonable investigation prior to the dismissal process and as such, the disciplinary process was procedurally unfair.\nCases like the above highlight the importance of procedure when dealing with dismissals. At Think People, we have seen situations where the appropriate reason for dismissal exists, however the company has not followed their formal disciplinary processes or dismissal procedures or such policies are not compliant and as such are falling at the final hurdle and come dangerously close to the daunting prospect of an unfair dismissal claim. In potential dismissal cases, employers need to demonstrate the following:\n- Genuine believe that it is a fair reason to dismiss;\n- Carried out proper and full investigations;\n- Followed relevant procedures ;\n- Informed employee in writing why they are being considered for dismissal and listened to their representations;\n- Afforded right to be accompanied;\n- Right of appeal.\nIn summary, in all misconduct dismissal cases the disciplinary panel need to demonstrate that they have tested the concept of reasonableness asking whether the employee could have expected to understand the consequences of their behaviour. If unable to demonstrate this then the employer is at risk of being unable to defend an unfair dismissal case.\nRelated article: Jill Gracey on the General Principles of Unfair Dismissal\nSummary dismissal is essentially where an employer dismisses an employee immediately and without notice nor pay in lieu. Summary dismissal normally occurs in matters of gross misconduct for very serious misconduct. Serious misconduct could be incidences of fraud or violence within the workplace, however each employer should have clear examples of what constitutes as gross misconduct within their disciplinary policies.\nSummary dismissal carries a certain amount of stigma as it is a risky pathway and it has been viewed as procedurally unfair. However, there are occasions where a summary dismissal is entirely necessary, the key lies in a succinct understanding of not only dismissal and its complexities, but also your company policies and procedures and terms of employment. If an employer chooses to summarily dismiss an employee, they must still follow a fair procedure. In order to summarily dismiss an employee, however, it must be an explicit term of their contract of employment. If summary dismissal is not an explicit term of a contract of employment, then the employer is bound to suspend on full pay in order to investigate.\nConstructive dismissal occurs when an employee does not want to leave, but is forced to resign from their position as a result of the employers conduct. This occurs when an employer breaches the terms and conditions of an employee’s contract. Examples of a breach of contract may be when an employer demotes an employee or decides not to pay an employee even though they have carried out the work and fulfilled their terms of the employment contract or when an employer changes the terms and conditions of a contract without prior consultation and mutual agreement. Breaches of contract can exist as a serious singular breach or can be a series of incidents which considered together amount to a serious breach.\nAs the employee has been forced to resign, it is interpreted as dismissal. The employee is entitled to terminate their employment with or without notice. However, the case of Western Excavating v Sharp 1978 outlined that an employee is only entitled to terminate their employment without notice in the event whereby a fundamental breach of contract has occurred. A fundamental breach occurs in a situation where the working relationship has become impossible and as such, the employee cannot continue to engage in the employment.\nThere is no overarching legislation which governs the concept of wrongful dismissal, it stems from common law. As such, wrongful dismissal is guided through the ever-changing spectrum of case law. Although there is no overarching governing legislation, the concept of wrongful dismissal has a long-standing position within employment practices.\nWrongful dismissal is often confused with unfair dismissal, however wrongful dismissal is an entirely different concept. Wrongful dismissal occurs when an employer decides to terminate an employee’s contract of employment, but in doing so, breaches the terms of the employee’s contract of employment. The emphasis of wrongful dismissal are the conditions which were agreed by both the employer and the employee upon beginning the employment relationship and whether the employer has failed to adhere to the agreed terms.\nAn example of wrongful dismissal can occur when the employer dismisses an employee but they do not adhere to the notice requirements as outlined within the contract of employment. A breach of contract works both ways, if an employee breaches an explicit term of their contract of employment, then the employer is entitled to dismiss them without notice or pay in lieu as they have broken their requirements of the contract.\nA key theme relating to dismissal which continually resonates in the advice that we give to our clients is the importance of knowing your policies and procedures and to understand the specifics of the situation that is at hand. It is the idea of stopping to think in its more pure form; do not give in to knee-jerk reactions as the implications could be costly.\nThe first step moving forward is to review your policies and procedures on a regular basis and make sure that all HR staff and managers are aware of them; they are there to guide through situations whereby a dismissal is necessary. If you are clear on what your rationale is for dismissal and you have a deep understanding of your, hopefully robust and best practice reflective policies and procedures, dismissal can be managed effectively and with confidence.\nMore on Disciplinary & Grievance\n- Ms. E. Gutfreund-Walmsley v Big Lottery Fund Limited \n- How should employers deal with SARs that relate to disciplinary and grievance procedures?\n- Can an employer refuse to pay an employee who is suspended pending investigation if they breach a term of their suspension?\n- Employee Performance Issues and Mental Health\n- Dealing with an employee's fluctuating performance levels\nThe information in this article is provided as part of Legal-Island's Employment Law Hub. We regret we are not able to respond to requests for specific legal or HR queries and recommend that professional advice is obtained before relying on information supplied anywhere within this article.","Federal and state laws prohibit employers from discriminating against employees based on the following characteristics:\nThe law also prohibits harassment that is based on these protected classes, and employer retaliation for asserting the right to be free of discrimination. These laws apply to hiring, promotion, job assignment, termination, and compensation.\nWhich Employees Does the Law Protect?\nFor most types of discrimination, a private employer is not subject to anti-discrimination laws unless it has had 15 or more employees for at least twenty weeks in the past year. For age discrimination, a private employer must have 20 or more employees, and with the Equal Pay Act virtually all employers are covered.\nWhat Evidence Is Required to Prove Employment Discrimination?\nThe best way to prove discrimination is direct evidence against an employer, such as a statement, e-mail, letter, or memo that shows the employer took action against you based on a discriminatory motive. In most cases, though, direct evidence isn’t available. Instead, you have to rely on circumstantial evidence to prove your case.\nCircumstantial evidence is evidence that would make a person believe discrimination occurred but is not direct proof of it. To win a case using either direct or circumstantial evidence, you need to show that a discriminatory reason more likely than not motivated your employer’s action against you. When circumstantial evidence is involved, a three-step test called the McDonnell Douglas framework is typically used.\nSteps for Proving Most Employee Discrimination Claims\nFirst, an employee must make a “prima facie” case of discrimination by showing that certain circumstances existed that would lead a person to presume that the employer’s action was based on discrimination.\nSecond, if the employee makes a prima facie case, then the employer must provide a legitimate, non-discriminatory reason for its action.\nThird, if the employer provides a legitimate, non-discriminatory reason, then the employee needs to show that the employer’s stated reason is merely “pretext,” or a cover-up, for discrimination.\nLearn more here and by clicking on the individual types of discrimination.\nWhat Remedies Are Available in a Discrimination Case?\nAn employee who wins a discrimination case may be entitled to the following, depending on the type of case and the severity of the discrimination:\n- Back pay: lost earnings resulting from the discrimination;\n- Front pay: lost future earnings resulting from the discrimination;\n- Benefits: lost benefits resulting from the discrimination, including health and dental insurance, pension or 401(k) plans, stock options, and profit sharing;\n- Compensatory damages: mental or emotional injuries (i.e. pain and suffering), medical expenses, job search costs, etc. resulting from the discrimination;\n- Punitive damages: a monetary award in malicious or reckless cases, which is designed to punish the employer and deter the employer from repeating its discrimination;\n- Attorney fees and costs: an award to cover attorney fees and costs, such as court fees and expert witness fees.\nThere are limits on the amount of compensatory and punitive damages available. Also, an employee who was discriminated against in hiring, firing, or promotion may be placed or reinstated to the desired position.\nWhat Protections Does the State of Nevada Have?\nNevada has similar laws that prohibit discrimination. The requirements depend on the type of discrimination. An employee may have a claim under both federal and state law, or, in some cases, just one or the other.\nWhat Is the Process for Filing a Discrimination Complaint?\nEmployees who wish to file a lawsuit against an employer for discrimination must first file a charge either with the Equal Employment Opportunity Commission (“EEOC”) or the Nevada Equal Rights Commission (“NERC”). There are a couple of exceptions. In most cases, an employee must file a charge with the EEOC or NERC within 300 days of when the discrimination occurred.\nThe EEOC or NERC will review the charge, investigate, and in most cases issue a “Notice of Right-to-Sue,” which allows an employee to file a lawsuit. In egregious cases of discrimination, the EEOC will file a lawsuit on an employee’s behalf; otherwise, the employee should hire an attorney.\nAn employee has 90 days to file a lawsuit after receiving the Notice of Right-to-Sue, with some exceptions.\nWhat to Do?\nContact me today if you believe you have a claim of employment discrimination, or if you are an employer who needs to defend against an employment discrimination claim.\n- What Is Required to Prove Discrimination in the Workplace?\n- What Are the Time Limits for Filing a Charge of Discrimination and a Lawsuit?\n- Which Employers Are Subject to Workplace Discrimination Laws?\n- Can I Request My Personnel File from My Employer in Nevada?\n- Can Nevada Employers Ask About Your Salary History?\n- Can Your Employer Discipline or Fire You for Using a Legal Product Off Duty?\n- Can an Employer Refuse to Hire an Applicant with a Criminal History?"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:d0023104-2c15-462f-93c4-bc8dc56bbb84>","<urn:uuid:8650fd93-a085-4d7f-a7a7-c7c944866cc7>"],"error":null}
{"question":"How do the arm movements differ between freestyle swimming and the dog paddle, considering their historical development and technical execution?","answer":"Freestyle (or crawl) and dog paddle have distinctly different arm movements. In freestyle, the arms move in a circular, windmill motion where they alternate - as one arm reaches forward, enters and pulls through the water, the other arm exits. The movement is performed on your stomach with your face in the water. In contrast, the dog paddle involves extending arms forward with hands alternating in a downward, circular pawing movement, mimicking a four-legged animal's motion. The dog paddle allows keeping the head above water, while freestyle requires turning the head to the side to breathe.","context":["The Top Ten Ways to Improve Your Triathlon Swims\nScroll down for Installments II and III (click on photos to enlarge).\nDoes this sound familiar?\nYou’ve done your laps at the pool diligently all winter long and have gotten into rather good shape; the best shape you’ve been in, in years! Sure your swim technique may not be the best, but still, you’ve done the work and can confidently swim a mile in the pool with no problem. You’re looking forward to your first Tri of the season. The gun sounds…\n• Scenario #1. Fifty yards into the swim… “Oh my God the water is cold. Crap, my goggles just fogged up. I can’t see anything except the blinding sun. Oh, God, there is a lot of thrashing going on around me! That jackass just kicked me in the face. OMG the water is cold, even in this new wetsuit, which by the way is tightening around my throat. I can’t even extend my arms. The Velcro strap is tearing away my flesh. I can’t breathe! I need air… #*!#%*…. I’m going to drown. PANIC. Where’s the life guard? Get me out of here.”\n• Scenario #2. Fifty yards into the swim… “I am so glad I warmed up. I fixed those foggy goggles and readjusted my wet suit; I got my heart rate up and my blood flowing. I am really feeling the buoyancy of this new wet suit. I am flying by all the rookies out here. There’s the first buoy. It’s time to churn.”\n#1. Avoid panic by warming up.\n- Panic, a frequent complaint in open water swims, happens to some of the strongest and ablest of swimmers. The best way to prevent panic is to get an adequate warm-up. The swim warm-up is the most disregarded of the essential pre-race activities. Unfortunately, the nature of the venue or the size of the field sometimes prevents triathletes from warming up, but if at all possible, get in, get moving and get acclimated to the cold. Not only will you get your cardiovascular system ready for the impending start, but you can test your goggles and wetsuit for mechanical problems.\nWhy is panic so common? There is a well-studied physiological response called the “mammalian diving reflex.” Simply stated, when a human or other mammal submerges their face in cold water, there is an immediate reflex which slows the heart rate and narrows the blood vessels in the extremities. This reflex conserves oxygen for the brain and enables diving mammals such as whales, dolphins and seals to stay submerged for prolonged periods. Unfortunately the reflex does not afford us the same advantage. Nevertheless, when starting a triathlon, the last thing you want to do is to lower your heart rate and constrict the flow of blood to your arms and legs. Without an adequate warm-up, the diving reflex will win out over your adrenaline and will deprive you of the oxygen you need. You will become anaerobic. Fear of drowning will overcome you and you will be in full panic mode.\nI’ve swum over 100 open water races in cold and warm water, in fresh and salt water, from 400 meters up to five miles. Panic still surprises me if I take shortcuts before an open water swim. It’s physiology. The diving reflex exists in our brain stem (or some dark recess) – a remnant from our primordial ‘aquatic’ past. Warm up and have a great swim!\nThe Top Ten Ways to Improve Your Triathlon Swims\nScroll down for Installment III (click on photos to enlarge).\n- #2. Swim horizontally in a streamlined manner – Water presents incredible resistance. To minimize this resistance (drag) you need to be as straight as an arrow. Your toes should be pointed and legs should be high in the water and close together. Keep your knees and ankles bumping each other while they float behind in the slipstream created by your head and torso.\n- Any body part outside the stream creates more drag. This includes the head when it’s held too high or during a breath, and the hips if they bend sideways. Most flaws in swimming technique directly affect the streamline position so you will hear more about this.\n- # 3. Don’t kick so furiously. Please stop kicking, especially with bent knees (bicycle kick). A powerful kick will certainly help good swimmers swim faster, but without good technique, kicking will slow you down, deplete your oxygen, and, ultimately, make you hate swimming. I tell struggling swimmers to stop kicking altogether and to practice with a pull buoy to improve your horizontal position and to eliminate your legs from the equation. Almost immediately, you will notice that it’s much easier to swim without the added resistance of thrashing legs and with the extra oxygen normally lost to them. Once the proper horizontal position is achieved, try to develop a proper kick which begins at the hips with propulsive forces traveling down the leg like a whip. It’s OK to use fins, but I recommend the longer fins rather than the short ones. Your knees will bend some, but the less so the better.\n- #4. Rotation – Just as in swinging a golf club or baseball bat, the power stroke in swimming is driven by rotation of the hips and is transmitted through the entire body from toes to fingertips. In addition to making it easier to breath because your head rotates with your body out of the water, the hip (and shoulder) rotation powers the underwater stroke where propulsion originates. When your right arm begins the pull, your right hip and shoulder should rotate out of the water powering the stroke. If you are swimming “flat”, without rotation, you will have less power and your body will naturally bend sideways at the hips to assist arm recovery creating more drag.\n- #5. Eliminate the arm crossover – An efficient “catch” initiates the power stroke. It should begin directly in front of your shoulder, not in front of your head, or worse, in front of your opposite shoulder. First problem: crossing over causes lateral movement of the hips which destroys your streamline, and 2nd, during the power stroke with a crossover, you push water sideways rather pulling and then pushing water from directly in front of you to directly behind you. The opposite problem, equally bad, is pulling too wide. During a proper pull, the path of your thumb should trace your midline. Do this with a slightly bent elbow, fingertips pointing down, but do not cross over the midline.\n- #6. Keep your head aligned with your spine. Triathletes who struggle with swimming complain that their legs sink. And they do! To overcome this, work on proper head position and use a pull buoy. Push your chest down. Your eyes should be looking straight down at the black line, not where you are going. (Open water swimming requires some additional visual skills – see below). The water line should be on the crown of your head, not on your forehead. Keep your forehead in the water at all times and keep one goggle in the water during the breath.\nTop Ten Ways to Improve Your Triathlon Swims\n#7. Breathe more often. You don’t hold your breath while running or biking. Don’t do it while swimming. If you breathe to one side only, then breathe every two or four strokes maximum. Right, left, is two strokes. You will be a lot happier with plenty of oxygen. If you breathe on alternate sides (a valuable skill) then every three strokes is great. If you are breathing every 5 or 6 strokes, then you are not working hard enough. Most important, exhale underwater and begin exhaling again as soon as your face is back in the water after your breath. Ventilate your lungs just as you would while running. Breathe in – breathe out – no breath holding (except while sighting – see below). Keep your head down and rotate your body to breathe. Your head will create a bow wave which creates an air pocket where you can breathe (see photo in Installment II). If you lift your head, your hips and legs will sink creating more drag and the air pocket will disappear.\n#8. Keep your elbow high during recovery, catch and pull. Recovery should be with a relaxed arm with elbow high and rotated body (see photo in Installment II). Once you initiate a catch, rotate that hip and shoulder downward to extend your reach underwater while keeping your wrist above your fingers, and your elbow above your wrist for maximum power. If you drop your elbow below your wrist, you’ve transitioned to dog paddle, which works OK for dogs, but not so much for humans. After the catch, when you begin your power stroke, that same shoulder and hip will rotate upward to drive the stroke.\n#9. Improve Your Open Water Sighting Skills. With a symmetrical stroke you can swim straight without sighting for considerable distance. Most of us do not swim symmetrically, so sighting is a key skill. Find something high on the horizon that you can spot in an instant. Don’t keep searching for buoys; they disappear in almost any wave action or turbulence caused by swimmers. Every ten to 20 strokes, depending on how straight you swim, take a quick glance and adjust. Do this by simply raising your head slightly without taking a breath using “alligator eyes” or by taking one or two “Tarzan” strokes with your head out of the water. If you are searching for a tall tree on the horizon or a house on a hill, then a quick glimpse is all you need to make adjustments. If the sun is shining, you can keep the position of the sun shining into the water as a reference point. Alternatively, you can draft off a comparable swimmer who will be doing all the work of sighting. If that swimmer is off course, you’ll discover that soon enough and you can move on to someone else.\n#10. Don’t just do lap swimming. Just as when you slog endless miles running slowly, endless laps may help you swim farther, but not faster. Work with a group in the pool, or if you must swim alone, go in with a plan and do interval workouts. Watch the clock and limit your rest. Break up the monotony with structured sets which vary in speed and distance. Plenty of workouts are available online. For example, go to http://swimopenwater.net/swim-workouts/Attention to these common swimming flaws will improve your swim overall, not necessarily making you faster right away, (that will come with your improved technique and increased swimming fitness), but it will get you out of the water with added confidence and energy and get you on your bike with an improved transition. Have a great swim!","No matter your age or skill level, learning to swim is a must -- it could save your life or the life of someone else. Propelling yourself through the water with confidence requires you to execute one of the six basic swimming strokes with proper technique. While some strokes have simple arm and leg movements, others offer advanced swimmers the challenge of timing and coordination.\nOften the first stroke a young swimmer learns, the dog paddle is very basic, easy to learn and allows you to keep your head above the water for easy breathing. The stroke mimics the movements of a four-legged animal as you propel through the water. You extend your arms forward and your hands alternate as they paw at the water moving in a downward, circular movement. Your legs extend backward with bent knees and your feet alternate in a quick up and down movement to perform a flutter kick.\nSometimes referred to as a back crawl, the backstroke is the fastest stroke performed on your back. Your arms alternate the pushing and pulling parts of the stroke with a circular, windmill motion. As one arm extends forward and enters the water, the other is exiting. Your legs alternate in an up-and-down motion, to perform the flutter kick. Your face is out of the water, which allows you to develop your own breathing pattern.\nSometimes referred to as the crawl, the freestyle is one of the fastest strokes. It is performed on your stomach with your face in the water and your whole body close to the surface. Similar to the backstroke, your arms move in a circular, windmill motion and you use the flutter kick. Your arms alternate, as one arm reaches forward, enters and starts to pull through the water, the other arm exists the water. To breathe, instead of lifting your head, turn it to one side to take a quick breath.\nThe sidestroke is performed while on your right or left side. The stroke starts with your bottom arm extending forward, above your head, your palm down and your top arm resting along your side. Simultaneously, pull your bottom arm backward in a sweeping, half-circular motion to the front of your chest, bend your top arm slightly and move it forward to the front of your chest. Your top arm then sweeps backward until it is fully extended. After a short glide you repeat the arm movements. The sidestroke uses a scissor kick -- you move your legs back and forth the way scissors open and close.\nThe breaststroke is more complex, requires precise timing and is performed on your stomach with your face in the water. Your arms extend forward, below the water, pull backward in an outward sweeping motion toward your chest and then extend again to glide and start the next stroke. As you pull your arms back, you lift your head to breathe. The frog kick is used and starts when your arms begin to reach forward to glide. You bend your knees, bring your feet up toward your body, move your feet outward and then extend and snap your legs together.\nThe butterfly stroke also requires precise timing in addition to coordination. You time the movement of your arms with a dolphin kick as your body undulates, similar to the way worms move, through the water. Lift both arms out of the water, reach forward, enter the water in front of your shoulders and pull your hands back through the water toward your feet. When your hands are near your thighs, lift your head to breathe, lift your arms out of the water and repeat the stroke. Take two dolphin kicks with each stroke. Keep your legs together, bend your knees and whip your feet downward.\n- Polka Dot Images/Polka Dot/Getty Images"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:c9280f3b-0489-48c0-b3af-86ce1986e207>","<urn:uuid:17e4bd7e-3ad2-4e77-a802-376a0e40c1b0>"],"error":null}
{"question":"Which treatment approach better addresses family involvement: 12-Step Facilitation or Family Therapy?","answer":"Family Therapy more directly addresses family involvement in addiction treatment compared to 12-Step Facilitation. Family Therapy allows family members to attend sessions together, helping the family unit heal broken relationships and rebuild trust. In contrast, 12-Step Facilitation focuses more on individual recovery through peer support groups and acceptance of the need for abstinence. However, both approaches recognize addiction's impact on families, as addiction is often referred to as a 'family disease' that affects not only the person with substance use disorder but also those surrounding them.","context":["Resources for Families and Loved Ones\nAddiction is a disease that not only affects the person with the substance use disorder (SUD), but also those surrounding them. As a friend or family member to someone who is abusing drugs or alcohol, you could be confused, frustrated, stressed, scared, and unsure of how to help the person you love.\nAlthough you cannot force your loved one to change their ways, there are ways to help them and to take care of yourself during this difficult time.\nEffects of Addiction on the Family\nIt’s no surprise many people refer to addiction as a “family disease.”\nWhile your loved one is probably struggling with many side effects of their addiction, there’s a chance that you’re contending with a lot too, such as:1\n- Economic strain due to helping your loved one pay their bills, legal costs, or deal with other expenses.\n- Emotional hardship as a result of feeling angry, anxious, worried, depressed, guilty, or embarrassed.\n- Unhappiness with your relationship with your loved one.\n- Familial instability, perhaps resulting from abuse or violence.\nWhat may make it harder is the desire to help your loved ones with the difficult consequences of their drug or alcohol use, though in some cases, this may be enabling the continued use of their substance of choice.\nFamily Member Resources\nRemember, you’re not alone in your struggles with managing a loved one who is abusing drugs or alcohol. You can turn to your support network for help, and you can reach out to many organizations that assist families and loved ones of those with substance use disorders.\nWe are also here for you at Recovery First. On this page you’ll find several different guides to help you learn about addiction, treatment, and recovery, as well as how approach your loved one about their substance abuse. Also, you can reach our Admissions Navigators at any time of the day or night at with questions, concerns, and solutions to how to get your loved one the best care for their situation.\nWe’ve created several guides for family members on the topic of helping a loved one with a substance use disorder. You can find them below:\n- Recovery First’s Guide for Families\n- Guide for Parents\n- Guide for Spouses\n- Guide for Children\n- Guide for Friends\n- Guide for Coworkers/Managers\nIn all of the guides above, you’ll be able to find helpful information regarding:\n- What substance abuse can look like.\n- Discussing drug or alcohol abuse with your family member.\n- Establishing boundaries in your relationship with your loved one.\n- Addiction treatment options.\n- The differences between inpatient/residential treatment facilities and outpatient program.\n- Different approaches to pay for treatment.\n- How you and your loved one’s support network can help their recovery.\nWhat to Remember During This Difficult Time\nIt may be hard to see the forest through the trees when it comes to your loved one’s addiction, but it’s important to remind yourself that you are not the cause of their addiction, nor can you control or cure it.2\nAddiction is a brain disease. Long-term substance abuse and addiction can change your loved one’s brain chemistry: drug or alcohol abuse over time can compromise their ability to choose, and drug use can become compulsive.3 You did not cause the substance abuse.\nWith that in mind, you can help. Encourage them to see a doctor. Provide them with details about treatment. Let them know how their drug or alcohol abuse is affecting your family.\nIt’s not easy to have these conversations—there’s a good chance your loved one will be defensive. But with your support, and the help of medical professionals and treatment, they have a chance at recovery.\n- Daley, D.C. (2013). Family and social aspects of substance use disorders and treatment. Journal of Food and Drug Analysis, 21(4), S73-S76.\n- Substance Abuse and Mental Health Services Administration. Remember the 7 Cs.\n- National Institute on Drug Abuse, National Institutes of Health, U.S. Department of Health and Human Services. (2012). Principles of drug addiction treatment, a research-based guide (third edition).","Addiction Treatment Therapies\nThose who need treatment for substance use disorders may be unaware of what the whole rehabilitation process entails—including the full range of potential therapeutic offerings.\nSome people may equate rehab with some form of medical detox, but detox alone is not a substitute for more comprehensive, longer-term addiction treatment.1 Continued treatment can help prevent relapse and lower relapse-related risks, such as overdose.1,2 Ongoing addiction treatment is likely to consist of several therapeutic approaches combined with the intention of increasing treatment engagement, changing maladaptive behaviors, and ultimately helping people achieve long-term abstinence.1\nCognitive Behavioral Therapy\nWhile originally utilized to treat depression, cognitive-behavioral therapy (CBT) was later adapted to help prevent relapse in people with substance use disorders such as alcoholism and cocaine addiction.3,4 The treatment method, and its application in addiction medicine, is based on the theory that individuals can best modify their substance use behavior by identifying learned patterns linking specific contexts, thoughts, and feelings as they relate to maladaptive behaviors (in this case, drug and/or alcohol use).4\nIn recognizing these patterns, people may begin to “unlearn” them. They may work toward these ends through the development of better coping skills—such as by avoiding high-risk situations, places, people, or other triggers, as well as by improving their ability to manage cravings and more-positively react to negative thoughts/moods.3,4\nRecovering from addiction isn’t just about working on the issues that brought someone to treatment, but about managing ones yet to emerge in the future. Patients need to be prepared for what awaits them post-treatment, and CBT techniques can help with that. While it would be ideal to altogether avoid triggers and other stressors, it’s not always possible. Armed with a set of improved coping skills learned through cognitive-behavioral approaches, individuals leaving treatment may be better prepared, should they encounter a new trigger, to take a step back and think about how to proceed before they act. That’s really what CBT is all about—retraining a potentially maladaptive pattern of thoughts and behaviors to a more healthy, positive one.\nMotivational Interviewing (MI) approaches clients in a manner that allows them to come to terms with the reasons they are in treatment and what they need to address on their own to make progress in recovery. This guided process uses specific methods of questioning clients so they can come to terms with what has been stopping them from moving forward with the changes they desire to make.\nIn many cases, continuing to drink or use drugs and getting help for the underlying substance use disorder(s) represent opposing desires for individuals in treatment. Using motivational interviewing techniques, therapists work with their patients to help resolve this ambivalence in favor of reinforcing the resolve and recovery efforts made to quit.3 There are four core principles of motivational interviewing. They are:5\n- Express empathy—Rapport may be better developed with the patient in recovery by demonstrating empathy. Therapists repeat patients’ statements back to them through reflective listening in order to convey that they are present and understand how they feel in terms of ambivalence to treatment.\n- Develop discrepancy—Therapists subtly point out how current patient behaviors may interfere with the positive changes they want to see happen in their lives. The perception of this discrepancy can motivate change\n- Roll with resistance—Therapists use compassion and empathy in the face of resistance, rather than confrontation or demands.\n- Support self-efficacy—Therapists can boost self-esteem and self-confidence of the individual in treatment. Believing in one’s own ability to make healthy changes helps motivate them to realize their goals.\nEye Movement Desensitization and Reprocessing\nEye Movement Desensitization and Reprocessing (EMDR) therapy is a type of psychotherapy for people with post-traumatic stress disorder, or PTSD. Its therapeutic potential comes from its ability to reduce the emotional distress associated with certain traumatic memories.6 Such a therapeutic effect may also be helpful when applied to cases of co-occurring substance use disorder and trauma-related mental health conditions.\nThrough EMDR, patients in recovery work to replace negative emotional reactions to specific problematic memories with less-charged ones or, even, positive ones.6 The person in treatment will be asked to recall specific traumas while performing a pattern of repetitive eye movements (or, in some cases, sequences of finger tapping or musical tones), and this “dual stimulation” helps that individual to work against the negative feelings that the trauma evokes.6\nA positive, replacement thought becomes the focus while the client engages in the repeated behavior for 20-30 seconds or however long the therapist deems necessary. After this process is complete, the client discusses the experience with the therapist. The EMDR process is then repeated as long as negative feelings remain present in an effort to retrain the mind in how to respond to the initial traumatizing thoughts.\nDialectical Behavioral Therapy\nDialectical Behavioral Therapy (DBT) is a cognitive-behavioral treatment approach initially developed to help decrease the self-harming and self-defeating behaviors commonly encountered in individuals with borderline personality disorder. This therapeutic approach has since been adapted to treat people with various other mental health issues, including substance use disorder.3,6 DBT combines the emotional regulation of CBT with the teaching of mindfulness skills, distress tolerance, and improved interpersonal effectiveness with the goal of decreasing continued substance use.3\nExpressive, Holistic and Complementary Therapies\nArt therapy and music therapy are both forms of expressive therapy used at some treatment centers to complement some of the more standard approaches to substance abuse rehabilitation. Neither require the client to be artistically inclined. Rather, individuals learn different art techniques through classroom sessions that focus on giving participants healthy and positive ways to relieve stress while doing something they enjoy.\nThese therapies are less about the end product—the musical performance or the finished artwork—and more about the creation process. Oftentimes, various feelings and issues may come up in the creation process that may be further explored in individual therapy.\nOther complementary approaches that may be encountered at certain treatment centers:\n12-Step Facilitation / Recovery Groups\nActive participation in peer support or self-help groups such as Alcoholics Anonymous, Narcotics Anonymous, or other 12-Step program varieties is an important element of many long-term treatment strategies. 12-Step facilitation, as a therapeutic intervention, strives to motivate acceptance of the need for abstinence and foster a willingness to actively engage with 12-Step fellowship programs as a means of lasting recovery.3,7\n12-Step meeting attendance provides support for those in recovery from substance abuse and behavioral addictions. These and other mutual support groups (e.g., SMART Recovery) are common components of many individuals’ aftercare routine, following completion of formal, comprehensive rehabilitation.\nIndividual, Group and Family Therapy\nBehavioral therapies—a grouping that may encompass the modalities outlined on this page as well as additional ones not covered here—are the most commonly utilized forms of substance abuse treatment.8 While individual treatment program designs will vary, these types of therapies are likely to be administered in some combination of individual, group, and/or family counseling settings.\nIndividual therapy provides important one-on-one time with a therapist or counselor. Solo sessions with a therapist or counselor create a safe space to share personal issues, and can further serve as opportunities to more closely evaluate recovery progress and allow adjustments to be made to the treatment plan, when needed. In addition, coping skills that will help the client to avoid relapse beyond the treatment period can be honed during these sessions.\nGroup settings may constitute much of the total time spent in therapy while completing a rehab program. The group dynamic avails the wisdom gained by others on similar recovery journeys and can facilitate better interpersonal interaction as a recovering individual prepares to enter the “real world” at the completion of the treatment period.\nThere are many ways family members can give and receive support during their loved one’s treatment experience, and family therapy is one of them. Attending sessions together allows the family unit to begin healing broken relationships and build a solid foundation of trust once again.\nAddiction treatment pharmacotherapy—sometimes referred to as medication-assisted treatment (MAT)—constitutes the standard of care for certain types of substance use disorders, including opioid use disorder and alcohol use disorder. MAT combines FDA-approved pharmacotherapies (and, in some cases, off-label uses for other medications) with some combination of behavioral therapeutic interventions (such as the others mentioned throughout this article) to help people with long-term abstinence in recovery.\n- National Institute on Drug Abuse. (2018). Principles of Drug Addiction Treatment: A Research-Based Guide (Third Edition)—Preface.\n- Partnership for Drug-Free Kids. (n.d.). Risks for Relapse, Overdose and What You Can Do.\n- Miller, S. C., Fiellin, D. A., Rosenthal, R. N., & Saitz, R. (2019). The ASAM Principles of Addiction Medicine, Sixth Edition. Philadelphia: Wolters Kluwer.\n- National Institute on Drug Abuse. (2018). Principles of Drug Addiction Treatment: A Research-Based Guide (Third Edition)—Cognitive-Behavioral Therapy.\n- National Institute on Drug Abuse. (2012). Motivational Interviewing Assessment: Supervisory Tools for Enhancing Proficiency.\n- National Alliance on Mental Illness. (n.d.). Psychotherapy.\n- National Institute on Drug Abuse. (2018). Principles of Drug Addiction Treatment: A Research-Based Guide (Third Edition)—Twelve-Step Facilitation Therapy.\n- National Institute on Drug Abuse. (2018). Principles of Drug Addiction Treatment: A Research-Based Guide (Third Edition)—Principles of Effective Treatment."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:3bcfc47d-cb2b-4603-9148-8f8efb3dde81>","<urn:uuid:e16dcf6a-55ca-4f27-af60-6202b8553d40>"],"error":null}
{"question":"I'm designing a power system. What's the advantage of using higher voltage for submersible motors?","answer":"Using higher voltage for submersible motors allows for smaller and less expensive drop cables. For example, a 200 hp motor operating at 2,300 volts requires only 49.4 amps compared to 247 amps at 460 volts. This means you can use much smaller cables (e.g., #8 size instead of 250 MCM cable for a 500-foot setting depth), making installation easier and reducing copper costs, despite needing better insulation.","context":["Learn about issues that can arise with submersible motors.\nSubmersible motor use in municipal systems is not new. The advantages of submersible motors are well known to this industry. Since these motors are placed deep into the earth, in a relatively stable environment, they are immune to the weather and environmental factors that plague hollow shaft motors driving conventional line shaft turbines (LSTs). Submersible motors and pumps eliminate the drive shaft and bearing systems of LSTs, thus reducing the mechanical complexity and required maintenance. Oil does not drip into the well for bearing lubrication. Wells can be located adjacent to housing areas. Submersibles also do not require structures to enclose them and do not produce surface noise.\nStandard water well types of submersible motors are water (water/glycol) filled and rely on water as the internal lubrication for the motor. These motors are extremely reliable when applied within their design limits of temperature, hydraulic loading and power requirements. Typical agricultural, domestic and municipal systems are excellent applications for these motors. Unfortunately, these motors often are used in applications that unknowingly exceed the design criteria of the motors. As a result, failures occur, and the advantages of submersible motors are lost or quickly forgotten.\nThe following problems are typical in municipal applications and result in the failure of motors.\nA very common problem affecting submersible motors is over-temperature. Causes for over-temperature include pumping hot water, overloading of the motor by the pump, loss of cooling flow past the motor, ochre or scale buildup and frequent motor starts and stops.\nSubmersible motors somehow must cool themselves. This is accomplished almost universally by transferring the motor's internally generated heat to the water that is flowing past the motor and into the pump. Most standard water well motors are designed to do this but add little safety margin (safety margins add cost).\nIn submersible motors, the thrust bearing supports the pump's thrust weight of the water column being lifted by the pump. In standard water well motors this thrust bearing is a water lubricated \"Kingsbury\" type of bearing. A very small film of water between the main elements of the thrust bearing provides lubrication between the two bearing surfaces. If the motor overheats for any reason, this water film can approach its boiling point. If it boils, the lubricating film is lost. At this point, the bearing surfaces come into contact with each other and rapid heating takes place. Catastrophic failure of the thrust bearing is likely to occur.\nStator failure is another problem that occurs when motors overheat. Typical wet wound, water-filled submersibles use a PVC insulation that insulates the copper windings while immersed in water. This wire usually has a maximum usable temperature of between 70º C for standard motors to about 100º C for higher temperature motors. Once these temperatures are exceeded, the insulation system is damaged and a winding turn to turn, winding phase to phase or winding phase to ground fault becomes likely. Once these faults develop, failure of the motor is unavoidable.\nAnother problematic area for submersibles in municipal applications is hydraulic shock loading or water hammer. Water hammer occurs when a rapidly moving column of water encounters an obstacle or suddenly changes velocity. The use of multiple pumps on a common supply manifold is a prime cause of water hammer. When a pump turns on or off, water hammer is generated. Additionally, when any kind of valving is actuated, water hammer can occur. (Fast acting, electromechanical valves can be the worst.)\nCheck valves in the pump discharge string and at the well head are recommended by all manufactures to reduce water hammer. Unfortunately, check valves may be of the wrong type (swing vs. spring-loaded), may be simply not used or may become corroded over time (spring corrosion is a common problem).\nWhen water hammer occurs, there is a sudden down thrust transmitted through the pump onto the motor's thrust bearing. This can cause several undesirable things to happen.\nMost standard water well motors use some type of rotating carbon thrust bearing running on stainless steel thrust pads. Carbon is an excellent material for bearings when the lubricant is water and water hammer is not severe. Carbon, while very hard and durable, is a brittle material. When hard, brittle materials are subjected to shock loading they can shatter. Extreme water hammer is known to completely shatter thrust bearings, causing motor failure.\nStandard thrust bearings in water-filled motors are lubricated by a thin water film. Water has a low viscosity. When put under a shock load, the water film in the bearing can be driven from between the bearing surfaces resulting in insufficient lubrication. This condition can lead to hot spots, excessive wear and bearing failure.\nThe seals at the motor's shaft keep the well fluid from getting into the motor. Both water-filled and oil-filled motors need to do this in order to keep abrasives out of the internal motor bearings. Additionally, in aggressive water, the low pH of acids or high salt levels can cause internal corrosion if allowed into the motor.\nStandard water well duty submersible motors use a single mechanical seal. This seal can be of carbon/ceramic or silicon/carbide. Silicon carbide is considered a premium seal and is a harder seal than carbon ceramic. It is very useful in sandy well applications.\nSubmersible motors, like any electric motor, require a good voltage supply at the motor terminals. A leading cause of submersible motor failure is under-voltage or voltage spikes.\nUnder-voltage typically is caused by sizing the drop cables (supply cables) too small or by the utility grid supplying low voltage to the site. Ohms law is always in effect in electrical circuits (V=I ¥ Z). Voltage equals the current flowing in a circuit multiplied by the impedance (the resistance of a cable is usually the majority of the impedance).\nAs smaller diameter drop cables are used (they are cheaper), the resistance increases (less copper = higher resistance). As the resistance of the lead goes up, the voltage drop in the lead goes up as well for a given operating current.\nFor a fixed supply voltage from a transformer, as the voltage drop in the lead goes up, the terminal voltage at the motor must go down. As the voltage at the motor terminals fall, the motor must have more current to produce the same horsepower (which induction motors want to do). Eventually, too much current is flowing for the size of winding wires used in the motor and internal overheating begins to take place. This leads to failure of the motor.\nIn large motors commonly used in the municipal industry, large power cables typically are required and used. If the setting of the motor and pump are deep, very large and very expensive cables must be used in order to provide the rated voltage at the motor's terminals.\nAn economical alternative to this is to design the motor to operate at higher voltages. If a motor is designed to operate at higher voltages, the required full load current will go down. As the current required goes down, the size of the drop cable can be reduced as well. Smaller drop cables use less copper and are less expensive-sometimes much less expensive.\nConsider a 200 hp motor. If the motor is designed for 460 volts, the full load current is 247 amps. If, however, the motor is designed to operate at 2,300 volts, the full load current falls to 49.4 amps.\nIn this situation, if a 500 foot setting depth is used, the recommended drop cable goes from a size 250 MCM cable for the 460 volt motor to a #8 size cable for the 2,300 volt motor. Higher voltage cable and components with better insulating values must be used, but the cost of these typically is more than offset by the savings in the copper of the drop cable. Smaller drop cables also are much easier for the installation crew to handle.\nA very serious problem for all induction motors is voltage spikes. These spikes typically are very short in duration and high in voltage. These spikes can be generated by lightning, other motors turning off or utility switch-gear opening.\nIn municipal well fields with multiple pumps and motors running, this can be a serious problem. Each time one of these motors is switched off, the stored inductive energy in the magnetic circuit of the motor is dumped back onto the power lines. This creates a very short but very high voltage spike.\nThese spikes often are of a greater voltage level than the motor's insulation system can tolerate and will burn through the insulation. Once this occurs, the motor has a potential site for an internal short and current can flow in a path that it was not designed for. Typically, this will raise the temperature of the windings near the shorted spot and further burning will take place until a catastrophic failure occurs.\nIf the voltage spike is large enough and has enough energy (e.g., lightning strike), it can completely burn a hole in the case of the motor.\nMotor manufactures can do little to protect against severe voltage spike problems. For this reason, external surge protection mounted near the motor starter is recommended. This tends to \"clip\" the voltage spike before it can travel down the cables to reach the motor. These surge protectors do nothing until the voltage reaches a certain critical value. At that point, they begin to conduct current and continue to do so until the voltage falls below the critical value. Large power distribution transformers that feed these motors also should be protected because these large voltage spikes can damage them as well. Warning: Consult with local suppliers or engineers for properly selecting and applying these devices.\nThese are the typical problems that occur in municipal operations using submersible motors. Failures never seem to occur at a convenient time. They are always a nuisance at best and more often a major problem."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:06f002b7-232e-495b-af5e-3b256efc4d8b>"],"error":null}
{"question":"What are the key mission transitions of USS Princeton compared to USS Peleliu's operational focus?","answer":"USS Princeton underwent several mission transitions, starting as an attack carrier (CVA-37), then converting to an antisubmarine aircraft carrier (CVS-37), and finally becoming an amphibious assault ship (LPH-5) carrying helicopters and marines. In contrast, USS Peleliu is described solely as an amphibious assault ship (LHA 5) serving as the flagship for the Peleliu Amphibious Ready Group with embarked Marine Expeditionary Unit.","context":["U.S. 5TH FLEET AREA OF RESPONSIBILITY\n- Sailors and Marines aboard amphibious assault ship USS Peleliu (LHA 5) participated in the Campbell’s Cup Sumo Challenge, Jan. 6.\nThe Campbell’s Cup Tournament is a deployment-long competition between teams of Sailors and Marines from around the ship. The tournament challenges the teams to feats of strength and stamina like the Sumo Challenge and the Pull-Up Competition that took place in late November.\nThe Sumo Challenge, which was sponsored by Morale, Welfare and Recreation (MWR), had 16 contenders battling for the title of, “Sumo Master.” The participants included officers and enlisted service members from different divisions throughout the ship.\nThe tournament ran off a bracket system where the winner from each match moved on to the next level. To win a match, one contestant had to win two out of three rounds by either throwing or knocking their opponent to the ground or, in some cases, push them off the mat.\n“We had the rules set up specifically, and we said, ‘Ok, this is how the rules are going to be,’ but the crowd was not ok with just pushing out of bounds,” said Electronics Technician 3rd Class Andrew Hoyt, master of ceremonies for the event. “They wanted to see people go down. So, as the person running it and being the MC, you have to give the people what they want.”\nHoyt is also a part of the MWR committee that helped coordinate the competition.\nHoyt added, “This is for the crowd. This is for the people. If they want a contest where out of bounds does not count and you have to bring them to the ground, then I’m not going be the guy telling them no.”\n“The original point [of the Sumo Challenge] was overall just to have a lot of fun, bring people out, and get them out of their workspaces,” said Hoyt. Judging by the size of the crowd, the competition did just that. The turnout was more than anyone expected.\n“It was really surprising to me that so many people showed up. We had an entire division, the AOs [aviation ordnancemen], that came out,” said Aviation Support Equipment Technician 3rd Class Maurice Williams, another MWR committee member that helped plan the event. “We even had a couple of engineers that came up, and a few officers. I felt great about this event. I think we pleased everybody.”\nAviation Ordnanceman 3rd Class Heather Zicopula “sumo-ed” in the first match of the evening.\n“It was awesome. You really felt a part of the ship jumping into one of those big giant suits. I felt bonded to everybody here,” said Zicopula.\nZicopula said she’s been to other MWR events, but she has never participated before. “You can definitely tell there’s been a bigger turnout. Everybody is starting to get into a routine and get bored so they’re trying to find ways to socialize.”\nWilliams and Hoyt said they are excited for the future MWR events they are planning. Some of the events in the works include the Peleliu Derby Boxcar Race, a 3-on-3 basketball tournament, and a Magic the Gathering™ fantasy card game tournament.\nThe MWR committee recognizes that it will be a lot of work to organize these events and come up with new ideas to keep things from getting boring, but Williams said he’s not afraid of a little extra work.\n“At the end of the day, it’s not work to us,” said Williams. “What we do and what we try to give to the Sailors and Marines here on the ship it’s not work. We thoroughly enjoy it and that’s why we consistently try to crank out new ideas and keep it fresh and exciting.”\nHoyt and Williams said they hope to see more new faces at the MWR events in the future.\nPeleliu is the flagship for the Peleliu Amphibious Ready Group and, with the embarked 15th Marine Expeditionary Unit, is deployed in support of maritime security operations and theater security cooperation efforts in the U.S. 5th Fleet area of responsibility.","USS Princeton (CV-37)\nUSS Princeton underway\n|Career (United States)|\n|Namesake:||Battle of Princeton, 1777|\n|Operator:||United States Navy|\n|Builder:||Philadelphia Naval Shipyard|\n|Laid down:||14 September 1943|\n|Launched:||8 July 1945|\n|Commissioned:||18 November 1945|\n|Decommissioned:||21 June 1949|\n|Recommissioned:||28 August 1950|\n|Decommissioned:||30 January 1970|\n|Renamed:||PCU Valley Forge to PCU Princeton|\n|Reclassified:||CVA-37, CVS-37 and LPH-5|\n|Fate:||Sold for scrap in 1971|\n|Class and type:||Essex-class aircraft carrier|\n27,100 tons standard\n888 feet (271 m) overall\n93 feet (28 m) waterline\n28 feet 7 inches (8.71 m) light\n8 × boilers\n4 × Westinghouse geared steam turbines\n4 × shafts\n150,000 shp (110 MW)\n|Speed:||33 knots (61 km/h)|\n|Complement:||3448 officers and enlisted|\n4 × twin 5 inch (127 mm)/38 caliber guns\n4 × single 5 inch (127 mm)/38 caliber guns\n8 × quadruple Bofors 40 mm guns\n46 × single Oerlikon 20 mm cannons\n4 inch (100 mm) belt\n2.5 inch (60 mm) hangar deck\n1.5 inch (40 mm) protectice decks\n1.5 inch (40 mm) conning tower\n|Aircraft carried:||As built:\nUSS Princeton (CV/CVA/CVS-37, LPH-5) was one of 24 Essex-class aircraft carriers built during and shortly after World War II for the United States Navy. The ship was the fifth US Navy ship to bear the name, and was named for the Revolutionary War Battle of Princeton. Princeton was commissioned in November 1945, too late to serve in World War II, but saw extensive service in the Korean War, in which she earned eight battle stars, and the Vietnam War. She was reclassified in the early 1950s as an attack carrier (CVA), then as an Antisubmarine Aircraft Carrier (CVS), and finally as an amphibious assault ship (LPH), carrying helicopters and marines. One of her last missions was to serve as the prime recovery ship for the Apollo 10 space mission.\nAlthough she was extensively modified internally as part of her conversion to an LPH, external modifications were minor, so throughout her career Princeton retained the classic appearance of a World War II Essex-class ship. She was decommissioned in 1970, and sold for scrap in 1971.\nConstruction and commissioning\nThe ship was laid down as Valley Forge — one of the \"long-hull\" Essex-class — on 14 September 1943 at the Philadelphia Navy Yard. She was renamed Princeton on 21 November 1944 to commemorate the light carrier USS Princeton (CVL-23), which was lost at the Battle of Leyte Gulf on 24 October 1944. The new Princeton was launched on 8 July 1945, sponsored by Mrs. Harold Dodds, and commissioned on 18 November 1945, Captain John M. Hoskins in command.\nAttack carrier (1945–1954)\nThen transferred to the Pacific Fleet, she arrived at San Diego, departing again on 3 July 1946 to carry the body of Philippine President Manuel L. Quezon back to Manila for burial. From Manila, Princeton joined the 7th Fleet in the Marianas, becoming flagship of Task Force 77 (TF 77). In September and October 1946, she operated in Japanese and Chinese waters, then returned to the Mariana Islands where she remained until February 1947. In 1947 she had Carrier Air Group 13 on board, and in October 1948 evacuated dependents from Tsingtao, returned to San Diego Dec 1948 and unloaded CAG13 Maneuvers in Hawaiian waters preceded her return to San Diego until 15 March. She cruised the West Coast, Hawaiian waters, and the Western Pacific (1 October – 23 December) in 1948. She then prepared for inactivation, and on 20 June decommissioned and joined other capital ships in the Pacific Reserve Fleet.\nReactivated with the outbreak of hostilities in Korea 15 months later, Princeton recommissioned on 28 August 1950. Intensive training refreshed her Reservist crew, and on 5 December she joined TF 77 off the Korean coast, her planes and pilots (Air Group 19) making possible the reinstitution of jet combat air patrols over the battle zone. She launched 248 sorties against targets in the Hagaru area to announce her arrival, and for the next six days continued the pace to support Marines fighting their way down the long, cold road from the Chosin Reservoir to Hungnam. By the 11th, all units had reached the staging area on the coast. Princeton 's planes, with other Navy, Marine, and Air Force squadrons, then covered the evacuation from Hungnam through its completion on the 24th.\nInterdiction missions followed, and by 4 April Princeton 's planes had rendered 54 rail and 37 highway bridges inoperable and damaged 44 more. In May, they flew against the railroad bridges connecting Pyongyang with Sunchon, Sinanju, Kachon, and the trans-peninsula line. Next, they combined close air support with raids on power sources in the Hwachon Reservoir area and, with the stabilization of the front there, resumed interdiction. For much of the summer they pounded supply arteries, concentrating on highways, and in August Princeton got underway for the U.S., arriving at San Diego on the 21st.\nOn 30 April 1952, Princeton rejoined TF 77 in the combat zone. For 138 days, her planes flew against the enemy. They sank small craft to prevent the recapture of offshore islands; blasted concentrations of supplies, facilities, and equipment behind enemy lines, participated in air-gun strikes on coastal cities, pounded the enemy's hydroelectric complex at Suiho on the Yalu River to turn off power on both sides of that river, destroyed gun positions and supply areas in Pyongyang; and closed mineral processing plants and munitions factories at Sindok, Musan, Aoji, and Najin.\nReclassified CVA-37 (1 October 1952), Princeton returned to California on 3 November for a two-month respite from the western Pacific. In February 1953, she was back off the Korean coast and until the end of the conflict launched planes for close air support, \"Cherokee\" strikes against supply, artillery, and troop concentrations in enemy territory, and against road traffic. She remained in the area after the truce on 27 July, and on 7 September got underway for San Diego.\nAnti-submarine carrier (1954–1959)\nIn January 1954, Princeton was reclassified CVS-37 and, after conversion at Bremerton, Washington, took up antisubmarine/ Hunter-Killer (HUK) training operations in the eastern Pacific. For the next five years she alternated HUK exercises off the West Coast with similar operations in the western Pacific and, in late 1957-early 1958, in the Indian Ocean–Persian Gulf area.\nAmphibious assault carrier (1959–1970)\nReclassified again, 2 March 1959, she emerged from conversion as an amphibious assault carrier, LPH-5. Capable of transporting a battalion landing team and carrying helicopters in place of planes, Princeton 's mission became that of vertical envelopment—the landing of Marines behind enemy beach fortifications and providing logistics and medical support as they attack from the rear to seize critical points, cut enemy supplies, sever communications, and link up with assault forces landed on the beaches. Since this was a Marine Corps mission, Marines made up a major portion of the ship's company in the Air, Operations, and Supply Departments.\nFrom May 1959 – January 1960, Princeton trained with Marine units from Camp Pendleton, then deployed to WestPac to train in Okinawan waters. For the next three years, she followed a similar schedule, gaining experience in her primary mission. Interruptions came in October 1961 when she rescued 74 survivors of two merchantmen Pioneer Muse and Sheik grounded on Kita Daito Shima and in April 1962 when she delivered Marine Corps advisors and helicopters to Sóc Trăng in the Mekong Delta area of the Republic of Vietnam (South Vietnam). From September–November 1962, Princeton served as flagship of Joint Task Force 8 during the nuclear weapons test series, Operation Dominic.\nIn October 1964, Princeton exchanged WestPac training for the real thing as she returned to Vietnam and joined the Pacific Fleet's Ready Group in operations against North Vietnamese and Viet Cong forces. Combat operations, interrupted in November for flood relief work, continued into the new year, 1965, and culminated in May off Chu Lai as she carried out her primary mission, vertical envelopment, for the first time in combat.\nReturning to her homeport, Long Beach, California, after that operation, she transported Marine Aircraft Group 36 to Vietnam in August, and in February 1966 got underway for another tour in the combat zone. Relieving Okinawa (LPH-3) as flagship for the Amphibious Ready Group, she engaged the enemy in operations \"Jackstay\", 26 March – 6 April, to clear the Rung Sat Special Zone of Viet Cong guerrillas, and \"Osage\", 27 April – 4 May, to protect Vietnamese in the Phu Loc area from Viet Cong \"harassment.\"\nSearch and destroy missions against Viet Cong and North Vietnamese Army units followed as Princeton provided transportation, medical evacuation, logistics and communication support for the amphibious operation \"Deckhouse I\", 18 – 27 June, in the Song Cau district and the Song Cai river valley, then supported 1st Air Cavalry and 101st Airborne units engaged in \"Nathan Hale\" to the south of the \"Deckhouse I\" area. \"Deckhouse II\" and support for \"Hastings\" followed as Navy, Marine, and Army units again combined, this time to impede enemy infiltration from the DMZ.\nAfter \"Hastings\", Princeton sailed for home, arriving on 2 September. She deployed again to Vietnam from 30 January–19 June 1967, and again ranged along that long embattled, highly indented coast. In March, she assisted in countering an enemy threat to the Marine artillery base at Gio Linh and evacuated wounded from Con Thien mountain. In April, she participated in \"Beacon Star\", in the Khe Sanh area, and supported search and destroy operations in conjunction with \"Shawnee\". In May, her helicopters lifted Marines to the DMZ to block enemy forces withdrawing across the Bến Hải River.\nA much-needed overhaul followed Princeton 's return to the west coast, and in May 1968 she again sailed west to Vietnam. There, as flagship for Amphibious Ready Group Alpha, she provided amphibious assault carrier services for operations \"Fortress Attack\" III and IV, \"Proud Hunter\", \"Swift Pursuit\", and \"Eager Hunter\". In December, she returned to the United States and in April 1969 she was designated the prime recovery ship for Apollo 10, the lunar mission which paved the way for Apollo 11 and the first manned landing on the Moon. Apollo 10 was recovered in the South Pacific on 26 May.\nOn 30 January 1970, Princeton was decommissioned and struck from the Naval Vessel Register, and sold for scrapping in May 1971.\nPrinceton received the Navy Unit Commendation for four periods: 5 December 1950 to 10 August 1951, 15 April to 18 October 1952, 13 March to 15 May 1953, and 11 June to 27 July 1953.\nThe flag of USS Princeton is now in Princeton University in the University Chapel.\nThe USS Princeton was used during the filming of the 1952 Monogram Pictures feature Flat Top\n|Wikimedia Commons has media related to USS Princeton (CV-37).|\n- This article incorporates text from the public domain Dictionary of American Naval Fighting Ships. The entry can be found here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:cf9794e2-46ad-4fd9-8925-2230c43b6d8f>","<urn:uuid:ff3f8323-89d1-421c-ac55-34b652d07d4d>"],"error":null}
{"question":"How did the scientific discoveries of 1901 differ from those recognized by the 2017 Nobel Prize in Chemistry in terms of technological advancement?","answer":"In 1901, scientific discoveries included fundamental findings like Europium's discovery by Eugène-Anatole Demarçay and Emil Fischer's work on dipeptide synthesis. In contrast, the 2017 Nobel Prize in Chemistry recognized a more technologically advanced achievement - the development of cryo-electron microscopy by Dubochet, Frank, and Henderson, which allows researchers to freeze biomolecules mid-movement and visualize them at atomic resolution, representing a revolutionary advance in biochemistry that enables detailed imaging of life's molecular machinery.","context":["1901 in science\nFrom Wikipedia, the free encyclopedia\n- Okapi, a relative of the Giraffe found in the rainforests around the Congo River in north east Zaire, is discovered (previously known only to local natives).\n- Publication of Robert Ridgway's The Birds of North and Middle America by the Smithsonian Institution begins.\n- Edmund Selous publishes the book Bird Watching in the U.K., giving rise to the term birdwatching.\n- May 27 – The Edison Storage Battery Company is founded in New Jersey.\n- Europium is discovered by Eugène-Anatole Demarçay.\n- Emil Fischer, in collaboration with Ernest Fourneau, synthesizes the dipeptide, glycylglycine, and also publishes his work on the hydrolysis of casein.\n- Edith Humphrey becomes (probably) the first British woman to obtain a doctorate in chemistry, at the University of Zurich.\n- December 13 (20:45:52) – Retrospectively, this becomes the earliest date representable with a signed 32-bit integer on digital computer systems that reference time in seconds since the Unix epoch.\n- August 6 – Discovery Expedition: Robert Falcon Scott sets sail on the RRS Discovery to explore the Ross Sea in Antarctica.\nHistory of Science\n- September 25 – Establishment of Deutsche Gesellschaft für Geschichte der Medizin und der Naturwissenschaften, the world's first history of science society.\n- May/June – Russell's paradox: Bertrand Russell shows that Georg Cantor's naive set theory leads to a contradiction.\n- Élie Cartan develops the exterior derivative.\n- Leonard Eugene Dickson publishes Linear groups with an exposition of the Galois field theory in Leipzig, advancing the classification of finite simple groups and listing almost all non-abelian simple groups having order less than one billion.\n- Aleksandr Lyapunov proves the central limit theorem rigorously using characteristic functions.\n- Publication begins of A Monograph of British Graptolites by Gertrude L. Elles and Dr Ethel M. R. Wood, edited by Charles Lapworth.\n- Albert Einstein publishes his conclusions on capillarity.\n- Owen Richardson describes the phenomenon in thermionic emission which gives rise to Richardson's Law.\n- Ivan Yarkovsky describes the Yarkovsky effect, a thermal force acting on rotating bodies in space, in a pamphlet on \"The density of light ether and the resistance it offers to motion\" published in Bryansk.\n- December 12 – Guglielmo Marconi receives the first trans-Atlantic radio signal, sent from Poldhu in Cornwall, England, to Newfoundland, the letter \"S\" in Morse.\nPhysiology and medicine\n- November 25 – Auguste Deter is first examined by Dr Alois Alzheimer in Frankfort leading to a diagnosis of the condition that will carry Alzheimer's name.\n- Jokichi Takamine isolates and names adrenaline from mammalian organs.\n- Ivan Pavlov develops the theory of the \"conditional reflex\".\n- Georg Kelling of Dresden performs the first \"coelioscopy\" (laparoscopic surgery), on a dog.\n- William C. Gorgas controls the spread of yellow fever in Cuba by a mosquito eradication program.\n- Scottish military doctor William Boog Leishman identifies organisms from the spleen of a patient who had died from \"Dum Dum fever\" (later known as leishmaniasis) and proposes them to be trypanosomes, found for the first time in India.\n- An improved sphygmomanometer, for the measurement of blood pressure, is invented and popularized by Harvey Cushing.\n- Karl Landsteiner discovers the existence of different human blood types\n- May 16 – TS King Edward is launched at William Denny and Brothers' shipyard in Dumbarton, Scotland. The first commercial merchant vessel propelled by steam turbines, she enters excursion service on the Firth of Clyde on July 1.\n- July 10 – The world's first passenger-carrying trolleybus in regular service operates on the Biela Valley Trolleybus route at Koeninggstein in Germany, pioneering Max Schiemann's under-running trolley current collection system.\n- August 30 – Hubert Cecil Booth patents the electrically powered vacuum cleaner in the United Kingdom\n- November 30 – Frank Hornby of Liverpool is granted a U.K. patent for the construction toy that will become Meccano.\n- December 3 – King C. Gillette files a U.S. patent application for his design of safety razor utilizing thin, disposable blades of stamped steel.\n- Ernest Godward invents the spiral hairpin in New Zealand.\n- Theodor Rall patents his design of rolling lift bridge.\n- H. G. Wells' \"scientific romance\" The First Men in the Moon and his collected articles on futurology Anticipations of the Reaction of Mechanical and Scientific Progress Upon Human Life and Thought.\n- First Nobel Prizes awarded\n- Wollaston Medal for Geology – Charles Barrois\n- January 14 – Alfred Tarski (died 1983), Polish Jewish logician and mathematician.\n- February 28 – Linus Pauling (died 1994), American chemist, Nobel Prize winner for chemistry and peace.\n- March 6 – Rex Wailes (died 1986), English engineer and historian of technology.\n- April 29 – Hirohito (died 1989), marine biologist and Emperor of Japan.\n- August 10 – Franco Rasetti (died 2001), Italian physicist.\n- September 29 – Enrico Fermi (died 1954), Italian physicist.\n- November 6 – Kathleen Mary Drew-Baker (died 1957), British phycologist.\n- December 5 – Werner Heisenberg (died 1976), German theoretical physicist.\n- December 16 – Margaret Mead (died 1978), American cultural anthropologist.\n- December 20 – Robert J. Van de Graaff (died 1967), American physicist.\n- January 21 – Elisha Gray (born 1835), American electrical engineer.\n- February 11 – Henry Willis (born 1821), English organ builder.\n- February 22 – George FitzGerald (born 1851), Irish mathematician.\n- April 16 – Henry Augustus Rowland (born 1848), American physicist.\n- Über die Bindungsstelle der Metalle in ihren Verbindungen und über Dinitritoäthylendiaminkobaltisalze.\n- \"DGGMNT\". Retrieved 2011-10-10.\n- Griffin, N. (2004). \"The Prehistory of Russell's Paradox\". In Link, Godehard (ed). One Hundred Years of Russell's Paradox: mathematics, logic, philosophy. p. 350. ISBN 978-3-11-017438-0.\n- Parshall, K. H. (1991). \"A study in group theory: Leonard Eugene Dickson's Linear groups\". Mathematical Intelligencer 13: 7–11. doi:10.1007/bf03024065.\n- Crilly, Tony (2007). 50 Mathematical Ideas you really need to know. London: Quercus. p. 141. ISBN 978-1-84724-008-8.\n- Einstein, A. (1901). \"Folgerungen aus den Capillaritätserscheinungen\" (PDF). Annalen der Physik 309 (3): 513–523. Bibcode:1901AnP...309..513E. doi:10.1002/andp.19013090306.\n- Nobel Foundation (1928). \"The Nobel Prize in Physics 1928: Owen Willans Richardson\". Nobelprize.org. Retrieved 2012-01-17.\n- Beekman, George. \"The nearly forgotten scientist Ivan Osipovich Yarkovsky\". Journal of the British Astronomical Association 115 (4): 207–212. Bibcode:2005JBAA..115..207B. Retrieved 2011-10-27.\n- Bussey, Gordon (2000). Marconi's Atlantic Leap. Coventry: Marconi. ISBN 0-9538967-0-6.\n- \"Alois Alzheimer\". Whonamedit?. Retrieved 2011-10-21.\n- Takamine, J. (1901). \"The isolation of the active principle of the suprarenal gland\". The Journal of Physiology (Cambridge University Press): xxix–xxx. See also American Journal of Pharmacy 73 (1901):525.\n- Todes, Daniel Philip (2002). Pavlov's Physiology Factory. Baltimore: Johns Hopkins University Press. pp. 232 et seq. ISBN 0-8018-6690-1.\n- Schollmeyer, Thoralf et al. (November 2007). \"Georg Kelling (1866-1945): the root of modern day minimal invasive surgery. A forgotten legend?\" (PDF). Archives of Gynecology and Obstetrics 276 (5): 505–9. doi:10.1007/s00404-007-0372-y. PMID 17458553. Retrieved 2011-10-19.\n- Porter, Roy (1997). The Greatest Benefit to Mankind: a medical history of humanity from antiquity to the present. London: HarperCollins. p. 474. ISBN 0-00-215173-1.\n- Leishman, W. B. (1903). \"On the possibility of the occurrence of trypanomiasis in India\". The British Medical Journal.\n- Dittmann, Frank (1991). \"Die gleislose Bielatalbahn\". Sächsische Heimatblätter (3): 177–180. ISSN 0486-8234.\n- Penguin Pocket On This Day. Penguin Reference Library. 2006. ISBN 0-14-102715-0.\n- \"Hornby's 1901 patent\". Retrieved 2010-08-14.\n- US 775134 \"Razor\"\n- \"Patent number 669348: T. Rall movable bridge\". United States Patent and Trademark Office (referenced online by Google Patents). 1901. Retrieved April 21, 2013.\n- Clarke, Mike (2009-01-05). \"A Brief History of Movable Bridges\". Retrieved 2012-02-09.","NOBEL PRIZE IN CHEMISTRY\nAlfred Nobel 1833-1896\nEstablished the Nobel Prizes \"for the Greatest\nBenefit to Mankind\"\nSince 1901, the Nobel Prize has been honoring men\nand women from all corners of the globe for\noutstanding achievements in physics, chemistry,\nphysiology or medicine, literature, and for work in\npeace. The foundations for the prize were laid in\n1895 when Alfred Nobel wrote his last will, leaving\nmuch of his wealth to the establishment of the\nNobel Prize. But who was Alfred Nobel? Articles,\nphotographs, a slide show and poetry written by\nNobel himself are presented here to give a glimpse\nof a man whose varied interests are reflected in\nthe prize he established.\n2017 ஆம்ஆண்டின்வேதியலுக்கான நோபல்பரிசுஅறிவிக்கப்பட்டுள்ளது.அமெரிக்கா, பிரிட்டன், சுவிட்சர்லாந்தை சேர்ந்த மூன்று விஞ்ஞானிகளுக்கு இந்த ஆண்டிற்கான வேதியலுக்கான நோபல் பரிசு அறிவிக்கப்பட்டுள்ளது.மூலக்கூறுகள் பற்றிய ஆய்வுக்காக ஜேக்கஸ் டிபோட்சே, ஜோஷின் பிராங்க், ரிச்சர்ட் ஹெண்டர்சன் ஆகியோருக்கு நோபல் பரிசு வழங்கப்படுகின்றது.இந்த அறிவிப்பினை ஸ்வீடன் தலைநகர் ஸ்டாக்ஹோமில் நோபல் பரிசுக்குழு தலைவர் கோரன் ஹான்சன் அறிவித்தார்.\nThey Captured Life in Atomic Detail\nJacques Dubochet, Joachim Frank and Richard Henderson are awarded the Nobel Prize in Chemistry 2017 for their development of an effective method for generating three-dimensional images of the molecules of life.Using cryo-electron microscopy, researchers can now freeze biomolecules mid-movement and portray them at atomic resolution. This technology has taken biochemistry into a new era.\nNobel Prize in Chemistry 2017\nCool microscope technology revolutionises biochemistry\nWe may soon have detailed images of life’s complex machineries in atomic resolution. The Nobel Prize in Chemistry 2017 is awarded to Jacques Dubochet, Joachim Frank and Richard Henderson for the development of cryo-electron microscopy, which both simplifies and improves the imaging of biomolecules. This method has moved biochemistry into a new era.\nA picture is a key to understanding. Scientific breakthroughs often build upon the successful visualisation of objects invisible to the human eye. However, biochemical maps have long been filled with blank spaces because the available technology has had difficulty generating images of much of life’s molecular machinery.\nCryo-electron microscopy changes all of this. Researchers can now freeze biomolecules mid-movement and visualise processes they have never previously seen, which is decisive for both the basic understanding of life’s chemistry and for the development of pharmaceuticals.\nElectron microscopes were long believed to only be suitable for imaging dead matter, because the powerful electron beam destroys biological material. But in 1990, Richard Henderson succeeded in using an electron microscope to generate a three-dimensional image of a protein at atomic resolution. This breakthrough proved the technology’s potential.\nJoachim Frank made the technology generally applicable. Between 1975 and 1986 he developed an image processing method in which the electron microscope’s fuzzy twodimensional images are analysed and merged to reveal a sharp three-dimensional structure.\nJacques Dubochet added water to electron microscopy. Liquid water evaporates in the electron microscope’s vacuum, which makes the biomolecules collapse. In the early 1980s, Dubochet succeeded in vitrifying water – he cooled water so rapidly that it solidified in its liquid form around a biological sample, allowing the biomolecules to retain their natural shape even in a vacuum.\nFollowing these discoveries, the electron microscope’s every nut and bolt have been optimised. The desired atomic resolution was reached in 2013, and researchers can now routinely produce three-dimensional structures of biomolecules. In the past few years, scientific literature has been filled with images of everything from proteins that cause antibiotic resistance, to the surface of the Zika virus. Biochemistry is now facing an explosive development and is all set for an exciting future.\nNomination and Selection of Chemistry Laureates\nNomination to the Nobel Prize in Chemistry is by invitation only. The names of the nominees and other information about the nominations cannot be revealed until 50 years later.\nProcess of Nomination and Selection\nThe Nobel Committee for Chemistry sends confidential forms to persons who are competent and qualified to nominate.\nThe right to submit proposals for the award of a Nobel Prize in Chemistry shall, by statute, be enjoyed by:\n1. Swedish and foreign members of the Royal Swedish Academy of Sciences;\n2. Members of the Nobel Committees for Chemistry and Physics;\n3. Nobel Laureates in Chemistry and Physics;\n4. Permanent professors in the sciences of Chemistry at the universities and institutes of technology of Sweden, Denmark, Finland, Iceland and Norway, and Karolinska Institutet, Stockholm;\n5. Holders of corresponding chairs in at least six universities or university colleges selected by the Academy of Sciences with a view to ensuring the appropriate distribution over the different countries and their centers of learning; and\n6. Other scientists from whom the Academy may see fit to invite proposals.\nDecisions as to the selection of the teachers and scientists referred to in paragraphs 5 and 6 above shall be taken each year before the end of the month of September.\nSelection of Nobel Laureates\nThe Royal Swedish Academy of Sciences is responsible for the selection of the Nobel Laureates in Chemistry from among the candidates recommended by the Nobel Committee for Chemistry. The Nobel Committee is the working body that screens the nominations and selects the final candidates. It consists of five members, but for many years the Committee has also adjunct members with the same voting rights as members.\nWho is eligible for the Nobel Prize in Chemistry?\nThe candidates eligible for the Chemistry Prize are those nominated by qualified persons who have received an invitation from the Nobel Committee to submit names for consideration. No one can nominate himself or herself.\nHow Are the Nobel Laureates Selected?\nNomination process.Below is a brief description of the process involved in selecting the Nobel Laureates in Chemistry.\nSeptember-October – Nomination forms are sent out. The Nobel Committee sends out confidential forms to around 3,000 people — selected professors at universities around the world, Nobel Laureates in Physics and Chemistry, and members of the Royal Swedish Academy of Sciences, among others.\nFebruary – Deadline for submission of nominations. The completed nomination forms must reach the Nobel Committee no later than 31 January of the following year. The Committee screens the nominations and selects the preliminary candidates. About 250–350 scientists are nominated as several nominators often submit the same name.\nMarch-May – Consultation with experts. The Nobel Committee evaluates preliminary candidates, advised by specially appointed experts for their assessment of the candidates' work.\nJune-August – Writing of the report. The Nobel Committee puts together a report with recommendations to be submitted to the Academy. The report is signed by all members of the Committee.\nSeptember – Committee submits recommendations. The Nobel Committee submits its report with\nrecommendations on the final candidates to the members of the Academy. The report is discussed at two meetings of the Chemistry Section of the Academy.\nOctober – Nobel Laureates are chosen. In early October, the Academy selects the Nobel Laureates in Chemistry through a majority vote. The decision is final and without appeal. The names of the Nobel Laureates are then announced.\nDecember – Nobel Laureates receive their Prize. The Nobel Prize Award Ceremony takes place on 10 December in Stockholm, where the Nobel Laureates receive their Nobel Prize, which consists of a Nobel Medal and Diploma, and a document confirming the prize amount.\nAre the nominations made public?\nThe statutes of the Nobel Foundation restrict disclosure of information about the nominations, whether publicly or privately, for 50 years. The restriction concerns the nominees and nominators, as well as investigations and opinions related to the award of a prize.\nDorothy Crowfoot Hodgkin, Henry Taube, Ernest Rutherford, Ada E. Yonath and Otto Walll Prizes in Chemistry The Nobel Prize in Chemistry has been awarded 109 times to 178 Nobel Laureates between 1901 and 2017.\nFrederick Sanger is the only Nobel Laureate who has been awarded the Nobel Prize in Chemistry twice, in 1958 and 1980. This means that a total of 177 individuals have received the Nobel Prize in Chemistry. Click on the links to get more information.\nNobel Prize in chemistry 1901-2017"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_profile","category_name":"novice_inquirer"}],"document_ids":["<urn:uuid:77edaaf5-8cba-4f14-9c3f-95da85598b03>","<urn:uuid:cc4024bd-03ee-4a3a-917c-b9a5735bbe45>"],"error":null}
{"question":"How do booklice feed and survive in human environments?","answer":"Booklice feed primarily on fungi, algae, lichen, and organic detritus. In human environments, they are commonly found amongst old books where they feed upon the paste used in binding. They can also infest food storage areas, feeding on dry, starchy materials. Booklice use their chewing mandibles and a modified central lobe of the maxilla (a slender rod) to brace themselves while scraping up detritus. They are scavengers and do not bite humans. Their eggs take 2-4 weeks to hatch, reaching adulthood in about 2 months, and adults can live for six months.","context":["Temporal range: 299–0Ma Early Permian – Recent\nPsocoptera are an order of insects that are commonly known as booklice, barklice or barkflies. They first appeared in the Permian period, 295–248 million years ago. They are often regarded as the most primitive of the hemipteroids. Their name originates from the Greek word ψῶχος, psokos meaning gnawed or rubbed and πτερά, ptera meaning wings. There are more than 5,500 species in 41 families in three suborders. Many of these species have only been described in recent years.\nThey range in size from 1–10 millimeters (0.04–0.4 in) in length.\nThe species known as booklice received their common name because they are commonly found amongst old books—they feed upon the paste used in binding. The barklice are found harmlessly on trees, feeding on algae and lichen. No member of this order is currently considered endangered; in fact, in 2007, Atlantopsocus adustus, a species native to Madeira and the Canary Islands, was found to have colonized the mild Cornish coast of southwest England.\nIn the 2000s, morphological and molecular evidence has shown that the parasitic lice (Phthiraptera) evolved from within the psocopteran suborder Troctomorpha. In modern systematics, Psocoptera and Phthiraptera are therefore treated together in the order Psocodea.\nAnatomy and biology\nPsocids are small, scavenging insects with a relatively generalized body plan. They feed primarily on fungi, algae, lichen, and organic detritus. They have chewing mandibles, and the central lobe of the maxilla is modified into a slender rod. This rod is used to brace the insect while it scrapes up detritus with its mandibles. They also have a swollen forehead, large compound eyes, and three ocelli. Some species can spin silk from glands in their mouth.\nThe forewings are up to 1.5 times as long as the hindwings, and all four wings have a relatively simple venation pattern, with few cross-veins. The legs are slender and adapted for walking, rather than gripping, as in the true lice. The abdomen has nine segments, and no cerci.\nThere is often considerable variation in the appearance of individuals within the same species. Many have no wings or ocelli, and may have a different shape to the thorax. Other, more subtle, variations are also known, such as changes to the development of the setae. The significance of such changes is uncertain, but their function appears to be different from similar variations in, for example, aphids. Like aphids, however, many psocids are parthenogenic, and the presence of males may even vary between different races of the same species.\nPsocids lay their eggs in minute crevices or on foliage, although a few species are known to be viviparous. The young are born as miniature, wingless versions of the adult. These nymphs typically molt six times before reaching full adulthood. The total lifespan of a psocid is rarely more than a few months.\nBooklice range from approximately 1mm to 2mm in length (1/25\" to 1/13\"). Some species are wingless and they are easily mistaken for bedbug nymphs and vice-versa. Booklouse eggs take 2 to 4 weeks to hatch and can reach adulthood approximately 2 months later. Adult booklice can live for six months. Besides damaging books, they also sometimes infest food storage areas, where they feed on dry, starchy materials. They are scavengers and do not bite humans.\nThe Order Psocoptera is divided into three suborders.\nTrogiomorpha is the smallest suborder of the Psocoptera sensu stricto (i.e. excluding Phthiraptera), with about 340 species in 7 families, ranging from the monospecific fossil family Archaeotropidae to the speciose Lepidopsocidae (over 200 species). Trogiomorpha comprises infraorder Atropetae (extant families Lepidopsocidae, Psoquillidae and Trogiidae, and fossil families Archaeotropidae and Empheriidae) and infraorder Psocathropetae (families Psyllipsocidae and Prionoglarididae).\nTroctomorpha have antennae with 15–17 segments and two-segmented tarsi.\nTroctomorpha comprises the Infraorder Amphientometae (families Amphientomidae, Compsocidae, Electrentomidae, Musapsocidae, Protroctopsocidae and Troctopsocidae) and Infraorder Nanopsocetae (families Liposcelididae, Pachytroctidae and Sphaeropsocidae). Troctomorpha are now known to also contain the order Phthiraptera (lice), and are therefore paraphyletic, as are Psocoptera as a whole.\nSome Troctomorpha, such as Liposcelis (which are similar to lice in morphology), are often found in birds' nests, and it is possible that a similar behavior in the ancestors of lice is at the origin of the parasitism seen today.\nPsocomorpha are notable for having antennae with 13 segments. They have two- or three-segmented tarsi, this condition being constant (e.g. Psocidae) or variable (e.g. Pseudocaeciliidae) within families. Their wing venation is variable, the most common type being that found in the genus Caecilius (rounded, free areola postica, thickened, free pterostigma, r+s two-branched, m three-branched). Additional veins are found in some families and genera (Dicropsocus and Goja in Epipsocidae, many Calopsocidae, etc.)\nPsocomorpha is the largest suborder of the Psocoptera sensu stricto (i.e. excluding Phthiraptera), with about 3,600 species in 24 families, ranging from the species-poor Bryopsocidae (2 spp.) to the speciose Psocidae (about 900 spp). Psocomorpha comprises Infraorder Epipsocetae (families Cladiopsocidae, Dolabellopsocidae, Epipsocidae, Neurostigmatidae and Ptiloneuridae), Infraorder Caeciliusetae (families Amphipsocidae, Asiopsocidae, Caeciliusidae, Dasydemellidae and Stenopsocidae), Infraorder Homilopsocidea (families Archipsocidae, Bryopsocidae, Calopsocidae, Ectopsocidae, Elipsocidae, Lachesillidae, Mesopsocidae, Peripsocidae, Philotarsidae, Pseudocaeciliidae and Trichopsocidae) and Infraorder Psocetae (families Hemipsocidae, Myopsocidae, Psilopsocidae and Psocidae).\n- Christopher O'Toole (2002). Firefly Encyclopedia of Insects and Spiders. Toronto: Firefly Books. ISBN 1-55297-612-2.\n- John R. Meyer (2005-03-05). \"Psocoptera\". North Carolina State University.\n- Alfonso N. García Aldrete (2006). \"New genera of Psocoptera (Insecta), from Mexico, Belize and Ecuador (Psoquillidae, Ptiloneuridae, Lachesillidae)\". Zootaxa 1319: 1–14.\n- BBC News, \"New insect species arrives in UK\" 8 November 2007\n- Yoshizawa, K.; Johnson, K. P. (2006). \"Morphology of male genitalia in lice and their relatives and phylogenetic implications\". Systematic Entomology 31 (2): 350–361. doi:10.1111/j.1365-3113.2005.00323.x.\n- Johnson, K. P.; Yoshizawa, K.; Smith, V. S. (2004). \"Multiple origins of parasitism in lice\". Proceedings of the Royal Society of London 271 (1550): 1771–1776. doi:10.1098/rspb.2004.2798. PMC 1691793. PMID 15315891.\n- Bess, Emilie, Vince Smith, Charles Lienhard, and Kevin P. Johnson (2006) Psocodea. Parasitic Lice (=Phthiraptera), Book Lice, and Bark Lice. Version 8 October 2006 (under construction). http://tolweb.org/Psocodea/8235/2006.10.08 in The Tree of Life Web Project, http://tolweb.org/\n- Hoell, H.V., Doyen, J.T. & Purcell, A.H. (1998). Introduction to Insect Biology and Diversity, 2nd ed. Oxford University Press. pp. 404–406. ISBN 0-19-510033-6.\n- US Army Public Health Command fact sheet. http://phc.amedd.army.mil/PHC Resource Library/BookliceFSMar08WestFinal.pdf\n- C. Lienhard & C. N. Smithers (2002). \"Psocoptera (Insecta): World Catalogue and Bibliography\". Instrumenta Biodiversitatis (Muséum d'histoire naturelle, Geneva) 5.\n|Wikimedia Commons has media related to Psocoptera.|\n|Wikispecies has information related to: Psocoptera|\n- National Barkfly Recording Scheme\n- Psoco Net\n- Tree of Life: Psocodea\n- Archipsocus nomas, a webbing barklouse on the UF / IFAS Featured Creatures Web site\n- HD Video of Barklice"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:2bc54c31-3775-4b71-9a2a-df417fccb457>"],"error":null}
{"question":"As a mental health researcher, I'd like to understand: How do physical withdrawal symptoms compare between substance addictions and behavioral addictions?","answer":"Substance addictions and behavioral addictions differ significantly in their physical withdrawal manifestations. In behavioral addictions, there is no physical addiction present, which means many signs that would make it easy to identify the addiction may not be visible. In contrast, substance addiction involves extensive physical withdrawal symptoms including abdominal cramps, irregular heart rate, breathing problems, nausea, changes in body temperature and blood pressure, chest pain, muscle pain and weakness, decreased motor coordination, muscle spasms, and many other physical manifestations. The severity of these physical symptoms varies based on the specific substance, with some drugs like benzodiazepines and alcohol potentially causing life-threatening withdrawal symptoms that require professional supervision.","context":["Addiction is a destructive disease that can lead to once-unthinkable consequences. And the addiction doesn’t have to be to heroin, alcohol, cocaine, or any chemical substance for that matter. It is possible and more common than one may think, to be addicted to a particular behavior. Substance addiction and behavioral addiction are different, but there are also a number of similarities between those who struggle with either form of addiction.\nWhen searching for a definition of addiction, most sources focus on chemical dependency and substance addiction. Even on the American Psychiatry Association (APA) website, addiction is defined as “a complex condition, a brain disease that is manifested by compulsive substance use despite harmful consequence”. Note the use of the term “substance use”. However, we discussed above that behavioral addictions are not that different from substance addiction. Unfortunately, the APA definition reads the way it does because the only behavioral addiction listed in the DSM-V is gambling due to insufficient evidence to support other behavioral addictions. The DSM-V is the most widely used publication guiding the diagnosis and treatment of mental health disorders.\nStanley Peele is a psychologist, psychotherapist, and co-author of the book Love and Addiction. In this publication, Peele describes addiction as when addicted individuals are dependent on a particular set of experiences and goes on to explain that the reaction to chemical substances is just one example of this dependence. This somewhat broader understanding of addiction seems more accurate when considering the reality of behavioral addictions.\nThe APA definition of addiction being a compulsion to use substances despite harmful consequences is quite accurate when referring to chemical substance addiction. Dependency is when the substance needs to be consumed in order for the individual to simply feel normal. An individual with chemical dependency likely will experience withdrawal symptoms if they discontinue drug use. Physical dependence is one criterion for diagnosing a substance use disorder, but the two usually go hand in hand. Treating drug and alcohol addiction requires addressing the physical effects, as well as the behavioral issues that may be associated with use.\nBehavioral addiction is a psychological dependence on a particular set of experiences that are not a reaction to chemical substances, rather behaviors. Unlike a drug or alcohol addiction, someone with a behavioral addiction is not experiencing a physical addiction to anything. This lack of physical addiction means many signs that make it easy to identify the addiction may not be present.\nExamples of behavioral addictions include:\n- Gambling addiction\n- Eating disorders\n- Sex Addiction\n- Gaming Addiction\n- Shopping Addiction\n- Pornography Addiction\n- Exercise Addiction\nHow are substance and behavioral addictions alike?\nThe multitude of similarities between substance addiction and behavioral addictions may be surprising to many. Most addictive drugs act on the pleasure and reward centers of the brain. Similarly, when behaviors such as gambling or sex are engaged, it triggers the release of pleasure hormones and initiates a reward signal. Additionally, addiction to behaviors can have consequences on the same areas of one’s life as drug addiction. The addict may display functional impairments in there work, relationships, and in various social situations. In both cases, the addict is also likely to continue on with their behavior despite negative consequences that arise.\nBehavioral Therapy & Addiction\nAlthough most behavioral addictions are not covered in the DSM-V, behavioral therapy is recognized as one of the most effective forms of treatment for addiction. The goal of behavioral therapy is to identify unhealthy behaviors and change them to produce better outcomes. Behavioral therapy is a promising treatment option for individuals struggling with any form of addiction, but treatment plans should be personalized to meet each individual’s needs.","One out of 12 Americans struggled with addiction in 2014, according to the National Survey on Drug Use and Health (NSDUH), and one of the common side effects of drug addiction is drug dependence. When someone uses drugs regularly, some of the brain’s chemistry is altered. Chemical messengers, called neurotransmitters, that send signals around the central nervous system to tell the body how to react to stress, when to feel pleasure, and work to control emotions are disrupted. This causes both physical and emotional fallout. The body can get used to the drugs being in its system with chronic and repeated use, and parts of the brain may even be rewired from regular drug abuse.\nPer the American Society of Addiction Medicine (ASAM), addiction is a brain disease wherein regions of the brain and its circuitry are impacted. When drugs are processed out of the body, levels of some of these neurotransmitters are impacted, and the brain may struggle to rebalance them, resulting in withdrawal symptoms and drug cravings.\nDrug withdrawal symptoms can vary depending on the type of drug abused. Some drugs, like stimulants (cocaine, methamphetamine, and prescription ADHD medications) for example, may have more intense emotional withdrawal symptoms. For others, such as with opioids (heroin and prescription painkillers), drug withdrawal can be extremely uncomfortable physically, mirroring a bad case of the flu. Withdrawal from benzodiazepines (prescription tranquilizers and sedatives) and alcohol can be so intense, it can include potentially life-threatening symptoms, Psychology Today warns, and detox should not be attempted without professional help. These drugs should not be stopped “cold turkey” after dependence has formed; they are usually tapered off slowly under the supervision of a trained professional. Drug withdrawal symptoms are optimally managed through a medical detox program as part of a comprehensive addiction treatment plan.\nCommon Physical and Psychological Symptoms of Drug Withdrawal\nDrug withdrawal symptoms can range from potentially fatal seizures to mild nausea and gastrointestinal upset. They often cause opposite reactions than those associated with the drug. For example, benzodiazepine drugs like Xanax and Valium are intended to reduce nerve firings and calm the body’s stress response by enhancing levels of the naturally occurring sedative gamma-aminobutyric acid, or GABA. Functions of the central nervous system, like body temperature, blood pressure, respiration, and heart rate, are depressed under the influence of these drugs. When benzos are then processed out of the body, a rebound effect can occur as levels of GABA dip and central nervous system functions speed up as a result. Grand mal seizures are potential side effects of benzodiazepine drug withdrawal, which The Journal of the Oklahoma State Medical Association warns can lead to coma or even death.\nIn general, common physical withdrawal symptoms include:\n- Abdominal cramps\n- Irregular heart rate\n- Problems with breathing\n- Nausea and stomach upset\n- Changes in body temperature\n- Heightened or lowered blood pressure\n- Changes in appetite and potential weight fluctuations\n- Chest pain\n- Alterations in pupil dilation\n- Back, joint, bone, and muscle pain\n- Muscle tension\n- Muscle weakness\n- Decreased sex drive\n- Decreased motor coordination\n- Runny nose and/or congestion\n- Muscle spasms\n- Sluggish movements\n- Teeth chattering or grinding\n- Involuntary eye movements\n- Increased sensitivity to pain\n- Rashes or skin irritations\n- Bloating and increased flatulence\n- Numbness or tingling in hands, feet, fingers, & toes\nDrug withdrawal encompasses both physical and emotional symptoms. Levels of serotonin and dopamine, some of the neurotransmitters that help to regulate moods and induce feelings of happiness, are impacted by drug abuse and dependence. During drug withdrawal, the body may be depleted of dopamine and/or serotonin, leaving individuals feeling irritable, moody, depressed, anxious, paranoid, disoriented, apathetic, tense and on edge, excitable, restless, and jumpy. People may have difficulties feeling pleasure from normal things. They may also feel detached from themselves or suffer from psychotic side effects like hallucinations (seeing or hearing things that are not actually there) or delirium. Nightmares, vivid dreams, and sleep disturbances are also common.\nAggression, hostility, and violent outbursts may also accompany drug withdrawal. During drug withdrawal, individuals often have trouble thinking straight and concentrating, and short-term memory functions may be impaired. Emotional lows, crying for no reason, drug cravings, and mood swings are common, as the brain struggles to restore healthy levels of its mood-regulating chemicals.\nIndividuals may be at a higher risk for suicidal thoughts, ideations, or actions during drug withdrawal, making medical detox and professional help all the more necessary.\nConcerned about treatment costs? Call now for FREE insurance and payment consultation.\nCall Now (888) 734-0116\nVariables in Drug Withdrawal\nDrug withdrawal symptoms vary from person to person, and their severity largely relies on the level of dependence on the drug. The longer a person has used the drug in question, the more severe the dependency. The method in which it was abused, the amount used at a time, and polydrug (using more than one drug at time) abuse can all influence drug dependency. Biological factors, such as metabolism, age, gender, and any medical or mental health conditions, also play a role in the intensity of drug withdrawal. Genetics and personal or family history of addiction also factors in. Environmental aspects, like home life and exposure to trauma and stress, can be involved in the level of drug dependence and therefore the significance and duration of the withdrawal side effects. The more dependent on a drug a person is, the more intense and longer withdrawal is likely to be.\nDrug withdrawal begins as soon as the drug stops being active in a person’s body. This can be different depending on the drug involved. Typically, opioid withdrawal begins about 12 hours after the last dose (closer to 30 hours for longer-acting opioids like methadone), the National Library of Medicine (NLM) reports. Stimulant drugs like cocaine are usually fast-acting and wear off quickly, so withdrawal symptoms may start sooner.\nFor most drugs, withdrawal symptoms typically peak within the first few days of stopping use and then start to gradually lessen over time. Acute withdrawal usually refers to the bulk of a person’s withdrawal symptoms and tends to follow this general timeline, the Substance Abuse and Mental Health Services Administration (SAMHSA) publishes:\n- Marijuana: 5 days\n- Alcohol: 5-7 days\n- Opioids: 4-10 days\n- Stimulants: 1-2 weeks\n- Benzodiazepines: 1-4 weeks\nAfter acute withdrawal, symptoms usually start to taper off, although sometimes people may suffer from protracted withdrawal. Protracted withdrawal syndrome can include ongoing drug cravings, sleep and mood disturbances, irritability, physical pain, decreased energy levels, and difficulties with memory and the ability to think clearly. Protracted withdrawal may continue for several weeks or even months without professional help.\nThe presence of co-occurring mental health disorders simultaneously with drug dependence and addiction can complicate and exacerbate withdrawal symptoms. When co-occurring disorders are present, integrated and comprehensive care models work to manage the side effects of both disorders at the same time, as each disorder may contribute to the other. Drug withdrawal symptoms can be successfully managed with the help of medications and a high level of supportive care that is provided through a medical detox program.\nIt’s Never Too Late to Get Help"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_profile","category_name":"expert_specialist"}],"document_ids":["<urn:uuid:9992e3d1-caf4-47e7-8dda-442e1e778741>","<urn:uuid:cd520888-a432-4e63-903b-e601cf80565a>"],"error":null}
{"question":"How did stone tomahawks and medieval maces differ in their combat effectiveness?","answer":"Stone tomahawks were blunt impact weapons that rarely caused serious harm, mainly used for counting coup (touching enemies) rather than killing, though occasionally they could break bones or skulls. In contrast, medieval maces were more lethal weapons designed specifically for clubbing and bruising in combat. The medieval mace typically featured a metal spiked ball or various head designs with spikes, making it a more formidable weapon than the early stone tomahawks which were often just shaped rocks attached to handles with rawhide.","context":["The tomahawk has become a popular weapon or perhaps a fad amongst the survival community. Not anywhere near as effective as a sidearm, the tomahawk nevertheless has an ancient and noble history as a melee weapon and was used by the American Indians.\nPerhaps that’s where the tomahawk’s popularity comes from. There is a sort of romance in many people’s minds associated with the Old West and our nation’s westward expansion.\nAmerican Indians were a part of this, and while often villainized by Hollywood, most Americans have a positive image of the noble Red Man and his culture.\nAmerican Indians, like many primitive people, were a warlike culture. Warfare secured a tribe’s territory as well as giving men, especially young men, a way of proving their valor. There was no room for a man who could not be counted on in battle. The most respected amongst them were those who had counted many “coup” in battle.\nWhile the American Indians are thought of as a bloody people, this is not the case – at least not by comparison to many other ancient cultures.\nWhile fighting was a way of life to them, it was considered a greater honor or a greater coup to touch an enemy or his weapons rather than to kill or even wound him. Touching an enemy without being wounded in the process was a greater coup than touching them and being wounded. So, while killing was normal in their battles, it was not all about killing.\nThis form of warfare was all about demonstrating one’s bravery. As such, it was considered a greater act of bravery to touch an enemy’s person or weapons with your hand, your bow, or your coup stick (an ornamental stick used to record coup), than to wound or kill him.\nThe fact that the warrior didn’t strike the enemy with the intent to wound, while the enemy was most likely trying to wound him, was a great act of bravery.\nThe weapons of these Indians were limited. The bow, one of the most used weapons throughout world history, was their main weapon of choice – especially for the Plains Indians, who needed a ranging weapon that could be used at a distance.\nIn addition, they carried either a tomahawk or a spear depending on the tribe. Most also carried a knife, although that was not normally used in battle.\nWe must remember that before the coming of the white man, the American Indians had no metal products of any type. Their arrowheads, tomahawks, spear points, and knives were mostly made of stone, although bone and shell were used in some regions as well.\nAs they were made of stone, tomahawks did not have the sharp edge or point that we see today. The most common shape was roughly triangular, with a wide forward edge and a more pointed trailing one.\nIt was more of a blunt impact weapon suited for counting coup than cutting or throwing. While it was possible to throw, that was rarely done as to throw the tomahawk would leave one unarmed.\nTomahawks were made by first shaping the head out of stone. Depending on the tribe and the type of stone they had available to them, this stone head could be nothing more than an appropriately sized river rock. Some went no farther than that. I\nn places where soft rock was available, which could be shaped by rubbing it on harder stone, the triangular shape was more popular. Only rarely were tomahawk heads made out of obsidian or some other rock that could be chipped or “knapped” into shape.\nThe tomahawk head was attached to a stick, bone, or antler handle with rawhide, which was wrapped around the handle and head in a crossing pattern to make it secure.\nAs the rawhide dried, it would shrink and make a very tight and strong bond between the head and the handle.\nThese blunt tomahawks were an ideal weapon for counting coup, as they would rarely cause any serious harm to those struck by them – although, occasionally a tomahawk would be used to break a bone or kill an enemy by breaking open their skull.\nThe tomahawks of today are little like the original Indian tomahawks. Rather, they more closely resemble trade tomahawks made in factories, which were then used to barter with the Indians for furs. This and other similar innovations brought about enormous changes in the culture of the American Indian. They were introduced to products other than what they could produce themselves and made dependent on those trade goods.\nAs metal tomahawks replaced stone ones, their use changed as well. Rather than being used to count coup by merely striking an enemy, the users of these metal tomahawks began using them to cause penetrating wounds – cutting open an enemy’s body with the sharp blade of the tomahawk.\nWhile some had the spike on the back side, which we recognize as being useful for throwing, most merely had a stub there, which was better suited for use as a hammer to strike and cause blunt-force injuries.\nThe Tomahawk in Battle\nThe tomahawk was the American Indian’s equivalent of a wide range of melee weapons used in Europe during the Middle Ages. Melee fighting is done at arm’s length, and any weapon is an extension of the arm – whether providing penetrating trauma or blunt-force trauma. Other than the spear and the coup stick, the tomahawk was the only melee weapon that was part of American Indian culture.\nIt is Hollywood that has made the idea of throwing the tomahawk popular – not the American Indian. It is clear that early stone tomahawks could not be used effectively in this way. The later metal ones could, but they really weren’t balanced for throwing.\nSo, while you could easily hit an enemy, you couldn’t be sure what part of the tomahawk would hit them or what sort of damage would be caused to them in the process. Throwing away your melee weapon on that uncertainty wasn’t a good idea.\nThe Tomahawk for us Today\nWhile there are many who like the tomahawk, I see it more as a novelty than anything else. It is not as effective as a hatchet for cutting wood, and it is not as effective as a pistol or even a bow for self-defense.\nWhile I would not hesitate to use a tomahawk to defend myself if I had nothing else that I could use, I would do so in the same way that I would use an ax, quarterstaff, or sword – of necessity rather than as a primary or even secondary weapon.","The Medieval mace was a popular weapon in medieval times, it was mainly used by the cavalry and was probably an infantry weapon initially. The medieval mace weapon was a fearsome looking weapon, the most common design incorporated a long handle usually made from metal or wood with some type of metal spiked ball at the end, however there were many different mace weapon designs which usually involved small changes to the head such as flanged, knobbed or spiked mace heads. Unlike medieval flails maces did not have any chain at the end of the shaft connected to the spiked ball. Often the Medieval mace would have a spike sticking out of the end of the head. The morning Star was also a weapon that could be described as being a Medieval mace.\nWhat were medieval mace weapons made from?\nMedieval maces usually had long handles that were sometimes made of wood or more commonly metal, mace weapons could be made of bronze, iron or steel, but the most common metal used to make a medieval mace weapon was probably bronze. If a mace weapon was to be used for any ceremonial purposes it could even be made from gold or silver.\nHeavy metal mace\nWhat type of weapon was Medieval mace\nThe Medieval mace was a close combat weapon that was classed as being a clubbing and bruising type of weapon, it was not designed to cut skin and draw blood, this meant that the medieval mace could be used by churchmen or clerics who were not allowed to draw blood in combat in medieval times. The later medieval mace heads looked similar to the design of a dart flight. Medieval maces were light weapons and they had a security chain at the bottom of the handle for the soldiers wrist to stop the weapon being lost in close combat fighting situations.\nWho used medieval maces\nMedieval Maces were often used in ceremonies by clerics, lawyers and royalty, it was also a weapon that was commonly associated with the medieval knight, used in medieval tournaments in melee and joust contests.\nGothic mace weapons\nDuring the medieval period mace weapons were being upgraded along with other medieval weapons and during the Gothic period the Gothic mace was introduced, it had a thicker shaft in the grip area and a guard and pommel was added that was similar to what you would find on a medieval sword handle.\nA German Knight Holding a German Heavy Mace\nHistory of the Medieval mace weapon\nMedieval maces were believed to have been used in the 10th century and they are depicted as a weapon used by the Normans in the Bayeux tapestry, maces are also shown in the maciejowski Bible of the 13th century and it is known that the Nomads and Turks used medieval mace weapons in combat that had animal head designs.\nThe morning Star – a medieval mace weapon?\nThe morning Star is a type of medieval mace, it combined the mace design with spear points. The head of the morning Star looks like a pineapple head, some historians have also suggested that the morning Star was a type of flail weapon attached to the staff by a chain, however this is not the common view taken. The head of the morning Star weapon can also be described as looking similar to a star with the head being globular and spiked.\nMedieval mace weapon facts:\nThe Medieval mace was a heavy club weapon with a metal head\nMedieval maces could be made from bronze, iron, steel, silver or gold\nSilver and gold maces were used for ceremonial purposes\nMedieval mace heads usually had an additional spike added to them\nThere were many different types of medieval mace designs but there were all similar\nA common medieval mace head design was the spiked ball\nThe Bayeux tapestry depicts soldiers using medieval maces\nThe first records of medieval maces being used is around the 10th century\nMedieval maces were used by lawyers, clerics, royalty, footsoldiers and knights.\nGothic maces were introduced in later medieval periods with thicker shafts at the grip\nAnimal headed maces were used by Nomads and Turks\nThe morning Star is thought to be a type of medieval mace"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"}],"document_ids":["<urn:uuid:e993b5c1-8c56-47f0-a868-1ca28cc50be6>","<urn:uuid:b89f7a80-3c6d-4484-b3db-1c031791ea29>"],"error":null}