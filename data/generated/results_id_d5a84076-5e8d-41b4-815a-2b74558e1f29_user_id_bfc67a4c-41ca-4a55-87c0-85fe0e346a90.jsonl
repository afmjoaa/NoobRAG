{"question":"How do female protagonists navigate societal pressures in contemporary African literature?","answer":"In Kintu, female characters grapple with traditional Buganda kingdom customs and modern challenges, while in Between Sisters, the teenage protagonist Gloria must navigate predatory relationships with 'sugar daddies' who target young girls using wealth and prestige, while also dealing with unpaid domestic labor masked as familial arrangements. Both works show young women demonstrating remarkable perception and bravery in confronting these societal pressures, with Gloria specifically being portrayed as 'strong and amazing' with extraordinary ability to understand and articulate her circumstances.","context":["To enjoy the full Mail & Guardian online experience: please upgrade your browser\n31 Aug 2018 00:00\nBridging the gap: Jennifer Nansubuga Makumbi, the author of 'Kintu', not only tells stories she wishes to tell but also those of previous generations.\n“Your daughter looks just like …” Jennifer Makumbi rattles off the name of one of my aunts whom I don’t see very often. “This one is a Barlow.\nYou wouldn’t have to say anything.\nThis was my introduction to Jennifer Nansubuga Makumbi. I was seen. Located. Like my mother, I yearn for this kind of recognition, when one is located without explanation or clarification because someone already knows your story.\nMakumbi is the author of Kintu, a novel that, in the four years since its release, has become a pillar in Ugandan literature. Kintu opens in 2004 in the streets of the Ugandan capital and, similar to Dust — another groundbreaking novel, by Kenyan writer Yvonne Adhiambo Owuor — the book begins with a mob justice killing.\nKintu is the story of a family curse. What is impressive is its temporal breadth. It leaps through time, moving deftly between 2004 (when the story opens) and the precolonial kingdom of Buganda in 1750.\nIt could be argued that Makumbi skips the best-known parts of Uganda’s history — colonialism under British rule and the reign of the infamous military officer Idi Amin. Instead of dwelling on these well-documented aspects of the country’s history, Makumbi points her pen elsewhere for a more considered, concentrated look at a country that has more to answer for.\nThe author sounds so much like my aunts in Kampala but her accent is unmistakably British. Or rather, unmistakably both.\nPolo Moji, a senior lecturer in the English language and literature department at the University of Cape Town, kindly lent us her office, where Makumbi tells me how, as a child, they would have story time at her grandfather’s house. They would all gather in the sitting room after having a shower, or squat around while their grandmother was cooking in the warm kitchen.\nI mention Nsangi, a Luganda folktale about a girl whose mother puts her in a cave to protect her from apes that ate prepubescent girls in their village. Together we would sing “Nsangi, Nsangi, Nsangi mwana wange [Nsangi, Nsangi, Nsangi child of mine],” the song that Nsangi’s mother would sing to her while she was in the cave.\nLaughing, we would then pretend to be the apes grunting, “Aha haa, aha haaa, singe ndide Nsangi!” our chests puffed out, fists clenched, arms forward, as if holding large pots. Again, in this tiny moment of a shared story, I am seen and located.\n“I don’t know if I would tell my children folktales,” I confess. I tell her that I worry that African culture and traditions are locked within a specific time before colonialism. I want to tell my children many stories, which also include the cities in which we find ourselves; the cities Africans continue to transform with our bodies and work and stories.\nMakumbi interrupts me. “The reason Africans insist on that is because it was denied that it existed. The colonial experience broke the creative process because new stories would have been created along the way, up to the 1960s. But this chain was broken, which is why every time you ask we skip colonialism, because we’re very sure of that.”\nUganda became a protectorate of the British Empire from 1894 until independence in 1962. Makumbi speaks about the amount of time spent by Ugandans trying to locate themselves within the colonial framework. Much of the time during that period was spent translating Ugandan traditions and culture into something that Britain could recognise.\n“I can’t find the Sixties; I don’t know them. I don’t even know the Seventies, even though by then I was born. All I can retrieve quite comfortably are the Eighties, when I was a girl. The Seventies, I can only remember my parents’ voices, my parents’ fears. So when you are looking for the stories in the Sixties and in the Seventies, look back at the history of those times because people create stories when they are comfortable. When there’s peace.”\nI tell her that in recent years my mother has said that her generation doesn’t talk about growing up under Amin and surviving others,such as Milton Obote I and Obote II. She uses colloquial shorthand for his two presidential reigns. Born in 1964, it was the silences that drove her to write her own memoir, titled Flame and Song.\nFor Makumbi, the rise of the middle class in the 1960s encouraged a new kind of writer. She says she and my mother were afforded a kind of luxury that their parents were not. “My father could not have written the story I have written. My grandfather could not because they could only write what [Chinua] Achebe wrote.”\nMakumbi resigns herself to telling their story for them but in a way that is truthful to her. She says she was put off by the way people spoke about Amin in Britain. They spoke about him as though he was a monster born from fairytales.\n“I wanted to make Amin human and now, in the second book, I do talk about it. But I still talk about it in that way of silences. Of explosions and silences, because that’s the way he came down to me.”\nI think of my parents. My own father’s past only spills out through the anecdotes of others. Jolted back into the scenes, recast. Listening to Makumbi, I try to imagine my father in his youth — tall, lanky and with his eyes too far apart. His stories coming in bursts.\nThis is how I found out about my father’s recurring dreams about soldiers, or how my parents and my father’s siblings were on their way to a gig and accidentally went down the wrong road. They were stopped by soldiers and a soldier attempted to ram his gun into my father’s head. My father hadn’t seen the soldier lift his gun and he accidentally shifted the car forward. They were pardoned that night.\nThese stories are always invoked, never shared voluntarily.\n“Some of the things that turn up in our books or in history, they are supposed to awaken the memory,” Makumbi says, snapping me out of my reverie. “It’s another aspect of the historical novel. The way it nudges, the way it awakens memory around that place, around the event, around the person. In a way, I think, if we start writing about, for example, those silences that you find around your family, they will awaken [memory].”\nKintu asks Ugandans to reframe themselves in the context of history by asking different questions. The book even questions the advent of homophobia in the Ganda tradition, when Makumbi writes about homosexuality as being quite normal. She discusses the expansion of the kingdom of Buganda and brings to the surface tribal tensions and questions of nationhood. Mental illness, the syncretism of Christianity with traditional beliefs, and the fear of HIV are all gently nudged in her writing.\n“I’m waiting to see what will happen when my second novel comes out, because then it talks about a very different aspect of Idi Amin, and we’ll see what that will do to people in their 50s, in their 60s, and some in their 40s.\n“You know, sometimes, we tend to forget what would be painful and some of that history is lost like that. Not intentionally but because the mind does not want to remember.” She pauses. “Now that we are 40 years away from the 1970s, what can people remember? Are they safe [from] those memories now that there are 40 years?”\nRead more from Faye Kabali-Kagwa\nCreate Account | Lost Your Password?","Ask a Ghanaian (adult) reader what books she read in her teens and she will list books by Enid Blyton, the Nancy Drew mysteries and Mill and Boon romance novels. Oh, and Tintin comics. What you will not hear are books by Ghanaian and African writers. There were very few books written for Ghanaian/African teenagers and young adults when I was growing up. So, I skipped that crucial stage in my reading life and proceeded straight to “serious” African literature, since there were plenty of those books in my household. The dearth of African YA literature persists. But, the publication of Between Sisters (2010) mollifies our screams for redress.\nThis is the story of Gloria Bampo, a teenager who lives with her parents and sister, Effie, in a suburb of Accra. The novel begins on the fateful day that Gloria will receive her Junior Secondary School (JSS) examination results. Like numerous Ghanaian teenagers, these results will likely decide her entire future.\n“Nii Tetteh, Kofi Andah, Gloria Bampo…” she (her teacher) called, checking our names off a list.\nWe made a single fine and walked up the corridor toward the office. In our school we filed for everything!\nWe crowded around the notice board searching for our names. I held back, almost too afraid to look.\nThere was my name, third on the list. I was the first to fail. Out of fifteen subjects I had failed thirteen, passing only needlework and art.\nGloria’s father has been unemployed for two years and her ailing mother supports the family on meager profits earned from selling medicinal herbs. They cannot afford to pay for Gloria to re-take the JSS exams nor can they afford to enroll her in any vocational training. It is decided, without any input from Gloria, that she will work as a nanny and a housekeeper for Christine, a medical doctor. The set-up is presented as a living arrangement between ‘sisters’ even though there is no discernible relationship between Gloria and Christine. Effie, always the bold and outspoken sibling, will not be fooled and says “only the poor give away their daughters like this”.\nGloria handles her duties well and adapts to Christine’s middle-class community of doctors and nurses. She is amiable and very quickly befriends some of the other teens in the community. Her new BFF is Bea, a smart and ambitious girl who is neglected by her doctor father. Gloria and Simon, an older teen, form a youth band and a budding romance begins between the two. What happens to Gloria is nothing out of the ordinary. Except she and Bea are attractive girls and older men begin to show interest in them; older men who are quite skilled in spotting a likely teenage prey. In West Africa, we call these predators sugar daddies. Their arsenal are their wealth and prestige. It was alarming to watch a sugar daddy prey on Gloria but also heartening to see how she negotiates her life through this particular mess. Gloria is a strong and an amazing young woman. She is very perceptive and brave. Her ability to understand and articulate what is happening to her and around her is extraordinary.\nIt is clear that the author, Adwoa Badoe, intends for the reader to always question how a young girl such as Gloria ends up as a nanny and housekeeper. Such an interrogation leads us back to Gloria’s education. Gloria is a functional illiterate after spending nine years in Ghana’s educational system. No one has bothered to read to or with this child; not her teachers and certainly not her parents. She was promoted year after year till she failed the first mandated national examinations. This happens to thousands of Ghanaian children every year who if they are to further their education, will need an extraordinary intervention. For Gloria, this intervention is Christine when the latter discovers Gloria’s problem. Most children in Gloria’s situation are not so lucky.\nThe book is also critical of the practice of unpaid housekeeping work when it is performed by young girls. Christine is to pay for Gloria’s vocational training and in gratitude, Gloria works for free. Christine admits to her husband that:\nI don’t pay her. I’ve taken over her upkeep and future education.\nHer husband is not pleased with the arrangement but Christine defends herself, saying “we are like a family”. To which her husband replies: “The operative word here is like, Christine.”\nI could go on about all the issues that are interrogated in this short book. But I will stop here. Adwoa Badoe, at the Accra launch of the West African edition of Between Sisters, admitted to using her writing to address social issues. And she has done that perfectly in this book. We , in Ghana, are quite familiar with Gloria’s journey. Most of us are either related to or we employed a girl like Gloria. Adwoa Badoe is imploring us to do right by them. To respect and uphold their human rights, especially their right to both a quality education and a protected and safe childhood.\nSo, is the book entertaining? Absolutely. It is well-written. The language is simple but its meaning profound. Urban Ghanaian life, with its sounds, sights and smells comes alive in this book. You will be crying and laughing with Gloria, who has a great voice and a strong point of view. I am proud of this Ghanaian girl. Now, all we need to do is get this book into the hands of young readers and of course, write more YA literature. We have readers to grow!\nBetween Sisters is highly recommended.\n(Review copy of book given to me by Smartline, the publishers of the West African edition. The book is sold at Amazon and at The Book Depository. In Ghana, check your local bookstore. )"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:dc5f74aa-1705-487f-8dc7-88fff108ef86>","<urn:uuid:63c0be23-aefa-455b-82ce-82d3f102e8c6>"],"error":null}
{"question":"As someone exploring DIY music spaces and indie labels, what are the key differences between Hungry City and Trans Trenderz in terms of their artist payment models and support systems?","answer":"Hungry City and Trans Trenderz have different but supportive payment approaches. Hungry City pays their artists, bartenders, sound mixers, lighting folks, and security well, while keeping entrance fees low. They prioritize fair compensation over profits, with leftover money going to cover costs for future shows. Trans Trenderz, on the other hand, operates through The Ghostly Beats Project, which covers release costs for Black Trans musicians and mobilizes industry allies to donate their professional services, including studio time at facilities like Studio G Brooklyn. Both organizations prioritize creating supportive environments for artists, though their specific support systems differ - Hungry City focuses on providing high-quality live performance spaces with professional sound equipment, while Trans Trenderz emphasizes recording opportunities and industry resources for Black Trans artists specifically.","context":["Indie label, TRANS TRENDERZ, launches THE GHOSTLY BEATS PROJECT\nin collaboration with STUDIO G\nThe label seeks to mobilize allies in the music industry to offer resources to Black Trans musicians\nBorn out of a desire to break the glass ceiling for Black Trans artists, indie label, Trans Trenderz, seeks to assure that the artistic contributions of Transgender artists are never erased. Providing artists with resources, opportunities, and community, they aim to bring Black Trans music to the mainstream.\nTrans Trenderz was launched in 2016 when artist Blxck Cxsper curated and produced a mixtape of the same name, featuring 14 Trans artists. The feeling at the release party was unlike anything they had experienced before. “You could feel how excited people were to finally see Trans folks on stage. It didn't feel like a show, more like a family reunion,” Blxck Cxsper explains. “There was no hierarchy between the artist and the audience, everybody was contributing to the love in that room in their own way.” Wanting to continue to provide that space for themselves and other like-minded artists, Trans Trenderz officially evolved into a record label, signing Transgender and gender non-conforming acts.\nIn June of 2020, when the Black Lives Matter movement regained traction, Blxck Cxsper offered to make beats for other Black Trans artists. Within days, 30 artists reached out to them to collaborate. Around the same time, Myla, a Black Trans woman from Dallas, TX, contacted Blxck Cxsper wanting to donate some of her PUA money to help Black Trans people. They decided to use the funds to cover release costs for Black Trans musicians, and The Ghostly Beats Project was born. As soon as it was announced, Black Trans artists and allies alike began to get in touch to become involved. Skilled professionals from mixing engineers to graphic designers wanted to donate their talents to the cause. Among them was notable producer/engineer Joel Hamilton from Studio G Brooklyn. Hamilton offered to partner with Trans Trenderz to bring Black Trans artists into the facilities to record singles, with the goal of bringing the songs onto mainstream platforms. He says, “Creating an environment where artists are able to thrive is the reason that Studio G Brooklyn exists. Partnering with Trans Trenderz feels like an organic extension of the founding principles that are at the core of how we do business. We are proud to be literally amplifying voices in the music scene (and society) that have been overlooked for far too long. With Trans Trenderz’s partnership and guidance, we hope to help create new opportunities for their artists.”\nBlxck Cxsper met Steph Durwin in the summer of 2018 when Durwin started working with them on Trans Trenderz. Durwin had graduated with an audio degree from The New England Institute of Art in Boston and had subsequently worked as an audio engineer at Mad Oak Studios and Berklee College of Music. After a few years playing in folk-rock band, Box of Birds, who were nominated for a 2016 Boston Music Award for Best Folk Band, Durwin moved to New York City and started interning for Studio G Brooklyn.\n“I began my solo career and released my first album, HomeGrown, under Trans Trenderz using recordings from before my transition, and mixing them with vocals with my new deeper voice post starting hormone replacement therapy,” says Durwin. \"When I first began my transition, I didn't know many trans people, but the ones I did were white trans people. Among them and the other white trans folks I saw online, the struggles in that community centered around transition-related issues, like surgeries and hormones and name changes. I didn't realize how different the struggles were in the black trans community though until I started working with Trans Trenderz.”\nDuring Durwin’s time with Trans Trenderz, he saw multiple artists experience homelessness. He says, “The question of gender-affirming surgeries were not even on the radar of many of them because their focus had to be on how they were going to eat, how they were going to survive. Additionally, I've witnessed explicit racism even while they were just trying to do their jobs. Despite all those struggles, they are able to make significant contributions to the artistic community. Super super talented, and remarkably skilled performers. Working with Trans Trenderz showed me the importance of not only uplifting, but prioritizing those voices, not because it is a duty. But because they deserve to be in those spaces.\"\nThe first two Trans Trenderz artists to record at Studio G will be indie-folk/soul artist, Apollo Flowerchild, who says of the opportunity, “To be very honest my goals for my music career have never been bigger than just making songs to survive so getting a chance to give my message to more folks who want to listen is nothing short of a blessing,” and electro-pop artist, Heather Hills. Says Hills, “Life taught me that everything that I am will hinder my progress in life. If I wanted to move forward successfully, I would have to mold myself into an ever-changing idea. This opportunity, to me, presents itself as an award for always refusing to conform. It represents the value in following your OWN path and showing the world that what makes you different, makes you powerful.”\nThis life approach, of course, is not lost on Blxck Cxsper whose own trajectory is a standard-bearer for self-empowerment and activism: “For me I’m French, grew up close to Paris, in Niger, and spent my high school years in Washington DC where I joined my first band. I didn’t speak English until I was 16 years old, but since I was a child my dream had always been to make music in the US.”\nAfter high school, they moved to Montreal to attend the Mel Hoppenheim School of Cinema at Concordia University. In 2014, they came out as Trans while overlapping a band situation with a solo career, releasing three albums, the last one in French, which broke open a slew of opportunities for inclusion on mainstream Spotify playlists based in Canada. Subsequent activity saw significant press uptake, including top outlets like the CBC, Radio Canada, Vice and Buzzfeed, among others, taking focus on Transness.\nIn 2019, they adopted the moniker Blxck Cxsper: “I decided to officially change my name and pronouns to better reflect the fact that I identify as non-binary, and because I felt like having a white man’s name in front of my last name was doing a disservice to my ancestors who were slaves.” They performed 2 back-to-back shows on the main stage of Pride Canada in 2017, as well as at the Queer Liberation March in NYC in 2019. Also worth noting that Blxck Cxsper led the Black Lives Matter action at Pride Montreal in 2017 which topped the ‘best queer moments’ list that year in Afropunk.\nTrans Trenderz plans to expand The Ghostly Beats Project beyond NYC, and are currently exploring studios in other cities, as well as seeking Black Trans talent worldwide. Both Apollo Flowerchild and Heather Hills will be releasing music in 2020.\n“There isn’t one way to look or sound Trans, and we want our repertoire to showcase that,” says Blxck Cxsper. “People in our communities are groundbreakers and we are expecting genres to mash and merge and new sounds to be created.”\nBlack Trans record label drops new mixtape (9/4/2020)","DIY Venue Hungry City Creates A Space for Artists to Grow & Flourish\nWritten by Vocalo Radio on October 2, 2019\nHere at Vocalo we understand that great music is born of scenes, and there are no stronger music scenes than the ones that we boast here in Chicago…\nOftentimes these scenes are stewarded by spaces, places where creativity can flourish, where connections can be made, where art can be expressed. Hungry City Collective is one of those places.\nHungry City is a community based art space that defies genre. Their goal is to give bands a platform to do whatever they want creatively. It’s a DIY space but it is unique in that they pay their artists, bartenders, sound mixers, lighting folks, and security.\nThe stewards and founders of the space are David Gates, Robert Rashid, and Zachary Sogh (also known as Woes). Woes has been a fixture on Vocalo lately and it’s his collaboration with the Hungry City folks and their band Spacebones that tipped us off to the power of this space. As a special treat, the guys gave us a brand new Spacebones and Woes track to premiere exclusively…\nWe chatted with founder and all around go-to-guy David Gates about the advantages of DIY, seasonal depression, and why community policing works when people feel supported…\nBreak down the team involved in Hungry City… Who keeps the place running?\nSound, lights, and most tech work is run by our man Dan Letchinger. Dan is a sound wizard, and has installed professional audio into the space. He runs a 22 channel Soundcraft Signature board, with a sweet new PA and great floor monitoring. So we are happy to boast that we have professional – high quality sound, not only for the audience, but also for performers; something that makes a world of difference for a live performance.\n“If the artists hear themselves well, the audience will hear them even better” – Dan Letchinger. As artists, we cant tell you how many “real” venues we play that have poor on stage monitoring; so this is a priority for us.\nOverall production is done by David Gates & Robert Rashid (@_Spacebones_) and Zachary Sogh (@boiwoes). Ah, production – what a fun word. There’s a lot going on at all times, before, during, and after these events. We book bands typically 4 weeks or more in advance. Theres a long waiting list and that sucks, because we hate turning away artists who want to perform; but we try to limit shows to twice a month.\nDuring events, we are floating, checking in on tech, running lights, working the door, helping with stage changeovers, keeping people out of the street in front and back, most importantly, making sure everyone is safe and comfy. Patrick Hubbard, Tyrone Reed, and Steve O’toole help all around as well.\nWhat is the mission of Hungry City? How does Spacebones play into that vision?\nHungry City keeps it simple. Our priority first and foremost is to build community. This has come to life mainly through our event space at Division and Western in Chicago’s Humboldt Park neighborhood. We book shows – for Chicago (and touring) artists of all mediums. Yes we focus mainly on musical performers, but we try to round out the events by inviting multimedia artists to sell or display their work throughout the space.\nAside from the event space, Hungry City Collective is also the overhead for our personal project Spacebones. We release, promote, and market our music through Hungry City Collective. As artists ourselves, we know what it’s like to book, promote, and play shows – maybe that gives us an edge… but it at least gives us real insight into what makes a great event and performance happen; in contrast to bar or club owner, whose main priority is to pull a profit each night.\nIt’s no secret this venture isn’t a big money maker, we keep entrance fees low, and we pay our artists and staff well – whatever’s left over goes into covering costs for the next show. I think, at least hope, people see Spacebones as a band that loves to consume music as much as create and perform it.\nWe love the music we are hearing from the Chicago scene, and nothing’s more fun than putting our friends on stage and watching them rock out.\nWhat role do DIY spaces play here in Chicago’s music scene that bars or venues do not?\nAttentive audiences is #1. Sure, some people just come to party – but for the most part audiences come to listen, watch, consume, and participate in the performances. Shows at bars and clubs might have an in-house crowd of people who have just come to drink. Thats fine, but these DIY spaces are art centered. Payouts to artists I think are often better than at venues – this depends on the ticket prices and draw of course – but from our experience, people running DIY venues are much more generous about paying out bands for the hard work they put into performing that night.\nSafety is another big thing – this is a tough one – an eternal struggle that we are still looking to improve with each show. But overall, there’s usually 1-2 door people/security at bars/venues…at DIY spaces everyone seems to be part of the policing process. We are happy to kick out people who are making anyone else feel uncomfortable. Community policing works!\nLastly, intimacy – at DIY spaces you are closer to the performers. You can see and often smell them – yikes.\nWe’ve talked about toxic patterns that can exist in established venues…What steps do you take to make Hungry City a safe space, especially a safe space for those from traditionally marginalized communities?\nOops, I touched on this above. But I will re-iterate: Community policing works when people speak up and feel supported.\nAt a venue or unfamiliar space, people might feel uncomfortable “causing a scene” by reporting something that makes them feel uncomfortable. At our space, we hope, people know David, Robert, Zack etc… our names and faces; and know they can come right to us and say “Hey that dudes being a jag off…”.\nWe have 2-3 door people working the front and back that keep an eye on things, while we float around the venue checking in on everyone for this reason. You ask about traditionally marginalized communities… as a white male (David speaking specifically here) I will never know what it’s like to live in another’s person’s shoes. So all we can do is have a zero tolerance policy for abuse or disrespect to and from anyone who walks into our space. As far as putting together bills, we do keep an eye especially on the fact that there are a lot of dudes in bands…We try to balance most bills to make sure everyone is getting a chance to take the stage. It is not easy; but its always on our minds.\nSame goes for genre – and the crowd each attracts. We host mostly rock shows, but try to switch it up every month or so and throw a hip hop event – this October 12th is a big one…Woes onstage baby.\nYou talk about how you feel you’ve gotten your start late in the game in Chicago’s DIY music community…\nWhat advantages do you think there are to the delayed start? How has that helped shape your mission and your execution of said mission?\nMmmhh, tough one. I would say our growth as humans outside of art – might be our greatest strength now. Robert and I both had totally opposite careers before starting these projects. We are blessed to have seen the world from many angles. Both traveled and worked internationally – Robert as an athlete and options trader, and myself as a teacher and volunteer. We have experimented…and I think that’s the key to happiness, and inevitably, success.\nYou need to push yourself, try everything out there. Comfort is a luxury, and for us, might be a sign of stagnation. If we get too comfortable we know something new needs to happen.\nFear is fantastic; if you’re trying to do big things, make change, or feel inspired…and you’re not at least a bit scared of what’s to come…you’re probably not pushing yourself hard enough. Shouts to Moms, Dads and jojos out there for hammering this into us.\nWhat other folks, organizations, or spaces in Chicago resonate with or inspire you?\n#1 hands down is Alex and Francis White of White Mystery. They are the DIY OG’s. Alex is the rock queen of Chicago. She is so kind, so talented; an awesome leader and someone to look up to in the scene. Alex has a hand in every event, and is one phone call away for any advice. White Mystery as a band is absolutely rad. They are a two piece and they rock the F out. I don’t know how many times we have seen them live. They are Independent with a capital I. Follow them now.\nThe Litterbox is absolutely killing it. A venue run by 3 badass women in logan. They have outdoor shows all summer – every event is packed – and produced amazingly. Great sound, great atmosphere, unique as hell. They are so kind, and have been supportive of what we are doing as well. Most or all of them are also artists, so they get it. Shouts Maddie, Kaitlyn & Jackie.\nNude Beach Collective – Insane. Constant shows, great production, projection, sound, lights – a huge warehouse space. The nicest dudes run the place. Spacebones played their single release show for MONSOONS – last friday. It was nuts. Shouts Danny and Ruby – thank you for having us.\nCharm School is dope – super intimate.\nBerenice House – an awesome garage style spot up north.\nCailey Davern of Underground Apex is amazing. Great photography and coverage of shows. She shot our EP cover; Super supportive of Chicago artists and has been covering all of these events, in and out of the DIY scene. She seems to be at every single show going on in the city – she must have clones. So much energy and dedication to what she’s doing. Someone to watch.\nIn another lane stands the art and music programs at Misericordia Home – a home for over 600 children and adults with developmental disabilities; we are so inspired by them. The heart Zingers are their choir, and the heartbreakers are their dance troupe.\nThe staff and residents put so much work into these programs – and prove that art has an invaluable power in every community. There are no boundaries to art – no limitations – it is accessible to everyone no matter what abilities or disabilities you might have.\nThe Artist in All is the coolest Art show in the city – held every spring at the Art Institute of Chicago; showcasing the work of the residents in the art program. This was an event that started small, scaled, and is now enormous – touching so many lives. As far as we are concerned Misericordia and their staff are the leaders of community art in Chicago; giving the most vulnerable populations a platform to share their passions with the world. This is Rock n’ Roll.\nWhat’s your favorite show that you guys have thrown in the last year?\nWe had Girl K, Rookie, Ex Okays, and Towncriers I believe all in one night. Insane performers. To have them all on the same bill was mad cool.\nOther notables are Sean Green – such a talented human being- he sings, writes, plays, and has a damn good whistler – playing “Millennial Jazz” the sweetest dude ever. Someone to see live.\nTell us a bit about the song we’re premiering today.\n“-22, +27” is like Hungry City smashed into one track. The song was written the week of the polar vortex in Chicago. It hit -22 degrees, and seasonal depression threw an uppercut. I was thinking about loved ones I missed, and how they never fade away and how growing up was inevitable. My 27th birthday had just passed. Our heat went out a few days that week.\nEverything felt heavy, and I knew I was’t the only young Chicago artist feeling this way. I looked around and realized I was surrounded by amazing positive humans like Robert and Zack, who just keep a smile on my face at all times. It was negative 22, but I was positive 27. The track is kinda indie pop rock with calm vocals to start – and out of nowhere comes Yomí the harpist. Her voice and her harp took this track to another level. Hard to describe her sound and contribution, but she is in a sonic lane of her own.\nVerse two comes around and Woes comes in hot. As always, he’s blatantly honest, clever, and unique in his delivery. He manages to tell a specific story that is extremely personal to him – yet in a way that feels universal to anyone who has felt heartbreak.\nThe song is for everyone, from everyone. O yea – and for some shameless self promotion – our EP Boy Am I Glad To See You – drops this Friday 10/4 – its a bop. Check it out everywhere.\nListen to -22,+27 by Spacebones ft. Woes and Yomí below\nWritten by: Seamus Doheny\nNext Hungry City Events:\nOctober 12th – Woes\nOctober 26th – Spacebones EP Release Show and Tour Sendoff with Engine Summer (single release), Towncriers and more\nDM for the address to these events: @_Spacebones_\nP.S. – Any Yogis out there want to help us put some Yoga events together at Hungry City? And/or mindfulness & meditation groups. Looking to stretch our brains and bodies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:d5386be9-3b31-43d8-919b-8c0fc8b71e27>","<urn:uuid:ad469529-b50b-4223-9261-cbbc5c612104>"],"error":null}
{"question":"How do the E-MAP genetic screening approach and RNA polymerase II functionally relate to gene expression regulation?","answer":"The E-MAP genetic screening approach and RNA polymerase II are interconnected in gene expression regulation. E-MAP technology helps quantify genetic interactions and examine point mutants of essential genes, specifically focusing on RNA polymerase II and its role in gene expression. RNA polymerase II is a complex 12-subunit enzyme (~550kDa) located in the nucleus that synthesizes nuclear pre-mRNAs and is responsible for transcribing most eukaryotic genes. It contains transcription factors and transcriptional regulators, and works with proteins that enable specific binding, DNA unwinding, and transcription speed control. The E-MAP approach allows mapping of structural features onto functional roles and characterizing relationships between RNA polymerase II and the wider gene expression apparatus.","context":["We have recently developed a genetic screening approach, termed E-MAP (Epistatic MiniArray Technology Profiling), that can quantify the strength of systematically generated pair-wise genetic interactions. The method identifies negative double mutant interactions, where combination of mutations causes defects leads to enhancement of growth defects or lethality. Such interactions often specify membership of parallel biochemical pathways. Additionally, E-MAP also identifies positive interactions, where combination of mutants show mutual suppression or lack any additive defects, which are enriched among physically interacting gene products. Using gene deletions of non-essential genes and hypomorphic alleles of essential genes, we have recently generated E-MAPs in S. cerevisiae that have focused on 1) the early secretory pathway and 2) chromosome biology, which includes transcriptional regulation, chromatin remodeling and DNA repair. We now propose the second generation of E-MAP analysis, allowing us to address the next level of complexity, via examination of point mutants of multifunctional and essential genes. Specifically, we will genetically dissect two essential, multisubunit, multifunctional complexes at the heart of gene expression and chromatin structure: RNA polymerase II (RNAPII) and the nucleosome. This approach will allow us to A) map the structural features of these complexes onto their functional roles, and B) characterize the functional relationships between RNAPII and the nucleosome and the wider gene expression apparatus.\nIn Aim #1, we will use the chromosome biology E-MAP to genetically examine a set of approximately 450 histone H3 and H4 mutants including A) complete alanine (or serine)-scans, B) comprehensive substitution of modifiable residues, and C) semi- systematic deletions of the N-terminal tails. The work, which is being done collaboratively with NIH Roadmap TCNP (Technology Center for Networks and Pathways) of Lysine Modification (PI Jef Boeke), will help reveal how histone-histone and histone-DNA contacts and histone modifications influence the steps of transcription and chromatin regulation.\nIn Aim #2, we will screen approximately 100 distinct and diverse point mutants of several essential RNAPII subunits isolated in collaboration with Craig Kaplan and Roger Kornberg.\nIn Aim #3, we will subject these data to hierarchical clustering and our recently developed metrics (S- and COP-scoring systems) to help identify functional relationships using the E-MAP data. We will also employ newly developed algorithms that identify functionally related sets of genes (or modules) from large-scale interaction datasets and allows for multi-functional genes to be members of more than one module. We anticipate that a systematic genetic approach described here will provide a more holistic view of chromatin function and transcriptional regulation in eukaryotic cells. Project Narrative/Relevance to Public Health The interplay between the gene expression machinery and the compaction of genetic material into chromatin is now recognized as highly dynamic and complex. These processes must be tightly controlled for normal cellular physiology and to prevent disease states such as cancer. We will use a systems approach to genetically dissect the structural and functional relationships between the highly conserved components of these pathways in the budding yeast, S. cerevisiae, in order to more easily understand and eventually manipulate the vast complexity of human cells.\n|Janke, Ryan; Kong, Jeremy; Braberg, Hannes et al. (2016) Nonsense-mediated decay regulates key components of homologous recombination. Nucleic Acids Res 44:5218-30|\n|Patrick, Kristin L; Ryan, Colm J; Xu, Jiewei et al. (2015) Genetic interaction mapping reveals a role for the SWI/SNF nucleosome remodeler in spliceosome activation in fission yeast. PLoS Genet 11:e1005074|\n|Ding, Li; Paszkowski-Rogacz, Maciej; Winzi, Maria et al. (2015) Systems Analyses Reveal Shared and Diverse Attributes of Oct4 Regulation in Pluripotent Cells. Cell Syst 1:141-51|\n|SÃ¡nchez, Arancha; Roguev, Assen; Krogan, Nevan J et al. (2015) Genetic Interaction Landscape Reveals Critical Requirements for Schizosaccharomyces pombe Brc1 in DNA Damage Response Mutants. G3 (Bethesda) 5:953-62|\n|Martin, Humberto; Shales, Michael; Fernandez-PiÃ±ar, Pablo et al. (2015) Differential genetic interactions of yeast stress response MAPK pathways. Mol Syst Biol 11:800|\n|Braberg, Hannes; Alexander, Richard; Shales, Michael et al. (2014) Quantitative analysis of triple-mutant genetic interactions. Nat Protoc 9:1867-81|\n|MartÃn, GlÃ²ria Mas; King, Devin A; Green, Erin M et al. (2014) Set5 and Set1 cooperate to repress gene expression at telomeres and retrotransposons. Epigenetics 9:513-22|\n|Moehle, Erica A; Braberg, Hannes; Krogan, Nevan J et al. (2014) Adventures in time and space: splicing efficiency and RNA polymerase II elongation rate. RNA Biol 11:313-9|\n|Kabeche, Ruth; Roguev, Assen; Krogan, Nevan J et al. (2014) A Pil1-Sle1-Syj1-Tax4 functional pathway links eisosomes with PI(4,5)P2 regulation. J Cell Sci 127:1318-26|\n|Braberg, Hannes; Moehle, Erica A; Shales, Michael et al. (2014) Genetic interaction analysis of point mutations enables interrogation of gene function at a residue-level resolution: exploring the applications of high-resolution genetic interaction mapping of point mutations. Bioessays 36:706-13|\nShowing the most recent 10 out of 47 publications","Figure: RNA polymerase. Image Source: Wikipedia. Image created with biorender.com.\nRNA polymerase definition\nRibonucleic Acid (RNA) Polymerase (RNAP) enzyme is a multi-subunit enzyme that applies its activity in the catalyzation of the transcription process of RNA synthesized from a DNA template.\n- And therefore, RNA polymerase enzyme is responsible for the copying of DNA sequences into RNA sequences during transcription.\n- The function of RNA polymerase is to control the process of transcription, through which copying of information stored in DNA into a new molecule of messenger RNA (mRNA.)\n- During transcription, the RNA polymer is contemporary to the template DNA that is synthesized in the direction of 5′ to 3′.\n- The enzyme RNA polymerase interacts with proteins to enable it to function in catalyzation of the synthesis of RNA.\n- The collaborator proteins assist in enabling the specific binding of RNA polymerase, assist in the unwinding of the double chemical structure of DNA, moderate the enzymatic activities of RNA polymerase and to control the speed of transcription.\n- The RNA polymerase enzyme has an interrupted mechanism whereby it continuously synthesizes RNA polymers of over four thousand bases per minute but they pause or stop occasionally to maintain fidelity.\n- RNA polymerase is an enzyme that is responsible for copying a DNA sequence into an RNA sequence, during the process of transcription. As a complex molecule composed of protein subunits, RNA polymerase controls the process of transcription, during which the information stored in a molecule of DNA is copied into a new molecule of messenger RNA.\n- RNA polymerases have been found in all species, but the number and composition of these proteins vary across taxa.\n- For instance, bacteria contain a single type of RNA polymerase, while eukaryotes (multicellular organisms and yeasts) contain three distinct types.\n- In spite of these differences, there are striking similarities among transcriptional mechanisms.\n- For example, all species require a mechanism by which transcription can be regulated in order to achieve spatial and temporal changes in gene expression.\nTypes of RNA polymerase\nProkaryotic (Bacteria, viruses, archaea) organisms have a single type of RNA polymerase that synthesizes all the subtypes of RNA, while eukaryotes (multicellular organisms) have 5 different types of RNA polymerases which perform different functions in the synthesis of different RNA molecules.\nProkaryotic RNA polymerase\n- The prokaryotes have a single type of RNA polymerase (RNAP) which synthesizes all the classes of RNA, i.e mRNA, tRNA, rRNA, sRNA.\n- The RNA Polymerase molecule is made up of 2 domains and 5 subunits:\n- Core and holoenzyme\n- Subunits (β, β’, α (αI and αII), ω,)\n- The promoter is the sequence of DNA that is required for accurate and specific initiation of transcription, and also, it is the sequence of DNA to which RNA polymerase binds accurately to initiate transcription.\n- The ‘a’ subunit is made up of two distinct domains. The N-terminal domain (a-NTD) and the C-terminal.\n- The N-terminal is involved in dimerization forming a2 and further assembly of the RNA polymerase.\n- The C-terminal domain functions such as binding to the Upstream Promoter (UP) DNA sequence at promoters for rRNA and tRNA genes and in communication with several transcriptional activators.\n- Each of the subunit structure is as follows:\nProkaryotic RNA Polymerase Subunits\n|β||150.4 kDa||The β’ + β form the catalytic center, responsible for RNA synthesis.|\n|β’||155.0 kDa||The β’ + β form the catalytic center, responsible for RNA synthesis.|\n|α (αI and αII)||36.5 kDa||It is made up of the enzyme assembly, and it also binds the UP sequence in the promoter.|\n|ω||155.0 kDa||It confers specificity for promoter; and binds to -10 and -35 sites in the promoter.|\nEukaryotic RNA polymerase\n- There are 5 known types of RNA polymerases each responsible for the synthesis of specific subtypes of RNA. These include:\n- RNA polymerase I that synthesizes a pre-rRNA 45S (35S in yeast), which matures and forms the major RNA sections of the ribosome.\n- RNA polymerase II synthesizes precursors of mRNAs and most snRNA and microRNAs.\n- RNA polymerase III synthesizes tRNAs, rRNA 5S, and other small RNAs found in the nucleus and cytosol.\n- RNA polymerase IV and V found in plants are not well understood, however, they make siRNA. The plant chloroplast encodes the ssRNAPs and uses bacteria-like RNA Polymerase.\n- Each of the nuclear RNA polymerases is a large protein molecule with about 8 to 14 subunits and the molecular weight is approximately 500,000 for each.\n- They commonly have 3 subunits, a, b and b’. The largest subunits being b and b’.\n- These subunits are used as catalytic promoters and for assembly of proteins.\n- Each of these polymerases has a different function:\nRNA polymerase I\n- This enzyme is located in the nucleolus of the cell.\n- It is a specialized nuclear substructure where the ribosomal RNA (rRNA) is synthesized by transcription and assembled into ribosomes.\n- The rRNA are component elements of the ribosomes and are important in the process of translation.\n- Therefore, RNA polymerase I synthesize almost all rRNAs except 5S rRNA.\n- In yeast, the enzyme has a mass of 600kDa and 13 subunits.\nRNA polymerase II\n- This enzyme is located in the nucleus.\n- Most organisms that possess RNA polymerase II have a 12-subunit RNAP II (with a mass of about 550 kDa)\n- It is structurally made up of holoenzyme and mediators, with General Transcriptional factors (GTFs).\n- They contain transcription factors and transcriptional regulators.\n- It functions by synthesizing all proteins that code for the nuclear pre-mRNAs in eukaryotic cells (mRNAs in prokaryotic cells).\n- It is responsible for transcribing most of the eukaryotic genes and especially found in human genes.\nRNA polymerase III\n- It is located in the nucleus.\n- The RNA polymerase III has 14 or more distinct subunits with a mass of approximately 700 kDa.\n- Its function is to transcribe transfer RNA (tRNA), ribosomal RNA (rRNA), and other small RNAs.\n- Some of its target points are important for the normal functioning of the cell\nRNA polymerases IV and V\n- They are exclusively found in plants, and they perform combined action in the formation of small interfering RNA and heterochromatin in the cell nucleus.\n- In Plants, the RNA polymerase is found in the chloroplast (plastids) and mitochondria, encoded by the mitochondrial DNA.\n- These enzymes are much more related to bacterial RNA polymerase than to the nuclear RNA polymerase.\n- Their function is to catalyze specific transcription of organelle genes.\nFunctions of RNA Polymerase\n- Generally, the RNA molecule is a messenger molecule that is used to export information that is coded in DNA out of the cell nucleus, to synthesize proteins in the cell cytoplasm.\n- RNA polymerase is used in the production of molecules that play a wide range of roles, of which one of its functions is to regulate the number and type of RNA transcript that is formed in response to the requirements of the cell.\n- The RNA polymerase enzyme interacts with different molecular proteins, transcription factors, and signaling molecules on the carboxyl-terminal, which regulates its mechanisms, which play a major role in gene expression and gene specialization in multicellular (eukaryotic) organisms.\n- The RNA enzyme also ensures irregularities and errors during the conversion of DNA to RNA (transcription). Such as ensuring that the right nucleotide is added to the newly synthesized RNA strand, inserting the right amino acid-base which is complementary to the template of the DNA strand.\n- When the right nucleotides have been inserted, the RNA polymerase can then catalyze and elongate the RNA strand, at the same time, proofread the new strand and remove incorrect bases.\n- RNA polymerase is also involved in the post-transcription modification of RNAs, converting them into functional molecules that facilitate the transportation of molecules from the nucleus to their site of action.\n- Besides its role in the synthesis of proteins, RNA performs other functions such as\n- Protein coding\n- Regulation of gene expression\n- Act as enzymes\n- Formation of gametes by the non-coding RNA (ncRNA)\n- Production of regulatory molecules.\nReferences and Sources\n- 5% – https://healthjade.com/rna-polymerase/\n- 4% – https://wikimili.com/en/RNA_polymerase\n- 3% – https://biologydictionary.net/rna-polymerase/\n- 2% – https://zeusbiologicalfacts.blogspot.com/2013/10/rna-transcription-by-rna-polymerase.html\n- 2% – https://www.coursehero.com/file/62664703/RNAdocx/\n- 2% – https://healthjade.net/rna-polymerase/\n- 2% – https://bio.libretexts.org/Bookshelves/Genetics/Book%3A_Working_with_Molecular_Genetics_(Hardison)/Unit_III%3A_The_Pathway_of_Gene_Expression/10%3A_Transcription%3A_RNA_polymerases\n- 1% – https://www.sciencedirect.com/topics/neuroscience/rna-polymerase-iii\n- 1% – https://quizlet.com/20545728/rna-transcription-flash-cards/\n- 1% – https://pediaa.com/difference-between-prokaryotic-and-eukaryotic-rna-polymerase/\n- 1% – https://firstname.lastname@example.org/Book%3A_Biology_for_Majors_I_(Lumen)/13%3A_Module_10%3A_DNA_Transcription_and_Translation/13.4%3A_RNA_Polymerase\n- 1% – https://bio.libretexts.org/Courses/Manchester_Community_College_(MCC)/Remix_of_Openstax%3AMicrobiology_by_Parker%2C_Schneegurt%2C_et_al/06%3A_Mechanisms_of_Microbial_Genetics/6.03%3A_Structure%2C_Function_and_Production_of_RNA\n- 1% – http://www.bx.psu.edu/~ross/workmg/TxnRNAPolCh10.pdf\n- <1% – https://www.thoughtco.com/normal-cells-versus-cancer-cells-373383\n- <1% – https://www.slideshare.net/vinaypatel17/rna-polymerase\n- <1% – https://www.cliffsnotes.com/study-guides/biology/microbiology/dna-and-gene-expression/protein-synthesis\n- <1% – https://studenthomeworks.com/subunit-structure-of-rna-polymerase/\n- <1% – https://quizlet.com/19375916/biochem-t1l11-flash-cards/\n- <1% – https://microbenotes.com/rna-properties-structure-types-and-functions/\n- <1% – https://en.wikibooks.org/wiki/Principles_of_Biochemistry/Cell_Metabolism_I:_DNA_replication"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:79a29093-22f3-47d4-bed6-338b19a307ac>","<urn:uuid:60ed74e3-af0e-4d5e-861a-e46a3ee885f3>"],"error":null}
{"question":"How far away is supernova remnant RCW 86 from Earth?","answer":"The supernova remnant RCW 86 is located about 8,000 light-years away from Earth.","context":["Telescopes help solve ancient supernova mystery\n(PhysOrg.com) -- A mystery that began nearly 2,000 years ago, when Chinese astronomers witnessed what would turn out to be an exploding star in the sky, has been solved. New infrared observations from NASA's Spitzer Space Telescope and Wide-field Infrared Survey Explorer, or WISE, reveal how the first supernova ever recorded occurred and how its shattered remains ultimately spread out to great distances.\nThe findings show that the stellar explosion took place in a hollowed-out cavity, allowing material expelled by the star to travel much faster and farther than it would have otherwise.\n\"This supernova remnant got really big, really fast,\" said Brian J. Williams, an astronomer at North Carolina State University in Raleigh. Williams is lead author of a new study detailing the findings online in the Astrophysical Journal. \"It's two to three times bigger than we would expect for a supernova that was witnessed exploding nearly 2,000 years ago. Now, we've been able to finally pinpoint the cause.\"\nIn 185 A.D., Chinese astronomers noted a \"guest star\" that mysteriously appeared in the sky and stayed for about 8 months. By the 1960s, scientists had determined that the mysterious object was the first documented supernova. Later, they pinpointed RCW 86 as a supernova remnant located about 8,000 light-years away. But a puzzle persisted. The star's spherical remains are larger than expected. If they could be seen in the sky today in infrared light, they'd take up more space than our full moon.\nThe solution arrived through new infrared observations made with Spitzer and WISE, and previous data from NASA's Chandra X-ray Observatory and the European Space Agency's XMM-Newton Observatory.\nThe findings reveal that the event is a \"Type Ia\" supernova, created by the relatively peaceful death of a star like our sun, which then shrank into a dense star called a white dwarf. The white dwarf is thought to have later blown up in a supernova after siphoning matter, or fuel, from a nearby star.\n\"A white dwarf is like a smoking cinder from a burnt-out fire,\" Williams said. \"If you pour gasoline on it, it will explode.\"\nThe observations also show for the first time that a white dwarf can create a cavity around it before blowing up in a Type Ia event. A cavity would explain why the remains of RCW 86 are so big. When the explosion occurred, the ejected material would have traveled unimpeded by gas and dust and spread out quickly.\nSpitzer and WISE allowed the team to measure the temperature of the dust making up the RCW 86 remnant at about minus 325 degrees Fahrenheit, or minus 200 degrees Celsius. They then calculated how much gas must be present within the remnant to heat the dust to those temperatures. The results point to a low-density environment for much of the life of the remnant, essentially a cavity.\nScientists initially suspected that RCW 86 was the result of a core-collapse supernova, the most powerful type of stellar blast. They had seen hints of a cavity around the remnant, and, at that time, such cavities were only associated with core-collapse supernovae. In those events, massive stars blow material away from them before they blow up, carving out holes around them.\nBut other evidence argued against a core-collapse supernova. X-ray data from Chandra and XMM-Newton indicated that the object consisted of high amounts of iron, a telltale sign of a Type Ia blast. Together with the infrared observations, a picture of a Type Ia explosion into a cavity emerged.\n\"Modern astronomers unveiled one secret of a two-millennia-old cosmic mystery only to reveal another,\" said Bill Danchi, Spitzer and WISE program scientist at NASA Headquarters in Washington. \"Now, with multiple observatories extending our senses in space, we can fully appreciate the remarkable physics behind this star's death throes, yet still be as in awe of the cosmos as the ancient astronomers.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:4170fd35-e6a6-4580-98cc-ec1212bcf359>"],"error":null}
{"question":"What is splicing in gene expression?","answer":"Splicing is a mechanism where non-coding gene sequences (introns) are removed and the remaining protein-coding sequences (exons) are joined together to form a final, mature messenger RNA (mRNA), which contains the recipe for making a protein.","context":["One of those processes is called splicing, a mechanism by which non-coding gene sequences are removed and the remaining protein-coding sequences are joined together to form a final, mature messenger RNA (mRNA), which contains the recipe for making a protein.\nFor years, researchers have understood the roles played by the molecular machines that carry out the splicing process. But, as it turns out, one of those familiar components plays a new, and altogether unexpected role.\nAs senior author Gideon Dreyfuss, PhD, the Isaac Norris Professor of Biochemistry and Biophysics at the University of Pennsylvania School of Medicine and colleagues report in Nature, one of the splicing machinery's components called U1 has a second, equally important role in gene expression: To enable gene sequences to be read out into their RNA transcripts in their entirety, rather than have that process prematurely stopped. Dreyfuss is also a Howard Hughes Medical Institute Investigator.\nThe researchers revealed an unexpected function for U1 in protecting mRNA transcripts from premature termination in addition to and independent of its role in splicing.\nAs Dreyfuss puts it, \"U1 is a guardian of the transcriptome.\" The transcriptome is the set of all RNA molecules in one cell.\nU1 is one of a collection of RNA-protein complexes, called snRNPs, that recognize splicing junctions, excise non-coding gene sequences called introns, and join the remaining coding sequences called exons together. The Dreyfuss team previously showed that loss of SMN, a protein that helps assemble snRNPs and is deficient in individuals with the common neurodegenerative disease spinal muscular atrophy (SMA), results in altered snRNP levels and abnormal splicing.\nSMN deficiency affects all snRNPs to one degree or another. The Dreyfuss team wanted to find out what would happen if just one snRNP was missing. They started with U1.\nThe team's expectation was that they would detect an increase in unspliced RNA transcripts, and indeed they saw evidence of that. But, to their surprise, the majority of the genes produced a very different and striking result. Their transcripts terminated prematurely and abruptly, generally within a relatively short distance from the transcription start site of the gene.\nWhen they sequenced the ends of the resulting truncated RNAs, they found that they had been prematurely cleaved and tagged with a long string of nucleotide building blocks called adenine. This string is a hallmark of a process called cleavage-and-polyadenylation, which normally occurs at the end of a gene's RNA transcript. The lack of U1 was causing the cleavage/polyadenylation machinery to kick into gear early.\nThe implication, Dreyfuss says, is that U1's normal role, in addition to splicing, is to keep the cleavage/polyadenylation machinery in check until the RNA polymerase enzyme that synthesizes the transcript reaches its finish line. The researchers propose a model in which U1 binds throughout the nascent RNA transcripts, stymieing the cleavage/polyadenylation machinery that tags along with the moving polymerase complex. This in turn protects the many potential polyadenylation signals encountered along the way, and could explain the relative abundance of U1 in cells compared to other snRNPs. The additional U1 corresponds to the greater amount needed for its additional function.\n\"The transcripts are under constant threat from the cleavage/polyadenylation machinery,\" Dreyfuss explains. \"This machinery doesn't patiently wait for the transcript to reach the end of the gene; rather, the nascent RNA transcripts are subject to becoming attacked by this machinery. It's a constant danger they face, and the U1 snRNP suppresses it.\"\nIt is, he says, \"a novel role\" for U1, and a critical component for correctly making mRNA. \"It is essential for the integrity of the transcriptome, the landscape of all mRNA molecules in the cell,\" he concludes.\nOther coauthors on the paper include Penn researchers Daisuke Kaida, Michael Berg, Ihab Younis, Mumtaz Kasim, Larry Singh, and Lili Wan. The research was funded by the Association Française Contre les Myopathies and the Howard Hughes Medical Institute.\nPenn Medicine is one of the world’s leading academic medical centers, dedicated to the related missions of medical education, biomedical research, and excellence in patient care. Penn Medicine consists of the University of Pennsylvania School of Medicine (founded in 1765 as the nation's first medical school) and the University of Pennsylvania Health System, which together form a $3.6 billion enterprise.\nPenn’s School of Medicine is currently ranked #2 in U.S. News & World Report’s survey of research-oriented medical schools, and is consistently among the nation’s top recipients of funding from the National Institutes of Health, with $367.2 million awarded in the 2008 fiscal year.\nPenn Medicine’s patient care facilities include:\nThe Hospital of the University of Pennsylvania – the nation’s first teaching hospital, recognized as one of the nation’s top 10 hospitals by U.S. News & World Report.\nPenn Presbyterian Medical Center – named one of the top 100 hospitals for cardiovascular care by Thomson Reuters for six years.\nPennsylvania Hospital – the nation’s first hospital, founded in 1751, nationally recognized for excellence in orthopaedics, obstetrics & gynecology, and psychiatry & behavioral health.\nAdditional patient care facilities and services include Penn Medicine at Rittenhouse, a Philadelphia campus offering inpatient rehabilitation and outpatient care in many specialties; as well as a primary care provider network; a faculty practice plan; home care and hospice services; and several multispecialty outpatient facilities across the Philadelphia region.\nPenn Medicine is committed to improving lives and health through a variety of community-based programs and activities. In fiscal year 2009, Penn Medicine provided $733.5 million to benefit our community.\nKaren Kreeger | EurekAlert!\nHow brains surrender to sleep\n23.06.2017 | IMP - Forschungsinstitut für Molekulare Pathologie GmbH\nA new technique isolates neuronal activity during memory consolidation\n22.06.2017 | Spanish National Research Council (CSIC)\nAn international team of scientists has proposed a new multi-disciplinary approach in which an array of new technologies will allow us to map biodiversity and the risks that wildlife is facing at the scale of whole landscapes. The findings are published in Nature Ecology and Evolution. This international research is led by the Kunming Institute of Zoology from China, University of East Anglia, University of Leicester and the Leibniz Institute for Zoo and Wildlife Research.\nUsing a combination of satellite and ground data, the team proposes that it is now possible to map biodiversity with an accuracy that has not been previously...\nHeatwaves in the Arctic, longer periods of vegetation in Europe, severe floods in West Africa – starting in 2021, scientists want to explore the emissions of the greenhouse gas methane with the German-French satellite MERLIN. This is made possible by a new robust laser system of the Fraunhofer Institute for Laser Technology ILT in Aachen, which achieves unprecedented measurement accuracy.\nMethane is primarily the result of the decomposition of organic matter. The gas has a 25 times greater warming potential than carbon dioxide, but is not as...\nHydrogen is regarded as the energy source of the future: It is produced with solar power and can be used to generate heat and electricity in fuel cells. Empa researchers have now succeeded in decoding the movement of hydrogen ions in crystals – a key step towards more efficient energy conversion in the hydrogen industry of tomorrow.\nAs charge carriers, electrons and ions play the leading role in electrochemical energy storage devices and converters such as batteries and fuel cells. Proton...\nScientists from the Excellence Cluster Universe at the Ludwig-Maximilians-Universität Munich have establised \"Cosmowebportal\", a unique data centre for cosmological simulations located at the Leibniz Supercomputing Centre (LRZ) of the Bavarian Academy of Sciences. The complete results of a series of large hydrodynamical cosmological simulations are available, with data volumes typically exceeding several hundred terabytes. Scientists worldwide can interactively explore these complex simulations via a web interface and directly access the results.\nWith current telescopes, scientists can observe our Universe’s galaxies and galaxy clusters and their distribution along an invisible cosmic web. From the...\nTemperature measurements possible even on the smallest scale / Molecular ruby for use in material sciences, biology, and medicine\nChemists at Johannes Gutenberg University Mainz (JGU) in cooperation with researchers of the German Federal Institute for Materials Research and Testing (BAM)...\n19.06.2017 | Event News\n13.06.2017 | Event News\n13.06.2017 | Event News\n23.06.2017 | Physics and Astronomy\n23.06.2017 | Physics and Astronomy\n23.06.2017 | Information Technology"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:d498b207-9c90-467f-8ba1-b433f478ecd9>"],"error":null}
{"question":"How do human activities like tourism and camping affect ecosystems, and what methods exist to reduce these environmental impacts?","answer":"Tourism and camping can significantly degrade ecosystems through hunting, poaching, and wildlife disturbance, which reduces wildlife populations. However, these impacts can be minimized through the Leave No Trace principles, which include planning ahead to avoid high-use periods, camping on established sites rather than creating new ones, properly disposing of waste by packing it out, leaving natural objects untouched, managing campfires responsibly by using existing fire rings, and respecting wildlife by maintaining distance and not feeding animals. Additionally, keeping group sizes small and being considerate of other visitors helps reduce the overall impact on natural environments.","context":["The seven “Leave No Trace” (LNT) concepts have been formed by the Go away No Trace group as a set of tips intended to teach campers and secure the environment.\nJust one of the tenets of outdoor recreation—camping, specifically—is the plan of making the most of the purely natural planet although minimizing effect as much as probable. For long run generations to be ready to take pleasure in spaces that we use right now, and for those purely natural spaces to prosper, it is very important that we get the job done towards defending and preserving wilderness.\nThe Go away No Trace concepts are not new awareness in actuality, any one who has used time in the outdoors will take into account the the greater part of these to be widespread sense. For the professional outdoor fanatic, the tips are a useful reminder of the duty we each individual have for newcomers, they are a road map to making the most of the wild responsibly and respectfully.\nSystem in advance and put together: Helps make sense, suitable? Though many of us take into account organizing in advance in an introspective trend (packing clothes, foodstuff and supplies that you will be working with), it’s also essential to take into account the other side of the tenting equation: the area you are touring to. Are there constraints or rules you should really know about? Climate conditions pertinent to the area?\nThe Go away No Trace concepts propose that, when organizing for a vacation, a person should really foresee squander-disposal units, maintain teams to a minimum amount to cut down effect on the environment and attempt to schedule your take a look at to prevent times of higher use.\nJourney and camp on strong surfaces: According to the LNT internet site, the most effective campsites are found, not produced. Stick to spots that are previously recognized for setting up camp, and prevent altering present campsites.\nDispose of squander properly: What you convey in, you must acquire out. This applies to squander as effectively as supplies. Convey trash receptacles, pack out rest room paper and other cleanliness products and make sure you handle human squander properly.\nGo away what you obtain: You may possibly have stumbled upon the most great wildflower ever, but don’t decide it. Normal environments continue to be purely natural only if we retain their harmony. This goes for historical objects as effectively.\nReduce campfire impacts: Campfires, when mismanaged, can have critical, lasting effects on the purely natural environment. To minimize harm, gentle fires only exactly where permitted, in hearth rings and mounds, and maintain blazes little. Make sure to burn off wood all the way to ash and scatter neat ashes.\nRegard wildlife: The LNT internet site cautions not to technique wildlife, or to follow it. Observe nearby creatures from a distance, and do not feed animals.\nIf you convey your pet on a vacation, make sure you command it at all times. If that cannot be finished, it is greater to go away your companion at home.\nBe considerate of other visitors: In most scenarios, there will be other campers close to as you embark on your vacation. Be courteous when you come across them on the path, and maintain loud noises to a minimum amount.\nFor entry to exceptional gear films, celebrity interviews, and much more, subscribe on YouTube!","Degradation of ecosystems\nThis article deals with the degradation and destruction of ecosystems caused due to natural calamities like floods, volcanic eruptions, storms, cyclones, etc and human activities like wars, population explosion, industrialization, urbanisation etc.\nNegative changes in composition, structure and functioning of an ecosystem is known as ecosystem degradation. Degradation in ecosystem reduces the biodiversity and the rate of nutrient cycle of the ecosystem. Ecosystem degradation can take place because of natural calamities or human activities, or both. The impact of natural calamities is quite intense, as they affect large geographical areas in a limited time. Some natural calamities are of common occurrence while others are restricted only to specific areas. However, we cannot stop ecosystem degradation happen through natural calamities.\nEcosystem degradation due to natural calamities\nNatural calamities like forest fires, floods, earthquakes, droughts, famines, etc. degrade our ecosystem.\n1. Forest fires:\n• Forest fires destroy large regions of forest.\n• Rare species of plants or trees are destroyed. Many important commercial trees are destroyed during forest fires.\n• Biodiversity gets lost.\n• Forest fires affect people's livelihood, which are directly dependent on the forest resources.\n• Resident animals are killed or forced to migrate.\n• Pollutants can be emitted due to forest fires.\n2. Floods: Floods are caused due to high torrential rains or release of excess water from dams or melting of snow.\n• Crops in the flooded farms get affected.\n• Floods can cause spread of water-borne diseases like malaria, typhoid, etc due to mixing of sewage water.\n• Floods can death of a large number of animals and humans.\n• Animals get drowned in flood water. Many animals migrate because of floods.\n• Severe earthquakes can change the topography of the affected areas.\n• Plants, animals and human beings die due to earthquakes.\n• They also change water levels of reservoirs like wells, lakes, or rivers.\n4. Droughts: A period of unusual dry weather arising due to lack of water is called drought.\n• Soils dry out plants and animals die under the conditions of drought.\n• Droughts affect agriculture adversely and destroy crops and livestock.\n• The flow of streams and rivers declines. Chances of soil erosion also increase due to droughts.\n• Fall in the levels of water in lakes, wells and reservoirs.\n5. Famines: Extreme shortage of food in an area is called famine.\n• Roots of plants hold soil tightly. Due to absence of plants, soil becomes more prone to erosion and degradation.\n• Famines affect ecosystems by increasing pressure on resources, both in the areas of drought as well as the places to which people and animals migrate.\n• Animals have to migrate from the famine affected area in search of food. People also have to migrate in search of food and job.\nEcosystem destruction due to human activities\nHuman activities such as mining/conversion of the wetlands into human settlements, uncontrolled hunting and migration, etc. can cause destruction of the eco-systems. The main types of causative factors of ecosystem degradation by human activities are as follows:\n1. Population explosion\n4. Industrialization and transportation\n5. Shifting cultivation\n7. Construction of dams\n1. Population explosion\n• As the population grows, the basic essentials needed for survival is increased. To meet these increasing needs man overharvests the eco-systems.\n• Due to change in lifestyle, use of synthetic materials has also increased. Some synthetic materials cannot be recycled in natural cycles and contribute to increasing quantity of wastes.\n2. Migration: The movement of population from one place to another place because of specific reasons is called migration.\n• Migration creates imbalance of population.\n• The imbalance of population in the newly migrated area affects eco-systems surrounding that area.\n3. Urbanisation: The process of formation of large urban areas is called urbanisation.\n• Cities grow at the cost of natural eco-systems and therefore it results in environmental degradation. Due to this, the ecosystems are destroyed completely.\n4. Industrialization and transportation:\n• Industries require large amount of raw materials such as metal ores, wood and sand.\n• This demand for wood is accomplished by converting natural forests into plantations.\n• Various pollutants from various industries are released into the environment. These pollutants, like mercury enter the food chain and are magnified through the process of biomagnification. This process disturbs the natural balance of the ecosystem.\n• Dumping of earth and rocks, after the process of extracting ores, onto forest areas or wetlands causes disturbances in balance of eco-system.\n• Construction of highways and railways across forest disturbs the natural habitats of wildlife and also affects migration of animals.\n5. Shifting cultivation:\n• Shifting is also known as 'slash-and-burn' system of agriculture. The forest land is cleared by felling trees and later burning the leaves and twigs in the spread-out manner as they lie. This makes the land cultivable.\n• The land is abandoned after a few years because such lands are only temporarily fertile.\n• The farmers then move onto the adjoining areas and repeat the process.\n• After a considerable lapse of time (some 20 years or so) the abandoned patch grows into a forest land naturally. The farmers come back to 'slash-and-burn' the patch again.\n• In modern times, the interim period between 'slash-and-burn' has significantly increased. It has degraded the eco-system severely.\n6. Tourism: Tourism is practiced for different purposes like sight-seeing, recreation and pilgrimage.\n• Tourism supports the livelihood of thousands of people all over the world.\n• It has detrimental effects on local ecosystems.\n• Hunting, poaching and disturbing the animals cause reduction in wild life.\n7. Construction of dams:\n• Dams create large reservoirs of water.\n• The ecosystem of the upstream side is forcibly converted into an aquatic ecosystem.\n• The downstream ecosystem is changed from a flowing water system to an ecosystem of dry bed of sand and stones.\n• Displacement of native people takes place due to construction of large dams and this leads to increased pressure on the ecosystem in the neighbouring areas.\n8. Wars: Wars occur as a result of competition and conflict between nations.\n• Wars take heavy toll of life.\n• Natural resources are used during wars. Natural ecosystems are destroyed partially or completely.\nNatural calamities and certain human activities prove harmful to natural ecosystems. However, we cannot control degradation caused due to natural calamities but can control destruction caused from man-made activities. Natural ecosystems play a key role in maintaining environmental balance in the biosphere. Hence, the ecosystems must be protected.\nNo responses found. Be the first to respond..."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:365597f5-24b2-40f2-8ebd-3674d44ca87a>","<urn:uuid:1705862b-0bd8-40b2-8aa1-ad11832967c5>"],"error":null}
{"question":"How do Playwrights Horizons and The Public Theater differ in their approach to supporting emerging artists and new works, considering their facilities and programs?","answer":"Both theaters support emerging artists but take different approaches. Playwrights Horizons has presented work by over 300 writers and focuses on maintaining an intimate production style with a small stage deliberately kept the same size as their old building. The Public Theater has more extensive programs specifically designed for emerging artists, including the Emerging Writers Group for early-career playwrights, Public Studio for developing new theater works, and various educational initiatives like Shakespeare Lab for actor development. The Public also operates multiple venues including five theater spaces, Joe's Pub for new work and performances, and the Delacorte Theater in Central Park, while Playwrights Horizons operates two main spaces - the Main Stage and Studio Theater on Theater Row.","context":["Home · Email · List of Projects · Project Sites and Links · Articles · Robert Davis Biography\nPlaywrights Horizons - New York, New York\nPeter Jay Sharp Studio Theater\nEntrance on 42nd Street\nPlaywrights Horizons was founded in 1971 and has become one of the nation's premier venues for new work. Playwrights has presented new work by more than 300 writers, including Christopher Durang's Sister Mary Ignatius Explains It All For You, A.R. Gurney's Later Life, Scott McPherson's Marvin's Room, Wendy Wasserstein's The Heidi Chronicles, and Alfred Uhry's Driving Miss Daisy. Playwrights produced critically acclaimed musicals such as William Finn's Falsettos and Stephen Sondheim and James Lapine's Sunday in the Park With George.\nPlaywrights is one of six Off-Broadway theaters along west 42nd Street called \"Theater Row\". The six new and renovated theaters range from 99 to 199 seats. A 499-seat Shubert Organization theater, the \"Little Shubert\", is Playwrights' next-door neighbor to the West.\nThe new Playwrights was built on the same lot as the old Playwrights building, which was vacated for a year during demolition and construction. The old Main Stage is duplicated almost exactly in the new building. The new building increases the seating capacity of the Main Stage auditorium, adds a Studio Theater, and provides a radical improvement in support space. The new facility includes administrative offices, a telemarketing office, one large and one small rehearsal room, dressing rooms, and spaces for stage departments such as carpentry, props, wardrobe, sound, and electrics. The ground floor lobby on 42nd Street includes a new home for Ticket Central, which is a cooperative box office for several theaters.\nThe building lot is very small so this project is an exercise in artful stacking. Mechanical equipment is located on the bottom and top floors for acoustical and other practical reasons. The left, right, and rear walls are lined with continuous mechanical chases the full height of the building to transfer ducts and conduits from floor to floor. The major spaces are separated acoustically by interstitial noise isolation floors. These noise buffer floors are filled with offices and other similar quiet activities that won't leak noise into the adjacent theaters and that don't mind a little noise leaking into them.\nPlaywrights was careful not to lose their unique style in the new building. Playwrights decided not to enlarge the stage, but instead kept the tiny stage the same size in the new building that it was in the old building so that their production budgets and their intense small production style would not have to expand or be diluted trying to fill more space in the new facility.\nThe entire stage floor is trapped, can be removed, and can open completely into the trap room below. The front edge of the stage and the proscenium walls left and right are removable to suit the needs of the performance. The counterweight rigging extends out past the front edge of the stage roughly half way back into the auditorium, so it is possible to stand at the locking rail and operate linesets over the auditorium chairs. The Main Stage is wired with the old data standard DMX, with the new data standard Ethernet, with Wybron data wires for scrollers, and anticipates dimmer-doubling on most of the 192 circuits.\nThe new Playwrights Horizons opened in 2003. The capital campaign to cover the cost of the building and an endowment is $32 million.\nArchitect: Mitchell Kurtz Architect PC","The Public Theater\n|Address||425 Lafayette Street\nNew York City\n|Designation||New York City Landmark|\nThe Public Theater is a New York City arts organization founded as The Shakespeare Workshop in 1954 by Joseph Papp, with the intention of showcasing the works of up-and-coming playwrights and performers. At Papp's death in 1991, the Public Theater was renamed the \"Joseph Papp Public Theater\" in his memory. It is headquartered at 425 Lafayette Street in the former Astor Library in the East Village section of Lower Manhattan.\nThe Public is dedicated to embracing the complexities of contemporary society and nurturing both artists and audiences, as it continues Joseph Papp's legacy of creating a place of inclusion and a forum for ideas.\nIn addition to five theater venues – the Shiva, the Martinson, the LuEsther Hall, the Anspacher and the Newman – the site is home to Joe's Pub, a cabaret-style setting used for new work, musical performances, spoken-word artists and soloists.\nThe Public also operates the Delacorte Theater in Central Park, where it presents Shakespeare in the Park, one of New York City's most beloved summer traditions. New York natives and visitors alike have been enjoying free Shakespeare in Central Park since performances began in 1954. Notable productions in recent years include: The Merchant of Venice in 2010, featuring Al Pacino as Shylock; Twelfth Night in 2009, with Anne Hathaway; the 40th anniversary production of Hair in 2008; and Mother Courage And Her Children, with Meryl Streep and Kevin Kline.\nPrograms and series\nIn addition to each season of full-scale theatrical productions, The Public also produces a number of different series, festivals and programs each year.\nIn 2008, The Public presented its inaugural Public LAB series, an annual series of new plays presented in collaboration with LAByrinth Theater Company. Public LAB lets New Yorkers see more of the work they love from The Public in scaled-down productions, and allows The Public to support more artists, as well as gives audiences immediate access to new plays in development at affordable prices. With each Public LAB show, corresponding speaker series are presented as after-show talkbacks to discuss prominent themes, ideas and topics in the plays. A number of plays that have appeared in the Public LAB series have gone onto full-scale productions, including Tracey Scott Wilson’s The Good Negro, which ran at The Public in 2009, and Bloody Bloody Andrew Jackson , which had a sold-out, thrice-extended Off-Broadway run at The Public in the spring of 2010 and transferred to Broadway that fall.\nPublic LAB was expanded in 2011 to include Public LAB SHAKESPEARE, a vital new platform for The Public's ongoing exploration of the Shakespeare canon that continues the growth of The Public's Shakespeare Initiative and expand the many ways The Public produces American interpretations of Shakespeare. The premiere production of Public LAB SHAKESPEARE was Timon of Athens in March 2011, featuring Richard Thomas in the title role.\nIn 2013, The Public launched the Mobile Shakespeare Unit, which tours free Shakespeare to various locations throughout the five boroughs, including prisons, homeless shelters, and community centers, before concluding its run at the Public Theater itself. Past venues include Rikers Island, Borden Avenue's Veteran's Shelter, and The Fortune Society.  The Public also launched its inaugural Public Works production in 2013. Public Works combines diverse groups of people throughout the five boroughs of New York City to watch theatre, participate in theatrical workshops, and perform in one full-scale Public Works production alongside professional actors at Shakespeare in the Park. Past Public Works productions include The Tempest, The Winter's Tale, and The Odyssey. \nThe Public Forum, begun in 2010, is an exciting series of lectures, debates and conversations that showcase leading voices in the arts, politics and the media. Curated by Jeremy McCarter, a senior writer at Newsweek, Public Forum events explore issues raised by plays in The Public's season, as well as the political and cultural headlines of today's world. In keeping with the best traditions of The Public, the Forum hosts a wide diversity of views and brings the theater into contact with the society around it. Notable participants in the series include Stephen Sondheim, Tony Kushner, Arianna Huffington, Alec Baldwin and Anne Hathaway.\nThe Public hosts the annual Under the Radar Festival, directed by Mark Russell. Begun in 2004, Under The Radar spotlights international artists ranging from emerging talents to masters in the field. The festival is a wild mix of works by ensembles, solo artists, writers, and creators. The ultimate goal of UTR is to offer a crash course in theater that is exciting, independent, and experimental, created by some of the most dynamic artists working today. As of April 2011, there have been 104 productions from over 18 countries, including the United States, Australia, Belarus, Bolivia, Brazil, France, Ireland, Mali, Mexico, the Netherlands, Russia, and the United Kingdom. Some artists who have collaborated with UTR include SITI Company, Elevator Repair Service, Nature Theater of Oklahoma, Superamas, Abbey Theatre, Mike Daisey, Reggie Watts, Teatro De Los Andes, and many more.\nThe Public serves as the home of the Emerging Writers Group, which seeks to target playwrights at the earliest stages in their careers. In so doing, The Public hopes to create an artistic home for a diverse and exceptionally talented group of up-and-coming playwrights. Through the Emerging Writers Group, The Public continues its rich legacy of supporting current and future generations of our country’s most important writers via The Public Writers Initiative – a long-term initiative that provides key support and resources for writers at every stage of their careers. The Public Writers Initiative creates a fertile community and fosters a web of supportive artistic relationships across generations. The Emerging Writers Group is a component of The Public Writers Initiative.\nThe Public also fosters Public Studio, a performance series dedicated to developing the works of new and emerging theater artists. Emerging playwrights get the opportunity to stage a piece somewhere between a workshop and a full production in front of an audience, as an opportunity to gage audience reaction and further develop their work. \nThe Public Theater invests in theater education, training classical actors through the annual summer acting intensive known as the Shakespeare Lab. The Shakespeare Lab is The Public Theater’s professional actor development program that immerses a carefully selected company of professional, mid-career actors in a summer intensive exploring the rigors, challenges, and joys of performing Shakespeare.\nThe Public also educates through its Shakespeare Spring Break, Summer ShakeUP, and A Midsummer Day’s Camp programs, all for teenagers interested in learning about and performing Shakespeare.\nSuzan-Lori Parks, Pulitzer Prize-winning playwright and Master Writer Chair of The Public, debuted her performance piece Watch Me Work as part of the 2011 Under The Radar Festival. As of April 2011, Parks continues to perform the piece in the main lobby of The Public Theater. A meditation on the artistic process and an actual work session, Watch Me Work features Parks working on her newest writing project in the main lobby of The Public Theater. The audience is invited to come and watch her work and/or to share the space and get some of their own writing work done. During the last fifteen minutes of the performance Parks answers questions the audience might have regarding their own work and their creative process.\nNotable works and awards\nThe most famous work to emerge from The Public, other than the original production of Hair, is the Michael Bennett musical A Chorus Line, based on the lives and careers of Broadway dancers. The show created such a stir of anticipation among the theatrical community that the entire limited run sold out long before opening night. Demand for tickets was such that the show moved uptown to Broadway's Shubert Theatre, where it remained \"one singular sensation\" for fifteen sold-out years. Over the years, revenue from the many worldwide productions, both professional and amateur, of the show has been a steady and main source of income for The Public.\nThe Public Theater has won 42 Tony Awards, 151 Obies, 41 Drama Desk Awards and four Pulitzer Prizes. Fifty-four Public Theater productions have moved to Broadway, including Sticks and Bones; That Championship Season; A Chorus Line; For Colored Girls Who Have Considered Suicide When the Rainbow Is Enuf; The Pirates of Penzance; The Tempest; Bring in 'da Noise, Bring in 'da Funk; Michael John LaChiusa's The Wild Party; The Ride Down Mt. Morgan; Topdog/Underdog; Take Me Out; Caroline, or Change; Passing Strange; the revival of HAIR; Bloody Bloody Andrew Jackson; The Merchant of Venice; \"The Normal Heart\", Well, Fun Home, and Hamilton.\nAstor Library Building\nThe Public has been housed in a landmarked Romanesque revival structure at 425 Lafayette Street since 1967, built between 1853 and 1881 as the Astor Library, which later merged with the Tilden and Lenox collections to become the New York Public Library. The library was built by William B. Astor, son of the library's founder, John Jacob Astor. A German-born architect, Alexander Saeltzer, who had been the architect of the Anshe Chesed Synagogue,  designed the building in Rundbogenstil style, then the prevailing style for public building in Germany. Astor funded two expansions of the building toward Astor Place, designed by Griffith Thomas (1856–1869) and Thomas Stent (1879–1881). Both large expansions followed Saeltzer's original design so seamlessly that an observer cannot detect that the edifice was built in three stages.\nIn 1920, the Hebrew Immigrant Aid Society purchased the building. By 1965 it was in disuse and faced demolition. The Public Theater, then the New York Shakespeare Festival, persuaded the city to purchase it for use as a theater. It was converted for theater use by Giorgio Cavaglieri between 1967 and 1976.\nThe building is a New York City Landmark, designated in 1965. It was one of the first buildings to be recognized as such by the newly formed Landmarks Preservation Commission of New York City, thanks to Joseph Papp’s perseverance.\nIn 2009, The Public began its “Going Public” campaign to raise funds for a major renovation of the historic building. Groundbreaking for the $35 million renovation occurred on March 9, 2010, with notables such as Liev Schreiber and Philip Seymour Hoffman in attendance. Plans include a renovation of Joe’s Pub; the Pub went on a three-month hiatus during the summer of 2011 to allow for construction. The building re-opened on October 4, 2012 after a renovation designed by Ennead Architects costing $40 million.\n- Epstein, Helen. Joe Papp: An American Life, Da Capo Press, March 1, 1996. ISBN 978-0306806766\n- Hetrick, Adam. \"Public Theater Appoints Patrick Willingham Executive Director,\" Playbill.com, Nov 2011.\n- “The Public Theater – About,” PublicTheater.org, April 2011.\n- “Shakespeare In The Park – History,” ShakespeareInThePark.org, April 2011.\n- “Public Theater – Public LAB,” PublicTheater.org, April 2011.\n- Hetrick, Adam. “Richard Thomas Will Star in Shakespeare’s ‘Timon of Athens’ At The Public,” Playbill.com, April 2011.\n- “The Public Theater – Public Forum,” PublicTheater.org, April 2011.\n- “Under The Radar – About UTR,” UnderTheRadarFestival.com, April 2011.\n- “The Public Theater – Emerging Writers Group,” PublicTheater.org, April 2011.\n- “The Public Theater – Shakespeare Lab,” PublicTheater.org, April 2011.\n- “The Public Theater – Shakespeare Programs For Teenagers,” PublicTheater.org, April 2011.\n- “Watch Me Work,” suzanloriparks.com, April 2011.\n- Israelowitz, Oscar. \"Oscar Israelowitz's Guide to Jewish New York City New York: Israelowitz Pub., 2004\n- New York City Landmarks Preservation Commission. \"Anshe Chesed Synagogue Designation Report\" (February 10, 1987)\n- White, Norval & Willensky, Elliot (2000). AIA Guide to New York City (4th ed.). New York: Three Rivers Press. ISBN 978-0-8129-3107-5.\n- Dimonstein, Barbaralee. The Landmarks of New York, Harry Abrams, 1998 p. 107\n- New York City Landmarks Preservation Commission; Postal, Matthew A. (ed. and text); Dolkart, Andrew S. (text). (2009) Guide to New York City Landmarks (4th ed.) New York: John Wiley & Sons. ISBN 978-0-470-28963-1, p.64\n- Epstein, Helen (1996). Joe Papp: An American Life. Boston: Da Capo Press. ISBN 9780306806766.\n- \"Enter Theatergoers, Gently Welcomed,\" Robin Pogrebin, The New York Times, April 27, 2009\n- Morrone, Francis (2002). The Architectural Guidebook to New York City. Salt Lake City: Gibbs Smith. ISBN 1-58685-211-6.\n- Hetrick, Adam.“McDonald, Hoffman, Schreiber, Benanti Attend Public's Groundbreaking; Renovation Details Announced,” Playbill.com, April 2011.\n- “Joe’s Pub Will Take Summer Hiatus as Public Continues Renovations,” Playbill.com, April 2011.\n- Lipinski, Jed. \"The Public Theater celebrates the end of a long refurbishments\" Capital (October 8, 2012)\n|Wikimedia Commons has media related to Astor Library.|\n- Official Website\n- Official Joe’s Pub Website\n- Official Shakespeare In The Park Website\n- Official Under The Radar Website\n- New York Shakespeare Festival records, 1954-1992. Held by the Billy Rose Theatre Division, New York Public Library for the Performing Arts\n- Joseph Papp at the Internet Broadway Database"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:8aa143aa-b9ea-46f1-8626-5389cbe823ae>","<urn:uuid:a1c0e325-e843-4be7-b5f4-f40a828686e5>"],"error":null}
{"question":"What's the difference between how BIO2CHP and rendering technology handle food waste to generate energy?","answer":"BIO2CHP and rendering technology handle food waste differently to generate energy. BIO2CHP utilizes raw residual biomass from the agro-food industry, particularly fruit waste, coffee grounds, and olive kernels, for small-scale, on-site electricity production. Rendering technology, which is standard in most western countries, processes animal byproducts to produce tallow for biodiesel and meat and bone meal (MBM) for biofuel, resulting in greenhouse gas emission reductions of approximately 70,000 tonnes per annum by replacing soybean, palm oil, and fossil fuels.","context":["A new report from a food waste recycler and a charity that promotes sustainable businesses in the UK includes a number of proposals that may help other countries, like Canada, also achieve zero waste food to landfill. The report also suggests that its recommendations will save the UK economy over £17bn a year by 2020 through the reduction of food wastes from households, businesses and the public sector. The data in the report are specific to the UK, though similar data, scaled for population and food consumption differences, likely also apply to Canada.\nThe recommendations include:\n- By treating food waste separately, it is possible to move it further up the waste hierarchy, reduce treatment costs and ensure that its true potential is realised.\n- Surplus food that is ﬁt for human or animal consumption sits higher in the food waste hierarchy and should be prioritised by businesses ahead of other means of treatment.\n- Food redistribution takes surplus but edible food and redistributes it to people in need.\n- The livestock animal feed industry is a well-established and regulated solution for handling surplus food. Feedstock for animal feed reprocessors is restricted to former foodstuffs that do not contain animal protein, such as clean bread, ﬂour, biscuits, crisps, dough, grains, fruits and vegetables. Depending upon the volume and type of foodstuff, these materials will have a commercial value and therefore, in most cases, represent a more economically viable solution than landfill or incineration.\n- Likewise, the pet food industry is a strong, growing and well-regulated sector.\n- Rendering is the industry standard technology in most western countries applied to handling animal byproducts. The resultant tallow can be used in the production of biodiesel and meat and bone meal (MBM) can be used as a biofuel to generate renewable energy.\n- The use of rendered products can be estimated to reduce GHG emissions by approximately 70,000 tonnes per annum. This is because they can be used as a direct replacement to soybean in the production of pet foods, palm oil in the manufacture of bio-diesel, and fossil fuels in the generation of energy.\n- Unavoidable food waste is a resource. Food, by its very nature, contains nutrients and energy and therefore solutions should seek to maximise the resource potential in the material.\n- Anaerobic digestion is a proven technology solution and has been widely used in the water treatment industry for many years. AD is a biological process. Food waste is first de-packaged, normally by machine, then pasteurised by being heated to 70°C for one hour to kill harmful bacteria, before passing into large digesters. Here good bacteria, in the absence of oxygen, work on the food and produce methane gas and a liquid/solid fraction called digestate, a low-carbon bio-fertiliser.\n- Energy from waste has a role to play in UK waste strategy, but it is important to safeguard against the loss of crucial resources by avoiding treating a large proportion of our waste in this way.\nThe full report contains many more details and can be found at http://www.vision2020.info/","Food waste has become one of the biggest environmental issues and contributors to greenhouse gas emissions. The food we throw away releases huge quantities of methane gas, which is 25 times more potent than carbon dioxide. Besides contributing to climate change, food loss is a waste of the natural resources that were used to produce it, including energy, water, and fuel. There are currently more than 800 million people facing hunger worldwide and yet, about one-third of global food supplies are wasted or lost. Food companies are under greater scrutiny to reduce their carbon footprint and address the issues related to the industry. These innovative food waste startups are leading the change, and have come up with original solutions to solve this pressing matter.\n11 Innovative Food Waste Startups\n1. GreenPod Labs (India)\nAfter working in the food science sector in the US for several years, entrepreneur Deepak Rajmohan moved back to India to start GreenPod Labs in a bid to alleviate food waste in his home country. Founded in 2019, the startup develops active packaging sachets which activate built-in defence mechanisms in fruit and vegetables to preserve their quality, and slow down the ripening process. The innovative idea is a much-needed postharvest solution to food waste, which is a huge problem for the Southeast Asian country. Despite being the second-largest producer of fruit and vegetables in the world, 40% of the fresh produce grown in India is lost before it reaches consumers.\n2. CHOMP (Hong Kong)\nHong Kong-based app CHOMP is addressing food surplus in Hong Kong’s F&B industry while simultaneously offering a solution to food waste. Founded in 2020 by Carla Martinesi and Chris Wettling, this bilingual solutions-based app offers unsold food items from local bakeries and restaurants to consumers packaged in “mystery boxes” and at a discounted price. Yet, their mission does not end here. According to Wettling, education is key to changing the many misconceptions surrounding food surplus and helping reduce food waste, not just locally but all over the world. For this reason, CHOMP’s team is encouraging more dialogue between F&B businesses and customers by posting regular tips and advice on their Instagram page and is collaborating with local schools to raise awareness among younger generations.\nYou Might Also Like: Chris Wettling on Tackling Hong Kong Food Waste and Surplus, and CHOMP\n3. FLYFARM (Singapore)\nCo-founded by Constant Tedder and Andres Crabbe, FLYFARM brings to the market an innovative and sustainable method to produce insect protein. While current farming systems and protein production put tremendous pressure on our environment, the agri-tech company works closely with renewable energy-powered larvae farms that upcycle organic waste into high quality protein for animal feeds. Food waste and by-products coming from agriculture and food processing – such as organic waste from food beverage manufacturing processes and supermarket waste – are upcycled by harnessing the power of insects, the natural converters of organic waste in nature.\n4. Kebony (Norway)\nHeadquartered in Oslo, Norway, Kebony is a global leader in modified wood manufacturing. Contrary to the food waste startups mentioned so far, this one focuses on using food waste to produce something different: softwood. Though this type of wood behaves differently from hardwood, Kebony has come up with an award-winning technology that uses waste coming from the food industry, modifying it to mirror the behaviour and characteristics of tropical hardwood, which is typically harder, stronger, and more durable. By doing so, besides making good use of food waste, the company is preventing deforestation and saving tons of greenhouse gas emissions.\n5. Too Good To Go (United Kingdom)\nAvailable in several European countries as well as in Canada and the US, this takeaway food app is fighting food waste by helping food stores sell their surplus food instead of throwing it away. Established in 2015 and served its first customer in 2016, Too Good To Go has since partnered up with big restaurants and food chains around the world to sell so-called ‘magic bags’ of goods at a discounted price. So far, the company claims to have saved over 52 million meals worldwide, enough to feed the entire population of South Korea.\nYou Might Also Like: How Does Food Waste Affect the Environment?\n6. BIO2CHP (Greece)\nGreek-based BIO2CHP utilises raw residual biomass feedstock from the agro-food industry – especially fruit waste, coffee grounds, and olive kernels – for small-scale, on-site electricity production. The waste-to-energy strategy has several advantages, from converting waste management costs into additional revenue to upgrading the environmental image and reducing the footprint of the end user. Established in 2017, the innovative startup received over one million dollars to fund its research and develop an award-winning technology that redefined the waste-to-energy system.\n7. Ottan Studio (Turkey)\nOttan Studio has made a name for itself by converting food and garden waste into bio-composite material from which it makes furniture, decorative items, and wall panelling. Interior design items are commonly produced with wood and other raw materials, but the founder of this Turkey-based startup, Ayse Yilmaz, saw the marvellous potential of biological waste as raw material and came up with an innovative process to upcycle tons of discarded fruit peels, nutshells, and leaves and manufacture them into high-quality products.\n8. TripleW (Israel)\nFounded in 2015, Israeli startup TripleW developed a circular solution to the environmental and financial challenges of waste management. The company developed a process to convert abundantly available organic waste – from supermarket and industrial food surplus to municipal source-separated food waste – into feedstock for the production of polylactic acid (PLA bioplastic). This renewable and highly in-demand material can be used to manufacture thousands of products, from yoghurt cups and tea bags to t-shirts and toy blocks. Additionally, the company enables chemical recycling of biodegradable plastics back into their patented process, further reducing wastage.\n9. Choco (Germany)\nNinth on our list of food waste startups is Choco. Founded in 2018, this Berlin-based food tech startup is on a mission to transform and fully digitalise the food industry’s business processes by 2026. With a network of more than 10,000 suppliers and 15,000 chefs, the app offers an efficient digital tool that replaces archaic processes such as pen-and-paper and manual spreadsheets that many foodservice businesses still rely on to order ingredients and record sales. It also claims to improve order accuracy, reducing mistakes and incorrect deliveries, thus ensuring that less food and money are wasted in the process. With a total of nearly USD$300 million in funding, Choco was able to rapidly expand its operations to serve markets in Germany, the US, France, Spain, Austria, and Belgium.\n10. Fridgely (US)\nFounded in 2015 by Justin Ehlert, a senior computer science major at the University of Texas at Dallas, the Fridgely app is a food expiration date tracker that alerts consumers when food is going to expire, allowing them to eat it before they go bad. Ehlert came up with this idea while living in an apartment with three other male college students and realising how much food the household wasted each week. In the US, the average family of four wastes about 25% of its food. Moreover, a staggering 80% of Americans throw away fresh produce and packaged products because they misunderstand expiration labels. Fridgely represents a unique opportunity to tackle this massive, though easily preventable, issue.\n11. Replate (US)\nSan Francisco-based Replate is on a mission to reduce food waste and alleviate food insecurity in communities everywhere. The tech-based social enterprise redistributes donated surplus food from businesses to serve communities in need. The company leverages data, artificial intelligence, and agile programming to recover surplus food from vendors and deliver it directly to non-profit organisations in the area. In addition to this, Replate also offers monthly impact metrics for all donations to track the environmental and social impact of contributions, including total number of pounds donated, total number of meals this translates to, total gallons of water saved as a result of donations, and total pounds of CO2 diverted as a result of these donations.\nYou might also like: 5 Sustainable Plant-based Seafood Companies Leading the Way"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:f947ba6b-b8cc-4b8f-b0e0-5da7e68dae35>","<urn:uuid:9121de7c-21bd-4778-91ff-81578f99a881>"],"error":null}
{"question":"In my fraud investigation work, I've noticed different types of fraud schemes - how do government-focused fraud schemes compare to occupational fraud schemes in terms of their categories and characteristics?","answer":"Government fraud schemes primarily fall into three broad categories: vendor fraud, diversion of public funds, and service consumer fraud, all of which affect public resources and taxpayer money. In contrast, occupational fraud schemes are specifically caused by organization members and include more specific categories like cash larceny (theft of recorded money), skimming (theft before recording in accounts), and financial statement fraud. While government fraud primarily impacts public services and programs, occupational fraud directly damages the organization through asset misappropriation, corruption, and financial reporting manipulation. Both types require different detection approaches, with government fraud often needing data mining techniques, while occupational fraud requires more internal controls and employee monitoring.","context":["Governments at all levels are faced with the challenge of setting agendas and creating the frameworks to improve the lives of citizens. As we all know, government touches each of us daily. Policies and initiatives come and go, each seemingly different from the last. Yet, they all have one thing in common: They all cost money, often a great deal of money. The undeniable truth of government is that it is an expensive business; typically, the more radical the agenda or the more holistic the policy, the more it will cost. Another sad truth is that where there is money, there is also fraud, abuse and error. Such misuse—intentional or not—costs the government and, ultimately, the taxpayers.\nConsider for a moment fraud in the government. If we accept that government exists to serve the people, to “improve our lives,” then fraud against the public purse removes some of the funding that could improve the lives of all of us, especially those most in need of government support. Not only is government fraud morally, ethically and legally wrong, it is the antithesis of everything good government stands for. Billions of revenue paid by hardworking citizens are lost each year due to improper payments, fraud, waste and abuse. Governments at all levels—federal, state and local—face the enormous challenge of rectifying this situation. What can agencies do to improve collection rates? How do they increase the productivity, effectiveness and efficiency of their auditors and investigators? By identifying a prioritized list of accounts that have a high likelihood of being fraudulent, agencies can optimize investigators’ time and increase the funds collected.\nImplementing a strategy and technology solution to find improper payments, fraud, waste and abuse helps governments ensure that vital services and programs that citizens desperately need are there for them. While fraud, waste and abuse have been identified as areas in which data mining is applicable, actually using data mining techniques for this application has historically relied on flagging cases where there are known problems, building models (or profiles) of these problems and “scoring” new data based on the profiles. Although this approach is useful, it is inappropriate to use only this technique because of the following limitations:\n1. A situation in which there are two identical records. Problem behavior is sometimes found in one record, while the other record is not even checked. This results in noisy data and problems with model building.\n2. The limitations of pattern recognition techniques. Pattern recognition techniques are only able to find patterns that have been found in the past. This means that the existing fraud, waste and abuse often remains undiscovered.\n3. Small numbers of identified cases of fraud, waste and abuse. When numbers are low, reliable models cannot be built.\n4. Offender behavior changes. Once offenders realize that a certain behavior triggers a problem, they no longer commit that behavior. Then models have to be built to capture the new offending behavior.\nFraud detection is generally hampered by the need for high-skilled investigators plowing their ways through backlogs of computer data, with successive findings triggering new questions involving new painstaking searches; in the meantime fraud, waste and abuse continue. This paper discusses systematic approaches to detecting fraud in three broad categories: vendor fraud, diversion of public funds, and service consumer fraud. Part 2 of fraud detection in government will provide an overview of public sector fraud schemes, then discuss traditional methods and data mining techniques for fraud detection, as well as the implementation of monitoring and reporting systems.","center23002457459410012100center818008745855Manav [email protected] email@example.com of Fraud Examination 9410036300Principles of Fraud Examination 213360838209410012100IntroductionFraud is a very steep problem at a global level these days.\nFraud is typically considered as a white collar crime and the process of examination involves investigation and analysis of complicated financial records. Fraud examination “”refers to a process of resolving allegations of fraud from inception to disposition, and it is the primary function of the anti-fraud professional”. Fraud examinations are focused on determining whether the fraud has occurred and, if so, then to gather evidence of the crime.Fraud examination is non-recurring and scope specific. It might be required by law.\nWe Will Write a Custom Essay Specifically\nFor You For Only $13.90/page!\nAn obligation to investigate can arise from statutes, regulations, contracts, or common law duties. Fraud examination became the need of the hour in the USA afterward the mishap of 9/11. Fraud examiners were then recognized by FBI to help them in combating the frauds. For instance, an establishment’s directors and officers owe a common duty of care to their association and shareholders, and therefore, when qualms of fraud arise, it might be essential for them to conduct an examination to confirm that they have complete knowledge of such issues affecting the company.It is a practical orientation as to how to prevent, detect and investigate fraud within a business. It can be used to classify the different types of fraud as well as to construct an environment in which fraud is minimized. However, dealing with complex issues of fraud, regulatory compliances and business disputes can diminish an organization in its efforts to prosper.Categorization of Fraud ExaminationFraud is hidden: Unlike other offenses, part of the method of fraud is to conceal its existence.\nLike hiding data which is suspicious. Reverse proof: When an examiner is to prove that fraud has been occurred, the proof must include attempts to prove that the fraud is not occurred. The existence of fraud: In resolving fraud issues, the examiner must postulate a theory of guilt or innocence in order to attempt to prove his theory. 2286001949459410012100Fraud Tree:Occupational Frauds are caused by members of an organization such as an employee, manager or even an owner of such an organization that might cause severe damage to the organization. Majorly, occupational frauds can be categorized as under:-403860176530002286001949459410012100Fraud Examination – Objective:Fraud examination can be defined as the procedure of determining allegations of fraud from inception to disposition. Fraud examination is used for much more than just reckoning out who committed the crime of fraud, but more specifically, fraud examination involves procurement of pieces of evidence and captivating statements, writing reports, testifying to findings, and assisting in the detection and prevention of fraud.\nIn specific, fraud examination can address a number of organizational objectives such as, classifying improper conduct, classifying the personnel responsible for fraudulent conduct, preventing the fraud occurrence, spreading the message throughout the organization that fraud will not be tolerated, determining the amount of potential accountabilities or losses that might exist, serving to facilitate the recovery of losses, discontinuing future losses, modifying other potential consequences and consolidation of internal control weaknesses. Steps in Fraud Investigation1729740229870Identity Type of Fraud00Identity Type of Fraud2484120344170001729740134620Create an Investigation PlanPlan00Create an Investigation PlanPlan248412022606000172974016510Interview or Re-Interview Victim00Interview or Re-Interview Victim2484120147320001729740270510Securing Evidence00Securing Evidence2286001981209410012100Fraud Examination can be divided into three basic aspects:Fraud Detection: The term “Fraud Detection” is normally perceived in Banking ; Financial Sectors, insurance, government agencies and several other Law Enforcement bodies. The important aspect of the fraud detection is to protect the clients and enterprises information, their assets, accounts, and transactions, via a real-time examination of actions by the users and other defined entities. It includes connecting all the data points in order to ascertain the potential fraudulent behaviour. Fraud Investigation: A fraud investigation helps to come to a conclusion whether a fraud has taken place or not and also gathers the evidence in order to protect the victims involved.\nFraud investigations involve the occurrence of a fraud which basically involves an intention to deceive the other party. Fraud Investigation also benefits the companies’ as to how to manage the risk, measure the financial inferences of the disputes and investigate alleged misconduct. Fraud Prevention: Fraud Prevention is a process which doesn’t have a starting or the ending point. Rather, it is an ongoing cycle which involves different stages such as monitoring, detection, decisions, case management and learning. In order to prevent an array of fraud attacks, an organization must follow three key steps, they are as follows:Capture and unify all available data types across the channels and incorporate them into an analytical process. Continually monitor transactions and apply behavioural analytics to enable real-time decision making.\nEmploy layered security techniques. Fraud comes in many forms but primarily takes three form like Asset misappropriation, corruption, and financial statement fraud. In order to be safe, the organizations, whether large or small should have a plan in place. Knowing your employees, making employees aware, Implementing Internal Controls, etc. be some of the ways in which a company may be able to keep a check on different kinds of frauds. 2286002286009410012100Fraud Examination Methodology: Evaluation of the accessible data: Evaluation procedures involves inspecting, cleaning, modifying, and presenting the data with the motive of discovering useful information, apprising conclusions, and supporting the decision-making while also keeping in mind the possibility of fraud resulting from the anonymous allegations. Generating a hypothesis: The hypothesis is unvaryingly a “worst-case” scenario which is based on the allegation, and what is the worst possible outcome? Fraud examiners create hypotheses for any specific allegation (e.g.\n, a bribery or kickback scheme, embezzlement, clash of interest, or financial statement fraud). When the hypotheses is complete, fraud examiners recognize that each and every specific scheme has its own unique characteristics that constitute the badges or “red flags” of fraud.Testing the hypothesis: Testing a hypothesis involves building a “what-if” scenario. In the hypothesis testing, an analyst firstly examines the statistical sample, with the goal of accepting or rejecting a null hypothesis.\nIf it isn’t true, the analyst formulates a new hypothesis to be tested, repeating the process until data reveals a true hypothesis.Refining and amending the hypothesis: After the testing is done by the examiner, then comes to the step of Refining and amending the hypothesis. If during the testing phase, the fraud examiner discovers that all facts do not fit a particular scenario then the hypothesis is again reviewed and retested.\nVarious types of organizational frauds like:Cash Larceny which is an occupational Fraud which involves the theft of money that has been already appeared in the company’s ledger. Skimming fraud which is known as off-book frauds, meaning thereby, the removal of cash before recorded in the company’s account.2209802514609410012100Case Studies:Like the famous Enron company case, as being one of the companies which fell down too fast due to mismanagement, conflict of interest, lack of fundamental ethical values and accounting fraud.Satyam Computer Services was the biggest accounting fraud case which gained recognition to the forensic accounting as the profession. In this we can again acknowledge that company’s ethics were compromised and fraud occurred. So we need to have a check internally and externally as well from time to time in order to secure the company from occurrence any possibility of fraud.\nNirav Modi and Kingfisher cases both can be classified into financial fraud categories as in the Nirav Modi case forensic financial analysis was conducted of the books and records of the mortgagors, merchant chronicles, bank chronicles, and all other relevant information of other entities constant with the investigation, and trace the movement of monies gained under the suspected fraud circumstances against the PNB to the debtors, and connected entities and individuals. While in the Kingfisher’s case was a serious financial fraud investigation was done as Kingfisher fraudulently induced banks to convert part of debt into preference shares by deceptive projects. Further even all the corporate ethics were compromised. Conclusion:Persons who indulge in committing fraud do not discriminate.\nIt can ensue in large or small companies in several industries and geographic locations. Professional fraud can result in an enormous financial loss, legal costs, and ruined reputations that can eventually lead to the downfall of an organization. Having the proper plans in place can meaningfully reduce fraudulent activities from occurring or cut losses if a fraud already occurred. Making the company policy known to employees is one of the best ways to deter fraudulent behaviour."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:38e6c007-92c6-40a6-9ca3-6db3f1856db8>","<urn:uuid:9fcba339-070f-4853-8328-3ba9fd2f5cc4>"],"error":null}
{"question":"What similarities exist between the defected MiG testing programs of the 1960s and the X-56A research in terms of their testing methodology?","answer":"Both programs employed systematic flight testing approaches with distinct testing categories. The MiG programs (HAVE DOUGHNUT/DRILL/FERRY) conducted technical analysis of performance and flight envelope, followed by operational testing in mock scenarios. Similarly, the X-56A program uses two distinct wing configurations (stiff and flexible) for testing, with initial airworthiness tests followed by specific flutter mode testing, and both programs extensively documented their findings through various measurement systems.","context":["At the height of the Vietnam War, the skies were filled with technologically-advanced American aircraft from both the US Navy and the USAF, yet the air battles were a thread-bare echo of past glories in the 1950s skies over Korea's MiG Alley. By 1967 the Navy had a kill ratio of only 3.7 to 1 (3.7 MiG kills for every Navy fighter lost to a MiG) and the USAF was even worse, with a kill ratio of only 2.2 to 1. By comparison, at the end of the Korean War, the USAF pilots of the North American F-86 Sabre alone had a kill ratio of 10 to 1. While various studies and reports like the Navy's Ault Report offered many suggestions, the basic fact of the matter was that the art of dogfighting as a skill had been lost. In the USAF, for instance, the solution in the 1960s to an increasing accident rate in the McDonnell Douglas F-4 Phantom II was to simply ban air combat maneuvering training (ACM)- the accident rate fell, but legions of Air Force Phantom drivers entered the skies of Vietnam with little experience in knowing that their aircraft could and couldn't do in a dogfight with North Vietnamese MiGs. While a the solutions that eventually restored American supremacy in the skies are complex and beyond the scope of today's blog post, the foundations were being laid down in the black world in the latter half of the 1960s.\nThe story begins on 16 August 1966 in the Middle East. Operation Rolling Thunder, the sustained air bombardment of North Vietnam, had begun the year prior and would continue until 1968. Monir Radfa, an Iraqi Air Force captain, took off in his Mikoyan MiG-21F from Rashid AB outside of the Iraqi capital for what was supposed to be a local navigation exercise. Instead, he made a dash to the southwest at low level, intending to defect to Israel. The Jordanians failed to intercept him as he streaked low across their country, the RJAF's Hawker Hunters too slow at low level. Once over Israel, he lowered his landing gear and wagged his wings to two intercepting Israeli Mirage III fighters, signaling his intentions and was escorted to Hatzor AB and given asylum. With the MiG-21 being one of the most potent fighters in the Arab air forces that threatened Israel, they immediately set about flight testing the MiG-21 for over 100 hours over the next 12 months, learning its strengths and weaknesses and teaching the Mirage III pilots (the French delta was the main fighter of the IDF in those days) how to defeat the MiG in a dogfight. Initially hesitant to share its prize with the United States, Israel eventually concluded an agreement brokered by the US Defense Intelligence Agency (DIA) to loan the MiG-21 to the US for study in exchange for being allowed to buy the F-4 Phantom II, the American front-line fighter of the day. At the time the Israelis had made several overtures to the Johnson Administration to purchase the Phantom, only to be rebuffed out of a fear by President Johnson of escalating matters in the Middle East. Now that the Israelis had leverage, the Phantoms would be on their way and the US would finally get to study its vaunted adversary in the skies of Vietnam up close.\nThe MiG was disassembled and transported by a Lockheed C-5 Galaxy to the USAF's secret testing base at Groom Lake, Nevada (Area 51). Responsibility for evaluation of the MiG-21 was given to the USAF's Foreign Technology Division (FTD) that was part of the Air Force Systems Command based at Wright-Patterson AFB in Ohio. AFSC assigned all of its programs with the code word prefix \"HAVE\". For example, the original stealth demonstrator aircraft that gave rise to the Lockheed F-117 Nighthawk was code named \"HAVE BLUE\". In the case of the MiG-21 on loan from Israel, it was code named \"HAVE DOUGHNUT\". Two categories of flight testing were performed on the MiG- the first type concerned technical analysis- performance, flight envelope, engineering, structures, and so on. The second part of the tests were operational- the MiG would be flown in mock dogfights against US fighter aircraft. Because AFSC/FTD's emphasis was technical in nature, most of the HAVE DOUGHNUT flying concerned technical analysis.\nThe first flight out of Groom Lake took place in January 1968 and continued until April of that year before the MiG was returned to Israel. For three intensive months, the MiG was flown in various profiles to determine how it could be detected by both radar and infrared systems, it flew against the bombers of the Strategic Air Command to see how well the bombers' systems could detect and counter it, and infrared signature tests were carried out using a specially-fitted T-39 Sabreliner that could mount the seeker heads of various missiles in the US inventory. Out of a total of 102 sorties flown as part of HAVE DOUGHNUT, 33 sorties were devoted to operational testing in mock dogfights with the USAF and 25 sorties were devoted to mock dogfights with the Navy.\nNot four months after the end of HAVE DOUGHNUT, two Syrian MiG-17Fs on a navigation exercise got lost and inadvertently landed at an Israeli air base. Acquisition of the MiG-17s was of high importance to the United States as the MiG-17, though slow and dated, was nimble and the main adversary type encountered in the skies of Vietnam. Though limited to subsonic performance, VNAF MiG-17s were flying circles around American fighter pilots leading to the dismal kill ratios I mentioned above. After testing by the Israelis, the two MiG-17s were then turned over to the United States for analysis. The first MiG-17 made its first US flight at Groom Lake in January 1969 with the code name HAVE DRILL. The second MiG-17 then flew in March of that year with the code name HAVE FERRY. Both programs wound down by June 1969 and the findings were shared with the Navy's new TOPGUN school that was established to reintroduce dogfighting skills to Navy pilots. In addition, the findings of HAVE DOUGHNUT, HAVE DRILL, and HAVE FERRY were shared with the instructors at the USAF's Fighter Weapons School at Nellis AFB, Nevada, where they would go on to establish the Red Flag exercises.\nOn 25 November 1969 a Cambodian Khmer Air Force pilot defected to South Vietnam in the Chinese copy of the MiG-17F, the Shenyang J-5. The USAF pilot who flew the MiG-17 in HAVE FERRY and HAVE DRILL, Col. Wendell Shawler, was tapped by the AFSC/FTD to go to South Vietnam and make several evaluation flights of the J-5 to establish that it had the same flight characteristics as the MiG-17. This short program of just five flights from Phu Cat AB in South Vietnam was code named HAVE PRIVILEGE.\nAs a result of these four top secret exploitation programs, both USAF and Navy fighter tactics were changed and pilots were once again trained to exact as much capability and performance out of the aircraft as possible to win the dogfight. It wasn't until 1989 that a Pentagon official confirmed that in the 1981 combat of two US Navy F-14 Tomcats versus two Libyan Sukhoi Su-17 fighters over the Gulf of Sidra that the tactics used had been developed out of mock combat testing with US-operating Soviet fighters. Not long afterwards, HAVE DOUGHNUT, HAVE DRILL, HAVE FERRY, and HAVE PRIVILEGE were declassified. What didn't get mentioned was that a much bigger program succeeded those programs and would remain top secret for 20 years! But more on that program in a future blog post!\nSource: Red Eagles: America's Secret MiGs by Steve Davies. Osprey Publishing, 2008, p16-20.","\"I worked on a program in the 1980s where flutter really bit us, and that program was eventually canceled,” recalled Ed Burnett, the X-56A unmanned research aircraft technical program manager for the Lockheed Martin Skunk Works. “It became a personal interest of mine ever since then to kill flutter.”\nFlutter occurs at the point where aerodynamic forces acting on an aircraft in flight and the structural dynamic properties of the vehicle combine, or couple, to produce an often violent, harmonic vibration. The resulting vibration creates a potentially catastrophic condition that can quite literally break a wing or tail surface off an aircraft.\nIn simple terms, think of a flag on a pole. The wind blows at a specific speed, but the flag, with a specific set of structural properties, doesn’t wave uniformly from side to side. It usually dips, or swells, or bends. When conditions are right to cause the flag to pulsate, the flag is basically experiencing induced flutter. If the wind is too strong and the grommets or stitching fail, the flag is ripped from the pole.\nFlutter has been a known hazard almost from the beginning of powered flight. Even with more than a century of aircraft design experience, conditions where flutter occurs can still be hard to predict precisely. Aircraft designers traditionally have dealt with flutter by trying to avoid it—through such methods as adding structure.\nBy building enough stiffness into a specific structure, the natural tendency of structural dynamics and aerodynamics to couple can be prevented—even at conditions way outside an aircraft’s normal operating flight envelope. But the increased stiffness also adds weight—sometimes considerable weight—which, in turn, increases fuel use. And the end result—reduced range.\nThe X-56 team hopes to do something that is completely different. “The aim of the X-56A program is to mature flutter suppression technologies,” notes Burnett. “If we can suppress flutter by using the same flight control technologies used to provide vehicle stability, designers can use longer, more flexible wings and lighter weight structures. That will allow future aircraft to fly higher, faster, and farther than before.”\nLong And Thin\nLong, thin, high aspect ratio wings are considered key to the design of many future long-range manned and unmanned aerial vehicles and fuel-efficient transports. These future aircraft will likely bear many of the same features designed into the X-56, including a blended body, long thin swept-back wings, and very thin airfoils.\nThe X-56 was funded by the US Air Force Research Laboratory, or AFRL. The contractor test team hopes to demonstrate that they can accurately predict the onset of flutter, and by using the flight controls, actively suppress the aeroelastic instabilities of the aircraft.\nTwo types of wings will be tested on the X-56—a stiff set and a flexible set. The stiff set of wings is designed in the traditional manner with beefed up structure to avoid having flutter within the flight envelope. These are used for the initial airworthiness tests of the aircraft and any future non-flutter flight research. Flutter was deliberately designed to be deep in the flight envelope of the X-56A’s flex wing configuration. Because flutter is a natural coupling of the aerodynamics and structural dynamics, simply flying fast enough causes it to occur.\nThe X-56A is designed to have three different flutter modes within the flight envelope—body freedom flutter, or BFF; symmetric wing bending torsion; and anti-symmetric wing bending torsion. The active flutter controls on the X-56 are designed to suppress all three modes simultaneously.\nThe aircraft has external mounts that allow the wings to be removed easily and replaced with the flexible wings. The external wing attach points are covered by an aerodynamic fairing that looks like an upside down hot dog bun. Switching the wings takes about fifteen hours to complete.\n“The internal structure of the wings is the same on both designs, but the flutter wings are flexible,” noted Kent Burns, the X-56A program manager. “Both sets are made of a carbon fiber internal structure of spars and ribs, but the stiff wings have very thick, highly tailored carbon fiber skins and the flex wings have very thin fiberglass and foam skins.” Three identical sets of flex wings were built, just in case one set fails in flight as a result of flutter.\nThe flutter wings have internal water tanks and water pumps. The weight of the water is used to simulate the fuel that would be required for long duration flight such as a long-range transport or high altitude reconnaissance aircraft. “Due to the nature of this type of research, it’s possible that we could break a wing off during testing,” Burns continued. “If that happens on this vehicle, we’ll only be spilling water instead of fuel.”\nFido And Buckeye\nThe two parts of the X-56 program each have a descriptor. The test program is the Multi-Utility Aeroelastic Demonstrator, or MAD, and the aircraft itself is the Multi-Utility Technology Testbed. So the team picked up the name MAD MUTT.\nFollowing up with the canine theme, the test team had a dog tag painted on the underside of each of the two identical aircraft. Fido—from a slogan the team uses—Flutter It, Drive On—is the primary test aircraft, while Buckeye—named for the Air Force program manager’s dog and the mascot of his alma mater—is the backup.\nThe 480-pound X-56A has a twenty-eight foot wingspan and is a little over seven feet in length. The aircraft measures nearly five feet from the ground to the top of the vertical surface of the wing tip. Construction took about eighteen months.\n“We have a really good team—AFRL, NASA, the Skunk Works, and our partner, GMFI Aerospace and Defense,” said Burnett. “This was the first flyable aircraft GMFI built. We learned a lot and so did they.” GMFI, located in Southern California, is better known for building custom cars.\nThe X-56 is powered by two eighty-pound thrust JetCat P400 turbine engines, with an additional hard point in the center of the fuselage aft upper deck for mounting an additional engine or a tail like a conventional aircraft, if required.\n“We didn’t want propellers inducing low frequency noise into the flutter tests,” added Burnett, who is also a Lockheed Martin senior technical fellow. “The jets are pretty powerful and located close together. They are also out of the airflow affecting the wings.”\nA forward-looking camera is installed in the aircraft’s nose that allows the ground-based pilot to fly the aircraft. A small microphone is installed next to the front camera to allow the pilot to hear the aircraft, which provides significant situational awareness to the pilot and test team. The GPS and communications antennae are also located on the nose of the aircraft, which is adorned with nose art similar to the Flying Tigers of World War II.\nWhile very much a real aircraft, the scale on the X-56 is different. The chocks are little blocks of wood and safing pins and the red Remove Before Flight streamers are actually key chain fobs. A shop-vac setup blows cooling air to the avionics bay while Fido or Buckeye is put through its preflight checks.\nAdditional cameras located on the composite fuselage of the X-56 are pointed outward toward the wingtips to record the flutter events. A grid pattern is painted on the inside of the wingtip to provide a visual reference to measure the amplitude of the flutter.\nThe X-56 is capable of a maximum speed of 150 knots, but flutter onset with the flexible wings comes at about 110 knots, a speed well within the aircraft’s flight envelope. The goal is to detect the onset of flutter using accelerometers on the wing structure and then to send commands to the flight controls to dampen the vibration and allow the aircraft to fly well beyond the flutter onset speed.\nThe aircraft also has a ballistic recovery system if the wings break during testing or if the aircraft departs controlled flight. The aircraft’s landing gear was sized for impact if the aircraft had to use the emergency system parachute with the aircraft at maximum weight. If the recovery system isn’t needed for a particular test, the cavity can be used for twenty-eight pounds of test payload.\nThe X-56A is more than just the aircraft—it’s a complete research system, with a ground control station, simulation capability, a system integration lab capability, and vehicle transportation and storage equipment.\nThe Skunk Works developed the ground control system using commercial off-the-shelf equipment and a commercial trailer to reduce costs. The pilot and test director sit side-by-side in the front of the trailer with the test coordinator sitting behind them. The flutter and flight control test engineers sit at the consoles along the side wall. The test data conductor works in the back of the trailer along with an air conditioner that keeps both the people and the racks of equipment cool.\nA second trailer is used to transport the aircraft. A third trailer, a small flatbed, is used to tow the aircraft and to swing it easily to conduct ground communications checks.\nNot A New Idea\n“I’ve been around a lot of programs over my career. We started talking about active control methods to combat flutter in the 1970s,” said Jeff Beranek, the X-56 chief engineer and flutter lead. “There is a paragraph in a Lockheed research report from that period that proposed a program exactly like what we’re trying to do now with X-56. The methods and technology of the era were not sufficiently mature to successfully achieve the integrated suppression we are doing today.”\nThe X-56A program began with small independent research and development contracts in 2005. “We built five small hand-launched vehicles that had wings made of carbon fiber and Styrofoam. They were essentially designed to break to demonstrate flutter prediction capability and then ultimately demonstrate flutter suppression,” said Beranek.The second phase of testing with Fido will use a flexible set of wingsThe second phase of testing with Fido will use a flexible set of wings\nThe relatively inexpensive and small, ten-pound, ten-foot wingspan vehicles built under the BFF program were designed to be flown to their physical limit. The miniaturization of computers in the late 1990s was the big push in moving flutter suppression studies forward. This allowed high-risk flutter suppression testing on the relatively inexpensive BFF platform. “The X-56 wouldn’t be possible without the increase in computational power since the early 2000s,” observed Beranek.\n“But testing those aircraft showed we could predict flutter onset and perform flutter suppression,” noted Burnett. “The results of the BFF research led to an AFRL contract to mature active suppression, and the building of two more BFF aircraft. Of the total of seven aircraft built, only two of the BFF vehicles survived and both of them have been used for additional research. We also conducted other studies. The combination of BFF research and those studies led to this program.”\nPrior to first flight, the X-56 was put through a series of static tests. Engineers used bags filled with lead shot and piled the shot bags on the wings, similar to conducting static testing from the 1930s.\nThe X-56 test team set up in the same hangar at Edwards AFB, California, which was used to prep the XP-59, America’s first jet aircraft, in 1942. The X-56A was flown for the first time at the NASA Dryden Flight Research Center at Edwards on 26 June 2013. Lockheed Martin test pilot Mike Hartenstein sat at the left-seat console flying Fido. On the test card was a drawing of a dog wearing goggles.\nThe stiff wing portion of flight envelope expansion was completed in eight flights through the end of September 2013. The flights lasted about twenty-five minutes each. This completed the first phase of its flight testing.\nThe second phase of testing with Fido will use a flexible set of wings that have known structural instabilities within the flight envelope. The team finalized development of the flight control law gains in the fall of 2014. These flight control law gains will simultaneously control the vehicle and the structure to make it seem to the pilot that the X-56 is a stable, rigid aircraft.\nAdditionally, the Skunk Works team is working with NASA engineers to ready Buckeye for the first phase of NASA’s flight test program. These test flights will provide NASA with confidence in their aircraft control system for the stiff wing configuration that will allow for future flight test programs.\nIt’s been said that flutter is a particularly nasty dragon that lives in one corner of the sky. “The work we’re doing with the X-56A may not kill flutter completely,” observed Burnett. “But we will have a much better understanding of how to tame it.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:0dc09d95-4464-4117-aec6-4a727c0356ec>","<urn:uuid:7867a55c-a03d-45ea-be08-4c8fdd6bc1d1>"],"error":null}
{"question":"What impact do shorter growing periods have on soybean water needs and biomass buildup?","answer":"Group 2 soybeans require much less irrigation despite having a shorter growing season. In terms of biomass accumulation, soybeans only accumulate 60-70% of their total biomass until R5 stage, with the remaining 30-40% accumulated during the seed filling period (R5-R7). This challenges traditional agronomic research which suggests that longer growing periods and more light capture should lead to larger plants and higher yields.","context":["What is in this article?:\n- Crop physiologist Larry Purcell’s research at the University of Arkansas focuses mostly on soybeans.\n- Means studying “how we can utilize the resources of light, water and nutrients more efficiently through crop management and genetic differences among lines.”\n- Also studying traits, drought-tolerance and irrigation.\nPurcell is also studying soybean drought tolerance. Sponsored by the United Soybean Board, the project includes researchers in Arkansas, Georgia, North Carolina, Missouri, Nebraska and Minnesota.\n“It’s a large, regional project that has been going on for several years,” he says. “The project is led by Tommy Carter with the USDA at Raleigh, N.C.\n“We’ve been looking at two main traits. The first involves the differences that we can visually notice in how quickly different soybean genotypes wilt. The second trait involves differences in the sensitivity of nitrogen fixation to drought among soybean genotypes.”\nThe main focus of the project, says Purcell, “is to get the genetic differences we see — in wild type soybeans and plant introductions and unimproved soybean germplasm — and move those traits into highly adapted, high-yielding varieties. We’ve been able to do that through molecular marker technology, tagging the genes and moving them into more adapted germplasm, and through traditional breeding methods, as well.”\nHe says “great progress” has been made on both traits. “We have soybean germplasm — related to prolonged nitrogen fixation during drought — that’s been released for breeders to freely use forever. We’ve also had soybean germplasm related to the delayed wilting trait. That’s been in USDA uniform trials, and these lines have outperformed all other germplasm of the same maturity group under drought conditions.”\nUnder well-watered, high-yielding conditions, “it’s neck-and-neck with other germplasm,” Purcell says. “But when drought has decreased yield to the 30 bushels to 45 bushels per acre range, these genotypes perform very well, having a distinct advantage and providing, usually, a 5 bushel to 9 bushel bump. That’s still very meaningful.”\nHow might Purcell’s research incorporate the Roundup Ready 1 (RR1) trait that’s becoming available?\n“From a breeding perspective, it shouldn’t be a big deal to incorporate the RR1 trait into our germplasm, although this is not something we’ve pursued.\n“We’ve taken a bit different approach. That is, we believe the way we can make the biggest impact and get this to the most farmers is by making all this breeding material freely available. That means any public institution or private company can pick it up and make crosses. That’s our philosophy as a group.”\nFor several years, Purcell has also studied a drought avoidance mechanism. “It incorporates work done by Glenn Bowers, Larry Heatherly, Lanny Ashlock and others on avoiding drought by planting Group 3 and Group 4 soybeans early.\n“We’re checking on ratcheting up that practice a bit. We’re looking at planting early-maturing soybeans in the northern portion of the Delta a bit later. Can we still get the same yield bump from drought avoidance or decreased irrigation?”\nPurcell and colleagues have looked at “everything from Group 00 to Group 5. “We’ve used narrow-row spacings and increased population densities, while trying to plant north of Interstate Highway 40, where you typically don’t want to plant mid-March, but rather in late April or early May. That’s because the soil temperatures provide a little later planting window.\n“The research has shown that when we’re in that planting window with drilled-row spacings and higher plant populations — and I’m not talking about really extreme numbers, maybe 150,000 to 170,000 plants per acre — we have obtained, under irrigated conditions, essentially the same yields from Group 2 soybeans as Group 4s.”\nThe findings have from been “exciting,” Purcell says. “We’ve shown it again and again, even though it goes against everything that’s been published in the scientific literature. The general idea in agronomic research is that the more light you’re able to capture, the bigger the plant, the larger the crop and the more yield you can expect.\n“But we’ve not seen that. Under irrigation, we’ve seen a Group 2 soybean with roughly 95 days from emergence to R-7. And it has almost identical yield to a Group 5 soybean with 120 days from emergence to R-7.”\nThat provides “a lot of flexibility as far as what the planting and harvest windows can be,” Purcell says. “And we’ve also found that when growing the shorter season varieties it means much less irrigation.”","The latest USDA-National Agricultural Statistics Service, Kansas Crop Progress and Condition report classified 67% of the soybean crop to be in good to excellent condition. Overall, only 4% of all soybeans in Kansas are dropping leaves with most of the crop entering into the “seed-filling” period.\nThe weather conditions expected in the next coming weeks will be critical for soybeans (mostly for early-planted crops) to define not only the final attainable seed weight, but also the final number of pods and seeds per pods (or seeds per unit area). The 8 to14-day weather outlook from NOAA indicates a normal to slightly above-normal probability for precipitation in the central and eastern parts of Kansas, but a below normal probability for the western region, potentially shortening the seed filling duration due to stress conditions. There is still quite a bit of yield to be defined in the next month of growth for soybeans. Relative to the final total yield (biomass) at harvest, soybeans only accumulate 60-70% until R5 stage, with the other 30-40% accumulated during the seed filling (R5-R7), lasting 30-40 days (see Figure 1, initial seed filling around 1000 °C d).\nFigure 1. Soybean biomass accumulation from planting to maturity, seed filling period from 1000 °C d to maturity. Figure prepared by Ignacio Ciampitti and Guillermo Balboa, K-State Research and Extension.\nSoybean dry matter accumulation and water changes during seed-filling\nSoybeans will reach final maturity with high seed water content, moving from 75-80% (R6) to around 50% (R7) from beginning of seed filling until final maturity (Figure 2). Final maturity is defined as the formation of the black layer in the seeds. The process of seed dry matter accumulation and moisture changes will depend on the maturity group (affecting the length of the season), planting date, and weather conditions experienced during the latter part of the reproductive phase.\nChanges in the water content during the seed-filling process (Figure 2) were previously described in our “Soybean Growth and Development” poster. As described for corn, seed water loss for soybeans can also divided in two phases: 1) before “black layer” or maturity, and 2) after black layer.\nFigure 2. Soybean seed filling process from full seed to full maturity. Photo and infographic prepared by Ignacio Ciampitti, K-State Research and Extension. Taken from Soybean Growth and Development.\nThe overall contribution of seed weight to final yield can be studied by evaluating changes in seed weight during the seed filling period, building a dataset portraying the rate and duration of the changes in seed filling (Figure 3).\nIn the example presented below, overall seed filling lasted more than one month (37 days) until black layer (no changes in seed weight) was achieved. The graph of seed filling provides a visual of the overall rate, increase in seed weight per day, and the duration of the seed filling (Figure 3). With this information, we can improve our understanding of potential impacts of stress conditions during this time of the season for soybeans. In the example presented in Figure 3, you can see the impact that decreasing the effective duration of the seed filling period has in the final yield. When the duration is reduced by one week (from 37 to 30 days), the attainable yield dropped from 61 to roughly 50 bushels per acre.\nPotential impacts on leaf green area imposed by insects, diseases, hailstorms, and any other potential abiotic stress conditions (extremely high temperature, cloudy days, lack of timely precipitation, and early frost) impacting the crop during the coming weeks will negatively affect the seed filling conditions for soybeans.\nFigure 3. Soybean seed weight changes from beginning of seed filling (R5) to full maturity. Photo and infographic prepared by Ignacio Ciampitti and Santiago Tamagno, K-State Research and Extension.\nIn summary, much of the yield for the Kansas soybean crop is still to be determined in the coming weeks. Scout your fields for any potential issues impacting overall plant health to maximize the chances of maintaining yields.\nIgnacio A. Ciampitti, Crop Production and Cropping Systems Specialist"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:375c235f-a78b-4c82-a74c-56f10e24075d>","<urn:uuid:fc47a482-2911-495f-8e49-ab946fd9325d>"],"error":null}
{"question":"How do modern digital banking features benefit businesses compared to historical banking practices?","answer":"Modern digital banking offers businesses significant advantages over historical banking practices. While traditional banking required adhering to bank operating hours and manual processes, as exemplified by the 1950s practice of physically depositing business earnings, today's digital banking enables 24/7 operations with features like automated payments, instant fund transfers, and real-time account monitoring. Businesses can now make decisions based on immediate transaction information rather than waiting for monthly statements, automate regular payments to employees, and manage banking tasks outside traditional banking hours, leading to increased productivity and efficiency.","context":["PIERCE’S IRON BANKING BUILDING\nby Ann Angel Eberhardt\nPIERCE’S IRON BANKING BUILDING\nThere was a lot of trust in the hearts of small-town citizens in the 1950s. In fact, I don’t remember if we even thought much about it. Trust was something that was taken for granted when doors were left unlocked overnight or we children ran about the neighborhood unsupervised. In those days, there were no such things as identity theft, car alarms or security cameras.\nAn example of this was my father’s instinctive trust, not only in us kids but in small-town society in general, when he sent us to the bank each week to deposit cash and checks from his printing business. I can still picture my brother or myself, about 9 and 11 years old, carrying that yellow King Edward cigar box weighed down with rolls of coins and checks as we walked along the dirt path that ran between our Second Street house and the Erie Railroad tracks. When we reached North Walnut Street we would leave the path to turn left, cross the tracks, then take a right on East Shenango Street.\nAfter another block or two, we reached a row of buildings that included the three-story First National Bank on the corner of North Mercer and East Shenango streets, less than a half-mile from our home. There, barely able to reach the teller’s window, we would slide the contents of the box under the teller’s cage, the teller would tally the items in a little bank book, date-stamp and initial the entries and return the book to us.\nThere was a bit of irony in those regular deposits that were earned by my father’s business. In earlier years, Dad was turned down by a bank’s employee when he asked for a loan to start up his printing business. Now that Dad’s business was doing well, each deposit must have been very satisfying to him.\nJames Pierce’s Legacy\nThe Iron Banking Company building, built in 1871 by General James Pierce.\nCorner of Mercer and Shenango Streets, Sharpsville, PA.\n[Click on image for enlargement.]\nAs children, we weren’t aware that the bank building we visited, like the Pierce Mansion we passed along the way (before its demolition in 1952), was already four decades old and part of the James Pierce legacy.\nThe structure was constructed in 1871 by “General” James Pierce (1810-1874), president and principal owner of Sharpsville’s first bank, the Iron Banking Company. It was built to resemble the Italianate style of architecture popular nationwide in the mid- to late-1800s, with its rectangular shape and its row of seven tall front windows that were rounded on top. The Geddes & Pierce Foundry supplied the cast iron front of the building.\nJames Pierce’s presidency was followed by that of his son Frank (1852-1931). The Iron Banking Company was later converted to the First National Bank of Sharpsville in c. 1905. In 1964 it became a branch of the McDowell National Bank in Sharon. Later, the building housed other banking institutions, including a PNC branch until 2013. As of 2015, the first floor was occupied by Meadville Area ONE Federal Credit Union. The two brick buildings on North Mercer Avenue are now part of the Sharpsville Borough Historic District.\nThere was another reason we kids regularly visited Sharpsville’s First National Bank back in the 1950s. Hoping that we would develop a savings habit, Dad made sure we belonged to the Christmas Club, a program that banking institutions had developed to promote their services as well as holiday spending. He belonged to such a club when he was a young lad in Cleveland, Ohio, memories of which he recorded in his memoir, “Trivia & Me.” The setting was in the 1920s, a bit earlier than the Great Depression, the period Wikipedia indicates as the time the Club became widespread. Dad’s descriptions of the Christmas Club generally match those that I remember experiencing in the 1950s. He writes:\nIt was the era when banks sponsored Christmas Clubs. People — especially youth — were encouraged to deposit small amounts of money each week for 50 weeks. Banks solicited five cents or 10, 25 or 50 cents to do the double job of teaching people to save money and promoting Christmas sales for merchants. The banks would issue a passbook in which a teller would record the weekly deposits and then initial the entry. Two weeks before Christmas, one could withdraw the savings in cash (without interest) for a shopping spree. For several years I managed to join the 10 cents club and was awarded the joy of a cash harvest of $5 at Christmas time.\nEven though the interest rate was low or nonexistent and fees were charged for withdrawals, I had a feeling of accomplishment when I received that check in early December. And the Christmas Club may have contributed to the way we siblings handled our finances since then, leaning more toward careful than spendthrift. The Club exists to this day, although primarily run by credit unions.\nPierce Opera House\nFor 40 years after Pierce’s bank building was constructed the 3,000-square-foot third floor served as Sharpsville’s cultural center, having been home to the Pierce Opera House. There is limited information about the shows performed in those early days, but it is known that the organization offered a variety of musical events and featured speakers. Once motion pictures became popular, they were shown as well.\nIn addition, the two upper floors were used for high school graduations during the late 1800s until c. 1920, an occasional basketball game in the early 1900s and as a meeting place for the Order of the Eastern Star and the Masons. The building also housed the original offices of the town’s early newspaper, “The Sharpsville Advertiser,” started by Walter Pierce, James Pierce’s son. After the 1920s this floor remained unused for some time.\nIn the early 2000s, Michael G. Wilson and his family began restoring the opera house which had been left neglected behind a concealing wall for some eight or nine decades. Wilson, owner of the building since 1999, had been a longtime Borough Manager of Sharpsville who retired January 2017. The Wilson family found — and preserved — much of the opera stage’s original trappings and equipment once the wall was removed. For photos of old-time ticket booth posters and graffiti, go to Sharpsville Area Historical Society’s “Opera House Pictures.”\nWanting to see the restoration continue in good hands, Mr. Wilson sold the building to Dr. Francisco Cano, an allergist/immunologist from Greenville, PA, himself professionally trained in operatic voice. Cano’s love of opera and the arts was a driving force behind the ongoing phases of restoration designed to house theatrical, musical, and opera performances once again. The first performance of the Pierce Opera House’s revival was in 2009.\nAccording to the July 2013 SAHS Newsletter,\nThe Pierce Opera House itself is worth the visit. This historic venue features beautifully restored woodwork, excellent acoustics, and a warm intimacy between the audience and the stage. Modern climate control and conveniences have been introduced to this 142-year-old local treasure.\nThe Valley Lyric Opera, which now resides in the Pierce Opera House, provides an excellent level and variety of programs. Past performances include the operas Aida, La Traviata, La Boheme, Rigoletto; musicals [performed by the Area Community Theatre of Sharpsville — ACTS] South Pacific, Man of La Mancha, as well as ballets, musical tributes to Neil Simon and Andrew Lloyd Webber and a host of other outstanding offerings.\nPierce Opera House has once again taken its rightful place as Sharpsville’s center for the arts. Visit them online for future developments and upcoming performances: www.valleylyricopera.org\nAngel, August D. Trivia & me: an octogenarian mirrors his twentieth century. London, KY: August David Angel, 2007. Print.\n“Bravo! Sharpsville steps into act with opera performances in July.” 22 March 2009. http://www.vindy.com/news/2009/mar/22/bravo-sharpsville-steps-into-act-with-opera/ [accessed 31-Oct-2017]. Internet resource.\n“Christmas Club.” Wikipedia website. en.wikipedia.org/wiki/Christmas_club [accessed 31-Oct-2017]. Internet resource.\nHanes, Gail Nitch, Sharpsville: Our Home Town — Then and Now.” 2012. Pp. 13-18 and 57-59. Powerpoint on PDF. Internet resource.\n“More About Rigoletto.” Sharpsville Area Historical Society Newsletter, July 2013, Vol. II, No., 2, page 2.\nPierce Opera House website. www.valleylyricopera.org [accessed 23-Oct-2017]. Internet resource.\nSharpsville Area Historical Society’s Newsletter, March 2017 issue, page 2, for more about the Opera House Block.","If you are a banker, online shopper, tech savvy person or even a regular bank customer, you must have definitely heard the word “digital banking”. Interestingly, most people have a different take on digital banking. There is a lot more to digital banking than just a few features that we can see on the surface. Digital banking is converting the brick and mortar banks into more greener and efficient places to operate. There are a plethora of options that people can opt for when it comes to banking. Now people can check their bank account details, pay their bills online, transfer money to other accounts, and all of this can be done from the very comfort of their home. All that the people need for banking these days is an internet connection.\nThe evolution of digital banking\nThe ATM cards and credit cards were pioneers in digital banking so to speak; these cards prepared the way for further development in digital banking. It was the introduction of internet in the early 90’s that brought about the revolution in the banking sector. The traditional banks on the street side started to think about various ideas that could help them to cut down on their operating cost and provided restricted online bank services.\nThe efforts in this area proved to be beneficial, and so the banks took another leap towards creating their own space on the internet by designing a website. The website opened up a way for many features like opening new accounts online, downloading the necessary forms, and also applying for online loans. Under the digital banking umbrella, with time, there were many new features added, such as income tax filing, bill payments, transfer of funds, opening recurring accounts, fixed deposit, etc.\nWith the advent of technology, many people prefer to do everything online. From online shopping to online dating, people just love to do everything from their couch. Digital banking has offered many comfortable features and possibilities to people. The mentioned below advantages of Digital banking highlight the importance of this crucial technology in our times.\n• Your banking is much easier\nThe online banking feature provides you the luxury for banking anytime and anywhere. Throughout the year, the website services are offered round the clock for internet banking; except for time needed for website maintenance. On the internet banking page, you can see the summary of your real-time bank account. This mode of banking helps to keep a check on the bank account at any time. Even if you want to make some payments, change your mailing address, or contact details, all that you need to do is click on few pages.\n• Advanced banking options\nThese days, there is a lot of competition in the banking sector, and most banks want to have an edge over other banks to be successful. Therefore, the banks come up with advanced websites that have several banking options. Some of the sturdy features of digital banking on advanced websites are loan calculators, premium calculators, financial planning tools, tools to help analyze investments, budgeting, forecasting, and also tax preparation, etc. This helps a consumer to keep alert with many different features in an effortless way. As a result, most of the financial planning can be done efficiently without the need to personally visit a bank.\n• Mobile banking options\nBanks come up with several advanced apps that help in online banking from the smart phones. As a result, the customers can do online banking from anywhere and at any time. The banks make mobile friendly websites and features that help customers to bank efficiently. With just a few clicks on the mobile phones, the money can be transferred and bills can be paid with great ease.\nAlong with the above mentioned benefits, online banking is also an eco-friendly option. With the help of digital banking, a lot of paper is saved and other such areas like office space, travel, and so on are made greener.\nHighly beneficial option for businesses\nMany businesses have built their brands and are thriving only because of the digital banking. If we did not have the luxury of online banking, the businesses like Amazon and eBay may not be in existence today. However, now the businesses can enable online payments option and every process is made simple after that. The businesses can see their banking activities such as the deposits, wired funds, bank checks, and so on. So rather than waiting for monthly statements, the businesses can take instant decision about their functioning. The errors and delays can be quickly sorted out before any negative impact to the business.\nIncreased productivity in business\nWith the help of digital banking, most businesses do not have to rely on the bank operation timings. Now the payments can be made and received even in the odd hours. There are also some processes like paying bills or making regular payments that can be automated in the digital banking platform. As a result, the businesses are able to save a lot of time on the manual processes and this has a great impact on their productivity.\nEasy transfer of payments\nOne of the most important things for businesses these days is time, and so they want to use it on something that is providing them more value. The digital banking helps the businesses to easily transfer payments in their employee bank accounts. Some of the regular transfers can also be automated, and so the professionals are able to use their valuable time on something that is more important. As a result, the internet banking adds more value to most of the businesses.\nMade everything easy\nWith the help of digital banking, now shopping, travelling, eating, healthcare, entertainment, business, etc. have become more efficient. Almost every purchase these days can be done with the help of internet banking. With the advancement in digital technology, now most people do not even visit their banks as the maximum amount of their work is done online or is automated. Digital banking has made everything easy for everybody.\nWide range of potential\nAbove mentioned are just some of the areas that we have seen in the digital banking. However, when we see the growth rate and applications in online banking, we can get a fair idea about what all things we can expect in the future. Even with hundreds of features to offer, it is right to say that digital banking has many more features to offer in the future.\nThis huge revolution in the banking world has benefited one and all. From the employee working in a small office to a businessman running a million dollar business, online banking has proved to be important in their success. With so many benefits offered for us, it is highly impossible for us to think a world without digital banking. However, still there are a lot of people that are reluctant to use digital banking or there are some who do not know how to make the best use of this important feature. Digital banking is the face of advanced technology, and so it is vital to embrace it in most of our personal and business banking areas."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:1b867a66-8f43-45d8-9c8c-0578944cdba0>","<urn:uuid:696047a3-fdd2-4e26-9204-e7c846dd0728>"],"error":null}
{"question":"What is the relationship between post-treatment care for laser procedures and cancer prevention protocols?","answer":"Post-laser treatment care and cancer prevention share crucial protocols, particularly regarding sun protection and risk reduction. After laser treatment, patients must avoid sun exposure and consistently use sunscreen to prevent new pigmentation from developing. Similarly, cancer prevention focuses on risk reduction through sun protection measures including broad-spectrum sunscreen, protective clothing, and avoiding peak UV hours (10am-3pm). Both contexts emphasize early detection and risk management - laser clinics require medical clearance for suspicious lesions and refer patients to general practitioners for assessment of potentially cancerous spots, while cancer prevention involves screening tests and monitoring of risk factors. The goal in both cases is to protect skin health and prevent cellular damage from UV radiation.","context":["What types of brown spots or pigmented lesions will laser remove?\nThe MedLite Q-Switch and Candela GentleLASE laser is most commonly used for removing brown age spots, freckles and many types of brown birthmarks. Your laser therapist will identify your specific type of lesion and discuss the removal success rate associated with it. Important: The laser cannot be used for the removal of lesions that are cancerous or suspected of being cancerous. If there is any suspicion that the lesion may be cancerous, we will strongly recommend that you consult your General Medical Practitioner without delay.\nIs laser safe?\nYes. The Q-Switch laser and GentleLASE laser produces a controlled beam of light specifically intended for the aesthetic treatment of pigmented lesions and approved for that use. The laser light, although bright is not strong enough to penetrate the skin below a depth of 1mm so it cannot damage other organs. The MedLite Q-Switch laser used by City laser Clinic for laser Skin Rejuvenation is one of the safest lasers available today.\nCan laser remove my normal skin pigment also?\nThe laser light was designed for its properties that selectively target dark concentrations of melanin. If after removal of the abnormal concentrations of melanin treatment continued on normal skin; some whitening could occur. However, your body’s natural regenerative processes would replace the normal melanin after a period of time, returning the skin to its natural colour.\nAre there any Precautions to take during treatment?\nAs with all laser treatment, you must avoid exposure to direct sunlight before treatment, during treatment and for some weeks following laser treatment. At City Laser Clinic, our experienced qualified laser therapists will advise you based on your skin type and the nature of the treatments that you are receiving. SPF must be worn daily. The City Laser Clinic laser therapists do sometimes need to refer clients to their General Medical Practitioner for assessment and when in doubt will not treat until a Medical Clearance is provided.\nDoes it hurt?\nThe level of discomfort is different for every person depending on sensitivity; other considerations include the location and surface area to be treated. Most clients do not request any pain control during treatment and relate the discomfort as comparable to mild sunburn. Your laser therapist will discuss your options with you based upon your own tolerance and the body area to be treated.\nAre there any possible adverse side effects?\nA small percentage of persons treated may experience side effects (unwanted outcomes), these side effects are usually temporary and mild, though can be more severe and longer term. They include:\n- Scarring though rare and more likely with darker skin types, it can occur with all skin types and especially if the required precautions (relating to sun exposure and some medicines) are not followed, before, during and after treatment;\n- Hyper-pigmentation (darker skin) can occur because of inflammation caused by laser treatment and a brown mark arises that usually takes some months to fade with the assistance of lightening agents;\n- Hypo-pigmentation (lighter skin): laser treatment can cause a loss of skin pigmentation in the treated area. This pigmentation usually returns but can take up to 18 months and in very rare cases can take so long that is considered permanent;\n- Bacterial or viral infection. Any infection in a wound created by a laser has the potential for scarring and laser treatments have been known to trigger herpes simplex outbreaks;\nHow does it look afterwards?\nAfter the laser treatment, the area will feel hot, similar to a sunburn, sometimes for up to several hours. Discomfort disappears over the next few hours, a scab or crust will develop over the area, and this looks darker than the original lesion. This will last between 12-18 days depending on the size and site (longer on arms and legs). When the scab comes off, the area will be pink for another couple of weeks and then will return to its final colour.\nHow much area can be covered in one laser treatment?\nAt least 80% of all common pigmented lesions are removed with one laser treatment. If a lesion covers a large area, your laser therapist may advise you to treat the area in several stages, to avoid unnecessary discomfort.\nHow long will the healing process take?\nEveryone heals at various rates depending on the depth of laser treatment, the particular areas treated and your own health and condition. Noticeable results are typically seen in a few weeks. Some areas may take up to 12 weeks for the full laser treatment benefits to become apparent.\nWhat are the possible unwanted effects of Laser treatment?\nLasers have been around for decades and are proven safe and approved for use, occasionally we see some mild unwanted effects including redness, swelling, and light bruising of the treated area. These side effects typically endure for only a few days, less frequently there may be a more noticeable change in skin pigmentation after laser treatment.\nIncreased pigmentation (Hyper-pigmentation)\nThis is more likely to occur in those individuals with darker skin. It may also occur if you expose your skin to the sun and solariums within three months after laser treatment. Whenever you go outdoors in the first three months after laser therapy, you must always wear effective sunscreen protection on the treated area for example an SPF 30+ sunscreen plus a good physical barrier such as a foundation or zinc cream. If hyper-pigmentation does develop, your family doctor can prescribe a de-pigmenting cream.\nDecreased pigmentation (Hypo-pigmentation)\nA small number of patients develop a lighter skin colour in the treated area. Skin colour usually returns to normal within 18 months.\nCan the excess pigmentation come back?\nCommon pigmented lesions do not return. Some lesions may return after a period of several months to a year. However, the procedure can be repeated with similar results. The conditions that we treat vary in extent and severity from client to client. Consequently, each client response to any laser treatment does vary, and we are unable to guarantee individual results. Unprotected sun exposure on the area will most definitely stimulate the Melanocytes and the treated lesion/s will return. New pigmentations can also arise, and be successfully treated.\nWhat type of post-treatment care is necessary?\nA shower can be taken the next day, but avoid scrubbing the treated area. Since most common pigmented lesions are caused by exposure to ultraviolet sunrays, it is imperative that you always wear sunscreen. Failure to wear a protective sunscreen will allow further sun damage to occur and result in new pigmented lesions.\nSome Final Advice – Please Protect your skin!\nFor best protection, Cancer Council Australia recommends a combination of sun protection measures:\nSlip on some sun-protective clothing – that covers as much skin as possible;\nSlop on broad spectrum, water resistant SPF30+ sunscreen. Put it on 20 minutes before you go outdoors and every two hours afterwards. Sunscreen should never be used to extend the time you spend in the sun;\nSlap on a hat – that protects your face, head, neck and ears;\nSlide on some sunglasses – make sure they meet Australian Standards.\nExtra care should be taken between 10am and 3pm when UV levels reach their peak. See www.cancer.org.au/cancersmartlifestyle/SunSmart/Preventingskincancer.html","Cancer prevention takes 2 basic forms: early detection and risk reduction. The purpose of early detection is to reduce the mortality associated with a cancer that already exists, but has yet to produce symptoms. This is the primary rationale for most cancer screening tests.\nThe purpose of risk reduction is to decrease the chances that a cancer will develop in the first place. The more we can learn about what causes cancer, the more likely we are to find ways to prevent it. In the laboratory, scientists explore possible causes of cancer and try to determine exactly what happens in cells when they become cancerous. Researchers also study patterns of cancer in the population to look for risk factors, conditions that increase the chance that cancer might occur and protective factors, which decrease the risk of cancer.\nWhat researchers do know is that cancer develops over time and is often due to a complex mix of factors including lifestyle, genetics, and environment.\nModifiable Risk Factors\nTo reduce the risk of cancer, concentrate on changing risk factors you can control.\nSmoking affects every cell in the body. When a cigarette is inhaled, the smoke and chemicals move into the bloodstream through the lungs. In the bloodstream, these harmful chemicals travel throughout the body. Some may be filtered by the kidneys, which produce urine. Urine is stored in the bladder until it is eliminated from the body. Other substances may be filtered by the liver or eliminated through the colon and rectum. Exposure to these chemicals causes irritation to cells and tissues. Long-term irritation is associated with changes in cells that can eventually lead to cancer.\nSmoking and using smokeless tobacco products and regular exposure to environmental tobacco smoke (secondhand smoke) are responsible for nearly one-third of all cancer deaths in the United States each year. Smoking is also associated with approximately 85% of all lung cancer deaths.\nPeople who smoke cigars or pipes have a risk for cancers of the oral cavity that is similar to the risk for people who smoke cigarettes. Cigar smokers also have an increased chance of developing cancers of the lung, larynx, esophagus, and pancreas. The use of smokeless tobacco (chewing tobacco and snuff) causes cancer of the mouth and throat.\nThere are several tools and options available to help you successfully quit smoking.\nWhile dietary factors clearly have a role to play, the connection between diet and cancer is complex and still not well understood. Some evidence suggests a link between a high-fat diet and colorectal, uterine, and prostate cancers. On the other hand, some studies suggest that foods containing fiber may help protect against colorectal cancer, though this has recently come into question. As a general rule, it is better to eat a well-balanced diet that includes nonsaturated fats, fruits and vegetables, and whole grains.\nObesity has been linked to cancers of the breast (among older women), prostate, pancreas, uterus, colon, and ovary. If you are overweight, talk to your doctor about how to safely lose the excess weight. A dietitian can help with meal planning. Once the weight is lost, it is important to take steps to maintain weight in a healthy range.\nIn the past, some studies suggested that antioxidant vitamins (C and E) and minerals (selenium) reduce the risk of certain cancers. More recent studies however, have failed to substantiate these findings. Similar conclusions are found with essential vitamins and dietary supplements, such as vitamin D and calcium. Herbs, vitamins, and supplements may interfere with traditional medications. Consult with your doctor before taking any medications. A dietitian can help with suggestions to get essential vitamins and minerals with proper nutrition.\nPhysical activity improves overall health and well-being. Regular exercise benefits the heart, blood vessels, muscles, bones, and mental health. Research also suggests a link between exercise and a reduced risk of cancer. Recommendations encourage at least 30 minutes of aerobic exercise per day on most days of the week. Aerobic exercise can be as simple as taking a brisk walk every day. Strength training can be done at least 2 days per week. More participation in physical activity increases health benefits.\nTalk to your doctor before starting any exercise plan.\nHeavy drinkers have an increased risk of cancers of the mouth, throat, esophagus, larynx, stomach, colon and breast, and liver. People who smoke cigarettes and drink heavily have an especially high risk of getting these cancers.\nThere is some evidence to support moderate drinking to reduce the risk of certain cancers. Moderate alcohol intake is 1-2 drinks per day for men and no more than 1 drink per day for women. It is not necessary to consume alcohol in order to gain health benefits if you choose not to drink.\nRadiation is all around us. Over time, exposure can lead to alterations in the cell structure or its DNA. These changes can sometimes lead to cancer.\n- Ultraviolet (UV) light from the sun—Causes premature aging of the skin and the 2 most common kinds of skin cancer, squamous cell and basal cell. UV light comes from other sources, like tanning booths and sunlamps. UV radiation has also been linked to melanoma, a skin cancer that can spread throughout the body causing death.\n- Ionizing radiation—From x-rays, radioactive substances, or other sources that cause cellular damage and DNA replication. Short- and long-term exposures increase cancer risk. Talk to your doctor or dentist before procedures that may expose you to radiation.\n- Radiation therapy for cancer may increase the risk of another cancer developing in the area where the treatment is aimed. Modern radiation therapy equipment is designed to deliver the minimum doses needed while still being effective. If you need radiation therapy, talk to your doctor about the risks and benefits.\nProtect yourself from UV light by:\n- Wearing protective clothing in the sun.\n- Staying in the shade when possible.\n- Wearing a wide-brimmed hat to protect your face.\n- Using broad-spectrum (UVA, UVB) sunscreen with sun protection factor (SPF)-15 or higher.\n- Using sunglasses that block UVA and UVB rays.\n- Avoiding the sun when it is strongest during the day (10 am-3 pm).\n- Avoiding the use of tanning booths or lamps.\nRepeated exposure to many chemicals commonly found in the workplace can increase the risk of cancer. Asbestos, nickel, cadmium, uranium, radon, vinyl chloride, benzidine, and benzene are examples of well-known carcinogens in the workplace. For example, inhaling asbestos fibers increases the risk of mesothelioma, a rare type of lung cancer. It is important to follow work and safety rules to avoid or minimize contact with dangerous materials. In general, employers are required to provide employees with information (material safety data sheets, [MSDS]) about dangerous chemicals in the workplace and the ways to protect against exposure.\nThese chemical may act alone or together with another carcinogen, such as cigarette smoke, to increase the risk of cancer.\nDoctors may recommend estrogen alone or in combination with progesterone (hormone replacement therapy) to control menopausal symptoms, such as hot flashes and vaginal dryness. Some studies have shown that the use of estrogen alone increases the risk of cancer of the uterus, which can be avoided by adding progesterone. Using both, though, has been recently shown to increase the risk of breast cancer along with other adverse effects. Because it is no longer clear that the risks of hormone replacement therapy outweigh the benefits, be sure to carefully review your options with your doctor.\n- Reviewer: Mohei Abouzied, MD\n- Review Date: 01/2016 -\n- Update Date: 05/13/2015 -"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:d7886b94-e14f-42c8-b226-15854e381317>","<urn:uuid:438353b7-c02f-4761-b093-76463920a4a4>"],"error":null}
{"question":"What is the role of pH balance in horse shampoos, and how does this relate to managing hoof health in wet conditions?","answer":"pH balanced horse shampoos should have a pH between 6.0 and 8.0, close to the neutral pH of 7.0 found in healthy skin. While pH balanced products are a good indicator of quality, there's limited regulation in the grooming industry. This pH consideration is particularly relevant when managing overall horse health, including hoof care, as excessive moisture can affect hoof integrity. In wet conditions, hooves can soften and become more susceptible to bacterial infection through gaps in the white line. To protect hoofs in varying weather conditions, hoof hardeners can be applied to protect from excessive moisture, while dressings like pine tar can help retain moisture during drought periods.","context":["Abscesses occur when bacteria get trapped inside the hoof (see photo). Trauma to the hoof sole from sharp objects such as nails, screws, and glass may carry bacteria and debris into the hoof from the outside environment. A horseshoe nail that is inside the white line (the junction of the hoof wall and sole), may also provide a route for bacteria to enter. Bacteria can also enter the deeper structures of the hoof when the hoof quality is poor and unable to protect the tissues properly. Poor hoof quality may be genetic or environmental. With wet winter and spring weather and in wet dirty stalls, the hoof wall can soften, permitting bacteria to migrate through the gaps in the white line. Very dry conditions, or alternating wet/dry weather, leads to brittle feet that are more likely to develop hoof cracks. Finally, poor hoof care resulting in long flared toes or crushed heels weakens the integrity of the white line and increases the risk of hoof abscesses.\nMost often with hoof abscesses, the horse becomes severely lame overnight. The hoof is like a fingernail in that there isn't much room for swelling; when pressure builds up it causes an extreme amount of pain and results in severe lameness. It is not uncommon for the owner to be worried about a fracture due to the degree of pain/lameness.\nOwners will likely not find any external wounds or swelling. If a nail or other object is seen in the hoof, resist the temptation to remove it and call your veterinarian immediately. The site and angle the object enters the foot will be important information for a veterinarian. Severe abscesses can lead to swelling and infection that extends up the leg. Owners may notice swelling and inflammation of the pastern (cellulitis), or swollen heel bulbs and coronary band. Often the hoof wall is warmer than usual and the pulses in the pastern region are more readily felt.\nA veterinarian will take a thorough history, and perform a lameness exam to ensure there are no fractures or other injuries. Hoof testers are used pinch parts of the foot, trying to elicit a localizing pain response. Sometimes, a crack or track can be identified after the hoof is cleaned and the old sole is scraped away. If a suspect area is found, a paring knife can be used to encourage the infection to drain. Some horses will need analgesics or local nerve blocks for this procedure. If no draining tract can be found, radiographs may be performed to look for gas (produced by the bacteria) within the hoof, as well as ruling out other potential causes for lameness.\nHorses with a mild infection can return to work in less than a week. Deep infections can take several weeks to heal and can lead to laminitis if not well-controlled. Call a veterinarian if:\n- The drainage continues after 48 hours or increases after that time\n- The horse remains painful or needs analgesics for more than 1-2 days\n- The horse loses its appetite\n- The horse shifts his weight frequently, rests his good leg or lies down more than usual\n- Tissue (proud flesh) grows out of the drainage hole\nThe goal of treatment is to drain the abscess and prevent further contamination. Due to the level of pain, it isn't humane to wait for the abscess to rupture on its own. A veterinarian will try to pare away a hole, just large enough to allow drainage of the pus pocket. The pain relief is usually immediate.\nAfter the abscess is opened, an antiseptic dressing, such as providone-iodine or a medicated poultice pad (i.e. Animalintex Poultice Pad) is applied to keep the abscess draining for the next 48 hours. A waterproof bandage such as a diaper applied with duct tape, or a hoof boot, is used to cover the poultice and hoof. It is imperative that the protective bandaging/boot stays clean to prevent prolonged infection and contamination of the draining hole. The horse should remain in a dry area, such as a clean, well-bedded stall, or small dry paddock with the bandage removed and changed daily. After the first 48 hours, antiseptic compounds and clean bandaging will generally suffice until the drainage stops and the wound is dry, however, the horse should remain in a clean, dry area.\nMultiple, daily warm water and Epsom salt soaks were previously considered an important part of therapy; however, it has been found that excessive soaking only further weakens the hoof and may have negative effects. Tap water soaks do help moisturize the sole, however, a veterinarian may recommend occasional foot soaks to encourage drainage. Phenylbutazone, firocoxib (Equioxx), or banamine may be prescribed if needed for pain relief or swelling control. The hoof bandage should remain in place until the drainage stops, the hole is dry, and the lameness is gone.\nTo prevent future hoof abscesses:\n- Maintain a clean, dry environment with routine stall cleaning and manure removal from paddocks.\n- If extreme changes in weather conditions are expected, hoof hardeners can be applied to protect the hoof wall from too much moisture, and dressings such as pine tar can be used to hold in moisture during drought.\n- Routine farrier care is important to maintain good hoof quality and strength.\n- Remove any nails, farm implements, metal pieces, and glass from the environment to minimize the risk of trauma.","Q: What is the best way to draw out a hoof abscess? I am receiving conflicting advice from the professionals who work on my horse. My veterinarian says to poultice, but my farrier says to just leave it alone, as poultice will soften the sole and make the foot more prone to injury.\nA: Actually, both your farrier and veterinarian are correct. A poultice and foot wrap are meant to draw out the infection more quickly than just letting time do its job. Keeping the hoof wet all the time definitely makes it soft, especially in a white-hooved horse. One way or the other, an abscess needs to drain to relieve pressure (and pain) and to get the infection out so healing can happen.\nSimilarly, controversy surrounds whether or not to give antibiotics for a foot abscess. Modern thinking suggests that antibiotics do not get into abscesses because there is no blood supply in an abscess; and that antibiotics do not go to the foot very well as the blood supply there is poor anyhow. Yet for decades, horse owners, farriers and veterinarians have prescribed oral antibiotics to treat foot abscesses.\nIdeally, your farrier should identify the location of the abscess, pare down the hoof so it can drain, and then you can apply a drawing salve to draw out the infection without affecting the entire hoof. There will be a hole in your horse’s hoof (whether made by the farrier, veterinarian, or nature when the abscess ruptures) and you want the infection out, not more dirt and bacteria in. Use short soaks or hosing to keep the area clean, fill the hole with a drawing salve, and keep the hoof clean and dry until the hoof wall heals.\nQ: My mare has begun peeing as soon as I take her saddle off after every ride. She never used to do this. Should I be worried about this change in behavior?\nA: This is definitely a reason to be concerned. Any behavior change is an indication of a problem. This particular behavior can suggest anything from pain to infection.\nThere are a few different things to check to figure out what’s going on. First, when you remove your saddle, is your mare’s back evenly wet everywhere the saddle was? Also, look at the line at the end of the saddle which often corresponds to the lower back, where the kidneys are. Is there sweat there also? Or is there a crease in the flesh? A dry spot, especially over the kidneys, suggests your saddle isn’t fitting right and is the cause of her problems.\nCheck the underside of your saddle and make sure the tree is solid and not broken. Check your saddle pads for wear, ensuring they are an even thickness all over. Even a saddle pad that breaks down and causes wear can make a difference, and create pain on your horse’s back.\nNext, have your veterinarian check a blood and urine sample to look for systemic infection, kidney disease, or bladder infection. Depending on the age of your mare, it is normal for kidneys to start to age along with the horse, but you shouldn’t notice changes in the bloodwork until 60% or 70% of the kidneys functioning abnormally. This tends to happen at a fairly advanced age.\nOnce your mare passes a standard veterinary exam, it’s time to check for chiropractic issues; is the location of the saddle on your horse’s back making her uncomfortable and “out of alignment”? Does your mare flinch when you touch her lower back? Are her muscles warm in that area? Veterinary spinal manipulation therapy (chiropractic) can restore comfort, motion and health to your mare, once all the other issues mentioned above have been addressed.\nQ: I have seen some equine shampoos labeled “pH balanced”. What is the pH of a horse’s skin, and would using these products actually make a difference to his well-being?\nA: There is very little regulation in the grooming product industry. In fact, the only products regulated by a government agency are those that contain insecticide (the EPA in this case). This means the quality of ingredients and the pH range of any shampoo can be quite broad.\nInterestingly, human shampoos tend to have more pH range than animal products. Most shampoos should be close to the neutral pH of 7.0 – the pH of normal, healthy skin. So a balanced shampoo will have a pH somewhere between 6.0 and 8.0. Human shampoos can range from a pH of 3.0 all the way to 10.0! (Smaller numbers are more acid, higher numbers are more alkaline.)\nSo, if a horse shampoo tells you it’s pH balanced, that’s a good first step in identifying a quality product. The next thing you would like your grooming products to do is list their ingredients. Sadly, there is no law that says they have to list ingredients; manufacturers will cite proprietary recipes as a reason for not doing so. However, if a horse breaks out in hives because of a new grooming product, you will want to know exactly what the ingredients are to find out what caused the allergic reaction. A shampoo manufacturer who will not share this information, citing proprietary discretion, is not helping the consumer.\nThe next thing to look for is a shampoo that contains more natural ingredients and fewer surfactants. Surfactants are the chemicals that make lots of bubbles and give the perception of extra cleanliness; SLS (sodium lauryl sulfate) — a petroleum derivative – is an example.\nScrutinizing your horse’s shampoo can actually be quite a science. Beginning with a balanced pH is a great start.\nQ: I have an elderly gelding with arthritis in both knees. He is fully retired, receives as much turnout as possible, and gets daily Previcox. He is generally quite a happy fellow – he enjoys his horse friends and is in good weight and health. I have, however, noticed some changes lately – he is slowing down, tends to look off in front, and doesn’t really lie down (or roll) anymore that we can tell. I am concerned about keeping him comfortable heading into the colder months. Is there anything else you would do for him? I know that we will soon likely have to make a decision about his quality of life, but I don’t think we are quite there yet.\nA: Arthritis is a complicated, and emotional, issue. As we age, we all have inflammation in our joints that makes us creak and pop as we walk, regardless of our species. Yet having arthritis also makes us think about quality-of-life issues, since it can become quite painful. Consequently, we often give nonsteroidal anti-inflammatories, such as Previcox.\nRegardless of species, daily use of anti-inflammatory medication becomes pro-inflammatory. As horse owners, we give anti-inflammatory medications because we want our horses to feel better. Sadly, long-term use of these medications can cause serious, and harmful, side effects. These side effects can range from worsening arthritis to intestinal bleeding and ulceration.\nAlternatively, approaches ranging from herbal medications to physical-type therapies can have fewer side effects. These therapies are based on the knowledge that movement inhibits pain, so the more your horse moves around the better. It can be as simple as walking on a lead line, or physical therapy.\nMedically, there are nice homeopathic and herbal remedies for the arthritic horse. Arnica Montana has been used for centuries to treat arthritic pain. Herbals such as Devil’s Claw and Boswellia can help. Herbal pharmacies make some great blends. Nutraceuticals like glucosamine and hyaluronic acid help arthritis patients.\nMassage, chiropractic, and acupuncture are fabulous physical modalities to help your horse feel better. Each of these treatments creates movement, which stimulates the body to naturally release its own beta-endorphins, reducing pain and restoring function to the elderly patient. There are even some newer therapies that can help the painful horse, including magnet blankets and electromagnetic fields therapy – called PEMF.\nSince your horse has been on Previcox for quite some time, it’s worth considering whether or not he has intestinal or stomach ulcers. There are some nice oral aloe blends that can be given over two or three months to help with such problems. And if you couple it with the natural treatments, your horse is going to feel better than he ever has before — simply by reducing the inflammation in his body.\nAnother thing to think about is that food is the greatest source of inflammation for horses. Rather than feed the standard pellet, look for a whole-grain feed, not one based on grain by-products and waste from the human food industry. Hay and flaxseed diets are wonderful sources of nutrition for the elderly horse. When needed, beet pulp can definitely help older horses pack on some weight in the winter."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:c809bd63-4407-4fbf-b44d-cebb8ac9d992>","<urn:uuid:38d176aa-7003-4581-9f42-d66bb325b5be>"],"error":null}
{"question":"Can both WAVE bioreactors and conventional stirred bioreactors handle CO2 accumulation effectively during cell cultivation?","answer":"WAVE bioreactors and conventional stirred bioreactors differ in their ability to handle CO2 accumulation. In stirred bioreactors, CO2 accumulation can become a serious problem due to low power input and aeration rates, which negatively impacts product quality, cell growth, and protein production. The WAVE Bioreactor system, through its wave-induced motion and unique design, provides effective aeration that can help prevent CO2 buildup. Both systems require careful management of gas exchange, but stirred bioreactors face particular challenges with CO2 accumulation, especially in large-scale operations, and require specific consideration of impeller and sparger positions to address this issue.","context":["Disposable bioreactors are increasingly used for cell cultivation because they improve process flexibility, shorten turnover times, and eliminate the requirement for cleaning and sterilization. This week at ESACT in Lille, France, GE Healthcare Life Sciences announced a new addition to the disposable bioreactor market; a next generation WAVE Bioreactor™, ReadyToProcess WAVE™ 25.\nAn early pioneer in the area of disposable bioreactors, the WAVE Bioreactor was the first single-use bioreactor system when it was presented to the bioprocessing community in 1996. Since then, the range of products has expanded to include larger scale systems and advanced optical sensor and control technologies. WAVE Bioreactor systems are today widely used in both research and manufacturing operations.\nWAVE Bioreactors have a design based on a noninvasive rocking technology that creates waves. The result is a simple but powerful instrument that meets all the conventional requirements for a bioreactor in terms of, for example, mixing and aeration. The rocking motion can be adjusted to provide conditions suitable for a variety of cell lines, such as robust CHO production cell lines with a high oxygen demand and more delicate primary cells and stem cells. WAVE Bioreactors also have a unique internal floating filter, which retains the cells in the bioreactor and filters only the media, eliminating the need for an external cell separator and flow loop.\nCell Culture Applications\nThe most popular applications include protein and virus production in smaller scale batch and fed-batch culture, and seed-train cultivations for scale-up to a larger bioreactor. For perfusion applications, the WAVE Bioreactor is quite flexible. For example, animal cell lines such as CHO, HEK, NS0, S2 and SF9 have been cultivated successfully in WAVE Bioreactor systems (1–7). In addition to the conventional applications, researchers are exploring novel ways of using the wave technology. I have included specific examples below of the innovative ways that the WAVE Bioreactor is being used in cell culture.\nIn traditional seed-train expansion, a series of fed batch cultures are gradually increased in volume until adequate cell number is reached for the final production bioreactor. Maximum cell concentration limits split ratios and as a result several bioreactors are typically required for expansion. By using WAVE Bioreactors in perfusion mode, 5-10 fold higher split ratios have been achieved when compared with traditional batch culture. Utilizing perfusion and the WAVE Bioreactor system in seed-train cultivation can enable a reduction in the number of expansion steps and the reduction of the number of bioreactors used. Disposable bioreactors with just a few liters of working volume have the potential to inoculate up to 1,000 liters of production culture.\nInsect Cell Cultivation\nInsect cell culture can be shear-sensitive and it requires a high supply of oxygen, higher than mammalian cells require. The WAVE Bioreactor has the capability of providing a high oxygen supply, which makes it a good choice in insect cell cultivation. In addition, the Cellbag permits both cell expansion for infection and the infection itself to occur inside the bioreactor, which limits challenging transfers. In one example, insect cells were seeded at a low volume (10 liters), then cells were grown to the appropriate cell density and culture can was increased (100 liters) to allow for infection. The virus seed can also be conveniently expanded in a bioreactor of the same type.\nPlant Cell Cultivation\nPlant cell culture is becoming more widely used in many areas including plant research, and in both naturally occurring and recombinant compound production. WAVE Bioreactors provide a good choice for plant cells because of the low-shear factor. Plant cells can be particularly sensitive to shear stress and shear that is too high can cause a reduction in cell viability, changed morphology, impaired growth, and more. In addition, the absence of an internal impeller or sparger in the WAVE Bioreactor eliminates problems of clogging or tangling in hairy root cell culture. Finally, many plant cell cultures are light-dependent and the open design of the WAVE Bioreactor enables easy access to light from a source placed nearby.\nThe New WAVE\nThe latest WAVE bioreactor, ReadyToProcess WAVE 25, is available for cultures from 300 mL to 25 L scale, and combines the ease-of use that comes with the rocking technology as such, with intelligent control and advanced sensor technology. A short video preview of the system was made available online June 24th. Registration to watch the video can be done by following this link https://promo.gelifesciences.com/gl/K12355/mail-june.html.\n- 1. Singh, V. Disposable bioreactor for cell culture using wave-induced agitation. Cytotechnology 30, 149–158 (1999).\n- 2. Namdev, P. K. and Lio, P. Assessing a disposable bioreactor for attachment dependent cell cultures. BioPharm Int. 13, 44–50 (2000).\n- 3. Tang Y. J. et al. Perfusion culture of hybridoma cells for hyperproduction of IgG(2a) monoclonal antibody in a wave bioreactor-perfusion culture system. Biotechnol. Prog. 23, 255–264 (2007).\n- 4. Birch, J. R, and Racher, A. J. Antibody production. Adv. Drug Deliv. Rev, 58, 671−685 (2006).\n- 5. Chartrain, M. and Chu, L. Development and production of commercial therapeutic monoclonal antibodies in mammalian cell expression systems: an overview of the current upstream technologies. Curr. Pharm. Biotechnol. 9, 447–467 (2008).\n- 6. Clincke M. et al. Very high density of CHO cells in perfusion by ATF or TFF in WAVE bioreactor™. Part I: Effect of the cell density on the process. Biotechnol Prog. 2013 Feb 22. DOI: 10.1002/btpr.1704.\n- 7. Clincke M. et al. Very high density of CHO cells in perfusion by ATF or TFF in WAVE bioreactor™. Part II: Applications for antibody production and cryopreservation. Biotechnol Prog. 2013 May 21. DOI: 10.1002/btpr.1703.","Mammalian cell culture bioprocesses are increasingly gaining importance in the biopharmaceutical industry. Due to the high demand of product quality, the incubated shaker as a production unit can become a limiting factor after a certain point. Learn more and watch the webinar at the end of this post.\nThe typical process cycle\nA specific product is usually produced by thawing the cells, cultivating these up to a certain amount of cell density in incubated shakers, followed by then transferring the process into a bioreactor. To increase the amount of product, the bioprocess is accordingly finally scaled-up to different unit sizes. However, such a sequence of steps is easier said than done. Many different factors, biological as well as physical parameters, need to be considered for a successful process transfer. Therefore, this post is aimed at shedding light on these.\nGet to know your bioprocess using real-time monitoring\nThe selection of cell culture cultivation units always depends on the application. Shake flasks are easy to handle, disposable, and allow quick preparation for performing bioprocesses. The shake flasks are often used for screening purposes or to understand the process in its initial stages. On the other hand, for a detailed understanding of the process, it is essential to gain insights into the bioprocess through real-time monitoring and control. Shakers allow this only to a limited extent, in which the information relevant to the process remains an unknown black box. This is because it can take up to hours to several days to analyse a sample and obtain the corresponding data.\nA tight real-time control is needed not only to identify the Critical Process Parameters (CPPs), which are important to be known to control the biology of the cells, but also to meet the regulation criteria regarding quality, efficacy and cost effectiveness (Asnaghi, Smith, Martin, & Wendt, 2014). To fulfil these monitoring requirements, it is crucial to have an automated bioreactor system which prioritizes real-time control, monitoring, data recording and management by employing a software that performs exactly those functions.\nBe aware of critical factors for process transfer\nA process transfer combined with a scale-up step from shake flask to bioreactor is not always easy and smooth. The environment for the cells changes totally, starting from the aeration to the cultivation method itself. The environment for the cells changes totally, starting from the aeration to the mixing method itself. The fact that the cells are not being shaken anymore and rather being stirred results in a change of mixing within the bioreactor. This dramatic change for the sensitive mammalian cells leads to shear stress and foaming issues, but nowadays these are quite well controlled due to the addition of antifoam and cell surfactants (Xing, Kenty, Li, & Lee, 2009).\nBesides the observations mentioned above, few more critical engineering parameters exist that need to be considered:\n1. Oxygen transfer:\nSince mammalian cell culture processes are aerobic, the oxygen transfer into the cells is a very critical factor. To ensure adequate oxygen availability, the agitation speed, gas flow rate and gas composition need to be controlled. As mammalian cells do not have a cell wall, the power inputs may need to be kept low, leading to inhomogeneities within the culture (Croughan, Hamel, & Wang, 1987).\n2. Culture media mixing:\nAnother important influential parameter is the mixing within the cell culture broth which needs to be considered. Hence, insufficient mixing results in pH and nutrient gradients, which has been shown to dramatically influence cell growth and antibody production (Bylund, Collet, Enfors, & Larsson, 1998; Osman, Birch, & Varley, 2001).\n3. CO2 accumulation:\nDue to the low power input and aeration rates, the CO2 concentration in the culture media is elevated, which negatively impacts product quality as well as cell growth and protein production (Gray, Chen, Howarth, Inlow, & Maiorella, 1996).\nChoose the right impeller design\nWithin every bioprocess lie a few challenges, but it is possible to overcome these. Fortunately, there is a common denominator underlying all the critical factors mentioned above – the impeller design. It helps to mitigate the negative effects by dispersing gas bubbles, leading to a better oxygen transfer and CO2 removal, which in turn leads to a more homogenous culture. However, at the same time, the impeller can play a large role in shear stress for the cells, putting the operator in a real dilemma.\nOne key aspect to be on the lookout for is the size of the eddies generated in the vicinity of impeller. The closer the eddy size to the size of the cells the more likely it is to observe cell damage. Optimized bioreactor designs therefore include an impeller that can operate at larger power inputs while maintaining a sufficiently large eddy size, effectively allowing for better gas transfer, mixing and CO2 removal without damaging the cell and expanding the operational window of the bioreactor.\nIt is a given that every modern cell culture bioreactor should provide a multi mass flow controller gassing system allowing for precise flow control and gas blending or a pH control loop that can utilize CO2 instead of a liquid acid. However, discussion of these features is not under the purview of this post.\nLearn from best practices\nEven though process transfer has been around in the scientific community for quite a while, the lack of expertise on the same is astounding. Consequently, a well-known phenomenon remains often underutilized or unperfected by the industry. Therefore, it can sometimes be confusing to those who already employ this technique and particularly challenging for those who are new to the field, wanting to transfer a process from shake flask to bioreactor. Notwithstanding your current level of ease with the process transfer, if you need guidance or are interested in finding out how others successfully proceed about it, have a look at the illustrative application of a characterised bioprocess transfer: “Process transfer of CHO cultivations using the Minifors 2 as an example”, by the Zurich University of Applied Sciences. The application note is available for download on the Minifors 2 product page.\nThe scaling-up recipe\nEven today, scale-up is an erratic procedure since bioprocesses are not properly monitored and understood in detail. Two very important process relevant parameters are agitation and aeration. On the one hand, agitation is set to achieve homogenous mixing and oxygen transfer into the cells. Aeration, on the other hand, is regulated to supply the cells with sufficient amount of oxygen as well as to eliminate excess amount of CO2. Especially in large-scale bioreactors, excess CO2 concentration can pose a serious problem. Therefore, it is important to consider the aeration and agitation rates, position of the impeller and sparger, which will affect bubble size (Gray et al., 1996). The specific engineering parameters which are needed to successfully execute the scale-up process are discussed below at length (derived from DECHEMA, Society of Chemical Engineering and Biotechnology).\n1. kLa – The volumetric mass transfer coefficient\nThe kLa coefficient denotes the efficiency of gas transfer from the gas phase into the liquid phase. It is dependent upon the design and operating conditions of the bioreactor. Moreover, it is also influenced by the impeller design, aeration and agitation rates. The higher the kLa value, the greater the aeration performance of the cultivation system. Several methods exist to determine this coefficient with a corresponding dissolved oxygen probe. Some of the most prominent approaches are as follows.\na) Dynamic gassing out technique\nIn the initial step, the bioreactor system is in a stationary state, which is disrupted by a sudden stop of oxygen supply. This causes a decrease in dissolved oxygen concentration through its consumption by the cells. After the oxygen is consumed, the supply is switched on again and the gas returns to its original .\nb) Static gassing out technique\nIn this technique, the oxygen is gassed out of the liquid after addition of nitrogen. Afterwards, the liquid is stirred and aerated again, and the increase of dissolved oxygen is monitored.\nc) Sulphite technique\nThis method is based on the reaction of sodium sulphite with oxygen, where the sulphite is oxidised with the help of metal ions to sodium sulphate. The consumption of oxygen then determined.\nFor the determination of the kLa coefficient (h-1), besides the oxygen transfer rate, OTR (mg O2*L-1*h-1), the equilibrium oxygen concentration, cO2* (mg/L) and the dissolved oxygen concentration cO2 (mg/L) need to be known.\n2. Power input\nThis engineering parameter is linked with hydrodynamic stress, which influences the growth of the cells and their productivity, particularly those of the more shear sensitive organisms. The most prominent technique to determine the power input is the torque-based measurement. This method is very easy to apply and is therefore the most frequently used. By using a torque sensor, the torque is measured on the stirrer shaft which then allows to determine power input. It must be pointed out that it is important to provide a vibration-free environment measurement. The formula below shows how the power input, P (W) can be calculated by determining the rotational agitator speed, N (min-1) and the torque in the liquid (N*m), as well as the torque in the empty vessel (N*m).\nAsnaghi, M., Smith, T., Martin, I., & Wendt, D. (2014). Bioreactors. https://doi.org/10.1016/B978-0-12-420145-3.00012-2\nBylund, F., Collet, E., Enfors, S. O., & Larsson, G. (1998). Substrate gradient formation in the large-scale bioreactor lowers cell yield and increases by-product formation. Bioprocess Engineering, 18(3), 171–180. https://doi.org/10.1007/s004490050427\nCroughan, M. S., Hamel, J. F., & Wang, D. I. C. (1987). Hydrodynamic effects on animal cells grown in microcarrier cultures. Biotechnology and Bioengineering, 95(2), 295–305. https://doi.org/10.1002/bit.21158\nGray, D. R., Chen, S., Howarth, W., Inlow, D., & Maiorella, B. L. (1996). CO2 in large-scale and high-density CHO cell perfusion culture. Cytotechnology, 22(1–3), 65–78. https://doi.org/10.1007/BF00353925\nOsman, J. J., Birch, J., & Varley, J. (2001). The response of GS-NS0 myeloma cells to pH shifts and pH perturbations. Biotechnology and Bioengineering, 75(1), 63–73. https://doi.org/10.1002/bit.1165\nXing, Z., Kenty, B. M., Li, Z. J., & Lee, S. S. (2009). Scale-up analysis for a CHO cell culture process in large-scale bioreactors. Biotechnology and Bioengineering, 103(4), 733–746. https://doi.org/10.1002/bit.22287"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:f7a43706-e1e8-40bf-8d2e-3d310bf78a0c>","<urn:uuid:69941125-883b-4118-bc21-4b4fd3bfe344>"],"error":null}
{"question":"What are the key differences between using home remedies versus medical treatments for red eye conditions, and when is each approach appropriate?","answer":"Medical treatments and home remedies serve different purposes for red eye conditions. Medical treatments are necessary for specific conditions - antibiotics for bacterial conjunctivitis, antihistamines for allergic reactions, and artificial tears for viral cases. These should be used after proper diagnosis by a doctor, especially if symptoms persist or worsen. Home remedies, on the other hand, are suitable for non-serious cases and include applying cold compresses, using chamomile tea compresses, cucumber slices, rosewater, or potato applications for reducing swelling. However, if symptoms don't improve with home remedies or are accompanied by pain, swelling, or burning, immediate medical attention is necessary.","context":["It is likely that all or many of us have experienced red eyes more than once in our lifetime. This irritation is due to inflammation and dilation of blood vessels in the sclera (the white surface of the eye).\nOther symptoms that may accompany red eyes are itching, eye discharge, swollen eyes and visual disturbances, such as blurring.\nWhy do they appear?\nRed eyes can appear due to various causes, some of the most common are allergies, eye strain, overuse of contact lenses, and eye infections.\nHowever, the redness of the eye can sometimes indicate a more serious condition, such as glaucoma. If red eyes persist or worsen, it is important to see a specialist for proper diagnosis and treatment.\nCommon Causes of Red Eyes\nConjunctivitis: One of the most common infections of the eyes, and is characterized by so-called \"red eye\". It is highly contagious and occurs when the conjunctiva is infected, so that the blood vessels become irritated and swell giving an appearance of red or pink eye. It commonly occurs in children of school age.\nDry eyes: Dry eye syndrome occurs when the tear glands produce an insufficient amount of tears or tears of a quality which does not adequately lubricate the eye. Chronic dry eye can cause the surface of the eye to become inflamed and irritated. It can be treated by using lubricating eye drops, but it is important to seek expert advice.\nAllergies: The reddening of eyes is often associated with allergies due to the release of histamine in the body, which is produced during an allergic reaction and can cause blood vessels in the eyes to dilate, causing them to become red and watery. If you are prone to allergies, it is important to avoid contact with agents that can trigger a reaction of this type.\nContact Lenses: One of the main causes for red and irritated eyes are contact lenses. Poor care, and wear caused by contact lenses can cause a buildup of irritants. If you have red eyes due to the use of contact lenses, this can indicate an infection - it is important to stop using them immediately and consult your doctor. The persistent use of contact lenses can also worsen red eyes if already present, because this reduces the amount of oxygen reaching the cornea, especially if they are not fitted properly. It is advisable not to use this type of lens if you are someone who suffers from red eyes.\nComputer vision syndrome: Spending long hours at the computer without a break may be cause red eye accompanied by a burning sensation and fatigue. Usually this is because you blink less when working at the computer screen and as a result the surface of the eye becomes dry. To reduce visual fatigue, it is advised to take regular breaks while working at the computer, and also the use of special glasses. Artificial tears can help control dry eyes due to computer vision syndrome.\nEye injuries: After a trauma or eyelid surgery, the eye may be red as a result. Eye injuries can range from minor scratches to deep puncture wounds or burns from a chemical agent. Whatever the cause, it should always be treated as an emergency and therefore it is important to consult your eye doctor immediately.\nOther causes of red eye include pregnancy, lack of sleep, swimming, smoking, exposure to environmental pollution and irritants. Red eyes also may also be due to diseases such as glaucoma, uveitis, etc.\nRecommendations to avoid red eyes\nGet enough sleep: A good rest prevents redeye, and other discomforts that may be associated with the eyes.\nDo not rub your eyes: Rubbing your eyes with dirty hands can cause irritation due to pollutants that are carried on the hands and fingers, along with the risk of injuring yourself.\nKeep your contact lenses clean: If you use this type of lens, a good standard of lens hygiene is very important, otherwise the lens may be contaminated which could lead to an infection.\nRest of the Monitor: Try to rest your eyes every 20 minutes if you are working in front of the monitor for at least 20 seconds.\nControl Allergies: Keep allergies at bay by avoiding contact with allergens and ask your doctor about your options for addressing them.\nCheck your eyes: If you notice recurring redness in the eyes, schedule an appointment with a specialist to rule out any associated diseases.\nDo not abuse the use of whitening eye drops: While they may be useful in the beginning, their continuous and prolonged use can aggravate red eyes.\nNatural remedies for red eyes\nTreatment for red eyes may vary according to the cause, so a diagnosis made by a professional is advisable.\nSome home remedies are useful, when the cause of red eye is not serious.\nApply cold compresses\nSplash ice cold water on the face and eyes, then wrap an ice cube in a clean cotton towel and place on eyelids closed without pressure. This helps relieve irritated eyes and eyelids.\nPrepare chamomile tea with a teaspoon of chamomile flowers in a cup of boiling water, let it stand, strain and then wait for it to cool. The infusion used to wash your eyes also can be used in compresses.\nYou can apply a gauze or cotton balls soaked in Hammamelis Water (Witchhazel) for swollen eyes.\nA very common and simple remedy is fresh cucumber slices. Apply them on your closed eyelids for 10 minutes. Cucumber properties make it ideal for refreshing and alleviating inflammation associated with red eyes.\nRosewater provides relief to irritated and itchy eyes. To apply, simply moisten a cotton ball in rose water and rest it on the closed eye (this may cause mild discomfort) for about 5 minutes.\nThe potato has astringent properties, which helps to reduce swollen eyes. To make use of it, you just have to grate a little potato and place on the closed eye, keeping it there for roughly 15 minutes (this may cause mild discomfort).\nIf after implementing these remedies your red eyes do not improve or get worse, do not forget to consult a specialist.\nFurthermore, if your red eyes are accompanied by other symptoms such as pain, swelling of eyelids or burning, make it a priority to see a specialist as soon as possible\nThe best of Health\nProducts related with Red Eyes: Causes, advice and Natural Remedies\nRelated categories of the Shop\nYou have not found what you were looking for?","©iStockphoto.com Image by: ©iStockphoto.com\nConjunctivitis (pink eye) is the inflammation of the conjunctiva, the thin membrane that covers the white of the eye. When it is irritated, the blood vessels of the eye become enlarged and inflamed. If your child's eye is pink or has redness, there's a good chance he has conjunctivitis, says Dr. Michael Dickinson, spokesperson for the Canadian Paediatric Society and head of pediatrics at Miramichi Regional Hospital in Miramichi, N.B. Other symptoms include a goopy or watery discharge, slightly swollen eyelids and complaints of a sandy or scratchy feeling in the eye (accompanied by lots of eye rubbing).\n3 causes of pink eye\nBacterial: Occurs when the eye is exposed to a bacterial organism. \"As a general rule, bacterial conjunctivitis tends to have a lot of pus or discharge that comes out of the eye,\" says Dr. Dickinson. You may notice crusting of the eyelids. It is highly infectious.\nViral: Often the result of being exposed to a person with a cold. It is highly infectious. It tends to have a watery discharge that can last from one to two weeks.\nAllergic: Can be caused by environmental factors such as smoke or other irritants (pollen, trees, etc.). Symptoms are itchy, watery eyes. Dr. Dickinson notes that it can be difficult to tell the difference between viral and allergic pink eye, but you can make an educated guess based the time of year. \"If you have a child who's known to have allergies and it's allergy season, and they have pink, watery eyes, it is likely allergic. If it's a child in day care and it's the winter months, then that's probably more likely to be viral.\"\nTreating pink eye\nTalk to your doctor if you suspect your child has pink eye. They will tell you what course of treatment is needed. Bacterial pink eye is treated with antibiotic eyedrops or ointment. Allergic pink eye can be treated with antihistamine eyedrops or children's oral antihistamines. Viral pink eye doesn't require much, if any, treatment, notes Dr. Dickinson. It just has to run its course. You can, however, use artificial tears to flush and soothe the eyes.\nNaturopathic treatment can also be helpful in mild pink eye cases, says Angela Lee, a naturopathic doctor and owner of VitalChecks, a naturopathic clinic in Richmond Hill, Ont. \"Warm herbal compresses with antibacterial properties such as goldenseal and echinacea are an effective option for bacterial pink eye,\" she says. (Echinacea should be avoided if the child has a ragweed allergy.) For viral pink eye, Lee recommends a warm compress.\nTo prevent the spread of infection, be sure to use separate compresses for each eye. If your child is suffering from allergic pink eye, Lee suggests cool compresses to help soothe itchy eyes. \"Cool chamomile tea bags are great for allergic pink eye. The tannins found the in the tea help to reduce itch and inflammation,\" she says.\nContact your doctor if you do not see any improvement of pink eye symptoms within 24 hours, says Dr. Dickinson. \"There might be other things going on with the eye, like a scratch in the cornea or a foreign body in the eye.\" Contact your doctor if your child still seems unwell or is running a fever.\nHow to prevent the spread of pink eye\nInfectious pink eye (not allergic) is very contagious, and can spread quite easily in day cares, kindergarten classrooms, summer camps and homes. It easily spreads through hand-to-hand contact. \"A sick kid rubs her eye, gets the secretion on her hands and then goes and touches a friend or a brother or a parent,\" says Dr. Dickinson.\nHelp prevent the spread by washing hands frequently, using hand sanitizer and avoiding touching your eyes—which is \"easy to say, not so easy to do,\" says Dr. Dickinson. Control the spread by changing pillowcases often and not sharing washcloths.\nIf your child has pink eye, it is best to keep him home until he has been diagnosed. Children with viral or allergic pink eye can return to school after they have seen a doctor. Those with bacterial pink eye can return after 24 hours of antibiotic treatment, says Dr. Dickinson. Be sure to check with your school or childcare provider about rules regarding attendance after a pink eye diagnosis.\nFrom how often you should see your eye doctor to what questions to ask, here is what you need to know to keep you and your family's eyes healthy."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:376f823c-c7dd-467f-8273-73c499be50b3>","<urn:uuid:a164a8e7-f27a-4c48-8fe3-542e6a92f659>"],"error":null}
{"question":"I'm interested in religious customs - what are the main ritual differences between Vrishabha Sankranti and the second day of Pongal regarding sun worship?","answer":"During Vrishabha Sankranti, devotees perform Sankramana Snan (holy bath) to pay homage to the Sun God and conduct Pitra Tarpan for ancestors. They also visit Lord Vishnu temples and observe fasts. In contrast, on the second day of Pongal, people specifically boil rice in earthen pots with turmeric plants tied to them as an offering to the Sun God. They also include sugarcane sticks, fruits, and flowers in the ritual, perform aarti, and pray while wearing traditional dress. Both festivals involve sun worship but have distinct ceremonial practices.","context":["In the Hindu Solar Calendar, the festival of Vrishabha Sankranti marks the onset of the second month. During this the planetary shift of Sun to the Vrishabha rashi from Mesha rashi, takes place. This transit of Sun corresponds to the movement from Aries Zodiac sign to Taurus Zodiac sign. Vrushabha Sankranti occurs during the month of ‘Vaishakh’ in Marathi, Kannada, Gujarati and Telugu calendars and in the North Indian calendar, it is observed during the Hindu month of ‘Jyeshta'. Vrishabha Sankranti is also famous as Vrushabha Sankraman in the Southern states of India and signifies the beginning of the Vrishabha season as per the solar calendar. It also marks the arrival of Vaigasi Masam in Tamil calendar, ‘Edavam masam’ in Malayalam calendar and ‘Jyeshto mash’ in the Bengali calendar. In the state of Orissa, this day is celebrated as ‘Brusha Sankranti’.\nThe word ‘vrushabha’ in Sanskrit implies ‘a bull’. Also in Hindu religion, ‘Nandi’, Lord Shiva’s carrier is thought to be a bull and the religious scriptures show some form of relation between these two. Therefore the celebration of Vrishabha Sankranti has immense religious significance for Hindu devotees. People worship Lord Vishnu on this propitious day to be blessed with a happy and prosperous life. They also plead the lord to get freedom from the continuous cycle of re-birth and attain salvation.\nVrishabha Sankranti 2023 is on May 15 Monday\n8 months and 27 days to go for the event\nRituals on Vrishabha Sankranti:\n- On the auspicious day of Vrishabha Sankranti, Hindu devotees do Daan as it is considered very fortunate. ‘Godan’, a practice of donating holy cow to a respected Brahmin when done on Vrishabha Sankranti is believed to be very auspicious.\n- Some devotees also observe a fast on this day, known as ‘Vrishabha Sankranti vrat’. They get up before sunrise and take a sanctified bath. The devotees worship ‘Rishabharudar’, a name of Lord Shiva and prepare a special ‘Bhog’ from payasham and rice. After praying to Lord Shiva, the bhog is distributed and eaten together with other family members. The observer of the Vrishabha Sankranti vrat should sleep on the floor during the night.\n- Devotees visit temples of Lord Vishnu temples on Vrishabha Sankranti and pray to their lord to seek wisdom to be able to differentiate between good and evil. Special arrangements are made for this day in the Lord Jagannath Temple, located in Puri.\n- Hindu pilgrimages remain crowded on this day as the devotees undertake the Sankramana Snan on this day. By performing this pious snan they pay homage to Sun God and also their forefathers. On Vrishabha Sankranti people also perform Pitra Tarpan to offer peace to their dead ancestors.\nImportant Timings On Vrishabha Sankranti\n|Sunrise||May 15, 2023 5:49 AM|\n|Sunset||May 15, 2023 6:56 PM|\n|Punya Kaal Muhurta||May 15, 5:49 AM - May 15, 11:49 AM|\n|Maha Punya Kaal Muhurta||May 15, 11:25 AM - May 15, 11:49 AM|\n|Sankranti Moment||May 15, 2023 11:49 AM|\n|Place : Ujjain [ India ] See More|\nSignificance of Vrishabha Sankranti:\nIn the Indian Vedic Astrology, Sankranti is personified of being approximately 432 Km long and wide. This personification, in accordance with Indian belief is considered to be inauspicious and therefore the Sankranti window is believed to be unsuitable for performing auspicious activities. Yet the duration of Sankranti is highly momentous for penance, charity and shradh rituals. Hindu devotees take a dip in the holy rivers, perform Shradh rituals for their forefathers and offer some form of charity to the needy.\nJust like other 12 Sankrantis in the Hindu calendar, the day of Vrishabha Sankranti is also favourable for performing Daan or Punya activities. However, only specific time just prior to or after the Sankranti moment is considered most favourable for observing all Sankranti related activities. On Vrishabha Sankranti, the duration between 16 Ghati (with 1 day equals 60 Ghatis) prior to Sankranti till the actual Sankranti moment is the best time for carrying out Daan or any good activities.\nVrishabha Sankranti festival dates between 2019 & 2029\n|2019||Wednesday, 15th of May|\n|2020||Thursday, 14th of May|\n|2021||Friday, 14th of May|\n|2022||Sunday, 15th of May|\n|2023||Monday, 15th of May|\n|2024||Tuesday, 14th of May|\n|2025||Thursday, 15th of May|\n|2026||Friday, 15th of May|\n|2027||Saturday, 15th of May|\n|2028||Sunday, 14th of May|\n|2029||Tuesday, 15th of May|","“PONGAL” – A Hindu festival that marks the beginning of UTTARAYAN. Uttarayan is the sun’s movement northward for a period of six months. All auspicious festivals are held during this period. Pongal is a four day long harvest festival that is celebrated with grandeur in Tamil Nadu.”THAI PONGAL” is celebrated with a sense of thanksgiving to nature which typically falls in the month of January. Pongal is celebrated from January 13th to 16th with all four days showcasing the customs and traditions of Tamil Nadu.\nThere is a famous Tamil saying “THAI PIRANDAL VAZHI PIRAKUM” which means the month of Thai brings hope and puts an end to all hardships in life. Pongal marks the beginning of goodwill in times to come. It spreads a feeling of positivity all around, where people rejoice with heads bent down in gratitude. Pongal symbolizes the end of farming season. It signifies a break given to farmers toiling on land with their monotonous routine. People offer prayers in temples on this day. They give away vegetables, sugarcanes and spices as a ritual. A series of festivals are triggered off, as per the Hindu calendar, after the commencement of Pongal.\nFirst day is celebrated as ‘BOGI’ in honour of Lord Indra, Supreme God of Rains. Rain brings prosperity to land and helps yield bountiful harvest. First and foremost paying homage to the rain God is of traditional importance. On this day, an auspicious ritual is held wherein old household articles and clothes are thrown into fire. It symbolizes end of past and welcoming the new with high spirits.\nSecond day is ‘PONGAL’. Rice is boiled in earthen pots with turmeric plants tied to it which is offered to the Sun God. Sugarcane sticks, fruits, flowers are a part of the ritual. Aarti is performed after offering prayers. Wearing traditional dress men and women pray for family prosperity and well being. ‘Kolam’- colorful designs drawn on floor with white lime powder adds to the charm of PONGAL.\n‘MATTU PONGAL’ marks the third day of celebration wherein we offer our respect to the cattle. On this day, cows are given a fresh look with their horns decorated in vibrant hues. According to a Hindu legend, Lord Shiva once asked his bull ‘BASAVA’ to tell mortals on earth to take bath daily and eat only once a month. Basava wrongly passed on a message to eat daily and bath only once a month. This angered Lord Shiva who banished Basava to live on earth, plough fields and produce food. Thus cattles came to be associated with this day.\n‘KAANUM PONGAL’ is observed as the fourth day. Remains of sweet pongal (a dish made from jaggery and boiled rice) and other food items are placed on a turmeric leaf. Prayers are offered while performing this ritual. Women whole heartedly pray to God for conferring peace upon their family. ’PONGAL PODI’ or pongal gifts are given to dear ones as a sign of affection.\nAnother celebration associated with Pongal is ‘JALLIKATTU’ or taming bulls. This is a show of power for men who tame wild angry bulls by holding on to its hump. Though it is fun, unforeseen dangers are pretty much neglected by people.\nWorld is witnessing rapid changes in civilization. Upholding customs and traditions that our forefathers believed in, helps us to sustain our identity. Pongal even today is celebrated with immense happiness all over Tamil Nadu. No matter what, the glitter of this festival will live on and will definitely be carried over to the next generation . PONGAL will always be known as a festival that gives bliss to people of all ages."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:f9568dc4-51bc-4b04-a862-3419c0c6dc50>","<urn:uuid:ef6f30a9-74a5-494c-a8f7-e1306ab339c0>"],"error":null}
{"question":"How did Tsar Samuil of Bulgaria die in 1014, and what events led to his death?","answer":"Tsar Samuil died of a heart attack after witnessing a horrific sight: Emperor Basil II had ordered 15,000 captured Bulgarian warriors to be blinded and sent back to Samuil. This cruel act came after Basil II's defeat at the fortress of Strumitsa by Gavrail Radomir's regiment, which had prompted the Byzantine emperor's vengeful response.","context":["Epic endeavours for indipendence\n968 - 1018\nAt long last, approaching danger sobered up some of the political circles in the Bulgarian capital. In 965 AD agreements of alliance with the Hungarians and the German emperor Otho I were concluded. These put an end to the country's foreign political immobilism and self-isolation from active international life.\nCrisis in the Bulgarian state was gaining momentum and this, by some tragic coincidence, concurred with a continued period of stabilization for the Byzantine empire. By the end of the 60s it had beaten off the Arab aggression and was able to converge all its might against Bulgaria. In the beginning of 966 AD emperor Nicephorus II Phocas undertook a campaign against Bulgaria, but the imperial troops refused to cross the border for, the memory of the Bulgarian landslide victories in the past was still fresh. This,though, did not make Byzantium give up its plan of military confrontation, but this time it had decided to make a cat's paw of other forces. In 968 AD Svyatoslav, the prince of Kiev, was hired for enormous sums of money to raid the northeastern Bulgarian lands with an army 60 000-strong. At the cost of great effort and losses he cut it really fine to change the course of the battle in favor of the Russians and, eventually, routed the 30 000-strong Bulgarian troops, occupied the castle of Preslavets (Little Preslav) and decided to found his own state in the newly seized north Bulgarian lands.\nScared by the loss of the northern territories, the Bulgarian palace aristocracy overthrew incapacitated tsar Peter, sent him to a monastery and gave the throne to his son Boris II. Possessing none of his great grandfather's makings the new Bulgarian tsar failed to lean on and to organize the powerful potential of the Bulgarian people in the struggle against the Russian aggression, and entered into an alliance with Bulgaria's sworn enemy, Byzantium, instead. The latter did not naturally send him any reinforcements during the subsequent Russian aggression in 969 AD. The Russians again, conquered and besieged the capital city of Great Preslav. Instead of continuing the war with the Russians (three quarters of the Bulgarian territory were still free with all military potential intact), Boris II concluded an anti-Byzantine treaty with Svyatoslav and made him a commander-in-chief of the joint Russo-Bulgarian troops. The power of Boris II was formal - the uneducated Russian prince had the whole of the country in his full disposition.\nIn the summer of 970 AD Svyatoslav got into the saddle and, at the head of a huge army of Russians, Bulgarians, Pechenegs and Hungarians invaded Byzantium. It was his dream to found upon the ruins of Bulgaria and Byzantium an enormous barbarian state, stretching from Kiev to Constantinople. The military commanding abilities of the barbarian were not consistent with his ambitions. The united troops were beaten by the Byzantines who, in 971 AD took the offensive and, after fierce fighting, seized the Bulgarian capital of Preslav. Svyatoslav was driven out of the Balkans. On his way back to Kiev he was ambushed and slain by the Pechenegs.\nThe prince's death coincided with the end of the independence of Bulgaria, at least in terms of the medieval practices. The capital was in Byzantine hands and the tsar captured and stripped of the insignia of royalty at an official ceremony in Constantinople. Exhausted by the battles, the Byzantine troops returned to their capital without formally establishing the emperor's power in the western lands of Bulgaria. These were expected, without hindrance, to be annexed to and ruled by the sceptre of Rome Reborn.\nThe district governors in western Bulgaria, however, refused to submit to Constantinople. Samuil, the governor of Sredets (modern Sofia) raised the standard of revolt against Byzantium. An efficient leader and a superb commander Samuil struck heavy blows on the Byzantine troops and was successful in freeing in 976 AD the occupied territories. Byzantium, as could be expected, was irreconcilable. A cruel war of attrition, a war to the knife, broke out and neither of the belligerents was ready to succumb.\nIn 978 AD tsar Boris II somehow managed to escape from captivity. With his brother Romanus he made his way to the Bulgarian border but was accidentally shot dead by a Bulgarian sentry. Romanus could not ascend to the throne as he had been castrated by the Byzantines and thus, doomed to leave no issue. This and other accidents left Samuil unravelled contender for the throne and he became tsar of the new Bulgarian empire (978-1014).\nIn 986 AD the Byzantine emperor Basil II undertook a build-up campaign against Sredets with all Byzantine armies converging on it. The chief Bulgarian troops were decoyed far into the south, in the vicinity of Thessalonica. However, Sredets stood a several-week state of siege. At the news of the Bulgarian troops approaching Sredets (Samuil had already brought his troops back from Thessalonica at the price of unbelievably tough daily marches), Basil II made haste on a return march. On 17 August 986 AD he encountered Samuil at the Traianus Gateway on the trans-European route to Asia. There, on that day, the Bulgarian army gained one of its most brilliant victories in all history. The Byzantine troops suffered utter defeat. Escorted by a small contingent the emperor had a miraculously narrow escape through a passage left unprotected for reasons unknown.\nThereafter, until the beginning of the second millennium AD, the Bulgarians had been unravelled masters of the Balkans. The Bulgarian armies struck severe blows on Byzantium in Thrace, Beotia, Thessaly, Attica and the Peloponnese. Byzantium's allies, the Serbian principalities, were swept away and so were the Hungarians. Tsar Simeon the Great's once advanced main policy of no compromise against Byzantium seemed to have come to life again.\nDuring the first years of the second millennium AD Byzantium restored the balance of power. The Bulgarian blow was followed by severe counter-blows. The alliance with Hungary ensured for Byzantium the division of the Bulgarian territory into two parts bridged by no obvious connection. The eastern part was soon subjected to Byzantine rule. Even so, the fighting in the western Bulgarian territories continued.\nThis situation drew to an end in 1014. In a battle near the village of Klyuch, Basil II captured the Bulgarian army 15 000- strong.spurred inadvertently his corps d'elite through to the important fortress of Strumitsa, he was defeated at its walls by the regiment of Gavrail Radomir, heir to the Bulgarian throne. Forced to withdraw and thus, venomed to the utmost limit, Basil II ordered for the 15 000 Bulgarian warriors taken prisoner after the previous battle, to be blinded and sent back to Samuil. At the terrible sight of his blind warriors' procession the Bulgarian tsar had a heart attack and died, winning a moral victory over his ruthless foe.\nTsar Samuil's death marked the beginning of the end. Feuds started flaring up in the Bulgarian aristocracy circles. The new Bulgarian tsar Gavrail Radomir (1014-1015) was murdered by his cousin Ivan Vladislav (1015-1018) who genuinely made serious efforts to save the country. At that time, though, geographically the strength of Bulgaria lay in its only surviving territory - the region of Macedonia. In a recklessly desperate attempt to keep it and, obviously wanting to leave a memorable legacy to posterity, the Bulgarian tsar threw himself to the front lines. He perished in a fierce man-to-man fighting for the Adriatic town of Dyrrachium. In the spring of 1018, the Byzantine troops made a ceremonial entry into the then Bulgarian capital, Ohrida. Some Bulgarian forts did not give up resistance up till the winter of 1019.\nThe death duellum of independent Bulgaria, which had gone on for nearly half a century, was brought to an end. The two sides involved in it, Bulgaria and Byzantium, overtaxed their potentialities to the utmost limit. Strongly impressed leading French medievalist Leon Gustave Schlumberger called it 'Byzantine epic'. Other European historians had no lesser reasons to call it 'Bulgarian epic'. For, in that fight Bulgaria and the Bulgarian people defended their state independence, not some abstract idea for a world-embracing empire.\n|We've tried to make the information on this web site as accurate as possible, but it is provided 'as is' and we accept no responsibility for any loss, injury or inconvenience sustained by anyone resulting from this information. You should verify critical information (like visas, health and safety, customs and transportation) with the relevant authorities before you travel.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:70d1a5eb-3ca0-4f6f-8a02-218b6e61f14c>"],"error":null}
{"question":"What are the key requirements for using deadly force in self-defense under Stand Your Ground laws, and how does this differ from the traditional duty to retreat?","answer":"Under Stand Your Ground laws, a person must reasonably believe that deadly force is necessary to prevent imminent death or serious bodily harm, or to prevent a forcible felony. The key difference is that Stand Your Ground laws remove the traditional 'duty to retreat' requirement, which historically required people to attempt escape if safely possible before using deadly force. However, Stand Your Ground protection only applies if the person has a legal right to be in that location, did not provoke the conflict, and is not engaged in criminal activity at the time. This differs from the traditional common law approach where people had to demonstrate they could not safely retreat before using deadly force, except when in their home under the 'castle doctrine'.","context":["The Castle Doctrine - Standing your ground in Texas\n(1) force is necessary to prevent or terminate another's trespass on land or unlawful interference with the property, or\n(2) deadly force is reasonably necessary to prevent another who is committing arson, burglary, robbery, aggravated robbery, theft or criminal mischief at night, or immediately fleeing with property after committing burglary, robbery, aggravated robbery, or theft during the nighttime from escaping with the property, and\n(3) the person reasonably believes that the property cannot be protected or recovered by any other method, or that the use of non-deadly force to recover the property would expose them to a substantial risk of death or serious bodily injury, then you will be legally justified in using deadly force to protect and/or recover your stolen property.While you may be legally allowed to use force under these conditions, we want to stress that using deadly force is most likely a very bad idea!\nThe crime of criminal trespass is not one of those listed in Texas Penal Code §9.42 or even under the \"Castle Doctrine\" statutes §9.31 and §9.32. A mere criminal trespass may, however, evolve into one of the above crimes where you may be justified in using deadly force to protect your property. For example, suppose that someone decides to sit on your lawn, and you yell at them to get them off your property. If the trespasser refuses to leave, you are almost certainly not justified in using deadly force to remove him. But if that same person sitting on your lawn gets up and charges towards your bedroom window with a firearm and a crow bar, you will very likely be legally justified in using deadly force to protect yourself and your home. His actions of charging you with a weapon make him more than just a trespasser under Texas law.\nCriminal Prosecution Even If You Were Justified.\nJust because Texas law affords you a legal justification for using deadly force when someone attacks you, or enters or removes you from your occupied habitation, vehicle, or workplace, this does not mean you are immune from being arrested or criminally prosecuted, even if you are completely in-the-right as far as the law is concerned. Your right to assert legal justifications is just that: a legal justification. It is not a “Get Out of Jail Free” card. In fact, always remember that there is a high possibility that you will have to go to jail and post bond long before the issue of justification is considered by the prosecutor. We see cases like this unfolding in Texas and other states on a regular basis. You may ultimately have to go to court and assert your justification defense before a judge or jury. This process may take months or even years to get resolved!\nDoes Texas Have a Stand Your Ground Law?\nThe phrase \"Stand Your Ground,” despite its common use in the media, is not found in Texas statutory law. Under certain circumstances, Texas law tells us that there is no duty to retreat if you are faced with a situation where you have to use force or deadly force to protect yourself or another. Even if by retreating you could avoid the entire confrontation, you do not legally have to. Texas Penal Code §9.31(e) and §9.32(c) state that in defending yourself or another person, you have no duty to retreat if: (1) you have a legal right to be at the location where force or deadly force is used, (2) you do not provoke the person against whom force or deadly force was used, (3) and you are not engaged in criminal activity at the time force or deadly force was used. These statutes are better considered \"No Duty to Retreat\" laws. Under these very limited circumstances, a prosecutor or law enforcement officer cannot argue that you had a reasonable \"escape route,\" or that you should have had to \"fall back\" before justifiably using force or deadly force. If you are facing a criminal charge, qualifying under this statute could mean the difference between a conviction or not!\nIn order to receive the \"No Duty to Retreat\" protection under the statutes, first, you must be justified under the Texas Penal Code in using force or deadly force. As we discussed above, Texas Penal Code §9.31 and §9.32 state that you will be presumed to be legally justified in using force or deadly force if someone is entering, attempting to enter, removing you or attempting to remove you from your occupied habitation, vehicle, or workplace. You will also be presumed to be justified in using force or deadly force if someone commits or attempts to commit: aggravated kidnapping, murder, sexual assault, aggravated sexual assault, robbery, or aggravated robbery. Force or deadly force can be used to stop any of these crimes, as well as when it may be immediately necessary to protect yourself or another person from the attacker's use of deadly force. If you are at a place you have a legal right to be, only then does the use of force or deadly force with no duty to retreat apply under the statute. To paraphrase a very effective jury argument, the statutes are designed to protect you when \"trouble finds you, but not when you go looking for trouble.\"\nDisqualifications for No Retreat Protection\nThere are multiple situations where your conduct may potentially disqualify you from the Texas \"No Duty to Retreat\" provision. In order to receive Texas Penal Code §9.31(e) and §9.32(c) \"No Duty to Retreat\" protection, you must first be justified in using force under Texas Penal Code §9.31. Second, the No Retreat statute themselves have three more qualifications that must be met before you gain the statutes’ protection.\nDisqualifying Under Texas Penal Code §9.31\nIf you want to protect yourself or another person, there are multiple situations under Texas Penal Code §9.31 where you will not be justified in using force or deadly force. If you fall under one of the following situations, you will not be given the \"No Duty to Retreat\" protection:\nThe use of force is not justified in response to verbal provocation alone. (If someone is only yelling at you, you are not justified in using force against them).\nYou will not be justified in using force to resist an arrest or search being made by a police officer (unless the officer uses greater than reasonable force), even if the arrest or search is ultimately proven to be unlawful.\nThe use of force against another is not justified if you consent to the force. (No dueling or consenting to gun fights).\nYou provoked the use of force, unless you have clearly abandoned the encounter.\nIf you seek a discussion with another person regarding your differences while unlawfully carrying a weapon, you will not be given the “No Duty to Retreat” protection. Unlawful carry of a weapon includes:\na non-CHL holder carrying in places other than their premises, place of business, vehicle or watercraft;\nhaving a handgun in plain view;\nengaging in criminal activity while carrying a weapon,\ncarrying a weapon by a person who is a member of a criminal street gang; or,\nCarrying a prohibited weapon.\nQualifying Under the No Duty to Retreat Statute\nAs we discussed earlier, the first thing that must be satisfied to receive the “No Duty to Retreat” Protection is that the person must have had a legal right to be in the location where deadly force was used. What does the law mean when it says that you must “be in a location where you have a legal right to be?” The best way to clarify this is to discuss places where you do not have a legal right to be. Any location where you would be considered a trespasser is, by definition, a place where you do not have a legal right to be. Under Texas Penal Code §30.05, a person becomes a criminal trespasser if they enter or remain on property without effective consent, or the person had notice that entry was forbidden or received notice to depart but failed to do so. Notice of trespassing includes: oral or written communication, fencing, signs posted on the property indicating that entry is forbidden, purple paint marks on trees or posts on the property, or crops for human consumption growing on the property. As long as you are in a place where you are not considered a trespasser by the law, you most likely have a legal right to be there under the “No Duty to Retreat” statutes.\nIn addition to the location test, you cannot have provoked the other's use or attempted use of force. You cannot start a fight and then claim justification for your use of force or deadly force. There is, however, an exception to this rule. If you abandon the encounter or clearly communicate your intent to abandon, and you cannot do so safely, and the other continues to use unlawful force against you, you do not have a duty to retreat.\nA very similar scenario played out in a district court in Harris County. An individual was convicted of murdering his neighbor during a conflict that started as a result of a noise complaint. The accused individual videotaped the entire confrontation. The last few minutes of the video seem to show that the man was justified in discharging his firearm after three men charged him. However, prior to the last few minutes, approximately twenty minutes of the video showed the accused leaving his property with his handgun, trespassing on his neighbor's property, and taunting the neighbors by flashing his pistol. As a result of these actions the man did not qualify under the \"No Duty to Retreat\" statutes. In fact, the prosecutor in the case told the jury that \"self-defense was never meant to protect the one that started the fight.\" The jury only deliberated for 90 minutes before returning a verdict of guilty on a murder charge and ultimately sentenced him to 40 years in prison.\nFinally, you cannot be engaged in any criminal activity, other than a Class C misdemeanor traffic offense at the time deadly force was used and claim self-defense.\nAs you can see, the Texas versions of the Castle Doctrine and Stand Your Ground laws are extremely complex and cannot be summarized with a simple catch phrase. These topics consume thousands of pages of legal treatises, so this article is only a brief overview of the intricacies that are involved with these topics.","What are “Stand Your Ground” Laws?\nThe shooting of Trayvon Martin has focused attention on so-called “Stand Your Ground” laws, which strengthen the right to self defense. Get Legal Lad’s expert take on the significance of such laws, and how they might be used by George Zimmerman, the defendant in the Martin case.\nToday’s topic: “Stand Your Ground” Laws\nAnd now, your daily dose of legalese: This article does not create an attorney-client relationship with any reader. In other words, although I am a lawyer, I’m not your lawyer. In fact, we barely know each other. If you need personalized legal advice, contact an attorney in your community.!\nThe Trayvon Martin Case\nOn February 26, 2012, 17-year-old Trayvon Martin was fatally shot in Sanford, Florida by George Zimmerman, a neighborhood watch coordinator for his community. Zimmerman has been charged with second-degree murder. Much of the reporting about the incident has focused on Florida’s so-called “Stand Your Ground” law, which reinforces the right to self-defense, leading some to the conclusion that this law is a license for vigilante justice.\nQuick and Dirty Tip: In reality, “Stand Your Ground” is merely one aspect of the right to self-defense, and does not permit unprovoked attacks.\nDisputed Facts About the Martin Case\nThe key facts about the Trayvon Martin case are disputed. Mr. Zimmerman claims that Martin attacked him, and that he (Zimmerman) acted in self-defense. According to reports, police records show that Zimmerman was injured, including a broken nose, when police arrived at the scene. If that’s true, it suggests that there was a fight, but it doesn’t tell us who the initial aggressor was. That could be key to the issue of self-defense.\nWhat is the Law of Self-Defense?\nSelf-defense is a doctrine that allows a person to avoid conviction for a violent crime, if he can establish that he acted to defend himself or another person. In Florida, as in most other states, the doctrine can be used to justify the use of deadly force by a person who (1) reasonably believes that such lethal force is necessary (2) to prevent imminent death or serious bodily harm to himself or another person, or to prevent a “forcible felony” (such as rape or kidnapping).\nWhat is the Duty to Retreat?\nThe right to use deadly force is pretty much universally available when people are defending themselves in the home. This is an old common law right, popularly known as the “castle doctrine” after the old saying that “a man’s home is his castle.” And a woman’s home, too, of course. But outside the home, the common law had a different rule known as the “duty to retreat.” Under this rule, a person being attacked is under a duty, essentially, to run for his life if he can do so in safety. In such situations, a person could assert the right to self-defense only if he could show that he could not escape.\n“Stand Your Ground” Laws\nFlorida and other older American states generally adopted the duty to retreat from English common law. But the rule lost popularity during the 19th century. Western states generally followed the “true man” doctrine, so called because “true men” don’t flee in the face of danger. This rule was the forerunner of the “Stand Your Ground” laws, which remove any duty to retreat as a pre-condition to using deadly force. According to some commentators, such laws have had limited effect, because the “duty to retreat” does not often come into play. In order to be under a duty to retreat, you have to be able to retreat in complete safety – and if your assailant has a gun, how can you know whether you can escape?\nThe key point, however, is that “Stand Your Ground” laws do not change the other requirements for self-defense. To go back to the Trayvon Martin case, George Zimmerman won’t have to justify his failure to retreat, but he will still have to show that he reasonably believed that Trayvon Martin was about to kill him or cause serious injury and that deadly force was necessary to stop him. Self-defense will almost certainly not work if Mr. Zimmerman was the initial aggressor – a question that has been raised by reports that a 911 dispatcher advised Zimmerman not to pursue Martin. If that’s true, then jurors will have to examine whether Zimmerman voluntarily chose to ignore the dispatcher’s advice and pursued Martin.\nThe Controversy over Zimmerman’s Murder Charge\nInitially, Florida prosecutors did not charge Zimmerman with any crime. However, a special prosecutor appointed by Florida’s governor has now charged Zimmerman with second-degree murder. Many commentators, including Harvard Law School’s Alan Dershowitz, have criticized this as disproportionate. Under Florida law, second degree murder requires that the killer acted with a “depraved mind” motivated by “ill will, spite, malice or hatred.” Even if Zimmerman fails to establish self-defense, the prosecution might have a difficult time proving beyond a reasonable doubt that he acted with a “depraved mind.”\nWhat do you think?\nThank you for reading Legal Lad’s Quick and Dirty Tips for a More Lawful Life.\nYou can send questions and comments to email@example.com. Please note that doing so will not create an attorney-client relationship and will be used for the purposes of this article only."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:d2fb2fdc-28e7-4d15-8202-7edbd4b650f5>","<urn:uuid:b723446f-ae02-4a98-b703-f408d6ebcbbf>"],"error":null}
{"question":"What's the relationship between session duration and user retention in Google Analytics, and how are they measured differently?","answer":"Session duration and user retention are measured differently in Google Analytics. Session duration is calculated by tracking the time frame during which there are regular active interactions on a website within a single visit - for example, adding together individual session durations (like 180 + 60 + 360 seconds) and dividing by the number of sessions to get an average. User retention, on the other hand, is measured through cohort analysis, which tracks how many users return to the site over time - for instance, out of 100 visitors on a given date, how many return on subsequent days. While session duration helps understand immediate engagement, retention metrics help measure long-term user engagement and loyalty by tracking returning visitors over extended periods.","context":["What You Need to Know About Google Analytics\nGoogle Analytics provides a lot of data when it comes to visitor behavior analysis. This includes bounce rate (the percentage of visitors that view one page), sessions (a collection of interactions within a 30-minute window), pages/session, goal completions and conversions. Get started now to learn more about all the data that is available through this program. Below are some tips for understanding these reports. These metrics can also be used to increase your company’s profitability.\nUnderstanding some metrics is key to understanding how visitors interact with your site. The average time spent on each page is one metric that you can use to improve your website’s performance. The session quality metric shows the number of pages viewed per session. You can use the session quality metric to identify channels that are not engaging users. Lastly, there is the conversion rate, which is the pan-ultimate metric in web analytics. The conversion rate measures how many visitors complete the desired actions, like signing up for your newsletter or making a purchase.\nIf your pageviews are high, it could indicate that visitors have not engaged with your site. A website that isn’t popular with the masses may have a low average pageview count. In addition, average session duration may not be an accurate reflection of user engagement. Hence, it is not always possible to use it as a key performance indicator (KPI).\nBesides the above-mentioned KPIs, Google Analytics provides an excellent overview of how people are using your website. Google Analytics also gives information about how people found specific pages or products. You can create the website you want to attract the best traffic by understanding what visitors are searching for. You can also use the analytics to improve your paid campaign, email marketing, or social media campaigns. You can increase the effectiveness of your marketing campaigns, and eventually make more sales.\nGoogle Analytics metrics also show you the amount of traffic to your site. The source of traffic can be either direct or from a search engine. It can be difficult to identify the source of traffic. Therefore, it is important to know where visitors come from. This information can be displayed in both the Audience and Behavior reports. If you want to learn how to make improvements in the customer experience, you can customize the content and design to make it more relevant to those countries.\nGoogle Analytics dimensions refers to various levels of organizational structure. A user may have multiple sessions, while one session can contain multiple hits. Google Dimensions include the User Type and New Sessions. E-Commerce Analysis can use the product-level scope to identify which metrics are important to a particular product. This reporting can also be used to compare user behaviour across segments. The more detail you can get, the better it will be.\nGoogle Analytics offers many options for combining the different dimensions. Google Analytics offers standard measurements but you have the option to create customized descriptions that measure your unique characteristics. You can use the Sessions metric to combine with hit-level dimension data in order to determine which keywords led you to phone calls. To collect data about the website’s usage time and log-in users, you can combine metrics and dimensions. Google Analytics can also import data from other sources than Google Analytics.\nCustom dimensions can be hit or user-scoped. After custom dimensions have been registered, they aren’t visible in GA4 reports. To implement custom dimensions, send the data to GA4 along with the registration of the parameter. It may take up to 24 hours for the data to show in reports. A custom dimension can take up to 24 hours to appear in the reports, so it’s best to wait for at least a day or two before implementing it.\nYou can view qualitative data when you use Google Analytics dimensions. For example, if you are an ecommerce merchant, you can use the dimension value landing page to see what pages are most popular among new users. Similarly, if you’re an ecommerce merchant, you can use the dimension value landing page to learn about the performance of your products. In addition to analyzing how well your products perform in each category, you can also view metrics that measure how well they rank on different pages.\nGoogle Analytics offers powerful features to isolate subsets of data to analyze and compare separately. Filters can be applied to all data. They may include metrics like Returning Users and Bounced Session, as well as dimensions such Converts. This allows you to compare and analyze data in more detail. Segments can also be used for many years, even after they are deleted. These are just a few of the many benefits that segments offer:\nSegmenting users based on their behavior is possible with the behavior segment. You can target specific audiences by using the date, number and frequency of each visit. You can also segment users by their browsing history and behavior, including transactions. You can create custom segments based on these characteristics, as well. To narrow your search to specific users, you can use the source traffic option. You can also use UTM parameter tags to segment your users by source.\nYou can create user-based segments that allow you to choose the dates your visitors will be able to visit your website. These date ranges are usually around 93 days. A single view can have up to 1000 segments. For user-based segments, the default range of dates is 93 days. If a user has more than 1000 sessions in the window, it will be treated as bot traffic. You can then see what pages are most popular.\nWhen analyzing the data in Google Analytics, you can create custom segments and metrics. Google Analytics offers a number of pre-defined segments and default system segments. You should review the available segments before creating yours. It is easier to compare results when you have custom segments. These segments can be used to analyze data. You’ll be able to determine which are the most lucrative for you. So go ahead and make use of Google Analytics!\nID of the user\nThe User ID feature allows you to monitor your customer’s behavior and track the various stages in their journey. It is important to note that User IDs only work when the user logs into your website. This feature is required to be able track anonymous users. It can be used in combination with email addresses or other identifiers. The User ID can be used to link sessions with Google Analytics if your site collects email addresses.\nFirst, enable the User ID feature within your Google Analytics account to get started. You can enable this feature on any website that provides login functionality or social media platforms. It’s best to enable this feature before setting up Google Analytics. Once you’ve activated the feature, you will need to implement the tracking code in your website and send IDs to Google Analytics. To get started, follow the steps outlined below.\nGoogle Analytics User IDs are unique combinations of alphanumeric characters which identify a user. It allows you to identify one user on multiple devices or browsers. This makes it easy to track and measure specific users’ behavior. It also allows you to associate multiple sessions with the same user across multiple devices. This is especially helpful for cross-device measurement. It also helps to fix attribution problems. You may want to set up a Google Analytics UserID for your app or website if you are a business owner.\nWhen it comes to the user experience, a User-ID is an important part of tracking. Regardless of how many people visit your website, you must understand how their behavior differs from that of a non-logged-in user. This feature allows you to track each user individually and see what makes them tick. The user ID can be used to track users’ activity on a different device. You can integrate the user ID feature in your analytics account.\nOne way to measure customer retention is to look at how long a person spends on your website. You can use time on your website to determine if you are building customer loyalty or keeping current customers satisfied. You should also consider negative testimonials and average order value. Google Analytics can help you understand how long your users stay on your website. Read on for more information.\nThe data retention period you set in Google Analytics is entirely up to you. You can use user data to create custom reports and apply segments to reports. You should remember that advanced features such as custom reporting or creating unique reports require the event and user data. You’ll have to delete data in the next month if you reduce your retention period. So if you’re looking to measure the value of your audience, consider changing the retention period to one month or three months.\nUser retention is best measured through the cohort chart. For example, if 100 people visit your website on September 9, two will return on September 16 and ten on September 10. This shows how much people are returning to your site and how many new users you’ve acquired. To understand how long a user has been on your site, you should track the number of visitors who come to your website through paid ads and organic search. Google Analytics can help you determine how many of those visitors are returning.\nAnother useful tool is the cohort analysis. Cohorts are groups of users with a common characteristic. For example, a user with the same Acquisition Date (ACD) as a new user will be grouped into the same cohort. Cohort analysis can also help you determine the percent of customers who come back after two or eight days. This type of analysis can be very useful for B2B organizations and industries where long-term engagement is a must.","A session in Google Analytics is a group of interactions recorded when a user visits your website within a given period. Google Analytics session begins when a user visits a page on your site and ends after 30 minutes of inactivity or when the user leaves.\nHow does Google Analytics determine session duration?\nTo calculate your average session duration, Google Analytics would add together the duration of each session (180 + 60 + 360) and divide the sum (600) by the number of sessions (3) to get an average session duration of 200 seconds, or 3 minutes and 20 seconds (which is sometimes displayed in Google Analytics as 00:03:\nHow is a session defined?\n1: a meeting or period devoted to a particular activity The football team held a practice session. 2: a single meeting (as of a court, lawmaking body, or school) 3: a whole series of meetings Congress was in session for six months.\nAre sessions visits in Google Analytics?\nSessions in Google Analytics are defined as the total number of visits to your site — including both new and repeat visits. So that same person who visited your site 100 times on the same device is counted as one user, but 100 sessions.\nWhat is the difference between page views and sessions?\nSessions represent a single visit to your website. Pageviews represent each individual time a page on your website is loaded by a User. A single Session can include many Pageviews, if a User navigates to any other web pages on your website without leaving.\nWhat does session duration mean?\nA session duration is defined as the time frame during which there are regular active interactions occurring from a user on a website. It is effectively the sum of the time-on-page for the different pages that a person visits on a website during a single session.\nWhere are sessions in Google Analytics?\nWhere can I find sessions in Google Analytics? To find out how many sessions your site had in the last 30 days, go to the Audience tab in the left hand column of Google Analytics, then click on Overview, then Sessions.\nWhat is the difference between users and sessions in Google Analytics?\nSessions in Google Analytics Means: Users = “Unique visitors”, or a person who has come to your website. Sessions = “ Visits ”, or different times that person came to your site.\nWhat is session Start Google Analytics?\nA session (once known as a visit) is the browsing period of a user. A session starts when the user first comes to your site and ends when any of the following happen: Inactivity – as a natural timeout, Google Analytics ends a session when the user hasn’t been active on your site for 30 minutes (by default)\nWhat’s the difference between visitors and sessions?\nA “session” and a “visit” are two different ways to categorize groups of interactions a visitor has on your website. Specifically, a visit is counted when a visitor arrives on your website from a page outside your website. A session, however, is a more comprehensive way of grouping interactions.\nIs sessions the same as traffic?\nSessions and Page views is the most commonly misunderstood metric in tracking website traffic. Sessions counts the number of visits to the website as a whole. Page views counts the number of pages that have been viewed.\nWhats the difference between a session and a user?\n“A user is an individual person who has come to your site whereas a session represents one of those visits to your site,” says Mark Barrera of TrustRadius. “So, a person could come once or many times, and that wouldn’t increment the ‘user’ count but would increase the number of sessions.”\nWhat do Sessions track?\nSession Tracking tracks a user’s requests and maintains their state. It is a mechanism used to store information on specific users and in order to recognize these user’s requests when they connect to the web server. HTTP is a stateless protocol where each request to the server is treated like a new request.\nAre page views or sessions more important?\nIf the same user leaves the page and comes back later, the second visit counts as a new session, but not as a new user. In general, the higher the number of pageviews a page gets, the better. However, unique pageviews are a more accurate metric when it comes to determining the true popularity of a page.\nWhy are page views higher than sessions?\nWhile there can be many causes why you are seeing more users than pageviews, most of the time this happens because on some of your pages, the pageview hits are not sent while the event hits are, thus registering that user and session in Google Analytics without any pageviews."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:ba89cf29-ec41-4a7d-b046-55d55a118afb>","<urn:uuid:f5c88659-bd88-4ad3-b2de-7df6c940dfd0>"],"error":null}
{"question":"How do the control methods differ between weevils and pantry moths in terms of cleaning and prevention strategies?","answer":"For weevils, control involves vacuuming the infested area, cleaning with a disinfectant solution, and placing vinegar-soaked washcloths around infestation sites. Prevention includes storing dry grains in tight-lid containers and keeping food items in the freezer. For pantry moths, especially Indianmeal moths, control requires thorough inspection of all susceptible foods, disposing of infested items, and vacuum cleaning cabinets and shelves, particularly in cracks and corners. Prevention strategies include storing foods in sealed glass, metal, or heavy plastic containers, or in the refrigerator or freezer. For both pests, chemical sprays are not recommended as they are ineffective and potentially hazardous.","context":["How to Kill Weevils Safely in Your Home How to Kill Weevils Safely in Your Home\nVarious kinds of weevils are found in garden spaces, feeding on crops and a variety of plants. However, some weevils will get inside your home and will hide inside your walls and windowsills, or will feed on stored food items like flour, rice, oats, or other grains. Using chemical sprays to kill these weevils may seem like the easiest choice but they are not the ideal solution. For starters, since rice and granary weevils are endemic to food sources, using chemicals raises the risk of the food becoming contaminated. Further, these sprays are a health hazard, particularly if there are young children and pets in the home. Therefore, you should be equipped with some basic knowledge about killing weevils in a comprehensive, yet safe manner.\nStep 1 - Get Prepared\nOnce you have confirmed weevil infestation in a part of the house, prepare to vacuum it. Equip yourself with a basic, protective gear like a mask and plastic gloves, and close all doors and windows of the room. Then, empty the infected food product into a plastic bag.\nStep 2 - Vacuum\nWeevils start scampering in all directions when they feel threatened. Therefore, it is vital that you are ready with your vacuuming device before approaching the infested site. Start vacuuming immediately and ensure that you scoop away the escaping pests. If your contaminated food is stored in a wooden pantry or cabinet, tap hard on the sides. This ensures that any weevils residing in small crevices are also removed.\nStep 3 - Clean\nGather the vacuumed weevil debris and the infested food items and pack them in new, sealed, plastic bags. Dump these bags, and then follow this with a thorough cleaning of the vacuumed site. An infested area usually has hard-to-see chemical traces that can attract weevils in the future, so prepare a cleaning mixture with some water and your basic kitchen/washroom disinfectant solution. Spray the solution over and around the infestation site and scrub it. Wipe down with a sponge afterward. Ideally, repeat this cleaning part at least once every day for the next two days to eradicate all weevil residues.\nStep 4 - Weevil Prevention\nYou should also take some basic precautions to ensure that weevil infestation does not repeat itself. This can be easily done by taking some hygienic measures. Ensure that all dry grains are stored in tight-lid containers; plastic and paper bags are generally inadequate for storage. Also, try to keep food items in the freezer if they need long-term storage. This is because cold temperatures are a natural deterrent to weevils. Ensure that all scraps of food in the kitchen and around the perimeter of your house are cleaned up on a daily basis. You can also place washcloths soaked with vinegar around sites that show signs of repeated weevil infestation. Vinegar has chemical properties that turn away these pests.\nIf you follow these steps as instructed, a weevil infestation shouldn't be a problem. They will be long gone before you know it.","Insect pests of stored foods\nInsects infesting stored foods are one of the most common household insect problems. The many different kinds of insects that infest stored dried foods are often referred to as \"pantry pests.\"\nPantry pests contaminate more food than they consume, and most people find the contaminated products unfit for consumption. Pantry pests are often discovered when they leave infested foods to crawl or fly about the house. They often accumulate in pots, pans or dishes or on window sills. Fortunately, they do not bite or sting people or pets nor do they feed on or damage the house structure or contents.\nWhat do they eat?\nNearly all dried food products are susceptible to insect infestation, including cereal products (flour, cake mix, cornmeal, rice, spaghetti, crackers, and cookies); seeds such as dried beans and popcorn; nuts; chocolate; raisins and other dried fruits; spices; powdered milk; tea; and cured meats. Non-food items that may be infested include birdseed, dry pet food, ornamental seed and dried plant displays, ornamental corn, dried flowers, garden seeds, potpourri, and rodent baits.\nStored food insects are most likely to infest products that have been opened but are also capable of penetrating unopened paper, thin cardboard, and plastic, foil, or cellophane-wrapped packages. They may chew their way into packages or crawl in through folds and seams. Insects within an infested package begin multiplying and can spread to other stored foods not only in the same area but in other rooms in a home. All stages (egg, larva, pupa, and adult) may be present simultaneously in infested products.\nWhere do they come from?\nA stored food product can become infested anywhere during the process from production until it arrives in your home. However, stored food is most likely to become infested in stores or in homes. Most of the stored food insects also are pests of stored grain or other commodities and may be relatively abundant outdoors. Food products that are left undisturbed on the shelves for long periods are particularly susceptible to infestation. However, foods of any age can become infested.\nIdentification and biology of common stored product insects\nIndianmeal moths (Plodia interpunctella) are the most common moths infesting food in homes. These moths have a wingspan of 1/2 to 5/8 inch. When at rest, they fold their wings behind themselves, over their bodies. The base of the front wing is pale gray or tan and the outer two-thirds is reddish-brown with a coppery luster. The wing markings are distinctive, but may be less clear if the scales have been rubbed from the wings. Indianmeal moths may be found inside infested products or flying around homes.\nThe larvae are whitish worms with shades of yellow, pink, green, or brown and grow to 1/2 inch long.\nOnly the larvae feed in stored products, which can be any dry stored food or whole grain. Foods infested with these insects will have silk webbing present on the surface of the product. Larvae often leave the food when mature and may move long distances before stopping to spin a cocoon. It is common to find caterpillars and cocoons on ceilings and walls. Adult moths may be seen up to several weeks after the food source has been removed.\nMeal moths (Pyralis farinalis) have a wingspan of about 3/4 - 1 inch. Their forewings have a dark reddish brown band across the top and bottom of the wings with an olive or yellowish green band, outlined by wavy white lines in the center. Their abdomen is typically curved up at a 90° angle when at rest. The larvae have a black head and a whitish body with some orange at the end of the body. They often form feeding tubes made of silk and tiny pieces of food. Meal moths are found feeding on a wide variety of flour and grain products and seeds, especially when they are damp. These moths are not common in homes.\nSawtoothed grain beetles (Oryzaephilus surinamensis) are about 1/10 inch long, slender, flattened, and brownish-red to almost black in color. They are easily identified by the saw-like teeth on each side of the thorax. Larvae are cream-colored, slender, and about 1/8 inch long, although they are rarely noticed by residents. Sawtoothed grain beetles are found in many different food items, including dried fruit, cereals, nuts, dried meat, macaroni, and seeds.\nDrugstore beetles and cigarette beetles (Lasioderma serricorne and Stegobium panicum) are about 1/8 inch long, oval, and brown. The head is bent downward giving the insect a humped appearance. Both species fly and can be found around windows. Larvae are 1/8 inch long when mature, and yellowish-white with a light brown head (the larvae are not usually noticed by residents). They have a curved body covered with fine hair. Cigarette and drugstore beetles feed on a wide variety of dried plant products such as spices, macaroni and other grain based foods, dried flowers, tobacco products, and even paper products, including books.\nFlour beetles (Tribolium confusum and T. castaneum) are 3/16 inch long, reddish-brown, and elongate oval in shape. Larvae are cylindrical, whitish, or cream-colored and up to 1/4 inch long and have two small pointed spines on the tail end (the larvae are not usually noticed by residents). Two species of flour beetles may be found: red flour beetles are common in homes and the confused flour beetle is a frequent pest in flour mills. Flour beetles infest many types of dried food products, such as flour, bran, cereal products, dried fruits, nuts, and chocolate.\nWarehouse and cabinet beetles (Trogoderma spp.) are elongate oval and 1/8 to 3/16 inch long. They may be solid black or mottled with yellowish-brown markings. Larvae are long and narrow, yellowish to dark brown, hairy and generally grow to about 1/4 inch (although they may not be noticed by residents). Warehouse and cabinet beetles feed in a wide variety of food products, such as grain products, seeds, dried fruits, animal by-products skins, fur, hair, and pet food. They are also known to feed on dead insects and animal carcasses.\nGranary, rice, and maize weevils (Sitophilus spp.) are slender insects with a conspicuous snout projecting forward from the head. They are dark brown, sometimes with four orangish spots on the wing covers. They are less than 3/16 inch long. Larvae are white, legless, and looked wrinkled and are only found inside whole kernels or seeds. These weevils attack only whole grains or seeds, leaving small round exit holes in infested kernels. They rarely are found in nuts, dried fruits, macaroni, and caked or crusted milled products such as flour. (A different, larger species of weevil can be found in homes during the fall due to emergence from acorns or hickory nuts collected and stored inside).\nSpider beetles (family Ptinidae) are reddish brown, 3/16 inch beetles with long legs and a somewhat, spider-like appearance. The larvae are C-shaped and whitish; they remain in infested material and aren't normally seen. Spider beetles infest a variety of dried plant products.\nBean weevils (Acanthoscelides obtectus) are a type of seed beetle. They are a mottled light and dark brown, broadly oval, and about 1/8 inch long. They have short wing covers which exposes part of the abdomen. Unlike other weevils, bean weevils lack a conspicuous snout. The larvae are small, whitish, legless, and C-shaped. They feed inside dried beans and peas.\nPurchase dried foods in quantities small enough to be used up in a relatively short period of time. Use oldest products before newer ones, and opened packages before unopened ones.\nInspect packages or bulk products before buying. Packages should be sealed and unbroken. Also check the freshness packaging date. Look for evidence of insects, including holes in the packaging or wrapping.\nStore insect-free foods in tightly closed glass, metal, or heavy plastic containers especially if you do not use up the food very quickly (this is less important for food that is used up more quickly). You can also store susceptible foods in the refrigerator or freezer.\nKeep food storage areas clean. Do not allow crumbs or spilled food to accumulate. Remove and discard old, unused products and inspect the remainder. Thoroughly clean cracks and corners with a vacuum cleaner. Also check and clean areas where pet food and birdseed are stored as these are particularly common sources of infestations.\nWashing areas with detergents, ammonia, or bleach will not prevent insect infestation. There is no evidence that proves that placing bay leaves or sticks of spearmint gum in a cupboard will prevent or deter stored food insect pests.\nThere are several ways you may become aware of a stored product infestation. If you find small beetles in susceptible food products, that is a sure sign of a problem. It is also common to find stored product beetles on counters and in cupboards. In some cases, the beetles are attracted to light and may be found around windows.\nYou may find Indianmeal moths flying around kitchens and other rooms. As caterpillars move away from infested food to pupate, they can be found on walls and ceilings in rooms adjacent to infestations. When examining food packages, you may not only find caterpillars but silk webbing inside infested packages.\nBe careful, as not all small beetles or moths found indoors are necessarily a panty pest. If there is not a direct association with food, be sure the insects are identified correctly by an expert to determine whether they are a stored product food insect.\nWhen you know a stored product problem is present, be sure to examine all susceptible food as there could be more than one infested source. When inspecting them, look at the top surface of products with a flashlight or pour the package contents onto a cookie sheet.\nWhen you find food that is infested, just throw it away.\nUse a vacuum cleaner to thoroughly clean cabinets and shelves, especially in cracks and corners. This will pick up crawling insects and spilled or infested material that is present. Empty the vacuum cleaner or discard the vacuum cleaner bag after use to prevent reinfestation. Washing shelves with detergent, bleach, ammonia, or disinfectants will not have any effect on insect pests.\nAs a precaution against re-infestation, store susceptible foods in sealable glass, metal, or heavy plastic containers or in the freezer or refrigerator until you are convinced the infestation is gone. It is not unusual to see an occasional Indianmeal moth flying for as long as 3 weeks after the infested sources have been eliminated. However, if you continue to see Indianmeal moths after three weeks, that indicates there is an infested food source that has not been discovered yet.\nIf you have older food products and you are not sure if they are infested, you can place these products in the freezer at 0 degrees for at least 4 days or in shallow cookie sheets or pans in an oven at 130 degrees for at least 30 minutes. These temperatures will kill any eggs or insects that may be present. If insects are infesting ornaments or decorations made with plant products or seeds, place the items in a freezer for at least four days.\nInsecticide sprays are not recommended for controlling insects in stored food cupboards. Household insecticides have no effect on insects within food packages and any control of insects outside of them is temporary unless the source of the infestation is found and eliminated.\nAlso, the amount of extra work that is necessary when treating cupboards or other areas usually is prohibitive. It is necessary to first remove dishes, glasses, and food packages so they are not contaminated by pesticides. Time is then needed to allow the spray to dry before items can be returned."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:ec32dfd4-da2e-4366-b6f0-7bf611350a68>","<urn:uuid:717e3582-0cea-438e-a2c7-f9a30abb717b>"],"error":null}
{"question":"Could you explain both the technical evolution of ecological genetics as a field and its relationship to pre-zygotic plant reproduction barriers?","answer":"Ecological genetics has evolved from its initial focus on fitness-influencing traits to now encompassing both molecular and field-based approaches to study genetic variation in natural populations. Modern ecological genetics incorporates molecular techniques while maintaining traditional population genetics approaches. This evolution of the field has enabled deeper understanding of reproductive barriers like pre-zygotic incompatibility in plants. For example, research has revealed specific mechanisms like the SPRI1 gene that controls interspecies barriers in plant reproduction, preventing hybridization between different species at the stigma level. This represents a perfect example of how ecological genetics combines molecular mechanisms with their adaptive significance in natural populations.","context":["Pre-zygotic interspecies incompatibility in angiosperms is a male–female relationship that inhibits the formation of hybrids between two species. Here, we report on the identification of STIGMATIC PRIVACY 1 (SPRI1), an interspecies barrier gene in Arabidopsis thaliana. We show that the rejection activity of this stigma-specific plasma membrane protein is effective against distantly related Brassicaceae pollen tubes and is independent of self-incompatibility. Point-mutation experiments and functional tests of synthesized hypothetical ancestral forms of SPRI1 suggest evolutionary decay of SPRI1-controlled interspecies incompatibility in self-compatible A. thaliana. Hetero-pollination experiments indicate that SPRI1 ensures intraspecific fertilization in the pistil when pollen from other species are present. Our study supports the idea that SPRI1 functions as a barrier mechanism that permits entrance of pollen with an intrinsic signal from self species.\nSubscribe to Journal\nGet full journal access for 1 year\nonly $4.92 per issue\nAll prices are NET prices.\nVAT will be added later in the checkout.\nRent or Buy article\nGet time limited or full article access on ReadCube.\nAll prices are NET prices.\nSequence data can be found at The Arabidopsis Information Resource database (https://www.arabidopsis.org/) or in the 1001 genomes website (1001genomes.org). Raw phenotype data used for the GWAS has been deposited in the GWA-Portal (https://gwas.gmi.oeaw.ac.at/#/study/4205/overview) and Arapheno (https://doi.org/10.21958/study:37). Raw data for pollen tube count was deposited to Mendeley (https://doi.org/10.17632/yzy85dtwk3.1). All other data are available in the article or in the Supplementary information.\nde Nettancourt, D. Incompatibility and Incongruity in Wild and Cultivated Plants (Springer, 2001).\nHogenboom, N. G. & Mather, K. Incompatibility and incongruity: two different mechanisms for the non-functioning of intimate partner relationships. Proc. Roy. Soc. B 188, 361–375 (1975).\nOkuda, S. et al. Defensin-like polypeptide LUREs are pollen tube attractants secreted from synergid cells. Nature 458, 357–361 (2009).\nTakeuchi, H. & Higashiyama, T. Tip-localized receptors control pollen tube growth and LURE sensing in Arabidopsis. Nature 531, 245–248 (2016).\nBoehm, T. Quality control in self/nonself discrimination. Cell 125, 845–858 (2006).\nIgic, B., Lande, R. & Kohn, J. R. Loss of self-incompatibility and its evolutionary consequences. Int. J. Plant Sci. 169, 93–104 (2008).\nFujii, S., Kubo, K. & Takayama, S. Non-self- and self-recognition models in plant self-incompatibility. Nat. Plants 2, 16130 (2016).\nHarada, Y. et al. Mechanism of self-sterility in a hermaphroditic chordate. Science 320, 548–550 (2008).\nKubo, K. et al. Collaborative non-self recognition system in S-RNase-based self-incompatibility. Science 330, 796–799 (2010).\nTakayama, S. et al. Direct ligand-receptor complex interaction controls Brassica self-incompatibility. Nature 413, 534–538 (2001).\nWheeler, M. J. et al. Identification of the pollen self-incompatibility determinant in Papaver rhoeas. Nature 459, 992–995 (2009).\nWilkins, K. A. et al. Self-incompatibility-induced programmed cell death in field poppy pollen involves dramatic acidification of the incompatible pollen tube cytosol. Plant Physiol. 167, 766–779 (2015).\nThomas, S. G. & Franklin-Tong, V. E. Self-incompatibility triggers programmed cell death in Papaver pollen. Nature 429, 305–309 (2004).\nLewis, D. & Crowe, L. K. Unilateral interspecific incompatibility in flowering plants. Heredity 12, 233–256 (1958).\nLewis, D. Incompatibility in flowering plants. Biol. Rev. 24, 472–496 (1949).\nHiscock, S. J. & Dickinson, H. G. Unilateral incompatibility within the Brassicaceae: further evidence for the involvement of the self-incompatibility (S)-locus. Theor. Appl. Genet. 86, 744–753 (1993).\nLi, W. & Chetelat, R. T. A pollen factor linking inter- and intraspecific pollen rejection in tomato. Science 330, 1827–1830 (2010).\nMurfett, J. et al. S RNase and interspecific pollen rejection in the genus Nicotiana: multiple pollen-rejection pathways contribute to unilateral incompatibility between self-incompatible and self-compatible species. Plant Cell 8, 943–958 (1996).\nLi, W. & Chetelat, R. T. Unilateral incompatibility gene ui1.1 encodes an S-locus F-box protein expressed in pollen of Solanum species. Proc. Natl Acad. Sci. USA 112, 4417–4422 (2015).\nTovar-Méndez, A. et al. Restoring pistil-side self-incompatibility factors recapitulates an interspecific reproductive barrier between tomato species. Plant J. 77, 727–736 (2014).\nBurdfield-Steel, E. R. & Shuker, D. M. Reproductive interference. Curr. Biol. 21, R450–R451 (2011).\nTsuchimatsu, T. et al. Evolution of self-compatibility in Arabidopsis by a mutation in the male specificity gene. Nature 464, 1342–1346 (2010).\nSeren, U. et al. GWAPP: A web application for genome-wide association mapping in Arabidopsis. Plant Cell 24, 4793–4805 (2012).\nHooper, C. M. et al. SUBAcon: a consensus algorithm for unifying the subcellular localization data of the Arabidopsis proteome. Bioinformatics 30, 3356–3364 (2014).\nIwano, M. et al. Calcium signalling mediates self-incompatibility response in the Brassicaceae. Nat. Plants 1, 15128 (2015).\nKlepikova, A. V., Kasianov, A. S., Gerasimov, E. S., Logacheva, M. D. & Penin, A. A. A high resolution map of the Arabidopsis thaliana developmental transcriptome based on RNA-seq profiling. Plant J. 88, 1058–1070 (2016).\nNovikova, P. Y. et al. Sequencing of the genus Arabidopsis identifies a complex history of nonbifurcating speciation and abundant trans-specific polymorphism. Nat. Genet. 48, 1077–1082 (2016).\nMurat, F. et al. Understanding Brassicaceae evolution through ancestral genome reconstruction. Genome Biol. 16, 1–17 (2015).\nMizukami, A. G. et al. The AMOR arabinogalactan sugar chain induces pollen-tube competency to respond to ovular guidance. Curr. Biol. 26, 1091–1097 (2016).\nHemler, M. E. Tetraspanin functions and associated microdomains. Nat. Rev. Mol. Cell Biol. 6, 801–811 (2005).\nMiyado, K. et al. Requirement of CD9 on the egg plasma membrane for fertilization. Science 287, 321–324 (2000).\nBoavida, L. C., Qin, P., Broz, M., Becker, J. D. & McCormick, S. Arabidopsis tetraspanins are confined to discrete expression domains and cell types in reproductive tissues and form homo- and heterodimers when expressed in yeast. Plant Physiol. 163, 696–712 (2013).\nTsuchiya, T. in Sexual Reproduction in Animals and Plants (eds Sawada, H. et al.) 305–325 (Springer, 2014).\nVanacker, H., Lu, H., Rate, D. N. & Greenberg, J. T. A role for salicylic acid and NPR1 in regulating cell growth in Arabidopsis. Plant J. 28, 209–216 (2001).\nDoughty, J., Wong, H. Y. & Dickinson, H. G. Cysteine-rich pollen coat proteins (PCPs) and their Interactions with stigmatic S (incompatibility) and S-related proteins in Brassica: putative roles in SI and pollination. Ann. Bot. 85, 161–169 (2000).\nTakada, Y. et al. Duplicated pollen–pistil recognition loci control intraspecific unilateral incompatibility in Brassica rapa. Nat. Plants 3, 17096 (2017).\nZhang, Z. et al. A pectin methylesterase gene at the maize Ga1 locus confers male function in unilateral cross-incompatibility. Nat. Commun. 9, 3678 (2018).\nTsuchimatsu, T. et al. Patterns of polymorphism at the self-incompatibility Locus in 1,083 Arabidopsis thaliana genomes. Mol. Biol. Evol. 34, 1878–1889 (2017).\nPlatt, A. et al. The scale of population structure in Arabidopsis thaliana. PLoS Genet. 6, e1000843 (2010).\nShimizu, K. K., Kudoh, H. & Kobayashi, M. J. Plant sexual reproduction during climate change: gene function in natura studied by ecological and evolutionary systems biology. Ann. Bot. 108, 777–787 (2011).\nSaito, K. et al. Luminescent proteins for high-speed single-cell and whole-body imaging. Nat. Commun. 3, 1262 (2012).\nRawat, V. et al. Improving the annotation of Arabidopsis lyrata using RNA-seq data. PLoS ONE 10, e0137391 (2015).\nWang, Z.-P. et al. Egg cell-specific promoter-controlled CRISPR/Cas9 efficiently generates homozygous mutants for multiple target genes in Arabidopsis in a single generation. Genome Biol. 16, 144 (2015).\nIwano, M. et al. Fine-tuning of the cytoplasmic Ca2+ concentration is essential for pollen tube growth. Plant Physiol. 150, 1322–1334 (2009).\nShiba, H. et al. Alteration of the self-incompatibility phenotype in Brassica by transformation of the antisense SLG gene. Biosci. Biotechnol. Biochem. 64, 1016–1024 (2000).\nHasegawa, J. et al. Three-dimensional imaging of plant organs using a simple and rapid transparency technique. Plant Cell Physiol. 57, 462–472 (2016).\nSeren, Ü. GWA-Portal: Genome-wide association studies made easy. Methods Mol. Biol. 1761, 303–319 (2018).\nThe 1001 Genomes Consortium 1,135 genomes reveal the global pattern of polymorphism in Arabidopsis thaliana. Cell 166, 481–491 (2016).\nBandelt, H.-J., Forster, P. & Röhl, A. Median-joining networks for inferring intraspecific phylogenies. Mol. Biol. Evol. 16, 37–48 (1999).\nSouth, A. rworldmap: A new R package for mapping global data. R Journal 3, 35–43 (2011).\nDurvasula, A. et al. African genomes illuminate the early history and transition to selfing in Arabidopsis thaliana. Proc. Natl Acad. Sci. USA 114, 5213–5218 (2017).\nKrogh, A., Larsson, B., von Heijne, G. & Sonnhammer, E. L. Predicting transmembrane protein topology with a hidden Markov model: application to complete genomes. J. Mol. Biol. 305, 567–580 (2001).\nHofmann, K. & Stoffel, W. TMBASE-A database of membrane spanning protein segments. Biol. Chem. 374, 166 (1993).\nKäll, L., Krogh, A. & Sonnhammer, E. L. L. Advantages of combined transmembrane topology and signal peptide prediction-the Phobius web server. Nucleic Acids Res. 35, W429–W432 (2007).\nTsirigos, K. D., Peters, C., Shu, N., Käll, L. & Elofsson, A. The TOPCONS web server for consensus prediction of membrane protein topology and signal peptides. Nucleic Acids Res. 43, W401–W407 (2015).\nHirokawa, T., Boon-Chieng, S. & Mitaku, S. SOSUI: classification and secondary structure prediction system for membrane proteins. Bioinformatics 14, 378–379 (1998).\nJuretić, D., Zoranić, L. & Zucić, D. Basic charge clusters and predictions of membrane protein topology. J. Chem. Inf. Comput. Sci. 42, 620–632 (2002).\nHaudry, A. et al. An atlas of over 90,000 conserved noncoding sequences provides insight into crucifer regulatory regions. Nat. Genet. 45, 891–898 (2013).\nAltschul, S. F., Gish, W., Miller, W., Myers, E. W. & Lipman, D. J. Basic local alignment search tool. J. Mol. Biol. 215, 403–410 (1990).\nMartin, M. Cutadapt removes adapter sequences from high-throughput sequencing reads. EMBnet. J. 17, 10 (2011).\nLangmead, B. & Salzberg, S. L. Fast gapped-read alignment with Bowtie 2. Nat. Methods 9, 357–359 (2012).\nLi, H. et al. The sequence alignment/map format and SAMtools. Bioinformatics 25, 2078–2079 (2009).\nAshkenazy, H. et al. FastML: a web server for probabilistic reconstruction of ancestral sequences. Nucleic Acids Res. 40, W580–W584 (2012).\nSlotte, T. et al. The Capsella rubella genome and the genomic consequences of rapid mating system evolution. Nat. Genet. 45, 831–835 (2013).\nRice, P., Longden, I. & Bleasby, A. EMBOSS: the European Molecular Biology Open Software Suite. Trends Genet. 16, 276–277 (2000).\nEdgar, R. C. MUSCLE: multiple sequence alignment with high accuracy and high throughput. Nucleic Acids Res. 32, 1792–1797 (2004).\nCouvreur, T. L. P. et al. Molecular phylogenetics, temporal diversification, and principles of evolution in the mustard family (Brassicaceae). Mol. Biol. Evol. 27, 55–71 (2010).\nTalavera, G. & Castresana, J. Improvement of phylogenies after removing divergent and ambiguously aligned blocks from protein sequence alignments. Syst. Biol. 56, 564–577 (2007).\nRonquist, F. et al. MrBayes 3.2: efficient Bayesian phylogenetic inference and model choice across a large model space. Syst. Biol. 61, 539–542 (2012).\nWe thank M. Okamura, M. Nara, T. Manabe, Y. Yamamoto, M. Niidome and M. Ishii for their technical assistance and A. Kawabe and Y. Kato for helpful discussions. This work was supported in part by Grants-in-Aid for Scientific Research on Innovative Areas (23113002, 16H06467 and 16H06464 to S. Takayama; 16H01467 and 18H04776 to S. Fujii; 17H05833 and 18H04813 to T.T. and 16H06469 to K.K.S.), Grants-in-Aid for Scientific Research (25252021 and 16H06380 to S.Takayama and 18H02456 to S. Fujii), Grant-in-Aid for Challenging Exploratory Research (15K14626 to S. Fujii) from the Ministry of Education, Culture, Sports, Science and Technology of Japan (MEXT), Swiss National Science Foundation to K.K.S. and Japan Science and Technology Agency (JST) PRESTO programme (JPMJPR16Q8) to S. Fujii.\nThe authors declare no competing interests.\nPeer review information: Nature Plants thanks Daphne Goring and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.\nPublisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nSupplementary Figs. 1–8, Supplementary Tables 1–5, and titles and legends for Supplementary Videos 1 and 2.\nTime-lapse imaging analysis of a M. littorea pollen grain attached on a Col-0 stigmatic papilla cell (left) and on a spri1-1 stigmatic papilla cell (right).\nTime-lapse imaging analysis of Col-0 and M. littorea pollen grains adjacently attached on a spri1-2 + SPRI1A stigmatic papilla cell.\nAbout this article\nCite this article\nFujii, S., Tsuchimatsu, T., Kimura, Y. et al. A stigmatic gene confers interspecies incompatibility in the Brassicaceae. Nat. Plants 5, 731–741 (2019). https://doi.org/10.1038/s41477-019-0444-6\nTwo subgroups of receptor-like kinases promote early compatible pollen responses in the Arabidopsis thaliana pistil\nJournal of Experimental Botany (2020)\nCrop wild phylorelatives (CWPs): phylogenetic distance, cytogenetic compatibility and breeding system data enable estimation of crop wild relative gene pool classification\nBotanical Journal of the Linnean Society (2020)\nRNA-Seq analysis of compatible and incompatible styles of Pyrus species at the beginning of pollination\nPlant Molecular Biology (2020)\nNature Plants (2019)","Ecology is the study of the relationships between organisms and their environments, whereas ecological genetics focuses more specifically on the genetics of ecologically important traits, i.e., traits that influence ecological relationships. At its inception, ecological genetics focused particularly on traits that influence fitness, such as those that affect survival and reproduction. This focus is maintained in its current form, although ecological genetics now also investigates the ecological and evolutionary processes that influence patterns of genetic variation in natural populations. Therefore, it can also be considered a study of genetic processes associated with microevolutionary change. Although both Charles Darwin and Alfred Russel Wallace brought together ecological and genetic concepts in the 19th century, the term “ecological genetics” was first used by E. B. Ford in his groundbreaking book Ecological Genetics (Ford 1964, cited under General Overviews: Textbooks). The field has evolved considerably since that time and now overlaps substantially with molecular ecology, a closely related field that uses molecular genetic tools to study questions in ecology. The only real difference between molecular ecology and ecological genetics is that the latter is not limited to studies based on molecular genetics. Instead, the term “ecological genetics” can refer to any study of the genetics of natural populations, whether they are based on molecular genetics, population genetics, or quantitative genetics. However, molecular genetic techniques are increasingly accessible and increasingly informative, and they often provide a relatively fast and cost-effective way to get data. As a result, the majority of ecological genetic studies now incorporate a combination of field and molecular genetic data, and the functional line between ecological genetics and molecular ecology is increasingly blurred.\nGeneral Overviews: Textbooks\nThe field of ecological genetics was formally developed by E. B. Ford, whose book Ecological Genetics was first published in 1964 (Ford 1964). In a review of this book, the eminent evolutionary biologist Theodosius Dobzhansky identified it as a very important contribution to the biological theory of evolution (also known as the synthetic theory of evolution, or the evolutionary synthesis), based largely on the work of a group of researchers at Oxford sometimes referred to as the “Oxford School” (Dobzhansky 1964). Ford describes the fruits of “a method which has in fact proved effective: one which combines field-work and laboratory genetics” (Ford 1964, p. 1). This was an extremely important book at the time, with Ford’s novel identification of ecological genetics as the field of study that “deals with the adjustments and adaptations of wild populations to their environments” (Ford 1964, p. 3). Ford further claimed “it supplies the means, and the only direct means, of investigating the actual process of evolution taking place at the present time” (Ford 1964, p. 3). In 1994, several renowned biologists contributed to Ecological Genetics (Real 1994), although records of this book are now relatively scarce. Ecological Genetics: Design, Analysis, and Application (Lowe, et al. 2004) provided what was at the time an extensive and extremely useful overview of ecological genetics with particular emphasis on analytical methods. In the same year, A Primer of Ecological Genetics (Conner and Hartl 2004) described basic concepts in population and quantitative genetics, including mathematics and statistics at a less advanced level with the aim of making subjects more accessible. Several additional books on related topics originally published by Beebee and Rowe in 2004 and Freeland, et al. in 2005, later updated in 2008 and 2011, respectively (see Rowe, et al. 2017; Freeland, et al. 2011). Since that time, there have been numerous methodological developments in the field, many of which are reflected in Rowe, et al. 2017.\nAllendorf, F. W., G. H. Luikart, and S. N. Aitken. 2013. Conservation and the genetics of populations. 2d ed. Chichester, UK: Wiley-Blackwell.\nAn in-depth overview of the concepts and tools that allow researchers to apply genetic information to biological conservation.\nAvise, J. C. 2004. Molecular markers, natural history, and evolution. 2d ed. Sunderland, MA: Sinauer.\nThe first edition of this book in 1994 introduced the idea that protein and DNA data could inform all sorts of ecological questions. This edition was similarly informative, although it includes topics unlikely to be included in the early 21st century, e.g., protein markers and random amplified polymorphic DNA markers (RAPDs, now rarely used in ecology), and DNA–DNA hybridization (a fairly short-lived approach). Treatments of phylogeography, speciation, and mating systems remain relevant, but are somewhat dated.\nCharmantier, A., D. Garant, and L. E. B. Kruuk. 2014. Quantitative genetics in the wild. Oxford: Oxford Univ. Press.\nExplains ways in which the study of the genetic basis of quantitative variation in recent years has provided insight into questions of evolutionary ecology in natural populations. Topics include life history theory, behavior ecology, sexual selection, responses to climate change, and senescence in natural environments. Includes studies based on molecular genetics, laboratory studies, and long-term data sets collected in the wild.\nConner, J. K., and D. L. Hartl. 2004. A primer of ecological genetics. Sunderland, MA: Sinauer.\nProvides an excellent introduction to the theory and applications of ecological genetics by explaining the way in which basic population and quantitative genetic principles can be applied to the study of population evolutionary dynamics. Discussions of concepts are accompanied by real data sets that have been taken from the primary literature. This was a very useful overview, but it is now somewhat dated with respect to molecular markers and genetic data.\nDobzhansky, T. 1964. Review of Ecological Genetics. E. B. Ford. Methuen, London; Wiley, New York, 1964. xv + 335 pp. Science 145.3629: 258–259.\nReview of Ford’s seminal book Ecological Genetics.\nFord, E. B. 1964. Ecological genetics. London: Methuen.\nIncludes chapter 11on chromosome polymorphism in Drosophila (“Drosophilosophy”; p. 105), chapter 14 on industrial melanism in moths, chapter 13 on mimicry in butterflies, chapter 9 on polymorphisms in snails, chapter 10 on mating systems in primroses, and topics that include genetic drift (chapter 3), sympatric evolution (chapter 4), and isolation and adaptation (chapter 15). Ford described research by many of his Oxford colleagues who are still well respected in ecology, evolution, and genetics, including Charles S. Elton, Ronald A. Fisher, and Julian Huxley. The importance of Ford’s book is reflected in four editions that were published from 1964 to 1975.\nFreeland, J., H. Kirk, and S. Petersen. 2011. Molecular ecology. 2d ed. Chichester, UK: Wiley-Blackwell.\nOriginally published in 2005. Provides a thorough overview of how molecular genetic information can provide insights into genetic diversity, gene flow, behavioral ecology, phylogeography, and conservation genetics. Includes a chapter on ecogenomics and quantitative trait loci (chapter 5) and is among recent books on ecological genetics. Although a useful resource, it is no longer up to date on the most recent genetic methods and techniques that are being applied to ecological questions.\nLowe, A., S. Harris, and P. Ashton. 2004. Ecological genetics: Design, analysis, and application. Malden, MA: Blackwell.\nPublication of this book followed the annual meeting of the Ecological Genetics Group in the United Kingdom (April 2000). It provides an excellent overview of molecular markers, analytical methods, applications, and interpretations of ecological genetics. Chapters cover sampling and genotyping methods (chapter 2), genetic diversity and differentiation (chapter 3), mating systems (chapter 4), phylogeography (chapter 5), and speciation and hybridization (chapter 6). The clear explanations of ecological genetics remain relevant, and the book’s main shortcoming is that it is dated because of the many developments in recent years that allow researchers to obtain vastly greater genetic data sets than in the past.\nReal, L. A., ed. 1994. Ecological genetics. Princeton, NJ: Princeton Univ. Press.\nFive leading experts of the time (Janis Antonovics, Michael Lynch, Montgomery Slatkin, Joseph Travis, and Sara Via) contributed overviews of topics that included gene flow, genetic differentiation, quantitative genetics, and plasticity. Many theoretical underpinnings remain valid, although the paucity of genetic data available in the late 1990s meant that the book did not receive as much attention as might be expected today.\nRowe, G., M. Sweet, and T. Beebee. 2017. An introduction to molecular ecology. 3d ed. Oxford: Oxford Univ. Press.\nThe first two editions of this book were co-authored by Beebee and Rowe in 2004 and 2008. This edition provides a very useful overview of methods that are used to characterize natural populations genetically and gain insight into processes as diverse as behavioral genetics, phylogeography, microbial ecology, and conservation.\nvan Straalen, N. M., and D. Roelofs. 2011. Introduction to ecological genomics. 2d ed. Oxford: Oxford Univ. Press.\nGenomics is a branch of genetics that incorporates information from the entire genome. The authors discuss the applications of genomics to ecological questions in the areas of stress response, population structure, genetic variation, adaptation, life histories, community structure, and nutrient cycling. Although genomics is still a stretch for many research labs, it is becoming increasingly accessible, and this book provides a useful overview of applications that will undoubtedly continue to grow.\nUsers without a subscription are not able to see the full content on this page. Please subscribe or login.\n- Adaptive Radiation\n- Ancient DNA\n- Behavioral Ecology\n- Canalization and Robustness\n- Character Displacement\n- Cognition, Evolution of\n- Constraints, Evolutionary\n- Convergent Evolution\n- Cooperation and Conflict: Microbes to Humans\n- Cooperative Breeding in Insects and Vertebrates\n- Cryptic Female Choice\n- Darwin, Charles\n- Disease Virulence, Evolution of\n- Ecological Speciation\n- Epigenetics and Behavior\n- Evidence of Evolution, The\n- Evolution and Development: Genes and Mutations Underlying ...\n- Evolution, Cultural\n- Evolution of Antibiotic Resistance\n- Evolution of New Genes\n- Evolution of Plant Mating Systems\n- Evolution of Specialization\n- Evolutionary Biology of Aging\n- Evolutionary Biomechanics\n- Evolutionary Ecology of Communities\n- Experimental Evolution\n- Field Studies of Natural Selection\n- Founder Effect Speciation\n- Frequency-Dependent Selection\n- Fungi, Evolution of\n- Gene Duplication\n- Gene Expression, Evolution of\n- Gene Flow\n- Genetics, Ecological\n- Genome Evolution\n- Geographic Variation\n- Group Selection\n- History of Evolutionary Thought, 1860–1925\n- History of Evolutionary Thought before Darwin\n- History of Evolutionary Thought Since 1930\n- Human Behavioral Ecology\n- Human Evolution\n- Hybrid Speciation\n- Hybrid Zones\n- Identifying the Genomic Basis Underlying Phenotypic Variat...\n- Inclusive Fitness\n- Innovation, Evolutionary\n- Kin Selection\n- Land Plants, Evolution of\n- Landscape Genetics\n- Landscapes, Adaptive\n- Language, Evolution of\n- Macroevolutionary Rates\n- Male-Male Competition\n- Mass Extinction\n- Mate Choice\n- Maternal Effects\n- Medicine, Evolutionary\n- Meiotic Drive\n- Modern Synthesis, The\n- Molecular Clocks\n- Molecular Phylogenetics\n- Natural Selection in Human Populations\n- Natural Selection in the Genome, Detecting\n- Neutral Theory\n- Niche Construction\n- Niche Evolution\n- Origin and Early Evolution of Animals\n- Origin of Eukaryotes\n- Origin of Life, The\n- Paradox of Sex\n- Parental Care, Evolution of\n- Personality Differences, Evolution of\n- Phenotypic Plasticity\n- Phylogenetic Comparative Methods and Tests of Macroevoluti...\n- Phylogenetic Trees, Interpretation of\n- Polyploid Speciation\n- Population Genetics\n- Population Structure\n- Psychology, Evolutionary\n- Punctuated Equilibria\n- Quantitative Genetic Variation and Heritability\n- Reproductive Proteins, Evolution of\n- Selection, Directional\n- Selection, Disruptive\n- Selection, Natural\n- Selection, Sexual\n- Selfish Genes\n- Sexual Conflict\n- Sexual Selection and Speciation\n- Sexual Size Dimorphism\n- Speciation Genetics and Genomics\n- Speciation, Sympatric\n- Species Concepts\n- Sperm Competition\n- Systems Biology\n- Taxonomy and Classification\n- Tetrapod Evolution\n- Trends, Evolutionary\n- Wallace, Alfred Russel"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:966b2621-b503-4dd3-b5d3-451e46187272>","<urn:uuid:7b610cdc-8631-4f6a-b3bb-84e4510e3407>"],"error":null}
{"question":"What are the key differences between phishing and scareware attacks in terms of their initial approach and end goals?","answer":"Phishing and scareware attacks differ in their initial approach and objectives. Phishing primarily attempts to acquire sensitive information like usernames and passwords through emails that request users to validate their credentials, often threatening account deletion. The end goal is to gain access to accounts and use them for sending spam. In contrast, scareware begins with pop-up ads or urgent emails claiming to detect viruses, directing users to download malicious software or enter credit card information. Scareware often serves as a gateway to more complex attacks like ransomware, aiming to either install malicious programs or obtain financial information for identity theft.","context":["FAQ - Phishing\nWhat is a Phishing?\nPhishing (pronounced \"fishing\") is a type of online identity theft. It is an attempt to acquire sensitive information. Here at the university they are usually attempting to get your Ball State username and password. There are many variations to a phishing e-mail, but the most common technique is asking you to provide sensitive information (username/password) by either replying to the message or clicking on a link and entering the information on a Web page. Review an example of a phishing e-mail.\nWhat should I do when I receive an e-mail from the Helpdesk, Webmail, IT, Internet Support Team, Security and E-mail asking me to validate my username and password so I don't lose access to my Webmail or any other technology resources?\nDelete the e-mail. Regardless of the SUBJECT or the message in your e-mail no one at the university is going to request you to validate your username and password. No one is going to ask you to enter your credentials so you don't lose your e-mail account or content. There is NO security incident taking place that requires you to validate your account.\nREMEMBER: THERE IS NO LEGITIMATE REASON FOR ANYONE AT THE UNIVERSITY TO REQUEST YOUR USERNAME AND PASSWORD! DELETE THE REQUEST.\nHow do I know if a message is a phishing scam?\nPhishing messages often:\na. Instruct you to supply your account information, including your username/password, by e-mail or by clicking on a link in the message and then entering the information via the Web. This is never a legitimate request.\nb. Have a \"From:\" line that sounds (and sometimes is) legitimate, but the message itself is vague.\nc. Contains a threat for not supplying the information, such as having your account deleted.\nd. Having spelling and grammatical errors. Legitimate messages aren't always perfect, but with careful reading many scam messages become obvious.\ne. Use a generic salutation rather than using your personal name.\nWhat to do if you think you may have responded to a phishing e-mail.\nChances are if you have responded to phishing e-mail you have compromised your Ball State account and will no longer be able to use your Ball State e-mail account. Contact the Technology HelpDesk. They are located in the Bracken Library (BL), room 101. You can reach them by phone at 765-285-1517 or by visiting their web site at www.bsu.edu/helpdesk.\nWhat risks are involved?\nIf you reply to a phishing scam with your username and password you have provided the scammer access to all your Ball State accounts. In addition your e-mail will be used to send thousands of spam e-mail to others on and off campus.\nWhat should I do to prevent my Ball State username/password from being compromised?\na. Be suspicious of messages requesting personal or account information.\nb. Be suspicious of messages threatening to suspend, terminate or close your e-mail account if you don't respond with the information requested.\nc. Do NOT respond to phishing scams at all.\nd. Do NOT click on links in a message.\ne. Do NOT click on links in a message.","What is Scareware\nScareware is a type of malware attack that claims to have detected a virus or other issue on a device and directs the user to download or buy malicious software to resolve the problem. Generally speaking, scareware is the gateway to a more intricate cyberattack and not an attack in and of itself.\nScareware attacks often begin with a pop up ad that appears to be from a legitimate security software provider or the computer’s operating system. If clicked, the scareware ad will direct the user to an infected website where they are given additional instructions to solve their so-called problem. This may include installing a new tool or program, running a computer scan, entering log-in credentials for more information or uploading their credit card information to continue the recovery process. This will often result in the user inadvertently and unknowingly downloading malicious programs, such as malware, ransomware, spyware, a virus or a Trojan onto their device.\nScareware attacks may also be conducted via email. In this type of attack, cybercriminals, also usually disguised as a fake antivirus software program, send a high-priority or urgent email that requests immediate action by the user. Clicking links within the email, which are often presented as ways to resolve the threat or scan the system, result in the user downloading and installing infected files, malicious code or malicious programs.\nScareware is often part of a multi-prong attack which incorporates social engineering techniques and spoofing to heighten the sense of urgency and drive the desired behavior. Scareware attacks, like many forms of malware attacks, are especially troublesome in that the scammer may gain access to the user’s account information or credit card details, which can put the user at risk of identity theft or other forms of fraud.\nScareware vs Ransomware\nScareware commonly falls into the category of a ransomware attack in that the cybercriminals’ end goal is to have the user download ransomware software. Ransomware is a type of malware that denies access to a user’s system and personal information, and demands a payment (ransom) to regain access.\nThat said, while some types of scareware lead to ransomware attacks, others are more of a nuisance. For example, these attacks may simply flood the screen with pop-up alerts without actually damaging files.\n2022 CrowdStrike Global Threat Report\nDownload the 2022 Global Threat Report to find out how security teams can better protect the people, processes, and technologies of a modern enterprise in an increasingly ominous threat landscape.Download Now\nWhat to do in the event of a suspected scareware attack\nIf you suspect that you are the victim of a scareware attack, it is important to act quickly and decisively to contain the problem. Follow these steps:\n- Disable WiFi or internet access from the affected device and disconnect it from any network.\n- If you are using a company-owned device, immediately contact your IT team for further instructions.\n- Otherwise, launch a full security scan using a reputable antivirus software provider to look for infected files and known threats, such as malware, ransomware, spyware, viruses and Trojans.\n- Restart the device in safe mode and run the sweep again.\n- If the scan reveals signs of infection, take it to a licensed and reputable computer specialist. Do not use the computer or mobile device or allow it to connect to a network, even if it appears to be operating normally.\nIn the event of a scareware attack, users should also take extra steps to safeguard against potentially compromised information. This may include:\n- Changing passwords or other long-in credentials\n- Performing a scan on other personal devices to ensure they were not inadvertently compromised\n- Requesting new credit cards from your bank or financial institution\n- Periodically checking your credit report to ensure you were not the victim of fraud or identity theft\nCan scareware be removed?\nThe best way to prevent a scareware attack as an individual user is through prevention. By recognizing the signs of a scareware scam, it is possible to avoid these cyber threats.\nIt is important to keep in mind that reputable antivirus software programs typically do not notify customers of a security incident via pop up ad—and none will require the user to share log-in credentials or credit card information within a pop up window.\nMany of the tips offered to avoid scareware scams are similar to the best practices used to prevent malware and spoofing attacks:\n- Never click links or download files from pop up ads or unfamiliar email senders.\n- Install a pop up blocker and spam filter which will detect many threats and even stop scareware pop up ads and infected emails from reaching your device.\n- Invest in cybersecurity software from a reputable antivirus vendor and ensure all installations are up to date.\n- Log into your account through a new browser tab or official app—not a link from a scareware alert, email or text message.\n- Only access URLs that begin with HTTPS.\n- Never share personal information, such as account numbers, passwords or credit card details, via phone, email or unsecured site.\n- Use a password manager, which will automatically enter a saved password into a recognized site (but not a spoofed site).\n- Enable two-way authentication whenever possible, which makes it far more difficult for attackers and scareware scammers to exploit.\nPreventing scareware attacks at the enterprise level\nAt the enterprise level, protecting against scareware attacks will be similar to protecting against malware, ransomware and other cybersecurity threats. These attack techniques are constantly evolving, making protection a challenge for many organizations. Follow these best practices to help keep your operations secure:\nTrain all employees on cybersecurity best practices\nEmployees are on the front line of your security. Make sure they follow good hygiene practices — such as using strong password protection, connecting only to secure Wi-Fi and being on constant lookout for phishing — on all of their devices.\nKeep the operating system and other software patched and up to date.\nHackers are constantly looking for holes and backdoors to exploit. By vigilantly updating your systems, you’ll minimize your exposure to known vulnerabilities.\nUse software that can prevent unknown threats.\nWhile traditional antivirus solutions may prevent known scareware and ransomware, they fail at detecting unknown malware threats. The CrowdStrike Falcon® platform provides next-gen antivirus (NGAV) against known and unknown malware using AI-powered machine learning. Rather than attempting to detect known malware iterations, Falcon looks for indicators of attack (IOAs) to stop ransomware before it can execute and inflict damage.\nContinuously monitor the environment for malicious activity and IOAs.\nCrowdStrike® Falcon Insight™ endpoint detection and response (EDR) continuously monitors endpoints, capturing raw events for automatic detection of malicious activity not identified by prevention methods and providing visibility for proactive threat hunting.\nFor stealthy, hidden attacks that may not immediately trigger automated alerts, CrowdStrike offers Falcon OverWatch™ managed threat hunting, which comprises an elite team of experienced hunters who proactively search for threats on your behalf 24/7.\nIntegrate threat intelligence into the security strategy.\nMonitor systems in real time and keep up with the latest threat intelligence to detect an attack quickly, understand how best to respond, and prevent it from spreading. CrowdStrike Falcon X automates threat analysis and incident investigation to examine all threats and proactively deploy countermeasures within minutes."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:665bed73-a4b6-4bc7-bea6-782231a4c039>","<urn:uuid:f4270cdb-3192-4a78-ab8a-6894e86f4dc1>"],"error":null}
{"question":"How do LED lights affect plant growth and CO2 requirements in controlled environments, and what adjustments are needed when transitioning from HID lighting?","answer":"LED lights affect plant growth by providing lower light intensities and less waste heat compared to HID lamps. This reduced waste heat leads to cooler leaf temperatures and lower evapotranspiration rates, resulting in reduced water requirements. The combination of low light intensity under the same air temperature setpoint reduces plants' CO2 fixation rate, meaning lower CO2 levels and less enrichment is required. When transitioning from HID to LED lighting, several adjustments are needed: increasing air temperature by 1-3 degrees Fahrenheit to compensate for reduced radiant heat, modifying irrigation rates and frequency, and potentially adjusting nutrient concentrations. Research shows that LED treatments can provide similar or better crop growth, harvest, and quality metrics compared to HPS while delivering 15% less supplemental light overall.","context":["One of the projects of Dr. Youbin Zheng, associate professor in the University of Guelph’s School of Environmental Sciences, has the goal of determining the best light spectral combinations and intensities for indoor/warehouse and greenhouse microgreen production. This study, supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) and Greenbelt Microgreens, involves investigation of both yield and quality (for example, nutritional value and post-harvest shelf life).\nZheng is also developing a feedback control system to manage greenhouse lighting in order to save energy and maximize crop production. The work is supported by the Ontario Ministry of Agriculture, Food and Rural Affairs (OMAFRA) and Heliospectra.\n“Our recent trial compared the use of a conventional HPS control strategy with real-time feedback-controlled LEDs (which are dimmable) for the production of cut gerbera in Ontario during the normal supplemental lighting season of November to March,” notes Zheng.\n“Results showed that the LED treatment had the same or better crop growth, harvest and quality metrics compared with HPS, while also delivering 15 per cent less supplemental light overall to the crop throughout the season.”\nDYNAMICALLY CONTROLLED LEDS\nZheng says these results provide a compelling argument for the use of dynamically controlled LEDs as a supplemental light source for greenhouse production in the darker months in northern latitudes.\nAnother of his studies involves using LEDs to control plant height. Bedding plant growers often produce hanging baskets above the main lower crop level (bench or floor) in the same greenhouse to maximize use of production space, he notes. This practice reportedly can result in stretched plants at lower crop level, commonly considered a result of a combination of high-density plantings, competition for available light and changes in spectral composition from passing through an upper canopy.\n“Our group has been investigating how to use different light spectral combinations and timing of light treatment on the morphological changes of several bedding plant species (e.g., petunia, marigold, geranium),” he explains, “to investigate whether we can control bedding plant height and flowering without using chemicals.”\nCUT FLOWER PROJECTS\nSupported by the International Cut Flowers Growers Association and Lumigrow, Zheng has also recently investigated the use of LEDs for producing cut flowers during the darker months in Ontario over three growing seasons.\n“The first year we compared HPS and LED supplemental lighting treatments (at the same crop-level intensity and photoperiod) to produce three cultivars of cut gerbera,” he says. “The LED treatment provided similar or better production, harvest quality and postharvest shelf-life indices compared with HPS treatment.”\nIn the second year, a single cultivar of cut snapdragons was produced using four different canopy-level supplemental LED intensities, and in the third year, two cultivars of cut gerbera (one starting in plug-stage and the other already in flower) were grown under five different supplemental light treatments.\nThe results of both of these trials can be used to help growers determine the optimum lighting setup for their specific production systems.\nZheng and his colleagues are also actively investigating the responses of different plant species to different LED light spectra and spectral combinations to unlock some of the mechanisms involved. This will guide the development of different lighting recipes for specific functions such as increasing or decreasing height, branching, increasing biomass production, promoting flowering and increasing certain nutrients in edible crops.\nYEAR-ROUND VEGETABLE CROPS\nOne research project led by Dr. Xiuming Hao is focussed on economical year-round vegetable production.\n“In Ontario and most places in Canada, we are short on light from October to March,” says Hao, who is based at the Agriculture and Agri-Food Canada (AAFC) centre in Harrow, Ontario. “Growers want to plant tomatoes, cucumbers and peppers in September or October in order to harvest around Christmastime to get the best price. Supplemental lights can achieve this, but variety, nutrition and climate control are important factors affecting plant growth and fruit production and need to be adjusted.”\nThis and Hao’s other projects are being funded by AAFC Peer Review Program, AAFC’s AgriInnovation Program and the Ontario Greenhouse Vegetable Growers.\nHao and his colleagues have already found that lower intensity over a longer duration costs less in capital expenses with fewer light fixtures required per unit area. However, crops exposed to longer periods of light can be pushed beyond their limits; the yellow spots of chlorosis will appear on leaves and there can be little yield increase.\n“We’ve developed a solution where we lower the temperature right after the supplemental lighting was shut-off during the darkness so the crop has a better rest and is not so stressed by the increased duration of light,” Hao explains. “A temperature of 13.5 to 14 C is suitable as a minimum for tomatoes, 13 C for cucumbers and 15.5 to 16 C for peppers, to maintain quality. There are also small differences among varieties.”\nNot only does the temperature drop give the crop a good rest, but while it occurs, leaves cool faster (due to their large surface area in relation to volume) than the fruit (larger volume). This causes the plant to focus on growing the fruit, resulting in faster fruit growth and increased yields.\nHao notes that it takes one to two hours to lower the temperature depending on the temperature outside, and 30 minutes to one hour to bring it back up.\n“You only want the temperature to stay lowered for about a half-hour to one hour,” he says, “just giving the plants a little shock.” In addition to ensuring better yield, lowering the temperature also saves growers money on heating.\nView the embedded image gallery online at:\nFAR-RED LED LIGHTING\nHao is also studying far-red LED light, wavelengths in the 720 to 750 nm range out of the visible range of humans. Applying far-red light at the top of plants is interpreted by them to mean they are being shaded, and in response stems and leaves stretch to reach for more light.\n“Most greenhouses currently use HPS lights that produce lots of heat and the plants are very compact,” Hao notes. “However, with bushy plants, the light is being blocked by leaves and not utilized efficiently by the plant. With far-red LED, you get a longer plant that can intercept and absorb more light, and can achieve higher yields in the early production period in tomatoes and sweet peppers. Later on, the canopy is thicker and there is no effect.”\nCucumbers do not show any effect, he explains, because they grow very quickly in comparison.\nHao is also in the first year of a three-year comprehensive study looking at the use of LED lighting to improve crop growth, fruit yield, quality (including anti-oxidants) and pest control efficiency on tomatoes, peppers and cucumbers. Team members include Dr. Rose Labbé at Harrow (biocontrols and pests); Dr. Aiming Wong of AAFC London Research Centre (how LEDs might help greenhouse vegetable crops increase tolerance or resistance to virus disease); Dr. Rong Cao of AAFC’s Guelph Research Centre (antioxidant levels in fruit); and University of Guelph scientist Dr. Bernie Grodzinski (how LEDs can speed up the movement of starch from the leaves to the fruit for increased yield).\n“Red light is good for growth but blue light and UV-A light is better for fruit quality,” Hao explains. “With LED lighting, you can change the wavelengths at various heights. With red light and far-red light at the top and blue light in the middle, we try to increase both fruit yield, and quality and antioxidants at the same time.”\nFor fruiting greenhouse vegetables such as tomatoes, the translocation or movement of photo-assimilate (sugar etc.) from leaves to fruit is very important. Jason Lanoue (a PhD student at the University of Guelph under the supervision of Grodzinski and Hao) is studying various spectra of LED and developing LED light recipe to promote the translocation of photo-assimilates from leaves to fruit for increasing fruit yield.\nAt Wageningen University in the Netherlands, Dr. Anja Dieleman has been working on LED greenhouse lighting for five years.\n“Several years ago, one of my colleagues, Tom Dueck did multiple trials in which he compared LEDs with HPS and hybrid systems (HPS+LED inter-canopy lighting; LED + LED inter-canopy lighting) in tomatoes,” she notes, “with the result that these systems were comparable in crop production.”\nDuring the last few years, within the framework of the EU HI-LED Project (www. Hi-led.eu), Dieleman and colleague Esther Meinen have been studying the effects of light colours on young pepper and tomato plants. They found two hours of green light at various times of day all provided taller plants with more open canopy, with light better able to penetrate the crop. These results show that green light provides plant ‘stretching’ effects similar to far red light. The effect of other colours was similar to white light.\nDieleman and her colleagues have also developed lighting recipes in which plants received a few hours of blue or green light in the morning followed by red light during the rest of the day (in addition to normal solar greenhouse light). Several hours of blue light at the start of the day led to an eight per cent increase in tomato production.\n“In the last few years we have been working on adding far-red light in the cultivation of tomatoes, with striking results,” Dieleman adds. “Adding far-red light favours assimilate partitioning to the fruits, at the expense of the leaves, which led to a five to 20 per cent increase in tomato fruit production depending on the variety.”\nDieleman and her colleagues are also still working on the design of low carbon footprint production systems for tomato production, based on LED lighting and varieties that suit this lighting system best.\nTreena Hein is a freelance writer in Ontario and a frequent contributor to Greenhouse Canada.","Understanding how carbon dioxide and vapor pressure deficit impact cannabis growth can save resources and lead to healthier, more productive crops\nAs the price of cannabis decreases and the cost of doing business increases, growers are actively looking for ways to optimize their use of energy, water and other resources, while maintaining the same level of productivity or even increasing output. At the same time, regulators and utility companies are concerned about the cannabis industry’s impact on regional power supplies, local ecosystems and watersheds, and waste stream management.\nFor those entities outside the sphere of cannabis cultivation and horticultural science, the solution seems obvious: regulate or incentivize more energy-efficient technologies, such as LED lighting, variable refrigerant flow HVAC systems, or other ready-for-market alternatives that have been successfully implemented on commercial and residential projects. Unfortunately, it’s not that simple, because changing an environmental input can have a profound effect on how the plant responds to other inputs, ultimately affecting how the grower manages crop productivity and profitability.\nBut how do energy-efficient technologies and strategies impact plant productivity? Are these technologies the right solution for maximizing efficiency and reducing impacts on utilities and the environment? Unfortunately, these technologies do not live in a vacuum.\nWhat is VPD?\nVapor pressure deficit (VPD) is the difference between the amount of water in the air and the maximum amount of water the air can hold for a given temperature (saturation). It is usually measured in kilopascals (kPa) or bars. A low VPD is indicative of high moisture content in the air and a high VPD is indicative of low moisture content.\nThe surface of leaves is commonly assumed to be saturated with water. Plant stomatal opening is directly related to VPD. If VPD is too high, stomata will close to conserve water. If it’s too low, stomata may be fully open, but evapotranspiration will be slow and nutrient uptake will be impacted. Both conditions can cause wilting, leaf tip burn and other crop maladies. When VPD is managed correctly, plants will transpire freely, move nutrients readily to cells and maximize CO2 uptake.\nPlant Responses to Environment\nAs a C3 plant (see sidebar), cannabis is particularly well-suited for temperate climates, which are characterized by relatively small swings in daily and seasonal temperatures and abundant rainfall.\nRegardless of type, all green plants are autotrophs, meaning they use the chemical reactions of photosynthesis to self-generate the energy required to grow and reproduce. To support photosynthesis, all plants require the same basic environmental inputs: light, water, nutrients and CO2.\nFor flowering plants, such as cannabis, a change in one or more environmental factors can shift it from a vegetative state (leaf production) to a reproductive state (flower and fruit production). During its vegetative state, cannabis likes long, sunny days to maximize photosynthesis and chlorophyll production, resulting in big, beautiful green leaves. As the sun wanes after the summer solstice (June 21), the plant recognizes that resources will become more scarce, triggering the plant to move toward a reproductive state.\nBy the time days and nights reach a 12-hour split, the cannabis plant is in full flowering mode. Greenhouse growers extend their growing season by using a combination of light-dep curtains and supplemental lighting to manipulate the photoperiod (day length) and evoke the flowering response, regardless of time of year. Indoor growers do the same thing by switching from an 18-hour photoperiod to a 12-hour photoperiod when they want to induce flowering.\nAlthough light is essential for plants to photosynthesize and sets a timer for reproducing plants, it is only one of many environmental factors that plants require to grow. Air temperature affects the metabolic rate of plants and their ability to synthesize CO2 and nutrients into primary compounds (think chlorophyll) and secondary compounds (think terpenes). The relative humidity at a given air temperature impacts the opening of leaf stomata, which are responsible for the exchange of gases, specifically CO2 and water vapor. The larger the stomatal opening, the more CO2 can come into the leaf and the more water vapor can evaporate from it (via evapotranspiration).\nBecause the combination of air temperature and relative humidity (commonly referred to as vapor pressure deficit or VPD) affects the rate of evaporation from the leaf, it also affects the rate at which water is taken up by the roots and delivered through the plant. Therefore, if the stomatal opening is large, the plant will need more water to keep up with the rate of evaporation. If the roots do not receive enough water, the plant will respond by wilting. Because plants also use evapotranspiration to transport nutrients from the water to plant cells, if water deficiencies persist, the plant will begin to exhibit nutrient deficiencies, such as leaf tip burn (calcium deficiency). Plants can even lose their ability to ward off pests and pathogens due to a depressed immune system.\nAlternately, when stomatal openings are smaller (due to too either too much or too little moisture in the air), evapotranspiration rates will be lower and plants will require less water. But if too much water is supplied to the plant under this condition, the roots can become starved for oxygen, causing them to exhibit behavioral signs similar to under-watering.\nCO2 enrichment is often used by indoor and greenhouse growers to increase the rate of photosynthesis and plant growth. Typically, growers target CO2 levels between 1,200 and 1,500 parts per million in the production room. Although there have not been academic studies specifically researching the optimal CO2 level for cannabis cultivation, research into other similar horticultural crops reveals that these target levels are likely too high based on growers’ targets for other environmental factors, specifically air temperature and light intensity.\nIn general, the rate of CO2 uptake has a positive relationship with air temperature and light intensity, such that plants uptake CO2 at a higher rate when both air temperature and light intensity are increased. But there is an upper limit, at which no matter how high the temperature or light level, the plant simply cannot use more CO2. For most plants, this upper limit comes around 1,200 to 1,500 parts per million. Additionally, when CO2 levels are elevated, the plant uses more resources to support leaf development, which is great during the veg stage, but could reduce yields if continued during flowering.\nLEDs to the Rescue?\nA lot of attention has been given to the energy-saving potential for replacing high-intensity discharge (HID) lamps with LEDs. Utility companies are especially interested in this technology, as they witness steep spikes in electricity demand across their service areas where cannabis cultivation facilities are permitted.\nGrowers are also looking for solutions to reduce operating costs, with energy use commonly the largest expense after labor. Because LED lamps are more efficient at converting electricity into light, they produce less waste heat, thereby reducing the HVAC cooling requirements. It is this cascade effect of simultaneously reducing the electricity needs for both lighting and HVAC that causes many people to consider LEDs the “holy grail” of energy-efficiency opportunities.\nBut plants respond differently to different environmental conditions. Most people recognize that LEDs generally put out lower light intensities than HID lamps, even with full-spectrum ratings, and adjustments are made to lighting layouts and floor plans to compensate for the plants’ response to different light inputs. Rarely, however, are adjustments made to the room or other systems to also compensate for the reduction in waste heat.\nPlant leaves are essentially black bodies, similar to our hands, in that they capture radiant energy from the surfaces around them. Radiant energy comes in the form of light and heat. Any spectrums of radiant energy outside the photosynthetically active radiation (PAR) spectrum — about 400 to 700 nanometers — is considered “waste heat.” This radiant heat affects leaf temperature, metabolic activity in the cells and the rate of evapotranspiration.\nIf you started growing in a facility outfitted with HID lamps, you have likely installed HVAC equipment and developed crop management strategies to accommodate this “high waste heat” condition, including finding the right balance of air temperature, relative humidity, irrigation rate and frequency, and fertilizer mix. But under a “low waste heat” condition, plants respond differently to these variables. Because they are being irradiated by less heat, the leaves will be cooler and evapotranspiration rates will be less, effectively reducing the plant’s thirst for water. And because there is less heat and moisture generated in the room, there will be a smaller requirement for cooling and dehumidification. Additionally, the combination of low light intensity under the same air temperature setpoint will reduce the rate at which plants fix CO2. Therefore, lower CO2 levels and less enrichment is required.\nBut that also raises other questions. If plants need less water and have a slower rate of metabolism under LED lights, does that also mean they need less nutrients, more concentrated nutrients, or a different balance of nutrients? Can your existing HVAC equipment handle the lower cooling and dehumidification loads? Can other environmental inputs be modified to achieve the same growth rate and produce the same phytochemical profile you’ve become accustomed to under an HID light environment?\nOne strategy to combat the change in metabolic activity is to increase the air temperature. If the plant isn’t going to receive radiant heat from the lamps, they can still receive convective heat from the air. It doesn’t take much to facilitate heat transfer from the air to the leaf — just 1-3 degrees Fahrenheit higher air temperature than leaf temperature.\nBy increasing the air temperature, you can further reduce the HVAC cooling requirements, though you may still need to dehumidify at a relatively high rate because plants transpire harder under higher temperatures. One potential drawback, if you use the same HVAC equipment under this new light condition, is that it could be too large for the needs. This could cause it to cycle on and off more frequently, increasing the risk of frozen cooling coils and shifting the equipment’s balance from heat to moisture, a condition most HVAC units aren’t adept to handle.\nUltimately, every environmental setpoint you target — light level, VPD, CO2, etc. — affects how the plant responds to other environmental factors in the production space. By understanding these plant-environment interactions, growers can make better decisions about how to manage their crop from seed (or clone) to flower, including what equipment to use, what environmental levels to target and when, and ultimately how to maximize yield and quality by managing resource inputs and minimizing waste.\nIf you are thinking about switching to LED lamps, be patient and recognize that everything you’ve known about growing a crop under HID lighting could change dramatically. But that doesn’t mean it’s not worth it. If you can discover the new balance between environmental factors, crop management strategies and technologies, LED lamps could be one solution to a profitable and more sustainable future.\nNadia Sabeh is the president and founder of Dr. Greenhouse (www.doctorgreenhouse.com), an engineering consulting firm focused on designing HVAC systems for indoor farms. She is a licensed mechanical engineer in California, received her Ph.D. in agricultural and biosystems engineering from the University of Arizona’s Controlled Environment Agriculture Center, and has nearly 20 years of experience helping clients maximize crop productivity by translating the plants’ needs into the design and operation of the facility."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:dcb15ea8-8d52-4dce-95ee-6d6975abad49>","<urn:uuid:90397e2d-d912-4ba8-abb7-79a6f7a2ea7b>"],"error":null}
{"question":"What are the key differences between the habitat requirements of the Florida Scrub Lizard and the Kangaroo Paw in terms of soil and environmental conditions?","answer":"The Florida Scrub Lizard requires well-drained, deep sand soil and is found in open sandy areas bordering Sand Pine scrub and sandhill associations. It needs quite high temperatures for peak activity. In contrast, the Kangaroo Paw requires well-drained soils and plenty of sunlight, and can grow in diverse locations including roadsides, eucalypt forests, river banks, and around swamps and shallow waters. The Kangaroo Paw is notably more adaptable, being able to grow almost anywhere in its arid native region of Western Australia.","context":["The general distribution of the Florida Scrub Lizard is restricted to Florida, specifically in Peninsular Florida. It is distributed in scattered sand pine and rosemary srcub areas from Marion and Putnam counties southward to Dade county (Carr 1959). The distribution of the Scrub Lizard is highly disjunct, probably due to the patchy distribution of suitable habitat. Most can be found in the extensive Sand Pine scrubs in the Ocala National Forest in north-central Florida. Atlantic coast populations can still be found in Brevard, Indian River, St. Lucie, Martin, Palm Beach, and Broward Counties. Populations of Scrub Lizards along the southwestern Gulf Coast of Florida in Lee and Collier counties may still exist (Moler 1992).\nThis \"heliothermic\" lizard prefers open sandy areas bordering Sand Pine scrub and sandhill associations.could be described as a forest-edge species. A dense crown of Sand Pine comprises the 'overstory' of the scrub habitat. The 'understory' is composed primarily of scrub oaks (Quercus chapmanii , Q. myrtifolia , and Q. virginiana) , while much of the ground is covered with lichen and leaf litter. Sandhill habitats are dominated by Longleaf Pine (Pinus palustris) and Turkey Oak (Q. laevis) . Wiregrasses are characteristic of this plant association. Rosemary occurs in both types of habitats, especially where fire is uncommon. Both plant associations occur on well drained, deep sand soil (Moler 1992).\nSceloporous woodi is known by its rough, overlapping scales, which usually number 40 or more from the occiput to the base of the tail, and by its clear-cut dark lateral stripe. It also has fermoral pores which number from 14-20, and a brownish color, with a conspicuous dark band on the side from the neck to the base of the tail. On the back of the lizard is a series of 8-10 more or less distinct wavy bars. There is an unmarked mid-dorsal area. These vertical markings are stronger in females than in males. Males have a conspicuous blue patch bordered with black on each side of the throat, and a similar blue area with a less heavy black border on each side of the belly. Females are generally white on the ventral area except for weaker blue patches like those of males. Average Snout-Vent Length is about 1.75 inches (Carr 1959).\nFemales reach sexual maturity around 47mm SVL(snout-vent length). Male SVL is probably slightly smaller at maturity. Courtship and mating occur from late March through June. For females in their second reproductive season, vitellogenesis begins in March, and oviposition of the first clutch occurs around mid-April. Smaller females that are in their first reproductive season may begin to develop follicles somewhat later, in April or May. Females that have not reached maturity by March may mature and yolk a clutch in mid- to late summer. It has been estimated that the largest females could lay up to five clutches in a single reproductive season under optimal conditions; however three clutches in a season is more likely. No females are gravid after August. Average clutch size is four, and clutches range from 2-8 eggs. Hatching occurs from late June until early November. Hatchlings reach sexual maturity in 10-11 months (Moler 1992).\nThe Scrub Lizard requires quite high temperatures to reach the peak of their activity, at which the males bob their heads very frequently, moving them up and down with incredible speed. If a Scrub Lizard finds itself \"bobbed at\" in this way, it can react in a variety of ways. A weaker male will flee immediately, but a female usually holds her ground. If the bobbing male, seeking to mate, approaches an unreceptive female, she arches her back like a cat, distends her body, and hops to the side with little jumps. A male threatens a rival by turning broadside and flattening his body so as to present the greatest area and display most prominently the glowing blue of the belly. When a human approaches, the spiny lizard usually remains motionless until the human comes within the \"flight distance\" of the lizard; then it flees in an instant. Captured animals defend themselves by raising their spiny scales. If a person seizes them awkwardly, they are very likely to autotomize, or lose, their tails (Grzimak 1975).\nThe Florida Scrub Lizard is a \"sit and wait\" predator that eats ants, beetles, spiders, and other small arthropods.\nBecause this species consumes insects, spiders, etc. on a daily basis, it effectively keeps the population of these small arthropods \"in check.\"\nAn increase in urbanization and conservation of scrub patches to citrus groves has resulted in a loss of habitat for species which must make their homes in scrub areas to survive (Mosesso, 1996).\nVery little is known about the demography, life history, ecology, and behavior of the Florida Scrub Lizard. There is extraodinarily little published on the habits and life history of this interesting species (Smith 1946). The large populations of lizards found in the Ocala National Forest offers excellent opportunities for research in these areas. Preservation of important scrub habitats would serve to protect a number of scrub species in addition to the Srcub Lizard. An immediate effort should be made to prevent the Gulf Coast population from going extinct (Moler 1992).\nhas been referred to by many common names such as: the Pine Scrub Lizard, the Rosemary Lizard, the Scrub Lizard, as well as the Florida Scrub Lizard.\nAnna Liza Antonio (author), Cocoa Beach High School, Penny Mcdonald (editor), Cocoa Beach High School.\nliving in the Nearctic biogeographic province, the northern part of the New World. This includes Greenland, the Canadian Arctic islands, and all of the North American as far south as the highlands of central Mexico.\nFound in coastal areas between 30 and 40 degrees latitude, in areas with a Mediterranean climate. Vegetation is dominated by stands of dense, spiny shrubs with tough (hard or waxy) evergreen leaves. May be maintained by periodic fire. In South America it includes the scrub ecotone between forest and paramo.\nthe area in which the animal is naturally found, the region in which it is endemic.\nscrub forests develop in areas that experience dry seasons.\nThe term is used in the 1994 IUCN Red List of Threatened Animals to refer collectively to species categorized as Endangered (E), Vulnerable (V), Rare (R), Indeterminate (I), or Insufficiently Known (K) and in the 1996 IUCN Red List of Threatened Animals to refer collectively to species categorized as Critically Endangered (CR), Endangered (EN), or Vulnerable (VU).\nCarr, .., .. Goin. 1959. Reptiles, Amphibians, and Freshwater Fishes of Florida. Gainesville: University of Florida Press.\nCowley, .. 1997-2000. \"NSiS: Florida Wildlife - Iguanids\" (On-line). Accessed \"February 8, 2000\" at http://www.nsis.org/wildlife/rept/liz-iguanid.html.\nGrzimek, .. 1975. Grzimek's Animal Life Encyclopedia. New York: Van Nostrand Reinhold Company.\nMoler, .. 1992. Rare and Endangered Biota of Florida (Vol.3). Gainesville: University of Florida Press.\nMosseso, J., T. Harlow. 23 October 1996. \"National Biological Service Funds Study of Rare Florida Lizard\" (On-line). Accessed March 5, 2000 at http://biology.usgs.gov/pr/1995/8-31a.html.\nPope, .. 1960. The Reptile World. New York: Alfred A. Knopf.\nScheper, J. 1997-1999. \"The Florida Scrub - Animal Gallery\" (On-line). Accessed March 1, 2000 at http://www.floridata.com/tracks/scrub/animals/menu_ani.htm.\nSmith, .. 1946. Handbook of Lizards. New York: Comstock Publishing Company.","The iconic Kangaroo Paw with its vivacious, bright and almost iridescent flower shines through the native foliage. It is a lively presence here where much else is muted.\nThe Kangaroo Paw calls Western Australia home and is endemic to this region. Find it growing wild along roadsides, in eucalypt forests, along river banks, around swamps and shallow waters…nearly anywhere it can take root. Since it requires well-drained soils and plenty of sunlight, that is nearly everywhere in this arid and sunny state.\nDifferent species flower in different months from July to December. And, it is responsible for creating a colourful and pleasing display along with other bright and cheery wildflowers that add their own signature hues to the scheme.\nIts remarkable features – the velvety stem and flower, the vibrant colours and the unusual paw-like flower head – make it popular and highly desired in gardens across the world. And, it is an export favourite being frequently shipped to the USA, Japan and Israel.\nThe unique red-green flowers are alluring. But a stunning palette of pink, yellow, orange and green is also available. And, Kangaroo Paw is perfect for both fresh and dried flower arrangements.\nIt stands as a testament to our dynamic yet delicate environment. Perhaps this is why it is chosen as a floral emblem for our state of Western Australia, and it even appears on many stamp issues.\nThis isn’t nearly enough. There is more to this amazing native flower.\nDid you know that there are 11 species and 13 recognised sub-species of Kangaroo Paw, with one species holding a genus all to itself? Did you know that Kangaroo Paw is pollinated by birds and can encourage native bird activity in your garden? Or, how about the fact that this dazzling, beautiful flower is resilient, strong and hardy beyond imagination.\nRead on as we rediscover a gem and discuss its fascinating aspects.\nA bit of history\nJacques-Julien Houtou de La Billardière, naturalist and botanist, was responsible for first describing Kangaroo Paw in 1792. He made the observation when the ship he was aboard, “d’Entrecasteaux’s ship Espérance”, made a stop for repairs in Esperance during its expedition to Australia.\nHis collections contained more than 4000 plants, of which three-quarters were previously unknown. And, valuable descriptions of the lands and peoples that the expedition visited, including detailed accounts of the ways of the Aboriginal peoples.\nThe Kangaroo Paw blooms are fan-like clusters attached to long stems. Each flower has a bright red ovary and unusual paw-shaped petals. Seeing this he named it Anigozanthos rufus.\nAnigozanthos – from the Greek anises, meaning unequal, and anthos, meaning flower, referring to the unequal perianth lobes of the flower. An allusion to the division of the flower into six unequal parts.\nSome lesser known facts\nOther than the Red and Green Kangaroo Paw a few other species are common. These include the Green Kangaroo Paw, which comes in a range of colours – from lemon yellow to emerald green, and Catspaw, which has smaller brightly coloured flowers.\nThe Aboriginal people call Kangaroo Paw Nollamara or Kurulbrang or Yonga Marra in the local Nyoongar language. They use it in preparing traditional medicine and the plant is of considerable significance to them.\nHere are other incredible facts:\n- The Red and Green Kangaroo Paw is the floral emblem of Western Australia since 1960.\n- Flowers have no fragrance. And the furry flower and stalk can irritate skin and eyes on constant contact.\n- Kangaroo Paws are pollinated by a variety of native birds, including honeyeaters and wattlebirds.\n- The stalk of the plant is sturdy enough to perch birds that are attracted to its bright colours. The shape and position of the pollen-bearing anthers enable pollen to deposit on the perching birds.\n- For feeding birds – As the bird pushes its beak into the tubular perianth to feed on the nectar, it brushes its head against the stamens which deposit pollen. Pollen is then transferred from flower to flower as the birds fly about.\n- Kangaroo Paw forms a rhizome or modified stem underground. The rhizome grows to about 5cms in diameter and is responsible for making the plant resistant to fire and drought. The plant can often survive harsh conditions and re-sprout when circumstances change to become favourable again.\n- Kangaroo Paws have tuberous roots which contain significant amounts of stored starch. These roots are eaten by Nyoongar people, similar to the way some orchids and lily species are too. Root tubers formed an important part of the traditional Nyoongar diet, and for this reason, it is possible that the roots of Kangaroo Paws were collected and gathered in large quantities.\n- The Red and Green Kangaroo Paw only occurs naturally in southwest Western Australia. Found commonly around Shark Bay to Scott’s River and at Mt. Barker – Manjimup, along the Murchison River, Busselton, Lake Muir, and King’s Park near Perth.\nKangaroo Paws at The Wetlands Centre\nIf you are considering planting a native garden, Kangaroo Paw is indispensable. You can start by planting at least a few varieties in your garden for that dazzling effect.\nIf you come down and take a stroll in the perimeter of Bibra Lake or North Lake you will find the familiar sight that is the Red and Green Kangaroo Paw. You will find them blossoming in our waterwise garden too. They thrive rather well in their natural environ here. And our team of staff and volunteers do a great job of caring for the native plant life in and around this area.\nTo learn more about our amazing native plants and animals, and to help us make sustained and continued efforts towards conserving and rehabilitating our wetlands, come join us at The Wetlands Centre.\nWe look forward to seeing you there!\nYoung, R. (2019). Paws for Thought – Wildflower Society of Western Australia. [online] Wildflowersocietywa.org.au. Available at: http://www.wildflowersocietywa.org.au/advice-and-tips/paws-for-thought/ [Accessed 1 Mar. 2019].\nKangaroo Paws – Anigozanthos – Australian Plant Information. (2019). Anbg.gov.au. Retrieved 3 March 2019, from https://www.anbg.gov.au/anigozanthos/\nLa Billardière, Jacques-Julien Houtou de (1755–1834), Australian Dictionary of Biography, National Centre of Biography, Australian National University. http://adb.anu.edu.au/biography/la-billardiere-jacques-julien-houtou-de-2316/text3007, published first in hardcopy 1967, accessed online 3 March 2019.\nKangaroo Paw Plants, Flowers – Care, Prune and Growers Guide (2018). (2019). Ozbreed Plants. Retrieved 4 March 2019, from https://www.ozbreed.com.au/velvet-kangaroo-paws/\nStewart, A. (2019). Growing Kangaroo Paws. [online] Gardeningwithangus.com.au. Available at: https://www.gardeningwithangus.com.au/growing-kangaroo-paws/ [Accessed 4 Mar. 2019]."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:be3f2b14-16c3-4155-b9a4-3a17ddd37f5a>","<urn:uuid:7bc4ff19-bdbb-483e-b52a-89de80333633>"],"error":null}
{"question":"Looking at future food needs: can vertical farming scale up, and would Mars contamination affect potential space agriculture?","answer":"Vertical farming currently faces scaling challenges - while it works at community levels, it has high energy requirements and investment costs that make larger-scale implementation difficult. Regarding Mars, contamination is a serious concern as Earth bacteria could survive on Mars and potentially interfere with future agriculture efforts - NASA found that 11% of bacterial strains on Curiosity could survive under multiple extreme conditions, and 19 organisms could process Mars substrates like perchlorate and sulfate.","context":["NASA: Admits Contaminating Mars…the question is it still in existence?If so what has it mutated into? Is it humanly possible to visit a planet and leave no trace? Can NASA or anyone fully decontaminate rovers, and all the instruments and equipment on it? The answer is “no” based on the testing that has been done.\nScientists worry about Earth bacteria on Mars for two reasons. One if there is life on Mars, our Earth bacteria may serve as an invasive species, killing off or out-competing the natives. The second is that it could fool scientists into thinking that life exists on Mars when it didn’t exist before we introduced it. Needless to say it would spoil a “discovery”!\nWould we like it if an alien species contaminated our planet with their microbial species? What if it had dire consequences? Like wiping out plant life or the human race? I think we would absolutely go insane. Like we do when foreign ships transport alien species via ship ballast water into our waters which is a proven mechanism for assisting invasion of non-native species into a new environment.\n“We know there’s life on Mars because we sent it there,” said John Grunsfeld, a science director at NASA, during a press conference with reporters in September 2015. Grunsfeld was kidding, a little bit, but also acknowledged that on of the big problems that comes when humans visit the moon, Mars or anywhere. “We’re being very careful that we don’t send a spacecraft to Mars with the intention of detecting Martian life—and find out that we detected the Earth life that we took with us,” Grunsfeld said. “That’s tough to do.”\nStephanie Smith, a microbiologist at the University of Idaho in Moscow says that that if we undertake future missions to find life on Mars we could very find bacteria that evolved on earth and was introduced by Curiosity to the planet in the form of bacterial stowaways. Her team “swabbed” Curiosity and found more than 60 microbial species. Curiosity, of course, is a car-sized robotic rover exploring Gale Crater on Mars as part of NASA’s Mars Science Laboratory mission (MSL). Curiosity was launched from Cape Canaveral on November 26, 2011, at 15:02 UTC aboard the MSL spacecraft and landed on Aeolis Palus in Gale Crater on Mars on August 6, 2012. The Bradbury Landing site was less than 1.5 miles from the center of the rover’s touchdown target after a 350,000,000 mile journey.\nBefore Curiosity landed in August 2012 on the red planet, prior to launch Smith found bacteria that withstood spacecraft cleaning methods before take off – which means it left earth with microbial species. That’s despite the fact that NASA spacecraft get disinfected about 10 to 30 times before they launch, says Stephanie Smith. But those decontamination sessions can’t catch everything. As she and her colleagues analyzed swabs that were taken from the surface of the Curiosity rover after cleaning and prior to launch. She presented the results to the American Society for Microbiology in May 2014. “When we embarked on these studies there wasn’t anything known about the organisms in this collection,” says microbiologist Stephanie Smith.\nHer study identified 377 strains found that a surprising number resist extreme temperatures and damage caused by ultraviolet-C radiation, the most potentially harmful type. The results, presented today at the annual meeting of the American Society for Microbiology, are a first step towards elucidating how certain bacteria might survive decontamination and space flight.\nNASA even concedes it is not really if, but how long do they live? As one NASA scientist put it, “any bacteria that successfully hitchhike aboard the wheels of NASA’s Mars rover Curiosity in 2012 might manage to scratch out a brief existence on the Martian surface.”\nHere is a passage from her abstract entitled Identification and Survival of Isolates Collected from the Mars Rover, Curiosity:\nOrganisms were collected during MSL’s planetary protection implementation campaign. Isolates were identified and characterized using standard culturing and molecular techniques. Results show that a 62% of the 377 organisms identified are related to members of the Bacillus genus although surprisingly, 31% belong to non-spore-forming genera. These isolates comprise 25 genera and 65 species. Data suggests that 19 of these organisms are able to reduce potential growth substrates, such as perchlorate and sulfate, found on Mars. Many isolates have shown resistance to desiccation (78%), and UVC radiation. Moreover, 94% of the isolates can grow in the presence of elevated salt conditions (≥10% NaCl) and 35% grow at low temperatures (4C). More strikingly, 11% of isolates could survive under multiple extreme conditions.\nHer abstract makes it clear that “knowledge about the hardiest of organisms on the spacecraft and could benefit the development of cleaning and sterilization technologies to prevent forward contamination.”\nThis is a work in progress, but the need to avoid contamination seems obvious, doesn’t it? And one very good reason is that Scientists don’t want to contaminate other planets with Earth life forms, as it could make it appear that they have detected alien life when in reality they’re really only finding Earth-origin hitchhikers or related.","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:911eca19-4d5a-41d4-9b09-ebecda53574e>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"What research methods and evidence support the effectiveness of different therapeutic approaches for traumatic brain injury rehabilitation?","answer":"Dr. Berry's speech synthesis research began with establishing a baseline using healthy young adults before expanding to TBI and stroke survivors, with results presented at the Conference of Motor Speech. His work included publishing the first accuracy study with the Wave system in the Journal of Speech, Language and Hearing Research. Meanwhile, Kessler Foundation conducted a study with ten severe TBI patients with memory impairment (below 5th percentile), comparing retrieval practice against massed restudy and spaced restudy strategies. Despite the small sample size, retrieval practice showed superior results for both short-term (30-minute) and long-term (1-week) memory recall, demonstrating robust effectiveness that suggests potential benefits in real-life settings. Both research approaches are technically and conceptually challenging, requiring further refinement and larger-scale studies to confirm their benefits.","context":["Every spring DISCOVER: Marquette University Research and Scholarship showcases some of the most interesting research happening on Marquette's campus. Learn more through the links below.\nMarquette Research IN BRIEF SPEAK FOR YOURSELF Speech language pathologists often give clients with severe speech disorders alternative communication tools — in other words, machines that do the talking for them. But Dr. Jeff Berry, a Marquette assistant professor of speech pathology and audiology, thinks we can do better. He wants to help survivors of traumatic brain injuries regain their own voice. “A lot of this arose from working with people who have severe motor speech disorders who were just dissatisfied with the idea of using, for example, a speech-generating device for the rest of their lives,” says Berry, who directs Marquette’s Speech and Swallowing Lab in the College of Health Sciences. Berry thinks the path to better rehabilitation could start with a portable electromagnetic tracking system called the Wave. Last year, he published the first accuracy study with the Wave in the Journal of Speech, Language and Hearing Research and, with help from Marquette engineering students, designed software that makes the commercial device even more useful. “It’s the only software in the world that I’m “By changing how the acoustics are occurring in real time, we can trick you into modifying how you’re articulating.” aware of that takes movements of the tongue, lips and jaw and converts them into real-time speech,” he explains. “We can take somebody who is unable to consistently and reliably produce voicing on their own but can move their mouth and, essentially, when they move their mouth, the system will provide the voice.” Most speech synthesis devices are text-to-speech systems in which the user types what he or she wants to say. But Berry’s innovation is more than just another way to create a robotic voice that speaks for you. “We want to be able to understand and trigger in people with motor disabilities some of the preserved reflexive abilities of the motor system in order to use that reflexive response to modify their speech,” he says. “By changing how the acoustics are occurring in real time, we can trick you into modifying how you’re articulating.” That could mean tricking people into pronouncing a vowel a different way or, in the case of people with severe motor speech disorders, adjusting tongue height to achieve the correct sound. Berry’s speech synthesis software is critical because, until now, researchers could only manipulate acoustics for healthy speakers who could produce a high-quality acoustic signal. Now involuntary adaptations can be studied in survivors of traumatic brain injuries. But first, Berry, who has funding from the American Speech and Hearing Foundation, is refining the technology. After developing a baseline using healthy young adults, he expanded the study to survivors of traumatic brain injury and stroke and presented the results at the Conference of Motor Speech in February. “It’s a technically challenging line of research and a conceptually challenging line of research,” he says, “but we’re making good progress.” — NSE 18 Discover","Researchers find retrieval practice improves memory in severe traumatic brain injury\nKessler Foundation researchers find retrieval practice improves memory in severe traumatic brain injury\nRobust results indicate that retrieval practice would improve memory in memory-impaired persons with severe TBI in real-life settings\nWest Orange, NJ. January 30, 2014. Kessler Foundation researchers have shown that retrieval practice can improve memory in individuals with severe traumatic brain injury (TBI). “Retrieval Practice Improves Memory in Survivors of Severe Traumatic Brain Injury,” was published as a brief report in the current issue of Archives of Physical Medicine & Rehabilitation Volume 95, Issue 2 (390-396) February 2014. The article is authored by James Sumowski, PhD, Julia Coyne, PhD, Amanda Cohen, BA, and John DeLuca, PhD, of Kessler Foundation.\n“Despite the small sample size, it was clear that retrieval practice (RP) was superior to other learning strategies in this group of memory-impaired individuals with severe TBI,” explained Dr. Sumowski.\nResearchers studied ten patients with severe TBI and memory impairment (\n<5th percentile) to see whether RP improved memory after short (30 min) and long (1 week) delays. During RP, also described as testing effect, patients are quizzed shortly after information to be learned is presented. RP was compared with two other learning strategies - massed restudy (MR), which consists of repeated restudy (ie, cramming) and spaced restudy (SR), for which individuals restudy information at intervals (ie, distributed learning).\nResults showed that recall was better with RP than with MR or SR. Moreover, RP was more effective for memory after short delay, and was the only strategy that supported memory after long delay. This robust effect indicates that RP would improve memory in this group in real-life settings. “If these individuals learn to incorporate this compensatory strategy into their daily routines, they can improve their memory,” Dr. Sumowski noted. “For example, rather than re-reading an article several times, it would be more effective if they quizzed themselves periodically, eg, after each paragraph or page.”\nFuture randomized controlled trials of RP training are needed to confirm the benefits of RP in larger numbers of patients with TBI, according to Dr. DeLuca, VP of Research and Training. Another challenge is convincing patients that this strategy is effective. “Most people, with and without TBI, favor MR for learning and memory,” said Dr. DeLuca. “We will need to educate individuals with TBI about the benefits of RP.”\nThis study was supported by Kessler Foundation and Children’s Specialized Hospital.\nAbout TBI Research at Kessler Foundation\nNancy Chiaravalloti, PhD, is director of TBI Research and Neuropsychology & Neuroscience Research. Kessler Foundation is one of 16 federally funded model systems that form a national comprehensive system of care, research, education and dissemination aimed at improving quality of life for people with TBI. The Northern New Jersey TBI System (NNJTBIS), a collaborative effort of Kessler Foundation, Kessler Institute for Rehabilitation, and Rutgers New Jersey Medical School, is supported by grant #H133A120030 from the National Institute on Disability and Rehabilitation Research (NIDRR), Office of Special Education and Rehabilitative Services, US Dept of Education. Drs. John DeLuca and Nancy Chiaravalloti are project directors of the NNJTBIS. In addition to NIDRR and NIH, TBI research is funded by the New Jersey Commission on Brain Injury Research, the Department of Veterans Affairs and Children’s Specialized Hospital. Kessler Foundation researchers have faculty appointments in the department of physical medicine and rehabilitation at Rutgers New Jersey Medical School.\nAbout Kessler Foundation\nKessler Foundation, a major nonprofit organization in the field of disability, is a global leader in rehabilitation research that seeks to improve cognition, mobility and long-term outcomes, including employment, for people with neurological disabilities caused by diseases and injuries of the brain and spinal cord. Kessler Foundation leads the nation in funding innovative programs that expand opportunities for employment for people with disabilities. For more information, visit KesslerFoundation.org.\nTell-a-Friend comments powered by Disqus"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:d6f35262-bf0c-4a49-b293-45b4885333ff>","<urn:uuid:e00f1d87-5d36-4eef-95f3-de74cdb225f8>"],"error":null}
{"question":"Hi! I want to understand how preferred stocks and bonds compare in terms of payments. Can you explain the differences between their payment obligations and how guaranteed they are?","answer":"Preferred stocks and bonds have different levels of payment guarantees. Bonds have stronger payment obligations - they require fixed interest payments at specific intervals (usually every six months) and the issuing corporation must pay the principal amount at a specific date. In contrast, preferred stocks do not force issuers to pay dividends to shareholders, even though preferred stockholders do get priority over common stockholders for dividend payments. While preferred stocks are rated by credit rating companies, their ratings are generally lower than bonds because preferred dividends do not carry the same guarantees as interest payments from bonds. Bonds overall offer guaranteed returns with lower risk, while preferred stock payments have less guarantee.","context":["No obligation for dividends: The shares do not force issuers to pay dividends to shareholders. For example, if the company does not have enough funds to pay In other words, if the corporation does not declare and pay the dividends to preferred stock, there cannot be a dividend on the common stock. In return for these A limited number of preferred stock issues pay their dividends on a monthly basis instead of on a quarterly basis. In general there is nothing special with these What the yield is for a preferred stock can be confusing as it depends on Overall, the preferred will pay $2.00 in dividends but lose $1.00 in value during the Like common stocks, preferred securities provide you with an ownership or debtor before any dividend payments to common stock shareholders are paid out. It's important to note that, unlike common shares, you typically do not have the represents an ownership interest, generally does capital structure, as dividends on preferred shares must be paid before those of common shares. This .\nThese are the stocks from the S&P Composite 1500 Index that have increased their dividend payments for 20 consecutive years or longer, and also pay above-average dividends. In other words, these\nLike bonds, preferred stocks are rated by the major credit rating companies. The rating for preferred stocks is generally lower than for bonds because preferred dividends do not carry the same guarantees as interest payments from bonds and because preferred-stock … Preferred Stock Paying Monthly Dividends | Innovative ... Technically speaking preferred stocks paying monthly dividends yield a bit more over the course of a year than those paying quarterly dividends–but it is not worth worrying about this tiny differential. Personally, as an investor, I like the dividends that hit my account on a monthly basis. Top 12 Railroads Stocks for Dividend Income - Dividend.com\nWhy do preferred stockholders get paid dividends before ...\nNov 18, 2019 · A dividend index fund does pay dividends to its shareholders, however it does so on a planned basis. This is a fund which specifically seeks out assets for their income generating potential Preferred Dividend (Definition, Formula) | How to Calculate? Preferred dividends, like interest on debts, create a legal obligation on the company. These are to be paid to shareholders in preference over any common stock dividend. The liability of the company to pay dividends is unconditional and absolute. Various jurisdictions impose penalties in case the company does not pay an outstanding preferred What Are Dividends and How Do They Work? | Investing 101 ... Mar 06, 2018 · Why do people invest in dividend stocks? Even though dividends aren't guaranteed, many investors rely on them as a source of income. Because companies pay their dividends at different times\nIndex Funds That Pay Dividends - TheStreet\nDividend Calendar | Nasdaq Find dividend paying stocks and pay dates with the latest information from Nasdaq. This page only contains cash dividends. If you do not opt-in you will not receive any emails from Nasdaq. 17 Monthly Dividends To Buy And Hold Forever\nNov 18, 2019 · A dividend index fund does pay dividends to its shareholders, however it does so on a planned basis. This is a fund which specifically seeks out assets for their income generating potential\nPreferred Stocks that Pay Monthly Dividends | I Prefer Income! Out of the 30 issues, I have selected 3 preferred stocks that pay monthly dividends that are priced below $25.50 with yields above 6%. In addition, they all have stock dividend payout ratio below .80, and preferred stock dividend payout below .20. Your Complete Dividend ETF Guide | The Motley Fool\nThough it's technically stock, a preferred share is more like a bond. The dividend paid by the corporation can provide steady income to the shareholder, much number of dollars before any dividend, on that same future date or immediately thereafter, can be paid on the common stock. From this standpoint the passage of Of course, the company's board of directors can decide whether or not to pay dividends, as well as how much is paid. Owners of common stock have “ preemptive","Difference Between Stocks vs Bonds\nIt is often seen that both the terms ‘stocks vs bonds’ are used simultaneously and interchangeably making us think that both are the same in the investment world. But both are significantly different but complement each other. Both are very much good options to be kept in the investment portfolio as they offer different benefits in different types of market environments.\n- Stocks help in having ownership in a business enterprise. When a company sells its stocks then it is selling a part of the company in exchange for cash. In simple words, stocks can be also understood as shares of individual companies. When a company thinks of expanding but is unable to do so with the income it is earning through its operations, it takes the help of financial markets for additional financing.\n- The company can split it up into shares and then sell these shares in the open market. So basically a person who buys a stock is having an actual share of the company. For this reason, the stock is also known as Equity. Stockholders are paid dividends only if the company declares a dividend. The biggest corporations trade their stocks on stock exchanges. Stocks are either publicly or privately issued. If it is publicly issued then it is traded on stock exchanges like NASDAQ. When stocks are privately issued then it is held by a small group of individuals having a substantial percentage of ownership.\nBonds mean long-term debt. When the government, corporation needs to have cash then it borrows money from the public market and then pays interest on the money raised to the investors. The issuing corporation of bonds makes a promise of paying the principal amount at a specific date. Bonds issued pay interest to the bondholders. As per a fixed contract, a fixed interest payment has to be made after specific intervals generally every six months. Bigger corporations may trade their bonds in the bond market. A corporation issues bonds to invest in plant and equipment or acquisition of another business. The government issues bonds generally to raise financing for capital improvement projects or other obligations.\nTypes of Stocks and Bonds\nBelow are the different types of stocks and bonds that are as follows:\n- Common Stock – This kind of stock gives general ownership in the company. The common stockholders can elect and vote but in case of liquidation, they come much after bondholders and preferred shareholders.\n- Preferred Stock – Shareholders under this category don’t have voting rights but are eligible to get dividends before common stockholders. They get fixed dividend payments.\n- Growth Stock- This stock invests its profits in helping to grow the company. This stock may not pay a dividend or may offer a very small dividend.\n- Dividend Stock – These types of stockholders are given much of the company’s profits as dividends. It may offer some capital appreciation but the main focus is the dividend yield.\n- Value Stock- These are the stocks that are taken out of favor by the general investing public.\n- Convertible bonds – These are corporate bonds but there is a provision to convert them into company stocks.\n- High-yield bonds – These bonds also known as junk bonds pay higher interest rates and are issued by issuers with low credit ratings.\n- Foreign Bonds – These are issued by foreign governments and corporations. Investors invest in these since they pay higher interest rates than domestic bonds.\n- Municipal Bonds – These types of bonds are issued by states, countries, and municipalities. The interest paid is tax-free.\n- S. Government bonds – These are the debt obligations of the US government and are known as treasuries. These are generally issued for terms of 20 and 30 years.\nHead to Head Comparisons Between Stocks vs Bonds ( Infographics)\nBelow is the top 8 Difference between Stocks vs Bonds\nKey Differences between Stocks vs Bonds\nlet us discuss some of the major Differences Between Stocks vs Bonds:\n- Stocks are financial assets issued by a company and have ownership rights. Bonds are long-term debt instruments issued to raise capital with a promise of payback of the principal along with interest.\n- Stocks are equity instruments and bonds are debt instruments.\n- The stocks give returns known as dividends while bonds give interest. Return on stocks is not guaranteed but there is a guarantee on return from bonds.\n- Stocks are riskier than bonds.\n- The stock market has a centralized trading system whereas bond is traded over the counter.\n- Stockholders are owners of the company while bondholders are lenders to the company.\nStocks vs Bonds Comparison table\nBelow is the Comparison table between Stocks vs Bonds\n|The Basis of Comparison||\n|Definition||These are a financial instrument which gives ownership interest and is issued by the company in exchange for cash.||The debt instrument issued by companies or governments to raise capital along with the promise of payback after fixed time with interest.|\n|Issuance||Issued by companies.||Issued by the government, financial institutions or companies.|\n|Instrument||This is an Equity instrument.||This is a Debt instrument.|\n|Type of Return||Gives a dividend.||Gives interest.|\n|Guarantee of Return||No guarantee.||Guaranteed.|\n|Risk Levels||High risk.||Low risk.|\n|Add on benefits||Stockholders have voting rights.||Bondholders get preference at the time of repayment.|\n|Ownership||Stockholders. Stockholders are owners of the company.||Bondholders. Bondholders are lenders to the company.|\nBoth stocks vs bonds are good ways of raising capital from the market and are very useful financial instruments. A well-balanced portfolio has both bonds and stocks and proper allocation can help in maximizing growth and minimizing risk.\nThis has been a guide to the top difference between stocks vs bonds. Here we also discuss the stocks vs bonds key differences with infographics, and a comparison table. You may also have a look at the following articles –"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:63faa1ac-5caa-4876-b7e6-bea2028bb6b3>","<urn:uuid:a2a2bce1-534d-4dae-b41c-5aaaa7e1acf8>"],"error":null}
{"question":"What is the divine-command theory of morality and how does it differ from other theistic moral views?","answer":"The divine-command theory of morality is a meta-ethical view where moral obligations critically depend on God's dictations. This differs from theistic Platonist views, where moral facts are considered too robust to depend on God's attitudes alone. Theistic Platonists instead believe moral authority comes from either God's essential nature, basic moral facts like 'we ought to conform to God's ideals,' or independent non-naturalistic moral facts. Some argue that a system of morality dictated by an omniscient, omnibenevolent God would be the best possible system, as God would have perfect epistemic access to all objective moral facts.","context":["Many theists believe in the divine-command theory of morality, the collection of meta-ethics which depend in some critical way on the nature and dictations of God. Others think that moral facts are too robust to depend on the (presumably contingent) attitudes of God, and that their authority to dictate our moral obligations depends on God’s essential nature, or on some basic moral fact like, “we ought to conform to the ideals inherent in God”, or on a collection of independent non-naturalistic moral facts. We might label this latter group of people “theistic Platonists.” Theists are often adamant that their moral theories are superior to atheistic offerings, so we might wonder, “does this hierarchy hold for theistic and atheistic versions of Platonism?”\nPopular blogger cl of the warfare is mental thinks that it does. In fact, he’s so confident that this is the case that he regards atheistic moral contemplation to be a “quest for second best.” First, a paraphrase of his argument:\n- There are objective moral facts.\n- God would have perfect epistemic access to all objective moral facts.\n- Therefore, a set of commands dictated by God would be the best possible system of morality.\nI think the idea is that an agent with infallible access to moral facts would be the best source of moral knowledge possible, such that atheists working from their own fallible intuitions and so on are bound to be competing with the best of all possible competitors. While cl appears to hold to non-naturalistic moral realism, note that the first premise of his argument is compatible with moral facts all being rooted in God’s essential nature (like Robert M. Adams’ second DCT). Later in his post, he cautions against responding with arguments against God’s existence, since they are not relevant to the conclusion of his moral argument. However, I think that the current dialectic surrounding the argument from divine hiddenness furnishes theists with reason to be uncomfortable with cl’s argument.\nThe Argument from Divine Hiddenness\nIf God is good and loving, we’d expect him to actively seek out a personal relationship with humans (or so the argument runs). This is because it is in the nature of a loving being to want an explicit relationship with the object of its love, just as parents naturally desire face-to-face interaction with their children. It would also be a very good thing for humans to interact with God, just as it’s instrumental to children’s psychological health that they be conscious of parents who love and protect them. This analogy is borne out in the paternalistic language of monotheistic religious texts and their insistence that God desires (antecedently, at least) personal communion with all humans at some point (correct me if I’m wrong here; I know it’s true of at least Christianity).\nOf course, God will not force humans into an immediate relationship if they are not consenting, so he remains at such an epistemic distance that humans can freely choose to interact with him should they be so inclined. It is at this point that philosopher J.L. Schellenberg argue that this epistemic distance need not be total; in fact, presenting a person with a meaningful opportunity to engage with God requires that the person in question at least knows that God exists. But there are persons who are ready to engage in a relationship with God, and yet through no fault of their own lack knowledge that God exists (let’s refer to them as “inculpable believers”). So the argument concludes with the non-existence of God.\nBut isn’t divine hiddenness a good thing?\nIn response to Schellenberg’s argument, a number of theists have counter-argued that divine hiddenness is in fact a very good thing, or at least there are good reasons for God to preserve it, such that we shouldn’t be surprised by the existence of inculpable non-believers given a loving God. For example, Michael J. Murray argues in his 2002 paper, “Deus Absconditus,” that the immediacy of God’s existence would unduly coerce humans to engage with God, interfering with the intrinsically good process that is soul-building. That soul-building requires considerable autonomy is affirmed by John Hick in the now-classic Evil and the God of Love (2010 reissue):\nGod must set man at a distance from Himself, from which he can then voluntarily come to God. But how can anything be set at a distance from One who is infinite and omnipresent? Clearly spatial distance means nothing in this case. The kind of distance between God and man that would make room for a degree of human autonomy is epistemic distance. In other words, the reality and presence of God must not be borne in upon men in the coercive way in which their natural environment forces itself upon their attention. The world must be to man, to some extent at least, etsi deus non daretur,’ as if there were no God’. God must be a hidden deity, veiled by His creation. He must be knowable, but only by a mode of knowledge that involves a free personal response on man’s part, this response consisting in an uncompelled interpretative activity whereby we experience the world as mediating the divine presence. (p.281)\ncl’s argument again\nRecall the third premise:\nA system of morality dictated by an omniscient, omnibenevolent God is therefore the best system of morality possible.\nThis is somewhat ambiguous, but I believe that cl intends to be read as arguing that, having received moral commandments from God, we would possess moral knowledge as certain as we could ever hope for. And, since the argument is about the best possible system of morality, not the one which is most plausible given our actual epistemic situation, considerations about epistemic plausibility (like the argument from evil etc.) simply aren’t relevant. But notice that the strength of our confidence in God’s delivered commands correlates positively with our assurance that they were actually delivered by God. And any assurance of this kind would also implicitly assure us of God’s existence.\nIt should be more clear now that cl’s argument critically relies on an inference that all theodicies for divine hiddenness contradict. For they all argue that epistemic distance, even to the point of lacking theistic belief altogether, is a good thing and to be expected were God to exist. By contrast, cl’s argument expects that God would obviate this epistemic distance to deliver his moral commands, thus obviating all the goods that the aforementioned theodicies worked so hard to articulate.\nSuppose that God just delivered his moral maxims to humans as in the form of intuitions which had no obvious connection to himself? But then moral beliefs would be no more secure then they are now, as all would be relying on moral intuitions which lack the warrant of God’s obvious approval.\nSo the theist must make the choice between cl’s argument and all theodicies for divine hiddenness. Since cl’s argument at best can only show that the best possible moral theory involves God, and the argument from divine hiddenness is about the actual world, I think it’s obvious that the theist has more to lose than gain by accepting it.\nIs sceptical theism the answer?\nAnother interesting facet of this argument involves cl’s stated commitments elsewhere on his blog. I take it that the following quotations to represent his current views on the matter (please correct me if I’m wrong):\nWhile I’ll still gladly engage anybody on the issue, these days, I’m leaning towards the conclusion that the atheist’s problem of evil arguments are fatally flawed. In the end, all variants I’ve encountered reduce to incredulity: reasoning from premises derived at via conceptual analysis and intuition, the atheist disbelieves that a morally sufficient reason can exist: “There’s no way a good God would allow this much evil in the world.” That’s it. I’ve not seen a single POE argument that doesn’t reduce thus, and I’ll leave it to you to decide whether disbelief is sufficient to warrant skepticism in this regard. I say no. (source)\nI recently said that all the POE arguments I’ve heard reduce to arguments from incredulity, and this argument is no different. Inability to conceive of a higher good is the only thing grounding the claim that any given instance of suffering is needless. 6 is a naked assertion sustained only by incredulity. That alone invalidates the argument in my opinion, but I can make a stronger case. (source)\nThis is the standard sceptical-theistic objection to the inference from, “there are evils for which we have no known moral justification” to “there are evils which actually have no moral justification.” If his objection is sound, it follows as a general principle that we cannot infer anything about what God would prevent or allow from what appears to be good or evil to our limited minds. And so the move from, “God knows all moral truths” to “God would reveal and guarantee these truths for humans” is unjustified. Perhaps God cannot deliver these commands to humans, because there is some opposing outweighing good which we aren’t aware of. Perhaps he can deliver these commands to humans, but only in such an attenuated manner that they persuade no-one. Perhaps he can deliver his commandments in such a spectacular fashion that everyone can’t help but be persuaded, but he is forced by some unknown outweighing good to deliver only false commands, commands which do not correlate with the actual moral facts.\nIt appears that accepting a moral argument of this kind requires one to abandon all theodicies for divine hiddenness, and sceptical theism besides. This is a high cost; the theist would have to concede that the problem of gratuitous evil and divine hiddenness are sound and extant. If one has the pragmatic desire to rationally preserve one’s theistic beliefs, then, it is this moral argument which ought to be abandoned."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:52364b4a-a72b-45de-98fe-5a0daa1c68eb>"],"error":null}
{"question":"I'm a manufacturing consultant - what's the relationship between employee training and resource optimization in achieving economic efficiency through lean practices?","answer":"Employee training and resource optimization are interconnected in achieving economic efficiency through lean practices. On-the-job training, particularly with veteran employees demonstrating processes firsthand, improves workers' comprehension of best practices and problem-solving methods. Cross-training team members on the entire manufacturing process helps optimize resource allocation by ensuring everyone understands their role and can contribute effectively. This ties directly to economic efficiency, which involves allocating resources wisely to minimize production costs. The training aspect is particularly crucial when firms need to reallocate freed-up resources from lean initiatives - proper training can help address skill gaps and enable more flexible resource utilization, though sometimes the skill and competency gaps cannot be overcome through training alone, especially in demand-constrained firms.","context":["What is the economic rationale for pursuing lean production? Much of the lean literature is concerned with the nuts and bolts of lean, and the economics of lean are somewhat less publicized. This article attempts to redress that imbalance, albeit in a very condensed way.\nFirms employ capital and labor to transform their raw materials into their output goods and services. The inputs of capital and labor are the so-called factors of production. This is given as a firm's production function, Q = ƒ(X1, X2, ..., Xn), where Q is the output quantity, and X1, X2, …, Xn are quantities of factor inputs, such as capital, labor, and raw materials. A firm's production function specifies the firm's output for all combinations of inputs.\nThrough a simple thought experiment, we can think of the firm as a large production process that takes the factor inputs and uses them to transform raw materials into output products and services (see figure 1). Note that this model applies equally well to nonmanufacturing firms, although these firms may transform things other than raw materials into outputs:\nA firm's production function describes how much output can be produced with a given combination of inputs. If we consider that the goal of a firm is to maximize its profitability, firms should always strive to be efficient in their operations. In economic terms, efficiency is thought of in two ways:\nTechnical (or technological) efficiency. Technical efficiency is achieving the maximum output possible from a given set of inputs. If a process is producing the maximum amount possible, given its labor and capital, then that process may be said to be technically efficient.\nEconomic efficiency. Economic efficiency is concerned with more than just technical efficiency. Economic efficiency is concerned with allocating resources wisely so as to minimize the costs associated with using a given combination of inputs to produce a given output. If a firm's goal is to maximize its profit, then the firm should consider which technically efficient processes best achieves that objective. An economically efficient production system is one that best allocates resources to produce output at the least cost.\nAchieving improvements in technical efficiency requires improving the technical means of production. These improvements may consist of changes to the methods and technologies used in production so as to obtain the greatest output from a given set of inputs. Lean production has a role to play here as many of techniques and tools of lean are designed to reduce or eliminate the waste and nonvalue-adding activity which prevents a process from realizing the maximum output from a given set of inputs.\nEconomic or allocating efficiency, on the other hand, is essentially a strategic question. What businesses should the firm be in? What markets and customer groups should it be serving? What products and services should be supplied and in what quantities? What inputs does the firm require, and how can the cost of these be minimized? Which capabilities and processes are required to serve customers, and how should these be designed and configured? These and other questions are strategic in nature and determine in large measure how a firm will allocate its resources so as to be economically efficient in its chosen business.\nFor a firm, the issue of profit maximization seems, on the surface, straightforward. Profit is given by the following simple equation: P = TR – TC, where P is a firm's profit, TR is the firm's total revenues, and TC is the firm's total costs. Profit maximization is not simply about maximizing revenues and minimizing costs. Rather, it is an optimization exercise that recognizes the interdependency between revenues and costs: As a firm increases its output to sell more and increase revenues, it also increases costs. The key question in this interdependent relationship is what happens at the margin—i.e., how do revenues and costs behave with each additional unit of production? For most firms, profits are maximized when marginal revenues equal marginal costs.\nA basic tenet of lean production is for a firm to operate its production processes synchronized to the rate of customer demand—the so-called takt time. This concept is not at odds with achieving technical efficiency. For example, using group technology or cellular manufacturing techniques to create work cells staffed by only the amount of labor needed to run the cell at the takt time achieves the objective of technical efficiency—i.e., the maximum output of product needed by the marketplace is produced with the minimum amount of labor as a factor input.\nAnother way to look at this is through the lens of the marginal product of labor. The marginal product of labor is the addition to output produced by each additional worker. In a lean cell or flow, the marginal product of labor is maximized relative to the amount of labor used in the cell—through work balancing, only that amount of labor needed to produce output at the required rate is used.\nWith respect to economic efficiency, lean has a role to play in designing and optimizing value chains to operate the least total cost. In the long run, a firm's production costs almost always decline when the scale of the firm's operation increases—the so-called economies of scale. There are several reasons why this is so. One is that production flows can be organized in a more efficient manner when greater quantities of output are being produced. Another is that larger production volumes allow a firm to take advantage of the opportunities afforded by greater task specialization. Lean can help firms achieve the cost reduction benefits normally associated with economies of scale through realizing economies of flow—i.e., using the least amounts of capital and labor to transform raw materials into output goods and services.\nIn economic terms, profit maximization in the short run is the process by which a firm determines the price and output level that returns the greatest profit. The general rule is that a firm maximizes its profit by producing that quantity of output where marginal revenue equals marginal costs. The profit maximization issue can also be approached from the input side by determining what the profit maximizing usage of the variable input is. To maximize profits, a firm should increase usage up to the point where the input's marginal revenue product equals its marginal costs.\nIn the short run, at least one of a firm's factor inputs is fixed, meaning it is invariant to the output quantity produced. Because lean can help a firm to achieve technical, and to some degree economic, efficiency, a firm can lower the costs associated with its variable factor input, thereby achieving a lower cost per unit of output. More precisely, a firm can shift its average total cost of production curve downward, thereby lowering the firm's break-even point.\nIn the diagram in figure 2, a firm operating in an imperfectly competitive marketplace is profitable in the short run for an output quantity for which its average total cost curve (ATC) lies below its demand curve (D). The profit-maximizing output quantity is Qp, where marginal revenue (MRp) is equal to marginal cost (MC). The firm charges the price (Pp) where this quantity intersects the firm's demand curve, and earns a profit that is the difference between the price charged and the average total cost of production (ATCp).\nWhen the firm undertakes lean production, the average total cost curve shifts down, mainly due to technical efficiencies gained, and this results in lower factor input costs (see figure 3). This increases the firm's profit at the profit maximizing quantity.\nSince, in the short run, it is usually a firm's labor costs that are the main variable cost, this exposes a dilemma for many firms: How are the labor resources that have been freed up from lean initiatives to be used? In trying to answer this question, it should be noted that labor costs are only reduced when a firm issues smaller and fewer paychecks. Freeing up labor in one area of a production system only to absorb it elsewhere does not reduce a firm's total costs. Many sins have been committed through accounting allocations that disguise the fact that some process improvements resulted in no reduction to a firm's total costs because the labor freed up was redeployed elsewhere in the firm.\nPart of the answer to this question may lie in effectively balancing two different concepts: efficiency and equity. Firms must, of course, strive to be economically efficient producers, but the achievement of that efficiency should not come at the expense of a net benefit to the firm as a whole. If lean initiatives result in downsizing and layoffs, mainly absorbed at the functionary level, the efficiency that a firm is striving to achieve may be undone due to a lack of morale and a withholding of contribution on the part of the remaining employees. In other words, to ensure sustainable improvement, a firm may have to be willing to trade off some loss in efficiency in return for the welfare and well-being of its members.\nWhere possible, firms should try to use freed-up resources elsewhere. For the capacity-constrained firm, this is often not too difficult because resources are scarce to begin with. For the demand-constrained firm, however, it is a more challenging issue. Demand-constrained firms, by definition, need to stimulate market demand, and this often requires specialized skills and competencies that are quite different from those possessed by employees who may be freed up from shop-floor process improvement initiatives. Often, this skill and competency gap cannot be surmounted by training, and the firm finds itself having too much of the wrong kind of resource.\nMy own perspective on this issue is that, rather than trying to solve the problem of what to do with excess resources freed up from lean, a firm should try to avoid this dilemma in the first place. If a firm is demand-constrained, implementing lean is often not the appropriate place to start generating improvement. In this case, a demand-constrained firm needs to refocus strategically and think through how it will generate additional demand and revenue. In economic terms, there is little point in a firm trying to optimize its supply curve when it does not understand, or know how to exploit, its demand curve.","Efficiency is important to every organization because it affects the bottom line. In manufacturing, efficiency tends to have a greater impact because the environment is process-driven, which means optimal productivity is critical to reduce labor costs and achieve desired profits. One way to improve efficiency in manufacturing is to maximize the use of available resources. This means making full use of human resources, materials and equipment. A lot of companies leverage Six Sigma principles to ensure processes are lean and that only required resources are procured. Design for manufacturability (DFM) principles often produce remarkable results when it comes to eliminating waste. In DFM, parts are created with minimal materials in order to reduce waste and optimize the return on investment. To further optimize efficiency, factory returns and scraps are recycled.\nProven Methods for Increasing Efficiency in Manufacturing\nOpportunities to increase efficiency and reduce costs are often found in the shipping process. Some companies are able to assess the materials used in shipping and make adjustments in order to slash expenses. For example, limiting the amount of bubble wrap based on what’s truly needed as opposed to simply guessing. This requires a more strategic approach and process testing. Another way to increase efficiency is through the use of a direct drive rotary compressor.\nPerhaps one of the best ways to increase efficiency and yield the biggest results is through employee training. Specifically, on-the-job training where employees are able to see the process performed with optimal effectiveness. This also provides an opportunity to demonstrate problem solving methods. Having a chance to experience a process firsthand while being trained by a veteran improves the employee’s ability to comprehend best practices. Cross-training team members on the entire manufacturing process helps everyone understand what’s required, value their individual role in the process and contribute to success.\nOne method of identifying where improvements can be made is to place a monetary value on each phase of the manufacturing process. This allows you to assess the cost of labor, materials and equipment involved and determine where improvements can be made. While assessing the value of each phase, you can identify any items that are unnecessary. For example, if there are tools or products that are not being used, there is likely an opportunity to cease purchase of those items.\nA common way in which companies boost efficiency in manufacturing is through the standardization of every task, no matter how large or small. Although some workers prefer autonomy, when it comes to manufacturing and productivity, following a strict process should be a priority. Having an explicit process that’s followed by everyone can significantly increase efficiency. A step-by-step process accompanied by a checklist is often a useful tool.\nLean manufacturing principles dictate that every process is separated by cells and each cell has it’s own task. In some spaces this is referred to as chunking because it allows the same person to perform the same tasks, which increases the efficiency in which each task is performed. This method is also believed to improve consistency."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:818a2970-b739-4e08-a08f-5905b2395c0b>","<urn:uuid:b8661b0d-e567-4f94-823b-168b90aa8b3d>"],"error":null}
{"question":"How much has Florida's land-buying program funding been reduced between 2009 and 2014?","answer":"The Florida Forever land-buying program's funding was slashed by more than 97 percent between 2009 and 2014.","context":["Craig Pittman reports for the Tampa Bay Times – “This year…, you could say that the big environmental issue is money. Or you could say that the issue is whether legislators ever pay attention to what the voters want… Florida once had a politically popular land-buying program called Florida Forever that preserved some of the remaining wild places in the state’s rapidly developing landscape… Between 2009 and 2014, lawmakers slashed its funding by more than 97 percent. So in 2014, frustrated environmental advocates mustered enough support to put on the ballot a measure called Amendment 1. This constitutional amendment was intended to require the state to revive the funding for Florida Forever… Last year, once again, the Legislature failed to come up with a dime for buying land under the Florida Forever program… Estus Whitfield, an environmental adviser to four governors who helped organize the Florida Conservation Coalition, believes the Legislature may siphon out so much money that there’s not going to be enough left to buy land for preservation.” Read One more time to revive Florida Forever before it’s Florida Never\nDanny Mcauliffe reports for Florida Politics – “Wraithmell said [Audubon Florida]… is concerned with management and acquisition of public lands, along with water issues, climate, growth management, and more… ‘We’ll continue to advocate for those (land conservation) programs to ensure we’re setting aside the places that we depend upon not just for recreation and jobs, not just for wildlife, but also as green infrastructure for our communities,’ Wraithmell said. She added that doing so makes the land more resilient, protecting the state from catastrophes while also helping to recharge water supply.” Read Audubon happy Rick Scott opposes drilling, releases Session priorities\nA.G. Gancarski reports for Florida Politics – “’Based on media reports, it is likely that the Department of interior will consider Florida as a potential state for offshore oil drilling – which is something I oppose in Florida,’ (Governor) Scott said. ‘I have already asked to immediately meet with Secretary Zinke to discuss the concerns I have with this plan and the crucial need to remove Florida from consideration,’ Scott added… Scott joins his likely opponent in this year’s Senate race, Democratic incumbent Bill Nelson, in opposing the expansion of offshore drilling proposed by the Trump White House.” Read Florida pols oppose proposed offshore oil drilling; Donald Trump isn’t worried\nThe Sun Sentinel Editorial Board writes – “President Trump has managed to unite Democrats and Republicans in Florida – against him. The bipartisan criticism stems from the Interior Department’s announcement… that the president wants to open almost all federal waters to offshore drilling… Current rules keep drilling at least 125 miles away from most of the Gulf Coast until 2022. Trump and Interior Secretary Ryan Zinke want to end the ban and allow drilling as close as 10 miles from shore, threatening Florida’s economy. President Trump isn’t stopping there. He’d also lease sales off Florida’s Atlantic coast and in the Florida Straits, near the Keys… So emotional is the drilling issue…, that even some of the president’s loudest Florida supporters have joined the choirs of opposition… Rep. Matt Gaetz, whose Panhandle district went for Trump by the highest margin in the state, said the proposal ‘would be catastrophic to our military, and to our local tourism economy.’” Read Trump’s drilling plan poses unacceptable risk to Florida\nEve Samples writes for the TC Palm – “Estimated at $1.6 billion, the reservoir project will not be cheap. That’s all the more reason to make sure we get it right. The water management district estimates its configurations of the project, when combined with other projects already in the works, would reduce annual average discharges from Lake O to the St. Lucie and Caloosahatchee rivers by about 60 percent. In particularly heavy discharge years… the flow would be reduced by about 45 percent… That still is a lot of water coming our way. Last year, 134 billion gallons of Lake O water were discharged to the St. Lucie River. That’s enough to cover the entire city of Stuart with 7 ½ feet of water… In 2016, it was almost double that – a whopping 237 billion gallons. Cutting those discharges in half still looks pretty awful for the St. Lucie River. Lawmakers would be wise to examine an alternative plan being advocated by the Everglades Foundation that could clean more water.” Read Pivotal moment for our estuaries and the Everglades\nJonathan Stacey reports for News4JAx – “The Environmental Protection Agency has identified more than 1,800 old manufacturing and chemical plants, landfills, waste treatment centers and military sites across the nation as hazardous areas that pose a risk to human and environmental health. They are called federal Superfund sites, but it now appears more than 300 of those sites are too close for comfort – if high water rushes in… Johnston lives just two blocks away from one of Jacksonville’s two federal Superfund sites that are either located in a floodplain or vulnerable to sea level rise… The Associated Press found dozens more sites like these across the state and found most of them are concentrated in South Florida… Despite EPA’s announced emphasis on expediting cleanups, the Trump administration’s proposed 2018 spending plan seeks to slash Superfund program funding by nearly one third.” Read Hundreds of Superfund sites face flood risks\nCraig Welch reports for National Geographic- “Marine waters… are losing oxygen thanks to climate change, upending where and how sea creatures live… Warm water simply carries less oxygen. It also stokes the metabolism of both microbes and larger creatures, causing them to use more of whatever oxygen there is. Finally, as climate change warms the ocean from the surface down, making the surface layer more buoyant – warm water is lighter than cold water – it makes it harder for fresh oxygen from the air to mix down into the deep layers where the oxygen-poor zones are located. Today, those low-oxygen zones are expanding toward the surface by as much as a meter a year… For some marine creatures, low oxygen waters can impair reproduction, shorten lifespans, and change behavior. Even brief exposure can change immune systems and increase disease. Low-oxygen waters can even affect future generations by altering gene expression in fish and other marine creatures. The change is already forcing everything from tuna and sharks… into ever-smaller bands of oxygen-rich water near the surface. Concentrating all these creatures makes them easier pickings for turtles, birds, and other surface predators – including fishing fleets.” Read Climate Change is Suffocating Large Parts of the Ocean\nOliver Milman reports for The Guardian – “The Trump administration’s plan to shrink four land-based national monuments has provoked howls of anguish from environmental groups, Native American tribes and some businesses... Accompanying changes to protected monuments in the oceans – vastly larger areas than their land-based counterparts – have received less attention, but could have major consequences for the livelihoods and ecosystems dependent upon the marine environment. Ryan Zinke, the secretary of the interior, has recommended… that three sprawling marine monuments, one in the Atlantic and two in the Pacific, be either opened up to the commercial fishing industry or reduced in size, or both… In 2009, George W Bush created the Pacific Remote Islands national monument around seven islands and atolls in the central Pacific. The monument, subsequently expanded by Barack Obama to become what was the largest marine protected area in the world, comprises ‘the last refugia for fish and wildlife species rapidly vanishing from the remainder of the planet,’ according to the Fish & Wildlife Service, boasting creatures such as sea turtles, dolphins, whales, sharks and giant clams… Conservation groups fret that commercial fishing would only be a precursor to further invasions, such as oil drilling or seabed mining. The Atlantic monument is considered particularly sensitive due to its dense forests of deep-sea corals and its role as a migratory route for the endangered North Atlantic right whale, which has experienced an alarming dip in numbers this year… Peter Baker, director of US oceans, north-east, at the Pew Charitable Trusts [said, ‘]It shouldn’t be too much to ask to protect 2% of the US’s exclusive economic zone off the Atlantic coast for future generations…’” Read Trump plan to shrink ocean monuments threatens vital ecosystems, experts warn\nFrom Our Readers\nThe information in this section is forwarded to you at the request of some of our readers. Inclusion in this section does not necessarily constitute endorsement by the FCC.\nUpcoming Environmental Events\nJanuary 9, 12:00 pm – Attend Springs Academy Tuesday, a lunchtime lecture series on Florida’s springs, in High Springs. January’s lecture is on Springs Biology with special guest Dr. Stephen Walsh of the USGS Wetland and Aquatic Research Center. For more information, click here or call (386) 454 – 2427.\nJanuary 15-16 – Participate in Florida Coasts & Ocean Citizen Advocacy Day in Tallahassee. This is your opportunity to speak out for clean water and healthy beaches. Citizens will support the plastic bag ban bills, water quality monitoring, beach access, and more. They will receive free training and food. For more information and to register, click here.\nJanuary 18, 7:00 pm – Attend The Islands at Apalachicola with Jeff Chanton & Susan Cerulean in Tallahassee. Experts will discuss how the Apalachicola River’s flow affects the configuration of St. Vincent Island and its wildlife. There is a social at 7:00 pm and the program begins at 7:30 pm. For more information, click here.\nJanuary 20, 9:00 am – Participate in the annual Newnan’s Lake Cleanup in Gainesville. Volunteers will meet at Earl P. Powers Park (5910 SE Hawthorne Rd) on the southwest edge of the lake. Current Problems will provide cleanup supplies such as bags, grabbers, gloves, nets, and scales. There will be snacks and drinks for volunteers. For more information, contact Megan Black at email@example.com.\nJanuary 30-31 – Participate in the Reclaiming Florida’s Future for All Advocacy Day in Tallahassee. Citizens will gather together to support climate action and land conservation funding. They will receive free advocacy training and may receive free lodging. For more information, click here.\nDo you know of an upcoming environmental event or meeting you would like to include in the FCC News Brief? Send us a quick e-mail and we will include it for you.\nWe hope you enjoy this service and find it valuable. Our goal is to provide you with the latest and most relevant environmental news for Floridians. Our hope is that you will use this information to more effectively and frequently contact your elected representatives, and add your voice to the growing chorus of Floridians concerned about the condition of our environment and the recent direction of environmental policies.\nPlease send all suggestions, comments, and criticism to Gladys Delgadillo at firstname.lastname@example.org.\nAbout the FCC: The Florida Conservation Coalition (FCC) is composed of over 80 conservation-minded organizations and over two thousand individuals devoted to protecting and conserving Florida’s land, fish and wildlife, and water resources.\nFor more information on the FCC visit https://www.wearefcc.org/"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:c31526ad-ad4b-46f7-a7bc-9e83d0ac4494>"],"error":null}
{"question":"How does prototype development support product validation, and what are the associated market demand uncertainties?","answer":"Prototype development supports product validation through a systematic process that includes identifying software/hardware requirements, reverse-engineering competing products, choosing cost-effective materials, creating mockups, and outsourcing mass production. While prototypes don't need to be perfect, they must be functional enough for stakeholder evaluation. However, there are significant market demand uncertainties - even with a working prototype, there's no assurance of broad market acceptance, and competitors might offer similar products at lower prices, potentially affecting the company's operations and financial condition.","context":["The word “prototype” originated from the Greek term “protypon.” “Protos” means “first” while “typos” refers to “mold” or “pattern.” So, a prototype is literally “the first of its kind.” It’s a working model of a machine or process that is built to demonstrate how an idea works.\nBusinesses, especially startups, create prototypes of their products to test them in actual conditions. They also show prototypes to their stakeholders, potential partners, and prospective customers so they can take a look at and get a feel of the product.\nRead More about a “Prototype”\nIn the U.S., the number of small business startups has reached 30 million and counting. However, a large percentage of them fail, with 10% closing during the first year. Based on the data provided by CB Insights, as much as 42% of startups fail because their products or services do not answer a current market need. This finding highlights the need for startups to test and refine their products or services by creating a prototype.\nHow to Create a Prototype\nCreating a prototype lessens the chances that a startup would fail. Here’s a step-by-step guide on creating one:\n1. Identify Software and Hardware Requirements\nBefore you can even start working on a prototype, check first if you have the necessary software or hardware to jumpstart the development process. You need to determine if you need applications that can mimic animation based on the use cases of your proposed product or service. One of the most common hardware required to develop a physical prototype is a 3D printer. Check if you need to invest in one.\n2. Reverse-Engineer Competing Products or Services\nIf your product or service is already available in the market but aims to fill in a gap, get hold of competing products or services, and study how they work. That should give you a better understanding of issues and test if your product or service addresses them.\n3. Choose Cost-Effective Materials\nReverse engineering competitors’ products or services is also an excellent way to come up with more cost-effective options for raw materials. Doing so allows you to produce products or services that can be mass-produced with high-quality materials while remaining affordable for intended consumers.\n4. Create a Mockup\nThe key to coming up with a functional prototype is to create it yourself. After all, you are the best person to bring your ideas to life. This process will also save you the trouble of having misunderstandings with your mass producer.\n5. Outsource Prototype Mass Production\nOnce you’ve created your mockup, it is best to get professionals to work on and refine it. That way, you also have an idea of how the product would look and function after manufacturing. When outsourcing, make sure to vet the mass producer’s trustworthiness. Better yet, have the manufacturer sign a nondisclosure agreement (NDA) to protect your intellectual property.\nKeep in mind that a prototype does not need to be perfect, but it has to be functional enough for stakeholders to see if it is worth investing in.","Investing in early stage companies is inherently high risk. You might lose your entire investment. Here we explain some of the risks. Please read these risks and take them seriously.\nThere are risks that you must consider when making an investment in a startup company or early stage company on Manhattan Street Capital. Investing in startups is very risky, speculative, and investments should not be made by anyone who cannot afford to lose their entire investment.\nCarefully consider the risks associated with the type of investment, security, and business before making any investment decision.\nPrincipal risk: Investing in startups will put the entire amount of your investment at risk. There are many situations in which the company may fail completely or you may not be able to sell the stock that you own in the company. In these situations, you may lose the entire amount of your investment. For investments in startups, total loss of capital is a highly likely outcome. Investing in startups involves a high level of risk and you should not invest any funds unless you are able to bear the entire loss of the investment.\nReturns risk: The amount of return on investment, if any, is highly variable and not guaranteed. Some startups may be successful and generate significant returns, but many will not be successful and will only generate small returns, if any at all. Any returns that you may receive will be variable in amount, frequency, and timing. You should not invest any funds in which you require a regular, predictable and/or stable return.\nReturns delay: Any returns may take several years to materialize. Most startups take five to seven years to generate any investment return, if any at all. It may also take many years before you will know if a startup investment will generate any return. You should not invest any funds in which you require a return within a certain timeframe.\nLiquidity risk: It may be difficult to sell your securities. Startup investments are privately held companies and are not traded on a public stock exchange. Also, there is currently no readily available secondary market for private buyers to purchase your securities. Furthermore, there may be restrictions on the resale of the securities you purchase and your ability to transfer. You should not invest any funds in which you require the ability to withdraw, cash-out, or liquidate within a certain period of time.\nInstrument risk: You may be investing in preferred equity, common equity, or convertible notes. These securities instruments all have different inherent risks caused by their structure. You should take the time to understand the nature of the securities instrument that you are investing in.\nDilution: Startup companies may need to raise additional capital in the future. When these new investors make their investment into the company they may receive newly issued securities. These new securities will dilute the percentage ownership that you have in the business.\nMinority stake: As a smaller shareholder in the business you may have less voting rights or ability to influence the direction of the company than larger investors. In some cases, this may mean that your securities are treated less preferentially than larger security holders.\nValuation risk: Unlike publicly traded companies that are valued publicly through market-driven stock prices, the valuation of private companies, especially startups, is difficult to assess. The issuer will set the share price for your investment and you may risk overpaying for your investment. The price you pay for your investment may have a material impact on your eventual return, if any at all.\nFailure risk: Investments in startups are speculative and these companies often fail. Unlike an investment in a mature business where there is a track record of revenue and income, the success of a startup often relies on the development of a new product or service that may or may not find a market. You should be able to afford and be prepared to lose your entire investment.\nRevenue risk: The company is still in an early phase, and may be just beginning to implement its business plan. There can be no assurance that it will ever operate profitably. The likelihood of achieving profitability should be considered in light of the problems, expenses, difficulties, complications, and delays usually encountered by companies in their early stages of development. The company may not be successful in attaining the objectives necessary for it to overcome these risks and uncertainties.\nFunding risk: The company may require funds in excess of its existing cash resources to fund operating expenses, develop new products, expand its marketing capabilities, and finance general and administrative activities. Due to market conditions at the time the company needs additional funding, it is possible that the company will be unable to obtain additional funding when it needs it, or the terms of any available funding may be unfavorable. If the company is unable to obtain additional funding, it may not be able to repay debts when they are due or the new funding may excessively dilute existing investors. If the company is unable to obtain additional funding as and when needed, it could be forced to delay its development, marketing and expansion efforts and, if it continues to experience losses, potentially cease operations.\nDisclosure risks: The company is at an early stage and may only be able to provide limited information about its business plan and operations because it does not have fully developed operations or a long trading history. The company is also only obligated to provide limited information regarding its business and financial affairs to investors.\nPersonnel risks: An investment in a startup is also an investment in the management of the company. Being able to execute on the business plan is often an important factor in whether the business is viable and successful. You should be aware that a portion of your investment may fund the compensation of the company’s employees, including its management. You should carefully review any disclosure regarding the company’s use of proceeds. You should also carefully consider the experience and expertise of the management team.\nFraud risks: It is possible that certain people involved in the company may commit fraud or mislead investors. If fraud or misleading conduct occurs, then your total investment may be lost. You should carefully review any disclosures regarding the company’s management team and make your own assessment of the likelihood of any potential fraud.\nLack of professional guidance: Many successful startups partially attribute their early success to the guidance of professional investors (e.g., angel investors and venture capital firms). These investors often play an important role through their resources, contacts, and experience in assisting startup companies in executing their business plans. A startup company primarily financed by smaller investors may not have the benefit of such professional investors. You should consider the existing professional investors in the company and whether or not they or any other professional investors are participating in the current round.\nGrowth risk: For a startup to succeed, it will need to expand significantly. There can be no assurance that it will achieve this expansion. Expansion may place a significant strain on the company’s management, operational and financial resources. To manage growth, the company will be required to implement operational and financial systems, procedures and controls. It also will be required to expand its finance, administrative and operations staff. There can be no assurance that the company’s current and planned personnel, systems, procedures, and controls will be adequate to support its future operations. The company’s failure to manage growth effectively could have a material adverse effect on its business, results of operations, and financial condition.\nCompetition risk: The startup may face competition from other companies, some of which might have received more funding than the startup has. One or more of the company’s competitors could offer services similar to those offered by the company at significantly lower prices, which would cause downward pressure on the prices the company would be able to charge for its services. If the company is not able to charge the prices it anticipates charging for its services, there may be a material adverse effect on the company’s results of operations and financial condition.\nMarket demand risk: While the company believes that there will be customer demand for its products, there is no assurance that there will be broad market acceptance of the company’s offerings. There also may not be broad market acceptance of the company’s offerings if its competitors offer products which are preferred by prospective customers. In such event, there may be a material adverse effect on the company’s results of operations and financial condition, and the company may not be able to achieve its goals.\nControl risks: Because the company’s founders, directors, and executive officers may be among the company’s largest stockholders, they can exert significant control over the company’s business and affairs and have actual or potential interests that may depart from yours. The company’s founders, directors, and executive officers may own or control a significant percentage of the company. In addition to their board seats, such persons will have significant influence over corporate actions requiring stockholder approval, irrespective of how the company’s other stockholders, including you, may vote. Such persons’ ownership may also discourage a potential acquirer from making an offer to acquire the company, which in turn could reduce the company’s stock price or prevent you from realizing a premium on your investment."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:1128c79b-55a1-41be-8f7e-973615f080fb>","<urn:uuid:3289e71f-ea58-4fd9-a5b8-46a1ff7cc86a>"],"error":null}
{"question":"What are the economic implications of carbon budgeting for cities, and how does urban heat affect their financial outlook?","answer":"Carbon budgeting can create economic opportunities while addressing climate challenges. In Edmonton's case, investments in climate-friendly infrastructure, green construction, and renewable energy can create tens of thousands of jobs while helping the city stay within its 135-million-tonne carbon dioxide equivalent budget. The city's plan creates a policy framework to support low-carbon industries and infrastructure development. However, unchecked urban heat poses significant financial risks - cities are projected to lose an average of 5.6% of their economic output by 2100 due to urban heat and local climate change impacts, with the most affected cities facing losses of up to 11% of their economic output. This makes addressing both carbon emissions and urban heat crucial for cities' economic sustainability.","context":["This week, Edmonton city council is considering a historic plan — one that incorporates a carbon budget limiting greenhouse gas emissions it can emit. It would be the first city to do so in Canada.\nThe plan sets a goal for Edmonton to become carbon neutral by 2050 — meaning the city would minimize how many greenhouse gas emissions it produces and offset the rest — while doubling its population to two million and fostering a vibrant economy. So there will be twice as many people powering their homes, travelling to and from work, shopping and going about daily life, all while producing net-zero greenhouse gas emissions.\nIt’s ambitious, almost unimaginable. And it is something every town and city in Canada needs to do.\nCities are responsible for 42 per cent of Canada’s greenhouse gas emissions, which, according to the latest data from Statistics Canada, hit their highest level in more than a decade in 2018. If we are going to meet our national climate goals and help limit global warming, every Canadian municipality must follow Edmonton’s lead and adopt a city plan with a carbon budget.\nJust as a fiscal budget sets a limit on how much money a city can spend, a carbon budget sets a limit on the emissions it can produce. In Edmonton, the city’s proposed carbon budget of 135 million tonnes of carbon dioxide equivalent sets a cap on how much it can emit — ever. It’s not a budget for a year or a decade, but for the city’s entire future.\nThat’s because, according to the global scientific consensus, the atmosphere can only absorb so many GHG emissions before dangerous levels of climate change occur. Edmonton’s carbon budget is based on a calculation of how many more emissions Edmontonians can produce if they use their fair share of a global carbon budget that would limit global warming to 1.5 C.\nSustainability Solutions Group, the climate planning consultancy of which I am a part, helped Edmonton crunch the numbers and determine what policies and actions will enable the city to stay within its budget while promoting local prosperity and well-being. We found that it is possible for the city to reduce emissions while thriving and overcoming the economic challenges created by low oil prices and the pandemic. Investments in climate-friendly infrastructure, including public transit and green space, and low-carbon industries, such as green construction and renewable energy, can create tens of thousands of jobs.\nNow, the city’s plan will make the carbon budget official policy. This is a key step, because planning decisions made today can lock in levels of emissions for generations to come. City plans shape the built environment, which shapes how people move around and the types of spaces they spend their time in, as well as the economic opportunities they can access.\nIn compact cities, it is easier for people to walk, bike or take transit to their destination, producing few to zero emissions. In sprawling cities, residents might spend hours on congested highways, emitting carbon dioxide — and losing time they could spend with their families or at work — along the way.\nIn cities with low-carbon building regulations, it takes less energy — and costs less — to heat, cool and power buildings. In cities where plans emphasize green space, there are more trees and plants to sequester carbon and help keep residents cool in a heat wave.\nTake Atlanta and Barcelona. Both cities have similar populations, but, on a per-capita basis, Atlanta’s carbon footprint is 10 times higher than Barcelona's. Why the difference? Barcelona’s population occupies a space one-twentieth that of Atlanta. Moreover, because Barcelona is so dense, it’s cost effective to take steps to reduce emissions, such as enhancing public transit so people use their cars less often, and building district energy networks that efficiently distribute energy within a neighbourhood. In Atlanta, such solutions are ineffective and expensive.\n\"In Edmonton, the city’s proposed carbon budget of 135 million tonnes of carbon dioxide equivalent sets a cap on how much it can emit — ever. It’s not a budget for a year or a decade, but for the city’s entire future.\" @SSGcoop @YuillHerbert\nEdmonton’s plan envisions 15-minute communities — places where residents can reach all their basic needs, from work to shopping to recreation, with a 15-minute walk or bike ride. Planning such communities would make it easier for Edmontonians to adopt low-emissions lifestyles.\nThe carbon budget creates incentives for city planners to follow through and consider climate throughout their work. At the same time, incorporating a carbon budget into Edmonton’s city plan will create a policy framework for the city to support industries and infrastructure that reduce emissions, including renewable energy, green-building retrofits and construction, public transit and low-carbon technologies.\nPerhaps, most importantly, policies that reduce emissions improve quality of life. Edmonton’s city plan proposes steps to reduce air pollution and emissions, such as planting two million trees and making public transit more accessible, that will enhance health and well-being. A carbon budget isn’t just a tool for fighting climate change, it’s a pathway to better cities for all.","A Hot Topic: Cities tackle rising temperatures\nBy Kurt Shickman, Executive Director, Global Cool Cities Alliance\nThis month, C40 will highlight Cool Cities: the cities working to address our overheated urban spaces. The Cool Cities Network is a city-driven partnership between C40 and the Global Cool Cities Alliance to share the successes, and challenges that cities experience as they strive to achieve a cooler, more resilient future.\nTemperatures in the world’s cities are substantially higher than those of surrounding rural areas due to a phenomenon called the urban heat island effect. Urban heat islands form because many of our cities are made of dark, impermeable surfaces like asphalt that absorb heat, and lack enough green space. In addition, human activity and industry generate heat, and large cities block or slow down natural wind patterns. Taken together, these factors make our cities several degrees hotter, on average, than rural areas. During peak temperature periods, though, urban temperatures may spike significantly above the average urban/rural difference.\nNot only are cities hotter, but they are heating up nearly twice the global average rate. Rising urban temperatures are occurring in the context of a massive global urbanization. Before the end of this century, two out of every three people will live in urban spaces where excess heat will play a critical role in their lives.\nExtreme heat events are getting more frequent and more intense. Heat kills more people than any other natural disaster, and heat-related deaths tend to be underreported. Cities on dangerously hot days consistently experience spikes in mortality from all causes, sometimes as high as an additional 17 deaths per 100,000 people in US cities alone. Nine of the ten most deadly heat waves on record have occurred since 2000 (killing nearly 130,000 people in total).\nRising urban heat is an important factor in nearly every aspect of urban life including health, air quality, energy demand, and social equity.\nThe economic impact of unchecked urban overheating will be staggering. By 2100, urban heat and local climate change impacts will cost the average city 5.6% of their economic output. The most affected cities will lose 11% of their economic output as a result of urban heat and local climate change.\nThe good news is that there are proven strategies for mitigating urban heat that deliver huge potential benefits to cities and their residents. Research shows that reflective roofs and pavements and green infrastructure can deliver 12 times their cost in net benefits and cool cities by 0.5° Celsius. Cooling of that magnitude is equivalent to cancelling over a third of total global warming over the last century.\nC40 cities in the Cool Cities Network have proven to be global leaders on addressing urban heat. New York City recently committed over $100 million to implement its Cool Neighborhoods program. Los Angeles is targeting a nearly 3°F reduction in their urban heat over 20 years by establishing strong cool roofing requirements. Durban has undertaken a comprehensive urban heat study to understand where they are hot and why. Athens, Barcelona, and Paris have not only mapped their heat, but also where they have vulnerable populations, working to ensure their citizens have ready access to cool places on hot days. Tokyo has laid down miles and miles of solar-reflective cool pavement and piloted “smog-eating” cool coatings that stay cleaner longer and improve air quality. Washington, D.C. has built cool roofs into their procurement policy for municipal buildings and will save millions of dollars as a result.\nAdapting to extreme heat has the great advantage of tangibly improving the quality of life of countless urban citizens. The measures that are being implemented by these cities will not only help them in reducing urban heat but will also bring significant health benefits by improving air quality. Members of the Cool Cities Network are committed to moving towards a climate resilient – and cooler – future."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:0e5a6ce8-50b1-45a9-9c0a-5ce95e99bd8a>","<urn:uuid:fb71ee5f-ae4a-4e46-b0a5-67cd3bc0a090>"],"error":null}
{"question":"What is the connection between trade vulnerability measurements and soil carbon initiatives in addressing global challenges?","answer":"Trade vulnerability measurements through the TVI and soil carbon initiatives like the '4 per 1000' are interconnected approaches to addressing global challenges. The TVI includes climate change and environmental factors among its proxy indicators for measuring trade vulnerability, while the '4 per 1000' Initiative provides concrete solutions through soil carbon management that can help mitigate these vulnerabilities. Both approaches recognize the importance of environmental factors in economic resilience, with the TVI measuring vulnerability to inform policy decisions and trade negotiations, while soil carbon initiatives offer practical solutions for increasing resilience through sustainable agricultural practices that contribute to climate change adaptation and food security.","context":["August 19, 2020\n“Vulnerability” in the post COVID-19 world has become almost a hackneyed word. In a previous submission to Afronomicslaw, we explored how COVID-19 was bringing new dimensions to our study of vulnerability, and how the global health pandemic gives credibility to – and ultimately validates – the relationship we have explored between vulnerability and trade using our Trade Vulnerability Index (TVI) we developed last year. That work had the aim of providing the quantitative tools for detecting, measuring and ultimately quantifying the degree of trade vulnerability of countries in aid of achieving greater trade negotiating outcomes for the most vulnerable countries and sectors at the World Trade Organization (WTO).\nCOVID-19 generated a new buzz around our work and renewed interest in the study of vulnerability. Since then, we have reappraised our initial piece, with the intention of moving beyond its conceptual foundations toward a more practical and concrete application of the work. We realized however, that for many, there is curiosity around the TVI project, and ultimately what we are trying to achieve through it. In this reflective piece, we present briefly the TVI in a nutshell – its aims, methodology and conceptual premises – and then provide initial thoughts on the way forward under our TVI project.\nThe work on the TVI emanated as a response to a proposal made by at the WTO by the United States to introduce an “objective” and binary approach to the question of eligibility for special and differential treatment (S&DT). In a Communication published in January 2019 entitled “An Undifferentiated WTO: Self-Declared Development Status Risks Institutional Relevance”, the US declared that the current S&DT model based on self-declaration as a “developing country” was no longer feasible and was to blame for the current deadlock in the WTO’s negotiation function. The US followed its Communication with a concrete proposal tabled for the WTO’s General Council’s consideration in February 2019. It proposed to exclude – in current and future WTO negotiations – from the developing country category any country that satisfies any one of the four criteria, including that a developing country that is classified as “high income” by World Bank would not qualify. This caused some consternation from the ranks of developing countries, including those in the Caribbean, some of which qualify as high-income based on GNI-per capita measurements of development.\nIn response, we proposed a TVI to steer the discussion away from GNI- or GDP- per capita as a basis for differentiation. While we did not take issue with an “objective-criteria” approach per se, we did object to the particular choice of criteria which in our view do not speak to the trade-related reasons why some countries have a greater claim to S&DT at the WTO than others.\nIn the past, small states at the WTO sought to create negotiating space around “vulnerability” by proposing a grouping of “Small Vulnerable Economies” (SVEs) to articulate and promote their negotiating positions in different negotiating fora. Caribbean and other small countries encountered pushback from other WTO members who objected to the creation of a new sub-category of developing countries at the WTO. The result was limited traction in pursuing SVE interests in the context of ongoing negotiations at the WTO. What? Our work sought to build on the SVE effort by developing the empirical basis for vulnerability and applying it to the WTO context. In sum, the TVI seeks to quantify the vulnerability of countries and, on that basis, propose S&DT that is responsive to those needs.\nBuilding on empirical work done already by the Caribbean Development Bank on a Multi-Dimensional Vulnerability Index, we first set out to establish the conceptual basis for a TVI, pointing out that detailed empirical work would be reserved for future study. We set out to distinguish our work from previous efforts by explaining that the TVI could be used to measure a country’s or sector’s vulnerability based on a methodology that uses select “trade proxy indicators” to develop a “scoring” or index that is linked to S&DT provisions. Based on that “scoring”, the TVI could be used as a guide for determining which countries and/or sectors could benefit from S&DT without creating a sub-category of developing countries. While it was contemplated that the majority of countries or sectors determined to be “vulnerable” would be those of small economies, there was no a priori exclusion of larger economies whose sectors might also qualify depending on which proxy was being tested or measured.\nWe adopted a methodological approach to the development of the TVI involving four simple steps: step 1 involves determining the causes of trade vulnerability; step 2 selects and compiles proxy indicators; step 3 applies normalization methodology to data; and step 4 compute sub-indices and aggregates the index.\nThe paper identified the causes of trade vulnerability and suggested possible proxy indicators but did not progress to steps 3 and 4 which involve the computation of the index. Under steps 1 and 2, we scoured the existing literature on vulnerability and proceeded to identify the following indicative “categories” of vulnerability – or “proxies” – upon which the TVI would be constructed, namely: export concentration, export destination, trade shock, trade openness, dependence on strategic imports, reliance on external finance, market share of global trade, remoteness, instability of agricultural production, economic diversification, small size, susceptibility of natural disasters, climate change (and now health pandemics), market flexibility, and political, social and environmental governance. Within each of these broad categories we went further to identify proxy indicators. We anticipate that once these proxies are compiled, a composite index would be a simple arithmetic average of the sub-indices. The assessment of country or sector vulnerability using the TVI is determined on the composite index score relative to a threshold.\nThe paper also engaged in a preliminary mapping exercise that sought to establish connections between the trade proxy indicators and the types of S&DT provisions that might be best suited to address the vulnerabilities identified. This is important because it makes the TVI a practical tool for use as part of an evidence-based approach to guide eligibility for S&DT.\nIn terms of next steps, our work will proceed on two main fronts: buttressing its empirical underpinnings and improving the prospects for its practical application and utilisation.\nOn the empirical front, work will commence under steps 3 and 4 outlined above. This will involve: greater specificity in the selection of proxy indicators, selection of the countries that will be used in the analysis, compiling of the proxy indicators for the time horizon of the analysis, aggregating the index and evaluating the results with other available information to assess its reliability. Our previous work identified options for proxy indicators for each of the categories of vulnerability in the TVI. The next step is to assess these options to determine the most suitable proxy indicator. The selection of the proxy indicator can be guided by criteria such as: suitability to the category of vulnerability that is being measured, simplicity of the proxy indicator and availability of data. To remove the perceived subjectivity in the selection of appropriate indicators it is also possible to utilize statistical methods such as Principal Component Analysis.\nDetermining the country coverage of the TVI is another important step in the construction of the TVI. It would be important to include in the research countries at various stages of development such as developed and developing. Furthermore, consideration should be given to countries with differing demographic characteristics such as Small Island Developing States and countries in different geographic regions, e.g. Caribbean, Asia-Pacific, Africa, Indian Ocean, Mediterranean and South China Sea (AIMS). This would enable improved comparability of results and enable the research to make stronger conclusions.\nCompiling the index relates to the mechanics of translating the data into meaningful information. This would involve technical steps including normalization of the data to facilitate direct comparison, weighting the sub-indices and aggregating the index. The final stage will be evaluating the results of the TVI. This will involve sensitivity analysis to gauge the robustness of the results and analysing the results at the aggregate and dis-aggregated levels.\nThe hardest part in completing the work under the TVI is not technical, but rather putting it into action as a tool for policymakers and trade negotiators and avoiding the pitfalls that stymied success in the past. Beyond the instructive value of the TVI in quantifying and measuring the degrees of vulnerability, the authors’ aim is for it to be used to justify claims for S&DT in specific negotiating contexts, and equally as a guide for separating out those vulnerabilities that can be overcome through trade rules – and therefore permit graduation by states – and those which are permanent features over which the international community can coalesce and galvanise resources both inside and outside trade negotiating arenas.","Climate change adaptation, mitigation and food security may be addressed at the same time by enhancing soil organic carbon (SOC) sequestration through environmentally sound land management practices. This is promoted by the “4 per 1000” Initiative, a multi-stakeholder platform aiming at increasing SOC storage through sustainable practices. The scientific and technical committee of the Initiative is working to identify indicators, research priorities and region-specific practices needed for their implementation. The Initiative received its name due to the global importance of soils for climate change, which can be illustrated by a thought experiment showing that an annual growth rate of only 0.4% of the standing global SOC stocks would have the potential to counterbalance the current increase in atmospheric CO2. However, there are numerous barriers to the rise in SOC stocks and while SOC sequestration can contribute to partly offsetting greenhouse gas emissions, its main benefits are related to increased soil quality and climate change adaptation. The Initiative provides a collaborative platform for policy makers, practitioners, scientists and stakeholders to engage in finding solutions. Criticism of the Initiative has been related to the poor definition of its numerical target, which was not understood as an aspirational goal. The objective of this paper is to present the aims of the initiative, to discuss critical issues and to present challenges for its implementation. We identify barriers, risks and trade-offs and advocate for collaboration between multiple parties in order to stimulate innovation and to initiate the transition of agricultural systems toward sustainability.\nThis is a preview of subscription content, access via your institution.\nBuy single article\nInstant access to the full article PDF.\nTax calculation will be finalised during checkout.\nSubscribe to journal\nImmediate online access to all issues from 2019. Subscription will auto renew annually.\nTax calculation will be finalised during checkout.\nBarre, P., H. Durand, C. Chenu, P. Meunier, D. Montagne, G. Castel, D. Billiou, L. Soucemarianadin, et al. 2017. Geological control of soil organic carbon and nitrogen stocks at the landscape scale. Geoderma 285: 50–56.\nBaveye, P.C., J. Berthelin, D. Tessier, and G. Lemaire. 2018. The “4 per 1000” initiative: A credibility issue for the soil science community? Geoderma 309: 118–123.\nChabbi, A., J. Lehmann, P. Ciais, H.W. Loescher, M.F. Cotrufo, A. Don, M. SanClements, L. Schipper, et al. 2017. Aligning agriculture and climate policy. Nature Climate Change 7: 307–309.\nChenu, C., D.A. Angers, P. Barré, D. Derrien, D. Arrouays, and J. Balesdent. 2019. Increasing organic stocks in agricultural soils: Knowledge gaps and potential innovations. Soil and Tillage Research 118: 42–51.\nCorbeels, M., K. Naudin, H. Guibert, E. Torquebiau, and R. Cardinael. 2019. Is 4 per 1000 soil carbon storage attainable with agroforestry and conservation agriculture in sub-Saharan Africa? Soil & Tillage Research 188: 16–26.\nde Vries, W. 2018. Soil carbon 4 per mille: A good initiative but let’s manage not only the soil but also the expectations: Comment on Minasny et al. (2017). Geoderma 292: 59–86.\nDiacono, M., and F. Montemurro. 2010. Long-term effects of organic amendments on soil fertility. A review. Agriculture for Sustainable Development 30: 401–422.\nDitzler, L., T.A. Breland, C. Francis, M. Chakraborty, D.K. Singh, A. Srivastava, F. Eyhorn, J.C.J. Groot, et al. 2018. Identifying viable nutrient management interventions at the farm level: The case of smallholder organic Basmati rice production in Uttarakhand, India. Agricultural Systems 161: 61–71.\nFrank, S., P. Havlík, J.F. Soussana, A. Levesque, H. Valin, L. Wollenberg, U. Kleinwechter, O. Fricko, et al. 2017. Reducing greenhouse gas emissions in agriculture without compromising food security? Environmental Research Letters 12: 105004.\nFujisaki, K., T. Chevallier, L. Chapuis-Lardy, A. Albrecht, T. Razafimbelo, D. Masse, and J.-L. Chotte. 2018. Soil carbon stock changes in tropical croplands are mainly driven by carbon inputs: A synthesis. Agriculture, Ecosystems & Environment 259: 147–158.\nHutchinson, J.J., C.A. Campbell, and R. Desjardins. 2007. Some perspectives on carbon sequestration in agriculture. Agricultural and Forest Meteorology 142: 288–302.\nIPCC. 2006. 2006 IPCC guidelines for national greenhouse gas inventories. In Prepared by the national greenhouse gas Inventories programme, eds. H.S. Eggleston, L. Buendia, K. Miwa, T. Ngara, and K. Tanabe. Japan: IGES.\nJackson, R.B., E.G. Jobbágy, R. Avissar, S.R. Baidya, D.F. Barrett, C.W. Cook, K.A. Farley, D.C. le Maitre, et al. 2005. Trading water for carbon with biological carbon sequestration. Science 5756: 1944–1947.\nKirkby, C.A., A.E. Richardson, L.J. Wade, J.B. Passioura, G.D. Batten, C. Blanchard, and J.A. Kirkegaard. 2014. Nutrient availability limits carbon storage in agricultural soils. Soil Biology and Biochemistry 68: 204–209.\nKon Kam King, J., C. Granjou, J. Fournil, and L. Cecillon. 2018. Soil sciences and the French 4 per 1000 Initiative—The promises of underground carbon. Energy Research & Social Science 45: 144–152.\nLadha, J.K., C.K. Reddy, A.T. Padre, and C.V. Kessel. 2011. Role of nitrogen fertilization in sustaining organic matter in cultivated soils. Journal of Environmental Quality 40: 1756–1766.\nLal, R. 2004. Soil carbon sequestration impacts on global climate change and food security. Science 304: 1623–1627.\nLal, R. 2019. Promoting “4 Per Thousand” and “Adapting African Agriculture” by south-south cooperation: Conservation agriculture and sustainable intensification. Soil and Tillage Research 118: 27–34.\nLeifeld, J., and L. Menichetti. 2018. The underappreciated potential of peatlands in global climate change mitigation strategies. Nature Communications 9: 1071.\nLugato, E., A. Leip, and A. Jones. 2018. Mitigation potential of soil carbon management overestimated by neglecting N2O emissions. Nature Climate Change 8: 219–223.\nMaroušek, J., M. Vochozka, J. Plachý, and J. Žák. 2017. Glory and misery of biochar. Clean Technologies and Environmental Policy 19: 311–317.\nMinasny, B., D. Arrouays, A.B. McBratney, D.A. Angers, A. Chambers, V. Chaplot, and L. Winowiecki. 2018. Rejoinder to Comments on Minasny et al., 2017 Soil carbon 4 per mille. Geoderma 292: 59–86.\nNath, J.A., R. Lal, G.W. Siles, K. Dasa, and A.K. Das. 2018. Managing India’s small landholder farms for food security and achieving the “4 per Thousand” target. The Science of the Total Environment 634: 1024–1033.\nPan, W.L., W.S. Schillinger, F.L. Young, E. Kirby, and G.G. Yorgey, et al. 2017. Integrating old principles and new technologies into win-win scenarios for farm and climate. Frontiers in Environmental Science. https://doi.org/10.3389/fenvs.2017.00076.\nPaustian, K., J. Lehmann, S. Ogle, D. Reay, G.P. Robertson, and P. Smith. 2016. Climate-smart soils. Nature 532: 49–57.\nPingali, P.L. 2012. Green revolution: impacts, limits and the path ahead. PNAS 109: 12302–12308.\nPittelkow, C.M., X. Liang, B.A. Linquist, K.J. van Groenigen, J. Lee, M.E. Lundy, N. van Gestel, J. Six, et al. 2015. Productivity limits and potentials of the principles of conservation agriculture. Nature 517: 365–368. https://doi.org/10.1038/nature13809.\nPoeplau, C., and A. Don. 2015. Carbon sequestration in agricultural soils via cultivation of cover crops—A meta-analysis. Agriculture, Ecosystems & Environment 200: 33–41.\nPoulton, P., J. Johnston, A. MacDonald, R. White, and D. Powlson. 2018. Major limitations to achieving “4 per 1000″ increases in soil organic carbon stock in temperate regions: Evidence from long-term experiments at Rothamsted Research. UK. Global Change Biology 12: 3218–3221.\nPowlson, D.S., A.P. Whitmore, and A.W.T. Goulding. 2011. Soil carbon sequestration to mitigate climate change: a critical re-examination to identify the true and the false. European Journal of Soil Science 62: 42–55.\nRumpel, C., F. Amiraslani, L.-S. Koutika, P. Smith, D. Whitehead, and E. Wollenberg. 2018. Put more carbon in soils to meet Paris climate pledges. Nature 564: 32–34.\nSanderman, J., C. Creamer, W.T. Baisden, M. Farrell, and S. Fallon. 2017. Greater soil carbon stocks and faster turnover rates with increasing agricultural productivity. Soil 3: 1–16.\nScharlemann, J.P.W., E.V.J. Tanner, R. Hiederer, and V. Kapos. 2014. Global soil carbon: understanding and managing the largest terrestrial carbon pool. Carbon Manag. 5: 81–91.\nSchiefer, J., G.J. Lair, C. Lüthgens, E.M. Wild, P. Steiner, and W.H. Blum. 2018. The increase of soil organic carbon as proposed by the “4/1000 initiative” is strongly limited by the status of soil development—A case study along a substrate age gradient in Central Europe. The Science of the Total Environment 628–629: 840–847.\nSmith, P. 2016. Soil carbon sequestration and biochar as negative emission technologies. Global Change Biology 22: 1315–1324.\nSmith, P., S.J. Davis, F. Creutzig, S. Fuss, J. Minx, B. Gabrielle, et al. 2016. Biophysical and economic limits to negative CO2 emissions. Nature Climate Change 6: 42–50.\nSommer, R., and D. Bossio. 2014. Dynamics and climate change mitigation potential of soil organic carbon sequestration. Journal Environmental Management 144: 83–87.\nSoussana, J.F., S. Lutfalla, R. Lal, C. Chenu, and P. Ciais. 2017. Letter to the editor: answer to the viewpoint “sequestering soil organic carbon: a nitrogen dilemma” by van Groenigen et al. (2017). Environmental Science and Technology 51: 11502.\nSoussana, J.F., S. Lutfalla, F. Ehrhardt, T. Rosenstock, C. Lamanna, P. Havlík, and R. Lal. 2019. Matching policy and science: Rationale for the “4 per 1000 - soils for food security and climate” initiative. Soil and Tillage Research 188: 3–15.\nSTC. 2017. The ‘4 per 1000’ Research Priorities. https://www.4p1000.org/sites/default/files/content/gov_cst_en_consortium_3-4-4p1000_research_priorities.pdf.\nTilman, D., C. Balzer, J. Hill, and B.L. Befort. 2011. Global food demand and the sustainable intensification of agriculture. Proceedings of the National Acadamy of Science USA 108: 20260–20264.\nTrost, B., A. Prochnow, K. Drastig, A. Meyer-Aurich, F. Ellmer, and M. Baumecker. 2013. Irrigation, soil organic carbon and N2O emissions. A review. Agronomy for Sustainable Development 33: 733–749.\nUNFCCC. 2018. Koronivia Joint Work on Agriculture (decision 4/CP.23).\nVan Groenigen, J.W., C. Van Kessel, B.A. Hungate, O. Oenema, D.S. Powlson, and K.J. Van Groenigen. 2017. Sequestering soil organic carbon: A nitrogen dilemma. Environmental Science and Technology 51: 4738–4739.\nVandenBygaart, A.J. 2018. Comments on soil carbon 4 per mille by Minasny et al. 2017. Geoderma 309: 113–114.\nWhite, R.E., B. Davidson, S.K. Lam, and D. Chen. 2018. A critique of the paper “Soil carbon 4 per mille” by Minasny et al. (2017). Geoderma 309: 115–117.\nAuthors would like to acknowledge the executive secretariat of the 4p1000 initiative, Charlotte Verger and Claire Weill for their valuable contributions during the preparation of this manuscript. The input of PS contributes to the UK NERC-funded Soils-R-GGREAT project (NE/P019455/1).\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nAbout this article\nCite this article\nRumpel, C., Amiraslani, F., Chenu, C. et al. The 4p1000 initiative: Opportunities, limitations and challenges for implementing soil organic carbon sequestration as a sustainable development strategy. Ambio 49, 350–360 (2020). https://doi.org/10.1007/s13280-019-01165-2\n- Carbon sequestration\n- Climate change\n- Food security"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:d89abff7-a406-402f-b135-5c69f8c367f4>","<urn:uuid:51989899-4046-4633-bd3a-74993a9f67f2>"],"error":null}
{"question":"Could you explain how the blank space rejection policies in immigration forms compare to emotional recognition in facial analysis when it comes to cultural sensitivity and bias?","answer":"Both systems demonstrate significant issues with cultural sensitivity and bias. USCIS's blank space policy failed to account for cultural differences, rejecting applications when applicants used their native alphabet or when questions weren't culturally relevant to them. Similarly, emotional recognition technology, despite claiming to detect universal emotions, faces challenges with cultural variations - for example, in Japan, smiles can indicate politeness rather than happiness, while Brazilians express happiness with broader smiles. Both systems have been criticized for potential discrimination: USCIS's policy particularly impacted vulnerable populations, while emotion detection technology has shown bias in assigning more negative emotions to black men's faces compared to white faces. These issues highlight how automated systems in both contexts can perpetuate biases and fail to account for cultural nuances.","context":["AILA's Featured Issues pages provide a one-stop shop on current immigration-related issues that AILA is actively tracking. This includes government actions and resources, AILA's policy recommendations, and materials and talking points to engage with Congress and the press.Start Your Research\nAILALink puts an entire immigration law library at your fingertips! Search the AILALink database for all your practice needs—statutes, regs, case law, agency guidance, publications, and more.\nAILA Doc. No. 20102030 | Dated April 1, 2021\nAs a result of class action litigation in Vangala v. USCIS challenging USCIS's “No Blank Space” policy, USCIS has agreed to stop implementing the rejection policy for asylum applications and U visa petitions starting December 28, 2020. On April 1, 2021, USCIS confirmed that it will no longer reject Form I-589, Form I-612, or Form I-918 if an applicant leaves a blank space.\nThis AILA policy brief describes how USCIS upended long-standing practice, without notice, and began rejecting forms that left questions blank or did not use specific terminology to indicate that a question was inapplicable, leading to capricious rejections.\nIn October 2019, USCIS began rejecting forms that left questions blank or did not use specific terminology to indicate that a question was inapplicable, upended long-standing practice, without notice. This has led to capricious rejections of many humanitarian benefit applications leading to significant consequences for vulnerable individuals. These rejections are particularly egregious as the majority of rejected applications left spaces blank for information that was not relevant to an individual’s eligibility, such as leaving blank the space asking for an individual’s name in a native alphabet when the native alphabet was the same as English.\nMany USCIS forms contain questions that are not relevant to all applicants. In addition, some questions—such as those concerning the applicant’s siblings—contain multiple similar fields to be filled. Under the new USCIS policy, an applicant who leaves a field blank will have their application or petition rejected as incomplete, even if the field is one that was not relevant to that individual’s eligibility.\nExamples of fields that might be left blank because they are not applicable to all applicants include:\nIn addition, many applicants, after listing their family members, will leave unused fields blank.\nThe new USCIS policy calls for rejected all forms that contain these (or similar) omissions. A recent AILA examination reviewed 189 rejected Form I-589 applications and found that all 189 had been rejected for leaving one or more fields blank.\nOfficials have also rejected applications that do not use the correct terminology to indicate that a certain field does not apply—even when a form’s instructions plainly state that other terms will be accepted! For example, the instructions for Form I-589 read:\n“If any question does not apply to you or you do not know the information requested, answer ‘none’, ’not applicable,’ or \"unknown.’”\nHowever, in many of the cases examined, the only accepted answer for questions that did not apply to the applicant was “N/A.” Applications that used other terminology enumerated in the instructions, such as “none” or “not applicable” were rejected.\nUSCIS has signaled its plans to expand the “no blank space” policy to a broad array of applications and petitions, but we must hold USCIS accountable to stop the spread of this unnecessary and egregious policy and demand that the agency reverse the damage it has already exacted on the most vulnerable populations.\nGet educated on the devastating consequences of this policy change and to better understand how it impacts your practice.\nCite as AILA Doc. No. 20102030.\nAmerican Immigration Lawyers Association\n1331 G Street NW, Suite 300\nWashington, DC 20005\nCopyright © 1993-2021\nAmerican Immigration Lawyers Association.\nAILA.org should not be relied upon as the exclusive source for your legal research. Nothing on AILA.org constitutes legal advice, and information on AILA.org is not a substitute for independent legal advice based on a thorough review and analysis of the facts of each individual case, and independent research based on statutory and regulatory authorities, case law, policy guidance, and for procedural issues, federal government websites.","Could a program detect potential terrorists by reading their facial expressions and behavior? This was the hypothesis put to the test by the US Transportation Security Administration (TSA) in 2003, as it began testing a new surveillance program called the Screening of Passengers by Observation Techniques program, or Spot for short.\nWhile developing the program, they consulted Paul Ekman, emeritus professor of psychology at the University of California, San Francisco. Decades earlier, Ekman had developed a method to identify minute facial expressions and map them on to corresponding emotions. This method was used to train “behavior detection officers” to scan faces for signs of deception.\nBut when the program was rolled out in 2007, it was beset with problems. Officers were referring passengers for interrogation more or less at random, and the small number of arrests that came about were on charges unrelated to terrorism. Even more concerning was the fact that the program was allegedly used to justify racial profiling.\nEkman tried to distance himself from Spot, claiming his method was being misapplied. But others suggested that the program’s failure was due to an outdated scientific theory that underpinned Ekman’s method; namely, that emotions can be deduced objectively through analysis of the face.\nIn recent years, technology companies have started using Ekman’s method to train algorithms to detect emotion from facial expressions. Some developers claim that automatic emotion detection systems will not only be better than humans at discovering true emotions by analyzing the face, but that these algorithms will become attuned to our innermost feelings, vastly improving interaction with our devices.\nBut many experts studying the science of emotion are concerned that these algorithms will fail once again, making high-stakes decisions about our lives based on faulty science.\nYour face: a $20bn industry\nEmotion detection technology requires two techniques: computer vision, to precisely identify facial expressions, and machine learning algorithms to analyze and interpret the emotional content of those facial features.\nTypically, the second step employs a technique called supervised learning, a process by which an algorithm is trained to recognize things it has seen before. The basic idea is that if you show the algorithm thousands and thousands of images of happy faces with the label “happy” when it sees a new picture of a happy face, it will, again, identify it as “happy”.\nA graduate student, Rana el Kaliouby, was one of the first people to start experimenting with this approach. In 2001, after moving from Egypt to Cambridge University to undertake a PhD in computer science, she found that she was spending more time with her computer than with other people. She figured that if she could teach the computer to recognize and react to her emotional state, her time spent far away from family and friends would be less lonely.\nKaliouby dedicated the rest of her doctoral studies to work on this problem, eventually developing a device that assisted children with Asperger syndrome read and respond to facial expressions. She called it the “emotional hearing aid”.\nIn 2006, Kaliouby joined the Affective Computing lab at the Massachusetts Institute of Technology, where together with the lab’s director, Rosalind Picard, she continued to improve and refine the technology. Then, in 2009, they co-founded a startup called Affectiva, the first business to market “artificial emotional intelligence”.\nAt first, Affectiva sold their emotion detection technology as a market research product, offering real-time emotional reactions to ads and products. They landed clients such as Mars, Kellogg’s and CBS. Picard left Affectiva in 2013 and became involved in a different biometrics startup, but the business continued to grow, as did the industry around it.\nAmazon, Microsoft and IBM now advertise “emotion analysis” as one of their facial recognition products, and a number of smaller firms, such as Kairos and Eyeris, have cropped up, offering similar services to Affectiva.\nBeyond market research, emotion detection technology is now being used to monitor and detect driver impairment, test user experience for video games and to help medical professionals assess the wellbeing of patients.\nKaliouby, who has watched emotion detection grow from a research project into a $20bn industry, feels confident that this growth will continue. She predicts a time in the not too distant future when this technology will be ubiquitous and integrated in all of our devices, able to “tap into our visceral, subconscious, moment by moment responses”.\nA database of 7.5m faces from 87 countries\nAs with most machine learning applications, progress in emotion detection depends on accessing more high-quality data.\nAccording to Affectiva’s website, they have the largest emotion data repository in the world, with over 7.5m faces from 87 countries, most of it collected from opt-in recordings of people watching TV or driving their daily commute.\nThese videos are sorted through by 35 labelers based in Affectiva’s office in Cairo, who watch the footage and translate facial expressions to corresponding emotions – if they see lowered brows, tight-pressed lips and bulging eyes, for instance, they attach the label “anger”. This labeled data set of human emotions is then used to train Affectiva’s algorithm, which learns how to associate scowling faces with anger, smiling faces with happiness, and so on.\nThis labelling method, which is considered by many in the emotion detection industry to be the gold standard for measuring emotion, is derived from a system called the Emotion Facial Action Coding System (Emfacs) that Paul Ekman and Wallace V Friesen and developed during the 1980s.\nThe scientific roots of this system can be traced back to the 1960s, when Ekman and two colleagues hypothesized that there are six universal emotions – anger, disgust, fear, happiness, sadness and surprise – that are hardwired into us and can be detected across all cultures by analyzing muscle movements in the face.\nTo test the hypothesis, they showed diverse population groups around the world photographs of faces, asking them to identify what emotion they saw. They found that despite enormous cultural differences, humans would match the same facial expressions with the same emotions. A face with lowered brows, tight-pressed lips and bulging eyes meant “anger” to a banker in the United States and a semi-nomadic hunter in Papua New Guinea.\nOver the next two decades, Ekman drew on his findings to develop his method for identifying facial features and mapping them to emotions. The underlying premise was that if a universal emotion was triggered in a person, then an associated facial movement would automatically show up on the face. Even if that person tried to mask their emotion, the true, instinctive feeling would “leak through”, and could therefore be perceived by someone who knew what to look for.\nThroughout the second half of the 20th century, this theory – referred to as the classical theory of emotions – came to dominate the science of emotions. Ekman made his emotion detection method proprietary and began selling it as a training program to the CIA, FBI, Customs and Border Protection and the TSA. The idea of true emotions being readable on the face even seeped into popular culture, forming the basis of the show Lie to Me.\nAnd yet, many scientists and psychologists researching the nature of emotion have questioned the classical theory and Ekman’s associated emotion detection methods.\nIn recent years, a particularly powerful and persistent critique has been put forward by Lisa Feldman Barrett, professor of psychology at Northeastern University.\nBarrett first came across the classical theory as a graduate student. She needed a method to measure emotion objectively and came across Ekman’s methods. On reviewing the literature, she began to worry that the underlying research methodology was flawed – specifically, she thought that by providing people with preselected emotion labels to match to photographs, Ekman had unintentionally “primed” them to give certain answers.\nShe and a group of colleagues tested the hypothesis by re-running Ekman’s tests without providing labels, allowing subjects to freely describe the emotion in the image as they saw it. The correlation between specific facial expressions and specific emotions plummeted.\nSince then, Barrett has developed her own theory of emotions, which is laid out in her book How Emotions Are Made: the Secret Life of the Brain. She argues there are no universal emotions located in the brain that are triggered by external stimuli. Rather, each experience of emotion is constructed out of more basic parts.\n“They emerge as a combination of the physical properties of your body, a flexible brain that wires itself to whatever environment it develops in, and your culture and upbringing, which provide that environment,” she writes. “Emotions are real, but not in the objective sense that molecules or neurons are real. They are real in the same sense that money is real – that is, hardly an illusion, but a product of human agreement.”\nBarrett explains that it doesn’t make sense to talk of mapping facial expressions directly on to emotions across all cultures and contexts. While one person might scowl when they’re angry, another might smile politely while plotting their enemy’s downfall. For this reason, assessing emotion is best understood as a dynamic practice that involves automatic cognitive processes, person-to-person interactions, embodied experiences, and cultural competency. “That sounds like a lot of work, and it is,” she says. “Emotions are complicated.”\nKaliouby agrees – emotions are complex, which is why she and her team at Affectiva are constantly trying to improve the richness and complexity of their data. As well as using video instead of still images to train their algorithms, they are experimenting with capturing more contextual data, such as voice, gait and tiny changes in the face that take place beyond human perception. She is confident that better data will mean more accurate results. Some studies even claim that machines are already outperforming humans in emotion detection.\nBut according to Barrett, it’s not only about data, but how data is labeled. The labelling process that Affectiva and other emotion detection companies use to train algorithms can only identify what Barrett calls “emotional stereotypes”, which are like emojis, symbols that fit a well-known theme of emotion within our culture.\nAccording to Meredith Whittaker, co-director of the New York University-based research institute AI Now, building machine learning applications based on Ekman’s outdated science is not just bad practice, it translates to real social harms.\n“You’re already seeing recruitment companies using these techniques to gauge whether a candidate is a good hire or not. You’re also seeing experimental techniques being proposed in school environments to see whether a student is engaged or bored or angry in class,” she says. “This information could be used in ways that stop people from getting jobs or shape how they are treated and assessed at school, and if the analysis isn’t extremely accurate, that’s a concrete material harm.”\nKaliouby says that she is aware of the ways that emotion detection can be misused and takes the ethics of her work seriously. “Having a dialogue with the public around how this all works and where to apply and where not to apply it is critical,” she told me.\nHaving worn a headscarf in the past, Kaliouby is also keenly aware of the importance of building diverse data sets. “We make sure that when we train any of these algorithms the training data is diverse,” she says. “We need representation of Caucasians, Asians, darker skin tones, even people wearing the hijab.”\nThis is why Affectiva collects data from 87 countries. Through this process, they have noticed that in different countries, emotional expression seems to take on different intensities and nuances. Brazilians, for example, use broad and long smiles to convey happiness, Kaliouby says, while in Japan there is a smile that does not indicate happiness, but politeness.\nAffectiva have accounted for this cultural nuance by adding another layer of analysis to the system, compiling what Kaliouby calls “ethnically based benchmarks”, or codified assumptions about how an emotion is expressed within different ethnic cultures.\nBut it is precisely this type of algorithmic judgment based on markers like ethnicity that worries Whittaker most about emotion detection technology, suggesting a future of automated physiognomy. In fact, there are already companies offering predictions for how likely someone is to become a terrorist or pedophile, as well as researchers claiming to have algorithms that can detect sexuality from the face alone.\nSeveral studies have also recently shown that facial recognition technologies reproduce biases that are more likely to harm minority communities. One published in December last year shows that emotion detection technology assigns more negative emotions to black men’s faces than white counterparts.\nWhen I brought up these concerns with Kaliouby she told me that Affectiva’s system does have an “ethnicity classifier”, but that they are not using it right now. Instead, they use geography as a proxy for identifying where someone is from. This means they compare Brazilian smiles against Brazilian smiles, and Japanese smiles against Japanese smiles.\n“What about if there was a Japanese person in Brazil,” I asked. “Wouldn’t the system think they were as Brazilian and miss the nuance of the politeness smile?”\n“At this stage,” she conceded, “the technology is not 100% foolproof.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:830ced0f-2798-4179-8780-48a3f9306837>","<urn:uuid:5581c4f2-30c6-4b90-b609-2fe5ab53dca8>"],"error":null}
{"question":"How did the Communist parties' reactions to rising political threats differ in Germany and Russia during their respective revolutionary periods?","answer":"The Communist parties in Germany and Russia had markedly different outcomes. In Germany, the Communists refused to cooperate with the Social Democrats despite together outnumbering the Nazis in the Reichstag, blinded by their hatred of socialists and belief that Hitler's fascism represented capitalism's last stage. This lack of cooperation, especially after the Reichstag fire, ultimately helped Hitler gain power. In contrast, in Russia, the Communists successfully organized and unified their efforts, leading the November 1917 revolution that overthrew the provisional government. Under Lenin's leadership, they established the first socialist state and succeeded in maintaining power despite facing opposition from 21 imperialist armies.","context":["What were the significant events that took place as Nazism became powerful?\nThe first significant event in the rise of National Socialism (\"Nazi\" was a pejorative term) was the failed Beer Hall Putsch in which Hitler and other National Socialists attempted to take over the government of Munich by coup, much as Mussolini had taken over the Italian government. Upon the failure of the Putsch, Hitler was sent to prison where he wrote his political testament, Mein Kampf. The National Socialists were aided by the Great Depression when German unemployment reached 43 per cent.\nOther political ideologies at the time were the Communists and the Social Democrats.The Communists refused to cooperate with the Social Democrats even though the two parties together outnumbered the Nazis in the Reichstag. German Communists were blinded by the hatred of socialists and by the belief that Hitler’s fascist ideas represented the last throes of capitalism and a communist revolution would soon follow. Social Democratic leaders pleaded with the Communists for a temporary alliance to stop Hitler, at one point even posing their pleas at the Soviet Embassy, but there was no use. This lack of cooperation was exacerbated by a fire which destroyed the Reichstag. The fire was blamed on a young Communist radical who was subsequently guillotined. There was substantial thinking among historians for many years that the Nazi’s had started the fire to intensify the dispute with the Communists; however recent historical evidence indicates that the young man was indeed a Communist agent intent on bringing down the government by any possible means. Ironically, his actions played into the hands of Hitler.\nHitler was an expert in dirty backroom politics. He gained the support of people in the army and big business, who thought they use him to their own advantage. Conservative and nationalistic politicians also thought that they could use him. Thus, when Hitler demanded that he be named Chancellor as a condition of joining the government, they accepted his demand. On January 30, 1933, President Hindenburg named Hitler as Chancellor. The Reichstag fire, after a particularly raucous election, played easily into Hitler’s hands. He used the fire to persuade President Hindenburg to invoke the Emergency Powers provision of the Constitution, which allowed rule by decree. Emergency acts were passed which practically abolished freedom of speech and assembly as well as most personal liberties. When the Nazi’s won only 44 per cent of the Reichstag seats Hitler outlawed the communist party and had its representatives in the Reichstag arrested. On March 23, 1933, the Reichstag passed the Enabling Act which gave Hitler absolute dictatorial power for four years. Under the guise of legality, the Nazi’s slowly dismantled the opposition, and Germany was soon a one party state in which only the National Socialist Party was legal.\nThere were a number of significant events in Germany as the Nazi Party rose to power.\nThe first was the Beer Hall Putsch in Munich. This attempted rebellion did not work, but it pushed the Nazis to prominence.\nThe second was the election of 1930. In this election, the Nazis gained enough seats in the Reichstag to become the second largest party in German politics.\nThis led eventually to the third, and most importatnt, event in the Nazis' rise to power. That event was the appointment of Adolf Hitler as chancellor of Germany. This happened in January of 1933.\nOf course, behind all of this was the economic and social turmoil that was rocking Germany in the years after WWI. There are no single important events in that process, but the turmoil allowed the Nazi ideology to become popular and allowed the Nazis to rise to power.","In 1917, two revolutions swept through Russia, ending centuries of imperial rule and setting in motion political and social changes that would lead to the formation of the Soviet Union. In March, growing civil unrest, coupled with chronic food shortages, erupted into an open revolt, forcing the abdication of Nicholas II, the last Russian Czar. Just months later, the newly installed provisional government was itself overthrown by the more radical revolutionary party in the former Soviet Union and another revolution broke up rapidly. The revolution was led by Vladimir Lenin and was based upon Lenin's writing on the ideas of Karl Marx, a political ideology often known as Marxism-Leninism. It marked the beginning of the spread of Communism in the 20th century. It was less sporadic than the revolution of February and came about as the result of deliberate planning and coordinated activity to that end.\nThe first Russian revolution of 1905 was the expression of a gigantic conflict between the growing forces of production on the one hand and reactionary, industrial and political conditions in Russia on the other. A rapidly growing capitalism demanded the freedom of the inner market, the failure of the Russian-Japanese War having made the extension of foreign markets impossible. But the home market was equally unresponsive. The predominant group among the Russian people was its peasantry, whose demands and buying power represented the basis for all further capitalistic development. They were equal, it is true, but equal in misery.\nThe Russian revolution of 1905 was said to be a major factor behind the revolution of 1917. The events of Bloody Sunday triggered a wave of protests. Amidst this chaos, a council of workers was convened in St. Petersburg and the beginning of a Communist political protest had begun. World War I prompted a Russian outcry directed at Tsar Nicholas II. It was another major factor that contributed to the retaliation of the Russian communists against their royal opponents. After the entry of the Ottoman Empire on the side of the central powers in October 1914, Russia was deprived of a major trade route through the Ottoman Empire. This was followed by a minor economic crisis, in which Russia became incapable of providing munitions to its army in the years leading to 1917.\nHowever, the problems were merely administrative and not industrial as Germany was producing a considerable arsenal of munitions, while constantly fighting on two major battlefronts. The war developed awareness in the city, owing to a lack of food because of the disruption in agricultural activity. The cities were almost always short of food. At the same time, rising prices led to demands for higher wages in factories and in January and February, 1916 revolutionary propaganda, aided by German funds, led to widespread strikes. These agitations became more frequent from the middle of 1915. Working class women in St. Petersburg reportedly spent about forty hours a week in the food lines, begging, turning to prostitution or crime, tearing down wooden fences to keep stoves heated for warmth, grumbling about the rich and wondering when and how this would all come to an end.\nAll these factors had by 1916 given rise to a sharp loss of confidence in the regime. Nicholas was blamed for the crises and what little support he had left began to crumble. As discontent grew, the State Dumas issued a warning to Nicholas in November 1916. It stated that a terrible disaster would inevitably grip the country unless a constitutional form of government was put in place. In typical fashion, however, Nicholas ignored the warnings and Russia's Tsarist regime collapsed a few months later during the February Revolution of 1917. In the beginning of February, Petrograd workers organised demonstrations and went on strike. On 16 March, a provisional government was announced. The representatives of the provisional government agreed to \"take into account the opinions of the Soviet of Worker's Deputies\" though they were also determined to prevent \" interference in the actions of the government\", which would create \"an unacceptable situation of dual power\".\nOn 18 June, the provisional government launched an attack against Germany, an offensive that failed miserably. The soldiers refused to follow the orders of the government. The soldiers and sailors, along with Petrograd workers, took to the streets in violent protest, calling for \"all power to the Soviet\". In the aftermath of the turmoil Lenin fled to Finland under threat of arrest while Trotsky, among other prominent Bolsheviks, was arrested. Alexander Kerensky, a young and popular lawyer and a member of the Socialist Revolutionary Party (SRP), increasingly became the central figure of the provisional government.\nBy October 1917, Lenin returned to Petrograd (St. Petersburg) as he became aware that the increasingly radical city posed a legal danger and also the second opportunity for revolution. Recognizing the strength of the Bolsheviks, Lenin began pressing for the immediate overthrow of the Kerensky government by the Bolsheviks. He was of the opinion that assumption of power should happen in both St. Petersburg and Moscow simultaneously, parenthetically stating that it made no difference which city rose up first, but expressing his opinion that Moscow may well rise up first. The Bolshevik Central Committee drafted a resolution, calling for the dissolution of the provisional government in favour of the Petrograd Soviet. The resolution was passed 10-2 (Lev Kamenev and Gregory Zinoviev prominently dissenting) and the November revolution began.\nThe Bolshevik, or the Russian Revolution, triumphed on November 7, 1917 (October 26 according to the orthodox Byzantine Calendar). Apart from the heroic episode of the Paris Commune, for the first time millions of downtrodden workers and peasants seized political power, sweeping aside the despotic rule of the capitalists and landlords. They were determined to create a socialist world order.\nIn the early days, the regime established by the revolution was neither bureaucratic nor totalitarian but the most democratic regime yet seen on earth. For the first time in history the success of the planned economy was demonstrated, not in the pages of Das Kapital but in an arena comprising a sixth of the planet’s surface. Not in the language of dialectics but in the language of steel, education, health care and electricity. In a gigantic and unprecedented experiment it was proved that it was possible to run society without capitalists, feudal landlords and money lenders. Despite the aggression of 21 imperialist armies, tremendous objective difficulties and obstacles, the abolition of the market mechanisms and the introduction of the planned economy revolutionized the productive forces and laid the basis for a modern economy.\nActually, the Russian Revolution of 1917 was one of the most significant events in the 20th century. It completely changed the government and outlook on life in the very large country of Russia. It had a profound impact on the entire world. It generated a new way of thinking about the economy, society, and the government. The Bolsheviks set out to cure Russia of all its injustices that are rooted in class differences. To an extent, they succeeded. The revolution marked the end of a dynasty that had reigned for 300 years and had concluded with the seizure of power by a small revolutionary group. The Tsar was replaced by a Council of People’s commissars and private ownership was abolished. The Communist movement began to expand worldwide, by which the entire capitalist world was unnerved. In spite of several difficulties, no one can deny that this unique revolution threw a very big challenge before the entire capitalist world.\nThe aftermath of the Soviet Revolution was far-reaching. The revolution spread a new message of hope and liberation for the toiling people all over the world and the peoples of the colonies. It was a message of liberation from all forms of exploitation -- national, social, economic and political. This was reflected in a series of declarations, legal pronouncements and diplomatic initiatives of the new Bolshevik Government. The October Revolution heralded a new era by creating a state of the workers and poor peasants whose interest was opposed to economic exploitation, war, aggressions, colonization and social discrimination. It brought into existence a socialist state that could work as a bulwark against war and imperialism. This revolution initiated the essay towards creating an alternative world socialist system based on equality and free of exploitation, renounced any form of aggression, colonization and social prejudice, as opposed to world capitalist system that is based on colonization, economic exploitation, racialism etc.\nThe socialist revolution was marked by the establishment of the first socialist state which till then was regarded by many as a distant dream. This is comparable to what happened in France in the 18th century. The Russian Revolution shook not only Russia, but radically changed the whole world. The world we see around today would be unthinkable without it just like the world of the 19th century would be unthinkable without the French Revolution. The British Prime Minister Lloyd George wrote: “The whole Europe is filled with the spirit of the revolution. There is a deep sense not only of discontent but of anger and revolt amongst the workmen against the pre-war conditions. The whole existing order in its political, social and economic aspects is questioned by the masses of the population from one end of Europe to the other.”\nThis is the 100th year of the November revolution. It is time to reflect on its contributions. In a word, it has taught us how to dream a dream. The underlying theme of the revolution was a society free from exploitation and to emancipate humankind. In the context of the November Revolution, Lenin proved that theory is grey, but life is green. It was his philosophy that attained fruition through the November Revolution. Many revolutions came and went, they wrought havoc to society, but the November Revolution was unique, unparalleled and novel in all aspects for it brought about momentous changes to all patterns of society and state policy for which the downtrodden humanity had longed for thousands of years.\nApril 8 & 9, 2016. The Statesman"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:7805bf01-b6d4-4718-8ee2-aa4a999a406c>","<urn:uuid:6166b734-ae7e-4975-8b23-89d6c218de8b>"],"error":null}
{"question":"As a medical student specializing in respiratory care, what are the normal ABG values for a healthy person?","answer":"The normal ABG (Arterial Blood Gases) values are: Ph 7.35-7.40, PCO2 35-40, PO2 80-100, HCO3 24, and O2 SAT 97-100%.","context":["“ABG’s” is another one of those medical acronyms that you hear all the time when you have asthma or lung disease. And if you’ve ever been unfortunate enough to be hospitalized for your asthma, no doubt you’ve had one of these tests done on you.\nABG stands for “Arterial blood gases”. An ABG is a blood test that measures the PO2 (oxygen) and PCO2 (carbon dioxide) in arterial blood ( blood that comes from an artery vs a vein). The Ph (the acidity/alkalinity of the blood ), HCO3 (Bicarbonate buffers) and SAO2 (O2 saturation) are also determined.\nThe test is performed by collecting a small sample of blood from a peripheral artery (not a vein) usually in your wrist where you feel your pulse. Because you can’t see arteries, the person drawing the sample has to go entirely by feel, which means it can often take more than one poke to hit the blood vessel ( and yes…it can hurt like crazy too).\nABG’s are usually only obtained on asthmatics who are experiencing severe respiratory distress and are not responding well to treatment. Because ABGs can only indicate a persons breathing status at that particular moment, they are often repeated several times during the course of a hospitalization. Patients who are in critical condition and who require frequent ABGs will usually have a special catheter inserted into the artery (called an Arterial Line). Whenever an arterial blood sample is needed, it can be drawn directly from a special port on the catheter instead of having to poke the patient with a needle every time.\nNormal ABG values would look something like this:\nPh 7.35-7.40 PCO2 35-40 PO2 80-100 HCO3 24 O2 SAT 97-100%\nAll 5 of these parameters are used in evaluating the respiratory status of a patient, but for the sake of this discussion the value we’re most interested in is the PCO2 (carbon dioxide). CO2 is a waste product of cellular metabolism and because we get rid of it by exhaling it out of our lungs, measuring how much CO2 is in our blood gives us a good indication of how well our lungs are doing their job. The faster and deeper we breath, the more CO2 we expel (we call this “Hyperventilation”). The slower we breath, the more CO2 we retain (we call this “Hypoventilation”). During normal breathing, the body maintains just the right level of CO2 (35-40)\nHere’s a scenario of what can happen to your CO2 levels during a severe flare;\nDuring a severe asthma attack it becomes very difficult to breath. As a result, you’re forced to use more breathing muscles than you normally would (what we call accessory muscle use) in order to get the air in and out of your lungs. This extra muscle use causes more CO2 (waste) to be produced. Ever notice that you breath faster during an asthma attack? It’s not only from air hunger. The body’s first line of defense against rising CO2, is to breath faster in order to blow it off and keep the levels within a safe range. However, if the work of breathing gets too severe, the lungs are unable to expel the CO2 fast enough and blood levels continue to rise …we call this “Respiratory failure”. The work of breathing can become so overwhelming, that the person begins to tire out and could eventually stop breathing all together… we call this “Respiratory Arrest”. Hopefully this will never happen to you.\nToo much CO2 can make the blood very acidic (decreased Ph). If the CO2 blood levels get too high or the Ph too low, it can cause damage to the vital organs such as the brain and heart. As a second line of defense the kidneys will hold on to more sodium bicarbonate which helps buffer the extra acidity.Bladder activity is also increased to help get rid of the acidic compounds. The problem with this 2nd line of defense, is that it takes much longer to kick in.\nOne way we can quickly the lower CO2 in someone who’s in respiratory failure, is to blow the CO2 off by mechanical means…either with BIPAP or a ventilator. By placing someone on a ventilator, we can control how much air moves in and out of a persons lungs thereby regulating how much CO2 moves out.\nThe example above, is of course an over simplification of what can occur during a severe attack. There are many other factors involved, but the basic goal in treating a critically ill asthmatic is to open up their airways and normalize their blood gas values.\nAs a Respiratory Therapist Ive done thousands of ABG’s on patients, and as a severe asthmatic myself, Ive had hundreds of them done on me. Here are some of my actually blood gas results between 2005 and 2008 while a patient in the UCSF hospital intensive care unit.\n**Following a more recent hospitalization(4/2013) I wrote a post regarding Bipap and how it effects ABGs**."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:8c2bf0d0-04db-47a3-aac9-344934b0d275>"],"error":null}
{"question":"What role do physical therapists play in both PCL tear diagnosis and hip arthroscopy recovery?","answer":"For PCL tear diagnosis, physical examination alone is usually insufficient, though doctors can perform specific tests like pushing back on the tibia with a bent knee to assess excessive movement. In hip arthroscopy recovery, physical therapists play a crucial role, with their assessment considered just as important as the medical assessment. They spend more time with patients than physicians do, often identifying important details missed by surgeons. They establish and adjust patient goals, manage rehabilitation progression, and maintain ongoing dialogue with physicians to modify protocols as needed. Their involvement begins ideally before surgery with 'prehab' and continues through various phases of postoperative recovery.","context":["Posterior Cruciate Ligament Tear\nThe Posterior Cruciate Ligament (PCL) is one of four main knee ligaments. It is a small band found at the back of the knee that links the tibia with the femur. Despite its size the PCL is the knee's strongest ligament, so Posterior Cruciate Ligament injuries are usually caused by harsh trauma or sudden damaging motions as opposed to more common sporting injuries. A PCL tear leads to knee instability and can make standing or bearing weight on that leg very difficult.\nSymptoms of Posterior Cruciate Ligament Tear\nThe symptoms can depend on the individual and the severity of the tear. Some people might experience knee pain or inflammation but this is not generally the case. This can make detecting the injury more difficult; however, the knee regularly feels unstable or unsteady and may collapse or wobble when bearing your weight. Other than this, the most reliable indicator is usually a traumatic incident involving the knee prior to the instability.\nCauses of Posterior Cruciate Ligament Tear\nVarious abrupt and sharp knee movements can lead to a Posterior Cruciate Ligament tear. Common causes include overextension or sudden twisting of the knee, as well as hard strikes to the shin or an impact that forces the knee backwards. These can all arise during fast paced contact sports such as football or rugby, although in many cases the injury is provoked by an incident not involving another athlete. Car accidents and other collisions are widespread causes outside of sport.\nDiagnosis of Posterior Cruciate Ligament Tear\nThe diagnostic process is the most important because the doctor will need to ascertain whether the Posterior Cruciate Ligament damage is isolated or linked to any other ligament injury. Physical exams are rarely enough to fully assess the injury, but an effective test involves the doctor pushing back on the tibia with the knee bent and judging whether the bone moved too far. Otherwise possible tests can include an x-ray, MRI or an arthroscopy procedure (with a small camera). These can help to show whether other ligaments are damaged, which is a common possibility with PCL tears. Fractures also need to be ruled out.\nMedical Treatment of Posterior Cruciate Ligament Tear\nIf the Posterior Cruciate Ligament tear occurs on its own, without any complications, then the treatment is likely to be conservative. This can mean a period of resting from activity and knee or leg immobilisation in order to keep the ligament in place as it heals. Pain medications and any other medicine will be prescribed at the doctor's discretion. A rehabilitation program of physical therapy will then gradually build up flexibility and power in the knee.\nSimultaneous damage to the Posterior Cruciate Ligament ligament and another knee ligament frequently necessitates a surgical procedure in which the injured ligaments are reconstructed by utilising a graft from an uninjured ligament or tendon. This surgery is often a choice and the doctor can talk you through the options and likely outcomes, but competitive athletes will usually opt for the surgery in an attempt to regain full strength in the knee. The other main option is a strengthening program. This can be arduous and last up to 6 months, but is the right option for some people.\n- Anterior Cruciate Ligament Tear\n- anterior knee pain\n- baker cyst\n- calf strain\n- fibula stress fracture\n- hamstring strain\n- hamstring tendinopathy\n- iliotibial band syndrome\n- lateral ligament sprain\n- medial ligament sprain\n- meniscus tear\n- patellar tendinopathy\n- patellofemoral instability\n- patellofemoral syndrome\n- pellegrini stieda syndrome\n- popliteus tendinitis\n- Posterior Cruciate Ligament Tear\n- sesamoid injuries\n- sever disease\n- sinding larsen and johansson syndrome\n- tibialis anterior tendinopathy\n- tibia fractures\n- tibialis posterior tendinopathy\n- Boot Stud Injuries\n- Pes Anserinus","Hip Arthroscopy: SMOG Rehab Protocol\nThis protocol is a generic outline of the postoperative management for patients undergoing hip arthroscopy. Depending on the exact diagnosis and the procedures performed, the surgeon may recommend variations of the therapy program. The therapist is encouraged to make recommendations as issues arise and should not hesitate to contact the surgeon to discuss the progress of these patients.\nIt is important to understand that rehabilitation after hip arthroscopy is very different than the traditional therapy that has been employed for open hip surgeries, e.g. total joint replacement or fracture stabilization. Traditionally physical therapy after open hip surgery employed the principles of gait training, oftentimes with weight-bearing restrictions and hip precautions. The emphasis was largely on regaining the ability to perform activities of daily living.\nThe goals of patients undergoing hip arthroscopy are strikingly different. Oftentimes the patients undergoing hip arthroscopy procedures are quite athletic and the goal is to return them to sports as quickly as possible. Early weight-bearing and range of motion exercises are emphasized. Progressive strengthening programs are started almost immediately and cross training activities are encouraged early in the rehabilitation process.\nIdeally, a therapist-patient relationship will have already been formed as many of the patients undergoing hip arthroscopy will have undergone a “prehab” program (See Prehab Handout). Many of the same exercises used preoperatively will be employed in the postoperative period, but in a slower and more progressive manner. The better conditioned the patient is preoperatively will largely determine the rate at which the patient improves postoperatively.\nThe first step of the postoperative program will begin just like any other protocol with the initial assessment and identification of the patient’s goals and expectations. The importance of the therapist’s assessment cannot be overemphasized. We consider the physical therapy assessment just as important as the medical assessment. The amount of time spent with the therapist by far exceeds the amount of time spent with the physician and thus important details will often be recognized that are not picked up by the surgeon. An ongoing dialogue with the physician is encouraged and changes in the protocol can be adjusted as necessary depending on this interaction.\nThe goals of the patient should be clearly established and should be reviewed and adjusted regularly. Unrealistic goals should be identified early and more realistic goals can be established. Again, the diagnosis, the surgery performed and the preoperative activity level of the patient will largely determine the ultimate goals – always remembering that the primary focus is to return these people back to sports and training regimens as expeditiously and safely as possible. Oftentimes in the elite athletes this will require “holding back the reigns” at times as these patients tend to try to progress too rapidly and can actually inhibit their rehabilitative potential. Again, this protocol is a general outline and can be accelerated or decelerated depending on each individual situation.\nI. Initial Phase\nGoals: Decrease soreness and swelling, gently increase range of motion to tolerance, inhibit further muscle atrophy\nA. Day of surgery\n1. Isometric glut sets, calf pumps\n2. Cold therapy\nB. Postoperative days 1-7\n1. Dressing change on postop day #1\n2. Partial Weight Bearing with crutches or walker\na. Labral debridement – 5-7 days only\nb. Osteoplasty (bone resection) – 2 weeks\nc. Microfracture – 4 weeks\n3. Postoperative exercises\na. Isometrics!!! Quad, gluts, hamstring, adductors/abductors!\nb. Active assisted range of motion in all planes (do not push through painful endpoints).\nc. Hip mobilization – straight plane distraction, inferior glides, posterior glides.\nd. Closed chain bridging, weight shifts, balancing drills\ne. Open chain standing abduction, adduction, flex/ext without resistance.\nII. Intermediate Phase\nGoals: Regain and improve strength, regain normal joint kinematics\nA. Postoperative weeks 2 – 3\n1. Normalize gait – eliminate limp!!\n2. Continue to increase range of motion with gradual sustained end-range stretches (still as pain tolerates).\n3. Begin progressive resistive exercises as tolerated.\na. Closed chain single leg bridging\nb. Open chain above knee resistive Theraband or pulley exercise in flexion, extension, adduction, abduction and hamstring curls as tolerated\nc. Bike as tolerated\nd. Pool exercises\nIII. Advanced Phase\nGoals: Increase functional strength and endurance\nA. Postoperative weeks 4-6\n1. Continue flexibility exercises\n2. Continue progressive resistive strengthening exercises\na. Closed chain exercises as tolerated: multiplane strength exercises, hamstring curls, knee extensions\nB. Gradual progression of activities\n1. Functional activities\n2. Sport-specific activities\n3. Return to sporting activity (with clearance from physician and physical therapist)\nAddendum – Distraction Mobilization Techniques\nIn athletes with painful hip disorders, distraction mobilization techniques can be very effective both preoperatively and postoperatively. Distraction reduces the compressive forces across the articular surfaces. This counterforce often provides significant relief to an inflamed and irritated joint. Over time, these counter-reactive forces promote a cartilage-healing environment in the hip which is an excellent adjunct to the traditional hip range-of-motion and strengthening exercises. The following is a brief review of the three distraction mobilization techniques for the hip:\n1. Straight-plane distraction: The patient is in the supine position. The therapist grasps the lower leg above the ankle and applies a manual traction force. It may be necessary for an assistant to provide countertraction by stabilizing the torso. The traction vector can be applied with the hip in various degrees of flexion and abduction. Best results are accomplished if progressive and sustained distraction for 10-15 seconds is performed. The patient should be frequently reminded to remain relaxed so that joint distraction can be accomplished. 5 repetitions are recommended.\n2. Inferior Glide distraction: The patient is supine with the hip and knee flexed 90 degrees. The therapist rests the patient’s lower leg on the therapist’s shoulder. A manual distraction force is applied to the proximal anterior thigh. This is best performed by interlocking both hands and then applying pressure, distracting in a distal direction. 5 repetitions are recommended.\n3. Posterior Glide distraction: The patient is supine with the hip and knee flexed 90 degrees. The applied force is directed downward on the knee such that posterior translation of the femoral head is accomplished. The therapist should be positioned directly over the knee such that the therapist’s body weight can be used to gently apply the posteriorly directed force. 5 repetitions are recommended. (Note: This exercise should not be performed in patients with posterior instability.)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:fc139d98-d5f7-4600-9b1f-9aef23a7284a>","<urn:uuid:343e35ee-10d4-41a2-9d95-0b20c7b15a48>"],"error":null}
{"question":"What are the key benefits and challenges of advertising on the Google Maps platform?","answer":"Google Maps advertising offers several benefits: enhanced visibility when users search for relevant keywords, precise targeting capabilities for specific demographics, and mobile accessibility for reaching customers on the go. However, there are also notable challenges to consider. These include managing advertising costs through careful budget monitoring, dealing with competition from other businesses in the area, and addressing privacy concerns related to location data collection. The platform operates on a pay-per-click model, meaning businesses only pay when users click on their ads, but costs can vary depending on factors like location and competition level.","context":["Advertising has evolved far beyond traditional methods. As businesses strive to expand their online presence and reach a wider audience, platforms like Google Maps have become invaluable tools for marketing. In this article, we will delve into the world of advertising on Google Maps, exploring its benefits, strategies, and how it can drive local business growth.\nTable of Contents\n- Understanding Google Maps Advertising\n- 2.1 What is Google Maps Advertising?\n- 2.2 How Does it Work?\n- Benefits of Advertising on Google Maps\n- 3.1 Enhanced Visibility\n- 3.2 Targeted Advertising\n- 3.3 Mobile Accessibility\n- Creating an Effective Google Maps Ad\n- 4.1 Claiming Your Business\n- 4.2 Optimizing Your Business Profile\n- 4.3 Crafting Engaging Ads\n- Measuring Success with Google Maps Advertising\n- 5.1 Key Metrics to Track\n- 5.2 Adjusting Your Strategy\n- Google Maps Advertising Best Practices\n- 6.1 Keep Information Accurate\n- 6.2 Utilize Visual Content\n- 6.3 Encourage Customer Reviews\n- 6.4 Utilize Location Extensions\n- Challenges and Considerations\n- 7.1 Managing Costs\n- 7.2 Competition\n- 7.3 Privacy Concerns\n- Success Stories\n- 8.1 Small Business Triumph\n- 8.2 Chain Store Expansion\n- 10.1 What is the cost of advertising on Google Maps?\n- 10.2 Can I target specific demographics with Google Maps ads?\n- 10.3 How can I make my business stand out on Google Maps?\n- 10.4 Are there any restrictions on the content of Google Maps ads?\n- 10.5 Is Google Maps advertising suitable for all types of businesses?\nIn an era where digital visibility can make or break a business, Google Maps has emerged as a potent advertising platform. With millions of users relying on it for navigation and local recommendations, advertising on Google Maps can significantly boost your business’s visibility and drive growth. Let’s explore the world of Google Maps advertising and how it can help your business thrive.\nUnderstanding Google Maps Advertising\n2.1 What is Google Maps Advertising?\nGoogle Maps advertising involves promoting your business on the Google Maps platform. When users search for local services or products related to your business, your ad appears on their screen, providing valuable information like your location, contact details, and website link.\n2.2 How Does it Work?\nGoogle Maps advertising utilizes pay-per-click (PPC) advertising, where you only pay when users click on your ad. You can customize your ad’s appearance, targeting specific demographics, locations, and keywords relevant to your business.\nBenefits of Advertising on Google Maps\n3.1 Enhanced Visibility\nGoogle Maps ads put your business on the map, quite literally. When users search for relevant keywords or are in proximity to your location, your ad appears prominently on their screen, increasing your chances of being noticed.\n3.2 Targeted Advertising\nOne of the most significant advantages of Google Maps advertising is its precision in targeting. You can tailor your ads to specific demographics, ensuring your message reaches the right audience.\n3.3 Mobile Accessibility\nWith the majority of users accessing Google Maps via their mobile devices, your ads are readily available to potential customers on the go, making it convenient for them to find and contact your business.\nCreating an Effective Google Maps Ad\n4.1 Claiming Your Business\nBefore you start advertising on Google Maps, it’s crucial to claim and verify your business. This allows you to manage your business information and maintain accuracy.\n4.2 Optimizing Your Business Profile\nA well-optimized business profile with accurate information, high-quality images, and customer reviews can significantly impact your ad’s effectiveness.\n4.3 Crafting Engaging Ads\nCompelling ad content with attention-grabbing headlines and clear calls to action (CTAs) can encourage users to click on your ad and explore your offerings.\nMeasuring Success with Google Maps Advertising\n5.1 Key Metrics to Track\nTo gauge the effectiveness of your Google Maps advertising, monitor key metrics like click-through rate (CTR), conversion rate, and return on investment (ROI).\n5.2 Adjusting Your Strategy\nUse the insights from your ad performance to refine your advertising strategy. Experiment with different ad formats and targeting options to maximize your results.\nGoogle Maps Advertising Best Practices\n6.1 Keep Information Accurate\nEnsure that your business information, such as hours of operation and contact details, is up-to-date to avoid frustrating potential customers.\n6.2 Utilize Visual Content\nHigh-quality images and videos can capture users’ attention and provide a visual preview of your business.\n6.3 Encourage Customer Reviews\nPositive reviews on your Google Maps profile can build trust and credibility, making users more likely to choose your business.\n6.4 Utilize Location Extensions\nAdding location extensions to your ads provides users with directions to your business, making it even easier for them to find you.\nChallenges and Considerations\n7.1 Managing Costs\nWhile Google Maps advertising can be highly effective, it’s essential to set a budget and closely monitor your spending to avoid overspending on advertising costs.\nExpect competition from other businesses in your area. Focus on creating compelling ads and maintaining an excellent business profile to stay competitive.\n7.3 Privacy Concerns\nBe mindful of users’ privacy concerns when collecting location data for advertising purposes. Ensure that you comply with relevant data protection regulations.\n8.1 Small Business Triumph\nDiscover how a local bakery increased its foot traffic and revenue by leveraging Google Maps advertising.\n8.2 Chain Store Expansion\nLearn how a national retail chain successfully expanded its presence using Google Maps advertising.\nIn the digital landscape, advertising on Google Maps offers a powerful means to connect with local customers, enhance visibility, and drive business growth. By following best practices, optimizing your ads, and staying attuned to user preferences, you can unlock the full potential of Google Maps advertising for your business.\n10.1 What is the cost of advertising on Google Maps?\nThe cost of advertising on Google Maps varies depending on factors like location, competition, and ad targeting. You set your budget and pay only when users click on your ad.\n10.2 Can I target specific demographics with Google Maps ads?\nYes, Google Maps advertising allows you to target specific demographics, ensuring your ads reach the most relevant audience for your business.\n10.3 How can I make my business stand out on Google Maps?\nTo stand out on Google Maps, ensure your business profile is complete and accurate, and encourage positive customer reviews. Engaging ad content can\nalso make your business more appealing.\n10.4 Are there any restrictions on the content of Google Maps ads?\nYes, Google has content policies in place to ensure a positive user experience. Ads must adhere to these guidelines to be displayed on Google Maps.\n10.5 Is Google Maps advertising suitable for all types of businesses?\nGoogle Maps advertising can benefit a wide range of businesses, particularly those with physical locations and local customer bases. However, its suitability may vary depending on your specific goals and target audience.\nIn conclusion, advertising on Google Maps can be a game-changer for businesses looking to expand their local presence. With the right strategies and a commitment to delivering an exceptional customer experience, you can tap into the immense potential of this platform and drive significant growth for your business."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:268b24db-fbef-497c-a7c1-db29a07a4a69>"],"error":null}
{"question":"What historical periods are represented in Greek archaeological sites and auction house collections, and how are these artifacts preserved?","answer":"Greek archaeological sites and auction collections span multiple historical periods, from ancient to Byzantine times. In Thessaloniki's metro excavations, artifacts range from late antiquity through the Byzantine era to the Ottoman period, including roads, buildings, and burial sites. The auction market features items from similar periods, including 4th/3rd century BC Thracian-Skythian silver work, 2nd/3rd century AD Roman artifacts, and 6th century bronze helmets. Preservation efforts are extensive, with Thessaloniki Metro planning to display significant finds in specially configured areas in central stations, while auction houses like Hermann Historica carefully catalog and preserve items for sale.","context":["Get 26 issues of Antique Trader delivered\n>>For just a $1 an issue!\nMUNICH, Germany – Approximately 6,800 lots from all areas of historic antiquities and militaria – antiquities, arms and armor, arts and crafts, hunting collectibles, orders as well as collectibles from history and military history – will be offered for bidding at Hermann Historica’s spring auction April 23-May 4.\nRare and exceptional objects, thematically diversified and of high quality, are put up for auction in the antiquities catalog. Examples include a set of silver phalerae from a bridle that can be traced to the northern Black Sea region and dates from the 4th/3rd century BC. The decorative ornamental discs with elaborate relief are made from sheet silver and give striking proof of the artistry of early Thracian-Skythian craftsmanship with precious metal. A female deity, lions and gryphons are depicted on the plates deriving from a Munich private collection. The piece carries a low estimate is nearly $20,000.\nA gold ring of a Roman officer from the 2nd/3rd century AD carries an estimate of about $10,500. The ring features a tapered shank with a rectangular panel that is decorated with the nielloed inscription “LEG V MAC” – a fact that designates the bearer as an officer of the Legio V Macedonica, founded by Octavian and located in Moesia.\nThis auction will also offer a fine assortment of early helmets, including an Illyrian bronze helmet from the 6th century with bright green patina and large face opening estimated at nearly $15,900 on the low end.\nAn expressive marble head of Silenus, also from the Roman Empire of the 2nd/3rd century, is offered for bidding. The oldest of the satyrs and son of the god Pan is depicted with a vine leaves’ wreath on his head, strong features and a curly beard. It can be won for at least $11,900.\nAn interesting object concerning cultural history and of museum quality is offered with lot 1449: a Balkan flask with reliefs on the body that can be dated circa 1500. Made from a leaded tin alloy, the flat flask features a depiction of Saint George as well as rich tendril decorations with stylised dragons’ heads on the handles. Apart from those Christian motifs, the influence of the Ottoman cultural region can be seen in the design. Comparable pieces are usually only seen in museums; the rare piece carries a low presale estimate of nearly $20,000.\nThe edged weapons offered for bidding range in the time of their origin from the High Middle Ages to the 18th century. An unusually long (44.69 inches) and striking medieval sword from the 12th century impresses by high-quality decorations and two-line inscriptions in gold and silver. It will be introduced at nearly $6,000.\nOf a more recent date, but just as interesting are two edged weapons from the 18th century. A chiselled and gilt French small-sword from circa 1760 convinces by its sophisticated workmanship as well as by its beautiful condition (estimate $4,764). A silver-mounted hunting hanger from the possessions of a Count of Erbach will be offered for bidding. Finely decorated with tendril ornaments, motto, coat of arms and princely hat, this South German weapon from 1730 will be introduced at $4,632).\nThe Orient and Asia category includes quality lots from the Ottoman Empire and India as well as from Japan and China. A shouldered vase from China stands out due to the exclusive colorfulness of the painted courtly scenes. Crafted circa 1700 during the late Ming Dynasty or the early Qing Dynasty, this vase in heavy Wucai export ware was supplemented with a gilt bronze mounting during the 19th century, and is now estimated at approximately $6,600.\nThe chapter of militaria contains the most exceptional items. The demand for memorabilia from the Bavarian royal house has remained unwavering, and the presentation brooch of King Ludwig II of Bavaria (1845-1886) can be seen as an outright sensation which now will be offered for sale. The blue medallion features a golden mirror monogram and is set with approximately 60 diamond roses. Considering the period, it was an extremely expensive present that was only given to the king’s closest personal confidants. This exclusive piece will be offered for bidding at $13,225.\nA 23 percent buyer’s premium will be added to all lots won. For more details, visit Hermann Historica.\nMore resources for Military collectors\n- Military Trader Magazine\n- Military Vehicles Magazine\n- Standard Catalog of U.S. Military Vehicles CD\n- Standard Catalog of Military Firearms\n- Military Small Arms of the 20th Century\nMore Related Posts from Antique Trader:\nMore Resources For Collectors & Sellers","The construction of the Metro, a vision of decades for the city of Thessaloniki, constitutes, on the one hand, a difficult endeavor and, on the other hand, a challenge, since a major technical project must be constructed in an urban environment abound in cultural treasures. The major unified archaeological excavation ever performed in Thessaloniki – expected to cover an area of 20,000 square meters in total – is about to be effected in view of serving the METRO construction needs.\nUtilizing its valuable experience from the archaeological excavation activities (79,000 square meters in total) effected for the construction of ATHENS METRO, ATTIKO METRO S.A. made the proper modifications to the technical design for THESSALONIKI METRO. The new design of this project foresees lowering of the tunnels underneath the archaeological layers at a depth ranging from –14 up to –31 meters (the initial provision was –7 up to –09 meters) in an effort to avoid any unpleasant entanglements with the execution of the Project.\nBased on the special study compiled by archaeologists for THESSALONIKI METRO, encountering archaeological finds in the section extending from NEOS SIDIRODROMIKOS STATHMOS to PANEPISTIMIO is very likely; however, antiquities are expected to be revealed at a higher rate in the part of Thessaloniki situated inside the city walls, i.e. from Demokratias Square up to Sintrivani.\nThree Stations are characterized as “high archaeological risk” ones (PLATEIA DEMOKRATIAS, VENIZELOS and AGIA SOFIA Stations), while another three Stations as “medium archaeological risk” (NEOS SIDIRODROMIKOS STATHMOS, SINTRIVANI and PANEPISTIMIO Stations). However, as the experience gained from the archaeological excavations conducted in the framework of the execution of technical works has taught us, due to the multiple interventions and modification works that settlements have undergone throughout the centuries, excavations often hold surprises for the archaeologists since archaeological finds can be encountered when least expected and vice versa!\nTo crown its unhindered and effective cooperation with the Ministry of Culture and the Department of Antiquities, ATTIKO METRO S.A. signed in July 2006 a Memorandum of Understanding and Cooperation in view of “facilitating, systematizing and accelerating the archaeological works”, and assumed the overall expenses of the archaeological work, from the initial stage of the excavations to the final stage related to the display of the archaeological finds in central Stations of the network.\nToday 300 persons of all specialties are engaged in the archaeological excavation works whose duration shall range from 6 months to 2 years depending on the depth, the significance and the density of the archaeological finds. It is anticipated that the overall expenditure for the archaeological excavations to be performed in the framework of Thessaloniki Metro Project shall be over 75 million EURO.\nATTIKO METRO S.A. in cooperation with the Ministry of Culture shall display the most significant archaeological finds of the Project in especially configured areas in the central stations of the network. Therefore, the benefit from the construction of the Project shall be double for the city of Thessaloniki: speed, safety, comfort and reliability using the best public transportation mode and invaluable knowledge about our cultural heritage.\nTHESSALONIKI METRO – 9th DEPARTMENT OF BYZANTINE ANTIQUITIES\nIn the framework of Thessaloniki Metro construction, the 9th Department of Byzantine Antiquities undertook the task of supervising the archaeological works carried out at Aghia Sofia and Venizelou Stations, which are two very interesting in terms of archaeology stations, since they are located in the center of the Byzantine metropolis where a significant number of monuments are still standing, as well as at Dimokratias Station and the Branching to Stavroupolis, to the west of Chrissi Pyli and within the area of the western graveyard , which, being located outside the city walls, are characterized as of medium interest in terms of archaeology. After the old-Christian Basilica was revealed, a part of Sintrivani Station also fell under the responsibility of our Department. The Byzantine Antiquities Department’s involvement at Sintrivani Station and the Branching to Stavroupolis was completed in January 2012.\nGiven that the archaeological excavation works are carried out in areas that have not been disturbed by the subsequent building activities, because the stations’ shells are normally located beneath the pavement of the present day Egnatia and Monastiriou streets, archaeological relics are being revealed in successive layers, which shed light to pictures of the city, thus partially recomposing the city’s life throughout its multi-century historical course.\nAt the north section of Aghia Sofia Station the archeological excavations revealed an ancient complex dating back to the late antiquity period, which was in use for a long period spanning until the late 6th – early 7th century. This is a part of one of the main road arteries on axis A-D, that was revealed for a length of at least 72,80m. The road is paved with marble slabs and delimited by marble curbs, while a short distance before its junction with the vertical road at the point where the present day Aghia Sofia Church is located this road is widened and forms a slab-paved square with a fountain which used to quench the thirst of travellers and residents. This complex is supplemented by a colonnade, but presently only a strong pillar remains visible, preserving in loco seven bases of columns of the 4th and 6th centuries and delimiting the road axis to the south and designating it as via colonnata. From the buildings on the south building line that existed during the old Christian era, only their facades and some door openings were identified. A complex of brick sewage pipes discharged into a large cross-section domed pipe transversely intersecting the current day Platonos street, while more clay and lead pipes constituted part of the city’s water supply network.\nWithin the boundaries of Venizelos Station, the same central road axis of the Byzantine era city is revealed, but this time with an earth or gravel pavement. The exposed Avenue is found to intersect throughout its width (5.50m.) with two vertical roads. Around these roads, there are densely built blocks constituting the neighbourhoods of the city’s Byzantine open-air market. Shops and workshops are facing the road, so that products are readily visible and sold. A multitude of minor artifacts and jewelry, such as pendant crosses, glass and copper bracelets, copper -mainly- and more rarely silver rings are witnessing the commercial character of the area in time, with the emphasis mainly placed on the trading of silver-gold artifacts.\nThe archaeological finds revealed at Dimokratia Station refer to the “extra muros” western open area of Thessaloniki, which partially retains it burial character up to the old Christian era. During the old Christian era, at the boundaries of the ancient graveyard, just outside Chrissi Pyli, partially located on the relics of a large storage complex dating back to the late antiquity era, an old Christian church was found, along with a burial structure to the south. It seems that this complex was destroyed in the early 7th century, possibly during the Slavs invasions.\nDuring the Ottoman period, the area acquires its fully commercial nature along the axis of Monastiriou street where inns and warehouses are build."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:0bb02792-d163-45f2-9aaa-f1426d43a930>","<urn:uuid:1f5e15ac-0da0-45e5-b35c-3942d3cec2e4>"],"error":null}
{"question":"What is the relationship between religious devotion and mental wellbeing, and how is this reflected in Sacred Heart teachings?","answer":"Research indicates that religion tends to positively impact mental health, particularly for those who are intrinsically motivated in their faith rather than serving selfish interests. This relationship is reflected in Sacred Heart teachings, which emphasize finding peace and protection within the Divine Heart, as exemplified in quotations like 'O Jesus, draw me into Thy Sacred Heart; and that I may dwell there in safety and in peace.' However, studies note that rigid and dogmatic religious orientations may reinforce less optimal mental functioning, suggesting the importance of authentic devotion versus superficial practice.","context":["holiest works for sanctification of the soul\nThoughts & Prayers for Meditation to Inspire Devotion to the Sacred Heart\n'I have chosen, and have sanctified this place, that my name may be there forever, and my eye and my heart may remain there perpetually.'\nII. Par. 7:16'Thou shalt love the Lord thy God with thy whole heart, with thy whole soul, and with thy whole strength.'\n'I will give them a Heart to know Me, and they shall be My people, and I will be their God, because they shall return to Me with their whole heart.'\nJeremiah 24:7'Set your hearts on all the words which I testify to you this day.'\nDeut. 32:46'O Jesus, my sweet hope, may the divine Heart, already torn by love for me and open to all sinners, be the safe asylum of my soul.'\nO Mary! we offer thee the Sacred heart of Jesus! Receive it, o tender mother! with those of all thy children, whose motto will be: All for the Heart of Jesus through the Immaculate Heart of Mary!\n'My Lord, I desire nothing but Thee, and never shall I find rest until I succeed in concealing myself entirely in Thy Divine Heart.'\nSt. Catherine of Genoa\nInvocation, 'Heart of Jesus'\nVen. Frances of Seraphs\nHeart of Jesus, fountain of love, source of graces, ocean of goodness, permit me to enter into thy wound to receive the forgiveness which Thou hast merited for me on the Cross.\nO Mary, place us near thee in the Sacred Heart of Jesus!\nAfter she met with a suitor, Christ appeared to the young St. Lutgard and revealed His Heart to her, saying: \"Look, this is what thou oughtest to love. Forsake the attractions of human love, and thou shalt find in my Heart ineffable delights.\"\nInvocation'The other day contemplating in my prayer the open side of Our Lord, and gazing upon His Heart, it appeared to me that our hearts were surrounding His, He being the King of the hearts.'\nSt. Francis of Borgia\nO precious wound of the side of Jesus, source of our happiness; attracted by thy sweetness, I establish my dwelling in thee, and I desposit in thee all I possess and all I hope for.\nSt. Francis de Sales'In every danger, as the dove when frightened flies for refuge to the hollows of the rock, so will the Christian soul seek protection in the cleft of the mystical rock, the Wounded Side of the Redeemer.'\nSt. Bernard'The Evangelist fittingly states that the soldier opened His Side, in order that in It, so to speak, may be opened the gate of life, through which issued the Sacraments of the Church, without which no one can enter the path leading to eternal life. Thus, the second Adam with a bent head (John 19:30) slept on the cross that a Spouse may be created for Him issuing form His Side. What is there purer than this blood? What more healing than this wound?'\nSt. Augustine'Never be tired of recommending devotion to the Sacred Heart; it is through It that God wills to save many souls from everlasting death. This Divine Heart is an assured refuge against God's just wrath towards sinners.'\nSt. Margaret Mary Alacoque\nAn Offering of the Hearts of Jesus and Mary\nby St. John Eudes\nO Jesus, only Son of God, only Son of Mary,\nI offer Thee the most loving Heart of Thy divine Mother\nwhich is more precious and pleasing to Thee than all hearts.\nO Mary, Mother of Jesus,\nI offer Thee the most adorable Heart of Thy well-beloved Son,\nwho is the life and love and joy of Thy Heart.\nBlessed be the Most Loving Heart\nand Sweet Name of Our Lord Jesus Christ\nand the most glorious\nVirgin Mary, His Mother,\nin eternity and forever.\n'Thy Heart has been wounded, that the visible wound may reveal to us the invisible one of love. For who would allow his heart to be wounded if love had not already attracted it? But also who would not seek, who would not love, a heart thus wounded?'\n'O Jesus! draw me into Thy Sacred Heart; and that I may dwell there, wash me from my iniquities, purify me from every stain. O most beautiful of the children of men, Thy Sacred Heart has been opened only that we may be able to dwell in it in safety and in peace.'\nTake from Me My Heart of Stone\nAufer A Me Cor Lapideum\nby Baldwin, Bishop of Canterbury (12th century)\nO LORD, take away my heart of stone, my hardened heart, my uncircumcised heart and grant to me a new heart, a heart of flesh, a clean heart! O Thou who purifieth the heart and loveth the clean heart, possess my heart and dwell in it, containing it and filling it, higher than my highest and more intimate than my most intimate thoughts. Thou who art the image of all beauty and the seal of all holiness, seal my heart in Thine image and seal my heart in Thy mercy, O God of my heart and the God of my portion in eternity.\nAufer a me, Domine, cor lapideum, aufer cor coagulatum, da mihi cor novum, cor carneum, cor mundum! Tu cordis mundator et mundi cordis amator, posside cor meum et inhabita, continens et implens, superior summo meo et interior intimo meo! Tu forma pulchritudinis et signaculum sanctitatis, signa cor meum in imagine tua: signa cor meum sub misericordia tua, Deus cordis mei, et pars mea Deus in aternum.\n'Blessed Elzear, Comte d'Arian, in Provence, having been long absent from his devout and chaste Delphina, she sent a messenger to him to inquire expressly for his health. Behold the reply she received: \"I am very well, my dear wife; but if you wish to see me, seek me in the wound of the side of our sweet Jesus: for it is there that I dwell, and there you will find me. Elsewhere you will seek me in vain.\" This was a Christian chevalier indeed.'\nSt. Francis de Sales'During our sojourn in this world, we should learn from the saints now in heaven, how to love God. The pure and perfect love of God they enjoy there, consists in uniting themselves perfectly to his will. It would be the greatest delight of the seraphs to pile up sand on the seashore or to pull weeds in a garden for all eternity, if they found out such was God's will. Our Lord himself teaches us to ask to do the will of God on earth as the saints do it in heaven: \"Thy will be done on earth as it is in heaven.\"\nBecause David fulfilled all his wishes, God called him a man after his own heart: \"I have found David. . . a man according to my own heart, who shall do all my wills.\"'\nSt. Alphonsus Maria de Liguori'Behold this Heart which has loved men so much, and yet men do not want to love Me in return. Through you My divine Heart wishes to spread its love everywhere on earth.'\nJesus to St. Margaret Mary in a vision'How sweet a thing it is, to die, after having had a true devotion to the Heart of Him Who is to be our Judge!'\nSt. Margaret Mary Alacoque'My son, give Me thy heart: and let thy eyes keep My ways.'","The relationship between mental health and religion has generated contradictory theoretical arguments and inconsistent empirical findings (Bergin 1983, Gartner et al. 1991, Larson et al. 1992, Batson et al. 1993). One basic question is whether religion contributes positively to individuals' mental health or undermines it. Another question concerns the support role of churches and the counseling role of clergy in the mental health system (McCann 1962).\nThe opposing theoretical perspectives regarding the relation between religion and mental health probably reflect biases in evaluations of religion. The view that religion enhances mental health emphasizes that religious beliefs help fulfill the basic human need for meaning, purpose, and confidence in the face of life's disappointments, frustrations, and exigencies. In addition, church attendance and involvement in religious groups provide reinforcement for these beliefs and also a social support network. The argument that religion undermines mental health emphasizes the notion that religion perpetuates immature dependency needs and unrealistic illusions, and prevents mature adjustment to the exigencies of life. These positive and negative theoretical orientations are reflected in the classical works of Jung and Freud, with Jung recognizing religion's importance in human experience and Freud emphasizing religion as a source of immature illusions. William James's (1958 ) classic distinction between the religion of \"healthy-mindedness\" versus the \"sick soul,\" plus his description of the positive effects of a conversion experience for the latter type, have clear implications for mental health.\nIn the voluminous research literature (Schumaker 1992, Pargament et al. 1993, Brown 1994), the weight of the evidence seems generally to support the notion that religion contributes positively to mental health, but this depends in part on how religion and mental health are defined and measured. Beyond a minimum definition based on absence of dysfunctional symptoms, mental health may include a sense of well-being and satisfaction with life, appropriate coping skills, a sense of ego integrity, and, optimally, continual growth and development of one's potential. Religiosity measures most often include beliefs, practices, and religious experience as different dimensions. Religious experience is less frequently measured in survey research, except for investigations of a conversion (or \"born-again\") experience, but may be a major element of case studies. The relationship between religion and mental health is most likely to be positive for persons for whom religion is intrinsically important (as opposed to serving selfish interests); however, a rigid and dogmatic religious orientation may help reinforce irrational and compulsive behaviors reflecting less than optimal mental functioning. It is also plausible that one's religiosity is itself a reflection of one's level of mental health.\nDoyle Paul Johnson\nC. D. Batson et al., Religion and the Individual (New York: Oxford University Press, 1993)\nA. E. Bergin, \"Religion and Mental Health,\" Professional Psychology 14(1983):170-184\nL. B. Brown (ed.), Religion, Personality, and Mental Health (New York: Springer-Verlag, 1994)\nJ. Gartner et al., \"Religious Commitment and Mental Health,\" Journal of Psychology and Theology 19(1991):6-25\nW. James, The Varieties of Religious Experience (New York: New American Library, 1958 )\nD. B. Larson et al., \"Associations Between Dimensions of Religious Commitment and Mental Health Reported in the American Journal of Psychiatry and Archives of General Psychiatry,\" American Journal of Psychiatry 149(1992):557-559\nR. V. McCann, The Churches and Mental Health (New York: Basic Books, 1962)\nK. I. Pargament et al., Religion and Prevention in Mental Health (Binghamton, N.Y.: Haworth, 1993)\nJ. F. Schumaker (ed.), Religion and Mental Health (New York: Oxford University Press, 1992).\n|return to Encyclopedia Table of Contents|"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:02cd95dd-338a-432c-b9f5-b9c9d9f0292d>","<urn:uuid:da769475-59e8-4cba-9557-14e0440a9e0f>"],"error":null}
{"question":"I'm getting married to someone from another country - how does international law affect our marriage recognition across borders?","answer":"In general, nations will recognize marriages that are validly performed in other nations. However, this becomes more complex with same-sex marriages, as many countries still don't recognize them. Even if a same-sex couple travels to another country to get married, their marriage may not be recognized as valid when they return to their country of residence. For property division, the Convention on the Law Applicable to Matrimonial Property Regimes allows spouses to choose which country's laws will apply - either where either spouse is a citizen, has primary residence, or where one spouse has started a new primary residence. If no choice is made, the law of the first state where they had their primary residence after marriage applies.","context":["International Family Law\nIf you are in a relationship with a foreign citizen, you may want to learn about the rights and responsibilities that you have under international family law. The Hague Conference on Private International Law has attempted to harmonize rules on marriage, divorce, and child custody across national boundaries.\nIn general, nations will recognize marriages that are validly performed in other nations. This may be more complex in the area of same-sex marriage, however, which many countries still do not recognize. A same-sex couple in a country that does not allow same-sex marriage may travel to another country to get married, but it is far from certain whether their marriage will be recognized as valid when they return to the country where they reside.\nDivorces also are generally recognized as valid across national boundaries under the Convention on the Recognition of Divorces and Legal Separations. There may be some exceptions when recognizing a divorce would be incompatible with a nation’s public policy. A nation where the divorce was not granted might not recognize its validity if it appears that one of the spouses did not have a fair opportunity to assert his or her rights during the divorce proceeding. A divorce also might not be recognized if the spouses were citizens of a state that did not allow divorce at the time.\nThe Convention on the Law Applicable to Matrimonial Property Regimes allows spouses to choose which laws will determine how property will be divided when their marriage ends. A couple can choose the law of any country where either spouse is a citizen, the law of any country where either spouse has a primary residence, or the law of any country where one of the spouses has started a new primary residence. If the couple does not make a choice, their marital property is divided according to the law of the first state where they had their primary residence after marriage.\nThe problem is that this Convention has not been widely ratified, so its rules do not apply broadly. Courts have needed to find creative solutions when marital property is in a country where neither spouse is a citizen or a resident, since the court does not have authority over this property. Sometimes, when one spouse controls these types of assets, the court simply will order that spouse to compensate the other spouse with assets over which the court does have authority.\nChild Custody and Child Support\nUniform laws on child custody have not yet been developed, but the Hague Conference on Private International Law has addressed the issues of parental child abduction and child support. Regarding abduction, states have agreed that a child should be returned to his or her country of primary residence if one parent takes the child away from that country. This means that nations will cooperate in restoring a custody arrangement after an attempt to violate it by one of the former spouses.\nThe Convention on the International Recovery of Child Support and Other Forms of Family Maintenance provides some transnational rules on child support. Former spouses who had been ordered to pay child support sometimes dodged these payments by moving to other countries where they could not be reached. This Convention responded by requiring nations to enforce child support agreements created in other nations, which means that people who move across borders will need to keep up with their support payments."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:89808e03-4322-486a-b739-b8f5ddd7e077>"],"error":null}
{"question":"What evidence supports or contradicts the role of global warming in Kilimanjaro's ice cap decline, and how has the ice cap changed over time?","answer":"The ice cap has been steadily declining since the first observations in the 1880s, with modern satellite images from 1976 to 2019 showing continuous shrinkage. However, human-induced global warming likely played a minimal role, joining 'quite late' after the ice loss was already well underway. The evidence suggests the large Victorian-era ice cap was actually the product of an unusually wet period rather than cooler global temperatures. The primary drivers have been changes in precipitation patterns and solar radiation rather than temperature changes. While there is conclusive evidence of Earth's warming over the past 100 years, Kilimanjaro's special conditions make it unlike other mountains whose glaciers are retreating due to rising temperatures.","context":["These images show the famous “snows of Kilimanjaro” in 1976, 2000, 2010, 2013, and 2019. Mount Kilimanjaro, almost 20,000 feet above sea level, is the highest peak in Africa. Though only about 200 miles from the equator, it has been capped by glaciers and snow for 11,000 years. This white cap shrinks and grows almost daily, but over the last century or more, its overall trend has been a steady decline. These images show durable, hard-ice glaciers, as well as the ephemeral snow on and around them. Scientists have focused on the glaciers, trying to understand why they are shrinking, how long they may last, and what they can teach us about the atmosphere today and the Earth long ago.\nIn these false-color images, vegetation appears green, drylands a brownish tan, and glaciers and snow cyan.\nBowen, Mark, 2005, Thin ice—unlocking the secrets of climate in the world’s highest mountains: New York, Henry Holt, 463 p.\nCullen, N.J., Mölg, T., Kaser, G., Hussein, K., Steffen, K., Hardy, D.R., 2006, Kilimanjaro glaciers—recent areal extent from satellite data and new interpretation of observed 20th century retreat rates: Geophysical Research Letters, v. 33, L16502.\nCullen, N.J., Sirguey, P., Mölg, T., Kaser, G., Winkler, M., and Fitzsimons, S.J., 2013, A century of ice retreat on Kilimanjaro: the mapping reloaded: The Cryosphere, v. 7, p. 419–431, https://doi.org/10.5194/tc-7-419-2013.\nHastenrath, Stefan, and Greischar, Lawrence, 1997, Glacier recession on Kilimanjaro, East Africa, 1912–89: Journal of Glaciology, v. 43, no. 145, p. 455–459.\nHastenrath, Stefan, 2001, Variations of East African climate during the past two centuries: Climate Change, v. 50, p. 209–217.\nHastenrath, Stefan, 2006, Diagnosing the decaying glaciers of equatorial East Africa: Meteorologische Zeitschrift, v. 15, no. 3, p. 265–271.\nHay, S.I., Cox, J., Rogers, D.J., Randolph, S.E., Stern, D.I., Shanks, G.D., Myers, M.F., and Snow, R.W., 2002, Climate change and the resurgence of malaria in the East African highlands: Nature, v. 415, p. 905–909.\nHemp, Andreas, 2005, Climate change-driven forest fires marginalize the impact of ice cap wasting on Kilimanjaro: Global Change Biology, v. 11, p. 1013–1023.\nHemp, Andreas, 2006, Vegetation of Kilimanjaro—hidden endemics and missing bamboo: African Journal of Ecology, v. 44, p. 305–328.\nKaser, Georg, 2004, Modern glacier retreat on Kilimanjaro as evidence of climate change—observations and facts: International Journal of Climatology, v. 24, p. 329–339.\nKruss, Phillip, 1983, Climate change in East Africa: A numerical simulation from the 100 years of terminus record at Lewis Glacier, Mount Kenya: Zeitschrift fur Gletscherkunde und Glazialgeologie, v. 19, p. 43–60.\nMaro, Paul S., 1988, Agricultural land management under population pressure—the Kilimanjaro experience, Tanzania: Mountain Research and Development, v. 8, no. 4, p. 273–282.\nMölg, T., Hardy, D.R., and Kaser, G., 2003, Solar-radiation-maintained glacier recession on Kilimanjaro drawn from combined ice-radiation geometry modeling: Journal of Geophysical Research, v. 108, no. D23, 4731.\nMölg, Thomas, and Hardy, Douglas R., 2004, Ablation and associated energy balance of a horizontal glacier surface on Kilimanjaro: Journal of Geophysical Research, v. 109, p. D16104.\nMölg, T., Cullen, N.J., Hardy, D.R., Kaser, G., and Klok, L., 2007, Mass balance of a slope glacier on Kilimanjaro and its sensitivity to climate: International Journal of Climatology, v. 28, p. 881–892.\nMölg, T., Hardy, D.R., Cullen, N.J., and Kaser, G., 2008, Tropical glaciers, climate change, and society—focus on Kilimanjaro (East Africa), in Orlove, B., Wiegandt, E., and Luckman, B.H., eds., Darkening peaks—glacier retreat, science, and society: Berkeley, CA, University of California Press, p. 168–182.\nMote, Philip W., and Kaser, Georg, 2007, The shrinking glaciers of Kilimanjaro: can global warming be blamed? American Scientist, v. 95, no. 4, p. 318.\nRevkin, Andrew C., 2004, Climate debate gets its icon—Mt. Kilimanjaro: New York Times, March 23, 2004.\nScoon R.N., 2018, Kilimanjaro National Park, in Geology of National Parks of Central/Southern Kenya and Northern Tanzania: Cham, Switzerland, Springer, p. 129–140, https://doi.org/10.1007/978-3-319-73785-0_12.\nThompson, L.G., Mosley-Thompson, E., Davis, M.E., Henderson, K.A., Brecher, HG.H., Zagorodnov, V.S., Mashiotta, T.A., Lin, P.N., Mikhalenko, V.N., Hardy, D.R., and Beer, J., 2002, Kilimanjaro ice core records—evidence of Holocene climate change in tropical Africa: Science, v. 298, no. 5593, p. 589–593.\nThompson, L.G., Brecher, H.H., Mosley-Thompson, E., Hardy, D.R., and Mark, B.G., 2009, Glacier Loss on Kilimanjaro continues unabated: Proceedings of the National Academy of Sciences, v. 106, no. 47, 19770–19775.\nVeettil, B.K., and Kamp, U., 2019, Global Disappearance of Tropical Mountain Glaciers: Observations, Causes, and Challenges: Geosciences, v. 9, no. 5, p. 196, https://www.mdpi.com/2076-3263/9/5/196.","The Shrinking Glaciers of Kilimanjaro: Can Global Warming Be Blamed?\nThe Kibo ice cap, a \"poster child\" of global climate change, is being starved of snowfall and depleted by solar radiation\nGlaciers and Global Climate\nThe observations described above point to a combination of factors other than warming air—chiefly a drying of the surrounding air that reduced accumulation and increased ablation—as responsible for the decline of the ice on Kilimanjaro since the first observations in the 1880s. The mass balance is dominated by sublimation, which requires much more energy per unit mass than melting; this energy is supplied by solar radiation.\nThese processes are fairly insensitive to temperature and hence to global warming. If air temperatures were eventually to rise above freezing, sensible-heat flux and atmospheric long-wave emission would take the lead from sublimation and solar radiation. Since the summit glaciers do not experience shading, all sharp-edged features would soon disappear. But the sharp-edged features have persisted for more than a century. By the time the 19th-century explorers reached Kilimanjaro's summit, vertical walls had already developed, setting in motion the loss processes that have continued to this day.\nAn additional clue about the pacing of ice loss comes from the water levels in nearby Lake Victoria. Long-term records and proxy evidence of lake levels indicate a substantial decline in regional precipitation at the end of the 19th century after some considerably wetter decades. Overall, the historical records available suggest that the large ice cap described by Victorian-era explorers was more likely the product of an unusually wet period than of cooler global temperatures.\nIf human-induced global warming has played any role in the shrinkage of Kilimanjaro's ice, it could only have joined the game quite late, after the result was already clearly decided, acting at most as an accessory, influencing the outcome indirectly. The detection and attribution studies indicating that human influence on global climate emerged some time after 1950 reach the same conclusion about East African temperature far below the peak.\nThe fact that the loss of ice on Mount Kilimanjaro cannot be used as proof of global warming does not mean that the Earth is not warming. There is ample and conclusive evidence that Earth's average temperature has increased in the past 100 years, and the decline of mid- and high-latitude glaciers is a major piece of evidence. But the special conditions on Kilimanjaro make it unlike the higher-latitude mountains, whose glaciers are shrinking because of rising atmospheric temperatures. Mass- and energy-balance considerations and the shapes of features all point in the same direction, suggesting an insignificant role for atmospheric temperature in the fluctuations of Kilimanjaro's ice.\nIt is possible, though, that there is an indirect connection between the accumulation of greenhouse gases and Kilimanjaro's disappearing ice: There is strong evidence of an association over the past 200 years or so between Indian Ocean surface temperatures and the atmospheric circulation and precipitation patterns that either feed or starve the ice on Kilimanjaro. These patterns have been starving the ice since the late 19th century—or perhaps it would be more accurate to say simply reversing the binge of ice growth in the third quarter of the 19th century. Any contribution of rising greenhouse gases to this circulation pattern necessarily emerged only in the last few decades; hence it is responsible for at most a fraction of the recent decline in ice and a much smaller fraction of the total decline.\nIs Kilimanjaro's ice cap doomed? It may be. The high vertical edges of the remaining ice make a horizontal expansion of the ice cap more difficult. Although new snowfall on the ice can accumulate over the course of months or years, new snowfall on the rocky plateau usually sublimates or melts in a matter of days (with the notable exception of the period of several months of continuous snow cover in late 2006 and into 2007), partly because thin snow above dark rock cannot long survive as the loss processes reduce the reflective snow and expose the sunlight-absorbing rock. If the cap ice were much thicker and shaped in a way that allowed ice to creep outward, gentle slopes could develop along the edges; new snow would be buffered against loss and would accumulate. But steep edges do not allow such expansion.\nImagine, though, a scenario in which the atmosphere around Kilimanjaro were to warm occasionally above 0 degrees. Sensible and infrared heating of the ice surface would gradually erode the sharp corners of the ice cap; gentler slopes would quickly develop. If, in addition, precipitation increased, snow could accumulate on the slopes and permit the ice cap to grow. Ironically, substantial global warming accompanied by an increase in precipitation might be one way to save Kilimanjaro's ice. Or substantially increased snowfall, like the 2006-07 snows, could blanket the dark ash surface so thickly that the snow would not sublimate entirely before the next wet season. Once initiated, such a change could allow the ice sheet to grow. If the Kibo ice cap is vanishing or growing, reshaping itself into something different as you read this, glaciology tells us that it's unlikely to be the first or the last time.\n- Cullen, N. J., T. Mölg, G. Kaser, K. Hussein, K. Steffen and D. R. Hardy. 2006. Kilimanjaro Glaciers: Recent areal extent from satellite data and new interpretation of observed 20th century retreat rates. Geophysical Research Letters 33:L16502. doi:10.1029/2006GL027084\n- Gaffen, D. J., B. D. Santer, J. S. Boyle, J. R. Christy, N. E. Graham and R. J. Ross. 2000. Multidecadal changes in the vertical temperature structure of the tropical troposphere. Science 287:1242-1245.\n- Kaser, G. 1999: A review of modern fluctuations of tropical glaciers. Global and Planetary Change 22 (1-4):93-103.\n- Kaser, G., D. R. Hardy, T. Mölg, R. S. Bradley and T. M. Hyera. 2004. Modern glacier retreat on Kilimanjaro as evidence of climate change: observations and facts. International Journal of Climatology 24:329-339. doi: 10.1002/joc.1008\n- Mölg, T., D. R. Hardy and G. Kaser. 2003. Solar-radiation-maintained glacier recession on Kilimanjaro drawn from combined ice-radiation geometry modeling. Journal of Geophysical Research 108(D23):4731. doi:10.1029/2003JD003546\n- Mölg, T., and D. R. Hardy, 2004. Ablation and associated energy balance of a horizontal glacier surface on Kilimanjaro. Journal of Geophysical Research 109:D16104.\n- Oerlemans, J. 2005. Extracting a climate signal from 169 glacier records. Science 308:675-677.\n- Osmaston, H. 1989. Glaciers, glaciation and equilibrium line altitudes on Kilimanjaro. In Quaternary and Environmental Research on East African Mountains, ed. W. C. Mahaney. Rotterdam: Brookfield, pp. 7-30.\n- Thompson, L. G., E. Mosley-Thompson and K. A. Henderson. 2000. Ice-core paleoclimate records in tropical South America since the Last Glacial Maximum. Journal of Quaternary Science 15:377-394.\n- Thompson, L. G., et al.2002. Kilimanjaro ice core records: Evidence of Holocene climate change in tropical Africa. Science 298:589-593.\n- Trenberth, K. E., et al. 2007. Observations: Surface and atmospheric climate change. Chapter 3 in Climate Change 2007: The Physical Science Basis. Contribution of Working Group 1 to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change. Cambridge, U.K., and New York: Cambridge University Press."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:4cf005f1-b852-44e0-9c5a-8fa8c67fb94e>","<urn:uuid:1a144ad7-50eb-4427-83b4-084e45516968>"],"error":null}
{"question":"How do the thought experiments of Schrödinger's cat and Cramer's transactional interpretation differ in their approach to explaining quantum measurement and wave function collapse?","answer":"Schrödinger's cat thought experiment presents a paradox where a cat in a box is simultaneously alive and dead until observed, highlighting the role of measurement in collapsing quantum superposition. The transactional interpretation, in contrast, explains measurement through an atemporal exchange of offer waves (OW) and confirmation waves (CW) between emitter and absorber, without requiring conscious observers. While Schrödinger's experiment suggests the absurdity of quantum superposition at macroscopic scales, Cramer's interpretation removes the need for consciousness or arbitrary collapse triggers by making the absorber, not the observer, responsible for wave function collapse. The transactional interpretation describes this collapse as happening atemporally along the entire four-vector connecting emission and absorption events, rather than at a specific moment of observation as implied in the cat experiment.","context":["John G. CramerJohn Cramer developed a new interpretation of the formalism of quantum mechanics called the \"transactional interpretation.\" The transactional interpretation makes no experimental predictions different from standard quantum mechanics. But it does remove some of the puzzling and perhaps unnecessary assumptions that are part of other Interpretations of quantum mechanics. In particular, it denies that conscious observers are needed to cause the \"collapse of the wave function\" (without which there is no actual \"outcome\" in the measurement process). The transactional interpretation adds nothing ad hoc to the standard theory, such as \"hidden variables or additional terms to the Schrōdinger equation to force a collapse. It is explicitly indeterministic and non-local. Cramer is exploring the radical possibility of sending information between entangled particles faster than the speed of light, as well as causal relations that go backwards in time (retrocausality). And, like Schrōdinger and the decoherence advocates, Cramer denies the existence of particles! The core physics in the transactional interpretation is a way of looking at photon emissions and absorptions as an exchange of advanced and retarded waves that is based on the 1945 Wheeler-Feynman Absorber Theory of radiation, which was abandoned by Feynman, who went on to develop the Path Integral formulation of quantum mechanics and later, with Julian Schwinger and Sin-Itiro Tomonaga, the theory of Quantum Electrodynamics (QED). While QED is a powerful theory that allows precise calculations of physical observables such as the motions of photons and electrons and the emission and absorption of a photon by an electron, the transactional interpretation is simply a way of looking at the emission and absorption of photons based on the Wheeler-Feynman attempt to describe the exchange of energy in the classical electromagnetic field as a time-symmetric process. Wheeler-Feynman proposed adding advanced field potentials (which look like never-seen-in-nature incoming spherical waves converging on light sources) to the normal outgoing spherical waves (with retarded potentials) of classical electrodynamics. Wheeler and Feynman's goal was to symmetrize electrodynamics with respect to time. One view of the advanced-potential incoming waves is that they are going backwards in time. There is nothing inherent in electromagnetic theory that explains the time asymmetry we see in radiation propagation (forward-in-time outgoing waves only). Cramer's transactional interpretation describes an electron as sending out probabilistic \"offer waves\" (OW) to potential absorbers. He adds what he calls \"confirmation waves\" (CW) incoming to an emitter from the many possible absorbers of an emitted photon. An offer wave is not an actual photon emission, and a confirmation wave is not an actual absorption or \"detection\" of a photon. But Cramer did see the two waves as connecting events in four-dimensional spacetime. Eventually, one advanced-potential confirmation wave indeterministically \"handshakes\" with the retarded-potential offer wave and produces an actual absorption. This \"handshake\" completes the transaction, but perhaps not at a single point in spacetime. Cramer sees the transaction as \"atemporal\" in that it takes place all along the four-dimensional spacetime vector between the emission and absorption events. Because it happens over the extended space of a worldline of a photon between emission and absorption, Cramer says it is \"explicitly nonlocal,\" but this linear space is tiny compared to the huge space of nonlocal behavior of two entangled particles in the EPR experiment, for example.\nIn the transactional interpretation the collapse of the state vector is interpreted as the completion of the transaction started by the OW and the CW exchanged between emitter and - absorber. The emergence of the transaction from the SV [state vector or wave function] does not occur at some particular location in space or at some particular instant of time, but rather forms along the entire four-vector that connects the emission locus with the absorption locus (or loci in the case of multiple correlated particles). The transaction employs both retarded and advanced waves, which propagate, respectively, along positive and negative lightlike (or timelike) four-vectors. Since the sum of these four-vectors can span spacelike and negative timelike and lightlike intervals, the \"influence\" of the transaction in enforcing the correlations of the quantum event is explicitly both nonlocal and atemporal.Although Cramer does not specifically discuss the case of two entangled particles in the EPR experiment, his remarks about transactional atemporality apply to the case of Alice and Bob measuring particles at point a and point b. It does not matter whether Alice or Bob measures \"first.\"\nSince the transaction is atemporal, forming along the entire interval separating emission locus from absorption locus \"at once, \" it makes no difference to the outcome or the transactional description if separated experiments occur \"simultaneously\" or in any time sequence. There is likewise no issue of which of the separated measurements occurs first and precipitates the SV collapse, since in the transactional interpretation both measurements participate equally and symmetrically in the formation of the transaction. Furthermore, the paths across which the correlation enforcing exchange takes place are lightlike four-vectors and remain so under any Lorentz transformation. Therefore the outcome and the transactional description of any correlation experiment is the same independent of the inertial reference frame from which it is viewed, as it must be if quantum mechanics and relativity are to be compatible theories.Cramer well knows that there are frames of reference moving with respect to the laboratory frame of the two observers in which the time order of the events can be reversed. In some moving frames Alice measures first, but in others Bob measures first. If there is a special frame of reference (not a preferred frame in the relativistic sense), surely it is the one in which the origin of the two entangled particles is at rest. Assuming that Alice and Bob are also at rest in this special frame and equidistant from the origin, we arrive at the simple picture in which any measurement that causes the two-particle wave function to collapse makes both particles appear \"simultaneously\" at determinate places with fully correlated properties (just those that are needed to conserve energy, momentum, angular momentum, and spin).\nthe Copenhagen interpretation implicitly associates with quantum events a time directionality that, while appropriate to macroscopic observers, is quite alien to and inconsistent with the even-handedness with which microphysics deals with the flow of time. Somehow the thermodynamic irreversibility of the macroscopic observer is intruding into the description of a fully reversible microscopic process. (p.651) Wigner and others have suggested that the process of collapse should involve a special role for consciousness (Wigner, 1962), for permanent recording of experimental results (Schrödinger, 1935) or for entry of the system into the domain of thermodynamic irreversibility (Heisenberg, 1960). (p.654) Where precisely is the border between macrophysics and microphysics and the border at which irreversibility begins? (p.683)In the information interpretation, the collapse is when information about an event (it may not be a measurement) is irreversibly recorded in the universe. It need not be a measurement by an observer. Indeed, information must be recorded (for example, by a measuring instrument) before it can be seen by an observer. Despite his description of the transactional \"handshake\" as atemporal, Cramer says the collapse occurs when the emitter accepts the confirmation wave from an absorber. It is the absorber that precipitates the collapse, he says,\nIn the transactional interpretation the collapse, i.e., the development of the transaction, is atemporal and thus avoids the contradictions and inconsistencies implicit in any time-localized SV collapse. Furthermore, the transactional description does not need to invoke arbitrary collapse triggers, such as consciousness, etc., because it is the absorber rather than the observer which precipitates the collapse of the SV, and this can occur atemporally and nonlocally across any sort of interval between elements of the measuring apparatus.Cramer is quite critical of the need for a \"conscious observer.\"\nThis \"consciousness\" interpretation, while it is a reasonable working hypothesis for an observer who does not wish to find himself dissolved into the state vector of the system he is measuring, does beg a number of questions. Did the SV of the universe remain uncollapsed until the first consciousness evolved? Where is the borderline between consciousness and unconsciousness? Will \"smart\" measuring instruments eventually achieve the abihty to collapse SV's, and how will one know when they do? And so on. Schrodinger (1935) suggested an alternative to the consciousness interpretation, which he called the principle of state distinction and which asserts, \"states of a microscopic system which could be told apart by macroscopic observation are distinct from each other whether observed or not. \" In other words, the SV collapses as soon as some macroscopic record of the result of a measurement is made, whether a conscious observer looks at that record or not. Heisenberg (1960) and others have suggested a variant of this position which asserts that as soon as the quantum measurement passes from the domain of reversible processes into the domain of thermodynamic irreversibility the SV collapses. The latter two \"collapse triggers\" are more appealing to most physicists than the former because they avoid giving some special significance to consciousness and because, as pointed out by Weisskopf (1959,1980), they correspond more closely to the operating assumptions that practicing physicists use in thinking about how quantum measurements are done. However, these models also beg the question of borders: Where precisely is the border between macrophysics and microphysics and the border at which irreversibility begins?The answer to Cramer's question about the border between microphysics and macrophysics is found in an analysis of the \"quantum-to-classical transition\" and in Heisenberg and von Neumann's speculations about the \"cut\" between quantum events and an observer's information, knowledge, or conscious awareness. Below the cut everything is governed by the wave function. Above the cut, Heisenberg and Bohr insisted a classical description must be used. Decoherence theorists claim that the quantum-to-classical transition is caused by environmental interactions, but the information interpretation claims it is when a macroscopic object contains such a large number of atoms that independent quantum events that they can be averaged over, that their random phases cancel out, and that there is statistical determinism. Heisenberg, von Neumann, Wigner, and many others puzzled over the location of the \"cut,\" perhaps none more than John Bell, who drew a diagram of possible places for what he called the \"shifty split.\" We can now edit Bell's diagram to point to the location of \"cut\" as the moment when irreversible information enters the universe.\nThe Possibilist Transactional InterpretationIn her 2012 book, The Transactional Interpretation of Quantum Mechanics, Ruth Kastner proposes to regard the outgoing offer wave and many incoming confirmation waves as \"possible\" transactions, only one of which indeterministically becomes \"actual.\" Kastner is a possibilist who argues that OWs and CWs are possibilities that are \"real.\" She says that they are less real than actual empirically measurable events, but more real than an idea or concept in a person's mind. She suggests the alternate term \"potentia,\" Aristotle's term that she found Heisenberg had cited. For Kastner, the possibilities are physically real as compared to merely conceptually possible ideas that are consistent with physical law (for example, David Lewis' \"possible worlds.\" But she says the \"possibilities\" described by offer and confirmation waves are \"sub-empirical\" and pre-spatiotemporal (i.e., they have not shown up as actual in spacetime). She calls these \"incipient transactions.\" The subtitle of Kastner's book is \"The Reality of Possibility.\" She says that her main thesis is that \"it is perfectly reasonable to be realist about the subject matter of quantum theory\" (p.28). And she calls for a new metaphysical category to describe \"not quite actual...possibilities.\" Normal | Teacher | Scholar","Thought experiments have been the bread and butter of modern physics since its inception. Galileo used a thought experiment to argue that all objects fall at the same rate regardless of mass, Isaac Newton used a thought experiment to argue for the absolute nature of space, Maxwell used a thought experiment to argue the nature of thermodynamics, and Einstein’s original paper that introduced the special theory of relativity was purely a thought experiment.\nQuantum mechanics has a long history of thought experiments, most geared towards pointing out weaknesses or oddities in particular interpretations of quantum mechanics. The newest addition to this history of thought experiments in quantum mechanics adds to this tradition by arguing that the common understanding of quantum mechanics is incorrect.\nIn a new paper published in Nature, physicists Daniela Frauchinger & Renato Renner present a thought experiment meant to point out an inconsistency in the orthodox interpretation of quantum mechanics. The particular thought experiment presented by the authors is a modification of one given by Eugene Wigner in 1961, which itself was a modification of the infamous Schrödinger’s cat thought experiment. Frauchinger & Renato’s setup involves two scientists making measurements on two friends who are each making measurements on an isolated quantum system. Their argument, in a nutshell, is that if the orthodox interpretation of quantum mechanics is correct, then the two outside observers will reach contradictory conclusions about the state of an experimental system. Contradictions are a no-go in physics so, therefore, something must be wrong with the orthodox interpretation and we should replace it with another.\nSchrödinger’s Cats, Wigner’s Friends, And QM Weirdness\nIn 1935 Erwin Schrödinger presented the following thought experiment: Say you have a box with a cat in it. Inside the box along with the cat are a vial of cyanide and a hammer rigged to break the vial, releasing the gas and killing the cat. The vial/hammer apparatus is itself wired to a Geiger counter, set up to detect the radioactive decay of a cesium isotope. If the atom spits out a beta particle, the hammer will break the vial killing the cat. If the atom does not spit out a beta particle, nothing will happen and the cat remains alive. You leave the box alone and come back an hour later, wondering whether the cat is alive or dead.\nWell, according to the orthodox interpretation of QM, also known as the Copenhagen interpretation, the answer is actually neither. The mathematical framework of QM describes the above set up as being in a superposition, a mixture of classical states: one in which the atom decayed and the cat is dead, and one in which the atom did not decay and the cat is alive. Before one actually opens the box to look, the cat is in a superposition of alive/dead. When an actual observation is made, the superposition “collapses,” causing the system to pick one definite state. Thus, the Copenhagen interpretation holds that observation of a quantum system causes that system to collapse from a superposition of multiple possible states into one definite state. (It is interesting to note that Schrödinger himself thought his hypothetical situation proved that the orthodox interpretation was absurd).\nIn 1961, Eugene Wigner offered a modification of Schrödinger’s gedankenexperiment. Wigner’s thought experiment replaces the cat in the box with a fellow scientist in a lab who themselves is measuring a quantum system S. Outside the lab is Wigner himself who is observing the entire system of the lab; his friend plus system S. The friend in the lab makes a measurement on an electron and records that it has a spin of -1/2. From the perspective of the friend in the lab, the wave function of system S has collapsed and given him a definite measurement. However, from the perspective of Wigner outside the lab, the entire lab, which includes both his friend and system S, is still in a superposition of two states, one where the electron was measured with a spin -1/2 and his friend observed -1/2, and one where the electron was measured with a spin 1/2 and his friend observed 1/2. It is only after his friend in the lab tells him the result of his experiment that the entire system of the friend plus S collapses into a definite state. From the perspective of the friend inside the lab though, the electron has already taken on a definite state as soon as he measured it. Many have held that Wigner’s thought experiment proves that QM is not applicable at all scales of reality.\nFrauchinger & Renner’s modification to Wigner’s thought experiment is meant to highlight an inconsistency in the orthodox interpretation by deriving an outright contradiction from the principles of the Copenhagen interpretation. Essentially, the thought experiment shows how four individuals each with a different piece of information following rules dictated by QM will result in contradictions. In the pair’s setup, there are two friends F and F each inside of labs L and L making measurements, each being observed by scientists W and W respectively. F flips a coin, and depending on the result, polarizes a particle S to have a certain spin. She then communicates this spin result to F who uses her knowledge of quantum mechanics to determine the outcome of F’s coin flip. So far so good, as F and F are in agreements about the outcome of the coin flip.\nNow, W and W enter the picture. Depending on the initial measurements made by F and F, W and W will observe L and L as being in particular superpositions. Depending on the result of the initial coin flip and the message sent from F to F, a situation will arise where W and W can, according to the mathematical rules of QM, be absolutely certain about the outcome of the initial coin flip. The only problem is that W and W will disagree about that outcome as being heads or tails. In other words, the rules of the orthodox interpretation require that W and W both be 100% certain about the occurrence of mutually exclusive outcomes, a blazing contradiction if ever there was one.\nFrauchinger & Renner point out that a contradiction arises only if one assumes a “single world” interpretation of QM; that there is only one reality where only one outcome can definitely occur. In a previous paper, Frauchinger & Renner have argued along similar lines that single world interpretations of QM will inevitably end up producing contradictions. According to “many-worlds” interpretations of QM, there is no collapse of wave functions; for every possible value of some variable, there is a branch of the universal wave function where that outcome actually occurs. So if a many worlds interpretation is true, then the apparent contradiction derived above is no true contradiction after all. W and W’s observations are consistent as they each describe a different branch of the wave function, a branch where different outcomes occurred. Frauchinger & Renner argue that considerations like their thought experiment compel us to adopt a many-worlds interpretation, as it is the only one that can reconcile the contradictions apparent in the orthodox view.\nMany-worlds interpretations of QM have gained a lot of popularity in recent years but are understandably still handled with some apprehension in the scientific community. The entire premise of the many-worlds interpretation seems impervious to experimental testing, as any possible experimental result could be explained away by referencing the branching nature of the universal wave function. However, strict logical and a priori analyses like that given by Frauchinger & Renner seem to give a kind of evidence for a many-worlds interpretation, on pain of reality being genuinely inconsistent; something no scientist would want to believe."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:ca6502a3-2647-49cc-ba32-ac557d89b0f5>","<urn:uuid:4b14a53d-4291-4f59-b3bd-26d176f7b010>"],"error":null}
{"question":"What is main difference in cooling functions between motor oil additives and metalworking flood coolants?","answer":"Motor oil additives and metalworking flood coolants have different approaches to cooling. While diesel engine oil additives primarily focus on reducing friction and wear through lubrication, with cooling being a secondary effect, flood coolants are specifically designed as heat transfer fluids to dissipate heat generated at the tool-chip interface. Flood coolants allow for heavier cuts, faster cutting speeds, and improved surface finishes in machining operations by actively removing heat through convection, thereby increasing the temperature gradient at the machining interface.","context":["Everything You Need to Know about Diesel Engine Oil Additives\nDiesel engines are known for their power and efficiency, but did you know that adding a specialized chemical compound to the engine oil can further enhance its performance? Diesel engine oil additives are precisely engineered chemicals that offer a wide range of benefits, specifically designed to optimize the functioning of diesel engines.\nIn the chemical reagent industry, where various chemical compounds are synthesized and manufactured, diesel engine oil additives play a crucial role. These additives are a part of the broader category of chemical reagents, specifically classified as \"other chemical additives.\" They are formulated to improve the lubrication, cleaning, and protective properties of engine oil, resulting in better engine performance and durability.\nOne of the key benefits of diesel engine oil additives is their ability to enhance lubrication. As the engine runs, the moving metal parts create friction, which can lead to wear and tear. Additives such as friction modifiers and anti-wear agents create a protective layer on the metal surfaces, reducing friction and minimizing engine damage.\nCleaning properties are another vital aspect of diesel engine oil additives. Deposits and contaminants can accumulate within the engine over time, affecting its efficiency. Detergent additives present in oil additives help dissolve and remove these deposits, keeping the engine clean and preventing potential issues.\nFurthermore, diesel engine oil additives offer protective properties. They can prevent corrosion and oxidation, safeguarding the engine against rust and degradation. Additives containing antioxidants and corrosion inhibitors act as a shield, prolonging the engine's lifespan and ensuring optimal performance.\nWhile diesel engine oil additives can provide numerous advantages, it is essential to choose the right additive for your specific engine and its requirements. Consulting with experts in the field and following the manufacturer's recommendations is crucial to ensure compatibility and maximize the benefits.\nIn conclusion, diesel engine oil additives are a vital component in the chemical reagent industry, particularly in the subcategory of \"other chemical additives.\" These additives enhance lubrication, cleaning, and protective properties of the engine oil, resulting in improved engine performance and durability. With their ability to minimize friction, remove deposits, and protect against corrosion, diesel engine oil additives play a significant role in optimizing diesel engine functionality.\n**Introduction:** In the fast-paced world of the electronics industry, efficiency and cost-effectiveness are key factors in maintaining a competitive edge. One often overlooked aspect of this is the use of motor oil additives to improve the performance and longevity of equipment. However, finding affordable solutions that don't compromise quality can be a challenge. In this article, we will explor\n# Introduction In today's fast-paced world, electronic devices have become an essential part of our daily lives. From smartphones to laptops, these gadgets play a crucial role in keeping us connected and informed. However, as technology continues to advance, so do the demands we place on our electronics. To ensure that our devices operate at their best, it is important to explore new and innovativ\n**Introduction** In the ever-evolving electronic industry, staying ahead of the competition is crucial. One way to achieve this is by utilizing TBN booster additives to enhance the performance of electronic products. These additives play a crucial role in improving the quality and efficiency of electronic devices, making them a must-have for manufacturers and distributors alike. **What are TBN Boo\n1. Introduction 1.1 The Role of Lubricant Additives in the Electronics Sector 1.2 Importance of Customization for Enhanced Productivity 2. Understanding Lubricant Additives 2.1 What are Lubricant Additives? 2.2 Types of Lubricant Additives in the Electronics Industry 3. Customized Lubricant Additives: A Game Changer 3.1 Tailored Solutions for Specific Industry Needs 3.2 Improved Efficiency and Per\nTable of Contents 1. Introduction 2. Understanding the Electronics Industry 3. Lubricant Additives: Enhancing Performance 4. Customized Lubricant Additives for Electronics 5. Benefits of Customized Lubricant Additives 6. Frequently Asked Questions (FAQs) 7. Conclusion 1. Introduction In today's fast-paced world, the electronics industry plays a vital role in our daily lives. From smartphones to la\nTable of Contents: 1. Introduction: The Importance of Lubricant Additives in the Electronics Industry 2. Understanding the Role of Lubricant Additives in Electronic Devices 3. Key Benefits of Customized Lubricant Additives for Electronics 4. Factors to Consider When Choosing Lubricant Additives for Electronics 5. Common Types of Lubricant Additives for Electronics 6. Frequently Asked Questions (FA","Metalworking lubricants, coolants, and fluids are used for metal forming, metal cutting, lapping, polishing, and grinding applications. They lubricate, decrease thermal deformation, and flush away removed material. The use of metal working fluids also improves surface finishes and increases tool life.\nMetalworking is any process used to shape, form, cut, or machine a metal workpiece. Metalworking lubricants, coolants, and fluids are specialized coatings and carriers for metal forming, metal cutting, lapping, polishing, and grinding applications.\nMetal forming is a process used to plastically deform a workpiece. In order to achieve the desired shape or geometry, a force must be applied that exceeds the material's yield strength. Metal forming oils, greases, and fluids dissipate heat, lubricate the working surface, and prevent localized strain that would otherwise cause damaging effects both to the tool and finished product.\nMetal cutting is a machining operation used to remove material and dissect workpieces. The primary function of the cutting fluid differs dependent on the cutting speed. At low speed the fluid acts as a lubricant prevent galling, friction welds, or other friction-induced wear at the cutting surface. At high speeds the fluid primarily acts as a coolant, preventing thermal deformation. Cutting fluids also function to remove cuttings, inhibit corrosion, and improve the cutting surface finish.\nLapping, polishing, and grinding are metalworking operations used to improve the surface of a workpiece. In lapping or polishing compounds, fluids or oils are used to carry abrasive powders. In grinding applications the main function of metal working lubricants, coolants, and fluids is to perform workpiece cooling.\nMetalworking fluids (MWFs) reduce or eliminate thermal deformation, localized strain, inhibit corrosion, and flush away removed material.\nThermal deformation is a phenomenon experienced in machining operations due to the combing effects of plastic strain and friction-induced heat at the tool and workpiece interface. The increased temperature gives rise to surface toughness, increased thermal error in precision machining operations, and decreased tool life. Cutting fluids counter these problems by lowering the coefficient of friction and transferring heat away via convection, thereby increasing the temperature gradient at the machining interface.\nThe following formula illustrates how heat is generated at the tool interface by friction:\nIc = Intensity of Friction Heat Source\nF = Friction Force\nVx = Sliding Velocity\nh = Plastic Contact Length\nb = Cutting Width\nLocalized strain occurs when there is a break-down in the lubrication regime. Localized areas that experience undesired solid-to-solid contact experience a spike in friction. This may cause the material to undergo some amount of strain, damaging either the tool or workpiece. Metalworking fluids (MWFs) are carefully designed to effectively lubricate the workpiece by anticipating the temperature and pressure environment that they experience in a given metal forming operation.\nCorrosion is a natural process where the surface of a substance, typically a metal, deteriorates due to a reaction that occurs within its environment. A corrosion inhibitor is added in small concentrations to the environment in order to control the rate or eliminate corrosion. In MWFs they are added to protect the untreated, exposed surface formed during the machining operation. To learn more about corrosion inhibitors please visit IHS's learn more page for rust preventives and corrosion inhibitors.\nMetalworking lubricants, coolants, and fluids include various types of fluids used in metalworking operations that are illustrated by the following table:\nElectrical Discharge Machining (EDM) Fluids\n|EDM fluids are dielectric fluids that provide specific voltage and amperage characteristics in EDM operations. They serve two functions, to stabilize a fixed spark gap ionization potential and to remove eroded debris.|\nFlood or Mist Coolants\n|Flood or mist coolants are heat transfer fluids used to dissipate heat that is generated at the tool chip interface. They allow for heavier cuts, faster cutting speeds, and improved surface finishes in almost all machining operations.|\n|Grinding fluids are coolants that may also include extreme pressure or chemically-active additives. They are used to improve and protect surface finishes and may also be used to disperse abrasive powders.|\nMetal Cutting Fluids\n|Metal cutting fluids are used in metal machining to improve tool life (reduce wear), increase lubrication, reduce workpiece thermal deformation, improve surface finish, and flush away chips from the cutting zone.|\nMetal Forming Fluids\n|Metal forming oils, greases, and fluids are designed to enhance lubrication during extrusion, wire drawing, stamping, bending, swaging, rolling, embossing, and other deformation processes.|\n|Mold releases and release agents are film-forming lubricating oils, solid lubricants, waxes, fluids, or coatings that prevent other materials from sticking or adhering to an underlying surface.|\n|Quenching oils and heat treatment fluids provide rapid or controlled cooling of metallic parts. They are used to temper, harden, or treat the material to achieve desired physical properties.|\nThere are three basic product forms of MWFs: fluids, greases, and solid lubricants or dry film lubricants.\nFluids are water-based liquids, oils, or fluids supplied in liquid form.\nGreases, gels, and lubricating pastes are thick, high viscosity products that do not run or flow off surfaces. Greases often consist of oil thickened with a sodium or calcium soap complex or non-soap thickener.\nSolid lubricants or dry film lubricants are compounds such as hexagonal flake graphite, boron nitride (BN), molybdenum disulfide, or polytetrafluoroethylene (PTFE) powders.\nMetalworking lubricants, coolants, and fluids vary widely in terms of chemical composition. They may be classified as being either petroleum or mineral oil-based, or synthetic or semi-synthetic.\nPetroleum or mineral oils\nPetroleum and mineral oil products are functional fluids derived from petroleum. They include a broad range of hydrocarbon-based substances of varying chemical compositions and a wide variety of physical properties. Specific constituents present include aromatic, naphthenic, and paraffinic fluids.\nSynthetic or semi-synthetic\nSynthetic or semi-synthetic fluids include fluids with a base of glycol or polyglycol, ester or diester, or silicone-based fluids. They exhibit outstanding thermal and dielectric properties. The characteristics, cost, and heat transfer performance of semi-synthetic fluids fall between those of synthetic and soluble oil fluids.\nOther Specialized Fluids\nOther specialized fluids include high water content fluids (HWCF), lithium complexes, aluminum complexes, waxes such as paraffin and stearate, and halogenated hydrocarbons including chlorofluorocarbon (CFC), halogenated fluorocarbon (HFC), halogenated chlorofluorocarbon (HCFC), and perfluorocarbon (PFC).\nImportant properties for metalworking lubricants, coolants, and fluids include concentration, flash point, and autogenous ignition temperature (AIT).\nConcentration is measured after dilution of the fluid solution on a volumetric basis.\nFlash point is the lowest temperature at which a liquid can give off sufficient vapors to form an ignitable mixture in air near the surface of the liquid.\nAutogenous ignition temperature (AIT) is the temperature at which ignition occurs spontaneously.\nASTM D2881 - This classification covers and is designed to standardize and consolidate the terminology, nomenclature, and classification of metalworking fluids and related materials.\nASTM D6482 - This test method covers the equipment and the procedure for evaluation of quenching characteristics of a quenching fluid by cooling rate determination.\nASTM E2275 - This practice addresses the evaluation of the relative inherent bio-resistance of water-miscible metalworking fluids, the bio-resistance attributable to augmentation with antimicrobial pesticides, or both. It replaces Methods D3946 and E686.\nRelated Products & Services\nDielectric Greases and Insulating Fluids\nDielectric greases and insulating fluids are applied to electric terminals in high voltage equipment to minimize disharges, insulate and lubricate non-conductive surfaces, exclude moisture, and conduct heat.\nHeat Transfer Fluids and Thermal Oils\nHeat transfer fluids, thermal oils, circulating coolants, and heater liquids are used to carry thermal energy in process heating and machine cooling applications.\nHydraulic Oils and Transmission Fluids\nHydraulic oils and transmission fluids are used to transmit power in hydraulic equipment and power transmission applications.\nIndustrial greases are thickened gels that consist of natural, synthetic, or semi-synthetic substances. They do not run off surfaces and are used in a variety of lubrication, sealing, and exclusion applications.\nIndustrial lubricants are oils, fluids, greases and other compounds designed to reduce friction, binding or wear and exclude moisture. Specialized characteristics may enhance thermal conduction across thermal interfaces or reduce electrical resistivity across electrical joints.\nSolid and Dry Film Lubricants\nSolid and dry film lubricants form a dry layer or coating that excludes moisture and reduces friction, binding, and wear. They often contain additives such as corrosion, oxidation, and rust inhibitors.\nSynthetic Oils, Greases, and Lubricants\nSynthetic oils, greases and lubricants are based on synthetic compounds such as silicone, polyglycol, esters, digesters, chlorofluorocarbons (CFCs) and mixtures of synthetic fluids and water."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:3006c84c-33d1-41a2-8689-08355225209d>","<urn:uuid:17640faa-1d6b-43c4-9623-7e40a248c850>"],"error":null}
{"question":"I am breeding horses and want to know - what are similarities between speed horses and PRE horses in terms of their characteristics and abilities?","answer":"While speed horses and PRE horses have distinct traits, they share some important characteristics. Both types demonstrate strong athletic abilities - speed horses excel at sprinting and show \"turn of foot,\" while PRE horses display \"brilliant, energetic, rhythmic and elastic\" movements. Both breeds are known for their intelligence and learning capacity - speed horses succeed in racing while PRE horses show \"great capacity for learning\" and easily adapt to various disciplines. Additionally, both types can succeed in specific conformational traits - speed horses often have muscular builds especially in shoulders and hindquarters, while PRE horses are noted for their \"proportionate conformation\" and \"remarkable general harmony.\" However, they tend to excel in different disciplines - speed horses in racing and sprinting, while PRE horses particularly shine in dressage and collection movements.","context":["By Edwin Anthony\nSpeed. Acceleration. Brilliance. These words draw attention and make the heart race. Speed is especially important in stallion prospects, which is why breeders are always on the lookout for horses with that elusive \"turn of foot.\" A horse might power to the lead from the gate and set eye-popping fractions, or rally with an explosive move on the final turn. Either way, speed wins races.\nMore importantly, speed is often a deciding factor in stallion success. Consider that there are a number of successful sires and broodmare sires, but far fewer successful sires-of-sires. After years of research, I've come to the conclusion that speed is the single most important factor in determining a horse's chances to succeed as a sire of sires. The speed merchants Blushing Groom, Danzig, Icecapade, In Reality, Mr. Prospector, Seattle Slew, and Storm Cat are continually changing the face of the industry through their descendants.\nSpeed influences the outcome of important stakes races and shapes the results of yearling and 2-year-old auctions. In an increasingly commercial market, velocity and precocity are always in fashion. The foundation provided by classic influences is as important now as ever, but speed is a very important part of the evolution of the Thoroughbred.\nNot long ago, sprinters (restricted to six furlongs) were frowned upon as stallions. If a colt couldn't win a graded stakes at a mile (or farther), breeders and yearling buyers weren't particularly interested. The success at stud of sprinters like Belong to Me, Carson City, Mt. Livermore, and Phone Trick has changed a lot of stubborn minds.\nPure speed is sometimes seen as a negative, because of the tendency of the sport's fastest runners to suffer injuries. An Indy 500 car needs more tune-ups and pit stops than your average car, so you can expect some wear and tear with the fleetest Thoroughbreds. Speed is a necessary element in pedigrees, and works quite well, as long as it is utilized properly. If tempered with the stamina and good bone provided by classic influences in pedigrees (Buckpasser, Fappiano, Nijinsky II, Ribot, Roberto, Secretariat, etc.), a runner sired by a sprinter or miler can win quality route races and stay sound.\nMany stakes winners are able to grind out victories with sustained moves over long distances, but these plodders are not the kind of colts that normally find success at stud. Some classic distance runners are able to succeed as stallions because they also have the speed to win at shorter distances. While Unbridled won the Kentucky Derby (gr. I) and Breeders' Cup Classic (gr. I), he also beat champion sprinter Housebuster at seven furlongs -- a noteworthy achievement. Many other classic winners and champions have been capable of winning at shorter distances as well.\nThe owners of horses with enough stamina to win stakes races at nine or 10 furlongs will logically campaign their horses in those events. The purse structure for two-turn stakes events is significantly better than it is for sprints. Most horses have a limited number of starts in them, and economic choices must be made.\nAt the same time, owners would greatly enhance their horse's residual value by allowing it to flash speed from time to time, even in a workout. Forty Niner (as an example) was a popular stallion because he was capable of winning at the classic distance of 10 furlongs, but also set a track record at one mile and was 2-year-old champion. Versatility is definitely a plus in the eyes of breeders.\nPure sprinters seem to have a distinctive build. An overly muscular shoulder and hind end, a wide chest, and a short-coupled body are the conformational hallmarks of a speed type, although good horses come in all shapes and sizes. Phalaris was one of the first sprinters to find success at stud, and he was a horse with size and scope -- the antithesis of the modern sprinter.\nMares with speed have had a profound impact on the breed as well. Crimson Saint (dam of Royal Academy, second dam of Storm Cat), Gold Beauty (dam of Dayjur, second dam of Sky Beauty), and Mumtaz Mahal (foundation mare of the Nasrullah, Mahmoud, Royal Charger, and Abernant families) were renowned speedballs during their racing careers. These mares prove that the time-honored practice of crossing speedy stallions and stamina-laden mares can work in reverse (stamina stallions over mares with speed).\nThe old handicapping adage says, \"Pace makes the race.\" In that same vein, you can say that speed drives the breeding industry.","The PRE Horse. Breed History.\nThe origin of the PRE Horse is lost in time to mingle with the evolutionary history of horses in general, without a specific date for its appearance in Spain.\nHowever, it is true that in the pre-Roman times, there were already equestrian references in what is known as Spain today.\nRoman authors such as Plutarch, Pliny the Elder and Seneca spoke of the horses from Hispania as a beautiful, docile, arrogant and brave animal, ideal for war and for the sports performed in the circuses of the time.\nKing Philip II of Spain organized the equine herds of his Kingdom and laid down the solid bases for the PRE Horse to reach its peak in forth-coming years. This was possible in 1567 with the creation of the Royal Stables of Cordoba, where the best stallions and mares from the provinces bordering the Guadalquivir River were brought together, which were the most productive horse breeders at that time.\nThus, the Royal Stud Farm was founded and with time, it became known as the National Stud Farm.\nMany horses were sent to the American continent and played a decisive role in exploring the lands. Likewise, these horses were the origin and basis for most of the breeds that were subsequently raised there.\nIn Europe, while Spain enjoyed its Golden Age, one of the most coveted gifts from the Spanish Monarchy was one of its magnificent horses. Soon, Spanish horses earned a great reputation, and were decisive in the birth of Central European breeds thanks to their beauty, temperament, intelligence and their learning skills. At the time, they were most outstanding in the exercises of the incipient Spanish High School, the origin of today’s dressage.\nIn successive centuries, the Spanish Horse continued its development in the hands of the National Stud farm, individuals and landowners.\nToday, there are some 175,000 PRE Horses in the world. The approximate 7,500 breeders are located in more than 65 countries the world over.\nBreeds such as the Lipizzaner, Lusitano, Paso Fino and the Central European Warmblood all owe their ancestry to the PRE Horse.\nImportant information about the PRE Horse\n1 | Official Name: PRE Horse\nThe official name of the breed is Pura Raza Española, or P.R.E., as it appears in the breed Stud Book.\nOther names such as Andalusian or Iberian Horse do not correctly describe the breed. In general, these terms are used to describe crossbreeds, which lack the quality and purity, as well as any official documentation or registration from Spanish Stud Book. Carthusian horses are a family (or bloodline) within the PRE Horse.\n2 | Official documentation for a PRE Horse: the Registry in Spain is the only one in the world\nOnly the Stud Book in Spain is authorized to officially register and issue documentation for the PRE Horses, whether in Spain or abroad, the world over. The Stud Book in Spain is the only organization that guarantees breed purity and that all internationally accepted controls for the breed have been applied; this includes DNA confirmation of paternity, written and graphic description and microchip at weaning, evaluation as breeding stock at 3 years-old, assignment of a registration code in the Stud Book, etc.\nThe passport issued by the Ministry of Agriculture of Spain and ANCCE is the only documentation that guarantees that the horse is a genuine PRE Horse and complies with all the requirements for equine identification imposed by the European Union.\n3 | Contents of the Stud Book\nThe following registers are included in the Stud Book:\na) Register of Births | This register is for those horses, whether colts or fillies, born of breeding stock included in the Main Register and that comply with the officially established conditions for their inclusion in the Stud Book.\nb) Main Register | This register includes breeding horses that have turned three years and that are already included in the Register of Births, comply with the breed prototype and have been accredited with the absence of disqualifying defects as established in this appendix, by means of a specific certificate issued by authorized personnel.\nWithin this Register, there is a special listing for those horses that have passed the qualification tests as laid out in this Law and that is called the Register of Qualified Breeding Stock.\nThere will also be a Register of Elite Breeding Stock, for those horses that undergo a genetic evaluation within the framework of the selection scheme.\nc) Register of Merits | This register is for breeding stock included in the main register, evaluated and/or elite which after confirming their successes in competitions, demonstrate excellent conformation and functionality qualities.\n4 | Aptitude for Breeding\n1. Horses in the Register of Births will be included the Main Register after having been evaluated (graded) that they comply with the breed prototype and that they are free of any disqualifying defects; authorized personnel will issue the corresponding certificate.\n2. Horses in the Main Register will be inspected by the commissions established to this effect, at the request of the owners.\n3. To be considered «qualified breeding stock» or «elite breeding stock», Purebred Spanish Horses will be evaluated (graded) to prove their locomotive, genetic and reproductive qualities, with the following types:\n«Qualified Breeding Stock» | Are considered those horses, three years of age or over, both males and females, that meet the standard for basic aptitude as breeding stock, according to the criteria established for the breed prototype or conformation, and pass a functional test and examination of their reproductive system and a veterinary verification may be considered Qualified Breeding Stock. This qualification appears in the horse’s passport or genealogical card.\n«Elite Breeding Stock» | Are those breeding horses, seven years of age or over, both males and females, which have already been included in the Register of Qualified Breeding Stock and that have undergone a genetic evaluation within the framework of the selection scheme. This evaluation includes the verification of conformational, functional and reproductive parameters of the horses and of their descendants and siblings. Although they have not been included in the Qualified Breeding Stock listing, those horses, whether males or females, and whose descendents have achieve outstanding competitive success within the framework of the improvement plan for the breed, can also be included in the list of Elite Breeding Stock.\n4. Artificial reproduction is authorized for those horses included in the registers of qualified and/or elite breeding stock, under the conditions that are officially determined.\nP.R.E. breed prototype\nA | General characteristics\nPRE Horses have average volume, intermediate lines with a sub-convex to straight profile, with proportionate conformation, remarkable general harmony and great beauty, with significant sexual dimorphism.\nTheir paces are brilliant, energetic, rhythmic and elastic with substantial elevation and extension and pronounced facility for collection.\nTheir energetic temperament, noble, docile and well-balanced personality combines with their great capacity for learning.\nB | CONFORMATIONAL CHARACTERISTICS\n1º The Head | Well proportioned, average length, lean, with a straight to slightly convex frontonasal profile. The medium sized ears are well placed. The forehead is wide, and slightly convex to house large, lively and expressive triangular eyes that are slightly arched without surpassing the profile.\nRelatively long and moderately narrow face (more so in the mares), slightly rounded and not fleshy with a tapered nose, softly and gently curving from the face. The nostrils are wide, non-projecting, surrounded by broad, non-fleshy cheek, with a long discreetly arched edge.\n2º The Neck | Of average size and length, lightly arched and muscular (less so, in the mares). Well inserted at the head and trunk. Abundant and silky mane.\n3º The Body | In proportion and robust. Wither discreetly wide and well defined, gently following into the back line. Consistent back, muscled and almost flat. Short, wide loins, well muscled and somewhat arched, well joined to the back and the croup. Croup of average length, rounded width and gently sloping. A low set-on tail that lies between the buttocks, with thick long and often wavy hair. Deep and broad chest. Ribs well arched, long and deep. Extensive flanks and correct stomach.\n4º Forelimbs | Shoulder blade, long, well-muscled, oblique and elastic. Strong shoulder, with a good angle. Potent forearm, of average length. Well developed and lean knee. Cannon bone in proportion with a well defined and extensive tendon. Lean, well-defined fetlock, with little hair. Pasterns with good conformation, slope and angle, of proportionate length. Compact hooves, well developed and balanced.\n5º Pelvic area or hind quarters | Muscled thigh, lightly arched and muscled buttock and long hind leg, Hock strong wide and neat. The regions located below the hock joint will be identical to those described for the forelimbs. In both, the limbs must be correctly aligned.\nC | Phaneroptical Characteristics\nFine, short hair. Dominant colors are gray and bay but other colors are acceptable.\nD | Behavior and Temperament\nRustic, sober, balanced and resistant horses. Long suffering and energetic. Noble and docile. Learn easy and are able to adapt to a variety of uses and situations.\nE | Functional Characteristics and Aptitudes\nExcellent aptitude for performing a variety of functions, with a good response to the rider’s aids and with a good mouth, making them obedient, easily understanding the rider and extraordinarily comfortable.\nTheir main use is for riding, being ideal for schooling work on the flat (high school, dressage and doma vaquera), for rejoneo (horseback bull fighting), acoso & derribo (chasing and testing of young bulls), for driving, working with livestock, field activities and other equestrian disciplines. Their movements are agile, high, extensive, harmonic and rhythmic. They have a special predisposition for collection and turns on the haunches.\nThe Spanish Dressage team won the team silver medal at the 2004 Athens Olympics, only surpassed by Germany.\nTwo PRE horses were part of this team: Invasor and Oleaje—ridden by Rafael Soto and Ignacio Rambla—only 8 years after their first Olympic appearance.\nThis was the cherry on the cake for one of the most brilliant equestrian careers. Atlanta ‘96 was the first time that the Spanish dressage team reached an Olympic final. In fact, it was the first time that PRE horses had participated in such a prestigious event. Their names: Evento, Flamenco and Invasor (the latter as reserve). Invasor was the youngest horse of all those entered for dressage, which is an example of the breed’s intelligence and willingness to work.\nSydney 2000 was the next step. The team included Invasor and Distinguido who finished in 7th place.\nSpain’s progress was more than outstanding at the 2002 JEREZ WEG, where Spain won the bronze medal, again with Invasor and Distinguido.\nAt the 2003 European Championships at Hickstead brought a silver medal for the Spanish national team.\nIn Competition Carriage Driving, the Purebred Spanish Horse has participated in several World Championships with Juan Robles Marchena and Antonio Carrillo Baeza as the most noted whips.\nThe PRE horse was used as foundation stock the creation of several Central European Warmblood breeds. Today, PRE sires are being used improve a good number of sports horse stud farms.\nTheir mental balance, intelligence, willingness to work and harmony of form bestow the PRE horse a special capacity for collection. The official 2002 JEREZ WEG video showed Gran Prix exercises with horses and riders implementing what the judges considered the very best performance. INVASOR and Rafael Soto appeared on three occasions: passage, collected canter and canter pirouettes.\nANCCE invests a great deal of effort into promoting of the use of PRE horses for riding. To achieve this, a pioneering initiative was undertaken—the creation of a High-Performance Center dedicated exclusively to the Purebred Spanish Horse.\nThis Center is located in the Chapin competition complex in Jerez, host city of the 2002 WEG, with world-class facilities.\nRafael Soto, renowned Spanish Olympic Dressage rider, is a consultant to the PRE High-Performance Center. Dutch trainer Henk Bergen is a special collaborator.\nThe objective of the CAR-ANCCE High Performance Center is to create a reserve of horses and riders so that in the not so distant future they can compete at top level competitions.\nANCCE also runs a program of competitions throughout Spain, known as the ANCCE CUP. These competitions include the disciplines of Doma Vaquera, Alta Escuela, Show Jumping, Dressage and Carriage Driving. Considerable economic prizes are awarded at these competitions, which also serve as a testing ground horses and riders who are starting out in the competitive world."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:5dbed5ab-67d2-4a03-84cf-9b64c745ceda>","<urn:uuid:9d28fed3-0306-46c5-acee-aab1bdaaed88>"],"error":null}
{"question":"Can you compare breast imaging services at general diagnostic facilities vs specialized breast centers?","answer":"General diagnostic facilities like Palm Harbor MRI offer breast imaging as part of their comprehensive services, including breast MRI alongside other diagnostic tests like CT scans, X-rays, and ultrasound. In comparison, specialized breast centers like Breastlink focus exclusively on comprehensive women's breast imaging services, featuring dedicated breast radiologists with fellowship training who lead their imaging departments. These specialized centers are FDA-certified according to the Mammography Quality Standards Act and offer state-of-the-art digital mammography equipment with an expert team of breast radiologists and technologists.","context":["PALM HARBOR MRI is a full service imaging and diagnostics testing facility located in Palm Harbor, Florida that was established in early 1995 and began by offering MRI and X-Ray. After recognizing the needs of our patients and developing relationships within our Medical Community, we expanded our services. Palm Harbor MRI is one of the few facilities in our area that offer both a High Field MRI and an Open MRI under the same roof for our patients who may be claustrophobic.\nWe take pride in offering our patients the latest in imaging and diagnostic technology while maintaining an environment that is friendly, relaxed and professional. Our modalities include: High Field MRI, MRI-Arthrography, MRI-Angiography, MRI-Venography, Breast MRI, Open MRI, CT Scan, X-Ray, Ultrasound, Mammography, Echocardiogram, and DEXA (Bone Density Scans), as well as a four room Sleep Diagnostics facility.\nMagnetic Resonance Imaging (MRI) is a method of obtaining detailed pictures of internal body structures without the use of radiation or radioactive substances of any kind. This is accomplished by placing the patient in a magnetic field while harmless radio waves are turned on and off.\nMRI Angiography (MRA) is an MRI study of the blood vessels. It utilizes MRI technology to detect, diagnose and aid the treatment of heart disorders, stroke, and blood vessel diseases. MRA provides detailed images of blood vessels without using any contrast material.\nArthrography is done most commonly to identify abnormalities associated with the shoulder, wrist, hip, knee and ankle. Patients who undergo this procedure usually have complained of persistent, unexplained joint pain or discomfort. Arthrographic images may allow identification of problems with a joint’s function or indicate a need for a joint replacement.\nMRI Associates is dedicated to the emerging technology of low-field, patient-friendly magnetic resonance imaging systems. Claustrophobic, obese and pediatric patients,\nMRI Associates is dedicated to the emerging technology of low-field, patient-friendly magnetic resonance imaging systems.\nBreast Magnetic Resonance Imaging (MRI) is another non-invasive breast imaging tool that produces detailed images of the breast. Breast MRI utilizes a magnetic field and radio waves as well as an intravenous injection of a contrast agent to highlight breast abnormalities.\nCT scan; Computed Axial Tomography (CAT) is a method of body imaging in which a thin x-ray beam rotates around the patient. Small detectors measure the amount of x-rays that make it through the patient or particular area of interest.\nAn X-ray examination uses electromagnetic radiation to make images of your bones, teeth and internal organs. Simply put, an X-ray allows your doctor to take pictures of the inside of your body.One of the oldest forms of medical imaging, X-ray is a painless medical test that can help your doctor in diagnosis and treatment\nUltrasound imaging is a common diagnostic medical procedure that uses high-frequency sound waves to produce dynamic images (sonograms) of organs, tissues, or blood flow inside the body. Prenatal ultrasound examinations are performed by trained professionals, such as sonographers, radiologists, and obstetricians.\nMammograms are specialized X-ray examinations of the breast. Two types of mammogram studies are commonly performed. A SCREENING MAMMOGRAM is performed on women who have no current symptoms or breast problems while a DIAGNOSTIC MAMMOGRAM is performed specifically to evaluate a breast problem or revisit a previous abnormal finding.\nEchocardiogram, often referred to in the medical community as a cardiac ECHO or simply an ECHO, is a sonogram of the heart. Also known as a cardiac ultrasound, it uses standard ultrasound techniques to image two-dimensional slices of the heart. The latest ultrasound systems now employ 3D real-time imaging.\nBone density scanning, also called dual-energy x-ray absorptiometry (DXA or DEXA) or bone densitometry, is an enhanced form of x-ray technology that is used to measure bone loss. DEXA is today’s established standard for measuring bone mineral density (BMD).\nSleep apnea means that you often stop breathing for 10 seconds or longer during sleep. The problem can be mild to severe, based on the number of times each hour that you stop breathing or how often your lungs don’t get enough air.\nThis may happen from 5 to 50 times an hour.","Comprehensive women’s breast imaging services are available at multiple Breastlink sites throughout Southern California.\nWhat is Breast Imaging?\nBreast imaging refers to the various diagnostic imaging procedures performed by radiologists for breast cancer prevention, diagnosis and treatment. Radiologists rely on several different breast imaging tools in the fight against breast cancer.\nWhy are Breast Imaging Procedures Performed?\nBreast cancer affects more women than any other non-skin cancer. Breast imaging plays an important role in the prevention and treatment of breast cancer. Breast imaging procedures are generally performed for one of two reasons:\n- to screen for potential breast abnormalities or\n- to determine the exact nature of a previously detected breast abnormality.\nMammography is the most widely used and most effective breast cancer screening procedure. Mammography can detect breast cancers up to two years before they become evident to a woman or her physician during a physical examination.\nCurrent guidelines recommend women aged 40 and above have a screening mammography performed every year. If breast screening reveals a potential abnormality in a woman’s breasts, she will be sent for further diagnostic mammography or other diagnostic imaging procedures. These diagnostic imaging procedures are performed to provide her physicians with more detailed information about her condition and to help them decide on the best course of treatment.\nTypes of Breast Imaging Procedures\n- Mammography: Mammography uses X-rays to produce images of breast tissue in order to detect lumps, tumors or other abnormalities that may be present in the breast. Images produced by mammography are called mammograms.\n- 3D Mammography: 3D mammography, also known as breast tomosynthesis, is a relatively new technology that produces 3-dimensional images of breast tissue rather than the 2-dimensional images produced by traditional mammography.\n- Breast Ultrasound: Ultrasound uses sound waves to produce images of breast tissue. Breast ultrasounds are generally performed to identify the exact nature of a breast abnormality previously detected by mammography or as supplemental breast cancer screening for high-risk patients. Breastlink of Orange and Breastlink of Temecula Valley feature automated breast ultrasound technology.\n- Breast MRI: Magnetic resonance imaging (MRI) uses magnetic fields and radiofrequency pulses to produce detailed images of internal body structures. Breast MRI can provide information not available from mammography or ultrasound, but is only used in combination with and never as a replacement for these breast imaging procedures.\nRadiologists’ Role in Breast Cancer Prevention & Treatment\nRadiologists with breast imaging fellowship training lead all of our breast imaging departments to ensure you receive an experienced interpretation of your mammogram study. Additionally, all of our centers are accredited and certified by the U.S. Food and Drug Administration (FDA) in accordance with the Mammography Quality Standards Act (MQSA).\nAs of April 1, 2013 the state of California requires imaging centers to inform patients if they have dense breast tissue as identified by a radiologist after interpreting their screening mammography study. Imaging centers are also required to inform women, classified as having dense breast tissue on a screening mammography study, that dense breast tissue may make it more difficult to detect potentially cancerous lesions in their screening mammography study. For more information on dense breasts, please visit our press release section.\nWant to Learn More About Breast Imaging?\nOur centers have state-of–the-art digital mammography equipment and an unmatched team of breast radiologists and technologists to ensure you benefit from the best screening available. Talk with your doctor about the frequency of screening appropriate for you.\nIf you are concerned with coverage, based on your age, check with your health insurance provider. Contact us online and we will get back to you within 1 business day to schedule your appointment.\nRecent Breast Imaging Blog Posts\nA lot of women don’t understand why radiologists ask them not to wear deodorant when [...]\nMyth: Mammograms are equally effective for all women. Fact: Up to 50% of small cancers may [...]"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"premise_categorization","category_name":"with premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:77ea43ad-0f0a-4e93-8a9a-c9ea1290742a>","<urn:uuid:c18c0eb9-fe5e-4aa0-943f-35b89fdc9f55>"],"error":null}
{"question":"Did both ancient Chinese and African traditional healers use herbs for dental treatments?","answer":"Yes, both cultures used herbal treatments for dental care. African traditional healers practiced herbalism as one of their therapeutic methods for oral health care. Similarly, in ancient China, herbs were used in dental treatments - for example, a tooth medicine from the fifth and sixth centuries contained honey locust fruit, ginger, foxglove, and lotus leaves boiled into a paste to whiten teeth, reduce gum inflammation, and ease toothache.","context":["A review of the role of African traditional medicine in the management of oral diseases\nBackground: Poverty, inadequacy of health services, shortage of health workers, infectious diseases scourges, rampant shortage of drugs and equipment in existing health facilities make traditional medicine an important component of healthcare in Africa, especially in oral health care. This review was done to document the role of African traditional medicine in oral health care delivery in order to provide a comprehensive documentation, identify research gaps, and suggest perspectives for future research.\nMaterials and Methods: Available references or reports in English and French on the use of African traditional medicine in oral health care were consulted from published scientific journals, books, reports from national, regional and international organizations, theses, conference papers and other grey materials. Literature was searched on international online databases such as Pubmed, MEDLINE, Science Direct, Scopus and Google using the MeSH words “Traditional medicine or healer”, “oral health”, “dental caries”, “dental”, “primary oral health care’ and “medicinal plants”.\nResults: Contemporary African communities operate a pluralistic health system whereby highly sophisticated biomedical health care co-exists and even competes with traditional medical practices. Though most patients opt for dual consultations, the introduction of biomedicine has never replaced traditional indigenous medicine and traditional healers are consulted for several reasons making dual treatment a common occurrence in many communities. Factors such as the lack of health care workers, inequalities in the health sector due to socio-cultural and socio-economic disparities prevent people from patronising both health care systems. Therapeutic methods used by African traditional healers include herbalism, psychotherapy, simple surgical procedures, rituals and symbolism. The types of medications used by traditional healers were classified as preventive and prophylactic medications. Some practices of traditional healers included tooth extractions with medicinal plants and also in other practices resulting in exposure to blood; practices involving the use of shared instruments had been reported to be responsible for the transmission of HIV/AIDS. Other harmful practices include gemectomy, uvulectomy and different forms of infant oral mutilations.\nConclusion: Traditional healers provide dental care, but their work was not integrated with that of a dentist. Traditional healers have special qualities that make them highly effective in primary oral health care therefore making them inevitable stakeholders in primary oral health care delivery. The research gaps in this review include the integration of traditional medicine into the oral health care systems of many African countries and the evaluation of the treatment outcomes of traditional medicine in dental care.\nKey words: Review, African traditional medicine, oral health.\nCopyright: Creative Commons Attribution CC.\nThis license lets others distribute, remix, tweak, and build upon your work, even commercially, as long as they credit you for the original creation. This is the most accommodating of licenses offered. Recommended for maximum dissemination and use of licensed materials. View License Deed | View Legal Code Authors can also self-archive their manuscripts immediately and enable public access from their institution's repository. This is the version that has been accepted for publication and which typically includes author-incorporated changes suggested during submission, peer review and in editor-author communications.","Dentistry and oral hygiene in ancient China\nIvory skin, shapely eyebrows, and a full figure: many of ancient China’s well-known beauty standards were based on looks that were hard to maintain in times of hardship and poverty. Likewise, a full, healthy set of teeth was a rare achievement in the days before electric toothbrushes, toothpaste, and dental floss.\nAccordingly, oral hygiene was highly valued: The ballad “Shuo Ren,” recorded in the 3,000-year-old The Book of Songs, depicted a (quite literally) toothsome woman as having “teeth like rows of melon seeds (齿如瓠犀)”—that is, white and even. “Song of a Beauty” added another characteristic: “When she sighs, her breath is as sweet as orchids (长啸气若兰).”\nThe oldest and most widely prescribed way of keeping one’s mouth and teeth clean was gargling. According to The Book of Rites, in the Zhou dynasty (1046 – 256 BCE), “the whole family washes up and rinses their mouths after the cock crows,” indicating that the habit has existed in China for at least 2,000 years.\nSun Simiao, the legendary Tang (618 – 907) physician known as the “King of Medicine,” prescribed salt water as the ideal mouthwash: “Every morning, put a pinch of salt in the mouth, take a mouthful of warm water, and hold it [in the mouth]…it will keep your teeth firm,” he wrote in Essential Formulas for Emergencies Worth a Thousand Pieces of Gold. In the 18th century novel Dream of the Red Chamber, the characters gargled with tea.\nSimply rinsing, though, has never been enough, and the Chinese began to brush their teeth quite early in history. In the Tang dynasty, it was common to soak a twig of willow in water every night before bed and chew on it the next morning. The fiber of the willow, protruding like a comb, would scrub the teeth clean. The Chinese idiom “晨嚼齿木 (chew wood at dawn)” concisely described this practice.\nIn 1984, a toothbrush dating back to the Tang dynasty was unearthed in Chengdu, Sichuan province. It looked remarkably similar to the modern version: a long handle made of animal bone, and two rows of holes drilled into the head, which were to be filled with bristles of animal hair. In the 13th century, scholar Wu Zimu recalled in his memoir A Dream of Millet that shops specializing in manufacturing and selling toothbrushes had become prevalent in the capital Lin’an (Hangzhou).\nToothpaste had emerged even earlier. Known as “tooth medicine” or “tooth powder,” ancient toothpaste shared ingredients with many other traditional remedies. According to the Song physician Zhang Gao, a popular tooth medicine in the fifth and sixth centuries was obtained by boiling honey locust fruit, ginger, foxglove, lotus leaves, and several other herbs into a paste that could whiten the teeth, reduce inflammation of the gums, and ease toothache.\nIn order to “吐气如兰 (exhale like an orchid),” some people sucked on oranges, parsley and Sichuan pepper before social visits. The most effective breath-freshening agent was said to be cloves, an expensive spice also known as “chicken tongue fragrance” (鸡舌香) in ancient times. According to The Rituals of the Han Palace, a Han dynasty (206 BCE – 220 CE) official once reported work with cloves in his mouth. His supervisor, impressed, spread his fragrant reputation to others, and it soon became fashionable among high officials and nobility to suck on cloves while speaking.\nDespite these efforts, dental disease was unavoidable. The earliest record of dentistry practice in China was found in the Han dynasty’s The Records of the Grand Historian, in which the physician Chunyu Yi used acupuncture and kuh-seng soup to cure an official’s tooth decay.\nZhang Zhongjing, the “Medical Sage” of the Han dynasty, prescribed a tooth decay remedy that called for crushing realgar, a ruby-colored mineral, and whitlow grass into a powder, then melting it lard. A cotton swab was dipped in the mixture, then “[lit] on fire to burn the affected area with.” (People now speculate that Zhang’s method simply killed the dental nerves.) Historical records from the Tang dynasty also showed dentists using pewter, silver foil, and mercury to fill in cavities.\nWhen all other remedies failed, the patient had to face tooth extraction. This painful solution could be deadly in the days before modern antiseptics—as indicated by China’s earliest recorded dental surgery in the year 329. According to Records of the Three Kingdoms, a 42-year-old prefectural governor named Wen Qiao had to have a diseased tooth pulled. Unfortunately, this caused a stroke, and Wen died within 10 days of the surgery. Doubtless, his tragic tale would have been used by future generations of parents to remind their children to chew more tree branches, and gargle with saltier water and stronger tea before bed—lest they be faced with a dangerous trip to the dreaded dentist’s chair.\n“Teething Trouble” is a story from our issue, “The Wellness Issue”. To read the entire issue, become a subscriber and receive the full magazine. Alternatively, you can purchase the digital version from the iTunes Store."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_length","category_name":"long"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:e94942e3-479d-4519-8d82-a213a0082f2c>","<urn:uuid:bb8d5fe2-d274-46ee-a40b-75adaf692f9f>"],"error":null}
{"question":"What are the key innovations in thermal bonding patterns for nonwoven fabrics?","answer":"Two notable thermal bonding patterns were developed: the 'Rib Knit' style pattern which provides knit-like texture in commercial nonwoven fabrics, and the 'S-Weave' pattern which offers unique strength and softness attributes. Both innovations resulted in two patents each.","context":["Ann L. McCormack\nLifetime Technical Achievement Award\n1978 Bachelor Science University of Wisconsin Microbiology\n1983 Master of Science Institute of Paper Science & Tech Eng Chem\n1979-81 Sr. Research Technician Nonwoven Research\n1983-86 Research Scientist New Business\n1986-91 Sr. Research Scientist Personal Care Matl Dev\n1991-98 Associate Research Fellow Nonwoven Research\n1999-04 Research Fellow Nonwoven Research\n2004- Technical Leader III Global Nonwoven Research\nMs. McCormack is a top rated Senior Technical Leader in Kimberly-Clark with a stellar history of Personal Care Business impacting nonwoven inventions. In her twenty five (25) year career she has held research positions encompassing new raw materials, spunbond, meltblown, co-form, and carded web technologies. She is, however, most notably known for bringing a “new” nonwovens into Kimberly-Clark’s nonwoven mills, namely ‘film’. Her work since the late 80’s has pioneered Kimberly-Clark’s low cost film and microporous film technology leading to the invention of two layer film/spunbond and three layer SMS/film/SB breathable laminates using both thermal and adhesive bonding. Key applications include low cost laminates for Personal Care backsheet (Huggies® Diapers and Depends® Undergarments) and high barrier laminates for Health Care applications (Microcool®). Many of her patents mark commercial product packages. Twenty two (22) US patents have been associated with her film and nonwoven innovations. Her work led Kimberly-Clark leadership via high speed in-line, wide width commercial base machines to produce first-to-the-world diaper backsheet, health care laminates and recreational laminates. Multiple inventions range from formulations, structure/ performance and process innovations.\nRecent developments and inventions have combined breathability with stretch (uniaxial and biaxial) with three filed applications (1 published) and new process developments leading to blown-based lamination technology for low cost, highlynfunctionalized laminates.\nDeveloped dispersible film laminates for flush disposal convenience. Two patents were granted.\nConceived multi-functional low cost nonwoven / film laminate fabrics which can be engaged with mechanical “hook” type fasteners. Bond pattern and three laminate patents were granted as a result of this work.\nDeveloped novel thermal bonding patterns supported new material attributes. A “Rib Knit” style bonding pattern was designed for thermal lamination which provides knit-like texture in commercial nonwoven fabrics. Two patents were granted covering these designs. An “S-Weave” nonwoven bond pattern was developed for unique strength and softness attributes. Two patents were also granted for this work.\nInvented advantaged hydroentangled absorbent composites containing superabsorbent with intake, storage and distribution behavior in Personal Care products. Three patents were obtained in this area. Additionally a novel high performance hydroentangled distribution material was invented, and several patents generated and granted.\nConceived of a drier diaper absorbent system involving several non woven layers (spunbonded liner/carded web liner/transfer layer). This concept was later commercialized on Huggies® Diapers. A patent was obtained covering this material.\nPolyethylene textured meltblown and co-form laminates were developed which led to an early baby wipe project utilizing this approach for a two-sided baby wipe product. A patent was obtained on this concept.\nChampioned an olefin polymer in-situ meltspray technology for fluff pad integrity for adult incontinence products. A patent was obtained and this process was commercialized.\nApertured and /or three dimensional nonwovens for multiple new product applications were developed. A patent application was granted in 1988. Commercial use of this material and process is still under evaluation for wipes and washcloths.\nInitiated the first commercial superabsorbent application in a commercial nonwoven process in Kimberly-Clark. Coordinated OSHA safe handling of airborne SAM in the mill environment.\nConducted early rheological screening of 80+polyolefin polymers which led to the first polyethylene meltblown produced in KC.\nKimberly-Clark Distinguished Performance Award.\nPatents and Trade Secrets\n- US Patents Issued: 41\n- US Applications/Pending: 13\n- Disclosures submitted: 113\n|US5192606A||Absorbent article having a liner which exhibits improved softness and dryness|\n|US6369292B1||Absorbent articles having reduced outer cover dampness|\n|EP0957873B1||ABSORBENT ARTICLES HAVING REDUCED OUTER COVER DAMPNESS|\n|US6583331B1||Absorbent product with breathable dampness inhibitor|\n|EP1191914A1||ABSORBENT PRODUCT WITH BREATHABLE DAMPNESS INHIBITOR|\n|US6177607B1||Absorbent product with nonwoven dampness inhibitor|\n|EP1191913A1||ABSORBENT PRODUCT WITH NONWOVEN DAMPNESSINHIBITOR|\n|US6589892B1||Bicomponent nonwoven webs containing adhesive and a third component|\n|US5997981A||Breathable barrier composite useful as an ideal loop fastener component|\n|EP1023171A1||BREATHABLE BARRIER COMPOSITE|\n|US6600086B1||Breathable diaper outer cover with foam dampness inhibitor|\n|US6663611B2||Breathable diaper with low to moderately breathable inner laminate and more breathable outer cover|\n|EP1231880A1||BREATHABLE DIAPER WITH LOW TO MODERATELY BREATHABLE INNER LAMINATE ANDMORE BREATHABLE OUTER COVER|\n|US6045900A||Breathable filled film laminate|\n|EP1023172A1||BREATHABLE FILLED FILM LAMINATE|\n|US20040122388A1||Breathable multilayer films for use in absorbent articles|\n|US6682803B2||Breathable multilayer films with breakable skin layers|\n|EP1218181A1||BREATHABLE MULTILAYER FILMS WITH BREAKABLE SKIN LAYERS|\n|US5855999A||Breathable, cloth-like film/nonwoven composite|\n|US5695868A||Breathable, cloth-like film/nonwoven composite|\n|EP0734321B1||BREATHABLE, CLOTH-LIKE FILM/NONWOVEN COMPOSITE|\n|US6037281A||Cloth-like, liquid-impervious, breathable composite barrier fabric|\n|US5707707A||Compressively resilient loop structure for hook and loop fastener systems|\n|EP0735830B1||COMPRESSIVELY RESILIENT LOOP STRUCTURE FOR HOOK AND LOOP FASTENER SYSTEMS|\n|EP1531980A1||DEVICE AND PROCESS FOR TREATING FLEXIBLE WEB BY STRETCHING BETWEENINTERMESHING FORMING SURFACES|\n|US6096668A||Elastic film laminates|\n|EP1015241B1||ELASTIC FILM LAMINATES|\n|US6111163A||Elastomeric film and method for making the same|\n|US5843057A||Film-nonwoven laminate containing an adhesively-reinforced stretch- thinned film|\n|EP0912788B1||FILM-NONWOVEN LAMINATE CONTAINING AN ADHESIVELY-REINFORCED STRECTCH-THINNED FILM|\n|US6783826B2||Flushable commode liner|\n|EP1458269A1||FLUSHABLE COMMODE LINER|\n|US5137600A||Hydraulically needled nonwoven pulp fiber web|\n|EP0483816B1||Hydraulically needled non woven pulp fiber web, method of making same and use of same|\n|EP1117533A1||LAMINATE HAVING BARRIER PROPERTIES|\n|US6238767B1||Laminate having improved barrier properties|\n|EP1023173A1||LAMINATE USEFUL AS A LOOP FASTENER COMPONENT|\n|US6713140B2||Latently dispersible barrier composite material|\n|US20030118850A1||Latently dispersible barrier composite material|\n|US5801107A||Liquid transport material|\n|EP0701637B1||LIQUID TRANSPORT MATERIAL|\n|US6653523B1||Low gauge films and film/nonwoven laminates|\n|EP0799128B1||LOW GAUGE FILMS AND FILM/NONWOVEN LAMINATES|\n|US6309736B1||Low gauge films and film/nonwoven laminates|\n|US6075179A||Low gauge films and film/nonwoven laminates|\n|US5837352A||Mechanically compatibilized film/nonwoven laminate|\n|EP0799131B1||MECHANICALLY COMPATIBILIZED FILM/NONWOVEN LAMINATES|\n|US20050043460A1||Microporous breathable elastic films, methods of making same, and limited use ordisposable product applications|\n|US6015764A||Microporous elastomeric film/nonwoven breathable laminate and method for making the same|\n|US5955187A||Microporous film with liquid triggered barrier feature|\n|US6277479B1||Microporous films having zoned breathability|\n|EP1042114B1||MICROPOROUS FILMS HAVING ZONED BREATHABILITY|\n|US5964742A||Nonwoven bonding patterns producing fabrics with improved strength and abrasion resistance|\n|EP1023477A1||NONWOVEN BONDING PATTERNS PRODUCING FABRICS WITH IMPROVED STRENGTH ANDABRASION RESISTANCE|\n|US5336552A||Nonwoven fabric made with multicomponent polymeric strands including a blend ofpolyolefin and ethylene alkyl acrylate copolymer|\n|EP0586936B1||Nonwoven fabric made with multicomponent polymeric strands including a blend ofpolyolefin and ethylene alkyl acrylate copolymer|\n|US6719742B1||Pattern embossed multilayer microporous films|\n|US5328759A||Process for making a hydraulically needled superabsorbent composite material and article thereof|\n|EP0540041B1||Process for making a hydraulically needled superabsorbent composite material|\n|US5620779A||Ribbed clothlike nonwoven fabric|\n|EP0736114B1||RIBBED CLOTHLIKE NONWOVEN FABRIC AND PROCESS FOR MAKING SAME|\n|US6096014A||Stable and breathable films of improved toughness and method of making the same|\n|EP0948558B1||STABLE AND BREATHABLE FILMS OF IMPROVED TOUGHNESS AND METHOD OF MAKING THE SAME|\n|US6589638B1||Stretch-pillowed bulked laminate useful as an ideal loop fastener component|\n|US5882769A||Stretch-pillowed, bulked laminate|\n|EP0604731B1||Stretch-pillowed, bulked laminate|\n|US5906879A||Ultra resilient three-dimensional nonwoven fiber material and process for producing the same|\n|EP0977914B1||ULTRA RESILIENT THREE-DIMENSIONAL NONWOVEN FIBER MATERIAL|"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"short"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:ee8f6063-3d8d-4e50-81b3-0c4cebc2eda0>"],"error":null}
{"question":"What role do temperature differences play in both theoretical thermodynamic cycles and real-world power plant efficiency?","answer":"In theoretical thermodynamics, according to Carnot's principle, the efficiency of a cycle depends solely on the temperature difference between hot and cold reservoirs. This is demonstrated in the Carnot cycle formula where efficiency equals 1 - TC/TH (cold temperature / hot temperature). In real-world applications, this principle is evident in the Stuttgart-Gaisburg power plant, which achieves high efficiency by utilizing temperature differences to generate both electricity and heat, with the ability to store excess heat when demand is low. The plant's 90% overall efficiency rate shows how modern facilities can approach theoretical limits by optimizing temperature differential use through combined heat and power systems.","context":["More energy, higher flexibility, lower emissions for new power plant Stuttgart-Gaisburg\nIt’s cold in the south of Germany. Like every year in winter, this means it’s time for the combined heat and power (CHP) plant in Gaisburg to deliver heat to homes and businesses in and around Stuttgart. Unlike every year for the past 60 years, that heat will no longer be generated from coal. After the old plant was switched off at the end of the past winter season, it is now being replaced by a new gas-powered thermal power station that will save up to 60,000 metric tons of CO2 per year.\nThe emissions aren’t the only thing being reduced – the new chimneys are half the height of the old ones, and the whole plant is smaller. At the same time, the efficiency and output are increased. The overall project consists of gas-fired boilers that produce up to 175 MW of thermal energy, as well as a district heating storage. In addition, MAN ES delivered three 20V35/44G gas engines that supply 31.2 MW of electrical power for the local grid and up to 30 MW of district heating simultaneously.\nThe main advantage of the whole system is flexibility. While the gas boilers purely produce heat and are primarily designed to cover the peaks in demand over winter, the gas engines will run if the electricity price is sufficient to provide both electricity and heat. By combining the plant with a district heating accumulator, EnBW can fully utilize the flexibility offered by the engines and react to price signals. When demand for heat is low, the waste heat from the engines can be stored.\n“This combined heat and power plant is a small, fast, efficient energy system that is perfectly tailored to local requirements. We can turn our engines off in less than three minutes, and ramp them back up to full load in three minutes. Other power plants of this size have much longer startup times,” explains Martin Domagk, who is heading the project at MAN Energy Solutions. Thanks to an overall efficiency rate of up to 90%, the plant also offers a particularly good fuel conversion. MAN remains on the project even after the plant’s successful commissioning in December 2018. The company’s global after-sales brand MAN PrimeServ will handle the engine’s service and maintenance for a further ten years.\nInside EnBW’s combined heat and power plant Stuttgart-Gaisburg:\nEnergy transition in Germany and beyond\nReplacing the existing coal plant with a modern, flexible and highly efficient gas-powered plant is part of a greater plan for a sustainable energy transition at EnBW (Energie Baden-Württemberg), who commissioned the new plant design. “In the context of the energy transition, we see facilities like this power station as a blueprint for further fuel-switch projects,” says Jens Rathert, project manager at EnBW. The partly state-owned energy supplier’s plans align perfectly with Germany’s climate goals as a whole. According to a recent study, replacing coal with gas plants across Germany could save around 70 million metric tons of CO2 annually, amounting to 40% of the reduction target set by the country’s long-term climate strategy.\nMartin Domagk and Jens Rathert commissioning the new gas engines in Stuttgart-Gaisburg\nProject head Martin Domagk agrees: “Our technology is making a major contribution to the reduction of pollutant emissions and increasing the efficiency of heat generation, as well as enabling flexibility and ensuring high levels of supply security. Completing this project has a signal effect on the German and European markets for decentralized heat generation.“\nLong-term goal: zero emissions\nIn the long run, projects like the one in Gaisburg provide the foundations that an emission-free future can be built on, as Matthias Zelinger, energy policy expert at the German Mechanical Engineering Industry Association, explains: “We have\na vision of how to make gas power plants completely emission-free. Twenty to twenty-five years from now we will certainly have the option to run them on a renewable gas, like hydrogen or synthetic methane produced from renewable electricity. Then\ngas-powered plants, which are still fossil-fueled today, will become completely powered by renewable energies. With this outlook, a conversion is not only an immediate investment in energy decarbonization, but also a first step towards future carbon-neutrality.”\nInspecting the delivered machinery at the combined heat and power plant\nThis vision can only become a reality when climate-neutral fuels become readily available – a scenario that, in Zelinger’s opinion, needs to be expedited. “Germany is a global leader in Power-to-X, a technology which can convert renewable energies into synthetic fuels. I believe that the technology is socially and politically necessary for future energy solutions. But at the moment, we are burdening such options and destroying the business case for an investment. Our energy system is diverse, so we need smart solutions and we need a political framework that encourages research and the implementation of the most promising results.“\nZelinger considers German technology companies to be perfectly positioned to export climate-friendly energy solutions. “The energy transition is already happening here, so German suppliers have been forced to meet its challenges. I think we are currently ahead of the rest of the world, and now we have to hold that position by looking towards future fuels.“\nDue to the principle of combined heat and power, our system achieves a very high level of fuel efficiency – up to 90%.","Carnot Cycle – Carnot Heat Engine\nThe second law of thermodynamics places constraints upon the direction of heat transfer and sets an upper limit to the efficiency of conversion of heat to work in heat engines. So the second law is directly relevant for many important practical problems.\nIn 1824, a French engineer and physicist, Nicolas Léonard Sadi Carnot advanced the study of the second law by forming a principle (also called Carnot’s rule) that specifies limits on the maximum efficiency any heat engine can obtain. In short, this principle states that the efficiency of a thermodynamic cycle depends solely on the difference between the hot and cold temperature reservoirs.\nCarnot’s principle states:\n- No engine can be more efficient than a reversible engine (a Carnot heat engine) operating between the same high temperature and low temperature reservoirs.\n- The efficiencies of all reversible engines (Carnot heat engines) operating between the same constant temperature reservoirs are the same, regardless of the working substance employed or the operation details.\nThe cycle of this engine is called the Carnot cycle. A system undergoing a Carnot cycle is called a Carnot heat engine. It is not an actual thermodynamic cycle but is a theoretical construct and cannot be built in practice. All real thermodynamic processes are somehow irreversible. They are not done infinitely slowly and infinitesimally small steps in temperature are also a theoretical fiction. Therefore, heat engines must have lower efficiencies than limits on their efficiency due to the inherent irreversibility of the heat engine cycle they use.\nCarnot Cycle – Processes\n- isentropic compression – The gas is compressed adiabatically from state 1 to state 2, where the temperature is TH. The surroundings do work on the gas, increasing its internal energy and compressing it. On the other hand the entropy remains unchanged.\n- Isothermal expansion – The system is placed in contact with the reservoir at TH. The gas expands isothermally while receiving energy QH from the hot reservoir by heat transfer. The temperature of the gas does not change during the process. The gas does work on the surroundings. The total entropy change is given by: ∆S = S1 – S4 = QH/TH\n- isentropic expansion – The gas expands adiabatically from state 3 to state 4, where the temperature is TC. The gas does work on the surroundings and loses an amount of internal energy equal to the work that leaves the system. Again the entropy remains unchanged.\n- isothermal compression – The system is placed in contact with the reservoir at TC. The gas compresses isothermally to its initial state while it discharges energy QC to the cold reservoir by heat transfer. In this process the surroundings do work on the gas. The total entropy change is given by: ∆S = S3 – S2 = QC/TC\nAn isentropic process is a thermodynamic process, in which the entropy of the fluid or gas remains constant. It means the isentropic process is a special case of an adiabatic process in which there is no transfer of heat or matter. It is a reversible adiabatic process. The assumption of no heat transfer is very important, since we can use the adiabatic approximation only in very rapid processes.\nIsentropic Process and the First Law\nFor a closed system, we can write the first law of thermodynamics in terms of enthalpy:\ndH = dQ + Vdp\ndH = TdS + Vdp\nIsentropic process (dQ = 0):\ndH = Vdp → W = H2 – H1 → H2 – H1 = Cp (T2 – T1) (for ideal gas)\nIsentropic Process of the Ideal Gas\nThe isentropic process (a special case of adiabatic process) can be expressed with the ideal gas law as:\npVκ = constant\np1V1κ = p2V2κ\nin which κ = cp/cv is the ratio of the specific heats (or heat capacities) for the gas. One for constant pressure (cp) and one for constant volume (cv). Note that, this ratio κ = cp/cv is a factor in determining the speed of sound in a gas and other adiabatic processes.\nAn isothermal process is a thermodynamic process, in which the temperature of the system remains constant (T = const). The heat transfer into or out of the system typically must happen at such a slow rate in order to continually adjust to the temperature of the reservoir through heat exchange. In each of these states the thermal equilibrium is maintained.\nIsothermal Process and the First Law\nThe classical form of the first law of thermodynamics is the following equation:\ndU = dQ – dW\nIn this equation dW is equal to dW = pdV and is known as the boundary work.\nIn isothermal process and the ideal gas, all heat added to the system will be used to do work:\nIsothermal process (dU = 0):\ndU = 0 = Q – W → W = Q (for ideal gas)\nIsothermal Process of the Ideal Gas\nThe isothermal process can be expressed with the ideal gas law as:\npV = constant\np1V1 = p2V2\nOn a p-V diagram, the process occurs along a line (called an isotherm) that has the equation p = constant / V.\nSee also: Boyle-Mariotte Law\nCarnot Cycle – pV, Ts diagram\nThe Carnot cycle is often plotted on a pressure- volume diagram (pV diagram) and on a temperature-entropy diagram (Ts diagram).\nWhen plotted on a pressure-volume diagram, the isothermal processes follow the isotherm lines for the gas, adiabatic processes move between isotherms and the area bounded by the complete cycle path represents the total work that can be done during one cycle.\nThe temperature-entropy diagram (Ts diagram) in which the thermodynamic state is specified by a point on a graph with specific entropy (s) as the horizontal axis and absolute temperature (T) as the vertical axis, is the best diagram to describe behaviour of a Carnot cycle.\nIt is a useful and common tool, particularly because it helps to visualize the heat transfer during a process. For reversible (ideal) processes, the area under the T-s curve of a process is the heat transferred to the system during that process.\nCarnot Cycle Efficiency\nSince energy is conserved according to the first law of thermodynamics and energy cannot be be converted to work completely, the heat input, QH, must equal the work done, W, plus the heat that must be dissipated as waste heat QC into the environment. Therefore we can rewrite the formula for thermal efficiency as:\nSince QC = ∆S.TC and QH = ∆S.TH, the formula for this maximum efficiency is:\n- is the efficiency of Carnot cycle, i.e. it is the ratio = W/QH of the work done by the engine to the heat energy entering the system from the hot reservoir.\n- TC is the absolute temperature (Kelvins) of the cold reservoir,\n- TH is the absolute temperature (Kelvins) of the hot reservoir.\nSee also : Causes of Inefficiencies\nExample: Carnot efficiency for coal-fired power plant\nIn a modern coal-fired power plant, the temperature of high pressure steam (Thot) would be about 400°C (673K) and Tcold, the cooling tower water temperature, would be about 20°C (293K). For this type of power plant the maximum (ideal) efficiency will be:\nηth= 1 – Tcold/Thot = 1 – 293/673 = 56%\nIt must be added, this is an idealized efficiency. The Carnot efficiency is valid for reversible processes. These processes cannot be achieved in real cycles of power plants. The Carnot efficiency dictates that higher efficiencies can be attained by increasing the temperature of the steam. This feature is valid also for real thermodynamic cycles. But this requires an increase in pressures inside boilers or steam generators. However, metallurgical considerations place an upper limits on such pressures. Sub-critical fossil fuel power plants, that are operated under critical pressure (i.e. lower than 22.1 MPa), can achieve 36–40% efficiency. Supercritical designs, that are operated at supercritical pressure (i.e. greater than 22.1 MPa), have efficiencies around 43%. Most efficient and also very complex coal-fired power plants that are operated at “ultra critical” pressures (i.e. around 30 MPa) and use multiple stage reheat reach about 48% efficiency.\nSee also: Supercritical Reactor"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"premise_categorization","category_name":"without premise"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:863b31c7-07fd-40c2-8051-273eb6286d58>","<urn:uuid:f22c4ee2-92f0-4cc1-8e71-3d2f4366a8a2>"],"error":null}