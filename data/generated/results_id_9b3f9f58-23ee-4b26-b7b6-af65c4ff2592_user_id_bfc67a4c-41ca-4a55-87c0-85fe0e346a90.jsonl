{"question":"What are the storage requirements for medical trays and how do they help prevent infections?","answer":"Custom procedure trays require specific storage conditions to maintain sterility and prevent infections. They must be stored in designated areas following manufacturer instructions, with high-value products in security zones and proper FEFO policy implementation. The storage area should be designed to prevent damage and allow stock rotation, with products kept above floor level away from water and sunlight. Proper storage helps prevent nosocomial infections, which cause approximately 1.7 million hospital-associated infections and 100,000 deaths annually in the United States. Unlike reusable equipment that relies on autoclave sterilization (which isn't always perfect), properly stored disposable medical supplies are individually wrapped and sterile, reducing the risk of cross-contamination between patients.","context":["Three Levels for Storing Custom Trays\nIt is always important to ensure that the custom procedure trays are stored in the right conditions so that they can always be available when required for use. As a general rule, it will involve maintaining the sterility of the products, proper arrangement of the products and keeping track of the product.\nCustom procedure trays are mostly stored at the point of use, may it be a clinic or operating theater. However, regardless of the location, there should be a designated storage area which is always monitored to ensure that all custom procedure trays are maintained properly. There are three levels for storing this equipment.\nReceiving and arranging them\nThis is the first and the most important task. Once the custom trays have arrived, the person in charge should ensure that;\n- There is sufficient storage\n- The custom procedure trays are not damaged in any form or manner\n- the storage area is clean.\nThere are several set standards for storing medical equipment that is generally followed, they are;\n- following the manufacturer’s instruction on the direction of stacking and storage conditions\n- storing high-value products in appropriate security zones.\n- Following FEFO policy at all times.\n- Ensuring that any identification labels are visible.\nKeeping track of products\nEach store should have a record of all stock items they have in their possession. Each product should have sufficient details describing its type, quantity and expiry dates (where applicable). The list should be kept updated at all time and should reflect the correct details.\nThe minimum information that should be on the records includes;\n- Product name and description\n- item codes\n- expiry dates\n- transactions reference\n- special storage conditions\nMonitoring product quality and safety\nThe custom procedure trays should always be cleared by the appropriate technical person that is mandated to regulate such equipment. The manufacturer is also required to ensure that the recommended safety and quality assurance tests are carried out for all the custom procedure trays. They should also clearly state the correct use of each tool. The medical practitioners should also ensure that the product is sterile by checking that;\n- the packaging is intact\n- the product is within the expiry date\n- the product has a sterilization indicator that confirms that it has undergone the right sterilization process.\nAs a rule of thumbs, custom procedure trays that have stayed for long without being used should be considered a product of overstocking and a proper assessment undertaken on its future. Other ways in which the product quality may be maintained include;\n- Designing the storage area in such a way that the packs are not damaged and still allow for flexible rotation of stock\n- Shelving should allow for easy access when cleaning. It should also be airy.\n- The products being stored should be at least above the floor level away from water and direct sunlight.\nLack of proper management of this areas may have an adverse effect on the integrity of the custom procedure trays and may render them unsuitable for use.","As infection control efforts in hospitals and other healthcare settings intensify, medical providers are turning to disposable medical products as a way of ensuring staff and patient safety. Nosocomial infections are infections caused by treatment in a hospital or other healthcare unit. Hospital-associated infections are a serious problem for patients and the healthcare industry. The Centers for Disease Control and Prevention (CDC) approximate that there are 1.7 million hospital-associated infections each year that cause nearly 100,000 deaths in the United States. Hospitals, clinics, and doctors’ offices are looking for ways to reduce these tragic statistics, and disposable medical supplies are an essential part of increasing patient safety.\nBefore disposable medical products were widely available, healthcare providers relied on autoclaves to sterilize all equipment. Autoclaves use high heat and intense pressure to sterilize items. The autoclave has been used for over 100 years and is generally very effective. Autoclaves are essential pieces of equipment in hospitals and many other medical settings.\nUnfortunately, the autoclaving process is not always perfect and medical products may not, in fact, be sterile after autoclaving. Some plastics cannot be autoclaved because they would melt. In addition, while autoclave sterilization kills viruses, bacteria, and fungi, it is not effective against prions that cause diseases such as Creutzfeldt-Jakob disease and Mad Cow Disease. Even when autoclave sterilization works well, it takes time and effort to run the autoclave. Autoclaves are still used in hospitals, labs, and even tattoo parlors to sterilize equipment that is not disposable. However, supplies that are available in a disposable form are certainly safer and easier to use.\nWhen items such as face masks, syringes, or scalpels are reused, they can spread infections from one patient to another. Many patients in hospitals have weakened immune systems, so the risk from cross-contamination is especially great. Hospital staff members are also at risk of being exposed to infections when they handle or clean used medical products.\nDisposable medical supplies are intended for single patient use, so they do not pose the risk of spreading infection. For example, if a patient needs an IV to replenish lost fluids, the procedure can be safely done using only disposable medical products. All of the products used, from the tourniquet, alcohol swab, and gauze to the stylet, catheter, tubing, and IV bag, are all disposable medical products that pose no risk to the patient or healthcare provider of transmitting an infection from a different patient.\nSterile Disposable Medical Products\nMany disposable medical products are sterile and individually wrapped to provide maximum patient protection. These disposable medical supplies are especially important for patients with compromised immune systems. In these cases, when infection control is a top priority, health care providers should use sterile disposable medical products as much as possible. For instance, if a nurse needs to take a patient’s temperature, a sealed disposable thermometer is a better option than an electric thermometer that has been used with other patients. Catheters, bandages, applicators, tubing, syringes, and more are all available as sterile disposable medical supplies.\nPersonal Protective Equipment\nHealth care providers must take great precautions to protect their health. They may be exposed to many pathogens every day, and proper protective gear is the best way for them to stay safe. Disposable medical supplies are essential to keeping doctors, nurses, and EMTs safe from infections. One of the most important safety products is the disposable glove. Health care providers should always wear gloves and change them frequently. Disposable gloves are available in latex or non-latex and in a range of colors and sizes. Other disposable medical products that are vital for personal protection include face masks, protective isolation gowns, protective shoe covers, and bouffant caps."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:8dfc4195-a6ef-45ff-9d29-50388485c453>","<urn:uuid:b53fab39-b01e-4900-9264-1f2f1d87caaa>"],"error":null}
{"question":"In my history report about World War II Pacific Theater, what was different between early 1942 and late 1944 regarding Japanese carrier-based air power?","answer":"There was a dramatic decline in Japanese carrier-based air power between early 1942 and late 1944. Early in 1942, Japan had sufficient naval air power to launch major operations, as demonstrated at the Battle of Midway. However, by late 1944, the Japanese carrier force had been severely weakened, particularly after the Battle of Philippine Sea where they lost approximately 400 carrier-based aircraft along with most of their pilots. This culminated in their remaining carriers being reduced to serving as mere decoys during the Battle of Leyte Gulf in October 1944.","context":["USS GEORGE H.W. BUSH (CVN 77) Hosts 68th Anniversary of the Battle of Midway Commemoration\nFrom USS GEORGE H.W. BUSH Public Affairs\nNORFOLK, Va. (NNS) -- USS GEORGE H.W. BUSH (CVN 77) hosted two World War II veterans and hundreds of Sailors from across the Hampton Roads area, as they observed the 68th anniversary of the historic Battle of Midway on the flight deck of the aircraft carrier June 4.\nThe commemoration, hosted by Commander, United States Fleet Forces, Adm. J.C. Harvey Jr., was attended by Battle of Midway veterans William Eckel and Howard Snell. Both men served aboard ships during the battle, and traveled to Norfolk to attend the ceremony.\nThe Battle of Midway, which is often referred to as the turning point of World War II, took place June 4-7, 1942, when the Japanese sent the majority of their naval force to capture Midway Island, which was being used by U.S. forces as an airfield. The battle was primarily fought by aircraft launched from aircraft carriers. By the battle's end, the Japanese had to retreat after losing vital air superiority. The U.S. lost the carrier Yorktown while four Japanese fleet carriers were lost along with their crew.\nThe commemoration featured musical selections played by Fleet Forces Band, a moment of silence, remarks from Harvey and Kilcline, and an invocation and benediction led by Cmdr. Cameron Fish, command chaplain. A vintage TBM-1 Avenger conducted a fly over during the ceremony. The Avenger got its combat debut during the decisive battle and is also the same model of aircraft flown by the aircraft carrier's namesake George H.W. Bush.\nThe ceremony also featured a wreath-laying presentation, as Harvey was joined by Eckel and Snell to pay homage to the brave men who lost their lives during the battle.\n\"It was important to me that this event be by Sailors, about Sailors,\" Harvey said. The event that took place 68 years ago today was done by young American Sailors and pilots wearing dungarees and khakis. So to make that connection with them we are wearing our flight suits, our flight deck jerseys along with our Navy working uniforms, to pay honor to those young Sailors who stared everything in the face, when everything was on the table and rose to the occasion.\"\n\"We come together today to honor the brave men who fought to defend Midway, they were just ordinary Americans who responded to the call to serve their nation, but at Midway they were heroes,\" said guest speaker Commander, Naval Air Forces Pacific, Vice. Adm. Thomas J. Kilcline.\nFish, whose father served in the Navy during World War II, recalled the huge impact that one naval battle had on not just the Navy, but countless individuals as well.\n\"The battle changed the tide of the war,\" he said. \"I remember my father saying that things looked bleak. The success at Midway both halted the advance of the Japanese and lifted the spirit and morale of the whole nation. The Battle of Midway affects me very personally because it affected my father and mother. If it were not for that victory, I might not be here today,\" he said.\nKilcline said that our victory was not without a cost, we lost the aircraft carrier Yorktown along with 145 of our aircraft and 307 Americans lost their life in that battle while paying the ultimate cost for victory.\nFish also explained the significance of having two veterans on board for the commemoration.\n\"Having the veterans in attendance was absolutely wonderful,\" he said. \"It's an honor and a privilege and extremely humbling to have them. These men were there, during the climax of World War II, and now they're here today.\"\nHarvey discussed the importance of the commemoration being held on the Navy's newest aircraft carrier.\n\"It was important to me that we commemorate this day from the modern version of the Hornet, Yorktown and enterprise, which struck the decisive blow to the Japanese fleet and tie together what those Sailors did then to what our Sailors are doing now,\" Harvey said.","Air University History Office\n/ Published July 17, 2019\nU.S. Navy Task Force 58 at anchor at Majuro, Marshall Islands, on April 25, 1944, prior to the battle of the Philippine Sea. It consisted of fifteen carriers, seven fast battleships 13 cruisers, 58 destroyers and about 900 aircraft at the time. (Courtesy U.S. Navy)\nPilots aboard the USS Monterey, built as a light carrier, keep busy while other pilots are attacking Japanese land-based aircraft, June 1944. During World War II, the number of aircraft aboard a light carrier was only one-half to two-thirds that of a full-sized fleet carrier. Similar to the smaller escort carrier, the light carrier had the speed for deployment with the larger fleet carriers. (Courtesy U.S. Navy)\nAn SBD Douglas dive bomber flying over Task Force 58 in the Marshall Islands on the way to attack targets in the Marianas on June 15, 1944, the day U.S. forces landed on Saipan. It proved to be an excellent Navy scout plane and dive bomber and sank more enemy shipping in the Pacific War than any other Allied aircraft. (Courtesy U.S. Navy)\nThe Navy F-6F Hellcat, such as these assigned to the second USS Lexington, in summer 1944, conducted many of the attacks on Japanese aircraft during the battle of the Philippine Sea. It served as the Navy's dominant fighter in the second half of the Pacific War. (Courtesy U.S. Navy)\nThe Japanese carrier Zuikaku (center), which took part in the Japanese attack on Pearl Harbor, Dec. 7, 1941, and two destroyers under attack by Navy carrier aircraft, June 20, 1944. American aircraft sank her in the battle of Cape Engano on Oct. 25, 1944, during the battle of Leyte Gulf. (Courtesy U.S. Navy)\nThe Japanese fleet under attack during the battle of the Philippine Sea, June 19-20, 1944. The Japanese navy lost three carriers, two oilers and approximately 400 carrier- and 200 land-based aircraft. (Courtesy U.S. Navy)\nAlthough the war in the Pacific and East Asia had gone badly for the Allies during the six months after the Japanese attack on Allied possessions in early December 1941, the tide of war had turned in favor of the Allies over the first six months of 1942.\nFollowing the American naval victories at Coral Sea (May 4-8) and Midway (June 4-7), both achieved by U.S. naval aviation, the Americans invaded Guadalcanal in the Solomon Islands and launched attacks along the northern coast of New Guinea.\nThroughout the long Allied counterattack across the Pacific, the Americans used Navy task forces, built around aircraft carriers, from the central Pacific across the western Pacific in amphibious invasions of selected Japanese-held islands. Senior Allied leaders knew that they did not have the manpower or war materiel to invade every Japanese-held island, so they adopted an “island-hopping” strategy—attacking selected islands and bypassing major strongholds.\nThis island-hopping strategy had three major goals: dislodge the enemy forces in some places, neutralize them in more heavily defended areas and secure airfields and supply bases as the launching points for future attacks on less defended islands. The Japanese defenders, left on these isolated, bypassed islands, would eventually weaken from starvation and disease.\nDuring these naval operations, aircraft carriers provided the major offensive firepower for the American invasion fleets as they moved westward. Smaller escort, or “jeep,” carriers provided additional aircraft for close-air support for the invasion forces and replacement crews and aircraft for the larger fleet carriers. Battleships and heavy cruisers, the backbone of the world’s major navies before 1941, served as flagships, command and control ships or gunnery support platforms.\nIn August 1942, the United States launched its first major amphibious landing of World War II against Guadalcanal. The objective was the seizure of the airfield on the island from which Japanese aircraft could have launched attacks against Allied supply routes to Australia and New Zealand. After a ferocious six-month struggle, marked by seven major naval battles, three major land battles and almost continuous air combat on, around and above the Solomon Islands, U.S. forces finally controlled Guadalcanal.\nWith Guadalcanal in American hands, Allied forces closed in on the Japanese stronghold at Rabaul on the northern tip of New Britain. However, the Americans decided to bypass this heavily defended anchorage and, instead, neutralized it through systematic air and naval attacks.\nAfter Guadalcanal, the Allies pursued a two-pronged offensive, using soldiers, airman, sailors, and Marines in a series of joint and combined operations. Allied forces, commanded by Army Gen. Douglas MacArthur, in a succession of landings along the northern coast of New Guinea, controlled most the island by the end of 1943. By early 1944, most of the southwest Pacific was under Allied control, and MacArthur looked to returning to the Philippines soon, which he had left in late December 1941.\nMeanwhile, Navy Adm. Chester Nimitz oversaw a series of amphibious operations, featuring fast carrier task forces, across the central Pacific westward toward the Marianas, starting with the invasion of the Gilbert Islands in November 1943. After costly campaigns to take Tarawa in the Gilberts and Peleliu in the Palau Islands, Nimitz’ forces moved westward to seize Saipan and Guam in the Marianas during June-August 1944.\nAs the Americans moved relentlessly westward through the central Pacific, Japanese military leaders concluded that they needed a major naval victory over the Navy to redress the balance of power in the western Pacific in their favor. On June 19-20, a Japanese fleet, consisting of nine carriers and five battleships under the command of Adm. Jisaburo Ozawa engaged the American fleet in the Philippine Sea.\nOzawa also expected to have several hundred additional land-based aircraft in the Marianas to support his strike force. However, unknown to him, American air attacks on June 11-12 had greatly reduced the available land-based Japanese aircraft.\nTo oppose the oncoming Japanese fleet, Vice Adm. Marc Mitscher stationed his Task Force 58 (fifteen carriers with 900 total aircraft and seven fast battleships) to the west of Saipan. On June 19, the largest naval air battle in history began around 8 a.m. and lasted, off and on, to sundown. By the end of June 20, the battle had cost the Navy 123 aircraft, including 80 lost after they ran out of fuel returning to their carriers in the dark from a late afternoon strike. Japanese aircraft scored a handful of hits against the American warships, but sank no ships.\nHowever, the Japanese lost three carriers, two oilers and approximately 400 carrier- and 200 land-based aircraft with most of their pilots. The disproportional Japanese aircraft losses from the American pilots during the battle led to the battle earning the nickname “The Great Marianas Turkey Shoot.” It crippled the Japanese air arm, still hurting from its losses at the battle of Midway two years earlier, and their remaining carriers with only a “handful” of aircraft becoming decoys during the battle of Leyte Gulf, Oct. 23-26, 1944."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:ea8f8a3f-607e-4502-b8d3-d0499f0b504c>","<urn:uuid:0d14b6a4-4f5f-490f-9606-d3b3fcb06cd5>"],"error":null}
{"question":"I work with international beef trade. Could you explain how the yield grading differs between the Japanese and American beef grading standards, particularly in terms of their grading categories and measurement approaches?","answer":"The Japanese and American systems have different approaches to yield grading. The Japanese system uses three yield grades (A, B, and C) based on estimated cutability percentage, measured between the sixth and seventh rib section. Grade A requires 72% and above, Grade B requires 69% and above, and Grade C is under 69%. The yield grading is objective and uses four specific carcass measurements. In contrast, the USDA system uses yield grades to determine the amount of usable lean meat on a carcass, with grades ranging from utility, cutter, and canner (used for processed foods) to the higher grades of select, choice, and prime. The USDA system also considers factors like marbling and fat content in determining overall grade.","context":["1. Lloyd S. S., Steele E. J., Dawkins R. L. Analysis of haplotype sequences. In: Kulski J. K., editor. Next Generation Sequencing-Advances, Applications and Challenges. InTech; 2016. pp. 345–368. [Cross Ref]\n2. Dawkins R., Leelayuwat C., Gaudieri S., et al. Genomics of the major histocompatibility complex: haplotypes, duplication, retroviruses and disease. Immunological Reviews. 1999;167:275–304. doi: 10.1111/j.1600-065X.1999.tb01399.x.[PubMed][Cross Ref]\n3. Dawkins R. L. Adapting Genetics. Dallas, TX, USA: Near Urban Publishing; 2015. [Cross Ref]\n4. Awdeh Z. L., Raum D., Yunis E. J., Alper C. A. Extended HLA/complement allele haplotypes: evidence for T/t-like complex in man. Proceedings of the National Academy of Sciences of the United States of America. 1983;80(1):259–263. doi: 10.1073/pnas.80.1.259.[PMC free article][PubMed][Cross Ref]\n5. Yunis E. J., Larsen C. E., Fernandez-Vina M., et al. Inheritable variable sizes of DNA stretches in the human MHC: conserved extended haplotypes and their fragments or blocks. Tissue Antigens. 2003;62:1–20. doi: 10.1034/j.1399-0039.2003.00098.x.[PubMed][Cross Ref]\n6. Alper C. A., Marcus-Bagley D., Awdeh Z., et al. Prospective analysis suggests susceptibility genes for deficiency of IgA and several other immunoglobulins on the [HLA-B8, SC01, DR3] conserved extended haplotype. Tissue Antigens. 2000;56:207–216. doi: 10.1034/j.1399-0039.2000.560302.x.[PubMed][Cross Ref]\n7. Raj P., Rai E., Song R., et al. Regulatory polymorphisms modulate the expression of HLA class II molecules and promote autoimmunity. eLife. 2016;5, article e12089 doi: 10.7554/eLife.12089.[PMC free article][PubMed][Cross Ref]\n8. Steinberg K. M., Antonacci F., Sudmant P. H., et al. Structural diversity and African origin of the 17q21.31 inversion polymorphism. Nature Genetics. 2012;44:872–880. doi: 10.1038/ng.2335.[PMC free article][PubMed][Cross Ref]\n9. Alper C. A., Larsen C. E., Dubey D. P., Awdeh Z. L., Fici D. A., Yunis E. J. The haplotype structure of the human major histocompatibility complex. Human Immunology. 2006;67:73–84. doi: 10.1016/j.humimm.2005.11.006.[PubMed][Cross Ref]\n10. Johnson G. C., Esposito L., Barratt B. J., et al. Haplotype tagging for the identification of common disease genes. Nature Genetics. 2001;29:233–237. doi: 10.1038/ng1001-233.[PubMed][Cross Ref]\n11. Wilton A. N., Cobain T. J., Dawkins R. L. Family studies of IgA deficiency. Immunogenetics. 1985;21:333–342. doi: 10.1007/BF00430799.[PubMed][Cross Ref]\n12. Cobain T., French M., Christiansen F., Dawkins R. Association of IgA deficiency with HLA A28 and B14. Tissue Antigens. 1983;22:151–154. doi: 10.1111/j.1399-0039.1983.tb01181.x.[PubMed][Cross Ref]\n13. French M. A., Dawkins R. L. Central MHC genes, IgA deficiency and autoimmune disease. Immunology Today. 1990;11:271–274. doi: 10.1016/0167-5699(90)90110-U.[PubMed][Cross Ref]\n14. Sekine H., Ferreira R. C., Pan-hammarström Q., et al. Role for Msh5 in the regulation of Ig class switch recombination. Proceedings of the National Academy of Sciences of the United States of America. 2007;104:7193–7198. doi: 10.1073/pnas.0700815104.[PMC free article][PubMed][Cross Ref]\n15. Khatib H. Molecular and Quantitative Animal Genetics. 2015. Wiley-Blackwell. [Cross Ref]\n16. Simm G. Genetic Improvement in Cattle and Sheep. Tonbridge, UK: Farming Press - Miller Freeman UK Ltd; 1998.\n17. Hill W. G. Applications of population genetics to animal breeding, from wright, fisher and lush to genomic prediction. Genetics. 2014;196:1–16. doi: 10.1534/genetics.112.147850.[PMC free article][PubMed][Cross Ref]\n18. Fadista J., Thomsen B., Holm L.-E., Bendixen C. Copy number variation in the bovine genome. BMC Genomics. 2010;11:p. 284. doi: 10.1186/1471-2164-11-284.[PMC free article][PubMed][Cross Ref]\n19. Hou Y., Bickhart D. M., Hvinden M. L., et al. Fine mapping of copy number variations on two cattle genome assemblies using high density SNP array. BMC Genomics. 2012;13:p. 376. doi: 10.1186/1471-2164-13-376.[PMC free article][PubMed][Cross Ref]\n20. Hou Y., Liu G. E., Bickhart D. M., et al. Genomic characteristics of cattle copy number variations. BMC Genomics. 2011;12:p. 127. doi: 10.1186/1471-2164-12-127.[PMC free article][PubMed][Cross Ref]\n21. Bickhart D. M., Hou Y., Schroeder S. G., et al. Copy number variation of individual cattle genomes using next-generation sequencing. Genome Research. 2012;22:778–790. doi: 10.1101/gr.133967.111.[PMC free article][PubMed][Cross Ref]\n22. Fontanesi L., Beretti F., Martelli P. L., et al. A first comparative map of copy number variations in the sheep genome. Genomics. 2011;97:158–165. doi: 10.1016/j.ygeno.2010.11.005.[PubMed][Cross Ref]\n23. Liu J., Zhang L., Xu L., et al. Analysis of copy number variations in the sheep genome using 50K SNP BeadChip array. BMC Genomics. 2013;14:p. 229. doi: 10.1186/1471-2164-14-229.[PMC free article][PubMed][Cross Ref]\n24. Cameron P. U., Tabarias H. A., Pulendran B., Robinson W., Dawkins R. L. Conservation of the central MHC genome: PFGE mapping and RFLP analysis of complement, HSP70, and TNF genes in the goat. Immunogenetics. 1990;31:253–264. doi: 10.1007/BF00204897.[PubMed][Cross Ref]\n25. McLure C. A., Kesners P. W., Lester S., et al. Haplotyping of the canine MHC without the need for DLA typing. International Journal of Immunogenetics. 2005;32:407–411. doi: 10.1111/j.1744-313X.2005.00549.x.[PubMed][Cross Ref]\n26. Alvarez C. E., Akey J. M. Copy number variation in the domestic dog. Mammalian Genome. 2012;23:144–163. doi: 10.1007/s00335-011-9369-8.[PubMed][Cross Ref]\n27. Locke M. E. O., Milojevic M., Eitutis S. T., et al. Genomic copy number variation in Mus musculus. BMC Genomics. 2015;16:p. 497. doi: 10.1186/s12864-015-1713-z.[PMC free article][PubMed][Cross Ref]\n28. Wang J., Jiang J., Fu W., et al. A genome-wide detection of copy number variations using SNP genotyping arrays in swine. BMC Genomics. 2012;13:p. 273. doi: 10.1186/1471-2164-13-273.[PMC free article][PubMed][Cross Ref]\n29. Fadista J., Nygaard M., Holm L.-E., Thomsen B., Bendixen C. A snapshot of CNVs in the pig genome. PLoS One. 2008;3, article e3916 doi: 10.1371/journal.pone.0003916.[PMC free article][PubMed][Cross Ref]\n30. Shiina T., Inoko H., Kulski J. K. An update of the HLA genomic region, locus information and disease associations: 2004. Tissue Antigens. 2004;64:631–649. doi: 10.1111/j.1399-0039.2004.00327.x.\nIn order to provide consistency and palatability (tenderness, flavor, juiciness) at the consumer level a meat quality grade standard needs to be used. In the US there is a standardized USDA grading system ranging from the lowest grade; Canner through to the three(3) most common grades; Select, Choice and Prime being the highest quality grade (min. IMF% range of 8-11) and attributing each of those grades is a set of parameters include yield, marble score, ossification(age), etc.\nDue to the abnormally high level of marbling that exists in this beef breed, it often exceeds the top USDA grade (Prime) and therefore it is difficult to grade those “high end” carcasses.\nJMGA (Japanese Meat Grading Association) Beef Carcass Grading Standard has been developed to measure those carcasses that are yielding higher marble scores. In 2008 Japan raised the bar on their grading standard whereby the BMS (Beef Marble Score) grade range is 3-12 (eliminating 1 and 2) and now a BMS 3 requires a min. IMF% of 21. If the US is going to raise cattle for export to Japan or compete with Japanese imports, it's important to have a fundamental understanding of the Japanese meat grading system.\nJapanese carcasses are cut or ribbed between the sixth and seventh rib throughout Japan. There are three yield grades: A, B and C - classified by yield percentages estimated by an equation. There are five quality grades: 1, 2, 3, 4 and 5 - based on marbling, meat colour and texture, and fat colour and quality. Yield score is determined by an estimated cutability percentage that is calculated by an equation which includes four carcass measurements. The measurements are obtained at the sixth and seventh rib section. The yield grading is absolutely objective, delivering an estimated yield percentage as follows.\n- Grade A - 72% and above\n- Grade B - 69% and above\n- Grade C - under 69%\n- Quality grade\nThe meat quality scores are determined in terms of beef marbling, meat colour and brightness, firmness and texture of meat, colour, lustre and quality of fat. The relationship between beef marbling evaluation and classification of grade is as follows:\n- Poor — 1\n- Below Average — 2\n- Average — 3-4\n- Good — 5-7\n- Excellent — 8-12\nMeat colour is evaluated by the Beef Colour Standard prepared as seven continuous standards. The average colour range is from No. 1 to No. 6 and carcasses in this colour range can be graded in 'Grade 3 or upper grades'. Beef 'brightness' is also a factor in this evaluation. Firmness and texture of meat are evaluated by visual appraisal and also classified into five grades. The firmness measure ranges from very good to inferior and the texture of the meat is evaluated on a scale from very fine to coarse. The colour, lustre and quality of fat is evaluated objectively against the Beef Fat Standards prepared as seven continuous standards. The remaining two factors, lustre and quality, are evaluated simultaneously by visual appraisal.\nIn recent years the Japanese have been focusing on developing objective carcass measurement utilitising the latest digital camera technology and image analysis software (Beef Analyzer II) to calculate important traits like;\n•Rib Eye Area\n•Rib Eye Shape\n•Fineness/ Coarseness Index - Marbling\nThis technology is currently in use in the US (3 cameras) and Australia (1 camera) as a research tool to collate accurate carcass data for possible use in parameter estimation for genetic analysis(BLUP). This technology was recently presentation at the annual Wagyu Conference in Coeur d' Alene by Japanese Researcher and developer; Prof Keigo Kuchida from Obihiro University. Below is a image taken with the technology here in the US with the Rib Eye traced.\nFor more information about Japanese BMS Grading, please view the following documents.","Editor’s note: For the next several weeks, The Times will be running grilling and barbecuing stories in the Food section. These stories are a companion to the Patio Pitmasters series that is currently running in Sunday Lifestyle.\nMaking the Grade\nWhen you’re talking about steaks, the grade is dependent on the amount of marbling – or fat dispersed between the muscle fibers in the meat. The amount of marbling relates to how tender, juicy and flavorful a meat will be after grilling. Beef also has a yield grade for the amount of usable lean meat on a carcass.\nJim Drewenski, manager of Rob’s Meat Chop & Deli in Dyer, said steaks are categorized in three different grades – select, choice and prime. He said his store carries only the upper two-thirds, which would be choice and prime cuts.\nAs described on the USDA’s website, “Select beef is very uniform in quality and normally leaner than the higher grades. It is fairly tender, but, because it has less marbling, it may lack some of the juiciness and flavor of the higher grades. Only the tender cuts should be cooked with dry heat. Other cuts should be marinated before cooking or braised to obtain maximum tenderness and flavor.”\nThe next step up is “choice” beef, which is higher in quality, but has less marbling than prime cuts. “Choice roasts and steaks from the loin and rib will be very tender, juicy, and flavorful and are suited for dry-heat cooking,” as stated on usda.gov. “Many of the less tender cuts can also be cooked with dry heat if not overcooked. Such cuts will be most tender if braised, roasted or simmered with a small amount of liquid in a tightly covered pan.”\nPrime falls at the top and its abundant marbling and tenderness comes from young, well-fed beef cattle. This is the grade you will often find in fine dining establishments and hotels. The USDA advises that prime roasts and steaks are excellent for broiling, roasting or grilling.\nThere are also standard and commercial grades of beef that are sold as ungraded or store brand meat. Utility, cutter and canner meat grades are used to produce ground beef, hot dogs and other processed foods.\nChoosing Your Cut\nWithin each section of cow are different cuts that can be used for different purposes. Within the loin category are the cuts of steak that are some of the most popular meats to go on a grill.\n“For grilling, some of our most popular cuts are the boneless ribeye, filet mignon, New York strips and porterhouses,” said Drewenski. “A ribeye has the highest fat content, so the best flavor.”\nAnother cut Drewenski said is available in their store that is becoming more popular is the Teres Major. It comes from the chuck section of the cow from the blade of the shoulder.\nThe loin comes from the back part of the cow. Where the meat is situated on the cow plays a big part in how tender the meat will be. “All meat on cows is muscles. How often it is used determines how tender. The muscles not getting used every day on the back are much more tender,” said Drewenski.\nColton Scadden, butcher at Woodfire Meat Market and Deli in Valparaiso names porterhouse, ribeye and filet mignon as his favorite cuts of meat for grilling.\nDrewenski said his favorite grilling meat is a bone-in ribeye, but his next favorite is an outer skirt steak. The outer skirt steak offers more flavor than an inside skirt steak, he said. It’s a cut that is utilized frequently in South American cuisine. After being grilled and thinly sliced, it works well for fajitas or topped with chimichurri sauce. It can also be seasoned, grilled and chopped to make a filling for steak tacos. If you’ve ordered carne asada or arrachera in a Mexican restaurant, you’re getting skirt steak.\nExpert Grilling Tips\nBeyond picking your piece of meat, there are a lot of ways to make your steak even better.\nMarinades: “Beer, apple juice and pineapple juice all make good marinades as they help break down the meat and create a juicer steak,” said Scadden.\n“As far as marinades go, you can use anything that is high in acidity,” said Chris Traynor, executive chef at PL8 Restaurant, a Chinese and Japanese restaurant in suburban Barrington, Illinois. “ If you’re grilling a skirt steak, you can use lime juice or pineapple juice.”\nMeat Temperature: Rather than taking meats from the fridge and putting them directly on the grill, let them sit out for a bit. “Thicker steaks cook better at room temperature,” said Scadden.\nGrill Temperature: “Making sure the grill is at the right temperature is key,” said Traynor. “If it is too hot, you will burn the meat and if it’s not hot enough you won’t get a good sear.”\nSeasoning: There are a myriad of seasonings on the market and you can always make your own based on your taste preference, but Scadden noted that sometimes simple works best. “Keeping it simple with salt and pepper is always ok,” he said.\nMake it Juicy: Marinades help to moisten meat, but another way to increase the juiciness is simply by salting meat before you cook it. “Salting before cooking helps pull juices to the surface to cook back through,” said Scadden.\n“The best advice I could give a novice cook is to salt an hour before grilling,” said Traynor.\nReverse Sear: Both Drewenski and Scadden recommend a reverse sear. In this method, you cook the steak to your liking at a lower heat and then finish at a higher heat to get a nice char on the outside. Drewenski said it works good on a charcoal grill. Scadden said it’s a good method for thicker steaks.\nGallery: Regionites mask up amid coronavirus pandemic\nRoni and Griffin Gold\nJeannie Pritchard and Bob Frankovich\nDebbie Walton Sexton\nTroy and Jennifer McQuen\nChris and Teri Grotte\nSean and Kieran Harris\nOrville Redenbacher statue\nWith our weekly newsletter packed with the latest in everything food."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:68291e96-222a-40e1-8797-1762c354afa5>","<urn:uuid:3e97ede0-1847-4b16-b7a9-3bdd45273181>"],"error":null}
{"question":"I'm researching modern architecture: what is the performance difference between older and newer buildings in terms of energy efficiency, and how does the Zuellig Building compare to these standards?","answer":"Older buildings often outperform newer ones in energy efficiency. For example, in New York, while the modern 7 World Trade Center scored below the minimum efficiency rating of 75, retrofitted older buildings like the Empire State Building and Chrysler Building scored 80 and 84 respectively. This is because older buildings typically featured practical designs with smaller windows, load-bearing walls with higher thermal mass, and natural daylight exploitation. In contrast, the modern Zuellig Building represents a successful integration of sustainable design, achieving at least 15% energy savings compared to conventional standards through technologies like low-emissivity glass, solar power systems, and water recycling that saves over 70% water annually. It demonstrates how modern buildings can achieve high performance when properly designed with sustainability in mind.","context":["Something surprising has happened with many so-called “sustainable” buildings. When actually measured in post-occupancy assessments, they’ve proven far less sustainable than their proponents have claimed. In some cases they’ve actually performed worse than much older buildings, with no such claims. A 2009 New York Times article, “Some buildings not living up to green label,” documented the extensive problems with many sustainability icons. Among other reasons for this failing, the Times pointed to the widespread use of expansive curtain-wall glass assemblies and large, “deep-plan” designs that put most usable space far from exterior walls, forcing greater reliance on artificial light and ventilation systems.\nPartly in response to the bad press, the City of New York instituted a new law requiring disclosure of actual performance for many buildings. That led to reports of even more poor-performing sustainability icons. Another Times article, “City’s Law Tracking Energy Use Yields Some Surprises,” noted that the gleaming new 7 World Trade Center, LEED Gold-certified, scored just 74 on the Energy Star rating — one point below the minimum 75 for “high-efficiency buildings” under the national rating system. That modest rating doesn’t even factor in the significant embodied energy in the new materials of 7 World Trade Center.\nWhat's going on with these supposedly \"sustainable\" buildings? Read on, after the break...\nThings got even worse in 2010 with a lawsuit [“$100 Million Class Action Filed Against LEED and USGBC”] against the US Green Building Council, developers of the LEED certification system (Leadership in Energy and Environmental Design). The plaintiffs in the lawsuit alleged that the USGBC engaged in “deceptive trade practices, false advertising and anti-trust” by promoting the LEED system, and argued that because the LEED system does not live up to predicted and advertised energy savings, the USGBC actually defrauded municipalities and private entities. The suit was ultimately dismissed, but in its wake the website Treehugger and others predicted, based on the evidence uncovered, that “there will be more of this kind of litigation.”\nWhat’s going on? How can the desire to increase sustainability actually result in its opposite?\nOne problem with many sustainability approaches is that they don’t question the underlying building type. Instead they only add new “greener” components, such as more efficient mechanical systems and better wall insulation. But this “bolt-on” conception of sustainability, even when partially successful, has the drawback of leaving underlying forms, and the structural system that generates them, intact. The result is too often the familiar “law of unintended consequences.” What’s gained in one area is lost elsewhere as the result of other unanticipated interactions.\nFor example, adding more efficient active energy systems tends to reduce the amount of energy used, and therefore lowers its overall cost. But, in turn, that lower cost tends to make tenants less careful with their energy use — a phenomenon known as “Jevons’ Paradox.” Increasing efficiency lowers cost, and increases demand — in turn increasing the rate of consumption, and wiping out the initial savings. The lesson is that we can’t deal with energy consumption in isolation. We have to look at the concept of energy more broadly, including embodied energy and other factors.\nThere are often other unintended consequences. A notable case is London’s sustainability-hyped “Gherkin” (Foster & Partners, 2003), where the building’s open-floor ventilation system was compromised when security-conscious tenants created glass separations. Operable windows whose required specifications had been lowered because of the natural ventilation feature actually began to fall from the building, and had to be permanently closed. The ambitious goal of a more sophisticated natural ventilation system paradoxically resulted in even worse ventilation.\nNo building is an island\nAnother major problem with green building programs happens when they treat buildings in isolation from their urban contexts. In one infamous example (“Driving to Green Buildings”), the Chesapeake Bay Foundation moved its headquarters to the world’s first certified LEED-Platinum building — but the move took them from an older building in the city of Annapolis, Maryland to a new building in the suburbs, requiring new embodied energy and resources. The added employee travel alone — what’s known as “transportation energy intensity” — more than erased the energy gains of the new building.\nThe theory of resilience discussed in our article, “Toward Resilient Architectures 1: Biology Lessons,” points to the nature of the problem. Systems may appear to be well engineered within their original defined parameters — but they will inevitably interact with many other systems, often in an unpredictable and non-linear way. We look towards a more “robust” design methodology, combining redundant (“network”) and diverse approaches, working across many scales, and ensuring fine-grained adaptivity of design elements.\nThough these criteria may sound abstract, they’re exactly the sorts of characteristics achieved with so-called “passive” design approaches. Passive buildings allow the users to adjust and adapt to climactic conditions — say, by opening or closing windows or blinds, and getting natural light and air. These designs can be far more accurate in adjusting to circumstances at a much finer grain of structure. They feature diverse systems that do more than one thing — like the walls that hold up the building and also accumulate heat through thermal mass. They have networks of spaces that can be reconfigured easily, even converted to entirely new uses, with relatively inexpensive modifications (unlike the “open-plan” typology, which has never delivered on expectations). They are all-around, multi-purpose buildings that aren’t narrowly designed to one fashionable look or specialized user. And perhaps most crucially, they don’t stand apart from context and urban fabric, but work together with other scales of the city, to achieve benefits at both larger and smaller scales.\nOlder buildings perform better… sometimes\nMany older buildings took exactly this “passive” approach, simply because they had to. In an era when energy was expensive (or simply not available) and transportation was difficult, buildings were naturally more clustered together in urban centers. Their shape and orientation exploited natural daylight, and typically featured smaller, well-positioned windows and load-bearing walls with higher thermal mass.\nThe simple, robust shapes of these buildings allowed almost endless configurations. In fact many of the most in-demand urban buildings today are actually adaptive reuse projects of much older buildings. The results of this passive approach are reflected in good energy performance. While New York’s 7 World Trade Center actually scored below the city’s minimum rating of 75 out of 100, older buildings in the city that had been retrofitted with the same efficient heating, cooling, and lighting technologies fared much better: the Empire State Building scored a rating of 80, the Chrysler Building scored 84.\nBut just being old is clearly not a criterion of success. The 1963 MetLife/PanAm building (Walter Gropius & Pietro Belluschi), now a half-century old, scored a dismal 39. Another mid-century icon, the Lever House (Skidmore, Owings & Merrill, 1952), scored 20. The worst performer of all was Ludwig Mies Van der Rohe’s iconic Seagram building, built in 1958. Its score was an astonishingly low 3. What’s the problem with these buildings?\nAs the earlier New York Times article noted, they have extensive curtain-wall assemblies, large window areas, large-scale “deep-plan” forms, and other limitations. On a fundamental level, as we can now begin to see from resilience theory, they lack many crucial resilient advantages of older building types. There may be something inherent in the building type itself that is non-resilient. The form language itself could be an innate problem — something that, according to systems thinking, no mere bolt-on “green” additions can fix.\nArchitectural critic Peter Buchanan, writing recently in the UK magazine, The Architectural Review, placed the blame for these failures squarely at the feet of the Modernist design model itself, and called for a “big rethink” about many of its unquestioned assumptions [“The Big Rethink: Farewell To Modernism — And Modernity Too”]. Modernism is inherently unsustainable, he argued, because it evolved in the beginning of the era of abundant and cheap fossil fuels. This cheap energy powered the weekend commute to the early Modernist villas, and kept their large open spaces warm, in spite of large expanses of glass and thin wall sections. Petrochemicals created their complex sealants and fueled the production of their exotic extrusions. “Modern architecture is thus an energy-profligate, petrochemical architecture, only possible when fossil fuels are abundant and affordable”, he said. “Like the sprawling cities it spawned, it belongs to that waning era historians are already calling ‘the oil interval’.”\nBuchanan is not alone in calling for a “big rethink” about the assumptions of Modernist design. It is fashionable among many architects today to attack Modernism, and argue instead for various kinds of avant-garde and “Post-Modernist” styles. Buchanan lumps these styles together under a category he calls “Deconstructionist Post-Modernism.” But he insists that the Deconstructionists have not actually transcended the Modernist paradigm they attack: they still operate almost entirely within the industrial assumptions and engineering methodologies of the “oil interval.”\nOnce again, resilience theory provides insight into the serious flaws carried by this family of related form languages — and indeed, flaws in their very conception of design. (Those will need to be examined in great detail.) Ironically, this “modern” model is now almost a century old, belonging to an era of “engineered resilience” — that is, resilience within only one designed system, but unable to cope with the unintended consequences of interactions with other systems (like urban transportation, say, or true ecological systems).\nBecause the Modernist form language and its successors are tied to the old linear engineering paradigm, they cannot in practice combine redundant (“network”) and diverse approaches, nor work across many scales, nor ensure a fine-grained adaptivity for design elements — though they can certainly create the symbolic appearance of doing so. Contrary to such dubious claims (in what sometimes takes on aspects of a massive marketing effort), they cannot actually achieve what C. H. Holling called “ecological resilience.” This seems to suggest an important explanation of the alarmingly poor performance of these buildings and places, when actually evaluated in post-occupancy research.\nSeen in this light, the various avant-garde attempts to transcend Modernism appear more as exotic new wrappings for the same underlying (and non-resilient) structural types and industrial methods. But as Albert Einstein famously pointed out: “A new type of thinking is essential if mankind is to survive and move toward higher levels.” Just as it is not possible to achieve resilience by merely adding new devices like solar collectors to these old industrial-Modernist building types, it is not possible to get meaningful benefits with dazzling new designer permutations and tokenistic ecological thinking within the same essentially industrial design process. We do need a “big rethink” about the most basic methods and systems of design for the future.\nA wave of neo-modernism\nYet if anything, in recent years there has been a remarkable resurgence of an even more unapologetic form of Modernism. In light of the evidence, this is a decidedly reactionary trend: we seem to be witnessing a “back to roots” movement — one that, like other such movements, is based more on doctrinal belief than on evidence. This fashionable Neo-Modernism ranges from outright “retro” boxy white buildings, interiors, and furnishings, to swoopy futuristic-looking buildings and landscapes. Stylistically, the shapes are eye-catching and often edgy, and some people (especially many architects) clearly like them.\nNot everyone seems to care for this new/old aesthetic, however. Some see the new structures as sterile, ugly, and disruptive to their neighborhoods and cities. Defenders of the designs often attack these critics for being presumably unsophisticated, nostalgic, or unwilling to accept the inevitable progress of a dynamic culture. This “battle of stylistic preferences” rages on, with the Neo-Modernists claiming the avant-garde high ground, where they tend to dominate the media, critics, and schools.\nOf course, fashions come and go, and architecture is no different: in a sense this is just another phase in the more or less continuous waxing and waning of architectural Modernism for almost a century now, along with raging debates about its aesthetic merits. Those debates have never really died down. Critics like Buchanan are not new: in the 1960s and 1970s equally vociferous critics like Christopher Alexander, Peter Blake, Jane Jacobs, David Watkin, and Tom Wolfe made withering critiques, but little has changed.\nWhat has now changed, however, is that we are asking newly urgent questions about the resilience of this kind of structure, at a time when we need to rigorously assess and improve that resilience. As this discussion suggests, it is not only the particular and practical issues of expansive glazed curtain walls, bulky and transparent buildings, and exotic assemblies overly reliant on petrochemical products that are the root of the problem. It is perhaps the very idea of buildings as fashionable icons celebrating their own newness, a quintessentially Modernist idea, which is fundamentally at odds with the notion of sustainability.\nAs they age, these buildings are destined to be less new and therefore less useful, not more so. The pristine Modernist (and now Post-Modernist and Deconstructivist) industrial surfaces are destined to mar, weather, and otherwise degrade. The eye-catching novelties of one era will become the abandoned eyesores of the next, an inevitability lost on a self-absorbed elite fixated on today’s fashions. Meanwhile the humble, humane criteria of resilient design are being pushed aside, in the rush to embrace the most attention-getting new technological approaches — which then produce a disastrous wave of unintended failures. This is clearly no way to prepare for a “sustainable” future in any sense.\nModernism is more than just a style\nIn this light, why have the form language and design methodologies of Modernism proven so stubbornly persistent? The answer is that Modernism is not merely a style that one may care for or not. It is part and parcel of a remarkably comprehensive — even totalizing — project of aesthetics, tectonics, urbanism, technology, culture, and ultimately, civilization. That project has had a profound effect upon the development of modern settlements, for better or worse, and (especially visible in the light of resilience theory) made a huge contribution to the current state in which we find our cities, and our civilization.\nThe origins of architectural Modernism are closely affiliated with the progressive goals of the early Twentieth Century, and the humanitarian ideals — even the utopian zeal — of well-meaning visionaries of that day. Those individuals saw a promising capacity, in the dawning industrial technology of the age, to deliver a new era of prosperity and quality of life for humanity. At their most credulous, its leaders were clearly enraptured by the seemingly infinite possibilities for a technological utopia. From that they developed an elaborate — and in surprising ways, still poorly-evaluated — theory about the necessary new tectonics and form languages of the civilization of the future. Their followers today still argue that it is, unquestionably, Modernism that is best positioned to don the mantle of sustainability.\nMany things did improve under this technological regime, of course, and today we can cure diseases, reduce backbreaking toil, eat exotic foods, travel fast in comfortable motoring and flying craft, and do many other things that would astonish our ancestors. But along with that new regime has come a calamitous ecological depletion and destruction of resources, and an erosion of the foundation on which all economics and indeed all life depends. So today, in an age of converging crises, it is well worth our asking hard questions about the assumptions of that industrial regime — and the complicity of architectural Modernism as a kind of alluring “product packaging” within it.\nThe story goes back to a remarkably small group of writers, theorists, and practitioners in the early 20th Century, and notably the Austrian architect Adolf Loos. We will need to look more closely at this history — and what its ongoing legacy means for us, and our very daunting design challenges today.\nMichael Mehaffy is an urbanist and critical thinker in complexity and the built environment. He is a practicing planner and builder, and is known for his many projects as well as his writings. He has been a close associate of the architect and software pioneer Christopher Alexander. He is a Research Associate with the Center for Environmental Structure, Alexander’s research center founded in 1967, and Executive Director of the Sustasis Foundation, a Portland, OR-based NGO dedicated to developing and applying neighborhood-scale tools for resilient and sustainable development.\nNikos A. Salingaros is a mathematician and polymath known for his work on urban theory, architectural theory, complexity theory, and design philosophy. He has been a close collaborator of the architect and computer software pioneer Christopher Alexander. Salingaros published substantive research on Algebras, Mathematical Physics, Electromagnetic Fields, and Thermonuclear Fusion before turning his attention to Architecture and Urbanism. He still is Professor of Mathematics at the University of Texas at San Antonioand is also on the Architecture faculties of universities in Italy, Mexico, and The Netherlands.","Building puts PH in green world mapBy Tessa R. Salazar |Philippine Daily Inquirer\nA 33-story Makati office tower that uses the same sustainable technology being used in the Freedom Tower in New York puts the Philippines in the green world map.\nThe Zuellig Building, sheathed in 28,000 square meters of floor-to-ceiling low-emissivity glass, joins the ranks of the world’s most state-of-the-art, environment-friendly buildings like the Bank of America Tower-New York City, Asia Square in Singapore, and Taipei 101.\nAll in all, there are only about a hundred LEED Platinum-certified high-rise office buildings worldwide. The Zuellig Building now joins this elite group.\nThe US Green Building Council (USGBC) awarded the Zuellig Building the Platinum certification level under its LEED-CS (Leadership for Energy and Environmental Design-Core and Shell) program this month. Platinum, the highest level of LEED certification, is actually an upgrade for Zuellig Building, which was awarded at precertification the Gold level under LEED-CS in June 2009.\nThe Zuellig skyscraper that stands on a landscaped 8,285-square-meter site at the intersection of Makati Avenue and Paseo de Roxas is the first development in the Philippines in the LEED-CS program, and among the first in Asia, to earn the highest level in the LEED rating system for “green” architecture, sustainable construction methodologies and resource-efficient building operations. The property has a total leasable area of 55,000-sq-m prime-grade office space for multitenant use.\nBased on available data from the USGBC, there are only three Platinum-certified projects in the Philippines. The other two are Bridgebury Realty Corp. Office, developers of the Zuellig Building (LEED Platinum for Commercial Interiors), and the Sunlife Philippines head office in Taguig (LEED Platinum for Commercial Interiors).\nCBRE Philippines told Inquirer Property in a statement that “as the outsourcing and offshoring sector gains strength in the country, we see more occupiers and developers prioritizing flight to quality, with green buildings becoming more the norm than the exception.”\nIt added that the Zuellig Building “raises the bar for green building standards in the Philippine prime office market.”\nThe team of architects and design consultants behind the Zuellig project included W.V. Coscolluela & Associates and Skidmore Owings & Merrill (SOM, New York), the same company behind the Freedom Tower. Adherence to LEED requirements was monitored throughout the construction process by the project’s sustainability consultant, Langdon & Seah.\nExperts have stressed that the construction sector is one of the most significant contributors to the carbon footprint of the industrial sector. Also, studies have shown that buildings account for over half of the world’s carbon emissions, making their impact on the environment a key concern. This is specially evident in Asia with its rapid growth of highly urbanized populations.\nInitiated by the USGBC in 2000, LEED is recognized as a global standard for the assessment of the sustainability of buildings.\nThe LEED-CS rating system allocates points based on a building’s environmental impact and human benefits in five major credit categories: sustainable sites, water efficiency, energy and atmosphere, materials and resources, and indoor environmental quality.\nEach of the performance categories includes preconditional mandatory measures. Certification is granted solely by the USGBC in Washington, DC and requires third-party verification of compliance.\nThe green technologies employed in the Zuellig Building achieve significant energy savings (at least 15 percent compared with a base building built to conventional US standards), thereby reducing greenhouse emissions and energy costs.\nThe building has an on-grid photovoltaic solar power system to generate renewable energy. Recycling of gray water and the collection of rainwater and condensate water result in over 70-percent water savings (29-million liters annually).\nAccess to public transport\nWith transportation being another major contributor to worldwide carbon emissions, the location of the Zuellig Building allows tenants easy connectivity and access to public transportation. Bicycle stands and showers are provided for office users who choose to pedal to work instead of burning fuel.\nGreen buildings are designed to support the well-being and productivity of occupants. In the Zuellig Building, 90 percent of the office space gets natural light. The Indoor Air Quality plan controls the choice of construction materials to eliminate the health risks of volatile organic compounds, and carbon dioxide sensors control and modulate the airflow.\nZuellig also received a Platinum LEED-CI certification (Commercial Interiors) for the fit-out of its corporate offices on the 32nd floor. Tenants who desire a LEED-CI certification for their individual fit-outs automatically gain a certain number of points simply by being located in the building."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:4a4b3424-97ae-4903-b880-4b443e32f498>","<urn:uuid:196a5e54-bc15-4f32-8036-49e3f1c68fe4>"],"error":null}
{"question":"What is the relationship between cybernetic principles and modern algorithmic governance, and how does this impact social decision-making processes?","answer":"Cybernetic principles, which focus on control and communication in complex systems, have evolved from mechanical self-regulation to influence modern algorithmic governance. While cybernetics originally dealt with mechanical and biological systems, its concepts of feedback and control are now applied in algorithmic decision-making across various sectors. However, this application has led to concerning social implications. Algorithms now influence crucial decisions in employment, law enforcement, education, and finance, often lacking transparency and potentially reinforcing societal biases. For instance, these systems can make determinations about individuals' creditworthiness, job suitability, or criminal risk based on data patterns that may discriminate against certain groups, raising issues about fairness and accountability in automated decision-making processes.","context":["Individual differences |\nMethods | Statistics | Clinical | Educational | Industrial | Professional items | World psychology |\nCybernetics is the interdisciplinary study of the structure of complex systems, especially communication processes, control mechanisms and feedback principles. Cybernetics is closely related to control theory and systems theory.\nContemporary cybernetics began as an interdisciplinary study connecting the fields of control systems, electrical network theory, mechanical engineering, logic modeling, evolutionary biology and neuroscience in the 1940s. Other fields of study which have influenced or been influenced by cybernetics include game theory; system theory (a mathematical counterpart to cybernetics); psychology, especially neuropsychology, behavioral psychology,cognitive psychology; philosophy and even architecture.[How to reference and link to summary or text]\nThe term cybernetics stems from the Greek Κυβερνήτης (kybernetes, steersman, governor, pilot, or rudder — the same root as government). Cybernetics is a broad field of study, but the essential goal of cybernetics is to understand and define the functions and processes of systems. Studies of this field are all ultimately means of examining different forms of systems and applying what is known to make artificial systems, such as business management, more efficient and effective.\nCybernetic was defined by Norbert Wiener, in his book of that title, as the study of control and communication in the animal and the machine. Stafford Beer called it the science of effective organization and Gordon Pask extended it to include information flows \"in all media\" from stars to brains. It includes the study of feedback, black boxes and derived concepts such as communication and control in living organisms, machines and organizations including self-organization. Its focus is how anything (digital, mechanical or biological) processes information, reacts to information, and changes or can be changed to better accomplish the first two tasks . A more philosophical definition, suggested in 1956 by Louis Couffignal, one of the pioneers of cybernetics, characterizes cybernetics as \"the art of ensuring the efficacy of action\" . The most recent definition has been proposed by Louis Kauffman, President of the American Society for Cybernetics, \"Cybernetics is the study of systems and processes that interact with themselves and produce themselves from themselves\" .\nConcepts studied by cyberneticists include, but are not limited to: learning, cognition, adaption, social control, emergence, communication, efficiency, efficacy and interconnectivity. These concepts are studied by other subjects such as engineering and biology, but in cybernetics these are removed from the context of the individual organism or device.\nOther fields of study which have influenced or been influenced by cybernetics include game theory; system theory (a mathematical counterpart to cybernetics); psychology, especially neuropsychology, behavioral psychology,cognitive psychology; philosophy; anthropology and even architecture.[How to reference and link to summary or text]\nThe Roots of Cybernetic theoryEdit\nThe word cybernetics was first used in the context of \"the study of self-governance\" by Plato in The Laws to signify the governance of people. The words govern and governor are related to the same Greek root through the Latin cognates gubernare and gubernator. The word \"cybernétique\" was also used in 1834 by the physicist André-Marie Ampère (1775–1836) to denote the sciences of government in his classification system of human knowledge.\nThe first artificial automatic regulatory system, a water clock, was invented by the mechanician Ktesibios. In his water clocks, water flowed from a source such as a holding tank into a reservoir, then from the reservoir to the mechanisms of the clock. Ktesibios's device used a cone-shaped float to monitor the level of the water in its reservoir and adjust the rate of flow of the water accordingly to maintain a constant level of water in the reservoir, so that it neither overflowed nor was allowed to run dry. This was the first artificial truly automatic self-regulatory device that required no outside intervention between the feedback and the controls of the mechanism. Although they did not refer to this concept by the name of Cybernetics (they considered it a field of engineering), Ktesibios and others such as Heron and Su Song are considered to be some of the first to study cybernetic principles.\nThe study of teleological mechanisms (from the Greek τέλος or telos for end, goal, or purpose) in machines with corrective feedback dates from as far back as the late 1700s when James Watt's steam engine was equipped with a governor, a centripetal feedback valve for controlling the speed of the engine. Alfred Russel Wallace identified this as the principle of evolution in his famous 1858 paper. In 1868 James Clerk Maxwell published a theoretical article on governors, one of the first to discuss and refine the principles of self-regulating devices.\nThe Early 20th centuryEdit\nContemporary cybernetics began as an interdisciplinary study connecting the fields of control systems, electrical network theory, mechanical engineering, logic modeling, evolutionary biology and neuroscience in the 1940s. Electronic control systems originated with the 1927 work of Bell Telephone Laboratories engineer Harold S. Black on using negative feedback to control amplifiers. The ideas are also related to the biological work of Ludwig von Bertalanffy in General Systems Theory.\nEarly applications of negative feedback in electronic circuits included the control of gun mounts and radar antenna during World War Two. Jay Forrester, a graduate student at the Servomechanisms Laboratory at MIT during WWII working with Gordon S. Brown to develop electronic control systems for the U.S. Navy, later applied these ideas to social organizations such as corporations and cities as an original organizer of the MIT School of Industrial Management at the MIT Sloan School of Management. Forrester is known as the founder of System Dynamics.\nW. Edwards Deming, the Total Quality Management guru for whom Japan named its top post-WWII industrial prize, was an intern at Bell Telephone Labs in 1927 and may have been influenced by network theory. Deming made \"Understanding Systems\" one of the four pillars of what he described as \"Profound Knowledge\" in his book \"The New Economics.\"\nNumerous papers spearheaded the coalescing of the field. In 1935 Russian physiologist P.K. Anokhin published a book in which the concept of feedback (\"back afferentation\") was studied. The Romanian scientist Ştefan Odobleja published Psychologie consonantiste (Paris, 1938), describing many cybernetic principles. The study and mathematical modelling of regulatory processes became a continuing research effort and two key articles were published in 1943. These papers were \"Behavior, Purpose and Teleology\" by Arturo Rosenblueth, Norbert Wiener, and Julian Bigelow; and the paper \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" by Warren McCulloch and Walter Pitts.\nIn the spring of 1947, Wiener was invited to a congress on harmonic analysis, held in Nancy, France. The event was organized by the Bourbaki, a French scientific society, and mathematician Szolem Mandelbrojt (1899-1983), uncle of the world-famous mathematician Benoît Mandelbrot.\nDuring this stay in France, Wiener received the offer to write a manuscript on the unifying character of this part of applied mathematics, which is found in the study of Brownian motion and in telecommunication engineering. The following summer, back in the United States, Wiener decided to introduce the neologism cybernetics into his scientific theory. The name cybernetics was coined to denote the study of \"teleological mechanisms\" and was popularized through his book Cybernetics, or Control and Communication in the Animal and Machine (1948). In the UK this became the focus for the Ratio Club.\nIn the early 1940's John von Neumann, although better known for his work in mathematics and computer science, did contribute a unique and unusual addition to the world of cybernetics: Von Neumann cellular automata, and their logical follow up the Von Neumann Universal Constructor. The result of these deceptively simple thought-experiments was the concept of self replication which cybernetics adopted as a core concept. The concept that the same properties of genetic reproduction applied to social memes, living cells, and even computer viruses is further proof of the somewhat surprising universality of cybernetic study.\nWiener popularized the social implications of cybernetics, drawing analogies between automatic systems (such as a regulated steam engine) and human institutions in his best-selling The Human Use of Human Beings : Cybernetics and Society (Houghton-Mifflin, 1950).\nWhile not the only instance of a research organization focused on cybernetics, the Biological Computer Lab at the University of Illinois, Urbana/Champaign, under the direction of Heinz von Foerster, was a major center of cybernetic research for almost 20 years, beginning in 1958.\nThe Fall and Rebirth of CyberneticsEdit\nFor a time during the past 20 years, the field of cybernetics followed a boom-bust cycle of becoming more and more dominated by the subfields of artificial intelligence and machine-biological interfaces (ie. cyborgs) and when this research fell out of favor, the field as a whole fell from grace. Recent endeavors into the true focus of cybernetics, systems of control and emergent behavior, by such related fields as Game Theory (the analysis of group interaction), systems of feedback in evolution, and Metamaterials (the study of materials with properties beyond the newtonian properties of their constituent atoms), have lead to a revived interest in this increasingly relevant field.\nSubdivisions of the fieldEdit\nCybernetics is an earlier but still-used generic term for many subject matters. These subjects also extend into many others areas of science, but are united in their study of control of systems.\nPure Cybernetics Edit\nPure cybernetics studies systems of control as a concept, attempting to discover the basic principles underlying such things as\n- Artificial intelligence\n- Control systems\n- Learning organization\n- New Cybernetics\n- Second-order cybernetics\n- Interactions of Actors Theory\n- Conversation Theory\nIn Biology Edit\nCybernetics in biology is the study of cybernetic systems present in biological organisms, primarily focusing on how animals adapt to their environment, and how information in the form of genes is passed from generation to generation. There is also a secondary focus on cyborgs.\nIn Complexity Science Edit\nComplexity Science attempts to analyze the nature of complex systems, and the reasons behind their unusual properties.\nIn Computer Science Edit\nComputer science directly applies the concepts of cybernetics to the control of devices and the analysis of information.\nIn Engineering Edit\nIn Management Edit\n- Entrepreneurial cybernetics\n- Management cybernetics\n- Organizational cybernetics\n- Operations research\n- Systems engineering\nIn Mathematics Edit\nMathematical Cybernetics focuses on the factors of information, interaction of parts in systems, and the structure of systems.\nIn Psychology Edit\nIn Sociology Edit\nBy examining group behavior through the lens of cybernetics, sociology seeks the reasons for such spontaneous events as smart mobs and riots, as well as how communities develop rules, such as etiquette, by consensus without formal discussion. Affect Control Theory explains role behavior, emotions, and labeling theory in terms of homeostatic maintenance of sentiments associated with cultural categories. These and other cybernetic models in sociology are reviewed in a book edited by McClelland and Fararo.\nFurther reading Edit\n- W. Ross Ashby (1956), Introduction to Cybernetics. Methuen, London, UK. PDF text.\n- Stafford Beer (1974), Designing Freedom, John Wiley, London and New York, 1975.\n- Lars Bluma, (2005), Norbert Wiener und die Entstehung der Kybernetik im Zweiten Weltkrieg, Münster.\n- Steve J. Heims (1980), John von Neumann and Norbert Wiener: From Mathematics to the Technologies of Life and Death, 3. Aufl., Cambridge.\n- Steve J. Heims (1993), Constructing a Social Science for Postwar America. The Cybernetics Group, 1946-1953, Cambridge University Press, London, UK.\n- Helvey, T.C. The Age of Information: An Interdisciplinary Survey of Cybernetics. Englewood Cliffs, N.J.: Educational Technology Publications, 1971.\n- Francis Heylighen, and Joslyn C. (2001), \"Cybernetics and Second Order Cybernetics\", in: R.A. Meyers (ed.), Encyclopedia of Physical Science & Technology (3rd ed.), Vol. 4, (Academic Press, New York), p. 155-170.\n- Hans Joachim Ilgauds (1980), Norbert Wiener, Leipzig.\n- P. Rustom Masani (1990), Norbert Wiener 1894-1964, Basel.\n- Eden Medina, \"Designing Freedom, Regulating a Nation: Socialist Cybernetics in Allende's Chile.\" Journal of Latin American Studies 38 (2006):571-606.\n- Paul Pangaro (1990), \"Cybernetics — A Definition\", Eprint.\n- Gordon Pask (1972), \"Cybernetics\", entry in Encyclopaedia Britannica 1972.\n- B.C. Patten, and E.P. Odum (1981), \"The Cybernetic Nature of Ecosystems\", The American Naturalist 118, 886-895.\n- Plato, \"Alcibiades 1\", W.R.M. Lamb (trans.), pp. 93–223 in Plato, Volume 12, Loeb Classical Library, London, UK, 1927.\n- Heinz von Foerster, (1995), Ethics and Second-Order Cybernetics.\n- Norbert Wiener (1948), Cybernetics or Control and Communication in the Animal and the Machine, Paris, Hermann et Cie - MIT Press, Cambridge, MA.\n- ↑ 1.0 1.1 Kelly, Kevin (1994). Out of control: the new biology of machines, social systems and the economic world, Boston: Addison-Wesley.\n- ↑ Couffignal, Louis, \"Essai d’une définition générale de la cybernétique\", The First International Congress on Cybernetics, Namur, Belgium, June 26-29, 1956, Gauthier-Villars, Paris, 1958, pp. 46-54\n- ↑ CYBCON discussion group 20 September 2007 18:15\n- ↑ Note: this does not refer to the concept of Racial Memory but to the concept of cumulative adaptation to a particular niche, such as the case of the pepper moth having genes for both light and dark environments.\n- ↑ McClelland, Kent A., and Thomas J. Fararo (Eds.). 2006. Purpose, Meaning, and Action: Control Systems Theories in Sociology. New York: Palgrave Macmillan.\nSee also Edit\n- Cybernetics projects, Physics, and High Voltage\n- Ego Death and Self-Control Cybernetics\n- Louis Couffignal's photos & documents\n- Web Dictionary of Cybernetics and Systems\n- Glossary Slideshow (136 slides)\n- Principia Cybernetica Web\n- Mindmap-based-page by Ragnar Heil\n- The Cybernetics Society\n- American Society for Cybernetics\n- Iberobotics - Portal de Robótica en Castellano\n- The Systemic Approach : an introduction\n- Cybernetics and Information Theory in the United States, France and the Soviet Union\n- Medizinische Kybernetik | Medical Cybernetics\n- Cybernetics category in the Open Encyclopedia Project\n- Systemics and cybernetics in a historical perspective (pdf document)\n|This page uses Creative Commons Licensed content from Wikipedia (view authors).|","When you browse online for a new pair of shoes, pick a movie to stream on Netflix or apply for a car loan, an algorithm likely has its word to say on the outcome.\nThe complex mathematical formulas are playing a growing role in all walks of life: from detecting skin cancers to suggesting new Facebook friends, deciding who gets a job, how police resources are deployed, who gets insurance at what cost, or who is on a “no fly” list.\nAlgorithms are being used—experimentally—to write news articles from raw data, while Donald Trump’s presidential campaign was helped by behavioral marketers who used an algorithm to locate the highest concentrations of “persuadable voters.” But while such automated tools can inject a measure of objectivity into erstwhile subjective decisions, fears are rising over the lack of transparency algorithms can entail, with pressure growing to apply standards of ethics or “accountability.”\nData scientist Cathy O’Neil cautions about “blindly trusting” formulas to determine a fair outcome. “Algorithms are not inherently fair, because the person who builds the model defines success,” she said.\nO’Neil argues that while some algorithms may be helpful, others can be nefarious. In her 2016 book, Weapons of Math Destruction, she cites some troubling examples in the United States:\nPublic schools in Washington, D.C. in 2010 fired more than 200 teachers—including several well-respected instructors—based on scores in an algorithmic formula which evaluated performance.\nA man diagnosed with bipolar disorder was rejected for employment at seven major retailers after a third-party “personality” test deemed him a high risk based on its algorithmic classification.\nMany jurisdictions are using “predictive policing” to shift resources to likely “hot spots.” O’Neill says that depending on how data is fed into the system, this could lead to discovery of more minor crimes and a “feedback loop” which stigmatizes poor communities.\nSome courts rely on computer-ranked formulas to determine jail sentences and parole, which may discriminate against minorities by taking into account “risk” factors such as their neighborhoods and friend or family links to crime.\nIn the world of finance, brokers “scrape” data from online and other sources in new ways to make decisions on credit or insurance. This too often amplifies prejudice against the disadvantaged, O’Neil argues.\nHer findings were echoed in a White House report last year warning that algorithmic systems “are not infallible—they rely on the imperfect inputs, logic, probability, and people who design them.”\nThe report noted that data systems can ideally help weed out human bias but warned against algorithms “systematically disadvantaging certain groups.”\nZeynep Tufekci, a University of North Carolina professor who studies technology and society, said automated decisions are often based on data collected about people, sometimes without their knowledge. “These computational systems can infer all sorts of things about you from your digital crumbs,” Tufekci said in a recent TED lecture. “They can infer your sexual orientation, your personality traits, your political leanings. They have predictive power with high levels of accuracy.”\nSuch insights may be useful in certain contexts—such as helping medical professionals diagnose postpartum depression—but unfair in others, she said. Part of the problem, she said, stems from asking computers to answer questions that have no single right answer. “They are subjective, open-ended and value-laden questions, asking who should the company hire, which update from which friend should you be shown, which convict is more likely to reoffend.”\nFrank Pasquale, a University of Maryland law professor and author of The Black Box Society: The Secret Algorithms That Control Money and Information, shares the same concerns. He suggests one way to remedy unfair effects may be to enforce existing laws on consumer protection or deceptive practices.\nPasquale points at the European Union’s data protection law, set from next year to create a “right of explanation” when consumers are impacted by an algorithmic decision, as a model that could be expanded. This would “either force transparency or it will stop algorithms from being used in certain contexts,” he said.\nAlethea Lange, a policy analyst at the Center for Democracy and Technology, said the E.U. plan “sounds good” but “is really burdensome” and risked proving unworkable in practice. She believes education and discussion may be more important than enforcement in developing fairer algorithms.\nLange said her organization worked with Facebook, for example, to modify a much-criticized formula that allowed advertisers to use “ethnic affinity” in their targeting.\nOthers meanwhile caution that algorithms should not be made a scapegoat for societal ills. “People get angry and they are looking for something to blame,” said Daniel Castro, vice president at the Information Technology and Innovation Foundation. “We are concerned about bias, accountability and ethical decisions but those exist whether you are using algorithms or not.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:25d69a17-56b2-46dc-9005-7cf09bc48e3e>","<urn:uuid:6ac4c8f9-4485-43b7-b764-b48a249516fb>"],"error":null}
{"question":"Hello, I am studying environmental organizations. Which one is older: The British Ecological Society founded in 1913 or the Natural Beekeeping Trust formed in 2009?","answer":"The British Ecological Society is older, as it was founded in 1913 and is actually the oldest ecological society in the world. In comparison, the Natural Beekeeping Trust was formed much more recently in 2009 in response to the need for an alternative approach to bee care.","context":["It’s more important than ever for strong organisations to fight for a better environment, enforce climate action, and champion the most critical issues on our planet today. Inspirational people play their part in every country around the world, and Britain is no exception.\nIn no particular order or priority, here are 12 of the most influential and important environmental organisations in the UK.\nFounded in 1992 as a result of the Earth Summit that year, the Energy Saving Trust started as a government sponsored initiative to promote efficient, clean energy and tackle climate change. Today, it operates in the form of an independent, non-profit organisation that works with consumers, businesses, and the government to advocate green energy solutions in the UK.\nFounded in 1913, it’s the oldest ecological society in the world. Its 5000 members are committed to research and the spread of knowledge in ecological matters, including biodiversity and habitat conservation. It works to inform the nation, while also lobbying lawmakers and government agencies to promote scientific, evidence-based solutions in the UK.\nNot just one, but a collection of 47 individual groups, The Wildlife Trusts work tirelessly to protect the natural environment in different parts of the UK. They are responsible for nature reserves, woods, and inner city parks, supported by their 800,000 members, who actively campaign for positive change on green issues.\nFounded initially as a not-for-profit magazine, Ethical Consumer now functions mainly as a website, but its mission remains the same. It aims to promote sustainability in business practices, and it hopes to achieve this by encouraging consumer pressure. Through its research of more than 40,000 brands and products, it’s developed an ethical rating system to inform buying decisions which could lead to greener practices by companies in the future.\nFriends of the Earth International started as a nuclear disarmament group in 1969, funded by American businessman and philanthropist Robert O Anderson. His first employee, Amory Lovins, established the organisation in the UK, where it now campaigns to save British bees, put an end to fracking, and reduce air pollution, among other worthy environmental causes.\nThis charitable body strives to promote scientific understanding, research, and debate around environmental topics, which it hopes will lead to the protection of the nature through sustainable development. The IES was founded in 1971, and is now international in its scope while maintaining its initial focus and working with the UK government to inform evidence-based, ethical policy-making.\nA registered charity that was formed as long ago as 1926, CPRE works to protect the countryside of the UK from a multitude threats. It’s concerned with avoiding damage to the natural environment through better waste management, clean energy, and green transport systems. Its 60,000 members also aim to limit urban sprawl and conserve green belts, which was CPRE’s founding mission statement.\nThis UK-based independent charity was formed in 1955 by the National Federation of Women’s Institutes as a response to the increasingly damaging throwaway culture of 1950s Britain. It’s mission today remains unchanged – keeping litter off our streets. It believes that it can reduce the UK’s waste problems by campaigning to foster better societal values in industries and communities, and by informing government policy with evidence-based research.\nThe ELF is a registered charity whose president is HRH The Prince of Wales. Its goal is to help more people be heard regarding environmental matters that might otherwise go unnoticed. The group offers advice and information through its own law clinics, based in communities and universities, and also presides over a network of specialised lawyers and advisors who offer up their services pro bono.\nThe national recycling campaign for England is funded by the government and managed by WRAP, a charity that campaigns for measures leading to a sustainable, resource-efficient economy. It was started in 2004 and is used by more than 90% of local authorities in England today. By providing information about what can be recycled and how to dispose of waste that isn’t dealt with by local waste collection services, Recycle Now claims to have turned 6 out of 10 of us into avid recyclers.\nThe mission of the UK’s leading renewable energy trade association is to get green energy into more homes around the nation. Its membership varies from small independent businesses to massive corporations, which means its reach is wide and its influence can be massive. Formally the British Wind Energy Association, it was founded in 1978 and now has more than 660 corporate members employing 250,000 people.\nThe CCC is a public body devoted to tackling climate change and promoting a low-carbon economy. It was established under the 2008 Climate Change Act, and is responsible for advising the UK government. Its remit includes carbon emissions target monitoring, policy analysis, and engagement with other organisations and the community.\nThere are countless brilliant organisations doing sterling work for the environment and global climate. We’ve missed a lot in this list. Which UK-based organisations would you recommend?\nContact us via Twitter and let us know.","The Bee Coalition formed in 2012 when the UK’s main environmental groups joined forces to bring attention to the plight of bees and other pollinators and to inspire and engage policymakers, industry and the public to protect these invaluable, irreplaceable animals.\nBuglife is the only organisation in Europe devoted to the conservation of all invertebrates. We're actively working to save Britain’s rarest little animals, everything from bees to beetles, worms to woodlice and jumping spiders to jellyfish. Our aim is to halt the extinction of invertebrate species and to achieve sustainable populations of invertebrates.\nWe are working hard to achieve this through:\n- Promoting the environmental importance of invertebrates and raising awareness about the challenges to their survival.\n- Assisting in the development of legislation and policy that will ensure the conservation of invertebrates.\n- Developing and disseminating knowledge about how to conserve invertebrates.\n- Encouraging and supporting invertebrate conservation initiatives by other organisations in the UK, Europe and worldwide.\n- Undertaking practical conservation projects that will contribute to achieving our aim.\nThere are more than 40,000 invertebrate species in the UK, and many of these are under threat as never before. Yet from food we eat, the fish we catch, the birds we see, the flowers we smell and the hum of life we hear, simply would not exist without bugs. Invertebrates are vitally important to a healthy planet – humans and other life forms could not survive without them. Each invertebrate species plays a unique and important role in the web of life, but once lost, they cannot\nButterfly Conservation was founded in 1968 and is a Uk charity dedicated to saving butterflies, moths and our environment. We are the largest insect conservation charity in the world with 29,000 members and over 70 staff. Our research provides advice on how to conserve and restore habitats. We run programmes for more than 100 threatened species and we are involved in conserving hundreds of sites and reserves. We co-ordinate national monitoring programmes and report on the status of butterflies and moths as indicators of the health of the wider environment. Volunteers and volunteering are core values of BC with over 15,000 volunteers who contribute hugely to recording, citizen science and conservation delivery.\nClientEarth uses law as a tool to mend the relationship between human societies and the Earth. We work in Europe and beyond, bringing together law, science and policy to create practical solutions to key environmental challenges. Strong laws are the best tools we have to protect the environment. By combining our legal expertise with scientific understanding, we work to tackle issues ranging from climate change to habitat loss, air pollution to deforestation.\nThe Environmental Justice Foundation (EJF) is a UK-based non-profit organisation working internationally to protect the environment and defend human rights.\nWe work to protect bees and other pollinators, like moths and butterflies, and to sustain the wild places and wild flowers vital to their existence and ours. We campaign against the use of harmful pesticides, such as neonicotinoids, and promote organic farming and gardening methods that reduce pesticides, provide vital pollinator habitats, increase biodiversity and sustain a healthier environment for them and us.\nBritain’s bees are in trouble. That’s bad for them - and for us. Bees pollinate many of the foods we take for granted from apples and beans to strawberries and tomatoes - even chocolate.\nFriends of the Earth launched The Bee Cause in 2012 to turn round the plight of our bees by tackling all causes of bee decline – from habitat loss to the use of pesticides.\nSo far this has led to:\n- Governments across the UK producing national action plans to help bees and other pollinators such as hoverflies and moths\n- Thousands of people getting closer to bees and boosting their understanding while gathering data via the Great British Bee Count, the largest citizen science bee project of its kind\n- Over 500 communities across the nation creating new improved spaces for bees and pollinators - called Bee Worlds - in their area.\nWe’ve brought people together to act for bees and pollinators - from businesses and scientists to farmers and local councils - to turn their concern into action. And with Bee Coalition colleagues and others such as The Women’s Institute we have also pushed for changes in how pesticides are tested and regulated.\nThe Natural Beekeeping Trust was formed in 2009 in response to the need for an alternative approach to the care of bees.\nThe key aims of the Trust are:\n- to disseminate information about bee-centred, natural beekeeping\n- to develop new understandings and ways of relating to the Bee that work with the natural behaviour of the creature rather than the enforced and stressful behaviour seen in conventional, chemical-dependent, beekeeping.\nThe Pesticide Action Network UK (PAN UK) is the only UK charity focused on tackling the problems caused by pesticides and promoting safe and sustainable alternatives to pesticides in agriculture, urban areas, homes and gardens.\nPAN UK has for many years been concerned about the harm that the use of pesticides is doing to bees and other pollinator species. We have been campaigning for an end to the use of bee and pollinator toxic pesticides and also for changes in the way agricultural pesticides are used in general in order to stop the negative impact they are having on pollinators and biodiversity more widely.\nThe RSPB is the country’s largest nature conservation charity, inspiring everyone to give nature a home. Together with our partners, we protect threatened birds and wildlife so our towns, coast and countryside will teem with life once again. We play a leading role in BirdLife International, a worldwide partnership of nature conservation organisations.\nThe Soil Association was formed in 1946 to pioneer a better world – one where we can live in health and in harmony with nature. Today we’re just as dedicated to making positive change happen. We’re farming and growing, buying, cooking and eating. We’re campaigning and researching. Together we’re transforming the way we eat, farm and care for the natural world. We believe in a world where people and planet thrive – but more importantly we’re pioneering practical solutions to make it happen.\nThe Scottish Wildlife Trust is Scotland’s leading nature conservation charity, representing over 40,000 members who care for wildlife and the environment. The Trust seeks to raise public awareness of threatened habitats and species and manages 120 reserves Scotland-wide.\nNo matter where you are in the UK, there's a Wildlife Trust protecting, championing and taking action for nature in your area. There are 47 Wildlife Trusts covering the UK, the Isle of Man and Alderney. Together, we manage over 2,200 diverse and beautiful nature reserves, covering more than 98,000 hectares. In England, 93% of people live within 6 miles of one of our reserves and we run marine conservation projects around the coast. Supported by 800,000 members, including 150,000 members in our junior branch, Wildlife Watch, we have a mission to create Living Landscapes and secure Living Seas: thriving areas of land and ocean rich in wildlife and delivering benefits for society. We touch the lives of millions of people, inspiring them to value wildlife and empowering them to act for nature."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b3cf70a4-e1e9-4464-a30c-04fdee1cc3e2>","<urn:uuid:771f5c6e-9ce8-4599-ac57-7b0ecd4fdbe6>"],"error":null}
{"question":"What killed many tender specimens in Overbecks garden during winter 1947?","answer":"The harsh winter of 1947 killed many of the tender specimens including the trees which made up the Eucalyptus Avenue on the east boundary of the site.","context":["Overbecks is an early C20 terraced garden in an outstanding natural location benefiting from a favourable microclimate which has enabled the development of a specialised collection of tender Mediterranean and Australasian plants. The subject of numerous articles in the horticultural press, Overbecks has become an influential garden in the late C20.\nThe site of Overbecks, originally known as Sharpitor after the local landmark Sharp Tor c 1km to the south, was undeveloped woodland when acquired by Albert Stumbles in the late C19. Sharpitor was one of the first houses to be built on the steeply sloping coastline south-west of Salcombe, and was initially let and subsequently sold to Edric Hopkins, who constructed a series of terraces on the east-facing slope below the house. Hopkins purchased additional land to the south which was laid out as a terraced tennis lawn, rocky dell and orchard. The property was sold in 1913 to Mr and Mrs George Vereker who continued to develop the gardens and rebuilt the house, which was used as a Red Cross convalescent home during the First World War. George Vereker died in 1924, but his widow continued to live at Sharpitor until 1928, when it was sold to Otto Overbeck, a scientist who had invented the 'Overbeck Rejuvenator', a machine which helped to make his fortune and enabled him to retire to garden in a favourable location. A fellow of the Zoological Society, Society of Arts and the Geological Society, and a well-known collector of natural history specimens, antiques, stamps and armour, Overbeck was also a committed supporter of youth organisations, boys' clubs being a particular object of his generosity. During his ownership areas such as the tennis lawn were converted to horticulture and the garden, which was regularly opened to the public, became 'renowned for its wealth of sub-tropical plants' (The Gazette 1937). Overbeck's obituary published in 1937 (ibid) also noted 'the far-famed eucalyptus avenue, citrus fruits, banana grove, and many shrubs of extreme rarity', while the greenhouse contained 'an unusual display of cacti'. Many of the Chusan palms, a particular feature of the garden, were planted by Overbeck in the 1930s. Under his will, the house, contents and gardens were left either to the National Trust or to Salcombe Urban District Council. Under the bequest, which was accepted by the National Trust, the house was to be used as a museum and youth hostel, the gardens opened to the public, and the name of the property changed to record the identity of the donor. From 1937 until 1964 the property was administered by one of Overbeck's friends, Lady Clementia Waring, while Overbeck's gardener, Ellis Manley, continued to work at the property until 1951. Wartime restrictions left the garden in poor condition, and the harsh winter of 1947 killed many of the more tender specimens including the trees which made up the Eucalyptus Avenue on the east boundary of the site. The work of restoration was begun by Edward Pilkington, who was appointed Head Gardener in 1950; it has been continued by Tony Murdoch from 1973 to his retirement in 1999, and by his successor Claire Gosling. Planting schemes for the Statue Garden, originally the tennis lawn and later Overbeck's 'formal perennial garden', have been developed in this period, as has a parterre decorated with citrus trees in pots adjacent to the greenhouse. Additional land was purchased to the north of drive leading to the property in 1997.\nLOCATION, AREA, BOUNDARIES, LANDFORM, SETTING\nOverbecks is situated 1km south-west of Salcombe on high ground to the south-west of South Sands and to the west of Splatcove Point. The 2.75ha site is bounded to the north by a steeply sloping wooded bank which falls away below the drive, while the west and short south boundaries are formed by stone walls and wooden fences which separate the property from woodland owned by the National Trust. To the east the boundary is formed by a track which serves for c 65m as a drive for The Anchorage, a property to the north-east, before continuing for c 150m as the Eucalyptus Avenue. The Avenue is separated from woodland on the slope below, now in separate ownership, by a post and wire fence. The site is located in an area of particular natural beauty, with fine views north across Salcombe Harbour towards Salcombe, and south-east across the estuary to Prawle Point and the sea.\nENTRANCES AND APPROACHES\nThe drive is adapted from an existing C19 track which ascends a steeply sloping zig-zag route from South Sands to high ground adjacent to Sharp Tor. A single, circular, rustic stone gate pier marks the north side of the entrance to the site adjacent to the drive leading south to The Anchorage. The drive climbs steeply west and is bounded to the south by a stone wall which retains grass banks and planting in the garden; to the north the ground drops steeply into a wooded valley, the slope being clothed with mixed deciduous trees including several mature sweet chestnuts. After c 150m the drive turns sharply east with a timber pedestrian gate set between circular rustic stone piers at the lower north end of the slope. It then ascends for c 80m between an avenue of Chusan palms (c 1930) underplanted with Crocosmia and Libertia, to a small carriage turning to the west of the house. A late C20 car park has been constructed c 25m west of the house. The former stables stand to the west of the carriage court, while the gardens are entered through an elaborate pair of Spanish-style timber doors decorated with metal ornaments dating from c 1930. Circular rustic stone gate piers support a trellised black-painted metal overthrow ornamented to each side by gilt metal coats of arms, with the name 'Overbecks' spelt out in white letters set on individual red squares. Within the gates a flight of sixteen tiled steps with late C20 serpentine wrought-iron hand rails designed by Alan Van der Steen descends through an avenue of Chusan palms to the level of the terrace to the east of the house.\nThe present house dates from 1913/14 and is of rendered two-storey construction under a slate roof with gabled wings projecting to the north-east and south-west of the principal facade. A single-storey lean-to timber and glass conservatory occupies the ground-floor space between the projecting wings and contains a collection of citrus plants and tender climbers. The house is lit by large mullioned plate-glass windows, and the wings have two-storey canted bay windows; the roof is ornamented by a glazed cupola above the north facade. The house is unlisted and is used as a museum and youth hostel.\nGARDENS AND PLEASURE GROUNDS\nThe gardens comprise two terraces on the east-facing slope below the house, with a further terrace lying below the present garden boundary, but within the registered site. Three terraces are located to the south of the house, while a more informal woodland garden occupies the slope below the north side of the house.\nThe upper terrace to the north and east of the house is retained by a wall of brick and stone construction with stone castellations which supported a pergola in the 1930s, but which today support planted pots during the summer. Late C20 wrought-iron gates c 10m east of the house lead to steps descending to the second terrace, while a semicircular bastion with a flag pole c 20m south-east of the house gives wide views across the garden and Salcombe Harbour. Planting on the terrace has a Mediterranean character which extends to the 'Museum Borders' surrounding a level rectangular lawn south-west of the house which is enclosed to the west and south by steep rocky banks. The grass slope above the west bank is planted with specimen trees and shrubs including Chusan palms and myrtle, and rises to the stone boundary wall in a corner of which a late C20 conical-roofed shelter has been constructed. The axis of the upper terrace continues south for c 30m as a gravel walk bordered by exotic herbaceous planting which rises south-west towards the Statue Garden, originally the Edwardian tennis lawn. Enclosed by a low stone wall to the west which retains a steep bank planted with palms, and by brick and stone pillars with linking metal supports set on low walls to the south, east and north, the garden has as its focus a bronze statue of a young girl, First Flight by Albert Bruce Joy (c 1900). To north and south four large rectangular herbaceous borders set in grass are lavishly planted in the summer with tender and exotic-looking plants in predominantly warm colours. A gravel path leads south to a small grassed enclosure planted with a young date palm, from which there is a view down on to a late C20 box-edged coloured gravel parterre which is decorated in summer by citrus trees in pots. To the north stand a large group of Japanese banana and a late C20 timber glasshouse.\nA further walk c 35m from the house flanked by Chusan palms, brick and metal pergola supports and a planting of Hydrangeas ascends west for c 30m towards a service gate. To the south lies the Rock Dell which appears to have been adapted from a former quarry and has a central bed planted with Cordyline and Phormium; surrounding planting includes Pittosporum. Stone steps at the south-west corner of the Rock Dell rise to the former orchard, now an east-facing grass slope laid out as a picnic area with seats placed against the stone west boundary wall and planted with specimen trees.\nThe second terrace below the house provides a north/south axis in the lower garden. Flanked by richly planted borders, the walk leads south c 50m to an arch in a stone wall which encloses a further irregularly shaped walled terraced garden. Within, a stone wall retains a grass walk to the west and beds containing specimen shrubs including a Magnolia cambelli which was planted in 1901 as a wall shrub and is now a famous spring feature of the garden. Other planting at the lower level of this garden includes a group of Japanese bananas which are said to have been grown in this situation since the late C19 (Tony Murdoch pers comm, 1998). A gate at the south-west corner of the walled garden leads to the glasshouse and parterre. To the east of the house the second terrace is planted with a group of Crinodendron hookerianum. The terrace leads north-west and enters a woodland garden with mature beech trees underplanted with a collection of young magnolias, before reaching the wooden pedestrian gate to the drive. A minor path partly lined with Chusan palms diverts north-east along a rocky spur to a viewpoint sheltered by a group of evergreen trees including oaks, Monterey cypress and pines.\nNorth-east of the walled garden a minor path descends south-east to a flight of stone steps which links the garden to the former Eucalyptus Avenue, which formed the third and lowest terrace, and which now (1998) lies outside the area open to the public. Entered through a timber gate, the walk extends c 150m south-west and south, and is flanked to the west by the east wall of the walled garden for c 50m. A semicircular recess c 20m from the north-east end of the wall contains a pool, and a line of regularly spaced Chusan palms is planted along the length of the wall. The Eucalyptus Avenue, in reality a single line of trees on the west side of the terrace, originally extended c 100m to the south of the wall. Some trees have regenerated since being cut back in 1947. In the early C20 the terrace allowed easterly views down a lightly wooded slope towards Salcombe Harbour, but now adjoins woodland which is in separate, private ownership.\nThere is no kitchen garden within the registered site. An area west-south-west of the house was formerly used as a kitchen garden (Tony Murdoch pers comm, 1998), but is no longer cultivated.\nEarly C20 staff cottages to the west of the house and service drive have associated gardens planted with early C20 trees and shrubs which are included in the registered site.\nGardening Illustrated (May 1937)\nThe Gazette, May 1937 [Otto Overbeck's obituary]\nGardeners' Chronicle, ii (18 November 1961), pp 380-1\nP M Synge, Gardens of Britain 1, (1977), pp 78-81\nB Cherry and N Pevsner, The Buildings of England: Devon (1989), p 709\nG S Thomas, Gardens of the National Trust (1979), pp 209-10\nThe Garden, (1985), pp 470-5\nOverbecks, gardens guide, (National Trust 1986)\nCountry Life, no 31 (4 August 1994), pp 54-7\nCountry Life Gardens, (Summer 1999), pp 50-9\nOS 25\" to 1 mile: 1st edition surveyed 1885, published 1886\n2nd edition revised 1905, published 1906\nCorrespondence and photographs c 1900-97 (National Trust and Overbecks Museum)\nDescription written: August 1998\nAmended: May 1999; May 2000\nRegister Inspector: JML\nEdited: July 2000"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:7a4d035f-ff70-4c50-bee1-a2d3a5c9f0eb>"],"error":null}
{"question":"How tall was the extinct Homo floresiensis species found in Indonesia?","answer":"Homo floresiensis stood just three feet tall, about the height of a modern human toddler. Nine skeletons of this species were found in Flores, Indonesia in 2003.","context":["Island life isn’t all sandy beaches and coconuts. Sometimes, it’s rough, with very little food or freshwater available, and if you can’t get off the island, you’d better adapt. That’s exactly what these 12 miniature species did over thousands of years due to scarcity of resources, eventually becoming smaller versions of their mainland relatives. They’re not all insanely adorable tiny animals that melt us into big piles of fawning goo, but they are fascinating, rare, and all too often endangered or extinct.\nLittle People of Flores\n(images via: wikipedia, science daily)\nCould a tiny sub-species of in the genus Homo have co-existed in Indonesia with humans as recent as 12,000 years ago? First dubbed a “hobbit-like human ancestor”, it was soon discovered that Homo floresiensis was in fact its own species, standing just three feet tall, about the height of a modern human toddler. Nine skeletons were found in Flores, Indonesia in 2003 and have been studied extensively since then, with some scientists still arguing that they are actually deformed Homo sapiens. The team that discovered H. floresiensis believe the species is an example of insular dwarfism, with their growth restricted by a limited choice of food on the island.\nPygmy Three-Toed Sloth\n(image via: bbc news)\nWhen it comes to sloths, opinions tend to be radically divided: some people think they’re adorable, while others find them absolutely terrifying. But the critically endangered pygmy three-toed sloth, found only on the tiny island of Isla Escudo de Veraguas near Panama, is a miniature version of its mainland relatives, and is especially cute when swimming – it almost looks like a fuzzy turtle!\nChannel Islands Pygmy Mammoth\n(image via: wikipedia)\nWhen you hear the word “mammoth”, you think of something epically huge. Not that the Channel Islands Pygmy Mammoth was a dainty little creature at 2,000 pounds, but it would still have been easily dwarfed by its 20,000-pound ancestor, the Columbian Mammoth. Remains of this species, which evolved to fit within the ecosystem of the now mostly-submerged Santa Rosae island off the coast of California, were first discovered in 1856.\n(images via: wikipedia)\nUnlike today’s pygmy elephants, which are subspecies of their own, prehistoric dwarf elephants evolved to be much smaller than modern elephants due to their insularity on islands around the world including Crete, Cyprus, Timor and the same island of Flores, Indonesia where pygmy human relatives were found. And unlike prehistoric dwarf mammoths, dwarf elephants really were small: the Cyprus dwarf elephant likely weighed around 440 pounds.\nChannel Islands Fox\n(image via: just chaos)\nAww, isn’t that a cute little kitten… oh… wait. It’s not a kitten at all. The Channel Islands Fox first evolved from the Gray Fox when they “rafted” over to the islands off the coast of California over 10,000 years ago and were faced with limited resources. They’re easy prey for eagles, being smaller than domestic house cats, and also highly susceptible to parasites and diseases brought over from the mainland.\n(image via: soham pablo)\nPygmy hippos are about the same size as pigs – though technically, hippos are more closely related to whales and dolphins than to any of their fellow land animals. Semi-aquatic vegetarians, these miniature mammals are difficult to study because they’re nocturnal and very shy. Only about 3,000 remain in the wild, mostly in Liberia.\n(image via: wikipedia)\nThe Bali Tiger may have been more comparable in size to leopards than to other tiger subspecies, but they were no less fierce. Sadly, these animals disappeared by the middle of the 20th century, though scientists believe there were never very many of them in the first place. These dwarf tigers were found exclusively on the island of Bali where they were hunted to extinction due to perceived threats and also the desire for jewelry made from their teeth and claws.\nCozumel Island Raccoon\n(image via: animalesextincion.es)\nWeighing just about 8-9 pounds, Cozumel Island racoons look exactly like their mainland relatives except for their diminutive size, the black bands on their throats and their golden yellow tails. They live on Cozumel Island off the coast of the Yucutan Peninsula in Mexico, and less than 300 remain. The Dwarf Coati, a relative of the raccoon, and a species of dwarf gray fox are also found on the island.\nBalearic Island Cave Goat\n(image via: mongabay)\nThe extinct Balearic Island Cave Goat wasn’t just a shorty at only 19.5” tall – its isolation on the rocky, nutrient-poor islands in the Mediterranean caused it to develop some even more unusual characteristics. Like crocodiles, this goat was able to grow at flexible rates, halting the growth process when food was unavailable. As far as scientists know, this goat was the only mammal ever to adapt in this way, and it probably helped the goat survive for five million years before being driven into extinction by human hunters.\nMindoro Dwarf Buffalo\n(image via: edmond valerio)\nThere are so few Mindoro Dwarf Buffalo left, it’s rare for anyone to spot more than a solitary individual. Originally found all over the island of Mindoro in the Philippines, its range has been dramatically reduced by human civilization, hunting and logging. In fact, sightings of this mini water buffalo are so unusual that scientists know very little about its ecology. After being declared a critically endangered species, the Mindoro buffalo population has experienced a slight but very encouraging uptick.\nBernissartia – Tiny Crocodiles\n(image via: wikimedia commons)\nImagine a cute “baby” crocodile that never grows up. That’s basically what Bernissartia, a prehistoric reptile from the Early Cretacious period around 130 million years ago, would seem like to us. Smaller than a house cat, Bernissartia looked just like modern-day crocodiles but had jaws more suited to catching fish than dragging a full-grown man underwater. It would have stood at sharp contrast to the nightmarishly enormous crocs of the day, like Sarcosuchus.\n(image via: wikimedia commons)\nKey Deer may not be around too much longer. Native only to the Florida Keys, this offshoot of white-tailed deer tops out at about 75 pounds and the antlers of males bear a signature white, velvety coating. Because of human encroachment, their habitat has been shrunken to a handful of lesser populated keys, and they swim from one island to another in search of fresh water."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:6762cbc6-b76d-45b6-8d82-b29f50b2a68d>"],"error":null}
{"question":"What is the typical recovery timeline after meniscal repair surgery?","answer":"The recovery timeline involves non-weight bearing for 6 weeks with ROM from 0-90° in a post-operative brace. At 6-8 weeks, the brace is removed and full weight-bearing with unrestricted motion begins. At 3 months, patients can return to full normal activities except competitive sports. At 6 months, unrestricted activity and return to competitive sports is allowed.","context":["synonyms:meniscal repair, medial meniscal repair, lateral meniscal repair\nMeniscal Repair CPT\nMeniscal Repair Indications\n- Ideal tear for repair is a longitudinal tearwithin the peripheral 3mm with a length of 1-2cm.\n- Bucket handle meniscal tear\n- Tears within 3-4mm of the meniscocapsular junction.\n- Horizontal cleavage tear in a young patient\n- Posterior Meniscal Root tears\n- General Repair guidelines: active patients (typically aged <50 years) with no significant osteoarthritis (Outerbridge grade 3 or 4), joint-space narrowing, or malalignment.\nMeniscal Repair Contraindications\nMeniscal Repair Alternatives\n- Nonoperative managment\n- Partial Meniscectomy\n- Meniscal Transplantation\nMeniscal Repair Pre-op Planning / Special Considerations\n- Lateral repairs do better than medial tears. PHLM has rich blood supply and does especially well with repair. Flap tears not repairable unless PHLM.\n- Results of meniscal repairs are better in done within 8 weeks of injury.\n- rasp tear surfaces to bleeding surfaces\n- Load to failure of various repair systems: Barber FA, Arthroscopy 2000,16:613).\nLateral Meniscal Tear Inside-Out Technique\n- CPT code = 29882(arthroscopy knee with medial OR lateral meniscus repair); 29883 (medial AND lateral repair)\n- Risks = peroneal nerve, popliteal vessels\n- posterolateral incision along the posterior margin of the IT band extended distal @ 3cm.\n- Dissection between anterior border of biceps and posterior margin of IT band.\n- Blunt dissection between arcuate complex and capsule anteriorly and the lateral gastroc posteriorly.\n- Popliteal retractor placed ensuring protecting of peroneal nerve.\n- Rasp tear surfaces to bleeding edges.\n- 2-0 Ethibond on tapered needle. Veritcal mattress is best, placed 4-5mm intervals inserted from both upper and lower surfaces\nMedial Meniscal Tear Inside-out Technique\n- Risks = saphenous nerve and vein, popliteal vessels\n- 3-6cm longitudinal incision in soft spot between posterior border of the superficial MCL and posterior oblique ligament.\n- Majority of incision is below joint line.\n- Ensure saphenous vein is not cut.\n- Dissect down to sartorial fascia.\n- Incise sartorial fascia anterior to the sartorius and retract pes tendons posteriorly.\n- Avoid excessive retraction on pes anserinus and sartorial branch of saphenous nerve which will lie posteriorly with knee flexed.\n- Palpate direct head of semimembranosus attachment to posterior tibial tubercle.\n- May need to release attachement if semimembranosus is too tight.\n- Develop plane between posterolmedial capsule anteriorly, semimembranosus inferiorly and medial head of gastroc posteriorly.\n- Place popliteal retractor just behind posterior capsule.\n- Rasp tear surfaces to bleeding edges.\n- 2-0 Ethibond on tapered needle. Veritcal mattress is best, placed 4-5mm intervals inserted from both upper and lower surfaces. Sutures are best placed through the contralateral portal while viewing from the ipsilateral portal.\nPosteriorMeniscal Root Repair\n- Positioned with the knee in 90° of flexion.\n- Medial and lateral parapatellar arthroscopic portals created.\n- Consider accessory posteromedial or posterolateral portals\n- Place guidepin from anteromedial aspect of the tibia exciting at the anatomic tibial attachment of the medial or lateral posterior meniscal root using an anterior cruciate ligament aiming device\n- Confirm guide pin position with direct arthroscopic visualization or fluoroscopy\n- Drill/ream 5mm transtibial tunnel over the guide pin.\n- Use accessory portal to place an arthroscopic grasper to firmly hold the torn root and to more effectively position it toward the suture passer.\n- Pass 2 No. 2 nonabsorbable sutures in a superior-to-inferior direction through the substance of the meniscal root, shuttled down the transtibial tunnel, and secured over the anteromedial tibial cortex with a cortical button for fixation\n- LaPrade RF, JAAOS 2015:23:71-76\nMeniscal Repair All-inside Techniques\n- All-inside techniques depend on device used. See manufactures technique.\n- Smith and Nephew Fast-Fix\n- USS Sports Meniscal Stapler XLS\n- Depuy RapidLoc meniscal repair\nMeniscal Repair Complications\n- Overall complication rate = 1.8% (Small NC, Arthroscopy 1988;3:215)\n- Failure of repair: approximately 25%\n- DVT: 9.9%, proximal DVT rate = 2.1% (Ilahi OA, Arthroscopy, 2005;21:727)\n- Stiffness / Arthrofibrosis\n- Chondral Injury / Arthritis\n- NVI (saphenous neuralgia medially; common peroneal nerve/popliteal artery laterally)\n- Fluid Extravastion / Compartment Syndrome\n- Complex Regional Pain Syndrome: rare\n- Synovial fistula\nMeniscal Repair Follow-up care\n- Non-weight bearing for 6 weeks to allow meniscal healing. Generally place in post-operative brace with ROM from 0-90°.\n- 6-8 weeks post-op: removed brace, begin full weight-bearing with unrestricted motion.\n- 3months: return to full normal activities, except competitive sports.\n- 6months: unrestricted activity. Return to competitive sports.\nMeniscal Repair Outcomes\n- All-inside meniscal repair success rate is between 80% and 90%\n- 89% heal (O'shea JJ, AJSM 2003;31:216).\n- 87% asymptomatic at 33 months post-op (Noyes FR, Arthroscopy 2000;16:822).\nMeniscal Repair Review References"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:edfcf514-9a3a-4466-915d-34df9b096064>"],"error":null}
{"question":"How do computer forensics specialists ensure the preservation of digital evidence during an investigation?","answer":"Computer forensics specialists take several critical steps to preserve digital evidence: They secure the subject computer system from any modification, damage, data corruption, or virus introduction. They ensure no evidence is deleted or compromised during examination. They protect extracted evidence from mechanical or electromagnetic damage. They establish and maintain a chain of custody, document all procedures for each case, and handle any inadvertently discovered client-attorney information ethically and legally.","context":["Computer Forensics and Data Recovery\nComputer forensics and Data Recovery is the science of analyzing a computer system hard disk using forensically sound methods and tools that have been tested and have had publications released such as the Department of Justice, NIST, Homeland Security and other approved forensic associations. The computer examination and analysis strategies might vary from case to case depending upon the evidence that is being attempted to be discovered to establish legal proof of the examination for legal cases. Computer Forensics and Data recovery can be utilized in a wide range of computer system criminal activity or abuse, consisting of but not limited to theft of data, theft of or violation of copyrights, and fraudulence. Computer experts could draw on a collection of approaches for uncovering data that resides in a computer system, and recovering deleted, secured, or damaged file details.\nBENEFITS OF PROFESSIONAL Computer system Forensics and Data Recovery\nThe unbiased computer Forensics professional that helps during a legal case will have experience on a large range of computer and software applications. This is constantly beneficial when your case has hardware and software applications with which this specialist is directly knowledgeable. Key computer components and software application execution is commonly comparable from one technology to an another, in which experience in one application or operating system area is frequently quickly transferable to a brand-new technology in a computer operating system.\nUnlike paper proof, computer system evidence can typically exist in numerous types, with earlier variations still easily accessible on a computer system disk. Understanding the possibility of their existence, also alternative formats of the same information can be uncovered. The discovery procedure could be offered well by a knowledgeable specialist recognizing additional opportunities that could be requested as perhaps relevant proof. Moreover, throughout on-site premises assessments, for situations where computer disks are not actually taken or forensically copied (see listed below), the forensics specialist could more quickly identify areas to look, indications to look for, and added information resources for relevant evidence. These could take the form of earlier versions of data documents (eg. memos, spreadsheets) that still exist on the computer’s disk or on backup media, or differently formatted versions of data, either developed or addressed by various other application software programs (eg. data processing, spreadsheet, email, timeline, organizing, or graphic).\nPreservation of data from changing is critical in computer forensic examinations. An experienced computer forensics professional will ensure that a subject computer system is carefully handled, documented to ensure that:\n- No feasible evidence is deleted, changed, or otherwise endangered by the procedures used to examine the computer system.\n- No feasible computer virus is presented to a subject computer system throughout the examination process.\n- Extracted and potentially relevant evidence is properly handled and safeguarded from later mechanical or electromagnetic damage.\n- A proceeding chain of custody is developed and kept.\n- Company procedures are documented for each case.\n- Any client-attorney details that is inadvertently discovered during a forensic examination is ethically and legitimately not disclosed.\nACTIONS TAKEN BY COMPUTER SYSTEM FORENSICS SPECIALISTS\nThe computer forensics professional will certainly take a number of mindful steps to determine and attempt to recover feasible evidence that could existing on a subject computer system:.\n- Secures the subject computer system during the forensic examination from any type of possible modification, damage, data corruption, or infection intro.\n- Discovers all files on the subject system. This includes existing typical documents, removed yet continuing to be archived documents, concealed data, password-protected documents,.and encrypted files.\n- Recovers all (or as much as possible) of discovered deleted files.\n- Reveals (to the extent possible) the contents of hidden files as well as temporary or swap files used by both the application programs and the operating system.\n- Accesses (if possible and if legally appropriate) the contents of protected or encrypted files.\n- Analyzes all possibly relevant data found in special (and typically inaccessible) areas of a disk. This includes but is not limited to what is called ‘unallocated’ space on a disk (currently unused, but possibly the repository of previous data that is relevant evidence), as well as ‘slack’ space in a file (the remnant area at the end of a file, in the last assigned disk cluster, that is unused by current file data, but once again may be a possible site for previously created and relevant evidence).\n- Prints out an overall analysis of the subject computer system, as well as a listing of all possibly relevant files and discovered file data. Further, provides an opinion of the system layout, the file structures discovered, any discovered data and authorship information, any attempts to hide, delete, protect, encrypt information, and anything else that has been discovered and appears to be relevant to the overall computer system examination.\n- Provides expert consultation and/or testimony, as required.\nWHO CAN USE COMPUTER FORENSIC EVIDENCE?\nMany types of criminal and civil proceedings can and do make use of evidence revealed by computer forensics specialists:\n- Criminal Prosecutors use computer evidence in a variety of crimes where incriminating documents can be found: homicides, financial fraud, drug and embezzlement record-keeping, and child pornography.\n- Civil litigations can readily make use of personal and business records found on computer systems that bear on: fraud, divorce, discrimination, and harassment cases.\n- Insurance Companies may be able to mitigate costs by using discovered computer evidence of possible fraud in accident, arson, and workman’s compensation cases.\n- Corporations often hire computer forensics specialists to ascertain evidence relating to: sexual harassment, embezzlement, theft or misappropriation of trade secrets and other internal/confidential information.\n- Law Enforcement Officials frequently require assistance in pre-search warrant preparations and post-seizure handling of the computer equipment.\n- Individuals sometimes hire computer forensics specialists in support of possible claims of: wrongful termination, sexual harassment, or age discrimination."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c82ba33d-be8b-44ed-b168-265775e028b1>"],"error":null}
{"question":"How do Vitamin B-12 and Vitamin D deficiencies compare in terms of their impact on mental health and mood?","answer":"Both vitamin deficiencies can affect mental health, but in different ways. Vitamin B-12 deficiency, if left untreated, can lead to depression and trouble concentrating, and in severe cases can cause dementia and nerve damage. Vitamin D deficiency similarly impacts mental health, as it is involved in the production of neurotransmitters like serotonin that regulate mood and emotions. People with low vitamin D levels may experience depression and mood swings.","context":["Graying hair, hair that loses its natural pigmentation and turns white or gray, is a natural process in aging. However, it can also be affected by your diet. Vitamin B-12 is an essential vitamin, and while gray hair is not one of the primary symptoms of low vitamin B-12 in your system, it can cause graying hair. Your hair can turn gray for other reasons, including environmental factors as well as the overall health and age of your body.\nVideo of the Day\nWhat Causes Gray Hair\nAll hair starts off as white and is then colored by melanin, a natural pigment, which determines the color of your hair. Melanocytes, the pigment cells near your hair follicles, inject that melanin into your hair as it grows, coloring it. As you age, melanocyte activity decreases, leading to less pigment in your hair and, gradually, white hair. Genetics plays a large part in when and how quickly your hair turns gray, although your chances of gray hair increase by 10 to 20 percent every decade after you turn 30. Other causes of graying hair including because of hormones and age, as well as external factors, such as exposure to chemicals, your daily climate and environmental toxins, such as pollutants.\nVitamin B-12 Anemia\nVitamin B-12 anemia, also known as pernicious anemia, is when you have a low red blood cell count because your body is lacking a substance that helps you absorb enough vitamin B-12. One symptom of vitamin B-12 anemia is gray hair; other symptoms include diarrhea or constipation, lack of energy, loss of appetite, pale skin, trouble concentrating, being regularly out of breath, bleeding gums and a swollen tongue. If left untreated, vitamin B-12 anemia can cause nerve damage, leading to dementia in severe cases, as well as poor balance, tingling in the hands and feet, and depression.\nImportance of Vitamin B-12\nVitamin B-12 is vital for producing red blood cells, and it is also important to your metabolism and keeping your central nervous system functioning and healthy. As a water-soluble vitamin, it needs to be replenished regularly although your body can store it in the liver. Your body flushes out unused vitamin B-12 through urination. Low vitamin B-12 levels can result from aging -- people over 50 often lose their ability to absorb dietary vitamin B-12 -- as well as from gastrointestinal surgery. Certain digestive disorders, such as celiac disease and Crohn’s disease, can also cause poor vitamin B-12 absorption. People who follow a vegetarian or vegan diet are at risk of vitamin B-12 deficiency because most vitamin B-12 is available in animal food sources.\nSources of Vitamin B-12\nAccording to MedlinePlus, the best way to meet your vitamin B-12 needs is to eat a variety of animal products. Vitamin B-12 is found in high quantities in liver and other organ meats and shellfish, especially clams. It can also be found in varying amounts in all red meats, poultry, eggs, milk and dairy products. Some foods, such as cereals and grain products, are fortified with vitamin B-12. However, in general, your body absorbs vitamin B-12 from animal sources much better than from plant-based foods. If you choose to take a vitamin B-12 supplement, your body absorbs it better when it is taken along with other B vitamins, such as niacin, vitamin B-6 and riboflavin.","Vitamin D is an essential nutrient that is crucial for our overall health and well-being. It plays a vital role in maintaining strong bones, boosting immunity, regulating the absorption of calcium and phosphorus, and protecting against diseases. Despite its importance, many people suffer from vitamin D deficiency, which can lead to a range of health problems. In this article, we will discuss the symptoms, causes, and treatments of vitamin D deficiency.\nWhy is Vitamin D so Important?\nVitamin D is a crucial nutrient for maintaining good health and well-being. Despite its importance, many people are deficient in this nutrient, which can lead to a range of health problems. In this article, we will explore why vitamin D is so important and what role it plays in our bodies.\nRegulates Calcium and Phosphorus Absorption\nOne of the most important functions of vitamin D is to regulate the absorption of calcium and phosphorus, two minerals that are essential for strong bones. Vitamin D helps the body absorb these minerals from the gut and distribute them to the bones, where they are stored. If the body doesn’t get enough vitamin D, calcium and phosphorus levels may drop, leading to weak and brittle bones, a condition known as osteomalacia.\nBoosts Immune System\nVitamin D plays a crucial role in boosting the immune system, helping the body fight off infections and diseases. The nutrient activates the immune system’s T-cells, which are responsible for recognizing and destroying harmful pathogens. It also helps regulate the production of cytokines, proteins that help coordinate the immune response.\nReduces Risk of Chronic Diseases\nStudies have shown that vitamin D may help reduce the risk of chronic diseases, such as heart disease, diabetes, and certain types of cancer. The nutrient is believed to play a role in regulating cell growth and division, and may also help protect against oxidative stress and inflammation, two factors that contribute to the development of chronic diseases.\nImproves Mood and Mental Health\nVitamin D is also important for maintaining good mental health and a positive mood. The nutrient is involved in the production of neurotransmitters, such as serotonin, which play a role in regulating mood and emotions. Low levels of vitamin D have been linked to depression, anxiety, and other mental health conditions.\nSymptoms of Vitamin D Deficiency\nVitamin D deficiency can present itself in a number of ways, and the symptoms may not be obvious at first. Some of the most common signs of vitamin D deficiency include:\n- Fatigue and tiredness\n- Muscle weakness\n- Pain in bones and muscles\n- Depression and mood swings\n- Decreased bone density\n- Impaired wound healing\n- Increased risk of infections\nCauses of Vitamin D Deficiency\nThere are several factors that can contribute to vitamin D deficiency, including:\n- Lack of sun exposure: Vitamin D is produced by the body when the skin is exposed to the sun’s UV rays. If you spend most of your time indoors or live in a place with limited sun exposure, you may be at risk of vitamin D deficiency.\n- Dark skin: Melanin, the pigment that gives skin its color, reduces the skin’s ability to produce vitamin D from sun exposure. This means that people with darker skin are at a higher risk of deficiency.\n- Aging: As we age, our skin becomes less efficient at producing vitamin D, which can lead to a deficiency.\n- Obesity: Vitamin D is stored in fat, and obese individuals may have lower levels of vitamin D in their body.\n- Malabsorption: Certain medical conditions, such as Crohn’s disease and celiac disease, can interfere with the body’s ability to absorb vitamin D, leading to a deficiency.\nTreatments for Vitamin D Deficiency\nThe treatment for vitamin D deficiency depends on the severity of the deficiency and the underlying cause. Here are some of the most common treatments:\n- Sun exposure: Spending time in the sun is the best way to get vitamin D. When your skin is exposed to the sun’s UV rays, it produces vitamin D. Aim to spend 10-15 minutes in the sun each day, especially between 10 AM and 3 PM, when the sun’s rays are the strongest.\n- Supplements: If you can’t get enough vitamin D from food and sun exposure, consider taking a supplement. Vitamin D supplements are available in different forms, including tablets, liquids, and gummies. Your doctor can recommend a supplement that’s right for you.\n- Diet: Fatty fish, such as salmon, mackerel, and tuna, are good sources of vitamin D, as are egg yolks and mushrooms. Some dairy products, such as milk and yogurt, are fortified with vitamin D, and can help increase your levels of the nutrient.\n- Fortified foods: Many foods, such as breakfast cereals and dairy products, are fortified with vitamin D, which can help increase your levels of the nutrient.\nIn conclusion, vitamin D deficiency can lead to a range of health problems, including bone weakness, fatigue, and depression. If you suspect you have a deficiency, it’s important to speak with your doctor, who can diagnose and treat the problem. By following a healthy diet, getting regular sun exposure, and taking vitamin D supplements, you can help keep your levels of the nutrient in check and maintain good health.\n- What are the symptoms of Vitamin D deficiency?\nThe symptoms of Vitamin D deficiency can include fatigue, muscle weakness, joint and muscle pain, and depression. In severe cases, Vitamin D deficiency can also lead to osteoporosis and increased risk of fractures.\n- What causes Vitamin D deficiency?\nVitamin D deficiency can be caused by a number of factors, including a lack of exposure to sunlight, a diet that is low in Vitamin D, certain medical conditions that limit the body’s ability to absorb Vitamin D, and certain medications that can interfere with Vitamin D metabolism.\n- How is Vitamin D deficiency treated?\nVitamin D deficiency is usually treated with supplements, either in the form of oral tablets or injections. The recommended daily dose and duration of treatment will depend on the individual’s unique needs and will be determined by their healthcare provider. It is also important for individuals with Vitamin D deficiency to make lifestyle changes, such as increasing their exposure to sunlight and consuming a diet that is rich in Vitamin D, to help prevent future deficiencies."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:10c65d25-c5f0-4ced-bdf6-fb125e5a9a37>","<urn:uuid:7b91b940-5deb-488e-a808-a8af0db6f6c0>"],"error":null}
{"question":"How do the quantum navigation mechanisms of European robins compare to the magnetic navigation abilities of loggerhead turtles?","answer":"European robins and loggerhead turtles use different mechanisms for magnetic navigation. Robins appear to use photoreceptors in their retina containing radical pairs - two molecules with single electrons that form correlated spin states sensitive to magnetic fields. This quantum mechanism may allow them to literally see magnetic field lines, though there's a challenge as the coherence between electron states would need to be sustained longer than current lab experiments have achieved. In contrast, loggerhead turtles use the Earth's magnetic field intensity to determine latitude and can detect magnetic conditions at different longitudes to orient themselves. They demonstrate this ability by swimming in different directions when exposed to magnetic conditions mimicking different locations in the Atlantic Ocean, essentially parsing magnetic vectors to determine their location relative to their destination. This suggests turtles have an internal magnetic map rather than using quantum mechanisms like the robins.","context":["In a real sense, everything is quantum mechanical: matter and interactions are governed by the rules of quantum physics. True, we haven't figured out how gravity fits in yet, but the structure of atoms, nuclei, molecules, and solids—along with the characteristics of light—are best described by quantum mechanics. However, we don't need to take the unique features of quantum mechanics into consideration when modeling many systems, including most in chemistry or biology.\nNevertheless, it is possible life has evolved to exploit some quantum phenomena, including coherence and tunneling. In a new Nature Physics review article, Neill Lambert and colleagues examined the evidence in favor of quantum biological phenomena in photosynthesis, photoreception, magnetic sensation, and even our sense of smell. They conclude that the evidence is ambiguous: compared with other biochemical processes, any uniquely quantum influences appear small.\nFor something to be considered fully quantum, the researchers used the criterion that it could not be fully described using heuristic models. (The authors refer to such heuristic depictions as \"classical.\" Though that's common, it's also misleading: talking about chemical bonds, electron transport, and the like isn't exactly the language of classical, Newtonian physics.) By contrast, quantum mechanics requires the use of the quantum state, the physical and mathematical description of a system that encodes the probability of the outcomes of measurements.\nThese states may be superpositions: combinations of mutually exclusive measurement outcomes, such as perpendicular polarizations of light, or spin orientations of electrons in atoms. Quantum information and computing are based on the preparation, manipulation, and entanglement of quantum states, but these generally require special conditions, including far lower temperatures than organisms can withstand. The present paper concerned itself with examining if life can get around those problems, exploiting quantum physics for evolutionary gain.\nTracing back along the food web, most of the energy organisms need to survive originates with the Sun. Green plants, cyanobacteria (also known as blue-green algae), and other organisms capture photons and use them to drive the chemical reactions of metabolism. Photosynthesis is incredibly efficient: nearly 100 percent of the energy from the photons absorbed by the photosynthetic machinery is transferred to the chemical reaction center.\nA set of chemistry experiments that began in 2007 showed that this efficiency may be due to quantum mechanics. The data demonstrated coherence between the electrons in the various pigment molecules during the transfer process, meaning that the states of the electrons were coordinated and they acted as a single system, even though they resided in different atoms. The clearest results were obtained at 77 Kelvin, but follow-up experiments showed the coherence could also be present at room temperature. Additionally, complicated theoretical models of the energy transport process seem to support the hypothesis that coherence plays a role in photosynthesis.\nAt the present time, no one has observed quantum coherence in a living organism. Additionally, it's not clear whether the boost that should come from exploiting quantum states would be sufficient to explain the efficiency of photosynthesis. After all, it's one thing to observe this phenomenon under lab conditions, it's another to demonstrate it in a living organism, and yet another to show that quantum coherence is the reason for the remarkable efficiency. Finally, as always in evolution, it's important to remember that a particular feature may be adaptive but may have evolved for other reasons: perhaps efficient photosynthesis was the side effect of some other adaptation.\nHowever, if coherence can be shown to exist in these biological systems at room temperature, then it's worth asking why it's there and how it might provide an evolutionary advantage. One possible explanation is that each pigment molecule experiences a lot of \"noise,\" or random jiggling inside the cell. By exploiting quantum coherence, these random fluctuations can be canceled out during the electron transfer, meaning fewer photons are lost to photosynthesis.\nMagnetic navigation in birds\nMany migratory species use Earth's magnetic field for navigation, including a number of birds. Our understanding of how animals sense magnetic fields—magnetoreception—is still lacking some details, and not all species appear to use the same mechanisms. The authors of the review noted that European robins, along with a handful of other species, seem to navigate using photoreceptors: cells in the retina that measure light intensity. The behavior of these cells can be disrupted using magnetic fields, suggesting there may be a dual dependence on light and magnetism for navigation.\nThe photoreceptors contain radical pairs: two molecules bound to each other that have single electrons in their outer layers. When they are correlated, the spins of these electrons form a state similar to one used in quantum information theory and computing.\nIn the radical pair model of magnetic navigation, an incoming photon induces one of the electrons to undergo a transition between quantum states. Since it is correlated with the other electron, this induces a second transition, which sends a signal through the bird's nervous system. Since the spin states are sensitive to external magnetic fields (useful for navigation), they can also be disrupted by laboratory fields.\nEven though the radical pair concept would explain all the aspects of navigation in some species, there's a major problem with this model. As the review points out, coherence between the electrons' states would need to be sustained for a relatively long time—longer than any current lab experiments have achieved. The very advantage of the model may end up making it untenable, unless researchers can figure out how the photoreceptor cells stabilize the quantum states for extended periods.\nTunneling our senses\nA less finicky phenomenon based on quantum states is tunneling, the process by which a particle (or some more abstract quantum system) can pass through a barrier between states—a transition that would be forbidden in classical terms. Tunneling is used in many applications, including some types of diodes, where electrons pass from one side of a junction to another. Certain chemical reactions in biology appear to depend on the tunneling of electrons or protons over distances equivalent to dozens of atoms.\nExperiments indicate that tunneling plays a part in photosynthesis and certain enzyme interactions, including possibly the sense of smell. In that case, it's not simply the size and shape of the molecules we perceive as odorants that trigger a smell: it's also the transfer of an electron from the odorant to the receptor in the nose via tunneling.\nFinally, the authors discussed the possibility that photoreceptor cells used to sense light could depend on quantum coherence. In this scenario, light triggers a very rapid molecular change in a protein-linked molecule in the retina, which then induces a secondary change in a protein. The speed and efficiency of these processes hints at a possible underlying quantum effect, but experimental evidence to support this has not been obtained yet.\nAn intriguing aspect of all of these possibilities is that perhaps evolution has figured out a better way of performing tricky quantum manipulations than we have. In a sense, that's not surprising: life has had a long time to evolve photosynthesis, photoreception, and navigation, while our understanding of quantum mechanics just began in the 1920s and '30s.\nIt may also turn out that the phenomena described above don't really rely heavily on the quantum state after all, since evidence is sketchy at present. Nevertheless, the hints are there: we may be at the point where we can test if life has solved the problem of manipulating quantum states, meaning quantum biology could be a new field of study in this century.","Back in January, science news was abuzz with reports that the lowly dung-beetle—shellacked trundler of balled-up excreta, stuck with one of nature’s least glamorous jobs—used a majestic method to find its way around: the Milky Way. The critters had already been shown to exploit the polarized light of the moon to orient and move their smelly cargo in straight lines. But researchers knew there was a back-up mechanism, since on nights when the moon rose late, the insects’ sense of direction was still pretty good. A team from Lund University in Sweden tested their subjects under the night sky of the Kalahari Desert and in the controlled environment of a planetarium. In both settings, beetles with a view of the burning band of the galaxy moved competently, while those which were offered only the brightest stars or none at all were lost. “It’s corny, but it’s a highway in the sky, a great big pathway: the Milky Way,” one of the researchers told The New Yorker.\nThe critters had already been shown to exploit the polarized light of the moon to orient and move their smelly cargo in straight lines.\nWe may pride ourselves on our ingenious means of transportation, but animals themselves are capable of remarkable feats of swimming, scuttling, and flying, often with pinpoint accuracy. Monarch butterflies orient by the sun to travel the thousands of miles between the northern United States and Mexico; seals appear to guide themselves through the seas by a lone star. While animals’ motives for moving around are simple—to breed and feed—the mechanisms by which they do it are not. Animals possess an exceptional array of compasses, clocks, distance-sensing techniques, and internal maps. The examples below are guided by the light of the 2012 book Nature’s Compass: The Mysteries of Animal Navigation, by James and Carol Gould.\nThe Sugar Rush of Bacteria\nTo move toward a food source, bacteria compare the concentration of chemicals—usually sugars—in one place over another, and move toward the area of higher concentration. They do so through a time-lapse technique. The microbe stops every few seconds and charges off in a random direction, usually with the aid of a flagellum, a microscopic tentacle, to propel it. It will “taste” the spot it finds itself in; special proteins in the cell walls then release a flood of charged ions into the cell according to the concentration of chemicals, and control whether the bacterium moves or stays still. If the new location has richer pickings than the last, the microorganism will continue in that direction for longer before trying out something new. And the quicker the concentration is increasing, the longer the little critter will wait before trying its luck again.\nThe Sex Wiggle of the Bermuda Fireworm\nGetting from place to place is as much about when as where. Perhaps the best display of chronological bravura is the luminescent Bermuda fireworm, which wriggles up from the mud to the surface of the ocean to mate in a burst of light, once every lunar cycle—to be more precise, on the third evening after the full moon, 57 minutes after sunset. The little annelids appear to have at least three timepieces (or maybe one that performs all these functions): a lunar clock to pick the right day; a daily clock to judge when the sun sets, in case the day is cloudy; and a stopwatch to figure out the number of minutes after sunset and to pinpoint “the consensual moment,” as the Goulds call it.\nThe March of the Cataglyphis Ant\nWhile ants in friendly climates can rely on scent trails or landmarks to find their way back to a burrow after scavenging for food, the hot sun, sharp winds, and parched sands of the Sahara make that strategy unworkable for the desert-dwelling Cataglyphis ant. Yet, no matter how tortuous the route to find food, these creatures almost unfailingly make it back home. A team of German and Swiss researchers used a novel and ghoulish method to figure out how. They kidnapped ants after they had found a food source, sticking pig bristles to the legs of some of the insects, and amputating part of the legs of others. When the unfortunate abductees were released, those with “stilts” overshot their nests, and those with “stumps” fell short. It seems the ants had been using an “odometer” to keep tally of their steps on the outward journey—probably using proprioreceptors, nerve cells wedged in parts of the ants’ anatomy that register their movements. This odometer allows the insects to perform what is called “path integration:” keeping track of the direction and distance travelled, so they know where they are relative to the starting point at all times. Once the length of the ants’ stride changed, their calculation of the route home was no longer accurate. But when the surgically enhanced ants were returned to their nest and then set out to forage the next day, they counted the steps accurately both ways and made it home just fine.\nThe Dance of the Honeybees\nBuzzing back and forth to the hive with bundles of pollen, honeybees travel up to five miles per trip and make up to 500 trips a day. When they find a rich source of food, they perform a “waggle dance” back at the hive: a figure eight turned on its side that encodes directions to the feast for the other bees.\nThe further away the food, the longer the bee will dance; each waggle communicates a unit of distance, with the length depending on the subspecies. The tilt of the line running through the two halves of the figure eight corresponds to the angle of the food relative to the direction of the sun. Baby bees can learn that the sun rotates clockwise (in the northern hemisphere) or counter-clockwise (in the southern hemisphere), and compensate for how far it will have moved across the sky in the time it takes to get back home to spread the news. When the sun is obscured, bees appear able to see the unique pattern of how light is polarized in the sky1 to determine where the sun is.\nWhen they leave the hive, bees keep track of distance by measuring “visual flow,” the rate of movement of textured surfaces beneath them. This seems to let them determine their position by way of dead reckoning, in which the direction and speed of movement away from a known location lets them plot a position. Bees may also be able to form cognitive maps—internal representations of a local area, rather than a set of orienteering instructions.\nThe further away the food, the longer the bee will dance; each waggle communicates a unit of distance.\nIn an experiment, James Gould trained bees to travel to a feeder station in a boat. When they got back to the hive, the scouts danced out the location. Bees hate to fly over water. When the dance pointed to a spot in the middle of a lake, none of the other bees could be convinced to follow it—but when the boat moved to the far shore, the recruits were happier to home in on it. One interpretation of the results is that the reluctant insects compared where the dance pointed them to their own maps of the area, and figured out in advance that it was surrounded by water. But when the dance pointed them somewhere close to shore, they were prepared to check it out.\nThe Brainy Navigation of Rats\nRats appear capable of creating maps of their environment. Let rats loose in a maze and they will learn its layout, even without the incentives of hidden food and punishment. Remove barriers to a reward and rats will know to take a shortcut. Take a rat out of a labyrinth with an electrified section, and days later it will remember to scuttle to the safe side. Both male and female rats methodically explore the eight arms of a spoke-shaped maze, although female rats prefer to use visual marks to identify which parts have been explored, while male rats tend to rely on recalling the angles of the turns they have taken.\nAt least three kinds of neurons appear to play a role in rat navigation. “Grid cells” fire as the rat moves through certain locations in space, corresponding to points on a remarkably regular hexagonal grid, while “head-direction cells” fire according to the way the creature is facing. These data trigger a third kind of neuron, “place cells,” which set themselves off when the rat moves through a familiar location. The Goulds compare this system, found in the hippocampus, to a “mental drafting table” that is hardwired at birth and filled in through experience and exploration. It is likely that this system exists in other mammals, including humans. One of the common early symptoms of Alzheimer’s patients is disorientation and inability to navigate, and one of the first areas of the brain to degenerate is the hippocampus.\nThe Smelly Flight of Seabirds\nThe big-winged family of seabirds known as Procellariiformes—which includes albatrosses, shearwaters, and petrels—are thought to sniff their way across the oceans. Their neuroanatomy suggests that they have a highly developed sense of smell, based on the size of the slug-shaped regions of their forebrains known as “olfactory bulbs.” During foraging expeditions for fish, carrion, and krill, albatrosses cover thousands of miles of sea before returning to the tiny slices of rock in the middle of the ocean where they like to make their homes.\nResearch by the neurobiologist Gabrielle Nevitt at the University of California Davis indicates that at small distances, up to a few hundred square miles, albatrosses use visual markers and odor plumes (such as from a dead fish or squid) to move toward their dinner. Across larger spaces, she speculates that they may be able to memorize olfactory maps based on the smell of microorganisms that accumulate, in part, according to the depth and topography of the ocean floor. Certain oceanic features, such as subaquatic mountains, tend to attract lots of plankton whose digestion produces a smelly sulfur-based compound that the birds can detect. At night the birds get lazy: they float on the water and wait for the current to drag them into the path of food.\nThe Internal Maps of Migrating Birds\nMigratory birds accomplish remarkable feats of navigation, but the science behind their skills is contentious and evolving. Night migrants like the meadow pipit, a songbird, are thought to set their star by the pole point, the spot in the sky due north around which the stars and sun rotate. Many species fly at dusk, when the dark band of highly polarized light slides across the highest point in the celestial vault. At this moment, the band is perpendicular to the line running between where the sun rose and where it set, known as the azimuth, and so makes it easy for these birds to determine true north, which lies where the polarized band disappears below the horizon, halfway between where the sun rises and where it sets.\nBut like all good navigators, migrating birds have back-up systems. If a European robin flies into a dark space from which it can’t escape, the pattern of its claw-prints will show it hopping northwest, regardless of the season. The birds are detecting the lines of magnetic attraction that emanate from the north and south poles to orient flight direction. However, magnetism is not the most reliable guide. The earth’s magnetic field lines do not run precisely north-south between the poles, which produces a divergence known as “declination” between the direction of a compass needle and true north that depends on where you are on the planet. The earth’s magnetic poles also wander, and switch their polarity every few thousand years. Consequently, it seems likely that any compasses or maps the birds derive from the magnetic poles would need to be calibrated against other measurements, such as their celestial compass.\nSome scientists, such as the neurobiologist Gerta Fleissner at Goethe University in Frankfurt, think that tiny magnetite crystals in the upper beaks of certain birds might alert the creatures to the direction of the magnetic field. This appears to be the mechanism at work in the confined robin example, and is also thought to be behind the orientation of newts, spiny lobsters, and sea turtles. However, research by Roswitha and Wolfgang Wiltschko back in the 1970s revealed that when a magnetic field is angled downwards—a metric known as “dip” or “inclination,” and which shifts slowly from straight up at the south pole to straight down at the north pole—robins will interpret that dip as north, regardless of the direction of the magnetic pull or “polarity.” Magnetite receptors can’t account for this phenomenon, and so researchers have been forced to look for more exotic explanations.\nMore recent research suggests that the magnetic crystals may be part of the birds’ immune systems, rather than detectors of magnetic information. Other studies point to the presence of a quantum mechanism in the eyes of avian migrants that enables them, perhaps literally, to see magnetic fields. An optical protein known as cryptochrome is thought to change into a quantum state when light strikes it, with two unpaired electrons zipping around two molecules in a configuration known as a radical pair. If these electrons spin in parallel, the system exists in what’s known as a triplet state; if they spin against each other, it’s known as a singlet. Running a magnetic field through this delicate biochemical web of subatomic particles will push the system towards either a singlet or a triplet state, depending on the direction of the field lines. The theory is that sensitivity to the balance of these states in the eye would allow the bird to perceive magnetic field lines, like a thread running through three-dimensional space, and use it to set their compasses for flight.\nOther studies point to the presence of a quantum mechanism in the eyes of avian migrants that enables them, perhaps literally, to see magnetic fields.\nBirds may even be able to internalize maps that encompass whole continents or even the globe itself. The long-beaked waterbird known as the bar-tailed godwit flies around 7,000 miles from Alaska to New Zealand every winter, sometimes stopping off at the tiny Melanesian Islands in the South Pacific. It is hard to account for such accuracy in the absence of a cognitive map. Similarly, each fall, flocks of white-crowned sparrows in northern Canada and the northwest of the United States fly several thousand miles south to breeding grounds in the southwest and Mexico. Researchers from the University of Lund in Sweden captured some of these birds in Washington state during their annual migration, and carried them in darkness to New Jersey, on the other side of the country. The adolescent birds flew south, apparently failing to register the displacement. But adults corrected for the change, flying west-south-west towards the correct destination, indicating that they have some kind of large-scale map acquired through experience.\nThe Magnetic Swim of the Loggerhead Turtle\nLoggerhead turtles swim vast distances across featureless oceans, then tirelessly find their way back to the beach where they were born. Whether it hatched on the coast of Mozambique or Florida, a baby turtle will first detect the spread of light around it and scuttle toward the brightest area it can see. Because of the reflective qualities of the water, this is almost always toward the ocean. Once they are surrounded by water in the sea, this method becomes ineffective, so the hatchlings switch to heading in the direction that the waves are coming from in the open sea.\nTurtles also appear to have internal magnetic maps of the Atlantic. In 2011, a team of researchers from the University of North Carolina at Chapel Hill plopped a clutch of baby sea turtles into a swimming pool and re-created the geomagnetic conditions of two points on the earth. Experiments had already shown that turtles could use the intensity of the earth’s magnetic field, which is strongest near the poles and varies on a roughly north-south axis, to judge latitude. But longitude sensitivity is more mysterious, since there are no obvious cues that run reliably east-west. Yet when the researchers plugged the magnetic conditions of the western Atlantic off Puerto Rico into the system, the hatchlings swam towards the northeast, as they would if they were navigating toward the swirl of currents known as the Atlantic gyre. When the team set the system to the waters around the Cape Verde Islands, 2,300 miles away in the eastern Atlantic at the same latitude but a different longitude, the turtles correctly oriented toward the northwest. In the way that humans use latitude and longitude to pinpoint a location on a map, the turtles parse magnetic vectors to determine where they are, relative to their destination."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7e7a95d1-85bc-4d43-be97-bc9a84c7d516>","<urn:uuid:df842d0d-3074-4f3f-be96-3cc90264a158>"],"error":null}
{"question":"What was achieved at the first national Arts in Health Conference?","answer":"The first national Arts in Health Conference, held at London's Royal Festival Hall, showcased 24 different arts projects with proven health benefits. The event brought together key health decision-makers, including the Secretary of State for Health and NHS England Chair, to demonstrate how creative projects could have tangible effects on people's health. The conference highlighted how arts initiatives could 'smuggle' therapeutic exercises into enjoyable activities, making patients more likely to practice them.","context":["The first national Arts in Health Conference was an inspiring example of a proactive, preventative approach to addressing social problems. Charity ‘Arts Enterprise with a Social Purpose’ (Aesop) showcased a series of pioneering arts projects that had proven benefits to people with health difficulties.\nThe day-long event, which took place at London’s Royal Festival Hall, was a celebration of the power of music, dance and visual art to achieve better health outcomes for people of all ages. As Aesop Chief Executive Tim Joss was keen to stress, it also had the important function of engaging health decision-makers: present at the showcase were Secretary of State for Health Jeremy Hunt; Sir Malcolm Grant, Chair of NHS England; and Professor Kevin Fenton, Director of Health and Wellbeing at Public Health England.\nMr Joss commented: “This country is blessed with two highly developed, world-class sectors. Each faces service and financial challenges, and a cool look at the relationship between the two reveals misperceptions, limited connections and few innovations on the road to operating at scale. And yet there is abundant potential for the arts to contribute to health improvement.”\nThe event was the chance for Aesop and other arts advocates to demonstrate how creative and expressive projects could have a tangible effect on people’s health and wellbeing. Twenty-four different arts projects were showcased, many of which used creativity to ‘smuggle’ therapeutic exercises into an enjoyable activity, meaning that patients were significantly more likely to practise the exercises. The activities were being combined with research and clinical trials that are gradually building a solid evidence base for the benefit of arts projects in promoting better health outcomes. By developing pilots of this nature, Aesop is making the case for innovative arts initiatives that have a preventative as well as a therapeutic function when embedded more solidly within local communities.\nOne such project was Aesop’s own ‘Dance to Health’ initiative, which was funded by Oxfordshire Community Foundation via our Surviving Winter appeal (now Health and Wellbeing Fund). The Dance to Health project runs intensive age-appropriate dance classes for older people to improve strength, stamina and balance, thereby preventing falls and accidents that can set an older person back.\nFalls are extremely traumatic for older people, and NICE estimates the cost to the NHS to be more than £2.3 billion a year. Existing falls prevention programmes have been described as “dull as dishwater” by the Royal College of Physicians, with very few providing ongoing support after a fall, resulting in any improved strength and balance being lost within 12 months. Whilst evidence-based programmes can reduce falls by 55%, only 38% of the programmes available follow the evidenced recommendations.\nDance to Health incorporates these evidence-based recommendations into a fun, sociable and ongoing activity for older people, that focuses both on immediate improvement and ongoing maintenance of strength and stability. Oxfordshire Community Foundation is delighted to have helped establish the national pilot by supporting classes at the Health and Wellbeing Centre in Abingdon, with other sessions happening in Banbury.\nParticipants have commented:\n- “When I arrive I have so many things going through my head and when I leave they have all disappeared. It’s so uplifting.”\n- “I enjoyed it because I didn’t realise how much mobility I still had. It gives me hope. It made me feel free.”\n- “I’ve got my mojo back!”\n- “It’s the first time I have laid down on the floor since I was in hospital. I didn’t think I could get up but I can and I did. I can crawl too.”\nDance to Health participants put on a performance at the end of the Arts in Health Conference. The project is documented in this inspiring video.\nOCF’s CEO Jayne Woodley, who attended the conference, commented: “OCF was proud to be able to fund the Dance to Health project – with Oxfordshire at forefront of the great work going on to implement proactive, preventative solutions to our society’s pressing social problems. I was inspired to see how many different organisations and approaches were brought together at the conference, demonstrating the power of convening like-minded organisations. This type of collaboration is something we are very keen to champion more locally in Oxfordshire.”\nFind out about other projects we have helped incubate – read our case studies"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:a80caf5c-8850-4b2d-9116-04c3006e9632>"],"error":null}
{"question":"How do Community Medical Centers and Yale New Haven Hospital support nursing education and development in their respective regions?","answer":"Both institutions invest significantly in nursing education but in different ways. Community Medical Centers invested nearly $1 million in training student nurses in fiscal year 2020, providing about 75,000 hours of hands-on education in their facilities. Yale New Haven Hospital runs a Nurse Residency Apprenticeship Program that was the first of its kind in Connecticut when established in 2005. The program receives $3,500 per nurse through federal grants and provides new nursing graduates with mentorship, computer-based training, monthly lectures, and stress management strategies over a one-year period. More than 180 nurses enrolled in YNHH's program since July 1, with 40 more expected to join.","context":["Caring for Central Valley families requires more than just medical care. It means working beyond hospital walls to partner with community groups who are feeding the poor and educating immigrants about diabetes, COVID-19 and vaccinations. It means investing to attract the brightest medical minds, to expand horizons for children and to enhance support for patients.\nLast year, Community Medical Centers provided $175 million in uncompensated care, medical education, outreach and patient support services to create a healthier Valley for us all. California requires hospitals to invest in the people and communities they serve as part of their nonprofit designation and report it publicly in an annual Community Benefit Report.\nPartnerships key to wide-ranging community outreach\nEfforts to improve the community’s health status vary from sophisticated medical research that addresses the Valley’s unique health needs to lactation education and support for new mothers. With community benefit funding last year, the healthcare system partnered with Centro La Familia to provide free blood sugar testing, blood pressure checks, retinal eye exams and bilingual health education. Community benefit funding also supported the Fresno Diabetes Collaborative’s website with bilingual resources.\nCommunity partnered with schools in fiscal years 2019 and 2020 to do mental health training for teachers and outreach to parents and students. And community benefit investment last year is helping double the efforts of Fresno Metro Ministry’s Food to Share grocery redistribution program.\nCommunity seeks opportunities to partner with area hospitals to make a greater impact with our investments. One such example is the Fresno Economic Opportunity Commission’s Food Express Bus — a refurbished school bus that provides meals to low-income students during school breaks. The capital funding for the bus transformation was paid in part by Community, Saint Agnes and Valley Children’s Healthcare. In the midst of the COVID-19 public health emergency, the bus served over 100,000 meals to children in southwest and southeast Fresno.\nHealth needs drive community benefit efforts\nCommunity benefit investments are based on a tri-annual Community Health Needs Assessment done jointly among more than a dozen hospitals in Fresno, Kings, Madera and Tulare counties and facilitated by the Hospital Council of Northern & Central California.\nMore than 1,000 community members, leaders and key stakeholders representing low-income and vulnerable populations provided primary data through focus groups, one-on-one interviews and an online survey. Primary data is coupled with federal, state and local health status information to identify the regions’ top needs.\nCommunity targets its community benefit investments to address these pressing needs.\nMore than $1.6 billion invested in last decade\nGoing beyond the services for patients by providing resources and education that benefit Central Valley families has been a part of Community’s mission for more than a century. Community has historically spent more on uncompensated community benefits than all other Fresno-area hospitals combined, investing an average of 15% of the healthcare system’s operating budget on “community benefit” work.\nSince 2011, Community has provided $1.6 billion in community benefit, with the single largest unreimbursed cost being for Medi-Cal patients. Statewide, Community Regional Medical Center had the second highest number of Medi-Cal patients discharged from the hospital.\nCommunity serves as the area’s main “safety net” provider, caring for our region’s most vulnerable populations and providing the only Level 1 trauma center and comprehensive burn center between Los Angeles and Sacramento.\nIncreasing access through training more doctors, nurses\nThe second largest portion of community benefit investment last year – $46 million – was on medical education to attract physicians to a region that has the lowest doctor-to-patient ratios in California. Community’s more than 40-year partnership with one of the nation’s top-rated medical schools, UCSF, has encouraged physicians to train – and stay – in the Valley. Community values the teaching partnership with UCSF Fresno Medical Education program and remains fully committed to investing in future physicians to serve our community.\nThat medical education investment means Valley patients are part of research studies that specifically address regional issues such as how environmental chemicals affect pregnancy and pre-term births or how to test faster for Valley Fever. Local patients also have access to clinical studies like the COVID-19 Remdesivir trial that has helped save lives.\nIn fiscal year 2020, Community invested nearly $1 million in training student nurses, providing close to 75,000 hours of hands-on, in-service education in the healthcare system’s facilities.\nThe healthcare system continues to partner with local non-profit organizations and educational institutions to serve the needs of low-income families and children in disadvantaged neighborhoods. Community looks toward future opportunities into the next decade and beyond to strengthen the communities and people it serves.\nCommunity’s Commitment to Making Care Accessible and Building Relationships\nOur mission of improving the health of our region is dependent on making healthcare accessible to all. We invest in facilities, expertise, research and technology so everyone, regardless of their circumstances, has access to top-level care. Community also innovates and networks with others to improve the health of Central California. Read more stories about access to care and about community partnerships.","Yale New Haven Hospital (YNHH) recently received a grant from the US Department of Labor to support its Nurse Residency Apprenticeship Program, a workplace training program for recently graduated nurses. According to YaleDailyNews.com, Yale New Haven Hospital will receive $3,500 for each nurse in the program through the American Apprenticeship Initiative Grant.\nYNHH’s program is one of many apprenticeship programs to receive funding from the Connecticut Department of Labor. The US Department of Labor has allotted a total of $5 million in grant money for programs in the state.\nThe Nurse Residency Apprenticeship program at YNHH was the first of its kind in Connecticut when it was established in 2005. During the one-year residency, new nursing school graduates receive mentorship, computer-based training and monthly lectures, and strategies for stress management.\nJudith Hahn, who oversees the apprenticeship program, tells YaleDailyNews.com, “The responsibility of the hospital is great in making sure that they’re comfortable and that they’re confident and that they have safe experiences for them and their patients, and that’s really costly to do right.”\nAll new nursing graduates who begin working at YNHH are required to go through the program. More than 180 nurses have enrolled in the program since July 1, with 40 more nurses expected to begin the program in the coming months. The newly graduated nurses have the opportunity to gain experience in a variety of hospital settings.\nTo learn more about Yale New Haven Hospital’s Nurse Residency Apprenticeship Program, visit here.\nWestern New Mexico University’s (WNMU) first group of students to complete the traditional pre-licensure Bachelor of Science in Nursing (BSN) degree program graduated at a pinning ceremony held this week.\nThese graduates are already leading in their field simply by enrolling in the BSN program, fulfilling the Institute of Medicine’s (IOM) recommendation that the number of nurses holding BSN degrees increase to 80 percent by 2020.\nWNMU uses the New Mexico Education Consortium (NMNEC) curriculum, which prepares nurses across the state to deliver patient care. Alexis Harsh, Assistant Professor of Nursing at WNMU, explains the focus on writing intensive courses to WNMU.edu:\n“This is the most comprehensive undergrad degree…Nursing school exams feature mainly application and analysis level questions. They aren’t looking for answers that you could memorize the night before the test. What you learned the first day, you have to know the last day. That’s a big change from how most of us do school.”\nThe nurses who graduated from the BSN program come from a variety of backgrounds and have a range of interests for their future nursing careers from pediatrics and NICU nursing to case management and behavioral health. As students, they worked clinicals at a variety of local medical centers and hospitals, averaging 12 clinical hours per week, as well as fulfilling clinical requirements in a child development center, Headstart program, and Walgreens.\nThis experience allowed these students to network and gain real job experiencing prior to graduating and joining the nursing workforce. The graduates will now study to take the NCLEX exam to receive their Registered Nurse license and begin practicing as nurses.\nTo learn more about WNMU’s first BSN nursing cohort, visit here.\nThe George Washington University (GW) has set plans in motion to revamp the School of Nursing’s flagship building on the Virginia Science and Technology campus. Renovations are set to begin soon and conclude in summer 2018, adding a communal space for online and graduate students, and a new patient simulation lab.\nThe new common space will be designed as a place for nursing students to gather, giving them a sense of community on the isolated campus. As a hub for student resources, the new space is expected to promote student engagement and improve campus culture.\nPamela Jeffries, dean of the GW School of Nursing, tells GWHatchet.com, “This investment in our students will hopefully lead to continuous improvement of retention, completion, employment and licensure outcomes.”\nGW offers 16 online nursing degree programs. The 303 online nursing students in the nurse practitioner program take their courses off campus but are required to come to campus for academic testing three times throughout the program. The lab expansion will offer new and improved resources for these students.\nRenovations to complete the lab expansion will include a new space to simulate patient care including private exam rooms, acute care rooms, and more advanced technology. Distance learning students shouldn’t feel that they are at a disadvantage, and GW Nursing hopes to make these students feel more included as a result of the renovation project.\nTo learn more about GW Nursing’s renovation plans to create communal student spaces and a simulation lab, visit here.\nOur Nurses of the Week are Diane Johnson, Mary Mangiamelli, and Judy Spaen, three nurses who spent the majority of their careers working in the same surgery unit of an Omaha hospital, amassing a combined 130 years of experience in the nursing profession. Sharing a common passion for helping others, these three women became like a second family to each other.\n“It’s an interesting career. It’s a rewarding career. Health care is always something people are going to need. There are always opportunities.”\nDuring their tenure at Creighton University Medical Center-Bergan Mercy in Omaha, NE, the three nurses treated each new day as a learning experience. Decades of working at the same hospital taught them how to work with different doctors and medical professionals and how to adapt to new and changing technologies.\nJohnson tells Omaha.com, “I think it’s a great career. Each day is different. You get to meet a lot of patients, and you see them at their worst. But sometimes you can see how far and how healthy they’ve become.” Johnson began her career as an aide at Bergan and went on to complete her nursing degree during her 42-year tenure at the hospital. Her nursing career reaffirmed to her the importance of caring about people and giving each patient the individual attention they deserve.\nMangiamelli’s career spanned 44 years, including many night shifts, which taught her to appreciate the time she was able to spend with her husband and three children. Looking back on the beginning of her career, Mangiamelli shared the following with Omaha.com: “I just thought the human body was a pretty fascinating thing to explore. I don’t think I had two cases the same, because each patient has a different problem. No day ever repeated itself.”\nSpaen’s career in nursing spanned 50 years in which she has “nearly seen it all.” She witnessed the Omaha hospital grow from one small wing to what it is today. Spaen remembers being unsure about whether nursing was the right field for her, but facing new challenges each day kept her engaged and drawn to the way the profession adapted, and she began to enjoy the work. Now retired, Spaen tells Omaha.com, “It’s an interesting career. It’s a rewarding career. Health care is always something people are going to need. There are always opportunities.”\nThese three women are a perfect example of the resiliency that nurses show every day. To learn more about their inspiring careers and passion for the nursing profession, visit here.\nTo help combat the stress of nursing school, the Student Nurses’ Association (SNA) at Grand Valley State University (GVSU) created a Transitions Mentorship Program which is in its third semester. The student-run organization immerses GVSU nursing students into the medical world through presentations, group meetings, and hands-on activities.\nJamie Platt, president of the GVSU SNA chapter, tells Lanthorn.com, “The idea behind the program is to empower our new student nurses. SNA believes that creating a strong environment through positive peer-student relationships during the beginning of nursing school will allow new students to feel confident during a vulnerable time in the nursing program.”\nThe Transitions program pairs lower-level nursing students with upper-level students so they can meet and discuss topics in their major and receive tips for studying for nursing exams. It offers students someone to lean on while studying in an intensive program.\nGVSU’s student nurse association decided to incorporate the mentorship program based on student feedback. Many older nursing students reported the struggles they went through and wished they had had someone to help them through the program. GVSU’s nursing program is comprised of five semesters, so students in their first or second semester are paired with a student in their third, fourth, or fifth semester.\nStudents in the mentorship program are required to meet five times per semester and are encouraged to meet biweekly. After meeting, the mentors report back on their conversations, many of which have revolved around clinical work which makes up half of the students’ time so that they can practice skills they learn in the classroom.\nThe program has received positive feedback thus far, making a positive impact on students. Many students feel the mentorship program helps them feel more confident and less apprehensive about future semesters. To learn more about GVSU’s student nurse mentorship program, visit here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1e0af608-dbc0-4b34-a2c2-fa202f9e967e>","<urn:uuid:d4e9749e-17cb-4280-aa21-17cff81f6568>"],"error":null}
{"question":"How do the collaborative mapping features of Wikimapia and the Statewide Parcel Map Program compare in terms of data sharing and accessibility?","answer":"Wikimapia allows any user to freely edit and add information about any point on Earth, including comments and location names, creating a casual and informal atmosphere. In contrast, the Statewide Parcel Map Program follows a more structured approach, developing a single layer of tax parcel polygons with assessment roll attributes that are shared in phases - first with state agencies, then other government entities, and eventually the public. While both systems involve collaborative data sharing, Wikimapia offers immediate open access to all users, while the Statewide Parcel Map Program maintains a controlled, phased approach to data distribution.","context":["- slide 1 of 8\nOnline maps featuring GPS access allows you to search specific locations on the planet using the site's coordinates. These maps offer very convenient online services especially if you have a device that operates using GPS coordinates. Use of these maps allow you to operate the devices more effectively using these GPS sites.\n- slide 2 of 8\nCombining mapping and the freedom of wiki sites, Wikimapia provides users a way to edit any point on Earth as they please. Comments on places can be included in every point. Even the name of the point on the map is up to whoever is creating or editing it. Having normal people tag and determine the different locations and points on the planet gives this online map a more casual and informal look and feel. Users can search for locations via city names and coordinates. The power of mapping is in everyone’s hands on this website, giving it a fun atmosphere.\n- slide 3 of 8\nOpen Street Map\nLike Wikimapia, Open Street Map is also a collaborative worldwide map. It gets its geographical data and location information directly from users. It can be easily edited by anyone who has access to the site. It allows users to search for addresses and GPS coordinates, and it even lets them export data. Users can also write diaries about certain locations, giving the site a more personal and casual feel.\n- slide 4 of 8\nPossibly the most popular mapping application and service in the world, Google Maps offers a wide array of tools and features to help users find any location on the planet. Users can search addresses and GPS coordinates. Once they have found a location, users can then get driving directions from an origin point to the current point on the map. Users can create their own maps that they can easily access whenever they want as long as they have a Google account. Maps can also be printed and forwarded via email. A line to specific locations is also offered so it can easily be shared via websites and other kinds of communication services. Maps can be viewed in different ways such as the standard map view, satellite view and Google Earth. All kinds of data related to a specific point can also be accessed, including photos, videos and even traffic conditions.\n- slide 5 of 8\nOne of the longest running mapping services available, MapQuest still offers an extensive amount of location information that can be searched via addresses and GPS coordinates. It has a clean user interface that is very intuitive. It offers a standard map view, a satellite view, a 360 view and a live traffic view. Users also get the chance to share points on the map via email and direct links. In addition to searching via addresses and coordinates, users can also search by type of establishments.\n- slide 6 of 8\nIf you want an easy to use user interface and an intuitive search function, Mappy is a good choice for an online map. You can search by address and GPS coordinates. Once you determine the specific location from your search, you can then do a proximity search which determines nearby establishments. The site is also designed to provide users complete itineraries relative to a specific location. Weather and prominent establishments are also highlighted by the site.\n- slide 7 of 8\nThese online maps with GPS coordinates let you search specific locations by using GPS coordinates or street addresses. Whether you are travelling or you are simply trying to explore the different establishments near you, you will find that any of these online maps can help you out.\n- slide 8 of 8\nReferences and Image Credits\nOpen Street Map, http://www.openstreetmap.org/\nGoogle Maps, http://maps.google.com/\nMap Quest, http://www.mapquest.com\nScreenshots by Author","The Past, Present, and Future of OpenStreetMap\nSteve Coast – Author and Founder – OpenStreetMap\nSteve will be presenting on the past, present and future of OpenStreetMap – discussing how to edit, licensing, community and more. Steve will also discuss his new book, “The Book of OSM”, containing interviews with 15 early adopters of OSM to capture the early growth and feeling of the project.\nUse of GIS to Survey and Identify New Industrial Discharges in a POTW Pretreatment Program\nMichael Burkett-Monroe County DES\nThe Clean Water Act required that certain Publicly Owned Treatment Works (POTWs) establish pretreatment programs to regulate industrial discharges to the treatment works. One of the requirements of a pretreatment program for POTWs is to show continuing surveying and identifying potential industrial discharges into their treatment system. Monroe County’s Industrial Waste Office used ArcGIS 10 to develop a database for tracking, surveying and identifying industrial discharges.\nCapturing Geographic Data from Google Street View\nJames Zollweg-SUNY Brockport\nGoogle Street View provides very detailed, panoramic views from positions along many streets in the world. Many important features such as hydrants, road signs, trees, utility boxes, pavement conditions, etc. are easily viewable. By simultaneously viewing corresponding locations in Pictometry Online, one can extract geographies/coordinates and attributes for these important features. This procedure may be the most efficient way to generate spatial data for some applications.\nSession 1B – Student Lightning Talks\nIdentifying Septic Field “Hotspots” using Pictometry Oblique Imagery\nNicole DeRose – SUNY Brockprot\nPictometry Oblique Imagery was successfully used to map septic fields in Oak Orchard watershed. Analysis of the imagery was efficient for finding leach fields as 66 to 81% of the septic fields previously mapped by the Genesee Orleans County Department of Health, were identified. Septic systems may not have been identified if the field was located under canopy cover, there was an absence of a leach field, or if the septic field postdated the imagery. Under ideal conditions (septic systems with leach fields and no canopy or shadows) the method should be able to identify at least 80% of the systems. Imagery taken during the transition from dormant to growing season proved best for identifying leach fields. A total of 1277 septic fields were mapped in the watershed, and one example of a plume from a short circuited system was recorded. Spatial distribution was heterogeneous, with dense sites of septic fields concentrated along residential road corridors. Approximately 4.2% of the leach fields were located less than 100 feet of a tributary. This is below the minimum separation distance of a leach field to a waterbody that is required by the NYS Department of Health code. The average distance of a leach field to a tributary is 327 meters with 50% of the leach fields occurring within 240 meters of mapped tributaries. Maps of important septic field “hotspots” were developed for watershed stakeholders and include tributaries along Batavia-Elba Townline Road, Marsh Creek, and tributaries near the intersections of Alleghany and Lockport Road, Judge and Knowlesville Road, Lockport, and Albion Road. Identifying where septic fields are relative to streams provides us with new opportunities for managing septic field nonpoint source pollution because we can retrofit those that may be contributing the most pollution to streams.\nFinding Sinkholes with Pictometry Oblique Imagery\nMike Rodgers – SUNY Brockport\nUsing Logistic Regression To Predict Spatial Distribution Of Oriental Bittersweet Habitat In New York State\nBrandyn Balch – SUNY Geneseo\nThe invasive vine oriental bittersweet has been having negative impacts on Eastern United States and Canadian ecology since the mid-1800s. Its rapid growth causes the uprooting and girdling of trees due to their excessive weight, and produces dense shade that inhibits new growth of native plant species. The ability to predict and map their suitable habitats will enable their manual removal before damage is done to the surrounding ecosystem.\nKatherine Berdan – SUNY Geneseo\nWhat are the Benefits of Trees on a College Campus?\nSarah Kowalski – SUNY Geneseo\nMaintaining a healthy tree community on a college campus provides opportunities for sustainability and ecological research, as well as community building and educational opportunities. After further completion of a tree map of the SUNY Geneseo campus, further projects and analyses have developed including interpolation of tree benefits such as carbon sequestration and storm water runoff, Tree Campus USA certification, and the creation of a tree tour.\nNutrient Loading Risk Assessment in Livingston County, NY’s Genesee River Watershed\nTori Roberts – SUNY Geneseo\nThe purpose of this GIS study is to use environmental data to explore potential nutrient-loading from agricultural land on the Genesee River basin focusing on major tributaries in Livingston County, NY. The analysis uses riparian zone presence, proximity to agricultural land, soil type, and digital elevation model data in order to rank tributaries accordingly.\nWTS is Up To with GIS! (“S” is for State, of course. What else were you thinking?)\nFrank Winters-NYS ITS\nFrank Winters, will describe how the state is migrating from separate agency GIS installations to an enterprise GIS. ShareGIS will offer GIS data and geoprocessing web services for use in desktop GIS, mobile tools, business systems, and web apps. ShareGIS will become the modern, web services replacement for the NYS GIS Clearinghouse and enable GIS use for many organizations that have been unable to make their own GIS investment.\nStatewide Parcel Map Program\nKatherine Kiyanitsa & Robert Gehrer-NYS GIS Program Office\nThe NYS GIS Program Office has a program to develop and share a statewide GIS tax parcel file. The objective is to assemble a single layer of tax parcel polygons with a set of assessment roll attributes that can be shared, first among all state agencies, then other government entities, and eventually the public. The presentation includes a brief history, current work, and future direction of the program; challenges and benefits of the data; and examples of uses of the file thus far.\nSmall Unmanned Aerial Systems & Applications\nA general overview presentation on the uses and applications of Small Unmanned Aerial Systems a new transformative technology. There are different types of sUAS available which will be briefly described. A system level review to understand the functionality of each element of the sUAS and how the different elements of a sUAS complement each other. This all leads to some of the current and soon to be applications for this type of system.\nDisaster Response with the Humanitarian OpenStreetMap Team\nThomas Hynes-Ulster County\nWhen major disaster strikes anywhere in the world, The Humanitarian OpenStreetMap Team (HOT) rallies a huge network of volunteers to create, online, the maps that enable responders to reach those in need. We’ll take a look to see how the Humanitarian OpenStreetMap Team accomplishes this and how you can volunteer to provide professional mapping assistance in times of crisis.\nArcGIS and Refugees: Practical Lessons Learned from the Rochester Institute of Technology (RIT) Kigeme Refugee camp, Rwanda Humanitarian Field Work Mission\nIn January, a faculty-led study abroad program from RIT went to Rwanda with nine students and Dr. Tomaszewski. The RIT team used a combination of GIS tools to create the first complete map of Kigeme Refugee camp. The presentation will discuss key GIS educational activities conducted during the trip. It will also outline the key highlights of the 16 day trip to Rwanda, explain and display the data collected, discuss follow-up plans and the lessons learned during the trip, and much more.\nDemystifying ArcGIS Online\nArcGIS Online is an exciting and powerful new mapping platform, however, inherent in this robust technology exists a significant amount of confusing new terminology and techniques. This presentation will “demystify” ArcGIS Online by explaining terms such as items, groups, feature layer, and web maps, as well as clarifying techniques such as: publishing services, managing users, organizing content into groups, credit usage, security, and deploying apps with both templates and Web App Builder.\nOpen Source Municipal GIS\nMickey Dietrich-Tug Hill Commission\nOpen source can be the pathway for a municipality to get started with GIS. The tools and resources for open source GIS have grown over the last few years and provide a way for municipalities to get started with GIS on a budget. In this presentation, you will see how some communities in the Tug Hill Region are using PostGIS, QGIS, and GIS Cloud. These solutions are providing municipalities on a budget with the tools needed to get started with GIS.\nIntroduction to UAS Flight Operations for Mapping\nBen Houston & Matt Mercurio-Spatial Analytix\nWe will explore how applying standard mapping principles can impact UAS data quality. We will describe the workflow to consistently produce high quality data, and pitfalls to avoid along the way. Included will be a demonstration of example data from several different UAS platforms collected for engineering design and mapping projects. Attendees will leave with a better understanding of the primary issues affecting data accuracy, precision, and repeatability.\nAnalytical Flood Risk Models for First Responder Use: Obstruction Detection and Risk Assessment\nBrett Edmond Carlock-RIT\nTwo models were developed to direct first responders during floods. The models were designed to be easy, fast & interconnected.\nThe Obstruction Detection Model (ODM) uses a reclassified slope analysis of LiDAR DEMs of the free surface of rivers to detect subsurface obstructions.\nThe Risk Assessment Model (RAM) uses a Risk Point from the ODM to generate flood scenarios using USGS Flood Classes. Demographic, parcel and Critical Infrastructure data are combined to rate each parcel by Risk Factor.\nSimplified Flood Inundation Mapping\nThis presentation looks into the approach and methods of producing a flood inundation map using a combination of historical flood levels, out-of-the-box GIS tools, flood gauge data and 30m DEM.\nDeveloping 3D Basemaps\nEric Brady-Bergmann Associates\n3D GIS continues to evolve as a tool to support Urban and Regional Planning. This talk will focus on the technical fundamentals of developing a 3D GIS base map while offering examples on how a 3D GIS can support common planning use cases.\nPictometry Critical 360 Integration\nCritical 360 is an indoor LiDAR and panoramic mapping program providing detailed analysis of a buildings interior layout. Acessed through Pictometry’s Connect360 viewer, the data seamlessly integrates with external 3D and oblique imagery\nSupporting a Mobile Workforce with ArcGIS Apps\nThe ArcGIS platform includes a host of components, and technologies to support the operation of a mobile workforce, collecting data in the field, and communicating with a home office. In this session, we will explore the use of apps such as Collector, Survey 123, Operations Dashboard, Navigator, Workforce, and others, integrating them in a distributed collaboration environment.\nCentimeter GPS Accuracy on your Tablet or Smartphone\nJean-Yves Lauture-EOS Positioning Systems\nThe explosion of mobile devices in the past few years has created a demand for high-accuracy GPS receivers that can be used with your iOS, Android or Windows smartphone or tablet. At the same time, high-accuracy GNSS receivers are becoming smaller an much more affordable for GIS users. These two trends are coming together to bring a powerful high-accuracy mapping solution for GIS users.\nGIS Data Collection with Carlson SurvCE/SurvPC\nMichael Hyman-Carlson Software\nAttendees will gain a better understanding about the process of GIS attribute with one of today’s most popular Land Surveying/GIS field data collection software packages, Carlson SurvCE /SurvPC. We will show attendees how to field collected data for ESRI. Students will learn the interface options for Point, Line and Polygon GIS features. SurvPC will be highlighted with a direct interface with ESRI MXD file. ESRI Image sources will also be discussed and shown within Carlson Software\nMAP TAB LAB – Downloading Census Data & Shapefiles\nDavid Kraiker-Census Bureau\nPresentation explains Census data, where to find it, how to get tables and varieties of information, and download those data with shapefiles that will easily join up in your mapping application. This is a backdoor method that will save you time and frustration during moments of duress, and will make you want to use Census data more readily."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:9085bc2d-4da1-4cc0-b764-487c041f4973>","<urn:uuid:29f9247b-11a2-4682-a487-4cb57e11af64>"],"error":null}
{"question":"How do data validation approaches differ between facial recognition systems and psychological research datasets?","answer":"In psychological research, data validation focuses on documenting metadata, conducting computational reproducibility checks, and facilitating dataset merging with other databases like the Chicago Face Database. In contrast, facial recognition systems validate data through technical measures like feature point extraction across 6 distinct facial zones, measuring compression/expansion rates, and tracking useless feature point ratios (which must stay under 35% for valid recognition). While psychological research emphasizes documentation and replicability of statistical analyses, facial recognition validation centers on mathematical algorithms and geometric measurements to ensure accurate face detection across multiple angles.","context":["Patrick S. Forscher, Abigail Noyce, Lisa M. DeBruine, Benedict C. Jones,\nJessica K. Flake, Nicholas A. Coles, Christopher R. Chartier\nPlease cite this blog post as:\nForscher, P. S., Noyce, A., DeBruine, L. M., Jones, B., Flake, J. K., Coles, N. A., & Chartier, C. R. (2020, May 26). Incentivizing Discovery through the PSA001 Secondary Analysis Challenge. https://psysciacc.org/2020/09/14/incentivizing-discovery-through-the-psa001-secondary-analysis-challenge/\nOn September 1, 2019, we announced a new initiative to promote the re-use of a large dataset we collected for the PSA’s first study. Called the Secondary Analysis Challenge, this initiative started from the premise that much of the potential richness in psychological datasets is currently untapped.\nThe Challenge aimed to tap that richness using two methods. The first method was relatively standard: we created extensive documentation of the project data and meta-data. The second method was a bit more unusual: we provided incentives to research teams who were willing to follow an analysis pipeline that we thought would minimize the chance of false positives and maximize the chance of true discoveries.\nSpecifically, with the help of a generous donation out of Hans Rocha IJzerman’s book income, we were able to offer $200 awards to up to 10 teams that:\n- Submitted a preregistered script that analyzed an exploratory segment of our data\n- Revised the script in response to a computational reproducibility check\n- Used the revised script to analyze a confirmatory segment of the data that was separate from the exploratory data\n- Posted a preprint to PsyArXiv with a brief description of the results\nWe received a total of eight submissions to the challenge. Each submission consisted of a link to a preregistered script that conducted an analysis on the exploratory segment of the PSA001 data.\nTwo members of our team (Abigail Noyce and Patrick Forscher) checked the submitted scripts for computational reproducibility. These checks involved running the submitted analysis script on one of our computers. We focused primarily on ensuring that the scripts ran without error, but we also sometimes commented on other issues, such as unusual parameter estimates, and possible bugs. We did not comment at all on the validity or theoretical grounding of the analyses. In most cases we did not even have access to this information.\nIf the computational reproducibility check led to changes in the code, we asked the submitting teams to add a comment to the top of the revised script listing the changes. These revised scripts were uploaded to the project OSF page.\nAfter these computational reproducibility checks, we released the confirmatory segment of the data. We asked the teams to use the revised scripts to analyze the confirmatory segment. To receive the award, the proposing team also had to write a brief preprint reporting the results to PsyArXiv. We did not give any strong requirements on the contents of these preprints; we merely asked that they be made public.\nCarlota Batres, one of our Challenge participants, mentioned specifically that the Challenge structure provided her with opportunities she would not otherwise have had. “I hope there will be more initiatives like this one which leverage the collaborative power of the PSA,” she commented.\nWhat are the preprints like?\nMost, but not all, of the preprints rely on connecting the PSA001 face rating data to some other secondary dataset, such as the Chicago Face Database, the Project Implicit Demo Website datasets, or the UN Migration database. Aside from that similarity, the preprints varied substantially in structure, format, and content. Some are fairly brief descriptions of results. Some are submission-ready articles with publication-quality figures. The topics range from an investigation of the “halo effect” to the possible link between face perception and the race and gender IAT scores averaged across regions. The Challenge seems to have elicited a variety of submissions that bear on a variety of scientific topics.\nWe think the preprints cover many interesting topics. However, no one within the PSA reviewed the preprints themselves for scientific veracity. You can help ensure that we advance science through this exercise by looking through the preprints yourself and providing comments and feedback to the authors. To facilitate this process, we have included links and descriptions of the preprints at the end of this post.\nLessons for Future Challenges\nWe think this Challenge was a success. The Challenge also holds some lessons for people who wish to facilitate their own, similar, initiatives:\n- Communicate a clear schedule. Clarity of communication helps keep the Challenge process predictable for the participants.\n- Conduct computational reproducibility checks. The computational reproducibility checks were effortful. I estimate that I spent at least an hour and a half per submission, including running code, debugging, and communicating with submitters. However, the checks uncovered multiple issues and sometimes led to substantive revisions to the analysis plan. These checks were effortful but worthwhile.\n- Enrich the target dataset with meta-data. Meta-data are data about the data — in other words, the time of data collection, information about the stimuli, and other details about the data collection process. These meta-data are important for primary studies, but are crucial for secondary analyses. In this Challenge, we archived a huge amount of meta-data alongside our primary data and documented these in our data management plan. These meta-data greatly facilitated the task of generating interesting exploratory analyses.\n- Build pipelines to other datasets. The extensive documentation of the PSA001 data made it easy to merge our dataset to other interesting datasets. In fact, we included Chicago Face Database ID variables in the data that we released and explicitly noted in our data management plan how to merge our data to the Chicago Face Database. This is another step that we took that, I think, allowed the Challenge to be generative for exploratory analysis.\nOur Challenge format should not be viewed as a substitute for other forms of peer review. By design, the Challenge did not evaluate the merits of the theoretical logic, the analysis plan, or criteria for inference. Hopefully, these are issues that peer reviewers evaluate if and when these proposals are submitted for consideration at scientific journals.\nOverall, we view the Challenge format as a promising supplement that can enhance the scientific value of large datasets. We look forward to observing other innovations people adopt to enhance the value of psychological data.\nAppendix: Secondary Analysis Challenge Preprints\nPreprint 1: PSA001 Secondary Analysis: Examining the “attractiveness halo effect”\nAuthors: Carlota Batres\nAbstract: Research has found that attractiveness has a positive “halo effect”, where physically attractive individuals are ascribed with socially desirable personality traits. Most of the research on this “attractiveness halo effect”, however, has been conducted using Western samples. Therefore, this report aims to examine the “attractiveness halo effect” across eleven world regions using thirteen ratings on faces, including attractiveness, that the Psychological Science Accelerator network collected. We found that for both male and female faces, attractiveness generally correlated positively with the socially desirable traits and negatively with the socially undesirable traits. More specifically, across all eleven world regions, individuals rated as more attractive were rated as more confident, emotionally stable, intelligent, responsible, sociable, and trustworthy as well as less weird. These results replicate previous findings of the “attractiveness halo effect” in Western samples and suggest that the positive effect of attractiveness can be found cross-culturally.\nPreprint 2: Is facial width-to-height ratio reliably associated with social inferences? A large cross-national examination\nAuthors: Patrick Durkee and Jessica Ayers\nAbstract: Previous research suggests that facial width-to-height ratio (fWHR) may be associated with behavioral tendencies and social judgments. Mounting evidence against behavioral links, however, has led some researchers to invoke evolutionary mismatch to explain fWHR-based inferences. To examine whether such an explanation is needed, we leveraged a large cross-national dataset containing ratings of 120 faces on 13 fundamental social traits by raters across 11 world regions (N = 11,481). In the results of our preregistered analyses, we found mixed evidence for fWHR-based social judgments. Men’s fWHR was not reliably linked to raters’ judgments for any of the 13 trait inferences. In contrast, women’s fWHR was reliably negatively associated with raters’ judgments of how dominant, trustworthy, sociable, emotionally stable, responsible, confident, attractive, and intelligent women appeared, and positively associated with how weird women appeared. Because these findings do not follow from assumptions and theory guiding fWHR research, the underlying theoretical framework may need revising.\nPreprint 3: Variance & Homogeneity of Facial Trait Space Across World Regions [PSA001 Secondary Data Analysis]\nAuthors: Sally Xie and Eric Hehman\nAbstract: This preregistration is part of the PSA secondary analysis challenge. We investigate how the facial ‘trait space’ shifts across countries and world regions, using the PSA_001 dataset shared by the Psychological Science Accelerator. The facial trait space refers to the interrelationships between many of the trait impressions that people infer from faces. Here, we examine whether this trait space is more homogeneous (or less differentiated) in some cultures than others.\nPreprint 4: Hester PSA001 Preregistration Preprint—Region- and Language-Level ICCs for Judgments of Faces\nAuthors: Neil Hester and Eric Hehman\nAbstract: We report the results of preregistered analyses of the PSA001 face perception data. We tested whether the target-level intra-class correlations (ICCs) would be higher in specific regions (i.e., more culturally homogeneous samples) than in the global data set (i.e., a less culturally homogeneous sample). We also report perceiver-level ICCs as well as by-trait perceiver- and target-level ICCs.\nPreprint 5: Do regional gender and racial biases predict gender and racial biases in social face judgments?\nAuthors: DongWon Oh and Alexander Todorov\nAbstract: Trait impressions from faces are more simplified for women than men. This bias stems from gender stereotypes; when strong stereotypes exist for a group of faces (e.g., of women’s or Blacks’), they are evaluated more positively/negatively when they fit/violate the stereotypes, making the impressions simpler (i.e., more one-dimensional). In this preregistered study, using trait impression ratings of faces collected from various world regions (+11,000 participants in 48 countries), scores of implicit associations (+18,000 and +212,000 participants in +200 countries), and mixed-effects models, we ask (1) whether simplified facial impressions are found for women and Blacks across regions and (2) whether the regional level of stereotypes about genders and races is correlated with the level of simplification in the face-based impressions of women and Blacks, respectively. The results were not coherent across analyses. The interpretation of the results and the limitations of the study are discussed.\nPreprint 6: Hierarchical Modelling of Facial Perceptions: A Secondary Analysis of Aggressiveness Ratings\nAuthors: Mark Adkins, Nataly Beribisky, Stefan Bonfield, and Linda Farmus\nAbstract: The Psychological Science Accelerator’s (PSA) primary project tested for latent structure using exploratory factor analysis and confirmatory factor analysis but we decided to diverge from this approach and model individual traits separately. Our interest mainly was in examining the interplay between “stimulus ethnicity” and “stimulus sex” to discover how differing levels of these criterion differ across region, country, lab etc. While the necessary and prerequisite hierarchical structural information about each trait could certainly be found within the primary project’s dataset, we did not assume that any specific factor structure from the PSA’s primary analysis would necessarily hold, therefore we based our decision to model the data from each trait separately using a mixed model framework.\nPreprint 7: Population diversity is associated with trustworthiness impressions from faces\nAuthors: Jared Martin, Adrienne Wood, and DongWon Oh\nAbstract: People infer a number of traits about others’ based simply on facial appearance. Even when inaccurate, face-based trait impressions can have important behavioral consequences including voting behavior and criminal sentencing. Thus, understanding how perceivers infer others’ traits is an important social and psychological issue. Recent evidence suggests that face-based trait impressions may vary by culture. In the present work, we attempt to understand cultural differences in face-based trait impressions. As part of the Psychological Science Accelerator’s Secondary Data Analysis Challenge, we report a set of pre-registered analyses testing how cultural differences in present-day diversity relate to a) 13 face-based trait impressions, b) sensitivity to physical features of the face, c) and the mental structure underlying trait impressions. We find that greater present-day diversity might be related to lower trustworthiness ratings, in particular. We discuss this finding in the context of other recent work and suggest further analysis of the mental structure of face-based trait impressions across cultures.\nPreprint 8: The Facial Width-to-Height Ratio (fWHR) and Perceived Dominance and Trustworthiness: Moderating Role of Social Identity Cues (Gender and Race) and Ecological Factor (Pathogen Prevalence)\nAuthors: Subramanya Prasad Chandrashekar\nAbstract: People effortlessly form trait impressions from faces, and these impressions can affect a variety of important social and economic outcomes. Trait impressions based on facial features can be approximated to distinct dimensions: trustworthiness and dominance (Oosterhof & Todorov, 2008). One of the facial features, the facial width-to-height ratio (face ratio) is associated with the trait impressions. I tested whether social category (gender, race) of the target being perceived shapes the relationship between face ratio and perception of dominance and trustworthiness. In this preregistered study, using trait impression ratings of faces collected from 8800 participants across 44 countries, I employ mixed-effects analysis and report results on (1) the direct influence of social categories (gender and race) of the target on perceived dominance and trustworthiness, (2) the moderating role of social categories (gender and race) on the direct relationships between face ratio and perceived dominance and trustworthiness, and (3) the moderating role of pathogen prevalence on the direct relationships between face ratio and perceived dominance and trustworthiness.","The first step in recognizing a face is to determine if a face exists in the image captured by the camera. If there is a face in the image, the position and size of the face are searched to determine whether or not the face is recognizable. If it is recognizable, it searches for 'eye' as the first step of recognition. Common face detection methods can be classified into a knowledge-based method, a feature-based method, a template-matching method, and an appearance-based method .\nThe knowledge-based method assumes that a person's face consists of two eyes, one nose and mouth, and each face element has a certain distance and position. And it is a method to detect faces considering the relation between these elements. Feature-based methods infer face size and position using face-specific features such as facial features, color, shape, and size , .\nIt detects faces through inferred data, and also detects faces through distance between facial elements, position on face, and so on. The template matching method is to create a basic template for the face, and then analyze the inputted face image to create a standard template for the face. Then, the standard template and the input face are compared and detected. The appearance-based method is to detect a face using a model learned by a set of learning images. This method uses statistical numerical values to detect face parts in complex images.\nThe final goal of this study is to improve the recognition rate of faces at various angles. That is, the face which is even in various angle changing environments can be recognized.\nAs a method of realizing this technology, it is to implement an algorithm that inputs two-dimensional flat photograph, extracting a feature point, and then recognizes faces rotated in three-dimension using extracted feature point. In order to realize this technology, vector compression and expansion methods are used. That is, it recognizes the face rotated in the up, down, left, and right direction only by the feature points of the flat photograph. That is, the feature point vector of the face is classified into six vector zones, and the rotation angle is obtained by increasing or decreasing the vector zones according to the angle of rotation of the face. In this way, the recognition can be attempted by coinciding with the feature points of the face rotating in three dimensions. By doing this, it recognizes faces rotated in top, bottom, left, and right with only the feature points of a flat photograph.\nII. THE SHAPE OF THE HEAD ANALYSIS\nWesterners' head shape is more three-dimensional compared to the shape Oriental’s head. Of course, among Oriental people, there are long-head figures like Westerners. The long size between the forehead and the back of the head is called a long-head. On the other hand, the back of the head is flat, and the short between the forehead and the back of the head is called the short-head. The middle is called the middle-head shape. Figure 1 shows the top and side views of the long-head and short-head.\nThe short-head is usually eyes are higher than the ears, and the distance between the eyes and ears is so close that glasses or sunglasses are not suitable. This is the reason why glasses flow down well. The long head type has lower ears than ears, and the distance between the eyes and ears is long, so the sunglasses are well suited. If an Oriental has a nose plastic surgery, it can be recognized immediately, it because of the difference in the shape of head. In this way, when the nose is raised from the face with short forehead and back, the face harmony is broken .\nA face which is narrow width and long between the forehead and the back is expressed three-dimensional at any angle. Statistically, the face shape that is classified as long -head in Korea is less than 5%, and the long-head is three-dimensional face that everyone feels difference. However, long-head shape is not always nice and it is not always bad for short-head shape .\nIn the face recognition system, the feature points are extracted slightly differently at the long-head and short-head shape. Therefore, there is a significant difference in the variable values of the face zones even in the multi-angle rotated faces of the upper, lower, left, and right sides.\nThe Westerner’s face has a big three-dimensional than Oriental. Thus, the nose is relatively high, and the eyes are relatively deep. As a result, eyes are dented and bigger than Oriental.\nThis shape of face is very suitable for extracting feature points, but the useless ratio of feature points is relatively high when the generating a side face’s feature point. The reason is that distinct feature points disappear from the side face. Therefore, it has been proven by data that it is very difficult to generate feature point from the side faces of Westerners.\nAs can be seen in Figure 2, the side faces of Western and Oriental people are distinctly different. The shape of the Westerner’s mouth is a clockwise depression from the nose to the mouth and then coming out of the jaw again. However, the shape of the Oriental’s mouth appears to protrude from the nose to the jaw in the counterclockwise direction .\nIt is not a problem to recognize these faces with mouth shapes by feature points extracted from front face only. However, it can be seen that an algorithm must be differently applied to Orientals and Westerners in order to recognize side faces or slope faces. In other words, it can be seen that the feature point compression and expansion algorithm is more suitable than the feature point generation algorithm for Oriental's faces, and the feature point generation algorithm has a higher recognition rate on the faces of the Westerners.\nIII. FEATURE POINTS COMPRESSION & EXPANSION\nThe feature point compression and expansion algorithm is inventive and new algorithm developed while carrying out this study. This algorithm can provide the fastest solution for rotated face recognition. The basic feature of this algorithm is to divide the feature points extracted from the face into 6 zones as shown in Fig. 3.\nIn left-right rotation, compression or expansion is divided into left and right. Thus, six zones are again grouped into two zones, one is grouped into three zones on the left, and the other zone is grouped into three zones on the right.\nTherefore, in the left and right rotation, the zone number 1, 3, 5 are bound one group, and the zone number 2, 4, 6 become another group and are compressed or expanded. When rotating up and down, it should be grouped into three zones. At this time, the zone number 1, 2 are bound one group, the zone number 3, 4 are bound second group, and the zone number 5,6 are bound another group.\nEach zone is interlocked or separately compressed and expanded. In each zone, extracted feature points are located, and the points are stored as vector values. Compression and expansion are not manipulations on simple position values. This is done by calculating a vector value.\nFig. 4 is an Illustration for the left-right rotation. As shown in the figure, when rotating left, the left zone is compressed and the right zone is expanded. Conversely, when rotated to the right, the right zone is compressed and the left zone is expanded.\nFig. 5 is an Illustration for the case of up and down rotation. As shown in the figure, when the head is tilted down, the lower zone is compressed and the upper zone is expanded. The intermediate zone is compressed and expanded selectively.\nConversely, when the head is lifted up, the upper zone is compressed and the lower zone is expanded. The intermediate zone is compressed and extended selectively.\nSince the compression and expansion of vectors are the basic framework of the algorithm, the basic algorithm, the linear operator, has been developed as follows.\nWhen k is a non-negative scalar,\nThe liner operator\nis said to be a scaling operator with factor k.\nThe reason, T (x, y) = (kx, ky) is a linear operator because it is a linear equation with kx, ky.\nHere, if 0≤k<1,\nthe operator T is called contraction.\nthe operator T is called dilation.\nA common geometric feature of contraction and expansion is preserving the direction of the vector. The difference is to reduce the size of the vector in the case of contraction, but to increase the size of the vector in the case of dilation.\nFor a linear operator T (x, y) = (kx, y),\nwhen k is scalar, not a negative,\nIf 0≤k <1,\nthe geometry on R2 is compressed in the x-axis direction.\nIf k> 1,\nthe geometry on R2 extends in the x-axis direction.\nTherefore, for the linear operator\nIf 0≤k<1, the operator T is referred to as compression in the x-direction by k,\nIf k> 1,\nthe operator T is expanded by k in the x direction (expansion in the x-direction with factor k).\nSimilarly, when k is scalar, not a negative,\nFor the linear operator\nthe geometry on R2 has the geometric characteristic of being compressed in the y-axis direction,\nIf k> 1,\nthe geometry of the figure on R2 extends in the y-axis direction.\nTherefore, for the linear operator\nthe operator T is referred to as compression in the y direction by k,\nIf k> 1,\nthe operator T is said to expand in the y direction by k.\nOne hundred Korean faces were photographed for the experiment of vector compression and expansion algorithm, which is for multi-angle face recognition.\nThe feature points were extracted based on the captured front face in the photograph, and based on them, the compression and expansion rates, number of useless feature points, and useless feature points rate for the up & down rotation angles of 15° and the left & right rotation angles of 15° and 30° respectively were obtained.\nAs a result, representative face models were selected, which was the closest to the average of the ratio of each feature point and the useless feature point.\nTherefore, the feature points of the representative face model are extracted, and the compression and expansion rates of each angle of up, down, left, and right, the number of feature points that are useless, and the useless ratio are analyzed. The reason for analyzing useless feature points is that they are closely related to the threshold of recognition rate.\nFig. 6 shows the representative face model and extracted feature points. Although the face selected as the representative model is slightly different from the traditional Korean face type, and according to the distribution of the feature points, it shows the future Korean female characteristics face type.\nIn addition, the analysis of facial recognition feature point data has distinctive features from Western female and can be positioned as a unique Korean beauty figure.\nThe total number of feature points of the representative model face is 128, and the number of feature points for each zone and the size of each zone by feature points are shown in Table 1.\n|Zone||Horizontal width||Vertical width||Feature points|\nA total of 128 feature points are somewhat larger than the average feature points of Koreans. The reason seems to be that the outline of the face is futuristic, and it possesses a clear aspect ratio. The reason why the number of feature points is evenly distributed in the right and left sections is the data proving that the face is symmetrical. The reason why the feature points are distributed widely in the zone 1, 2 is analyzed because it is a distinctive shape.\nFig. 7 shows the representative model’s multi-angle. Fig. 8 shows extracted feature point that is replaced with the face angle of the representative model in Fig. 7. Therefore, in Fig. 8, the two left feature points show rotation angles of 30 ° and 15 °, the right feature points show the rotation angles of 15 ° and 30 °, and the up & down rotation angles are 15 °. Fig. 8 shows the state of compression and expansion in the direction of rotation from the original extracted feature point which located in the center.\nYellow is the feature point within the threshold, and red is a useless feature point. If the total number of feature point does not exceed 35% of useless feature points, there is no problem recognizing the face.\nIn this study, it was tried to find the compression and expansion mean values according to the face rotation angle. Therefore, the average value of 100 persons face was investigated in order to obtain the average compression and expansion rate in up, down, left and right rotation.\nIt was found that the compression and expansion rates vary widely depending on the width and length of the face. The average value showed a large deviation according to the size of the face, and it was not a meaningful value.\nHowever, the result obtained incidentally was that the expansion rate on the opposite side of the zone to be compressed was 10 to 15% smaller on average than the compression rate.\nThat is, if the compression rate of the zone 1 is 30%, the expansion rate of the zone 2 is 25 ~ 27% on average. That is, the compression ratio and the expansion rate are not proportional.\nBased on these results, further study is needed to recognize the multi-angle rotation faces by this vector compression and expansion algorithms. It is needed to find the average parameter for the compression and expansion rate according to face width and length.\nRecognizing faces rotated by multiple angles using only feature points extracted from a flat photo is one of the most difficult assignments. Various algorithms have been devised and attempted to solve this assignment.\nEspecially, in this study, it was tried to estimate and convert the three-dimensional shape by adjusting the interval of the feature points by the rotation angle of the face.\nSince the feature points can’t be adjusted one by one, they are grouped and the feature point intervals are adjusted via group adjustment.\nThe compression and expansion rates determined by the face rotation angle. However, this value was found to vary widely depending on the width and length of the face. Therefore, it is considered that the average value obtained from 100 face data is not enough to be used as the average parameter of this algorithm.\nIn order to realize effective three-dimensional face recognition using this algorithm, more face feature point data should be analyzed and established.\nThrough this study, it was obtained a basic algorithm that can recognize three-dimensional faces through vector compression and expansion. Further studies should continue to attempt to obtain an average parameter. If the average parameter is obtained, the face is considered to be recognized at a rotation angle larger than the current maximum recognizable face rotation angle."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:99355b47-0d89-4eb1-b5eb-42de6726d0c4>","<urn:uuid:7a45284f-f219-4e45-87bc-25a4f3944c1d>"],"error":null}
{"question":"As an HR professional interested in modernizing our systems, what are the benefits of implementing HRIS technology and what potential algorithmic biases should we be concerned about?","answer":"HRIS technology offers several key benefits: it enables systematic data storage for individual employees, aids in planning and decision-making, improves HR department productivity, and facilitates reporting to external agencies. It also helps with essential functions like employee selection, recruitment, training, and continuous knowledge development. However, there are important algorithmic biases to consider. Automated systems can systematically disadvantage certain groups, as they rely on imperfect inputs and logic. For example, algorithmic hiring systems have been shown to discriminate against certain candidates, like in the case where a person with bipolar disorder was rejected by multiple retailers based on automated personality tests. Additionally, these systems can make inferences about individuals from their digital data, potentially leading to unfair outcomes in employment decisions.","context":["Human resource development hris human resource information system executive summary: human resource is the part of the organization dealing with the employees it starts with selecting the employees, recruiting the employees, training the new joiners and engaging the existing employees in a continuous process of training and knowledge gaining. Android mobile using human resources system information technology essay chapter – 1: introduction topic of the system after observing the existing situation under project background, it is being aimed to do develop a mobile application for an organization. Human resource information system (hris) is a systematic way of storing data and information for each individual employee to aid planning, decision making, and submitting of returns and reports to the external agencies. Human resource information systems - introduction human resource information systems (hris) can provide an organization a wide variety of functionalities that improve the productivity of the hr department while supporting the desires and requirements of the rest of the organization. The human resource information systems information technology essay the human resource information systems is introduced by presenting the various definitions, development, costs and benefits, as well as their functions and relationship with hrm.\nThe rapidly transforming business landscape means that there are currently many human resource management challenges which will continue to evolve for years to come. Human resource information system (hris): important and emerging opportunities and challenges in the business horizons suggest that ,”it is not technology . A human resource management system or human resource information system (hris) or hr system is the systems and processes between human resource management (hrm) and information technology (it) hrm is a discipline which blends its basic hr functions and processes with the information technology. Impact of information technology in human resources management dr seyni mamoudou, phd management’s support of the hr information systems application,.\nToday, the field of human resource management (hr) is experiencing numerous pressures for change shifts in the economy, globalization, domestic diversity, and technology have created new demands for organizations, and propelled the field in some completely new directions. Human resource management essay of technology generated challenges correspondent to the pay levels of staff as well as within the information transformation . Explain what a human resources information system (hris) does, and identify its main components chapter 3 human resources management and technology 51.\nResource planning (erp) systems erp systems are an information technology (it) infrastructure that facilitate the flow of information between all supply chain processes in. The human resource information system (hris) is a software or online solution for the data entry, data tracking, and data information needs of the human resources, payroll, management, and accounting functions within a business. The emerging challenges in hrm human resource information system that they can overcome information technology challenges 7 proper performance evaluation . Essay # 1 meaning of human resources: by the term human resources we mean the size of population of a country along-with its efficiency, educational qualities, productivity, organisational abilities and farsightedness.\nDescribe the various challenges that or human resource information system, is a solution for the challenges of information technology in management the human- side of leadership | switch & shift. Some of the common types of management information systems include process control systems, human resource management systems, sales and marketing systems, inventory control systems, office automation systems, enterprise resource planning systems, accounting and finance systems and management reporting systems. Providing employee training on how to secure data and prevent privacy breaches to keep business information secure are challenges human resources faces in this area.\nA study of issues & challenges of implementation of information technology 437 4 literature review 41 the paradigm shift information technology has greatly impacted the human resource management. This paper will highlight on how a hr manager can meet the challenges of workplace diversity, how to motivate employees through gain-sharing and executive information system through proper planning, organizing, leading and controlling their human resources. Human resource information systems the purpose of this paper is to identify other companies who have faced similar human resources issues in regards to information technology through benchmarking different companies, we can learn how other companies have handled certain human resources issues related to information technology, information .\nChallenges to human resource management since the 1990’s many public sector managers have begun to address the daunting issue of upgrading their legacy human resource information systems to. The human resource management review (hrmr) is a quarterly academic journal devoted to the publication of scholarly conceptual/theoretical articles. Impact of globalization on human resource management need to take advantage of technology and data analytics to build a global human resource information system . Human resource information systems (hriss) have the potential to improve organizational efficiency and effectiveness by facilitating workforce planning .","When you browse online for a new pair of shoes, pick a movie to stream on Netflix or apply for a car loan, an algorithm likely has its word to say on the outcome.\nThe complex mathematical formulas are playing a growing role in all walks of life: from detecting skin cancers to suggesting new Facebook friends, deciding who gets a job, how police resources are deployed, who gets insurance at what cost, or who is on a “no fly” list.\nAlgorithms are being used—experimentally—to write news articles from raw data, while Donald Trump’s presidential campaign was helped by behavioral marketers who used an algorithm to locate the highest concentrations of “persuadable voters.” But while such automated tools can inject a measure of objectivity into erstwhile subjective decisions, fears are rising over the lack of transparency algorithms can entail, with pressure growing to apply standards of ethics or “accountability.”\nData scientist Cathy O’Neil cautions about “blindly trusting” formulas to determine a fair outcome. “Algorithms are not inherently fair, because the person who builds the model defines success,” she said.\nO’Neil argues that while some algorithms may be helpful, others can be nefarious. In her 2016 book, Weapons of Math Destruction, she cites some troubling examples in the United States:\nPublic schools in Washington, D.C. in 2010 fired more than 200 teachers—including several well-respected instructors—based on scores in an algorithmic formula which evaluated performance.\nA man diagnosed with bipolar disorder was rejected for employment at seven major retailers after a third-party “personality” test deemed him a high risk based on its algorithmic classification.\nMany jurisdictions are using “predictive policing” to shift resources to likely “hot spots.” O’Neill says that depending on how data is fed into the system, this could lead to discovery of more minor crimes and a “feedback loop” which stigmatizes poor communities.\nSome courts rely on computer-ranked formulas to determine jail sentences and parole, which may discriminate against minorities by taking into account “risk” factors such as their neighborhoods and friend or family links to crime.\nIn the world of finance, brokers “scrape” data from online and other sources in new ways to make decisions on credit or insurance. This too often amplifies prejudice against the disadvantaged, O’Neil argues.\nHer findings were echoed in a White House report last year warning that algorithmic systems “are not infallible—they rely on the imperfect inputs, logic, probability, and people who design them.”\nThe report noted that data systems can ideally help weed out human bias but warned against algorithms “systematically disadvantaging certain groups.”\nZeynep Tufekci, a University of North Carolina professor who studies technology and society, said automated decisions are often based on data collected about people, sometimes without their knowledge. “These computational systems can infer all sorts of things about you from your digital crumbs,” Tufekci said in a recent TED lecture. “They can infer your sexual orientation, your personality traits, your political leanings. They have predictive power with high levels of accuracy.”\nSuch insights may be useful in certain contexts—such as helping medical professionals diagnose postpartum depression—but unfair in others, she said. Part of the problem, she said, stems from asking computers to answer questions that have no single right answer. “They are subjective, open-ended and value-laden questions, asking who should the company hire, which update from which friend should you be shown, which convict is more likely to reoffend.”\nFrank Pasquale, a University of Maryland law professor and author of The Black Box Society: The Secret Algorithms That Control Money and Information, shares the same concerns. He suggests one way to remedy unfair effects may be to enforce existing laws on consumer protection or deceptive practices.\nPasquale points at the European Union’s data protection law, set from next year to create a “right of explanation” when consumers are impacted by an algorithmic decision, as a model that could be expanded. This would “either force transparency or it will stop algorithms from being used in certain contexts,” he said.\nAlethea Lange, a policy analyst at the Center for Democracy and Technology, said the E.U. plan “sounds good” but “is really burdensome” and risked proving unworkable in practice. She believes education and discussion may be more important than enforcement in developing fairer algorithms.\nLange said her organization worked with Facebook, for example, to modify a much-criticized formula that allowed advertisers to use “ethnic affinity” in their targeting.\nOthers meanwhile caution that algorithms should not be made a scapegoat for societal ills. “People get angry and they are looking for something to blame,” said Daniel Castro, vice president at the Information Technology and Innovation Foundation. “We are concerned about bias, accountability and ethical decisions but those exist whether you are using algorithms or not.”"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:eb424396-03ce-4483-8f6c-9f1bb7cba41d>","<urn:uuid:6ac4c8f9-4485-43b7-b764-b48a249516fb>"],"error":null}
{"question":"Did Jefford Watson and James Ervi both serve in Korea during the same time period?","answer":"No, they served at different times. Jefford Watson served in Korea in 1951 until his death from Manchurian fever on November 18, 1951. James Ervi first saw combat in Korea in August 1950, and later returned for another tour in 1961.","context":["Jefford Rex Watson Born 26 February 1928 - Died 18 November 1951 PDF Print Share Key details Contribute › Service number206403 AWMM Also known as ForceArmy AWMM Last rankGunner AWMMRoyal New Zealand Artillery, 16 Field Regiment AWMM WarKorean War, 1950-1953 AWMM Lay a poppy Auckland War Memorial Museum, Hall of Memories. Korea, Malay … Read more View gallery Hide sources Show empty fields Identity About Contribute › Title ForenamesJefford Rex AWMM SurnameWatson AWMM Ingoa Also known as Service number206403 AWMM GenderMale AWMM Iwi Hapū Waka Rohe Religion Images and documents Images Contribute › Documents Contribute › Civilian life About birth Contribute › Date of birth26 February 1928 AWMM Place of birthPongaroa, Manawatu-Wanganui, New Zealand AWMM Birth notesPongaroa, Manawatu, New Zealand AWMM Occupation before enlistmentDriver and Fencer AWMM Post war occupation Address before enlistmentUnknown Pahiatua, New Zealand AWMM Next of kin on embarkation Relationship status Service Wars and conflicts Contribute › WarKorean War, 1950-1953 AWMM Campaigns ForceArmy AWMM Service number206403 AWMM Military service Promotions/ Postings/ Transfers Miilitary decorations Contribute › Medals and AwardsNew Zealand Operational Service Medal AWMM Training and Enlistment Contribute › Military training Enlistment Age on enlistment Embarkations Contribute › Embarkation details Prisoner of war Contribute › Capture details Days interned Liberation date POW liberation details POW serial number Medical history Contribute › Medical notesDied of Disease, Cause of Death AWMMDied of disease, Manchurian fever AWMM Last known rank Contribute › Last rankGunner AWMMRoyal New Zealand Artillery, 16 Field Regiment AWMM Biographical information Biographical information Contribute › Served with Kayforce 1951Prime Minister Helen Clark and Defence Minister Mark Burton presented the New Zealand Operational Service Medal (NZOSM) to next of kin of New Zealanders who were killed or died in military operations since 1945 at a ceremony at Parliament 3 September 2003.The New Zealand Operational Service Medal was received on his behalf by his nephew Mr Jefford Watson of Palmerston North.An article on Manchurian fever appeared in the New Zealand Herald (19 November 1951, p. 8) claiming that only one soldier (not named) in Kayforce was affected and that he was not seriously ill. \"Health concerns had been raised by the upsurge in cases of haemorrhagic (Songo or Manchurian) fever, which occured late in 1951.During November, thirty-one cases were reported among the Commonwealth troops. These included two members of 16 Field Regiment, Gunners J.R. Watson and D.C. Jackson, both of whom died of illness.Other victims spent months in hospital recovering.\" (McGibbon 1996, vol. 2, p. 240) AWMM Read more Death About death Contribute › Age at death23 AWMM Date of death18 November 1951 AWMM Cause of deathDied of Disease AWMM Place of deathKorea AWMM Death notesKorea AWMM Cemetery nameUnited Nations Cemetery, Pusan, Korea AWMM Grave reference Memorial nameAuckland War Memorial Museum, Hall of Memories. Korea, Malaya-Borneo. AWMM Memorial reference Obituary Roll of Honour Remember Jefford Rex Watson by laying a poppy. James Wallace Watson Jean Annetta Watson Jefford Rex Watson Jessie Gertrude Watson John Watson Lay a poppy for 2015 Leave a note Leave a tribute or memory of Jefford Rex Watson Leave a note Contribute › Processing Sources Sources Contribute › External linksNew Zealand Defence Force. Roll of Honour. Personnel who have died on operational service since 3 September 1945.http://medals.nzdf.mil.nz/info/roll-of-honour/index.htmlSources UsedNew Zealand Navy in the Korean War. Navy Museum.http://www.navymuseum.mil.nz/history/time/korean/default.htmFurther ReferenceNew Zealand in the Korean War. NZHistory online.http://www.nzhistory.net.nz/war/korean-warFurther Reference DocumentsSecond New Zealand Expeditionary Force. (1945). Nominal Roll Second New Zealand Expeditionary Force No. 10 (Embarkations from 1st January, 1943 to 31st March, 1943). Wellington, N.Z.: Govt. Printer. AWMMSecond New Zealand Expeditionary Force. (1945). Nominal Roll Second New Zealand Expeditionary Force No. 10 (Embarkations from 1st January, 1943 to 31st March, 1943). Wellington, N.Z.: Govt. Printer. AWMMMcGibbon, I.C. (1992-1996). Volume II: Combat Operations, 1996, pp. 240, 371, 413. AWMMHopkins, G. (2002). Tales from Korea : the Royal New Zealand Navy. Auckland, N.Z.: Reed. AWMM Seen an error on this page? Tell us about it.","Friends and family have shared their relationship to show their support.\nHow do you know James R. Ervi?\nWe are sorry for your loss.\nHelp others honor James's memory.\nCoping with Grief\nWe would like to offer our sincere support to anyone coping with grief. Enter\nyour email below for our complimentary daily grief messages. Messages run for up to one year and you\ncan stop at any time. Your email will not be used for any other purpose.\nJames R. Ervi\nFebruary 19, 1933 ~ May 1, 2023 (age 90) 90 Years Old\n9 Trees, Flowers, or Condolences have been shared with support of James's family - View on Tribute Wall\nServices for Jim Ervi will be Saturday, May 6, 10:00, at the Presbyterian Church in\nSalado with Carl Thompson officiating. A burial will follow at Salado Cemetery\nwith Military Honors. Jim was born on February 19, 1933, in Electra, TX. He was\nraised by his mother, Gladys Ervi, until her death when he was 8 years old. He\nmoved to Bartlesville, Oklahoma, where he lived with his grandparents on a farm.\nHis grandmother died when Jim was 15, and with the assistance of his aunts, who\nlied about his age, he enlisted in the US Army at age 16. Jim did well at his basic\ntraining and attended leadership school. After being in the Army for only a year,\nhe was sent to Korea to fight in the Korean War. He saw his first combat in August\n1950. He became a platoon sergeant while in the war, still only 18 years old. He\nreturned from Korea and was stationed at Fort Sill, Oklahoma. He began visiting\nback home in Bartlesville, where he went on a blind date with Roxie Berry, his\neventual wife of 51 years. They married in late 1952, while Roxie was in the\nmiddle of her Senior year in high school. Jim had received orders to Germany, and\nRoxie followed him there after graduating high school in May 1953. Jim graduated\nfrom helicopter school in Fort Rucker, Alabama in 1957. He was sent to Korea for\na year in 1961. Jim moved his family to Killeen in 1964, and he and Roxie bought\na home there. Jim was sent to fly Huey gunships in the Vietnam War in 1965.\nAfter his return from Viet Nam, Jim and his family moved to Verona, Italy for two\nand a half years. Then Jim returned to Vietnam for another year before retiring\nfrom the US Army in 1969. He then worked for a Military Service Company selling\nto the varied units on Fort Hood. He was very successful, and then operated a car\nlot in Killeen for a few years, while also aiding Roxie in operating a convenience\nstore in Granger. They moved to Salado on 10 undeveloped acres that they\ntransformed into a small cattle ranch. Jim treasured every inch of the place and\nspent the rest of his life trying to make it a little more beautiful every year. Jim\nwas a devoted grandfather. He attended all his grandchildren’s sporting events.\nHe is survived by his three sons: James and wife Kitty of Killeen, Mark and wife\nMary of Killeen, and Barry, who lives in Carrollton. He has 6 grandchildren and\ntwo great grandchildren. The visitation will be one hour prior to the service. In\nlieu of flowers, please send donations to: Presbyterian Church of Salado, 105\nSalado Plaza Drive, Salado, TX 76571. Services are in the care of Broecker Funeral\nHome of Salado. Please sign the guestbook: Broeckerfuneralhome.com"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:5ec08c7c-ed14-4961-bf7a-1d6ca06c837b>","<urn:uuid:a6be114a-4eae-445a-9041-1ab210063b6a>"],"error":null}
{"question":"For my security evaluation project: what's the key integration benefit that DigitSec S4 brings to Copado's platform, and what are the different modern approaches to application security testing?","answer":"The key integration benefit is that DigitSec S4 enables Copado users to identify security flaws early in development through a seamless integration available via AppExchange package, making security testing an integral part of the CI/CD pipeline. Modern application security testing approaches have evolved from manual processes to include various automated tools: SAST for static code analysis, DAST for runtime testing, IAST which combines both approaches, MAST for mobile applications, SCA for third-party component analysis, and RASP which can actively prevent attacks at runtime. Each of these approaches provides different capabilities for detecting and addressing security vulnerabilities.","context":["DigitSec and Copado Announce Developer Cybersecurity Integration\nDigitSec, a Seattle-based independent software vendor, and Copado, the leading Salesforce devops platform vendor based in Chicago, this week announced a developer cybersecurity integration. Starting immediately, the DigitSec S4 and Copado integration is available using an AppExchange package. DigitSec S4’s suite of cybersecurity tools are then invoked in a Copado devops pipeline, helping customers to find security flaws in code before they become security incidents.\nDigitSec S4 is a suite of tools that detect and correct application security flaws early in the development process. “Proactive security awareness is very important to Copado customers as they manage their Salesforce development process,” said Blaine Kaho’onei, vice president of alliances for Copado. “The seamless integration of DigitSec S4 into our leading DevOps platform offers Copado customers a very comprehensive DevSecOps solution in their Salesforce CI/CD pipeline to accelerate digital transformation with confidence, quality, and security,” added Mr. Kaho’onei in a press release.\nDigitSec S4 Fills a Gap\nCopado fills a gap in its offerings with this DigitSec S4 integration. Until now, it was up to Copado customers to use Static Application Security Testing (SAST) tools to “shift left” their security efforts. The “shift left” movement refers to giving software developer tools to scan their code for security flaws early in the development lifecycle. Fixing security flaws in production can be literally thousands of times more expensive than fixing them in development. So, the DigitSec S4 value proposition is strong for almost every Copado customer.\nBetter Managed Pipelines\nWith DigitSec S4 now an easy addon to Copado, devops managers now have easier access to the SAST, dynamic application security testing (DAST), software composition analysis (SCA), and interactive application security testing (IAST) features of DigitSec S4. This mitigates the cybersecurity time bombs teams inadvertently leave behind when they take on big projects.\n“The Copado DevOps platform creates great efficiencies for Salesforce development teams. Those efficiencies can also introduce risk if there are no guardrails in place to protect against vulnerabilities and exposures. DigitSec and Copado together will ensure that each time a customer ships, they do so with the confidence that the release is secure and has been analyzed by DigitSec’s comprehensive security testing platform,” DigitSec CEO Waqas Nazir in the same press release.\nMr. Nazir was asked for more details on how the integration is accomplished. He replied, “The integration allows users to run DigitSec’s security scan from within a Copado pipeline via automation templates or directly from a user story.”\nProductive Business Development\nCopado needed to make a deal with a cybersecurity company like DigitSec to offset moves by competitors. Notably, AutoRABIT acquired DigitSec competitor Codescan earlier this year. That deal turned out a bit strange. Codescan says they intend to work as a cybersecurity scanner with AutoRABIT competitors.\nThis Copado-DigitSec deal makes sense for the Salesforce community. This is because it leaves DigitSec alone as a cybersecurity specialist, free to work with other Salesforce devops platforms. It is also an encouraging move by Copado. This level of integration with DigitSec by Copado demonstrates that Copado has a functional partner development ecosystem.\nHopefully this trend continues, which provides more opportunities for independent software vendors and customers alike.","What is Application Security Testing\nApplication security testing (AST) is the process of making applications more resistant to security threats, by identifying security weaknesses and vulnerabilities in source code.\nAST started as a manual process. Today, due to the growing modularity of enterprise software, the huge number of open source components, and the large number of known vulnerabilities and threat vectors, AST must be automated. Most organizations use a combination of several application security tools.\nStatic Application Security Testing (SAST)\nSAST tools use a white box testing approach, in which testers inspect the inner workings of an application. SAST inspects static source code and reports on security weaknesses.\nStatic testing tools can be applied to non-compiled code to find issues like syntax errors, math errors, input validation issues, invalid or insecure references. They can also run on compiled code using binary and byte-code analyzers.\nDynamic Application Security Testing (DAST)\nDAST tools take a black box testing approach. They execute code and inspect it in runtime, detecting issues that may represent security vulnerabilities. This can include issues with query strings, requests and responses, the use of scripts, memory leakage, cookie and session handling, authentication, execution of third-party components, data injection, and DOM injection.\nDAST tools can be used to conduct large-scale scans simulating a large number of unexpected or malicious test cases and reporting on the application’s response.\nInteractive Application Security Testing (IAST)\nIAST tools are the evolution of SAST and DAST tools—combining the two approaches to detect a wider range of security weaknesses. Like DAST tools, IAST tools run dynamically and inspect software during runtime. However, they are run from within the application server, allowing them to inspect compiled source code like IAST tools do.\nIAST tools can provide valuable information about the root cause of vulnerabilities and the specific lines of code that are affected, making remediation much easier. They can analyze source code, data flow, configuration and third-party libraries, and are suitable for API testing.\nMobile Application Security Testing (MAST)\nMAST tools combine static analysis, dynamic analysis and investigation of forensic data generated by mobile applications. They can test for security vulnerabilities like SAST, DAST and IAST, and in addition address mobile-specific issues like jailbreaking, malicious wifi networks, and data leakage from mobile devices.\nSoftware Composition Analysis (SCA)\nSCA tools help organizations conduct an inventory of third-party commercial and open source components used within their software. Enterprise applications can use thousands of third-party components, which may contain security vulnerabilities. SCA helps understand which components and versions are actually being used, identify the most severe security vulnerabilities affecting those components, and understand the easiest way to remediate them.\nRuntime Application Self-Protection (RASP)\nRASP tools evolved from SAST, DAST and IAST. They are able to analyze application traffic and user behavior at runtime, to detect and prevent cyber threats.\nLike the previous generation of tools, RASP has visibility into application source code and can analyze weaknesses and vulnerabilities. It goes one step further by identifying that security weaknesses have been exploited, and providing active protection by terminating the session or issuing an alert.\nRASP tools integrate with applications and analyze traffic at runtime, and can not only detect and warn about vulnerabilities, but actually prevent attacks. Having this type of in-depth inspection and protection at runtime makes SAST, DAST and IAST much less important, making it possible to detect and prevent security issues without costly development work.\nApplication Security Testing Best Practices\nShift security testing left\nNew organizational practices like DevSecOps are emphasizing the need to integrate security into every stage of the software development lifecycle. AST tools can:\n- Help developers understand security concerns and enforce security best practices at the development stage.\n- Help testers identify security issues early before software ships to production.\n- Advanced tools like RASP can identify and block vulnerabilities in source code in production.\nTest internal interfaces, not just APIs and UIs\nIt is natural to focus application security testing on external threats, such as user inputs submitted via web forms or public API requests. However, it is even more common to see attackers exploit weak authentication or vulnerabilities on internal systems, once already inside the security perimeter. AST should be leveraged to test that inputs, connections and integrations between internal systems are secure.\nNew vulnerabilities are discovered every day, and enterprise applications use thousands of components, any of which could go end of life (EOL) or require a security update. It is essential to test critical systems as often as possible, prioritize issues focusing on business critical systems and high-impact threats, and allocate resources to remediate them fast.\nThird-party code security\nOrganizations should employ AST practices to any third-party code they use in their applications. Never “trust” that a component from a third party, whether commercial or open source, is secure. Scan third-party code just like you scan your own. If you discover severe issues, apply patches, consult vendors, create your own fix or consider switching components.\nImperva RASP Solutions\nImperva provides RASP capabilities, as part of its application security platform. Imperva RASP keeps applications protected and provides essential feedback for eliminating any additional risks. It requires no changes to code and integrates easily with existing applications and DevOps processes, protecting you from both known and zero-day attacks.\nIn addition, Imperva provides multi-layered protection to make sure websites and applications are available, easily accessible and safe. These application security solutions include:\n- DDoS Protection—maintain uptime in all situations. Prevent any type of DDoS attack, of any size, from preventing access to your website and network infrastructure.\n- CDN—enhance website performance and reduce bandwidth costs with a CDN designed for developers. Cache static resources at the edge while accelerating APIs and dynamic websites.\n- Cloud WAF—permit legitimate traffic and prevent bad traffic. Safeguard your applications at the edge with an enterprise‑class cloud WAF.\n- Gateway WAF—keep applications and APIs inside your network safe with Imperva Gateway WAF.\n- Attack analytics—mitigate and respond to real security threats efficiently and accurately with actionable intelligence across all your layers of defense.\n- Account takeover protection—uses an intent-based detection process to identify and defends against attempts to take over users’ accounts for malicious purposes.\n- API security—protects APIs by ensuring only desired traffic can access your API endpoint, as well as detecting and blocking exploits of vulnerabilities.\n- Advanced bot protection—analyzes your bot traffic to pinpoint anomalies, identifies bad bot behavior and validates it via challenge mechanisms that do not impact user traffic."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:0bca960c-61bb-43d7-b925-5b83e93e5216>","<urn:uuid:1f9d94c3-cbca-4673-95bf-b8ee71b3962c>"],"error":null}
{"question":"When was the Saint-Martin church built and what are its architectural highlights?","answer":"The Saint-Martin church was built in 1585 using bricks and freestone in the Gothic style. It underwent major restoration in the 1980s and 1990s. The church features a Renaissance-style relief above the entry door depicting Saint Martin sharing his coat. Inside, notable features include the chevet of the choir, well-proportioned nave, and a triptych at the center of the choir. At the foot of the chair of truth, there is a 'mystical delta' symbol of divine mercy from the late 18th century.","context":["Rue de Saint-Martin\n01 January - 31 December\nMon 9.00 - 18.00\nTue 9.00 - 18.00\nWed 9.00 - 18.00\nThu 9.00 - 18.00\nFri 9.00 - 18.00\nSat 9.00 - 18.00\nSun 9.00 - 18.00\nfirstname.lastname@example.org +32-68-26 51 70\nSaturday 6.30 pm\nThe church, constructed of bricks and freestone, was built in 1585.\nThis is a building in the Gothic style that was the object of a major restoration in the 1980s and 1990s.\nOn the outside, above the entry door, a relief with the look of the Renaissance can be seen, which shows us Saint Martin sharing his coat.\nIn the interior, the chevet of the choir, the beautiful proportions of the nave, and the radiance of the whole create a lovely feeling of intimacy.\nOf special note at the centre of the choir, the triptych and at the foot of the chair of truth, the “mystical delta”, a symbol of divine mercy dating from the end of the 18th century.\nAth has the particularity of hosting two invaluable Burial Places, one of which is one of the oldest in Europe. The one we have here dates from 1520-1550. Carved in the white stone of Avesnes, it finally recovered its polychrome colour in 2009 and found its place in an air-conditioned display case. It is believed to have been a piece of a funerary monument of a knight of the Holy Sepulchre, perhaps Jean Zuallart himself. When the Calvary was placed to the right of the church, this burial was placed in a burial chamber under it. It is now protected inside the building.\nOur church has the particularity of having, in the heart of the city, a calvary dated between 1525 and 1575. It seems that with the contemporary Burial, it was the last station of a Way of the Cross erected on the city walls. During the dismantling of the walls, he would have joined the surroundings of the church and would have been placed under the responsibility of the Brotherhood of the Passion.\nOriginally composed of six characters, it now has only five, the statue of Mary Magdalene having been destroyed by the bad weather in 1922. Protected for 20 years by an awning, it bears ancient traces of polychromy.\nDial placed in the spire of the building, which has the particularity of having only one hand. It is in fact one of the oldest clocks in Europe (an orloge in the Middle Ages), since traces of the maintenance of a clock mechanism can be found in the parish accounts of... 1360.\nOriginally placed in the belfry (knocked down in 1774) on the Grand-Place, it was sheltered in the church bell tower, before being restored to its original function after the restoration in the 1990s, its original mechanism being simply helped by modern mechanics.\nAccording to a document found in the parish archives in 2010, the parish priest and the city's aldermen ordered in August 1605 from the painter François de Saive a triptych representing a crucifixion in the middle, surrounded on the left by a carrying of the cross and on the right by a descent of the cross. The back of the side panels depicts a Christ triumphant over death and a mysterious donor. Originally, the portraits of Saint Arnould and Saint Erasmus, which were not represented at all, were to be found. Other paintings by the same master appear in the inventory of the Hopital de la Madeleine founded by Philippe le Bon, Duke of Burgundy.\nAttributed to the Thudinian factor Sébastien Lachapelle, it was delivered in 1748 according to the parish archives. Restored in 1776 by Pieter Van Peteghem, then in 1804 by his son, it originally included about twenty games. Between 1820 and 1911, restorations, notably by the Slootmaekers brothers, profoundly modified the instrument. In 1986, the Delmotte company intervened but the result was not convincing. In 1995, the Schumacher company took over the construction site and restored the organ to its beautiful tone. A new intervention to give back games is planned in the coming months. It should be noted that the rood pole wears medallions representing King David and Saint Cecilia.\nOriginally placed at the entrance of the church, in a small baptistery, they were moved to the Sacred Heart Chapel about ten years ago to be highlighted. This piece, unique for its beauty and sobriety, was donated by Philippe Desmaistres to the parish in 1591. The blue stone tank of Hainaut is circular, the sides are filled with large torus. On the base, one can easily read the monogram of the donor, and on the coping, one discovers the inscription: \"Honorable man Philippe Desmaistres gave the present vase for the church 1591\". A copper cover was placed on the whole in 1803, but it disappeared. It was reconstituted in 2002 by the Clabots de Dinant company.\nAlso noteworthy is the bas-relief above the portal dated from 1585, probably from the old Saint-Martin church in the neighbouring village of Brantignies, representing a Saint Martin sharing the mantle with the poor, all under a stylised Greek temple with a characteristic pediment.\n08.00 - 18.00 Open\n09.00 - 19.00 Open\n09.30 - 17.30 Open\n7911 Frasnes-lez-Buissenal (Frasnes-lez-Anvaing)\n7063 Chaussée-Notre-Dame-Louvignies (Soignies)\n14.00 - 20.00 Open"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:5a7c4f2b-35a5-450f-a07d-68279b3e7284>"],"error":null}
{"question":"How does cloud computing infrastructure work in terms of architecture, and what vulnerabilities arise from its virtualization technology?","answer":"Cloud computing architecture consists of two main parts: the front end (client-side) that users interact with through browsers or interface software, and the back end comprising servers and data storage systems connected via the Internet. The system uses a central server with protocols and middleware to manage traffic and enable communication between networked computers. However, this architecture creates security vulnerabilities through virtualization technology, where physical servers are split into virtual machines. This means multiple organizations could potentially access data stored on the same servers, especially if the provider has weak security controls. Additionally, since maintenance and upgrade management is controlled by the cloud provider, organizations lack control over server configurations, which could compromise business security.","context":["What Is Cloud Computing With Example\nCloud Computing or Cloud is just another word for online. So storing your data on the cloud is just another way of seeing you store your data online. As an example think of email in the past we had an email account your emails were downloaded and stored on your own computer if you lost your computer you lost your email now things have gotten easier with web-based emails like Gmail or yahoo mail your email sits on their servers and you can access it anywhere you have an internet connection. In other words, your email is on the cloud so things like Gmail and Yahoo Mail could be thought of as email on the cloud and it’s the same for storing photos in the past you start photos on your computer on your own hard disk and if you lost your computer you lost your photos.\nNow Flickr allows you to store your photos online or in other words it allows you to store your photos on the cloud, email and photo storage are examples of the many things we do online. These days we have accounting software video editing software even word processing software all operated through your web browser with things like Google Docs and office 365 we even move word processing and Excel spreadsheets online so you can create edit save and collaborate all your documents on the cloud the only software you need to install on your own computer would be a web browser and that’s it.\nNow there are good reasons to do this, first companies like Flickr, Microsoft and Google are far better at keeping in our tithing data than you so it’s far less likely that you’ll lose your photos or documents due to a hard disk crash or a stolen laptop secondly, you don’t have to worry about updates since the software sits on the cloud or under cloud providers servers. The cloud provider usually takes care of the software for you including the updates and everything else lastly its price unlike traditional software that you install in a computer cloud offerings have a different pricing model so instead of forking out $120 for a copy of Microsoft Office, you pay $7 a month.\nIn the long run, this works out to be more but you can always cancel your cloud subscription after a month or two months or three months but you can’t cancel a CD that you bought from Microsoft plus the added benefits of being able to view your documents anywhere in the world on any machine and not having to worry about updating the software means a lot of people and especially companies prefer the cloud computing model and what we described the simplest form of cloud computing that offers the entire software as a service or more commonly known as SAS which is the predominant type of cloud computing you find out there.\nAdvantages Of Cloud Computing\nCloud computing eliminates the capital expense of buying hardware and software and sitting up and running on-site data centers the racks of servers, the round-the-clock electricity for power and cooling it experts for managing the infrastructure it all adds up pretty quickly.\nMost cloud computing services are provided self-service and on-demand so even vast amounts of computing resources can be provisioned in minutes typically with just a few mouse clicks giving businesses a lot of flexibility and taking the pressure off of capacity planning.\n- Global scale\nThe benefits of cloud computing services include the ability to scale elastically in cloud speak that means delivering the right amount of its resources for example more or less computing power storage bandwidth right when it is needed and from the right geographic location.\nOn-site data centers typically require a lot of racking and stacking hardware setup software patching and other time-consuming management tools cloud computing removes the need for many of these tasks so it teams can spend time on achieving more important business goals.\nThe biggest cloud computing services run on a worldwide network of secure data centers which are regularly upgraded to the latest generation of fast and efficient computing hardware this offers several benefits over a single corporate data center including reduced network latency for applications and greater economies of scale.\nMany cloud providers offer a broad set of policies technologies and controls that strengthen your security posture overall helping protect your data apps and infrastructure from potential threats.\nHow Does Cloud Computing Work\nThe cloud computing architecture is comprised of two parts the front end and the back end which are connected by the Internet. The front end represents the computer that you as a client sees, this side requires you to access the cloud computing system gaining access can be simple as using an internet browser or more complex by using a unique interface software which lets you access the cloud. The back end of a cloud computing system is comprised of the computers servers and data storage systems which store all your files and information. This is the part that does all the work there is a central server that administers the system monitoring traffic and client demands to ensure everything runs smoothly.\nIn addition, this central server follows a set of rules known as protocols the central server also uses software called middleware that allows the network computers to communicate with each other. Naturally, cloud computing companies build and redundancy where they save multiple backup copies of your work in case of problems. However, the more clients they have the more storage space they need so cloud computing companies require at least twice the number of storage devices to store all their client’s information.\nWhy choose cloud computing as a viable option for data storage\nBecause cloud computing applications are limitless.\n- Using cloud computing allows you to access your applications and data from anywhere so long as you can link with the cloud through the internet none of your data would be confined to a single hard drive or location.\n- With the movement of your files to the web, you no longer have to pay for expensive high memory computers you simply need a device that is powerful enough to run the middleware needed to connect to the cloud system.\n- In a company-wide setting when you use the cloud your employer will not need to buy software or software licenses for every employee instead they will pay a fee to a cloud computing company to let all their employees access a suite of software online.\n- Servers and digital storage take up physical space which you may have to rent cloud computing companies store your data on their hardware so no physical space is needed at the front end if streamlining the software and hardware will reduce it problems and costs.\n- The cloud computing systems back-end is a network of computers you may be able to take advantage of the network’s combined processing power to speed up operations.\nTypes Of Cloud Computing/Deployment Models\nThere are 4 main types of cloud computing: private clouds, public clouds, hybrid clouds, and community clouds. There are also 3 main types of cloud computing services: Infrastructure-as-a-Service (IaaS), Platforms-as-a-Service (PaaS), and Software-as-a-Service (SaaS).\nWhat is Public, Private, Hybrid and Community Cloud\n- Public cloud\nIn the public cloud, the cloud service provider provides the resources such as applications and stories which are available to the general public over the internet this is the flat form that is open for all. That’s why they are a popular choice for hosting everyday apps like email, customer relationship management, and other business support apps.\nExample: Google – through the world anyone can access the data available on google.\nAdvantages of public cloud\n● Easy and Inexpensive set\nThat means the public cloud shares the same resources with a large number of users so hardware application and bandwidth costs are shared amongst users.\nResources can be selected up or down according to the requirement then.\nDisadvantages of public cloud\n● Low security\nIn public cloud data and a resource are shared publicly therefore it does not insure higher level of security.\n- Private Cloud\nIn a private cloud, the cloud service provider provides resources such as applications and storage which are dedicated to only a particular organization and not shared with other organizations. The organization can also control and customize it to fit their needs. More organizations have moved to private loss due to security concerns private clouds are more expensive and more secure compared to public clouds.\nAdvantages of Private Cloud\n● High security\nPrivate cloud is operated only within a single or nation and not available to the general public therefore it ensures high security and privacy.\n- Hybrid Cloud\nIt is the combination of a public and private cloud that can deploy sensitive and private data on private cloud and use the public cloud to host less critical information hybrid cloud is a cloud computing solution for those seeking a balance of accessibility and security.\nAdvantage of Hybrid Cloud\nThe ability to put sensitive data on a private cloud and keep the bulk of everything else on the public server.\n- Community Cloud\nIt provides cloud computing services to a group of organizations or individuals community clouds are designed for organizations working on joint projects which require a central cloud computing service. It is comparatively more secure than the public but less secure than the private cloud.\nCloud Computing Models/Services\nCloud computing offers different services based on three delivery models. When arranged in a pyramid form. They follow the order of SaaS, PaaS and IaaS.\nWhat is SaaS?\nSaaS or software as a service. It is a service that offers on-demand pay per use of application software to users. Unlike licensed bought programs. This service is platform independent and you don’t need to install the software on your PC. The cloud runs a single instance of the software and makes it available for multiple end-users. This makes cloud computing cheap. All the computing resources responsible for delivering SaaS are entirely managed by the vendor. This service is accessible via a web browser or lightweight client applications.\nWho use Saas – End customers are frequent users of SaaS.\nExample product and services of SaaS.\nPopular SaaS providers offer the following products and services. The Google ecosystem such as Gmail, Google Docs and Google Drive. Microsoft Office 365, HR and helpdesk solutions and customer relationship management services such as Salesforce.\n- Universally accessible from any platform.\n- No need to commute you can work from any place.\n- Excellent for collaborative working.\n- Vendor provides modest software tools.\n- Allows for multi-tenancy.\n- Portability and browser issues.\n- Internet performance may dictate overall performance.\n- Compliance restrictions.\nWhat is PaaS?\nPaaS or platform-as-a-service. This service is mainly a development environment and is made up of a programming language execution environment, an operating system a web server and a database. All of this encapsulate the environment where users can build, compile and run their programs without worrying at the underlying infrastructure. In this model you manage data and the application resources. All other resources are managed by the vendor.\nWho uses PaaS – This is a domain for developers.\nExample products and services of PaaS.\nCloud providers have the following as the PaaS products and services: Amazon Web Services elastic Beanstalk, Google App Engine, Windows Azure, Heroku and force.com\n- Cost-effective rapid development.\n- it’s scalable.\n- Faster market for developers.\n- Easy deployment of web applications.\n- Private or public deployment is possible.\n- Developers are limited to providers languages and tools.\n- Migration issues such as the risk of vendor lock-in.\nWhat is IaaS?\nIaaS or infrastructure-as-a-service this service offers the computing architecture and infrastructure that is it offers all computing resources but in a virtual environment so that multiple users can access them. These resources include data storage, virtualization, servers, and networking. Most vendors are responsible for managing the above resources. If you will use this service you will be responsible for handling other resources such as applications, data, runtime, and middleware\nWho use IaaS – IaaS is mainly for SysAdmin.\nExample products and services of IaaS – They include Amazon EC2, GoGrrid and Rackspace.com\n- The cloud provides the infrastructure.\n- Enhanced scalability dynamic workloads are supported.\n- IaaS is flexible.\n- Security issues.\n- Network and service delays.\nCompanies that use Cloud Computing.\n- Amazon’s AWS or Amazon Web Services :\nWhen it comes to companies using cloud computing, AWS takes the lead. This cloud computing company offers IaaS and PaaS services to its customers. it’s popular for its Elastic Compute cloud EC2. Among other services such as elastic beanstalk, Simple Storage Service (S3) and relational database service or RDS. Apart from the complete suite of cloud computing. It offers other cloud-related services including the internet of things (IOT), cloud security, and mobile services.\n- iCloud :\nThis cloud from Apple is majorly for Apple products and allows you to backup and store all your multimedia and other documents online. This content is then seamlessly integrated into all your devices or apps. In case you access it from them.\n- Microsoft Azure :\nThis cloud is used and offered by Microsoft. It offers IaaS, PaaS, and SaaS for its enterprise software and developer tools. If you have ever used Office 365 products, then you have used SaaS.\n- Google Cloud :\nThe Google cloud platform is a universal cloud for Google’s vast ecosystem and also for other products such as Microsoft Office it allows collaboration, storage of data, and also other services offered by its cloud computing suite.\n- IBM SmartCloud :\nUsing Private, Public, and Hybrid deployment models IBM SmartCloud provides a full range of IaaS, Paas, and SaaS cloud computing services to businesses. Using the pay-as-you-go model, this cloud generates revenue for IBM.\nRecommended : What is Edge Computing\nKeywords: What Does It Mean To This Client, What Do You Do In Cloud Computing, What Is The Purpose Of Cloud Computing, Why Do We Use Cloud Computing, What is the future of cloud computing","There are two sides to every coin, and nowhere is that more evident than on your cloud server. The exact same attributes that make cloud storage servers so cost-effective and convenient also make them much more difficult to secure.\nIn the cloud, maintenance and upgrade management falls entirely to your cloud hosting provider. On one hand, this frees up internal resources, meaning less time (and money) is spent on routine maintenance. On the other hand, you don’t have control over server configurations and upgrades, which could spell trouble where your business security is concerned.\nSimilarly, the same virtualization technology that allows you to rapidly deploy new cloud servers also puts you at risk for data breaches and other cybercriminal events. In cloud computing, physical servers are split into virtual machines, which means other organizations could theoretically access the data stored on them—especially if your provider is lax with security controls.\nThese issues highlight just how diligently business IT support teams must work to secure cloud storage servers and databases. Beating hackers at their own game requires robust, thorough security strategies like those described below.\nEnd-to-end data encryption\nImplementing encryption solutions is one of the most crucial moves you can make to protect your data from leaks and ransomware attacks. Most cloud storage services automatically encrypt data while in transit; however, once that data is saved to your cloud server, there’s no guarantee that it’s secure. And even if a third-party cloud storage service does encrypt the data stored on its servers, the company ultimately holds the key to that information, not you. In other words, if that third party is compromised, your data could go down with it.\nThese facts make a good case for implementing some kind of encryption method before you store data in the cloud, but regardless of whether you use a cloud storage service or have a separate cloud environment, protecting your data with encryption software offers security against brute-force attacks, data leaks and ransomware. Your business IT support provider should be able to help you design an encryption technique for your cloud storage solutions—for instance, at MyITpros, we offer whole-disk encryption for our cloud services.\nSecure data transfers\nKeep in mind that data is not only at risk when it’s sitting on cloud storage servers, it’s also vulnerable when in transit (i.e. while being uploaded, downloaded or moved on your server). Although most cloud service providers encrypt data transfers as a rule, this is not always a given.\nTo ensure data is protected while on the move, make certain that transfers go through secure HTTP access and are encrypted using SSL. Your business IT support provider should be able to help you obtain an SSL certificate and configure your cloud service to use it. You may also want to install HTTPS Everywhere on all devices that connect to your cloud.\nLocal data backups\nThe cloud often lulls business owners into a false sense of security where data integrity is concerned. After all, if one of the main cloud benefits is that your data is backed up automatically, there’s no need to save it locally, right?\nNot necessarily. Hackers know that many businesses don’t save data locally, and they exploit this to their advantage when they launch ransomware attacks. Without local backups, you might feel pressure to surrender large sums of money to get your data back.\nHowever, the FBI recommends that you don’t pay ransoms. Not only is there no guarantee that hackers will play fair and return your data (these are criminals, after all), paying up could brand you as an easy target and make you more likely to be hacked in the future. Backing up your data locally will give you the confidence you need to refute a ransomer’s fee.\nDistributed denial-of-service protections\nYou may already be familiar with distributed denial-of-service (DDoS) attacks in which a hacker drowns your server, website or application with multiple requests for data, essentially rendering it useless. Cybercriminals have been launching denial-of-service attacks for at least 20 years, but today’s access to bots and IoT devices makes it even easier for hackers to coordinate the attacks through multiple systems, hence the term “distributed.”\nYou can fight back by building multi-layered protections into your networks and systems. Using web application firewalls, intrusion protection systems (IPS), load balancers and other tools, you’ll be able to better detect and prevent DDoS attacks and handle high-volume requests that might otherwise paralyze your network. A good managed service provider will be intimately familiar with these controls and able to add them to your infrastructure and servers.\nHackers are constantly at work, which means your IT support provider must be, too. To keep your company’s data, applications, websites and networks secure, you’ll need to stay one step ahead of cybercriminals.\nEssentially, this boils down to identifying system weaknesses before they cost you. You may want to engage your business IT support team in performing a vulnerability assessment, which will involve your provider testing your cloud storage networks to locate weaknesses that may allow hackers in and then developing a plan to address these. Performing these assessments regularly keeps your networks like a shark: always swimming.\nAn unfortunate truth about security threats is that they don’t necessarily come from outside a business. Internal team members with access to sensitive data on the cloud can wreak havoc in their own right, whether they take the form of disgruntled employees stealing confidential data for malicious purposes or negligent personnel accidentally exposing information housed in the cloud.\nRole-based access controls (RBACs) help you regulate the access given to various employees, allowing you to designate which servers and files individual users can open, edit or copy. Administrators can assign roles and privileges to users based on need, authority and the employee’s position within the company. Implementing these controls may also be required to stay compliant with statutory and regulatory requirements for your industry.\nSo what if there are two sides to the cloud computing coin? With the right cloud security controls, the odds will be in your favor every time!"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:b5b53afb-b4c3-4636-baaf-1252a0f62859>","<urn:uuid:7485cb1f-5a98-4332-a04e-ffa03d3c8686>"],"error":null}
{"question":"How do rising temperatures affect both Wisconsin's agricultural production and coral reef ecosystems, and what are the economic implications for both?","answer":"Wisconsin's agricultural production, which leads nationally in cheese and ranks highly in milk, butter, and various crops, faces significant challenges from rising temperatures. Climate change makes it more difficult to grow crops, raise animals, and maintain traditional farming practices. The state's livestock operations are particularly vulnerable to heat stress and drought, which can reduce feed quality and availability. Meanwhile, in marine ecosystems, rising ocean temperatures severely impact coral reefs, which support some of the world's richest ocean ecosystems. This affects both biodiversity and local tourism income. The economic impact is significant in both cases - Wisconsin is seeing declining farmland (over 620,000 acres lost between 2007-2012) affecting its agricultural economy, while coral reef degradation threatens the tourism industry and the millions of people who depend on ocean ecosystems for their income.","context":["Global Warming Likely to Hurt Wisconsin Agriculture\nAgriculture has been a critical dimension of Wisconsin from early settlement and the logging era, through industrialization, and remains an important economic, social and cultural component of the Dairy State. Wisconsin ranks first nationally in cheese production, and second for milk and butter production. Yet Wisconsin is also second in milk cows, oats, carrots, and sweet corn used in processing. It remains the national leader in processed snap beans, cranberries, corn for silage, mink pelts and milk goats. It is also among the top five states for important agricultural commodities such as potatoes, maple syrup, mint for oil, and cucumbers for pickles.\nContinue global warming is predicted to bring increases in both the frequency and the severity of droughts and floods, resulting in increased challenges for Wisconsin farmers and livestock operations. Warmer water temperatures are also likely to cause the habitat ranges of many fish species to shift and disruption of ecosystems already stressed by human activity. Overall, climate change in Wisconsin resulting from global warming will make it more difficult to grow crops, raise animals, and catch fish in the same ways and places as in the past.\nChanges in the frequency and severity of droughts and floods could pose challenges for farmers and ranchers. Meanwhile, warmer water temperatures are likely to cause the habitat ranges of many fish species to shift, which could disrupt ecosystems. Overall, climate change could make it more difficult to grow crops, raise animals, and catch fish in the same ways and same places as we have done in the past.\nHeat waves, which are projected to increase under climate change, could directly threaten livestock. A number of states have each reported losses of more than 5,000 animals from just one heat wave. Heat stress affects animals both directly and indirectly. Over time, heat stress can increase vulnerability to disease, reduce fertility, and reduce milk production.\nDrought may threaten pasture and feed supplies. Drought reduces the amount of quality forage available to grazing livestock. Some areas could experience longer, more intense droughts, resulting from higher summer temperatures and reduced precipitation. For animals that rely on grain, changes in crop production due to drought could also become a problem.\nClimate change may increase the prevalence of parasites and diseases that affect livestock.The earlier onset of spring and warmer winters could allow some parasites and pathogens to survive more easily. In areas with increased rainfall, moisture-reliant pathogens could thrive.\nIncreases in carbon dioxide (CO2) may increase the productivity of pastures, but may also decrease their quality. Increases in atmospheric CO2 can increase the productivity of plants on which livestock feed. However, studies indicate that the quality of some of the forage found in pasture lands decreases with higher CO2. As a result, livestock would need to eat more to get the same nutritional benefits.\nThe negative effects of climate change will be exacerbated by continuing declines in the acreage of Wisconsin farmland. Wisconsin lost more than 620,000 acres of farmland from 2007 to 2012, a 4% decrease according to the Census of Agriculture, which is conducted every five years by the U.S. Department of Agriculture. Likely causes of these declines are urban sprawl development, enabled by federal, state and local highway expansions which replace vegetation and topsoil with heat-absorbing Portland cement or asphalt in order to facilitate continued growth in motor vehicle driving (more fossil fuel burning) and more employment and profits for the construction and bridge building industry.\nSources: U.S. EPA; U. S. Department of Agriculture; Wisconsin Initiative on Climate Change Impact","Indian Ocean coral, bleached, but able to recover. Image: By CC BY-SA 2.0 fr, via Wikimedia Commons\nHeat extremes on land can kill. Ocean heat waves can devastate coral reefs and other ecosystems – and these too are on the increase.\nHeat extremes on the high seas are on the increase, with ocean heat waves disturbing ecosystems in two hemispheres and two great oceans, US scientists report.\nAnd these same sudden rises in sea temperatures don’t just damage coral reefs, they kill the corals and start the process of reef decay, according to a separate study by Australian researchers.\nAndrew Pershing of the Gulf of Maine Research Institute and colleagues report in the Proceedings of the National Academy of Sciences that they examined data from 65 marine ecosystems over the years 1854 to 2018 to work out how frequently ocean temperatures suddenly rose to unexpected levels.\nThey found such deviations from the average in the Arctic, North Atlantic, eastern Pacific and off the Australian coasts. They expected to find evidence of occasional hot flushes. But they did not expect to find quite so many.\n“Severe marine heatwave events can have a far more severe impact than coral bleaching – the animal dies and its underlying skeleton is all that remains”\n“Across the 65 ecosystems we examined, we expected about six or seven of them would experience these ‘surprises’ each year,” Dr Pershing said. “Instead, we’ve seen an average of 12 ecosystems experiencing these warming events each year over the past seven years, including a high of 23 ‘surprises’ in 2016.”\nIntense and sudden changes in sea temperatures affect crustaceans, algae, corals, molluscs and many millions of humans who depend on the oceans for income. And a new study by researchers from Australian universities reports that even a rise of 0.5°C is reflected in deaths during an outbreak of coral bleaching.\nCorals live in symbiosis with algae: ocean warming periodically disturbs this normally beneficial relationship. The coral animals evert (turn out) the algae and once-lurid reefs will bleach, and become more vulnerable to disease.\nCorals support the world’s richest ocean ecosystems so such changes are a challenge, both to the survival of biodiversity and to local incomes from the tourism linked to the beauty of the reefs.\nVery warm water\n“What we are seeing is that severe marine heatwave events can have a far more severe impact than coral bleaching: the water temperatures are so warm that that the coral animal doesn’t bleach – in terms of a loss of its symbiosis – the animal dies and its underlying skeleton is all that remains,” said Tracy Ainsworth of the University of New South Wales.\nThe researchers report in the journal Current Biology that they used computer tomography scanning techniques to explore the marine destruction. In 2016, more than 30% of the northern part of Australia’s Great Barrier Reef experienced temperatures higher than those in which corals can survive.\n“We find that the skeleton is immediately overgrown by rapid growth of algae and bacteria,” said Bill Leggat of the University of Newcastle, a co-author.\n“We show that this process is devastating not just for the animal tissue but also for the skeleton that is left behind, which is rapidly eroded and weakened.” − Climate News Network\nAbout the Author\nTim Radford is a freelance journalist. He worked for The Guardian for 32 years, becoming (among other things) letters editor, arts editor, literary editor and science editor. He won the Association of British Science Writers award for science writer of the year four times. He served on the UK committee for the International Decade for Natural Disaster Reduction. He has lectured about science and the media in dozens of British and foreign cities.\nBook by this Author:\nScience that Changed the World: The untold story of the other 1960s revolution\nby Tim Radford.\nLife After Carbon: The Next Global Transformation of Cities\nby Peter Plastrik , John Cleveland\nThe future of our cities is not what it used to be. The modern-city model that took hold globally in the twentieth century has outlived its usefulness. It cannot solve the problems it helped to create—especially global warming. Fortunately, a new model for urban development is emerging in cities to aggressively tackle the realities of climate change. It transforms the way cities design and use physical space, generate economic wealth, consume and dispose of resources, exploit and sustain the natural ecosystems, and prepare for the future. Available On Amazon\nby Elizabeth Kolbert\nOver the last half-billion years, there have been Five mass extinctions, when the diversity of life on earth suddenly and dramatically contracted. Scientists around the world are currently monitoring the sixth extinction, predicted to be the most devastating extinction event since the asteroid impact that wiped out the dinosaurs. This time around, the cataclysm is us. In prose that is at once frank, entertaining, and deeply informed, New Yorker writer Elizabeth Kolbert tells us why and how human beings have altered life on the planet in a way no species has before. Interweaving research in half a dozen disciplines, descriptions of the fascinating species that have already been lost, and the history of extinction as a concept, Kolbert provides a moving and comprehensive account of the disappearances occurring before our very eyes. She shows that the sixth extinction is likely to be mankind's most lasting legacy, compelling us to rethink the fundamental question of what it means to be human. Available On Amazon\nClimate Wars: The Fight for Survival as the World Overheats\nby Gwynne Dyer\nWaves of climate refugees. Dozens of failed states. All-out war. From one of the world’s great geopolitical analysts comes a terrifying glimpse of the strategic realities of the near future, when climate change drives the world’s powers towards the cut-throat politics of survival. Prescient and unflinching, Climate Wars will be one of the most important books of the coming years. Read it and find out what we’re heading for. Available On Amazon\nFrom The Publisher:\nPurchases on Amazon go to defray the cost of bringing you InnerSelf.comelf.com, MightyNatural.com, and ClimateImpactNews.com at no cost and without advertisers that track your browsing habits. Even if you click on a link but don't buy these selected products, anything else you buy in that same visit on Amazon pays us a small commission. There is no additional cost to you, so please contribute to the effort. You can also use this link to use to Amazon at any time so you can help support our efforts."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:89c2917c-c46a-481f-9382-af11bf59d309>","<urn:uuid:06cba509-9aa0-4627-ae97-730f6e3146e7>"],"error":null}
{"question":"As a student learning about Earth's motion, I'm curious about the planet's orbital speed and the main cause of seasons - how are they connected?","answer":"Earth's orbital speed varies between 18.2 miles per second (65,518 mph) at aphelion around July 4th and 18.8 miles per second (67,741 mph) at perihelion around January 3rd. However, this orbital speed variation and the changing distance from the Sun are not the main causes of seasons. The seasons are primarily caused by Earth's axis being tilted 23.4 degrees relative to its orbital plane. This tilt means that for half the year, one hemisphere is more directly exposed to the Sun's rays than the other, with the maximum exposure occurring around June 21 for the Northern Hemisphere and December 21 for the Southern Hemisphere.","context":["The Earth orbits the Sun once every 365.256363 (mean solar) days relative to the distant stars. The Earth’s orbital speed ranges from 18.2 miles per second at aphelion, around July 4th, to 18.8 miles per second at perihelion, around January 3rd. In units we’re perhaps more familiar with, that’s 65,518 mph at aphelion and 67,741 mph at perihelion. That’s a difference of 2,223 miles per hour!\nAs we are on a spinning globe, the direction towards which the Earth is orbiting is different at different times of the day. When the Sun crosses the celestial meridian, due south, at its highest point in the sky around noon (1:00 p.m. daylight time), the Earth is orbiting towards your right (west) as you are facing south. Since the Earth is orbiting towards the west, the Sun appears to move towards the east, relative to the background stars—if we could see them during the day. Since there are 360° in a circle and the Earth orbits the Sun in 365.256363 days (therefore the Sun appears to go around the Earth once every 365.256363 days relative to the background stars), the Sun’s average angular velocity eastward relative to the background stars is 360°/365.256363 days = 0.9856° per day.\nThe constellations through which the Sun moves are called the zodiacal constellations, and historically the zodiac contained 12 constellations, the same number as the number of months in a year. But Belgian astronomer Eugène Delporte (1882-1955) drew up the 88 constellation boundaries we use today, approved by the IAU in 1930, so now the Sun spends a few days each year in the non-zodiacal constellation Ophiuchus, the Serpent Bearer. Furthermore, because the Earth’s axis is precessing, the calendar dates during which the Sun is in a particular zodiacal constellation is gradually getting later.\nAstrologically, each zodiacal constellation has a width of 30° (360° / 12 constellations = 30° per constellation). But, of course, the constellations are different sizes and shapes, so astronomically the number of days the Sun spends in each constellation varies. Here is the situation at present.\nThe apparent path the Sun takes across the sky relative to the background stars through these 13 constellations is called the ecliptic. A little contemplation, aided perhaps by a drawing, will convince you that the ecliptic is also the plane of the Earth’s orbit around the Sun. The Moon never strays very far from the ecliptic in our sky, since its orbital plane around the Earth is inclined at a modest angle of 5.16° relative to the Earth’s orbital plane around the Sun. But, relative to the Earth’s equatorial plane, the inclination of the Moon’s orbit varies between 18.28° and 28.60° over 18.6 years as the line of intersection between the Moon’s orbital plane and the ecliptic plane precesses westward along the ecliptic due to the gravitational tug of war the Earth and the Sun exert on the Moon as it moves through space. This steep inclination to the equatorial plane is very unusual for such a large moon. In fact, all four satellites in our solar system that are larger than our Moon (Ganymede, Titan, Callisto, and Io) and the one that is slightly smaller (Europa) all orbit in a plane that is inclined less than 1/2° from the equatorial plane of their host planet (Jupiter and Saturn).\nSince the Moon is never farther than 5.16° from the ecliptic, its apparent motion through our sky as it orbits the Earth mimics that of the Sun, only the Moon’s angular speed is over 13 times faster, completing its circuit of the sky every 27.321662 days, relative to the distant stars. Thus the Moon moves a little over 13° eastward every day, or about 1/2° per hour. Since the angular diameter of the Moon is also about 1/2°, we can easily remember that the Moon moves its own diameter eastward relative to the stars every hour. Of course, superimposed on this motion is the 27-times-faster-yet motion of the Moon and stars westward as the Earth rotates towards the east.\nNow, take a look at the following table and see how the Moon’s motion mimics that of the Sun throughout the month, and throughout the year.\nSo, if you aren’t already doing so, take note of how the Moon moves across the sky at different phases and times of the year. For example, notice how the full moon (nearest the summer solstice) on June 27/28 rides low in the south across the sky. You’ll note the entry for the “Jun 21” row and “Full” column is “Low”. And, the Sun entry for that date is “High”. See, it works!","Causes and effects\nIllumination of Earth at each change of astronomical season\nThe seasons result from the Earth's axis of rotation being tilted with respect to its orbital plane by an angle of approximately 23.4 degrees. (This tilt is also known as \"obliquity of the ecliptic\".)\nFour temperate and subpolar seasons\nRegardless of the time of year, the northern and southern hemispheres always experience opposite seasons. This is because during summer or winter, one part of the planet is more directly exposed to the rays of the Sun (see Fig. 1) than the other, and this exposure alternates as the Earth revolves in its orbit. For approximately half of the year (from around March 20 to around September 22), the Northern Hemisphere tips toward the Sun, with the maximum amount occurring on about June 21. For the other half of the year, the same happens, but in the Southern Hemisphere instead of the Northern, with the maximum around December 21. The two instants when the Sun is directly overhead at the Equator are the equinoxes. Also at that moment, both the North Pole and the South Pole of the Earth are just on the terminator, and hence day and night are equally divided between the two hemispheres. Around the March equinox, the Northern Hemisphere will be experiencing spring as the hours of daylight increase, and the Southern Hemisphere is experiencing autumn as daylight hours shorten.\nThe effect of axial tilt is observable as the change in day length and altitude of the Sun at solar noon (the Sun's culmination) during the year. The low angle of Sun during the winter months means that incoming rays of solar radiation are spread over a larger area of the Earth's surface, so the light received is more indirect and of lower intensity. Between this effect and the shorter daylight hours, the axial tilt of the Earth accounts for most of the seasonal variation in climate in both hemispheres.\nIllumination of Earth by Sun at the northern solstice.\nIllumination of Earth by Sun at the southern solstice.\nDiagram of the Earth's seasons as seen from the north. Far right: southern solstice\nDiagram of the Earth's seasons as seen from the south. Far left: northern solstice\nAnimation of Earth as seen daily from the Sun looking at UTC+02:00, showing the solstice and changing seasons.\nTwo images showing the amount of reflected sunlight at southern and northern summer solstices respectively (watts / m²).\nElliptical Earth orbit\nCompared to axial tilt, other factors contribute little to seasonal temperature changes. The seasons are not the result of the variation in Earth's distance to the Sun because of its elliptical orbit. In fact, Earth reaches perihelion (the point in its orbit closest to the Sun) in January, and it reaches aphelion (the point farthest from the Sun) in July, so the slight contribution of orbital eccentricity opposes the temperature trends of the seasons in the Northern Hemisphere. In general, the effect of orbital eccentricity on Earth's seasons is a 7% variation in sunlight received.\nOrbital eccentricity can influence temperatures, but on Earth, this effect is small and is more than counteracted by other factors; research shows that the Earth as a whole is actually slightly warmer when farther from the sun. This is because the Northern Hemisphere has more land than the Southern, and land warms more readily than sea.\nAny noticeable intensification of southern winters and summers due to Earth's elliptical orbit is mitigated by the abundance of water in the Southern Hemisphere.\nMaritime and hemispheric\nSeasonal weather fluctuations (changes) also depend on factors such as proximity to oceans or other large bodies of water, currents in those oceans, El Niño/ENSO and other oceanic cycles, and prevailing winds.\nIn the temperate and polar regions, seasons are marked by changes in the amount of sunlight, which in turn often causes cycles of dormancy in plants and hibernation in animals. These effects vary with latitude and with proximity to bodies of water. For example, the South Pole is in the middle of the continent of Antarctica and therefore a considerable distance from the moderating influence of the southern oceans. The North Pole is in the Arctic Ocean, and thus its temperature extremes are buffered by the water. The result is that the South Pole is consistently colder during the southern winter than the North Pole during the northern winter.\nThe seasonal cycle in the polar and temperate zones of one hemisphere is opposite to that of the other. When it is summer in the Northern Hemisphere, it is winter in the Southern, and vice versa.\nThe tropical and subtropical regions see little annual fluctuation of sunlight. However, seasonal shifts occur along a rainy, low-pressure belt called the Intertropical Convergence Zone (ICZ). As a result, the amount of precipitation tends to vary more dramatically than the average temperature. When the Zone is north of the Equator, the northern tropics experience their wet season while the southern tropics have their dry season. This pattern reverses when the Zone migrates to a position south of the Equator.\nMid-latitude thermal lag\nIn meteorological terms, the solstices (the maximum and minimum insolation) do not fall in the middles of summer and winter. The heights of these seasons occur up to 7 weeks later because of seasonal lag. Seasons, though, are not always defined in meteorological terms.\nIn astronomical reckoning by hours of daylight alone, the solstices and equinoxes are in the middle of the respective seasons. Because of seasonal lag due to\nthermal absorption and release by the oceans, regions with a continental climate, which predominate in the Northern Hemisphere, often consider these four dates to be the start of the seasons as in the diagram, with the cross-quarter days considered seasonal midpoints. The length of these seasons is not uniform because of Earth's elliptical orbit and its different speeds along that orbit."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:c735edb1-0524-4dd2-b6f5-8a3bbad94e82>","<urn:uuid:eb842ea3-c43c-477a-af91-9e1f062f89c1>"],"error":null}
{"question":"How do financial ties between industry and professional organizations impact valuation processes and diagnostic guidelines, and what are the limitations of disclosure policies?","answer":"Financial ties significantly impact both valuation processes and diagnostic guidelines. In valuation contexts, experts must carefully analyze company profitability, growth, and market realities, ensuring that valuation multiples and assumptions are reasonable and supportable. However, when it comes to professional organizations, financial relationships can compromise objectivity. For example, nearly 70% of DSM-5 task force members have financial relationships with pharmaceutical companies, raising concerns about bias. Disclosure policies have severe limitations - they merely shift from 'secret bias' to 'open bias,' can overwhelm readers with too much information, and may actually increase bias through 'moral licensing,' where experts feel absolved of responsibility after disclosure. Research shows that disclosure can lead to greater bias in advice-giving, as advisers may exaggerate to counteract anticipated discounting of their advice.","context":["You should be!\nThe valuation of ESOP stock is of particular importance to the ESOP trustees and ESOP participants. One of an ESOP trustees’ fiduciary responsibilities is to understand and approve the ESOP stock valuation. For participants, the ESOP stock value directly impacts their retirement benefits. These issues are enough keep you up at night!\nSince ESOP valuations can seem daunting and confusing, we put together the following to help ESOP trustees and participants get a better night’s sleep!\nFirst Things First!\nESOP valuations are somewhat unique and require that the valuation expert have sufficient experience in order to recognize the nuances of them. A business appraiser that only handles a limited number of ESOP engagements may not recognize certain issues that are specific to ESOPs.\nAsk your appraiser about his/her ESOP valuation experience. How many ESOP valuations has he/she prepared? The ESOP appraiser should be knowledgeable of relevant ERISA and U.S. Department of Labor guidelines and issues.\nPinocchio had Jiminy Cricket but the ESOP valuation expert is your guide to thoroughly understand the valuation process. Not only should the ESOP appraiser lead the trustee through the valuation methodology and process, he/she should also make sure that the trustee understands the company specific factors that contribute to or detract from the ESOP stock value. The ESOP valuation expert should be ready, willing and able to answer ALL of your questions, no matter how simple or complex. Asking questions will foster a better understanding of the valuation process and conclusion.\nESOP conferences, such as those presented by The ESOP Association and The National Center For Employee Ownership, are a great resource for identifying ESOP related issues, providing a forum where questions are more than welcome and give the opportunity to network with other ESOP companies and share experiences and solutions.\nA + B = Value\nThe ESOP stock value is a function of both controllable (internal) and uncontrollable (external) factors. Understanding what macro factors (technology, economy, competition) impact value and the critical company characteristics (capacity, reputation, depth of management) will give trustees and participants insight into what they can do to improve the ESOP stock value.\nThe company’s profitability and growth have the greatest impact on the ESOP stock value. Hence, every employee/ESOP participant can do their part to contribute to the company’s profitability by working efficiently to increase productivity and providing customer service in a manner that enhances the company’s reputation in the industry. Management has a responsibility to put the company in the best position to be innovative, expand markets and products/services and compete.\nAnalyze/vet/confirm the valuation’s underlying assumptions. Ensure that the appraiser was aware of and focused on ALL of the key issues impacting the company’s value. The underlying assumptions are typically developed using the company’s history as a base so make sure to fact check the analyses.\nWhat the little voice is saying\nThe valuation expert examines the company, prepares financial analyses, creates models and ultimately, opines to the conclusion of the value of the ESOP stock. However, it is the responsibility of the trustees to determine if the conclusion makes sense before approving it.\nThe trustee should: (i) make sure that the valuation expert utilized the company’s most recent data (financial and other) and that it was used in an appropriate manner; (ii) confirm that the key issues and factors cited by the appraiser are consistent with management’s view of the business; (iii) verify that underlying assumptions are steeped in reality; and (iv) establish that growth expectations, which can have a significant impact on value, are reasonable, achievable and consistent with management’s and industry view.\nThe ESOP stock valuation should be consistent with market realities. The trustee will want to verify that the valuation multiples applied are reasonable for the company given its size, performance and industry. Required returns should take into account all of the company’s risk factors, including those specific to the industry.\nThe valuation report will bring all of the pieces together in a cohesive manner. A good report provides valuable insight into the key elements of the valuation (analyses, methodologies and conclusions) in a way that is easily followed and understood by the layman. The company’s “story” should be consistent from beginning to end.\nThe More You Know\nIn summary, becoming familiar with the valuation process from beginning to end is the best way for the ESOP trustees to become more comfortable with and confident that the ESOP stock value is reasonable and supportable. Sharing the valuation findings with ESOP participants will give them some degree of comfort that their retirement nest egg will be there for them when the time comes.\nSo now, sleep tight and don’t let the valuation bugs bite!","Citation: The PLoS Medicine Editors (2012) Does Conflict of Interest Disclosure Worsen Bias? PLoS Med 9(4): e1001210. https://doi.org/10.1371/journal.pmed.1001210\nPublished: April 24, 2012\nCopyright: © 2012 PLoS Medicine Editors. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nFunding: The authors are each paid a salary by the Public Library of Science, and they wrote this editorial during their salaried time.\nCompeting interests: The authors' individual competing interests are at http://www.plosmedicine.org/static/editorsInterests.action. PLoS is funded partly through manuscript publication charges, but the PLoS Medicine Editors are paid a fixed salary (their salary is not linked to the number of papers published in the journal).\nAbbreviations: APA, American Psychiatric Association; COI, conflict of interest; DSM, Diagnostic and Statistical Manual of Mental Disorders\nProvenance: Written by editorial staff; not externally peer reviewed.\nThe PLoS Medicine Editors are Virginia Barbour, Jocalyn Clark, Melissa Norton, Paul Simpson, and Emma Veitch.\nOn March 13, 2012 PLoS Medicine published an analysis by Lisa Cosgrove and Sheldon Krimsky  that examined the financial conflicts of interest of members of the American Psychiatric Association (APA) responsible for updating the Diagnostic and Statistical Manual of Mental Disorders (DSM), the so-called bible of psychiatry. Despite a new APA policy designed to address conflicts of interest (COIs), nearly 70% of current DSM-5 task force members have financial relationships with pharmaceutical companies, up from 57% for the manual's previous version. 83% of current contributors to the psychotic disorders section, and everyone responsible for the sleep disorder section, have links to the pharmaceutical industry. Wide media coverage and commentary about these findings – have raised concerns that so many of the experts charged with the responsibility of defining mental health conditions and treatments have financial ties to the very companies that sell drug treatments for mental health. It is widely established that financial conflicts of interest impair objectivity and integrity in medicine.\nConcerns about the conflicts of interests associated with the APA—undeniably the leading authority for psychiatry and mental health—are critical, not least because of the association's legacy of involvement with the pharmaceutical industry: the psychiatric profession receives more money than any other medical specialty  and has recently been scandalized by cases of ghostwriting and publication bias. Their judgments define mental illness, thus legitimizing some disorders and denying others, and determine what warrants treatment and how. The DSM is used by insurance companies, hospitals, courts, prisons, schools, researchers, regulators, and government agencies to define who is sick/abnormal and who is not. The expansion of diagnostic categories and new diagnoses (and thus markets) in every DSM is said to be a virtual “bonanza for the pharmaceutical industry” . And on the other side of the coin, the DSM is a boon for the APA, which sold over a million copies of the DSM-IV; 20% of APA funding is said to now come from pharmaceutical companies .\nCosgrove and Krimsky also identified several worrying gaps in the APA's new COI policy (previous DSMs in 1952, 1968, and 1980 were not subject to COI policies). While the policy limits the amount panel members can receive from drug companies annually to US$10,000 and of their company stock holdings to US$50,000, these are still considerable amounts. (Even small gifts invoke obligations to reciprocate ). Worse, the policy does not consider unrestricted research grants from pharmaceutical companies to be problematic and does not require they be disclosed. Participation in lucrative speakers' bureaus (networks of prominent physicians designed to influence communities of prescribers and usually forbidden in medical schools) is likewise permitted under the APA's policy, and the monies received for participation are required only to be reported as honoraria, thus concealing their true genesis. The APA has responded to the PLoS Medicine analysis by saying that the DSM-5 development process “is the most open and transparent of any previous edition of the DSM” .\nBut are disclosure mandates simply a band-aid on a unrelenting problem of bias?\nDisclosure is generally considered preferable to nondisclosure, because it makes explicit and transparent details that are important to the interpretation, credibility, and value of the information presented—vital in the context of clinical decision-making and patient care. But the overemphasis and reliance on disclosure policies is exactly what leaves the real problem of the conflict of interest unaddressed.\nDisclosure has severe limits as a strategy for mitigating bias. Cosgrove and Krimsky mention three reasons: that disclosure alone merely shifts “secret bias” to “open bias”; that it sometimes involves so much information about ties to the industry, for example, that the reader is blinded by the sheer “signal to noise ratio”; and that disclosure may be perceived as absolving a person from their responsibility for managing their conflict .\nEven more compelling is evidence emerging from the social sciences that suggests disclosure to be not only ineffective but also regressive. Decision scientist George Loewenstein and colleagues have argued that disclosure can actually lead doctors to give biased advice, either through strategic exaggeration (whereby more biased advice is provided to counteract anticipated discounting), or “moral licensing” such that advice is legitimized because advisees “have been warned” (that is, caveat emptor or “buyer beware”) . Their experiments have essentially shown that bias is considerably greater when conflicts of interest are disclosed. Worse, because Loewenstein and colleagues have demonstrated that advisees (i.e., patients) both think that their advisers (i.e., doctors) would never intentionally mislead them and tend not to discount advice in light of conflicts, disclosure policies will never be the solution and are very likely exacerbating the problem of bias in medicine .\nExtending this analysis to the APA's DSM, the result would be disastrous if the public or physicians were to disregard their concerns about financial conflicts of interest in deference to the authority of the APA. And if clinical experts were to believe that disclosure alone made them impervious to bias, their advice forming the DSM may be even more favorable toward the pharmacological products and markets their industry funders seek and uphold.\nIndeed, if disclosure worsens bias, then this is a game-changer for discussion and debate about managing conflicts of interest in medicine. Journals, professional associations, clinical guideline developers, and others need to worry not just that disclosure provides a band-aid to the real problem of the COI itself, but that any attempt to stem the trouble through disclosure policies may actually be worsening the problem.\nWrote the first draft of the manuscript: JC. Contributed to the writing of the manuscript: VB JC MN PS EV.\n- 1. Cosgrove L, Krimsky S (2012) A Comparison of DSM-IV and DSM-5 Panel Members' Financial Associations with Industry: A Pernicious Problem Persists. PLoS Med 9: e1001190.\n- 2. [No author listed] (16 March 2012) DSM-5 Criticized for Financial Conflicts of Interest. ABC News. Available: http://www.12newsnow.com/story/17154951/dsm-5-criticized-for-financial-conflicts-of-interest. Accessed 27 March 2012.\n- 3. Ledford H (16 March 2012) Industry ties remain rife on panels for psychiatry manual. Nature News. Available: http://www.nature.com/news/industry-ties-remain-rife-on-panels-for-psychiatry-manual-1.10206. Accessed 27 March 2012.\n- 4. Aldhous P (13 March 2012) Many authors of psychiatry bible have industry ties. New Scientist. http://www.newscientist.com/article/dn21580-many-authors-of-psychiatry-bible-have-industry-ties.html. Accessed 27 March 2012.\n- 5. Yeung Y (16 March 2012) Study finds ties to drugmakers on mental health panel. UT San Diego News. Available: http://www.utsandiego.com/news/2012/mar/16/study-finds-ties-to-drugmakers-on-mental-health-pa/. Accessed 27 March 2012.\n- 6. Angell M (14 July 2011) The Illusions of Psychiatry. The New York Review of Books. Available: http://www.nybooks.com/articles/archives/2011/jul/14/illusions-of-psychiatry/?pagination=false. Accessed 27 March 2012.\n- 7. Katz D, Caplan AL, Merz JF (2003) All gifts large and small: Toward an understanding of the ethics of pharmaceutical industry gift-giving. Am J Bioeth 3: 39–46.\n- 8. Loewenstein G, Sah S, Cain DM (2012) The unintended consequences of conflict of interest disclosure. JAMA 307: 669–670.\n- 9. Frances A (26 June 2009) A Warning Sign on the Road to DSM-V: Beware of Its Unintended Consequences. Psychiatric Times. Available: http://www.psychiatrictimes.com/print/article/10168/1425378. Accessed 27 March 2012."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:95f3ae88-8cea-4ba1-a417-471dcd167f96>","<urn:uuid:eb4564c3-164c-4009-aeb4-91c95ce68f75>"],"error":null}
{"question":"How do the animals in Hopster Coding Safari represent different continents?","answer":"The game features seven different animals, each representing a specific continent: Bear represents North America, Fox represents Europe, Tiger represents Asia, Elephant represents Africa, Monkey represents South America, Penguin represents Antarctica, and Kangaroo represents Australia. These animals are used in various logic problems throughout the game.","context":["- Size 62.1 MB\n- Ratings 789\n- AgeRating 4+ [?]\n- Developer: Plato Media Ltd.\n- Website: Official website\n- Release Date: 2018-06-24\n- Supports: Family Sharing\n- Languages: English\n- Compatibility: Requires iOS 8.0 or later. Compatible with iPhone, iPad, and iPod touch.\nIntroducing kids to coding early is seen as vitally important by parents, educators, and tech leaders. Give your child a head start in learning how to code with Hopster Coding Safari.\nWe have created this animal themed pre-coding logic game to help kids learn the fundamentals of coding. Children are presented with a series of logic problems to solve, getting different animals to where they need to be.\nWhat your child will learn from Hopster Coding Safari:\n\\- The fundamentals of computational thinking\n\\- Foundations of coding\n\\- Problem solving\nThis game will engage young children in computational thinking - the fundamental techniques needed to understand coding - without them realising they are learning. It will also ignite their passion for puzzles and problem-solving - all while playing a fun game with animals.\nIt could be the Tiger that needs to get back to her den, the Penguin trying to return to its Antarctic Ice Hole, the hungry Monkey desperate to get to the banana tree, or one of the other animals in this app - here, play is at the heart of learning to code.\nThe tasks in Hopster Coding Safari become increasingly complex to resolve, and completing them gently guides your child through the four fundamental techniques of computational thinking:\n\\- Pattern recognition\nThese are the basic techniques children need in order to learn how to code. So our game doesn't teach any specific programming languages but gives young children these foundations from which they will build on and learn to code.\nOur Learning Goals:\n1\\. Introducing children to computational thinking\n• Going through a problem-solving process\n• Breaking it down & looking for patterns\n• Developing a step-by-step solution\n• Reflecting on their choices\n2\\. Delivering key computer science standards for UK (Early Years & Key Stage 1) and US Curriculum (Kindergarten through 2nd Grade)\n• Understand what algorithms are\n• Create and follow algorithms\n• Model the way programs store and manipulate data by using symbols to represent information\n• Develop programs with sequences and simple loops, to express ideas or address a problem\n• Decompose the steps needed to solve a problem into a precise sequence of instructions\n• Debug errors in algorithm that includes sequences and simple loops\n• Use logical reasoning to predict the behaviour of simple programs\nToddlers and preschoolers will love the animal theme, and each one has been chosen to represent one of the 7 continents - Bear (North America), Fox (Europe), Tiger (Asia), Elephant (Africa), Monkey (South America), Penguin (Antarctica), and Kangaroo (Australia).\nBuilt by the team behind Hopster, the award-winning kids learning and entertainment app, Hopster Coding Safari is 100% safe and ad-free.\nUnlock the second coding world through a single in-app purchase or by using your existing Hopster subscription*\n*You will need to create a Hopster login to use an existing subscription across other apps and platforms.\n- Nurikabe puzzle\n- Nurikabe Vault\n- Nurse & Newborn Baby - Hospital Makeover & Dress Up\n- Nurse Girl Dress Up\n- Nurse in Crazy Hospital - Dress Up Game for Girls and Kids\n- Nurse Last Minute Makeover\n- Nurse New-Born Baby Hospital Baby Care Rush game\n- Nurse Vacation Winter Fun : The Snowboard Cold Sports Girls Weekend\n- Nurse's Aide Exam Success\n- Nurse's Wedding Love Story!\n- Nursery Alphabets\n- Nursery Baby Care\n- Nursery Baby Care and Fun\n- Nursery baby caring center - kids hospital game"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7543e114-fcfd-41ec-a65f-d11063e062e5>"],"error":null}
{"question":"What are the key differences in how Louise Farrenc and Zbigniew Penherski approached musical experimentation in their compositions?","answer":"The two composers had distinctly different approaches to musical experimentation. Farrenc's experimentation was more conventional, focused on creating variations on existing melodies and composing structured etudes that combined specific forms of intricacy with enjoyable music-making. In contrast, Penherski was more radical in his experimentation, employing controlled aleatorism, unique sonorous techniques, and radio-studio techniques. He specifically explored overlapping various tempos and expanded zones of humming through atypical techniques and articulation rather than instrument preparation. While Farrenc worked within established musical traditions, Penherski deliberately avoided traditional forms and developed his own modern musical language, free of postmodernist references.","context":["February 27, 2020\nLouise Farrenc: Etudes and Variations for Solo Piano. Joanne Polk, piano. Steinway & Sons. $17.99.\nLaura Netzel: Tarentelle; Humoresque; Suite, Op. 33; La Gondoliera; Berceuse et Tarentelle; Elfrida Andrée: Sonata in B-flat; Amanda Röntgen-Maier: Sonata in B minor. Paula Gudmundson, flute; Tracy Lipke-Perry, piano. MSR Classics. $12.95.\nThe recent focus on giving women their due in music and many other fields has sometimes led to presentation of less-than-compelling material that is offered only because it was created by females. At other times, though, works, musical and otherwise, show up that are excellent in and of themselves and just happen to have been written by women. That is the case with the new Steinway & Sons recording of piano music by Louise Farrenc (1804-1875): there is a great deal of marvelous material here, no matter its provenance. But lest contemporary opinion be too quick to attribute these works’ obscurity solely to the fact that their composer was a woman, it is worth recalling that much other music of the same time period received extremely high praise for a while and then fell into near-total obscurity – the creations of Kalkbrenner, Thalberg, Pixis and Herz, for instance. In a few other cases, piano music that fell into near-oblivion has recently been revived through the efforts of a champion, or a few of them: Alkan’s comes immediately to mind. And Farrenc, who was respected and successful in her own time, may well have found just the needed modern champion in Joanne Polk, who performs on this CD with utter dedication and compete involvement in the material. Indeed, Polk treats some of the works here as rather more consequential than they are: the weakness of Farrenc’s music lies in its superficiality and its reasons for being – partly to display Farrenc’s own considerable talents as a piano virtuoso, partly to help train the would-be virtuoso students whom she taught for 30 years at the Paris Conservatoire. Whether the Farrenc piano pieces heard here will prove to have staying power is to be determined – but whether they make an excellent impression in Polk’s hands is not: that is already quite clear. Display pieces these may be, but Polk displays them to excellent effect, in the process providing great insight into Farrenc’s compositional skill as well as what were clearly her considerable performance abilities. Three works here are from the standard-for-its-time category of variations on exotic or well-known tunes. Air Russe Varié is of the former type, subjecting a folk melody to a wide variety of intricate presentations. Les Italiennes, Op. 14: No. 1, Cavatine de Norma falls into the well-known-tune area, using a still-famous Bellini melody as its basis; likewise, Souvenir des Huguenots rings multiple changes – very effectively – on an excerpt from Meyerbeer’s sprawling and once super-popular opera. Collectively, these three works shine a light on Farrenc as virtuoso; but they take up only one-third of Polk’s recital. The remainder of the CD focuses on Farrenc as teacher – and here the material, although clearly created with an academic purpose, rises well above its reason for being, as the three sets of variations do not. Farrenc wrote 30 etudes in major and minor keys, collecting them in two “books” published as her Op. 26. Polk offers Nos. 3, 5, 9-12, 14 and 15 from Book I, and Nos. 17, 18, 21, 22, 24, 25 and 29 from Book II. This half-helping of the totality is more than enough to whet the appetite for a recording of all 30 of these beautifully formed pieces. Farrenc’s etudes do not push the boundaries of the form into near-unrecognizability, as Alkan’s do: their instructional elements remain clear and in the forefront, and their lengths are in the modest two-to-five-minute range. But within their genre, these etudes offer far more listening pleasure than most, thanks to Farrenc’s well-constructed themes and the way she combines specific forms of intricacy with genuinely enjoyable music-making – a fine way to captivate piano students. Thus, the galloping Presto of No. 11, the finely constructed neo-Baroque two-voice fugue of No. 12, the heart-on-sleeve Andante affettuoso of No. 15, the juxtaposition of the piano’s high and low ranges in No. 22, the bravura Allegro energico of No. 25 – these elements and many others display Farrenc’s compositional prowess in a distinct way, in addition to and independent of the pieces’ academic value. It is by no means certain that Farrenc’s piano music – or, for that matter, her other music, which includes everything from chamber pieces to three symphonies – will go through a complete revival for 21st-century audiences. But Polk’s recording constitutes a strong argument in favor of hearing a good deal more of it a good deal more frequently.\nThe rediscoveries on a new MSR Classics flute-and-piano CD are more modest, and while they too have their pleasures, there is less that comes across as distinctive in the works of Laura Netzel (1839-1927), Elfrida Andrée (1841-1929), and Amanda Röntgen-Maier (1853-1894) – at least those heard here – than there is in those of Farrenc. Part of the issue with this disc featuring Paula Gudmundson and Tracy Lipke-Perry is that only one piece on it, Netzel’s Suite, Op. 33, was actually written for flute and piano. The other pieces were intended for violin and piano. Röntgen-Maier’s was transcribed by Carol Wincenc; the remaining works were arranged by Gudmundson herself. Certainly there is nothing wrong with wanting to expand the repertoire for one’s own instrument, and certainly these pieces generally sound fine on flute. But it is a bit much to ask an audience of non-performers to discover or rediscover all this music and explore its merits while hearing it in a different instrumentation from that which the composers intended. The pieces are pretty much what one would expect from their titles: Netzel’s Tarentelle is bouncily rhythmic; her Humoresque flows pleasantly; her Suite explores considerable technical and expressive territory for the flute, for which it was written; La Gondoliera has a gentle, meandering quality throughout; and Berceuse et Tarentelle contrasts lyrical, long-lined warmth with considerable bounce and perkiness – the work’s conclusion is especially pleasing. Andrée’s sonata, although pleasant enough, is not particularly distinctive either thematically or compositionally: it is enjoyable to hear but rather forgettable in its surface-level way. Röntgen-Maier’s sonata is more substantive and, thanks to its minor key, has a stronger emotional pull – if not really any significant depth. The extended first movement (half the work’s 20-minute total length) sounds violinistic in its runs and in the interrelationship of the two instruments; indeed, parts of the flute part border on shrillness here. The second movement gestures toward plaintiveness without quite attaining it, and the good-humored finale bubbles along attractively enough but without ever quite establishing a distinctive compositional voice. So this is a disc featuring discovery or rediscovery of music by three female composers who, like so many male composers of their time, were certainly competent and capable of producing well-crafted works that skilled performers such as Gudmundson and Lipke-Perry can play with dedication and involvement. But the fact that these composers were women does not make their works any more inventive or engaging than the works of moderately capable composers who were men. This is an enjoyable enough (+++) disc, to be sure, and flute players in particular may welcome the chance to expand their repertoire by considering the performance of some of this material. There are, however, no major revelations here of unjustly neglected brilliance that fell victim to gender imbalance.","composer; b. 26 January 1935 in Warsaw, d. 4 February 2019 in Warsaw. He studied composition with Stefan Bolesław Poradowski at the State Higher School of Music in Poznań (1955-56) and with Tadeusz Szeligowski at the State Higher School of Music in Warsaw (1956-59), where he later studied conducting with Bohdan Wodiczko (1960-63). As a scholarship holder of the Dutch Government, he also studied at the Institute of Sonology in Utrecht (1969).\nPenherski's music was performed in Poland and abroad, including Austria, Belgium, Denmark, France, the Netherlands, Korea, Mexico, Germany, Norway, the United States, Great Britain, Italy, and the former Soviet Union.\nHe was a prize winner of composition competition such as: Youth Competition of the Polish Composers' Union (1960) for Ostinati, Grzegorz Fitelberg Competition (1964) for Musica humana; Artur Malawski Competition (1976) for Masurian Chronicles II; Composing Competition in Gdansk (1992) for Cantus; Polish Radio Competition (1995) for Genesis. His Little Music for the End of century (1999) was recommended by the International Composers' Tribune in Paris (2001).\nIn 2013, a CD of Penherski's music was released in the series \"Polish Music Today – Portraits of Contemporary Polish Composers\", published by the Polish Composers' Union / Polish Music Information Centre POLMIC in cooperation with the Polish Radio and with the support of the Ministry of Culture and National Heritage.\nHe was a member of Polish Composers' Union (since 1959), Association of Authors ZAIKS (since 1961), Polish Contemporary Music Association, and Warsaw Music Society. He was also an honorary member of the Scottish Society of Composers (1987) and an honorary citizen of Kragujevac in Serbia, Russe in Bulgaria, and Ho-Chi-Minh in Vietnam.\nZbigniew Penherski received the Silver Cross of Merit (1975) and the Award of the Prime Minister of Poland for works for children and youth (1982), as well as the Annual Award of the Polish Composers' Union for outstanding compositional creativity and artistic independence (2015).\nupdated: 2019 (ac)\nPenherski's work consists predominantly of orchestral and chamber music, radio works, and operas. The most important compositions include: Musica humana for baritone solo, mixed choir and symphony orchestra (1963), Street Music [Muzyka uliczna] for chamber ensemble (1966), opera The Twilight of Peryn [Zmierzch Peryna] (1972), Masurian Chronicles II [Kroniki Mazurskie II] for symphony orchestra and tape (1973), String Play for string orchestra (1980), opera Edgar - the Son of Walpor [Edgar - syn Walpora] (1982), Scottish Chronicles [Kroniki Szkockie] for symphony orchestra (1987), Little Music for the End of Century [Muzyczka na koniec wieku] for recorder, two percussions, organ and tape (1999).\nHis personality is very expressive and strong. His perception of world events and art is higly indyvidual, natural and modest, colored by great expressiveness and sensitivity, while at the same time bearing markings of reserve, sarcasm and irony against being pompous, artificial and pretentious. These characteristics account for his reluctance to mimic in his works any fashionable currents and aestetic tendencies, as well as for his focus on his own aestetic and technical solutions, to express his individual artistic credo and philosophy of life. From these arise his interest in exoticism as a potential source of inspiration, facilitating departure from eclectic activities. It results in a certain isolation from the main trends of music, not striving for fame and popularity, but focusing on matters that the composer deems vital to himself and his creations. Penherski composes music full of contrasts, saturated with expression and color. One of the typical features of his music is the manifestation of \"pure form\".\nQuote from an interview:\nI belive that I have not, in any of my pieces, reffered to forms that used to be valid in the past, either to sonata, rondo, or others. Form is extremely important to me, but it is always my own form. If I were to look for references, I would refer to architecture and mathematics.\nIn shaping the form, a particular significance is gained by the rule of gradation, namely development of the score from the state of nearly nonexistence, through gradual intensification of sound, to culmination, frequently broken rapidly. Hence, noticeable are the composr's predilections for arch form, often constructed with an almost mathematical precision (Radio Symphony [Symfonia radiowa], Signals [Sygnaly], Little String Litany [Mala litania smyczkowa]). Sometimes the form is enriched with elements of \"happening\", which, however, are not supposed to entertain its listener, but to emphasize the structure of the music (Incantationi I, Instrumental quartet [Kwartet instrumentalny]). For example, Incantationi I starts with only one performer, with the other musicians gradually joining in. The piece ends in similiar pattern: musicians leave the stage, everything dies away and the last performer drops a ping-pong ball, which bouncing, taps a rhythm of natural \"morendo\". In a number of his pieces the composer employs programmatic titles, but he treats the titles as a type of addition to the musical content. For instance, the title String Play could be understood as a purely musical idea or as a term of multiple meanings, which are evoked by the word \"play\" in informal language. String Play deserves even more attention as an instant of an indyvidual view at the non-traditional organization of movement. The introduction of the metronome is designed to control the pace of a few different layers of sound, combined with one another. As the composer said: \"It is my approach to a kind of aleatorism, absolutely controlled\". The aleatorism understood in this way appears for the first time in Penherski's radio opera The Trial of Samson [Sad nad Samsonem], and subsequently in Instrumental quartet, Radio Symphony, chamber opera Island of Roses [Wyspa róż], Introduction and Genesis.\nQuote from an interview:\nI do not rally belive in aleatorism. I think that, by leaving too much freedom to performers (in an extreme case for the full utilization of graphic transcript!), it may lead either to very good results, or, unforunately very frequently, to results having nothing or very little to do with composer's idea. The later case is usually a consequwnce of the lack of ability or the lack of good will on the part of the performer. In order to avoid such unfortunate misunderstandings, I prefer to use relatively simple overlapping of various tempos.\nA feature of Penherski's music is sonorous ingenuity and refinement. It expands zones of humming not so much by preparation of instruments, but by employing atypical technics and articulation. Multidirectional sonorous penetrations manifest themselves in his unique predilection for percussion ( Scottish Chronicles, Three recitatives for soprano, piano and percussion [Trzy recytatywy na sopran, fortepian i perkusje]. The language employed by the composer is always consequently modern, free of postmodernist references.\nQuote from an interview:\nAll these musical currents like neoclassicism, neoromanticism, have never been of interest to me. I think, my first compositions are stylistically almost the same as the most recent ones. I have tried to develop my own language, and I belive I have been loyal to it.\nA separate group of compositions consists of those created for radio, and resulting from many years of collaboration of the composer with the Polish Radio ( opera The Trial of Samson, Radio Symphony, Opera etude [Etiuda operowa], Genesis). This form allows for the use of various techniques, differing from those typically achived electronically, \"studio-like\".\nQuote from an interview:\nWhat is attractive in radio technique is that I may employ ways of production which on stage may be troublesome, and even difficult for performers. Although the use of four metronomes in Instrumantal quartet was relatively easy, the three metronomes in String Play needed to be put in such a way, that the tempo would be set by the visible movement of the pendlums, not the ticking sound ( in newer metronomes it would have to be a light signal). Hence, my tendency for resorting to radio-studio techniques, allowing to use sound, which while being conventional (i.e. not electronic), departs from the typically European traditions, both in aspect of rhythmicity and structure.\nPenherski's music does not try to be grand, pathetic, and does not intend to awaken the greatest emotion. The composer does not employ grand themes, rich in various extra-musical references to national or universal values, He sets himself simpler challenges. The composer wants, most of all, to awake the interest of his listener, using purely musical means, with the aid of original organization of sound and of forms suggesting a certain plot. It presents the world as seen through the eyes of an uncompromising artist, who measures and evaluates the means employed in such a way that they agree with his own vision of the secret of entity and art.\nTwo Lullabies for mezzo-soprano and piano * (1955)\nThree Songs for soprano and piano * (1955)\nFour Preludes for piano * (1956)\nOstinati for mixed choir and orchestra (1960)\nChoral Pictures for mixed choir * (1960)\nStudies in Colour for piano * (1961)\nThree Gipsy Songs for three-part women choir, to the texts by J. Ficowski * (1961)\nThree Songs to Old Polish Texts for mixed choir * (1961)\nContrasts for string orchestra, selected instruments and vocal ensemble (1962)\nMusica humana for baritone solo, mixed choir and symphony orchestra, to the texts from the Bible [Latin] * (1963)\nThree Recitatives for soprano, piano and percussion * (1963)\nSuite for oboe and piano * (1963)\nChamber Alphabet Book, 6 cyclic pieces for various ensembles * (1964)\nMissa Abstracta for tenor solo, reciting voice, mixed choir and symphony orchestra, to the texts from the Bible [Latin] and by T. Różewicz (1966)\nStreet Music for chamber ensemble (1966)\nSamson Put on Trial, radio opera. Libretto by J. Prutkowski (1967)\nThree Recitatives for soprano, piano and percussion [new version] (1968)\n3M-HI for tape (1969)\nChildren’s Improvisations for instrumental ensembles * (1969)\nHymnus laudans for choir and chamber orchestra, to the texts of medieval hymns (1970)\nInstrumental Quartet for piano (or other keyboard instrument) and 3 optional instruments * (1970)\nIncantationi I for 6 percussionists * (1972)\nThe Twilight of Peryn, opera (1972)\nMasurian Chronicles for symphony orchestra and tape (1973)\nAnamnesis for symphony orchestra (1975)\nRadio Symphony for 2 performers, for tape (1975)\nEdgar – the Son of Walpor, opera. Libretto by composer after the drama of S. I. Witkiewicz (1982)\nJeux parties for saxophone and percussion * (1984)\nThree Impressions for soprano, piano and 4 percussionists * (1985)\nScherzino for four violins * (1985)\nScottish Chronicles for symphony orchestra (1987)\nThe Island of the Roses, chamber opera. Libretto by K. Meissner (1989)\nThree Songs for Children for two- or three-part choir * (1990)\nSignals for symphony orchestra (1992)\nCantus for mixed choir, to the texts from the Bible (1992)\nIntroduction and Toccata for clarinet, trombone, cello and piano (1994)\nGenesis for bass solo, vocal ensembles, reciting voices, selected instruments, concrete and electronic sounds (1995)\nSignals II for symphony orchestra (1995)\nIntrada for symphony orchestra (1995)\nFour Easy Pieces for piano (1997)\nLyric Waltz for street bells, flute, three trumpets and two percussionists (1998)\nScherzino for street bells, flute, clarinet, three trumpets and two percussionists (1998)\nToccata for clarinet, trombone, cello and piano (1998)\nX-Play for orchestra of 30 flutes (1999)\nLet cellos play for 8 cellos (1999)\nLittle Suite in Old-Fashioned Style for chamber string orchestra (1999, 2007)\nCantus II for mixed a cappella choir (2000)\nLittle March for piano, for 4 hands (2000 )\nDas Wohlpentatonierte Klavier - Scherzo for piano (2000 )\nImpressions on a Theme of B-A-C-H for piano (2000 )\nDance for three recorders (2001 )\nLittle String Litany for chamber string orchestra\nLittle \"Autumn\" Symphony (2006)"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:de0611e5-edf3-4318-8981-075b41d36f72>","<urn:uuid:c7914296-05b2-4e30-a61d-a3b9c943b1d5>"],"error":null}
{"question":"What are the environmental threats to hedgerows and streams in rural landscapes, and how do well-managed hedgerows help address these issues?","answer":"Rural landscapes face several environmental threats. Streams are affected by soil erosion, sediment deposition, and nutrient losses from agricultural practices. They also experience more extreme flow patterns - running faster in winter and having low flows during dry periods due to landscape drainage. Along stream banks, poor pollard management and grazing can be detrimental, while riverside alders suffer from fungal disease without adequate replacement. As for hedgerows, both over-management and neglect pose threats. When trimmed too tightly for several years, they become top-heavy with gaps, while lack of management leads to tall, aging hedges at risk of collapse. However, well-managed hedgerows provide important solutions - they help stabilize soil, reduce surface water run-off that can wash soils and nutrients into rivers, and store significant amounts of carbon - offsetting about 13 million tonnes of carbon per year, equivalent to a quarter of yearly farming emissions in the UK.","context":["The catchment area of the Upper Clun is underlain by rocks of Silurian age, which 420 million years ago were deposited as sandy silts and muds, eroded from the surrounding land, into a marine basin. These sediments consolidated to give rise to red and grey shales, mudstones and siltstones (the Clun Forest and Cefn Einion formations respectively). The rocks are relatively weak and the hills have a characteristically steep-sided, shallow-domed shape with few outcrops. We can only see them in rare exposures or more easily (for the Cefn Einion siltstones) in the walls of the older buildings in Newcastle, Clun and the surrounding hills. To see a detailed geological map use the British Geological Survey’s viewer at BGS Map App.\nA free-draining acid brown soil has formed over the siltstones, which would have supported moorland and rough grazing in the pre-Roman era. Today the land is mostly grazed, although arable production is possible in some areas. Alluvial silts and glacial clays are somewhat richer but these are confined to the river valleys and dingles.\nThe area falls into Natural England’s Character Area 98 – ‘Clun and North West Herefordshire Hills’ – of which the key characteristics are described as:\n- Rolling, rounded upland hills divided by narrow valleys widening to the east.\n- Uniform landforms, similar to mid-Wales and contrasting with the rest of the Welsh Marches.\n- Small, wooded, enclosed upper valleys broadening to flat-bottomed intensively-farmed lower valleys.\n- Irregular field patterns in valleys and around settlements contrasting with large rectilinear fields on higher ground.\n- Large, recent conifer plantations, contrasting with remnant, ancient, semi-natural woodland.\nTo the west our area merges into the high moorland of Powys while further east the landscape becomes more domesticated. Hedges reach up to enclose most hill tops although patches of heathland and unenclosed upland grassland remain. On the ridge tops, the hedges are generally low with occasional trees in the more sheltered hollows. The unimproved pasture is sometimes in a patchwork with the smooth greens of improved grassland and occasional patches of arable land. There are panoramic views, which emphasise the area’s plateau origin, as well as long views down narrow, twisting valleys which widen eastwards.\nThe heads of the tributary valleys are narrow and deeply incised with woodland on the steepest slopes. Down-valley they widen to a narrow floodplain, widening at the confluence of the Folly Brook with the Clun, at Newcastle. On the lower slopes, there is a generally regular pattern of large fields cut through by small streams, often with drifts of streamside woodland. Around the farmsteads, and hamlets the field pattern becomes denser and more irregular. In the Clun valley itself, hedges are lower and intermittent and trees more infrequent.\nA changed habitat?\nHedgerows vary in density and management. In some of the upland areas gaps have been renewed or replaced with post and wire. On the floodplains also they are locally intermittent and sparse, with few hedgerow trees.\nThe improvement of pasture and some arable cropping onto higher ground has changed the historic pattern of cultivation in the valley bottoms and rough pasture on the hills. The deciduous woodlands can be adversely affected by grazing, thus reducing their capacity for natural regeneration and their nature-conservation interest.\nAlong stream banks, grazing can be detrimental and pollard management is poor. Riverside alders are suffering from fungal disease and are not being replaced at the rate of die-back. Agricultural land use is likely to be affecting water quality through soil erosion and sediment deposition, together with nutrient losses. The streams are also flashier, running faster in winter and suffering from low flows at times of low rainfall, because water is not retained in the landscape as much as it was – land has been drained, and peaty/boggy areas improved for grazing. See this leaflet for information on managing wetlands for wildlife – Managing wetlands for wildlife.\nSome of the coniferous woodlands are nearing the end of their first cycle. There is an opportunity for improving the landscape and amenity value of the woodland, with an increase in broadleaf woodland where appropriate, particularly along the riparian corridor.","Information on how to manage a hedgerow for biodiversity\nWhy does the National Park support planting and management of Hedgerows?\nHedgerows are integral to the character of the National Park. They are a much-loved feature of this managed landscape that epitomise the coexistence of farming and nature. We don’t know exactly how much hedgerow we have, but one estimate puts it at 3,500 miles in the National Park. If the average density is 5 plants per yard, that represents 30 million trees and shrubs.\nFarmers have created hedgerows to demarcate their land, create stock barriers and provide shelter. They may have been created in Wales as long ago as the bronze age, and many surviving hedges are centuries old. Hedgerows often give insights into the cultural history of our landscape, for example marking ancient monuments or outlining where old farms were situated.\nHedgerows are included in the Environment (Wales) Act (2016) Section 7 list of habitats of principal importance for the purpose of maintaining and enhancing biodiversity in relation to Wales. They bring many benefits to nature and form one of our largest nature reserves. Across the UK, more than 600 plant, 1,500 insect, 65 bird and 20 mammal species use hedgerows – for food, shelter and to move between areas. Mature trees—sometimes pollarded—are often found in ancient hedgerows. The verge or bank beneath the hedge may contain bluebells, wood anemones, cow parsley and greater stitchwort, and the hedge itself may be a rich mixture of woody shrubs such as hazel, dogwood and guelder rose. Hedgerows are both habitats in their own right, as well as corridors for wildlife, that create vital connections between other, more isolated habitats.\nHedgerows lock up huge amounts of carbon dioxide and store it in their roots, branches and leaves, and in the soil. They have been estimated to offset a quarter of all the yearly emissions from farming in the UK: about 13 million tonnes of carbon per year. The Climate Change Committee has recommended that the extent of hedgerows be increased by 40% as part of measures against the climate emergency.\nHedgerows also provide other ecosystem services, including stabilising soil and reducing surface water run-off, which can wash soils and nutrients into our rivers. As the pattern of extreme weather events changes, the role of hedgerows in providing shelter to livestock is becoming more important. In urban areas, hedges can help capture air pollution and reduce summer temperatures.\nBannau Brycheiniog, in partnership with Stump up for Trees and the Woodland Trust, are implementing the project “Traditional Boundaries of Wales”, which supports farmers to restore and revive hedgerows in the Park. The project supported the planting of three kilometres of new hedgerow – 13,000 trees and shrubs – in 2023. The Park will also work with farmers to improve the management of hedgerows as biodiverse habitats, through support for training and hedge laying.\nWhat is a healthy hedge?\nHedgerows are making a come-back. A huge extent of hedgerow was lost between 1950 and 1975, as farmers were encouraged to exploit as much land as possible for food production, and the losses continued into the new millennium. The total length of managed hedgerows in the UK decreased by 6.1% (26,000km) between 1998 and 2007. However, changes to agricultural policy have placed greater emphasis on protecting habitats on farms and a steady revival of hedgerows is underway. Hedgerows are also now protected by the Hedgerows Regulations (1997), which require landowners to notify local authorities if they want to remove hedgerows. Click here for information on planning in the National Park.\nThe best hedgerows for wildlife have a mixture of woody species, like hawthorn, blackthorn, field maple, and hazel, with the occasional ramblers, such as rose, holly, and honeysuckle. Healthy hedgerows are dense, often a bit wild, and allowed to gradually grow in height for several years. This prevents stems from dying, keeps them dense, and keeps the hedge healthy for longer. However, lack of management is also a threat to hedges and leads to tall, aging hedges with many gaps and at risk of collapse. Thin lines of tall, mature trees that are disconnected at ground level can be found around the Park, providing evidence of what happens to hedges when they are left unmanaged. These provide other habitat—nesting space for birds and flight paths for bats—but they represent a loss of the habitat associated with a dense hedge.\nHedge laying is a traditional practice that is still carried out in the Park, contributing to hedgerow health and biodiversity. Hedgerow trees reach a certain height and then are partially cut near the base, laid horizontally, and woven to form a strong, stock-proof barrier: a living fence. The discerning eye can even identify locations in the Park according to the unique local styles of hedge laying.\nMaintaining a healthy hedge\nMany farmers manage their hedgerows well, and in corners of the National Park the practice of hedge laying is still practiced. However, many are not maintained in a state that allows biodiversity to thrive. Hedgerows need management, which comes at a cost, and both over-management and neglect can be a threat. Farmers sometimes over-manage their hedgerows in their effort to keep them tidy, or for safety reasons, such as to maintain visibility along roads, but when they are trimmed too tightly for several years, they can become top-heavy with many gaps.\nThe Good Agricultural and Environmental Conditions (GAEC) of the Common Agriculture Policy outline some requirements for hedgerow management, including when hedge cutting and laying may and may not be carried out. The National Farmers Union provides a summary of rules and guidance on hedge trimming.\nFurther advice on hedgerow management is available from Hedgelink, including these 12 management principles:\n- Consider the Complete Hedge\n- Promote joined-up Hedge Landscapes\n- Create Structural Diversity across the Farm\n- Encourage a Range of Shrubs and Trees\n- Keep the Shrub Layer Dense\n- Allow Shrubs to Flower and Fruit\n- Look after Mature Trees and Encourage New Ones\n- Encourage Out-Growths\n- Encourage Thick Basal Vegetation\n- Encourage Flower-Rich Margins\n- Manage Ditches\n- Keep Fertilisers and Pesticides away from Hedge Bases and Ditches\nHedgerow condition can be assessed according to its structural health and its potential benefits to wildlife. Structural health includes the height, width and thickness (number of gaps) of the hedge as well as associated features (e.g. presence of a ditch or bank). Indicators of the benefit to wildlife include hedgerow plant diversity, presence of wildlife, and how well the hedgerow connects different habitats or other hedges.\nThe Hedgelink website includes 9 advisory leaflets for particular animals or assemblages of animals associated with hedgerows, including bats, bumblebees, dead wood insects, ditch invertebrates, dormice, grass snakes, hairstreak butterflies, hedgehogs, and pollinators.\nCreating new hedgerows\nMany farmers in the National Park are planting new hedgerows and there are number of grants available for this, including from the National Park under the Traditional Boundaries of Wales project. Hedges serve several purposes, which determines where they are best located: for example, filling gaps and providing connections between other hedgerows and woodlands, demarcating historic boundaries, or creating a windbreak.\nHedgerow plants are best planted between late October and early March, and usually the earlier the better, so that the plant can establish before the onset of spring, although clay soils and poorly drained soils may suit planting in March.\nSelection of suitable species depends on the geology, altitude and climate as well the purpose of the hedge, and the preferred species changes from east to west in the Park. In general, the more woody species in the hedge, the better it is for wildlife. Different schemes may stipulate different planting densities, but in general 6-7 plants per metre in a 2-metre-wide hedge is acceptable. Plants that do not survive the first year will need to be replaced. Fencing and tree guards may be necessary to protect against damage from sheep, deer and rabbits, but this can be gauged according to local conditions to avoid unnecessary costs and complications. Farmers are encouraged to maintain mature trees at intervals in their hedge and these may need to be tagged for protection when the hedge is first planted. Further guidance is available from the Long Forest project."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1ab08801-51f6-4f08-9b2d-b12f3a49f83d>","<urn:uuid:5bef9c39-c8f8-421f-b57c-e9bda77eb3a4>"],"error":null}
{"question":"What were the uniform challenges faced by World War II Army nurses, and how do traumatic military experiences correlate with substance abuse disorders?","answer":"World War II Army nurses initially faced significant uniform problems, as their only military attire before America entered the war was a white duty nurse's uniform with white shoes. The only military elements were a second lieutenant's gold bar on the right lapel and a caduceus with an 'N' on the left lapel. Regarding trauma and substance abuse, there is a strong correlation between PTSD and substance use disorders, with about 50% of people seeking treatment for substance abuse showing PTSD symptoms - five times higher than in the general population. Evidence suggests PTSD typically precedes and predicts substance use disorders, as individuals often use substances to cope with PTSD symptoms, though this can lead to a cycle where substance withdrawal can worsen PTSD symptoms.","context":["- STAFF PICKS\n- GIFTS + GIFT CARDS\n- SELL BOOKS\n- FIND A STORE\nThis item may be\nCheck for Availability\nAnd If I Perish: Frontline U.S. Army Nurses in World War IIby Rosemary Neidel-Greenlee\nSynopses & Reviews\nOperation Torch--U.S. Army Nurses in the Invasion Force\nD-Day North Africa\n8 November 1942\nI spotted Lt. Vilma Vogler descending a ladder at my side. Our eyes met for a moment in mutual shock, and then we quickly descended into a waiting barge. At that moment she and the other nurses had ceased to be \"the women.\" We were all comrades in equally dangerous footing, trying to survive the insanity of combat.\n-Edward E. Rosenbaum, MD, former captain, U.S. Army Medical Corps, \"Wartime Nurses: A Tribute to the Unsung Veterans,\" New Choices (July 1989)\nAn artillery shell exploded sixty yards off the starboard side of HMS Orbita. Lieutenant Helen Molony, seated on board in the officers' mess hall, felt her hand shake as she raised her coffee mug to her mouth. It was early morning, 8 November 1942. A convoy of Allied war- and transport ships, including the Orbita, the Santa Paula, and the Monarch of Bermuda, lay two miles off the coast of Algeria. On board these British ships were not only combat troops but the men and women of the 48th Surgical Hospital, including Lieutenant Molony. She was one of 57 U.S. Army nurses who, along with the hospital's 48 officers and 273 enlisted men, were waiting to land, side by side with the combat troops, on the beachheads of Arzew and Oran in Algeria.\nThe sun had not risen yet and the ships were still under cover of darkness. Molony glanced around the officers' mess. The thunder of artillery had begun an hour earlier, and now, at 0515, she saw that the tables in the mess were crowded with officers, male and female, dressed in combat gear. Aside from the clanking of silverware and an occasional word or two spoken in hushed tones, the large wardroom was strangely quiet. In less than an hour, Molony knew that her part in Operation Torch-the invasion of North Africa-would begin. What she could not know was that her participation in the D-Day invasion would become a landmark in U.S. military history.\nOnly a few months earlier, in midsummer, the 48th Surgical Hospital had crossed the Atlantic on the USS Wakefield as part of what was, at that time, the largest convoy ever to sail from the United States. On 6 August, the 48th Surgical had disembarked at Greenock, Scotland, and taken a one-day train ride to Tidworth Barracks in the area of Shipton-Ballanger and Kangaroo Corners in southern England. The unit remained there for two and a half months, and Molony underwent the closest thing to military training the army nurses would receive, a regimen of hardening exercises of five- and ten-mile hikes, complete with field packs.\nFor the nurses of the 48th Surgical Hospital, as for all the army nurses sent overseas before July 1943, uniforms presented a definite problem. Before America entered World War II, the sole uniform the U.S. Army nurses had was a white duty nurse's uniform and white nurse's shoes. The only thing military about the uniform was the second lieutenant's gold bar, worn on the right lapel, and the caduceus with an \"N\" superimposed upon it on the left lapel. The caduceus had been a symbol of the Army Medical Department for decades. Doctors wore the caduceus plain, while nurses had a superimposed \"N\" for nursing, the dentists a superimposed \"D,\" and veterinarians a superimposed \"V.\"\nAs for the clothing itself, the army provided blue seersucker dresses for the nurses in\nIn World War II, 59,000 women voluntarily risked their lives for their country as U.S. Army nurses. When the war began, some of them had so little idea of what to expect that they packed party dresses; but the reality of service quickly caught up with them, whether they waded through the water in the historic landings on North African and Normandy beaches, or worked around the clock in hospital tents on the Italian front as bombs fell all around them.\nFor more than half a century these women’s experiences remained untold, almost without reference in books, historical societies, or military archives. After years of reasearch and hundreds of hours of interviews, Evelyn M. Monahan and Rosemary Neidel-Greenlee have created a dramatic narrative that at last brings to light the critical role that women played throughout the war. From the North African and Italian Campaigns to the Liberation of France and the Conquest of Germany, U.S. Army nurses rose to the demands of war on the frontlines with grit, humor, and great heroism. A long overdue work of history, And If I Perish is also a powerful tribute to these women and their inspiring legacy.\nA galvanizing narrative of the wartime role played by U.S. Army nurses—from the invasion of North Africa to the bloody Italian campaign to the decisive battles in France and the Rhineland.\nMore than 59,000 nurses volunteered to serve in the U.S. Army Nurse Corps alone: 217 lost their lives (16 by enemy action), and more than 1,600 were decorated for meritorious service and bravery under fire. But their stories have rarely been heard. Now, drawing on never-before-published eyewitness accounts—many heroic, some mundane and comic—Monahan and Neidel-Greenlee take us to the front lines, to the withering fire on the beaches of Anzio and Normandy, and to the field and evacuation hospitals, as well as bombed and burned hospital ships. We witness the nurses—and the doctors with whom they served—coping with the physical and psychological damage done to the soldiers in combat. We see them working—often with only meager supplies and overwhelmed by the sheer number of casualties—to save the lives and limbs of thousands of wounded troops. With them we experience the almost constant packing up and moving on to keep up with advancing troops, foxholes dug under camp beds, endless mud, and treacherous minefields. The vividness and immediacy of their recollections provide us with a powerfully visceral, deeply affecting sense of their experiences—terrifying and triumphant, exhausting and exhilarating.\nA revelatory work that at last gives voice to the nurses who played such an essential role in World War II.\nFrom the Hardcover edition.\nAbout the Author\n\\Evelyn M. Monahan, a retired psychologist, served in the Women's Army Corps from 1961 until 1967 as a corpsman and psychiatric technician. She subsequently took her M.Ed. and Ph.D. at Georgia State University and her M.Div. in theology and ethics at Emory University. She worked at the Department of Veterans Affairs from 1980 to 1996.\nRosemary Neidel-Greenlee served in the U.S. Navy Nurse Corps on active duty from 1962 until 1965, and on reserve duty between 1989 and 1991. She has a master's degree in nursing from Emory University, and worked at the Veterans Affairs Medical Center in Atlanta from 1981 to 2002.\nMonahan and Neidel-Greenlee are co-authors (with Agnes Jensen Mangerich) of Albanian Escape: The True Story of U.S. Army Nurses Behind Enemy Lines and co-authors of All This Hell: U.S. Nurses Imprisoned by the Japanese.\nWhat Our Readers Are Saying\nHealth and Self-Help » Health and Medicine » History of Medicine","Author: Bertha K Madras, PhD\nPost-Traumatic Stress Disorder\nPost-traumatic stress disorder (PTSD) is a state of mind activated by either witnessing or experiencing a shocking, frightening, horrifying episode(s). The ordeal can involve threats of or actual physical violence to themselves, to family or friends they are bonded with, or strangers. Some individuals who have experienced a terrifying event may find it difficult to cope and adjust for some time, but gradually they improve as the emotional memories fade. If symptoms continue or get worse after prolonged periods of time (months or years later), interfere with daily life and function, and persist long after there is no danger of a recurrence, the condition is designated post-traumatic stress disorder. In PTSD, memories of the original trigger persist, along with feelings of distress, fear, or trauma. The constellation of symptoms can range from nightmares, flashbacks, to severe anxiety, as thoughts and memories reply the events and cannot be suppressed.\nWho Is at Risk for PTSD?\nVeterans with combat experience are at great risk for PTSD, but civilian populations also harbor this persistent condition. Experiencing or witnessing rape; torture; kidnapping and captivity; child abuse; car, train, or airplane accidents; natural disasters such as hurricanes, floods, or earthquakes; and unexpected death of a family member or friend are examples of traumas that can trigger PTSD in vulnerable populations.\nWhat Are PTSD Symptoms?\nA short-term symptoms of anxiety and stress are common following a traumatic event, as most people respond with some functional disability immediately afterwards. A full constellation of persistent PTSD symptoms, lasting at least a month or years, indicates a disease state. Symptoms may surface within a few months of the events, or, in some individuals, years later. They may fluctuate between intense and weak, depending on environmental and personal “triggers,” such as repeated exposure to unrelated traumatic events or cues. The symptoms can be severe enough to interfere with work, relationships or social function. Recovery may take fewer than 6 months, or persist much longer or become chronic. Symptoms can include (a) intrusive memories, (b) avoidance, (c) negativity, (d) hyperemotional responses.\nIntrusive memories can be experienced over and over in the form of unwanted flashbacks, bad dreams, or reliving the traumatic event, with severe distress or reactions to situations that conjure up the event. Triggered by memories, situations, sounds, or words, the intrusive memories can interfere with everyday life.\nAvoidance is a complicated reaction to an episode. It can manifest by staying away from people, places, objects or activities that remind the person of the experience (for example, driving), by losing interest in pleasurable activities, or with problems remembering the event.\nNegativity manifests as an unenthusiastic, pessimistic outlook on life, which may include negative feelings about oneself or others, emotional numbness, loss of interest in enjoyable activities, hopeless outlook for the future, difficulty with memory or maintaining relationships.\nHyperemotional responses may manifest as tension, sleep difficulties, angry outbursts, stress, irritability, aggression, hyper-vigilance for dangerous situations, hyper-startle, all symptoms that can interfere with daily life.\nCan PTSD Be Predicted?\nIt is challenging to predict who will or will not develop PTSD after a traumatic event. Predisposing factors may include underlying mental health problems such as anxiety and depression, the nature and extent of other adverse life experiences, temperament (genetic predisposition), brain response to stress, and exposure to multiple traumatic events. PTSD can occur in men, women, and children, with women more likely to develop PTSD than men. Susceptibility may run in families but powerful experiences such as being a victim, or witnessing others hurt or killed, and feeling helpless, or extreme fear can generate the symptoms. The risks of PTSD after a traumatic experience can be compounded by inadequate social support or additional stressors (pain, death in family, work problems). The risks of PTSD can be attenuated by support from family, friends, support groups and powerful emotions that counter the trauma. When confronted with danger, a resilient person will perceive their response as appropriate, pro-active and effective.\nThe main approaches to PTSD treatment are psychotherapy, medications, social support, or a combination of all:\n- Cognitive therapies designed to diminish negative perceptions of normal scenarios, to make sense of haunting memories;\n- Exposure therapy designed to enable facing, confronting, coping and controlling the fears that triggered PTSD;\n- Other psychotherapies designed to explain how trauma affects individuals, to transform reactions to PTSD symptoms, to teach anger management skills, to identify and reduce guilt, shame, or other negative feelings.\nThese approaches can be combined with antidepressants or anti-anxiety medications to relieve symptoms of PTSD.\nPTSD and Substance Abuse\nPTSD and substance use disorders are interlocked by shared genetic and environmental factors. Alcohol and other substance use disorders are very common in people with posttraumatic stress disorder. Of people seeking treatment for substance use disorders, approximately 50% harbor the symptoms of PTSD, a percentage 5 times higher than PTSD in the general population. Because of the high rates of this co-occurrence, it is likely that PTSD and substance use disorders are related to one another.\nWhether PTSD causes substance use disorders has not been proven. Evidence is accumulating that PTSD precedes and predicts the development of a substance use disorder rather than the reverse. Individuals apparently use substances to cope with PTSD and its negative moods. Initially, alcohol and other drugs are consumed to relieve PTSD symptoms. However, if a substance use disorder develops, there is an inherent risk of increasing PTSD symptoms, especially during the withdrawal phase. In the midst of the PTSD hyper-arousal state, withdrawal from substances can accelerate relapse to substance use. It is clear that during detoxification, meticulous control of drug withdrawal and PTSD arousal symptoms should be implemented.\nThe relationship between PTSD symptoms and consumption of alcohol and other drugs is robust. When one condition deteriorates, the other condition also worsens. When PTSD symptoms are greater, alcohol craving also intensifies. Indeed, in a laboratory setting, when alcohol dependent individuals with PTSD are presented with a trauma cue, they report an increase in their alcohol craving. Conversely, if PTSD symptoms decline, substance use decreases. Treatment of the substance use disorder in PTSD individuals results in a significant decrease in PTSD symptom severity, even if PTSD is not being treated. Treatment needs to be tailored to the individual, for substance use disorders and PTSD affect individuals differently.\nPTSD is an added liability for successful treatment of a substance use disorder. Treatment success is greater in populations with a substance use disorder alone, compared with populations that harbor both a substance use disorder and PTSD. PTSD patients undergoing treatment for a substance use disorder describe more intense cravings for drugs and/or alcohol and are prone to relapse more quickly after completing a treatment protocol. Currently, there is no consensus on best approaches for treating people with both diagnoses, but treating both is essential."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:8ba889f2-5f55-4a5e-99c3-34117ea80e83>","<urn:uuid:e2a4b620-a29b-47ae-920e-6f508278c27f>"],"error":null}
{"question":"What's the proper technique for shooting ducks?","answer":"The recommended shooting technique for duck hunting is the swing-through method. Hold the shotgun with one hand behind the trigger and the other under the fore stock. The hand under the fore stock should raise the barrel while the other hand brings the end of the shotgun to your shoulder. Swing your shotgun to aim ahead of the ducks in their flight path. Use a 12-gauge shotgun for best results, though 20-gauge and 10-gauge options are also available. If ducks aren't flying your way, you may need to reposition your blinds.","context":["Duck hunting is simply the process of tracing waterfowls, including ducks either for sport or for food. In most countries, this activity is considered to be an outdoor sporting activity and is prohibited at commercial levels in western nations. Today we will try to provide you with the most relevant duck hunting tips that will help you improve your hunting skills and become a pro.\nFirst of all you should know that this is not an easy outdoor activity. There are various things that you need to observe in order to be successful. But we have good news: it doesn’t really matter whether you are experienced or not.\nWith the right guidelines, you will be able to build your skill and you will be an expert in no time. By the time you are reading the conclusion, you will be fully equipped to hunt ducks among other waterfowls.\nWhat You Need for The Hunt\nBefore you can engage the “how to steps” for shooting ducks, you first need to know what you need for the hunt. This information includes duck basics, hunting gear, and any other relevant information.\nYou need to know the various species of ducks and be able to identify them correctly when you are in the field. This kind of information is very useful to you as a birder and a hunter, because you cannot hunt what you do not know.\nWith correct knowledge of the various species, you will be able to know the endangered species and the ones that you can shoot down without any repercussions.\nThe most popular duck species that you must be familiar with, especially if it is your first time to hunt ducks, are:\n- Mallard Duck. These are the world’s commonest species, which can be located in New Zealand, Southeast Australia, North America, Asia and Europe. They have short tails and the males do have curled central tail feathers for attracting females. Their wings are covered in long flight feathers. They have eyes on the sides of their heads, which are resourceful for detecting marauders, even from behind.\n- Paradise Shelducks. Also referred to as parries, these ducks are common in New Zealand, from sports fields, urban parks, farms and wild areas. These ducks are a bit larger than normal ducks, but not larger than geese. The females of this species are the ones who are colorful. The males do have attractive textures in their plumage, but they lack the exceptional chestnuts of the females.\n- Grey Ducks. These ducks are common in Australia and New Zealand. Males as well as females are the same and they are lightly similar to the female mallard ducks. They have dark grey bills with a black tip. Their upper body feathers are brown with a narrow buff edge without central markings. Their wings have a green speculum with a thin white line on the rear edge. Also, the top of their heads is nearly black.\n- Shoveler Ducks. Also referred to as the Northern Shoveler; this species breed in northern parts of North America, Asia and Europe. It is the only species with the oversized bill that it derives its name from. The males are slightly larger than the females during the fall season and they are considerably larger in spring. The brighter colors of the conjugal plumage in spring contribute to some of the visual differences.\nSuccessful hunts are largely determined by the gear you bring along. You should match the gear with the venue.\nNormally, what you bring has to contribute to the primary purpose, which is a successful duck hunt. The most useful gear you should have includes the following:\n- Decoys: These are imitations of ducks that you can use to lure ducks close. They are made of plastic or wood and are painted to appear like ducks. You are required to set them in the proximity of your blinds. They can be stationary or moving. Moving ones are referred to as roboducks. Good decoys should have front weighted keel design; non-chip paint, rubberized molding and imitate feeding ducks.\n- Shotgun: The ideal shotgun for shooting ducks should have a rifled barrel and scope. Suitable ammunition is very important. Rifling in the barrel imparts spin to the slug, thus stabilizing as well as increasing accuracy. You should choose a properly configured shotgun for effectiveness. You should consider a 12-gauge shotgun. Even though, almost any shotgun with an open choke can be convenient, the best choice is one that has your desired features and specifications. Do read our reviews of the best hunting guns to get you started on your adventure.\n- Duck Calls: These are useful for attracting ducks into your blinds. Most calls are designed from wood and they have a small reed inside. A duck call’s operation is simple; whenever you blow the reed vibrates into the call. The best model should be able to make loud hail calls as well as soft nasal Your duck call should be able to mimic the soft flexible tissue of a duck’s tongue and neck.\n- Hunting Clothes: Ducks, unlike any other waterfowls, have very good eyesight. They can easily spot you hiding in the reeds along the edge of a lake if you are wearing the wrong attire. You must camouflage yourself by wearing concealing clothes. You should look for camouflaging pattern that matches your hunting blinds when choosing facemasks, gloves, hats and jackets. You have to go with green camouflaging pattern if the plant life is green. You should go with the brown camouflaging pattern when plant life is not green. For a review of the best hunting clothes for you to choose from, read our earlier piece on this all-important topic.\n- Footwear: Duck hunting footwear is important, because of the wet and muddy environments. The most recommended footwear is hip boots and waders. They will ensure you do not get wet and muddy. The footgear has to fit tightly at the ankles, or else they can easily pull off when you are walking on mud. Duck footgear will always come in handy when setting up decoys in flooded fields or shallow water. Make sure you choose footwear that will not be filled with mud and debris. In addition, they should be warmer and comfortable to wear. Check our list of the top hunting boots to make the experience more comfortable.\n- Binoculars: Optics is necessary if you are to select the best hunting areas.\nLong distance glassing of ducks needs exceptional binoculars.\nThe binoculars you buy should present you with the following features: twenty-feet close focusing distance; stunning HD clarity; four hundred plus feet field of view at a thousand yards; durable design; twelve-millimeter eye relief; and high magnification, at least 7X.\nStep-by-Step Duck Hunting Guidelines\nNow that you know the basics and the various gears you need for the hunt, you can go ahead and learn how to duck hunt.\nStep 1 – Scout Potential Hunting Spots and the Prevailing Species\nYou have to scout for potential hunting grounds and identify the dominant species. To do this, you need a reliable topographic map of the area of interest along with a plat book that indicates land ownership. Spend some quality time looking for ducks on water or in the air.\nYou probably need a pair of binoculars for spotting waterfowls from a distance, especially when they are in the air. You will also be able to spot hard-to-see fowls that are swimming in vegetation or along shorelines.\nMost puddle ducks are usually active in early morning and in the evening. Therefore, these are the recommended times to scout. On the other hand, diving ducks tend to stay in close proximity to their feeding sources. Thus, you can scout for them at any time of the day. Always determine the depth of the water; you can use your footwear for this, such as waders.\nStep 2 – Check If the Land Is Private or Public\nCheck your plat book to confirm ownership of the land. If your potential hunting ground is on private land, then you will need permission from the owner. If the land is private, politely introduce yourself to the owner and ask for permission.\nWhen asking for permission, be very specific as far as the section of the land is concerned. You should always remember to ask for permission for any subsequent hunts you wish to conduct. Do not make any assumptions.\nStep 3 – Determine the Best Blind Spots\nFor your hunt to be successful you need to choose good blinds. You should establish blind sites with respect to different wind conditions. There are different blinds that you can find.\nUsually, the best blind spot is natural vegetation. You may have to make a blind by cutting vegetation or set up an artificial version if there is no vegetation around. A bank of a small pond can also be a reliable blind, especially when tracing around swamps and marshes. Also, your blinds have to blend well with the surrounding environment.\nStep 4 – Set Up Decoys for Diving Ducks & Use Calls for Puddle Ducks\nSet up your decoys within shooting range. The number of decoys you can set depends entirely on the size of your hunting area. You should place enough decoys to increase the chances of luring ducks.\nYou should place your decoys less than nine meters away from your blind spot. This way, it will be harder for you to give up your cover. Ducks have the tendency of landing in open areas; therefore, you must consider wind conditions.\nYou are advised to set up your decoys at locations where the wind is at your back. This is so, since ducks are used to land into the wind. Ducks will spread their wings before they land. The mechanism is simple; the wind helps them to land by blowing against their wings.\nAvoid direct sunlight, because sunlight will make it easier for the ducks to spot you and it will definitely make it harder for you to see the ducks.\nCalling ducks take practice and you may not be able to successfully call ducks in the beginning. However, practice makes perfect. To use a duck call, hold the uncovered end of the thin tube in the web of your forefinger and thumb. Then place the other end of the call to your lips. Then blow into the mouthpiece while opening your fingers as the pressure builds up behind them in order to create a quacking sound.\nStep 5 – Shoot Your Target\nOnce the ducks are within shooting range, go ahead and take the shot. The most recommended firearm for this activity is a shotgun. A shotgun can withstand the worst situations and weather conditions that might occur during hunting. You can choose your desired shotgun with respect to the bore diameter (gauge).\nThere are three bore diameters you can choose from, which are: 20-gauge, 12-gauge and 10-gauge.\nThe recommended diameter for duck hunting is 12 gauge shotguns. However, you can opt to experiment each of the gauges to see which one will work best for you. There are no guesses when it comes to shooting or else all your efforts would have been for nothing. You need to be able to adjust your angle of shooting with respect to the ducks’ flight speed.\nThe most practical technique that you can use is the swing-through. This technique requires you to hold your shotgun with one hand behind the trigger and the other hand under the fore stock.\nYour hand that is positioned beneath the fore stock should raise the barrel and your other hand brings the end of the shotgun to your shoulder.\nYou should swing your shotgun in order to aim ahead of the ducks in their flight path. At times, you may observe that ducks are not flying your way. When this happens, you may have no choice but to reposition your blinds.\nHunting ducks is easy as long as you have the correct information and gear. Since you may face challenges if you do not have the correct tips to guide you, we provided you with more than enough tips for the most important and crucial information about ducks and the best hunting gear. The “how to” steps are discussed in detail so you should have a successful hunt.\nYou do not need much to be an expert duck hunter, all you need are the tips provided in this article. Do you think we have left out any crucial tips? If so, please let us know in comments."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:b0dd7924-083e-49f5-b2cb-7e5ebc427ada>"],"error":null}
{"question":"What are the key features of the Typewriter Database for collectors, and how does it compare to postcard collecting in terms of valuation factors?","answer":"The Typewriter Database is the largest source of typewriter serial numbers online and includes photo galleries from collectors worldwide, which are linked to manufacturers and serve as a research resource. As for postcard collecting, the valuation factors include rarity, historical relevance, appeal, design, depiction, and demand, with prices ranging from $20 to thousands of dollars per piece. Both collectibles require attention to condition - typewriters are documented through photo galleries for reference, while postcards use a grading system from mint to poor condition to determine their market value.","context":["The Typewriter Database Our Mission »\nThe \"Typewriter Database\" is the biggest source of typewriter serial numbers on the Internet today. As far as we know, it's even the most complete collection of serial numbers ever. But when you see the number of given brand names, it's still only a beginning.\nThis Database is also a collection of typewriter photo galleries from the collections of enthusiasts all over the world. These galleries are linked to manufacturers and not only serve as a valuable additional resource for research about various machines, but also are fun to page through and see what collectors have in their typewriter collection.\nNews read more »\nVoss page updated! Sun, 13 Sep 2020\nThe Voss galleries were a bit of a mess, undoubtedly because Voss model names are fairly obscure and are not usually marked on the machine. I myself was pretty fuzzy on the topic, so before cleaning them up, I had to do a crash course on Georg Sommereger's 2012 research on the company, and look up the only published age list in Berghagen 1962 and work out the technical notes in German therein. Those sources suggested that there was no Model 51 (they're just Model 50's made in 1951) and no Model \"De Luxe\" (those are all S, ST or STD 24's or 32's). I have reorganized the Vossen accordingly and then discovered some new info from having the galleries reorganized and added that stuff to the page as well. It should be fairly easy to figure out your Voss model from the age list now.\nBurroughs Age List now available Sat, 12 Sep 2020\nThe Burroughs typewriter age list turned up, so all the Burroughses with serial numbers have been dated. Date your own here!\nHello, fellow Typewriter Hunters! We've started a Patreon page as a way to allow you, our fellow typewriter lovers, to help make the TWDB even better. A small monthly pledge helps pay the bills that keep The Typewriter Database online. Please sign up now.\nArchivist, The Typewriter Database\nMost Recently Edited Typewriter Galleries see more »\nVINTAGE Royal 2300 Portable Electronic Typewriter Open Box Top Collector\nCurrent Price: USD 179.99\nTypewriters > Elmwood Park,IL,USA\nEnding: 2020-12-09 22:50:53 (GMT)\nRefurb IBM Selectric Personal Typewriter w/self correction key-Truly a Collector\nCurrent Price: USD 708.40\nTypewriters & Word Processors > St. George,UT,USA\nEnding: 2020-12-14 19:14:45 (GMT)\nRheinmetall Typewriter GDR Office Old Antique Decor Vintage Collector\nCurrent Price: USD 77.16\nTypewriters > Germany\nEnding: 2020-12-02 17:47:10 (GMT)\nVintage Collector IBM Typewriter Ad sheets- YOUR CHOICE OF ONE BELOW\nCurrent Price: USD 7.34\n1930-69 > St. George,UT,USA\nEnding: 2020-12-21 23:37:05 (GMT)\nRare Vintage Remington Standard 12 Wide Receiver Typewriter Excellent Collector\nCurrent Price: USD 399.00\nTypewriters > Corbin,KY,USA\nEnding: 2020-12-29 01:14:53 (GMT)\nBrother AX-250 Electric Typewriter Rare Brand New Never Opened Vinatge Collector\nCurrent Price: USD 549.99\nTypewriters & Word Processors > Santa Rosa,CA,USA\nEnding: 2020-12-10 22:47:04 (GMT)\nAntique ROYAL Magic Margin Touch Control Typewriter, Vintage, Collectorâ€™s Item\nCurrent Price: USD 239.99\nTypewriters > Marrero,LA,USA\nEnding: 2020-12-09 09:32:11 (GMT)","The Top 10 Valuable Postcards: A Guide For Collectors\nImagine stumbling upon an old postcard in your attic, only to discover that it is worth thousands of dollars. This scenario may seem like a distant dream, but it is a reality for many collectors and sellers of antique and vintage postcards. Postcards have both historical significance and monetary value, making them a fascinating area of interest for those who appreciate the art and history of communication.\nIn this article, we will explore the top 10 most valuable postcards, including their historical relevance, rarity, and appeal. We will also provide a comprehensive guide for collectors and sellers, covering everything from postcard grading to where to buy and sell them.\nPostcards have come a long way since their inception in the late 1800s. From simple black and white images to elaborate hand-colored designs, postcards have evolved into a form of art that captures a moment in time.\nThe rarity of postcards in the 21st century is a significant driver of their contemporary value, with some antique and vintage postcards ranging from $20 to thousands of dollars per piece. The value of a postcard is determined by various factors, including its age, condition, rarity, and historical significance.\nIn this article, we will delve into the world of valuable postcards, providing insights into the factors that determine their value and the types of postcards that collectors and sellers should look out for.\nPostcard basics, including their historical significance, rarity in the 21st century, and factors influencing their value, are essential knowledge for collectors and sellers. These collectibles have been around since the 19th century and have played a vital role in the foundation of the collectibles world.\nThe Golden Age of postcards lasted from 1890 to 1915, and postcards were widely used for communication during this period. However, their usage declined after World War I. Postcards hold significant contemporary value due to their rarity in the 21st century.\nAntique and vintage postcards can range from $20 to thousands of dollars per piece, with a postcard's worth depending on its rarity, historical relevance, appeal, design, depiction, and demand. Picture postcards are more adored than their plain equivalents, and real photographic postcards are more valuable than printed and colored postcards.\nA postcard's condition determines its price and market appeal, and grading terms such as mint, near mint, excellent, very good, good, fair, and poor are used to describe a postcard's condition. Finally, collectors clubs, auction platforms, and e-commerce sites are good places to buy and sell postcards.\nHistory and Evolution\nThe evolution of the postcard industry has spanned over a century, tracing its roots back to the 19th century when it first emerged as a mode of communication and transitioned into a collectible with historical significance.\nDuring the Golden Age of postcards, which prevailed from 1890 to 1915, postcard usage surged, and many postcards were produced, depicting various themes, including landscapes, architecture, and people.\nHowever, postcard usage declined after World War I, as other modes of communication, such as the telephone, gained popularity.\nDespite the decrease in usage, deltiology, the study and collection of postcards, began in the late 19th century, and antique and vintage postcards remain popular among collectors today.\nThe rarity of postcards in the 21st century is a significant driver of their contemporary value. Postcards hold historical significance and offer a glimpse into the past, reflecting cultural and social history.\nThe study of postcards has also contributed to the foundation of the collectibles world, making it an essential aspect of the hobbyist community.\nLooking for more Carnival Glass?\nFactors That Determine Value\nVarious factors contribute to determining the value of antique and vintage postcards, including their rarity, historical relevance, appeal, design, depiction, and demand. Rarity is a major factor that drives the value of postcards, as the scarcer a postcard is, the more valuable it becomes. Historical significance also plays a significant role in postcard value, as postcards that capture important cultural or political events tend to be more valuable. Additionally, the design and appeal of a postcard can influence its value, with postcards featuring aesthetically pleasing designs or appealing subject matter commanding higher prices.\nThe depiction of a postcard can also affect its value, with postcards featuring rare or unusual subject matter often being more valuable. Demand is another key factor in determining the value of a postcard, as postcards that are highly sought after by collectors will command higher prices. Collectors should also consider the condition of the postcard, as postcards in excellent condition are generally worth more than those in poor condition. Overall, a combination of these factors can contribute to a postcard's value, and collectors should carefully evaluate each factor when assessing the worth of a particular postcard.\n|Rarity||The degree to which a postcard is scarce||Penny Penates|\n|Historical Relevance||The importance of the cultural or political event depicted on the postcard||Olympic Three Men of Carlisle|\n|Appeal/Design||The aesthetic quality of the postcard's design||Ellen Clapsaddle Signed Christmas Postcard|\n|Depiction||The rarity/uniqueness of the subject matter depicted on the postcard||African American Mother and Son Postcard|\n|Demand||The level of interest from collectors in the postcard||Cleveland Indians Postcard|\n|Condition||The quality of the postcard's physical condition||The Launch of the R.M.S Titanic postcard|\nTypes of Postcards\nCategorization of postcards is an essential aspect of deltiology, and they can be divided into two broad categories, namely topographical and topical cards.\nTopographical postcards are also known as view cards, depicting geographical locations, landmarks, and natural scenery. These cards often feature a caption or title on the front and a blank space for writing on the back. Topographical postcards can be further classified into subcategories such as city views, street scenes, aerial views, and landscapes. These cards are often popular among collectors interested in local history, architecture, and urban development.\nOn the other hand, topical postcards showcase various themes such as holidays, events, people, animals, and hobbies. These cards may feature illustrations, paintings, or photographs, and they often have a caption or message on the front and a divided back for writing and addressing. Topical postcards can be further classified into subcategories such as holiday cards, art cards, comic cards, sports cards, and transportation cards. These cards are often popular among collectors interested in specific themes, artists, or historical events.\nThe value of a topical postcard depends on its rarity, appeal, and historical significance, among other factors.\nReal Photographic vs Printed and Colored Postcards\nDistinction between real photographic and printed and colored postcards is crucial in determining the value of postcards.\nReal photographic postcards are printed using a photographic process that involves exposing light-sensitive paper to a negative image. The result is a detailed image with a high degree of clarity, depth, and contrast. These postcards are highly valuable because they provide authentic and realistic depictions of the subject matter. Real photographic postcards are usually more expensive than printed and colored postcards due to their rarity and the high level of skill required to produce them.\nPrinted and colored postcards, on the other hand, are created using printing techniques that involve lithography, offset printing, or letterpress printing. These postcards are usually less valuable than real photographic postcards because they lack the authenticity and detail of their photographic counterparts. However, printed and colored postcards can still be valuable if they have historical significance, unique designs, or are part of a limited edition.\nOverall, the value of a postcard is determined by several factors including rarity, historical relevance, appeal, design, depiction, and demand, and the type of postcard is a critical consideration in determining its worth.\nPostcard grading is an essential aspect of determining a postcard's condition and market value. The grading process involves assessing factors such as the card's age, condition, rarity, and appeal. The grading system typically ranges from mint, which is a card in perfect condition, to poor, which signifies a card in terrible condition.\nHere are three key factors that affect postcard grading:\nAge: The age of a postcard can significantly impact its grading. Older postcards tend to be rarer and, therefore, more valuable. However, age can also contribute to wear and tear, which can negatively affect a card's condition and grading.\nCondition: The condition of a postcard is a crucial aspect of grading. Factors such as creases, stains, tears, and fading can significantly impact a card's value. Postcards in excellent condition with no visible defects are graded as mint, while those with significant wear and tear are graded as poor.\nRarity: The rarity of a postcard plays a crucial role in determining its value. Unique, hard-to-find postcards tend to be more valuable than those that are widely available. Rarity is often determined by factors such as the number of cards produced, the demand for the card, and the card's historical significance.\nWhere to Buy and Sell Postcards\nOne potential concern some collectors may have when buying and selling postcards is the risk of fraud or counterfeit cards, but there are reputable auction platforms and dealers that offer authentication services to help mitigate this issue. For example, eBay has a program called 'Authentication Guarantee,'which verifies the authenticity of high-end collectibles, including postcards, before they are shipped to the buyer. Other auction platforms, such as Heritage Auctions and Weiss Auctions, also offer authentication services for postcards. It is essential for collectors to research and verify the authenticity of the postcards they intend to purchase to avoid falling victim to fraud or counterfeit cards.\nIn addition to auction platforms, collectors can also buy and sell postcards through collectors clubs, online marketplaces, and dealers. Collectors clubs, such as the Metropolitan Postcard Club of New York City, provide a community for postcard enthusiasts to connect, share knowledge, and buy and sell postcards.\nOnline marketplaces, such as Etsy and Ruby Lane, offer a vast selection of postcards from different eras, categories, and prices. Dealers, such as Postcard Connection and Antique Postcards, specialize in vintage and antique postcards and can provide expert advice on buying and selling postcards. It is crucial to do research and compare prices and services before buying or selling postcards to ensure a fair and profitable transaction.\nThe Most Valuable Postcards\nThe worth of antique and vintage postcards can range from a few dollars to thousands of dollars per piece, with the top 10 most valuable postcards being highly sought after by collectors and investors alike. The most valuable postcards are often rare, historically significant, and visually appealing.\nThe Penny Penates postcard, for example, is considered the most valuable postcard in the UK and features a depiction of Roman gods and goddesses. The African American Mother and Son postcard is the most valuable American postcard and showcases a touching image of a mother and son. The Cleveland Indians postcard, valued at £11,000, is the most valuable postcard from 1910, while the Young Boys Pump House postcard, sold for £6,000, is the most valuable postcard of 1908.\nOther valuable postcards include The Chinese Fruit Seller postcard, which sold for $4,800 and reflects China's rich culture, and the Ellen Clapsaddle Signed Christmas Postcard, which sold for up to $1,400 and features a charming image of a toddler girl giving a card to Santa Claus. The Governmental Postcard, issued in 1872 to reduce the costs of using postcards, is plain and simple but among the most precious cards and sold for $1,236.17.\nCollectors and investors can find these valuable postcards at auctions, online marketplaces, and through private sales.\nWhere to Buy Postcard\nLooking for Postcard? Find Postcard for sale now!\nOne Ben 1909 Franklin Card Uncanceled Co Detroit Publishing Cent Rare PostSee Details\nEbny Embossed Easter Greetings Cool Antique Card Rare Germany Rabbit PostSee Details\nAutograph Of One Mantle Kind Mickey Psa Business Card Auto ASee Details\nNeal Negro Postcard 1909 Real League Photograph Buxton George Wonders RareSee Details\nPicture Autograph Karen Postcard Isak Envelope Blixen Signature With DinesenSee Details\nAutographed Signed New John Copacabana Postcard Wayne YorkSee Details\nRare Vintage Stamp Post Cent On George 1 Card WashingtonSee Details\n1 Rare Cent George Washington U Stamp Card Vintage 1 Post S PostageSee Details\nCut Robinson Perez Psa Autograph Signed Jackie Dodgers Steele Postcard BrooklynSee Details\nCard Cent Stamp Red 2 George Washington 3 Rare Two Post On VerySee Details\nBen 1939 1 Cent Franklin Card Postmarked Post Stamp OnSee Details\nClarity Cent George Rare Stamp Incredible On 1 Postcard Washington 1924See Details\nCelebration 10 Set Fame 50th Hall Of Anniversary Perez-steele AutographedSee Details\nFl33991 Plots Cemetery Ridge - Two Veterans Cemetery Coral Coral CapeSee Details\nCondition 2 Card 1922 Post Excellent Red Stamp Birthday Used On CentSee Details\nPostcard Cent On One 1910 Green Franklin A Benjamin StampSee Details\nDiff Massive Lot - Hutson Hof 106 Of Nfl Hinkle Brown Signed Jim Shula PostcardsSee Details\nRoomsign 12 Mint - 10 Motel Inn Mantle 1962 Psa Holiday Gem Mickey PostcardSee Details\nAll California Postally Country More Gold Used 192 Gold PostcardsSee Details\nCalifornia More Posted Motel 826 Hotel All San Francisco PostcardsSee Details\nAntique Im Schonlte Card Ich Ein German Blumchen Rare Ilt Post Geben War Und DasSee Details\nSee Also: Lefton China"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:527bed12-0ed2-4d1b-8fcc-21f1fbb869d9>","<urn:uuid:0ad099cf-2798-46ef-8e97-81af9c444560>"],"error":null}
{"question":"As an art history student diving into time-based art, I'd love to know how Roman Opalka and Peter Campus differently approached the concept of time in their works!","answer":"Roman Opalka approached time through his OPAŁKA 1965 / 1-∞ programme, attempting to depict the passing of time by creating measurable and visible forms, dedicating much of his life to this single project of representing something immeasurable. Peter Campus, on the other hand, explored time through sophisticated technical video articulation, creating 'paintings' in paused video moments that mix light and color, meditating on the passage of time and humanity's place in the natural world. While Opalka focused on systematically documenting time's progression, Campus explored time through the relationship between reality and virtuality, emphasizing the interaction between artwork and spectator.","context":["Surprising is the recently born Cartagena de Indias International Biennial of Contemporary Art in Colombia, that opened within the framework of one the most beautiful cities in the world. Under the direction of the well-known curator Berta Sichel, and a team of curators such as the American Barbara S. Krulik, the Nigerian Bisi Silva, the Belgian Paul Wellemsen and the German Stephanie Rosenthal, over 120 international and Colombian artists showed work that not only impressed us but also left an indelible mark in the art world.\nRomuald Hazoume, from Benin, brought us a work that consisted of a motorcycle used in his country to transport petroleum and water to far-away areas, painstakingly built to reach equilibrium, with which the artist held a coherent discourse that the future wars in the world will not be caused by petroleum but by water. He also made a balanced criticism on the social problems facing his country.\nJanet Biggs (USA) presented an impressive 4-channel video called “Step on the Sun”, filmed in the Ljen volcano in Eastern Java (Indonesia), where brave men in the midst of total adversity, recover the sulphur left by the burning lava of the active volcano, as a means of subsistence in an extreme environment, in the ambiguity of nature’s beauty and the human exploitation, thus dealing with a discourse on global biopolitics and humanism.\nNick Cave (USA), mono-channel video and “soundsuit”. These are objects and mannequins wearing strange and colorful dresses made in the 90s for dancing performances in stop-action, furnished with visual effects and singing of crickets and Native American drum beats.\nAnother interesting work during the biennial is by Helena Almeida from Lisbon. By means of photography, she made a series of visual reflections on space-time and the adverse human relations in the contemporary world in such a way that the spectator is forced to reflect on their own human condition, limitations, and possibilities.\nPeter Campus (USA) brought us a beautiful and poetic work, posing a conflict between what is real and what is virtual, the relationship between a work of art and the spectator, existentialism and narcissism. His work stands out for the highly sophisticated technical articulation in terms of the creation of an image. He presented four videos during the biennial, using this technique to create “paintings” in the videos paused in the moment, mixing light and color showing some kind of painting imagery, meditating on the passage of time and the concept of humanity in the natural world.\nBill Viola (USA) came with one of his sensorial perception videos, “Walking on the Edge”. This mono channel video (2012) showed another universal human experience, using the internal language of subjective thoughts, representing the separation between the father and the son when they take different paths in life, a separation symbolized in a nebulous desert, and at a certain moment, the paths cross and they walk side by side to be separated again.\nThe Colombian Elias Heim, sculptor and installation artist used a diverse mechanic technology in his works envisioning the world from three aspects as he himself noted: religion, science, and art. His work in the Biennal is an installation called “Gulgolet”, where these three aspects stand out with well configured skulls in neon light placed in a frontal mural of the exhibition space, and a compilation of the same profiles in pieces of wood piled up on the floor. This reminds us of the Shoah burial grounds and the unmarked burial grounds of the wars in Colombia.\nJulie Mehretu (USA-Germany) painted in huge format, in a complex pictorial language that reflected in a mimetic way on the changing cosmology, she uses an abstract and chaotic language that contains brushstrokes, graffiti, calligraphy, and architectural elements.\nIt’s important to mention the works by Ming Wong (video-performance in a dramatic work speaking of the genre of travestism and melodrama), Khalil Rabah (Palestinian conceptual artist with an installation of piles of postcard photographs taken in different Palestinian villages that are now occupied, organized in the space as an aerial cartography), Xi Ziuzhen (mysterious libraries in which the books are covered in dress clothes, as testimonies of a determined period or moment of history) and Candida Hofer with a diaporama of beautiful realistic photographs on the daily life of the Turkish people in Germany. It would be too extensive to name all the artists, but it was certainly a refreshing biennial that should be see\n* Picture on slider – Bill Viola – video Walking On The Edge","Dire il tempo / Telling Time by Roman Opałka\n4 May 2019 – 20 July 2019\nRoman Opałka. Telling Time, a project curated by Chiara Bertola, is conceived and produced by BUILDING and Fondazione Querini Stampalia.\nAn exhibition in two parts: in Venice at the Fondazione Querini Stampalia museum and in Milan at BUILDING. Both exhibitions are centred on the OPAŁKA 1965 / 1-∞ programme, which the artist worked on for much of his life in an attempt to depict the passing of time and to circumscribe the infinite in visible and measurable forms. In Venice a nucleus of works by Mariateresa Sartori (Venice, 1961) – with whom the artist had formed an intense relationship in the city – will be presented. Interested in neuroscience, music and language, her works initiate a dialogue with Opałka’s, through their common research on the themes of time, duration and contingency, and the shared search for something visible capable of expressing the invisible. Dire il tempo / Telling Time springs from the desire to examine Roman Opałka’s work through a selection of works which are fundamental stages of his research, many of which have never been presented or exhibited in Italy before, coming from important private and public collections, including the Muzeum Sztuki in Łodź and above all the Fonds de Dotation Roman Opałka, with which a close relationship was formed during the ideation of the project.\nFrom 4 May until 20 July 2019, Roman Opałka, a Retrospective, the first chapter of this project, opens to the public in the Milanese spaces of BUILDING, which is setting aside all of its four exhibition floors to the exhibition. From 7 May until 24 November 2019, the second chapter, Roman Opałka. Mariateresa Sartori will run in the rooms of the Museum Home at the Fondazione Querini Stampalia in Venice, where the works of Roman Opałka and Mariateresa Sartori are placed in relation both to each other and to those of the institution’s historic collection. Both the Milanese and the Venetian chapters revolve around OPAŁKA 1965 / 1-∞, a project the artist dedicated much of his life to in the attempt to depict something immeasurable: the passing of time. Of particular importance for the critical awareness of the artist, the Venetian chapter reunites and presents for the first time two fundamental works for the entire OPAŁKA 1965 / 1-∞ programme: the first Détail, on loan from the Muzeum Sztuki, Łódź in Poland and exhibited for the first time in Italy, and the last, which remains unfinished, has never presented to the public and is on loan from a private collection. The Alpha and the Omega finally reunited. As the curator Chiara Bertola writes: “Seeing them together for the first time and being able to grasp the whole picture of what the artist had decided to trace one day is a strong and moving emotion. The project that is so explicitly shown also bears the tragedy of its very assumption: “He is no longer with us; there is the completed work”. Alongside these two important Détails is a series of photographic self-portraits and a sound recording of the artist’s voice, as well as two ‘exercises’, two ‘artist tests’ that Opałka made shortly before beginning his OPAŁKA 1965 / 1-∞ programme, which have also never been shown to the public before.\nOn 6 and 7 May 2019 Didier Morin’s film “Le dernier Detail peint de Roman Opałka” will be presented to the public for the first time. It depicts Opałka’s painting time, set to the rhythm of everyday life, in his atelier at Bois Mauclair, where everything turns sacred when the artist repeats out loud the numbers he is painting. The artist was filmed and recorded over approximately forty sessions while he was working on what would be his last Détail. “Le dernier Detail peint de Roman Opałka” duration: 225 minutes language: Polish, French, with English subtitles Screenings:\n- 6 May 11am first screening, 6pm second screening\n- 7 May 10am first screening, 2pm second screening\nIt is with these works that Mariateresa Sartori establishes a dialogue in the rooms of the Querini Stampalia Museum. As the curator highlights: “Her work provides the chance to create emptiness and new spaces of perception and understanding reality beyond pre-established meanings; it provides the entry key to new languages. Thanks to juxtapositions, deviations, superimpositions and intersections between different alphabets, Sartori’s work intercepts the subtle thread of relations which exist between nature and artifice, the epic and the everyday, visible and invisible, past and present, and objectivity and subjectivity, without ever being defined, always leaving the tension towards the infinite open… The purpose of all her research is about identifying the mechanism and the possible fragmentation in which the infinite seems to let itself to be harnessed. Above all, her work is about managing to represent the infinite in something finite, measurable and visible. Just as it was for Opałka…” In the site-specific installation Il tempo del suono. Onde / The Time of Sound: Waves on display here, the sheets of paper – individual particles of a wider totality – are recomposed on the wall in a single continuous series. The work, which translates the sound of the sea waves into a visual form, represents her attempt to listen to the passing of time and is the result of an immense musical score which codifies the sonorous and temporal flow.\nBefore reaching the room where the installation is housed, you walk down a long corridor where some self-portraits by Roman Opałka are placed in sequence. As you walk down the corridor you listen to the Polish sonority of the artist’s voice, which pronounces the numbers he is painting. These two different forms of representing infinite time resound contextually. Time is also etched in the two series of pinhole photos, Feuilles and Cronache. With a simple black cardboard box held together with sticking tape to make a pinhole camera, Mariateresa Sartori goes around gathering instances of the sensitive world, almost like evidence “that reality exists”. The results are the small images spread on the engraved table in the mythological room and those entirely covering the mirrors mounted in the stucco decoration of the museum boudoir. Cronache (Chronicles), the series of pinhole photos exhibited on the table in the museum’s mythological room, constitutes one of the site-specific works present in the exhibition. The subjects of the photos are isolated details taken from paintings found hanging in that room: the face of an elderly man with a beard, the head of a little dog, a child leave the narration of their own time and suddenly and dramatically become current to the eyes of the person observing them: facts of contemporary chronicle which each of us can find in our own personal memory. It is evidence of how the artist has managed to make the vision of paintings which risked no longer being seen and visible newly significant. This always happens with projects from the “Conserving the Future” programme: every time a contemporary artist relates to the past, the past demonstrates that it still has a lot to say.\nThanks to “Conserving the Future” another work present in the exhibition was made in 2008: Il suono della lingua, 11 audiobooks, which are now part of the museum’s permanent collection. The languages undergo an unusual process and are stripped of their meaning to assume another in terms of musicality, rhythm and melody. Once again the order that composes beauty is a question of rhythm, as is highlighted in another work in this exhibition in the Museum Portego: Tutti quelli che vanno / All Those Who Come. Two graphics but also two magnificent and enigmatic drawings which represent the flow of people walking around St Mark’s Square in Venice at a precise time. This cycle of drawings arises from an association with a research group from the University of Bologna which studies pedestrian flows from a physicalmathematical viewpoint. Finally, in the wardrobe room of the boudoir, the video Omaggio a Chopin / Homage to Chopin is screened, about which there was an important exchange between Mariateresa Sartori and Roman Opałka regarding a crucial point. The video is dedicated to Roman Opałka, who died before he could see it with the formal solution that he had suggested. “Both of these artists,” writes Chiara Bertola, “has created a system, invented a metaphor, a new code, a mechanism, a model, in order to get closer to and brush up against the infinite. The emotional dimension has always allowed them to translate arid scientific data into something universal and broader, reminding us that although we move around in our small everyday space we do so withinimmeasurable spatial and temporal coordinates.”\nBorn 27 August 1931 in Hocquincourt, France, to a family of Polish origin. The Opałkas returned to Poland in 1935, to then be deported to Germany in 1940, where they remained in a work camp until the end of the war. Once freed, they returned to France to then return finally to Warsaw, where Opałka attended the graphic design school Wałbrzych Nowa Ruda (1946-1948) and the art and design school in Łódź (1949). Between 1950 and 1956 he studied at the Warsaw Academy of Fine Arts and in 1957 he moved to Paris. In 1966 he held his first solo show at the Galeria Dom Artysty Plastyka in Warsaw. The following year he began the OPAŁKA 1965 / 1 – ∞ project, which he would dedicate the rest of his life to from 1970. Opałka would thus become inextricably linked to conceptual art. Between the 1960s and 1970s he won numerous prizes: the Grand Prize at the First British International Print Biennial, Bradford (1968), two awards at the seventh International Biennial Exhibition of Prints and the Art Museum Ohara, Tokyo (1970), and first prize from the Polish Ministry of Culture and Arts (1971). In 1972 he went to the USA for the first time. In 1979 he moved to Bazérac, in France, and won a prize at the fourteenth Biennale of San Paolo. In 1985 he became a French citizen. Between 1985 and 1990 he taught at the Summer Academy of Salzburg. In the following years Opałka would exhibit on numerous occasions and receive various prizes, such as the National Painting Prize, Paris (1991), and the Special Prize from the Polish Ministry of Foreign Affairs in Warsaw (1996). In 1992 he had an exhibition at the Musée d’Art Moderne de la Ville de Paris and in 1995 he represented Poland at the Venice Biennale. In 2002-2003 a large travelling anthology of his work would visit various European cities. In 2009 he was awarded the title of Chevalier des Arts et des Lettres in Paris, and the Gloria Artis Gold Medal in Warsaw. Opałka died in Chieti on 6 August 2011.\nBorn in Venice in 1961 where she lives and works. He research revolves around three thematic fulcrums: empirical scientific method; behavioural dynamics, often in relation to neuroscience; music and sound in relation to language.\nThe educational aspect is important to her artistic practice: she has taught drawing for over a decade, applying the Betty Edwards Drawing with the Right Side of the Brain method, which is based on the same neuroscientific presuppositions which inspire her own artistic research. The artist often works in association with experts in the fields she explores: geologists, theoretical physicists, musicologists, musicians, singers, actors, botanists, ornithologists. Concrete data is empirically revealed and then analysed from perspectives which vary from work to work and which have different outcomes, from video to drawing, from pinhole photography to sound work. Constants and not exceptions, the universal and not contingencies drive her research, which is aimed at a clearly unattainable objectivity. She has exhibited in numerous museums and galleries in Italy and abroad in personal and group shows: Cairn Centre d’art, Digne les-Bains, France; MMOMA, Moscow Museum of Modern Art; Palazzo Fortuny, Venice; Museum of the Russian Academy of Fine Arts, Saint Petersburg; Fondazione Bevilacqua La Masa, Venice; ICA,\nThe Showroom, London; NGBK Berlin; Hangar Bicocca, Milan; Macro, Rome; Neue Galerie, Graz; Palazzo delle esposizioni, Rome; Mucsarnok Hall of Art, Budapest; Careof, Milan; Folkwang Museum, Essen; Fondazione Querini Stampalia, Venice; Auditorium Parco della musica, Rome; Museo di Palazzo Poggi, Bologna; Serra dei giardini della Biennale, Venice; XLV Venice Biennale; Museo Mambo, Bologna; Kunsthaus Centre d’art Pasquart, Biel, Switzerland; the Hermitage Museum, Saint Petersburg; Les Ateliers d’artistes, Marseille.\n7 May 2019 – 24 November 2019\nFONDAZIONE QUERINI STAMPALIA, Castello 5252, Venice"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"atemporal"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:7088fa46-a89a-4f69-bc65-f9d7797d311b>","<urn:uuid:df7ce01e-bb5b-4f76-bdfe-ffd56f70224e>"],"error":null}
{"question":"How do control-related issues affect both stressed individuals and difficult patients in healthcare settings?","answer":"Both stressed individuals and difficult patients struggle with issues of control in healthcare settings. Difficult patients often experience loss of control during medical visits, which can contribute to challenging encounters with healthcare providers. Similarly, individuals under stress may experience a lack of control over their emotions and behaviors, leading to symptoms like angry outbursts and changes in eating habits. The solution approach for both groups involves helping them regain a sense of control - for difficult patients, this can be done by asking questions like 'What would make things better?' and offering realistic options, while for stressed individuals, it involves learning to identify triggers and developing stress management strategies they can control, such as breathing techniques and sleep schedules.","context":["Although most patient encounters with physicians go smoothly, some do not. Most if not all physicians get a difficult patient from time to time. Difficult patients have a common thread: a need for something a provider cannot provide, said Karen Broquet, MD, Associate Professor of Medicine/Psychiatry at the Southern Illinois University School of Medicine in Springfield. “For doctors, it’s self-evident that we can’t meet patients’ needs all the time, but that’s not always evident to patients,” she said.\nDirect-to-consumer health care advertising has led to a growing number of patients with unrealistic treatment expectations, she noted.\nMost of the time, difficult patients are not a major problem. If their behavior interferes with treatment or becomes overwhelming or physically dangerous, there are ways to handle it or work through most conflicts constructively. Doctors also may contribute to a difficult patient encounter. They could just be having a bad day, or it may be a pattern. Doctors who are defensive, fatigued, harried, dogmatic, or arrogant are more likely to run into difficulties with patients, Dr Broquet said. Practices need to identify physicians who, compared with their colleagues, receive more patient complaints or poor ratings related to interpersonal behavior, and then address the problem.\nCertain office triggers, such patient or provider lateness to an appointment, may contribute to difficult patient interactions. Practices should create protocols for addressing these problems.\n“No doctor of staffer should have to reinvent what to do every time one of these things comes up,” she said. “Setting limits can be a good thing – especially for the patient who really doesn’t know that showing up 2 hours late is not okay.”\nJodi De Luca, PhD, a clinical psychologist at Erie Colorado Counseling, PLLC, and Behavioral Health staff member at Boulder Community Hospital Emergency Department in Boulder, Colorado, emphasizes that stressed out health care providers working long hours and meeting high demands can make difficult patient encounters increasingly challenging. “We are responsible for an individual’s overall well-being,” Dr De Luca said. “This alone creates incredible pressure … and our own frustrations, fear of error, and other variables contribute to the challenges of difficult patients.”\nPatients’ medical issues, loss of control, and fear of procedures also can contribute to difficult encounters. “From an emotional and psychological perspective, a patient’s visit with a physician, in particular, can be quite overwhelming,” Dr De Luca said. “Hence, the behavioral manifestations can be very difficult for the health care team to deal with. Physicians and nurses, in particular, bear the brunt of the negative behavior.”\nSome types of patients can present greater difficulty than others, such as patients who gets angry when they thinks they are not being treated well. Other challenging patients include those with what Dr Broquet calls “high emotional needs” and self-destructive patients, such as those who do not following treatment recommendations or reject help for addictions and other maladies.\nAnother difficult patient is the one with many somatic complaints not tied to specific physical conditions. These individuals tend to be high utilizers of health care services who are unhappy because they are not getting better. Often, there is an underlying mental health condition contributing to their behavior, Dr Broquet said.\nWorking with challenging patients can be a drain on an office’s resources. These patients can decrease productivity, waste staff time, reduce time seeing other patients, and increase burnout and resentment toward patients. Calming patients and getting back to treating patients as quickly as possible is imperative. One way to do so is just listening. Dike Drummond, MD, contends that allowing people to be heard typically diffuses bad situations. For this reason, he created the Universal Upset Person Protocol with 6 phrases providers can say to calm someone:\n1. “You look really upset.”\n2. “Tell me about it.”\n3. “I’m so sorry this is happening to you.”\n4. “What would you like me to do to help you?\n5. “Here’s what I’d like us to do.”\n6. “Thank you for sharing your feelings; it’s important that I understand you today.”\nDr De Luca recommends helping the patient gain a sense of control by using questions like: “What would make things better?” “What options do you propose?” If their recommendations are not workable, offering other realistic options is helpful. Once presenting alternative options to patients, a recommended follow-up question is, “Which do you think is best for you?”\nOffice staff can also be used to smooth situations by alternating who works with challenging patients so no one person gets the brunt. If possible, bring in staff who has a good relationship with that patient. It may be wise to bring in a third party during visits with known difficult patients, Dr De Luca said.\nIn rare instances, visits can turn violent. Known high-risk settings include emergency departments, wards holding people with mental health issues, and crowded spaces. The risk of violent encounters, Dr Broquet said, is elevated with the following kinds of patients:\n- People with a history of violence\n- Young men\n- Those who are intoxicated or withdrawing from substances\n- People with personality disorders or psychotic mental illnesses\n- Individuals who are delirious, have a head injury, or a history of brain trauma\nPatient behaviors that may herald violent outbursts include repetitive movements or phrases, pacing, jumpiness, loud speech, cussing, accusations of conspiring against them, or using discriminatory or sexual language. If a situation rises to the case where someone is yelling or throwing things, providers should focus on safety. The point of maximum benefit from simply listening and being kind has passed, Dr Broquet said.\n“Offices might want to have 1 person available on call or a security officer for these situations,” she said. “There is a skill level to working with these patients that everyone can’t be trained for.”","How Does Stress Affect The Body: Symptoms And Solutions\nUpdated August 27, 2020\nMedically Reviewed By: Lori Jones, LMHC\nAre you exhausted all the time? Do you have a lot of muscle tension and pain? Is your head or stomach bothering you? It could be stress. There is a long list of symptoms that falls under the question, “How does stress affect the body?” And, learning how to identify the symptoms can help you find the right solution.\nStress levels have been on the rise in Americans over recent years. It’s impacting people of all ages and spans a wide range of worries and concerns.\nThe Impact Of Chronic Stress\nEveryday stress can have a negative impact on multiple areas of your life. However, when the stressful situation passes, you may find that things return to normal even if you didn’t do anything to address your stress. This isn’t the healthiest way to get through stress, but it happens this way for some people.\nHowever, if you’re experiencing chronic stress, it’s not going to just go away. It may not be tied to a specific situation in your life. Instead, it might be the result of poor habits or not knowing how to deal with past trauma. It will not just go away if left untreated.\nThe Effect Of Stress On The Body\nStress can wreak havoc on your body if it’s left unchecked. Not only does occasional stress show up in your body, but chronic stress can also have long-term negative consequences for your physical health. When you are feeling stressed, you may experience:\n- Increased heart rate\n- Rising blood pressure\n- Muscle tension\n- Upset stomach\n- Lack of sexual desire\n- Change in appetite\nAnd these are just a few of the symptoms that you may experience. If you suffer from chronic stress, the symptoms above can start to turn into more serious health consequences.\nChronic stress can lead to health problems such as heart disease, high blood pressure, gastrointestinal problems, heart attack, and strokes, among others. These are clear indicators that allowing chronic stress to continue in your life can be detrimental to your physical health and well-being.\nHow Stress Affects Mental Health\nStress also impacts your mental health and wellness. It can lead to you experiencing many different negatives and difficult emotions such as sadness, anger, frustration, and fear.\nSome of the mental health symptoms that you may notice in your life from stress include:\n- Lack of motivation\n- Irritability and anger\n- Lack of concentration and focus\nThese are serious symptoms that should not be taken lightly. If you experience chronic stress, you may begin to think that these symptoms are just a normal part of life. But, they’re not. All of these symptoms can grow into more serious problems if you don’t work on addressing them.\nHow Stress Affects Behavior\nStress can also impact your behavior. If you look at the symptoms listed above under physical and mental health, it can be easier to understand how stress changes your behavior. If you’re living under constant overwhelm and anxiety and experiencing things like frequent headaches or stomach aches, it can be easy to lose your temper with your loved ones, for example. Here are some of the other behavioral changes that you may experience in your life as a result of stress:\n- Angry outbursts\n- Eating too much or not enough\n- Substance use or abuse\n- Social withdrawal\nThese behaviors can have a negative spiral effect on your life. For example, as your withdrawal from friends and family because of stress, you may find that you struggle even more to cope with stress in your life. This can lead to additional problems which keep you away from a social activity even more. This is why it’s important to learn to recognize and healthily address your stress.\nStress Management Tips To Overcome Chronic Stress\nThankfully there are many things that you can do to address your chronic stress and learn to overcome it. This doesn’t mean that you’ll never experience stress again. Instead, it means that when you do go through stressful situations, you’ll have tips and strategies that you can use to relieve stress and handle it healthily.\nSome of the stress management solutions you may benefit from include:\nLearn to identify your stress triggers\nWhen you start to feel stressed, it can be helpful to take time to identify where the feelings are coming from. This allows you to begin investigating what you can do to make to address it.\nWhile there will be some things causing you to stress that you can’t do anything about, there will be some things that you can address. For example, if a family member’s behavior is causing you to feel stressed, you probably aren’t going to be able to control how they are behaving. But you may be able to establish boundaries in your life that stop the other person’s behavior from having as large of a negative consequence on you.\nThere will be some things that you find are short term stressors. But there also might be habits that you identify that are causing you unnecessary stress. When you learn where the stress is coming from, you can start to take your first steps to address or removing it.\nPractice Deep Breathing\nWhen you’re starting to feel the stress and tension build up within your body, deep breathing can help to break up some of the physical symptoms that you’re experiencing. For example, you may notice that you start to breathe faster as your frustration grows. This can cause your heart to race, as well. And, as your heart beats faster, your blood pressure rises. These physical symptoms can continue to build and even lead to things like full-blown panic attacks.\nDeep breathing can help to stop your physical symptoms from progressing. As you start taking slow, deep breaths in and out, you may notice that it feels like your blood pressure is lowering, and your heart rate is returning to normal.\nYou may also find that deep breathing can help you to slow your thoughts. Your mind will be forced to temporarily shift from your stress and worry to the breathing technique that you’re using. This can help you to regain mental clarity and look for solutions to the stressful situation or problem that you’re facing.\nThere are multiple types of breathing techniques that you can use, so practice a few of them to find what works best for you. It can also help to practice them when you’re not under stress, so when you find your stress starting to build, you will know how to put the breathing exercise to use without too much thought.\nNot getting enough sleep can make it even harder to deal with stress. You may find that you struggle to be patient with others, and you cannot think clearly to look for solutions. If you’re having problems falling asleep or staying asleep due to stress, it’s an important symptom to address.\nMany different things may help improve sleep troubles. A few that you could try include:\n- Keeping a strict sleep schedule\n- Cutting out caffeine\n- Not exercising too close to bedtime.\n- Sleeping in a dark, cool room\n- Using white noise\nHowever, if you’re continuing to struggle, don’t be afraid to talk with your doctor to explore additional options.\nGet More Physical Activity\nPhysical activity and exercise can help you release tension that has built up from chronic stress. It also releases chemicals in your brain that work to boost your mood. But these chemicals also act as natural pain killers, which can help reduce some of the physical symptoms you’re experiencing.\nThere are other ways that physical activity and exercise can help with stress. You may find that you sleep better when you exercise. And, you may experience a boost in your self-esteem as well.\nThe Anxiety and Depression Association of America shares that you may start to experience these positive mental boosts after just five minutes of physical activity. So, if you’re feeling stressed, you don’t need to feel like you have to get in a full workout. Simply getting moving for a few minutes can start to help.\nTalk To Someone\nHaving a trusted person to turn to for support can help when you’re going through stressful situations or experiencing chronic stress. This could be a friend or family member. It could also be a support group. For example, if you’re under stress as a result of losing a loved one, you may benefit from connecting in a group for others experiencing grief from losing someone.\nIf you don’t have anyone to turn to or could use additional support in handling your stress, a licensed therapist is an effective option to consider. Not only can they listen as you talk through the stress in your life, but they also have education on how to help you overcome it. A therapist, like those at BetterHelp, can assist you in finding stress-relieving strategies that work for your specific situation.\nPrevious ArticleHow Stress Can Lead To Emotional Breakdowns And What You Can Do To Avoid It\nNext ArticleAre You Under Too Much Stress? Symptoms, Treatment And Tips\nLearn MoreWhat Is Online Therapy? About Online Counseling\nAbuse ADHD Adolescence Alzheimer's Ambition Anger Anxiety Attachment Attraction Behavior Bipolar Body Dysmorphic Disorder Body Language Bullying Careers Chat Childhood Counseling Dating Defense Mechanisms Dementia Depression Domestic Violence Eating Disorders Family Friendship General Grief Guilt Happiness How To Huntington's Disease Impulse Control Disorder Intimacy Loneliness Love Marriage Medication Memory Menopause MidLife Crisis Mindfulness Monogamy Morality Motivation Neuroticism Optimism Panic Attacks Paranoia Parenting Personality Personality Disorders Persuasion Pessimism Pheromones Phobias Pornography Procrastination Psychiatry Psychologists Psychopathy Psychosis Psychotherapy PTSD Punishment Rejection Relationships Resilience Schizophrenia Self Esteem Sleep Sociopathy Stage Fright Stereotypes Stress Success Stories Synesthesia Teamwork Teenagers Temperament Tests Therapy Time Management Trauma Visualization Willpower Wisdom Worry\nFeeling Overwhelmed? Learn These Stress Management Strategies How To Stop Stressing: 7 Tips To Find Balance And Relax How Stress Can Lead To Emotional Breakdowns And What You Can Do To Avoid It Are You Under Too Much Stress? Symptoms, Treatment And Tips 7 Tips On How To Handle Stressful Situations Stress Management That Works: How To Be Less Stressed"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:5e15da6b-26fc-4e0c-a916-31c9d4542a47>","<urn:uuid:1fe21db8-8096-462d-a9b3-85cbac331785>"],"error":null}
{"question":"I'm writing a research paper on the final battle of Fort Hindman. Could you describe the events that led to its capture by Union forces in January 1863?","answer":"The capture of Fort Hindman began when Federal forces under Major General John McClernand arrived at a nearby plantation on January 9, 1863. The next afternoon, they initiated a bombardment using ironclads and other naval vessels. On January 11, after the navy had silenced the fort's guns, McClernand launched a land attack. The Confederates surrendered late that afternoon. Nearly 5,000 Confederate troops were captured and sent to northern prison camps. Following the victory, the Federal forces destroyed as much of the fort as possible before departing. This battle effectively ended military activity in the area for the remainder of the war.","context":["Located on the Arkansas River near the site of Arkansas Post, Fort Hindman served as an important Confederate defensive fortification during the Civil War. Captured by a combined force of Federal troops and the Union navy, the fort was destroyed in 1863, and the site was eventually claimed by the river.\nOn September 28, 1862, Major General Theophilus Holmes ordered the construction of fortifications along the Arkansas and White rivers. The construction of these fortifications was in direct response to Federal movements on the Mississippi River and followed a Union fleet attacking a Confederate post at St. Charles (Arkansas County), located on the White River. Located about twenty-five miles above the mouth of the Arkansas River, Arkansas Post was selected as the site of one of these forts. The location selected placed the fort in a horseshoe bend that afforded the garrison a view of approximately one mile both upstream and downstream.\nLieutenant John Dunnington of the Confederate navy was placed in charge of the construction of the fort along with two civilian engineers, a company of sappers (combat engineers), impressed slaves, and Confederate troops providing additional labor. Work progressed quickly, and the main fortification was completed by late November. It was named for Major General Thomas C. Hindman, commander of the District of Arkansas and former commander of the Department of the Trans-Mississippi.\nThe fort was a square, with exterior parapets 100 yards in length. Surrounding the fort was a ditch, twenty feet wide and eight feet deep. A firing step for infantry ran the length of the interior walls, and two casemates were constructed in the fort, each housing a single gun. Three heavy guns defended the fort: two nine-inch columbiads and one eight-inch. The guns previously served on the CSS Pontchartrain, and thirty-five Confederate sailors, along with Dunnington, were transferred to the fort to operate the weapons. Four ten-pound Parrott rifles and four six-pound guns were also placed in the fort, resting on artillery platforms. Inside the fort perimeter were a well, two magazines, and three buildings. Outside of the fort, a line of trenches ran to the west for about 720 yards before it terminated at Post Bayou. Additional rifle pits were located to the northeast. A hospital opened in the nearby town of Arkansas Post, and winter quarters began to be constructed several hundred yards to the north of the fort.\nIn December, the garrison at the fort was increased, bringing the total number of units to three brigades of mixed infantry, cavalry, and dismounted cavalry, with some artillery support, for a total of 5,000 men. The troops hailed from Arkansas, Texas, and Louisiana and were joined by a new commander, Brigadier General Thomas James Churchill. A failed Union attempt to take the fort in late November and early December emboldened the Confederates, who captured the Federal transport Blue Wing the last week of December. The ship, which was carrying munitions and coal, was brought to the post and unloaded. This action brought the fort back to the attention of Union commanders, and a force under the command of Major General John McClernand embarked on a mission to neutralize Fort Hindman.\nWith a combined force of troops and naval vessels, the Federals arrived at a plantation near the fort on January 9, 1863, and launched a bombardment from the ironclads and other ships in the fleet the next afternoon. On January 11, McClernand launched a land attack after the navy had silenced all of the guns in the fort. Late that afternoon, the Confederates surrendered.\nAlmost 5,000 men were captured and shipped to prison camps in the north. The Federal force destroyed as much of the fort as possible before departing the area. The area saw no fighting for the remainder of the war.\nThe remains of Fort Hindman continued to stand alongside the Arkansas River for several years, but the entire structure had been taken by changes in the course of the stream by 1880. Some of the trenches still exist, but little else remains of the fort. The site is managed by Arkansas Post National Memorial.\nFor additional information:\nArkansas Post National Memorial. http://www.nps.gov/arpo/index.htm (accessed November 24, 2020).\nBearss, Edwin. “The Battle of the Post of Arkansas.” Arkansas Historical Quarterly 18 (Autumn 1959): 237–279.\nChrist, Mark K. Civil War Arkansas, 1863: The Battle for a State. Norman: University of Oklahoma Press, 2010.\nColeman, Roger. The Arkansas Post Story: Arkansas Post National Memorial. Professional Papers No. 12. Santa Fe, NM: Southwest Cultural Resources Center, 1987.\nThe War of the Rebellion: A Compilation of the Official Records of the Union and Confederate Armies. Series 1, Vol. 17, Part I. Washington DC: Government Printing Office, 1889.\nHenderson State University\nLast Updated: 11/24/2020"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:3c8f1540-e8f5-4584-85e2-22cbebe454e1>"],"error":null}
{"question":"What are the pigment characteristics of Ultramarine blue, and how does equilibrium affect its behavior in chemical reactions?","answer":"Ultramarine is a warm blue pigment with moderate to high tinting strength and beautiful transparency. It's a synthetic pigment that dries slowly in oil and produces clean but granular washes in watercolor. It mixes well with Alizarin colors to create purples and violets, though it can dull when mixed with white in acrylic form. In chemical reactions, Ultramarine follows equilibrium principles where its behavior is governed by Le Chatelier's Principle - changes in temperature, pressure, volume, and concentration can affect its equilibrium position, while catalysts do not affect the equilibrium but only the reaction rate. The pigment may discolor if exposed to acid due to its sulfuric content.","context":["Utrecht Artists' Oil Paint - Indigo, 37 ml tube\nSave For Later\n- My Wish Lists\n- My BlickU List(s)\n- Artists' Oil\n- 37 ml (1.25 oz)\nThis color contains the following pigments:\ncharred animal bone\ncarbon + calcium phosphate\nC + Ca3(PO4)2 or C x CaPO4\nIvory Black is a cool, semi-transparent blue-black with a slight brownish undertone and average tinting strength. It mixes well with any color, and creates a range of dull greens when mixed with yellow. It has good properties for use in oil, can be slow to dry in oil form, and should never be used in underpainting or frescoing. Ivory Black is denser than Lamp Black.\nIvory Black is very lightfast and has good permanence, though it is considered the least permanent of the major black pigments.\nIvory Black has no significant hazards.\nIvory Black is a carbon based black first named as Elephantium, and described in the 4th century BCE as produced by heating ivory scraps in clay pots to reduce the ivory or bone to charcoal. The deviation in names is because the more expensive varieties of this pigment were made by burning ivory, and the less expensive ones by burning animal bone. In the 19th century, the name Ivory Black was finally permitted to be applied to Carbon Black pigments made from bone. True Ivory Black is rare in modern times due to the protection of ivory, and the synthetic variety produced today was discovered in 1929. Bone Black is produced as an industrial pigment.\ncomplex silicate of sodium and aluminum with sulfur\nNa8-10Al6Si6O24S2-4 or Na6-8Al6Si6O24S2-4\nUltramarine is the standard warm blue, a brilliant blue pigment that has the most purple and least green in its undertone. It has a moderate to high tinting strength and a beautiful transparency. Synthetic Ultramarine is not as vivid a blue as natural Ultramarine. Ultramarine dries slowly in oil and tends to produce clean, though granular, washes in watercolor. French Ultramarine mixes well with Alizarin colors in oil and watercolor form to create a range of purples and violets. It can dull when mixed with white in acrylic form, but mixes well with other colors. The shade varies based on manufacturer. Considered a great color for glazes, it is not suitable for frescoing.\nUltramarine has excellent permanence, although synthetic Ultramarine is not as permanent as natural Ultramarine. It may discolor if exposed to acid because of its sulfuric content.\nUltramarine has no significant hazards.\nThe name for this pigment comes from the Middle Latin ultra, meaning beyond, and mare, meaning sea, because it was imported from Asia to Europe by sea. It is a prominent component of lapis lazuli and was used on Asian temples starting in the 6th century. It was one of the most expensive pigments in 16th century Europe, worth twice its weight in gold, and so was used sparingly and when commissions were larger. Ultramarine is currently imitated by a process invented in France in 1826 by Jean Baptiste Guimet, making blue affordable to artists and extending the range of colors on their palettes.\norganic, vat dyes\ncomplex, insoluble anthraquinone\nIndanthrene Blue is a clear, clean, deep blue organic pigment. It has moderate to high tinting strength and is not as overpowering as Phthalo Blue. Hansa Yellow Deep, Benzimidazolone Orange, and Raw Umber are its best mixing complements.\nIndanthrene Blue is permanent with excellent lightfastness in both masstone and tints.\nIndanthrene Blue varies in its acute toxicity, though toxicity is generally slight.\nIndanthrene Blue is the oldest vat dye, discovered and patented in 1901 by Rene Bohn. It is considered the first anthraquinone vat dye, a group of dyes characterized by excellent lightfastness. The pigment originates from this dye.\nSafety Data Sheet\nUPC Code: 655802100597","2 Characteristics of Equilibrium Closed system – nothing in, nothing outForward and reverse reactions are occurring at the same rate – known as ‘dynamic equilibrium’Macroscopic properties (e.g. Colour, concentration, pH) remain constantChanges in temp, pressure, volume and concentration can change the equilibrium position. Catalysts do not.\n3 Equilibrium Graph - Concentration A BThis reaction starts with A only, with the [A] (concentration of A) at a maximum at Time = 0 s.As the reaction proceeds, A gets used up and [B] steadily increases until equilibrium is reached which can be seen as no change in either concentration.In this graph, [B] > [A] which means the forward reaction is more favoured than the reverse.What would the graph look like if the reverse was favoured?\n4 Equilibrium Graph – Rate of Reaction A BThis reaction starts with A only, so the rate of the forward reaction is at a maximum and slows down as [A] decreasesAs the reaction proceeds and [B] steadily increases, the rate of the reverse reaction increases until equilibrium is reached when the rates of the forward and reverse reactions are equal.\n5 Equilibrium constant - Kc productsreactantsIndices are from the coefficients in the balanced chemical equation\n6 Equilibrium constant - Kc Units for Kc will vary depending upon the reaction. In fact, there may be no units.The value of Kc for a particular reaction is only affected by changes in temperatureIn a homogeneous reaction, all of the states of matter are the sameIn a heterogeneous reaction, there are different states of matter. Solids do not take part in the equilibrium constant\n7 Equilibrium constant - Kc When Kc >> 1 When Kc << 1Equil goes far right (forward rxn almost to completion) Equil goes far left (forward rxn hardly proceeds)What happens when Kc = 1? Kc = 0?\n8 Le Chatelier’s Principle “An equilibrium system that is exposed to a stress will shift the equilibrium position to oppose that stress”Note: a stress can be a change in temperature, pressure, volume or concentration. Catalysts do not affect equilibrium; they simply affect the rate of the forward and reverse reactions.\n9 Le Chatelier’s Principle – Concentration Consider the following reaction:A + B C + DAdding a reactant:This will stress the systemTo relieve the stress, the system can produce more productsEquilibrium shifts rightAdding a product:This will stress the systemTo relieve the stress, the system can produce more reactantsEquilibrium shifts leftRemoving a chemical substance is a stress that is relieved by producing more of that substance, shifting the equilibrium towards that direction. For example, removing a product will shift the equilibrium towards the right, making more products.What happens if you remove some reactant or product?\n10 Le Chatelier’s Principle – Temperature (exothermic) Consider the following exothermic reaction:A + B C + D + heatIncreasing the temperature:This is like adding a productTo relieve the stress, the system can reduce the heat by producing more reactantsEquilibrium shifts leftDecreasing the temperature:This is like removing a productTo relieve the stress, the system can produce more productsEquilibrium shifts right\n11 Le Chatelier’s Principle – Temperature (endothermic) Consider the following endothermic reaction:heat + A + B C + DIncreasing the temperature:This is like adding a reactantTo relieve the stress, the system can reduce the heat by producing more productsEquilibrium shifts rightDecreasing the temperature:This is like removing a reactantTo relieve the stress, the system can produce more reactantsEquilibrium shifts left\n12 Le Chatelier’s Principle – Pressure Consider the following reaction:Assume all species are gases2A + B C + DNotice that there are 3 moles of gas on the left and 2 moles on the rightIncreasing the pressure:This means there is less room for the particlesTo relieve the stress, the system can reduce the pressure by producing less moles of gasEquilibrium shifts rightDecreasing the pressure:This means there is more space for particlesTo relieve the stress, the system can increase the pressure by producing more moles of gasEquilibrium shifts leftIncreasing the volume is the same as decreasing the pressure and decreasing the volume is the same as increasing the pressureWhat happens if you increase or decrease the volume? How does this relate to pressure?\n13 Le Chatelier’s Principle – Catalysts Consider the following reaction:A + B C + DAdding a catalystThis will not affect the equilibrium position or KcCatalysts reduce the activation energyThis speeds up the forward and reverse reactions equallyEquilibrium is reached fasterSource of graph:\n14 Production of Ammonia – The Haber process In 1912, German scientist, Fritz Haber developed a process for manufacturing ammonia from nitrogen and hydrogen.N2 (g) + 3H2 (g) 2NH3 (g) + 92kJSource:Notice that this reaction is reversible which can establish equilibrium. This means that Le Chatelier’s Principle applies to the chemistry of this process.Also, note that the forward reaction is exothermic\n15 Ammonia and Le Chatelier N2 (g) + 3H2 (g) 2NH3 (g) + 92kJUsing your knowledge of Le Chatelier’s Principle, describe what the optimum conditions (in terms of yield and rate) for this reaction will be in relation to the following:TemperaturePressureUse of a catalyst\n16 Optimum ammonia production RateYieldCostSource: Chemistry Contexts 2, 2006\n17 Conditions for Haber Process Pressure - high(250 atm) – to shift equilibrium right and increase rateTemperature – moderate (4500C) – low favours equilibrium, high favours rate. This temperature is a trade-offCatalyst – use of an iron catalyst helps to increase the rate and overcome the relatively low temperature required.Removal of ammonia – shifts the equilibrium towards the products.\n18 Production of Sulfuric Acid – The Contact Process Sulfuric acid is made in 3 steps:Diagram source:Steps to make sulfuric acidSulfur dioxide is madeSulfur trioxide is made from sulfur dioxideSulfuric acid is made from sulfur trioxide\n19 Contact Process Chemistry Step 1:Sulfur is roasted in oxygen to produce sulfur dioxideS(s) + O2(g) SO2(g)Step 2:Sulfur dioxide is reacted with oxygen using a Vanadium catalyst to produce sulfur trioxide in a reversible exothermic reactionSO2(g) + O2(g) SO3(g) + heatStep 3:Sulfur trioxide is converted to sulfuric acid through a series of reactionsSource of animation:\n20 Contact Process conditions Using Le Chatelier's principle, the equilibrium yield of sulfur trioxide (step 2) should increase when: - temperatures are low, since the reaction is exothermic; - pressure is high; - excess reactants are present.However the rate of the reaction is high when: - temperature is high, hence an obvious conflict exists with the equilibrium yield; - the pressure is high; - a catalyst is used.Predict the conditions that would be used in this process. Justify your answer\n21 Contact Process conditions Pressure - low(1-2 atm) – the process already favours the foward reaction. The extra pressure is not cost effectiveTemperature – moderate (4500C) – low favours equilibrium, high favours rate. This temperature is a trade-off same as HaberCatalyst – use of an vanadium catalyst helps to increase the rate and overcome the relatively low temperature required.Excess oxygen– shifts the equilibrium towards the products."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:d36ceb00-31b1-4ebe-8e4a-339b7edaf819>","<urn:uuid:fbf9a4d8-1665-4b2e-9ca6-0b907cc9a975>"],"error":null}
{"question":"I'm growing tomatoes and corn in my backyard. What benefits can mycorrhizal fungi provide to these crops?","answer":"Tomatoes and corn are among the crops that thrive with mycorrhizal fungi. The fungi provide several benefits: they improve growth by acquiring nutrients, reduce nutritional deficiencies by extracting minerals like phosphorus, manganese, copper, and zinc, delay wilting by accessing deep water sources, reduce fertilizer needs, protect against root diseases, increase salt resistance, and enhance plant production resulting in larger fruits and vegetables.","context":["You are busy checking the soil for proper nutrients and watering your plants to ensure they grow exceptionally. But are you aware that there is a symbiotic relationship that is happening in the hidden world of your garden soil?\nMany years ago, higher green plants and the mycorrhizal fungi agreed to work together. Mycorrhizal fungi translate to fungus roots. Found naturally in the soil, it attaches itself to about 85 to 90 percent of plants all over the world.\nA thriving mycorrhizal fungus will become a secondary root system for your plant. Its ability to extend farther into the soil helps your garden plants by extracting mineral elements and water from the soil. On the other hand, your garden plants provide energy to the mycorrhizal fungi by releasing sugars and carbohydrates after photosynthesis. But why is it important to have this fungus in your garden?\n1. Growth Improvement\nNo gardener wants to spend their time on plants that do not grow. Luckily, the mycorrhizal fungi acquire nutrients that are efficient in helping garden plants maintain their optimal growth. The fungi help the growth of both the roots and the green plant above the soil, ensuring that no compromise is made as the plant grows longer.\n2. Reduces Nutritional Deficiencies\nThe Mycorrhizal fungi work by mining out the growing medium. The roots of the fungi efficiently extract and bring nutrients to your garden plants.\nThis especially happens in regions where the roots of the plant are not present. Most of the nutrients that are transported by the fungi include phosphorus, manganese, copper, and zinc. By providing these nutrients the fungi prevent your garden plants from experiencing any nutrient deficiencies and their symptoms from becoming visible.\n3. Wilting is Delayed\nThe mycorrhizal fungi can penetrate the depths of the soil where other plant roots cannot reach. Thus, they acquire water from the growing regions that your plants cannot access. More water means that your plants will not experience water stress and any possible wilting will be delayed.\n4. Reduces Fertilizer Use\nThe process of mining out the growing medium that the fungi are involved in is usually for fertilizer elements. Thus, if your soil has thriving mycorrhizal fungi you can reduce the amount of fertilizer that you apply in your garden.\n5. Reduces Attacks on Root Disease\nBy reducing wilting, the effects of stress that occur on plant roots will be delayed. Thus, any thriving mycorrhizal fungi will make the roots of your plant less liable to attacks by root rot pathogens. This is because the presence of the fungi on plant roots competes with these pathogens by consuming root exudates like carbohydrates. Besides, the fungi thicken the cell walls of the root making it difficult for any pathogen to penetrate.\n6. Increased Resistance to Toxicity from Salt\nIf you live in a region that has high amounts of salt, you can still nurture your garden to fruition. The mycorrhizal fungi will protect your plants from toxicities that arise from micronutrients and excessive salt presence.\n7. Increased Resistance to Transplant Shock\nUnlike your garden plant, the roots of the fungi establish quickly whenever they are in new soil environments. Therefore, your plants will have an easier time with transplant shock in the presence of the fungi. The ease of extraction of water and nutrients helps the fungi to provide the plant with buffer abilities until the plant has adjusted to the new soil environment\n8. Increased Plant Production\nBy now it is obvious that the mycorrhizal fungi help garden plants to grow at optimum rates. With the reduced effects of stress, the plant edibles gain more resources and the increased ability to produce larger and more fruits and vegetables.\nYour garden will have larger plants with the presence of thriving mycorrhizal fungi even if you are in an area that has low fertility and poor quality soil.\nEven with all the mycorrhizal fungi excitement, you must remember that many plants in your garden naturally possess the fungi. However, you can still add the fungi to your soil especially if you are transplanting your plants or if your garden has sterile soil.\nAlso, remember that not all plants enjoy the mycorrhizal fungi benefits. For instance, some leafy greens and beets do not fare well with mycorrhizal fungi. On the other hand, shrubs, rose bushes, trees, and crops such as corn, pumpkins and tomatoes thrive in mycorrhizal fungi environments.\nLastly, avoid killing your mycorrhizal fungi. There is a high possibility that your garden already possesses the fungi. Over-tilling, using excess fertilizer, overwatering, and pest barriers can significantly reduce the mycorrhizal fungi content in your garden soil."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:d1a0afee-2dcf-47fc-ae19-09f946fe713e>"],"error":null}
{"question":"What educational requirements exist for public health professionals, and what mobile technology security measures are needed in public healthcare settings?","answer":"Public health professionals must complete accredited education programs that include courses in environmental health, public health management, epidemiology, biostatistics, and social/behavioral sciences. Their responsibilities include health inspections, program administration, public education, and environmental monitoring. Regarding mobile technology security in healthcare settings, several measures are essential: physical security of devices, network security through encryption and firewalls, application security with proper validation and updates, and user security through three-step authentication processes. Healthcare organizations must also protect against data breaches, which have increased to 24.5 percent globally, by implementing secure Wi-Fi networks, encryption protocols, and compliance with HIPAA security requirements.","context":["An education in health care is available from a number of accredited schools and colleges that provide students with the option of obtaining a certificate or degree. With an accredited degree in health care you can train to become a health information technician, health educator, physician assistant, health sciences professional, medical professional, public health worker, or other related position.\nCourses offered depend on the level of degree desired and the career you would like to pursue. With an associates, bachelors, masters, or doctorates degree in the field of health care you can study to work in a variety of areas. Hospitals, clinics, medical centers, schools, and state or other health care facility.\nMORE ABOUT ACCREDITED HEALTHCARE COURSES and DEGREE PROGRAMS\nAccredited schools and colleges offer training for students looking to obtain a degree in cardiovascular technology. Cardiovascular technology students have the option to choose from several specific areas of study including echocardiography, invasive cardiology, and vascular technology. Educational facilities allow students to study courses in chemistry, anatomy and physiology, cardiovascular physiology, cardiovascular rehabilitation\n, echocardiography, and cardiovascular pharmacology. With an associates, bachelors, or masters degree you will be prepared for a career working in a hospital, physicians' office, laboratory, medical center, or educational facility.\nHealth Education Certificate\nAn education in health education can train you for an associates, bachelors, masters, or doctorates degree. Degree programs are available in fitness, drug education, nutrition\n, human health, science, computer technology, and more. Professionals who obtain a certificate in health education will be prepared to work in a variety of places depending on the specific area they choose. Coursework may include the study of heart diseases, obstetrics, immunizations, STD's, pregnancy, and other related courses. The National Commission for Health Education Credentialing, Inc is an agency that provides optional certification to those with a degree in health education.\nHealth Information Technician\nWith a degree in health information you can become a registered health information technician (RHIT). Health information technicians are responsible for collecting and maintaining complete and accurate medical records for patients. With an education from an accredited school or college you can study health care administration\n, procedures regarding medical reports, x-rays, medical histories, billing information, medical charts, consent forms, and more.\nWith an accredited education in health sciences you can obtain an Associate of Science degree, (AS) Bachelor of Science degree (BS), and many others, including certificates. With a degree you will be trained to work in a physician's office, hospital, nursing facility, health insurance company, clinic, and many other agencies. Students may learn subjects like nutrition, consumer health, drugs and alcohol, environmental health, psychosocial health, and chronic and communicable diseases. Health sciences professionals find employment as addiction counselors, anesthesiologists, dietitians, home health aides\n, medical assistants, and more.\nBy attending an accredited education program you can obtain an associates, bachelors, masters, or doctorates degree in the field of health services. With a graduate or undergraduate degree in health services you can become a health services manager, nurse, health care administrator, and much more. There are a number of courses like biology\n, anatomy, sociology, economics, and public health which can provide you with the necessary skills to find employment in health services administration, mental health services, nursing, and more.\nWith an education as a physician assistant you will be able to carry out tasks like splinting, suturing, casting, and other first aid duties, as well as completing related paperwork and ordering tests. Degree programs are offered from accredited schools and colleges and will teach students the knowledge to take patient histories, collect samples, initiate diagnosis and treatment, prepare medical instruments, and more. Course training can include subjects like pathology, biochemistry, anatomy, physiology, and pharmacology. With a bachelors or masters degree you can find employment in hospitals, clinics, schools, prisons, home health care agencies, private offices, and more.\nWith a degree from an accredited school or college you can choose to pursue a career in occupational medicine, public health practice, aerospace medicine, environmental medicine, international medicine, clinical medicine, academic medicine, policy development, consulting, or research. Students who obtain a degree in preventive medicine can pursue employment in academic institutions, voluntary health agencies, health departments, international or global institutions, government agencies, and the military. Courses of study can include epidemiology, biostatistics, administration\n, management, clinical preventive medicine, occupational health, environmental health, and more. The American Board of Medical Specialties (ABMS) provides certification to those who have completed their medical residency and one year internship.\nPublic health professionals are responsible for a number of jobs, including health inspections, administration of public health programs, public education, monitoring the environment, and more. With an accredited education you can learn to find foster care for children, houses for homeless, care for addicts, and food stamps, employment and Medicaid for those who need it. Course may vary depending on the degree, but may consist of environmental health, public health management and policy, epidemiology, biostatistics, public health practice, and social and behavioral sciences\n. With a degree in public health you can prepare for a career in nursing, health education, environmental health, health services, and much more.\nSurgical assistants are trained to organize and prepare surgical areas, evaluate patients, and assist the surgeon during procedures. With an accredited education you can study microbiology, pathology, pharmacology\n, bioscience, and more to prepare you for your career. Students must have a degree and license in order to find employment, and can obtain certification from the National Surgical Assistant Association (NSAA). As a surgical assistant in the health care field you can find work in facilities, like hospitals and veterinary\noffices, working with doctors, nurses, anesthesiologists, and other medical professionals.\nLEARN MORE ABOUT ACCREDITED HEALTH CARE SCHOOLS and COLLEGES\nWhen looking into receiving an education make sure that the school or college you choose carries full accreditation. Agencies like the Accrediting Bureau of Health Education Schools (ABHES)\nand others are authorized to provide full accreditation to educational program that meet certain criteria.\nDISCOVER THE TOP HEALTH CARE SCHOOLS and COLLEGES LOCATED AT PETAP.orgSelect a program from above\nto discover a number of health care educations. Contact several accredited schools and colleges today and request more information regarding the degree of your choice.PETAP Articles Relating to This Profession:Traits and Characteristics of Health Care ProfessionalsPersonality Traits of Health Education ProfessionalsTraits of Effective Health Sciences ProfessionalsHealth Services Administration Specialists and your Qualities","By Mohammad Bajwa, PhD, CPHIMS, CPHI, RHIA, MCSE\nWhile it has only just emerged this century, the use of mobile health technology (mHealth) in the delivery and management of healthcare is gaining traction due to the global implementation of health information technology. This is exemplified mostly by electronic health record (EHR) systems, wherein patients’ health information resides on computers and travels on computer networks. Normally, the information stored on computers is accessed through wires and is limited to places where such infrastructure exists. With the advent of wireless technology, electronic health information can be accessed by anybody, anytime from anywhere and on any mobile device as long as wireless connectivity is available. Despite this advantage, the security of health information stored on and accessed through mobile devices is cause for public concern. Wireless communication is prone to hacking, and mobile devices, given their size and value, increase the chance they will be lost or stolen.\nmHealth is defined as the use of mobile devices (mDevices) in the practice of medicine. These include mobile/cellphones, iPads, tablets, personal and notebook (laptop) computers, personal digital assistants (PDAs), and similar other devices that use wireless technology to access health information networks. Such devices often use radio waves for communication either through central access points (hot spots) or satellites. Of these, mobile phones have emerged as the greatest mobile technology gadgets.\nSince wireless signals travel through the atmosphere, diffuse in all directions, and can pass through most physical barriers, they are liable be intercepted. The intent of this article is to review wireless technology, its use in healthcare, its security issues, and best practices to make safe use of this novel technology for the delivery and management of healthcare.\nTypes of mHealth Security\nBefore discussing the security of mHealth, it is appropriate to enumerate four types of information security requirements: physical, network, application, and user security.\nPhysical security implies the security of computers, network hardware, and storage media against intruders and natural disasters. The most common mitigative security measures for this purpose includes using ID badges and biometric authentication for entry to premises, locking computers with desks, and regular backups for data restoration in case of any human-made or natural disasters.\nNetwork security denotes the protection of data and information residing on computers and in transit over the network/internet through techniques like encryption (encodes data, rendering it unreadable by unauthorized persons) and technologies such as firewalls (controls both inbound and outbound data based on the nature of traffic and source); virtual private networks (makes private data communication channel in the public network such as internet); network intrusion detection (notifies the network administrator of any intrusion by email, text message, alert); and intrusion detection and prevention systems (blocks intruder and reports to network administrator).\nApplication security requires validation of services accessing the information, level of information access and use permissions, software patches and updates, software logs, and input validation techniques (best defense against malware/spyware injection).\nUser security is a three-step process that entails user identification through user ID, authentication through password, biometric or two-factor authentication (verifies user), and authorization through permissions (what the user can access and do).\nData Transmission Media\nData and information on the network travels either on wires (wireline transmission media)—the most common being the copper cables (network and coaxial or TV cables) and fiber optic (FO) cables—or wirelessly through the atmosphere. For the discussion of data security, it is pertinent to examine how data travels on these two types of transmission media and their security vulnerabilities.\nWireline Transmission Media\nThe two types of wireline transmission media are copper cables and fiber optic (FO) cables. Copper cables transmit data through voltages (electrical signals), called electromagnetic (EM) waves, as pulses (5 volts representing 1) or no pulses (representing 0). The data from these cables can be stolen by placing a device close to the cable, which can sense pulses and covert them back to 1s and 0s. To avoid this, some copper cables are shielded (covered with aluminum foil) to stop pulses from being detected externally. One typical example of this type of cable is TV (coaxial) cable that not only shields data being braided but can also transmit it at multiple voltage levels, thus catering to the needs of TV, internet, and digital phone requirements simultaneously. Contrary to the copper cables, FO cables are made of glass or highly transparent plastic and transmit data as light pulses. Its twofold benefits include data transmission at almost the speed of light (several times faster than copper cables) and data security, as light pulses cannot be detected externally. Besides being fast and secure, FO cables transmit data in multiple frequencies, enabling several data channels in one cable (video, internet, phone). Verizon FiOS is one such example.\nWireless signals are also EM waves, but unlike the wireline media, they travel through the air or atmosphere at different frequencies (amplitudes). They are advantageous in areas where wired networks cannot be set up (old and historical buildings) or are required temporarily (training or classroom setup). Because the wireless signals radiate in all directions and can pass most physical barriers, they are very valuable in hospital environments or healthcare organizations with mobile workforces. However, they are the least secure, as they can easily be intercepted by monitoring devices, and data can be interrupted, interjected, modified, stolen, rendered unusable, or even destroyed.\nMobile Technology (mTechnology)\nmTechnology is based on wireless technology that moves with the user. It is a portable two-way communication network and includes internet-enabled devices like smartphones, tablets, notebook computers, and navigation devices. mTechnology touches all aspects of our daily lives from starting the day with the alarm clock in your phone; managing the day through your calendar, reminders, and notes; communicating through emails, texts, social media; and accessing online information like maps, books, magazines, files, photos, and videos. The common four types of mTechnologies are:\n- Cellular or radio networks: Use distributed cell towers that enable mobile devices (cellphones) to switch frequencies automatically and communicate without interruption across large geographic areas.\n- Packet switching technology (5G network): Organizes data into packets for transmission and reassembles packets into information at destination.\n- Wi-Fi: Radio waves that connect devices to internet through localized routers.\n- Bluetooth: Connects devices over short distances using short-wavelength radio waves.\nMobile Health (mHealth)\nmHealth is the use of mTechnology in healthcare. The World Health organization (WHO) defines mHealth as the “use of mobile and wireless technologies to support the achievement of health objectives,”1 while the National Institutes of Health (NIH) describes it as “use of mobile and wireless devices to improve health outcomes, healthcare services, and healthcare research.”2\nThus, mHealth is the practice of medicine and healthcare over mobile devices, like smartphones, tablets, and iPads, such as eVisit (only mode of remote healthcare delivery service during COVID-19 pandemic). Although traditionally used for wellness management, their use for healthcare both by patients and providers has increased exponentially.3\nCurrently, mHealth technologies are being used for patient monitoring, patient-provider communication, telehealth, and e-health. Although the lines between telehealth (also referred to as telemedicine) and e-health are blurred, they are two different technologies. E-health is the electronic communication of information for improving patient’s health, while telehealth uses video, smartphones, or any wireless tools or telecommunications technology (network/internet) for specific healthcare delivery and is named after the type of healthcare service: telenursing, telepharmacy, telerehabilitation, teleradiology, teletrauma care, telepsychiatry, telepathology, and teledermatology. Telehealth requires license and an infrastructure to practice, while e-health has no such requirements. Both became routine during COVID-19 pandemic.\nThe global health data breaches in 2020 increased to 24.5 percent against financial services breaches (7.3 percent), once being the reverse.4 Thus, it is pertinent to elaborate on some mHealth security issues to understand their specific security vulnerabilities.\nMobile devices use air interface to communicate and transmit data through the atmosphere. The wireless medium is broadcast (signals diffuse in all directions), and signals can pass through physical barriers (e.g., walls, roofs, and concrete). They are vulnerable to active (through injecting, deleting, altering the message) or passive (redirecting the information) attacks. The signals traveling through the atmosphere can be intercepted, modified, destroyed, hacked, and rendered unusable through ransomware. Being portable, mobile devices are small and lucrative and can be lost or stolen along with data, have low processing power to handle encryption, and can use public unsecure Wi-Fi. Bring your own device (BYOD) policies poses additional risks of healthcare cybersecurity, as healthcare workers can use their own devices to access organizational network, access patient data, and enter medical orders. BYOD policies are advantageous because of higher productivity, as the providers can access health information from anywhere and anytime; they increase job satisfaction by supporting flexible work arrangements; and they increase effectiveness due to more comfort and speed with the use of people’s own devices. However, they suffer from several disadvantages, like data breaches due to lost or stolen personal devices, lack of firewall and anti-virus software, dearth of encryption power, and increased IT costs for supporting personal devices.\nHIPAA and mHealth\nHIPAA requires security of the protected health information (PHI) through its Security Rule that mandates the security of health information through a three-pronged approach: administrative (policy-based), physical (physical access to facility, workstation, and storage devices), and technical (technological requirements to control access to information).5 mHealth devices have potential HIPAA Security Rule compliance issues such as physical loss or theft of devices, data transmission over unsecured Wi-Fi, unencrypted text messages/emails, inadequate or lack of authentication protocols, and poor adherence to BYOD policies and procedures.\nBeside the potential to have HIPAA security compliance issues, other specific mHealth device issues include interference with implanted medical devices (pacemakers) and their use in monitoring vital signs and supporting life-threatening and critical care situations that use a part of electromagnetic spectrum, like mobile devices. They also suffer from diagnostic interpretations due to small screen size, which may conceal some crucial details in EKG, ultrasound, MRI, and X-rays.\nBest Security Practices for Securing mHealth Devices\nAlthough no technique or technology can provide fool-proof security for the data contained in mHealth devices or accessed through them, some measures can minimize the security risks. These include implementation of strong user authentication (biometric or two-factor authentication), automatic lock after excessive number of incorrect logins, remote wiping when a device is lost or stolen, employing encryption for conversations (emails, text messages), developing an application policy for BOYD devices, regular updating to keep vulnerabilities mitigated, and installing security programs/antivirus, as being networked they are apt to all sorts of malware attacks.\nInternet of Things in Healthcare (IoHT)\nYet another breed of mHealth devices are Internet of Healthcare Things (IoHT) or Internet of Medical Things (IoMT) devices, which is the use of Internet of Things (IoT) in healthcare. IoHT devices are sensor-based interconnected devices used for tracking assets and resources. In hospitals, they can be used for locating patients, medical staff, and visitors (called smart hospitals), and collection and integration of health data (generation, collection, and communication of health data through wearable devices).6 Other benefits of IoHT devices include savings in medical cost, reduction in medical errors, improved patient experience, manageability of medical drugs and adherence, better control over wastage in healthcare sector, and better outcomes of medical treatments.\nSecurity Mitigation of IoHT Devices\nMitigation of security for IoHT devices include both technological and administrative measures. The technological mitigation measures comprises use of blockchain technology to protect data in IoHT devices, enable IoHT devices to use authentication to validate user identity and access privileges, enable IoHT devices to use encryption for all health-related communication, enable integrity on IoHT devices to verify devices to ensure that they are unaltered and uninterrupted, and ensure IoHT devices are patched and updated to avert any vulnerability.\nThe administrative mitigation measures comprise using the principle of least privilege for required actions and types of communication, logging all user activities and events and monitoring them regularly for unusual activities (duplicate device ID or elevation in privileges), reviewing device data regularly to identify unusual trends or patterns, implementing security best practices associated with protecting and securing sensitive data, and conducting formal education, training workshops, certifications, and participation in mobile security conferences.\nFuture of IoHT\nIoHT devices have a great future in healthcare applications, especially through the integration of artificial intelligence with IoHT in the form of ingestible sensors (ePills, which are pill-sized devices to monitor internal physiology and act as diagnostic devices sending medical information and images to outside connected devices); nano-devices to monitor human physiology and deliver drugs to targeted areas like cancer cells; connected lenses that would determine tear glucose and eyes diseases; and blood clot monitoring sensors to avert heart attacks. Their potential healthcare implications include smart hospitals, virtual clinics, microsurgery, vital sign monitoring systems (which can analyze real-time data inputs from critically ill intensive care unit patients), activity trackers for heart patients to collect lifestyle information, and fitness wearables, to name a few.\nIn view of the significance of mTechnology in healthcare, the Journal of Mobile Technology in Medicine is published to disseminate the mHealth research results.\n- World Health Organization. https://www.who.int/goe/publications/goe_mhealth_web.pdf\n- National Institutes of Health. https://grants.nih.gov/grants/guide/pa-files/PAR-14-028.html\n- Phaneuf, A. “How mHealth apps are providing solutions to the healthcare market’s problems” (2019). Retrieved form What Is MHealth? Apps, Examples & Mobile Health Industry Trends (businessinsider.com).\n- US Department of Health & Human Services. “A Retrospective Look at Healthcare Cybersecurity.” 2020. https://www.hhs.gov/sites/default/files/2020-hph-cybersecurty-retrospective-tlpwhite.pdf\n- US Department of Health & Human Services. Summary of the HIPAA Security Rule Guidance Portal. https://www.hhs.gov/guidance/document/summary-hipaa-security-rule-1\n- Bajwa, M. “Opportunities and Challenges for Internet of Things in Healthcare (IoHT).” International Journal of General Medicine and Pharmacy (IJGMP): Vol. 9 (2020): 1-4.\nMohammad Bajwa ([email protected]) is a professor of health informatics at the University of Maryland-Global Campus.\nLeave a commentSyndicated from https://journal.ahima.org/mobile-health-mhealth-security-matters-and-mitigation/"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:17f9cf9e-ebb4-480d-a25e-1a03e6ebc549>","<urn:uuid:556cfcd0-022c-4723-a4e5-a1f05c8b063e>"],"error":null}
{"question":"How do epic poems typically begin?","answer":"Epics typically begin in medias res (in the middle of the action) and include an invocation to the Muse for inspiration in telling the story.","context":["|What is an Epic?\n\"A long narrative poem\nin elevated style presenting characters of high position in adventures forming an organic\nwhole through their relation to a central figure and through their development of episodes\nimportant to the history of a nation or a race.\" (Harmon & Holman. A\nHandbook to Literature. 7th ed.)\nthe Net,\" 1999, oil on canvas by Derek Walcott.\n|Characteristics of an Epic\n- Characters are larger-than-life beings of national importance and\nhistorical or legendary significance.\n- The setting is grand in scope, covering nations, the world, or even\n- Action consists of deeds of great valor and courage.\n- Style is sustained in tone and language.\n- Supernatural forces interest themselves in human action and often\n|Particular Epic Techniques in the\n- An invocation to the Muse for inspiration in the\ntelling of a story.\n- Epics tend to start in medias res. \"In\nthe middle of the action.\"\n- Epic catalogues list warriors, armies, etc.\n- Dialogues tend to be extended, formal speeches.\n- Epic similes are frequent.\nsimile: \"a long, grand comparison which is so vivid that it temporarily displaces\nthe object to which it is compared.\"\nQuestions about Omeros' Epic Nature\n- Look over the general definition and characteristics of an epic.\nWhich aspects of these has Walcott adopted for his epic? Which aspects has he\naltered? What is he trying to suggest by doing this--about the nature of the\nCaribbean? about the nature of heroics? about the nature of race and culture?\n- Look at the invocation on pages 2983-2984. What are the\ncharacteristics of Achille that Walcott praises? What is he choosing to stress\nabout Caribbean life here? Compare this with the invocation to Homer's Iliad\n- Look at the selection from Book Six on pages 2981-2982. This is\nan example of a modern epic catalog. Why would Walcott put together such a list?\n- Read the dialogue between Achille and his ancestor Afolabe.\nWhat do we learn about the nature of Achille's lost heritage?\n1990, oil on canvas by Derek Walcott\n|\"Sing, O goddess, the anger of Achilles\nson of Peleus, that brought countless ills upon the Achaeans. Many a brave soul did it\nsend hurrying down to Hades, and many a hero did it yield a prey to dogs and vultures, for\nso were the counsels of Jove fulfilled from the day on which the son of Atreus, king of\nmen, and great Achilles, first fell out with one another. And which of the gods was it\nthat set them on to quarrel? It was the son of Jove and Leto; for he was angry with the\nking and sent a pestilence upon the host to plague the people, because the son of Atreus\nhad dishonoured Chryses his priest.\"\nHomer. Iliad Book 1. trans. Samuel\nQuestions about Omeros' General Message\n\"Domino Players,\" gouache on\npaper, done by Derek Walcott in 1999.\n- How would you describe Achille's making of a pirogue (dugout canoe)?\nIs it a spiritual experience? Why or why not?\n- What is Walcott suggesting about the Caribbean past and present by\ndescribing the myth of the sunken galleon?\n- What kind of realm does the underworld represent for Achille?\n- What does Achille's dream return to the Nigeria of the Yoruba\nrepresent? What do we learn of him?\n- What connection does Achille see between the Christmas dance and his\n- What connection does Walcott draw between his visiting of the trail\nof tears in the United States and his culture's past?"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:5a3c30ea-ba8c-4674-b440-ffa1c2b6b5a3>"],"error":null}
{"question":"What new measurement techniques are emerging in cosmic radiation and power grid systems?","answer":"In cosmic radiation research, scientists developed a new measurement technique using polarized light from Milky Way dust to precisely calibrate their instruments, allowing them to accurately detect rotation in cosmic microwave background radiation polarization. This represents a significant improvement over previous measurement attempts. Similarly, in power grid systems, advances in software control systems have enabled new measurement and control capabilities for microgrids, allowing them to collectively optimize interconnected distributed energy resources and provide various energy services. However, both fields still face technical challenges - cosmic measurements need to reach higher confidence levels, while microgrids require better standards for advanced protection coordination and device communications.","context":["A twist in the universe’s first light could hint that scientists need to rethink physics.\nA pair of Japanese scientists looked at the polarization or orientation of light from the cosmic microwave background radiation, some of the earliest light emitted after the universe’s birth. They found the polarization of photons, or light particles, might be slightly rotated from their original orientation when the light was first produced. And dark energy or dark matter may have been responsible for that rotation. (Dark energy is a hypothetical force that is flinging the universe apart, while proposed dark matter is a substance that exerts gravitational pull yet does not interact with light.)\nThe rotated signature of the photon polarization tells the scientists that something may have interacted with those photons — specifically something that violates a symmetry physicists call parity. This symmetry or parity says that everything looks and behaves the same way, even in a flipped system — similar to how things look in the mirror. And if the system was following this parity rule, there wouldn’t be this rotation change.\nParity is shown by all subatomic particles and all forces except the weak force. However, the new results suggest that whatever the early light might have interacted with might be violating this parity.\n“Maybe there is some unknown particle, which contributes to dark energy, that perhaps rotates the photon polarization,” said study lead author Yuto Minami, a physicist at the Institute of Particle and Nuclear Studies (IPNS) of High Energy Accelerator Research Organization (KEK) in Japan.\nWhen the cosmic microwave background radiation, or CMB, was first emitted 13.8 billion years ago, it was polarized in the same direction. Looking at how the light’s polarization has rotated over time allows scientists to probe the universe’s history since then, by looking at how the light has changed as it travels across space and time.\nPreviously, scientists have studied the CMB’s polarization and how it’s been rotated over time, but they weren’t able to measure it accurately enough to study parity because of large uncertainty in the calibration of the detectors that measure the photon’s polarization. In the new study, reported Nov. 23 in the journal Physical Review Letters, researchers figured out a way to precisely measure the rotation of the instruments by using another source of polarized light — dust from within the Milky Way. Because this light hasn’t traveled as far, it’s likely not strongly affected by dark energy or dark matter.\nUsing the dusty Milky Way light, the scientists were able to figure out precisely how their instruments were oriented, so they knew the rotation in the light was real, not something caused by their instruments. This allowed them to determine the polarization rotation of CMB light was non-zero, which means that the light has interacted with something that violates parity. It’s possible something in the early universe affected the light, but it’s more likely that it was something along the light’s path as it traveled toward Earth, Minami told Live Science.\nThat something could be dark energy or dark matter, which would mean that the particles that make up these mysterious substances violate parity.\nThe authors reported their findings with 99.2% confidence, meaning there’s an 8 in 1,000 chance of getting similar results by chance. However, this isn’t quite as confident as physicists require for absolute proof. For that, they need five sigma, or 99.99995% confidence, which likely isn’t possible with data from just one experiment. But future and existing experiments might be able to gather more accurate data, which could be calibrated with the new technique to reach a high-enough level of confidence.\n“Our results do not mean a new discovery,” Minami said. “Only that we found a hint of it.”\nOriginally published on Live Science.\n#Twisted #light #beginning #time #reveal #brandnew #physics","TODAY’S STUDY: A Microgrids Primer\nMicrogrids: Expanding Applications, Implementations, and Business Structures\nNadav Enbar, Dean Weng, Ryan Edge, and John Sterling, December 2016 (Smart Electric Power Alliance and Electric Power Research Institute)\nHistorically, microgrids have been employed to provide an additional layer of electricity supply reliability for customers in remote locations with limited access to the grid, or for large institutions managing campus-style energy systems. However, new interest in these systems is now emerging, driven by the changing energy landscape, specifically: nEfforts to modernize the electricity system to more effectively leverage rising penetrations of interconnected distributed energy resources (DERs)\nnDesire to accommodate increased customer choice\nnNeed to provide critical or emergency services, and enable greater grid resiliency in response to more frequent extreme weather events.\nThis report characterizes the latest developments in microgrid deployment, the expanding capabilities of these systems, the business models being used in their deployment, and the obstacles and opportunities that lie ahead.\nBecause the term “microgrid” is often used to refer to a range of distributed energy systems, for this report we use the U.S. Department of Energy (DOE) definition, which is considered the standard for the industry.\nThe DOE defines a microgrid as: “. . . a group of interconnected loads and distributed energy resources (DERs) within clearly defined electrical boundaries that act as a single controllable entity with respect to the grid, and that can connect and disconnect from the grid to enable it to operate in both grid-connected and ‘island’ mode.”\nThe microgrid concept is not new; it is simply a reformulation of local power systems as they were originally designed. Like the traditional, centralized electric grid, microgrids generate, distribute, and regulate the supply of electricity to customers, but do so locally and on a much smaller scale. However, the systems themselves and interest in them have evolved.\nnSuperstorm Sandy and other extreme weather events in recent years have underlined the need for enhanced resiliency and reliability for critical public services—ranging from hospitals to police stations to military bases. Microgrids offer the capability to “island,” or to operate disconnected from the grid, for long periods of time and provide uninterrupted power to mission critical entities or remote rural communities.\nnThe falling cost of solar power and battery storage make these technologies a compelling economic choice for microgrid installations. Additionally, they do not introduce the risk of fuel cost volatility.\nnAdvances in software control systems and the desire to integrate “smart” loads—such as networked thermostats, lighting and occupancy-sensing controls, and building energy management systems—have resulted in improving microgrid control systems. These systems can collectively optimize interconnected DERs to provide a range of energy and grid services.\nMicrogrid Implementation Types and Business Models\nnMicrogrids are customized to their purpose and location—ranging from commercial or industrial, to military, to remote rural systems—thus, the details of their design and construction vary significantly.\nnWhile the different use cases for microgrid installations are multiplying, and individual technologies are not exclusive to any one installation. Most are built to increase reliability and resiliency.\nnUntil recently, a large portion of microgrids have been third-party installations serving a single customer. However, utility-owned microgrids are also being developed, primarily due to statelevel policies and directives, in tandem with technology maturity and expanding applications for these systems. A small number of hybrid, “unbundled” microgrids are also emerging.\nMicrogrid Barriers And Challenges\nAlthough the technology and equipment necessary for creating microgrids are available today, off-the shelf commercial solutions are rare. A number of technical, economic, and regulatory issues must be addressed to unlock the full potential of microgrids. For example:\nnTechnical: Considerable technical challenges exist when toggling a microgrid between gridconnected and islanded modes. For example, during transition to island mode, phase and frequency drift is highly likely, which could cause loads and DERs to trip. Without a finely calibrated synchronization process, grid reconnection could damage generators and loads within the microgrid and in surrounding systems.\nnEconomic and regulatory: Determining standardized methods for valuing microgrids—from either the customer or utility perspective—is difficult due to an intersecting and fluid set of economic and regulatory issues. For example, regulatory and market uncertainties affect the upfront costs and life-cycle economics of microgrids and associated DER technologies.\nA second challenge is that a microgrid’s costs and benefits can be difficult to monetize, or nonmonetizable, thus complicating value stream calculations.\nnStandards:Current technical standards offer guidance for microgrid development, but do not address more nuanced issues germane to system design. For example, further definition is required for protocols governing advanced protection coordination, multilayer-device communications and controls, microgrid-to-grid interactions, and grid resynchronization.\nMicrogrids can be justified across a wide variety of use cases based on a specific set of major drivers.\nBehind these use cases, the ownership and control of the component technologies range along a continuum between the extremes of customer and utility control. Since no two markets or utilities are alike, microgrids will continue to proliferate based on unique served loads, targeted drivers, and deployed technologies."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:40db6cb7-b421-4664-9af2-c3d3f360641d>","<urn:uuid:352824a0-bd81-474e-a97d-4832c6c3a6fb>"],"error":null}
{"question":"As a business owner concerned about climate risks, I'd love to know: what's the difference between how large corporations and small enterprises are adapting to climate change? 🤔","answer":"Large corporations and small/medium enterprises (SMEs) show significant differences in their adaptation capabilities. Large corporations are actively 'climate-proofing' their operations by relocating buildings to low-risk areas, purchasing weather insurance, and reducing water and energy usage. In contrast, SMEs struggle to adapt due to lack of resources - they typically don't have in-house climate experts or funds for outside consultants, and often lack access to affordable financial products like loans and insurance, particularly in developing countries. This makes SMEs less prepared for climate impacts and more likely to suffer from them, as evidenced by cases like Honda's $250 million loss from flooding in Thailand and Munich Re's 38% quarterly profit drop from Australian flood claims.","context":["This is the first installment of WRI’s new blog series, Adaptation and the Private Sector. Each post will explore ways to engage the private sector in helping vulnerable communities adapt to the impacts of climate change.\nAdapting to the impacts of climate change—like heat waves, increased floods, and natural disasters—is an enormous challenge. It’s also one that comes with an enormous price tag. Although it’s difficult to calculate the extent of the costs, the World Bank estimates that developing countries need $70 to $100 billion USD per year through 2050 to meet their current and future climate adaptation needs.\nThe Climate Policy Initiative, however, estimates that in 2011, only $4.4 billion USD in adaptation finance went to developing countries. This leaves a gap of anywhere from $65.6 to $95.6 billion USD per year between what developing countries need and what developed nations are giving.\nSo who can help fill this gap?\nThe private sector may be the answer to this question. Already, proactive private companies are beginning to address climate change in their investments and business planning. With a little work on the part of the public sector, the private sector may be inclined to invest more in adaptation—to reduce their own risks, as well as those of vulnerable populations. Our new five-part blog series, Adaptation and the Private Sector, will outline some of the roles the private sector should play and the ways to support their contributions to climate change adaptation.\nBut first, it’s important to understand the progress that’s already being made and the challenges that exist.\nHow Is the Private Sector Responding to the Impacts of Climate Change?\nWhile corporations and businesses have reacted to international and domestic pressures to curb their CO2 emissions and invest in mitigation, they have been less engaged in building climate-resilient communities. They have little incentive to invest in an area that’s largely seen as a public good. If you look closely, however, you see that they are starting to make some progress.\nFirst, the private sector is engaged in adaptation; their goods and services are just not labeled as such. Businesses and individuals have a desire to protect their assets and livelihoods from climate change and are willing, if finances are available, to spend money on goods and services that provide this protection. Because demand is there, businesses will respond with new products. These products, however, aren’t marketed as “climate-resilient” and don’t come with an “adaptation-friendly” sticker. For example, buildings with a rooftop garden are marketed as green buildings or energy-saving strategies, yet they also provide adaptation benefits in the form of alleviating heat island effect.\nSecond, most private sector action on climate change has gone to “climate-proofing” operations. Companies are relocating buildings to low-risk areas, purchasing weather insurance, and reducing water and energy usage—which are all good practices that protect them against climate hazards. In some instances, such activities may even help vulnerable populations, as is the case when corporations climate-proof their supply chains. For global companies, suppliers can be small-holder farmers, miners, or artisans in developing countries. These suppliers can build their adaptive capacity when corporations make their own supply chains more resilient, such as giving farmers access to drought-resistant seeds. The public sector should look to support and encourage further action in this area.\nChallenges to the Private Sector\nThe private sector also faces a variety of challenges when dealing with climate change impacts. Addressing some of these challenges could go a long way toward creating a better enabling environment for the private sector.\nFirst, businesses need better, more actionable information on climate change and its projected impacts. Levels of long-term uncertainty are difficult to take into account when making short-term investment decisions. For example, a survey of 72 businesses found that most respondents thought climate change information was hard to incorporate into their business plans because of uncertainty about the magnitude, timescale, and precise location of climate impacts. Furthermore, scientific information about the climate system is difficult to decipher for many audiences, which compounds the challenge of making informed decisions on how to best respond. Businesses, therefore, need information from public and academic sources that help them make informed decisions on dealing with climate change impacts.\nSecond, while more and more corporations are investing in making their operations more climate-resilient, few small and medium enterprises (SMEs) are able to do so, due to lack of resources. They are unlikely to have in-house experts on climate change and sustainability, and their funds to bring in outside consultants are limited. SMEs, therefore, are less prepared for climate impacts and more likely to suffer from them. Furthermore, SMEs in developing countries often do not have key types of affordable financial products—such as loans and insurance—available to them. This lack of financial and technical resources makes it difficult for SMEs to invest in adaptation planning.\nThis is by no means an exhaustive list—the challenges to scaling up adaptation finance are numerous and complex. But by understanding these key limiting factors and acknowledging the current actions the privates sector is taking, we can already start to see ways that governments and the international community can support, encourage, and scale-up private sector adaptation actions and investments. Understanding the obstacles can help the public sector engage the private sector in making vulnerable communities more climate-resilient.\nOur ongoing blog series, Adaptation and the Private Sector will explore various ways to support and scale up private sector investments in climate change adaptation. Check back for the next installment, which will look specifically at the role multinational corporations can play in building more climate-resilient societies.\n- LEARN MORE: Check out a related blog post, Is Adaptation Short-Changed? The Imbalance in Climate Finance Commitments","Extreme weather can spark extreme business costs. It matters, therefore, that extreme weather is trending upward—by a factor of three over the past 35 years, according to an analyst with insurer Munich Re. It also matters that businesses can mitigate against these risks and buffer against attendant costs through actions focused on climate resilience.\nA few corporate examples vividly underscore why businesspeople would be wise to carefully assess the potential corporate risks of extreme weather, as well as to identify proactive mitigation actions. The Center for Climate and Energy Solutions has reported on numerous examples, including, in 2011, automaker Honda lost more than US$250 million when unprecedented rains led to serious flooding in Thailand. In 2012, Energy company Dominion Resources was forced to shut down operations within a Connecticut-based nuclear plant because record-setting summer heat significantly elevated ocean temperatures, and cooling the facility with seawater was not feasible. And Munich Re had a 38 percent quarterly profit tumble when it had to pay out US$350 million in claims after flooding in Australia in 2010-2011.\nClearly, avoiding these extreme-weather financial losses are in all corporate decision-makers’ best interest, particularly as these events are occurring with greater frequency and intensity due to climate impacts. As corporate decision-makers begin to assess extreme-weather risks and opportunities, as well as consider investments in building resilience, they should keep the following in mind.\n- Severe weather is on an upward trend in frequency and intensity, as National Oceanic and Atmospheric Administration data on U.S. billion-dollar disaster events has shown.\n- Businesses can mitigate the impacts of extreme-weather-related risks by starting with assessments of specific risks and suites of appropriate responses—as outlined in a BSR report on Thailand, among a growing body of work on climate resilience. Both the upside and downside is significant enough now to warrant insurance companies, such as Zurich Insurance, to offer “resources and knowledge to help its customers build greater climate resilience.”\n- The effectiveness of the corporate response will be contingent upon accurate assessments. Since extreme weather comes in many forms—floods, droughts, hurricanes, intense rainfall and snowfall, and high and low temperatures, among others—corporate responses must take into account how specific weather events will affect supply chains or operations (see recommended analytical approaches in a BSR blog on resilience).\nBSR has developed an approach to help companies properly diagnose climate risk throughout their operations, supply chains, and surrounding communities. The approach, which is part of BSR’s Resilience and Adaptation Initiative (READI), also helps companies respond through actionable methods to enhance corporate and societal resilience. READI builds collaboration among companies and leading adaptation experts on the issue of corporate risk and resilience.\nEarly corporate adopters—who are future-proofing supply chains and operations in light of extreme weather—are finding that actions exist to address vulnerabilities to extreme-weather-related business disruptions. It is becoming clear that businesses can afford to invest in resilience, when these investments consider risks and potential losses. In fact, key business stakeholders are asserting that the risk is big enough to warrant action today and are willing to advise on pathways forward to extreme weather resilience, as the Zurich Insurance example shows.\nThe pathway forward is clear and leads toward less risk and more reward in the form of avoiding or mitigating operational disruptions. It is now time for companies to roll up their sleeves and convene to assess and address extreme weather risk, resilience, and reward."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:c9c1626e-4104-4e95-a65a-65b9a567baa9>","<urn:uuid:81b4a2fe-a934-43f6-acda-a835d6efdb29>"],"error":null}
{"question":"How do steroid treatments compare between pemphigus foliaceus and rheumatoid arthritis?","answer":"In pemphigus foliaceus, topical treatment with corticosteroids is usually sufficient for mild cases. For rheumatoid arthritis, corticosteroids like prednisone and methylprednisolone are used to reduce inflammation and pain and slow joint damage, but long-term use can lead to serious side effects including easy bruising, thinning of bones, cataracts, weight gain, a round face, and diabetes. Doctors typically prescribe steroids for rheumatoid arthritis to relieve acute symptoms with the goal of gradually tapering off the medication.","context":["What is pemphigus foliaceus?\nThe pemphigus family are rare autoimmune blistering diseases affecting the skin and mucous membranes. Pemphigus foliaceus is a rare relatively benign form of pemphigus. In New Zealand pemphigus foliaceus is more often encountered than its more serious relative pemphigus vulgaris, although worldwide pemphigus vulgaris is more common. Pemphigus foliaceus is characterised by blistering lesions on otherwise healthy-looking skin. Blisters tend to form when the skin is rubbed (Nikolsky sign). There are currently six subtypes:\n- Pemphigus erythematosus\n- Pemphigus herpetiformis\n- Endemic pemphigus foliaceus\n- IgA pemphigus foliaceus\n- Paraneoplastic pemphigus foliaceus\n- Drug-induced pemphigus foliaceus.\nWho gets pemphigus foliaceus?\nPemphigus foliaceus affects people of all races, age and sex. It appears most commonly between the ages of 50-60 years.\nWhat causes pemphigus foliaceus?\nPemphigus foliaceus is an autoimmune disorder, which basically means that an individual's immune systems starts reacting against his or her tissue.\nThe building block cells of the epidermis are called keratinocytes. These cells are cemented together at special sticky spots called desmosomes. In pemphigus foliaceus, autoantibodies bind to a protein called desmoglein-1, which is found in desmosomes in the keratinocytes near the top of the epidermis. The result is the surface keratinocytes separate from each other, and are replaced by fluid: the blister. Because the blister is very close to the surface of the skin, the blisters rupture easily. In most cases, the autoantibodies are immunoglobulin type G (IgG) but in IgA pemphigus, the autoantibodies are type A (IgA).\nPemphigus foliaceus is sometimes provoked by sun exposure.\nEndemic pemphigus foliaceus occurs in South America, where it is commonly known as Fogo selvagem. It appears to be set off by a virus transmitted by an insect bite.\nPenicillamine, nifedipine, captopril, enalapril or nonsteroidal anti-inflammatory drugs most often provoke drug-induced pemphigus foliaceus. If the drug is stopped, there is a 50% chance the pemphigus foliaceus will clear up.\nWhat are the clinical features of pemphigus foliaceus?\nPemphigus foliaceus produces superficial blisters confined to the skin, without involvement of mucous membranes. This is in contrast to pemphigus vulgaris where there may be extensive mucous membrane involvement (mouth, eyelids etc.).\nThe patient with pemphigus foliaceus is usually otherwise in good health. Small fluid-filled blisters first form on the trunk. Being superficial within the upper epidermis, they rupture very easily, and only erosions may be seen. On the face, scalp and upper trunk, the lesions are often scaly and crusty on a red and inflamed base. A burning sensation or localised pain may be felt.\nPemphigus erythematosus (Senear-Usher syndrome)\nPemphigus erythematosus (or Senear-Usher syndrome) is a variant of pemphigus foliaceus which shares clinical, histopathological, and serological features with systemic lupus erythematosus (SLE); they rarely coexist.\nIt accounts for 10% cases of all pemphigus disease. Clinically and immunologically, Senear-Usher syndrome shares features of SLE such as the presence of erythematous, scaly, and crusted plaques over the malar region and nose. Additionally, the disease may be worsened by exposure to sunlight as seen in SLE. Immunofluorescence demonstrates the presence of IgG and C3 at the dermo-epidermal junction (a feature distinguishing it from pemphigus foliaceus) in addition to intraepidermal IgG. Presence of anti-desmoglein (anti-dsg) 1 antibodies are seen and in 30-80% of patients; elevated ANA is also present.\nThe principles of management are the same as other forms of pemphigus, however it can be managed with lower steroid doses.\nHow is pemphigus foliaceus diagnosed?\nDiagnosis generally requires a skin biopsy, which shows typical features of rounded-up separated keratinocytes (called acantholytic cells) within the blisters in the upper layers of the epidermis.\nPemphigus foliaceus is confirmed by direct immunofluorescence staining of the skin sections to reveal antibodies.\nIn some cases, circulating antibodies can be detected by a blood test (indirect immunofluorescence test).\nWhat is the treatment of pemphigus foliaceus?\nThe primary aim of treatment is to prevent new areas from developing bacterial infections and promote healing of affected areas.\nTopical treatment with corticosteroids is usually all that is necessary for mild pemphigus foliaceus. For more severe cases treatment is similar to that for pemphigus vulgaris. Oral antibiotics may be prescribed to treat secondary bacterial infection.\nWhat is the outcome of pemphigus foliaceus?\nSpontaneous remission may occur in some patients while in others, the problem may persist for several years.","Rheumatoid arthritis is an inflammatory form of arthritis that causes joint pain and damage. Rheumatoid arthritis attacks the lining of your joints (synovium) causing swelling that can result in aching and throbbing and eventually deformity. Rheumatoid arthritis is two to three times more common in women than in men and generally occurs between the ages of 40 and 60. But rheumatoid arthritis can also affect young children and older adults.\nSigns and symptoms of rheumatoid arthritis may include:\n- Joint pain\n- Joint swelling\n- Joints that are tender to the touch\n- Red and puffy hands\n- Firm bumps of tissue under the skin on your arms (rheumatoid nodules)\n- Morning stiffness that lasts at least 30 minutes\n- Weight loss\nRheumatoid arthritis usually causes problems in several joints at the same time. Early rheumatoid arthritis tends to affect your smaller joints first — the joints in your wrists, hands, ankles and feet. As the disease progresses, your shoulders, elbows, knees, hips, jaw and neck can also become involved. Rheumatoid arthritis signs and symptoms may vary in severity and may even come and go. Periods of increased disease activity — called flare-ups or flares — alternate with periods of relative remission, during which the swelling, pain, difficulty sleeping, and weakness fade or disappear.\nRheumatoid arthritis occurs when white blood cells — whose usual job is to attack unwanted invaders, such as bacteria and viruses — move from your bloodstream into the membranes that surround your joints (synovium). The blood cells appear to play a role in causing the synovium to become inflamed. The inflammation causes the release of proteins that, over months or years, cause the synovium to thicken. The proteins can also damage the cartilage, bone, tendons and ligaments near your joint. Gradually, the joint loses its shape and alignment. Eventually, it may be destroyed. It’s likely that rheumatoid arthritis occurs as a result of a complex combination of factors, including your genes, your lifestyle choices, such as smoking, and things in your environment, such as viruses.\nFactors that may increase your risk of rheumatoid arthritis include:\nSex. Women are more likely to develop rheumatoid arthritis than men are.\nAge. Rheumatoid arthritis occurs most commonly between the ages of 40 and 60. However, it can also occur in older adults and in children (juvenile rheumatoid arthritis).\nFamily history. If a member of your family has rheumatoid arthritis, you may have an increased risk of the disease. Doctors don’t believe you can directly inherit rheumatoid arthritis. Instead, it’s believed that you can inherit a predisposition to rheumatoid arthritis.\nSmoking. Smoking cigarettes increases your risk of rheumatoid arthritis. Quitting can reduce your risk.\nDiagnosing rheumatoid arthritis usually begins with a physical exam. Your doctor will ask you about your signs and symptoms and examine your affected joints.\nBlood tests. People with rheumatoid arthritis tend to have an elevated erythrocyte sedimentation rate (ESR, or sed rate), which indicates the presence of an inflammatory process in the body. Other common blood tests look for antibodies called rheumatoid factor and anti-cyclic citrullinated peptide (anti-CCP) antibodies in the blood. While commonly found in the blood of people with rheumatoid arthritis, rheumatoid factor and anti-CCP antibodies aren’t present in all cases. In early rheumatoid arthritis, the presence of rheumatoid factor and anti-CCP antibodies in the blood may be associated with an increased risk of joint damage. Rheumatoid factor and anti-CCP antibodies can be present in people who have chronic infections, such as active tuberculosis, and other autoimmune rheumatic diseases, such as lupus and Sjogren’s syndrome.\nJoint fluid analysis. Your doctor may draw fluid from your joint using a needle. The fluid can be tested to help rule out other diseases and conditions.\nX-rays. Your doctor may recommend X-rays to help track the progression of rheumatoid arthritis in your joint over time.\nTreatment for rheumatoid arthritis aims to reduce inflammation in your joints in order to relieve pain and prevent or slow joint damage. Early and aggressive rheumatoid arthritis treatments may slow joint damage and help reduce the risk of disability. Treatment typically involves medications, though surgery may be necessary in cases of severe joint damage.\nRheumatoid arthritis medications can relieve pain and slow or halt the progression of joint damage. Medications used to treat rheumatoid arthritis include:\nNSAIDs. Nonsteroidal anti-inflammatory drugs (NSAIDs) can relieve pain and reduce inflammation. Over-the-counter NSAIDs include ibuprofen (Advil, Motrin, others) and naproxen sodium (Aleve). Stronger versions of these NSAIDs and others are available by prescription. NSAIDs have risks of side effects that increase when used at high dosages for long-term treatment. Side effects may include ringing in your ears, gastric ulcers, heart problems, stomach bleeding, and liver and kidney damage. Consuming alcohol or taking corticosteroids while using NSAIDs also increases your risk of gastrointestinal bleeding.\nSteroids. Corticosteroid medications, such as prednisone and methylprednisolone (Medrol), reduce inflammation and pain, and slow joint damage. In the short term, corticosteroids can make you feel dramatically better. But when used for many months or years, they may become less effective and cause serious side effects. Side effects may include easy bruising, thinning of bones, cataracts, weight gain, a round face and diabetes. Doctors often prescribe a corticosteroid to relieve acute symptoms, with the goal of gradually tapering off the medication.\nDisease-modifying antirheumatic drugs (DMARDs). Doctors prescribe DMARDs to limit the amount of joint damage that occurs in rheumatoid arthritis. These drugs are typically used in the early stages of rheumatoid arthritis in an effort to slow the disease and save the joints and other tissues from permanent damage. You may need to take DMARDs for weeks or months before you notice any benefit. For that reason, they may be combined with other medications that give you more immediate relief from signs and symptoms, such as NSAIDs or corticosteroids. Common DMARDs include hydroxychloroquine (Plaquenil), the gold compound auranofin (Ridaura), sulfasalazine (Azulfidine), minocycline (Dynacin, Minocin) and methotrexate (Rheumatrex).\nImmunosuppressants. These medications act to tame your immune system, which is out of control in rheumatoid arthritis. In addition, some of these drugs attack and eliminate cells that are associated with the disease. Some of the commonly used immunosuppressants include leflunomide (Arava), azathioprine (Imuran), cyclosporine (Neoral, Sandimmune) and cyclophosphamide (Cytoxan). These medications can have potentially serious side effects such as increased susceptibility to infection.\nAnakinra (Kineret). Anakinra is similar to a naturally occurring chemical in your body — interleukin-1 receptor antagonist (IL-1Ra) — that stops a certain chemical signal from causing inflammation. You might consider anakinra if you have moderate to severe rheumatoid arthritis and haven’t been helped by conventional DMARD therapy. Anakinra is given as a daily self-administered injection under the skin, and is sometimes combined with methotrexate. Potential side effects include injection site reactions, decreased white blood cell counts, headache and an increase in upper respiratory infections. There may be a slightly higher rate of respiratory infections in people who have asthma or chronic obstructive pulmonary disease. If you have an active infection, don’t use anakinra.\nAbatacept (Orencia). Abatacept reduces the inflammation and joint damage caused by rheumatoid arthritis by inactivating T cells — a type of white blood cell. People who haven’t been helped by TNF-alpha inhibitors might consider abatacept, which is administered monthly through a vein in your arm (intravenously). Side effects may include headache, nausea and mild infections, such as upper respiratory tract infections. Serious infections, such as pneumonia, can occur.\nRituximab (Rituxan). Rituximab reduces the number of B cells in your body. B cells are involved in inflammation. People who haven’t found relief using TNF inhibitors might consider using rituximab, which is usually given along with methotrexate. Rituximab is administered as an infusion into a vein in your arm. Side effects include flu-like signs and symptoms, such as fever, chills and nausea. Some people experience extreme reactions to the infusion, such as difficulty breathing and heart problems.\nWhat medications you can consider will depend on the severity of your rheumatoid arthritis, the length of time that you’ve been experiencing signs and symptoms, results from blood tests and X-rays, your overall physical function, and other medical problems you have. Doctors use these factors to determine the duration of your disease, its severity and your prognosis, which help to develop a treatment plan.\nIf medications fail to prevent or slow joint damage, you and your doctor may consider surgery to repair damaged joints. Surgery may help restore your ability to use your joint. It can also reduce pain and correct deformities. Rheumatoid arthritis surgery may involve one or more of the following procedures:\nTotal joint replacement (arthroplasty). During joint replacement surgery, your surgeon removes the damaged parts of your joint and inserts a metal and plastic prosthesis.\nTendon repair. Inflammation and joint damage may cause tendons around your joint to loosen or tighten. Your surgeon may be able to repair the tendons around your joint.\nRemoval of the joint lining (synovectomy). If the lining around your joint (synovium) is inflamed and causing pain, your surgeon may recommend removing the lining of the joint.\nSurgery carries a risk of bleeding, infection and pain. Discuss the benefits and risks with your doctor.\nYou can take steps to care for your body if you have rheumatoid arthritis. Consider trying to:\nExercise regularly. Gentle exercise can help strengthen the muscles around your joints, and it can help fight fatigue you might feel. Check with your doctor before you start exercising. If you’re just getting started, begin by taking a walk. Try swimming or gentle water aerobics. Public pools and health clubs in your area may offer classes. Avoid exercising tender, injured or severely inflamed joints. If you feel new joint pain, stop. New pain that lasts more than two hours after you exercise probably means you’ve overdone it. If pain persists for more than a few days, call your doctor.\nEat a healthy diet. A healthy diet emphasizing fruit, vegetables and whole grains can help you control your weight and maintain your overall health. However, there’s no special diet that can be used to treat rheumatoid arthritis. It hasn’t been proved that eating any particular food will make your joint pain or inflammation better or worse.\nProtect your joints. Find different ways to approach everyday tasks in order to take stress off your painful joints. For instance, if your fingers are sore, pick up an object using your forearms. Lean into a glass door to force it open, rather than pushing on the door with sore arms.\nUse assistive devices. Assistive devices can make it easier to go about your day without stressing your painful joints. For instance, using specially designed gripping and grabbing tools may make it easier to work in the kitchen if you have pain in your fingers. Try a cane to help you get around. Your doctor or occupational therapist may have ideas about what sorts of assistive devices may be helpful to you. Catalogs and medical supply stores may also be places to look for ideas.\nApply heat. Heat can help ease your pain and relax tense, painful muscles. One of the easiest and most effective ways to apply heat is to take a hot shower or bath for 15 minutes. Other options include using a hot pack or an electric heat pad set on its lowest setting. If your skin has poor sensation or if you have poor circulation, don’t use heat treatments.\nApply cold. Cold may dull the sensation of pain. Cold also has a numbing effect and decreases muscle spasms. Don’t use cold treatments if you have poor circulation or numbness. Techniques may include using cold packs, soaking the affected joints in cold water and ice massage.\nRelax. Find ways to cope with pain by reducing stress in your life. Techniques such as hypnosis, guided imagery, deep breathing and muscle relaxation can all be used to control pain.\nIf you’re interested in trying complementary and alternative medicine therapies for your rheumatoid arthritis pain, discuss these treatments with your doctor first. Some common complementary and alternative treatments that have shown promise for rheumatoid arthritis include:\nPlant oils that contain gamma-linolenic acid (GLA). GLA is a type of omega-6 fatty acid that comes from plant oils, such as evening primrose, borage and black currant. Some studies indicate GLA may help with rheumatoid arthritis pain and morning stiffness, though more research is needed. Side effects may include nausea, diarrhea and gas. Some plant oils can cause liver damage or interfere with medications, so check with your doctor first.\nFish oil that contains eicosapentaenoic acid (EPA) or docosahexaenoic acid (DHA). EPA and DHA are omegea-3 fatty acids commonly found in fish oil. Some preliminary studies have found that fish oil may reduce rheumatoid arthritis pain and stiffness, but more study is needed. Side effects can include nausea, belching and a fishy taste in the mouth. More serious side effects can include bleeding and mercury poisoning. Fish oil can interfere with medications, so check with your doctor first.\nTai chi. This movement therapy involves gentle exercises and stretches combined with deep breathing. Many people use tai chi to relieve stress in their lives. Small studies have found tai chi may reduce rheumatoid arthritis pain, though more study is needed. Talk to your doctor if you’d like to give tai chi a try. When led by a knowledgeable instructor, tai chi is safe. But don’t do any moves that cause pain.\nBe careful when considering alternative therapies. Many are expensive and some may be harmful. Before taking any complementary medications or dietary supplements, talk with your doctor to learn about potential dangers, particularly if you’re taking other medications."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:bd2a61d4-9e04-48fc-9663-80feed76f98a>","<urn:uuid:0e9309c1-f7e7-44ee-b422-4bf4657365ee>"],"error":null}
{"question":"How are different types of metals recycled from waste?","answer":"Metals are sorted using a Material Sorting Facility (MSF) that separates steel from aluminum using a magnet. The separated metals are then baled into large cubes and sold to a merchant, who sells them to a foundry. At the foundry, these metal cubes are melted down into metal ingots, which manufacturers can use to produce new items such as cans, cars, or aeroplanes.","context":["After we have collected your recycling from the kerbside, that is just the start of the journey. We process some of your recycling at Freighter House, our recycling and waste depot, using our Material Sorting Facility (MSF). This uses the latest technology to separate different materials, such as the many different varieties of plastic packaging.\nWe then pass it on to an accredited merchant or a reprocessing factory where the material will be reused, or recycled into a new product.\nWe use our MSF to sort the steel from the aluminium using a magnet.\nWe then bale up the separate metals into large cubes and sell them to a merchant, who in turn sells the cubes to a foundry.\nThe foundry melts down the cubes into metal ingots so that manufacturers can turn them into more cans, cars or even aeroplanes.\nWe send food waste to an anaerobic digestion plant. Anaerobic Digestion uses microorganisms to break down the food waste in an enclosed system that doesn’t contain oxygen.\nThis process produces a bio-gas, which is used to generate electricity. It also creates a nutrient-rich soil that farmers can use for their fields to help grow more food.\nWe send garden waste on to a local industrial composting facility. Due to the enormous size of the operation, the grass and leaves decompose very quickly. They are then recycled into nutrient-rich compost, which is used on local farm land.\nWe send glass to a reprocessing factory. They load mixed glass into a machine that uses magnets to attract any metals mixed in and suction to remove any plastics. They then send the remaining glass through a picking station, where workers manually remove any further contaminants.\nThe clean glass is then crushed and sorted it into colours to produce ‘cullet’. Glass manufacturers can use cullet to make more glass items, such as bottles and jars. The construction industry can also use it as an aggregate in concrete or as loft insulation.\nPaper and cardboard\nWe collect paper and cardboard separately from houses because the materials are made up of different quality fibres. This means that they are recycled into different products.\nOnce collected, we sell the paper and cardboard to an accredited merchant, who sells it on to a paper mill. The paper mill turns paper and card into pulp before creating new paper or cardboard sheets.\nCardboard is generally recycled back into cardboard packaging or plasterboard. Paper is recycled into new magazines, newspapers and office paper.\nOnce collected, we sort plastics using our MSF. Plastic bottles are generally made from high-density polyethylene (HDPE) and polyethylene terephthalate (PET). A re-processing factory cleans and grades the plastic and then turn it into flakes or pellets. They then send the plastic onto a manufacturer, who turns it into more plastic bottles and food packaging.\nOther types of plastics are sorted for onward transfer to a variety of different re-processors, and recycled into similar products. ‘Energy from waste’ companies will take any plastic that can’t be recycled into new packaging and turn it into electricity.\nClean clothes and paired shoes\nWe pass our collected clean clothing and paired shoes to an accredited merchant who checks all items by hand. Most items are of good quality and will be sent to developing countries so that someone can wear them again.\nItems of poor quality can still be used, normally as filler or insulation.\nWaste Electrical and Electronic Equipment (WEEE)\nWe send WEEE to a reprocessing factory where they separate hazardous and non-hazardous equipment from each other. The factory uses special processes to break down and recycle the hazardous equipment such as fridges and freezers. Non-hazardous equipment such as TVs and CD players are smashed into their raw materials. Their plastics, metals and glass are then sent on to be recycled into new products."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2c67ffea-a982-46b3-a8a8-77978a8b2663>"],"error":null}
{"question":"How does AWS support learning and ensure deployment security?","answer":"AWS offers free digital training with unlimited access to over 100 courses built by AWS experts, and can be learned without prior IT experience or coding skills. For security during deployment, AWS implements a multi-layered security approach where WAF filters web traffic, web servers are placed behind firewalls, and databases are stored in private subnets. They recommend storing sensitive data in RDS and S3 with encryption services and implementing multi-AZ deployment for better security and availability.","context":["What Are Aws Services?\nWhat exactly does AWS do? As the lead cloud computing platform, Amazon Web Services (AWS) is the primary profit driver for Amazon. AWS provides servers, storage, networking, remote computing, email, mobile development, and security. AWS accounts for about 13% of Amazon’s total revenue as of Q2 2021.\nIs AWS training free? At aws. training, you can enroll in free digital training and get unlimited access to more than 100 new courses built by AWS experts. You can also access previews of more advanced training on Machine Learning and Storage.\nIs AWS easy to learn? The title says that it’s the hard way to learn to AWS, but it’s actually not. If you work hard, take your time, and build skills, you will learn how to deploy and manage your infrastructure on AWS cloud. There is no prerequisite required to enroll for this course.\nWhat Are Aws Services? – Related Questions\nIs Amazon IaaS or PaaS?\nAWS (Amazon Web Services) is a comprehensive, evolving cloud computing platform provided by Amazon that includes a mixture of infrastructure as a service (IaaS), platform as a service (PaaS) and packaged software as a service (SaaS) offerings.\nWhat are the 3 types of cloud services?\nThere are also 3 main types of cloud computing services: Infrastructure-as-a-Service (IaaS), Platforms-as-a-Service (PaaS), and Software-as-a-Service (SaaS).\nWhat is AWS and how it works?\nAmazon Web Services (AWS) is a secure cloud services platform, offering compute power, database storage, content delivery and other functionality to help businesses scale and grow. Deliver static and dynamic files quickly around the world using a Content Delivery Network (CDN).\nHow much does it cost to use AWS?\nThe total cost of hosting your personal website on AWS will vary depending on your usage. Typically, it will cost $1-3/month if you are outside the AWS Free Tier limits. If you are eligible for AWS Free Tier and within the limits, hosting your personal website will cost around $0.50/month.\nIs coding required for AWS?\nNo. Getting started with and learning AWS does not require any coding skills, many basic tasks can be performed without coding. However dependent on the job / skills you have (or need) you may still be required to learn some programming skills.\nCan I learn AWS on my own?\nDigital training allows you to learn online at your own pace. Whether you are just starting out, building on existing IT skills, or sharpening your cloud knowledge, AWS Training and Certification can help you be more effective and do more in the cloud.\nCan you learn AWS with no experience?\nCan you learn AWS without experience? Yes. It’s possible to learn AWS and get certified without an IT background or degree, provided the necessary training hours are completed. The most approachable AWS exams are the “cloud practitioner” or the “associate” exams.\nCan I learn AWS a month?\nHow long will it take to learn the basics of AWS? You can start to understand AWS core services in a few days, gain practical knowledge with those core services in a few weeks, and in a month or two, you could expect to be undertaking professional work in AWS.\nCan you use Python in AWS?\nThe AWS SDK for Python (Boto3) enables you to use Python code to interact with AWS services like Amazon S3.\nIs AWS cloud easy?\nAWS is a comprehensive, easy to use computing platform offered Amazon. The platform is developed with a combination of infrastructure as a service (IaaS), platform as a service (PaaS) and packaged software as a service (SaaS) offerings.\nIs Gmail IaaS or PaaS?\nGmail is one famous example of an SaaS mail provider. PaaS: Platform as a Service The most complex of the three, cloud platform services or “Platform as a Service” (PaaS) deliver computational resources through a platform.\nIs Google Compute Engine a SaaS?\nFrom a network that spans the globe to innovative solutions that transform organizations, Google Cloud has SaaS built into our DNA.\nIs Google App Engine a SaaS?\nBuilding Your Own SaaS with Google App Engine\nWith GAE, developers can build a SaaS with the language of their choice while reaping the benefits of cloud computing in hosting their application: infinite and automatic horizontal scalability, metered usage and on-demand deployment of services.\nWhat are three 3 key aspects of cloud computing?\nCloud computing can be broken up into three main services: Software-as-a-Service (SaaS), Infrastructure-as-a-Service (IaaS) and Platform-as-a-Service (PaaS). These three services make up what Rackspace calls the Cloud Computing Stack, with SaaS on top, PaaS in the middle, and IaaS on the bottom.\nDoes Google use AWS?\nAWS and Google Cloud both provide web-based consoles. Each console allows users to create, manage, and monitor their resources. AWS and Google Cloud also provide a command-line interface (CLI) for interacting with the services and resources. AWS provides the Amazon CLI, and Google Cloud provides the Cloud SDK.\nDoes Microsoft use AWS?\nIs Microsoft software supported on AWS? Yes. AWS Support has been successfully supporting our customers who run Microsoft Windows-based EC2 instances in the AWS cloud since 2008 when we first launched Windows Server on EC2.\nWhat is salary of AWS?\nAs per Glassdoor, the average AWS salary in India starts from Rs. 6,07,000 per annum. It could even start at Rs. 3,83,000 per annum if you don’t get a chance to work at a top firm or don’t have relevant experience.\nHow people use AWS?\nAmazon Web Services (AWS) is a secure cloud services platform, offering compute power, database storage, content delivery and other functionality to help businesses scale and grow. Using managed databases like MySQL, PostgreSQL, Oracle or SQL Server to store information.\nHow much is AWS monthly?\nPricing for AWS Support Plans | Starting at $29 Per Month | AWS Support.\nWhat is required to learn AWS?\nThe following are the prerequisites to learn AWS: Basic knowledge of operating systems. Basic knowledge of Virtualization. Basic knowledge of any programming language.\nIs AWS Certification enough to get a job?\nNo. On its own, an AWS certification is not a guarantee of a job. Job hunters in the cloud industry will need to have demonstratable hard-skills e.g. programming as well as relevant experience and soft-skills such as communication and teamwork.","The intense conversations around Cloud security are growing louder and passionate. Although the industry has been working continuously to develop higher security standards and best practices, the paranoia that an enterprise is only as strong as its weakest link, persists. Often that link is a careless lapse by an employee. Sometimes it could be a significant lack of governance or policy structure.\nSecurity by Design (SbD) is not a new concept but it has gained currency in the wake of new challenges brought about by cloud deployment and the connected world. Nishant Sharma, CTO of Umbrella explains how Security by Design is enabling higher security for enterprise customers.\nWhat is AWS Security by Design?\nAWS Security by Design is a conscious effort to incorporate security approaches while designing IT architecture. Security is not an afterthought but must be incorporated from inception at every layer of the AWS architecture—from the perimeter, the host, application, database and the data itself. It comprises policies, best practices and governance structures using AWS services that not only automate the monitoring processes but also prevents attacks and leaks by sending alerts and auto-correction. Automation is a key hallmark of AWS SbD.\nWhy is Security by Design more relevant in the context of Cloud deployment?\nCloud deployment gives rise to new complexities—deployments in the public network gives rise to unique set of challenges, more vulnerabilities. For example, in a traditional datacentre, securing the perimeter pretty much secured the environment as application usage could be restricted within the enterprise.\nHowever as applications move away from the enterprises, it is exposed to more risks in the public network. Therefore security has to become an intrinsic part of cloud deployment, woven into the fabric of the cloud journey as a basic ingredient. This means security should come into play while designing the architecture and at every decision making point—what kind of architecture; which services to provision at which layers; how to interconnect services; allowing permissions; provisioning alerts and alarms; accessing and transporting data securely; encrypting data at rest; back-up and fail-over strategies.\nAs cloud-based applications and infrastructure become more prolific, the need for governance, automated monitoring and best practices at every layer of the IT infrastructure become more pertinent. Today, SbD is not an option but is something we breathe and think in all cloud deployments.\nWhat are the guiding principles of AWS Security by Design approach?\nAWS offers a range of services and customers have the flexibility to choose a combination. The flip side of this flexibility is the increased complexity customers have to deal with while implementing services in a secure manner.\nBut if we adhere to the basic principles of SbD we can mitigate many risk factors by eliminating manual processes, optimizing audit ratification processes by automation.\nWe have successfully deployed AWS Infrastructure for many customers by following SbD principles. If you examine our deployments, every decision is based on enhancing security and availability of data.\nAt Umbrella we believe that security must be built at every layer. For example, we recommend the use of a WAF in front of the load balancer to filter web traffic. Next, web server is placed behind a firewall and access to the web application is only through ports 443 and 80. The database layer is placed within a private subnet and can be accessed only from the web server. So we have effectively ensured that every layer is well defined, and segregated with different levels of security layer.\nWhile designing a deployment we always design for failure and availability strategies. For example, we encourage customers to deploy database and servers in multi-AZ mode and use auto-scaling for web servers. Another important aspect related to security is data storage and usage itself for which different options are available. For example, businesses must identify sensitive data and always encrypt sensitive data at rest and in transit. We recommend storing sensitive data in RDS and S3 where encryption services are available. Turning on versioning in S3 and enabling MFA for deletion rights provide added layers of security for sensitive data.\nHow can one implement Security by Design on AWS?\nUmbrella recommends four-phased approach to implement SbD:\nUnderstand your requirements: Outline your policies, document the controls you inherit from AWS, document the controls you own and operate in the AWS environment and decide what security rules you want to operate in the AWS environment.\nBuild a secure environment: Build a secure environment aligned with your requirements and goals, defining appropriate configurations—including encryption requirements, providing permissions, authorizing compute images, enforcing logging and Cloud Trail services. Umbrella also recommend using Cloud Formation Templates which provide comprehensive rules sets and can be enforced.\nEnforce the use of templates: Enable Service Catalog and enforce the use of templates in the catalog. For example, we used Cloud Formation for an ecommerce customer for fast deployment and traceability since the business required continuous development. This ensure that when new environments are created, it is secure and adhered to compliance.\nPerform validation activities: Perform validation activities through audit automation processes. If you have designed the AWS deployment correctly using Service Catalog and secure environment templates, you will have an audit-ready environment. Using Lambda and Cloud Config allows you to perform validation activity.\nWhat are the benefits of Security by Design?\nSbD architecture automates the customer’s assurance, governance and compliance capabilities and creates an enforceable environment. SbD architecture creates functions that cannot be overridden by users; establishes reliable operations controls; enables continuous, real-time auditing; and programmatically manages the governance policy.\nWe send newsletter monthly"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:649ba64e-66c0-4607-9bcc-9a1e7311006c>","<urn:uuid:7cbf7c8a-d298-4ccb-b671-2d92856337cc>"],"error":null}
{"question":"How do noise warning signs function on construction sites, and what are the long-term health impacts of excessive noise exposure for workers?","answer":"On construction sites, weatherproof noise warning signs like the SafeEar Max XXL and SafeEar IP65 light up at pre-set trigger levels to inform workers when hearing protection is needed. As for long-term health impacts, construction workers face a 60% chance of losing their hearing if exposed to hazardous noise (above 90dB) throughout their career. This hearing loss develops gradually and can manifest as difficulty hearing soft sounds, understanding speech in noisy environments, and hearing high-pitched noises. Additional impacts include tinnitus, nervousness, reduced concentration, sleep problems, decreased productivity, and increased risk of work-related accidents. Workers may also experience social isolation and communication difficulties with co-workers.","context":["Noise monitoring for Building sites\nAll construction sites are likely to produce noise which cannot be completely prevented. This noise, however, can be a danger to construction workers and a nuisance to people who live in the vicinity. Control methods exist to minimise noise exposure levels and prevent loss of hearing to workers on building sites.\nWork patterns in the construction industry vary greatly and can last from days to years depending on the project. As such, a noise monitoring solution that can measure noise on a short-term or a long term basis is often obligatory in this type of environment and industry.\nConstruction noise combines both the type of long-term noise exposure that falls under the category of Noise at Work and, also, environmental noise issues which can cause a nuisance from a distance, such as construction or demolition work taking place close to private housing.\nConstruction Noise Levels\nEmployees working on a construction site often need to make quick decisions on a daily basis. The nature of their jobs may require them to move from one location to another, use moving or mobile machinery and operate in difficult to reach areas. Therefore the availability of a noise measurement device that is rugged, easy to use and portable may be the answer to checking on their noise exposure, especially in multi-site scenarios.\nRegular noise ‘spot checks’ using a sound level meter at different locations on a construction site help to determine quickly if there is a possibility of a problem by showing a reading in decibels. Some noise meters can be mounted on a tripod at a boundary position and left to measure noise over a specific period of time or over a number of ‘fixed duration’ measurements.\nThere are also a number of instruments that can help identify whether workers are potentially at risk from excessive noise and determine whether hearing protection is needed. It is worth noting that, for safety reasons, it is not always advisable for workers on sites to wear hearing protection continuously as alarm systems or warnings for example could be missed.\nOther tools to carry out professional noise surveys on sites and measure personal noise exposure can be used. These are better known as PSEM or Personal Sound Exposure meters. These are tiny units that can be mounted on the worker’s body such as the shoulder and monitor the hearing second by second over a working shift. The data is stored internally and can be simply read at the end of each shift or transferred to a PC to both provide a record and allow the company to do an analysis of the noise. Used with a range of precision integrating sound level meters, they can enable a safety professional to investigate the noise, including the selection of appropriate hearing protection.\nNoise Warning Signs\nPulsar has developed two weatherproof noise warning signs suitable for use on construction sites, the SafeEar Max XXL sign and also the new SafeEar IP65. Both of these interactive noise warning signs are rated at IP65 and are suitable for use outdoors (or in!). These signs light up at a pre-set trigger levels and tell workers and visitors when they need to put on their hearing protection and when it’s safe to take it off.","When considering construction worker injuries many people think of examples of injuries caused by physical accidents from impacts such as a fall from a ladder or scaffolding. However, there are many other types of injuries that occur in addition to those that occur from strictly physical accidents. In fact, one of the most common injuries suffered by construction workers is hearing loss.\nConstruction workers are often exposed to loud noises which make them especially susceptible to the loss of hearing. According to Audicus, there are around 30 million workers in the U.S. who are exposed to hazardous noise on the job.\nHazardous noise is defined as noise which exceeds 90bD (decibels) for an extended period of time. There is a 60 percent chance of construction workers losing their hearing if they are exposed to hazardous noise throughout their career.\nCompared to a 9 percent risk factor for no noise exposure, and a 30 percent risk in manufacturing, construction, along with mining, remain the top two high-risk professions in industry.\nThe Main Causes of Hearing Loss on a Job Site\nThe most common cause of hearing loss on a job site is intuitive: a lack of use of protective hearing equipment, which is required by law. There may be one of two common reasons why construction workers do not use hearing equipment.\nFirst, the employer may not make them aware of the risks of working in an environment with hazardous noise and may not provide them with appropriate hearing protection. Secondly, the worker may choose not to wear protective hearing equipment.\nA third, often overlooked, reason is that existing hearing protection may be insufficient. When hearing protection does not prevent hazardous noise from reaching the ear, or fails to do so effectively, the worker may be unknowingly exposed to excessive noise.\nLack of equipment to control noise can be a contributing factor to hearing loss as can failing to take breaks when working in an area containing hazardous noise. Finally, a lack of training about how to wear hearing protection can contribute to the risk of overexposure and hearing loss.\nWhere Hearing Loss Occurs\nHearing loss typically occurs in areas where noise levels exceed 90bD – the established noise level that is deemed “hazardous” to workers’ hearing. Job sites where workers are exposed to, or must use or interact with, heavy machinery, jackhammers, sledgehammers, nail and bolt drivers, electrical saws, and constant noise from demolition pose the most serious risk.\nWhen Hearing Loss Occurs\nHearing loss usually does not occur upon a single overexposure, although if the noise is loud enough, it may permanently or temporarily cause hearing loss. Permanent hearing loss occurs over time when repeated overexposure is experienced by the worker. Workers who reach the age of 50, and have been working in construction for at least several years prior, are the most likely to experience hearing loss.\nWorkers who start young, and enjoy a lifetime of work in construction, may be at an increased risk for hearing loss, particularly if their exposure levels exceeded the maximum threshold during most workdays.\nHow Hearing Loss Occurs\nHearing loss happens in one of two ways.\nConductive Hearing Loss:\n- Sound energy is blocked before it can reach the inner ear. This typically occurs because of an obstruction like earwax or a tumor is blocking sound from getting to the ear. This is typically not associated with damage caused by overexposure to noise or sound on a construction site, but it must be ruled out by a physician.\nSensorineural Hearing Loss:\n- Sensorineural hearing loss is a type of hearing loss which occurs because of some type of damage to the ear. Damage may occur to the inner ear or the nerves that are responsible for hearing (auditory nerves). Damage can also occur to the tympanic membrane (ear drum) which will prevent hearing.\nWith repeated overexposure to hazardous noise, construction workers are most likely to be exposed to sensorineural hearing loss at some point during their lives. Hearing does not immediate disappear. Rather, as damage accumulates, hearing gradually fades away.\nOver time, the individual may notice this manifesting as various symptoms. Symptoms of hearing loss include:\n- Muffled or distorted hearing;\n- Difficulty hearing high-pitched noises, like birds singing;\n- Difficulty hearing soft noises like crickets chirping;\n- Difficulty hearing alarm clocks, watch alarms, telephones, or doorbells;\n- Difficulty understanding telephone conversations while in public or where background noise is present; and,\n- Ringing in the ear, also called tinnitus, after overexposure;\nHearing loss can be mild, moderate, or severe. Mild hearing loss is usually associated with the inability to hear soft sounds or difficulty understanding speech in noisy environments. Moderate hearing loss occurs when workers are unable to hear soft and moderately loud sounds and have considerable difficulty understanding speech, especially background noises.\nSevere hearing loss occurs when some loud sounds are inaudible and communication is impossible or extremely difficult without a hearing aid.\nPrevention and Treatment\nPrevention is the best treatment. Loud noises are dangerous to human ears, and the impact they can have can be devastating. Noise levels of just 100dB for 15 minutes are enough to cause damage to the inner ear. 100dB is the “loudness” of a typical MP3 player at full volume.\nWorking in environments where construction is going on represent an inherent danger, because the noise level often exceeds the 100dB level. Once destroyed, the microscopic hair cells of the inner ear don’t grow back. They also cannot be artificially recreated by any known medical means. Once destroyed or damaged, permanent hearing loss occurs.\nTinnitus is just one symptom that results from the damage and destruction of those hair cells.\nIn addition to hearing loss, workers may experience nervousness, a reduced ability to concentrate, a reduction in the quality of sleep, an inability to get to sleep in a timely or efficient manner, reduced productivity, and an increased risk of work-related accidents.\nWorkers who lose their hearing are also more susceptible to feeling isolated and may have trouble communicating with co-workers.\nFor these reasons, workers should always wear specially-designed ear protection that is rated for the construction job and site they are working on. Hearing protection comes in two versions: passive and active.\nPassive protective devices or uniform-attenuating protectors use mechanical blocks or means to filter sounds to provide equal attenuation across the audible frequency range. Attenuation refers to the general reduction in strength of a signal. In the context of noise, it means the reduction of noise or sound. Attenuation is expressed in dB.\nThe sounds heard with passive protection in place are more natural, clearer and less distorted than protection provided by low-quality alternative or conventional hearing protectors. Workers who are continually exposed to loud noises benefit the most from this type of hearing protection. When communication is secondary to work, this type of hearing protection may be ideal.\nFor passive hearing devices to work, however, they must be properly fitted and used. As workers get hearing protection without distortion or muffling, they feel less isolated on the job site. If possible, the hearing protection device should be custom-fitted for the individual, and considerable time should be spent explaining the importance of always wearing hearing protection.\nSome workers may have the tendency to only wear hearing protection when they are currently working, but may not wear the protection when they have finished the job but are still on the construction site in an area where hazardous noise exists. Workers must wear their passive hearing protection until they are clear of hazardous noise areas.\nActive hearing protection or “level dependent” hearing protectors block sound and also use electronic means to transmit low-level sounds through the hearing protector.\nThe electric signal amplifies incoming sounds up to a specific level. Anything that exceeds that level is automatically reduced, thus preserving the wearer’s hearing through limiting and preventing overexposure.\nThese devices also employ amplifiers for background noise. Rather than providing equal attenuation, they provide it selectively. Loud noises, even sudden loud noises, are reduced to comfortable levels, while low dB noises are not muted or attenuated. The result is that the worker can carry on a conversation with another co-worker without the need for special communication equipment and without removing the protective gear, risking overexposure.\nElectronic devices also allow users control over the attenuation of hazardous noise, essentially allowing customization of hearing. These provide the best hearing protection in environments where hazardous noise occurs, but where communication is still important, even critical.\nAnother benefit of these devices is that special FM or infrared (wireless) capabilities can be integrated into the device to provide one or two-way communication. This allows workers to communicate with each other without risking overexposure to hazardous noise levels in situations where they are spaced apart and cannot effectively communicate by other means.\nAgain, for these devices to work, they must be properly fitted and worn during times when hazardous noise threatens the individual. Like the passive device, it should be custom-fitted and tested prior to use. Because it’s electronic, it should be inspected prior to each use to make sure that the power is on, or that the device is properly charged and functioning normally.\nPeriodic replacement and maintenance is necessary with these types of ear protectors, which makes them a more expensive option.\nHearing Aids, Implants and Resources\nOnce hearing loss occurs, options for treatment are limited. There is no official cure for hearing loss, but there are some treatments which may improve hearing or augment it. While cochlear implants are an available technology, they are still relatively expensive and full hearing restoration is not guaranteed. A second option is a hearing aid, which does not restore lost hearing, but rather amplifies sound, making hearing easier.\nGetting A Cochlear Implant\nCochlear implants have been approved by the Food and Drug Administration for children and adults who are deaf or severely hard-of-hearing. As of December 2012, there have been roughly 324,000 individuals worldwide who have had the procedure and received implants.\nIn the U.S., only 58,000 implants have been done. Some adults who have lost their hearing completely may be candidates for implants. For these individuals, they must relearn how to hear after the procedure and full hearing restoration is never guaranteed. Patients must learn to associate the signal provided by an implant with sounds they remember.\nHealth insurance may cover the procedure, but not always.\nThe implant itself consists of a microphone, which is capable of picking up sounds from the environment. A speech processor arranges sounds that are captured by the mic. A transmitter receives signals from the speech processor and then converts them into electrical impulses. Finally, an electrode array collects and transmits the impulses from the stimulator to various regions of the auditory nerve.\nThe implant will not restore normal hearing. Instead, it gives deaf or hard-of-hearing individuals useful representations of sounds in the environment. It can also help the individual better understand speech.\nA hearing aid is a device that is fitted into the ear which effectively amplifies sound. Hearing aides are commonly used by older adults or individuals who are hard-of-hearing. They do not replace the human ear, and are not a replacement for cochlear implants.\nInstead, they capture the sounds of the environment and convert them into electrical signals. The hearing aid amplifier increases the strength of the signal. Then, the aid converts the signal back into sound and sends it into the inner ear.\nThe brain hears and understands the sound just as it would unaided. The technology requires that the patients still have basic functional hearing and that the ear drum assembly, inner, middle, and outer ear are structurally intact. The labyrinth system of the ear must also be normal or at least not so abnormal that hearing becomes physically impossible.\nWhat Should You Do If Your Hearing Is Already Affected?\nIf you feel that the risks that you’re being asked to take are unreasonable, you’ve suffered hearing damage on the job, or if you’ve been injured in some other way on a construction site, don’t hesitate to speak with an attorney. You have rights, and you shouldn’t be afraid to assert them."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:869c00eb-a833-4be0-8d76-e6f13dfb36b5>","<urn:uuid:21c43838-3700-4391-abaa-4c1da88fd373>"],"error":null}
{"question":"What are the traditional medicinal properties of chamomile in medieval monasteries, and what modern scientific evidence supports its therapeutic uses?","answer":"In medieval monasteries, chamomile was used in infirmary gardens for sedative effects and digestive issues. It was combined with dittany, scabious, and pennyroyal as a remedy against poison. According to modern scientific research, while chamomile is widely used for various conditions including sleep disorders, anxiety, digestive conditions, and skin infections, there is not enough reliable research to definitively support these uses. The German Commission E has authorized topical chamomile for skin diseases, and preliminary studies suggest it may help with conditions like diarrhea in children and wound healing. However, despite its reputation as a gentle medicinal plant, there are many reports of allergic reactions, including life-threatening anaphylaxis.","context":["A monastery’s infirmary herb garden grew specialist plants that were used in medieval medicine to help the body heal itself. Here are nine plants that you’d find there which you can still grow in your own herb garden today.\nGardens dedicated to medicinal herbs alone were quite rare in medieval times, except in large institutions like monasteries, for example Rievaulx Abbey in Yorkshire (pictured), where there were lots of people to care for.\nMedieval medicine was based on the notion of the body having four ‘humours’ related to the four elements:\n- blood (air) was hot and moist\n- phlegm (water) was cold and moist\n- yellow bile (fire) was hot and dry\n- black bile (earth) was cold and dry.\nIt was the physician’s job to work out how to restore the balance of a person’s humours if they became ill, and so plants and herbs were ascribed properties to redress the balance. A cooling herb would be used if you were considered to have too much blood or yellow bile, for example.\nHere are nine plants to sow for a herb garden inspired by monastic infirmary gardens in the Middle Ages:\n(We wouldn’t recommend brewing your own herbal remedies without plenty of research.)\nSage (Salvia officinalis)\nSage, whose first botanical name comes from the Latin salveo, meaning “I am well” , was used by the Romans in medicine and cooking. As with some other herbs mentioned below, ‘officinalis’ is a reminder of its monastic medicinal use — the officina being the monastic storeroom where herbs and medicines were stored.\nIn the medieval period sage was described as being ‘fresh and green to cleanse the body of venom and pestilence’. It was also chewed to whiten teeth and used very frequently in cooking along with lots of onions and garlic. This means that sage and onion stuffing has a medieval pedigree!\nSage is best grown in well drained soil with full sun and can be grown either from seed, from cuttings or from plug plants.\nBetony (Stachys officinalis)\nThis was once an incredibly popular herb, and used for curing anything and everything you can think of – including a few extras like fear, ‘violent blood’, and ‘chilly need’.\nDepending on the variety, betony grows between 25cm and 90cm tall.\nIts flowers, generally purplish but sometimes white, appear between June and October. It’s long-lived and slow-growing and prefers dampish but not waterlogged areas.\nClary Sage (Salvia sclarea or wild clary is Salvia verbenaca)\nAnother member of the salvia family, Clary Sage was also known as ‘clear eye’ and ‘Oculus Christi’ (Eye of Christ) as its main use was as an eyewash, made by infusing sweet scented leaves in water.\nIt’s a biennial with purple-blue flower spikes from late spring to mid-summer and attracts honey-bees and other pollinators.\nHyssop (Hysoppus officinalis)\nIn medieval herb gardens, hyssop was considered a hot purgative. Drunk in oil, wine or syrup, it was meant to warm away cold catarrhs and chest phlegm. It was also rubbed on bruises to soothe them and had purifying, astringent and stimulant uses.\nIt has spikes of blue, pink, or red flowers and prefers well drained soil.\nRue (Ruta graveolens)\nThis was used as a strong purgative for plague and poison, and as a holy water sprinkler in exorcisms. Its medicinal properties have now largely been disproved, and its use in cures may be dangerous. Its smell is a repellent to Japanese beetles, dogs and cats and it attracts some species of butterfly.\nYou can recognise rue plants by their bushy, bluish-green, fernlike leaves ,and yellow flowers with wavy edges and green hearts. Rue can grow up to 90cm tall.\nBest grown in well drained soil with full sun – rarely needs watering. Take care when handling the plant – its sap can be a strong irritant.\nChamomile or Camomile (Chamaemelum nobile)\nChamomile is said to revive the sickly and drooping plants growing near it. It’s a very tough plant, sometimes grown in ‘chamomile lawns’ — which take a lot of work to establish. Since the daisy-like flowers are very small, lots of them are needed to be of use.\nOnce you have enough of them, chamomile flowers are good for making sedative and digestive infusions that also combat flatulence. Chamomile tea with dittany, scabious and pennyroyal was a preferred medieval remedy against poison.\nThis perennial herb grows best in cool conditions and prefers part-shade and dry soil.\nDill (Anethum graveolens)\nThe word dill derives from the Anglo-Saxon dilla which means ‘to lull’. It was used as a kitchen herb for flavouring fish, pickles and pottages, as well as in the infirmary for cordials. Along with cumin and anise, its seeds were made into spice cakes to eat after rich meals or illness to help with digestion.\nIts delicate fronds can reach 60-90cm in height.\nCumin (Cuminum cyminum)\nInfirmarers grew cumin to use its seeds in soothing ointments for the complexion and eyes, as well as for its culinary uses. Cumin was grown more widely than dill outside monastic gardens.\nPeasant rents were sometimes paid in cumin, along with hens and eggs.\nIt’s native to the Mediterranean and requires a long hot summer, so isn’t the easiest plant to grow in the UK.\nComfrey – also known as Boneset (Symphytum officinale)\nComfrey has a long history of use in medicine, and was grown in infirmary gardens for its power to heal wounds and inflammations and (as its nickname suggests) help to set broken bones.\nComfrey needs rich, moist, alkaline soil and generally prefers shady areas. It can grow up to 120cm tall and has long, hairy, deep-green leaves. Take care when handling the plant, which can irritate sensitive skins.\nPingback: THE LANGUAGE OF FLOWERS – Little England\nPingback: Apothecary Roses of Germantown","Anthemis arvensis, Anthemis cotula, Anthemis nobile, Anthemis nobilis, Anthemis xylopoda, apigenin, Asteraceae/Compositae (family), baboonig, babuna, babunah, babunah camomile, babunj, bunga kamil, camamila, camamilla, camomile, camomile sauvage, camomilla, Camomille Allemande, Campomilla, chamaemeloside, Chamaemelum nobile L., chamomile flowers, Chamomilla, Chamomilla recutita, chamomillae ramane flos, chamomille commune, classic chamomile, common chamomile, double chamomile, Echte Kamille (Dutch), English chamomile, feldkamille (German), fleur de chamomile (French), fleurs de petite camomille (French), Flores Anthemidis, flos chamomillae, garden chamomile, German chamomile, Grosse Kamille, Grote Kamille, ground apple, Hungarian chamomile, Kamille, Kamillen, kamitsure, kamiture, Kleine, kleme kamille, lawn chamomile, low chamomile, manzanilla, manzanilla chiquita, manzilla comun, manzanilla dulce, matricaire, Matricaria chamomilla, Matricaria maritime (L.), Matricaria recutita, Matricaria suaveolens, matricariae flos, matricariae flowers, may-then, Nervine, pin heads, rauschert, Romaine, romaine manzanilla, Roman chamomile, Romische Kamille, single chamomile, STW 5 (containing Iberis, peppermint, chamomile), sweet chamomile, sweet false chamomile, sweet feverfew, true chamomile, whig-plant, wild chamomile.\nChamomile has been used medicinally for thousands of years and is widely used in Europe. It is a popular treatment for numerous ailments, including sleep disorders, anxiety, digestion/intestinal conditions, skin infections/inflammation (including eczema), wound healing, infantile colic, teething pains, and diaper rash. In the United States, chamomile is best known as an ingredient in herbal tea preparations advertised for mild sedating effects.\nGerman chamomile (Matricaria recutita) and Roman chamomile (Chamaemelum nobile) are the two major types of chamomile used for health conditions. They are believed to have similar effects on the body, although German chamomile may be slightly stronger. Most research has used German chamomile, which is more commonly used everywhere except for England, where Roman chamomile is more common.\nAlthough chamomile is widely used, there is not enough reliable research in humans to support its use for any condition. Despite its reputation as a gentle medicinal plant, there are many reports of allergic reactions in people after eating or coming into contact with chamomile preparations, including life-threatening anaphylaxis.\nThese uses have been tested in humans or animals. Safety and effectiveness have not always been proven. Some of these conditions are potentially serious, and should be evaluated by a qualified healthcare provider.\nChamomile is not well-known for its cardiac effects, and there is little research in this area. Large, well-designed randomized controlled trials are needed before a firm conclusion can be made.\nIn early study, inhaling steam with chamomile extract has been reported to help common cold symptoms. Further research is needed to confirm these results.\nDiarrhea in children\nPreliminary study reports that chamomile with apple pectin may reduce the length of time that children experience diarrhea. Further research is needed before a strong recommendation can be made.\nThe German Commission E authorizes the use of topical chamomile for diseases of the skin. However, little research has been done on topical chamomile for eczema and further research is needed.\nChamomile is used traditionally for numerous gastrointestinal conditions, including digestion disorders, \"spasm\" or colic, upset stomach, flatulence (gas), ulcers, and gastrointestinal irritation. However, currently there is a lack of reliable human research available in any of these areas. Additional study is needed.\nHemorrhagic cystitis (bladder irritation with bleeding)\nPreliminary study reports that the combination of chamomile baths plus chamomile bladder washes and antibiotics is superior to antibiotics alone for hemorrhagic cystitis. Additional research is necessary before a conclusion can be reached.\nPreliminary study reports that chamomile ointment may improve hemorrhoids. Better evidence is needed before a strong recommendation can be made.\nChamomile is reputed to have anti-spasmodic activity, but there is little research to substantiate this claim. Additional research evaluating chamomile alone is needed.\nMucositis from cancer treatment (mouth ulcers/irritation)\nPoor-quality studies have used chamomile mouthwash for the prevention or treatment of mouth mucositis caused by radiation therapy or cancer chemotherapy. Results are conflicting, and it remains unclear if chamomile is helpful in this situation.\nQuality of life in cancer patients\nA small amount of research suggests that massage using chamomile essential oil may improve anxiety and quality of life in cancer patients. However, this evidence is not high quality. Additional study is needed before a firm conclusion can be reached.\nTopical chamomile preparations have traditionally been used to soothe skin inflammation. The existing human evidence shows that chamomile may be of little, if any, benefit while animal studies support its anti-inflammatory action. Additional human research is needed in this area.\nSleep aid / sedation\nTraditionally, chamomile preparations, such as tea and essential oil aromatherapy, have been used for insomnia and sedation (calming effects). Better research is needed before a recommendation can be made.\nVaginitis (inflammation of the vagina)\nVaginitis may involve itching, discharge, or pain with urination. Chamomile douche may improve symptoms of vaginitis with few side effects. Because infection (including sexually transmitted diseases), poor hygiene, or nutritional deficiencies can cause vaginitis, medical attention should be sought by people with this condition. Better research is needed before a conclusion can be drawn regarding the role of chamomile in the management of vaginitis.\nThere is promising preliminary evidence supporting the topical use of chamomile for wound healing. However, the available literature is not adequate to support a recommendation either for or against this use.\nPost-operative sore throat/hoarseness due to intubation\nChamomile spray has not been found to prevent post-operative sore throat and hoarseness any more than normal saline.\n*Key to grades:\nA: Strong scientific evidence for this use;\nB: Good scientific evidence for this use;\nC: Unclear scientific evidence for this use;\nD: Fair scientific evidence against this use (it may not work);\nF: Strong scientific evidence against this use (it likely does not work).\nThe below uses are based on tradition, scientific theories, or limited research. They often have not been thoroughly tested in humans, and safety and effectiveness have not always been proven. Some of these conditions are potentially serious, and should be evaluated by a qualified healthcare provider. There may be other proposed uses that are not listed below. Abdominal bloating, abortifacient, abrasions, abscesses, acne, anorexia, antibacterial, anticoagulant, antifungal, antioxidant, antipruritic, antispasmodic, antiseptic, anxiety, aromatic, arthritis, asthma, back pain, bedsores, bladder disorders, blood purification, bruises, burns, cancer, canker sores, carpal tunnel syndrome, catarrh, chicken pox, constipation, contact dermatitis, cough, Crohn's disease, croup, delirium tremens (DTs), diaper rash, diaphoretic, diuretic (increasing urination), diverticulitis, dry skin, dysmenorrhea (painful menstruation), ear infections, eye disorders (blocked tear ducts), eye infections, fatty liver, fever, fistula healing, frostbite, gallstones, gingivitis, gout, hay fever, headaches, heartburn, hives, hypoglycemia (low blood sugar), hysteria, impetigo, inflammatory conditions, insect bites, insomnia, intestinal cramps, irregular menstrual cycles, irritable bowel syndrome, kidney disorders, leg ulcers, liver disorders, low back pain, malaria, mastitis (breast inflammation), menopause, menstrual cramps, menstrual disorders, morphine withdrawal, motion sickness, muscle strength, nasal inflammation, nausea, nervous stomach, neuralgia (nerve pain), nightmares, oral hygiene (mouthwash), osteoporosis, parasites/worms, peptic ulcers, perineal trauma, poison ivy, post-natal depression, psoriasis, rash (heat), respiratory inflammatory, restlessness, rheumatism, Roehmheld's syndrome, sciatica, seizure disorder, sinusitis, stomach cramps, sunburn, sunstroke, teething pain (mouth rinse), tension, tics, toothache, travel sickness, tuberculosis, ulcerative colitis, ulcers, uterine disorders, uterine stimulant, uterine tonic, vaginal infections, viral infection (flu-like symptoms or polio), vomiting, vomiting/nausea during pregnancy.\nThe below doses are based on scientific research, publications, traditional use, or expert opinion. Many herbs and supplements have not been thoroughly tested, and safety and effectiveness may not be proven. Brands may be made differently, with variable ingredients, even within the same brand. The below doses may not apply to all products. You should read product labels, and discuss doses with a qualified healthcare provider before starting therapy.\nAdults (18 years and older)\nCapsules/tablets containing 400 to 1,600 milligrams in divided doses have been taken by mouth daily. As a liquid extract (1:1 in 45% alcohol), 1 to 4 milliliters three times daily has been taken by mouth. As a tincture (1:5 in alcohol), 15 milliliters three to four times per day has been used. As a mouth rinse, a 1% fluid extract or 5% tincture has been used.\nChamomile is frequently consumed as tea, and 1 to 4 cups of chamomile tea taken daily (from tea bags) is a common dose.\nThere are no standard doses for chamomile used on the skin. Some natural medicine publications have recommended paste, plaster, or ointment containing 3% to 10% chamomile flower heads. Chamomile has been also used as a bath additive and as a douche.\nChildren (younger that 18 years)\nThere is not enough reliable scientific data available to recommend the safe use of chamomile products in children.\nThe U.S. Food and Drug Administration does not strictly regulate herbs and supplements. There is no guarantee of strength, purity or safety of products, and effects may vary. You should always read product labels. If you have a medical condition, or are taking other drugs, herbs, or supplements, you should speak with a qualified healthcare provider before starting a new therapy. Consult a healthcare provider immediately if you experience side effects.\nThere are multiple reports of serious allergic reactions to chamomile taken by mouth or as an enema, including anaphylaxis, throat swelling, and shortness of breath. Skin allergic reactions have been frequently reported, including dermatitis and eczema. Chamomile eyewash can cause allergic conjunctivitis (pinkeye).\nPeople with allergies to other plants in the Asteraceae (Compositae) family should avoid chamomile. Examples include: aster, chrysanthemum, mugwort, ragweed, and ragwort. Cross-reactions may occur with celery, chrysanthemum, feverfew, tansy, and birch pollen. Individuals with allergies to these plants should avoid chamomile. Contact skin allergy has been reported.\nImpurities (adulterants) in chamomile products are common and may cause adverse effects. Atopic dermatitis (skin rash) has been reported.\nChamomile in various forms may cause drowsiness or sedation. Use caution when driving or operating heavy machinery. In large doses, chamomile can cause vomiting. Due to its coumarin content, chamomile may theoretically increase the risk of bleeding. Caution is advised in patients with bleeding disorders or taking drugs that may increase the risk of bleeding. Dosing adjustments may be necessary. Increases in blood pressure are possible.\nPregnancy and Breastfeeding\nIn theory, chamomile may act as a uterine stimulant or lead to abortion. It therefore should be avoided during pregnancy. There is not enough scientific data to recommend the safe use of chamomile while breastfeeding.\nMost herbs and supplements have not been thoroughly tested for interactions with other herbs, supplements, drugs, or foods. The interactions listed below are based on reports in scientific publications, laboratory experiments, or traditional use. You should always read product labels. If you have a medical condition, or are taking other drugs, herbs, or supplements, you should speak with a qualified healthcare provider before starting a new therapy.\nInteractions with Drugs\nChamomile interactions are not well studied scientifically.\nChamomile may increase the amount of drowsiness caused by some drugs. Examples include benzodiazepines such as lorazepam (Ativan©) or diazepam (Valium©), barbiturates such as phenobarbital, narcotics such as codeine, some antidepressants, and alcohol. Caution is advised while driving or operating machinery.\nIn theory, chamomile may increase the risk of bleeding when used with anticoagulants or antiplatelet drugs. Some examples include aspirin, anticoagulants (\"blood thinners\") such as warfarin (Coumadin©) or heparin, anti-platelet drugs such as clopidogrel (Plavix©), and non-steroidal anti-inflammatory drugs (NSAIDs) such as ibuprofen (Motrin©, Advil©) or naproxen (Naprosyn©, Aleve©).\nChamomile may interfere with the way the body processes certain drugs using the liver's \"cytochrome P450\" enzyme system. As a result, the levels of these drugs may be increased in the blood and may cause increased effects or potentially serious adverse reactions. Patients using any medications should check the package insert and speak with a healthcare professional including a pharmacist about possible interactions.\nBe aware that many tinctures contain high levels of alcohol and may cause vomiting when taken with metronidazole (Flagyl©) or disulfiram (Antabuse©).\nAn extract containing Matricaria chamomile, Sideritis euboea, Sideritis clandestine, and Pimpinella anisum was associated with selective estrogen receptor modulator (SERM) properties against osteoporosis. Theoretically, chamomile may interact with SERM drugs like raloxifene (prescription drug used for osteoporosis) or tamoxifen (a prescription drug used for cancer).\nConstituents in chamomile may alter blood sugar or blood pressure. Patients taking medications that affect blood sugar or blood pressure should be cautious.\nChamomile may have anti-inflammatory effects. Theoretically, use of chamomile with other anti-inflammatory drugs, such as NSAIDs or ibuprofen, may have additive effects.\nChamomile may interact with medications that act as cardiac depressants, central nervous system depressants, calcium channel blockers, cardiac glycosides, and respiratory depressants.\nChamomile may also interact with antibiotics, antifungals, antihistamines, diuretics, as well as drugs for high cholesterol, ulcers, diarrhea, or gastrointestinal disorders.\nInteractions with Herbs and Dietary Supplements\nChamomile may increase the amount of drowsiness caused by some herbs or supplements. Caution is advised while driving or operating machinery.\nIn theory, chamomile may increase the risk of bleeding when taken with other products that are believed to increase the risk of bleeding. Multiple cases of bleeding have been reported with the use of Ginkgo biloba, and fewer cases with garlic and saw palmetto. Numerous other agents may theoretically increase the risk of bleeding, although this has not been proven in most cases.\nChamomile may interfere with the way the body processes certain drugs using the liver's \"cytochrome P450\" enzyme system. As a result, the levels of other herbs or supplements may become too high in the blood. It may also alter the effects that other herbs or supplements possibly have on the P450 system. Patients using any medications should check the package insert and speak with a healthcare professional including a pharmacist about possible interactions.\nChamomile may have anti-estrogenic effects and interact with herbs and supplements like red clover or soy.\nBased on preliminary study, constituents in chamomile may alter blood sugar or blood pressure. Patients taking herbs or supplements that affect blood sugar or blood pressure should be cautious.\nChamomile may have anti-inflammatory effects. Theoretically, the use of chamomile with other anti-inflammatory herbs and supplements may have additive effects.\nChamomile may interact with herbs and supplements that act as cardiac depressants, cardiac glycosides, respiratory depressants, or spasmolytics.\nChamomile may also interact with antibacterial, antifungal, antihistamine, or diuretic herbs and supplements, as well as herbs and supplements used for high cholesterol, ulcers, diarrhea, or gastrointestinal disorders.\nThis information is based on a systematic review of scientific literature edited and peer-reviewed by contributors to the Natural Standard Research Collaboration (www.naturalstandard.com).\nNatural Standard Bottom Line Monograph, Copyright © 2011 (www.naturalstandard.com). Commercial distribution prohibited. This monograph is intended for informational purposes only, and should not be interpreted as specific medical advice. You should consult with a qualified healthcare provider before making decisions about therapies and/or health conditions.\nWhile some complementary and alternative techniques have been studied scientifically, high-quality data regarding safety, effectiveness, and mechanism of action are limited or controversial for most therapies. Whenever possible, it is recommended that practitioners be licensed by a recognized professional organization that adheres to clearly published standards. In addition, before starting a new technique or engaging a practitioner, it is recommended that patients speak with their primary healthcare provider(s). Potential benefits, risks (including financial costs), and alternatives should be carefully considered. The below monograph is designed to provide historical background and an overview of clinically-oriented research, and neither advocates for or against the use of a particular therapy.\nThe information in this monograph is intended for informational purposes only, and is meant to help users better understand health concerns. Information is based on review of scientific research data, historical practice patterns, and clinical experience. This information should not be interpreted as specific medical advice. Users should consult with a qualified healthcare provider for specific questions regarding therapies, diagnosis and/or health conditions, prior to making therapeutic decisions.\n- Aertgeerts P, Albring M, Klaschka F, et al. [Comparative testing of Kamillosan cream and steroidal (0.25% hydrocortisone, 0.75% fluocortin butyl ester) and non-steroidal (5% bufexamac) dermatologic agents in maintenance therapy of eczematous diseases]. Z Hautkr 2-1-1985;60(3):270-277.\n- Benetti C, Manganelli F. [Clinical experiences in the pharmacological treatment of vaginitis with a camomile-extract vaginal douche]. Minerva Ginecol 1985;37(12):799-801.\n- de la Torre MF, Sanchez MI, Garcia Robaina JC, et al. Clinical cross-reactivity between Artemisia vulgaris and Matricaria chamomilla (chamomile). J Investig Allergol Clin Immunol 2001;11(2):118-122.\n- Glowania HJ, Raulin C, Swoboda M. [Effect of chamomile on wound healing—a clinical double-blind study]. Z Hautkr 9-1-1987;62(17):1262, 1267-1271.\n- Hormann H, Korting H. Evidence for the efficacy and safety of topical herbal drugs in dermatology: part 1: anti-inflammatory agents. Phytomedicine 1994;1(2):161-171.\n- Ikram M. Medicinal plants as hypocholesterolemic agents. J Pak Med Assoc 1980;30(12):278-281.\n- Konig GM, Wright AD, Keller WJ, et al. Hypoglycaemic activity of an HMG-containing flavonoid glucoside, chamaemeloside, from Chamaemelum nobile. Planta Med 1998;64(7):612-614.\n- Kyokong O, Charuluxananan S, Muangmingsuk V, et al. Efficacy of chamomile-extract spray for prevention of post-operative sore throat. J Med Assoc Thai 2002;85 Suppl 1:S180-S185.\n- Maiche A, Grohn P, Maki-Hokkonen H. Effect of chamomile cream and almond ointment on acute radiation skin reaction. Acta Oncol 1991;30:395-397.\n- Maiche A, Maki-Kokkonen H, Grohn P. [Comparative trial of chamomile cream in radiotherapy]. Suomen Laakarilehti 1991;46(24):2206-2208.\n- Patzelt-Wenczler R, Ponce-P©schl E. Proof of efficacy of Kamillosan cream in atopic eczema. Eur J Med Res 2000;5:171-175.\n- Rycroft RJ. Recurrent facial dermatitis from chamomile tea. Contact Dermatitis 2003;48(4):229.\n- Saller R, Beschomer M, Hellenbrecht D, et al. Dose dependency of symptomatic relief of complaints by chamomile steam inhalation in patients with common cold. Eur J Pharmacol 1990;183:728-729.\n- Weizman Z, Alkrinawi S, Goldfarb D, et al. Efficacy of herbal tea preparation in infantile colic. J Pediatr 1993;122(4):650-652.\n- Wilkinson S, Aldridge J, Salmon I, et al. An evaluation of aromatherapy massage in palliative care. Palliat Med 1999;13(5):409-417.\nCopyright © 2011 Natural Standard (www.naturalstandard.com)\nCopyright © 2011 Natural Standard (www.naturalstandard.com)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:f96a9677-31ce-4ccc-ac22-b0e277fd149c>","<urn:uuid:f9fc6c94-b923-49b6-b31d-6780d8cd7ea0>"],"error":null}
{"question":"What are the recommended mowing heights for bermudagrass and tall fescue, and how do their maintenance requirements compare?","answer":"For bermudagrass, the best mowing height should be no greater than 1.5 inches, while tall fescue should be mowed at 2 inches height. Regarding maintenance, bermudagrass (particularly seeded varieties) has lower water and fertilization requirements than most southern grasses, requiring proper mowing and fertilizing to avoid major disease or insect problems. Tall fescue, on the other hand, has excellent heat and drought tolerance for southern summers, remains green all winter even when dormant, and can be more resilient when planted as a blend of two or more enhanced turf-form varieties.","context":["Following good cultural practices is the primary method for managing insect damage to lawns. Growing appropriate grass species for a particular climatic region and providing lawns with proper care are especially important. Practices such as irrigating and fertilizing have a major impact on lawn health. Practices such as thatch removal, choice of mowing height and frequency, and providing grass with more light by pruning tree branches are also important in certain situations. Naturally occurring biological control agents, such as predators and parasites, may limit some insect pests. Most home lawns in California do not need to be treated with pesticides if proper cultural practices are followed. Pesticides should never be applied unless a pest has been identified, is present at damaging levels, and is present in a susceptible life stage. If pesticide applications are necessary, choose IPM-compatible materials (Table 2) that have minimum impacts on beneficial organisms and the environment.\nThe best way to prevent damage from lawn pests is to keep grass healthy. Healthy lawns require few, if any, pesticide applications. Also, if the turfgrass is under stress and a pesticide is applied, it stands a greater chance of suffering phytotoxic damage from the pesticide itself. The UC Guide to Healthy Lawns on the UC IPM web site and the publications on managing your lawn listed in References give detailed information on how to grow a healthy lawn. Table 1 lists cultural practices important for preventing specific problems.\nThere are a number of grasses available for planting in California. These grasses are often referred to as either cool-season grasses (examples include bentgrass, fine fescue, Kentucky bluegrass, perennial ryegrass, and tall fescue) or warm-season grasses (bermudagrass, kikuyugrass, St. Augustinegrass, seashore paspalum, zoysiagrass, and buffalograss). Warm-season grasses produce most of their growth during summer and usually have a dormant period when they turn brown during winter. Cool-season grasses are green year-round, but they produce most of their growth in spring and fall. The type of grass and the varieties within each type vary in their shade tolerance, salinity tolerance, water needs, disease resistance, and cultural needs. A formerly thriving lawn variety may decline with changes in light, such as more or less shade caused by growth or removal of nearby trees. These factors are outlined in Turfgrass Selection for the Home Landscape. Selection of the appropriate grass species and variety will allow you to grow a hardy lawn with minimal maintenance inputs.\nInappropriate irrigation is the most common cause of lawn damage. Overwatering (shallow, frequent sprinkling) predisposes turf to diseases, retards deep root growth, and increases lawn susceptibility to stress. Poorly maintained sprinklers can apply too much water in certain spots while underwatering other areas. Brown spots from uneven water applications occur frequently and are often caused by improperly spaced sprinkler heads, sunken or tilted heads, or unmatched heads that apply differing amounts of water. Correcting these physical problems with irrigation systems can decrease water waste significantly, decrease water bills, and, most importantly, improve the health of your lawn. Lawns should be irrigated deeply and not more than twice a week.\nAppropriate fertilization encourages a dense, thick lawn that allows grass to tolerate some insect feeding. The appropriate timing and amount of fertilizer (primarily nitrogen) varies depending on factors including season, grass species, and local growing conditions. In general, most California grasses used for lawns require from 2 to 4 pounds of actual nitrogen over a 1,000 square foot area annually during their active growing season. Some native grasses and drought tolerant species like buffalograss and fine leaf fescue may require less. If grasscycling is practiced (leaving lawn clippings on the lawn after mowing, instead of removing), then the lower rate of nitrogen application may be used.\nKeep the blades on your lawn mower sharp and cut your turf at a mowing height appropriate for that type of lawn grass so as to minimize depletion of food reserves needed to outgrow insect injury. Mowing frequency and height depend on grass species, season, and the particular use of that lawn. Cool-season lawns have suggested mowing heights of 1 1/2 to 3 inches, while warm-season lawns should be mowed to a height of 3/4 to 1 inch. No more than 1/3 of the grass height should be removed at one time.\nLawns also benefit from aeration. To increase water penetration and reduce soil compaction, periodically remove soil plugs using hollow tines. In bluegrass and bermudagrass lawns, thatch (the layer of undecomposed organic material on the soil surface) can build up and result in poor water, fertilizer, and air penetration, as well as provide a protected home for many unwanted pests and pathogens. Presently, most California lawns are planted with tall fescue, which is not as susceptible to thatch buildup as other species. Thatch that is greater than 1/2 inch thick encourages caterpillar and chinch bug populations. Thatch also reduces insecticide efficacy because insecticides cannot penetrate to reach root-feeding insects. Prevent thatch by avoiding excess nitrogen application, irrigating deeply and infrequently, and minimizing the use of broad-spectrum lawn pesticides that can reduce populations of microorganisms responsible for decomposing the thatch. If more than 1/2 inch thick, physically remove thatch with a mechanical dethatcher, vertical mower, or power rake. Other methods (although not as effective in reducing thatch) include topdressing lawns by adding a thin layer (1/8–1/4 inch) of soil and raking or sweeping it into the thatch to encourage decomposer microorganisms. Core aerification also mixes soil into thatch, speeding decomposition.","Choosing the best turf grass for your southern lawn depends on two factors: how much sun your lawn will receive, and its geographical position within the warm seasonal grass region. For southern lawns the key grass options are: bermudagrass, St. Augustine, centipedegrass, zoysia, tall fescue, and buffalograss.\nBermudagrass Bermudagrass is a vigorous warm grass of the season that spreads through widespread stolons and rhizomes. This is a common crop in the South and Southwest. It needs full sun, and shade tolerance is very low. There are two basic classes of bermudagrasses — those which can be formed from seed, and those which can be planted only from sprigs or sod. Seeded bermudas like normal, Yuma, Sundevil, and U3 are less expensive to set up, and appear to be cheaper to maintain. The seeding rate goes from 1 to 1 1/2 lbs. By 1000 sq. M. Ft. The best time to start a bermuda lawn from seed is from late spring when temperatures in the night remain above 65 degrees until late summer.\nSodding is the most common method by which a hybrid bermudagrass lawn is formed, although sprigging is also used. Bermuda lawns can be sodded at any time of the year, but the best time is in the summer.\nFor planted bermudagrass the best mowing height is no greater than 1.5 inches. The hybrid types, often used on golf courses, have a lower height of mowing. Bermudagrasses, when properly mowed, fertilized, and fed, have no major disease or insect problems. Seeded bermudagrasses have the lowest water and fertilization requirements of all southern grasses except for buffalograss.\nSt. Augustine St. Augustine is a popular and widely used lawn grass from Florida to California in the tropical coastal areas of the south. Its popularity is primarily due to the southern grasses having the greatest shade tolerance, though it thrives even in full sun.\nSt. Augustine, with a larger blade of leaves than most other lawngrasses, spreads through the stolons. With proper fertilization and moisture, it is fairly easy to establish by sodding or plugging, and will do well in most types of soil. It is however very susceptible to freezing temperatures and killing winter; thus restricting its use in the upper south.\nThe available St. Augustine grasses vary greatly in their resistance to the cold. Before you buy, make sure the one you pick has good winter survival records in your area.\nSeveral different St. Augustine strains are available to provide a green, dense lawn for the homeowner. But only those identified as resistant to a disease called St. Augustine Decline (SAD) should be bought. By planting plugs of the latest SAD-resistant strains, older St. Augustine lawns that have SAD can be strengthened.\nDuring the summer months the best time to sod St. Augustine is when temperatures stay above 65 degrees. Of optimal growth it generally uses more water than bermuda. St. Augustine needs at least 2 inches in sun and 2.5 to 3 inches in shadow.\nZoysiagrass Zoysia isn’t as shade tolerant as St. Augustine, but is much more shade tolerant than bermudagrass. Since zoysia is the southern grass’s most winter-hardy, it does better in the upper southern regions. Zoysia isn’t resistant to drought like bermuda or buffalograss. It is planted most often as sod or as plugs. Zoysia spreads through stolons and rhizomes, and it takes a significant period of time before plugs form a complete cover.\nThere are typically two core forms of zoysia available: Emerald and Meyer. Emerald has a leaf blade much finer than Meyer, and can form a rather dense, dark green lawn. Meyer’s leaf blade is average textured like a typical bermudagrass, but can also form a thick green lawn.\nMeyer is usually more resistant to drought than Emerald although both are well suited to dry conditions. Thatch growth can be a major problem with zoysia grasses but with proper mowing, water, and fertilization, it has no serious disease or insect problems. The optimum height for mowing is about 1.5 inches.\nBuffalograss This is the only turfgrass from Texas to Canada originating in the North American great plains. This is a warm turfgrass of the season, spreading through stolons.\nBuffalograss has fine blades with a blue-green colour. It won’t form turf as thick as other grasses in the South. Buffalograss is a growing interest for low-maintenance lawn areas. It can survive extreme drought conditions, has low fertility requirements, and when left unmowed, it will usually not grow by more than 4 to 5 inches. Yet buffalograss has little resistance to the darkness.\nSome recent varieties of buffalograss, such as Prairie, Buffalawn, and 609 are only available as sod. A increasing number of varieties known as seeds are available, including Comanche, Texoka, Plains and Topgun. Buffalograss isn’t suitable for everyone, nor for many lawn areas. But its drought tolerance and low maintenance requirements in the upper south and semi-arid regions have increased interest among many homeowners. The best height to mow is 2 inches.\nTall Fescue While turf form tall fescue is generally considered a northern, cool seasonal grass, its use is increasing in the upper south, especially on well shaded lawns instead of St. Augustine’s. For the southern summers, tall fescues also have excellent heat and drought tolerance and can withstand cold winter temperatures with very few issues.\nTall fescue is a common species because it remains green, even when dormant, all winter. Using two or more enhanced tall fescues of the turf form as a blend also provides a heartier lawn than using a single older fescue variety such as K-31.\nTall fescue is a bunch of grass, and is usually formed by seeding. Fall or early spring is the best time of the year to plant tall fescue. The general rate of seeding is 8 livres. By 1000 sq. Ft., but follow the recommendations of the manufacturer set out in the box or bag. For tall fescue the optimal mowing height is 2 inches.\nCentipedegrass Centipedegrass is well suited in the south and upper south to most soils and climatic conditions, but is not so well adapted to the more and western regions of the Mississippi. The leaf blade is of a medium texture and forms a thick, fine, low-growth turf. Centipede can be formed either by seed or vegetative sod planting, and it spreads against the soil by stolons which lie flat. Sowing 1 lb at a time. By 1000 sq. Ft. It is fairly cold tolerant, and usually works well in changing areas of shade. Centipedegrass is considered a low-maintenance grass and typically produces a lawn of lesser quality than bermudagrass or St. Augustine. It does not require frequent mowing but 1.5 inches is the best mowing height.\nIn the south, many homeowners use improved perennial ryegrass for supervised bermudagrass, which keeps the lawn green throughout the winter. In the upper south, the best time to be supervised is during the first half of September. The last half of September into the first part of October is the best time in the lower South regions. Ideally, perennial ryegrass should be planted around 6 to 8 weeks before the first heavy frost’s average date, and dormant before the bermudagrass goes. The lowest seeding rate throughout the winter is 3 lbs, just to give the lawn a little green colour. By 1000 sq. 5. Ft. Nevertheless use 8 to 10 lbs. By 1000 sq. Ft. to keep the green lawn relatively dense during the winter months.\nSupervision requires no special equipment other than seed spreader or fertilizer spreader. The biggest drawback potential to supervise with perennial ryegrass is that some plants may persist as the weather turns warmer into the summer. In the Bermuda lawn the ryegrass then becomes weeds. Low mowing and less water promote bermudagrass over ryegrass, or use a post-emergent herbicide to control ryegrass.\nWhen combined, the southern grasses produce no quality lawns. Because they are distributed by stolons, mixed varieties appear to segregate each variety and form distinct patches. Choose from the southern grasses that best suit your geographic area and unique lawn conditions, i.e., sunny, cool, damp or arid. Then determine whether to use seed, sod or sprigs. Doing yourself can be a lot of work but a nice, well-kept lawn is worth it! It’s going to benefit your house, family and the world.\nMake sure you buy seed of quality!\nBe cautious about low-cost seed mixtures, as a homeowner. They also contain grasses normal, unadapted, temporary and of low quality. Better seed varieties can generally cost a little more but the potential for better lawn quality lies within that crop.\nRead the seed label and see what you get inside the box or bag."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7d454374-9e20-4517-8da3-5d1ba6ae751b>","<urn:uuid:ad59b294-d9a7-4477-ad45-04a2325e20d0>"],"error":null}
{"question":"I've heard conflicting theories about dinosaur extinction - was it solely caused by the Chicxulub crater impact in Mexico, or were there other significant factors involved?","answer":"The extinction wasn't caused by a single factor, but rather by multiple significant events. The Chicxulub crater impact in Mexico was indeed substantial, creating a 180km diameter crater from a 10km object. However, there was potentially an even larger impact called Shiva in India, creating a 500km crater that may have been 100 times more powerful than Chicxulub. Additionally, there were massive volcanic eruptions forming the Deccan Traps in India. The extinction was likely caused by a combination of these events - the meteorite impacts and the volcanic activity. Furthermore, dinosaur biodiversity had already decreased by about 40% from its peak during the mid-Cretaceous before these events. It's also worth noting that not all dinosaurs went extinct - birds, which are dinosaurs, survived and still exist today.","context":["Stephen Jay Gould riffed on this theme in his essay \"The Case of the Creeping Fox Terrier Clone,\" which appeared first in Natural History and was later published in his essay collection Bully for Brontosaurus. He looks at the claim that an early horse species, Hyracotherium, was \"the size of a fox terrier\" -- something that Gould found quoted in dozens of books on prehistoric animals (and which has therefore been used as a gauge of the animal's size in countless classrooms). It turns out that it originated with a paleontologist, O. C. Marsh, who said Hyracotherium was the \"size of a fox\" -- a significant underestimate, as both foxes and fox terriers top out at around twenty pounds, and Hyracotherium weighed in at something closer to sixty. But the analogy stuck, and people continued to pass it along without checking its veracity -- giving us the impression of tiny dog-sized horses, lo unto this very day.\nAnother example of this, from the same field, is that mammals were small, few in number, and low in biodiversity until along came a meteorite that for some reason selectively killed all the dinosaurs, leaving the mammals to throw a great big party and evolve like mad into the species we have around today. This is incorrect on a variety of levels:\n- The K-T (Cretaceous/Tertiary) Extinction of 66 million years ago seems to have been caused by a double whammy -- the aforementioned meteorite, which left the Chicxulub Crater in what is now the Gulf of Mexico, and the formation of the Deccan Traps, a lava field from a colossal supervolcano eruption, all the way around the Earth in what is now India.\n- Dinosaur biodiversity had been decreasing for some time before the K-T Extinction, and in fact by some estimates was already down 40% from its peak during the mid-Cretaceous.\n- ...however... All the dinosaurs didn't go extinct 66 million years ago, and I'm not talking about Nessie, Ogopogo, and Mokélé-Mbembe. We still have dinosaurs around, we just call 'em birds. The evidence is now incontrovertible. Think about that next time you're putting out sunflower seeds for the chickadees.\n- The extinction hit pretty much every taxon that existed at the time. The hardest-hit were large carnivores -- a vulnerable spot in the food chain at the best of times -- but no one escaped unscathed. In fact, one group that got wiped out completely were the ammonites, a cephalopod mollusk that had thrived for 350 million years before getting clobbered during the K-T Extinction.\n- Most pertinent to this post, the mammals weren't just skulking around waiting for their opportunity; they'd been thriving alongside dinosaurs since the Triassic Period, 154 million years earlier. This was the topic of a paper released a couple of months ago in Biology Letters by Tiago Bosisio Quental of the University of São Paulo and Mathias Pires of the University of Campinas.\nA late Cretaceous multituberculate, Catopsbaatar [Image licensed under the Creative Commons, Artwork by Bogusław Waksmundzki. Article by Zofia Kielan-Jaworowska and Jørn H. Hurum, Catopsbaatar, CC BY 2.0]\nEven within groups, the extinction didn't have uniform effects. \"Extinctions were concentrated among the specialized carnivorous metatherians and insectivorous eutherians,\" Pires said, \"whereas more generalized eutherians and multituberculates survived and maintained higher diversity.\"\nHe added, \"This means that studies of macroevolutionary phenomena focusing on broad taxonomic groups may miss a much richer macroevolutionary history, which can be perceived only at finer taxonomic scales.\"\nWhich can more generally be summed up as \"the simple explanation is usually wrong.\" It'd be nice if things weren't so complex, especially for we non-scientists. But like Gould's fox-terrier-horse, many of these oversimplifications are flat-out incorrect -- and the truth is so much more interesting.****************************************\nThis week's Skeptophilia book recommendation is one of personal significance to me -- Michael Pollan's latest book, How to Change Your Mind. Pollan's phenomenal writing in tours de force like The Omnivore's Dilemma and The Botany of Desire shines through here, where he takes on a controversial topic -- the use of psychedelic drugs to treat depression and anxiety.\nHallucinogens like DMT, LSD, ketamine, and psilocybin have long been classified as schedule-1 drugs -- chemicals which are off limits even for research except by a rigorous and time-consuming approval process that seldom results in a thumbs-up. As a result, most researchers in mood disorders haven't even considered them, looking instead at more conventional antidepressants and anxiolytics. It's only recently that there's been renewed interest, when it was found that one administration of drugs like ketamine, under controlled conditions, was enough to alleviate intractable depression, not just for hours or days but for months.\nPollan looks at the subject from all angles -- the history of psychedelics and why they've been taboo for so long, the psychopharmacology of the substances themselves, and the people whose lives have been changed by them. It's a fascinating read -- and I hope it generates a sea change in our attitudes toward chemicals that could help literally millions of people deal with disorders that can rob their lives of pleasure, satisfaction, and motivation.\n[If you purchase the book from Amazon using the image/link below, part of the proceeds goes to supporting Skeptophilia!]","EVERYONE knows that the dinosaurs were exterminated when an asteroid hit what is now Mexico about 65m years ago. The crater is there. It is 180km (110 miles) in diameter. It was formed in a 100m-megatonne explosion by an object about 10km across. The ejecta from the impact are found all over the world. The potassium-argon radioactive dating method shows the crater was created within a gnat’s whisker of the extinction. Calculations suggest that the “nuclear winter” from the impact would have lasted years. Plants would have stopped photosynthesising. Animals would have starved to death. Case closed.\nWell, it now seems possible that everyone was wrong. The Chicxulub crater, as it is known, may have been a mere aperitif. According to Sankar Chatterjee of Texas Tech University, the main course was served later. Dr Chatterjee has found a bigger crater—much bigger—in India. His is 500km across. The explosion that caused it may have been 100 times the size of the one that created Chicxulub. He calls it Shiva, after the Indian deity of destruction.\nDr Chatterjee presented his latest findings on Shiva to the annual meeting of the Geological Society of America in Portland, Oregon, on October 18th. He makes a compelling case, identifying an underwater mountain called Bombay High, off the coast of Mumbai, that formed right at the time of the dinosaur extinction. This mountain measures five kilometres from sea bed to peak, and is surrounded by Shiva’s crater rim. Dr Chatterjee’s analysis shows that it formed from a sudden upwelling of magma that destroyed the Earth’s crust in the area and pushed the mountain upwards in a hurry. He argues that no force other than the rebound from an impact could have produced this kind of vertical uplift so quickly. And the blow that caused it would surely have been powerful enough to smash ecosystems around the world.\nIn truth, agreement on the cause of the mass extinction at the end of the Cretaceous (when not only the dinosaurs, but also a host of other species died) has never been as cut and dried among palaeontologists as it may have appeared to the public. One confounding factor is that the late Cretaceous was also a period of great volcanic activity. In India, which was then an island continent like Australia is today (it did not collide with Asia until 50m years ago), huge eruptions created fields of basalt called the Deccan Traps. Before the discovery of Chicxulub, the climate-changing effects of these eruptions had been put forward as an explanation for the death of the dinosaurs. After its discovery, some argued that even if the eruptions did not cause the extinction, they weakened the biosphere and made it particularly vulnerable to the Chicxulub hammer-blow.\nThere are also puzzling anomalies in the pattern of extinction. The greatest of these is that, as fiery and horrible as the impact would have been, the survivors included many seemingly sensitive animals like birds, frogs and turtles. Moreover, close inspection of the fossil record shows that many “Cretaceous” species disappear both well before, and well after, the signs of the impact that are found in the rocks.\nIronically, it was while he was investigating the Deccan Traps that Dr Chatterjee came across the evidence for Shiva. First, he found dinosaur nests that had been built between lava flows 10-15 metres thick—evidence that the animals were coping well with the volcanic activity rather than being weakened by it. Then, quite suddenly, 65m years ago, a layer of lava nearly 2km thick appears. This led him to wonder what could possibly have caused such a sudden volcanic surge.\nHe knew that the west coast of India had been the site of an ancient impact of unknown age and size. It was not until he was reading through a paper published by an oil company that had collected geological information in the area that he realised the volcanic surge he had seen might be related to a cosmic collision.\nFurther examination revealed a crater rich in shocked quartz and iridium, minerals that are commonly found at impact sites. (These are also the telltales in distant layers of ejecta that the rock in question has come from an impact.) Most important, the rocks above and below Shiva date it to 65m years ago. Dr Chatterjee therefore suggests that an object 40km in diameter hit the Earth off the coast of India and forced vast quantities of lava out of the Deccan Traps. As well as killing the dinosaurs the impact was, he proposes, responsible for breaking the Seychelles away from India.These islands and their surrounding seabed have long looked anomalous. They are made of continental rather than oceanic rock, and seem to be a small part of the jigsaw puzzle of continental drift rather than genuine oceanic islands.\nRead the full story here."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"current"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:728ede4d-ec7a-4281-971f-49078699d64f>","<urn:uuid:d355b7a3-83ff-465e-b957-8451a3e1521a>"],"error":null}
{"question":"As an environmental protection advocate, I'd like to know what types of land acquisitions are used for conservation and how much funding was recently allocated in California?","answer":"Conservation land acquisitions occur through various methods, including outright purchases, donations, trades, and bargain sales, as well as partial interest transfers like development rights and easements. In California specifically, the Wildlife Conservation Board recently approved $23.5 million in grants in May 2021. Examples include a $950,000 grant to acquire a conservation easement over 2,476 acres in Lassen County and a $2.56 million grant to acquire 320 acres near Chino Hills for protecting various wildlife species and habitats.","context":["Wildlife Conservation Board Funds Environmental Improvement and Acquisition Projects\nAt its May 20, 2021 quarterly meeting, the Wildlife Conservation Board (WCB) approved approximately $23.5 million in grants to help restore and protect fish and wildlife habitat throughout California. Some of the 35 approved projects will benefit fish and wildlife — including some endangered species — while others will provide public access to important natural resources. Several projects will also demonstrate the importance of protecting working landscapes that integrate economic, social and environmental stewardship practices beneficial to the environment, landowners and the local community.\nFunding for these projects comes from a combination of sources including the Habitat Conservation Fund and bond measures approved by voters to help preserve and protect California’s natural resources.\nFunded projects include:\n- A $1.5 million grant to Save the Redwoods for a cooperative project with the California Department of Parks and Recreation and the National Park Service to enhance forest health and reduce hazardous fuels through selective thinning on 1,000 acres of mixed conifer forest and four miles of road removal in Redwood National and State Parks in Del Norte and Humboldt counties.\n- A $950,000 grant to the Trust for Public Land for a cooperative project with the Sierra Nevada Conservancy to acquire a conservation easement over approximately 2,476 acres of land for the protection of significant wet meadow and montane riparian habitat and fish and wildlife resources near the town of Westwood in Lassen County.\n- A $1.58 million grant to California Waterfowl Association for a cooperative project with the California Department of Fish and Wildlife (CDFW) to restore 680 acres of wetland habitat and 169 acres of upland nesting habitat within the CDFW Grizzly Island Wildlife Area in Solano County.\n- A $748,000 grant to San Mateo Resource Conservation District for a cooperative project with the City and County of San Francisco, California State Coastal Conservancy and Natural Resource Conservation Service to remediate a dam and fish ladder that block the passage of coho salmon and steelhead trout to 4.5 miles of upstream habitat on Mindego Creek, 21 miles southeast of the city of Half Moon Bay in San Mateo County.\n- A $2.56 million grant to the Mountains Recreation and Conservation Authority to acquire approximately 320 acres of land for the protection of oak woodlands, chaparral, coastal sage scrub, cactus scrub and grasslands that support numerous wildlife species including least Bell’s vireo, southwest willow flycatcher, yellow warbler, coastal cactus wren and other sensitive species located near Chino Hills in San Bernardino County.\n- A $925,111 grant to the University of California, Santa Barbara to upgrade the wastewater treatment system and install two new prefabricated shipping container homes and associated utilities at the Santa Cruz Island Reserve on Santa Cruz Island in Santa Barbara County.\n- A $1.42 million grant to Rivers and Lands Conservancy (RLC) and the acceptance of a U.S. Fish and Wildlife Service Habitat Conservation Plan Land Acquisition grant, and the approval to subgrant these federal funds to RLC to acquire approximately 98 acres of land for the protection of habitat for the coastal California gnatcatcher, cactus wren, intermediate Mariposa lily, Coulter’s matilija poppy and other federally listed species as well as other sensitive species located in the community of Silverado in Orange County.\nFor more information about the WCB please visit wcb.ca.gov.\nJohn Donnelly, Wildlife Conservation Board, (916) 445-0137\nAmanda McDermott, CDFW Communications, (916) 322-8907\nMountain Meadows. Photo by Markley Bavinger, provided by The Trust for Public Land.","Property Acquisition and Easements\nConservationists, including conservation commissions and agencies, agricultural land trusts, watershed associations, parks and open space groups, and traditional land trusts, regularly acquire land or partial interests in land to protect it. Wetlands protection laws and the framework of the National Environmental Protection Act track the general outlines of historic preservation laws at the local and federal levels, but a large proportion of conservation activity in the country is dedicated to private, voluntary land transactions.\nSome of these deals are outright purchases or acquisition through donation, trade, or bargain sale, while many are transfers of partial interests, including development rights or easements. When a conservation organization acquires a site outright, they might make it into a publicly accessible preserve or park, or lease it to a sensitive user, or resell it after putting long-term protections in place. Easements are a tool that can be used to protect natural, historic, cultural, scenic, agricultural, or social values, and they can be created to last for perpetuity or for a term of years. For information on what an easement is, what it can do, and how changes in the federal tax code affect deductibility of easement donations, read our easements overview.\nFor preservationists, saving a place through acquisition can mean everything from attracting private investment from a preservation developer for rehabilitation and reuse, to a preservation organization taking ownership and operating the site as a museum or other community use, to acquisition of a preservation easement or development rights.\n- In the conservation and preservation contexts, acquisition of full or partial property interests gives an organization or agency the capacity to protect the site from inappropriate or destructive changes.\n- As a voluntary tool, acquisition can usually occur without a public vote.\n- It is a flexible tool that can combine protection of many values, and can be configured in a wide range of structures.\n- This approach gives organizations a very tangible stake in the community and the cause.\n- These real estate interests can be a liability to an organization, creating lingering obligations, and any deal presents inherent risks. Protecting land this way requires considerable legal and other professional assistance.\n- When accepting donations of real estate interests, it is important for organizations to ensure that the acquisitions is strategic and advances well-conceived protection priorities.\nComprehensive Protection and Real Property Interests\nBecause conservation organizations and agencies often use the acquisition approach, it is fairly common for them to end up with a stake in a property that contains historic resources, along with the natural and/or agricultural resources that drew them into the protection project. Most conservationists could really use some preservation help now and the, whether a land trust is struggling with planning for a small historic bridge in their preserve's trail system, greening their historic downtown headquarters, or trying to determine how to deal with a donor's wish that a historic cemetery is preserved as part of a conservation easement project. Sometimes, this means a phone call to the local historic commission or State Historic Preservation Office. Other times, retaining a preservation professional can solve their problem, or a formal partnership with a preservation nonprofit is in order.\nThe most meaningful assistance of all is often the most basic – any preservationist can:\n- create relationships with conservationists\n- help them understand and get excited about historic resources their work could affect\n- connect them with resources that can spark good planning for assessment, preservation, use, and/or maintenance\nBuilding a relationship with your local or regional land trust and serving as a resource to them can greatly help facilitate protection of historic resources as part of land conservation projects.\nPreservationists can also work to save whole places through their own management of real property interests. Historic sites are having great success in ensuring lasting protection of their landscapes through transfer of conservation easements to land trusts. Often, they are partnering with land trusts to acquire conservation easements on important viewsheds beyond the bounds of the site, as well. Ideally, a preservation organization negotiating an easement donation on a historic home would also identify opportunities to connect the owner with an agricultural agency to keep a farm in production, or with a land trust, for an easement on sensitive habitat.\nIn many places, preservationists and conservationists have forged alliances, formal or informal, adding dimension to their ownership and easement projects. Models for these kinds of partnerships abound. In order to make sure that assets aren't destroyed or left vulnerable, we need strong relationships between the conservation and preservation communities, at all levels.\n- Find Funding page\n- Find a Statewide and Local Partner\n- Find a land trust\n- Secretary of the Interior's Standards for the Treatment of Historic Properties\n- Basic Preservation\n- National Trust Easement Resources\n- Land Trust Standards and Practices\n- Best Practices for Preservation Organizations Involved in Easement and Land Stewardship\n- Establishing and Operating an Easement Program\n- Buying Time for Heritage: How to Save an Endangered Historic Property J. Myrick Howard\n- Conservation Easements Handbook Land Trust Alliance\n- Doing Deals Land Trust Alliance"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:5ef06125-9109-4a54-aefd-556b937c8e94>","<urn:uuid:f6c31d02-13c6-4ebf-807f-d300020b7e4c>"],"error":null}
{"question":"What are the key benefits of Transportation Spend Management (TSM) and what security risks must it address?","answer":"Transportation Spend Management provides benefits including real-time visibility across operations, improved performance measurement, and optimization of spend across supply chain stages from raw materials to reverse logistics. However, it must address significant security risks, particularly in third-party interactions, as 56% of companies have experienced cybersecurity breaches from suppliers. Companies implementing TSM need to incorporate security protocols like third-party risk assessments and clear data ownership requirements to protect against cyberattacks, theft, and fraud that could disrupt operations.","context":["Managing Total Cost to Serve and Delivering the Win\nIn the most constrained and supply-challenged global transportation logistics landscape the world has likely ever seen, one thing is certain: it's more essential than ever to manage performance, control cost and, ultimately, optimize your operation’s total cost to serve.\nOptimizing the delicate balance of performance vs. cost across your entire operation is a strategic imperative. The principles, practices and outputs of a quality, data-driven transportation spend management (TSM) program will enable the accurate measurement of spend, assessment of performance across all operational channels, and lead to final spend optimization. TSM, when coupled with a real-time event tracking solution, will give any executive charged with understanding how their business is performing, and managing the spend to do so, with enterprise-wide visibility to make strategic, data-informed decisions that deliver success.\nHere are the steps required for best-in-class implementation for total cost to serve management:\nUnderstanding the Flow of Goods and Information\n- Flow of goods: Are our manufactured products reaching distribution and then retail on time, intact and was it done efficiently? What factors can interrupt this flow or make things more efficient between points A and B?\n- Flow of information: Do we have real-time data accessibility across the operation?\n- Financial flow: The flow of money in terms of payments and the effective process to make them.\nTSM's Role in Managing Supply Chain Stages\nBeyond managing goods, data and financials across your network, in a typical supply chain there are five legs, each incurring different levels of cost and each with their own processes, challenges and costs. The legs include:\n- Raw materials to manufacturing\n- Site to market\n- Market to trade consumer\n- Distribution center to retail or direct to consumer\n- Reverse logistics (returns)\nAs you and your team or partners work to use TSM data and practices to improve efficiencies across each leg, to look at cost reduction, performance improvements and cost-to-time tradeoffs, it’s also important to understand the nuances of different legs. This is particularly crucial with the three major stages of the supply chain: inbound, outbound and reverse logistics.\nWith inbound it’s essential to have visibility into all shipping activity and their associated costs. This can be demanding, complex and data-intensive, especially with multiple suppliers and shipments from all over the country or world. For every one of these inbound shipments from every supplier, especially if you're liable for the transportation costs, compliance is essential in terms of shipment routing and the provision of ASNs (advance ship notices) to properly manage the process. When an inbound shipment goes wrong there's rapid escalation. Cost to serve goes up every time there's disruption, but having the data and visibility to predict, see and react to problems can help manage cost.\nOutbound is all about standardizing and maintaining internal compliance while delivering flawlessly to customers or internal stakeholders who rely on you for consistent performance and delivery. If a major retailer doesn’t receive your flat-screen TV or in-demand plushy for Black Friday, both your orders and reputation will suffer.\nWhile the positive of outbound logistics of course is that you have the benefit of having more control over how those goods are shipped, outbound logistics can still present a challenge where the supply chain is decentralized. The recipients of your goods also expect on time and in full delivery — meaning perfect order, on time, in the right place with the right level of service and quality specified. Every time. If you don’t receive raw materials to manufacture the goods to ship your products on time, if shipments get delayed en-route, if a sales order from a customer comes in late and requires expedited shipping — all of these factors will increase cost to serve and lower profitability if allowed to continue over time. Outbound shipping also often requires less than truckload (LTL) or parcel modes of transportation which means a higher cost per unit weight and increased cost to serve overall, especially if using expedited services.\nPriorities here include creating and optimizing control over the method of how returns are processed and inventory is handled.\nThe Need for Options to Improve Performance\nWhatever leg of the supply chain or region you're attempting to manage and optimize over time, it can’t be done without having options — especially if management of those regions and legs is decentralized. Have a solid plan B when market variables or events derail plan A. You can be one of the fastest growing companies in the world with an extensive supply chain operation, but it won’t shield your business from delays and missed deadlines.\nUsing TSM to Manage Typical Global Supply Chain Challenges\nAs any transportation logistics executive begins the implementation of TSM practices you should look to your strategies and processes to begin addressing the typical supply chain challenges that impact cost to serve:\n- Cost management\n- Performance and cost visibility\n- Information, analytics and systems transportation management systems and event stream processing (ESP) systems\n- People and processes\n- Cost and service improvement\nIntegrating TSM Into a Mature Control Tower Ecosystem Strategy\nMature transportation logistics companies are increasingly adopting best-in-class control tower strategies and processes, where all systems are interconnected and informed by a master data management protocol. TSM has an important role to play in this kind of approach or strategic ecosystem. Working within an overall transportation management system and alongside functions like freight audit and pay (FAP), TSM is capable of improving strategic procurement of materials (best options, best prices, best deliveries); carrier management (using a scorecard system for monitoring and assessing performance against set goals); and managing invoices and payments in concert with your FAP system, sustainability, overall performance and more.\nWherever your business is on its transportation logistics maturity journey from initial launch to implementation across your global network, TSM can help. Adoption will speed the maturity of your operation, create interconnected data and financial flows, and optimize performance to deliver improved cost to serve for you and improved service delivery for your customers.\nRelated story: Your Guide to Managing Carriers and Spot Quotes\nChris Cassidy is the executive vice president of global sales & strategic partnerships for Trax Technologies, the global leader in Transportation Spend Management (TSM) solutions for Freight Audit and Payment (FAP) services.\nBefore joining Trax, Cassidy served as a leadership partner with Gartner, where he advised enterprise supply chain leaders on their Supply Chain strategies in the healthcare and life science industry. He also led the global healthcare logistics strategy for UPS, leading to the creation of a dedicated UPS Healthcare business unit managing $6B final-mile spend and leading to the launch of a new dedicated UPS Premier product. Additionally, he served as head of global logistics at GlaxoSmithKline (GSK) managing over $1B transportation and warehouse spend, among other various operations, IT and supply chain management and mfg. packaging roles during his 12 year tenor.\nCassidy is an accomplished, trusted and resourceful end-to-end supply chain logistics & transportation leader to SaaS tech, logistics organizations globally.\nHe is a graduate of the Georgia Institute of Technology with a degree in Industrial Engineering (ISyE). Cassidy has lived abroad in Japan, London UK, and now lives in metro Atlanta, GA with his wife and three children","Maintaining a secure supply chain can be a challenge. Within a supply chain, numerous people, companies, third-party suppliers, and nations are likely involved, which can lead to serious security issues in logistics.\nMany companies depend on their own resources as well as industry standards to fight security issues. Depending on the industry, companies can be subject to security issues in logistics such as cyberattacks, theft, fraud, terrorism, sabotage, and more. A 2018 Ponemon Institute report found that 56% of companies suffered a breach caused by one of their third-party vendors. Although breaches in a supply chain are common, many companies remain unprepared.\nWhy is supply chain security important?\nAttacks to the supply chain are twofold. The first is usually meant to disrupt or cripple actual supply chains logistics. The second is to use supply chains as a channel to attack potentially thousands of connected partners and suppliers. By finding and exploiting weak links within the supply chain, attackers can jump between linked systems, stealing data and sabotaging businesses.\nTech, defense, financial services, and energy are favorable targets for hackers, but no industry is immune. In fact, there are now what experts call “island hoppers” who aren’t just attacking one organization, but instead multiple.\nA security issue within the supply chain should be a high priority for companies considering any incident or breach could greatly damage or disrupt operations; vulnerabilities could lead to unintended costs, inefficient delivery schedules, and a loss of intellectual property.\nThreats to supply chain security are on the rise across all industries, with nearly two-thirds of companies reporting a breach—spiking 78% last year-over-year. The top threat: cyberattacks. In fact, a recent report found that 56% of respondents have experienced a cybersecurity breach from a third-party supplier. But risks can occur at all stages—design, development and production, distribution, acquisition and deployment, maintenance, and disposal.\nOnce an attack occurs and disrupts the supply chain, a company’s reputation can be damaged. From that point on, it can sow doubt into consumers’ minds about reliability, productivity, and their security, including financial and personal information.\nHow can companies protect themselves from security issues in logistics?\nCompanies must be proactive in developing defense protocols and standards for supply chains. To ensure your company supply chain is secure, the proper management and safety regulations need to be put in place. For the majority of organizations, poor risk management is often a result of resource scarcity and ineffective processes.\nAs most breaches are caused by third parties, third party risk assessments—along with an efficient vetting software for suppliers—can reduce the threat of security breaches in a supply chain. Organizations should identify and categorize suppliers who either have direct access to their internal network or access to critical enterprise data.\nSegregating third-party vendors according to their functional role can also help organizations better understand who these contributors are, the type of information they have access to, and how they are connected to the company framework.\nAdditionally, companies need to define specific data ownership requirements for every vendor. All the stakeholders across the value chain need to be clear as to who maintains the ownership of data being shared and the acceptable use of that data—doing so can greatly improve security issues in logistics."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:1e4910d8-4b00-4a76-9a7e-c68897504648>","<urn:uuid:17e41b2a-b79c-4a5e-a244-7b4e4d21da41>"],"error":null}
{"question":"What construction strategies are used at Carleton University and UBC's geological school to minimize environmental impact?","answer":"At Carleton University, construction focuses on bird-friendly design elements, with visual markers (decals) applied to glass surfaces and bird-friendly glazing incorporated into new construction and renovation projects. At UBC's geological facility, they employ strategies like using the 'Bigfoot' system of pre-molded bell-shaped footings for minimal excavation, consolidating buildings to reduce the development footprint, and utilizing locally produced wood materials with corrugated galvanized metal panels for cladding.","context":["Carleton University is committed to protecting and enhancing the ecosystems and green spaces that the university owns, manages, or impacts, in order to improve biodiversity and support a landscape that can respond to future climate change.\nThe Carleton Campus is fortunate to be located in a very natural and green setting, bordered by the Rideau Canal and Rideau River. Students have long identified it as one of the most remarkable parts of the Carleton experience.\nBiodiversity in Campus Plans and Policies\nCarleton University’s land use and management of the 62-hectare campus impacts the local ecosystem through the use of water, campus development and construction, landscaping applications (such as fertilizers in the summer or salt in the winter), and stormwater management. The university has a policy for integrated pest management and has not used chemical pesticides on the grounds for at least five years.\nOutdoor Space Master Plan\nCarleton University developed an Outdoor Space Master Plan in 2020 to help us better plan and develop the outdoor spaces on the campus. This document will assist the university in identifying and prioritizing outdoor improvements to the campus, such as:\n- Sidewalks, paths, and trails across the campus to enhance safety and access for pedestrians and cyclists\n- Future open space projects to develop green space\n- Renovations to existing open spaces to make them more welcoming and multi-seasonal\n- Development of a standard for street furniture and lighting\n- Indigenous-focused elements\n- New pedestrian bridge connection to Vincent Massey Park\nCampus Master Plan\nCarleton University has undergone significant changes since the adoption of the first campus master plan in 2010, including the construction of Richcraft Hall, Canal Building, and the new Health Sciences building. The purpose of the current master plan is to establish policies, parameters, and directions for the physical development of the campus, including green spaces. READ MORE\nCarleton is committed to protecting and enhancing biodiversity on its campus. To prevent bird collisions, visual markers (decals) have been applied to the exterior surface of the glass on multiple buildings on campus. It is also now a standard in our design guidelines to incorporate bird-friendly glazing on new construction and major renovation projects involving the exterior of the building.\nCurrent Buildings with Bird-Friendly Glazing\n- Engineering Design Centre (EDC) – extension on Mackenzie Building\n- Health Sciences Building\n- Nicol Building\n- Pigiarvik (ᐱᒋᐊᕐᕕᒃ)\n- Southam Hall\nCarleton has also partnered with SafeWings Ottawa to report bird collisions and work towards campus-wide data and identifying key buildings for additional visual markers. If you observe a bird collision, you can report it through the SafeWings Ottawa website or email Sustainability Carleton with your concerns.\nNature Positive University\nIn 2022, Carleton joined other universities around the world in a pledge to become Nature Positive. Being a Nature Positive university means “restoring species and ecosystems that have been harmed by the impacts of a university and its activities and enhancing the university’s positive impacts on nature” (Becoming Nature Positive).\nAs part of our commitment to the Nature Positive Universities program, Carleton is working towards assessing a biodiversity baseline and setting specific, measurable targets towards enhancing biodiversity on campus and becoming Nature Positive. We will also report on our progress annually within our comprehensive sustainability reports, which can be found here.\nFor more information about the Nature Positive Universities program, click here.\niNaturalist Biodiversity Inventory\nDo you want to help play a part in developing Carleton’s biodiversity inventory?\nJoin our iNaturalist project and submit your observations of species you find on campus.","ARCHITECT BattersbyHowat Architects Inc.\nLOCATION Oliver, British Columbia\nThe facilities for the University of British Columbia (UBC) Geological Field School are located at the edge of an 80-acre parcel of land near Oliver, British Columbia, adjacent to the White Lake Grasslands Protected Area and the Susie Mine. The site falls within the Southern Okanagan Basin Ecosection, characterized by dramatic rock outcroppings, dry grasslands and open pine forests.\nThe intention is to build 10 new buildings to replace the existing camp buildings that are reaching the end of their lifespan and to augment the facilities for increased capacity while better facilitating course instruction. The new camp consists of a dining and lecture hall combined with a map-study cabin, as well as sleeping and bathing facilities for students and faculty. The Geological Field School will be used by UBC Earth and Ocean Science students, faculty and staff for two weeks in May and will accommodate approximately 90 people.\nDesign investigations led to a substantial reconsideration of the project scope both programmatically and architecturally. The total number of structures containing the facility program was reduced from 13 to 10. Sleeping accommodations were consolidated into larger bunkhouses and washing facilities were centralized. In addition, the teaching facilities and dining hall were consolidated to create a more efficient and physically substantial central structure. This programmatic modification allowed for a reduction in the development’s footprint and the consolidation of services while minimizing physical disruption of this site’s sensitive ecology.\nTaking the thought of minimizing the site impact further, one of the main design components is the use of canted walls throughout the project to minimize the buildings’ footprint at grade without compromising interior volumes. These canted walls do not carry roof loads but they allow sleeping bunks to be staggered, opening up what might otherwise feel like cramped sleeping quarters. The canted wall meets the roof at its outer edge eliminating the requirement for soffits resulting in less area to be finished and ultimately maintained.\nThe new structures remain in the vicinity of the replaced camp buildings to minimize the impact on the site and to maintain existing utilities. The dining and lecture hall plays a central role, located near the entrance of the property and across from the communal fire pit with views to a seasonal creek and open field beyond. Conjoined with the dining/lecture hall is the map cabin. These two pieces of program were brought together with a covered exterior deck that can be used for communal and social gathering purposes. The resultant building mass provides the necessary weight to identify this as the facility’s centre while also creating an outdoor classroom to augment the indoor facilities. This covered deck space also acts as an oversized porch, reinforcing the essential communal nature of the “camp and campus” experience and atmosphere.\nOn a smaller scale, clusters of sleeping cabins also incorporate social entry porches, and are placed along a meandering and permeable pedestrian path. The cabins are oriented to maximize interaction and exposure to neighbouring cabins. Wash houses are centrally located between the clusters to allow for short travel distances.\nFurther strategies were employed to reduce the site impact. The structures are founded on concrete piers and isolated pad footings with a system called “Bigfoot.” The footing forms are pre-molded bell-shaped footings that are easily placed with minimal excavation. As a result, concrete usage is limited along with the site disturbance, further reducing trauma to the root systems of existing trees and vegetation. Material that is excavated from the site is used to create a semi-circular berm around the proposed communal fire pit. This proposed berm also addresses concerns surrounding noise pollution that has historically disturbed neighbours.\nA low-tech water collection system will be used to capture the runoff water of the main buildings in an effort to manage storm water. The water will be utilized for rock cutting and equipment cleaning to help reduce the pressure on the well.\nMaterially, the project is constructed and finished almost entirely in wood which is readily available and locally produced. Inspired by the vernacular of rural buildings around the site while also adhering to a budgetary limit, the low-slope roofs and canted walls are clad with corrugated galvanized metal panels. Recessed areas and exterior walls at entries are painted in a palette of muted colours inspired by the lichens and grasses that cling to the rugged hillsides that surround the site.\nDC: The tiered/staggered section for the top bunks in relation to its lower bunks creates a critical mid-scale detail which in concert with the minimizing footprint creates what appears to be the defining canted exterior wall condition in the project. As a result, a somewhat playful but well-earned generative form emerges, enough so that the volume of the pitched roofs results in an eaveless, tight-wrap volume. Accordingly, a legitimate syntax is deployed in which overhangs and breezeways elegantly accommodate inflected walls in plan often resulting in a pleasing figure-ground balance of public-to-private spaces.\nMCC: The relationship between the landscape and buildings is key in this project, and the pavilion approach allows nature to exist between these humanly scaled discrete structures. The form and materiality of the buildings are sensitive to their natural context, and the variations in shape of each pavilion bring a richness to the whole.\nBH: This project has a great program--a professional summer camp. A light-touch site-ordering strategy that is responsive to the landscape, combined with an intelligent and deeply habitable cross-section creates a delicate balance of understated architecture entirely appropriate for this building form.\nClient University of British Columbia Department of Earth and Ocean Science\nArchitect Team David Battersby, Heather Howat, Bettina Balcaen, Jose Casal\nStructural Axis Engineering Ltd.\nLandscape BattersbyHowat Architects Inc.\nInteriors BattersbyHowat Architects Inc.\nProject Manager Crystal Roche, UBC Properties Trust\nGeneral Contractor Scuka Enterprises\n3D Model and Renderings Tomas Machnikowski\nArea 10,140 ft2 building area on 80 acres\nCompletion May 2014"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"historical"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:afcd784c-a883-40d4-820d-5c21656f4f1e>","<urn:uuid:6a4337c5-0441-4b98-9738-c2745ad97b2b>"],"error":null}
{"question":"Would sauerkraut or kimchi be more effective for managing gut disorders?","answer":"Both sauerkraut and kimchi are fermented cabbage products with probiotic benefits, but kimchi appears to have more comprehensive digestive health benefits. While sauerkraut serves as a carrier of probiotic microorganisms and provides vitamin C, kimchi has been specifically documented to be anti-constipation and promotes colorectal health. Additionally, kimchi contains beneficial lactic acid bacteria that aids digestion and has been shown to have anti-obesity properties, making it particularly effective for overall gut health management.","context":["When you sit down to a meal or grab a quick snack, you are not just eating for your health, but you are also eating for the health of your microbiome. Your microbiome, the ecosystem of microbes living primarily in your digestive tract, is a delicate and wise system that is responsible for governing almost every system in your body.\nWhat Are Probiotics?\nOriginally derived from the Greek and Latin languages meaning “for life,” probiotics are live strains of healthy bacteria and yeast that are good for your digestive tract, microbiome, and so much more.\nThese strains of good and natural (i.e., not added by food manufacturers) bacteria are plentiful in some foods and lacking in others. Ancient and traditional diets were once primarily made up of homegrown foods that grew in healthy probiotic-rich soil and were then often fermented for safekeeping––increasing the beneficial bacteria your ancestors naturally ate. Today’s typical foods are more processed and refined, and the soil is not as healthy, leaving the majority of the Western diet bankrupt of natural probiotics.\nWhile it may take a bit more thoughtfulness to eat a diet rich in nature-made probiotics, it is well worth the effort for your digestive health and overall wellness.\nWhat Probiotics Do for You\nProbiotics do a number of helpful things for your mind and body.\n- Improve Your Digestion: When your microbiome is well balanced, you experience healthier digestion. Research shows that symptoms such as bloating, diarrhea, and various forms of inflammatory bowel disease (IBD) will improve with a diet rich in nature-made probiotics. Natural probiotics have also been shown to repair candida overgrowth (an imbalance of yeast in the microbiome), help heal leaky gut syndrome, and decrease digestive inflammation.\n- Restore Your Microbiome After Taking Antibiotics: When you treat an infection or pathogen with antibiotics, many of your good bacteria are killed off along with the bad. Research shows it is important to replenish your system with natural probiotics for a healthy and balanced microbiome. Researchers have found that using natural probiotics after a treatment of antibiotics can help reduce antibiotic-associated diarrhea.\n- Keep Your Immune System Strong: You know when cold and flu season comes around that it is important to keep your immune system strong, including getting plenty of rest, washing your hands frequently, taking your vitamin C, and eating a diet rich in nature-made probiotics. Researchers have found that probiotics enhance immune function through supporting the regeneration of the intestinal lining, keeping the bad bacteria from entering the blood stream and therefore supporting the overall health of the immune system.\n- Uplift Your Mood: Did you know that 90 percent of your body’s 5-HTP aka serotonin (a neurotransmitter that plays an important role in regulating mood, appetite, and sleep) is produced in your digestive tract? Often called the “second brain,” your microbiome has a direct impact on your brain chemistry. Probiotics––often referred to as psychobiotics when referencing brain chemistry––can have a brain-altering effect through their innate mechanism to produce neurotransmitters such as serotonin, GABA, and acetyl-choline. Studies show that regulation of mood and symptoms of depression can be improved through improving gut health via regulation of the microbiome with a diet high in natural probiotics.\n- Reduce Autoimmune Symptoms: Autoimmunity is a condition in which the immune system attack the body’s own healthy cells and tissues. This leads to various forms of chronic pain and inflammation affecting an estimated 23.5 million Americans with disorders including rheumatoid arthritis, multiple sclerosis, Hashimoto’s disease, celiac disease, and lupus. Research shows that if the gut microbiome is well-nourished and balanced, it can improve some of the common symptoms associated with autoimmune disease. Eating a diet that is rich in probiotics has proven to be a therapeutic approach to treating autoimmune disease.\nFermented Foods High in Probiotics\nFermented and cultured foods are a wonderful source of natural probiotics. While fermenting food for storage is an age-old tradition, here are some common fermented foods high in natural probiotics that you can find at your local grocery store.\n- Yogurt: This dairy-based culture may be the most common probiotic food, found in many varieties all over the world, including yogurt made from goat milk, sheep milk, and cow milk. It’s important to choose a high-quality variety to improve your gut health. Look for organic yogurt that is high in probiotics––especially lactobacillus delbrueckii subsp. and bulgaricus—made from the milk of grass-fed cows, goats, or sheep, with no sugar added. You can sweeten with a little bit of raw honey, if you like.\n- Cultured Vegetables: Cultured vegetables such as sauerkraut, kimchi, pickles, and kvass are a sure way to feed your microbiome with nature-made probiotics. Dating back thousands of years as a way to store food through the long winter months or to store food without refrigeration, culturing vegetables is a practice that many cultures around the world have used. The fermentation process naturally promotes beneficial probiotics. You can find many varieties at your local grocer. Look for the varieties that are refrigerated to preserve the natural probiotics.\n- Apple Cider Vinegar: Apple cider vinegar (ACV), made from apples broken down by fermentation, is an age-old elixir known for its many probiotic-rich properties. ACV is rich in probiotics and enzymes. For the most potent probiotic-rich ACV, choose organic, raw, and unfiltered.\n- Miso: Often consumed as a breakfast broth in Japan, fermented soybean paste, known as miso (cultured with rice, barley, and salt), is a tasty ingredient that not only makes a wonderful soup or sauce, but is also known for its nature-made probiotics. As an age-old fermented food, miso provides the gut microbiome with an abundance of good bacteria. When choosing miso, look for the unpasteurized, enzyme-rich variety that is stored in the refrigerator.\n- Sourdough Bread: Not all breads are created equal, but a good old-fashioned sourdough may surprise you with its nature-made probiotic potency. The fermentation process that creates the sourdough bread adds a few significant improvements––including probiotics and prebiotics––improving the bread’s digestibility, and making the other nutrients it contains more absorbable.\nNon-Fermented Foods High in Prebiotics\nPrebiotics are functional foods that promote the friendly bacteria in your gut microbiome—essentially feeding your probiotics. Typically, prebiotics are found in the form of three primary types of fiber (or non-digestible carbohydrates)––inulin, fructo-oligosaccharides, and oligosaccharides. Prebiotics support your good bacteria’s ability to flourish. While there are many prebiotic foods, here are a few foods that are particularly rich in prebiotics for you to consider including in your diet.\n- Bananas: Along with being a well-known fruit, bananas are a great source of probiotics. Bananas are high in gut-resistant carbohydrates––those that pass through your digestive tract unchanged and feed the probiotics, making them a great prebiotic food to power-up your microbiome. One study showed that the prebiotic fiber in bananas supports digestion and decreases bloating.\n- Oats: You may have eaten a bowl of oatmeal for your warm breakfast cereal but have you tried steel-cut oats? Steel-cut oats are a rough-cut version of whole oats with even more fiber and a greater prebiotic levels. High in beta-glucan soluble fiber, whole oats feed those beneficial probiotics.\n- Flaxseeds: Known for their high omega-3 content, flaxseeds are a healthy powerhouse prebiotic. Researchers have found that the fiber in flaxseeds promotes good gut flora and a healthy gut and strong digestion in obese, postmenopausal women.\n- Garlic: Garlic can add a spicy and pungent flavor to your meals. While garlic is a tasty addition, it is also a prebiotic food due to its high inulin levels and the naturally occurring fructo-oligosaccharides (FOS)––making it a dynamite naturally occurring prebiotic.\nThere are many ways to nourish your microbiome and support good gut health with natural probiotics. Eating an array of naturally fermented foods along with prebiotics will ensure a well-balanced and effective digestion that contributes to your overall wellness.\n*Editor’s Note: The information in this article is intended for your educational use only; does not necessarily reflect the opinions of the Chopra Center's Mind-Body Medical Group; and is not a substitute for professional medical advice, diagnosis, or treatment. Always seek the advice of your physician or other qualified health providers with any questions you may have regarding a medical condition and before undertaking any diet, supplement, fitness, or other health programs.\nDetox with our signature Ayurvedic cleanse and enjoy one-on-one guidance to awaken your body’s innate healing abilities at our six-day Perfect Health retreat. Learn More.","5 Natural Probiotic Foods For A Healthy Gut And Stronger Immunity\nConsidered a superfood, probiotics are food products fermented by lactic acid bacteria. They play a major role in modulating the gut flora, thereby managing many gut disorders. Yogurt, buttermilk, tempeh, miso, kefir, sauerkraut, kimchi, and cheese are some probiotics that provide various health benefits. It's easy to make them at home.\nBenefits Of Probiotics\n- Improves immunity by boosting the good gut bacteria\n- Helps manage acute and chronic gut disorders1\n- Relieves lactose intolerance symptoms\n- Shortens diarrhea caused by rotavirus2\n- Relieves ulcerative colitis and pouchitis (inflammation of an artificial rectum)\n- Improves symptoms of irritable bowel syndrome\nYour gut is a complex network of gastric acid, bile, intestinal microflora, and some good and bad bacteria. So you want to make sure that there’s more of the good bacteria that can help you maintain a balance. That’s where probiotics come in. Probiotics comes from the fusion of two Greek words – “pro” meaning for and “biotics” meaning “life.”\nProbiotics have been the buzzword among the health conscious for quite some time now. Touted as a superfood, probiotics which are food products fermented by lactic acid bacteria, have a lot to offer, including protection against cancer.\nToday, stores are flooded with probiotic products – we even have probiotic ice-cream. Live cultures are being added to a variety of foods to make them gut-friendly. But rather than trying supplements, try these 5 natural probiotics you can make at home.\n- Just add about 5 tablespoons of leftover yogurt to 500 ml of warm (not hot) milk.\n- If you want to make it from scratch, squeeze a lemon into warm milk and let if ferment overnight.\nWith a diet rich in yogurt, you can be assured of a healthy gut as it balances the intestinal flora. Several studies even report that the natural probiotics in yogurt have a role to play in inhibiting tumor formation.3\nIt is also useful in chronic liver disease as well and also has a cholesterol-lowering effect.45\n- Boil full-fat milk.\n- While it is still hot, add enough lemon juice to curdle the milk.\n- Chunks of cottage cheese will appear.\n- Drain the leftover fluid and tie the cottage cheese tightly in a muslin cloth or cheesecloth to bind it.\nThere is good news for cheese lovers. Indulging in this food can give you a probiotic boost. Cheese is a great delivery vehicle for probiotic cultures. It is also loaded with conjugated linoleic acid and bioactive peptides that have health benefits.6\nBut all cheeses might not do the trick. Aged cheeses like Gouda, mozzarella, and cheddar are good sources. Cottage cheese also packs in probiotics and can be easily made at home.\n- Mix 1.5 spoons of salt to shredded cabbage (medium-sized) and toss for a few minutes.\n- Leave it for about 10 minutes or till it starts leaving some water.\n- Flavor it with caraway seeds (optional).\n- Cram the cabbage tightly into mason jar along with the water it released.\n- Forget about it for 3 days and enjoy it afterward.\nIf you are vegan or have sworn off dairy products, sauerkraut might just be your thing. Another wonderful non-dairy source of probiotics is sauerkraut prepared from shredded cabbage fermented by lactic acid. Fermented products like sauerkraut can serve as carriers of probiotic microorganisms.7 It also carries a punch of vitamin C from the cabbage.\nIt is widely relished in European countries as a side dish, alongside burgers and on top of sausages. It has a sour-salty taste.\n- In 1 glass of whole milk add 1 tsp active kefir grains.\n- Cover the glass with a cheesecloth or paper napkin, and secure it with a rubber band.\n- Store it at room temperature away from sunlight for 12–48 hours.\n- Strain out the kefir grains (they can be reused) and drink up.\nThe word for the fermented drink comes from Turkish and means “pleasure” or “good feeling.” It’s quite good for the health too! Studies have shown that kefir has antimicrobial, antitumor, anticarcinogenic, and immunity-modulating activity. It also improves lactose digestion.8\nIn postmenopausal rats, kefir has also shown improved bone mass and microarchitecture, which are key to bone quality.9\n5. Kimchi Salad\n- Mix in together 1 cabbage (cut lengthwise), 1/4 cup salt, and 4 cups of water. Let it sit 3 hours or overnight.\n- Drain the cabbage and rinse with cold water.\n- To the cabbage, add julienned radish, scallions, and grated ginger.\n- Add a thick paste made with 1 tbsp water, sugar, pepper flakes, and nori.\n- Mix it in and place it in a glass jar in sunlight. Your kimchi should be ready in 24 hours.\nThis pungent and spicy Korean salad works as a side dish and is mostly made by fermenting cabbage. It is a good source of lactic acid bacteria, that helps in digestion.\nKimchi is anti-cancer, anti-obesity, anti-aging, and anti-constipation. It promotes immunity, brain, and colorectal health and reduces cholesterol.[ref]Park, Kun-Young, Ji-Kang Jeong, Young-Eun Lee, and James W. Daily III. “Health benefits of kimchi (Korean fermented vegetables) as a probiotic food.” Journal of medicinal food 17, no. 1 (2014): 6-20.\nThese are some healthy probiotic foods that you can include in your diet. You can choose them according to your taste.\nReferences [ + ]\n|1.||↑||Fooks, L. J., and G. R. Gibson. “Probiotics as modulators of the gut flora.” British Journal of Nutrition 88, no. S1 (2002): s39-s49.|\n|2.||↑||Ouwehand, Arthur C., Seppo Salminen, and Erika Isolauri. “Probiotics: an overview of beneficial effects.” Antonie Van Leeuwenhoek 82, no. 1-4 (2002): 279-289.|\n|3.||↑||Lourens-Hattingh, Analie, and Bennie C. Viljoen. “Yogurt as probiotic carrier food.” International dairy journal 11, no. 1 (2001): 1-17.|\n|4.||↑||Liu, Jun-E., Yan Zhang, Jing Zhang, Pei-Ling Dong, Ming Chen, and Zhong-Ping Duan. “Probiotic yogurt effects on intestinal flora of patients with chronic liver disease.” Nursing research 59, no. 6 (2010): 426-432.|\n|5.||↑||Ataie-Jafari, Asal, Bagher Larijani, H. Alavi Majd, and Farideh Tahbaz. “Cholesterol-lowering effect of probiotic yogurt in comparison with ordinary yogurt in mildly to moderately hypercholesterolemic subjects.” Annals of Nutrition and Metabolism 54, no. 1 (2009): 22-27.|\n|6.||↑||Hayes, M., M. Coakley, L. O’sullivan, and C. Stanton. “Cheese as a delivery vehicle for probiotics and biogenic substances.” Australian Journal of Dairy Technology 61, no. 2 (2006): 132.|\n|7.||↑||Heller, Knut J. “Probiotic bacteria in fermented foods: product characteristics and starter organisms.” The American Journal of Clinical Nutrition 73, no. 2 (2001): 374s-379s.|\n|8.||↑||Leite, Analy Machado de Oliveira, Marco Antonio Lemos Miguel, Raquel Silva Peixoto, Alexandre Soares Rosado, Joab Trajano Silva, and Vania Margaret Flosi Paschoalin. “Microbiological, technological and therapeutic properties of kefir: a natural probiotic beverage.” Brazilian Journal of Microbiology 44, no. 2 (2013): 341-349.|\n|9.||↑||Chen, H-L., Y-T. Tung, C-H. Chuang, M-Y. Tu, T-C. Tsai, S-Y. Chang, and C-M. Chen. “Kefir improves bone mass and microarchitecture in an ovariectomized rat model of postmenopausal osteoporosis.” Osteoporosis International 26, no. 2 (2015): 589-599.|"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"temporal_focus","category_name":"future"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7cf89a7e-4c5c-46a5-a644-ceebd9b22114>","<urn:uuid:7784bee4-f9b4-439f-89d2-b3fa902ce494>"],"error":null}