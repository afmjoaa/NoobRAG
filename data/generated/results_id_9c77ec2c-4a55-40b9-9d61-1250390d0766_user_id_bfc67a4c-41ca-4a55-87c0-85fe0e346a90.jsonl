{"question":"How do the tracking capabilities of Hawkeye technology compare to the sensitivity of the Yamamoto Hula Swimmer's tail movement in terms of detecting and analyzing motion?","answer":"Hawkeye uses six roof-mounted high-performance cameras to precisely track ball trajectory, providing detailed analysis of line, length, spin, and swing of bowling deliveries. In contrast, the Yamamoto Hula Swimmer's tail movement, while visually impressive with its kicking action and undulating head, produces such minimal vibration that it's undetectable during retrieval, despite having a paddle tail design that creates a lively swimming motion.","context":["Creature Fever: The Yamamoto Hula Swimmer is a short skirted enticer!\nTotal Score: 7.58 -\nYamamoto bait company has achieved nothing short of legendary status in the Bass\nfishing world, aided along years ago by their release of the original Senko. It\ndidn't take long for the rest of the nation, and the world for that matter, to\ntake notice that the Senko was indeed a special product. Since then, Yamamoto\nhas released several variations of this amazing bait, including the popular Swim\nSenko. Taking the design of this bait one step further is the Hula Swimmer,\nwhich incorporates a 4.5” Swim Senko style body with a short hula-skirt head.\nWill this mop-top bait prove to be another winner?\nArmed and ready with 3 bags of Yamamoto Hula Swimmers\nyou ever fished a Swim Senko but wished it had a little something extra to get\nnoticed or entice a strike? The Hula Swimmer offers just that, featuring a 4.5”\nSwim Senko style body and a 12 point hula-skirt head. The body and head are all\none single piece, so rigging is a snap. Right out of the bag this bait strikes\nyou as one that will catch fish. The colors, size, and just overall “fishy”\nlook of the bait inspire confidence before the first cast is even made. Like\nother Senko style baits, it is also infused with a generous amount of salt for\neven more appeal. 12 standard colors are available, as well as several special\norder combos available on the Gary Yamamoto website. This allows the customer\nto choose their own skirt and body color combination for a truly custom bait.\nRigged up on a 5/0 keel weighted swimbait hook\nTesting with the Hula Swimmer took place on a variety of south Florida ponds,\nlakes, and canals. 14Lb monofilament, as well as 20lb braid were the lines of\nchoice. Though this bait can be used as a trailer on a lure like a swim jig, I\nelected to test it alone rigged Texas style with a bullet weight, and with a\nkeel-weighted swimbait hook.\nA closer look at the 12 point hula skirt head\nall baits in the Senko family, these have a nice density to them which allows\nfor effortless casting with just a light added weight. I really enjoyed tossing\nthe Hula Swimmer in shallow water with just enough weight to keep it down in the\nstrike zone. While the hula-skirt head adds a tiny amount of wind resistance to\nthe bait, I found it wasn't a concern whatsoever during casting, even into the\nwind. If the skirt was both longer and thicker, it would likely then pose more\nof a problem. Any rod that you like to fish Senkos on will be perfect to cast\nthis bait. I personally prefer a 7'-7'3” M to MH rod for fishing them.\nA TackleTour autopsy of sorts, a well worn bait is cut open to reveal the salty\nHula Swimmer is a very easy bait to fish. My best success comes when slow\nrolling it like a spinnerbait, with an occasional speed variance or twitch\nduring retrieve. The bait has a nice feel coming through the water, and does so\nwith just a minimal resistance. It's a killer tool for probing around, through,\nor tickling the tops of submerged weeds. Since it is weedless and pretty thin,\nit comes through cover easily. While the tail portion has fantastic kicking\naction, you can't really feel any of it's vibration during the retrieve. The\npaddle tail is relatively small and thin, so this is not really a surprise.\nThough undetectable, the action of the bait is impressive, as it truly does have\na very lively look with it's kicking action and undulating head. The head also\ncauses the bait to push a bit more water than a standard Swim Senko, which in\nturn allows Bass to find it easier in dirtier water.\nSlow rolling the bait through a bit deeper water with an occasional twitch and\nfound the durability to be just ok. The bulky first half of the body is\nbasically a Senko, so it's pretty chunky and fairly durable. The problem I\nencountered had to do with the rear tail half of the bait. Nearing the tail,\nthe bait flattens and narrows substantially, tapering down to a thin weak point\nbefore the paddle tail. This design definitely improves swimming action, but at\nthe cost of durability. Several baits had their tails sheared clean off by very\nsmall Bass that chomped or pecked at the bait during retrieve.\nFrustration set in when several baits had their tails sheared clean off by small\nfish chomping and pecking at the tail during retrieve\nThe bags used\nfor packaging seem to be of very good quality and have an equally good zip\nclosure at the top. Bags like these are great as I can simply toss a bunch of\nthem in my tackle bag for the day and not have to worry about lugging a worm\nbinder around. Each bag contains 7 Hula Swimmers at a cost of $7.49-$7.99.\nCompared to a similarly sized 4”-5” Senko, the price is comparable except that\nyou get 3 less Hula Swimmers per bag. It would be nothing to go through a whole\nbag of these baits during a good fishing day, which could get a bit costly\nTop view of the tail portion. The thinnest section in the middle wasn't solely\nto blame as the bite-offs occurred along the entire tail portion.\nratings standard for\n2008 and have\nincluded a key at\nthe bottom of the\nfollowing matrix as\nto be a\n: 2 =\npoor : 3\n: 4 =\n: 5 =\n: 6 =\nfair : 7\n= good :\n: 10 =\nPluses and Minuses:\nWith low water, high temps, and light wind each day, big fish were absent during\ntesting but willing 2lbers filled the void\nFever in full swing here at TackleTour, one cannot think of plastic baits\nwithout the Yamamoto name immediately coming to mind. The Hula Swimmer joins\ntheir ever-expanding lineup of Senko-based offerings, all made with the\nsignature quality and triggering qualities Yamamoto has built a reputation on.\nFor Bass anglers, especially those favoring shallow water, these are a bait\nworth looking into. For fish that have “seen it all”, sometimes showing them\nsomething even slightly different is all it takes!\nLooking for the Yamamoto Hula Swimmer?","What is Hawkeye? Cricket technology explained\nCricket fans are glued to the telly as Ashes fever takes hold. But what about the masses of tech used in the modern game? Learn your Hawkeye from your Hot Spot with our guide.\nFor 47 long, (hopefully) hot summer days, the eyes of the cricketing world will be on England and Wales, as the hosts fight to reclaim the Ashes.\nRevenge is the only thing on captain Alastair Cook’s mind, after a bewildering 5-0 whitewash suffered Down Under at the hands of the Aussies last time out.\nBut there could be even more head scratching this summer for out-of-practice armchair fans.\nIf you haven’t watched a televised Test recently, you’ll probably be bowled over (sorry) by the masses of graphical information.\nWhat’s with all the tech? Well since the introduction of the Decision Review System (DRS), teams are now given the chance to refer disputed decisions to a third umpire.\nThree pieces of kit make up the DRS –read on to avoid being caught out this summer.\nIn a nutshell: Hawkeye is used to analyse and review leg before wicket (LBW) decisions.\nBy tracking the trajectory of balls in flight, Hawkeye tells us where a ball:\n- was pitched\n- hit the leg of the batsman\n- would’ve ended up (had it not hit the leg)\nAn umpire gives a batsman ‘out’ if he deems the ball was pitched in line with the stumps and would’ve gone on to hit them, were it not for the batsman’s pad.\nIf the bowlers think the umpire has failed to or wrongly calls LBW, they can use a referraland ask the third umpire to double check Hawkeye footage from the stands.\nIt works via a system of six roof-mounted, high-performance cameras which track the ball from different angles.\nAnother handy use of Hawkeye is for statistical TV analysis of a bowler’s deliveries – showing line, length, spin or swing.\nDid you know? Since landing in cricket in 2001, Hawkeye technology has found its way into several other leading sports such as tennis, snooker and football.\nIn a nutshell: Hot Spot reveals whether a ball has hit the batsman, his bat, or pad.\nIt only takes the slightest of contact for a batsman to be deemed caught behind, or given LBW. And with balls flying around at speeds of up to 100mph, you can forgive umpires for being fallible.\nThat’s where Hot Spot comes in. Using infra-red technology, these cameras detect heat caused by the friction of the ball making contact with bat.\nWhile non-audible, this option is often decisive in proving the tiniest of knick of the edge of the bat.\nOccasionally, Hot Spot has provoked controversy for not picking up clear ‘edges’ due to a lack of friction from fast balls.\nDid you know? The four specialist cameras used at each match cost around £7,500 every day.\nSnicko (Real-time snickometer)\nIn a nutshell: Imposes video with sound - judging whether a ball has made contact with bat.\nSnicko is a godsend for officials, for whom detecting those cheekiest of edges off the bat, has long been one of the most complicated parts of the job.\nSnickometers were originally only for TV use and deemed untrustworthy for DRS – as they required a technician to overlay pictures with sound recorded from a stump microphone.\nBut the real-time version debuted in 2013. Now used in tandem with Hot Spot, the all-new automated version combines audio and video - making the review system faster and far more consistent.\nThe real-time video complete with captured sounds, generates disturbances on a graph – helping the officials to separate fine snicks from false alarms.\nDid you know? Snickometers were being used by Channel 4 as far back as 1999."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:d06d7b26-9492-497a-b251-97a801785a0f>","<urn:uuid:11d73379-f8bb-448d-916a-db630ebe54f3>"],"error":null}
{"question":"¿Cuál es la relación between magnetic flux density and the unit volt in terms of how they measure electrical phenomena?","answer":"Magnetic flux density (B) measures the strength of a magnetic field and is calculated as force per unit length per unit current (B = F/Ixl). The volt, on the other hand, is the unit of electrical potential difference and electromotive force, equal to the difference in potential between two points in a conductor carrying one ampere current when the power dissipated is one watt. These units are related through electromagnetic phenomena, as demonstrated in the weber unit definition, where one weber of magnetic flux linking an electrical circuit produces one volt of electromotive force when reduced to zero in one second.","context":["Learn something new every day More Info... by email\nThe measure of the strength of a magnetic field is called magnetic flux density, or magnetic induction. A magnet is said to have a north and a south pole; two magnets will repel each other when like poles face each other and attract each other when opposite poles come close to each other. Electrically charged particles are also deflected in magnetic fields.\nMagnetic flux density is analogous with features of electric and gravitational fields. Electrical field strength is the force acting on a body per unit charge, and gravitational field strength is the force acting on a body per unit mass. Magnetic fields are created by a electric current. Magnetic flux density (B) is force (F) acting on a conducting material per unit length (l) and per unit of current (I), which can be written as the equation B = F / I x l. The unit of magnetic flux density is called the Tesla.\nA magnetic field is always generated at 90 degrees to a moving electric field. The direction of the electric current, magnetic field and magnetic force are defined by John Ambrose Fleming's left-hand rule. Holding the left hand's first two fingers and thumb at right angles to each other will indicate the relative directions the thrust, field and current. As suggested by the similarity of the words' first sounds, the thumb shows the relative direction of the thrust, and the the first finger shows the relative direction of the field. The second finger shows the relative direction of the current.\nIt is possible to see a magnetic field using just a bar magnet, a sheet of white paper and some iron filings. This is done by placing the magnet underneath the paper and lightly sprinkling the paper with the filings. The fillings will align themselves with the magnetic field's curved lines of flux. More filings will be attracted to either end of the magnet than in the middle, meaning that the magnetic flux is strongest at these points. This also is the case with the Earth's magnetic field.\nThe Earth's magnetic field is caused by the present of a molten, rotating iron core at its center. Its flux density is strongest at the North and South poles. Electrically charged particles from the sun are attracted to the poles, which causes the aurora borealis, or northern lights, and the aurora australis, or southern lights.\nWeaker magnets will align themselves with the flux lines of stronger magnets. It is this phenomenon that is exploited by a compass, the needle of which is a very weak magnet. By convention, the compass' south pole points to the North Pole of the Earth, but if the compass was magnetized in the opposite direction, it would point to the South Pole.\nOne of our editors will review your suggestion and make changes if warranted. Note that depending on the number of suggestions we receive, this can take anywhere from a few hours to a few days. Thank you for helping to improve wiseGEEK!","We encounter some of the electrical units listed below to measure electrical phenomena in our everyday lives. The power of a light bulb is measured in watts. The load of a household circuit breaker is measured in amperes. Others, such as the coulomb and the henry, measure more intangible quantities.\nThe ampere is the unit of electric current in the SI, used by both scientists and technologists. Since 1948 the ampere has been defined as the constant current which, if maintained in two straight parallel conductors of infinite length of negligible circular cross section and placed one metre apart in a vacuum, would produce between these conductors a force equal to 2 x io7 newton per metre of length. Named for the 19th-century French physicist André-Marie Ampere, it represents a flow of one coulomb of electricity per second. A flow of one ampere is produced in a resistance of one ohm by a potential difference of one volt.\nThe coulomb is the unit of electric charge in the metre-kilogram—second-ampere system, the basis of the SI system of physical units. The coulomb is defined as the quantity of electricity transported in one second by a current of one ampere. Named for the I8th—I9th-century French physicist.\nA unit of energy commonly used in atomic and nuclear physics, the electron volt is equal to the energy gained by an electron (a charged particle carrying unit electronic charge when the electrical potential at the electron increases by one volt). The electron volt equals 1.602 x IO2 erg. The abbreviation MeV indicates 10 to the 6th (1,000,000) electron volts and GeV, 10 to the 9th (1,000,000,000).\nThe faraday (also called the faraday constant) is a unit of electricity used in the study of electrochemical reactions and equal to the amount of electric charge that liberates one gram equivalent of any ion from an electrolytic solution. It was named in honour of the 19th-century English scientist Michael Faraday and equals 9.64853399 x 10 to the 4th coulombs, or 6.02214179 x 10 to the 23rd electrons.\nThe henry is a unit of either self-inductance or mutual inductance, abbreviated h (or hy), and named for the American physicist Joseph Henry One henry is the value of self-inductance in a closed circuit or coil in which one volt is produced by a variation of the inducing current of one ampere per second. One henry is also the value of the mutual inductance of two coils arranged such that an electromotive force of one volt is induced in one if the current in the other is changing at a rate of one ampere per second.\nThe unit of electrical resistance in the metre—kilogram-second system is the ohm, named in honour of the 19th-century German physicist Georg Simon Ohm. It is equal to the resistance of a circuit in which a potential difference of one volt produces a current of one ampere (1 ohm = 1 V/A); or, the resistance in which one watt of power is dissipated when one ampere flows through it. Ohm's law states that resistance equals the ratio of the potential difference to current, and the ohm, volt, and ampere are the respective fundamental units used universally for expressing quantities. Impedance, the apparent resistance to an alternating current, and reactance, the part of impedance resulting from capacitance or inductance, are circuit characteristics that are measured in ohms. The acoustic ohm and the mechanical ohm are analogous units sometimes used in the study of acoustic and mechanical systems, respectively.\nThe siemens (S) is the unit of electrical conductance. In the case of direct current (DC), the conductance in siemens is the reciprocal of the resistance in ohms (S = amperes per volts); in the case of alternating current (AC), it is the reciprocal of the impedance in ohms. A former term for the reciprocal of the ohm is the mho (ohm spelled backward). It is disputed whether the siemens was named after the German-born engineer-inventor Sir William Siemens(1823-83) or his brother, the electrical engineer Werner von Siemens (1816-92).\nThe unit of electrical potential, potential difference and electromotive force in the metre—kilogram—second system (SI) is the volt; it is equal to the difference in potential between two points in a conductor carrying one ampere current when the power dissipated between the points is one watt. An equivalent is the potential difference across a resistance of one ohm when one ampere is flowing through it. The volt is named in honour of the I8th—I9th-century Italian physicist Alessandro Volta. These units are defined in accordance with Ohm's law, that resistance equals the ratio of potential to current, and the respective units of ohm, volt, and ampere are used universally for expressing electrical quantities.\nThe watt is the unit of power in the SI equal to one joule of work performed per second, or to 1/746 horsepower. An equivalent is the power dissipated in an electrical conductor carrying one ampere current between points at one volt potential difference. It is named in honour of James Watt, British engineer and inventor. One thousand watts equal one kilowatt. Most electrical devices are rated in watts.\nThe weber is the unit of magnetic flux in the SI, defined as the amount of flux that, linking an electrical circuit of one turn (one loop of wire), produces in it an electromotive force of one volt as the flux is reduced to zero at a uniform rate in one second. It was named in honour of the 19th-century German physicist Wilhelm Eduard Weber and equals 10 to the 8th maxwells, the unit used in the centimeter-gram—second system.\nThis 100+ page e-book is a great guide for those who have a basic interest in the field of electricity. This well-illustrated e-book, coupled with some basic knowledge of electricity, will give you a broad theoretical background in this fundamental subject.CONTENTS"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:93f1e656-0328-4793-86e3-3b036c949a8e>","<urn:uuid:3124a9f7-8854-4181-ace8-0b020871c729>"],"error":null}
{"question":"Can a seller reserve title to goods after delivery under UK law?","answer":"Yes, under Section 19 of the Sales of Goods Act in the United Kingdom, a seller can reserve title to goods even after delivery to the buyer. This reservation of title is not considered a breach of the seller's obligations to deliver. However, as established in the Aluminium Industrie Vaassen BV v Romalpa Aluminium Ltd 1976 case, the reservation of title must be expressly incorporated into the contract - it does not apply automatically. Sellers often use this as a means of security against the buyer until payment is received for the goods.","context":["The United Nations Convention on Contracts for the International Sale of Goods 1980 (CISG) does not deal directly with the question of who has title to the goods. ‘Title’ to the goods is another way of saying the legal right or legal liability in relation to the goods. It is an apparently simple term that masks a plethora of complex legal issues. We tend to think of title as synonymous with ownership, which to some extent it is. However, it is a concept that is quite complex.\nCritically analyze this statement in relation to the provisions of CISG with a local sale of goods act in another country of your choice with decided case laws.\nThis scenario focuses on the interesting fact that the ‘United Nations Convention on Contracts for the International Sale of Goods’ (hereafter; the Convention) has, in a manner, sidestepped the important issue of identifying the exact point where the Title to the goods transfer from the Seller to the Buyer.\nDoes the Title to the goods pass over to the Buyer at the point when goods are shipped by the Seller? Or does it happen only after the goods reach the Buyer? Or is it after the Buyer makes the payment and Seller receives it? The exact point when the transfer of the title takes place is a questionable one.\nAlthough the word ‘Title’ has not been specifically mentioned, one of the provisions that indirectly comes into contact with this situation can be found in Part III of the Convention i.e. Sale of Goods.\nIn Part III, Chapter II (Obligation of the Seller), Section II (Conformity of the Goods and Third Party claims), Article 38 says:\n(1) The buyer must examine the goods, or cause them to be examined, within as short a period as is practicable in the circumstances.\n(2) If the contract involves carriage of the goods, examination may be deferred until after the goods have arrived at their destination.\n(3) If the goods are redirected in transit or re-dispatched by the buyer without a reasonable opportunity for examination by him and at the time of the conclusion of the contract the seller knew or ought to have known of the possibility of such redirection or re-dispatch, examination may be deferred until after the goods have arrived at the new destination.\nThis provision makes it clear that shipping of the goods by the Seller alone does not satisfy the requirements of the Buyer.\nThe Buyer should be able to examine the goods and satisfy himself about the quantity and / or quality of the goods. If the goods are not up to his satisfaction he will not grant his consent to pay for the goods received.\nThe Title to the goods will not pass over from the Seller to the Buyer if the Buyer is not satisfied with the goods he received.\nSo it can be pointed out that it is a compulsory requirement that the goods should reach the buyer and should be examined by him before the title of the goods pass over to him from the Seller.\nHowever, unlike the Convention, Sale of Goods Ordinance 11 of 1896 is very clear and precise at this point.\nIt is mentioned in the Part II of the Ordinance (Effects of the Contract) which deals with ‘Transfer of Property as between Seller and Buyer’;\n16. Where there is a contract for the sale of unascertained goods no property in the goods is transferred to the buyer unless and until the goods are ascertained.\n18. (1) where there is a contract for the sale of specific or ascertained goods, the property in them is transferred to the buyer at such time as the parties to the contract intend it to be transferred.\n(2) For the purpose of ascertaining the intention of the parties, regard shall be had to the terms of the contract, the conduct of the parties, and the circumstances of the case.\nThis means that the property in the goods is passed over to the Buyer from the Seller only when the goods are ascertained – or in other words, verified or confirmed about their authenticity – by the Buyer. Until then goods are considered as the property of the Seller.\nSale of Goods Ordinance 11 of 1896 further notes five rules where there can be exceptional situations to the above:\n19. Unless a different intention appears, the following are rules for ascertaining the intention of the parties as to the time at which the property in the goods is to pass to the buyer:\nRule 1 — where there is an unconditional contract for the sale of specific goods, in a deliverable state, the property in the goods passes to the buyer when the contract is made, and it is immaterial whether the time of payment or the time of delivery, or both, be postponed.\nRule 1 applies to situations where the property of the goods of the Seller is passed over to the Buyer at the time of the formation of the contract. The time when the goods are shipped by the Seller, the time when the goods are delivered to the Buyer, time when the Buyer makes payment or the time when the Seller receives payment, etc. does not determine the passing of the property in the goods.\nRule 2 —where there is a contract for the sale of specific goods, and the seller is bound to do something to the goods for the purpose of putting them into a deliverable state, the property does not pass until such thing be done and the buyer has notice thereof.\nRule 2 states that the Seller should perform all tasks relating to the goods to make them deliverable to the Buyer. Or else the property does not pass from the Seller to the Buyer.\nRule 3 —Where there is a contract for the sale of specific goods in a deliverable state, but the seller is bound to weigh, measure, test, or do some other act or thing with reference to the goods for the purpose of ascertaining the price, the property does not pass until such act or thing be done and the buyer has notice thereof.\nRule 3 states that the Seller should perform all tasks relating to the goods to ascertain the price of the goods. Or else the property does not pass from the Seller to the Buyer.\nRule 4 —When goods are delivered to the buyer on approval, or “on sale or return “, or other similar terms, the property therein passes to the buyer—\n(a) When he signifies his approval or acceptance to the seller, or does any other act adopting the transaction;\n(b) If he does not signify his approval or acceptance to the seller, but retains the goods without giving notice of rejection, then, if a time has been fixed for the return of the goods, on the expiration of such time, and, if no time has been fixed, on the expiration of a reasonable time. What is a reasonable time is a question of fact’.\nRule 4 signifies the acceptance and / or approval of the goods by the Buyer. If not, the property of the goods does not pass from the Seller to the Buyer.\nRule 5 —\n(1) Where there is a contract for the sale of unascertained or future goods by description, and goods of that description and in a deliverable state are unconditionally appropriated to the contract, either by the seller with the assent of the buyer or by the buyer with the assent of the seller, the property in the goods thereupon passes to the buyer. Such assent may be expressed or implied, and may be given either before or after the appropriation is made.\n(2)Where in pursuance of the contract, the seller delivers the goods to the buyer or to a carrier or other bailee (whether named by the buyer or not) for the purpose of transmission to the buyer, and does not reserve the right of disposal, he is deemed to have unconditionally appropriated the goods to the contract.\nRule 5 if the goods are appropriated to meet the requirements in the contract by the Seller with the agreement of the Buyer (or by the Buyer with the agreement of the Seller) the property of the goods pass over to the Buyer from the Seller.\nAlthough these five rules have been listed as applicable to exceptional situation, it can be understood that in all circumstances that the satisfaction of the Buyer with regard to the expected standards of the goods have been attached much importance.\nSale of Goods Ordinance 11 of 1896 makes it clear that if the Buyer is not satisfied with the goods, the property of the goods does not pass over to him from the Seller.\nThe same situation is highlighted in the In Part III, Chapter II (Obligation of the Seller), Section II (Conformity of the Goods and Third Party claims), Article 38 of the Convention.\nHence, it is safe to assume that the Convention and the Ordinance both agrees at this one point with regard to the Title of the goods. Which is, that until the Buyer is satisfied with the goods he / she received the title will not pass over to him / her from the Seller.\nThis means the Title to the goods remain on the Seller until the Buyer expresses his satisfaction about the goods he receives. Accordingly, the Title to the goods pass over to the Buyer at some point only after the Buyer expresses his satisfaction about the goods he receives.\nThat exact point when the Title to the goods pass over to the Buyer from the Seller – after the Buyer expresses his satisfaction on the goods he received – has yet to be identified.\nIn Greenwood v. Bennet (1973) decided that if the true owner stands by and allows an innocent vendee to pay over money to a third party, the true owner will be estopped from denying the third party’s right to sell.\nAnd in Folkes v. King (1923) the bench of judges held that the purchaser obtained good title to the car from the mercantile agent because he has possessing the car with the owner’s consent for the purpose of sale.\nAccording to Hunt v Silk (1804), if a party has received any part of the benefit that he contracted for, there cannot be a total failure of consideration. If therefore, the vendor has no title to the goods, he is liable in damages to the vendee.\nIn the case of Rowland v Divall (1922), Rowland bought a motor-car from Divall and used it for four months. Divall had no title to the car, and consequently Rowland had to surrender it to the true owner. Rowland sued to recover the total purchase money he had paid to Divall.\nThe Court held that Rowland was entitled to recover in full, notwithstanding that he had used the car for four months. This case makes it clear that in order for there to be a valid sale of goods contract the vendor have to have the right to sell such goods.\nIn the book, “The sale of goods” written by P. S. Attiyah, John N. Adams and Hector MacQueenreads as follows: “In CIF contracts the risk once again passes on shipment, and if the goods are lost at the sea the vendee is still bound to pay the price, although he will as a rule have the benefit of the insurance policy. The law is the same even if the vendor knows that the goods have been lost when he tenders the shipping documents. So also, the inability of the vendee to have the goods discharged at the port of destination (because, for instance, he cannot obtain an import license) is of no concern to the vendor, and cannot be a frustrating event. The delivery of the goods on board the vessel, followed by the delivery of correct documents, is a complete performance by the vendor of his duties under a c.i.f contract; what happens after that is of no concern to him, subject to some special cases (for instance, where the goods are shipped in an undivided bulk).”\nIn Clements Horst Co v. Biddel Bros 1912; “a contract was made for the sale of hops to be shipped from San Francisco to London, CIF net cash. The vendee refused to pay for the goods until they were actually delivered. Held, that possession of bill of lading was in law equivalent to possession of goods, and that under CIF contract the vendor was entitled to payment on shipping the goods and tendering to the vendee the documents of title.”\nIn this case the vendee was in possession of the goods and as well as the bill of lading. Thus the title of the goods has already passed to the vendee. Thus the vendor did not have any insurable interest in the goods at the time of the loss.\nWhen the situation discussed in above, comparing with a system of another country it is easier to understand the real world situation on the conventional provisions and the issue mentioned about the ‘title’. I hope to compare the law on title in Switzerland herewith.\nOn the initiative of the UNCITRAL the Convention on the International Sale of Goods was concluded and came into force on 1 January 1988 and today there are sixty eight member states to the convention, including Switzerland.\nIn the Section 2 (1) of the English Sale of Goods Act 1979 defines a contract for the sale of goods as: “… a contract by which the vendor transfers or agrees to transfer the property in goods to the vendee a money consideration, called the price.”\nArticle 184 Section 1 of the Switzerland’s Code of Obligations states the following: “A contract of sale is a contract whereby the vendor obligates himself to deliver to the vendee the object of purchase and to transfer title thereto to the vendee, and the vendee obligates himself to pay the purchase price to the vendor”.\nMost of time parties are agreeing totitleput remain with the vendor despite the fact that the goods were handed-over. Such kind of provisions are usually made to keep away from transfer of title, before the vendee paid the purchase price. It’s important to mention that title transfers when the goods were delivered based on an “iusta causa”.\nWhether vendee has paid or not the purchase price doesn’t affect the transfer of title. According to Law of Switzerland such kind of provisions are only valid and functioning when they has been entered in the public register of the transferee’s domicile which is kept for this purpose by the bankruptcy office.\nAnd; both South African and Swiss Laws on title are similar. In South Africa there’s not an obviously act on the subject of Sales of Goods. Therefore, the needs for a legally valid contract have to derive from common law.G. R. J. Hackwill says that; ‘a sales contract is a mutual contract for the transfer of possession of an object in exchange for a price.’\nIn Concor Construction, Cape (PLC) Ltd v Stantam Bank Ltd; bench of judges are highlighted that “The derivative mode of acquisition of ownership on which the plaintiff relies is delivery. The requirements for the passing of ownership by delivery, inter alia,\n(a) That the transferor must be capable of transferring ownership;\n(b) Delivery must be effected by the transferor with the intention of transferring ownership and taken by the transferee with the intention of accepting ownership; and\n(c) Payment where the sale is a cash sale.”\nAnd in the case Lend lease Finance (Pty) Ltd v Corporation de Mercadeo Agricola and others heard in 1976 the court held that “… ownership cannot pass by virtue of the contract of sale alone: there must, in addition, be at least a proper delivery to the purchaser of the contract goods ….”\nHowever, we have to concern about, what is the way have to proceed when parties are not chosen the applicable Law. In Switzerland; if the parties of an international sale contract have not chosen the law of governing the title then the law of the place where the movable object is physically located applies. In Marcard Stein & Company v Port Marine Contractors Private (Ltd) it was held that: “… should in general apply the principle of the lex situs in determining the passing of ownership in movable property when the case involves a foreign element and there is a potential conflict of laws.”\nIt might happen that it is not possible to determine where the goods were located at the time of the transaction. In these cases the principle of “lex situs” cannot be applied. A court then might use the principle of “lex domicilii” of the owner; like in Swiss Law the place of destination. Finally, if it is not possible to determine where the owner is, it is possible to apply the law governing the contract of sale (“lex causae”).\nFurthermore; Under Section 19 of the Sales of Goods Act, United Kingdom the vendor may reserve title to the goods even after delivery to the vendee. This reservation of title is not-to-be treated as a breach of the vendor’s obligations to deliver.\nIn Aluminium Industrie Vaassen BV v Romalpa Aluminium Ltd 1976 states that; the vendor may be inclined to reserve title as a means of security against the vendee until he is paid for the goods. The reservation of title will not automatically apply, the vendor must expressly incorporate it into the contract.\nIn the law of sale of goods, ownership and title has great importance. Although the United Nations Convention on Contracts for the International Sale of Goods does not highlighting the value of title itself, many ore countries contained provisions on ownership and title to their own acts concerning Sale of Goods."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:47a0d3f6-654d-4c0c-bf29-5d8c0a3a45a9>"],"error":null}
{"question":"What are the key differences between Meryl nylon and Merino wool in terms of odor control and durability?","answer":"Meryl nylon and Merino wool have different performance characteristics. Meryl has inherent odor control properties that naturally inhibit the growth of odor-causing bacteria, and its abrasion resistance is more than 20 times greater than wool. Merino wool, while also naturally odor-resistant, is less resilient compared to synthetic materials and cotton, though it offers good breathability and temperature regulation.","context":["Frequently Asked Questions About Meryl.\nWhat is a denier?\nDenier is the unit of weight, most commonly used in the United States, which measures the thickness of a filament yarn. Denier is the weight in grams of 9,000 meters of yarn. As the number gets larger, the yarn becomes thicker, as the number gets smaller, the yarn becomes finer.\nWhat is decitex (dtex)?\nIt is the weight in grams of 10,000 meters of yarn. Decitex is primarily used in European and Asian countries for measuring filament yarn.\nWhat is a microfiber?\nUnderstanding Nylon 6.6 (Polyamide 6.6)\nWhat are the main differences between Meryl and other nylons?\nThe difference between Meryl and other nylons is two fold and can best be summarised as: 1) A guarantee of the highest quality, and 2) The support of a globally recognised and universally respected brand together with Nylstar's unrivalled experience, technical ability and its growing range of services.This premium synthetic fiber was developed from its inception to equal the touch and comfort of the best natural fibres. We have been producing nylon 6.6 since 1953 making us the oldest and most experienced producer in Europe. Today, over 60 years of innovation later, Meryl remains the premium synthetic fiber of choice but with an array of designed-in performance properties. Even what you may refer to as 'standard' Meryl yarns have been designed to meet and exceed the specific needs demanded by our customers and the market. Meryl truly is the best of both worlds, the fantastic comfort of natural fibers with endless possibilities to tailor it to the needs of the future.Meryl fibres are only produced in Blanes and are not made by any partners or licensees. All Meryl fibres are made under the same roof to the same exacting standards. This means that if you use Meryl in your products you can be assured that the yarn was produced to the highest standards by Nylstar in Blanes and from that point on it can be traced throughout he supply chain to the end garment. Meryl is a guarantee of consistent quality.\nWhat are the primary advantages of Meryl Microfibers?\nThe most important advantage is the comfortable feeling or the \"soft hand\" to the consumer. Also, it is possible today to manufacture high-performance water resistant fabrics by weaving Meryl microfibers into a high density structure.\nWhat are Meryl characteristics?\n1) Superior softness: The name Meryl was first used in 1996 as the name for Nylstar's Premium microfibers. The name soon became synonymous with the luxuriously soft touch customers came to expect from Meryl. For this reason Meryl has long been the choice for lingerie.\n2) Superior Stretch and Recovery: 1) Stretchable comfort. With its inherent stretch, Meryl works with elastomers to give a more comfortable overall stretch to fabrics. 2) Non-deforming - Meryl has an excellent inherent elastic recovery, (without any elastomer). Garments return to their original shape and you won’t end up with sagging or bagging meaning the garment stays looking new.\n3) Excellent moisture management: Probably the single-most important feature of any fabric used in sportswear. Meryl fibres are hydrophobic, meaning they have a very low affinity for water and dry quickly. However their ability to absorb a small amount of moisture (around 4.5% of their mass) ensures that skin stays feeling dry.\n4) Superior dye-ability & run-ablity: More vibrant colours with greater colour fastness, and a greater efficiency and lower waste in fabric and garment production.\n5) Meryl FD yarns offer high UV protection\n6) Inherent Odor Control: Meryl yarns naturally inhibit the growth of odour causing bacteria in textiles\n7) Anti-static properties: That prevent the discomfort of static charge and unsightly 'clingy' garments.\nMeryl main characteristics include light weight, high strength and softness with good durability. Even when wet, Meryl's overall strength may decrease only about 15%. And, that is minimal compared to most other natural and synthetic fibers. Meryl also dries very quickly when wet. Its proven ability to withstand abrasion is more than 20 times greater than wool and more than 10 times greater than silk or cotton. The versatility of Meryl in activewear, athletic equipement and outdoor garments has been established.\nHow does Meryl perform when Dyeing?\nColors dye rich and vibrant with a superior color fastness that manufacturers and consumers can count on. Meryl has an excellent resistance to dirt, alkalis, decay, mold and most common organic solventes. Meryl is able to withstand moderately high temperatures without losing strength.\nWhat is Wicking?\nWicking is an efficiently designed system for dispersing perspiration throughout the fabric which results in rapid evaporation of perspiration and body vapor. Meryl's Microfibers are wicking fibers.\nWhy is wicking important to the wearer of the garment?\nBecause of the wicking activity, the garment stays dry and comfortable to the wearer.\nWhy is Meryl nylon more white than other nylons?\nQuality vs Quantity\nWe give value to slower and more traditional processes when they give better products. For instance, we have maintained the original double step process for flat yarns (FDY vs. FOY). Quality starts with your ingredients and Nylstar only uses premium European polymers for the production of it’s yarns.\nWhat is Meryl Lab?\nUnderstanding Fabrics Functionality\nWhat is the difference between waterproof and water repellent?\nSimply, water repellent indicates that water beads on the fabric. Waterproof is the degree of water pressure that can be applied to a fabric, yet still keeps the water on the outside of the fabric.\nWill a fabric be waterproof even after the water repellent finish has partially worn off?\nOver time water repellent finishes tend to wear off. Yet, if the fabric is fully coated and seam taped securely you can expect the garment to be waterproof for an extended period of time.\nIs water repellency related to moisture permeability?\nYes, water repellency is closely related to moisture permeability. If water repellency is poor, water will layer on the fabric surface, which eventually prevents moisture vapor from the inside being released to the outside of the garment.\nWhat is dew condensation?\nWhat is the importance of a high density woven fabric?\nThe main purpose is for water/wind proofing and to still allow the fabric breathability since it is uncoated.\nWhat is lamination?\nIn the process of lamination, membrane is bonded to the fabric with adhesive by applying pressure and heat.\nWhat is coating?\nCoating consists of spreading a layer of polyurethane resin directly onto the fabric.\nWhich is more durable, fabrics with coating or fabrics with lamination?\nIt is difficult to say. The difference is that coatings come off gradually while delamination happens more rapidly, once the fabric begins to delaminate.\nWhat is pilling\nSmall knots are produced by individual fibers working out of the yarns during the use. They are on the surface of fabric.\nHow does pilling reduce or not form?\nThe yarn is subjected to a twisting process without giving up softness.\nWhat does it mean UV Protection?\nThe streching of a fabric during the use, perspiration or water during use affect the sun protection factor. Meryl Nateo can reach UPF 80 of the UV 801 Standard.","Wicking fabric is a new technical material that drains moisture from the body. Wicking fabrics are produced using high-tech polyester, which absorbs water extremely little compared to cotton.\nThere are several clothing alternatives available to assist you in managing if you have hyperhidrosis or extreme contextual sweating.\nThat’s why clothing made of sweat-wicking, anti-odor, breathable, and sweat-absorbing textiles is available and makes dealing with perspiration simpler.\nWhat is Wicking Fabric?\nWicking fabrics keep a user dry and convenient by pulling the skin’s moisture away and onto the fabric’s outer layer. Due to the greater surface area of these advanced fabrics, more water is absorbed away from the skin, and evaporation occurs more quickly.\nMoisture is carried via a network in the fabric, which resembles a complicated “capillary,” to the surface, where it is dispersed into a thinner layer.\nWicking fabrics are also called breathable fabrics, as air may enter or leave easily. These characteristics make wicking materials ideal for sports apparel, where increased body temperature and unavoidable sweat occur.\nSportswear often uses advanced draining materials to help athletes keep cool and maximize their performance. Also, wetsuits, jackets, and t-shirts are just a few examples of clothing items constructed using these wicking materials.\nThe speaker cloth used in arts and crafts projects and for covering speaker grilles is also from wicking fabric.\nModern materials for athletics or sportswear are designed to drain sweat and moisture away from the skin’s surface to the outside of the garment, where it may evaporate and keep the user dry and cool.\nHow is Wicking Fabric Made?\nWicking fabric types like wool, cotton and regenerated cellulose are hygroscopic fibers that absorb and desorb water vapor, which is recognized as the fabric’s buffer qualities. These fibers also give increased pleasure or at least perceived additional comfort.\nPolyester merely absorbs 0.4 percent of its weight in water, compared to cotton’s 7% absorption rate.\nFor instance, with a relative humidity of 65 percent, wool will absorb around 15 percent of its mass as water vapor and up to 35 percent at a relative humidity of 100 percent.\nWicking fabric fibers absorb and desorb water vapor when the relative humidity of the environment varies.\nAlso, because most synthetic fibers have low surface energy, clothes made from them are hydrophobic until a surface-active chemical (like detergent) is added to the liquid.\nThe ability of the interior structure to retain water and the fiber’s hygroscopic characteristics to buffer moisture are also important factors.\nA wicking gradient is created for cloth to allow moisture to be wicked from one side to the other.\nFor instance, this is easily accomplished in a two-layer fabric structure by knitting a fabric with one side made mainly of a hydrophobic yarn and the other made primarily of a more hydrophilic yarn.\nThe yarn’s affinity for liquid water is determined by a balance between the characteristics of the fiber surface.\nA Brief History of Wicking Fabric\nIt is believed that Robert Kasdan and Stanley Kornblum of New York, New York, and Monmouth Junction, New Jersey, were the first people to produce a cloth that could wick away moisture.\nThey made this discovery after knitting a synthetic material made of microfiber yarn and finding that it possesses wicking properties.\nKevin Plank, who was a student at the University of Maryland in 1996, is believed to have been the first person to sell moisture-wicking materials on a commercial scale. Plank would go on to found Under Armour.\nThe question therefore is, who should get the credit for inventing this fabric?\nEven though no one can say for certain, there is no doubt that somebody is making a lot of money off of this idea.\nTypes of Wicking Fabric\nWicking fabric, like other fabric types, consists of many types based on their level of absorbency and how they are made.\nSome of the various types of wicking fabric are:\nPolyester, a synthetic mix, is a trustworthy fabric for wicking away sweat. Polyester is an excellent material for functional clothing since it is light, breathable, and fast to dry when combined with other fabrics.\nOne disadvantage of polyester is that it may help bacteria develop and tends to hold smells.\n- Merino Wool\nMany suitable textiles are available now for wicking away sweat, and one of them is merino wool. It is a natural fabric that is lightweight and breathable, making it ideal for warmer climates.\nIt does not absorb scents as the polyester fabric does.\nHowever, compared to cotton and other synthetic materials, it is less resilient.\nNylon wicks away moisture, fends against mildew and dries rapidly. It is light, flexible, and pleasant to wear, making it ideal for training clothing. For this purpose, a lot of workout equipment is composed of nylon or probably has a high nylon ratio.\nAlthough fine nylon is very effective at wicking away moisture, it has a few significant downsides.\nIf you perspire a lot, nylon may keep odors in even after you wash it. In addition, the level of breathability of nylon can be affected by the size of the yarn as well as the weave.\nNatural fibers like wool are excellent at wicking away moisture. It is the ideal fabric for winter clothes since it regulates body temperature, is cozy, and is comfortable.\nWool, as opposed to merino wool, is not as soft, and it has the potential to irritate skin that is already sensitive. Additionally, it does not possess the same level of tensile strength as man-made materials.\nAlmost the same as polyester, polypropylene is a thermoplastic polymer. Polypropylene is perfect for use in chilly wear and apparel because, in addition to wicking moisture, it dries rapidly and is well acclaimed for its thermal qualities.\nHowever, like polyester, it has a propensity to retain smells and isn’t quite as soft as some of the other textiles on the list.\n- Bamboo fabric\nBamboo is made from plants, much as micro modal; therefore, its fibers naturally wick moisture. It helps maintain homeostasis efficiently and has a soft, relaxed feel, making it perfect for year-round use.\nBut bamboo moisture-wicking clothing is more costly than cotton or other natural fibers.\nOne of the most breathable sweat-wicking materials currently in use is micro modal. Micromodal fabric offers outstanding temperature-regulating abilities, making you feel convenient in any climate.\nBecause it has a very smooth, silk-like feel, it is perfect for loungewear, intimate apparel, and undergarments.\nMicro modal requires greater attention than moisture-wicking textiles since they may sometimes pill.\nAlso, micro modal fabric aids in controlling body temperature but doesn’t retain heat or maintain a warm temperature.\nThe average cost of wicking fabric by the yard\nWicking fabric types are numerous and may vary depending on various factors such as their production methods.\nWicking cloth costs average between $7 and $55 per yard. It’s important to note that the cost varies according to the wicking fabric’s kind and quality.\nThe production process’s components make certain other fabrics more expensive.\nWicking fabric usage and application\nWicking fabrics are breathable and can be used in many ways. Some of the applications you’ll find wicking fabric includes:\n- Outdoor clothing\nWicking fabric is the ideal material for any athletics, activewear, and clothes used for outdoor activities due to its sweat-diffusing qualities. It’s also becoming more common for daily clothing, undergarments, and even bedding in hotter regions of the globe.\nSportswear is an excellent use for fabrics with moisture-wicking qualities since they draw sweat away from the body and into the outside air. Wearers feel more at ease and are cooled off by this.\nMerino wool is now obtainable in lightweight fabrics perfect for undershirts and athletic wear, so it is not only reserved for sweaters.\n- Baby Clothing\nMany baby care specialists and product manufacturers now employ these fabrics due to the moisture-wicking abilities of some of the wicking fabric types listed above.\nThey come in handy in the production of baby clothing and diapers.\nWicking Fabric Care and Maintenance Tips\nWicking fabric must be cared for and maintained properly to ensure it retains its moisture-wicking property, and failure to ensure this may result in the fabric becoming damaged.\nYou can follow these tips outlined below to do this.\n- Before washing the fabric, check the label to ascertain the maintenance method suited for that fabric.\n- Wicking fabric may be spot-cleaned and is constructed of low-maintenance materials. So you do not need much to ensure proper care of the fabric.\n- When necessary, you can wash the cloth with a mild detergent in moderate washer settings. Harsh chemicals may damage the fabric.\n- You may air dry your fabric to prevent damage or melting from a dryer’s heat, as some fabric types are pretty soft. The time required for air drying is substantially less than for materials that absorb water since most water will bounce off the material. Just shake the material to remove extra water droplets, then leave it to dry.\n- Store your wicking fabric in cool, dry areas to avoid dampness and moisture absorption, which could result in mold formation. You wouldn’t want that to happen to your fabric."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:15d478e6-b949-431b-9fe9-dd1da45d5e15>","<urn:uuid:64dab131-75bd-42a8-a454-7854c3fd51a8>"],"error":null}
{"question":"I study molecular biology and transcription regulation. Could you compare the depth measurements of the Congo River with the depth at which transcription initiation mechanisms operate?","answer":"While the Congo River reaches remarkable depths of 220 meters at its deepest point, transcription initiation mechanisms operate at the molecular level. The processes of transcription initiation and regulation involve interactions between RNA polymerase, DNA, and various protein factors, with changes in specificity occurring through contact between small patches of proteins or small regions of DNA. These molecular-level interactions are many orders of magnitude smaller than the Congo River's depth.","context":["- American Academy of Microbiology, 2009\n- American Cancer Society Postdoctoral Fellow, 1980-1982\n- Ph.D., University of Illinois, 1980\n- M.S., University of Illinois, 1976\n- B.S., University of North Carolina at Chapel Hill, 1974\nOur overall goal is to elucidate how the process of transcription initiation and activation is regulated at a molecular level.\nControlling the process of transcription is fundamental to gene expression, gene regulation, and development. In all organisms, RNA polymerase, a complex protein machine that transcribes genomic DNA into RNA, performs transcription. A single RNA polymerase in bacteria and archaea—and only three different polymerases in eukaryotes—program an amazing variety of developmental pathways. In most cases, factors that alter the initiation, elongation, or termination of transcription control the vast array of transcriptional outcomes.\nBecause transcription initiation proceeds through multiple steps, this process provides multiple points for regulation. In particular, transcriptional activators, co-activators, and repressors can interact with the template DNA and/or RNA polymerase to modulate core promoter selection. Early work suggested that the process and regulation of transcription initiation fundamentally differed between bacterial polymerase and higher organisms. However, more recent work has demonstrated that all kingdoms of life retain many features of this process. Biochemical and structural studies reveal significant functional similarities among bacterial, archaeal, and eukaryotic RNA polymerases. Throughout life, factors that interact with RNA polymerase and with sequences close to or within the core promoter itself can alter promoter recognition. In many cases, these factors interact with only a small surface of RNA polymerase, yet they impose a major specificity change through this contact.\nMy lab focuses on elucidating these mechanisms of transcription initiation and regulation. We employ simple bacterial and bacteriophage model systems, because these systems can be defined in detail biochemically and investigated at a molecular level. In particular, we have investigated the activation of phage T4 promoters during T4 infection and the regulation of promoters that express virulence gene products in the pathogens Bordetella pertussis and Vibrio cholerae.\nOur work on T4 promoter activation established a new paradigm for transcriptional activation, called sigma appropriation. We demonstrated how the binding of a small T4 protein structurally remodels a portion of the specificity subunit (sigma) of RNA polymerase. This, in turn, allows a T4 activator to interact with a portion of sigma that would normally be occluded by RNA polymerase and to interact with a portion of the DNA that would normally be bound by sigma. These interactions result in the formation of a remodeled specificity factor for RNA polymerase that recognizes a new promoter sequence. This work reveals at a molecular level how reconfiguring a small portion of RNA polymerase can completely alter promoter specificity.\nTo conduct our work on virulence gene regulation, we collaborate with the laboratory of Dr. Scott Stibitz at the U.S. Food and Drug Administration. B. pertussis is a reemerging pathogen and although there is a vaccine for pertussis, its effectiveness wanes after only a few years. We are interested in elucidating the details of gene regulation by the B. pertussis global response regulator, BvgA, which regulates all the known pertussis virulence genes. Our overall goal is to provide the intellectual basis for developing more effective strategies for combating the disease. Our work has revealed a unique architecture for the interaction of the Bordetella pertussis response regulator (BvgA) with a particular virulence gene promoter. We have found that BvgA and two subunits of RNA polymerase occupy the same region of DNA. Using molecular modeling, we have demonstrated how this is possible: they are located like spokes on a wheel around the DNA double-helix.\nMost recently, we have collaborated with the laboratory of Dr. Christopher Waters at Michigan State University to investigate how the V. cholerae response regulator VpsR together with the small molecule regulator c-di-GMP, regulates biofilm formation. Understanding this process is fundamental to the development of anti-bacterial strategies because biofilms shield pathogens from environmental stresses, nutrient loss, and most, importantly antibiotics. Our goal is to understand this activation process at a molecular level.\nIn all of our systems, we combine classic protein and nucleic acids biochemistry with state-of-the-art structural and molecular modeling techniques to understand the protein-protein and protein-DNA contacts that are needed for regulation.\nApplying our Research\nThe emergence of antibiotic resistant pathogens forces us to consider other ways to combat bacterial infections. Our work contributes to the development of anti-microbial strategies using two systems. First, we investigate bacteriophages, viruses that infect bacteria and contain multiple mechanisms to disturb, overtake, and kill a bacterial cell. Bacteriophage T4 disrupts the ability of E. coli RNA polymerase to interact correctly with DNA. Our study of the molecular mechanism of this process yields insight into how small molecules can be generated to thwart bacteria. Second, we study Bordetella pertussis, which causes pertussis (whooping cough), a highly contagious upper respiratory infection. Although B. pertussis infections are one of the most prevalent preventable diseases, vaccine efficacy wanes after a few years; the recent increase in pertussis disease, including thousands of cases in the United States each year, has been attributed in part to decreased vaccination coverage and suboptimal vaccines. Sadly, this has resulted in regional epidemics of pertussis and infant mortality. Understanding how B. pertussis expresses its virulence genes at a molecular level provides a basis for better drug and vaccine design. Understanding the expression of genes needed for biofilm formation, using Vibrio as a model system, can lead to the development of small molecules that disrupt formation/maintenance of biofilms, whose presence shields pathogens from typical antibiotics.\nOur work also has broader implications. There are multiple human diseases in which normal cell development has gone awry, such as cancer and autoimmune conditions like diabetes, rheumatoid arthritis, and Crohn’s disease. Although regulation of gene expression in human cells is certainly more complicated than that in bacteria, it is now clear that the basic steps of transcription initiation and activation are in fact shared among all organisms. Consequently, fundamental research into the mechanics of transcription and gene expression in simple bacterial systems provides insights that can be extended throughout biology.\nNeed for Further Study\nThe ability of RNA polymerase to correctly select the right promoter sequence at the right time is fundamental to the control of gene expression in all organisms. However, how the interaction of a factor or factors with RNA polymerase redirects polymerase to different promoter sequences is not well understood at a molecular level. This need for more knowledge exists despite available structures of RNA polymerases and various activators. A major question in our field is how contact between a small patch of protein with a small region of DNA or another protein can determine whether a gene is expressed or is silent.\n- VpsR Directly Activates Transcription of Multiple Biofilm Genes in Vibrio cholerae.\n- Hsieh ML, Waters CM, Hinton DM.\n- J Bacteriol (2020 Aug 25) 202. Abstract/Full Text\n- VpsR and cyclic di-GMP together drive transcription initiation to activate biofilm formation in Vibrio cholerae.\n- Hsieh ML, Hinton DM, Waters CM.\n- Nucleic Acids Res (2018 Sep 28) 46:8876-8887. Abstract/Full Text\nResearch in Plain Language\nBecause gene expression is crucial for normal development, all organisms have multiple mechanisms to ensure that the correct genes are expressed at the correct time. Many of these mechanisms occur during transcription, the process of transcribing DNA into RNA. We study the regulation of transcription in very simple cells as model systems for understanding how this process is regulated at a molecular level. Our work has identified proteins and DNA sequences that play key roles in the expression of virulence genes in Bordetella pertussis, the causative agent of whooping cough, in biofilm formation in Vibrio cholerae, and in a virus that infects E. coli. Our work provides insights into how similar processes work in higher organisms and may reveal new antibacterial strategies.","Interested to know about fun facts about rivers? Of all the bodies of water in the world, few are as impressive as the great rivers. Crucial to the development of some of the world’s most noteworthy cities, these rivers are the thoroughfares from which much of civilization was built. Here is a list of the ten longest and most important rivers in the world.\nThe Amur River is the tenth longest river in the world. It originates from western Manchuria and flows eastward to form the border between China and Russia. From there it courses to the southwest in a 400 kilometer arc.\nIn terms of water discharged, the Congo is the second largest river in the world. Located in Africa, the Congo River also has the distinction of being the world’s deepest rivers, measuring 220 meters at its deepest point. Its length has been measured at more than 4,700 kilometers.\nLocated in the southern section of Central South America, Parana River runs through Brazil, Paraguay and Argentina. Its length has been measured at over 4,800 km. One of the most important rivers in the region, Parana River is rivaled only by the Amazon River in terms of sheer length.\nOb River is located in the western Siberian region of Russia. Like the Yenisei, it is one of the three major rivers in Siberia and flows into the Arctic. The river is also noteworthy for having the longest estuary in the world as its gulf.\nThe Yellow River is second only to the Yangtze as the longest river in China. it is also the world’s sixth-longest river with a length that has been estimated at more than 5,464 km. The Yellow River starts out at the Bayan Har Mountain range of Qinghai Province in the western section of the country, and it meanders through nine Chinese provinces before emptying into the Bohai Sea.\nOf all the river systems that flow into the Arctic, the Yenisei River has the distinction of being the largest. One of the three Siberian Rivers that wind into the Arctic Ocean, the Yenisei River starts out in Mongolia and courses throughout much of the central portion of Siberia.\nThe Mississippi is the main river of North America’s largest river system. Its entire length is located in the United States, beginning in the northern section of Minnesota and winding south to the Mississippi River Delta. Its length has been measured at more than 6,275 kilometers.\nThe Yangtze River has the distinction of being Asia’s longest river. It is also the world’s third longest river, with an impressive length of more than 6,300 kilometers. The Yangtze starts out from the Qinghai-Tiber glaciers and empties out into the East China Sea at Shanghai province.\nThe largest river in the world in terms of volume of water discharged is the Amazon. It is also the world’s second longest river at 6,400 kilometers, and its drainage basin spanning a vast expanse of 7,050,000 square kilometers is the largest in the world.\nThe Nile is the longest river in the world, with a length of about 6,650 kilometers. In recent years, the status of the Nile as the world’s longest river has been the subject of contention, due to issues with regard to the true source of the Amazon and therefore, its actual length. At present however, the Nile is still widely considered to be the longest river in the world."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:3cea60e0-e59b-4cd4-9f91-4ceecb977708>","<urn:uuid:b1a574e8-ffd0-4d0b-b0b4-90e49dfd1a0f>"],"error":null}
{"question":"What's the standard way to print 'hello world' in Factor?","answer":"In Factor, to print 'hello world', you simply write: 'hello world' print. This uses the io vocabulary where print is a word that takes a string from the stack and outputs it to the current output stream.","context":["This article relies too much on references to primary sources. (July 2019)\n|Paradigm||multi-paradigm: functional, concatenative, stack-oriented|\n0.98 / July 31, 2018\n|Typing discipline||strong, dynamic|\n|OS||Windows, macOS, Linux|\n|Joy, Forth, Lisp, Self|\nFactor is a stack-oriented programming language created by Slava Pestov. Factor is dynamically typed and has automatic memory management, as well as powerful metaprogramming features. The language has a single implementation featuring a self-hosted optimizing compiler and an interactive development environment. The Factor distribution includes a large standard library.\nSlava Pestov created Factor in 2003 as a scripting language for a video game. The initial implementation, now referred to as JFactor, was implemented in Java and ran on the Java Virtual Machine. Though the early language resembled modern Factor superficially in terms of syntax, the modern language is very different in practical terms and the current implementation is much faster.\nThe language has changed significantly over time. Originally, Factor programs centered on manipulating Java objects with Java's reflection capabilities. From the beginning, the design philosophy has been to modify the language to suit programs written in it. As the Factor implementation and standard libraries grew more detailed, the need for certain language features became clear, and they were added. JFactor did not have an object system where you could define your own classes, and early versions of native Factor were the same; the language was similar to Scheme in this way. Today, the object system is a central part of Factor. Other important language features such as tuple classes, combinator inlining, macros, user-defined parsing words and the modern vocabulary system were only added in a piecemeal fashion as their utility became clear.\nThe foreign function interface was present from very early versions to Factor, and an analogous system existed in JFactor. This was chosen over creating a plugin to the C part of the implementation for each external library that Factor should communicate with, and has the benefit of being more declarative, faster to compile and easier to write.\nThe Java implementation initially consisted of just an interpreter, but a compiler to Java bytecode was later added. This compiler only worked on certain procedures. The Java version of Factor was replaced by a version written in C and Factor. Initially, this consisted of just an interpreter, but the interpreter was replaced by two compilers, used in different situations. Over time, the Factor implementation has grown significantly faster.\nFactor is a dynamically typed, functional and object-oriented programming language. Code is structured around small procedures, called words. In typical code, these are 1–3 lines long, and a procedure more than 7 lines long is very rare. Something that would idiomatically be expressed with one procedure in another programming language would be written as several words in Factor.\nEach word takes a fixed number of arguments and has a fixed number of return values. Arguments to words are passed on a data stack, using reverse Polish notation. The stack is used just to organize calls to words, and not as a datastructure. The stack in Factor is used in a similar way to the stack in Forth; for this, they are both considered stack languages. For example, below is a snippet of code that prints out \"hello world\" to the current output stream:\n\"hello world\" print\nio vocabulary that takes a string from the stack and returns nothing. It prints the string to the current output stream (by default, the terminal or the graphical listener).\nThe factorial function can be implemented in Factor in the following way:\n: factorial ( n -- n! ) dup 1 > [ [1,b] product ] [ drop 1 ] if\nNot all data has to be passed around only with the stack. Lexically scoped local variables let you store and access temporaries used within a procedure. Dynamically scoped variables are used to pass things between procedure calls without using the stack. For example, the current input and output streams are stored in dynamically scoped variables.\nFactor emphasizes flexibility and the ability to extend the language. There is a system for macros, as well as for arbitrary extension of Factor syntax. Factor's syntax is often extended to allow for new types of word definitions and new types of literals for data structures. It is also used in the XML library to provide literal syntax for generating XML. For example, the following word takes a string and produces an XML document object which is an HTML document emphasizing the string:\n: make-html ( string -- xml ) dup <XML <html> <head><title><-></title></head> <body><h1><-></h1></body> </html> XML> ;\ndup duplicates the top item on the stack. The\n<-> stands for filling in that part of the XML document with an item from the stack.\nFactor includes a large standard library, written entirely in the language. These include\nA foreign function interface is built into Factor, allowing for communication with C, Objective-C and Fortran programs. There is also support for executing and communicating with shaders written in GLSL.\nFactor is implemented in Factor and C++. It was originally bootstrapped from an earlier Java implementation. Today, the parser and the optimizing compiler are written in the language. Certain basic parts of the language are implemented in C++ such as the garbage collector and certain primitives.\nFactor uses an image-based model, analogous to many Smalltalk implementations, where compiled code and data are stored in an image. To compile a program, the program is loaded into an image and the image is saved. A special tool assists in the process of creating a minimal image to run a particular program, packaging the result into something that can be deployed as a standalone application.\nEdited: 2021-06-18 18:12:58"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f7e892b9-9abe-466e-8d5c-2471b57a5af1>"],"error":null}
{"question":"How do the fuel injection methods differ between gasoline and diesel engines?","answer":"In diesel engines, fuel meets the compressed in-cylinder air at the point of combustion, while in gasoline engines, the fuel and air mix before combustion takes place. Diesel engines can use multiple fuels introduced sequentially into the combustion chamber, with a low-reactivity fuel followed by a high-reactivity fuel, allowing for higher compression and thermodynamic efficiency.","context":["The Advantage of Renewable Fuels in High-Efficiency Engines\nApproximately 95% of the energy used in transportation applications comes from fossil fuel sources, so improving the efficiency of engines can substantially reduce their fuel consumption. Furthermore, by making our fuels from renewable sources, such as plants, we can reduce the environmental impact of combustion. However, using new types of fuels in more efficient engines can sometimes produce undesired results such as increased emissions, so basic research backed by systematic engine testing is critical to ensuring that engines and fuels perform optimally. Scientists at the Combustion Energy Frontier Research Center (CEFRC) have developed a more efficient and less polluting engine concept and verified its performance using new fuels through extensive fundamental research.\nThe engine concept uses multiple fuels that are introduced into the combustion chamber sequentially. By injecting a low-reactivity fuel followed by a high-reactivity fuel, the engine can compress the gases more and achieve higher thermodynamic efficiency than typical engines. Due to their ready availability and large difference in reactivity, gasoline (low reactivity) and diesel (high reactivity) are frequently used. By varying the relative amounts of these fuels, the engine's computer can optimize the emissions and efficiency simultaneously.\nIn a recent study, the team tested several methods of further optimizing this concept. They compared the efficiency and nitrous oxide (NOx) emissions in four cases, using two fuel combinations in two engine designs. The engine designs were a piston optimized for high efficiency and a stock engine piston. The fuels tested were methanol/diesel and gasoline/diesel, recognizing that methanol is a renewable fuel.\nThe results showed that substituting methanol for gasoline as the low-reactivity fuel while using the stock piston increased the NOx emissions from the engine. Using the high-efficiency piston increased the NOx emissions with both methanol/diesel and gasoline/diesel fuel combinations compared to the stock piston using the same fuel combination. Higher temperatures were achieved in the engine cylinder with the high-efficiency piston, leading to the higher NOx emissions. Nonetheless, the NOx emissions were still below target emission levels established by the government. This result shows that trade-offs between different objectives are often required when optimizing engine designs.\nThe experiments also revealed some advantages of the new engine concept. For instance, using the methanol/diesel fuel combination with the stock piston design improved the efficiency of the engine operation by approximately 5% compared to using the gasoline/diesel combination. Similarly, for the high-efficiency piston design, the efficiency of the engine when operating with methanol/diesel fuel combinations was higher than when operating with gasoline/diesel fuel combinations. The researchers found that the efficiency of the engine increased by nearly 10% when using the methanol/diesel fuel combinations with the high-efficiency piston compared to using the gasoline/diesel combinations with the stock piston. Thus, by using methanol with the high-efficiency piston, the efficiency of the engine can be substantially increased leading to a similar increase in gas mileage.\nThese promising results for methanol suggest that investigating other renewable fuel options may uncover other advantages, perhaps by using renewable fuels for the high-reactivity fuel in addition to the low-reactivity fuel.\nThe work presented here was supported by the Direct-injection Engine Research Consortium (NR Walker) and the Combustion Energy Frontier Research Center (AD Dempsey), an Energy Frontier Research Center, funded by the U.S. Department of Energy, Office of Science, Office of Basic Energy Sciences.\nDempsey AB, NR Walker, and R Reitz. 2013. \"Effect of Piston Bowl Geometry on Dual Fuel Reactivity Controlled Compression Ignition (RCCI) in a Light-Duty Engine Operated with Gasoline/Diesel and Methanol/Diesel.\" Society of Automotive Engineers International Journal of Engines 6(1):78-100. DOI: 10.4271/2013-01-0264","One of the primary differences between diesel and gas engines exists in the type of combustion each uses. As was discussed above, in a diesel, when fuel finally meets the compressed in-cylinder air, combustion is the result. In a gasoline engine, fuel and air mix before combustion ever takes place.\nDiesel Fuel Filters Are Different - ECOGARDSep 19, 2017 · However, for anyone driving a diesel-powered vehicle, these guidelines for changing fuel filters do not apply. As a professional installer, its important to know that some of your customers may not understand why diesel fuel filters are different. Clean fuel is essential for all diesel fuel systems.\nTo understand that all diesel fuels are not the same. OBJECTIVES:The student will:1. Learn to properly identify the different types of diesel fuel. 2. Learn how each fuel can affect MPG. 3. Learn how to select diesel fuel by anticipated weather conditions. LESSON / INFORMATION:Diesel fuels are classified 1D, 2D, and 4D.\nDiesel fuel explained - U.S. Energy Information Diesel fuel is made from crude oil. Diesel fuel is refined from crude oil at petroleum refineries. U.S. petroleum refineries produce an average of 11 to 12 gallons of diesel fuel from each 42-gallon (U.S.) barrel of crude oil. Before 2006, most diesel fuel sold in the United States contained high quantities of sulfur. Sulfur in diesel fuel\nDiesel vs. Gasoline Engines - Differences Explained - Auto Aug 27, 2020 · Diesel engines can run off all sorts of different types of fuel that you wont find at the pumps of your local gas station. The four primary alternative fuels are biodiesel (fuel made from a combination of various resources, often from recycled products), Fischer-Tropsch ( F-T ) diesel, dimethyl ether ( DME ), and vegetable oils.\nApr 29, 2020 · Diesel fuel simply packs more energy in every gallon than gas fuel, which makes it more economical overall. Diesel engines are still more efficient than gas\nDiesel vs. Gasoline:Everything You Need to KnowDiesel fuel simply packs more energy in every gallon than gas fuel, which makes it more economical overall. Diesel engines are still more efficient than gas engines, but less so for those who are\nFuel types explained:e10, 91, 95 & 98 Savings.auPetrol types in Australia. When it comes to the different fuel types, its the number that really matters 91, 95 and 98. These numbers are called the octane-rating, and are an indication of how well the fuel resists burning too early inside the cars engine.\nDiesel Fuel Grades. Historically, the quality of automotive fuels in the United States was specified by ASTM standards. Diesel fuels are covered by the ASTM D975 standard. Since 2004, the D975 standard has covered seven grades of diesel, Table 1. Heavier fuel oils Grade 5 and 6 (residual), which are used primarily for heating purposes, are\nPetrol vs. Diesel:The Difference Between These Fuels and Jul 12, 2017 · As mentioned, despite both fuel types being created from crude oil, there are many differences between the two. The following is a list of some of the more notable differences between\nThe Truth About Premium Diesel Fuel Auto Expert by Dec 04, 2017 · The diesel fuel quality standard lays out the complete list of legislated diesel properties >> The point is:Your diesel engine is designed to run on diesel fuel that meets this standard. And it cannot be sold here unless it meets the standard. And this means the truth about premium diesel is 95 per cent marketing bullshit.\nUNDERSTANDING THE DIFFERENCES BETWEEN DIESEL FUELS. The Center for Quality Assurance (CQA) has developed a new standard for diesel fuel known as the Top Tier Diesel Fuel is different from the program for diesel fuel. Among these differences is the fact that the gasoline\nUnderstanding Diesel Fuels - dummiesStandard diesel fuel (sometimes called diesel oil) comes in two grades:Diesel #1 (or 1-D) and Diesel #2 (or 2-D). The higher the cetane number, the more volatile the fuel. Most diesel vehicles use fuel with a rating of 40 to 55. You wont have to worry about which type to use because all diesel automakers specify Diesel #2 for normal driving conditions.\nWhat is Difference Between Petrol and Diesel Engine Sep 21, 2015 · S.No. Petrol Engine:Diesel Engine 1. The petrol engine works on Otto cycle i.e. on constant volume.:The diesel engine works on diesel cycle i.e. on constant pressure. 2. The air and petrol are mixed in the carburetor before they enter into the cylinder.:The fuel is fed into the cylinder by a fuel injector and is mixed with hot compressed air inside the cylinder.\nNov 28, 2017 · There are two types of injectors in a diesel injection system. These are categorized depending upon how the fuel is injected in the system:Mechanical fuel injectors; Electronic fuel injectors; This is all for the basic differences between the injection systems. Hope you understand how the engine type affects the injection of a fuel.\nWhat is the Difference Between Jet Fuel & Diesel Fuel Jet fuel of types A and A-1 is composed of mostly kerosene, and Jet B is a naptha-kerosene mix. Diesel gas is approx. 75 percent kerosene, with added lubricants and a low-sulfur content.\nWhat is the Difference Between Jet Fuel & Diesel Fuel Jet-A is more similar to Diesel #1, which is lighter than Diesel #2 (automotive diesel), and so is not a perfect substitute, but in emergency situations is a potential fuel source. Jet-A usage will not cause instant physical damage but long-term wear will increase due to lack of lubrication, different burn temperatures and energy output.\nDiesel fuels are broken up into 3 different classes:1D(#1), 2D(#2) and 4D(#4).The difference between these classes depends on viscosity (the property of a fluid that causes a resistance to the fluids flow) and pour point (the temperature at which a fluid will flow). #4 fuels tend to be used in low-speed engines.#2 fuels are used in warmer weather and are sometimes mixed with #1 fuel to"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:86924f32-b04e-4add-a82f-e001d3ae5ced>","<urn:uuid:bf0006df-efc4-41ff-9d5c-2addd1b224c5>"],"error":null}
{"question":"Hey! I'm fascinated by ancient shipwrecks - could you compare the preservation methods used for the Antikythera wreck's human remains versus the Mary Rose's hull structure?","answer":"The preservation methods for these shipwrecks were quite different. The Antikythera human remains were recovered from the seabed after 2,100 years and appear to be in fairly good condition, awaiting DNA analysis at the Natural History Museum of Denmark. In contrast, the Mary Rose's hull structure required continuous spraying at controlled temperatures during its initial restoration process in a special restoration shed. The exposed timbers of the Mary Rose had rotted over centuries, but the portion buried in mud and clay survived remarkably well, requiring careful lifting and preservation techniques in Portsmouth's historic naval shipyards.","context":["An international team of archaeologists excavating the famous Antikythera shipwreck in the Aegean Sea off the coast of Greece have found the remains of a human skeleton among the 2,100-year-old wreckage.\nIf they can extract DNA from the bones, it'll help us understand more about the passengers on board the ship when it sank around 65 BC – and could provide vital insight into the mysterious Antikythera Mechanism it was carrying, better known as the world’s first computer.\n\"Archaeologists study the human past through the objects our ancestors created,\" said team member Brendan Foley, from the Woods Hole Oceanographic Institution (WHOI) in the US.\n\"With the Antikythera Shipwreck, we can now connect directly with this person who sailed and died aboard the Antikythera ship.\"\nIn case you’re unfamiliar, the Antikythera shipwreck – which was originally discovered by sponge divers in 1900 – is the largest ancient shipwreck ever found.\nDuring the first excavation by the sponge divers, they managed to pull up a series of marble statues and thousands of other sea-battered artefacts.\nAmong the random assortment of things was the Antikythera Mechanism, a clockwork device that might have been used to predict astronomical events and is widely referred to as the world’s first analogue computer.\nResearchers have dated the device to around 100 or 150 BC, which means that – after it sank – similar technology didn’t pop up around the world until at least the 14th century AD, which makes the device all the more fascinating.\nIn 1976, famed ocean explorer Jacques-Yves Cousteau and his crew returned to wreckage after it laid untouched for nearly 80 years. They were able to pull up some 300 objects, including skeletal remains, though DNA technology wasn't available back then to fully analyse them.\nNow, a new team – consisting of researchers from WHOI and the Hellenic Ministry of Culture and Sports – has uncovered more remains beneath shards of pottery, and this time they hope to shed new light on who was on board the ill-fated ship, where they were from, and where they might have been going.\nThe newly found remains were pulled up on 31 August and – after they get released by the Greek government – will be sent to researchers at the Natural History Museum of Denmark in Copenhagen for further analysis.\n\"Against all odds, the bones survived over 2,000 years at the bottom of the sea and they appear to be in fairly good condition, which is incredible,\" said DNA expert Hannes Schroeder, from Natural History Museum of Denmark.\nIn addition to finding these incredible remains, the archaeologists working at the site have also started to use 3D imaging to instantly share their findings with other researchers – and the public, too – without even having to pull them to the surface.\nWe're still waiting on a peer-reviewed paper to be published about the new find, so right now we're taking the researchers' words for it, but we imagine they're going to hold off writing anything up until they have the DNA analysis results.\nThat said, there's no guarantee DNA sequencing will work on a skeleton of this age – especially one that's been at the bottom of the sea for millennia – but if it does, we could finally have more information on one of the oldest, largest, and most mysterious shipwrecks ever to be found.\nWe love how excited the archaeologists were when they first spotted the remains. We're right there with you, guys.","Conway Maritime established a reputation for a series of books that provided a high quality set of images, many unique, that described the structure and technology of the subjects. Since then, the imprint has passed through a number of hands and each new owner has managed to maintain the high standards originally achieved. This new book has demonstrated that Bloomsbury Publishing is a safe pair of hands for the Conway imprint. There is text and it is very good descriptive material that fully supports the fine selection of images. The subject is unique and this book shows just why the Mary Rose is such an important vessel. Those who filed through the original restoration shed were awed by the remains of this warship. It was a damp cold dark experience because the remaining hull structure had to be sprayed continuously at a controlled temperature during the initial restoration process. The author provides a brief history and introduction with some outstanding images and specially produced drawings. Highly Recommended.\nNAME: Anatomy of the Ship, Tudor Warship Mary Rose\nAUTHOR: Douglas McElvogue\nPUBLISHER: Conway, Bloomsbury Publishing\nBINDING: soft back\nGENRE: Non Fiction\nSUBJECT: Tudor, 16th Century, naval architecture, guns, artillery development, firearms, bows, major warships, line-of-battle, sailing navy, wooden warships, archaeology, restoration\nDESCRIPTION: Conway Maritime established a reputation for a series of books that provided a high quality set of images, many unique, that described the structure and technology of the subjects. Since then, the imprint has passed through a number of hands and each new owner has managed to maintain the high standards originally achieved. This new book has demonstrated that Bloomsbury Publishing is a safe pair of hands for the Conway imprint. There is text and it is very good descriptive material that fully supports the fine selection of images. The subject is unique and this book shows just why the Mary Rose is such an important vessel. Those who filed through the original restoration shed were awed by the remains of this warship. It was a damp cold dark experience because the remaining hull structure had to be sprayed continuously at a controlled temperature during the initial restoration process. The author provides a brief history and introduction with some outstanding images and specially produced drawings. Highly Recommended.\nThe story of the Mary Rose may seem sad, or tragic, but as a warship she enjoyed a relatively long life and, when she sank, she was to become the only restored example of a key development in the history of the sailing warship. She set off from Portsmouth under the eyes of King Henry VIII to take on a French fleet that was offshore. As she turned on leaving harbour, she started to take on water and rapidly sank, with heavy loss of life. There have been several theories as to why she sank as she did, but much information has emerged during her careful restoration.\nSinking on her side, the Mary Rose sank into the mud and clay so that an almost complete elevation was buried. Over the centuries that followed, the exposed timbers rotted but what was buried survived remarkably well. The decision to raise and preserve the remains was a brave one and there was no certainty that the process would be successful, or that anything significant could be preserved. There was simply no comparable experience to give any assurance.\nA special lifting frame was constructed, taken out to the wreck site and fitted around the remains. The lift barge then began the painstaking process of slowly bringing the remains to the surface and then to the restoration shed in Portsmouth’s historic naval shipyards. It was a memorable sight for anyone fortunate to watch the process – one of those great memories that lasts for ever, in every detail.\nOne of the early tasks was in removing remaining mud and sifting it for artefacts. It was only then that the restoration team realized how much unique and priceless items were contained in what was left of the hull.\nMary Rose is now part of a fascinating museum in a new building. It is one of those maritime museums that must be visited.\nThis new Conway book provides a detailed view into a key piece of naval technology. It does this in a unique way with drawings and photographs that are each of the highest quality and finest detail. This tells a story that can only be told in this way and the publisher has done an outstanding job of production, at an amazingly low cover price.\nWhat makes the Mary Rose such an important find and restoration is that not only is there an almost complete elevation, but the team restoring the remains have been able to extract so much additional detail with an amazing collection of fittings and personal equipment, together even with a skeleton ship’s dog. The work that has gone into the archaeology is unparalleled and this is covered very well in the book.\nWhat makes the Mary Rose such an important ship is not just that she is the only vessel of her type to be restored to provide such a detailed collection of knowledge. The Mary Rose was one of a small number of vessels that marked a significant change in naval technology. She carried her guns mainly in broadside batteries and that allowed her and her fleet to sail in line as the first line-battle-ships. She was the Dreadnought of her age, changing naval warfare for ever.\nBefore the Mary Rose and a handful of similar ships, England had maintained very few vessels designed specifically and exclusively for war. King Arthur did build a relatively small navy of longships to defend against Norse attacks, but they do not appear to have been consistently maintained as a naval force, rather they were built, enjoyed a short life and periods passed when few if any were serviceable. The differences between fighting longships and their commercial knar sisters were not great. They were open boats equipped with a single square sail but were normally fought under oar power. As commercial craft developed and became larger, there is no evidence that warships were specifically built for the purpose. It seems that Kings impressed commercial vessels into naval service and carried out modifications that were later removed if the vessel returned to commercial use. The primary modification was to add a castle at stern and bow to provide fighting platforms for archers and spear-men. In some cases, the lookout platform high in the main mast may have been enlarged for archers. As guns became a more common weapon, these were added to impressed merchant craft but were issued in small numbers and mainly sledge mounted. The common round in use was stone balls. The Mary Rose also used stone ball ammunition but her guns were capable of firing iron shot.\nHenry VIII was a keen developer of naval and land power. He commissioned castles, designed specifically for guns, and watched the Mary Rose sink from his vantage point at Southsea Castle which was, and is, a fine example of the Tudor gun fort. He also went to considerable effort to acquire modern guns, both artillery and personal firearms. As with the construction of castles designed to mount guns, he encouraged the building of ships to carry guns. The Mary Rose did include a large number of bows with a large supply of arrows, but she also carried personal firearms. What is notable is the extent of her gun armament and the size of the largest pieces.\nThe Mary Rose was a Great Castle ship. She introduced the broadside as a method of engagement, although she carried a number of smaller guns in the fore and aft castles, including guns that could fire down into the waist of the ship against boarders or mutineers. Most interestingly, she carried 40 pounder long guns on her main gun deck, larger than those carried in 1805 by HMS Victory. Tudor heavy naval guns had a greater range than those of Nelson’s day. What is not entirely clear is why the Tudor warships carried such a range of firearms. Some have speculated that this demonstrates an exciting period in the development of the gun where many different sizes and forms were experimented with. Alternatively, it has been suggested that the variation was a matter of supply rather than preference of or experiment. There is certainly evidence that Henry VIII pursued a number of sources of supply in an attempt to acquire the numbers of guns he required.\nEven in 1805, a major warship would carry a number of sizes of gun, with the lowest gun deck carrying the largest guns that were all of one type and size. The next gun deck carried smaller guns and the third gun-deck the smallest long guns, but with the heaviest guns being short range cannonades, or smashers, on the same deck. Swivel guns were carried on the upper deck and in the fighting tops, being used also on the larger ships boats. Against the mixed armament of 1805 on major warships, perhaps the Tudor use of a number of different types of gun is not remarkable. However, the 1805 RN warship did have a standard gun format of cast iron canon, mounted on four wheel trucks, and a small number of short barrel heavy guns on slide mounts. In shape and construction, the 32 pounder, 24 pounder and 12 pounder guns where the same, differing only in size, and using the same design of four wheel truck mount. The Mary Rose demonstrates a variety of mounts, including four wheel wooden trucks, sledge mounts and swivels. The muzzle loading cast gun was in common use, but the forged gun was equally common and, although forged guns were prone to the forging breaking down, with a short in service life, it has been argued that they were cheaper and quicker to produce, offered improved performance initially, and also offered a higher rate of fire because they were supplied with three chambers that were charged and breech loaded one after the other. Where the forged canon started out on a wooden sledge, the Mary Rose demonstrates the next development where two wheels were mounted to the front of the carriage but a heavy elevating quadrant was the third point of contact with the deck. This arrangement would have improved the control for gun laying. As long as the gun was breech loaded, it would not have required running fully out after reloading, but recoil on firing would have driven the gun inboard and running it fully out to fire again would have required more effort than for a four wheel truck mount.\nThe armament and other items of equipment have been as carefully illustrated as the ship and its component parts. As the Mary Rose is not a complete elevation, the drawings of the hull have included an estimate of the forecastle, most of which had rotted away. As with other books in the series, the flaps of the cover fold out to reveal full hull drawings. In all, there are more than 200 drawings and plans, together with more than 40 reference photographs. Although the original readership for the series was expected to be model makers and serious naval history enthusiasts, this book and other books in the series provide a level of artwork to attract a much wider readership and the story of the Mary Rose and her restoration will appeal to many interests."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:1baaeeb3-bad1-4304-84e6-15c58b6cf5b9>","<urn:uuid:7ed16747-603b-464c-aba0-eabe436adca6>"],"error":null}
{"question":"Could you explain how the ancient Babylonian queen Enmebaragesi was discovered to possibly be female instead of male?","answer":"A fragment from an Old Babylonian story that appeared in 1982 suggested that Enmebaragesi was actually female, a queen, rather than a king as previously believed. However, scholars now debate whether Enmebaragesi, regardless of gender, might have been a fictional composite character.","context":["31 invention that MIT undergraduates: For the MIT survey, see http://goo.gl/32Tdrv.\n32 Orthography mavens are like birders: As always on the Internet, the challenge is to differentiate the few quality sites (such as the highly recommended www.omniglot.com) from the much larger assemblage of sites that propagate misinformation.\n33 The Babylonians believed: Schmandt-Besserat (1992).\n33 Others interpret the evidence: On the abrupt versus gradual appearance of writing, see Daniels (1996) and Powell (2009) for differing views.\n33 domestication of cattle in Sumeria: Walker (1990); Daniels & Bright (1996).\n34 whose name is known: Michalowski (2003).\n34 Every element of this account: Walker (1990).\n34 Enmebaragesi is the name: Scholars deduced that Enmebaragesi was a king responsible for a variety of administrative innovations, but then a fragment from an Old Babylonian story that appeared in 1982 suggested that E. was actually female, a queen. Michalowski (2003) argues that E., male or female, was a fictional composite.\n34 “Passion is inversely proportional”: Benford’s law of controversy, due to novelist Gregory Benford. I learned about it here: Mark Liberman, “The Long Tail of Religious Studies?,” Language Log, August 5, 2010.\n35 writer Janet Malcolm: Malcolm quoted in Zoë Heller, “Cool, yet Warm,” New York Review of Books, June 20, 2013.\n36 oldest known cave paintings: The famous French cave paintings are about 30,000 years old, but others may be older. John Noble Wilford, “Cave Paintings in Indonesia May Be Among Oldest Known,” New York Times, October 8, 2014.\n36 modern forms of writing: The full figure is discussed by DeFrancis (1989), 84–85.\n37 Increase in abstraction: The explanation is not very different from how the quality of one’s own writing is affected by the precision of the writing instrument, the medium being written on, the amount to be written, how rapidly it has to be produced, and who has to be able to read it. Pictographs were hard to standardize and draw; writing with the wedge-shaped stylus was an improvement in those respects but worse for drawing realistic images.\n37 Writing systems that fully: A detailed timeline is here: Geoffrey Nunberg, “Timeline of the History of Information,” University of California Berkeley School of Information, http://goo.gl/axssdV.\n38 “unnatural act”: Gough & Hillinger (1980). Gough is one of the great figures in modern reading science, an iconoclast who turned out to be right about important things.\n38 “theory of mind”: Frith & Frith (2005). A funny contemporary theory of the functions of cave paintings.\n42 theory could be disconfirmed: On undeciphered codes and precursors to cuneiform, see several chapters in Daniels & Bright (1996).\n42 an elegant theory: Schmandt-Besserat (1986).\n42 Thousands of small objects: For pictures of the tokens, with more about the theory, see “The Evolution of Writing,” Denise Schmandt-Besseret, https://goo.gl/8Vv14V.\n42 world but suppositories: Schmandt-Besserat (1978).\n46 Mark Liberman: “What Is Writing?,” University of Pennsylvania Department of Linguistics.\n47 The pictographic information: Walker (1990); on the “cursive” hieroglyphic, see “Ancient Egyptian Scripts,” Omniglot, http://www.omniglot.com/writing/egyptian_hieratic.htm.\n48 Some characters can act: Note that in Figure 3.4, the horse radical (on the left in foal) is narrower than the horse phonetic (on the right in mother). This convention reflects the greater importance of the phonetic in recognizing the character.\nMost words consist of two characters, representing two syllables. Monosyllabic words such as mā (mother) are atypical. The word almost always occurs in the disyllabic form 妈妈. The “mà curse” illustrates the four tones in Mandarin, which change the meanings of the ma syllables (curse courtesy of Tianlin Wang):\n48 In another expression: In the modern era a number of characters containing the woman radical (女) can be perceived as sexist, such as\n奷 crafty, villainous, false\n妄 absurd, foolish, reckless, false\n妖 strange, weird, supernatural\nFrom Joe, “Sexist Chinese Characters Discriminate Against Women,” chinaSMACK, January 28, 2010, http://goo.gl/NiUYlT.\n48 A second system: For examples of how the scripts are intermixed, see “Japanese Writing System,” Wikipedia, https://goo.gl/hZFY6U. The “List of Gairaigo and Wasei-Eigo Terms,” Wikipedia, https://goo.gl/vn7ruF, is entertaining.\n49 kanji’s meaning depends on: For a furigana example, see “Japanese Hiragana,” Omniglot.com.\n49 A word is fully identified: For the full sets of k-t-b words in Hebrew and Arabic, see “K-T-B,” Wikipedia.\n49 Alphabets seem to have taken: In fact the relationships between form and meaning are not wholly arbitrary, extending well beyond familiar examples of “sound symbolism,” such as the glisten-gleam-glint and snort-snot-snivel clusters. Data crunching on large word lists has revealed many other nonarbitrary correspondences between sound and meaning. For example, names for males and females in English tend to have different phonological properties, and knowledge of these properties affects the interpretation of brand names; Cassidy, Kelly, & Sharoni (1999). See Dingemanse et al. (2015) for an overview.\n51 Languages with more complex: DeFrancis (1989).\n51 major elements of Japanese phonology: Taylor & Taylor (2014) is a masterful account of Japanese, Chinese, and Korean writing.\n52 “The depicting of objects”: Rousseau (1754), from Barton (1995).\n52 “The invention of the Greek alphabet”: From Olson (1996), an erudite analysis of Western scholars’ traditional belief in the superiority of the type of writing system they happen to use, the alphabet.\n52 High praise: Quoted by Barton (1995).\n53 borrowing the Phoenician system: The alphabets for Semitic languages were consonantal, but a few consonants also functioned as vowels in some contexts, similar to the use of y in English (as a consonant in yet but a vowel in lady).\n53 the world would be: For the credo of the Quixotes at “Uniskript,” see “What Is UniScript?”\n54 around 15,000 syllables: For one such count, see Chris Barker, “How Many Syllables Does English Have?,” Semanticsarchive.net.\n54 spoken languages have ended up: For some years I have been smartly issuing the epigram that “languages get the writing systems they deserve” (e.g., Seidenberg 2011). From a letter in the New Yorker, June 20, 2016, I learned that M. A. K. Halliday, a well-known linguist who worked on a broad range of interesting topics, made essentially the same observation. It’s found in Halliday (1983, 28), where he wrote, “In the course of [writing’s] long evolution, a language usually got the sort of writing system it deserved.” The idea has probably occurred to others as well.\n54 Chinese speakers who keyboard: Victor Mair, “Character Amnesia,” Language Log, July 22, 2010.\n54 Skeptics focus: The standard objection is that pinyin would be hard to read because it eliminates important visual cues that characters provide. Spoken Chinese languages have a large number of homophones, which speakers often disambiguate by gesturing the character. The problem is illustrated in an exaggerated way by “The lion-eating poet in the stone den,” a story consisting of the syllable “shi” spoken with different tones. This is the first line written in pinyin:\nshi2 shi4 shi1 shi4 shi1 shi4 shi4 shi1. shi4 shi2 shi2 shi1. shi4 shi2 shi2 shi4 shi4\nThe whole poem can be seen and heard here: “The poet Shih Shih, so fond of eating lions.”\nRichard Sproat, a scholar in Sinitic languages, writing systems, and computational linguistics, argues that pinyin or another suitable alphabet would be workable. See Sproat (2000). As in other cases like speed reading, what is tolerable in short bursts may be intolerable over longer stretches. Victor Mair favors a mixed system in which characters are annotated with pinyin, much like kanji can be annotated with furigana in Japanese."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:bf6c30eb-cc0e-4ca8-8072-52958bba4850>"],"error":null}
{"question":"What are the key differences between the SDO and Hinode solar observation spacecraft in terms of their imaging capabilities and data collection?","answer":"The SDO, launched in 2010, provides images with clarity 10 times better than high-definition television and sends 1.5 terabytes of data back to Earth daily. It includes three instruments: the Helioseismic and Magnetic Imager for mapping magnetic fields, the Atmospheric Imaging Assembly for photographing the sun's surface in 10 different wavelength bands, and the Extreme Ultraviolet Variability Experiment. In comparison, Hinode, launched in 2006, carries the Solar Optical Telescope which can measure both vertical and horizontal magnetic fields, the X-ray Telescope for observing million-degree gas, and the Extreme Ultraviolet Imaging Spectrometer which can tune into specific wavelengths of light from different particles in the sun's atmosphere.","context":["A full-disk multi-wavelength extreme ultraviolet image of the sun taken by SDO on March 30, 2010. False colors trace different gas temperatures. Reds are relatively cool; blues and greens are hotter. (NASA image)\nSome of the images from the spacecraft show never-before-seen detail of material streaming outward and away from sunspots. Others show extreme close-ups of activity on the sun's surface. The spacecraft also has made the first high-resolution measurements of solar flares in a broad range of extreme ultraviolet wavelengths.\n\"These initial images show a dynamic sun that I had never seen in more than 40 years of solar research,\" said Richard Fisher, director of the Heliophysics Division at NASA Headquarters in Washington. \"SDO will change our understanding of the sun and its processes, which affect our lives and society. This mission will have a huge impact on science, similar to the impact of the Hubble Space Telescope on modern astrophysics.\"\nThe NASA Website also has video clips of solar activity here.\nLaunched on Feb. 11, 2010, SDO is the most advanced spacecraft ever designed to study the sun. During its five-year mission, it will examine the sun's magnetic field and also provide a better understanding of the role the sun plays in Earth's atmospheric chemistry and climate. Since launch, engineers have been conducting testing and verification of the spacecraft's components. Now fully operational, SDO will provide images with clarity 10 times better than high-definition television and will return more comprehensive science data faster than any other solar observing spacecraft.\nSDO will determine how the sun's magnetic field is generated structured and converted into violent solar events such as turbulent solar wind, solar flares and coronal mass ejections. These immense clouds of material, when directed toward Earth, can cause large magnetic storms in our planet's magnetosphere and upper atmosphere.\nSDO will provide critical data that will improve the ability to predict these space weather events. NASA's Goddard Space Flight Center in Greenbelt, Md., built, operates and manages the SDO spacecraft for the agency's Science Mission Directorate in Washington.\nSpace weather has been recognized as a cause of technological problems since the invention of the telegraph in the 19th century. These events produce disturbances in electromagnetic fields on Earth that can induce extreme currents in wires, disrupting power lines and causing widespread blackouts. These solar storms can interfere with communications between ground controllers, satellites and airplane pilots flying near Earth's poles. Radio noise from the storm also can disrupt cell phone service.\nSDO will send 1.5 terabytes of data back to Earth each day, which is equivalent to a daily download of half a million songs onto an MP3 player. The observatory carries three state-of the-art instruments for conducting solar research.\nThe Helioseismic and Magnetic Imager maps solar magnetic fields and looks beneath the sun's opaque surface. The experiment will decipher the physics of the sun's activity, taking pictures in several very narrow bands of visible light. Scientists will be able to make ultrasound images of the sun and study active regions in a way similar to watching sand shift in a desert dune. The instrument's principal investigator is Phil Scherrer of Stanford University. HMI was built by a collaboration of Stanford University and the Lockheed Martin Solar and Astrophysics Laboratory in Palo Alto, Calif.\nThe Atmospheric Imaging Assembly is a group of four telescopes designed to photograph the sun's surface and atmosphere. The instrument covers 10 different wavelength bands, or colors, selected to reveal key aspects of solar activity. These types of images will show details never seen before by scientists. The principal investigator is Alan Title of the Lockheed Martin Solar and Astrophysics Laboratory, which built the instrument.\nThe Extreme Ultraviolet Variability Experiment measures fluctuations in the sun's radiant emissions. These emissions have a direct and powerful effect on Earth's upper atmosphere -- heating it, puffing it up, and breaking apart atoms and molecules. Researchers don't know how fast the sun can vary at many of these wavelengths, so they expect to make discoveries about flare events. The principal investigator is Tom Woods of the Laboratory for Atmospheric and Space Physics at the University of Colorado, Boulder. LASP built the instrument.\n\"These amazing images, which show our dynamic sun in a new level of detail, are only the beginning of SDO's contribution to our understanding of the sun,\" said SDO Project Scientist Dean Pesnell of Goddard.\nSDO is the first mission of NASA's Living with a Star Program, or LWS, and the crown jewel in a fleet of NASA missions that study our sun and space environment. The goal of LWS is to develop the scientific understanding necessary to address those aspects of the connected sun-Earth system that directly affect our lives and society.\nCopyright 2016 © Godem Online Inc. | Web and server solutions by NewTech Solutions.","November 1, 2011\nHinode’s First Light And Five More Years\nOn October 28, 2006, the Hinode solar mission was at last ready. The spacecraft launched on September 22, but such missions require a handful of diagnostics before the instruments can be turned on and collect what is called \"first light.\"\nHopes were high. Hinode had the potential to provide some of the highest resolution images of the sun the world had ever seen -- as well as help solve such mysteries as why the sun's atmosphere is a thousand times hotter than its surface and how the magnetic fields roiling through the sun create dramatic explosions able to send energy to the farthest reaches of the solar system.The X-ray telescope (XRT) began taking images on October 23, the Solar Optical Telescope (SOT) opened its front door on October 25, and the Extreme Ultraviolet Imaging Spectrometer (EIS) started collecting spectroscopic images on October 28.\nThe images were beautiful, the data good; first light science had been achieved.\nAnd so started five years in the life of a solar mission that would offer unprecedented details into the dynamics of the sun. Hinode — the word means \"sunrise\" in Japanese — is a mission led by the Japan Aerospace Exploration Agency (JAXA) with collaboration from NASA and other partners in the US, Europe, and Japan. Its instruments produce fantastic detail of both visible and magnetic features on the sun's surface and in its atmosphere, the corona. Such detail was unprecedented at its launch and still prized today. Hinode has helped find the origin of the solar wind, discovered potential candidates for how the corona gets so hot, and provided images of the complex magnetic structures looping up and out of active regions on the sun.\n\"One of the most notable contributions Hinode has made,\" says Jonathan Cirtain the co-investigator for the X-ray Telescope and an astrophysicist at NASA's Marshall Space Flight Center, \"is to produce high resolution observations of what the magnetic fields look like on the sun.\"\nThe patterns of magnetic fields inside the sun, on its surface, and in the atmosphere give rise to space weather that can traverse billions of miles away from the sun. Previous solar experiments in space could only measure magnetic fields along the line of sight — that is, vertical fields climbing directly up from the sun's surface. But thanks to the Solar Optical Telescope, Hinode can also measure the horizontal component of these fields. The SOT incorporates these detailed magnetic measurements into its white and ultraviolet images of the sun and so has been able to map out complex magnetic loops on the surface of the sun. Most importantly, it offered the first space-based observations of the horizontal magnetic field structure, showing fields that were always on the move and far more dynamic than expected. This, in turn, holds clues to how magnetism deep inside the sun causes the constant change we observe in its atmosphere.\nThe X-ray Telescope observes million-degree and higher gas. These temperatures are reached in the sun's corona. Observing near the poles at areas of lower magnetism on the sun called coronal holes, the XRT saw ultra-hot, X-ray jets shooting up into the atmosphere. Such things had been seen, but never in such abundances, says Cirtain, who says he was surprised when he first found them.\n\"After the shock wore off, I ran around dragging other scientists into my office to show them the movie,\" he says. \"It looks like the twinkle of Christmas lights, randomly oriented. It's very pretty.\"\nMore than just being pretty, these jets carry huge amounts of material high up into the corona. Scientists found that this extra material could account for some 20 to 25% of the particles that stream away from the sun, known as the solar wind. Understanding this wind is particularly important during solar minimum, when it becomes one of the strongest components of space weather throughout the solar system. Later work showed that these jets contain additional material that can be seen in other wavelengths besides X-rays and so may, in fact, account for all of the solar wind.\nThe third instrument is the Extreme Ultraviolet Imaging Spectrometer, which has been particularly helpful in trying to understand how the corona can be so much hotter than the sun's surface. The EIS tunes into specific wavelengths of light emitted by different particles in the sun's atmosphere. Since particles at particular temperatures emit light at a specific wavelengths, the spectrometer can be used to track a burst of material at a certain temperature as it moves through this complex system.\n\"A spectrograph can help you get really precise,\" says Therese Kucera at NASA's Goddard Space Flight Center in Greenbelt, Md who has worked with Hinode data. \"With imagers alone you see a broad range of light mixed together, but the spectrograph can zero in on a narrow temperature band.\"\nThe spectrograph helped find one potential candidate to explain how extra heat is carried up to the corona — something called \"nanoflares.\" Scientists have long been able to see occasional, gigantic solar flares erupting from the sun, but EIS spotted near-constant smaller bursts at active regions of the sun. Comparing these areas to XRT data showed mini bright flashes, and these numerous flares are now one candidate to explain the coronal heating mystery.\nBut Hinode, using the SOT, also found another candidate for heating the corona — fountains of hot material called spicules. Spicules had been seen before upon occasion, but Hinode helped show that these giant jets were far more common than believed, so spicules, too, might be carrying enough hot material up into the solar atmosphere to heat it to the recorded temperatures.\nNewer missions like NASA's Solar Dynamics Observatory, launched in 2010, and planned missions like NASA Interface Region Imaging Spectrograph (IRIS) and a Japanese solar mission currently designated simply Solar-C will continue to build on these observations, teasing apart the way the constantly twisting and moving magnetic fields of the sun sends energy out to the corona, to Earth's magnetosphere, and ultimately to all corners of the solar system.\nHinode is a joint mission of the Japan Aerospace Exploration Agency, the National Astronomical Observatory of Japan, the National Aeronautics and Space Administration and the Particle Physics and Astronomy Research Council.\nThe Marshall Space Flight Center managed the NASA instrument component integration for NASA Headquarters, manages the science operations for NASA and also supports science operations in Japan.\nImage 1: Vivid orange streamers of super-hot, electrically charged gas (plasma) arc from the surface of the Sun, revealing the structure of the solar magnetic field rising vertically from a sunspot. This extremely detailed image of the Sun was taken by Hinode's Solar Optical Telescope on November 20, 2006 and showed that the Sun´s magnetic field was much more turbulent and dynamic than previously known. Credit: Hinode, JAXA/NASA\nImage 2: Sunspot image from December 6, 2006. The left image was taken by the SOlar Heliospheric Observatory (SOHO), which was one of the first tools to monitor the sun from space, launched in 1995. The right image from Hinode's Solar Optical Telescope shows its high resolution. Credit: ESA/NASA/SOHO (left) and JAXA/NASA/Hinode (right)\nImage 3: Artist's concept of the Hinode spacecraft in orbit around the Earth with an active sun in the background. Credit: JAXA\nOn the Net:"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:7a1b280b-b29a-46de-8b2d-41ae63caf8b0>","<urn:uuid:27ff8716-f9a1-4bc5-8f57-dc36fe930f62>"],"error":null}
{"question":"Ketosis vs metabolic stress - what happens in body? Please compare physiological response. How organs adapt? 我想了解这两种状态下的身体反应。","answer":"Traditional medical resources viewed ketosis as a crisis reaction to carbohydrate lack, but this view has been challenged recently. During ketosis, the body produces ketone bodies from fatty acid breakdown, with most cells utilizing fatty acids for energy while glucose is synthesized through gluconeogenesis for specific cells like neurons. This is a normal physiological state. Regarding metabolic stress concerns, while some claim gluconeogenesis causes stress through cortisol release, this is a misconception - cortisol is only released during severe hypoglycemia, not during normal gluconeogenesis. In fact, studies indicate ketogenic diets can improve cortisol metabolism rather than create stress.","context":["To use all functions of this page, please activate cookies in your browser.\nWith an accout for my.bionity.com you can always see everything at a glance – and you can configure your own website and individual newsletter.\n- My watch list\n- My saved searches\n- My saved topics\n- My newsletter\nKetosis (pronounced /kiːˈtoʊs\nAdditional recommended knowledge\nMost medical resources regard ketosis as a physiological state associated with chronic starvation. Glucose is regarded as the preferred energy source for all cells in the body with ketosis being regarded as a crisis reaction of the body to a lack of carbohydrates in the diet. In recent years this viewpoint, both the body's preference for glucose and the dangers associated with ketosis, has been challenged by some doctors.\nKetone bodies, from the breakdown of fatty acids to acetyl groups, are also produced during this state, and are burned throughout the body. Excess ketone bodies will slowly decarboxylate into acetone. That molecule is excreted in the breath and urine. When glycogen stores are not available in the cells (glycogen is primarily created when carbohydrates such as starch and sugar are consumed in the diet), fat (triacylglycerol) is cleaved to give 3 fatty acid chains and 1 glycerol molecule in a process called lipolysis. Most of the body is able to utilize fatty acids as an alternative source of energy in a process where fatty acid chains are cleaved to form acetyl-CoA, which can then be fed into the Krebs Cycle. During this process a high concentration of glucagon is present in the serum and this inactivates hexokinase and phosphofructokinase-1 (regulators of glycolysis) indirectly, causing most cells in the body to use fatty acids as their primary energy source. At the same time, glucose is synthesized in the liver from lactic acid, glucogenic amino acids, and glycerol, in a process called gluconeogenesis. This glucose is used exclusively for energy by cells such as neurons and red blood cells.\nKetosis should not be confused with ketoacidosis (diabetic ketoacidosis or the less common alcoholic ketoacidosis), which is severe ketosis causing the pH of the blood to drop below 7.2. Ketoacidosis is a medical condition usually caused by diabetes and accompanied by dehydration, hyperglycemia, ketonuria and increased levels of glucagon. The high glucagon, low insulin serum levels signals the body to produce more glucose via gluconeogenesis and glycogenolysis, and ketone bodies via ketogenesis. High levels of glucose causes the failure of tubular reabsorption in the kidneys, causing water to leak into the tubules in a process called osmotic diuresis, causing dehydration and further exacerbating the acidosis.\nSince non-medically trained people often confuse ketosis with ketoacidosis, Dr Robert Atkins proposed that ketosis that occurs through deliberate carbohydrate restriction be referred to as \"benign dietary ketosis.\"\nIf the diet is changed from a highly glycemic diet to a diet that does not substantially contribute to blood glucose, the body goes through a set of stages to enter ketosis. During the initial stages of this process the adult brain does not burn ketones, however the brain makes immediate use of this important substrate for lipid synthesis in the brain. After about 48 hours of this process, the brain starts burning ketones in order to more directly utilize the energy from the fat stores that are being depended upon, and to reserve the glucose only for its absolute needs, thus avoiding the depletion of the body's protein store in the muscles.\nWhether ketosis takes place can be checked by using special urine test strips such as Ketostix.\nDeliberately induced ketosis through a low-carbohydrate diet has been used to treat medical conditions although most such treatments remain controversial.  The ketogenic diet is an approach to treating epilepsy, and the Atkins Nutritional Approach (and many similar diets) is marketed for treating obesity. The very low calorie, medically supervised Bernstein, Lighter Life, Cambridge Diet, Lindora diets and Medifast also use ketosis for weight loss.  \nThe Merck Manual -\n|This article is licensed under the GNU Free Documentation License. It uses material from the Wikipedia article \"Ketosis\". A list of authors is available in Wikipedia.|","Gluconeogenesis (GNG) is a metabolic process of making glucose, a necessary body fuel, from non-carbohydrate sources such as protein (amino acids), lactate from the muscles and the glycerol component of fatty acids.\nBlood glucose levels must be maintained within a narrow range for good health. If blood sugar is too high, it results in tissue and organ damage. If it is too low, cellular respiration and energy production can suffer, especially if the body is \"carbohydrate-adapted,\" meaning the body uses glucose as it's primary fuel.\nTherefore, the ability of the liver and kidneys to “make new sugar” and regulate blood sugar levels is critical. The main advantage of this process is that it helps the body maintain steady blood sugar levels when foods containing carbohydrates or stored sugars (glycogen reserves) are unavailable.\nWithout gluconeogenesis, you wouldn't live very long, especially without food, as your body must have a constant and steady level of blood glucose to keep the brain and red blood cells going.\nIf you decide to stop eating, or you decide to follow a low carb ketogenic diet, carbohydrate intake drops. To make up for the missing carbohydrate in your diet, the liver creates the blood glucose it needs by breaking down the glycogen stored in your muscles and liver from your last meal. This process is called glycogenolysis.\nAfter about 30 hours with no food, a great deal of this stored glycogen is broken down, and the body must then begin making glucose by breaking down stored fatty acids or amino acids from the protein in your muscles.\nSome dietitians and trainers insist that this process is the reason that carbohydrates are \"essential foods\". They reason that people should eat lots of carbohydrates because otherwise, the liver will burn up muscle tissue trying to make enough glucose for the brain each day.\nBut this is not true when following a low carb diet. Once you reach a state of nutritional ketosis, your body can use ketones as a primary fuel, and make all the glucose it needs from the protein and fat that you eat.\nIn other words, if you eat enough protein each day to provide for body maintenance, it will provide enough amino acids for gluconeogenesis, and your muscles will stay intact. This has been confirmed in many studies.\nYour body will burn your stored fat for fuel instead, and this is what everyone wants - to take fat out of storage in the fat cells, and burn it for fuel.\nThis is exactly what happens when you limit your carbohydrate intake.\nEveryone experiences this process of gluconeogenesis constantly throughout the day, and especially at night while you sleep. Over the 6-9 hours that you are sleeping and not taking in food, your body is busily making new sugar to maintain its narrow blood sugar range.\nFor some people, this nightly process is so robust that morning blood sugar levels are higher than they were when the person went to sleep. This higher than normal morning blood sugar is called the Dawn Phenomenon, and is especially important to understand for Type 1 and insulin dependent Type 2 diabetics, as more insulin has to be injected to counteract the higher blood sugar overnight.\nThe bottom line is that there is no requirement for dietary carbohydrate, because your body can make all the glucose it needs from the protein that you eat, or from the glycerol released when fatty acids are broken down.\nIn fact, most metabolic syndrome health issues such as high blood pressure, high blood sugar, cholesterol imbalances, and high triglycerides are improved by reducing the amount of carbohydrates consumed.\nOne of the more recent myths surrounding ketogenic diets is that they are stressful to the body, because the internal process of having to make glucose from protein and glycerol causes a release of cortisol, the so called \"stress\" hormone.\nThis misconception arise from the incorrect belief that the process of GNG ALWAYS requires the secretion of cortisol.\nThere's a great discussion and breakdown of this myth here. The authors cite several references which report that cortisol is not released in the GNG process until blood sugar drops so low as to cause a hypoglycemic reaction.\nAnd Peter over at the Hyperlipid blog mentions a study here that reports that ketogenic diets improve cortisol metabolism. He writes \"Translation: LC eating is what is KEY to IMPROVING glucocorticoid metabolism.\""],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:bc76ae82-c2a5-4e2a-a437-619b81426821>","<urn:uuid:f9dc193d-b46e-476a-bc2f-cc7e279e38c0>"],"error":null}
{"question":"I study psychology - what's connection between homophone effect and tech ethics?","answer":"The homophone effect (Bye-Now Effect) shows how words with similar sounds can unconsciously influence behavior, especially when people are tired or distracted - for example, seeing 'bye' can make people more likely to 'buy' things. This psychological principle relates directly to ethical concerns in technology design, as designers must consider whether they are exploiting such cognitive biases. According to ethical design principles, creators have a responsibility to avoid manipulating users' mental vulnerabilities and should prioritize user benefit over business interests. This is especially important given technology's persistent presence and ability to affect behavior through multiple channels.","context":["Research just released suggests that when we read words like 'bye' or 'wait', we automatically think of and act on words that have the same sound, such as 'buy' or 'weight', especially when we're tired.\nThis is such an interesting and new-found bias!! Indeed, it’s so new that we’ve had to give it a name! What we’re calling the ‘Bye-Now Effect’ all revolves around two separate concepts. The first, and most specific is that of the homophone - a word that has the same pronunciation as another word, but with a different meaning and spelling.\nThis is twinned with the concept of priming - exposing you to one piece of information affects your response to something else afterwards. We’ll cover Priming in more detail as a separate gem, but for example, if you read a list of words including the word ‘brain’, and are later asked to complete a word starting with ‘bra’, the chance that you’ll answer ‘brain’ is greater than if you’re not primed.\nMashing them together, you get Homophone priming, which happens when your brain can’t ignore the meaning of the other related word. For instance, if I said the word ‘Ewe’ (A female sheep), you might think of the word ‘you’, and its related meaning. Research shows that it’s not only the meaning that goes into your head, but also any judgements you might make, such as what you think of a person’s character (Sela and Shiv 2009), or any choices you might make (Wheeler and Berger 2007).\nAn experiment done for a name-your-own-price restaurant used either the words “bye bye” or “so long” in a piece of text participants were asked to read before naming their price for the meal. Those whose brains were occupied and heard the words “so long” opted an average price of around $32, whereas those who heard “bye-bye” chose a staggering $45.50. The Bye-Now Effect basically makes us much more likely to buy more.\nThe power of the Bye-Now Effect depends to some extent on a person’s reading ability - low-skilled readers are more affected (Gernsbacher and Faust, 1991), and the level of demand on the brain - the more distracted you are (i.e. if you’re multitasking), the more effect it has (Shiffrin and Dumais, 1981).\nIt also doesn’t work in both ways for associated words that aren’t that common. I.e. Saying ‘ewe’ might make me think of ‘you’, but saying ‘you’ is far less likely to make me think ‘ewe’ (Picoult and Johnson 1992).\nFrom Bye to Buy: Homophones as a Phonological Route to Priming Article (Davis & Herr, 2014)\nHard to Right and Easy to Bye: Priming Consequences of Homophone Confusion PDF (Davis & Herr, 2013)\nUnraveling Priming: When Does the Same Prime Activate a Goal versus a Trait? PDF (Sela & Shiv, 2009)\nThe Mechanism of Suppression: A Component of General Comprehension Skill PDF (Gernsbacher & Faust, 1991)\nControlling for Homophone Polarity and Prime-Target Relatedness in Cross-Modal Lexical Decision Tasks PDF (Picoult & Johnson, 1992)\nA Sound Idea: Phonetic Effects of Brand Names on Consumer Judgments PDF (Yorkston & Menon, 2004)\nLateralization in the visual perception of Chinese characters and words Article (Cheng & Yang, 1989)","On the other hand, the last time you shopped on eBay, you may have noticed the use of multiple design elements encouraging you to buy an item (“last item”, “3 watched in the past day”). While these design techniques are used to persuade users, they are usually not deceptive and are considered white hat techniques.\nA third example comes from Google I/O 2018 last month when the world heard Google Duplex make a call to a salon for an appointment and carry out a fluent conversation mimicking human mannerisms so well that the person at the other end did not realize she was talking to a machine. The machine did not misrepresent itself as human, nor did it identify itself as a machine, which, in my book, puts it in a gray area. What’s stopping this from being used in black hat design in the near future?\nAs you see from the three examples above, the use of persuasive tactics can fall anywhere on a spectrum from black hat at one extreme to white hat at the other, with a large fuzzy gray area separating the two. In today’s world of online and email scams, phishing attacks, and data breaches, users are increasingly cautious of persuasive tactics being used that are not in their best interest. Experience designers, developers, and creators are responsible for making decisions around the ethical nature of the tactics we use in our designs.\nThis article will present a brief history of persuasion, look at how persuasion is used with technology and new media, and present food for thought for designers and developers to avoid crossing the ethical line to the dark side of persuasion.\nHistory Of Persuasion\nPersuasion tactics and techniques are hardly new — they have been used for ages. Aristotle’s Rhetoric, over 2000 years ago, is one of the earliest documents on the art of persuasion. The modes of persuasion Aristotle presented were ethos (credibility), logos (reason), and pathos (emotion). He also discussed how kairos (opportune time) is important for the modes of persuasion.\nFast forward to today, and we see persuasion methods used in advertising, marketing, and communication all around us. When we try to convince someone of a point of view or win that next design client or project, chances are we are using persuasion: a process by which a person’s attitudes or behavior are, without duress, influenced by communications from other people (Encyclopedia Britannica).\nWhile Aristotle first documented persuasion, Robert Cialdini’s Influence: The Psychology of Persuasion is more commonly referenced when talking about modern persuasion. According to Cialdini, there are six key principles of persuasion:\nPeople are obliged to give something back in exchange for receiving something.\nPeople want more of those things they can have less of.\nPeople follow the lead of credible, knowledgeable experts.\nPeople like to be consistent with the things they have previously said or done.\nPeople prefer to say yes to those that they like.\nConsensus (Social Proof)\nEspecially when they are uncertain, people will look to the actions and behaviors of others to determine their own.\nWe have all been exposed to one or more of these principles, and may recognize them in advertising or when interacting with others. While that has been around for ages, what is relatively new is the application of persuasion techniques to new technology and media. This started off with personal computers, became more prominent with the Internet, and is now pervasive with mobile devices.\nPersuasion Through Technology And New Media\nBehavior scientist B.J. Fogg is a pioneer when it comes to the role of technology in persuasion. Over two decades ago, he started exploring the overlap between persuasion and computing technology. This included interactive technologies like websites, software, and devices created for the purpose of changing people’s attitudes or behaviors. He referred to this field as captology, an acronym based on computers as persuasive technologies, and wrote the book on it, Persuasive Technology: Using Computers to Change What We Think and Do.\nInteractive technologies have many advantages over traditional media because they are interactive. They also have advantages over humans because they can be more persistent (e.g. software update reminders), offer anonymity (great for sensitive topics), can access and manipulate large amounts of data (e.g. Amazon recommendations), can use many styles and modes (text, graphics, audio, video, animation, simulations), can easily scale, and are pervasive.\nThis last advantage is even more pronounced today, with mobile phones being an extension of our arms, and increased proliferation of smart devices, embedded computing, IoT, wearable technology, Augmented Reality, Virtual Reality, and virtual assistants powered by AI being embedded in anything and everything around us. In addition, today’s technological advances allow us to time and target moments of persuasion for high impact, since it is easy to know a user’s location, context, time, routine, and give them the ability to take action. This could be a reminder from your smartwatch to stand or move, or an offer from the coffee shop while you are a few blocks away.\nEthics And New Technology And Interactive Media\nThe use of persuasion in traditional media over the past decades has raised questions about the ethical use of persuasion. With new media and pervasive technology, there are more questions about the ethical use of persuasion, some of which are due to the advantages pervasive technology has over traditional media and humans. Anyone using persuasive methods to change people’s minds or behavior should have a thorough understanding of the ethical implications and impact of their work.\nOne of the key responsibilities of a designer during any design process is to be an advocate for the user. This role becomes even more crucial when persuasion techniques are intentionally used in design, since users may be unaware of the persuasion tactics. Even worse, some users may not be capable to detect these tactics, as may be the case with children, seniors or other vulnerable users.\nBJ Fogg provides six factors that give interactive technologies an advantage over users when it comes to persuasion:\nPersuasive intent is masked by novelty\nThe web and email are no longer novel, and most of us have wizened up to deceptive web practices and the promises of Nigerian Princes, but we still find novelty in new mobile apps, voice interfaces, AR, VR. Not too long ago, the craze with Pokémon Go raised many ethical questions.\nPositive reputation of new technology\nWhile “It must be true — I saw it on the Internet” is now a punchline, users are still being persuaded to like, comment, share, retweet, spread challenges, and make fake news or bot generated content viral.\nWould you like a used car salesman following you around after your first visit, continually trying to sell you a car? While that thankfully does not happen in real life, your apps and devices are with you all the time, and the ding and glowing screen have the ability to persistently persuade us, even in places and times that may be otherwise inappropriate. This past Lent, my son took a break from his mobile device. When he started it after Easter, he had hundreds of past notifications and alerts from one mobile game offering all sorts of reminders and incentives to come back and use it.\nControl over how the interaction unfolds\nUnlike human persuasion, where the person being persuaded has the ability to react and change course, technology has predefined options, controlled by the creators, designers and developers. When designing voice interfaces, creators have to define what their skill will be able to do, and for everything else come back with a “Sorry I can’t help with that”. Just last month, a social network blocked access to their mobile website, asking me to install their app to access their content, without an escape or dismiss option.\nCan affect emotion while still being emotionless\nNew technology doesn’t have emotion. Even with the recent advances in Artificial Intelligence, machines do not feel emotion like humans do. Back to the Google Duplex assistant call mentioned at the beginning, issues can arise when people are not aware that the voice at the other end is just an emotionless machine, and treat it as another person just like them.\nCannot take responsibility for negative outcomes of persuasion\nWhat happens when something goes wrong, and the app or the technology cannot take responsibility? Do the creators shoulder that responsibility, even if their persuasion strategies have unintended outcomes, or if misused by their partners? Mark Zuckerberg accepted responsibility for the Cambridge Analytica scandal before and during the congress hearings.\nWith these unfair advantages at our disposal, how do we, as creators, designers, and developers make ethical choices in our designs and solutions? For one, take a step back and consider the ethical implication and impact of our work, and then take a stand for our users.\nMany designers are pushing back and being vocal about some of the ethically questionable nature of tech products and designs. There’s Tristan Harris, a former Google Design Ethicist, who has spoken out about how tech companies’ products hijack users’ minds. Sean Parker, Napster founder and former president of Facebook, described how Facebook was designed to exploit human “vulnerability”. And Basecamp’s Jonas Downey ruminates on how most software products are owned and operated by corporations, whose business interests often contradict their users’ interests.\nDesign Code Of Conduct\nAIGA, the largest professional membership organization for design, has a series on Design Business and Ethics. Design Professionalism author Andy Rutledge also created a Professional Code of Conduct. Both are very detailed and cover the business of design, but not specifically ethics related to design that impacts or influences human behavior.\nOther professionals who impact the human mind have ethical principles and codes of conduct, like those published by the American Psychological Association and the British Psychological Society. The purpose of these codes of conduct is to protect participants as well as the reputation of psychology and psychologists themselves. When using psychology in our designs, we could examine how the ethical principles of psychologists are applicable to our work as creators, designers, and developers.\nPrinciples And Questions\nUsing the Ethical Principles of Psychologists as a framework, I defined how each principle applies to persuasive design and listed questions related to ethical implications of design. These are by no means exhaustive but are intended to be food for thought in each of these areas. Note: When you see ‘design’ in the questions below, it refers to persuasive techniques used in your design, app, product or solution.\nPrinciple A: Beneficence And Nonmaleficence\nDo no harm. Your decisions may affect the minds, behavior, and lives of your users and others around them, so be alert and guard against misusing the influence of your designs.\n- Does your design change the way people interact for the better?\n- Does the design aim to keep users spending time they didn’t intend to?\n- Does the design make it easy to access socially unacceptable or illegal items that your users would not have easy access to otherwise?\n- How may your partners (including third-party tools and SDKs) or “bad guys” misuse your design, unknown to you?\n- Would you be comfortable with someone else using your design on you?\n- Would you like someone else to use this design to persuade your mother or your child?\nPrinciple B: Fidelity And Responsibility\nBe aware of your responsibility to your intended users, unintended users and society at large. Accept appropriate responsibility for the outcomes of your design.\n- During design, follow up answers to “How might we…?” with “At what cost?”\n- What is the impact of your design/product/solution? Who or what does it replace or impact?\n- If your design was used opposite from your intended use, what could the impact be?\n- Does your design change social norms, etiquette or traditions for the better?\n- Will the design put users in harm’s way or make them vulnerable, intentionally or unintentionally (Study Estimates That Pokémon GO Has Caused More Than 100,000 Traffic Accidents)? How can it be prevented?\nPrinciple C: Integrity\nPromote accuracy, honesty, and truthfulness in your designs. Do not cheat, misrepresent or engage in fraud. When deception may be ethically justifiable to maximize benefits and minimize harm, carefully consider the need for, the possible consequences of, and be responsible for correcting any resulting mistrust or other harmful effects that arise from the use of such techniques.\n- Do you need users’ consent? When asking for their consent, are they aware of what exactly they are consenting to?\n- What’s the intent of the design? Is it in the best interest of the user or the creator? Are you open and transparent about your intentions?\n- Does your design use deception, manipulation, misrepresentation, threats, coercion or other dishonest techniques?\n- Are users aware or informed if they are being monitored, or is it covert?\n- Is your design benefiting you or the creators at the expense of your users?\n- What would a future whistleblower say about you and your design?\nPrinciple D: Justice\nExercise reasonable judgment and take precautions to ensure that your potential biases, the limitations of your expertise does not lead to, or condone unjust practices. Your design should benefit both the creators and users.\n- Does your design contain any designer biases built in (gender, political, or other)?\n- Does your design advocate hate, violence, crime, propaganda?\n- If you did this in person, without technology, would it be considered ethical?\n- What are the benefits to the creators/business? What are the benefits to the users? Are the benefits stacked in favor of the business?\n- Do you make it easy for users to disconnect? Do users have control and the ability to stop, without being subject to further persuasion through other channels?\nPrinciple E: Respect For People’s Rights And Dignity\nRespect the dignity and worth of all people, and the rights of individuals to privacy, and confidentiality. Special safeguards may be necessary to protect the rights and welfare of vulnerable users.\n- Are your designs using persuasion with vulnerable users (children, seniors, poor)?\n- Does your design protect users’ privacy and give them control over their settings?\n- Does the design require unnecessary permissions to work?\n- Can your design use a less in-your-face technique to get the same outcome? (e.g. speed monitors on roads instead of surveillance)\n- Does your design make your users a nuisance to others? How can you prevent that?\nIf you have been designing with white hat techniques, you may appreciate the ethical issues discussed here. However, if you have been designing in the grey or black area, thank you for making it all the way to the end. Ethics in persuasive design are important because they don’t prey on the disadvantages users have when it comes to interactive technology. As creators, designers, and developers, we have a responsibility to stand up for our users.\nDo good. Do no harm. Design ethically.\nThis article is part of the UX design series sponsored by Adobe. Adobe XD tool is made for a fast and fluid UX design process, as it lets you go from idea to prototype faster. Design, prototype and share — all in one app. You can check out more inspiring projects created with Adobe XD on Behance, and also sign up for the Adobe experience design newsletter to stay updated and informed on the latest trends and insights for UX/UI design.\nSource link https://www.smashingmagazine.com/2018/06/ethics-of-persuasion/"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:45286be9-ce90-499a-9dbb-0ee6b45eeae8>","<urn:uuid:7b7cf513-4db7-47b3-88c8-15fa942f765e>"],"error":null}
{"question":"What are the main drivers behind the growth of biometric sensors compared to BIM and digital twin technologies in the construction industry?","answer":"The main drivers for biometric sensors are increasing security threats, privacy concerns, rising demand for smart gadgets, and expanding applications in medical & defense sectors. In contrast, BIM and digital twin growth is driven by increasing digitalization in the private sector, growing building IoT penetration, sustainable building techniques, governmental support, business potential from urbanization, and enhanced stakeholder collaboration in construction projects.","context":["Biometric Sensors Market By Type (Optical Sensors, Thermal Sensors, Capacitive Sensors, Ultra Sound Sensors, Electric Field Sensors), Application (Iris Scan, Vein scan, Hand Scan, Facial Scan, Voice Scan, Finger Scan, Others), Industry Trends, Estimation & Forecast, 2017 - 2025\nBiometric Sensors Market is expected to grow with a CAGR of x.x% during the forecast period of 2018 to 2025.\nA biometric sensor is a transducer that converts a biometric treat (fingerprint, iris, face, voice) of a person into an electrical signal. The introduction and adoption of biometrics in identification process has substituted the use of passcodes & PINs, as they were ineffective, and prone to be accessed by unauthorized users. The global biometric sensors market is growing at a fast pace due to its versatile functionality, making it essential for various applications. The increasing security threats and privacy concerns, rising demand for smart gadgets with biometric sensors, increasing application of biometric sensors in medical & defence sector and high R&D cost are certain factors driving the growth of biometric sensors market. Moreover, the need of biometric sensors in biometric driver assistance systems is creating lucrative opportunities for the key players involved in the market. In order to sustain in the market, the market leaders are acquiring local market players and increasing their market grasp.\nBiometric Sensors Market Research Objective\n• To define the scope and simplify the research study based on type, end user, application, and region.\n• Estimate the current market size and forecast the same for the period (2017-2024).\n• Market share by revenue for each segment and region analyzed.\n• To provide insights on the major market dynamics (drivers, restraints & opportunities) and their impact analysis for the forecast period.\n• Micro and macro level analysis of the market to elucidate lucrative investment opportunities.\n• Porter’s five forces analysis to deliver a comprehensive buyer-seller scenario and the state of business environment.\n• Identification of the Biometric sensors market trends in current scenario and its growth indicators.\n• Benchmarking leading vendors in the Biometric sensors industry based on their strategic attempts, financial status, and other internal and external parameters.\nBiometric Sensors Market Segmentation\n• Optical Sensors\n• Thermal Sensors\n• Capacitive Sensors\n• Ultra Sound Sensors\n• Electric Field Sensors\nBy End User\n• Consumers Electronics\n• Medical Research & Lab\n• Bank & Finance Service Sector\n• Commercial Centers & Buildings\n• Iris Scan\n• Vein scan\n• Hand Scan\n• Facial Scan\n• Voice Scan\n• Finger Scan\n• North America\n• Rest of the World\nFor this specific report, we offer 20% of free customization in order to deliver a tailored research report that specifically covers areas of client’s interest in the Biometric sensors market. Following are some most desired customization offers on this report:\n• Split of the regional market into specific countries as per client’s research requirements.\n• Further breakdown of the major segments into sub-segments (as per request).\nVendor Profile Customization\n• Further exhaustive analysis of additional companies operating in the market as per request.\n• Research report on Biometric sensors market covering specific country/region only.\n• Requests for data tables only (specific requirement to quantitative research).\nTable of Content\nList of Companies\nWhat Information does this report contain?\n• What was the market size of the Biometric Sensors Market in 2017 and the expected market size by 2025, along with the growth rate?\n• An in-depth analysis of the current impacting factors, opportunities and challenges/restraints in the market\n• Which are the largest revenue generating products, services or regions and their comparative growth rate?\n• Which technology is in trend and how would it evolve during the forecast period (2017 – 2025)?\n• Which are the leading companies in the Biometric Sensors Market and their competitive positioning basis their market share, product portfolio, strategic attempts and business focus?","The purpose of this research is to explore your company’s ‘Growth Zone’ in the architecture, engineering, and construction (AEC) industry through building information modelling (BIM) and digital twin opportunities. The study highlights market developments such as the evolution of the BIM and digital twin platform, impact of major technologies in the construction industry, important driving and restraining factors that influence market growth, top predictions, latest trends, market measurements at a global and regional level, competitive landscape, key company profiles, and growth opportunities and strategic imperatives for market participants to capitalise on in this high-growth market.\nBIM and digital twin is a technology-based software and services market that has evolved from the 2D and 3D design tools, which involved a lot of paperwork. BIM and digital twin, which is one of the modern digital construction tools, enable AEC industry professionals to proficiently plan, design, construct, operate, and manage buildings and infrastructure. The implementation of BIM and digital twin platform for construction projects has been increasing at an accelerated rate in the last 5 years. Both public and private sector stakeholders play a crucial role in bringing awareness and business interests in BIM and digital twin. The main drivers identified for BIM and digital twin adoption are increasing digitalisation in the private sector; increasing penetration of building Internet of Things (IoT); sustainable building techniques; governmental support for BIM implementation; business potential with increasing urbanization; and a higher level of collaboration between stakeholders in construction projects. The global BIM and digital twin market was estimated at $5,225.6 million in 2019 and will grow at a CAGR of 14.5% until 2026.\nThe publisher has segmented the overall market as BIM and digital twin, based on end-user verticals such as commercial, industrial, residential, and others, and based on end-users such as architects, engineers, contractors, and building owners. The geographies covered in the research are North America, Europe, Asia-Pacific, and Rest of the World.\nBIM: The scope of the study includes BIM software and services used for creating virtual 3D static models of a built environment, enabling collaboration and visualisation of design and construction for buildings value chain stakeholders.\nDigital Twin: The scope of the study includes the digital twin software and services that help create a dynamic and digital replica of physical building assets, such as heating, ventilation, and air conditioning (HVAC), lighting, elevators, escalators, fire safety and security systems, manufacturing industries’ equipment, water and wastewater industries’ equipment. The platform combines real-time spatial, occupancy, assets, and environmental data, along with analytics, converting building data into building intelligence, supporting decision making for overall building performance optimisation and building lifecycle management.\nThe study captures the market revenues of BIM and digital twin, leveraged from the pre-construction phase to the operational phase of buildings. The study does not cover construction management software exclusively used for project bidding, financial planning, project scheduling, field operations, and quality control. However, if these features are included with the BIM and digital twin software, their revenues are included.\nLeading companies identified in the research are Autodesk, Nemetschek Group, Bentley Systems, Trimble, Hexagon, RIB, Glodon, Dassault Systemes, Siemens, Aveva, and Procore. Some of the dynamic and emerging companies identified are Invicara, Willow, YouBIM, Ecodomus, Flair3D, Vertex Systems, Sanveo, and Tetris.\nKey Issues Addressed\n- What are the key growth opportunities for market participants to continue on their double-digit growth in the forecast period?\n- What are the challenges and strategic imperatives for market participants in the BIM and digital twin market?\n- What are the technologies that will drive BIM and digital twin market growth in the forecast period?\n- What is the global and regional market share of leading companies in the BIM and digital twin market?\n- What are the opportunities and upcoming key infrastructure projects in the major regions?\n1. Executive Dashboard\n- Purpose of this Experiential Study\n- 5-step Process to Transformational Growth\n- Leading, Dynamic, and Emerging\n- Strategic Imperatives for Market Participants\n2. Growth Environment - Market Overview\n- Market Scope and Definitions\n- Market Segmentation\n- Drivers and Restraints\n- Impact of Top Technologies on the Construction Industry\n- Global Mapping of Digitalisation in the Construction Industry\n- Evolution of BIM and Digital Twin in the Construction Industry\n3. Market Forecasts\n- Revenue Forecast\n- Revenue Forecast by Products\n- Revenue Forecast Discussion\n- Revenue by End-user Verticals\n- Revenue by End Users\n- Regional Developments - Developed Economies\n- Regional Developments - Emerging Economies\n4. Competitive Landscape\n- Market Share\n- Top BIM and Digital Twin Companies to Watch Out for\n5. Regional Findings\n- North America\n6. Visioning Scenarios\n- Macro to Micro Visioning\n- Trends Impacting the BIM and Digital Twin Market\n- Top Predictions for the BIM and Digital Twin Market\n7. Growth Pipeline\n- Levers for Growth\n8. Vision and Strategy - Growth Opportunities\n- Growth Opportunity 1 - Aligning BIM and Digital Twin Solutions to Promote Sustainability and Net-zero Buildings\n- Growth Opportunity 2 - Flexible Business Model Development and Incentivisation\n- Growth Opportunity 3 - Targeting Critical End-user Segments to Mitigate COVID-19 Damage\n- Growth Opportunity 4 - Targeting Smart City Infrastructure Projects in High-growth Regions\n- Growth Opportunity 5 - Forming Strategic Partnerships to Strengthen Regional Foothold\n- Growth Opportunity 6 - Acquisition of Dynamic Smart Building Participants to Strengthen Digital Twin Offerings\n- Growth Opportunity 7 - Facilitating Open BIM Standards\n- Growth Opportunity 8 - Incorporating AI/ML in BIM and Digital Twin Platform\n- Growth Opportunity 9 - Increasing Investments in R&D Activities to Foster Innovation\n9. Brand and Demand - Growth Opportunities\n- Growth Opportunity 10 - Following Sustainable Business Practices\n- Growth Opportunity 11 - Upskilling Student Community to Create Skilled Professionals\n- Growth Opportunity 12 - Integrating Smart Building Platform into BIM and Digital Twin Platform\n10. Growth Opportunities Matrix\n- Identifying Your Company’s Growth Zone\n- Growth Opportunities 1-9: Vision and Strategy\n- Growth Opportunities 10-12: Brand and Demand\n- Growth Opportunities Matrix\n11. Growth Strategy and Implementation\n- Growth Strategies for Your Company\n- Prioritised Opportunities Through Implementation\n- Legal Disclaimer\n- Abbreviations and Acronyms Used\n- List of Exhibits\n- Bentley Systems\n- Dassault Systemes\n- Nemetschek Group\n- Vertex Systems"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:7ce44c55-0e3e-4533-848d-c4591bedece2>","<urn:uuid:a953a9da-056c-4130-b922-40a8921f7bef>"],"error":null}
{"question":"Como estudiante de historia militar, me gustaría saber: ¿Cuál fue el primer despliegue militar significativo del Regimiento DeMeuron y del Regimiento 39th Foot en territorios coloniales?","answer":"The DeMeuron Regiment's first colonial deployment was to the Cape of Good Hope in 1782, where they landed in Capetown before quickly being re-embarked to Ceylon to reinforce the Suffrens. The 39th Foot's first colonial deployment was to India in 1754, where they were the first British Government troops (rather than East India Company mercenaries) to serve there, being stationed in Madras under Colonel Adlercron.","context":["In the year 1781 French authorities helped to raise a new regiment in the Swiss canton, soliciting recruits in and around Neuchatel. The regiment was called the deMeuron Regiment, taking it's name from the commander, Comte Charles de Meuron, born in Neuchatel in1738 and who served in the French regiment D'Erlach. The DeMeuron Regiment consisted of 1020 men, 10 companies of 102 men each, fully armed and equipped.\nThis regiment was to serve the Dutch East India Company protecting the Dutch colonies, the Cape of Good Hope in particular. A Dutch base was established there in 1652, to act as a supply depot for the provisioning of ships on the long voyage from Europe to India. By the 1700's, Dutch settlers, the Boers, were well established there.\nOn January 7, 1782, the regiment landed in Capetown, only to re-embark shortly on the Hermione to Ceylon where they were to reinforce the Suffrens. Ceylon had been under Dutch rule since 1665, yet in the latter part of the 18th century, the British began to move eastward from India. In Ceylon, the DeMeuron Regiment took part in the expulsion of the British from Cuddalore, which was \"sorely beset by the English under General Stuart.\"\nAfter peace, the DeMeuron Regiment shared garrison duty with the French Regiment de Pondicherry, in Capetown. Many duels were fought between the men of the two regiments. Tensions were developing between the Swiss and their French `Allies', and between the English and Dutch settlers on the Cape of Good Hope. Many of the De Meurons deserted at this time, enticed by the Dutch Boers to become farmhands. The DeMeurons were eventually relieved by the Regiment de Wurtenburg, in time to prevent complete disintegration.\nIn 1786 the DeMeuron Regiment was sent back to Ceylon, leaving in Capetown a depot of 33 men. Ceylon was governed at this time by Governor Van Angelbeek, in Colombo. While in Ceylon, the regiment answered to him and to Colonel Pierre de Meuron, brother of Comte Charles de Meuron, who had returned to Switzerland. Some time during these period the regiment was \"borrowed\" from the Dutch by the French. the DeMeurons served as marines on board a fleet operation against the British.\nOn August 26, 1795, two companies of the DeMeuron Regiment were taken prisoner, by the British at Trincomalee, when the British stormed Ceylon. After a weak attempt at defence, Holland surrendered Ceylon to the British. The Dutch East India Company went bankrupt, and could not pay its troops. In the tradition of the time, the DeMeuron Regiment entered into British service.\nUnder the terms of the agreement, finalised in 1798, the DeMeuron Regiment\nfinally entered into full service with the British Army. It consisted of\n2 battalions, of 5 companies of infantry each.\nIn the west and south of India there were two \"vigorous and expansive powers\" working against the British, the Maràthàs and Mysore. there had already been three campaigns against the Sultan Tippu-Saib, who led the Mysore power. The third Mysore War, led by Cornwallis, lasted from 1789 to 1792. Cornwallis had thought he had brought Sultan Tippu-Saib to bay.\nWellesley decided first to strike at Mysore, still a formidable military power and avowedly hostile. The British stormed Seringapatam on May 4th, 1799.\nThe DeMeuron Regiment took no small part in this campaign. The men conducted themselves bravely. They were the first to go \"over the top,\" as the Forlorn Hope. The Sultan Tippu-Saib was isolated and died in the fighting. There were rejoicings among the British over this \"unexpectedly complete victory.\" The DeMeuron suffered 80 casualties.\nWellington reported the good record of the DeMeurons. In spite of this, the Indian natives refused to regard the regiment with the respect they accorded to a British unit. As being foreigners (Swiss,) the natives held the idea that the DeMeurons had been bought as slaves. Wellington also reported that the DeMeuron Regiment was equal to or better than British regiments in good conduct, discipline and military ability.\nComte Charles-Daniel de Meuron returned to the country of his birth shortly after the seizure of Seringapatm, leaving command of his regiment to Pierre-Frederick de Meuron. Charles de Meuron died on April 4, 1806.\nThe DeMeuron Regiment remained in India until 1806. There, they served\nin several campaigns.\nThe DeMeurons were sent from Malta to England, 35 Officers and 132 other ranks. In England, they were stationed on the Isle of Wight, and at Lymington, where their Regimental Depot was established. The DeMeuron companies had been decimated in the Eastern Wars. In England, they were restructured as a line regiment, rebuilt and re-equipped. They did not remain in England for long.\nIn 1809 they were sent back to the Mediterranean to join two other Swiss regiments, DeRoll and DeWatteville. Based in Gibraltar and later again in Malta, the regiment was swollen by the addition of 500 recruits. They were mostly Swiss and German soldiers, conscripted into Napolean's army, who deserted at the first opportunity to join the British. There is no doubt that there were also some Italians who joined the DeMeuron ranks.\nThe DeMeuron Regiment fought in the Peninsular War, in Spain. This was also known as the War of Independence, by the Spanish. They were once again under the command of Sir Arthur Wellesley. He had been Knighted, given the title Duke of Wellington, and now commanded the British forces in the Mediterranean. Wellesley was to become one of history's few unbeaten generals.\nThe DeMeuron's were sent to fight where ever Wellesley thought they were needed most. They were in Spain, Sicily, and Italy.\nWhile in Malta, in 1813, a number of new recruits*, fleeing from Napoleon were added to the ranks. Of the more than 2,000 DeMeurons in the British Army, in 1813, about 800 of them were Swiss, at least 500 were German, 300 were Dutch, and 200 were Alsatians. The rest were largely Italians and Poles, but included Austrians, Spanish, and every other nationality from Europe. Most had joined the DeMeuron Regiment to fight against Napoleon, instead of being drafted by conscription to fight for him!\n|*Recent evidence indicates that almost all of the new recruits were left in Europe, most likely as part of the Kings German Legion. The names of men who came to Canada indicate that almost all of them were Swiss, and the remainder were German.|\nLieutenant-General Oakes cannot suffer the Regiment DeMeuron to quit this garrison where they have so long been stationed under his command, without assuring them of the satisfaction which their good conduct and attention to military discipline have constantly afforded him, and which have been equally conspicuous in every rank. The will embark from hence as fine and well appointed a regiment as any in His Majesty's Service.\nThe Lieutenant-General has no doubt that by their conduct and gallantry, on the desirable service on which they're are about to be employed, they will confirm the high opinion he has formed of them, and will equally merit the praise and approbation of the General under whose orders they will soon be placed, to whom he shall not fail justly to set forth their merits.\nHe begs leave to assure the regiment of his warmest\nwishes for their glory and success and of the sincere interest he shall\never take in their welfare.\n|Return Main||Whistle Signals||Regt de Meuron\n|Regt de Meuron\n|Photo Gallery||War of 1812||Cooke's Mill||Laura Secord||Musket Amunition||Platoon\nExercise & Musketry","The oldest parts of the Regiment is the militia. Compulsory military service within the counties of Devon and Dorset dates back to the days of the Saxons but it is in the feudal system of Medieval Britain, that a force that is recognisably a militia was organised. The militia was organised on a local basis for service within its county or within the England and eventually the United Kingdom.\nThe first known, documented, mention of the word 'militia' dates back to reign of Queen Mary (1553-1558), when her husband King Philip entered into discussion over a 'Project for a Land Militia'. At times of threat to the country or internal disturbance within the counties, the resulting force was an important asset but, in times of peace and stability, the militia represented an expensive drain on the counties. Service in the militia could also be unpopular and was often the result of ballot - and payment for someone to serve in a nominated persons place was not unknown.\nA dispute between Charles I and Parliament over the control of the militia was one of the underlying causes of the English Civil War.\nThe Militia, essentially an infantry force, was regularly mobilised through out the seventeenth, eighteenth and nineteenth centuries for defence of the United Kingdom. The Militia formally became a part the county regiments in 1881 and finally disappeared into the 3rd Special Reserve Battalions in the 1908 reorganisation of the Territorial Army.\nThe Regular Army dates back to the Restoration of the Monarchy in 1660 when Charles II raised regiments of Guards. However, it was the protestant Duke of Monmouth's landing at Lyme Regis in June 1685 that gave birth to the senior of our constituent regiments, which eventually became the Devonshire Regiment. Monmouth landed just west of The Cobb and assembled his 150 men in the Market Square, where, with banners flying, they rallied support for a rebellion against Catholic King James II. One hundred men from Lyme joined the rebels, and marched north.\nWith the Duke of Monmouth and his army, made up largely of peasants armed with pitchforks and scythes, approaching Bristol, the local commander of the Kings Army, the Duke of Beaufort was commissioned to raise:\n'a corps of musketeers and pikemen composed of men of distinguished loyalty who resided in the disturbed districts of Devonshire, Somersetshire and Dorsetshire'.\nThis corps included the Beaufort Musketeers, who were to become the North Devonshire regiment.\nBy 24 June 1685, attack seemed imminent. He therefore drew up what forces he had, some on Radcliffe Mead and a smaller force in the Lamb Ground to meet the intruders. The following day tension in the city was at its height. What the inhabitants do not appear to have known was that Monmouth had been deflected from his course by two parties of royalist Horse who had come upon him at Keynsham - about four miles south-east of the city. After calling a council of war, the rebel leader led his now dispirited army back to Bridgwater and to ultimate defeat at Sedgemoor on the night of 5th July 1685.\nIn the aftermath of Monmouth's Rebellion the King called for well-disciplined and regularly paid army but after the Civil War, during which the people of England having gained a healthy suspicion of the military, the need for a sizeable standing army was only grudgingly acceded too.\nThe Glorious Revolution\nHaving been raised to keep James II on his thrown three years earlier, Hamner's Regiment, as a part of John Churchill's (later the Duke of Marlborough) Army failed to save the king for a second time. As a result of religious strains, in November 1688 the Dutch Prince William of Orange was invited by protestant politicians to invade England. Catholic King James II fled London when John Churchill and the Army declined to fight the 'invader'. By February 1689, William and his English wife Mary were offered the crown.\nThe constitutional settlement following the Glorious Revolution shaped the formation of the British state for the following centuries, and the ensuing wars that the regiments took part in, contributed to Britain's rise as an international power.\nThe first of these wars began almost immediately, when James returned to Ireland determined to take back his throne.\nOperational Service in Ireland\nThe Regiment's first engagement and battle was fought in Ireland on the banks of the River Boyne, on 1 July 1690 for King William III against his father in law, the erstwhile king, James II. James, with the support of Louis XIV of France, who was at the height of his drive to make himself master of Europe, landed in catholic Ireland at the head of a small French force, in March 1689. Around this force he gathered support from the openly rebellious Irish. At stake were the British throne, French dominance in Europe and Religious power in Ireland.\nHanmer's regiment had been in Ireland from 1689, when it took part in the relief of the siege of Londonderry. The following year William led his army of 36,000 men south to confront James's 25,000 strong Jacobite Army.\nWilliam distracted a large proportion of the Jacobite army by a flank march to the west, leaving the centre, including Hanmer's Regiment free to attack across the Boyne against a lightly held font. Having established a bridgehead the Wiliamite Army drove the Jacobite Army but had to hold out against a series of cavalry counter attacks. The enemy cavalry nearly broke through but were held by the disciplined volleys of the infantry and were finally broken by the arrival of Williams and his cavalry.\nJames's Army were forced back but they made a disciplined withdrawal and escaped the destruction that William had planned for them. Hanmer's Regiment had played a creditable part in what was its first proper battle.\nRaising of the Dorset Regiment\nOnly a twelve years later as what became the War of the Spanish Succession, the Dorsets, or Colonel Coote's Regiment, was raised by a Royal Warrant dated 12 February in Ireland in 1702. The Regiment, eventually numbered the 39th of Foot, was partly formed from soldiers of a disbanded regiment (Lisburne's) and was raised as a part in another increase in size of the Army. Signing the officer's commissions was virtually William III's last act on his deathbed. Colonel Richard Coote was shortly afterwards killed in a duel and was succeeded by Colonel Sankey, whose name the Regiment took.\nThis time, additional troops were required to take part in the Duke of Marlborough's campaigns on the continent of Europe but Sankey's regiment spent the first five years of its existence in garrison duties in Ireland. Though neither of the regiments that were to become the Devons and the Dorsets were to take part in any of the four major battles in the twelve years of the war, they campaigned in the Low Countries, France, Germany Spain and North America. In an age when warfare had more to do with manoeuvre and sieges, the two regiments marched countless miles, scaled many a rampart and fought many bloody actions that fell short of full battle.\nSankey's Regiment's first campaign was in Portugal and Spain under the command of the Earl of Peterborough. This famous officer mounted Sankey's regiment on mules to provide mobility that enabled them to keep up with the cavalry. In May 1709, reaching the battlefield of Caya, in such manner, the Regiment distinguished itself, earning praise for its exceptional steadiness and bravery and the nickname 'Sankey's Horse'.\nIn 1712 Sankey's was in Portugal and the following year made its first acquaintance with the fortress Rock of Gibraltar. Service in Minorca followed, along with a period as marines aboard Admiral Byng's fleet, taking part in the Battle of Messina.\nIn 1726 the Regiment, having been hurried to Gibraltar to reinforce the garrison, spent a strenuous year on the Rock, during the first siege by the Spaniards. The Regiment now numbered the 39th of Foot, saw service in the West Indies and, after returning home from Jamaica, had another spell of active service afloat as marines.\nHanmer's (11th/Devonshire) Regiment\nRemaining in Ireland until 1703, the Devonshires then joined John Churchill, the Duke of Marlborough's, army in Holland, where they assisted to capture the fortress of Huy and the city of Limburg, then held by the Spaniards. The Regiment's next campaign was in Portugal in 1705, and, after a visit to England, again in 1708.\nThe year 1709 was a memorable year for the Devons, as they were again in Holland and with the Duke of Marlborough's forces, taking part in the siege of Mons. While working in the trenches within one hundred yards of the French palisades, the Regiment was attacked by a strong force of the enemy. The grenadiers, who were protecting the working party, were thrown into confusion by the attack. Nothing daunted, the soldiers of the Regiment threw down their picks and spades, drew their swords and counter-attacked with such vigour that the French were driven back over the palisades of Mons, with severe losses. Some of the soldiers of the 11th even followed the enemy over the palisades into the fortified town. During the attack and counter-attack the Regiment sustained 160 casualties. For some twelve months the 11th Foot was engaged in operations against fortified towns on the French Belgian border: Douai, Béthune, Aire, and St. Venant.\nIn 1711 the 11th formed part of the unsuccessful expedition against the French in Canada.\nExcept in 1715 and 1719, where the 11th gave a good account of itself in Scotland, the Regiment's history is uneventful for some thirty years. In 1719 Spanish troops landed in Ross-shire, and were joined by the Scottish clans of the neighbourhood. The 11th formed part of a small force which, charging with the bayonet, dispersed Spaniards and Highlanders, won the Battle of Glenshiel, and broke the insurrection.\nThe War Of The Austrian Succession\nIn 1740, the deaths of two leading European monarchs led the continent to war. In Prussia, the throne passed to ruthlessly ambitious Frederick, who inherited Europe's most capable army a bureaucracy of considerable efficiency. In using this instrument, Frederick was to earn the appellation 'The Great'.\nCharles VI, Emperor of Austria, died later in 1740, with his daughter Maria Theresa, inheriting his throne. This was the excuse that Frederick needed to ignore Prussia's commitment to his father's alliances and seized Silesia, marching his troops into the capital, Breslau, and taking control of the rich Austrian province.\nMaria Theresa was not easily cowed and declared war on Prussia and counter-invaded Silesia, starting a war that was rage for twenty-five years. The conflict did not finally end until the Treaty of Paris in 1764 confirmed Prussia's ownership of Silesia.\nThe first part of the war between 1740 and 1748, known as the 'War of the Austrian Succession', saw George II, King of England and Elector of Hanover, send British troops to join the Allies centred on Maria Theresa. However, George's principal concern was the French, who he feared would advance through the Low Countries and invade his beloved Hanover. Keeping a major power out of Flanders was a key tenant of British foreign policy; consequently, George was able to persuade Parliament to deploy the Army to the continent.\nIn 1743, back again in Flanders, the 11th Foot was a unit in an expeditionary force of 37,000 personally commanded by King George II. The Army, with its infantry, marched south from Flanders to the Frankfurt region of Germany. There it was joined by George II. The battle of Dettingen followed, fought against the Duc de Noailles's French Army of 60,000 men. The disciplined fire of the British infantry drove French cavalry and infantry into a disordered retreat. Dettingen though by no means the Regiments' first battle was the senior battle honour borne on the Devon and Dorset's colours.\nIn 1745, on the breaking out of Bonnie Price Charlie's (Son of James II, the 'Young Pretender') rebellion in Scotland, the 11th Foot was ordered home, and took part in the actions whereby Carlisle was recaptured and the Young Pretender's force was driven back into Scotland and eventually overthrown.\nReturning to the Netherlands, in 1746, the 11th Foot joined the Allied Army and helped to check the French advance. At Roucoux, on October 11th, the Devonshire Regiment with the 19th Foot (Yorkshire Regiment) were ordered to hold a valley to the last man. Three villages were held by eight English, Dutch, and Hessian battalions. Against them, preceded by a heavy cannonade, the French threw a force of some fifty battalions. Overwhelmed by numbers, the 19th and the 11th, earning the nickname, 'Ever Faithful', held on, until wave after wave of the enemy had been broken and crumpled up. The enemy were beaten; the losses of the 11th in that gallant action were 10 officers and 180 rank and file killed or missing; 2 officers and 26 rank and file wounded.\nWhen peace was made with France, the 11th Foot returned to England and in 1751 by Royal Warrant the regimental facings were changed to full green. It is probable that the tawny of older days was rather a lighter green than the orange colour usually associated with tawny. In 1756 the strength of the Regiment was raised to twenty companies, divided into two battalions, the 2nd of which two years later became the 64th Foot, later the North Staffordshire Regiment.\nIn 1754 the 39th Foot was deployed, as the first British Government troops, rather than East India Company mercenaries, to India. The Regiment served in Madras, under Colonel Adlercron, until August 1756, when as a result of the 'Black Hole of Calcutta' a part of the Regiment was detached to Bengal. In May, 1757, Colonel Adlercron commanded a force, including part of the 39th, sent to relieve Trichinopoiy, and was engaged in operations in Wandewash.\n.Another three companies (about 300 men) of the 39th served under General Clive in the operations against the Nabob of Bengal, which ended with the total defeat of the enemy at Plassey, near Calcutta, in June 1757. The British force on this occasion consisted only of 3,000 men, most being Indian troops. The Nabob's army mustered 40,000 infantry, 15,000 cavalry, and guns, some of which were manned by French artillerymen. This formidable force was completely routed by the desperate bravery of the small British force, which captured the whole of the enemy's camp, baggage, guns and stores. This victory constituted one of the most complete and overwhelming achievements in military history.\nThe Seven Years War\n.It was not until early 1755, during the period of tension prior to the outbreak of Seven Years War, that a further expansion of the Army brought the 54th of Foot into existence. It was raised at Salisbury, in Wiltshire by Lieutenant Colonel John Campbell, Duke of Argyll. It had a very gaudy uniform of scarlet, with 'popinjay' green facings profusely laced, the officers wearing silver and other ranks yellow lace. Initially the regiment was numbered 56th Foot but was renumbered 54th in 1757 when two more senior regiments were disbanded.\nThe 39th returned home from India in 1758 with a glorious record, which was worthily upheld by the detachment of the Regiment that served under the Marquis of Granby in his campaign in Germany in 1759.\nMeanwhile, the 54th Regiment served as marines in the Mediterranean fleet under Lord Hawke and did garrison duty at Gibraltar before coming home in 1765, having being relieved at Gibraltar by the 39th Regiment.\nWith War again in progress, the 11th again faced her old enemies, and from 1760 to 1763 was fighting against the French. On 7 November 1761, while encamping in the snow, the 11th with two other regiments were attacked by a strong French force. Seizing their arms, the men with great gallantry drove off the attackers. Again, in 1762, the 11th in brigade with the 23rd (Royal Welch Fusiliers), 33rd (West Riding Regiment), and 51st (Yorkshire Light Infantry) surprised a French camp, captured the baggage and equipage, and dispersed the French army. In their retreat a division of the enemy army was surrounded, and only two of its battalions escaped.\nAfter peace was concluded, in 1763, the 11th Foot, garrisoned the island of Minorca during its occupation by the British.\nThe 39th remained at Gibraltar for many years from 1756 and was one of the six splendid regiments that took part in the four year long 'Great Siege'. During the American War of Independence, France and Spain took the opportunity to settle old scores and joined against the British in assistance to the revolting American colonists. One of the enemy's most important strategic objectives was the fortress of Gibraltar, to which the Spaniards laid siege.\nIn 1782, the regiments were designated 11th 'The North Devonshire Regiment', 39th 'East Middlesex'\" and 54th 'West Norfolk' Regiments. Officers were directed to 'cultivate an acquaintance with that part of the county, so as to create a mutual attachment between the inhabitants and the Regiment'. For that purpose, however, little time was allowed, for in 1783 the 11th Regiment went to Gibraltar, where it remained for eight years.\nMeanwhile, the 54th Foot was serving in America, where after some fighting at Brooklyn for instance, it garrisoned New York, Charlestown, Rhode Island, and other towns during the War of Independence. It is an interesting fact that the sergeant-major of the 54th Regiment at this period was William Cobbett, who became famous as a social reformer, and died a Member of Parliament. His condemnation of flogging in the army eventual led to its abolition.\nOne former officer who served with the 39th remained active in North America: Captain George Cartwright."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:75380fce-9cb6-4592-9a8b-a4073eee7b62>","<urn:uuid:acf62719-e8fa-4419-9fd0-a28817c6a43d>"],"error":null}
{"question":"What are the three main structural components of river deltas, and which are the largest river deltas in the world by area?","answer":"River deltas consist of three main parts: the upper delta plain (nearest to land with least water and highest elevation), the lower delta plain (middle transition zone), and the subaqueous delta (below water level, closest to the sea). Regarding size, the Ganges-Brahmaputra Delta in India is the largest in the world, covering 105,640 square kilometers. Other notably large deltas include the Mekong Delta (36,209 square miles), the Niger Delta (36,000 square kilometers), and the Volga Delta which drains 20% of Europe's rivers.","context":["Prior to understanding deltas it is first important to understand rivers. Rivers are defined as fresh bodies of water that generally flow from high elevations toward the ocean, lake or another river. In some instances however, they do not make it to the ocean - they instead flow into the ground. Most rivers begin at high elevations where snow, rain and other precipitation run downhill into creeks and small streams. As these small waterways flow farther downhill they eventually meet and form rivers.\nIn many cases these rivers then flow toward larger the ocean or another body of water and oftentimes they combine with other rivers. At the lowest part of the river is the delta. It is in these areas where the river's flow slows and spreads out to create sediment-rich dry areas and biodiverse wetlands.\nFormation of River DeltasThe formation of a river delta is a slow process. As rivers flow toward their outlets from higher elevations they deposit particles of mud, silt, sand and gravel at their mouths because the flow of water slows as the river joins the larger body of water. Over time these particles (called sediment or alluvium) build up at the mouth and can extend into the ocean or lake. As these areas continue to grow the water becomes more and more shallow and eventually landforms begin to rise above the surface of the water. Most deltas are only elevated to just above sea level though.\nOnce the rivers have dropped enough sediment to create these landforms or areas of raised elevation the remaining flowing water with the most power sometimes cuts across the land and forms different branches. These branches are called distributaries.\nAfter the deltas have formed they are typically made up of three parts. These parts are the upper delta plain, the lower delta plain and the subaqueous delta. The upper delta plain is the area nearest to the land. It is usually the area with the least water and highest elevation. The subaqueous delta is the portion of the delta that is closest to the sea or body of water into which the river flows. This area is usually past the shoreline and it is below water level. The lower delta plain is the middle of the delta. It is a transition zone between the dry upper delta and the wet subaqueous delta.\nTypes of River DeltasAlthough the aforementioned processes are generally way in which river deltas form and are organized, it is important to note that world's deltas are highly varied \"in size, structure, composition and origin\" due to factors such as climate, geology and tidal processes (Encyclopedia Britannica).\nAs a result of these external factors there are several different types of deltas all over the world. The type of delta is classified based on what controls a river's deposition of sediment. This can usually be the river itself, waves or tides. The main types of deltas are wave-dominated deltas, tide-dominated deltas, Gilbert deltas, inland deltas and estuaries. A wave-dominated delta is one where wave erosion controls where and how much sediment remains in the delta after a river drops it. These deltas are usually shaped like the Greek symbol, delta (∆). An example of a wave-dominated delta is the Mississippi River delta. A tide-dominated delta is one that forms based on the tide and it has a dendritic structure (branched, like a tree) due to newly-formed distributaries during times of high water (Wikipedia.org). The Ganges River delta is an example of a tide-dominated delta.\nA Gilbert delta is a steeper type of delta that is formed by deposition coarse material. Gilbert deltas can form in ocean areas but it is more common to see them in mountainous areas where a mountain river deposits sediment into a lake. Inland deltas are deltas that form in inland areas or valleys where a river will divide into many branches and rejoin farther downstream. Inland deltas, also called inverted river deltas, normally form on former lake beds.\nFinally, when a river is located near coasts that have large tidal variation they do not always form a traditional delta. They instead form estuaries, or a river that meets the sea. The Saint Lawrence River in Ontario, Quebec and New York is an estuary.\nHumans and River DeltasRiver deltas have been important to humans for thousands of years because of their extremely fertile soils. Major ancient civilizations grew along deltas such as those of the Nile and the Tigris-Euphrates rivers and the people living in them learned how to live with the natural flooding cycles of deltas. Many people believe that the ancient Greek historian Herodotus first coined the term delta nearly 2,500 years ago as many deltas are shaped like the Greek delta (∆) symbol (Encyclopedia Britannica).\nToday deltas remain important to humans because they are a source of sand and gravel. In many deltas this material is highly valuable and is used in the construction of highways, buildings and other infrastructure. In other areas delta land is important in agricultural use. For example the Sacramento-San Joaquin Delta in California is one of the most agriculturally productive areas in the state.\nBiodiversity and Importance of River DeltasIn addition to these human uses river deltas are some of the most biodiverse areas on the planet and as such it is essential that they remain healthy to provide habitat for the many species of plants, animals, insects and fish that live in them. There are many different species of rare, threatened and endangered species living in deltas and wetlands. Each winter, the Mississippi River delta is home to five million ducks and other waterfowl (America's Wetland Foundation).\nIn addition to their biodiversity, deltas and wetlands can provide a buffer for hurricanes. The Mississippi River delta, for example, can act as a barrier and reduces the impact of potentially strong hurricanes in the Gulf of Mexico as the presence of open land can weaken a storm before it hits a large, populated area such as New Orleans.","Rivers come in all shapes and sizes. Today, we will look at the largest river deltas on earth and where to find them.\nRELATED: The 12 Largest Rivers in Africa\nA river forms a delta when it flows, along with its sediments, into a larger, more stagnant water body. We can compare a delta to a natural funnel that feeds fresh water into salt water and a river into the sea. This unique landform usually looks like a triangle. Its striking resemblance to the fourth letter of the Greek alphabet earned it the name delta.\nDeltas form a crucial part of our ecosystem. The accumulation of sediments by rivers and floods creates a natural sieve that stops water pollutants. Furthermore, deltas are home to beneficial life forms such as crabs, oysters, mugwort, dolphins, and even tigers.\nWhat are the parts of a Delta?\nA delta is a middle ground between river and sea. This intermediary landform should have the following:\n- Subaqueous delta: this is the lower part of the delta. Different sizes of sediments form this layer. The subaqueous layer has a dangerously steep slope caused by a drastic drop in velocity as the particles move from the river to the much more profound and broader sea. The soil profile contains silt, rich in nutrients and minerals, and supports plant growth.\n- Subaerial delta: we can split the subaerial delta further into lower and upper deltaic plains. Waves and tides influence the lower table, while the river’s flow shapes the upper plain.\nTop 10 Largest River Deltas on Earth (and Where to Find Them)\nTigris-Euphrates Delta in Southeastern Asia\nThis delta arises from joining two main rivers. They are Tigris and Euphrates, along with other smaller tributaries. The delta drains rivers across five countries; Turkey, Iran, Iraq, Syria, and Kuwait. It covers about 18,000 square kilometers and runs for 7,142 square miles. Mesopotamia is the area between the Tigris and Euphrates rivers.\nThis sizable delta proves the economic value of deltas. Locals believe that the first Mesopotamian civilization arose around this delta.\nNiger Delta in Nigeria, Western Africa.\nThe Niger Delta is arguably one of the most revenue-generating rivers in Nigeria. The Niger River and its delta are home to many marine animals, including endangered species. The delta runs for about 150 miles from north to south, covering an expanse of 36,000 square kilometers and emptying into the Gulf of Guinea. The delta also has some historical importance. Through this delta, certain tribes built fortunes and began to trade agricultural products instead of the slave trade. This delta plays a significant role in agriculture, fishery, energy, and transportation.\nOrinoco Delta in Venezuela, South America\nThe Orinoco Delta remains uninhabited by humans. It is home to rivers, canals, swamps, and rainforests that preserve the delta’s wildlife profile. Within the delta is a national park, Mariusa National Park. This park houses a vast array of wildlife, birds of prey, anacondas, and other beautiful and deadly creatures. The delta and park preserve several endangered species, including the giant river otter, harpy eagles, bush dogs, Amazon river dolphins, and many more.\nThe Orinoco delta leads the Orinoco river, one of the longest rivers in South America, into the Atlantic Ocean. The delta runs for about 8,700 square miles.\nVolga Delta in Russia\nThe Volga Delta is Europe’s largest delta. This bountiful delta drains 20% of Europe’s rivers, including the Volga River, accounting for 80% of the Caspian River’s inflow. It runs for 10,500 square miles. A view of this delta from space reveals that the delta splits into over 1000 ways before reaching the Caspian Sea.\nThis delta relieves thousands of migratory birds, including raptors and passerines. Sturgeons, carp, and catfish populations also seem to thrive well here.\nIndus Delta in Pakistan\nThe Indus Delta begins where the Indus River meets the Arabian Sea. The climate surrounding this delta is arid. Thus, the delta receives only about 25 to 50 centimeters of rainfall every year. Lately, geographers call it the Dying Delta. This is because it suffers from a loss of vegetation. Its freshwater channels are running dry, causing the mangrove forests to shrink from 600,000 acres to a mere 182,000.\nThe mangrove forests in the Indus delta supply fuel wood. The Indus Delta covers about 11,400 square miles.\nMississippi Delta in the United States\nThe Mississippi Delta, which runs through Memphis, Helena, New Orleans, and Venice, is famous for its affiliations to music and art. Proponents refer to this delta as the cradle of American music, saying that it was here that music styles such as blues, Cajun music, jazz, and zydeco evolved.\nCotton plantations bloomed in the Mississippi in the era before the American Civil War. Because of the excellent planting soil, Mississippi saw an influx of planters who relied on the labor of enslaved Africans. Today, this area remains one of the most culturally diverse regions in South America.\nUnfortunately, this great delta is shrinking fast. Every 100 minutes, open water swallows a patch of land the size of a football pitch. We have the constantly rising sea level to thank for that.\nHuang He Delta in China\nRecords reveal that the location of this delta changes ever so often. The Huang He River, also called the Mother River, is termed ‘Wanderer.’ This is because its lower reaches often change course, thus shifting the position of the delta by a few hundred kilometers over the centuries. Recently, the Chinese have engineered the natural drifting of the delta. They hope to protect an immense field of oil and gas wells close to the delta.\nThe delta leads the Huang He river, also called the Yellow River, into the Yellow Sea. It is the most sediment-filled delta on Earth.\nLena Delta in Russia\nThe Lena Delta spans 30,000 square kilometers and protects an extensive range of wildlife. The delta, which is frozen tundra for seven months every year, temporarily becomes a lush wetland in the fifth month. The delta supports trade and travel; it is possible to transport cargo over the Lena Delta.\nThe Lena Delta Natural Reserve protects a part of the Lena Delta and is one of the largest protected areas in Russia. The Lena Delta is an excellent breeding ground for fish.\nMekong Delta in Southeastern Asia\nDenizens call the Mekong Delta the Nine Dragon River Delta because it flows through nine main channels into the Southern China Sea. It is a biological treasure trove, housing over 1,000 animal and plant species.\nAsians tend the high-yielding land surrounding this delta for agriculture. The produce from the Mekong Delta feeds about 20% of the Vietnamese population.\nAnimal geographers have spotted new mammals, reptiles, and amphibian species in some areas of the Mekong Delta.\nThis delta spans 36,209 square miles.\nGanges-Brahmaputra in India\nThe Ganges-Brahmaputra Delta is the largest delta on Earth. It is a highly fertile land, warranting the name Green Delta. It also supports the highest population density among all deltas. The delta leads the Brahmaputra, Ganges, and other smaller rivers into the Bay of Bengal.\nIt is a typical arcuate delta measuring 105,640 square kilometers and 40,790 square miles.\nTypes of Deltas\nDeltas can be classified based on their shapes or conditions leading to their formation.\nBased on shapes, Deltas can be:\n- Arcuate delta: the first kind of delta is the upright, triangular deltas. They are also called bow-shaped or fan-shaped. They have a convex surface. A prominent example is the Nile Delta in Egypt, which empties into the Mediterranean Sea.\n- Cuspate delta: a cuspate delta forms when a river empties into a water body with solid waves. The strong waves prevent the sediments from settling rapidly, so the resulting delta is tooth-shaped. For example, the Ebro delta in Spain.\n- Bird-foot delta: this delta has widely spaced distributions that extend like the claws on a bird’s foot. This is because the high velocity from the river carries fine particles farther into the seawater. The freshwater from the river is lighter than the seawater and thus floats over it. The Mississippi River is one such example.\n- Inverted triangle: this relatively rare kind of delta forms when the narrower end of the delta is closer inland and the wider mouth nearer to the sea. An example is the San Sacramento River Delta.\nDeltas are of appreciable economic importance. They support plant and animal life, agriculture and fishing, transport, and tourism.\nThe largest delta on Earth is the Ganges-Brahmaputra Delta in India, Asia. It has an area of 105,645 square kilometers.\nIt is no surprise that the most copious rivers flow into the largest deltas. For example, the Brahmaputra river is on our list of the 15 Largest Rivers in the world.\n- Land of Lakes: The 20 Largest Lakes in Minnesota\n- What is a Lake? 5 Features that Define a Lake\n- Discover The 12 Longest Rivers in Asia\nThe photo featured at the top of this post is © 1WorldTravels/Shutterstock.com\nFAQs (Frequently Asked Questions)\nWhich two rivers form the largest and fastest-growing deltas?\nThe Ganges and the Brahmaputra rivers pour into the Ganges-Brahmaputra delta, the world’s largest delta.\nWhich is the second largest inland delta in the world?\nThe Okavango Delta in Botswana is the second largest inland delta known to man. You can visit this delta through the Moremi Game Reserve within the vicinity. It spans up to 15,000 square kilometers.\nHow many deltas are in the world?\nThere are about 11,000 deltas worldwide, and they come in different shapes and sizes\nWhich is the most famous delta in the world?\nThe Nile Delta is the most famous. Historians mention it, and the shape of this landform inspired the name delta.\nThank you for reading! Have some feedback for us? Contact the AZ Animals editorial team."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:3bfb1fb0-c604-4fc4-9c68-4747aa57569b>","<urn:uuid:fefce884-53cd-45cf-882c-a62bf3b95a1e>"],"error":null}
{"question":"What are the space requirements for container herb gardens, and what are the essential steps for preparing homemade fertilizer solutions?","answer":"For container herb gardens, a 12-inch diameter pot is recommended to start with, accommodating five or six herbs. Some herbs like mint need individual pots due to their vigorous root systems. For fertilizer preparation, the process varies by scale - small gardens can use 5-gallon buckets with simple compost tea solutions, while medium-scale gardens require 25-gallon containers. When mixing mineral fertilizers, phosphorous, nitrogen, potassium, magnesium, and calcium salts must be dissolved in hot water, with calcium added last to prevent precipitate formation. Thorough mixing is essential, particularly in larger containers where ingredients may settle in corners.","context":["Build Your Container Herb Garden Using Cartons, Pots & Boxes!\nNot everyone has an easy and convenient space in which to grow herbs, but even if you don’t, you can still cultivate herbs. if you don’t have a garden you can use a small balcony or patio area using containers to create a container herb garden.\nMany herb gardening beginners start their herb gardening by using very basic containers such as plastic ice cream or yogurt cartons or wooden boxes lined with plastic sheeting. Any of these will work, but its much better if you can get hold of some clay pots. Clay pots aren’t expensive and they’ll last a lot longer than plastic or wood. The other big benefit of clay pots is that they keep the soil inside cool on a hot day.\nPlanting herb pots and containers isn’t a new thing. Hundreds of years ago the Romans, Greeks and Egyptians grew their herbs in pots or containers, and more recently the Victorians were big fans of the container herb garden.\nIn fact the Victorians designed and made lots of different types of clay pots for their gardens, and it’s still possible to get hold of good looking Victorian garden pots in antique shops and at antique fairs. If you want to make your container herb garden rather special try and find an antique Victorian pot that you can use to create a container herb garden. The one illustrated in the photograph is a very superior example, but there are also lots of less impressive and cheaper pots around.\nIf you decide to use clay garden pots, you’ll find lots of different types and shapes to choose from at your local garden center with styles ranging from modern to classic (like the one in the photograph). Use your imagination to select a pot that’s right for the herb garden you plan to create.\nIf you visit your local garden center or shop on-line you’ll probably find herb boxes (also called herb planters) for sale. These are sometimes prefilled with a selection of herbs. They are a good choice if you just don’t have the time to create your own container herb garden and don’t mind spending a few more dollars just to get started with your container herb garden.\nWhat Size Pot? How Many Pots?\nWhen you set about creating your herb garden make sure that you get a container that’s big enough to take a reasonable number (five or six at least) of herbs. A 12″ diameter pot is a good size to start with, although I can guarantee that after the first year with your container herb garden you’ll want to get a larger pot and grow a wider range of herbs.\nIn the picture opposite the herbs have been planted in wooden troughs up against a sunny wall.\nIf you’re creating a container herb garden it’s not always a good idea to put all your herbs together in one pot. Some herbs are best grown in individual pots.\nMint is a good example of a herb which should be grown in its own pot because it has a strong and vigorous root system and has the habit of taking up more than its fair share of soil space wherever it’s planted. For this reason lots of gardeners choose to plant their mint in a separate pot alongside all their other herb boxes or pots.\nWhen you are choosing the pot for your container herb garden make sure you get one that doesn’t have too narrow a top. Those wonderfully-shaped Ali Baba-type garden pots might look fantastic, but the’re not a good choice if you want to plant more than a couple of herbs in them. I have used Ali Baba pots for my lavender plants. I plant each pot with just one lavender plant so that they have plenty of room to grow and spread out. When they are fully grown and in flower they look fantastic – and they are really appreciated by the bees.\nThe “Soil Mix” for Your Container Herb Garden\nWhen you’re planting a container herb garden it’s important for the soil to have the right texture to allow the roots of the herbs to expand and take in air and water. You can use ordinary garden soil, but soils in different locations can vary a lot, so it’s best to make up your own soil mixture. An alternative is to buy a specially prepared compost.\nIf you feel like getting really “hands on”, why not make your own “herb compost”. The ideal mixture is a combination of good quality compost (which contains the necessary plant nutrient) mixed with a gritty/sandy material that will help with drainage and breathing.\nIf you decide to make your own herb compost you can use any good quality compost mixed with Perlite. This combination will give you the ideal soil mix for your herbs. Perlite is easy to get hold of from garden centers and DIY stores, and can also be purchased on-line for a good price. Use 1 part of Perlite to 2 parts of compost and mix them together thoroughly. There are lots of different views on how to make an ideal soil mix for a container herb garden, some of them quite complicated, but I find that the compost and Perlite mix works very well.\nBefore you put your compost and Perlite mix into your container pot, lay some stones or stony gravel over the hole in the bottom of the pot. This will help to ensure that the drainage is good and the roots of your herbs don’t become waterlogged. I use old broken pot pieces in my container pots. Use whichever you can get hold of.\nFill your pot almost to the top with your soil mixture, and then plant your herbs making sure that they are at the same depth as they were in the pots they came in. Also, make sure that there is sufficient space between each plant to avoid overcrowding as they grow. These distance requirements vary from herb plant to herb plant because some herbs can grow very large and over-shadow and stifle smaller less vigorous herbs.\nThe video below contains lots of useful information about choosing the herbs for a container herb garden.\nCreate a “Mobile” Container Herb Garden\nOne big advantage of planting an herb garden in pots or herb planters is that you can move them around to catch or avoid the sunlight in your garden.\nUnfortunately, once you have planted your container herb garden with herbs it can become too heavy to move easily.\nYou can solve this problem by buying or constructing a container herb garden trolley (often called a “dolley”).\nHere are some more examples of plant container trolleys.\nCare of Your Container Herb Garden\nIn general herbs like lots of sunlight and water. You should water your herbs once a day, but avoid over-watering. However, the disadvantage of watering container-grown plants a lot is that the water leeches the nutrients out of the soil mix.\n|Miracle-Gro Liquid Fertilizer\n|| Miracle-Gro Solid Fertilizer\n| Miracle-Gro Organic Potting Compost\n||Miracle-Gro Potting Compost Mix|\nA lot of composts you can buy have a mixture of compost and added nutrients, but even so the soil in your pot or container needs to be provided with additional fertilizer regularly to make up for the nutrients that are washed away.\nTo make up this nutrient loss either use liquid fertilizer (ideally once a week) or lightly fork a solid fertilizer (plant food) into the soil around your herbs. If you use solid fertilizer you don’t need to use it as frequently.\nI have been using the Miracle-Gro products for several years and have found them to be excellent. They can all be purchased from Amazon at either very competitive or “sale” prices.\nIf you shop regularly with Amazon you may also be able to get Amazon products delivered free of charge.\nAt the end of the growing seasons many of the herbs in your container herb garden will have grown very large. If you intend to continue growing your herbs in pots or containers the best thing to do is to throw these plants away after you have harvested them and start afresh the following year. However, when you get more experienced you will be able to take stem and root cuttings from some of your herbs and use these to create a new herb garden the following year.\nAnother alternative is to transplant these overgrown herbs into your garden (if you have one).\nThe information in this article should be sufficient to enable you to make a good start with your container herb garden. My book “The Secrets of Successful Herb Gardening” will provide you with lots of extra, useful information to help you with your herb growing.\nWishing you every success with your container herb garden.","Fertilizing a garden can become expensive when using premixed commercial-grade solutions. In many cases, premixed concentrates for liquid fertilizers will not contain all of the required nutrients because of chemical incompatibilities at high concentrations. To avoid these problems, solutions can be mixed at home using one of several nutrient recipes and repurposed plastic containers. The type and number of mixing tanks required will depend on the scale of your fertilization needs.\nFertilizer for small-scale container gardening can be prepared using 5-gallon buckets and an aquarium air pump. Compost tea or worm-casting tea is the simplest homemade fertilizer to create at this scale. Place 1 gallon of compost or worm castings into a plastic 5-gallon bucket, add 1/2 cup of molasses, use the aquarium air pump to aerate the solution and let it sit for 2 days. The resulting liquid will be ready to use for fertilizer.\nA medium-scale garden uses more fertilizer than is convenient to mix in 5-gallon buckets and can be mixed in 25-gallon tubs or containers. This method prepares fertilizer from individual ingredients and requires several 5-gallon buckets for premixing. These containers should not be porous, such as concrete or pottery, because nutrients will leach into the walls and change the fertilizer concentration. Plastic or metal is more appropriate. Phosphorous, nitrogen, potassium, magnesium and calcium salts are first mixed in hot water to dissolve them into solution and added to the larger 25-gallon tub. Calcium should be added last and slowly because in concentrated form it will form insoluble precipitates with magnesium and phosphate salts. Micronutrients are also mixed separately in a form more concentrated than required and diluted into the 25-gallon tub.\nThe 5-gallon buckets used for compost teas and premixing mineral fertilizers are common waste items on home-construction sites. Used for a wide variety of bulk products, such as house paint, wooden deck sealer and even pelletized lawn fertilizers, these buckets can be washed out and used for mixing your new liquid fertilizers. If waste buckets are not available, unused plastic 5-gallon buckets are an inexpensive purchase at home improvement stores. The 25-gallon buckets for mixing fertilizers from mineral recipes can be obtained by repurposing large trash cans or the common, rectangular, thick-walled storage tubs found in home and office storage departments. The tall, thin shape of the trash cans and rectangular shape of the storage tubs are not ideal for mixing, but round 25-gallon planters usually have large drainage holes that are not easily covered.\nThe larger the container is for holding liquid, the harder it will be to mix the ingredients appropriately. At 5 gallons, the liquid can still be mixed by hand or with an agitator such as an aquarium pump. At 25 gallons, special care will need to be taken to make sure that some ingredients are not sitting in a corner unmixed, and the liquid will need to be stirred longer than in a smaller container to ensure adequate mixing even of dissolved compounds. Fertilizer ingredients are sold as solid minerals and are not always easily dissolved. They may take an extended amount of stirring and waiting.\n- Pennsylvania Department of Environmental Protection: How to Make Compost Tea\n- Hydroponics as a Hobby; James C. Schmidt, John M. Gerber, J. W. Courter\n- University of Florida IFAS Extension: Grow Your Own Vegetables Without Soil\n- British Columbia Ministry of Agriculture: Preparing a Complete Fertilizer Solution\n- Utah State University: Nutrient Solution Recipes\n- University of Florida IFAS Extension: Nutrient Solution Formulation for Hydroponic (Perlite, Rockwool, NFT) Tomatoes in Florida\n- Polka Dot RF/Polka Dot/Getty Images"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:93f6150f-3302-42e7-ba5e-498a5c465552>","<urn:uuid:8241b849-386d-4e5d-a550-fd503e22b1dc>"],"error":null}
{"question":"What was discovered about Secretariat's genetic legacy in horse racing?","answer":"Secretariat's genetic legacy showed an interesting pattern known as the maternal grandsire effect. While most of Secretariat's direct offspring (foals) performed poorly in races, and his sons' offspring also raced poorly, many of his daughters' foals became outstanding racehorses. This demonstrates how certain genes, including speed genes in racehorses, can skip a generation and only express in grandchildren if their carrier was a particular sex.","context":["Though placentas support the fetus and mother, it turns out that the organ grows according to blueprints from dad, says new Cornell research. The study, published in the Proceedings of the National Academy of Sciences in June, shows that the genes in a fetus that come from the father dominate in building the fetal side of the placenta.\nGenes work in pairs: one from each parent. But about 1 percent of mammalian genes choose sides, a phenomenon called genomic imprinting. Imprinted genes use molecules that bind to DNA (epigenetic tags) to quiet one half and let the other lead. In the study, the researchers discovered 78 new imprinted genes using horse-donkey hybrids.\n\"This is the first study to offer an unbiased profile of novel imprinted genes in a mammal other than mice,\" said lead author Xu Wang, a postdoctoral associate in the laboratory of Andrew Clark, professor of molecular biology and genetics and the study's senior author.\nUsing mouse studies, only about 100 genes with imprinted expression had been identified. To determine whether other genes exhibit imprinted expression, Wang and colleagues sequenced all of the expressed genes (the transcriptome) of hinnies (whose mothers are donkeys; fathers horses) and mules (whose mothers are horses; fathers donkeys) and looked for parent-of-origin differences.\nBecause the genomes of horses and donkeys differ by approximately one in every 200 base pairs (differences called single nucleotide polymorphisms or SNPs), the paternal versus maternal contributions in their offspring can be genetically tracked. Using SNPs, the authors were able to identify the parent-of-origin for 7,000 genes.\nOf those genes, transcriptome data from placental tissue revealed that 93 genes were imprinted. While only 15 of the 40 known imprinted human genes were identified in this set, their expression bias was identical to that of humans, indicating a highly conserved function for these genes between the horse family and humans.\n\"[The other 78 candidates] were partially imprinted, not 100 percent,\" indicating a highly dynamic process, said Wang.\n\"Genomic imprinting in the placenta may be an adaptive mechanism [that promotes functional] plasticity in response to changing environmental conditions during gestation,\" according to the study.\nStrikingly, a majority of the imprinted candidates were paternally expressed, and this expression bias was lost when the transcriptomes of fetal tissues were examined. At the same time, as in mice, paternally imprinted genes heavily regulate placental development in these animals.\n\"Mouse experiments showed that if all DNA comes from the mother, the embryo grows quite well, but not the placenta, suggesting some degree of sex-based division of labor between programming the placenta and the embryo,\" said Wang. \"Our results confirm what these past findings suggested.\"\nThe methods used in the study may also help breeders.\n\"This discovery explains what breeders call the maternal grandsire effect,\" said co-senior author Doug Antczak, equine geneticist at Cornell's College of Veterinary Medicine. \"Some genes, like so-called speed genes in great racehorses, skip a generation and only express in grandchildren if their carrier was a certain sex. For example, most foals of history's best racehorse, Secretariat, raced poorly. So did his sons' offspring. But many of his daughters' foals were outstanding racehorses. We've developed a new approach that can identify imprinted genes that may be linked to racehorse traits and which could help breeders' decision-making.\"\nBetter understanding of genomic imprinting may offer insights into several human diseases. Mistakes in imprinting genes can impair development, spurring genetic problems that can cause gigantism, dwarfism, neurological failures, incomplete sexual development and others.\nExplore further: Genome reveals how Hessian fly causes galls in wheat"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:fb540ad9-781a-4d58-a7ad-ea98293c4d3a>"],"error":null}
{"question":"How do the Djenné Mosque in Mali and Putrajaya's Iron Mosque differ in their yearly maintenance practices?","answer":"The Djenné Mosque requires annual replastering in spring, which has become an important social event. In contrast, the Iron Mosque in Putrajaya employs modern architectural solutions that don't require such maintenance, using durable materials like stainless steel iron and architectural wire mesh imported from Germany and China.","context":["This is the mosque of Djenné city in central Mali. Djenné is, along with Timbuktu further north, classified as a world heritage by the UNESCO. The whole city has a unique style of architecture, all in mud and bricks, and the Grand mosque is the jewel of the crown.\nThe city was founded at about 800 AD and was an important trade centre from the 16th century. The first mosque at this site was built in 1240, but this one is only 100 years old. The architect was Ismaila Traoré, who was head of the masons' guild of Djenné at that time. The mosque is replastered every year in the spring in a ceremony that has become an important social event.\nThis picture was scanned from a Kodachrome slide.\nCritiques | Translate\nSteveH (6243) 2008-02-07 2:29\nHi Gert, What an incredible building! Were you able to go inside, if so, what was the interior like? It is a shame that the sky is white, but that is difficult to do anything about. The inclusion of the goats and people gives us a glimpse of the everyday life too. cheers, Steve\nbobocortis (15988) 2008-02-07 5:47\nAmazing sand-paint picture.\nThe sheeps on close up complete the outcome.\nbaclama (18279) 2008-02-07 7:05\ntres beau point de vue de cette belle mosquée..peut-etre ça manque un peu de contratse?\nTopGeo (38206) 2008-02-07 7:16\nHi Gert! excellent captured! of mosque! A picture that with the almost monochrome creates impression. As besides and the form and the architecture of mosque! The composition I liked with travelled in a point that I do not know! I thank and praise you.\nsaxo042 (37596) 2008-02-07 7:42\nI get a strong feeling that this picture was in reality taken around the 19th century. I can just imagine you going around with a topee and a lot of helpers to carry your luggage and your big, oldfashioned camera.\nIndeed a very fine picture!\nHälsningar från Göteborg\njmdias (57601) 2008-02-07 16:38\nwonderful pov and architecture. I think light isn't the best and colors aren't so nice. Maybe an option for B&W could be more interesting here. Anyway I liked this photo\nryno (6863) 2008-02-07 20:52\nHi Gert, this is a great shot of this superb mosque with it's very distinct and appealing architectural style. A good POV with the goat in the foreground and splashes of blue in the people's clothing. The sky is very grey and doesn't add much to the photo, so I would crop some of that if I were you, but otherwise it's an excellent image.\nmolla (7129) 2008-02-08 0:38\nAnd here you have the wonder of Africa , A true temple of all african stories you can imagine. The sandcastle of all sandcastle. it is a true impressive building and you framed it well, and in this picture it doesn't matter that we have a flat sky it just strengthens the architecture of the mosque\nAsiulus (1176) 2008-02-18 1:48\nThis one is good, too. Full view, people helping us to estimate the size of the mosque and animals in front of it all, making it more dimensional... I like the running/jumping(?) figure under the tree as well. Did you crop the picture?\nskoogmi (39) 2008-02-27 7:36\nUtan tvekan ett av dina bästa bilder, från en helt annan tidsålder. Inga som helst störande moderna element eller har du tagit bort alla bilarna här famför med Photoshop?\ncrhieatt (5347) 2008-04-02 9:44\nI guess this is a famous landmark these days - perhaps it always was.\nThe muted tones of this shot work pretty well here because the star is obviously the architecture itself.\nadores (39272) 2008-04-06 19:21\nThis is a really beautiful mosque. Good to see it on this old photo, surrounded by people. Great daily scene. I like the goats in foreground.Good composition!\nJCG (35129) 2008-06-07 8:12\nA really interesting scan on this mosque. These countries do not have the modern means of construction which have the developed countries, however I am astonished by this marvellous architecture. The day when this mosque is replastered must be memorable. Thank you for sharing Gert.\nWith all my friendships and a very good WE,\nfanni (14381) 2008-07-11 11:23\nreally incredible building which looks more fantastic than real! to me it seems almost like something from quite a different (maybe even extra-terrestrial) world... looks like a scene from a fantastic movie!\nAnyway, I like the composition and the very warm colours, and the soft (and yet sharp enough) lines of the mosque. It looks very three-dimensional!\nmafegan (8624) 2008-07-14 4:56\nWhat an amazing building from Mali. Seems almost surreal and from 1982. I do appreciate you sharing this image, Marlene\nnoborders (1010) 2008-08-29 5:02\nHi Gert, I'm still attracted a lot by your old photos, when I saw this mosque in 2001 it was surrounded by buildings and it was also the day of the market, plenty of little tents were partly blocking the view, a pity in a way as the mosque looks so nice in that empty place - superb scan and great photo (and splendid building !), bravo !\nfritzi007 (7638) 2010-08-18 6:45\nWonderful foto and object, and an interesting comment, very good work!\nunsalan (149) 2011-01-04 13:41\nIt is a nice photograph. Thin and lonely goat gives an impression of difficult life in this area. Thank you for sharing,\nleo61 (0) 2011-02-10 11:07\nI found this in your random gallery and I like this film oldy a lot.\nit`s a unique mosque architecture,very well captured by you.Good composition with the few people around and these goats in the foreground.\nadidas5nb (5671) 2013-05-26 18:55\nthis is an absolute stunner of a mosque/structure.love your pov and the composition with the goats in the fg.after michael palin,it's you,who has given me a detailed insight into this part of the wold.THANK YOU.\nkato (11630) 2013-10-04 17:05\nYou're capturing well place by a good composition and excellent tone/clarity, so it's interesting and impressive to see the mosque built with mud and brick, with unique shape. It's a very mysterious for seeing this environment included the ground and wall to me, and we can know that this is the dry place.\nMiguel82 (25547) 2014-08-04 21:08\nGert, this is a value report from Mali, many tensions nowadays in that subsaharian region. That sand color mosque is situated in a desertic environment, the workshop is welcome, thanks, greetings\n- Copyright: Gert Holmertz (holmertz) (43293)\n- Genre: Places\n- Medium: Color\n- Date Taken: 1982-02-00\n- Categories: Daily Life, Architecture, Artwork\n- Photo Version: Original Version\n- Theme(s): Mosques Around The World (III) (Africa), other favorites (2008) [view contributor(s)]\n- Date Submitted: 2008-02-07 1:32\n- Favorites: 1 [view]","For many, seeking serenity has different meanings. It takes on a different path for a believer who is immersed in his relations to God. In Putrajaya, your faith in God gets stronger when you witness and feel the path you have been looking for exude both elements, that is, beauty and serenity. It is always an issue of serenity meeting beauty in many ways than one. For sure, you will be awed by the beauty and the serenity that these two great mosques in Putrajaya exude.\nPutra Mosque or Masjid Putra is truly a great mosque, as it straddles across 1.37 hectares of land. Touted as one of the most modern mosques in the world, Putra Mosque is located within the 600-hectare of Putrajaya Lake. Overlooking the die-for scenic view of the lake, one cannot deny that the mosque is Putrajaya’s most distinctive landmark. It is located next to Perdana Putra which houses the Malaysian Prime Minister’s office and Dataran Putra (Putra Square), a large public square with flagpoles flying Malaysian states’ flags. All these three areas are linked to wide and large boulevards as far the eyes can see. It is like being right there at the boulevard of Champs dé Élysee but the view is even more fascinating without the array of high fashion boutiques like the one in Paris. From the onset, Putra Mosque, built in June 1997, looks like a reddish-pinkish rose clay building, as it was built with rose tinted granite which gives its desert-pink hue that offsets the cengal woodwork on the doors, windows and panels. It has an unusual and unique mixture of Malaysian, Persian and Arab-Islamic architectural designs with combination of traditional intricate design motifs; employing local and foreign craftsmanship.\nThe main entrance is designed in a similarity of a public building gate in early Persia. The mosque has a huge reddish-pink 166 metre-high minaret with symmetrical designs on it, with tinges of influences of Sheikh Omar mosque in Baghdad. One should go down to Putra Mosque’s basement wall to see the design which is similar to the King Hassan’s mosque in Casablanca, Morocco.\nThe Prayer Hall is just simply exquisite and intricately designs on all sides of the walls, with 12 huge columns supporting the 36-meter diameter main dome above. The mimbar (pulpit) and mehrab (niche that denotes the direction of Mecca) are adorned with khat or Islamic calligraphy. A unique feature has been added to the sound system design - front throw speakers are used to create the effect of all sounds originating from the direction of the imam. The mosque can comfortably accommodate a maximum of 10,000 worshippers while its compartmentalised complex is spacious enough that it can be utilised to hold seminars, conferences and symposiums for another 1,000 people. A paved courtyard in front of the Putra Mosque’s prayer hall is landscaped with water sprouts and fountains. It is so spacious enough that it can hold an additional of 5,000 people. But beware! Do not attempt to walk barefooted on the courtyard under the scorching heat of the sun, as your feet will be grilled like a skewer of fresh meat.\nTuanku Mizan Zainal Abidin Mosque\nThe “Iron Mosque” is popularly known by the locals for its physical outlook of stainless steel iron from afar. Officially, this mosque is known as Tuanku Mizan Zainal Abidin Mosque or Masjid Tuanku Mizan Zainal Abidin named after the former Yang DiPertuan Agong (King of Malaysia). Well, it is the second principal mosque in Putrajaya after the Putra Mosque. It is located in Putrajaya’s Presint 3, next to Millennium Monument. Construction began since April 2004 and was fully completed on August 2009. It was officially opened on Friday at the eve of the month of Ramadhan 1430 Hijra.\nThe mosque was built to cater to approximately 30,000 residents including the government servants working around the city center as well as areas within Precinct 2, 3, 4 and 18. Tuanku Mizan Zainal Abidin Mosque’s area is twice that of Putra Mosque, which is located 2.2 kilometers north. The mosque was officially opened on 4 September 2009 by the 13th Yang di-Pertuan Agong, Tuanku Mizan Zainal Abidin.\nThe “Iron Mosque” features a district cooling system, and without assembly of fans or an air conditioning system. The mosque employs architectural wire mesh imported from Germany and China, which is also constructed at the Santiago Bernabéu Stadium in Madrid, as well as the Bibliothèque Nationale de France in Paris. The main entrance is reinforced with glass reinforced concrete to increase the integrity of the structure and uses fine glass to create an illusion of a white mosque from afar.\nThe path towards the mosque crosses a skyway known as the Kiblat Walk which stretches an area of 13,639 m². This skyway contains landscaping adapted from the ancient castles of Alhambra. The interior will be decorated with Al-Asmaul-Husna (names of Allah) calligraphy of the Thuluth variation. The entrance to the main prayer hall will be adorned with verse 80 of Sura Al-Isra from the Qur’an.\nThis is also added with the Mihrab wall made of 13 meter-high glass panel imported from Germany inscribed with 2 verses from Sura Al-Baqarah on the right-hand side and Sura Ibrahim on the left. The mihrab wall is designed so that no light will be reflected, creating an illusion that the verses are floating on air. The 40-feet long edges of the mosque’s roof are able to shelter the people praying outside of the main prayer hall from rain."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:f29e8dea-3168-4d54-b3ce-f6518e3ca950>","<urn:uuid:c38aa8b9-efe1-49af-b2b8-5a7b84878833>"],"error":null}
{"question":"What role does body mass index (BMI) play in diagnosing obesity compared to its relevance in anorexia nervosa?","answer":"BMI is explicitly used as a diagnostic tool for obesity, where a BMI over 30 is considered obese, and between 25-29 is considered overweight. The measurement can be calculated using either the English formula (weight in pounds x 703 divided by height in inches squared) or the metric formula (weight in kilograms divided by height in meters squared). In contrast, while anorexia nervosa involves severe weight restrictions and an irrational fear of weight gain, the documents do not specifically mention BMI as a diagnostic criterion for anorexia. Instead, anorexia is characterized by the individual's distorted body image, where they believe they are overweight even when extremely thin.","context":["What is obesity?\nObesity is defined as a generalized accumulation of body fat. An adolescent is considered obese if he (or she) is significantly over the ideal weight for his or her height.\nOverweight refers to increased body weight in relation to height, but this may be due to excess body fat and/or lean muscle. (In professional athletes, for example, overweight does not necessarily mean too fat).\nResearch studies suggest that overweight adolescents may become overweight adults.\nWhat causes obesity?\nDuring the 1990s, one physiologist proposed a set point theory which has continued to gain support. This theory suggests that weight is determined by complex interactions of neural, hormonal, and metabolic factors. Genetic and familial influences contribute to metabolic rates, and physical activity levels are important to energy expenditure.\nExperts now identify and address two types of obesity:\nWhat are the physical side effects of obesity?\n- Endogenous: obesity with specific organic cause or origin\n- Exogenous: obesity caused by simple excessive caloric intake, genetic/familial, psychogenic, or mixed factors\nPhysical factors contributing to excess body fat in adolescents include the following:\nHow do adolescents become obese?\n- Increased insulin levels\n- Elevated lipid and lipoprotein levels\n- Elevated blood pressure\nOver long periods of time, many behaviors can lead to obesity, including:\nWhat are the symptoms of obesity?\n- Excessive intake of high-calorie foods\n- Inadequate exercise in relation to age\n- More sedentary lifestyle\n- Low metabolic rate\n- Increased insulin sensitivity\nThe following are the most common symptoms that indicate an adolescent is obese. However, each adolescent may experience symptoms differently. Symptoms may include:\nAdolescents who are obese often experience significant social pressure, stress, and difficulties accomplishing developmental tasks. Psychological disturbances are also very common. The symptoms of obesity may resemble other conditions or medical problems, and a physician should always confirm the diagnosis.\n- Facial features often appear disproportionate\n- In males, fat cells in the breast region\n- Large abdomen (sometimes with white or purple marks)\n- In males, external genitals may appear disproportionately small\n- Early onset of puberty\n- Increased number of fat cells in the upper arms and thighs\n- Symptoms of knock knees\nHow is obesity diagnosed?\nObesity is always diagnosed by a physician, and body mass index (BMI) is usually used to help confirm the diagnosis. Two categories of obesity are defined:\n- BMI at the 95th percentile or more for age and sex, or BMI of more than 30 (whichever is smaller). BMI findings in this category indicate the need for a complete medical work-up.\n- BMI between the 85th and 95th percentile, or BMI equal to 30 (whichever is smaller). Findings in this category suggest the need for a second level screening which includes evaluation of the following five health risks:\nWhat is the treatment for obesity?\n- Family history of cardiovascular disease\n- Total cholesterol levels of parents\n- Parental diabetes or obesity\n- Elevated blood pressure\n- Total cholesterol level\n- Large increases in BMI assessments from year to year\n- Concerns about weight, including personal (emotional or psychological) concerns related to weight and perception of self as overweight\nThe specific treatment for obesity will be determined by the adolescent's physician based on the following factors:\nWhat are the treatments for obesity?\n- The adolescent's age, overall health, and medical history\n- Extent of the condition\n- The adolescent's tolerance for specific medications, procedures, or therapies\n- The child or parent’s opinion or preference\nThe treatment for obesity usually involves a nutritionist, qualified mental health professionals, and an exercise specialist. Specific actions may include the following:\nTreatment goals should be realistic, focus on modest reduction of food intake and changes in eating habits, and incorporate a healthy exercise-oriented lifestyle.\n- Nutritional and individual diet counseling\n- Modification of diet and caloric content\n- Increased exercise or participation in an appropriate exercise program\n- Behavior modification\n- Individual or group therapy focused on changing behaviors and confronting feelings related to weight and normal developmental issues\n- Support and encouragement for making changes and following recommended treatment recommendations\nHow can obesity be prevented?\nSpecific programs or procedures to prevent obesity have not been identified at this time. Encouraging healthy eating habits and realistic attitudes toward weight and diet are important. Early detection and intervention into unhealthy eating and lifestyle habits can impact weight-related issues, enhance the adolescent's normal growth and development, and improve the quality of life experienced by adolescents with a physical predisposition or behavioral tendencies toward obesity.\nDetermining Body Mass Index (BMI)\nWhat is body mass index (bmi)?\nDetermining how much an adolescent should weigh is not a simple matter of looking at an insurance height-weight chart, but includes considering the amount of bone, muscle, and fat in his/her body's composition. The amount of fat is the critical measurement.\nA good indicator of how much fat an adolescent carries is the body mass index (BMI). Although it is not a perfect measure, it gives a fairly accurate assessment of how much of an adolescent's body is composed of fat.\nTo calculate BMI using the English formula:\nExample: a person who weighs 165 pounds and is 5 feet 4 inches tall has a BMI of 28.\n- BMI can be calculated using pounds and inches.\n- BMI = Weight in Pounds x 703 divided by\n(Height in Inches) x (Height in Inches)\nTo calculate BMI using the Metric formula:\n- 165 lbs x 703 = 28 divided by\n(64 inches) x (64 inches)\nExample, a person who weighs 99.79 kilograms and is 1.905 meters tall has a BMI of 27.5.\n- BMI can be calculated using kilograms and meters.\n- BMI = Weight in Kilograms divided by\n(Height in Meters) x (Height in Meters)\nBMI between 25 and 29 is considered overweight. Any measurement over 30 is considered obese.\n99.79 Kg = 27.5 divided by\n(1.905 Meters) x (1.905 Meters)\nBack to Top\nObesity - Departments & Programs - Children's National Medical Center","What is anorexia nervosa?\nAnorexia nervosa is an eating disorder characterized by an irrational fear of weight gain. People with anorexia nervosa believe that they are overweight even when they are extremely thin.\nIndividuals with anorexia become obsessed with food and severely restrict their dietary intake. The disease is associated with several health problems and, in rare cases, even death. The disorder may begin as early as the onset of puberty. The first menstrual period is typically delayed in girls who have anorexia when they reach puberty. For girls who have already reached puberty when they develop anorexia, menstrual periods are often infrequent or absent.\n- What is osteoporosis?\n- The link between anorexia nervosa and osteoporosis\n- Osteoporosis management strategies\n- For your information\nWhat is osteoporosis?\nOsteoporosis is a condition in which the bones become less dense and more likely to fracture. Fractures from osteoporosis can result in significant pain and disability. In the United States, more than 53 million people either already have osteoporosis or are at high risk due to low bone mass.\nRisk factors for developing osteoporosis include:\n- Thinness or small frame.\n- Family history of the disease.\n- Being postmenopausal and particularly having had early menopause.\n- Abnormal absence of menstrual periods (amenorrhea).\n- Prolonged use of certain medications, such as those used to treat lupus, asthma, thyroid deficiencies, and seizures.\n- Low calcium intake.\n- Lack of physical activity.\n- Excessive alcohol intake.\nOsteoporosis often can be prevented. It is known as a silent disease because, if undetected, bone loss can progress for many years without symptoms until a fracture occurs. Osteoporosis has been called a childhood disease with old age consequences because building healthy bones in youth helps prevent osteoporosis and fractures later in life. However, it is never too late to adopt new habits for healthy bones.\nThe link between anorexia nervosa and osteoporosis\nAnorexia nervosa has significant physical consequences. Affected individuals can experience nutritional and hormonal problems that negatively impact bone density. Extremely low body weight in females can cause the body to stop producing estrogen, resulting in a condition known as amenorrhea, or absent menstrual periods. Low estrogen levels contribute to significant losses in bone density.\nIn addition, individuals with anorexia often produce excessive amounts of the adrenal hormone cortisol, which is known to trigger bone loss. Other problems, such as a decrease in the production of growth hormone and other growth factors, low body weight (apart from the estrogen loss it causes), calcium deficiency, and malnutrition, may contribute to bone loss in girls and women with anorexia. Weight loss, restricted dietary intake, and testosterone deficiency may be responsible for the low bone density found in males with the disorder.\nStudies suggest that low bone mass is common in people with anorexia and that it occurs early in the course of the disease. Girls with anorexia may be less likely to reach their peak bone density and therefore may be at increased risk for osteoporosis and fracture throughout life.\nOsteoporosis management strategies\nAnorexia is often identified during mid to late adolescence, a critical period for bone development. The longer the duration of the disorder, the greater the bone loss and the less likely it is that bone mineral density will ever return to normal. The primary goal of medical therapy for individuals with anorexia is weight gain and, in females, the return of normal menstrual periods. However, attention to other aspects of bone health is also important.\nNutrition: A well-balanced diet rich in calcium and vitamin D is important for healthy bones. Good sources of calcium include low-fat dairy products; dark green, leafy vegetables; and calcium-fortified foods and beverages. Supplements can help ensure that people get adequate amounts of calcium each day, especially in people with a proven milk allergy. The Institute of Medicine recommends a daily calcium intake of 1,000 mg (milligrams) for men and women up to age 50. Women over age 50 and men over age 70 should increase their intake to 1,200 mg daily.\nVitamin D plays an important role in calcium absorption and bone health. Food sources of vitamin D include egg yolks, saltwater fish, and liver. Many people may need vitamin D supplements to achieve the recommended intake of 600 to 800 International Units (IU) each day.\nExercise: Like muscle, bone is living tissue that responds to exercise by becoming stronger. The best activities for your bones are weight-bearing and resistance exercises. Weight-bearing exercises force you to work against gravity. They include walking, climbing stairs, and dancing. Resistance exercises – such as lifting weights – can also strengthen bones.\nAlthough walking and other types of regular exercise can help prevent bone loss and provide many other health benefits, these potential benefits need to be weighed against the risk of fractures, delayed weight gain, and exercise-induced amenorrhea in people with anorexia and those recovering from the disorder. For these reasons, individuals recovering from anorexia should speak with a doctor before beginning an exercise program.\nHealthy lifestyle: Smoking is bad for bones as well as the heart and lungs. In addition, people who smoke may absorb less calcium from their diets. Alcohol also can have a negative effect on bone health. Those who drink heavily are more prone to bone loss and fracture, because of both poor nutrition and increased risk of falling.\nBone density test: A bone mineral density (BMD) test measures bone density in various parts of the body. This safe and painless test can detect osteoporosis before a fracture occurs and can predict a person’s chances of fracturing in the future. The BMD test can help determine whether medication should be considered.\nMedication: There is no cure for osteoporosis. However, medications are available to prevent and treat the disease in postmenopausal women, men, and both women and men taking glucocorticoid medication.\nNIH Osteoporosis and Related Bone Diseases ~ National Resource Center\nFor more information on anorexia, visit:\nNational Institute of Mental Health\nFor your information\nThis publication contains information about medications used to treat the health condition discussed here. When this publication was developed, we included the most up-to-date (accurate) information available. Occasionally, new information on medication is released.\nFor updates and for any questions about any medications you are taking, please contact\nU.S. Food and Drug Administration\nToll Free: 888-INFO-FDA (888-463-6332)\nFor additional information on specific medications, visit Drugs@FDA at https://www.accessdata.fda.gov/scripts/cder/daf. Drugs@FDA is a searchable catalog of FDA-approved drug products.\nNIH Pub. No. 18-7895"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:7eaa8e25-21a1-43d9-8511-33a12692abe5>","<urn:uuid:edddee14-d042-4ed7-b4eb-79b241bf8d09>"],"error":null}
{"question":"What's the most severe type of burn injury? burns serious","answer":"Third-degree burns are the most severe type of burn injury. They damage all layers of skin and can affect muscle, fat, and even bone. These burns appear white and dry, black, or charred. They don't heal on their own and often require extensive treatment, including skin grafts. Interestingly, victims often don't feel pain because the burn destroys the pain sensors. Third-degree burns can also damage internal organs and typically require prolonged healing times.","context":["Burns are typically a type of tissue injury. Although they are often minor, they can also be very severe depending upon the severity, or degree, of the burn.\nMost Burns Are Minor In Nature\nEvery year in the United States, on average, 450,000 individuals sustain burn injuries and receive medical treatment for these injuries. Roughly 3,400 of those will die from their burn injuries.. Many of those individuals are hospitalized at a medical facility that is equipped with specialized burn centers that are solely dedicated to treating severe burns and assisting with the recovery process.\nSince the severity of burns can vary significantly, there are three classifications for burn injuries – first degree, second degree, and third degree burns. A first-degree burn is a burn with the least damage, while a third-degree burn affects deeper tissues and in some cases can be fatal. Typically a second-degree burn is not fatal. However, if large parts of the body are covered by the burns they can be very dangerous.\nWe’re available Sunday-Thursday, 11AM-10PM and Friday-Saturday 11AM-11PM\nFirst Degree Burns\nThe least serious kind of burn is the first-degree burn. The skin’s outer layer is burned but it isn’t deep enough so that the second layer is is affected. Often this degree of burn will cause the skin to turn red, and there will be some swelling in the area of the injury along with pain or discomfort at the site of the burn. Sunburns commonly cause first degree burns. The burns:\n- Involve just the epidermis, which is the skin’s outermost layer\n- Are painful\n- Do not form any blisters\n- Heal on their own and do not scar\nThis type of burn is the least serious and usually does not require the person to be hospitalized. Usually only home treatments are needed. The main goal when treating this type of burn is to make the victim feel physically bettter. The injuries heal on their own in a couple of days up to a week. Using a skin cream can help to expedite the healing process. More damage might be caused by some of these burns and might require specialized treatment and hospitalization.\nSecond Degree Burns\nThis type of burn results when the skin’s first layer is burned through into the second layer so that it too is damaged or burned. Blisters develop around and at the burn site. The skin can have an intensely splotchy and reddened appearance. A second-degree burn may produce intense pain and severe swelling.\nSecond degree burns are classified into two types, including deep partial thickness burns and partial thicknesss burns. A second-degree burn:\n- Involves the dermis and epidermis\n- Forms blisters\n- Has a moist appearance\n- Are painful, given that the pain sensors are intact still\nThese kinds of burns are the hardest to diagnose given that they might be just slightly worse compared to a first-degree burn, or it might deeply affect the skin similar to a third-degree burn.\nThird Degree Burns\nThis is the most extreme type of burn that an individual can suffer. All layers of skin are involved. Extensive destruction of the underlying structures as well as the skin are caused by a third-degree burn which results in tissue being permanently damaged. Muscle, fat, and even bone might be affected. Parts of the body might appear white and dry, be black, or charred. Carbon monoxide poisoning, difficulty exhaling and inhaling, and other types of toxic effects might occur if the burn is accompanied by smoke inhalation.\nThese burns frequently need significant treatment for both cosmetic improvements when possible and for healing wounds. A third-degree burn:\n- Involves all layers of skin\n- Vary in color, and can be brown, red, waxy, or white\n- Lack elasticity and are dry\n- Do not heal\n- The victim often does not feel pain because the burn has destroyed the pain sensors\nThis type of burn might also damage internal organs as well as other structures of the body. Following a fire or explosion, victims frequently experience severe third degree burns. They require extensive treatment and prolonged healing times. Frequently skin grafts are utilized where skin is removed by the doctor from one area of the victim’s body and then placed on top of the area that has been injured to help with the healing process. Additional cosmetic procedures might be necessary during or after the healing process in an attempt to reduce the severity or appearance of scars.\nIf you have experienced a burn injury and you can safely come to ER Specialists, we would love to help you. If you are alone or have any doubt about getting to our facility safely, or, believe you are experiencing a medical emergency please call 911."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:f0aafcd3-53aa-4911-9d54-43e8ffa979e7>"],"error":null}
{"question":"What is the specific problem with rack circuit-failover in data centers and how can load shedding address it?","answer":"The problem occurs when load balancing between circuits is needed due to loss of one failover or circuit. When loads are combined between two or more circuits, their total capacity may exceed the capacity of a single circuit, causing failover failure and potential loss of entire racks. Load shedding provides a solution by simply turning off outlets on a PDU when capacity limits are approached, thus preventing the system from exceeding the capacity of a single rack or set of racks.","context":["Capping of Power Consumption in Data Centers through Load Shedding\nLoad shedding is a means of distributing electrical power demand across multiple power sources. When the demand for power is greater than what the primary power source can supply, load shedding helps to relieve stress on the primary power source. Every wish of an organization is to reduce power consumption while ensuring uninterrupted normal operations.\nMost buildings including data centers Toronto, purchase power from a utility provider. Needless to say, data centers are power gluttons. This is because of the various functions that they perform from the explosion of digital content, big data, e-commerce and internet traffic which all contribute to making them the fastest growing power consumers in the world. At this stage, there may need to embark on load shedding as a data center solution of power consumption reduction. Here, the data center operator may negotiate an agreement with the power provider to voluntarily load shed on a pre-schedule or on-demand basis. Since all buildings have a secondary power source, when there is a load shedding the building sources power from the secondary such as on-site diesel generators, wind-based renewable power or even contracted solar photovoltaic.\nWith such a load shedding agreement, energy-intensive data centers may opt to load shed during peak usage periods. However, this may cause disruptions to the data center systems. To avoid this, data centers can install uninterruptible power supply systems and power distribution units that will always moderate the flow of power to data center sensitive equipment. Also, make sure that you have a high-quality power distribution control from secondary sources.\nAs now perceived, load shedding can be the solution to many data center problems that are power related. Rack circuit-failover failure is one of the most common problems that load shedding could provide a solution to. There is no need to put a hard power capacity limit on a rack or set of racks to ensure failover success.\nPDUs are made in such a way that they have limits to the load capacity of a single device. Others may prefer to call them ‘floor PDU circuit breakers’. They work just like the normal circuit breakers. But when it comes to the capacity limit of a rack, it may be way much and exceed the limit of a single rack circuit. What does this mean?\nEach circuit is always under a specific load capacity of its own. In some cases, you may be forced to load balance between two or more circuits by adding the loads together. This may happen because of loss of one failover or circuit. When load balancing happens, the capacity of the two circuits may exceed the capacity of one of those circuits. When this happens, the failover will not succeed and in most cases, you could lose the whole rack or set of racks that were being connected to load balance.\nOne way that is very effective in ensuring that you do not exceed the capacity of a single rack or any set of racks is load shedding. When such cases arise, the outlets on a PDU will just be turned off thus disabling their usage."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:30ca391a-7170-4c06-ba1b-a50dbca27c52>"],"error":null}
{"question":"Who are the editors of the book 'Transforming Ethnographic Knowledge'?","answer":"The book 'Transforming Ethnographic Knowledge' is edited by Rebecca Hardin and Kamari Maxine Clarke. Rebecca Hardin is associate professor at the University of Michigan, and Kamari Maxine Clarke is professor of anthropology at Yale University.","context":["The University of Wisconsin Press\nAnthropology / Social Sciences\nTransforming Ethnographic Knowledge\nEdited by Rebecca Hardin and Kamari Maxine Clarke\nAn engaging, reflective, and deeply personal book that prompts a rethinking of both the limits and possibilities of ethnography\nThe ethnographic methods that anthropologists first developed to study other cultures—fieldwork, participant observation, dialogue—are now being adapted for a broad array of applications, such as business, conflict resolution and demobilization, wildlife conservation, education, and biomedicine. In Transforming Ethnographic Knowledge, anthropologists trace the changes they have seen in ethnography as a method and as an intellectual approach, and they offer examples of ethnography’s role in social change and its capacity to transform its practitioners.\nSenior scholars Mary Catherine Bateson, Sidney Mintz, and J. Lorand Matory look back at how thinking ethnographically shaped both their work and their lives, and George Marcus suggests that the methods for teaching and training anthropologists need rethinking and updating. The second part of the volume features anthropologists working in sectors where ethnography is finding or claiming new relevance: Kamari Maxine Clarke looks at ethnographers’ involvement (or non-involvement) in military conflict, Csilla Kalocsai employs ethnographic tools to understand the dynamics of corporate management, Rebecca Hardin and Melissa Remis take their own anthropological training into rainforests where wildlife conservation and research meet changing subsistence practices and gendered politics of social difference, and Marcia Inhorn shows how the interests in mobility and diasporic connection that characterize a new generation of ethnographic work also apply to medical technologies, as those mediate fertility and relate to social status in the Middle East.\nRebecca Hardin is associate professor in the School of Natural Resources and Environment and in the Department of Anthropology at the University of Michigan, Ann Arbor. She is coeditor of Corporate Lives: New Perspectives on the Social Life of the Corporate Form, a special edition of the journal Current Anthropology.Of Related Interest:\nKamari Maxine Clarke is professor of anthropology at Yale University and author of Fictions of Justice: The International Criminal Court and the Challenge of Legal Pluralism in Sub-Saharan Africa\nMedia & bookseller inquiries regarding review copies, events, and interviews can be directed to the publicity department at email@example.com or (608) 263-0734. (If you want to examine a book for possible course use, please see our Course Books page. If you want to examine a book for possible rights licensing, please see Rights & Permissions.)\nUrban Conflagration and the Making of the Modern World\nEdited by Greg Bankoff, Uwe Lübken, and Jordan Sand ....\nLC: 2011052830 GN\n248 pp. 6 x 9 1 table\nPaper $29.95 s\neBook $19.95 s\nAdobe Digital Edition\nAbout our e-books\nPrinting and cut/paste allowed, access on six different devices.\nMary Catherine Bateson, Kamari Maxine Clarke, Rebecca Hardin, Csilla Kalocsai, Macia Inhorn, George Marcus, J. Lorand Matory, Sidney Mintz, Melissa Remis.\n\"This multifaceted, energizing collection reminds us of the many reasons that ethnography’s sustained engagement with others is so vital--not just to anthropology but also for analysis and action in the contemporary world.\"\n—Kirin Narayan, author of Alive in the Writing: Crafting Ethnography in the Company of Chekhov\nHome | Books | Journals | Events | Textbooks | Authors | Related | Search | Order | Contact\nIf you have trouble accessing any page in this web site, contact our Web manager.\nUpdated February 22, 2012© 2012, The Board of Regents of the University of Wisconsin System"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:16eed71a-8db8-4472-868c-c6ccddb7d9cb>"],"error":null}
{"question":"How did steam power evolve from its initial use in mines during the First Industrial Revolution to its role in Quality 4.0 and modern manufacturing?","answer":"Steam power evolved dramatically from its humble beginnings to modern manufacturing. Initially, in 1712, Thomas Newcomen created the first usable steam engine as an atmospheric steam pump for mines, though it was limited to just 12 strokes per minute. James Watt later revolutionized this technology in 1778 by creating a more powerful and fuel-efficient double-acting pump with a separate condenser. This led to applications in railways and steamboats. Today, in the Fourth Industrial Revolution (Quality 4.0), steam power's legacy continues as part of the broader digital transformation, where manufacturing processes have evolved to incorporate IoT, artificial intelligence, machine learning, and robotics. This transformation has particularly impacted medical device manufacturing, where these technologies are used to produce safer devices and maintain quality standards.","context":["Quality 4.0 derived from Industry 4.0, also known as the fourth industrial revolution. Prior to the fourth revolution, the first revolution started with machine manufacturing, steam power, and the move to cities by agriculturalists. In the second industrial revolution, production was automated and mass manufacturing cut the cost of consumer and industrial products. The third revolution involved the use of electronics and control systems in manufacturing, which helped drive down costs and resulted in increased product complexity and lower product costs. Today’s fourth industrial revolutionary change is driving new quality paradigms, processes, and technologies (https://blog.lnsresearch.com/quality40).\nIn the medical device manufacturing market, in particular, there are numerous challenges to design and produce safer devices. Regulatory requirements are just one component—but incorporating emerging and continually evolving technologies requires device companies to raise their game if they want to stay ahead of the competition and leverage cutting edge technologies—from the Internet of Things (IoT) to artificial intelligence and machine learning to augmented reality and even robotics. Quality 4.0 is the convergence of these new technologies that make up the manufacturing landscape.\nTechnology to Reshape the Market\nIndustry 4.0 represents the dawn of the digital transformation having begun with the third revolution, connecting the physical and natural worlds. The impact of digital data, analytics, connectivity, scalability, and collaboration are the drivers empowering this fourth industrial revolution and informing Quality 4.0 strategies. As we find new ways to connect people, devices, and data, the democratization of technologies is introducing transformative capabilities in analytics, material science, and connectivity. For the medical device industry, such technologies empower a quality transformation of culture, leadership, collaboration, and standards (https://blog.lnsresearch.com/quality40).\nQuality 4.0 is reshaping device designs, functionality, manufacturing processes, supply chain strategy, customer service, and the methods of maintaining quality systems that are compliant with regulatory bodies like the FDA and ISO. Intelligent and connected technologies are rapidly becoming more widely used as manufacturers seek an advantage to introduce inventive products to market and leapfrog existing competitors.\nThe Move to Digital\nMarketsandMarkets forecasts the medical device market will grow to $63.43 billion by 2023. Smarter, more automated, and connected devices are improving the state of healthcare, making it possible to perform remote surgeries with doctors on the other side of the globe. Medical device companies’ ability to leverage Quality 4.0 technologies will be key to market success in the years ahead. LNS Research sees Quality 4.0—the application of Industry 4.0 technologies to quality initiatives—as following in IoT’s path. In fact, industry experts have determined that a quarter of medical device manufacturers’ digital transformation technologies drive quality improvements. These same technologies are being used to design and deliver products. What does this mean for medical device manufacturers?\nFor starters, digital transformation trends have helped define Quality 4.0 strategies to eliminate reliance on disparate paper-based quality management systems and processes. The move away from manual systems reduces errors, silos, collaboration barriers, and traceability issues. Furthermore, the digitization and automation of design and production processes enables small and global companies to scale their design and supply chain processes quickly. For instance, one such company, RefleXion Medical, a leader in biology-guided radiotherapy systems for cancer treatment, knew they needed to implement a completely connected QMS that could scale to support their path to digital transformation and improved compliance. They required a flexible platform that could grow with their team, products, and path throughout the quality compliance process.\nNewer Quality Standards along the Supply Chain\nIn order to speed products to market, today’s medical device manufacturers rely on distributed teams and supply chains, including contract manufacturers, design partners, and tiered component suppliers. Companies that have embraced new technologies and cloud-based systems understand the unique benefits that digital transformation technologies can bring to a device manufacturer’s product requirements, product capabilities, and regulatory compliance objectives.\nIn the life sciences realm, digital therapeutics, medical diagnostic equipment, implantable devices and disposable devices are just a few of the wide array of devices that strive to be problem-free while delivering higher throughput and maintaining compliance.\nIntegration with Connected Teams\nUsing a connected, or more product-centric quality management system (QMS) can help to meet the demands created by Quality 4.0 trends. As complexity of products increases with AI, IoT, robotics, and related 4.0 technologies, quality teams must have a unified system to identify issues, address audits, and resolve quality incidents. This ensures that the full, complex product design comprised of electrical, mechanical, and software components is contained within a single system.\nThis foundation allows for complete, connected quality and corrective action records and provides increased visibility and transparency as teams collaborate through each phase of new product development and introduction.\nMedical device companies will gain competitive advantages by having more intelligence-driven product and quality process insights. This, in turn, will lead to better data-driven decisions and cross-functional visibility with quality, engineering, operations, and supply chain teams. These Quality 4.0 transformational technologies add complexity in meeting strict medical device requirements:\n- ISO quality system requirements\n- Complaint management and corrective action preventive action (CAPA)\n- FDA 21 CFR Part 820\n- FDA 21 CFR Part 11\n- ISO 13485\n- To ensure compliance, a product-centric QMS solution makes it possible to:\n- Create a completely connected quality and product process\n- Establish quality product processes to avoid audit issues\n- Ensure traceable design and change controls\n- Manage product bills of materials linked directly to quality records\n- Drive closed-loop quality and CAPA processes to faster resolution\n- Improve quality compliance and supplier management processes\nThe last point about supplier management process is essential because the right QMS solution can unify quality and product record management to provide complete control and traceability, simplify audits, and decrease risks.\nUse Cases from Device Makers\nSwan Valley Medical is a manufacturer of surgical instruments and accessories for applications in the field of urology. They were burdened with inefficient paper-based manual processes that resulted in potential compliance exposure due to misplacing critical documentation. During audits, when missing or inaccurate information was found, it was difficult to recover and could take hundreds of hours or even several months.\nWith a cloud-based product-centric QMS solution, accelerated root-cause analysis and risk management observation processes were implemented. This enabled different processes to connect with others through linked product and quality records, thus supporting audits with cross-linked evidence chains.\nClosed-loop CAPA processes help quality teams identify, analyze, and resolve quality issues faster. In one example, Pulse Biosciences deployed a closed-loop quality system across its organization. All team members were able to work collaboratively on the most current product definition and latest quality records with a product-centric QMS. With streamlined CAPA processes, Pulse Biosciences was able to address urgent corrective actions rapidly with better audit trails.\nKinsa offers the ﬁrst FDA-cleared, doctor-recommended smart thermometers. They implemented product-centric QMS to assist in the design of the company’s ﬁrst-ever connected device. The application of Quality 4.0 technologies has helped Kinsa revolutionize how healthcare can be reimagined in an IoT universe through connected technologies to streamline product development, improve quality management, and shorten the time to resolve customer issues.\nKeeping up with Quality 4.0\nThe march to deliver more intelligent devices is moving ahead at full speed. Both large, established medical device companies and smaller innovators are racing to improve healthcare by connecting people and data using newer technologies like IoT, AR, and robotics to deliver better outcomes to patients around the globe.\nThe advances made with Industry 4.0 have helped drive Quality 4.0 technologies and strategies to improve quality compliance. The ability to align complex product development and quality processes is critical to compete in today’s global economy. We not only develop products virtually with distributed teams, but we now have the ability to heal and perform surgeries where doctors and patients are separated by multiple time zones with intelligent robotics and augmented reality devices.\nSo, consider your goals and realities for creating and delivering complex products that meet regulations from the FDA to ISO. Make sure you can create a completely traceable, reduced-risk environment to protect your patients as well as your profits and long-term viability.\nQuality 4.0 trends require smarter, connected strategies to ensure your company isn’t left behind.","The Industrial Revolution can be summed up as the revolution that happened to the industrial sector in the past 300 years. But it still can't be explained in just one sentence, as it doesn’t do justice to the greatness of the revolution. It brought about a radical change that was direly needed.\nIndustrial Revolutions of Our World through Time\nThe First Industrial Revolution: 1760 – 1840\nThe Second Industrial Revolution: 1870 – 1914\nThe Third Industrial Revolution: 1969 – 2000\nThe Fourth Industrial Revolution: the digital revolution occurring since the middle of the last century\nRevolutions are a result of humankind's desire to develop, expand, and grow. This has led to all the significant inventions in our society.\nIf we look back in time, many of the technologies that we use today are improvements of the basic concepts laid during the previous revolutions.\nThe spark that led to the proliferation of innovation and inventions was off during the first and second industrial revolutions. However, the gears were set in motion as early as the 1700s.\nThe period from 1750 onward interests many because humankind saw a great deal of change in that era.\nIt was the period that gave us many socio-economic reforms along with some of the most practical technical marvels.\nLet’s understand how the First and Second Industrial Revolutions changed the world by learning all about their great technological advancements and inventions.\nThe First Industrial Revolution\nThe First Industrial Revolution started in 1760 and is a part of the history that ushered the future.\nThis revolution holds a crucial place in history as it marked the era of mechanization.\nIt was a time where man began to understand and use different energy sources; it was the time when industries began to reign over the world.\nThe First Usable Steam Engine\nIt all started when we found out a new form of energy – Steam.\nThen, Thomas Newcomen, a British engineer, in 1712 made history with his prototype steam engine.\nHe made the atmospheric steam engine that can be used to pump water from mines. The need for such an invention came when Newcomen knew about the high operating costs of using horses to pump water out of mines.\nThe steam-powered pump was used in mines to extract water out of mining shafts. The reason for the limited use was that the engine could only manage about 12 strokes per minute.\nThe Age of Textiles - Spinning Jenny\nThe textile industry was booming in the 1700 and elites were going gaga over silk and intricately woven clothes. But the demands posed a severe challenge for the workers as the spinning process took a lot of time, especially the hand-woven materials.\nThe British weaver James Hargreaves invented something that revolutionized the textile industry. He invented the Spinning Jenny that considerably reduced the time taken to produce threads from raw materials.\nThe Spinning Jenny enabled a single worker to produce eight threads as opposed to one thread per worker norm.\nHence, the output of a single worker rose to 8-times compared to the previously obtained output.\nJames Watt’s Take on the Steam Engine in 1778\nIt is here where the crux of the industrial revolution lies. Even though the steam engine was built well before the time of James Watt, it only produced a reciprocal motion and to move something like a wheel a rotary motion was needed.\nJames Watt believed that steam energy had an untapped potential that could be applied to countless industrial processes. Also, until Watt’s invention, conventional steam engines were slow and inefficient.\nThe story of James Watt is pretty interesting as he was always fascinated by stories he heard about steam-powered devices. But one day, he got his hands on a Newcomen engine and tried to improve on its flaws.\nHe subsequently went to tinker the equipment and made his version of the Newcomen engine with a separate condenser and other useful modifications. The result was a double-acting pump that was powerful and fuel efficient.\nIt was a stepping stone which proved that steam packs a serious punch and can be used for more power demanding applications.\nThe ripples of the new steam engine propagated like wildfire and led to the following advancements:\nThe first railway steam locomotive | Inventor – Richard Trevithick (1804)\nThe first commercially successful steamboat name Clermont | Inventor – Robert Fulton (1807)\nThe Rise of Power Looms\nSince the cotton industry was on a roll, the demand for textiles just kept on increasing. The stroke of innovation had already changed how the threads were made, and the invention of water frame made spinning easy.\nBut the weaving process could not keep up with other industrial machinery.\nThis need led to the creation of the Power Loom by Edmund Cartwright. It was an ordinary loom mechanized by a drive shaft to reduce worker input and increase overall output.\nAfter the invention of steam engines, the power looms utilized steam power to automate the process.\nSince then, the machine had a lot of improvements by many inventors to increase efficiency and effectiveness further.\nThe Age Of Iron\nWhen you look back at the industrial revolution, one thing is clear; the revolution wasn’t just about steam, cotton, and coal. There was another critical element that added to the overall industrialization – Iron.\nIn the 1700s, if one was to convert cast iron to wrought iron, they had to heat the whole piece in a furnace and then beat it to perfection. Henry Cort from Lancaster was someone who loved to tinker with different processes associated with Iron.\nHe wanted a system that was cost-effective and less demanding in terms of human effort. For creating his method of metal curing, he bought a Slitting mill forged at Fontley.\nHe used a process called puddling. The process involves stirring molten iron in a reverberation furnace. The molten metal was then decarbonized to make a real mixture of thick molten metal.\nThis thickened Iron was called puddled iron and had many properties that were not available with pig iron. This puddled iron was then formed into bars using a grooved roller which he had patented.\nThe finished product was better and pure than wrought iron, and the bar shape was apt for immediate use. The beauty of this method was that all these processes were mechanized using steam engines and the furnaces did not require charcoal or coke.\nThis was one of the great advancements that shaped the First Industrial Revolution.\nThe Cotton Gin\nThe innovations in the textile industry just kept on coming one after the other. After the waves of industrialization hit the different manufacturing processes associated with textile manufacturing, the transformation began to improve the necessary means of raw materials procurement.\nThe process of separating cotton from cotton seeds, which was mostly done by workers using their hands.\nThis process changed when American inventor Eli Whitney made the first cotton gin. The cotton gin was a mechanized device that can easily separate the cotton seeds from cotton. He patented the product in 1794.\nThese new inventions in addition to the plethora of other inventions made the textiles industry unbelievably efficient in a matter of few years.\nThe Second Industrial Revolution\nThe period during the First Industrial Revolution saw the rise of industries mechanized by steam energy, the fast-paced textile industry, evolving stages of metallurgy, and metal works.\nApproximately, a century after the end of the First Industrial Revolution, the world witnessed a rapid shift from the conventional forms of the previous innovations. This change was a target in utilizing the power of electricity, oil, and gas.\nIn this era, the world witnessed innovations in communication, transport, and manufacturing.\nTo start things off into the second revolution, a remarkable invention in the field of communication was made – the telegraph.\nThe Invention of Telegraph and Morse\nThe telegraph stands as the cornerstone of modern day communication systems. It was invented by Samuel Morse in the 1800s, but the first working telegram station came into operations only in 1844.\nIt is believed that Morse got the idea of using electricity for communication during a conversation that took place while he was returning from Europe in 1832 on a ship. The passengers on the ship were discussing the recent invention of the electromagnet by Michael Faraday, and that is when Morse thought of sending a coded message over a wire.\nThis dots-and-dashes technology revolutionized communication systems and enabled people to communicate over distances.\nThe Telephone by Meucci\nEven though telegram proved as a means of long-distance communication, it was in no way a means to share personal messages. What if a person standing a considerable distance away from you could hear your voice in real time as you spoke?\nThis is, precisely, what happened in the year 1876 as Alexander Graham Bell took the patent for a device called telephone. Yes! In a way, he is the forefather of today’s smartphones that we use on a daily basis.\nMany believe that Alexander Graham Bell was the one who invented the telephone, but this was proved wrong. The real credit for developing the phone goes to the mechanical genius named Antonio Meucci. History hailed Graham for the invention, but it was the blood, sweat, and tears of Meucci that made the “Talking Telegram.”\nThus, making Antoni Meucci the father of modern communication.\nEdison's Quest to Light up the World\nAgain, Edison wasn’t the person behind the first light bulb. But, he was the one to perfect the light bulb. The bulbs before Edison’s intervention worn out pretty quickly, hence not viable for day to day use.\nThe first patent from Edison was filed on October 14, 1878. The patent was for the improvement of electric lights. Edison continued his research on bulb even after filing the patent to perfect the design.\nIt was in 1906 that the patent for bulbs with tungsten based filaments was filed by Edison. The manufacturing of bulbs brought about changes that were beyond imagination.\nPeople were able to light up their homes and work even at odd hours. It also accelerated the adoption of electricity.\nThe First Flight\nThere have been many attempts to sail on the skies in the mid-1800s. Most of them relied on wind energy to fly their dreams to the sky. But, this method had a serious flaw, as wind alone could not propel an aircraft’s weight.\nThe Wright brothers, however, succeeded in solving this problem The solution can be called as powered flight. Then again, powered flights had a bad reputation for being uncontrollable.\nTherefore, the real genius of the Wright Brothers came into play. They invented a three-axis system that made the aircraft maintain equilibrium even at high speed.\nThis fundamental principle remains the same even today in the aviation field.\nHenry Ford's Model T\nOwning a car was a costly affair back in the 1900s, which meant that only the wealthiest would get the privilege to own one. But that soon changed with the introduction of Model T.\nIt was the innovation of the legend, Henry Ford. The introduction of the assembly line was a reason why Model T was such a hit. This brought down the cost considerably and changed how Americans travel forever.\nIt is estimated that over 15 million Model Ts were sold worldwide in 15 years.\nUshering us into a New Age\nWhen we look back at the First and Second Industrial Revolution, we see an age that genuinely defined where we are now. We cannot deny that automation and industrial revolution brought on some adverse effects to the world.\nThen again, we proved to triumph the adversities to enjoy the fruits of our labor. It is truly amazing how the phase of the world changed with a handful of inventions.\nThis brings us to the universal fact by Heraclitus, the Greek philosopher– the only thing that is constant is change!"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:6bfe1986-cf09-45a8-8926-54401e456e4f>","<urn:uuid:46cc392e-8f26-40dc-936d-1241c57297ae>"],"error":null}
{"question":"What are the comprehensive maintenance requirements for boilers, and which organizations provide third-party inspection certification services?","answer":"Boiler maintenance requires tracking multiple data points including boiler pressure, temperature, exhaust temperature, feedwater pressure and temperature, and boiler water column blowdown timing. Regular checks must include burner flame inspection, control valve operation, feedwater tank operation, and water treatment systems. These tasks should be performed on weekly, monthly, semi-annual, and annual basis according to manufacturer recommendations. Regarding third-party inspection certification, several organizations are authorized to provide these services: State Inspectors, Insurance Inspectors, and specialized agencies like DAMARC Quality Inspection Services. These inspections are vital for public safety and must verify conformity with construction codes adopted by building standards boards.","context":["Boiler Efficiency and Safety Specialists\nWe repair, install and service all brands of boilers and burners. We provide preventative service agreements for annual and customized maintenance tailored to meet your needs. We have the most experienced technicians in the State of Oregon and are able to complete certified inspections meeting WA clean air act requirements.\nComplete boiler room services including:\nWe repair, install and service all brands of boilers and burners\n- A.O. Smith\n- Advanced Thermal Hydronics\n- AERCO Benchmark\n- Cleaver Brooks/Superior/B & W/Gabriel/Kewanee/Fulton and many others\nFireye/Siemens combustion controls\n- Gordon-Piatt/IC/Weishaupt/Beckett/Riello/Power Flame Oil and Gas Burners\n- Industrial Combustion Authorized Factory Burner Representative for the Pacific NW\n- HydroTherm KN\n- Power Flame\n- Ray Oil & Gas Burners\nParallel Positioning and O2 Trim -- Increase Efficiency and Optimize Fuel-to-Air Ratio\nParallel positioning controls help a burner to optimize its fuel-to-air ratio by using dedicated motorized actuators for the fuel and air valves. These parallel positioning controls directly tie into the electronic firing rate control, enabling more efficient and consistent performance. Adding O2 trim helps minimize excess air and keeps the burner from going excessively rich, which leads to inefficient combustion. Among the other benefits:\n- Parallel positioning saves up to 3% in fuel costs by eliminating hysteresis\n- O2 trim increases efficiency by up to 2%\n- Variable speed drive can help save as much as 40% in electrical costs\nParallel positioning (often referred to as “linkage-less” control) is a precise, repeatable method of controlling the flow of fuel and air through a modulating burner. The fuel control valve and air damper are each moved by their own dedicated stepper motor actuators, each of which is electronically controlled by an air-fuel ratio controller.\nOn the other hand, a “linkage” burner uses a single servo motor to modulate both the fuel valve and air damper which are mechanically connected together by a series of adjustable jackshafts and linkage arms. The difference is similar to how modern electronic fuel injection systems compare to carburetors in automotive engines.\nWhen air and fuel flow are parallel position systems, when these elements are accurately controlled, a well-designed burner can be precisely tuned to minimize leftover oxygen in the exhaust throughout the full turndown range. This leftover oxygen in the exhaust is referred to as “excess air” and is inversely proportional to combustion efficiency. That means that lower excess air means lower fuel consumption.\nThere are many factors that can affect the net fuel savings realized by switching to parallel positioning controls. A typical estimate would be 3%-5%, although savings of 15% or more are quite possible in certain circumstances. Linkage-less platforms like the Fireye Nexus, Weishaupt W-FM and Siemens LMV are generally known to be safer, more reliable, and require less maintenance over time than conventional burner controls.\nBoiler inspections, operation, and maintenance\nBoilers can be extremely dangerous if not properly maintained, the processes for inspecting them are rigorous and required by law by the state or insurance companies.\nThe State of Oregon requires that Operational Inspections be done by a State Boiler Inspector or Insurance Inspector. The frequency of inspections varies depending on the type of boiler or pressure vessel. N.C.C. is able to complete certified inspections to meet the state of Washington annual Clean Air Act testing and reporting requirements.\nBoiler inspections typically include all internal and external walls and surfaces which are inspected; looking for any signs of leaks, corrosion, overheating, or other structural issues within the boiler. All waterside areas of the boiler are inspected, including blow-down, water connections, and steam connections. All fireside conditions are inspected inside the boiler, including superheaters, deaerators, and economizers. All external trim and control devices are also inspected.\nBoiler Operation consists of many tasks to keep a boiler operating safely and efficiently. These tasks include tracking many data points, such as boiler pressure and temperature, boiler exhaust temperature, feedwater pressure and temperature, and boiler and water column blowdown timing. Continual checks should also be a part of any maintenance program, including burner flame for proper combustion, proper operation of control valves for all systems, feedwater tank or deaerator operation and level, water treatment systems, and taking water samples for comparison to recommended chemistry guidelines.\nTypical annual maintenance items\nThere are other equally important tasks to ensure proper boiler operation that should be performed on a weekly, monthly, semi-annual, and annual basis in accordance with the manufacturers and insurance recommendations. Failure to comply with the equipment manufacturer’s recommended maintenance schedule may result in equipment failure due to problem indicators being missed from lack of regular maintenance. Keeping a boiler operator’s log helps to ensure all data points are recorded, all checks are performed, and poor performance indicators are addressed.\nEvery Model and Manufacturer have different recommended annual maintenance procedures. Our highly experienced and skilled boiler technicians know the right maintenance and frequency for the various types of boilers and different brands. Whether it is checking for leaks or cleaning condensate traps, our experts have safety first and efficiency as their priorities.","then examine the role of third party inspections in other contexts such as steam boiler 2 Federal regulatory agencies, such as the Environmental Protection Agency (EPA), the Occupational Safety and Health Administration (OSHA) as well as the U.S. Chemical Safety and Hazard Investigation Board (CSB) are aware of this concern.\nCurrent Bulletin Click on the link below to view the current Boiler Inspection Program Bulletin. What's New | Colorado Division of Oil and Public Safety Jump to navigation\n4 8. BOILER - A closed vessel in which water or other liquid is heated, steam or vapor is generated, steam or vapor is superheated, or any combination thereof, under pressure or vacuum, for use external to itself, by the direct application of energy from combustion of fuels, electricity or solar energy.\nDAMARC Quality Inspection Services, LLC is the very first Authorized Inspection Agency in the world recognized as such by the National Board of Boiler and Pressure Vessel Inspectors that is not an insurance company. We have and use Commissioned Boiler Inspectors to provide an array of third-party inspection services which are vital to the safety of the public.\nsteam gas boilers for home heating; coal fired steam boiler in myanmar; 500kw power plant boiler; third party steam boiler inspection in bhopal; 200 hp steam boiler in Libya; famous oil fired boiler brand; Hot Article. oil gas control panel boiler; china supplier wood pellet steam boiler; automatic straw pieces hot water boiler; coal fired fuel steam boiler\nMay 25, 2018 · Inspection of Steam Boiler is a must activity should be carried out as per Govt. guidelines. Steam is required for power generation through a steam turbine or steam engine.\n(C) \"Authorized Inspection Agency\" means an entity, accepted by the \"National Board,\" that provides third party inspection services in which boilers and pressure vessels are inspected during construction, repairs, and alterations to verify their conformity with the code of construction adopted by the board of building standards.\nOur Management - We at Bolster Engineering Solutions Private Limited are Service Provider of Brazing Training Programs, Expediting Services, Qa /Qc/Inspection Training, Third Party Inspection Service and Graduation & High Education Programs in Bhopal, Madhya Pradesh\nThe Boiler Inspection Program enforces nationally adopted standards governing the installation, operation, maintenance and repair of boilers and some pressure vessels in commercial and public buildings. Periodic inspections are performed by State Inspectors or by special (insurance company) inspectors commissioned by the Boiler Inspection Program.\nhydrogen gas generation in my water heater - Home Forums\\nNov 6, 2009 I have a Bradford and White water heater installed 4 months ago and it is producing a LOT of hydrogen gas. i.e. I get gas bubbles if the hot Ask PriceEmail Us\\nHydrogen Gas gene\nADOSH - List of Authorized companies for Boiler Inspections; ADOSH - List of Authorized companies for Boiler Inspections. Arizona Division of Occupational Safety and Health (ADOSH) Authorized Inspection Companies for non-insured boilers, lined water heaters, and pressure vessels in the State of Arizona. ABI, ARIZONA BOILER INSPECTORS, LLC\nMill Test Certificates, EN 10204 3.1, Chemical Reports, Mechanical Reports, PMI Test Reports, Visual Inspection Reports, Third Party Inspection Reports, NABL Approved Lab Reports, Destructive Test Report, Non Destructive Test Reports, India Boiler Regulations (IBR) Test Certificate: ASTM A213 T2/ ASME SA213 T2 Alloy Steel Tube Form\nThe Institute offers Third Party Inspection Services and Vendor Assessment for Utilities. CPRI is also offering its services for programmes initiated by Government of India. Training: CPRI has been in the forefront for disseminating the knowledge assimilated by way of in-house Research through organising Technical Programmes.\nAppointing the Third Party Inspection Agencies looking into project inspection requirement & considering the budget taken in tender stage. Conducting internal review meeting periodically will all concern departments to assess the project progress in comparison with planned schedule of Project.\nBulk Stockist for G.I.Items, Carbon Steel, Mild Steel, Gun Metal Cast Iron, Forged Carbon Steel, All Types of Pipe Fittings, Gauges, Flanges, Boiler Mounting Available as per Requirement Standard, Specification and Customer Drawing. This all above Items my be Subject to Satisfactory Stage Inspection, Radiography & Third Party Inspection.\n• Notified Body for the European Pressure Equipment Directive (PED) 97/23/EC • Agreed Notified Body for ESPN • Internationally Accredited by ANSI-ASQ and KAB to provide ISO 9000, ISO 13485, AS-9000, and ISO 14000 Registration Services • Accredited as a recognized third party inspection agency for all major boiler\nAuthorized Inspection Companies for non-insured boilers, lined water heaters, and pressure vessels in the State of Arizona. ABI, ARIZONA BOILER INSPECTORS,\nThe Directorate of Boiler comes under the Department of Commerce, Industries and Employment. The directorate is responsible for the inspection and issuance of registration numbers for new boilers and renewal of certificates for existing boilers, registration of, boiler & boiler component manufacturers, repairers and erectors under Boiler Act 1923 & Indian Boiler Regulations 1950\nThird Party certification for Boiler/Economizers during use u/s 34(3) of the Boilers Act, 1923 by authorizing BOE for the state of Gujarat (1.70 MB) On IFP for Payment Related Matter, Write to [email protected]\nHeading: Third Party Inspection Services, City: Bhopal, Results: Quality Evalution and Systems Team Pvt Ltd, Involvements: Field Quality Assurance Services Project Monitoring Services Expediting Vendor Evaluation Services near me with phone number, reviews and address.\na steam boiler function as the main gas pressure regulator. F. Control Panel recognized and . Ask Price Email Us. Boiler Operator Study Guide - Redwater Creek Steam. Boiler Operator Study Guide 10421 He shall be able to explain the function and operation of all .\nHas 15 years experience in technical and management related to projects, maintenance of power station, petrochemical industries, third party inspection, fabrication and erection of heavy machineries. He is expertise in the field inspection of Boiler of power plant unit up to 300 MW, welding inspection, NDT, design and drafting software such as\ninspection and request that other boilers not be blown down during your inspection. Make sure all fuel, steam, blowdown, feedwater, and chemical feed valves are turned off and locked out. Never use a 110 V light bulb and lead in the boiler to avoid electric shock. Touch the bare metal before entry to ensure the boiler is not too hot for inspection.\nThird Party Inspection for Shell and Tube Heat Exchanger - Visual and Dimensional Inspection of Tubes, Tube Bundle and Shell. The third party inspector carries out following controls: Tubes after bending for thinning particularly on the back of bends. Tube sheet and baffle plate, including tube hole, heads, flanges, tubes, flange facing finish.\nImproving Environmental Safety Through Third Party Inspection. Howard C. Kunreuther1, Patrick J. McNulty1 and Yong Kang 1 October 2001. 1 The Risk Management and Decision Processes Center, The Wharton School of the University of Pennsylvania, 1325 Steinberg Hall-Dietrich Hall, Philadelphia, Pennsylvania 19104-6366.\nThird party inspection services have an important and critical role to play in the modern day world. Consumers and manufacturers conduct the inspection to the best of their ability, in order to ensure the safety, credibility, and longevity of the products.\nThe Boiler Safety Bureau is responsible for the inspection of all high-pressure boilers and all low-pressure boilers, except those located in dwellings of less than six families. In accordance with Industrial Code Rules 4 & 14, the inspections are due for: High-pressure boilers each year ; Low-pressure boilers\nThird Party Inspection Rexal Tubes offers its products to client with third party inspection also. The inspection Agency can be nominated by clients also or Rexal Tubes arrange the Third Party Inspection by world reputed Third Party Inspection Agencies like\nThird Party Inspection refers to independent inspection services that are provided by inspection agencies which provides good impartiality in inspection results. Key Benefits Safety, credibility, and longevity of the products.\nIndustrial units with boilers in Maharashtra: In 3 years, no self-declarations, third party inspections The official added that the reports were required to be submitted after third party inspection by the industrial establishments to take policy decisions at the government level\nIndependent Third-Party Inspection Organizations: A company in the business of providing third party inspection services, which has government recognition to perform inspection and design reviews for boilers and pressure vessels.\nproposes to empanel Third Party Inspection (TPI) Agency to partly share our endeavours for ensuring quality of inputs purchased by various BHEL units & division as given below: 1. Heavy Electrical Equipment Plant, Hardwar 2. Central Foundry Forge Plant, Hardwar 3. Heavy Electrical Plant, Bhopal 4. Transformer Plant, Jhansi 5.\nCertified (Third Party) Inspections - 3 - SECTION 100 OVERVIEW. The Certified (Third-Party) Inspections Program is Fairfax County, Va.’s policy for “third-party” inspections of commercial projects requiring construction or maintenance inspections by the Commercial Inspections Division. This program is\nIn a third party exports, the overseas order is obtained by a third party exporter. So the Foreign Inward remittance is received by the third party exporter, as he had obtained the export order. So the purchase order from overseas buyer, Foreign Inward Remittance Certificate (FIRC) etc. will be in the name of third party exporter and not in the name of manufacturer exporter.\n- Complying with ASME/European standards strictly controlled by in house & Third party Inspection Encore Specialized Services Abhiramapuram, Chennai 42/47,2nd Floor, Behind G.K.B Opticals C P Ramasamy Road, Abhiramapuram,, Abhiramapuram,\n1 Third-party inspection agencies also play an important role in the inspection of BPVs. Under the Regulation they must perform those inspections on behalf an insurer. Oversight requirements for insurers would include their use of third-party inspection agencies.\nWholesale Prices for Aluminium Products in Bhopal Mill TC EN 10204 3.1, Third Party Inspection, NDT/DT Test Reports. Latest Prices: Call Us Now +91-9833604219, 9892451458. Reviews There are no reviews yet. Be the first to review “Aluminium Suppliers in Bhopal” Cancel reply.\nThird Party Inspection as an Alternative to Command and Control Regulation Howard C. Kunreuther1, Patrick J. McNulty1 and Yong Kang 1 January 2000 INTRODUCTION Over the past decade government agencies have recognized that relying on traditional command and control procedures for enforcing its regulations might not be the\nSteel Pipe Suppliers in Bhopal: Manufucturers, Dealers, Distributors and Stockist. FerroPipE is one of the leading Steel Pipe Suppliers in Bhopal.With state of art inventory management system, we assure on time deliveries of steel pipes with the most optimum packing detailing.\nThird Party Inspection refers to independent inspection services that are provided by inspection agencies which provides good impartiality in inspection results. Key"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:5a9f80e8-6f35-4da4-876c-43693ef2f832>","<urn:uuid:c49cc307-dc7b-4fa0-b98e-ca54ce759c6d>"],"error":null}
{"question":"What is shadscale's elevation range and habitat preferences, and how does it contribute to different vegetation communities?","answer":"Shadscale grows at elevations between 600-2200 meters (2,000-7,500 feet) and typically occurs on saline substrates, though it can also grow on non-saline soils. It can be found in multiple vegetation communities including greasewood, mat-atriplex, salt desert shrub, sagebrush, pinyon-juniper, and ponderosa pine communities. In these areas, particularly in harsh exposed habitats like badlands, it can become a dominant shrub, contributing to both the semi-desert shrub-steppe and desert shrubland ecosystems.","context":["Shrubs, dioecious, 3-8 dm, spinescent. Leaves persistent, alternate; petiole 1-4 mm; blade orbiculate to ovate, elliptic, or oval, 9-25(-45) × 4-20(-25) mm, margin entire, apex obtuse. Staminate flowers yellow, in clusters 2-4 mm wide or in spikes to 1 cm, axillary, in foliose-bracteate, divaricately branched panicles 3-15 cm. Pistillate flowers in similar paniculate inflorescences. Fruiting bracteoles sessile or subsessile, suborbiculate to rhombic or elliptic, 4-12 mm and wide, body indurate, terminal teeth distinct, foliaceous, shorter than bracteoles, entire or toothed below, terminal teeth spreading at maturity, faces smooth, lacking appendages. Seeds 1.5-2 mm wide. 2n = 18, 36, 54+. Flowering spring-fall. Gravelly to fine-textured soils in greasewood, mat-atriplex, other salt desert shrub, sagebrush, pinyon-juniper, and ponderosa pine communities; 600-2200 m; Ariz., Calif., Colo., Idaho, Mont., Nev., N.Mex., N.Dak., Oreg., Tex., Utah, Wyo. Shadscale forms hybrids with Atriplex canescens, A. garrettii, A. corrugata, and A. gardneri varieties. It is, however, closely allied to A. parryi and A. spinifera. The plants are widely dispersed, typically on saline substrates but less commonly on essentially non-saline ones, through large areas of the western United States and adjacent Canada and Mexico, on both raw and exposed geological strata and on alluvium.\nFNA 2003, Heil et al 2013, Carter 2012\nCommon Name: shadscale saltbush Duration: Perennial Nativity: Native Lifeform: Shrub, Subshrub General: Dense, spiny, mound-shaped shrub, 30-80 cm tall. Leaves: Evergreen, alternate, and short-petiolate to nearly sessile along the spine-tipped branches; petioles to 1-4 mm long; blades round to oval-shaped, 1-2 cm long, with entire margins; gray-green and covered with scurfy pubescence. Flowers: Male and female flowers on separate plants; staminate (male) flowers tiny and yellow, in 2-4 cm clusters or 1 cm spikes; clusters arranged in foliose-bracteate, divaricately branched panicles, 3-15 cm long; pistillate (female) flowers tiny, lacking petals or sepals, arranged in similar paniculate inflorescences; inflorescence bracts are pink or purple-tinged. Fruits: Fruiting bracteoles sessile or subsessile, suborbiculate to rhombic or elliptic, 4-12 mm long, the body indurate (firm); the terminal teeth distinct, foliaceous, and shorter than the bracteoles; lower margin of fruiting bracteoles can be entire or toothed Ecology: Found in desert scrub, sagebrush, pinyon-juniper, and ponderosa pine communities; often on saline substrates, badlands, or alluvium, from 2,000-7,500 ft (610-2286 m); flowers from April-July. Distribution: w US and adjacent CAN and MEX; most common in the Great Basin Desert, from Four-Corners to e CA, north to OR, ID, and WY. Notes: This spiny, silvery-leaved shrub can be the dominant shrub in harsh, exposed habitats such as badlands. It is inconspicuous unless seen at just the right time when purple bracts subtend the tiny, inconspicuous flower clusters. Look also for whitish spines that appear to be extensions of the branches, and silvery, roundish leaves. Traditionally placed in the goosefoot family (Chenopodiaceae), that entire family was recently lumped into Amaranthaceae. Ethnobotany: Smoke used as a treatment for epilepsy; leaves used to make a liniment for sore muscles and aches; leaves eaten as a potherb; seeds used as a grain, wood used to make arrow points. Etymology: Atriplex is the ancient Latin name for other plants in this genus, derived from the Greek name atraphaxes; confertifolia translates to crowded leaves. Synonyms: Obione confertifolia Editor: AHazelton 2015, AHazelton 2017","The diversity of plant species native to the area results from the wide variations in elevation, slope, exposure, soils, and moisture availability found at Dinosaur NM. Six major vegetative community types exist in the monument: montane forest, montane and semi-desert woodlands, montane shrub-steppe, semi-desert shrub-steppe, desert shrublands, and riparian woodlands and wetlands.\nAt Dinosaur, the montane forest is found in the highest and coolest places. There Ponderosa pine, Douglas fir and quaking aspen inhabit the cool, north-facing cliffs and slopes of Dinosaur's mountain terrain.\nThe montane forest is found only above 7,000 feet/2,134 meters and covers just three percent of the monument.\nTo see this plant community, peer over the rim at Canyon Overlook or Harpers Corner Overlook, both on Harpers Corner Drive.\nMontane and Semi-desert Woodlands\nThe sheltering montane & semi-desert woodlands are dominated by pinyon pine and Utah juniper and cover more than half the terrain in Dinosaur.\nThe pinyon pine and juniper woodlands provide habitat for many species of birds: nearly a third of all the bird species in the monument have been spotted here.\nExplore the montane and semi-desert woodlands on Harpers Corner Drive with a stop at Iron Springs Bench Overlook or hike the Harpers Corner Trail to see (and smell) some beautiful examples of pinyon pine and Utah juniper. Some of these trees, gnarled and aged, have an almost sculptural beauty.\nThe drier and rockier soils of the wind-swept montane shrub-steppe support a variety of shrub species, including mountain big sagebrush, mountain mahogany, antelope bitterbrush and Utah serviceberry.\nFire will temporarily transform shrubland to grassland in this system, but the shrubs recover quickly, usually in a decade or less.\nThis plant community is found near the upper portions of the pinyon-juniper zone.\nAn expansive mosaic of sagebrush and grassland, the semi-desert shrub-steppe is home to Wyoming big sagebrush, rabbitbrush, shadscale and a variety of grasses.\nUnlike the montane shrub-steppe, fire will convert this plant community to grassland for several decades.\nThe semi-desert shrub steppe is found near the lower portions of the pinyon-juniper zone, where deep, well-drained soils support a warmer, drier shrub-steppe ecosystem.\nExplore the semi-desert shrub-steppe on the Yampa Bench, where this plant community can be found in both its unburned state and in several stages of post-fire development. For a landscape-scale view of the Yampa Bench and this plant community, stop at Iron Springs Bench Overlook on Harpers Corner Drive.\nFound at lower elevations, desert shrublands produce some of the most unusual adaptations to life in Dinosaur's arid environment.\nOpen-canopy shrub vegetation and dry inhospitable soils characterize this ecosystem, several variants of which can be seen in the western portion of Dinosaur, on the Utah side of the monument.\nGrey-green shrubs, including four-wing saltbush, shadscale, greasewood, Wyoming big sagebrush, and mat saltbush are prevalent here.\nMany of Dinosaur's most unusual wildflowers occur in this plant community.\nRiparian Woodlands and Wetlands\nThe riparian woodlands and wetlands is the smallest plant community in Dinosaur's otherwise arid landscape, but it contributes in huge measure to the diversity of wildlife here.\nWater is life and the ribbon of green vegetation clinging to the river's edge is full of living things.\nBoxelders and netleaf hackberry trees dominate this plant communityin the river canyons, cottonwoods and willows along the meandering floodplain reaches."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:dffb75aa-20db-40f3-8e48-45735149dcf4>","<urn:uuid:496ca000-2be7-47aa-a326-2157a9dc7b3b>"],"error":null}
{"question":"For my thesis research - what types of samples are typically examined using scanning gate microscopy?","answer":"Typical samples examined using scanning gate microscopy are mesoscopic devices, often based on semiconductor heterostructures, such as quantum point contacts or quantum dots. Carbon nanotubes have also been investigated using this technique.","context":["- Scanning gate microscopy\nScanning gate microscopy (SGM) is a\nscanning probe microscopytechnique with an electrically conductive tip used as a movable gate that couples capacitively to the sample and probes electrical transport on the nanometerscale. Typical samples are mesoscopicdevices, often based on semiconductor heterostructures, such as quantum point contacts or quantum dots. Carbon nanotubes too have been investigated.\nIn SGM one measures the sample's\nelectrical conductanceas a function of tip position and tip potential. This is in contrast to other microscopy techniques where the tip is used as a sensor, e.g., for forces.\nSGMs were developed in the late 1990s from\natomic force microscopes. Most importantly, these had to be adapted for use at low temperatures, often 4 kelvins or less, as the samples under study do not work at higher temperatures. Today an estimated number of ten research groups worldwide use the technique.\n* Coherent Branched Flow in a Two-Dimensional Electron Gas: A. Topinka \"et al.\", \"Nature\" 410, 183 (2001)\n* Scanned Probe Imaging of Single-Electron Charge States in Nanotube Quantum Dots: M. T. Woodside and P. L. McEuen, \"Science\" 296, 1098 (2002)\n* Spatially Resolved Manipulation of Single Electrons in Quantum Dots Using a Scanned Probe: A. Pioda \"et al.\", \"Phys. Rev. Lett.\" 93, 216801 (2004)\n* Imaging and controlling electron transport inside a quantum ring: B. Hackens \"et al.\", \"Nature Phys.\" 2, 826 (2006)\nWikimedia Foundation. 2010.\nLook at other dictionaries:\nScanning voltage microscopy — (SVM) sometimes also called nanopotentiometry is a scientific experimental technique based on atomic force microscopy. A conductive probe, usually only a few nanometers wide at the tip, is placed in full contact with an operational electronic or… … Wikipedia\nScanning probe microscopy — Part of a series of articles on Nanotechnology … Wikipedia\nScanning tunneling microscope — Image of reconstruction on a clean Gold(100) surface … Wikipedia\nTransmission electron microscopy — A TEM image of the polio virus. The polio virus is 30 nm in size. Transmission electron microscopy (TEM) is a microscopy technique whereby a beam of electrons is transmitted through an ultra thin specimen, interacting with the specimen as it… … Wikipedia\nEnvironmental scanning electron microscope — Wool fibers imaged in an ESEM by the use of two symmetrical plastic scintillating backscattered electron detectors … Wikipedia\nNear-field scanning optical microscope — Near field scanning optical microscopy (NSOM/SNOM) is a microscopic technique for nanostructure investigation that breaks the far field resolution limit by exploiting the properties of evanescent waves. This is done by placing the detector very… … Wikipedia\nMagnetic resonance force microscopy — (MRFM) is an imaging technique that acquires magnetic resonance images (MRI) at nanometer scales, and possibly at atomic scales in the future. MRFM is potentially able to observe protein structures which cannot be seen using X ray crystallography … Wikipedia\nFeature-oriented scanning — (FOS) is a method intended for high precision measurement of nanotopography as well as other surface properties and characteristics on a scanning probe microscope (SPM) using features (objects) of the surface as reference points of the… … Wikipedia\nCounter-scanning — (CS) is a method for measuring surface topography with a scanning probe microscope enabling correction of raster distortions resulted from drift of the microscope probe relative to the surface being measured. Two surface scans, viz. direct… … Wikipedia\nMicroscope — This article is about microscopes in general. For light microscopes, see optical microscope. Microscope Us … Wikipedia"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:67ea34a4-1d84-4b75-ab74-6b84f4f4ec91>"],"error":null}
{"question":"Which tech needs more data: deep learning algorithms or blockchain systems?","answer":"Deep learning algorithms require vastly more data than blockchain systems to function effectively. Deep learning needs extensive labeled training data to effectively train neural networks, making the sourcing and curation of high-quality datasets a critical challenge. In contrast, blockchain technology primarily processes transaction data within its distributed ledger system, focusing on securing and validating individual transactions rather than requiring large training datasets. The blockchain's primary data requirement is the transaction information itself, which is then cryptographically secured and shared across the network.","context":["Deep learning is an advanced subset of machine learning, a current and ever-evolving field of technology that has the potential to revolutionize the way we interact with and understand the world around us. In recent years, deep learning has gained significant traction, becoming a widely discussed and utilized technology in a variety of industries and applications.\nAt its core, deep learning is a complex algorithmic approach to building and training artificial neural network architectures. These networks are inspired by the structure and function of the human brain, with interconnected nodes, or neurons, that work together to process and interpret data patterns. Through the use of these neural networks, deep learning algorithms can autonomously learn to identify and extract features from large datasets, enabling them to recognize and understand complex patterns and make accurate predictions or decisions.\nOne of the key strengths of deep learning lies in its ability to handle unstructured data, such as images, videos, and natural language. This makes it particularly well-suited for applications like image recognition, speech recognition, natural language processing, and even autonomous vehicles. For example, deep learning algorithms powering self-driving cars can process visual input from cameras and sensors to quickly and accurately identify objects, pedestrians, and potential hazards in real-time, enabling the vehicle to make split-second decisions to ensure safety.\nThe breadth of potential applications for deep learning is vast and diverse. In healthcare, deep learning algorithms can be used to analyze medical images, diagnose diseases, and predict patient outcomes. In finance, they can help detect fraudulent transactions and make investment recommendations. In marketing, they can analyze customer behavior and preferences to personalize product recommendations. In manufacturing, they can optimize production processes and predict equipment failures. The possibilities are virtually endless, and as the technology continues to mature, we can expect to see even more innovative and impactful applications emerge.\nOne of the most significant recent developments in the field of deep learning is the use of generative adversarial networks (GANs). GANs are a type of neural network architecture consisting of two networks – a generator and a discriminator – that work together to produce realistic synthetic data. This has huge implications for tasks like image and video synthesis, where GANs can be used to create lifelike images of non-existent people, places, or objects, or even produce deepfake videos that are nearly indistinguishable from real footage. While the ethical considerations of this technology are complex, it’s an exciting testament to the power and potential of deep learning.\nIn addition to its practical applications, deep learning also presents a number of challenges and considerations. One of the most significant challenges is the need for vast amounts of labeled training data to effectively train deep neural networks. As a result, sourcing and curating high-quality datasets is a critical step in the deep learning process. Additionally, deep learning models are often large and computationally intensive, requiring significant computational resources to train and deploy. This can be a barrier for smaller organizations or research groups with limited resources.\nPrivacy and security are also major concerns when it comes to the widespread adoption of deep learning. As deep learning algorithms become more adept at processing and interpreting personal data, there is an increased risk of privacy breaches and misuse. It’s essential for organizations to implement robust data protection measures and ensure transparency in their use of AI-driven technologies to maintain user trust and compliance with regulations.\nDespite these challenges, the continued growth and advancement of deep learning hold immense promise for the future of technology and society as a whole. With ongoing research and innovation, we can expect to see even more sophisticated deep learning models that push the boundaries of what is possible and drive new breakthroughs in fields like healthcare, finance, education, and beyond.\nIn conclusion, deep learning is a foundational technology with far-reaching implications. Its ability to process, analyze, and interpret complex data is driving significant advancements in a wide range of industries and applications. As the technology continues to evolve, it’s important for organizations and researchers to remain mindful of the ethical and practical considerations that come with harnessing the power of deep learning.\nRecent news in the field of deep learning includes the release of OpenAI’s GPT-3, the third generation of their groundbreaking language prediction model. GPT-3 represents a significant leap forward in natural language processing and understanding, with the ability to generate human-like text and perform a wide range of language-based tasks, such as writing essays, answering questions, and even writing code. While the technology has generated excitement and intrigue, it has also sparked discussions around responsible AI use and the potential consequences of highly advanced language models in the wrong hands.\nThis recent development serves as a testament to the rapid pace of advancement in deep learning and the potential for profound impacts on how we communicate and interact with technology. As deep learning continues to progress, it’s critical for researchers, developers, and policymakers to collaborate on establishing ethical guidelines and standards for the responsible development and deployment of AI technologies. By doing so, we can ensure that the potential of deep learning is harnessed for the collective benefit of society, while minimizing risks and vulnerabilities.","What is Blockchain?\nBlockchain. It’s a term that’s been on the tip of everyone’s tongue following recent, exciting advances in the technology, and growing media attention on the cryptocurrency market. Rightfully so, there is also a healthy mix of skepticism and cautious optimism depending on who you speak with. This year’s Health Datapalooza program aims to address the hype with multiple sessions including a main stage panel titled “AI, Blockchain, Machine Learning, IOT...from Buzzwords to Reality in Healthcare.” We hope this two-part blog will help you understand how blockchain works, considerations for health-related applications, and what the future holds.\nAt its core, blockchain technology represents a data structure that makes it possible to create a digital ledger of transactions and share it among a distributed network of computers with the potential for increased security, reductions in cost, decreased transaction times, and greater transparency - all while eliminating the need for a trusted third-party or intermediary. Blockchain technology uses cryptography to allow each participant in the network to make additions to the ledger in a secure manner without the need for a central authority, as long as any additions and changes are validated and agreed upon by network participants as adhering to the rules of the specific blockchain protocol.\nBlockchain technology, conceptualized by early pioneers such as the pseudonymous programmer(s) Satoshi Nakamoto with the widely known Bitcoin blockchain platform, was crafted in response to problems related to trust and the need for disintermediation in transaction processing. Blockchain’s elimination of central authorities, third-parties, or intermediaries for common transactions result in ‘trustless’ transactions where two parties conduct a transaction (i.e., peer-to-peer) by trusting the rules and cryptography in the underlying blockchain protocol. These features alleviate the common double spend problem (i.e., using the same digital money files such as Bitcoins more than once) with blockchain transactions structured to be irreversible and final within their timestamped universal ledger.\nBlockchain technology can also be classified into three categories based upon the entities who control and participate in the specific blockchain platform ecosystem: Private, permissioned, and public.\nPrivate blockchains are controlled by single entities managing all of the nodes and validators (i.e., key members of the organization’s blockchain network which independently check the data being conveyed through the network). Their distributed ledgers provide auditability of transactions within an organization, and the self-contained nature promotes privacy versus public blockchains. However, private blockchains lack true decentralization, leaving them potentially susceptible to the dictates of centralized governance and decision making. Attackers targeting an organization’s validator nodes, can also lead to the risk of blockchain rule changes or ledger modifications which compromises the integrity of the blockchain.\nPermissioned, or consortium, blockchains are those in which a group of participants (typically in a formal business agreement) each run a validator node. This promotes greater decentralization than private blockchains depending on the number of participants and validator nodes in the system. However, potential collusion between a majority participants or compromising the majority of validator nodes can result in a compromised blockchain. Permissioned blockchains have shown promise for multiple parties focused on eliminating issues of trust within their network using a transparent ledger supporting inter-party transactions. Permissioned and private blockchains afford greater privacy and transaction throughput with fewer validator and participating nodes, at the cost of security and potentially immutability.\nPublic blockchains, like the Bitcoin and Ethereum® platforms, allow anyone with a computer and internet connection to participate by downloading the necessary client software to run a node, regardless of whether they choose to actively conduct transactions or act as a validator. Public blockchains offer the greatest degree of decentralization, security, and likelihood of a ledger remaining immutable, at the cost of lower throughput, greater overhead in the form of transaction fees (typically paid to nodes that secure the network and confirm transactions), and lack of substantial privacy controls. These limitations make public blockchains a more difficult proposition for organizations from a data security/anonymity standpoint, though some interesting breakthroughs on are on the horizon in this regard.\nAs with the internet, which evolved from smaller networks or intranets, it is likely that many participants will initially gravitate towards private or permissioned blockchain technology. As the technology matures and new applications beyond cryptocurrency gain traction, the advantages of public blockchains will help advance their use.\nOkay, but HOW does it work?\nThe general concept of blockchain technology is that it represents a distributed, decentralized ledger that is secured cryptographically and, except under rare circumstances, immutable. The following figure outlines the processes occurring on a public blockchain from transaction origination to confirmation.\nBlockchain can be perceived as a key for a specific lock on a specific door, rather than a master key for doorway. There are additional nuances with various blockchain protocols and applications in addition to distinctions between public, permissioned, and private blockchains. These differences result in potential trade-offs and should be assessed by any organization considering the application of blockchain to their business.\nWhen should I consider blockchain?\nUltimately, blockchain technology brings a new and potentially game-changing solution to help address traditionally challenging issues of trust and the secure exchange of information in many multi-party transactional processes. In the context of health care, whether it’s health records and registries, claims processing, aid disbursement, or provenance in supply chain or chain of custody, blockchain can help close the trust gap through trustless, mathematically verifiable transactions, the elimination of intermediaries in a decentralized peer-to-peer network, and automation of conditional agreements.\nThe advent of more advanced blockchain platforms such as Ethereum® and Hyperledger™ (which comprises multiple blockchain technologies) offer the capability to incorporate logic-based, conditional agreements commonly referred to as “smart contracts” which can support the automation of transactions based on coded conditions and logic. This capability greatly expands the potential use cases for blockchain beyond peer-to-peer payments and simple value transactions to more ambitious applications built on an “Internet of Agreement” such as:\n- Blockchain enabled crowdfunding;\n- Supply chain management and other provenance-based use cases;\n- Identity verification, management, and eventually, self-sovereign identity;\n- Registries (Birth, Death, Land, Voting, etc.);\n- Asset management (tokenizing physical assets on the blockchain);\n- Decentralized data storage and dissemination;\n- Unique incentive programs for cross-market use; and,\n- Many more… it’s likely the best use cases have not even been thought of yet!\nThese use cases show exciting potential as blockchain technology matures at a rapid pace. Bitcoin, the oldest blockchain, has been around for eight years but has only gained widespread use in the past year. Other blockchain platforms such as Ethereum® and Hyperledger’s suite of solutions face a number of challenges - from scalability and access (most of the public blockchains have not achieved widespread use and scalability), to security and privacy protection, the technology and processes underlying blockchain are rapidly evolving and will take time to mature which makes a strong case for small scale projects and piloting proof of concepts.Whatever its ultimate use, blockchain technology can no longer be ignored by the federal, commercial, and non-profit health care sectors which rely upon the secure exchange of highly sensitive information. Stakeholders in the health care ecosystem are starting to get their feet wet with blockchain. Collaborations abound and an increasing interest in small projects and pilot initiatives to allow organizations test the technology – and their risk tolerance. While prudence is necessary, the significant potential advantages of blockchain technology present an incredible opportunity for early adopters in health care.\nPlease stay tuned for our second blog post which will further explore specific applications of blockchain and expand on the future potential for this exciting technology in research, clinical, and public health."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:0d08b97a-a0bf-4297-bb0e-9d390a06a8b5>","<urn:uuid:57dae13f-bc5c-40f8-b65e-8f1899092c1b>"],"error":null}
{"question":"What was the scope of Operation Overlord on D-Day, and how many casualties did the 79th Infantry Division suffer during their WWII service?","answer":"Operation Overlord was the largest amphibious invasion in world history, involving over 160,000 soldiers landing on June 6, 1944: 73,000 Americans, 61,715 British, and 21,400 Canadians. It also involved 195,700 Allied naval and merchant navy personnel in over 5,000 ships. The invasion covered a 50-mile stretch of Normandy coast divided into five sectors: Utah, Omaha, Gold, Juno, and Sword. As for the 79th Infantry Division, during their 248 days of World War II campaign, they suffered 15,203 total casualties, including 10,971 wounded and 14,875 non-battle injuries. They also took 35,466 prisoners of war.","context":["Baade, Paul William, born on 16-04-1889, four days before Adolf Hitler\n(did you know\n) in Fort Wayne, Indiana, served with the 322nd\nInfantry Division in France during World War I. With the 322nd\nInfantry Division in the occupancy of the St. Die sector, the Vosges and on to Verdun. He fought as a Lieutenant Colonel in the bitter actions east of the Somedian Sector and in the Meuse Argonne. During World War II he served as Commanding General of the Thirty-Fifth Division. He held the Distinguished Service Medal, the Silver Star with Oak Leaf Cluster, the Legion of Merit, the Bronze Star with two Oak Leaf Clusters and the Purple Heart. AS Commander of the 35th\nDivision, nickname “Santa Fe”\nhis division landed on Omaha Beach on 08-07-1944, former commander was William Hood “Bill” Simpson\n. Total casualities of the Santa Fe Division during the European campaign, 7.296, KIA 1.018 ; WIA 6.278. During World War I, Battery D of the 129th\nField Artillery Regiment had, as a battery commander, Captain Harry Shipp Truman\n, later President of the United States.\nThe Operation Overlord, planned by a team under Lieutenant-General Frederick Morgan\nwho died age 73, on 19-03-1967, in Northwood, was the largest amphibious invasion in world history and was executed by land, sea and air elements under direct Anglo-American command with over 160.000 soldiers landing on 6 June 1944: 73.000 Americans, 61.715 British and 21.400 Canadians. 195.700 Allied naval and merchant navy personnel in over 5.000 ships were also involved. The invasion required the transport of soldiers and materiel from the United Kingdom by troop-laden aircraft and ships, the assault landings, air support, naval interdiction of the English Channel and naval gunfire support. The landings took place along a 50-mile (80 km) stretch of the Normandy coast divided into five sectors: Utah, Omaha, Gold, Juno and Sword. American casualties at Omaha on D-Day numbered around 5.000 out of 50.000 men, most in the first few hours, while the Germans suffered 1.200 killed, wounded or missing. Franz Gockel\na German machinegunner of the 726th\nInfanterieregiment of the 716th\nInfanteriedivision, claimed to have shot hundreds of American soldiers of the 116th\non Omaha Beach, stationed at his Widerstandnest 62, with his Czech water-cooled MG, machinegun, from WW I, a booty from the Czechian invasion in 1939.\nHe retired to his home in Santa Barbara, California with his wife Margaret and lived their until his death on the 09-10-1959 at the age of 70. Paul Baade is buried with his wife Magarete Craig, who died age 80 in 1977, on Arlington Cemetery, Virginia USA, Section 7, almost next to Air Corps Lieutenant General., Head of the American Air Force, Frank Andrews\n. Close by in Section 7, the first Allied Airborne Army, U.S. 2* Air Force Lieutenant General, Operation “Market Garden”, Louis Brereton\n, General, Chief of Staff of Sixth Army, George Decker\n, Brigadier General, Commander U.S. Marine Corps, Lemuel Shepherd Jr.\nand General, Vice Chief of Staff of the U.S. Army, John Hull\n. General, Commander 85th Infantry Division, nickname “Custer” , Wade “Ham” Haislip\n, The division had 8.774 casualties in 260 days of combat.","79th Infantry Division (United States)\n|79th Infantry Division|\n79th Infantry Division shoulder sleeve insignia\n|Branch||United States Army|\n|Nickname(s)||\"Cross of Lorraine\" (special designation)|\nWorld War I\n- Activated: August 1917\n- Overseas: July 1918\n- Major operations: Meuse-Argonne\n- Casualties: Total-6,874 (KIA-1,151 ; WIA-5,723)\n- Commanders: Maj. Gen. Joseph E. Kuhn (25 August 1917), Brig. Gen. W. J. Nicholson (26 November 1917), Maj. Gen. Joseph E. Kuhn (17 February 1918), Maj. Gen. Joseph E. Kuhn (16 April 1918), Brig. Gen. W. J. Nicholson (22 May 1918), Maj. Gen. Joseph E. Kuhn (8 June 1918), Brig. Gen. W. J. Nicholson (28 June 1918), Maj. Gen. Joseph E. Kuhn (31 December 1918)\n- Returned to U.S.: May 1919\n- Inactivated: June 1919\nThe division was first activated at Camp Meade, MD in August 1917, composed primarily of draftees from Maryland and Pennsylvania. After a year of training the division sailed overseas in July 1918. The 79th Division saw extensive combat in the Meuse-Argonne Offensive area where it earned the name of \"Cross of Lorraine\" for their defense of France. The division was inactivated June 1919 and returned to the United States.\nThroughout its entire World War I campaign, the division suffered 6,874 casualties with 1,151 killed and 5,723 wounded. Private Henry Gunther, the last American soldier to be killed in action during World War I, served with the 313th Infantry Regiment of the 79th Division.\nWorld War II\n- Activated: 15 June 1942 at Camp Pickett, Virginia\n- Overseas: 7 April 1944\n- Campaigns: Normandy, Northern France, Rhineland, Ardennes-Alsace, Central Europe\n- Days of combat: 248\n- Distinguished Unit Citations: 8\n- Awards: Medal of Honor-3 ; Distinguished Service Cross (United States)-13 ; Distinguished Service Medal (United States)-1 ; Silver Star-962; Legion of Merit-11 ; Soldier's Medal-27 ; Bronze Star-4,916 ; Air Medal-78\n- Commanders: Maj. Gen. Ira T. Wyche (June 1942 – May 1945), Brig. Gen. Leroy H. Watson (May–July 1945), Maj. Gen. Anthony C. McAuliffe (July–August 1945), Brig. Gen. Leroy H. Watson (August 1945 to inactivation).\n- Returned to U.S.: 10 December 1945.\n- Inactivated: 20 December 1945.\n- Reactivated: (Organized Reserve division 29 November 1946).\nThe division was activated at Camp Pickett, Virginia on June 15, 1942. It participated in the Tennessee Maneuver Area, after which it moved to Camp Laguna near Yuma, Arizona, where it trained in the desert. It was then ordered to Camp Phillips, Kansas for training in winter conditions. At the beginning of April 1944, the division reported to the Port of Embarkation at Camp Myles Standish, Massachusetts.\nThe division arrived in Liverpool on April 17 and began training in amphibious operations. After training in the United Kingdom from 17 April 1944, the 79th Infantry Division landed on Utah Beach, Normandy, 12–14 June and entered combat 19 June 1944, with an attack on the high ground west and northwest of Valognes and high ground south of Cherbourg. The division took Fort du Roule after a heavy engagement and entered Cherbourg, 25 June. It held a defensive line at the Ollonde River until 2 July 1944 and then returned to the offensive, taking La Haye du Puits in house-to-house fighting, 8 July. On 26 July, the 79th attacked across the Ay River, took Lessay, crossed the Sarthe River and entered Le Mans, 8 August, meeting only light resistance. The advance continued across the Seine, 19 August. Heavy German counterattacks were repelled, 22–27 August, and the division reached the Therain River, 31 August. Moving swiftly to the Franco-Belgian frontier near St. Amand (east of Lille), the division was then moved to XV Corps in eastern France, where it encountered heavy resistance in taking Charmes in street fighting, 12 September. The 79th cut across the Moselle and Meurthe Rivers, 13–23 September, cleared the Forêt de Parroy in a severe engagement, 28 September–9 October, and attacked to gain high ground east of Emberménil, 14–23 October, when it was relieved, 24 October.\nAfter rest and training at Lunéville, the division returned to combat with an attack from the MignevineMontiguy area, 13 November 1944, which carried it across the Vezouse and Moder Rivers, 18 November–10 December, through Haguenau in spite of determined enemy resistance, and into the Siegfried Line, 17–20 December. The division held a defensive line along the Lauter River, at Wissembourg from 20 December 1944 until 2 January 1945, when it withdrew to Maginot Line defenses. The German attempt to establish a bridgehead west of the Rhine at Gambsheim resulted in furious fighting. The 79th beat off German attacks at Hatten and Rittershoffen in an 11-day battle before withdrawing to new defensive positions south of Haguenau on the Moder River, 19 January 1945. The division remained on the defensive along the Moder until 6 February 1945. During February and March 1945, the division mopped up German resistance, returned to offensive combat, 24 March 1945, crossed the Rhine, drove across the Rhine-Herne Canal, 7 April, secured the north bank of the Ruhr and took part in clearing the Ruhr Pocket until 13 April. The division then went on occupation duty, in the Dortmund, Sudetenland, and Bavarian areas successively, until its return to the United States and inactivation.\nThroughout its 248 days of the World War II campaign, the division suffered 15,203 total casualties, with 10,971 wounded and 14,875 non-battle injuries. Three soldiers from this division were awarded the Medal of Honor. The division took 35,466 prisoners of war.\nAssignments in European Theater of Operations\n- 18 April 1944: VIII Corps, Third Army.\n- 29 May 1944: Third Army but attached to VII Corps, First Army.\n- 30 June 1944: Third Army, but attached to First Army.\n- 1 July 1944: VIII Corps.\n- 1 August 1944: VIII Corps, Third Army, 12th Army Group.\n- 8 August 1944: XV Corps.\n- 24 August 1944: XV Corps, Third Army, 12th Army Group, but attached to First Army.\n- 26 August 1944: XV Corps, First Army, 12th Army Group.\n- 29 August 1944: XII Corps.\n- 7 September 1944: XV Corps, Third Army, 12th Army Group.\n- 29 September 1944: Third Army, 12th Army Group, but attached to the XV Corps, Seventh Army, 6th Army Group.\n- 25 November 1944: XV Corps, Seventh Army, 6th Army Group.\n- 5 December 1944: VI Corps.\n- 6 February 1945: Seventh Army, 6th Army Group.\n- 17 February 1945: Seventh Army, 6th Army Group, but attached to the XVI Corps, Ninth Army, 12th Army Group.\n- 1 March 1945: XIII Corps.\n- 7 March 1945: XVI Corps.\n- 7 April 1945: XVI Corps, Ninth Army, 12th Army Group.\nThe 79th Infantry Division is now the 79th Sustainment Support Command (SSC) headquartered at Joint Forces Training Base (JFTB) Los Alamitos, California. The 79th SSC was officially activated on December 1, 2009 with the mission of providing trained, ready, cohesive, well-led sustainment units for world-wide deployment to meet the U.S. Army’s rotational and contingency mission requirements in support of the National Military Strategy. The 79th SSC is the higher headquarters of over 20,000 U.S. Army Reserve sustainment soldiers organized into over 200 units dispersed throughout the western half of the United States. Major subordinate commands of the 79th SSC include the 4th Sustainment Command (Expeditionary) (ESC) in San Antonio, Texas, the 311th ESC in Los Angeles, California, the 364th ESC in Marysville, Washington, and the 451st ESC in Wichita, Kansas. As the operational command posts of a theater sustainment command – the ESCs plan, coordinate synchronize, monitor, and control operational- level sustainment operations for Army service component commands, joint task forces and joint forces commands throughout the world.\n- Reactivated: 1 December 2009\n- Maj. Gen. William D. Frink, Jr. (1 December 2009 – 8 February 2013)\n- Maj. Gen. Megan P. Tatu (9 February 2013 – 4 December 2015)\n- Maj. Gen. Mark Palzer (5 December 2015 - Present).\n- Nickname: Cross of Lorraine Division.\n- Shoulder patch: White bordered blue shield on which is superimposed a cross of Lorraine.\nThis article incorporates public domain material from the United States Army Center of Military History document \"The Army Almanac: A Book of Facts Concerning the Army of the United States U.S. Government Printing Office, 1950\"."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:d69fa31e-8aba-4066-b4aa-447166a68f01>","<urn:uuid:8179b18e-736c-4cf4-9fd8-405de6b5357e>"],"error":null}
{"question":"What are the similarities and differences between the pathophysiology of anterior cerebral artery occlusion versus middle cerebral artery occlusion?","answer":"The anterior and middle cerebral artery occlusions have distinct characteristics. The middle cerebral artery occlusion typically presents with hemiplegia and unilateral anesthesia on the opposite side of the body, and in the dominant hemisphere can cause total sensorimotor aphasia. The anterior cerebral artery occlusion characteristically causes paralysis of the opposite foot and leg, with a lesser degree of arm paresis, and can lead to urinary incontinence. While middle cerebral artery occlusions are common and can cause severe deficits, anterior cerebral artery occlusions are rare and usually well-compensated by collateral blood flow. However, both conditions share the fundamental pathophysiological mechanism where blood flow drops below critical thresholds (18 ml/100g/min for synaptic failure and 8 ml/100g/min for cell death), leading to energy failure, ionic imbalance, and potential cell death in affected regions.","context":["Middle cerebral artery occlusion syndromes\nBranch of the middle cerebral artery in the brain cortex arterial blood supply side surface of the cerebral hemispheres, with the exception of the frontal pole, strip, located along the border verhnevnutrenney frontal lobe, which are supplied with arterial blood, anterior cerebral artery, and the lower temporal gyrus related to the basin of the posterior cerebral artery.\nMiddle cerebral artery supplies arterial blood following brain regions:\n- cortex and white matter of the brain from the side and bottom of the frontal lobe\n- motor cortex (fields 4 and 6, the cortical centers of sight, motor speech center in the dominant hemisphere Broca's area)\n- cortex and white matter of the brain side of the parietal lobe (sensory cortex, angular gyrus and supramarginal)\n- lateral and upper parts of the temporal lobe and the islet brain\nPerforating branches of the middle cerebral artery supplying arterial blood following brain regions:\n- outer division of the globus pallidus of the brain\n- posterior thigh of the internal capsule below the plane that intersects the upper boundary of the globus pallidus\n- adjacent part of the radiate crown\n- body of the caudate nucleus\n- upper and lateral segments of the head of the caudate nucleus\nEmbolism and thrombosis, most often exposed to middle cerebral artery basin vessels. With complete occlusion of the lumen (occlusion) blocked artery trunk as perforating branches to the deep white and gray matter of the brain, and large branches to the surface of the cerebral cortex. The classic picture of the lesion is characterized by hemiplegia (paralysis of half of the body) and unilateral anesthesia (loss of sensitivity than half of the body) on the opposite side of the body.\nWith involvement in the pathological process in the dominant hemisphere stroke patient's brain, there is also a total sensorimotor aphasia (impairment of perception and reproduction of speech). In cases of non-dominant hemisphere lesions of the brain clinical symptom complex is complemented apraktoagnoziey and anosognosia (inability to understand or recognize objects). If the patient has dysarthria (impairment proper motion of the vocal apparatus during speech playback), then dysphasia (impairment of speech perception and reproduction) is observed.\nVisible neurologic manifestations of lesions of the middle cerebral artery basin often noted at occlusion (occlusion) of the lumen of this artery trunk embolus. To establish an effective collateral blood circulation of the cerebral cortex at a specific time, but it leads to the development of partial occlusion of the lumen of syndromes in the trunk of the middle cerebral artery on the background of its atherosclerosis with subsequent thrombosis.\nPartial neurological syndromes are also found at middle cerebral artery basin embolism. Embolus can get into the trunk of the artery, to spread further upward (distally), penetrate into the overlying (distal) branch and subject to further dilution (lysis). In accordance with the process objective and subjective neurological symptoms varies. Certain syndromes are diseases caused by embolic occlusion (occlusion) of one of the branches include:\n- weakness in the wrist\n- only weakness of the upper extremity (shoulder syndrome)\n- lesion mimic muscles with motor aphasia and weakness in his hand or without it (frontal opercular syndrome)\nSensory impairments, weakness in the limbs and motor aphasia suggests embolism at the site of exit from the upper branches of the division. When re-aphasia without paresis, probably a lesion in the lower division of the middle cerebral artery, as coming from a branch supply blood to the rear area of ??the dominant hemisphere sensory cortex. The sudden appearance of the difficulties caused by ignoring half of the body and spatial agnosia, with no evidence of paralysis of the lower division of the damage of the middle cerebral artery non-dominant hemisphere of the brain of the patient.\nMiddle cerebral artery syndrome:\nSigns and symptoms\n|Paralysis of the contralateral face, arm, and leg; sensory impairment over the same area (pinprick, cotton touch, vibration, position, two-point discrimination, stereognosis, tactile localization, barognosis, cutaneographia)||Somatic motor area for face and arm and the fibers descending from the leg area to enter the corona radiata and corresponding somatic sensory system|\n|Motor aphasia||Motor speech area of the dominant hemisphere|\n|Central aphasia, word deafness, anomia, jargon speech, sensory agraphia, acalculia, alexia, finger agnosia, right-left confusion (the last four comprise the Gerstmann syndrome)||Central, suprasylvian speech area and parietooccipital cortex of the dominant hemisphere|\n|Conduction aphasia||Central speech area (parietal operculum)|\n|Apractognosia of the nondominant hemisphere, anosognosia, hemiasomatognosia, unilateral neglect, agnosia for the left half of external space, dressing “apraxia,” constructional “apraxia,” distortion of visual coordinates, inaccurate localization in the half field, impaired ability to judge distance, upside-down reading, visual illusions (e.g., it may appear that another person walks through a table)||Nondominant parietal lobe (area corresponding to speech area in dominant hemisphere); loss of topographic memory is usually due to a nondominant lesion, occasionally to a dominant one|\n|Homonymous hemianopia (often homonymous inferior quadrantanopia)||Optic radiation deep to second temporal convolution|\n|Paralysis of conjugate gaze to the opposite side||Frontal contraversive eye field or projecting fibers|\nAnterior cerebral artery occlusion syndromes\nAnterior cerebral artery of the brain has two segments:\n- pre-communicating (A1 ) segment of the circle of Willis, or stem, the segment that connects the internal carotid artery from the anterior communicating artery\n- post-communicating (A2) segment, originating from the junction of A1 segment of the anterior communicating artery\nA2 segment of anterior cerebral artery cortical branches through their supplies blood 2/3medialnoy front surface of the orbital part of the frontal lobe, frontal lobe pole, strip the bark along the border and verhnesredinnoy 2/3mozolistogo front of the body. On the other hand, the A1 segment of anterior cerebral artery gives a lot of deep penetrating branches, going mainly to the anterior thigh of the internal capsule, anterior perforated substance, almond-shaped body, the anterior hypothalamus and the lower part of the head of the caudate nucleus of the brain.\nCerebral infarcts (strokes) in a pool of anterior cerebral artery are rare. Blockage (occlusion) of the trunk or A1 segment of anterior cerebral artery is usually well compensated by the possibility of collateral blood flow from the opposite side. The most serious impairments arise in cases where both anterior cerebral artery originated from a single trunk (in case of congenital anatomical features of the structure of the patient), blockage (occlusion), which leads to a massive heart attack in the basins of the anterior cerebral arteries of both hemispheres of the brain.\nClinical manifestations of blockage (occlusion) in a pool of both anterior cerebral arteries include bilateral pyramidal disorders with paraplegia (paralysis of the left and right halves of the body) and marked changes in the psyche in connection with bilateral lesions of the frontal lobes of the brain.\nAnterior cerebral artery syndrome:\nSigns and symptoms\n|Paralysis of opposite foot and leg||Motor leg area|\n|A lesser degree of paresis of opposite arm||Arm area of cortex or fibers descending to corona radiata|\n|Cortical sensory loss over toes, foot, and leg||Sensory area for foot and leg|\n|Urinary incontinence||Sensorimotor area in paracentral lobule|\n|Contralateral grasp reflex, sucking reflex, gegenhalten (paratonic rigidity||Medial surface of the posterior frontal lobe; likely supplemental motor area|\n|Abulia (akinetic mutism), slowness, delay, intermittent interruption, lack of spontaneity, whispering, reflex distraction to sights and sounds||Uncertain localization—probably cingulate gyrus and medial inferior portion of frontal, parietal, and temporal lobes|\n|Impairment of gait and stance (gait apraxia)||Frontal cortex near leg motor area|\n|Dyspraxia of left limbs, tactile aphasia in left limbs||Corpus callosum|\nThe vascular plexus of anterior artery occlusion syndromes\nAnterior artery of the choroid plexus of the brain begins from the internal carotid artery supplies blood back hip of the internal capsule and white matter of the brain from the side and behind of it, through which the portion of the optic fibers from the lateral geniculate body to the calcarine sulcus. This area of the brain supplied with blood of:\n- penetrating vessels going from the trunk of the middle cerebral artery (artery of lenticular nucleus and the striatum)\n- penetrating branches of the posterior communicating artery\n- posterior artery of the choroid plexus\nTherefore, the full clinical syndrome of contralateral hemiplegia in the form of (paralysis of the muscles on the opposite side), hemianesthesia (hypoesthesia) and gomonimnoy hemianopsia (loss of half the visual field on the affected side of the brain) can not develop. Instead, there are syndromes with a minimum severity of neurological focal disorders.\nIndeed, in cases of surgical occlusion of the anterior artery of the choroid plexus for the treatment of Parkinson's disease in some patients does not show signs of lack of blood circulation in the area of ??its basin. Patients who originally observed the detailed clinical symptoms often recover completely or partially, apparently thanks to a sufficient level of collateral blood flow in this part of the brain.\nInternal carotid artery occlusion syndromes\nThe clinical picture of occlusion of the internal carotid artery of the brain varies depending on what is the cause of ischemia: spreading thrombosis, embolism, or low blood flow. Occlusion (blockage) of the internal carotid artery may be asymptomatic. Extensive cerebral infarction (stroke) with involvement of deep gray and white matter, cortical surface develops rarely, if thrombus occluding the lumen extends to the internal carotid artery and penetrates into the trunk of the middle cerebral artery and anterior cerebral artery or if the fragment is separated thrombus embolism entails a high or anterior cerebral artery.\nSymptoms identical to those in the occlusion of the middle cerebral artery trunk. At the same time engaging in the pathological process of anterior and middle cerebral arteries hemiplegia, aphasia or unilateral anesthesia and is often accompanied by anosognosia stupor. When the posterior cerebral artery comes from the internal carotid artery (fetal posterior cerebral artery ), it can also be clogged by the mechanisms described above, which is accompanied by symptoms of damage her basin.\nWhen symptomatic lesions of atherosclerosis with thrombosis of the internal carotid artery, regardless of the cause of cerebral ischemia in the basin often suffers from a region of the middle cerebral artery supplied with blood. Cerebral infarction (stroke) due to low blood flow, often localized in the distal cortical branches of the basin of the middle cerebral artery, leading to the development of transient or gradually increasing weakness in the muscles of the pelvic and shoulder girdles and upper limbs. Sometimes there are episodes of transient cerebral ischemia, accompanied by dysphasia (impairment of the understanding or reproduction of speech) or hemiparesis (muscle weakness, half of the body) with a duration of 10-15 minutes, followed by regression of neurological symptoms. The maximum number of similar episodes of transient ischemic attack (TIA) patients per day up to 5-10.\nWith the involvement of the dominant hemisphere of the brain in transient ischemic attack (TIA), one can observe transient aphasia or dyscalculia (impairment of the account). With involvement of the non-dominant hemisphere of the brain may occur transient neglect of the body. With the damage of the lower division of the middle cerebral artery of the dominant hemisphere aphasic The expressed frustration with the fugitive jargon, with the written and spoken language the patient can not understand (Wernicke aphasia). Even arterioarterialnyh embolism neurological symptoms is often transient in nature due to the fact that emboli can cause an incomplete occlusion of the trunk or branches of the middle cerebral artery, or be subjected to lysis (dissolution) and transported to the distal (above the blockage of the lumen) direction.\nIn most cases, it turns out that if the neurological symptoms of cerebral ischemia when held long enough, but less than 24 hours, then it is due to embolism of the artery. In those cases where the neurological symptoms of cerebral ischemia is transient (transient) in nature and is retained only a few seconds or minutes, it is usually difficult to differentiate between embolic and hemodynamic its nature.\nIn addition to the brain, the internal carotid artery supplies blood optic nerve and retina through the ophthalmic artery. Approximately 25% of clinically manifested occlusion (blockage of the lumen) of the internal carotid artery occasionally arises transient blindness in one eye. In the future a high probability of permanent blindness. Describing such an episode, a patient may tell the doctor about a feeling or subsiding and disappearing shadows crossing the field of view or loss of peripheral regions of the visual field. There are also complaints of blurred, blurred vision in the affected eye or in the absence of upper or lower half of the visual field. Most often these symptoms persist for a few minutes. Less commonly, a stroke at the same time celebrate the occlusion (blockage of the lumen), ophthalmic artery or central retinal artery.\nCommon carotid artery occlusion syndromes\nWhen the common carotid artery occlusion can be observed all the neurologic symptoms of occlusion of the internal carotid artery. At the \"pulseless disease\" or syndrome, aortic arch possible occlusion of both common carotid arteries in the places they are released.\nIn the diagnosis of this condition by occlusion of both common carotid arteries are the leading symptoms of the following:\n- the absence of pulsations on carotid and radial artery\n- fainting when rising from a horizontal position\n- recurrent episodes of loss of consciousness\n- pain in the neck\n- transient blindness (one-or two-sided)\n- blurred vision during exercise\n- early cataracts\n- atrophy and retinal pigmentation\n- atrophy of the iris\n- arteriovenous anastomoses peripapillyarnye\n- atrophy of the optic nerves\n- intermittent weakness of the masticatory muscles\nOften there is a partial syndrome of the aortic arch, consisting of various combinations of stenosis and occlusion of the carotid, subclavian and innominate artery.","1.3 Pathophysiology of acute ischemic stroke .1 Mechanisms of ischemia .1 Mechanisms of ischemia\n1.3.2 Cellular pathophysiology\nLow respiratory reserve and complete dependence on aerobic metabolism make the brain tissue particularly vulnerable to a compromised vascular supply to the brain that is called ischemia (Deb et al., 2010). The brain’s response to acute ischemia depends on the severity and duration of compromised vascular supply. It has been suggested that there are different ischemic thresholds for cerebral dysfunction and cell death. When blood flow drops from the normal value of 50 to 55 ml/100 gram/minute to about 18 ml/100 gram/minute, the brain has reached the threshold for synaptic transmission failure, however, these cells have the potential for recovery.\nThen, when blood flow drops to about 8 ml/100 gram/minute, cell death can result (Bandera et al., 2006; Braeuninger and Kleinschnitz, 2009). However, due to the presence of collateral circulation, different degrees of severity can be observed in the affected region of the brain. Consequently, part of the brain parenchyma named the\n“core”, undergoes immediate death, while other parts, the “penumbra”, may be partially injured but still have the potential to recover (Deb et al., 2010).\nOn the cellular level, the local depletion of oxygen or glucose leads to a failure of the mitochondria to produce high-energy phosphate compounds, such as adenosine triphosphate (ATP) that can trigger cell death. Although this energy failure does not immediately precipitate cell death, 5 to10 minutes of complete occlusion can lead to irreversible brain injury, and even a partial occlusion for a prolonged period can cause harmful effects (Karaszewski et al., 2009). Furthermore, as approximately\n70% of the metabolic demand in the brain is due to the Na+/K+ ATPase pump that maintains the ion gradient responsible for neuronal membrane potential, an inadequate energy supply leads to malfunctioning of the ion gradient, which results in a loss of potassium in exchange for sodium, chloride and calcium ions (Lo et al., 2003; Deb et al., 2010). This is accompanied by an inflow of water, resulting in rapid swelling of neurons and glia leading to cytotoxic edema (Kim et al., 2011). An ischemic cascade also stimulates the release of excitatory neurotransmitters in the brain. An uncontrolled release of glutamate in ischemic area, for example, enhances the excitotoxic synaptic transmission that leads to further sodium and calcium ion influxes, which uses the already depleted ATP to maintain a calcium balance, and the disordered activation of protease, lipase, and nuclease enzymes ultimately leading to cell death (Lo et al., 2003; Henson et al., 2010), (Figure 1.1).\nFigure 1.1: Major pathways implicated in ischemic cell death. (After ischemic onset, loss of energy substrates leads to mitochondrial dysfunction and the generation of reactive oxygen species (ROS) and reactive nitrogen species (RNS). Additionally, energy deficits lead to ionic imbalance and excitotoxic glutamate efflux and build up of intracellular calcium. Downstream pathways ultimately include direct free radical damage to membrane lipids, cellular proteins, and deoxyribonucleic acid (DNA)\n(Reprinted by permission from Macmillan Publishers Ltd: [Nat Rev Neurosci] (Lo et al.,), copyright (2003).\nOn the other hand, an ischemic cascade also activates neuroprotective mechanisms as a defence against cell death (Liu et al., 2009). The first protein to be released after ischemia is heat shock protein 70 (HSP70), and its messenger ribonucleic acid (mRNA) is expressed within 1 to 2 hours of ischemia. Studies on animals showed, that the HSP70 inducer is efficacious in limiting the infarct volume, and inhibiting monocyte/macrophage activation (Giffard and Yenari, 2004; Liu et al., 2009).\nOther neuroprotective mechanisms may be activated to compensate the effects of ischemia. Anti-apoptotic B-cell lymphoma 2 (Bcl-2) gene family members suppress the release of sequestered proteins and modulate calcium fluxes (Thomenius et al., 2003). The prion protein may have a neuroprotective effect, it is up-regulated during hypoxia, and inhibits neuronal cell death (Weise et al., 2006). In addition, neurotrophin-3 is the growth factor that is especially essential for the survival and maintenance of neurons, and its expression could play a role in neuronal survival after brain ischemia (Galvin and Oorschot, 2003). Interleukin-10 gene is another neuroprotective mechanism, its expression is elevated in most central nervous system diseases and aids in neuronal and glial cell survival via blocking the effects of pro-inflammatory cytokines and by promoting the expression of cell survival signals (Strle et al., 2001).\n1.4 Classification, clinical diagnosis and syndromes of acute ischemic stroke Acute ischemic stroke classifications are largely based on clinical findings and pathophysiology. The most common schemes that have been developed to classify subtypes are the Trial of Org 10172 in Acute Stroke Treatment (TOAST) and the Oxfordshire Community Stroke Project (OCSP).\nThe TOAST classification system is mainly based on the etiology of the attack and includes five categories (Jackson and Sudlow, 2005; Kirshner, 2009). Moreover, the diagnoses are based on clinical features and on data collected by tests such as brain imaging by computed tomography (CT) or magnetic resonance imaging (MRI), cardiac imaging (echocardiography), duplex imaging of extracranial arteries,\narteriography, and laboratory assessments for a prothrombotic state (Adams et al., 1993); (Table 1.1).\nTable 1.1. TOAST classification scheme of acute ischemic stroke\nSubtype classification criteria Large artery\n- Cortical, cerebellar, or brain stem dysfunction.\n- Cortical, cerebellar, or brain stem lesions > 1.5 cm upon brain imaging.\n- Diagnosis supported by > 50% stenosis of a major brain artery or branch cortical artery upon angiography or duplex imaging.\n- History of TIA in the same vascular territory, and/or exclusion of a cardioembolic source.\nCardioembolism - Cortical, cerebellar, or brain stem dysfunction.\n- Cortical, cerebellar, or brain stem lesions > 1.5 cm upon brain imaging.\n- Identified source of cardioembolism (e.g., AF or valvular disease).\n- Previous TIAs in > 1 vascular territory.\nLacunar - No evidence of cortical dysfunction.\n- Cortical, cerebellar, or brain stem lesions < 1.5 cm upon brain imaging.\n- Less than 50% stenosis of major brain artery or branch cortical artery upon angiography or duplex imaging.\n- Known lacunar syndrome.\n- History of diabetes or hypertension Other\n- Diagnosed nonatherosclerotic vasculopathy, hypercoagulable state, or hematologic disorder.\n- Inability to classify after extensive evaluation.\n- Evidence of ≥ 2 stroke subtypes (e.g., AF and stenosis >\nAbbreviations: AF: Atrial fibrillation; TIA: Transient Ischemic Attack; TOAST: Trial of Org 10172 in Acute Stroke Treatment.\nIn addition, acute ischemic strokes are also categorized according to the OCSP classification system. The OCSP classification depends on the signs and symptoms present at the time of maximal deficit after a stroke attack, and it includes total anterior circulation infarct (TACI), partial anterior circulation infarct (PACI), lacunar infarct (LACI), and posterior circulation infarct (POCI) (Bamford et al., 1991;\nJackson and Sudlow, 2005). Additionally, this classification is a reasonably valid way of predicting the site and size of cerebral infarction, the functional recovery and rates of fatality after an attack. Therefore, it can be used very early after ischemic stroke onset, before the infarct appears on the scan (Bamford et al., 1991); (Table 1.2).\nTable 1.2. OCSP classification scheme of acute ischemic stroke Subtype\nSubtype classification criteria\nTACI - Charaterized by hemiparesis, dysphasia, and homonymous hemianopia.\n- Large cortical MCA infarct or > 50% of the MCA territory plus ACA or PCA territory.\n- Subcortical infarct > 1.5 cm\nPACI - Presentation with 2 of the following: hemiparesis, dysphasia, or homonymous hemianopia.\n- Isolated dysphagia.\n- Cortical MCA infarct < 50% of the MCA territory.\n- Border zone cortical infarct between ACA and MCA or PCA and MCA territories.\nLACI - Pure motor stroke, pure sensory stroke, sensorimotor stroke, or ataxic hemiparesis.\n- Subcortical infarct < 1.5 cm.\nPOCI - Brainstem or cerebellar dysfunction and/or isolated homonymous hemianopia.\n- Cortical infarct in PCA territory.\n- Brainstem or cerebellar infarct.\nAbbreviations: ACA: anterior cerebral artery; LACI: lacunar infarct; MCA: middle cerebral artery;\nOCSP: Oxfordshire Community Stroke Project; PACI: Partial anterior cerebral infarct; PCA: posterior cerebral artery; POCI: posterior circulation infarct; TACI: total anterior circulation infarct.\nIschemic stroke clinical symptoms depend on the area of the brain and the arterial territories affected (Figure 1.2). It is usually present with an acute loss of brain functions; these functions usually involve the area of motor, sensory, language, vision, visuo-spatial perception or consciousness. And the common signs of stroke include: acute hemiparesis or hemiplegia, acute hemisensory loss, complete or partial hemianopia, monocular or binocular visual loss, or diplopia, dysarthria or aphasia, ataxia, vertigo, or nystagmus, and sudden decrease in consciousness (Blumenfeld, 2002).\nFigure 1.2. Major vascular territories of the brain and important anatomic structures.\nAbbreviations: ACA: anterior cerebral artery; MCA: middle cerebral artery; PCA: posterior cerebral artery.\n(Adapted with permission from Blumenfeld HJ. Neuroanatomy through clinical cases. Sunderland [MA]: Sinauer Associates; 2002:375).\nMotor weakness is the most frequent clinical manifestation of ischemic stroke. About two thirds of patients present with uniform hemiparesis involving face, hand, shoulder, foot, and hip. In addition, monoplegia, which occurs in approximately 19%\nof strokes, usually indicates small infarcts of the motor cortex or centrum semiovale.\nIn majority of cases, faciobrachial weakness is caused by superficial middle cerebral artery (MCA) infarcts, and distal hemiparesis indicates cortical involvement (Blumenfeld, 2002). Furthermore, sensory abnormalities are the second most frequent manifestation of stroke that occur in 50% of stroke patients, and involve the hemiface, arm, trunk, and leg. Stroke is the most common cause of pure sensory loss.\nIn addition, cortical strokes typically produce impairment of discriminative sensations with relative preservation of protopathic sensations (Sullivan and Hedman, 2008). Dysarthria occurs in nearly 8.7% of ischemic strokes. Pure dysarthria is frequently associated with cortical lesions, whereas dysarthria with other neurological signs is more frequently caused by pontine involvement (Kumral et al., 2007)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:ae1c7d4f-2273-4b24-b76d-fa22f2e2cb14>","<urn:uuid:0b5554d1-27ca-4e37-bd18-a6694d214092>"],"error":null}
{"question":"What are the environmental benefits of vertical farming compared to traditional agriculture, and how can consumers verify if companies' eco-friendly claims about such practices are genuine?","answer":"Vertical farming offers several environmental advantages over conventional agriculture: it uses 90% less water, requires significantly less space while increasing yield per unit area, and prevents the need for pesticides through enclosed, controlled environments. The system allows for nutrient and water flow reuse, reducing resource consumption. However, to verify companies' eco-friendly claims about such practices, consumers should: look for detailed proof of claims on websites and social media rather than vague terms like 'eco-friendly'; research the parent company's overall sustainability practices; carefully read ingredient lists and labels; and check for legitimate certifications like EU Ecolabel or EU Organic Logo rather than potentially fake approval seals.","context":["How to tell if an ‘eco-friendly’ firm is greenwashing\nWe’re seeing more ‘eco-friendly’ and ‘all natural’ products on our shelves, as consumers demand more sustainable choices.\nBut as companies come under increased pressure to meet their Environmental, Social, and Governance (ESG) goals, how can we tell if some are twisting the truth?\nWhen their sustainable claims are untrue or exaggerated they can be accused of ‘greenwashing’.\nThis basically means they are misleading consumers to make them believe their products or services – or the business itself is sustainable, when that is not the case.\nAre there laws to protect consumers from greenwashing?\nThere are no laws in Ireland that specifically challenge greenwashing.\nHowever, there are more general consumer protections, such as the Consumer Protection Act 2007 (CPA), as Victor Timon, Partner at law firm Lewis Silkin explained.\n“Under the CPA, a seller must not make false claims about any goods or services they are selling,” he said.\nThe CPA now also incorporates the EU Directive on Unfair Commercial Practices as part of that legislation.\n“Under the directive, a commercial practice is unfair if it is misleading and is likely to distort your buying decision,” Mr Timon noted.\nThe Sale of Goods and Supply of Services Act 1980 also includes an implied term that goods will correspond with their description.\nThe Advertising Standards Authority in Ireland (ASAI) also sets rules and issues decisions in respect of false or misleading advertising.\nHowever, Mr Timon pointed out that unlike its UK counterpart, it has no power to enforce those decisions.\nWhat are the penalties for firms that do engage in greenwashing?\nThe Competition and Consumer Protection Commission (CCPC) is charged with policing consumer protection law in Ireland.\nMr Timon said the CCPC can issue compliance notices, apply to court for prohibition orders against traders to prevent certain practices, or ultimately prosecute a trader in court and ask the judge to apply a fine or even imprisonment in certain cases.\nHowever, the CCPC does not get involved in individual cases between traders and consumers, which Mr Timon said must be pursued by the consumer through the courts.\n“Penalties under the CPA for summary judgements (lower courts) are up to €3,000 or a prison term of six months, or both,” Mr Timon said.\n“Convictions on indictment (higher courts) are up to €60,000 or a prison term of 18 months, or both,” he added.\nWhat firms have been accused of greenwashing?\nOne of the most talked about cases is the Volkswagen emissions scandal.\nIn 2015, the US Environmental Protection Agency found that many Volkswagen cars being sold in America had been fitted with a ‘defeat device’.\nMr Timon explained that this was basically a piece of software in diesel engines that could detect when they were being tested, and then changed the performance to make the results look better.\n“Volkswagen was making a big push to sell diesel cars in the US at the time.\n“It subsequently admitted the offence which spilt over into car sales in Europe also – and paid billions of dollars in compensation and fines,” he said.\nSeparately, in 2020, the UK’s Advertising Standards Authority (ASA) upheld a complaint about a Ryanair advertisement, which claimed that the airline had the lowest carbon emissions in Europe.\n“Ryanair has the lowest carbon emissions of any major airline – 66g CO2 for every passenger kilometre flown,” the advert stated.\nHowever, Mr Timon explained that the comparison to other “major airlines” only included four others – and there is no industry definition of what is considered to be a “major airline”.\n“Perhaps the nail in the coffin, was that the figures were based on efficiency rankings from 2011 – of little value for a substantiation claim in 2019,” he said.\nMore generally at the end of 2020, the European Commission and national consumer authorities conducted a sweep of websites across the member states specifically targeting greenwashing.\nMr Timon said in more than half of the cases, the trader did not provide sufficient information for consumers to judge the claim’s accuracy.\n“In 37% of cases, the claim included vague and general statements such as ‘conscious’, ‘eco-friendly’ and ‘sustainable’, which aimed to convey the unsubstantiated impression to consumers that a product had no negative impact on the environment,” Mr Timon explained.\nHe said in 59% of cases the trader had not provided easily accessible evidence to support its claim.\nIn their overall assessment, they believed that in 42% of cases the claims may be false or deceptive – and could therefore potentially amount to an unfair commercial practice under the Unfair Commercial Practices Directive.\nWere any Irish companies fined after the EU sweep last year?\nThree traders in Ireland were approached by the Competition and Consumer Protection Agency in relation to their green claims following the EU wide sweep last year.\nHowever, no penalties were issued.\nA spokesperson for the CCPC said in all three cases, the companies engaged with the CCPC and provided the additional information sought.\nAs a result, the firms carried out a number of actions.\nThese included amendments to their websites and the removal of errors relating to product details.\nThey also amended their websites to provide clarifications and ‘additional substantiation’ regarding environmental claims.\nA spokesperson for the CCPC said they were satisfied with the actions taken by the traders and did not deem that there was a need for enforcement actions to be taken.\nThe CCPC did not provide the name of the companies in question.\n“Due to the nature of the CCPC’s enforcement role and ongoing market surveillance in this area, we are unable to provide any further details relating to the traders in question, at this time,” the spokesperson said.\nThe spokesperson added that the CCPC “regularly” participates in European ‘Consumer Protection Cooperation Network’ (CPC Network) sweeps, which are a set of market surveillance checks that are carried out by the CPC Network across Europe.\nHow can I spot companies that are greenwashing?\nIf you are unsure as to the authenticity of a business or brand’s ‘green claims’, a spokesperson for the Competition and Consumer Protection Commission suggests you follow a number of steps before you buy.\n1 Look for proof of claims\nIf you are unsure about a brand’s ‘green claims’, do some quick research online or via social media to find out more.\nBe wary of businesses with vague or superficial references to sustainability.\nMost genuine brands will want to share details of their ethical and sustainable approach to doing business, so, instead, look for brands with sections of their website or social media pages, dedicated to outlining their ‘green credentials’.\n2 Research the parent company\nAlthough one particular brand or product may prove genuine in terms of their ‘green claims’, it’s important to examine the bigger picture, including any parent company it may be owned by or affiliated with.\nResearch the parent company’s green credentials and look to the overall sustainability lifecycle.\nFor example, a particular brand may use 90% recycled packaging materials, but what about the other processes of the wider business group – such as manufacturing, logistics or sourcing materials?\n3 Don’t judge a brand by its packaging\nWatch out for vague language such as ‘eco-friendly’ or ‘all-natural’, as well as ‘green’ imagery which can be used to convey the product or brand as being sustainable or ethical.\nDon’t judge a brand by its packaging alone.\nBe sure to read the list of ingredients and check the labels thoroughly before you buy to ensure you are satisfied that the claims are genuine.\n4 False certifications\nLook out for fake ‘approved by’ seals or standards marks by institutions that may not even exist.\nIf you’re unsure about a certification seal, do some quick research to find out more before you buy.\nInstead, look out for approved seals such as EU Ecolabel, EU Organic Logo, Fair Trade Certified, and Rainforest Alliance Certified.\nWhat should I do if I suspect a company is greenwashing?\nThe CCPC monitors compliance with consumer protection law on an ongoing basis.\nTheir enforcement work is based on information they obtain through a range of channels including; contacts to their consumer helpline, research, and market surveillance.\n“If a consumer believes that they have been misled about ‘green claims’, we would ask that they contact us so that we can assist them in addressing their issue,” a spokesperson for the CCPC said.\nYou can call the helpline on 01 402 5555 or log on to ccpc.ie for more information.\nAre stronger laws on the way to protect consumers?\nWith the focus growing on sustainability, it is inevitable that specific legislation dealing with green labeling is on the way.\nIn December 2019, the European Commission published the European Green Deal to tackle environmental challenges.\nMr Timon said this will result in an EU regulation requiring manufacturers to be able to prove any statements they make about the sustainability, eco-friendliness, or other “green” attributes of their products.\n“The new regulation has been expected this year – probably to come into force two years after that,” he said.\nIt is expected that this will be followed by specific legislation to empower consumers to pursue greenwashing claims.\nIn the meantime, in respect of a consumers’ general rights, Mr Timon said the ‘New Deal for Consumers’ published by the EU in 2018 contains GDPR type fines for breaches of consumer law.\n“Regulators like the CCPC will have the right to make test purchases and evaluate products against a manufacturer’s or seller’s claims about their performance – including any green credentials,” he said.","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:d3ba5008-ee5e-4a63-96d7-23bb17a42143>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"How many seamounts were discovered during Pacific expeditions since 2017, and what evidence exists about their protected status impact on marine life?","answer":"Since 2017, the number of discovered seamounts increased from 18 to 62 by mid-expedition in 2021. Regarding the impact of protection on marine life, studies show mixed results. Protected seamounts tend to have more species (22 species/sample) and higher biomass (6 kg/sample) compared to shallow unprotected seamounts (9 species/sample; 1 kg/sample). However, 4-9 years after prohibiting bottom trawling, protected seamounts showed lower abundance (1-3 individuals/m2) compared to both trawled (3-5) and natural seamounts (5-18), while species richness was similar to trawled seamounts (46 species/1,270 m2) but lower than natural ones (52).","context":["There may be newly-inspired, future marine biologists and fisheries managers in Nuu-chah-nulth communities thanks to the Pacific Seamounts Expedition 2021 school outreach events that took place on June 24.\n“We were just so lucky to have the expedition take place at a time when the schools were still open; in past years, the expeditions took place during summer break,” said Alison Wale, Capacity Building Coordinator for Uu-a-thluk.\nTwo one-hour video outreach sessions were delivered to elementary, middle and high school students via Zoom. A third, more casual session dubbed the ‘seaside chat,’ was also offered to the general public in the evening.\nA total of 155 youth and 13 educators from School Districts 70 and 84 participated in the events, including students from Kyuquot Elementary Secondary School in remote Kyuquot First Nation territory.\nPacific Seamounts Expeditions have been taking place every year since 2017, with the exception of summer 2020 which saw the expedition cancelled due to COVID-19.\nDuring the expeditions, Fisheries and Oceans Canada (DFO), in partnership with the Canadian Coast Guard (CCG), Ocean Networks Canada, Council of the Haida Nation and the Nuu-chah-nulth Tribal Council (NTC) discovers, explores and maps British Columbia’s seamounts (underwater mountains) and the marine life living in and around them using a submersible camera named BOOTS.\nIn 2017, 18 seamounts were discovered, and this year, by mid-expedition, the number had risen to 62.\nData collected are used to inform future protection measures for species living in the proposed Marine Protected Areas that the CCG vessel John P. Tully travels 150 kilometres to 350 kilometres offshore for researchers to explore.\nHighlights from past expeditions include the discovery of a dense ecosystem of a newly identified glass sponge species that scientists named ‘Spongetopia,’ as well as a rare sighting of a salmon shark (at the ocean surface) scratching itself against a barnacle-encrusted log to rid itself of parasites attached to its fins.\nThis year, researchers discovered what is believed to be a nursery ground for deep sea skates at the top of a seamount, as well as a whale fall (a deceased whale at the bottom of the ocean). They also experienced another rare sighting like they did in 2019.\nWhile descending, BOOTS captured footage of a ghost jellyfish, a species that has only been witnessed by science groups eight times in 30 years.\nSimilar to the ship-to-shore broadcast that was delivered to Uu-a-thluk summer science campers during the 2019 expedition (which Nuu-chah-nulth representatives Joshua Watts and Aline Carrier joined), this year’s school outreach sessions involved a live chat with crew aboard the CCGS John P. Tully, including DFO marine biologists Tammy Norgard (expedition lead) and Dr. Cherisse Du Preez (deep-sea ecology lead).\n“The teachers were thrilled about the fact that the scientists on board the vessel were able to answer the kids’ questions in real time,” Wale said.\nVideo hosts from the various partner organizations who were situated on land facilitated the call, including Sabrina Crowley, Uu-a-thluk Southern Region Biologist, and Jared Dick, Uu-a-thluk Central Region Biologist, who represented the NTC.\nTo prepare for the outreach events, teachers were sent fun and educational pre-learning lessons for their students, and were asked to submit any preliminary questions for the panel of hosts. The remaining questions posed during the events (and there were plenty!) came in live via Zoom’s Q&A feature.\n“What is the most interesting thing you have done or seen while you have been out at sea?”\n“How long can BOOTS stay underwater?”\n“What is your favourite part about your job?”\nThe third question elicited a candid response from DFO’s Du Preez that is sure to have a lasting impact on the youth who were listening.\n“It’s the adventure of discovery. Somebody gives us a paycheque to go to places that have never been explored; to find animals that have never been seen before; to document them – take photos, videos – and get them into a medium that we can share with the public, and ask the public to explore with us.”\n“I probably just described my entire job, but that’s my favourite part,” Du Preez added humbly.\nThe Pacific Seamounts Expedition 2021 school outreach events are available for viewing on YouTube at:\n1. Elementary level\n2. Middle and high school level\n3. General public ‘seaside chat’","Designate a Marine Protected Area and prohibit bottom trawling\nOverall effectiveness category Unknown effectiveness (limited evidence)\nNumber of studies: 3\nBackground information and definitions\nFishing can impact subtidal benthic invertebrates through species removal or habitat damage from fishing gear entering in contact with the seabed (Collie et al. 2000). Mobile fishing gear such as bottom trawls are known to be particularly damaging as they are dragged along/above the seabed, but can be prohibited within an area. Specific areas can be designated as protected, and specific management measures taken to control for impactful activities, such as bottom trawling (Huvenne et al. 2016). Inside protected areas where bottom trawling is prohibited, the threat from bottom trawling to subtidal benthic invertebrates is removed, and previously impacted populations are, in theory, able to recover over time (Hiddink et al. 2017). However, species and populations are still subjected to the effects of other fishing activities allowed (for instance commercial potting or recreational fishing).\nWhen this intervention occurred outside of a marine protected area, evidence has been summarised under “Threat: Biological resource use – Cease or prohibit bottom trawling”. Evidence for other related interventions is summarised under “Threat: Biological resource use”.\nCollie J.S., Hall S.J., Kaiser M.J. & Poiner I.R. (2000) A quantitative analysis of fishing impacts on shelf‐sea benthos. Journal of Animal Ecology, 69, 785–798.\nHiddink J.G., Jennings S., Sciberras M., Szostek C.L., Hughes K.M., Ellis N., Rijnsdorp A.D., McConnaughey R.A., Mazor T., Hilborn R. & Collie J.S. (2017) Global analysis of depletion and recovery of seabed biota after bottom trawling disturbance. Proceedings of the National Academy of Sciences, 114, 8301–8306.\nHuvenne V.A.I., Bett B.J., Masson D.G., Le Bas P. & Wheeler A.J. (2016) Effectiveness of a deep-sea cold-water coral Marine Protected Area, following eight years of fisheries closure. Biological Conservation, 200, 60–69.\nSupporting evidence from individual studies\nA replicated, site comparison study in 1997 of 14 seamounts south of Tasmania, South Pacific Ocean, Australia (Koslow et al. 2001) found that seamounts within a protected area closed to trawling tended to have different invertebrate community composition, more species and higher biomass of invertebrates, compared to shallow unprotected seamounts, but not compared to deep unprotected seamounts, after two years. Results were not tested for statistical significance. Invertebrate community composition appeared typically similar at protected seamounts and deep unprotected seamounts, but different to that of shallow unprotected seamounts (data presented as graphical analyses). Protected seamounts tended to have more invertebrate species (22 species/sample) and biomass (6 kg/sample) compared to shallow unprotected seamounts (9 species/sample; 1 kg/sample) and similar to deep unprotected seamounts (20 species/samples; 7 kg/sample). The low diversity and biomass at shallow unprotected were associated with the loss of coral substrate from intense trawling. In 1995, a protected area was established and closed to trawling. In 1997, invertebrates (including corals) (>25 mm) living on the seamounts inside (6 seamounts; 12 samples) and outside (8 seamounts; 22 samples) the protected area (peaks at approximately 660–1,700 m depths) were sampled using a dredge. Invertebrates were sorted into groups and weighed by groups. Shallow unprotected seamounts were heavily fished, but deep seamounts were only lightly fished.Study and other actions tested\nA randomized, replicated, site comparison study in 1992–1993 in four areas of mixed seabed inside the Great Barrier Reef Marine Park off northern Queensland, Coral Sea, Australia (Burridge et al. 2006) found no difference in the biomass of non-commercial unwanted catch (invertebrates and fish discard) or in the number of ‘common’ and ‘rare’ discard species between areas closed to trawling and adjacent fished areas, seven to eight years after the closure. Data were reported as statistical model results. A 10,000 km2 area of the Great Barrier Reef Marine Park was closed to trawling in 1985. Two surveys were carried out, one in 1992 and one in 1993. During each survey, 25 randomly selected sites were sampled at each of four areas within the marine park, two closed areas, and two fished areas located 10 nm away, using both a benthic dredge and a prawn trawl. A total of 156 dredges (86 in closed areas, 70 in fished areas) and 122 trawls (68 in closed areas, 54 in fished areas) were towed. For each tow, discard species were collected, identified, counted, and weighed from subsamples (amount not specified). Total weight of discard was estimated from the subsamples. Species were either recorded as ‘common’ (found in at least 11 of the 25 sites) or ‘rare’ (found in 10 or fewer sites).Study and other actions tested\nA replicated, site comparison study in 2006 of 25 deep-sea seamounts located south of Tasmania, South Pacific Ocean, Australia (Althaus et al. 2009) found that, four to nine years after prohibiting bottom trawling in marine protected areas, invertebrate community composition was different and abundance lower at protected seamounts compared to trawled and natural (never trawled) seamounts, and diversity and species richness was similar to trawled but lower than at natural seamounts. Community data were reported as graphical analyses and diversity data as diversity indices. Species richness was similar at protected (46 species/1,270 m2) and trawled seamounts (46), but lower than natural seamounts (52). Abundance was lowest at protected (1–3 individuals/m2), compared to trawled seamounts (3–5), and natural seamounts where abundance was the highest (5–18). Species richness, diversity, and abundance were positively related to the cover of habitat-forming corals, which was higher on protected seamounts (3%) than trawled seamounts (0.1%), but lower than on natural seamounts (52%). Invertebrates (including corals) were identified and counted at 25 seamounts from videos transects (up to 4.7 km long, from 1,100 to 1,400 m depth; 38 transects in total). Ten seamounts were located either in continuously trawled areas or in areas where trawling had stopped following establishment of reserves (at some point between 1997 and 2003), and 15 were in never-trawled natural areas. Fishing history of individual seamounts was verified using logbook data from the Australian Fisheries Management Authority.Study and other actions tested"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:65c31725-80d7-40d5-a668-5b7420fc5ab2>","<urn:uuid:fb742932-b61c-4a73-ba27-cc8bb2940b09>"],"error":null}
{"question":"¿Pueden las mediciones eléctricas del cerebro ayudar tanto en cirugías de tumores cerebrales como en el tratamiento de la epilepsia? Please explain the connection between these medical applications.","answer":"Yes, electrical brain measurements are valuable for both brain tumor surgeries and epilepsy treatment. For brain tumors, the Inselspital developed a hybrid probe that uses electrical signals to map motor pathways during surgery, helping surgeons avoid causing paralysis with a remarkably low 3% rate of permanent mobility restrictions. For epilepsy, electrodes are used to measure brain cell activity related to seizures, and these LFP (Local Field Potential) signals could potentially detect and stop impending seizures by injecting electrical current. Both applications rely on measuring and interpreting electrical activity in the brain to improve patient outcomes.","context":["A safety concept for removing brain tumours developed at the Inselspital leads to the best results for patients anywhere in the world, in honour of which it was awarded a prize at this year’s European Congress of Neurosurgery in Madrid.\nOperations close to the brain’s motor centres are common (one in three brain tumours) and risky. If the surgeon has to remove a tumour from this area, incidental damage to motor pathways risks causing the patient to lose the use of an arm or leg. In order to prevent this, in 2014 neurosurgeons at the Inselspital in Bern developed a new safety instrument – the first of its kind in the world – which enables surgeons to operate near motor pathways or centres without endangering the safety of the operation.\nLong-term study proves patient safety\nNeurosurgeons in Bern have already used this method to operate on more than 200 patients, 182 of whom were included in a long-term study. This study showed that the surgical instrument permits enables tumours to be removed from areas close to the brain’s motor centres and pathways at low risk, significantly reducing the chance of long-term paralysis.\nMore specifically, the rate of permanent restrictions in mobility after the operations conducted in Bern was just 3%, one of the lowest in the world. The procedure also excelled at an international level, with Dr Kathleen Seidel and Professor Andreas Raabe receiving the prize for the best paper at the annual meeting of the European Association of Neurosurgical Societies in Madrid on 21 October.\n“Our experiences from a large number of operations led us to develop this new instrument, which has transformed tumour surgery in this critical region,” says Professor Andreas Raabe, Physician-in-Chief of Neurosurgery. “We are very honoured to receive this award, as it reflects the outstanding results we have achieved in brain tumour surgery in Bern with our new hybrid probe.”\nBern’s hybrid probe soon to be used worldwide\nThe new surgical instrument can suction out the tumour tissue while stimulating the motor centres with an electrical radar, which tells the surgeon how close they are to these areas. In order to alert him or her to potential dangers, it uses acoustic signals similar to those found in electronic parking aids. The instrument therefore allows surgeons to operate while receiving constant feedback on their position, resulting in considerable improvements to patient safety.\nStudy author Dr Kathleen Siedel explains the instrument’s practical benefits: “Knowing exactly how close you are to a motor pathway leads to a significant reduction in surgical risk.”\nNeuroscientists at the Inselspital in Bern have dedicated years of their work to improving brain tumour surgery. They teamed up with the German company Inomed to develop the hybrid probe, which can create a risk map in real time. This safe surgical instrument has now been approved by the medical authorities and will soon be in use all around the world.\nProfessor Andreas Raabe, Director and Physician-in-Chief of the Department of Neurosurgery, +41 (0)31 632 35 35, Andreas.Raabe@insel.ch.\nProspective study of continuous dynamic mapping of the corticospinal tract during surgery of motor eloquent intraaxial brain tumors, Kathleen Seidel, Jürgen Beck, Philippe Schucht, Andreas Raabe, Department of Neurosurgery, Inselspital, Bern University Hospital, Bern, Switzerland.\nMonika Kugemann | idw - Informationsdienst Wissenschaft\nNew insight into the brain’s hidden depths: Jena scientists develop minimally-invasive endoscope\n27.11.2018 | Leibniz-Institut für Photonische Technologien e. V.\nNew China and US studies back use of pulse oximeters for assessing blood pressure\n21.11.2018 | University of British Columbia\nResearchers from the University of Basel have reported a new method that allows the physical state of just a few atoms or molecules within a network to be controlled. It is based on the spontaneous self-organization of molecules into extensive networks with pores about one nanometer in size. In the journal ‘small’, the physicists reported on their investigations, which could be of particular importance for the development of new storage devices.\nAround the world, researchers are attempting to shrink data storage devices to achieve as large a storage capacity in as small a space as possible. In almost...\nThe more objects we make \"smart,\" from watches to entire buildings, the greater the need for these devices to store and retrieve massive amounts of data quickly without consuming too much power.\nMillions of new memory cells could be part of a computer chip and provide that speed and energy savings, thanks to the discovery of a previously unobserved...\nWhat if, instead of turning up the thermostat, you could warm up with high-tech, flexible patches sewn into your clothes - while significantly reducing your...\nA widely used diabetes medication combined with an antihypertensive drug specifically inhibits tumor growth – this was discovered by researchers from the University of Basel’s Biozentrum two years ago. In a follow-up study, recently published in “Cell Reports”, the scientists report that this drug cocktail induces cancer cell death by switching off their energy supply.\nThe widely used anti-diabetes drug metformin not only reduces blood sugar but also has an anti-cancer effect. However, the metformin dose commonly used in the...\nA research team from the University of Zurich has developed a new drone that can retract its propeller arms in flight and make itself small to fit through narrow gaps and holes. This is particularly useful when searching for victims of natural disasters.\nInspecting a damaged building after an earthquake or during a fire is exactly the kind of job that human rescuers would like drones to do for them. A flying...\n12.12.2018 | Event News\n10.12.2018 | Event News\n06.12.2018 | Event News\n17.12.2018 | Studies and Analyses\n17.12.2018 | Life Sciences\n17.12.2018 | Power and Electrical Engineering","Tapping the brain orchestra\nJülich, Norge, 19 December 2011 - Researchers at the Norwegian University of Life Sciences (UMB) and Forschungszentrum Jülich in Germany have developed a new method for detailed analyses of electrical activity in the brain. The method, recently published in Neuron, can help doctors and researchers to better interpret brain cell signals. In turn, this may lead to considerable steps forward in terms of interpreting for example EEG measurements, making diagnoses and treatment of various brain illnesses.\nResearchers and doctors have been measuring and interpreting electrical activity generated by brain cells since 1875. Doctors have over the years acquired considerable practical skills in relating signal shapes to different brain illnesses such as epilepsy. However, doctors have so far had little knowledge on how these signals are formed in the network of nerve cells.\n\"Based on methods from physics, mathematics and informatics, as well as computational power from the Stallo supercomputer in Tromsø, we have developed detailed mathematical models revealing the connection between nerve cell activity and the electrical signal recorded by an electrode,\" says Professor Gaute Einevoll at the Department of Mathematical Sciences and Technology (IMT) at UMB.\nMicrophone in a crowd\nThe problem of interpreting electrical signals measured by electrodes in the brain is similar to that of interpreting sound signals measures by a microphone in a crowd of people. Just like people sometimes all talk at once, nerve cells are also sending signals \"on top of each other\".\nThe electrode records the sounds from the whole orchestra of nerve cells surrounding it and there are numerous contributors. One cubic millimetre can contain as many as 100,000 nerve cells.\nTreble and bass\nSimilar to bass and treble in a soundtrack, high and low frequency electrical signals are distinguished in the brain.\n\"This project has focused on the bass - the low frequency signals called \"local field potential\" or simply LFP. We have found that if nerve cells are babbling randomly on top of each other and out of sync, the electrode's reach is narrow so that it can only receive signals from nerve cells less than about 0.3 millimetres away. However, when nerve cells are speaking simultaneously and in sync, the range can be much wider,\" Einevoll says.\nLarge treatment potential\nBetter understanding of the electrical brain signals may directly influence diagnosing and treatment of illnesses such as epilepsy.\n\"Electrodes are already being used to measure brain cell activity related to seizures in epilepsy patients, as well as planning surgical procedures. In the future, LFP signals measured by implanted electrodes could detect an impending epilepsy seizure and stop it by injecting a suitable electrical current,\" Einevoll says.\n\"A similar technique is being used on many Parkinson's patients, who have had electrodes surgically implanted to prevent trembling,\" Researcher Klas Pettersen at UMB adds.\nEinevoll and Pettersen also outline treatment of patients paralysed by spinal cord fracture as another potential area where the method can be used.\n\"When a patient is paralysed, nerve cells in the cerebral cortex continue to send out signals, but the signals do not reach the muscles, and the patient is thus unable to move arms or legs. By monitoring the right nerve cells and forwarding these signals to for example a robot arm, the patient may be able to steer by his or her thoughts alone,\" Einevoll says.\nThe Computational Neuroscience Group at UMB has already established contacts with clinical research groups in the USA and Europe for further research on using the approach in patient treatment.\nThe research team recently published the article \"Modeling the spatial reach of the LFP\" in Neuron. Researchers at Forschungszentrum Jülich in Germany, the site of one of Europe's largest supercomputing centers, contributed the detailed simulation of a piece of brain tissue comprising about 100,000 neurons and 1 billion synapses to provide the LFP model of the Norwegian group with brain activity similar to a living animal.\nThe international interest in this field of study is highlighted by the mobility of the crew. First author Henrik Lindén recently joined the prestigious KTH Royal Institute of Technology in Stockholm, Sweden. Tom Tetzlaff, the second author and Research Fellow of Professor Gaute Einevoll, moved to the partner institute to join Professors Sonja Grün and Markus Diesmann in Juelich.\nThe project is mainly financed by the Research Council of Norway's eScience programme and is an example of the increased importance of computational neuroscience in modern brain research.\nEinevoll was recently appointed one of four new directors of Organization for Computational Neurosciences, and is also co-leader of the Norwegian national node of INCF (International Neuroinformatics Coordinating Facility).\nBoth organisations work to promote the use of methods from informatics, mathematics and physics in brain research.\nModeling the Spatial Reach of the LFP\nHenrik Lindén, Tom Tetzlaff, Tobias C. Potjans, Klas H. Pettersen, Sonja Grün, Markus Diesmann, Gaute T. Einevoll\nNeuron - 8 December 2011 (Vol. 72, Issue 5, pp. 859-872)\nModeling the Spatial Reach of the LFP (Neuron)\nInstitute of Neuroscience and Medicine (INM-6), Forschungszentrum Jülich\nComputational Neuroscience Group am UMB:\nOrganization for Computational Neurosciences:\nInternational Neuroinformatics Coordinating Facility (INCF):\nProf. Markus Diesmann\nForschungszentrum Jülich, Institute of Neuroscience and Medicine (INM-6)\n+49 2461 61-9301\nProf. Gaute Einevoll\nNorwegian University of Life Sciences\n+47 951 24 536"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:ceaad6b9-2b10-4332-8686-50f90c5ef877>","<urn:uuid:378545d6-df0c-42c0-9ba7-c33d86a2bce3>"],"error":null}
{"question":"Please explain what makes ocean plastic dangerous for sea animals - from how it attracts them to how much enters oceans each year?","answer":"Ocean plastic becomes dangerous to sea animals in several ways. First, it develops a coating of algae and microbes that makes it smell like food to marine animals - this has been specifically proven with loggerhead turtles who exhibit foraging behavior when exposed to ocean-soaked plastic. At least 700 species of marine animals can become entangled in plastic or mistake it for food. The scale of the problem is massive - 8 million tonnes of plastics enter the ocean annually, and by 2050, the total mass of ocean plastic will exceed that of fish. Around 80% of ocean plastic originates inland, entering through rivers and streams, often due to poor waste management and industrial activity.","context":["- Plastics smell like food to turtles after they have been in the ocean for a while, a study has found.\n- The build-up of algae on ocean plastic causes the rubbish to give off the aroma of food for loggerhead turtles.\n- The turtles showed no reaction to clean plastic.\nThe amount of plastic pollution in the oceans is rapidly increasing. This is problematic, as at least 700 species of marine animals – including sharks, whales, seabirds and turtles – can become entangled in the stuff or mistake it for a tasty snack.\nWhile we know that some species seem to eat plastic because it looks like jellyfish or some other food source, less research has been carried out into what plastic smells like to marine animals.\nBut now, a study from the University of North Carolina at Chapel Hill has found that the coating of algae and microbes that naturally builds up on ocean plastics causes the rubbish to give off the aroma of food.\nRead more about ocean plastic:\n- Has Blue Planet II had an impact on plastic pollution?\n- Drowning in plastic: can we solve the marine pollution problem?\nThe researchers took 15 captive-reared loggerhead turtles, each around five months old, and placed them in a laboratory aquarium. They then piped in aromas of clean water, clean plastic, turtle food, and plastic that had been soaking in the marine environment for five weeks.\nThe turtles showed no reaction to the odours of clean water or clean plastic. But when they were exposed to the smells of ocean-soaked plastic or turtle food, they exhibited foraging behaviour – like poking their noses out of the water and showing increased activity.\n“This finding is important because it’s the first demonstration that the odour of ocean plastics causes animals to eat them,” said Dr Kenneth J Lohmann, who took part in the study.\n“It’s common to find loggerhead turtles with their digestive systems fully or partially blocked because they’ve eaten plastic materials. There also are increasing reports of sea turtles that have become ill and stranded on the beach due to their ingestion of plastic.”\nAccording to the researchers, areas of the ocean with dense concentrations of plastic may trick turtles and other animals into thinking that there is an abundant food source, when the reverse is true.\n“Once these plastics are in the ocean, we don’t have a good way to remove them or prevent them from smelling like food,” said Lohmann. “The best thing we can do is to keep plastic from getting into the ocean at all.”\nReader Q&A: How does plastic get into the oceans?\nAsked by: Tamsin Nicholson, via email\nAround 80 per cent of the plastic waste found in the oceans today originated inland. Littering, poor waste management and industrial activity can all allow plastic to enter the natural environment.\nA significant proportion of this then blows into rivers and streams, which carry it into the ocean. This is particularly common in countries where waste infrastructure is lacking: an estimated two billion people worldwide don’t have access to solid waste collection.\nOn top of this, wastewater from our homes often contains tiny pieces of plastic, including microbeads from cosmetics (now banned in the UK) and fibres from polyester clothing. Tackling plastic pollution therefore requires individuals, governments and companies across the globe to work together to reduce plastic consumption and waste.\n- Does the plastic debris found in bottled water affect our bodies?\n- Why are some plastics recyclable and others are not?","Originally published by WWF-Indonesia and WWF-Singapore. Written by Sharon Salim.\nLast Sunday, a 9.5-metre sperm whale was found stranded off the coast in Kapota Island, Wakatobi — dead. Cause of death: unknown. But what we do know: a staggering 5.9 kg of plastic items found in its stomach.\nWWF-Indonesia’s Dewi Satriani tell us more about this appalling incident.\nBelow, exclusive information povided by WWF-Indonesia on the timeline and all the things you need to know:\n#1 The whale was found by a local on Sunday evening\nWWF-Indonesia received a report from a community member of Kapota Island in Wakatobi on Sunday evening (Nov 18) informing them about an incident of a dead whale stranded off the coast.\nThe next Monday morning (Nov 19) at 7.30 AM local time, WWF-Indonesia, Wakatobi National Park, Wakatobi Marine and Fisheries Community Academy were deployed to the scene and discovered a whale carcass suspected to be a sperm whale.\n#2 The whale was already in an advanced state of decay\nThis is the reason why it was not possible to conduct a necropsy to investigate the cause of death. In this stage of decay (level 4), the carcass was already releasing an overpowering stench and the body was not intact. A necropsy can only be done if the stranded whale is still in level 2 stage of decay; when the skin still appears tight, emitting no smell, and the eyes still shine.\n#3 Identifying the whale was still possible\nEven though the whale carcass was no longer intact, WWF-Indonesia Marine Species Specialist Dwi Suprapti conducted a photo analysis of the carcass sent by the WWF-Indonesia team and identified the stranded whale as sperm whale (physeter macrocephalus). It has a block-shaped head, narrow jaw and teeth.\nSperm whale is also the largest toothed-whale, unlike other large-sized counterparts that are toothless. The possibility of it being a baleen whale was also ruled out thanks to the absence of prominent, iconic features of baleen like the rostral ridge and ventral grooves. This further confirms the fact that the stranded whale is a sperm whale (and not a blue whale as some news outlets have reported).\n#4 Finding a stranded whale in Wakatobi waters is not common\n“Whales do migrate through Wakatobi waters. In certain seasons we can see many different kinds of whales passing through Wakatobi islands, but stranding is not common. This is the only time we’ve found a stranded whale, which is also (alarming) because we found plastics in its abdomen,” Dewi revealed.\n#5 The locals decided to cut open the abdomen of the dead whale\nThe chunks of black material (left) are plastic raffia strings. A rubber-canvas sandal (right) was among the finds.\nTo be exact: there were 115 of plastic cups (750g), 19 pieces of hard plastic (140g), four plastic bottles (150g), 25 plastic bags (270g), a nylon sack (200g), and more than 1,000 pieces of plastic raffia string (3,260g) which totals to 5.9 kg inside the stomach, according to the identification result conducted by Wakatobi Marine and Fisheries Community Academy.\n“This discovery is deeply upsetting. It is a wake-up call for Indonesia about how plastic pollution is causing irreparable damage to our oceans and marine life. We urge businesses and governments to work together to address this issue urgently to prevent further plastic leakage into our oceans.” said Dwi Suprapti, Marine Species Specialist WWF-Indonesia.\n#6 There was no crowd management or public boundaries during the incidence\nSo as not to interfere or disturb the stranded whale, public members without any Personal Protective Equipment (PPE) including eye and face protection, gloves, and coveralls were advised to remain at a distance. This is a crucial protective measure to avoid any exposure of bacteria, virus or other dangerous microorganism emitted by the mammal and transmissible to human.\n#7 The whale was buried two days after it was discovered\nIt was buried on Tuesday (Nov 20) at Kolowawa Beach, North Kapota in Wakatobi for later retrieval of the bone specimen by the Wakatobi Marine and Fisheries Community Academy.\n#8 Plastic pollution is a transboundary problem that all countries share\nAlthough it remains uncertain whether the plastics were lodging the sperm whale’s ingestion organ or caused infections, we can’t turn a blind eye to how plastics production and pollution are still growing exponentially.\nEvery year, 8 million tonnes of plastics enter the ocean. By 2050, the total mass of ocean plastic will exceed that of fish. Plastic debris kills an estimated 100,000 marine mammals annually, as well as millions of birds and fish.\n#9: Beach clean-ups are not enough to solve the problem\nA big part of marine pollution comes from land sources, including rivers. An effective response to this crisis requires a global systemic change involving businesses and governments — and supported by consumers.\n#10 We cannot solve this issue alone\nKnowing that marine plastic pollution goes beyond Indonesia and the region, the global crisis requires global solutions.\nBusinesses need to take responsibility for the full life cycle of their products and play their part in helping governments deal with this issue."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:246088e1-2d90-4aef-afc8-f864da755b68>","<urn:uuid:d1103ef1-d057-45b2-9e68-b78458d5ce54>"],"error":null}
{"question":"What are the challenges faced by single-parent families in Scotland regarding poverty, and how does climate change impact tourism-dependent economies in the Caribbean that could affect employment opportunities for all families?","answer":"Single-parent families in Scotland face higher risks of child poverty compared to couple parent families. They encounter specific barriers to finding and sustaining employment, and are particularly impacted by Universal Credit, which drains working single parents of approximately £1350 per year on average. Meanwhile, climate change poses severe threats to Caribbean economies, where tourism accounts for a quarter of the total economy and a fifth of all jobs. The region faces challenges including intense tropical cyclones, changing rainfall patterns, sea-level rise, and coastal degradation, which threaten the tourism sector that many families depend on for employment.","context":["This guest blog from John H. McKendrick, Professor of Social Justice at Glasgow Caledonian University and co-director of The Scottish Poverty and Inequality Research Unit (SPIRU), welcomes the Poverty and Inequality Commission’s advice on reducing child poverty, and reflects on parts that are specifically relevant to single parent families.\nThere is much to enthuse those concerned to tackle poverty in the Advice on the Scottish Government’s Child Poverty Delivery Plan 2018, the first such note published by the Poverty and Inequality Commission in February 2018. The Commission has made 40 recommendations, which span general principles for delivery, three substantive priorities for reducing child poverty (work and earnings, social security and housing), and a reminder of the importance of considering the quality of life as lived. It will be interesting to see how much of this advice is acted upon by the Scottish Government as it articulates its first delivery plan in accordance with the Child Poverty (Scotland) Act 2017, published in March this year.\nBack in 1990, I started out on my research career by undertaking postgraduate research at the University of Glasgow, which became my PhD thesis The Quality of Life of a Deprived Population Group: Lone Parents in Strathclyde Region, awarded in 1995. Time has not been kind to the key components of that thesis. The behemoth that was Strathclyde Region was abolished in 1996, while single parents have persisted to be a population that continues to experience more than its fair share of poverty and deprivation.\nThe future may be brighter – for single parents, if not the proponents of two-tier local government. The Scottish Government, through the Child Poverty (Scotland) Act 2017 committed Scottish Ministers to: (i) set out in their Delivery Plans what (if any) measures they proposed to take in relation to children living in single-parent households; and (ii) include in their annual progress report the effect of those measures on reducing the number of children living in single-parent households for each of the four poverty targets.\nThis then, is to be a strategy that aims to tackle single parent poverty, a point that was not lost on the Poverty and Inequality Commission in their Advice Note. In addition to acknowledging the challenges that poverty presents to all parents and the generic actions that would tackle poverty for all in Scotland, the Advice Note gives due consideration to the specific challenges that are faced by single parents. It notes:\n- the higher risk rate of child poverty for those living in single parent, compared to couple parent families (page 8);\n- those expectations of Universal Credit that are particular to single parents (page 15);\n- the barriers to finding and sustaining employment that lone parents face (page 16);\n- the factors that make for a successful employment programme for lone parents (page 17) (the evidence draws on the work of Helen Graham and Ron McQuaid);\n- the evidence from the Resolution Foundation that Universal Credit, on average, drains working single parents of £1350 per year (page 23);\n- the research to which One Parent Families Scotland contributed on school clothing grants (page 34);\n- and that parental separation is one of the household events that can push families into poverty (page 36).\nWhile it would be churlish – if not downright misguided – to suggest that the Poverty and Inequality Commission were not attuned to the challenges of tackling single parent poverty, it is prudent to issue a cautionary note. Recommendation 4 states: “The Delivery plan should focus particularly on a core set of actions that are likely to have the biggest impact on reaching the child poverty targets”. What is most striking about single parent poverty (as with that for families with younger parents, those from BME households and those in which a disabled person resides) is the higher relative risk of poverty for these groups.\nAlthough the number of children in Scotland living in poverty in these households is far from insignificant, there are many more children living in poverty in couple parent households (and families with older parents, non-BME households and those without a disabled person). A watching brief is required to ensure that the search for the “biggest impact” to meet the 2030 targets does not lead to a disproportionate focus on the largest sub-populations, to the point that we lose sight of minority populations and the particularities of the poverty that they experience.\nThe variegated nature of child poverty is also addressed within the Advice. Recommendation 15 might serve as a useful call-to-action to tackle single parent poverty in a nuanced way. It states:\n“The Delivery Plan needs to particularly recognise the barriers that may be faced by those at greatest risk of poverty, including single parents, households with a disabled member and black and minority ethnic households, and consider how it can address the specific needs these households may have. In doing so the Delivery Plan should consider the recommendations made in Addressing Race Inequality in Scotland: The Way Forward and actions to ‘reduce by at least half the employment gap’ between disabled and non-disabled people set out in A Fairer Scotland for Disabled People.”\nSo, how did the Scottish Government respond? In Every Child Every Chance, the first delivery plan for 2018-2022 under the Child Poverty (Scotland) Bill 2017, single parents are identified as one of six ‘priority groups’, i.e. families most at risk of poverty. Significantly, the expected impact on single parents (as a priority group) is specified for each of the key actions that the Scottish Government proposes. At first glance, the Scottish Government seems to have listened to the Poverty and Inequality Commission and seems to be taking tackling single parent poverty seriously.\nHowever, some caution should be noted. First, as is often the case with single parents, there is a need to ensure that, in practice, identification as a ‘priority group’ does not lapse into regarding single parents as a belligerent problem population. More carrot, less stick is what single parents require to achieve their potential. Second, it is interesting to note that the specific references to actions to support single parents only pertain to employment. Acknowledging the particular challenges that single parents face in the world of work is to be welcomed. However, perhaps there is also a need for a single parent specific focus when it comes to the other key drivers of child poverty, i.e. reducing the cost of living and social security. This point is linked to a final cautionary note: the projected impact on single parents of many of the key actions that are proposed are either vaguely specified (“all priority groups should benefit”) or not specified (“we will consider how best this programme can be delivered to all the priority groups”).\nTo summarise: for single parents, as for Scotland as a whole, this is a promising start, but there is much still to do.\nJohn McKendrick is based at the Scottish Poverty and Inequality Research Unit (SPIRU), Glasgow Caledonian University.","Today Minister Bartlett delivered his keynote address at the Latin American and Caribbean Development Bank ( CAF) session, making it official in his speech suggesting:\nWe are Caribbean, we are the Solution:\nTranscript: Hon. Edmund Bartlett speech:\nAs the disruptive impact of climate change is projected to intensify for economies dependent on tourism, particularly in the Caribbean—the most tourism-dependent region in the world — there is widespread acknowledgment that an urgent shift in the values, attitudes, and behaviors of all involved in the tourism chain is imperative.\nThis collective reorientation of purpose is necessary to steer tourism toward a more balanced, resilient, and sustainable trajectory. This vision will be achieved by redressing current practices and trends in the industry that contribute to the imprudent use of limited natural resources and contribute to the misalignment of economic growth with the conservation of both land and ocean and marine ecosystems.\nUltimately, the thrust toward sustainable, resilient, and balanced tourism emphasizes the integration of environmentally sustainable and climate-resilient practices into every facet of the tourism product —from building design, construction, accommodation, and other room services to transportation marketing, recreational activities, energy use, food production, customer service, waste management, maintenance, water supply, and utility consumption.\nThe Caribbean region has been specifically recognized by the United Nations Secretary-General as ground-zero for the global climate change emergency as he emphasized that small island low-lying coastal states in the Caribbean are exceptionally susceptible to what he described as the “most significant challenge confronting our world today”- the climate crisis.\nSimilarly, the UNDP recently projected that the Caribbean will become the world’s most vulnerable tourist destination between 2025 and 2050. This prediction stems from the observation that the impacts of climate change and global warming will continue to produce dire consequences for the fragile and undiversified economies of the Caribbean.\nIndeed, although Latin America and the Caribbean account for only 10 percent of global greenhouse gas emissions, the region is at the forefront of global efforts to tackle climate change due to its disproportionate impact which includes a higher occurrence of the most intense tropical cyclones (TCs), storm surges, droughts, changing rainfall patterns, sea-level rise (SLR), warmer temperatures, biodiversity loss, flooding, saline intrusion into aquifers, food and water insecurity, beach erosion coastal degradation, mangrove loss, coral bleaching, and the growth of invasive species.\nClimate change constitutes a major threat to coastal and marine tourism which is the backbone of Caribbean countries, accounting for a quarter of the total economy, and a fifth of all jobs.\nAdmittedly, the relationship between tourism and the environment in the Caribbean is complex since the tourism industry presents both a challenge and an opportunity for achieving environmental sustainability. Healthy marine and coastal systems are critical assets to the region’s tourism competitiveness.\nThe region’s tourism product that has been traditionally built around the sun, sea, and sand” concept relies on the environmental resources or natural endowments of the region to attract international travelers.\nThis is against the backdrop that coastal and marine tourism is the largest economic sector in the Caribbean with over 80 percent of tourism occurring along coastal towns and cities. Healthy coastal and marine ecosystems also serve as vital sources of food, income, trade and shipping, minerals, energy, water supply, recreation, and tourism for these small island economies.\nThe coral reef-mangrove-seagrass complex also brings increased safety to coastal communities and infrastructure such as hotels and resorts as these systems act as a natural barrier, decreasing the impact of floods and storms.\nOn the other hand, marine and coastal ecosystems are also significantly threatened by tourism development.\nWEF estimated that the global tourism industry is responsible for a staggering 8% of all global greenhouse gas (GHG) emissions, including flights, hotel construction and operation, air conditioning and heating, and land and maritime transport.\nThe areas that attract tourists have been coming under increasing pressure from the damage and pollution caused by tourist facilities and the supporting infrastructure.\nAt the same time, the impacts of climate change, overfishing, other unsustainable practices, and even some marine tourism activities damage marine ecosystems such as coral reefs which are vital for maintaining ecological diversity and regulating climate.\nThe United Nations has estimated the cost of reduced tourism due to coral bleaching at $12 billion annually.\nGiven the context outlined, there is now a greater imperative among Caribbean tourist destinations to improve the management of vital marine and ocean ecosystems.\nThis can be achieved through the adoption of the blue economy pathway.\nThe World Bank defines the Blue Economy as “the sustainable use of ocean resources for economic growth, improved livelihoods and jobs, and ocean ecosystem health.”\nThis definition imposes moral responsibility on all industries, especially those that significantly harness or exploit ocean and marine resources in their value chains, to make greater efforts to protect fragile and gradually depleting ocean and marine systems that have become increasingly susceptible to man-made phenomena such as ocean pollution, shipping and transport, dredging, offshore drilling, deep-sea mining, over-fishing and the degradation of coastal and marine ecosystems linked to sea level rise/global warming.\nCaribbean tourist destinations can take the lead in global efforts that champion the blue economy and climatic resilience.\nThey are uniquely positioned to achieve value creation through differentiation and diversification of their tourism product since the region offers significant opportunities for the development of potentially lucrative niche tourism segments that balance environmental sustainability and ecological conservation with economic development.\nThese include health and wellness, medical, culture and heritage, eco-tourism, and wildlife or nature tourism.\nThe creative and cultural industries that Caribbean destinations are renowned for can be leveraged to not only bolster the regional economy but also to reduce the carbon footprint associated with long-distance imports.\nThere are also significant opportunities for Caribbean tourist entities to incorporate sustainable energy sources that are naturally available in the region such as solar power, wind, geothermal, or biomass into tourism infrastructure to reduce the sector’s reliance on fossil fuels, contributing to a more climate-resilient energy framework.\nMany Caribbean tourist destinations have already been at the forefront of implementing solutions that actively support the preservation and health of ocean and marine ecosystems.\nThrough a multifaceted approach, some destinations have championed initiatives such as coral reef restoration projects and mangrove conservation efforts.\nPartnerships with local communities, governmental bodies, and conservation organizations have led to the establishment of marine protected areas, fostering safe havens for marine life to thrive.\nCollaborations with local communities and NGOs have led to beach clean-up campaigns and waste management programs, significantly reducing marine debris and pollution.\nFurthermore, education and awareness initiatives integrated into tourism experiences have raised visitors’ consciousness about the importance of preserving these ecosystems, encouraging responsible behavior, and fostering a deeper appreciation for the oceans’ health and biodiversity.\nEncouraging sustainable diving and snorkeling practices, promoting responsible tourism guidelines that safeguard delicate marine habitats, and advocating for reduced plastic usage have also been integral parts of their strategies.\nIn Jamaica, the government’s ban on single-use plastic bags, straws, and polystyrene has set a precedent for responsible environmental stewardship that has positively influenced conservationist attitudes in the tourism industry.\nIn closing, I want to reiterate that preserving the marine and ocean ecosystems and fortifying resilience against climate change is not just a choice for Caribbean destinations—it is a priority.\nThese actions safeguard not only the region’s natural assets but also the livelihoods and cultural heritage intertwined with these ecosystems. By taking proactive measures and fostering sustainability, Caribbean destinations can pave the way for a resilient future, inspiring global efforts toward environmental conservation.\nThe significance of these endeavors extends far beyond local borders, shaping a world where harmony between humans and nature is paramount for a sustainable tomorrow.\nWhat is COP28?\nThe 2023 United Nations Climate Change Conference or Conference of the Parties of the UNFCCC, more commonly referred to as COP28, is the 28th United Nations Climate Change Conference, being held from 30 November until 12 December 2023 at Expo City, Dubai."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:51ebd918-1f88-4fa6-9d03-ed4a6c0027a7>","<urn:uuid:ac4176ec-39c0-441c-8623-5eb02b2099e0>"],"error":null}
{"question":"How do seasonal changes affect social grouping patterns in impala compared to wildebeest?","answer":"Impala naturally form distinct social groups during the wet season, with three main organizations: territorial males with/without breeding females, bachelor herds, and breeding herds of females and juveniles. During dry season, males can be found together or mixed with female herds. In contrast, according to observations in Ithala Game Reserve, wildebeest show atypical behavior by increasing their group size during the dry season. Both species maintain spatial sexual segregation through territorial behavior - impala bulls during the rut and wildebeest bulls almost year-round. Outside these periods, social sexual segregation relates to differing activity budgets.","context":["The impala is found from northeast South Africa to Angola, south Zaire, Rwanda, Uganda,and Kenya. (Wilson and Reeder, eds, 1993)\nThe impala is found in woodland which contains little undergrowth and low to medium height grassland. Also a close source of water is desired, however is not needed when there is abundance of grass. (Estes, 1991)\nImpala are sexually dimorphic. In this species only the males have S shaped horns that are 45 to 91.7 cm long. These horns are heavily ridged, thin, and the tips lie far apart. Both sexes are similarly colored with red-brown hair which pales on the sides. The underside of the belly, chin, lips, inside ears, the line over the eye, and tail are white. There are black stripes down the tail, foreheard, both thighs, and eartips. These black stripes might aid in recognition between individuals. (Estes, 1991; Jarman, 1979)also have scent glands on their rear feet beneath patches of black hair as well as sebaceous glands on the forehead.\nMales test the females' urine to detect estrous. The male then roars, snorts, or low stretches to advertise himself. After chasing the female, the male may show behaviors such as nodding and tongue flicking before copulation. (Eltringham, 1979; Estes, 1991; Jarman, 1979)\nFemale impalas are reproductively mature and conceive at 1.5 years. Males have the ability to breed at age 1, but often do not establish territories until age 4. Most breeding occurs in March through May. Gestation is 194-200 days. (Eltringham, 1979; Estes, 1991; Jarman, 1979)\nThe female impalas isolate themselves before calving. Calving usually occurs in the midday. Usually there is only one calf. The mother and calf will rejoin the herd after 1-2 days. Impalas place the young in creches which are groups of young that play, groom, and move together. Young impala are weaned at 4.5 months. (Eltringham, 1979; Estes, 1991; Jarman, 1979)\nImpala are diurnal and spend the night ruminating and lying down. The peak activity times for social activity and herd movement are shortly after dawn and before dusk.\nImpala have different social structures depending on the season. The average size of the female herd is between 15-100 individuals depending on space available. Females live in clans within a home range of 80-180 ha. During the wet season the ranges are heavily defended, but during the dry season there is much overlap between individuals in the clan and even between different clans. There are slight differences between behavior in southern and eastern impala. Southern impala are more likely to intermix during the dry season, while eastern impala will remain more territrorial during the dry season.\nImpala form distinct social groups during the wet season. Three main organizations are found: territorial males with and without breeding females, bachelor herds of non-territorial adult and juvenile males, and breeding herds of females and juveniles (including young males less than 4 years). During the dry season, males can be found together or mixed with female herds.\nThe male impala changes its territory to match the season. During the breeding season the male keeps a much smaller territory which is heavily defended. The males will imprint on their original territory and always come back to that same territory to declare dominance.\nThe male impala uses a variety of techniques to defend its territory (including keeping females). Tail-raising, forehead marking, forehead rubbing, herding, chsing, erect posture, fighting, and roaring are used. (Estes, 1991; Jarman, 1979)\nImpala are ruminants. The upper incisors and canines are absent and the cheek teeth are folded and sharply ridged. Impala are intermediate feeders. While predominately a grazer, the impala will adapt to any amount of grass and browse. Impala feed mostly on grass during times of lush growth following the rains and will switch to browse during the dry season. (Estes, 1991; Jarman, 1979)\nAepyceros melampus petersi is listed as endangered by the U.S. ESA and IUCN. Pressure resulting from habitat loss and damage have been linked to the decline in impala numbers. (Delany and Happold, 1979; Wilson and Reeder, eds, 1993)\nBarbara Lundrigan (author), Michigan State University, Karen Sproull (author), Michigan State University.\nliving in sub-Saharan Africa (south of 30 degrees north) and Madagascar.\nyoung are born in a relatively underdeveloped state; they are unable to feed or care for themselves or locomote independently for a period of time after birth/hatching. In birds, naked and helpless after hatching.\nhaving body symmetry such that the animal can be divided in one plane into two mirror-image halves. Animals with bilateral symmetry have dorsal and ventral sides, as well as anterior and posterior ends. Synapomorphy of the Bilateria.\nuses smells or other chemicals to communicate\nranking system or pecking order among members of a long-term social group, where dominance status affects access to resources or mates\nanimals that use metabolically generated heat to regulate body temperature independently of ambient temperature. Endothermy is a synapomorphy of the Mammalia, although it may have arisen in a (now extinct) synapsid ancestor; the fossil record does not distinguish these possibilities. Convergent in birds.\nan animal that mainly eats leaves.\nA substance that provides both nutrients and energy to a living thing.\nAn animal that eats mainly plants or parts of plants.\noffspring are produced in more than one group (litters, clutches, etc.) and across multiple seasons (or other periods hospitable to reproduction). Iteroparous animals must, by definition, survive over multiple seasons (or periodic condition changes).\nhaving the capacity to move from one place to another.\nthe area in which the animal is naturally found, the region in which it is endemic.\nhaving more than one female as a mate at one time\nscrub forests develop in areas that experience dry seasons.\nbreeding is confined to a particular season\nreproduction that includes combining the genetic contribution of two individuals, a male and a female\none of the sexes (usually males) has special physical structures used in courting the other sex or fighting the same sex. For example: antlers, elongated tails, special spurs.\nassociates with others of its species; forms social groups.\nuses touch to communicate\nLiving on the ground.\ndefends an area within the home range, occupied by a single animals or group of animals of the same species and held through overt defense, display, or advertisement\nthe region of the earth that surrounds the equator, from 23.5 degrees north to 23.5 degrees south.\nA terrestrial biome. Savannas are grasslands with scattered individual trees that do not form a closed canopy. Extensive savannas are found in parts of subtropical and tropical Africa and South America, and in Australia.\nA grassland with scattered trees or scattered clumps of trees, a type of community intermediate between grassland and forest. See also Tropical savanna and grassland biome.\nA terrestrial biome found in temperate latitudes (>23.5° N or S latitude). Vegetation is made up mostly of grasses, the height and species diversity of which depend largely on the amount of moisture available. Fire and grazing are important in the long-term maintenance of grasslands.\nreproduction in which fertilization and development take place within the female body and the developing embryo derives nourishment from the female.\nDelany, M., D. Happold. 1979. Ecology of African Mammals. New York: Longman Group Limited.\nEltringham, S. 1979. The Ecology and Conservation of Large African Mammals. New York: The Macmillan Press Limited.\nEstes, R. 1991. The Behavior Guide to African Mammals. Los Angeles: The University of California Press.\nJarman, M. 1979. Impala Social Behaviour: Territory, Hierarchy, Mating,and the Use of Space. Berlin: Verlag Paul Parey.\nWilson, D., D. Reeder, eds. 1993. Mammal Species of the World. Washington, DC: Smithsonian Institution Press.","Habitat selection, numbers and demographics of large mammalian herbivores in Ithala Game Reserve, KwaZulu-Natal.\nO'Kane, C. A. J.\nMetadataShow full item record\nWith the purpose of improving the conservation management of Ithala Game Reserve and other similar reserves, the aims of this study were to determine the reserve's large mammalian herbivores' habitat occupancy, numbers and demographics, to investigate the feasibility of road strip counts as a census method for the same herbivores and to establish what environmental factors influence their habitat occupancy, numbers and demographics. Four years of demographic data were collected by vehicle transects on giraffe, kudu, wildebeest and impala. During the final two years additional positional data, using GPS, were collected on these and the reserve's other large herbivores. Sightings were recorded on the basis of habitat type occupied, a GIS was then used to define area sampled and hence derive habitat occupancy densities. GIS was further used to determine both absolute population sizes and, by over-laying other available GIS data, the relevance of distance to surface water, soil type and degree of slope to species' habitat preferences. Species showed non-random, significant habitat selections broadly in line with established preferences. Deterioration in habitat quality in winter generally lead to changes in habitat selection and the extent and nature of these changes related to the severity of resource pressure for individual species. This in turn was influenced by the species digestive strategy i.e. ruminant versus non-ruminant, grazer versus browser. Generally species showed a dry season move down the slope, moving, in some cases, onto heavier soils. Hartebeest, warthog, wildebeest and impala were strongly attracted to winter grass flushes. Lack of predation may be influencing the habitat selection decisions of impala and giraffe and kudu females, as well as allowing giraffe, wildebeest and impala to attain comparatively high densities. Giraffe density (effectively 1.8 km - 2) was abnormally high and their habitat quality poor, leading to a decline in numbers and low fecundity-related demographics. Wildebeest density (6 km -2) was also abnormally high and this may be instrumental in the poor performance of the rare tsessebe population, which is in decline and shows low fecundity-related demo graphics, increased dry season pressure on other grazers in general and impala 111 unexpectedly preferring browse habitats, rather than grasslands, in the wet season. Wildebeest fecundity declined in response to lower rainfall over the early period of lactation. Herbivores with an open social structure generally showed a dry season decrease in group size, although wildebeest and hartebeest showed, atypically, an Increase. Giraffe, zebra and impala adult sex ratios were comparatively less female biased, probably due to minimal predation. Territorial behaviour, virtually year round by wildebeest bulls and over the rut by impala bulls, imposed spatial sexual segregation between breeding and bachelor herds in these species. Outside of these periods, and generally in species not exhibiting territoriality, social sexual segregation was maintained and appeared to relate to differing activity budgets. Areas of concern for management are highlighted. Numbers results were generally acceptable and the method is proposed as a cost effective alternative in reserves with diverse topography. Underlying environmental determinants of habitat occupancy, numbers and demographics, together with associated annual or seasonal changes, were habitat quality, competition and predation."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:51426efe-478c-4db9-a36b-5303df028b7f>","<urn:uuid:d1bd4de3-e6f2-456c-9f4a-90e04e442fca>"],"error":null}
{"question":"How do table manners regarding bread differ between traditional French and Moroccan dining customs?","answer":"In Moroccan dining, bread serves a functional purpose as it's used to scoop up food and sop up sauce, replacing silverware which isn't used for religious reasons. In French dining, bread has different customs - it should be broken rather than sliced, and is served in a basket on the table with different varieties available. It's particularly important for serving with cheese platters, where crackers are not served, just bread.","context":["As you may already have assumed from previous posts, Moroccan people are extremely hospitable and always ready to lend a hand. It is not uncommon for a new friend to invite you into their home and proudly share a meal with you. While Moroccans are very accepting and eager to welcome foreigners it is very important for that visitor, in this case you, to be familiar with Moroccan social and table etiquette and to respect the customs and traditions that encompass the family dinner table. Social etiquette and table manners are taken quite seriously in Morocco and people are judged on their behavior in public.\nTraditional Moroccan table etiquette has its roots in Islam and these traditions and customs are still adhered to today, even among the youth. If you receive an invitation to a Moroccan home there are a few key behaviors and traditions that should be followed to ensure the proper respect and gratitude is shown to your host.\nWhen invited for dinner at a Moroccan household it is seen as a sign of respect and gratitude to your host to present a small gift of nuts, dates, or flowers with you. Dressing well and taking off your shoes at the door is also a sign of respect and should be followed.\nOnce invited inside, the host will show you to the dinner table, most likely a knee-high table surrounded by pillows or the traditional Moroccan sofas that line the walls of the room. As the honored guest, you will be sat directly next to your host.\nLooking at the table you will notice that there is no silverware, don’t panic! Silverware is not used at Moroccan dinner tables because it is the same material that is used in currencies and is a non-Islamic practice. Instead, Moroccans eat with their right (not left) hands using only their thumb and first two fingers. They also use the famous Moroccan bread as a means to scoop up food and sop up any sauce. As hands are used and cleanliness is very important to Moroccans, a bowl is presented to each guest to wash his/her hands. The host, or member of the household, will pour water over your hands for you; don’t pour the water your self!\nAfter all hands have been washed the food will be presented. Saffron and orange scented couscous, a bubbling tajine full of succulent lamb and roasted vegetables, a large loaf of fresh crusty Moroccan bread – don’t let your stomach get the best of you! It is extremely important to wait until the host has blessed the food and started eating before you dig in!\nAll of the food is presented in communal bowls and each member of the table takes a portion and places it on their plate. Make sure you take food from the part of the bowl nearest to you; don’t reach all the way over the food for that one really yummy looking piece of lamb. As the guest of the meal all of the best cuts of meat will be presented to you anyway, so you won’t have to reach far to get a good piece.\nIt is important to accept and try everything that is offered to you by your host. Even if you just take a nibble. Insisting food upon a guest is a sign of hospitality so don’t feel overwhelmed if they keep telling you to eat more. If you feel you have eaten your weight in food and simply can’t eat another bite take a very small amount from the bowl and take very small bites chewing slowly. It will tell the host that you appreciate their hospitality and respect their food.\nGuests may also be offered souak, or swak, to wash and clean their teeth after the meal. Souak is black walnut dried bark that is whittled into a stick and due to its whitening and antiseptic qualities is used as a natural toothpaste after Moroccan meals.\nAfter all is said and done you can leave your host’s house knowing that you showed the upmost respect and gratitude for the wonderful company and delicious meal.","27 things you need to know about French food etiquette\nPUBLISHED: 15:40 02 May 2017 | UPDATED: 16:15 02 May 2017\nEating and drinking is a crucial part of French culture and social life. There are rules and codes to be aware of so here are some of the dos and don’ts of French dining etiquette.\nGeneral French food etiquette\n1. Eat together and at the dinner table, not in front of the TV.\n2. Break the baguette, don’t slice it.\n3. Know your wine glasses: small oval ones for white wine, large round glasses for red.\n4. Never spread foie gras but place it on the piece of bread.\n5. Use fingers when eating frog’s legs but eat fruit with a knife and, sometimes, a fork.\n6. Fold lettuce leaves, don’t cut them.\nIf you are invited to a dinner party\n7. Arrive 10 to 15 minutes late to give your host time to prepare everything perfectly. If you are going to be more than 15 minutes late, warn your hosts with a phone call.\n8. Never come empty handed when invited to a French dinner party. Good gifts include a bottle of wine, flowers or a plant, macarons or perhaps something homemade like jam.\n9. Look people in the eye when you raise a toast and don’t cross other people’s arms when reaching to clink your glass with someone.\n10. Wait until the cook sits down and says ‘bon appétit’ to begin eating.\n11. Always keep hands above the dinner table.\n12. Finish your plate but don’t ask for seconds: your host will offer seconds if there is any left.\nIf you are hosting a dinner\n13. Start the evening with an apéritif which includes a drink and some light appetizers.\n14. Systematically provide bread - offer different types of bread in a basket on the table - and water: a jug of still water and a bottle of fizzy water.\n15. Order is important: white wine before red, cheese before dessert.\n16. When serving a cheese platter, have a different knife for each type of cheese and don’t serve crackers, just bread.\n17. Generally, children eat at the same time and the same meal but in smaller portions as the adults.\n18. Stay up to date with local news and national stories to keep the conversation interesting. It’s not unusual for the French to have heated political debates around the dinner table.\nDining etiquette at the restaurant\n19. Do not be late.\n20. Always greet your waiter with a ‘bonjour’ or ‘bonsoir’.\n21. Close your menu to indicate you are ready to order.\n22. To get the attention of a waiter, catch their eye and say ‘s’il vous plaît’ or give a little waive. Never call out ‘garçon’ or snap your fingers!\n23. If you need to leave the table but have not finished your dish, place your knife and fork with the handles facing up, as if you were about to take them in your hands.\n24. When you are finished with your meal, place your knife and fork side by side across the plate.\n25. You’ll have to ask for the bill – the usual gesture is to catch your waiter’s eye and pretend to sign a cheque in mid air.\n26. Unless you’ve discussed otherwise, whoever gave the invitation for the restaurant will pay the bill and it is expected that you will pay the next meal out.\n27. Service is included in your final bill but you can leave a little extra for your waiter."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:a4379f45-061b-40a8-8258-929e0f66e7c7>","<urn:uuid:823a5205-e8cf-4cae-b217-c2d711d53ae8>"],"error":null}
{"question":"Hello! I am looking at seasonal energy costs in my area. What is interesting comparison between average home electricity consumption during winter-summer and roof placement efficiency of monocrystalline vs. polycrystalline solar panels on small roofs?","answer":"Consumer energy demand is typically higher during winter and summer months compared to fall and spring, affecting electricity rates. For solar panel efficiency, roof size is a crucial factor - on small roofs, monocrystalline panels are more advantageous as they provide higher efficiency (15-20%) compared to polycrystalline panels (13-16%), allowing you to generate more power in limited space. The average U.S. home consumes 897 kWh per month, and while both panel types can help meet this demand, monocrystalline panels' higher efficiency means fewer panels are needed on a small roof to achieve the same power output, making them more suitable for space-constrained installations despite their higher cost.","context":["Although conventional long-term contracts may offer you very low kilowatt-hour prices, these contracts also impose many obligations and hefty penalties for non-compliance. No deposit electric plans tend to have a slightly higher energy price that long-term contracts, but allow more freedom in how you pay for your electricity. The most relevant differences between no deposit electricity plans and conventional contracts are summarized in the following table:\nCustomers can find deals in competitive electricity markets if they take the time and effort to look at web sites such as powertochoose.org, the official comparison shopping site of the Public Utility Commission. The study cited a PUC survey of retail electricity offerings in Houston that showed nine deals in March that were lower than the regulated price of electricity in San Antonio.\nThe table below shows simple comparison of electricity tariffs in industrialised countries and territories around the world, expressed in US dollars. The comparison does not take into account factors including fluctuating international exchange rates, a country's purchasing power, government electricity subsidies or retail discounts that are often available in deregulated electricity markets.\nWhen you’re choosing a new energy deal, think about whether to go for dual fuel (where you get both your gas and electricity from the same company) or separate tariffs (where you get gas from one company, and electricity from another). It’s worth checking both options, as the combined price of separate tariffs can sometimes be less than a dual fuel offer.\nIn Texas' deregulated energy market, customers must pick their own electricity provider, all of which offer different rates per hour of power usage. You can shop for other power plans on the state-run website, www.powertochoose.org, or try an alternative website, like www.texaspowerguide.com to help find the cheapest plan. Keep in mind that many retail electricity contracts carry penalties for early termination.\nAnyone on a standard rate tariff is at risk of seeing rising energy bills – so one of the best ways to protect from energy price increases is to switch to a fixed rate tariff. This means that for the duration of the deal, the cost of your energy and gas will be fixed. You may be able to switch to a cheaper fixed price tariff at any point, or you may have to pay a fee if you switch before the end of the deal – so check your paperwork.\nShould you choose a short-term, long-term, month-to-month, or prepaid plan? The short answer: it depends on your specific needs. How long do you anticipate living at your location? Are you deciding in the peak season (summer in Houston) or off season? All electricity providers in Houston offer a broad selection of plans for different contract lengths. Many also offer month-to-month and prepaid electricity plans. The bottom line is that everyone’s needs are different and all contract term lengths offer advantages and disadvantages.\nKeeping on top: With deregulation, a whole host of electric resellers jumped into the market because there’s a whole lot of electricity to sell: if Texas were a country, it’d be the 11th largest electricity consumer in the world! Just by itself, it uses as much electricity as Spain or Great Britain! That means there’s a whole lot of information you have to find, absorb, and process to make sure you’re getting the best rate for your needs.\nIronically, technology can make the utilities’ problem worse, not better—at least in the short term. In the past, grids were developed because it was cheaper to generate large quantities of power and distribute it over wide distances, rather then generate smaller quantities closer to the place of use. It is for this reason that electricity is seen as a business that benefits from “economies of scale”.\nLooking for the cheapest energy rates from the best light companies in Texas? With Quick Electricity, you have the power to choose your home or business energy plan from the top rated electric companies in Texas. Whether you’re looking for prepaid electricity companies for pay as you go electric, a 1-24 month fixed energy rate plan or solar panels, we will help you find the most affordable plan to suit your needs. Most energy companies in Texas have a free same day connection and some offer no deposit and no credit check electricity for those with credit problems. To make the switch easier for our customers, Quick Electricity has partnered with the #1 prepaid electricity provider in Texas, Payless Power. Prepaid energy is the best way to get your lights on today for a small fee of $30. For assistance, give us a call at (877) 509-8946.\nThe average home in the U.S. consumes 897 kilowatt hours (kWh) of electricity per month. Bills vary by state and region, as cost per kWh differs. To estimate average energy bills, multiply the average home’s electricity usage (897 kWh) by the cost per kWh in your state for that month. For example, the average cost per kWh in July for Colorado homes was 12.67 cents, which amounts to an average bill of about $113.65 (12.67 cents x 897 kWh) that month.\nWhy are so many African power utilities effectively bankrupt? For one thing, they are incredibly inefficient. Efficiency can be improved by proper metering, investing in the system to reduce losses, improving collections and being able to cut off non-payers. This last one being easier if there is up-to-date metering and certain big players like government departments and military installations are also forced to obey the rules. These operational improvements and efficiencies will improve the supply of power but will not go far enough.\nSince consumer demand directly affects energy rates, it’s important to note seasonal trends of demand throughout the year. Overall, demand tends to be higher during the winter and summer months and lower during the fall and spring months. This pattern isn’t shocking since it’s normal for people to require more energy during extreme heat and extreme cold.\nLots of sites can say 'CHEAPEST ELECTRICITY IN TEXAS!', but only Texas Electricity Ratings gives you the tools to know you're getting a great company to go with the cheap rate. Because what good is a cheap rate if your bills get screwed up and your payments get lost? We've collected thousands of reviews from customers just like you, who need to save money on their electricity bill but don't want the headaches and hassles of a fly-by-night electricity supplier.\nPower generation projects, which have to sell their power to these bankrupt utilities, require creative financing structures to get around these problems. In a bid to reduce their risk when financing these projects, bankers employ financial tools like put call options agreement or World Bank partial risk guarantees. The problem is these tools add complexity and cost which end up being passed on to the end-user or worsen the financial state of the power utility.\nThere are over 60 different energy suppliers competing for your business on any given day in Texas. Many of these electric companies have websites that are confusing and nearly impossible to navigate, their rates and fees hidden by dense industry jargon and misleading advertising. Who has the spare the time to sort through the choices spread out over all these different sites and companies?","Monocrystalline vs Polycrystalline Solar Panels\nShould you choose monocrystalline or polycrystalline solar panels for your home? Here we explore the key differences between the two main types of solar panels to help you decide.\nChoosing solar panels for your home can be a daunting task at first, not only because you want to ensure you invest in a quality and reliable brand of solar panel, but also because there are often multiple choices within each brand’s product range. This is due to the fact that there are two main types of solar PV panel: monocrystalline (mono) and polycrystalline (poly).\nBoth mono and poly solar panels will convert energy from the sun into usable electricity for your home, but there are some differences between the types of solar panels.\nMonocrystalline vs polycrystalline: what’s the difference?\nBoth monocrystalline and polycrystalline solar panels will generate free and clean electricity for your home using energy from the sun. Both types will do this very efficiently, but there are some differences between the two.\nThe difference between monocrystalline and polycrystalline solar panels lies in the silicon cells used in their production. Monocrystalline solar panels are made of single crystal silicon whereas polycrystalline solar panels are made of up solar cells with lots of silicon fragments melted together. In terms of visual difference, monocrystalline panels are black while polycrystalline are dark blue.\nMonocrystalline solar panels\nMonocrystalline solar panels are regarded as the higher quality product as they tend to deliver a higher level of efficiency, i.e. they can produce more electricity than polycrystalline. They are also sleeker in design and therefore, arguably, more aesthetically pleasing.\nIn order to produce monocrystalline solar panels the silicon is formed into bars before being cut into wafers. The cells are made of single-crystal silicon which means that the electrons have more space to move around and can therefore generate more energy. However, because the panels are more efficient, they are usually more expensive than polycrystalline.\nPolycrystalline solar panels\nPolycrystalline (also known as multicrystalline or many-crystalline) solar panels are generally cheaper because they are less efficient. These panels are made of lots of silicon crystals which have been melted together to form a cell. Because of the high number of crystals per cell, the electrons do not have as much space to move and therefore produce less energy. They are usually blue in colour which can mean they stand out more on a roof.\nMonocrystalline vs. polycrystalline solar panels: which panels are right for you?\nOne of the main motivations behind installing solar panels is often to reduce electricity bills. By generating your own free electricity supply you can buy less from your supplier and reduce your dependence on the grid. Whether you choose monocrystalline or polycrystalline, you will be able to reduce your electricity costs.\nMonocrystalline panels are more efficient reaching efficiencies between 15-20% on average while polycrystalline panels are only 13-16% efficient. For this reason, if maximising electricity generation and reducing costs is a priority, monocrystalline are likely to be slightly more effective.\nWhile it may not be a major consideration for everyone, some people may have a preference in terms of the the colour of the panels. Monocrystalline panels are black or at least much darker in colour than polycrystalline which are blue in colour.\nIf you have a small roof then the efficiency of your solar panels should be a major consideration. Properties with small roofs should opt for monocrystalline solar panels which will maximise electricity production. However, properties which do not have the same space restrictions, buying a higher number of polycrystalline panels may be more cost effective.\nWhat power output do you need?\nSolar panels are given a power output rating which is measured in watts (W). The majority of solar panels have power outputs between 250-360 W although they can reach 400 W. In order to determine what power output you need, you should look at recent electricity bills to determine how much electricity your household uses during a given period.\n|House Type||Average Annual Electricity Usage (kWh)||Average Daily Electricity Usage (kWh)|\nOn average, the typical UK household uses 8-10 kWh of electricity per day, but this will obviously vary depending on the size of the property and the number of inhabitants. A 250 W solar panel could generate 1,125 watts per hour (Wh) with 4 hours of direct sunlight. To meet the electricity demands of an average home, more than one panel would be required which is known as a solar panel array or solar panel system.\nHow many solar panels do you need?\nThe majority of solar panels arrays in the UK are sized between 1-4 W kW which are made up of between 2-16 panels. To determine how many solar panels to install, you need to determine how much electricity you use each day, how much power the panels can produce and how much sunlight your roof will be exposed to (on an average day).\nOn average, a UK home receives 4.5 hours of direct sunlight each day, although this is impacted by cloudy weather and will increase/decrease depending on the season. To work out how much electricity a solar panel will generate for your home we need to multiply the number of sunshine hours by the power output of the solar panel.\nFor example, in the case of a 300 W solar panel, we would calculate 4.5 x 300 (sunlight hours x power output) which equals 1,350 watt-hours (Wh) or 1.35 kWh. If we then divide this figure by your average daily electricity usage you can determine approximately how many solar panels you need.\n|Average Daily Electricity Usage (kWh)||Number of 240 watt solar panels||Number of 300 watt solar panels||Number of 400 watt solar panels|\nThis table shows the number of panels required if you want a solar array to meet your home’s total electricity consumption. If you only want solar panels to cover some or most of your electricity consumption, you can install fewer solar panels.\nGet quotes for solar panels\nBefore you make a final decision between monocrystalline vs polycrystalline solar panels it is important to seek advice from a professional solar installer. They will be able to assess your home’s size and electricity consumption before recommending the best size and type of solar panel system for your home.\nTake a few moments to complete our simple online form and you can get free quotes from up to 3 MCS or equivalent solar panel installers based in your local area. Comparing multiple quotes will give you a much greater chance of finding the most competitive price for a solar panel installation where you live."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:2c82094b-b507-4dad-9cd7-6575c0e48c67>","<urn:uuid:a9cb964b-de9a-4b3a-b437-99092961b26e>"],"error":null}
{"question":"As someone suffering from posterior uveitis who's experiencing vision loss and floaters, what are the main treatment options available for managing this condition?","answer":"Treatments for posterior uveitis include oral steroids (such as prednisone at 1 mg/kg/day), injections around the eye, and local steroid therapy like fluocinolone acetonide intravitreal implants. Additional options include anti-inflammatory medicines, antibiotics, and antivirals. Dark glasses may also be used. The primary goals of treatment are to control inflammation and minimize treatment-related complications.","context":["Improve Your Vision With Uveitis Treatments\nWhether you have been diagnosed with uveitis, or you suspect that you have the condition, it is important to know that you are not alone. There are a number of treatments available to help you improve your vision.\nTreatment for anterior uveitis\nUsually, anterior uveitis is treated by applying eye drops containing steroids. This treatment is effective in reducing the pain and inflammation of the uveitis. However, the side effects of steroids include cataract formation and elevated intraocular pressure. These may be reduced by using dilating drops.\nInfliximab is a chimeric antibody directed against TNF alpha. Infliximab is administered every two to four weeks. It is associated with side effects such as upper respiratory tract infection and secondary glaucoma. The downside to infliximab is that it can cause a relapse of the disease. Alternatively, patients can be treated with oral corticosteroids.\nIn rare cases, the inflammatory cells in the anterior chamber may persist for years. This recurrence of the disease is called recurrent iritis. There is no clear explanation as to why the inflammatory cells in the anterior chamber remain. In these cases, a thorough lab examination is necessary. The underlying etiology of the disease should be ruled out before systemic corticosteroids are administered.\nThe use of topical corticosteroids is the first line of treatment for anterior uveitis. The duration of the steroid’s effect depends on the strength and type of corticosteroid. Typically, prednisolone acetate 1% is the most common corticosteroid prescribed. Oral and regionally injected corticosteroids may also be used.\nA more accurate diagnosis can be made by taking a detailed history and conducting a comprehensive eye exam. Anterior uveitis can occur due to a number of factors, including autoimmune conditions. In addition, a yearly eye exam can prevent the development of idiopathic uveitis. If a patient’s condition worsens, additional medications may be needed.\nThere are three main types of uveitis: idiopathic, chronic, and acute. Symptoms and signs of uveitis vary among the different types. Idiopathic uveitis is the most common type of uveitis. Acute uveitis can occur suddenly, while chronic uveitis is usually not associated with a specific etiology. Acute uveitis has been known to be associated with HLA-B27 conditions. Typically, patients with an HLA-B27 allele have a 50% chance of developing acute uveitis. A blood test can be performed to determine the underlying cause of the uveitis.\nTreatment for intermediate uveitis\nDuring the past few years, there has been a significant amount of progress in the treatment of uveitis. Advances include the development of immunosuppressive therapies, the use of steroid implants, and the introduction of new imaging methods such as Optical Coherence Tomography (OCT).\nThe goal of uveitis treatment is to achieve a durable remission of the disease. This enables patients to lead normal lives without the burden of sight-threatening complications. Although systemic corticosteroids are the most common treatment, they have numerous undesirable side effects. They should not be used for long periods. Instead, they should be used in the early stages of the disease.\nRecently, novel treatments for uveitis have been evaluated in clinical trials. These include adalimumab, which blocks the production of TNF-a, a cytokine that is important in the immune system. However, there are concerns that adalimumab may increase the risk of developing the demyelinating disease. In contrast, slow-release corticosteroid implants offer better safety and efficacy for uveitis.\nOne trial examined the use of an intravitreal implant of dexamethasone in the treatment of intermediate uveitis. The results showed that the patients who received the implant experienced significantly reduced intraocular inflammation. They also had improved visual acuity for six months.\nThe study also identified a number of factors that were associated with reduced BCVA. These included the length of time that the patient had been in the uveitis, the presence of macular edema, and the incidence of a flare of AC activity.\nThe use of steroid eye drops and intravitreal steroid implants has become more widespread. For patients with unilateral uveitis, a fluocinolone acetonide implant is an option. It has been shown to maintain a clear advantage in controlling inflammation for 54 months.\nIn the Multicenter Uveitis Steroid Treatment (MUST) trial, 479 intermediate, posterior, and panuveitis eyes were examined. Approximately 35% of these patients showed blindness in at least one eye.\nAmong those whose symptoms were not present, a small number received steroid treatment. In five cases, the steroid was given to patients who had pure ocular localization. In one case, the first diagnosis was made through a necropsy.\nTreatment for posterior uveitis\nTypically, the symptoms of posterior uveitis include vision loss, floaters, and scarring in the retina. It may also affect the optic nerve or the choroid. It is important to treat the condition before it becomes severe and causes permanent damage. There are several treatments available, including oral steroids, injections around the eye, and dark glasses.\nOral prednisone at 1 mg/kg/day can be used to help control inflammation. However, it is not recommended to be given continuously for long periods of time. As the disease improves, the dose can be reduced. In addition, systemic steroids have significant side effects and can lead to the development of cataracts.\nLocal steroid therapy, such as fluocinolone acetonide intravitreal implants, is often used to treat uveitis. They have been shown to prevent ocular complications from recurrent inflammatory episodes in animal models. Moreover, sustained-release local steroid therapy has been found to provide a steady state in uveitis control.\nThe primary goals of NIU treatment are to control inflammation and to minimize the risk of treatment-related sequelae. These goals are achieved by targeting the intraocular inflammatory response.\nIn patients with severe uveitis, systemic corticosteroids may be indicated. These medications have well-established systemic side effects, including hypertension and weight gain. In addition, the use of chronic steroids increases the risk of glaucoma.\nIn addition to these therapies, posterior uveitis may require a series of laboratory tests to rule out an underlying autoimmune disorder. Anti-inflammatory medicines, antibiotics, and antivirals may be required. It is important to discuss these risks with the patient and to weigh them against the benefits.\nFor those with an autoimmune disorder, immunosuppressive therapy is also an option. This includes medications such as methotrexate and azathioprine, as well as vaccines. The American College of Rheumatology has developed guidelines for these vaccines. If the patient is receiving a vaccine, it is important to wait at least one week after the vaccination before beginning methotrexate or azathioprine.\nTopical steroid drops are also used for anterior uveitis. These can be given at frequent intervals. They are less effective than systemic steroids because they are not absorbed into the ocular structures. They are also cumbersome for patients.\nVarious factors contribute to the diagnosis of uveitis. These include the onset age, location of the disease, and etiology of the disease. These factors also depend on the portion of the uveal tract affected.\nSeveral autoimmune diseases are associated with uveitis. The most common of these are sarcoidosis and white dot syndrome. These can become chronic inflammatory processes if not treated. In addition, a variety of infections are often associated with uveitis. Some infectious uveitis includes the systemic varicella virus, adenovirus, and cytomegalovirus. These diseases may produce symptoms such as pain, blurred vision, and light sensitivity. These symptoms are indicative of inflammation, but they are not specific to any particular disease.\nOther causes of uveitis include trauma, cataract surgery, and glaucoma. These types of uveitis should be differentiated from idiopathic uveitis, which occurs without an identifiable cause. An ophthalmologist is the best physician to diagnose uveitis. A rheumatologist and neurologist should also be familiar with the diagnosis of uveitis.\nInflammation is usually the primary etiological factor in uveitis. It can affect the iris, the ciliary body, or the choroid. Visual loss may occur if the anterior or posterior uvea is affected.\nUveitis is an immune-mediated response to an antigen. The offending antigen can be an internal or external molecule. It can be a virus, a bacteria, a fungus, or a parasite. There are a number of tests to evaluate the underlying disease, including toxoplasma serology and keratic precipitates.\nThe majority of patients with uveitis do not have a well-defined diagnosis. In fact, 30 percent of uveitis is idiopathic. It is important to identify the offending antigen and treat it to prevent the development of systemic disease. In addition, a thorough clinical examination and systemic review may help to narrow down the causes of uveitis.\nAs in any medical specialty, a comprehensive work-up is required to make an accurate diagnosis of uveitis. This may involve imaging, basic laboratory tests, and urine and blood tests. The patient’s clinical picture should be the basis for these investigations. If the diagnosis is positive, targeted treatment should be pursued. The ophthalmologist may refer the patient to another specialist for further diagnostic testing.\nHealth A to Z. (n.d.). HSE.ie. https://www2.hse.ie/az/\nU.S. National Library of Medicine. (n.d.). https://www.ncbi.nlm.nih.gov/\nDirectory Health Topics. (n.d.). https://www.healthline.com/directory/topics\nHealth A-Z. (2022, April 26). Verywell Health. https://www.verywellhealth.com/health-a-z-4014770\nHarvard Health. (2015, November 17). Health A to Z. https://www.health.harvard.edu/health-a-to-z\nHealth Conditions A-Z Sitemap. (n.d.). EverydayHealth.com. https://www.everydayhealth.com/conditions/"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:473219b6-6448-4ac6-9fcb-9339a9de87b1>"],"error":null}
{"question":"I'm curious about treatment structures. What are the key differences between how mindfulness is implemented for psychosis patients versus mild cognitive impairment patients?","answer":"The treatment structures differ significantly. For psychosis patients, the mindfulness program consisted of twice-weekly sessions for 5 weeks, followed by 5 weeks of home practice with meditation CDs. Participants were taught mindfulness of the breath and encouraged to observe unpleasant experiences without judgment. For MCI patients, the program was structured as an eight-week mindfulness-based stress-reduction (MBSR) course, with emphasis on practicing at least 20 minutes daily. While the psychosis program specifically focused on managing distressing psychotic experiences, the MCI program was more general and aimed at improving cognitive reserve and reducing stress impacts on the hippocampus.","context":["Displaying 1 - 4 of 4\nObjective. This pilot randomized controlled trial (RCT) assesses Person‐Based Cognitive Therapy (PBCT), an integration of cognitive therapy and mindfulness, as a treatment for chronic depression.Method. Twenty‐eight participants with chronic depression were randomly allocated to treatment as usual (TAU) or PBCT group plus TAU. Assessments of depression (Beck Depression Inventory, BDI‐II) and mindfulness (Southampton Mindfulness Questionnaire) were conducted before and after therapy. Results. Intention‐to‐treat analysis found significant group by time interactions for both depression and mindfulness. Secondary analyses showed depression and mindfulness scores significantly improved for PBCT participants but not for TAU participants, with 64% of PBCT participants showing reliable improvement in depression, compared with 0% of TAU participants. Conclusions. PBCT is a promising treatment for chronic depression. Findings suggest a full RCT would be warranted.\nBackground: The clinical literature cautions against use of meditation by people with psychosis. There is, however, evidence for acceptance-based therapy reducing relapse, and some evidence for clinical benefits of mindfulness groups for people with distressing psychosis, though no data on whether participants became more mindful. Aims: To assess feasibility of randomized evaluation of group mindfulness therapy for psychosis, to replicate clinical gains observed in one small uncontrolled study, and to assess for changes in mindfulness. Method: Twenty-two participants with current distressing psychotic experiences were allocated at random between group-based mindfulness training and a waiting list for this therapy. Mindfulness training comprised twice-weekly sessions for 5 weeks, plus home practice (meditation CDs were supplied), followed by 5 weeks of home practice. Results: There were no significant differences between intervention and waiting-list participants. Secondary analyses combining both groups and comparing scores before and after mindfulness training revealed significant improvement in clinical functioning (p = .013) and mindfulness of distressing thoughts and images (p = .037). Conclusions: Findings on feasibility are encouraging and secondary analyses replicated earlier clinical benefits and showed improved mindfulness of thoughts and images, but not voices.\nThe study's objective was to assess the impact on clinical functioning of group based mindfulness training alongside standard psychiatric care for people with current, subjectively distressing psychosis. Data are presented from the first 10 people to complete one of four Mindfulness Groups, each lasting six sessions. People were taught mindfulness of the breath, and encouraged to let unpleasant experiences come into awareness, to observe and note them, and let them go without judgment, clinging or struggle. There was a significant pre-post drop in scores on the CORE (z=−2.655, p=.008). Secondary data indicated improvement in mindfulness skills, and the subjective importance of mindfulness to the group process (N=11). The results are encouraging and warrant further controlled outcome and process research.\nThis study investigates the psychological process involved when people with current distressing psychosis learned to respond mindfully to unpleasant psychotic sensations (voices, thoughts, and images). Sixteen participants were interviewed on completion of a mindfulness group program. Grounded theory methodology was used to generate a theory of the core psychological process using a systematically applied set of methods linking analysis with data collection. The theory inducted describes the experience of relating differently to psychosis through a three-stage process: centering in awareness of psychosis; allowing voices, thoughts, and images to come and go without reacting or struggle; and reclaiming power through acceptance of psychosis and the self. The conceptual and clinical applications of the theory and its limits are discussed.","WINSTON-SALEM, N. C. — Mild cognitive impairment (MCI) is often a precursor to Alzheimer’s disease. Unfortunately, thus far no effective method of preventing further mental decline in MCI patients has been developed. However, researchers from Wake Forest Baptist Medical Center may have found a safe and non-pharmacological treatment that can help people living with the condition: mindfulness meditation.\n“Until treatment options that can prevent the progression to Alzheimer’s are found, mindfulness meditation may help patients living with MCI,” says Dr. Rebecca Erwin Wells, associate professor of neurology with Wake Forest Baptist Health, in a media release. “Our study showed promising evidence that adults with MCI can learn to practice mindfulness meditation, and by doing so may boost their cognitive reserve.”\nMindfulness is defined as maintaining a blank, moment-by-moment awareness of one’s own thoughts, feelings, and surroundings. In other words, it is the process of training one’s mind to stay completely in the moment and relieve itself from outside distractions and anxiety triggers.\n“While the concept of mindfulness meditation is simple, the practice itself requires complex cognitive processes, discipline and commitment,” Wells explains. “This study suggests that the cognitive impairment in MCI is not prohibitive of what is required to learn this new skill.”\nPrior research has already established that constant feelings of stress have an impact on the hippocampus, the part of the brain responsible for learning and memory, and can raise one’s risk of eventually developing MCI and / or Alzheimer’s. As far as other non-drug solutions, some research has indicated that regular aerobic exercise can help mitigate the effect of stress on the brain.\nIn order to test if a mindfulness-based stress-reduction (MBSR) program could help people living with MCI, researchers gathered 14 men and women between the ages of 55-90 diagnosed with MCI. Each participant was then randomly assigned to either an eight-week mindfulness meditation course or a “waiting list” control group.\nInitial results revealed nine participants who were enrolled in the meditation course exhibited improved cognition and well-being, as well as signs of a positive impact on the hippocampus and other areas of the brain linked to cognitive decline.\nThen, researchers refined their findings by conducting interviews with each MBSR course participant following the eight-week course.\n“While the MBSR course was not developed or structured to directly address MCI, the qualitative interviews revealed new and important findings specific to MCI,” Wells says. “The participants’ comments and ratings showed that most of them were able to learn the key tenets of mindfulness, demonstrating that the memory impairment of MCI does not preclude learning such skills.”\nParticipants who practiced meditating at least 20 minutes per day seemed to understand the process of mindfulness the most, Wells also noted.\nWhile these findings are very promising, the research team caution that more research is needed before any definitive claims can be made regarding mindfulness meditation and MCI treatment. The study’s authors noted the experiment’s small sample size and educational discrepancies among participants as a few of the study’s limitations.\nThe study is published in the Journal of Alzheimer’s Disease."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:17d5e102-34f2-4581-b02d-ea02c0d1d409>","<urn:uuid:b2b9759a-99ef-4aee-9066-3071d91afb84>"],"error":null}
{"question":"I'm working with embedded systems and servo motors. What are the key hardware connections needed for a FRDM-KL25Z servo setup, and what security considerations should I keep in mind for such user-accessible devices?","answer":"For the FRDM-KL25Z servo setup, you need to connect the FRDM-TFC Shield to the FRDM-KL25Z Board, connect a servo motor to the Shield's header (red wire to +6V, black wire to GND, white wire to PWM signal pin), and connect a 7.2V battery. For security considerations with user-accessible devices, software security alone is insufficient. You need both hardware and software security measures, including protection against design security breaches (IP protection, encrypted configuration), hardware security (resistance to physical attacks like DPA), and data security (ensuring authentic communications). Implementation should include risk assessment, protection planning, attack scenario testing, and DPA side-channel analysis to create a robust protection network without single points of failure.","context":["This example shows how to use Simulink Coder Support Package for NXP FRDM-KL25Z Board to control a servo motor connected to FRDM-TFC Shield.\nSimulink Coder Support Package for NXP FRDM-KL25Z Board enables you to create and run Simulink® models on a FRDM-KL25Z Board.\nIn this example you will learn how to create a Simulink model that controls a standard servo motor which is connected to FRDM-TFC Shield.\nWe recommend completing Getting Started with Simulink Coder Support Package for NXP FRDM-KL25Z Board.\nTo run this example you will need the following hardware:\n- NXP FRDM-KL25Z Board\n- FRDM-TFC Shield\n- Standard servo motor\n- 7.2V battery\nTask 1 - Hardware Connections\n1. Place the FRDM-TFC Shield firmly on the FRDM-KL25Z Board as shown in the following figure.\n2. Connect the NXP FRDM-KL25Z Board to your computer with a USB cable.\n3. Connect your servo motor to the FRDM-TFC Shield. There is a header on the Shield labled with an image of a servo motor. This is where you should connect the servo. Servo motors have three wires: power, ground, and PWM signal. Connect them as described below and shown in the following figure.\n- Connect the power wire (red) to the +6V pin.\n- Connect the ground wire (black) to the GND pin.\n- Connect the PWM signal wire (white) to the servo PWM pin.\nThe header's pins are labeled on the bottom of the board.\n4. Connect the battery supply to FRDM-TFC Shield.\nTask 2 - Set the Servo Motor Position\nIn this task you will set the position of the servo motor shaft using a sine wave. The shaft angle will vary between -20 and 20 degrees.\n1. Open the Servo Control model.\n2. Double-click on the Sine Wave block to see that it is set to vary between -20 and 20, with a step size of 0.01. This means that the Servo block gets a new angle for the servo motor shaft every 0.01 second.\n3. In your Simulink model, click the Build Model button on the toolbar.\n4. When the model starts running on the NXP FRDM-KL25Z board, observe the servo's motor shaft position sweep between -20 and 20 degrees.\nTask 3 - Control the Servo Motor Position via a Potentiometer\nIn this task you will set the position of the servo motor shaft manually, using a potentiometer on the FRDM-TFC Shield.\n1. Open the Servo Control Using Potentiometer model.\n2. Notice that the Potentiomter block's output is multiplied with the value 20, using a gain block. This is done to scale the Potentiometer's output, normally -1 to 1, to -20 to 20.\n3. Double-click on the Potentiometer block to see that potentiometer A has been selected, and that it will be updating the servo's values every 0.01 seconds.\n4. In your Simulink model, click the Build Model button on the toolbar. The model will now be deployed to the NXP FRDM-KL25Z Board.\n5. When the model starts running on the NXP FRDM-KL25Z Board, observe the servo's motor shaft position sweeps between -20 and 20 degrees based on the position of Potentiometer A.\nOther Things to Try\nExperiment with other blocks in the NXP FRDM-KL25Z block library. For example:\n- Use Digital Input block to adjust the motor shaft position depending on the external control signal or signals.\nThis example showed you how to use Simulink Coder Support Package for NXP FRDM-KL25Z Board to control a standard servo motor. In this example you learned that:\n- Standard Servo Write block allows you to set the servo motor shaft position, in the range from a minimum angle to a maximum angle degrees.","Biggest security threats for embedded designers\nEmbedded system designers face a number of threats to the applications that they develop for the Internet of Things (IoT). One of the biggest threats comes from IoT devices that end-users can access, such as commercial networked HVAC systems, wireless base stations, power stations, network gateway systems, and avionics networking.\nAnother example is the connected car, including the advanced driver assistance system (ADAS) that encompasses intelligent, interconnected vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) systems. Since vehicles are fielded systems, they are accessible by people with malicious intentions. There can be serious consequences – up to and including loss of life – if, for instance, ADAS systems cannot ensure that V2V and V2I messages originate from a trustworthy source and are not modified between sender and receiver.\nWith these and other systems, software security, alone, has proven inadequate to protect user-accessible devices against known threats. What is needed is a combination of software and hardware security. For example, today’s FPGA SoCs can be used to implement a hardware security scheme that compliments the software and strengthens the system. Ideally, the hardware and software solution should combat three types of security breaches:\n1. Design security: This includes IP protection and ensuring that configuration bit streams are encrypted and protected. In addition, designs need to incorporate a method to ensure there is no overbuilding or cloning of the design possible.\n2. Hardware security: Designers also need to certify that user-accessible devices are resistant to physical attacks. For example, differential power analysis (DPA) attacks can extract keys and other vital device information.\n3. Data security: This element ensures that communications into and out of the device are authentic and secure.\nEmbedded system program managers and development teams must design these types of protections into their products while best leveraging the characteristics of the underlying platform. The result should be a robust protection network with no single point of failure. Some key methods for achieving this goal include:\n∑ Risk assessment: System penetration testing should be used for a detailed system evaluation, to assess critical system data/functions, discover vulnerabilities, enumerate threats, and outline the likelihood and consequence of system compromise.\n∑ Protection planning: Using risk assessments and any other compiled data, developers should seek to understand protection implementation costs and design options for mitigating identified system vulnerabilities and ensuring successful system verification and validation.\n∑ Attack scenario testing: This can include a black box approach, pitting experienced reverse engineers with state-of-the-art attack tools against a system in a deployed setting to reveal vulnerabilities that cannot otherwise be found during most other evaluation exercises.\n∑ DPA side-channel analysis and mitigation: Side-channel attacks are currently the most practical method for compromising cryptography implementations. It is important to regularly perform measurable, objective, and repeatable testing for resistance to side-channel attacks for applications where adversaries have the ability to observe side channels (i.e., power draw, timing, EM emanations) during on-device cryptographic operations.\nIn today’s cyber hacking world, it is essential for every public and private organization to proactively address security issues. Embedded system designers can help their customers in this area by creating secure designs that are protected from today’s rapidly evolving threats, including those posed by a rapidly growing ecosystem of interconnected, user-accessible hardware."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:8e0e25dd-31b0-42f9-afc5-4fc1ab51cdeb>","<urn:uuid:6fd90e2b-d1fc-4ceb-b428-5c07dc11374d>"],"error":null}
{"question":"What lures best for walleye trolling day and night, structure locations?","answer":"For daytime trolling, effective lures include crawler harnesses (best in late spring/early summer), crankbaits like Rapala's Shad Rap (summer), spinners, and spoons. For night trolling, shallow diving stickbaits are preferred, specifically Rapala Husky Jerk 12 and 14, Rapala No. 18 Minnow, Smithwick Perfect 10, and Yo-Zuri Crystal Minnow series. Regarding structure, walleye love weed beds, drop-offs, and other sub-surface features, which should be the first choice for fishing locations. In deeper open water, walleye school in smaller groups. When fishing structures, it's best to make long sweeps along them rather than directly over them.","context":["During the winter and early spring months most walleye fishing usually involves a jig or bait.\nHowever, once the spawning season ends and the temperatures begin to rise your focus should turn to trolling. The best walleye lures for trolling will be crankbaits, spinners, small flatfish lures and Rapala's.\nWalleye trolling is much like other trolling for other freshwater species like salmon or lake trout except the gear may be slightly lighter and you may use a hybrid rig like a crawler harness.\nTraditional trolling rigs for walleye are just as effective as a harness. But in the early season when you need to slow things down a bit a harness allows you to troll at super slow speeds.\nTrolling for Walleye\nTrolling for walleye consists of trolling one of the following setups behind a boat usually in conjunction with a fish finder to help determine the depth:\n- Crawler harness\n- Spinners and spoons\nThe types of lures you choose will vary depending on the time of year and the depth at which you are trolling.\nCrawler harnesses in the late spring and early summer and then larger crankbaits such as Rapala's Shad Rap in the summer.\nSpinners and trolling spoons for walleye can be used at speed in the summer but spinners are most effective when used as part of a crawler harness at slower speeds when walleye are feeling lazy.\nSmaller light lures such as walleye jerkbaits can also be trolled but to get the best performance from them you will need to twitch the rod tip and troll with rod in hand.\nWalleye tend to love structures so best to start there and if it is not producing you an head for more open water.\nIn deeper open water walleye can start to school in smaller groups hitting one of these schools can be a little bit hit and miss.\nWeed beds, drop offs and other sub-surface structures and features should always be your first choice.\nOnce you find them at these structures try to make long sweeps along the structures rather than directly over them.\nWalleye Trolling Speed\nThe best trolling speed for walleye will fall somewhere in the range of 0.5 to 3 mph. It will of course depend on a number of factors such as:\n- Type of lure\nDuring late spring and into the early summer your speed will need to be on the slower side. Walleye are often quite lazy following their spawning season and as a result are less inclined to chase a lure that is moving too fast.\nOnce the warmer summer days arrive walleye will tend to feed in a much more aggressive manner. There is also a lot more food on offer.\nOnce they are done with spawning you can start to use lures that require a slightly faster speed to get the best action out of them.\nWhen trolling larger crankbaits you can troll at the higher end speeds up to around 3 mph.\nHowever, most crankbaits for walleye will perform best at around the 2 mph mark.\nWalleye Trolling Tips\nAlthough there are a lot of fishermen who will just tie on a crankbait let out lots of line and hope for the best this really is not the best approach to catching walleye from a boat.\nYou need to stop and think about some of the factors already mentioned above, some walleye trolling tips to follow:\n- Season can determine your trolling speed\n- Use a fish finder to find the walleye\n- Pay attention to the contours of the bottom\n- Cover as much ground as you can\n- Change your speed\n- Move the boat in an 'S' pattern\nWalleye Trolling Setup\nThe type of walleye fishing gear required for trolling is a lot different to when you are jigging.\nIf you are using more than two rods on your boat then you may need to use planer boards to get keep the out side lines as wide as possible from the inner lines.\nTo get to the desired depth you can control your lure by adding weight to the line. There are usually two options used when using weight whilst trolling:\n- Bottom bouncing\n- Three way rig\nBoth require a little experimentation to get the desired depth just right.\nIf your budget allows for it you can of course invest in a downrigger.\nA downrigger allows you to get a much more accurate running depth for you lures.\nBut a downrigger can require the use of a heavier rod as the constant strain is likely to ruin a lighter rod.","Captain Terry Kunnen of TKO Charters caught this monster walleye\nnight trolling using crankbaits and Off Shore Tackle Side-Planer boards.\nThe best boat for night walleye trolling is one that has a place for everything and everything is in that place. On a night walleye fishing adventure just take the necessary gear and nothing else. Keeping things simple is the best way to be successful and avoid frustrations when fishing after dark.\nWhen fishing in the dark an angler is going to need some dependable lighting options. The cockpit lights in most fishing boats aren’t adequate to perform important tasks like tying on fishing lures or monitoring the lead lengths on a line counter reel.\nMost anglers use an LED head lamp for these purposes. I keep mine, plus a spare, stored in the glove box of the boat along with plenty of spare batteries.\nAdding LED rope lighting around the gunwale of the boat is a good way to flood light onto the floor of the boat and provide enough ambient light for tasks like hooking up planer boards, unhooking fish from the landing net, setting lines, etc.\nTo really light up the night, anglers might want to consider more powerful LED lighting options. Cisco Fishing Systems produces the Sur-Lok Boat Lighting System that mounts to risers which slide into their track system. These lights can literally be mounted to any flat surface in the boat, turning night into day. Like everything produced by Cisco these lights are high quality and made right here in America. For more details on these very cool lights for night fishing, visit www.ciscofishingsystemsltd.com.\nWhen the sun sets walleye go on the prowl. Trolling\ncrankbaits after dark is one of the best ways to target\ntrophy walleye that get conditioned to feed\nmore at night than during the day.\nLike most open water trolling applications a good fiberglass or graphite/fiberglass composite trolling rod matched up to a dependable line counter reel and 10 to 12 pound test monofilament line is the right set up for night walleye trolling.\nMy set up consists of a 8.5 foot telescopic rod that fishes nicely when extended and fits neatly into my rod locker when telescoped down. A 20 size line counter reel works best for walleye fishing and I can’t express enough the importance of running premium fishing lines designed with trolling in mind.\nI fish 12 pound test Maxima in the Ultra Green color and have found this line to be tough as nails when fishing around rip rap and dealing with the abrasion created by putting a planer board on the line and taking it off countless times. Maxima is very popular among steelhead fishermen who know how tough this line is, but for the most part walleye anglers have not discovered this super premium line choice.\nNight walleye trolling is a game placed with crankbaits and the brands/models of lures that routinely produce fish is an amazingly short list. Most night walleye fishing takes place at or near the surface so shallow diving stickbaits or what bass anglers refer to as jerkbaits are among the best choices.\nThe list of “must have” stickbaits for walleye trolling include the Rapala Husky Jerk 12 and 14, Rapala No. 18 Minnow, the Smithwick Perfect 10 and the Yo-Zuri Crystal Minnow series of baits. Obviously other stickbaits on the market will catch walleye, but this “short list” are the baits the hard core fishermen carry every time on the water.\nIn some situations it might be necessary to fish a little deeper than these stickbaits naturally dive. The depth of these lures can easily be increased by adding a one ounce Off Shore Tackle Snap Weight to the line about 20 to 25 feet ahead of the lure. A one ounce weight will add about 1/3 to the natural diving depth of a floating/diving style crankbait. In other words a bait that dives 15 feet without weight will reach about 20 feet when a one ounce weight is used.\nNotice the planer boards in this picture. They have been\nequipped with strips of reflector tape which makes them highly\nvisible when night trolling. Other anglers prefer to tape a cyalume\nstick to the flag of their boards when night trolling.\nThe Xi5 also features the GPS navigation powers of the Pinpoint Auto-Pilot System which makes it easy to follow a compass heading and keep the boat traveling in a desired direction hands free. Even better, once a few fish have been caught and the location of those fish saved as waypoints on a Lowrance HDS sonar/GPS unit, the Xi5 can be programmed to duplicate a precise trolling track. Thanks to the Xi5 it possible to troll from waypoint to waypoint like a beagle following a rabbit track!\nThis amazing technology is made possible by combining the GPS features of the Xi5 electric motor with the Sonar/GPS capabilities of any Lowrance Generation Two or Three HDS units. Essentially the electric motor and sonar/GPS unit are communicating using a system called Gateway, that allows the electric motor to be controlled using a foot control, a key fob and also using the touch screen on Lowrance HDS units.\nCatching walleye by trolling after dark is about finding fish, but it’s just as important to stay on those fish once they are found. With the Gateway System it’s easy to duplicate a productive trolling pass time and time again.\nBOARDS ARE BEST\nAn angler can long line his favorite crankbaits out the back of the boat and catch a few night time walleye. Incorporating in-line boards like the famous Off Shore Tackle OR12 Side-Planer is the fast track to night trolling success.\nIn order to see these boards in the dark and detect strikes, anglers use some ingenious tricks. Ordinary cyalume sticks sold at dollar stores are a good value and they also make a great board light option when taped to the flag of an OR12 Side-Planer. The typical cyalume stick lasts for six or eight hours and provides plenty of light to easily monitor boards in the dark.\nAnother option is to purchase Coast Guard approved reflector tape and place a few strips on the board flag. This tape will reflect even the subtle light produced by a boat’s bow and stern lights and also head lamps.\nAnother little known trick is to set the bait clicker function on the trolling reels and then to back off on the reel drag until just enough tension in the drag is achieved to hold the boards in place while trolling. When a fish is hooked, the extra pressure on the line causes the line to slip a little and the bait clicker gives an audible clue as to which rod/reel combination has hooked a fish.\nWHERE IS THE ACTION\nWalleye can be caught at night anywhere these fish are found. In-land lakes, impoundments, rivers and of course the Great Lakes are all great places to try your hand at “night shift” walleyes."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:4aebaf89-3e27-48ef-abc1-ad057de2f70e>","<urn:uuid:1a26a0ae-6694-4101-a72d-1361a3bed8b8>"],"error":null}
{"question":"What are the main treatment goals of CBT for chronic pain versus DBT for mental disorders?","answer":"CBT for chronic pain aims to help patients reduce symptom intensity, regain functioning, and reduce suffering through techniques like pacing, spouse involvement, quota-based return to functioning, and coping skills. In contrast, DBT for mental disorders focuses on five key functions: enhancing capability through skillful behavior, improving motivation to change, ensuring generalization of treatment changes, maintaining therapist motivation, and restructuring the environment to support progress toward goals.","context":["Status: Strong Research Support\nBehavioral Therapy (BT) and Cognitive Behavioral Therapy (CBT) for CLBP are terms for psychological interventions that often get applied inter-changeably in the CLBP literature. Therapies based upon these principles seek to help the patient with pain reduce symptom intensity, regain functioning, and reduce suffering. Many techniques get incorporated into this form of therapy and rarely are single components applied in actual practice. Techniques can include time-contingent pacing, spouse involvement and reinforcement of adaptive responding, use of quotas and goals for gradual return of functioning, reframing of affective and cognitive responses, learning of coping skills, and learning of the relaxation response (e.g. progressive muscle relaxation, biofeedback). In order to better learn and integrate skills into one’s life style, CBT relies upon self-monitoring, skill rehearsal, and social reinforcement. CBT for CLBP is most often administered either individually or in small groups over 8-12 sessions and is often incorporated into a broader medical and/or physical therapeutic program.\nKey References (in reverse chronological order)\n- Morley, S., Eccleston, C., Williams, A. (1999). Systematic review and meta-analysis of randomized controlled trials of cognitive behaviour therapy and behaviour therapy for chronic pain in adults, excluding headache. Pain, 80 (1-2), 1-13.\n- Turner JA, Jensen MP. (1993). Efficacy of Cognitive therapy for Chronic Low Back Pain. Pain, 169-177.\n- Basler, HD, Jakle, C., Kroner-Herwig, B. (1997). Incorporation of Cognitive-Behavioral Treatment into the Medical Care of Chronic Low Back Pain Patients: A Controlled Randomized Study in German Pain Treatment Centers. Patient Education and Counseling, 31, 113-124.\n- Turner, JA, Clancy S. (1988). Comparison of Operant Behavioral and Cognitive-Behavioral Group Treatment for Chronic Low Back Pain. Journal of Consulting and Clinical Psychology, 56 (2), 261-266.\n- Chou, R., Huffman, LH. (2007). Nonpharmacological Therapies for Acute and Chronic Low Back Pain: A Review of the Evidence for an American Pain Society/American college of Physicians Clinical Practice Guideline. Annals of Internal Medicine, 147, 492-504.\n- Chou, R. et al (2007). Diagnosis and Treatment of Low Back Pain: A Joint Clinical Practice Guideline from the American College of Physicians and the American Pain Society. Annals of Internal Medicine, 147 (7), 478-491.\n- Hoffman, BM, Papas, RK, Chatkoff, DK, Kerns, RD. (2007). Meta-Analysis of Psychological Interventions for Chronic Low Back Pain. Health Psychology, 26(1), 1-9.\n- Van Tulder, MW, Ostelo, R, Vlaeyen, JWS, Linton, SJ, Morley, SJ, Assendelft, WJJ. (2000). Behavioral Treatment for Chronic Low Back Pain. Spine, 26(3), 270-281.\nFormal training in CBT and BT for pain management is often available through workshops held at the American Pain Society, International Association for the Study of Pain, and the Association for the Advancement of Behavioral Therapy. Several centers conducting trials of CBT also provide informal training, predoctoral training, psychological internship rotations, or postdoctoral fellowships in CBT for pain management. For information about training opportunities at these centers contact the following centers:\n|Laurance A. Bradely, Ph.D.\nDivision of Rheumatology\n178A Shelby Research Building\nUniversity of Alabama – Birmingham\nBirmingham, AL 35294\n|Francis J. Keefe, Ph.D.\nDirector, Pain Management Program\nDuke University Medical Center\nDurham, NC 27710\n|Dennis C. Turk, Ph.D.\nUniversity of Washington\nSeattle, WA 98195\n|David A. Williams, Ph.D.\nChronic Pain and Fatigue Research Center\nDept. of Internal Medicine/Rheumatology\nUniversity of Michigan\n24 Frank Lloyd Wright Drive, Lobby M\nAnn Arbor, MI 48105","What is DBT?\nDialectical Behavior Therapy (DBT) is a comprehensive multi-diagnostic, modularized behavioral intervention designed to treat individuals with severe mental disorders and out-of-control cognitive, emotional and behavioral patterns. It has been commonly viewed as a treatment for individuals meeting criteria for Borderline Personality Disorder (BPD) with chronic and high-risk suicidality, substance dependence or other disorders. However, over the years, data has emerged demonstrating that DBT is also effective for a wide range of other disorders and problems, most of which are associated with difficulties regulating emotions and associated cognitive and behavioral patterns.\nAs the name implies, dialectical philosophy is a critical underpinning of DBT. Dialectics is a method of logic that identifies the contradictions (antithesis) in a person's position (thesis) and overcomes them by finding the synthesis. Additionally, in DBT a client cannot be understood in isolation from his or her environment and the transactions that occur. Rather, the therapist emphasizes the transaction between the person and their environment both in the development and maintenance of any disorders. It is also assumed that there are multiple causes as opposed to a single factor affecting the client. And, DBT uses a framework that balances the treatment strategies of acceptance and change - the central dialectical tension in DBT. Therapists work to enhance the capability (skills) of their client as well as to develop the motivation to change. Maintaining that balance between acceptance and change with clients is crucial for both keeping a client in treatment and ensuring they are making progress towards their goals of creating a life worth living.\nDBT clearly articulates the functions of treatment that it addresses. They are:\n1. to enhance an individual's capability by increasing skillful behavior,\n2. to improve and maintain a client's motivation to change and be engaged with treatment,\n3. to ensure generalization of change occurring through treatment,\n4. to enhance the motivation of therapists to deliver effective treatment, and\n5. to assist the individual in restructuring or changing his or her environment such that it supports and maintains progress and advancement towards goals.\nWhat to expect in DBT treatment\nWe hope that understanding what to expect from DBT will help you be an informed consumer. While not all treatment delivery will include every element listed below depending on the Stage of treatment needed, what follows is a description of the elements of evidence-based DBT delivery for clients in need of the most intensive form of DBT to begin with.\nThere are four things that comprehensive DBT includes.\n1. Individual Therapy (where you work one-on-one with your DBT therapist to identify and work on your goals and targets and how to apply new skills and strategies.)\nDBT individual therapy sessions are organized by a target hierarchy. The first focus is to assess and problem-solve any suicidal or non-suicidal self-injurious behaviors (including urges, fantasies and thoughts). The second target is to address any behaviors that get in the way of actually doing the therapy – like arriving late, cancelling appointments or other “therapy interfering behaviors”. (Therapy interfering behaviors of the therapist are included in this target.) The third target of individual therapy is to problem-solve barriers to developing and maintaining a quality of life that facilitates building and sustaining a “life worth living”. DBT therapists will apply a variety of strategies within individual sessions to facilitate your goals, including a DBT diary card (a way of tracking what you are working on and the progress you are making), learning and using DBT skills and doing Behavioral Chain Analysis (a way of discovering what maintains a behavior you know is a problem but just seem to keep repeating. It helps uncover the chain of controlling variables that lead up to the problem behavior and helps you “break the links” in the chain. Once again you will be learning new skills to use instead of your old behaviors).\n2. Skills Training (Core Mindfulness, Emotion Regulation, Interpersonal Effectiveness and Distress Tolerance)\nSkills training is typically delivered in a group format that meets one time per week for approximately 2.5 hours. Skills training is organized in two segments - review of homework practice from the preceding week and teaching of skills.\n3. Phone Coaching - where you will have access to your DBT therapist for assistance in your day-to-day life when you need help in applying the skills when they really matter and may be hard to do. It is NOT therapy over the phone. It is coaching of skills in the moment.\n4. Consultation Team for therapists –\nDBT therapists attend weekly DBT consultation team meetings in order to consult with one another about their delivery of DBT with their clients. The focus of the meeting is to help each therapist deliver DBT with fidelity to the treatment manual."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:d10a6464-b9c7-4708-85b2-45cdc98890c6>","<urn:uuid:fc9daf91-0ce9-411b-badf-c0b86dd42a9f>"],"error":null}
{"question":"What's the difference between how marine bacteria and pathogenic bacteria feed?","answer":"Marine bacteria and pathogenic bacteria have different feeding mechanisms. Marine bacteria gather on carbon-rich particles and use chemical signals (AHLs) to coordinate with other bacteria before collectively releasing enzymes to break down complex molecules into digestible bits like amino acids. In contrast, pathogenic bacteria can feed by directly attaching to host cells and using them for nutrition, stealing nutrients like iron, or secreting enzymes that digest elements around them to turn them into food. Some pathogenic bacteria can even trick host cells to swallow them up and multiply inside until the cells rupture.","context":["It’s wondrous how the vast and the infinitesimal combine to make our planet work. Scientists at Woods Hole Oceanographic Institution (WHOI) have found that bacteria in the ocean, gathering in sort of “microbial block parties,” communicate and cooperate with each other to have a significant impact on how carbon dioxide is transferred from the atmosphere to the deep sea.\nIn this newly discovered mechanism, bacteria coalesce on tiny particles of carbon-rich detritus sinking in the ocean. They send out chemical signals to discern if other bacteria are in the neighborhood. If enough of their compadres are nearby, the bacteria en masse commence sending out enzymes that break up the particles into more digestible bits (see interactive).\nAs a result, a substantial amount of carbon does not sink to the depths, which affects both the marine food web and the planet’s climate. The re-released carbon can be reused by marine plants, and less carbon dioxide, a heat-trapping greenhouse gas, is drawn out of the air into the ocean. In addition, less carbon is effectively transferred to the bottom of the ocean, from where it cannot easily return to the atmosphere.\nThe finding represents the first evidence that bacterial communication plays a crucial role in Earth’s carbon cycle. It was reported in the Sept. 29, 2011, online issue of Environmental Microbiology Reports, by WHOI marine biogeochemists Laura Hmelo, Benjamin Van Mooy, and Tracy Mincer.\nThe carbon cycle works like a planetary conveyor. Carbon is spewed into the atmosphere in the form of carbon dioxide gas—naturally and gradually over geological time from volcanoes and plants, but much more rapidly in recent history from industrial smokestacks and cars. The gas is absorbed by photosynthetic plants, which convert it into bigger organic carbon molecules that they use as food and building blocks for their cells. The oceans harbor multitudes of marine plants; the detritus created when they die or are eaten is sticky and it agglomerates into particles that are heavy and start to sink. These particles, raining into the deep, are sometimes visible and often called “marine snow.”\nMarine bacteria see food in these particles and begin to colonize them. But the carbon- and nitrogen-rich material is in the form of complex molecules too big to get through bacterial cell membranes. “A protein is too big,” Hmelo said. “The bacteria want one amino acid.”\nThe bacteria can manufacture enzymes to break down the molecules, but here’s the catch: It costs precious energy to make the enzymes, and a lone bacterium doesn’t stand a chance.\n“For any one bacterium to engage in this, there’s a risk that they wouldn’t get a return on their investment,” Van Mooy said. “It would have to send out enzymes and hope that they hit something and blow it up so they can get something to eat out of it.”\nInstead, the bacteria conserve their resources by making other chemicals, called acylated homoserine lactones, or AHLs, which cost much less energy to produce, Hmelo said. They send these AHLs into the environment to see if any other bacteria are around.\nIf no other AHL-producing bacteria are in the vicinity, the AHLs will diffuse away and quickly degrade, and bacterial “silence” will prevail. But if enough AHL-producing bacteria are nearby, the concentration of AHLs outside the bacteria will eventually rise. That’s the chemical signal to each bacterium that it’s got a lot of buddies in the area. In this way—a process called quorum sensing—the bacteria sense that they have achieved a quorum, in this case a sufficient density to begin to make and release enzymes. You might think of it as the microbial equivalent of sending out tweets to organize a block party.\nThe scientists investigated this unexplored mechanism by using mesh traps to collect particles of marine snow sinking in Clayoquot Sound off Vancouver, where fjords extend into the ocean. Tides go back and forth over the shallow bottoms of the fjords, stirring up nutrients that spark blooms of marine algae.\nBack in the lab at WHOI, the scientists incubated the collected particles and confirmed the presence of AHLs in the marine snow. They also added AHLs to samples and found that the chemical signals stimulated bacteria to make enzymes that broke apart the particles—confirming that bacteria “talk about it” before launching a coordinated assault.\nIn the natural world, Van Mooy said, only about 10 percent of particles makes it all the way to the ocean depths; the scientists theorize that this quorum-sensing mechanism plays a key role in disaggregating most of the particles.\n“So microscopic bacteria buffer the amount of carbon dioxide in the atmosphere through their ‘conversations,’ ” he said. “I think it’s amazing that there is an infinite number of these conversations going on in the ocean right now, and they are affecting Earth’s carbon cycle.”\nThe research was funded by the National Science Foundation.","Bacteria are a vast group of unicellular microorganisms, visible only under the microscope. They populate virtually every natural environment and interact with essentially every living organism. They are actually one of the first life forms and have evolved into an incredibly wide range of microorganisms. For the most part, bacteria are either harmless or beneficial and co-exist with living organisms. However, a small number are pathogenic and produce disease in various life forms. Bacteria that are pathogenic to humans are usually classified as gram-negative and gram-positive bacteria, differentiated by shape, peculiarities of cell structure and life-sustaining activities.\nWhat are bacteria?\nBy definition, bacteria denominates one of three major divisions of cellular life and refers to a large and varied group of generally microscopic microbes. Almost all forms are microscopic, meaning they are visible only under the microscope, but not with the unaided eye. There are two exceptions to this: Thiomargarita namibiensis and Epulopiscium fishelsoni. Bacteria are unicellular, or single-cell organisms, although they can organize and form multi-cellular structures. Bacteria that organize in multiple-cell structures are more efficient and resilient to antibacterial agents. Because they exhibit life-sustaining activities, they are considered a type of life form.\nOne of their distinctive traits is their lack of a nucleus, a central element in the cell that contains the cell’s genetic material and regulates DNA replication. Bacteria also don’t have specialized cell subdivisions enclosed in separate membranes to carry out different functions. Those who do have some semblance of such subdivisions do not have them separated from the rest of the cell material (cytoplasm). In addition to a cell nucleus, bacteria also lack a subdivision called mitochondria, the powerhouse of the cell responsible for generating energy for cell processes that regulate its life-sustaining activities. They will, however, have different types of such subdivisions with similar functions. After all, bacteria too need to convert some sort of food or fuel into energy to be able to grow and multiply. They are essentially simpler, one-cell life-forms.\nBacteria types and shapes\nBacteria can be successfully classified according to their morphology into several categories.\nBacteria according to cell wall structures are commonly classified as gram negative and gram positive. The classification is obtained as a result of the Gram staining test.\n1) Gram positive bacteria have a thick, outer cell wall made of what is known as peptidoglycan which surrounds the cell membrane. This thick outer layer serves a protective role. During the Gram stain test, the peptidoglycan retains a purple dye, which is confirmation of the bacteria being gram positive. Read more about the difference between gram negative and gram positive bacteria.\n2) Gram negative bacteria have an outer membrane, a thin layer of peptidoglycan and a cell membrane. The outer membrane serves as protection against some antibiotics and prevents them from reaching the peptidoglycan layer and causing it do malfunction. The thin layer of peptidoglycan of these bacteria does not retain the initial purple dye applied in the first phase of the Gram stain test, only the second pink dye. This result is what allows for the classification as gram negative.\nBacteria according to shape:\n1) Spherical or ovoid (coccus, cocci)\n2) Rod-shaped (bacillus, bacilli)\n3) Spiral-shaped (spirillum, spirilla)\n4) Filamentous, elongated shapes.\nBacteria according to organizational structures. Bacteria are single-cell organisms, but have the ability to organize in structures for more efficient use of resources, better survival rates and increased resistance to antibacterials. They are known as micro-colonies.\nLesser organizational structures of bacteria can be called arrangements and include pairs, chains, bunches, clusters, threadlike filaments, helix arrangements and others.\nBacteria according to size. Microscopic forms are usually between 0.5 micrometers (half of a millionth of a meter in length) and 5 micrometers (5 millionths of a meter in length). Bacteria smaller than 0.1 to 0.3 micrometers are called ultramicrobacteria. Thiomargarita namibiensis (not microscopic) is the largest bacterium, measuring between 100-750 micrometers (or 0.1 to 0.75 millimeters). Epulopiscium fishelsoni (also not microscopic) measures between 200-700 micrometers (or 0.2 to 0.7 millimeters).\nBacteria according to metabolism (life sustaining activities) can be divided into several categories:\n1) Aerobic or anaerobic or facultative.\nAerobic bacteria use oxygen to produce energy, while anaerobic bacteria do not require it. Bacteria can be obligate aerobe organisms, meaning they rely on oxygen to grow (example: Mycobacterium tuberculosis). Some are facultative anaerobe, meaning they use oxygen if available to produce energy for the cell to use, but are also capable to survive and grow without it. Obligate anaerobic bacteria are damaged by the presence of oxygen (example: all Clostridium). Microaerophile bacteria require low oxygen concentrations (up to 10% atmospheric oxygen concentration), but are damaged by higher values (example: Helicobacter pylori). Aerotolerant anaerobes do not require or use oxygen if present, but are not damaged by it (example: Streptococcus mutans).\n2) Fermenting bacteria (example: Lactobacillus) rely on fermentation to produce energy for the bacterial cell and support its metabolism. They can be facultative anaerobic or microareophilic. These are the bacteria that convert sugar or starch into lactic acid.\n2) Motile or non-motile. How do bacteria move? For example, Escherichia coli have a flagellum, a tail-like projection that allows them to move. Helicobacter pylori have multiple such projections that allow them to eat their way into the stomach lining. Neisseria meningitidis has shorter, hair-like protuberances that allows it to attach to cells. Myxococcus xanthus moves by propelling itself in a gliding movement. Pseudomonas aeruginosa moves by crawling over surfaces with the help of hair-like projections called pili. Klebsiella pneumoniae and others are not motile.\n3) Spore forming or non-spore forming. Gram positive bacteria like Bacillus and Clostridium form spores. In simple words, spores are inactive seed-like productions that carry the genetic material of a bacterium. They do no grow or multiply and are resilient to external factors that would normally cause the active bacterium to cease to be viable. They can however produce infection once the right conditions are met, ensuring the propagation of the bacterium strain. Most gram negative bacteria do not form spores.\nBacteria types in relation to host with examples\nSome bacteria thrive in the environment, others need to be in a relationship with a host to live, grow and multiply. This relationship bacteria-host is called symbiosis and is a form of relatively long-term biological partnership. According to it, the following types arise:\n1) Commensal bacteria: they benefit from their host, but don’t help or harm it. Some strains of Staphylococcus aureus are commensal bacteria. They are normally part of the human skin micro-flora or present in the mucous membranes of the nose or mouth, where they thrive thanks to conditions provided by the host. However, they are not pathogenic and do not cause infection and disease. At the same time, they do not provide any benefits for their host.\n2) Mutualistic bacteria: they benefit from the association with a host and, at the same time, produce beneficial outcomes for said host. Examples: probiotic human gut bacteria (also referred to as gut flora or microbiota). For the most part, they produce various metabolites that inhibit the growth of other pathogenic bacteria or fungi and contribute to normal gut flora populations in exchange for nutrients. This often results in better immunity for the host, especially at the level of the gastrointestinal tract.\nFor example, Lactobacillus is known for converting carbohydrates (sugars, starch) into lactic acid in the digestive tract through a process called fermentation. Other Lactobacillus species help synthesize vitamin K or B vitamins like vitamin B12 or vitamin B9 (folic acid) or inhibit Candida growth. Some mutualistic strains help prevent tooth decay.\n3) Pathogenic bacteria: they infect a host and produce disease. Some infect specific tissues, some can only produce disease inside a cell, others are opportunistic and require specific conditions to cause disease (like lowered immune system defenses from other diseases). Some strains will feed off other microorganisms. Examples of bacteria pathogenic to humans and the diseases they may cause:\n– Escherichia coli: diarrhea, traveler’s diarrhea, meningitis.\n– Haemophilus influenzae: bacterial meningitis, pneumonia, bronchitis, infections of the upper respiratory tract.\n– Helicobacter pylori: gastritis, gastric ulcers, stomach cancer.\n– Mycobacterium: tuberculosis.\n– Pseudomonas aeruginosa: pneumonia, endocarditis (type of heart inflammation), sepsis.\nWhat do bacteria do?\nBacteria are living organisms and their purpose is to essentially survive, grow and multiply. In order to do so, they may need a host to get food from and support life sustaining activities (metabolism). Depending on the type of association with a host, they may produce infection and disease, benefits for the host or neither of the two.\nHow do bacteria make you sick?\nBacteria can make us ill in several ways. One way is by attaching to host cells and using them for their own nutrition. So basically they feed off host cells to support their own life sustaining processes (metabolism) which allows the bacteria to grow, multiply and make you sick. Some bacteria steal nutrients such as iron to support their metabolism. Others trick host cells to swallow them up. This alone affects the cell. Some move on from one cell to another. Most divide inside our cells until they rupture and release the multiplied bacteria.\nWhen Gram negative bacteria come into contact with immune system cells, they release toxins from their outer cell wall which generate inflammation and cause symptoms such as fever, diarrhea, aches or even sepsis in severe cases. Only pathogenic bacteria make you ill and not all of them all the time. Some require very specific conditions to produce disease and are called opportunistic pathogens.\nHow do bacteria feed to grow and multiply?\nDifferent bacteria have different metabolisms, meaning they use different elements to grow and multiply as well as obtain them through various processes. Bacteria may feed on organic elements such as carbohydrates, protein or fats or use water. Different species may rely on carbon, hydrogen, sulfur, iron or ammonia oxidation to produce energy for the cell and sustain metabolic processes. Some can produce carbon themselves, for example, while others ingest it from their environment. There are those that secrete enzymes that digest elements around them and turn them into food. Aerobic, anaerobic respiration, fermentation are three common processes through which bacteria produce energy to fuel themselves.\nHow do bacteria grow and multiply?\nOnce they are in the right environment, bacteria need necessary food or fuel to use for life-sustaining processes. These processes allow them to grow. Bacteria essentially multiply by creating clones of themselves, a process called fission. One bacterial cell will grow, then replicate its DNA which will then move to the opposite part of the cell. Finally, the cell becomes longer and divides, forming two identical copies or clones (binary fission). Sometimes, one cell can create several copies (multiple fission). It can take anywhere from less than 10 minutes to 30 minutes and up to more than 3 days for bacteria to multiply.\nThis post was updated on Friday / July 31st, 2020 at 11:10 PM"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:7949de0b-d86b-4428-8443-94518e6883db>","<urn:uuid:d8c7d4a6-d422-4c84-a398-826adec378f9>"],"error":null}
{"question":"¿Cuál es la diferencia principal entre un proceso de mediación familiar y un caso de small claims en términos de alcanzar un acuerdo? What are the key differences in reaching agreements?","answer":"The main difference is that Family Dispute Resolution mediation is completely voluntary in reaching agreements, while in Small Claims cases, if one party fails to respond or show up, the judge can enter a default judgment. In Family Dispute Resolution, both parties need to actually agree for any resolution to be reached, which increases the likelihood of compliance. In Small Claims court, the judgment is a written decision that will be enforced regardless of mutual agreement, though the court does not guarantee collection of judgment monies.","context":["Family Dispute Resolution is a collaborative negotiation process facilitated by a post graduate trained family dispute resolution practitioner designed to help people reach agreement about parenting and property settlements after separation.\nInteract Online provides our services fully online to make sure that we don’t have waiting lists, that our services are safe even if there has been family violence in the past and that you can get started as soon as you are ready.\nBefore Mediation you must participate in a pre-mediation session. https://interact.support/product/fdr-pre-mediation/\nStage One – Identifying the issues\n1. Mediators Opening Statements – reminding you that it is a confidential process that can’t be used as evidence in court. They will remind you of the process and ground rules that will ensure that you both feel safe and treat each other respectfully in the way you speak and in considering each other’s points of view.\n2. Parties Opening Statements – the mediators will assist each of you in turn to identify the issues you want to resolve during the session.\n3. Agenda for the mediation – based on what each of you has identified as being important to resolve the mediators will establish an agenda for the mediation session to guide the resolution process.\nStage Two – Understanding the issues and options\n4. Exploration of the issues – you will now speak directly with each other to ensure that you have a common understanding of the issues and their impact on each of you. The mediators will facilitate the discussion and help make sure that you don’t fall into negative conversational patterns from past unconstructive attempts to resolve the dispute. No debating or arguing about each other’s point of view!\n5. Brainstorming options for resolution – once you both understand the issues from each other’s perspectives the mediators will guide you towards starting to look at ways to resolve them. This part of the mediation is part brainstorming session, part problem-solving. It is not the same as what you might have done in the past to try and force each other to agree to your ideas for what needs to happen. If you have properly engaged in the exploration phase you should have new ideas for a resolution that will be more acceptable to both of you. The FDR Practitioner will make sure that you talk about the impact of the proposals on your children.\n6. Private Sessions – this is the individual and confidential check-in session. It may have happened earlier if someone is a bit stuck in old thinking or requests a break or check-in but will definitely happen before the negotiation begins to lock down agreed actions so that you have a chance to stop and reflect on what has happened so far.\nStage Three – Agreements and future actions\n7. Negotiation – negotiation is the process of making and considering proposals for future courses of action. If the mediation is productive each issue that you have identified as being important to resolve will have some sort of resolution negotiated. The mediators will help you to look at the proposals from various perspectives so that you consider the short and long term potential consequences as well as any other people who will be impacted by what is proposed.\n8. Agreement – reaching an agreement in Family Dispute Resolution mediation is voluntary. That is a good thing because if you only need to agree if you actually agree it is more likely that you will both actually try to do what you have said you will do. The mediators will help you to document the agreement you reach in language that makes sense to you.\nIn the case of parenting agreements, you can sign the agreement once you are ready and then it becomes a Parenting Plan. We’ll provide you with information about what that means before the mediation session.\nIn the case of property settlement agreements the agreement doesn’t become legally enforceable just because you sign it. You need to either apply for Consent Orders or get a binding financial agreement drawn up. Both processes usually involve the assistance of lawyers so we’ll talk about what is needed if you do reach a suitable property settlement agreement.\n9. Closing the session – At the end of the time scheduled for the mediation, the mediator will close the session, talk with you about how they will get any documented agreement to you and ask if you want to book in for any further mediation or review sessions. It is then up to you to take the next steps to resolve the dispute by doing what you have said you will do.\nFDR Parenting Process\nThe FDR Parenting process steps. Breaks or private sessions with each of you can occur at any time.\nFDR Property Process\nThe FDR Property process steps. Breaks or private sessions with each of you can occur at any time.\nFrequently Asked Questions\nWe have hardship rates available for people who are on a very low fixed income. If you are on a low income and feel that you can’t afford to pay full fee for our services contact us to request hardship considerations.\nFamily Dispute Resolution is a voluntary process but there is an expectation set by the family court that people attempt to resolve their issues before applying to the court for a judgement.\nThat creates a bit of an unusual situation where you can refuse to participate but it isn’t a good idea. If you have concerns about safety or your ability to negotiate freely talk to your mediator. It is their job to make sure that the process is safe and fair.\nIf Family Dispute Resolution about a child doesn’t proceed the Family Dispute Resolution practitioner will issue a section 60i Certificate. The type of certificate will vary based on the situation.","Small Claims Court\nFrequently Asked Questions\nWhat is a small claims action?\nIt’s when there is a dispute between two parties, and the issue in controversy does not exceed $5,000. You should review Chapter 34 of the Florida Statutes for a description of which causes of action are covered under small claims in the county court. Also review the small claims rules of procedure located in section 7 of the Florida Rules of Court for more detailed information. The statutes and rules of procedure which apply will depend upon the type of case filed. It is strongly recommended that you familiarize yourself with the laws that apply to your particular case by visiting your local law library.\nThe first step in filing a small claims action is obtaining and filling out the necessary forms usually consisting of a statement of claim and a notice or service of process which requires the parties to appear at a specified pre-trial conference. Small claims forms are available at your local Clerk of Court's Office.\nWhat happens after filing?\nOnce the statement of claim has been completed and filed with the clerk of court, the plaintiff must provide the defendant with the copies of the statement of claim and the service of process form. In a small claims case, there are two general ways to provide the defendant with these forms.\nIf the defendant lives in Florida, the statement of claim and services of process form can be sent by certified mail through the United States Postal Service. If the defendant refuses to accept the service of process through certified mail, or does not live in the Florida, or is a corporation within Florida, the statement of claim and service of process must be delivered to the defendant by the sheriff of the county where the defendant lives or is located. A private process server who is authorized to serve legal papers may also serve the defendant.\nOnce the defendant has been served, the process may continue. If service of process is done incorrectly or not done at all, the case may not proceed. Once the defendant has been served with the statement of claim and the service of process form, both parties must attend a pre-trial conference.\nWhat happens at the Pre-trial Conference?\nAt the Pre-trial Conference, the judge will review the pleadings and documents and may simplify the issues, refer the case to mediation, and take care of any other matters as needed. You may settle the case with the other party before or at the pre-trial conference by entering into and filing a stipulation agreement. This agreement can settle all or part of the case and becomes part of the court order.\nSometimes, the participation of a third, neutral party is helpful in aiding the litigants in reaching an agreement. This process is called mediation, and your case may be referred to it at the pre-trial conference.\nWhat if we are not able to reach an agreement through mediation?\nIf the parties are unable to reach an agreement through stipulation or mediation, they must appear for trial on the date and the time scheduled by the Clerk during the pre-trial conference. When the defendant fails to respond or show up to defend against the statement of claim, the Clerk or the judge may enter a default judgment. All parties are bound by the applicable law and the court cannot help you to present your case. The parties must make certain to bring all evidence and witnesses to the trial but keep in mind that letters, affidavits, and estimates may not be accepted as evidence.\nWhat is a judgment?\nA judgment is the written decision of the case and includes the amount of money to be paid by one party to another, acts which must be performed, or property which must be transferred. After the judgment is signed, it will be recorded in the county's public records and will be provided to the parties either at the conclusion of the trial or later by mail.\nIf the parties feel the judge has made an error, they have ten days from the date the judgment is entered by the court to file with the clerk for a rehearing.\nAfter the judgment is entered, the winning party may need to execute and enforce the terms of the judgment in order to collect money or property. If judgment monies are paid in full, the plaintiff must furnish the defendant with a completed satisfaction of judgment form which can be obtained from your local Clerk of Court.\nPlease note that the Court does not guarantee collection. It is important to realize that just because you have won a judgment does not mean collection of judgment monies is guaranteed. The Court cannot and does not guarantee collection of judgment monies.\nSmall Claims Resources"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_expressive"}],"document_ids":["<urn:uuid:0be91abe-4cff-4297-a181-c855d7b0f94c>","<urn:uuid:4337f2ae-7896-4ff0-b43b-167d36451cf3>"],"error":null}
{"question":"Which is more common between GERD and genetic inheritance como causa of Barrett's esophagus?","answer":"GERD (gastroesophageal reflux disease) is more common as a cause of Barrett's esophagus. The documents indicate that most cases of Barrett's esophagus arise from GERD, a chronic condition where stomach acid regurgitates into the lower part of the food pipe. While genetic factors do play a role in susceptibility to Barrett's esophagus, environmental factors such as obesity, gastroesophageal reflux, smoking, and diet are largely responsible for the increasing incidence of the condition.","context":["A report on the study — which was led by a team from Columbia University Medical Center (CUMC) in New York City, NY, and published in the journal Nature — describes how they used mice and human tissue to pinpoint the \"cell of origin\" for Barrett's esophagus.\nIn Barrett's esophagus, some of the tissue that lines the esophagus, or the tube that carries food from the mouth to the stomach, changes into tissue that is more like that which lines the intestines. This can be felt as heartburn and difficulty swallowing food.\nMost cases arise from gastroesophageal reflux disease (GERD), a chronic condition in which acid from the stomach regurgitates into the lower part of the food pipe.\nBetter screening and treatment needed\nThe incidence of esophageal adenocarcinoma has increased rapidly in the United States over recent decades. But unfortunately, this has not been matched by improved screening and treatment.\nAs with other cancers, early detection is the key to prolonging survival in esophageal cancer. At present, under 20 percent of patients survive for longer than 5 years after diagnosis.\nScientists probing the origins of Barrett's esophagus have put forward models based on at least five different cell types.\n\"However,\" explains study leader Jianwen Que, an associate professor of medicine at CUMC, \"none of these experimental models mimics all of the characteristics of the condition.\"\nHe and his colleagues believed, therefore, that the \"cell of origin\" for Barrett's esophagus was still waiting to be discovered.\nThe gastroesophageal junction\nThe researchers started their investigation by breeding mice genetically engineered to develop Barrett's esophagus, and then examining the area where the food pipe joins the stomach. This area, called the gastroesophageal junction, is where abnormal tissue that is typical of Barrett's oesophagus occurs.\nHere, at the gastroesophageal junction, the tissue that lines the digestive tract, or the epithelium, changes gradually — as it nears the stomach — from that of the food pipe to that of the intestines. At this \"transitional epithelium,\" the cell types transition from \"stratified squamous epithelium cells\" to \"simple columnar cells.\"\nProf. Que explains that, while the examination revealed that all the \"known cells in this tissue remained the same,\" the team found \"a previously unidentified zone populated by unique basal progenitor cells.\"\nProgenitor cells are stem cells that have just started to differentiate. Much like stem cells, they still have the potential to become different types of cell, but they have already started down the path that will lead to a particular tissue type.\n'Cell of origin' for Barrett's esophagus\nIn the next phase of the study, the team used a method called \"lineage tracing\" to find out whether the unique progenitor cells that they discovered can develop into Barrett's esophagus.\nThey used several mouse models to show how genetic changes or exposure to bile acid reflux can cause the cells to grow and give rise to Barrett's esophagus.\nThe researchers also replicated these findings in \"organoids\" grown from unique basal progenitor cells sampled from the gastroesophageal junctions of mice and humans. Organoids are masses of cells that are grown in the laboratory and which have many of the tissue properties of organs.\nIn an accompanying article, experts comment that the findings set the stage \"to investigate whether the transitional epithelium is the sole origin\" of Barrett's esophagus, and \"what role this tissue has in the progression to oesophageal cancer.\"\n\"\"Now that we know the cell of origin for Barrett's esophagus, the next step is to develop therapies that target these cells or the signaling pathways that are activated by acid reflux.\"\nProf. Jianwen Que","An international consortium co-led by researchers at Fred Hutchinson Cancer Research Center and the QIMR Berghofer Medical Research Institute in Australia has identified four genetic variants associated with an increased risk of esophageal cancer and its precursor, a condition called Barrett's esophagus.\nThe findings, by corresponding author Thomas L. Vaughan, M.D., M.P.H., a member of the Epidemiology Program in the Public Health Sciences Division at Fred Hutch, are published online ahead of the December print issue of Nature Genetics. Vaughan co-led the project with co-author David Whiteman, Ph.D., head of the Cancer Control Group at QIMR (formerly known as the Queensland Institute for Medical Research).\nBoth are members of the international Barrett's and Esophageal Adenocarcinoma Consortium, or BEACON, an open scientific forum for research into the causes and prevention of esophageal cancer and Barrett's esophagus that involves more than 40 scientists in North America, Europe and Australia.\n\"Epidemiologic findings, largely based on the work of BEACON investigators, clearly demonstrate that environmental factors such as obesity, gastroesophageal reflux , smoking and diet are largely responsible for the rapidly increasing incidence and mortality from esophageal adenocarconima,\" said Vaughan, who serves as BEACON's chair and is also a professor of epidemiology at the University of Washington School of Public Health. \"However, a growing body of evidence also suggests an important role for inherited susceptibility.\"\nTo better understand the role of genetics in Barrett's and esophageal cancer, Vaughan and his BEACON colleagues pooled data and DNA specimens from 15 international studies conducted in the past 20 years to estimate the heritability of these conditions and identify genetic variants associated with increased risk. Altogether they gathered DNA samples and lifestyle risk-exposure data from more than 8,000 study participants, including about 5,500 with esophageal cancer or Barrett's esophagus and about 3,200 participants without these conditions who served as a comparison group.\nThe DNA samples were genotyped at Fred Hutch using a high-density array that allowed for the simultaneous and accurate assessment of more than 1 million genetic variants. To increase the statistical power of the study and its ability to identify potential causal genetic mutations, information on control subjects gleaned from public data repositories was added to the mix. The data analysis was conducted at the University of Washington in collaboration with the QIMR research group in Queensland.\nAfter combing through all of the data, the researchers identified genetic variants at three locations – on chromosomes 3, 9 and 19 – as being significantly associated with esophageal adenocarcinoma and Barrett's esophagus. In addition, they found that a genetic variant on chromosome 16 that had been previously linked to Barrett's esophagus was also associated with an increased risk of esophageal adenocarcinoma.\nVaughan and colleagues also found that the role of inherited susceptibility to this cancer appears to be much stronger in the early stages of disease – that is, the development of Barrett's esophagus – rather than the progression of Barrett's to cancer.\n\"These findings establish strong starting points for further epidemiologic studies to pin down the causal variants, and laboratory studies to identify the mechanisms by which the causal variants might affect the development of Barrett's esophagus and esophageal adenocarcinoma,\" Vaughan said. \"The fact that all four of the new loci are in or near genes associated with early development of the esophagus or already associated with oncogenic activity is particularly exciting, as it implies that we may be close to finding some important pathways in the development of this highly fatal disease.\"\nUltimately, the researchers believe these findings will contribute to the development of new screening tools to identify those at highest risk of esophageal adenocarcinoma and its precursor, particularly when combined with established risk factors such as obesity and gastric reflux. \"Down the line we anticipate that a better understanding of the pathophysiology of these diseases will lead to better and earlier treatments,\" Vaughan said.\nBarrett's is associated with chronic heartburn and affects an estimated 1 million to 2 million Americans. While the risk of developing esophageal cancer in a person with Barrett's is only about 0.5 percent per year, the outlook is grim if the disease is not diagnosed early. The majority of patients with invasive esophageal cancer die within a year of diagnosis.\nThis year, esophageal cancer will strike nearly 18,000 Americans and kill more than 15,000. Esophageal adenocarcinoma, which accounts for more than 60 percent of esophageal-cancer cases, is seven times more common in men than women.\nThe National Cancer Institute provided primary funding for this research, the first large-scale genome-wide association study of esophageal adenocarcinoma.\nAt Fred Hutchinson Cancer Research Center, home to three Nobel laureates, interdisciplinary teams of world-renowned scientists seek new and innovative ways to prevent, diagnose and treat cancer, HIV/AIDS and other life-threatening diseases. Fred Hutch's pioneering work in bone marrow transplantation led to the development of immunotherapy, which harnesses the power of the immune system to treat cancer with minimal side effects. An independent, nonprofit research institute based in Seattle, Fred Hutch houses the nation's first and largest cancer prevention research program, as well as the clinical coordinating center of the Women's Health Initiative and the international headquarters of the HIV Vaccine Trials Network. Private contributions are essential for enabling Fred Hutch scientists to explore novel research opportunities that lead to important medical breakthroughs. For more information visit http://www.fredhutch.org or follow Fred Hutch on Facebook, Twitter or YouTube.\nKristen Woodward | EurekAlert!\nCancer diagnosis: no more needles?\n25.05.2018 | Christian-Albrechts-Universität zu Kiel\nLess is more? Gene switch for healthy aging found\n25.05.2018 | Leibniz-Institut für Alternsforschung - Fritz-Lipmann-Institut e.V. (FLI)\nThe more electronics steer, accelerate and brake cars, the more important it is to protect them against cyber-attacks. That is why 15 partners from industry and academia will work together over the next three years on new approaches to IT security in self-driving cars. The joint project goes by the name Security For Connected, Autonomous Cars (SecForCARs) and has funding of €7.2 million from the German Federal Ministry of Education and Research. Infineon is leading the project.\nVehicles already offer diverse communication interfaces and more and more automated functions, such as distance and lane-keeping assist systems. At the same...\nA research team led by physicists at the Technical University of Munich (TUM) has developed molecular nanoswitches that can be toggled between two structurally different states using an applied voltage. They can serve as the basis for a pioneering class of devices that could replace silicon-based components with organic molecules.\nThe development of new electronic technologies drives the incessant reduction of functional component sizes. In the context of an international collaborative...\nAt the LASYS 2018, from June 5th to 7th, the Laser Zentrum Hannover e.V. (LZH) will be showcasing processes for the laser material processing of tomorrow in hall 4 at stand 4E75. With blown bomb shells the LZH will present first results of a research project on civil security.\nAt this year's LASYS, the LZH will exhibit light-based processes such as cutting, welding, ablation and structuring as well as additive manufacturing for...\nThere are videos on the internet that can make one marvel at technology. For example, a smartphone is casually bent around the arm or a thin-film display is rolled in all directions and with almost every diameter. From the user's point of view, this looks fantastic. From a professional point of view, however, the question arises: Is that already possible?\nAt Display Week 2018, scientists from the Fraunhofer Institute for Applied Polymer Research IAP will be demonstrating today’s technological possibilities and...\nSo-called quantum many-body scars allow quantum systems to stay out of equilibrium much longer, explaining experiment | Study published in Nature Physics\nRecently, researchers from Harvard and MIT succeeded in trapping a record 53 atoms and individually controlling their quantum state, realizing what is called a...\n25.05.2018 | Event News\n02.05.2018 | Event News\n13.04.2018 | Event News\n25.05.2018 | Event News\n25.05.2018 | Machine Engineering\n25.05.2018 | Life Sciences"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"opinion_recommendation"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:1ca312b4-0c18-4a2c-9aad-6bf3c31fae30>","<urn:uuid:cf79f2d6-d452-4111-94f3-af8e866907fe>"],"error":null}
{"question":"Looking at sustainable outdoor activities - how do traditional moose tracking methods compare to modern crypto mining in terms of resource consumption?","answer":"Traditional moose tracking relies on low-impact methods like following tracks (6.5 inches long), examining scat patterns, and looking for feeding signs like stripped bark and clipped fir bows. Hunters can use simple tools like canoes, metal coffee cans for calls, or manually constructed scent stations. In stark contrast, cryptocurrency mining requires enormous computational power and specialized equipment, consuming massive amounts of energy primarily from fossil fuels, leading to significant greenhouse gas emissions. While moose hunting uses minimal resources, crypto mining's energy consumption is comparable to that of entire countries.","context":["A moose hunt in the state of Maine, is a once in a lifetime opportunity afforded a small number of lucky sportsmen. The lottery provides the dismal odds of a 3-10 percent for residents and 1-3 for non-residents of being awarded a tag, depending on their number of accumulated bonus points. Considering the percentages, the probability of being drawn, is bleak. Not even in Vegas are those odds most of us would bet on!\nIf you are fortunate enough to be drawn, no assurance exists of being picked for the select moose hunting areas, unless specific reference was made on the initial application. An extremely lucky hunter will not only be drawn for a moose tag BUT it will be a bull / cow tag, timed to occur during the rut and in one of the favorable northern areas of the state. These sportsmen typically enjoy high rates of success.\nSuccess Rate - For the remaining hunters, their chances diminish rapidly when assigned to the central and southern areas of the state with low moose densities. In addition, the southern zone moose season occurs mostly outside of the moose rut and at a time when moose hunters will be in direct competition with deer hunters. These variables add more complexity to an already difficult hunt. While statewide moose hunters boast an impressive 85 percent success rate, hunters in more centralized zones like 23 and 22 see only a limited 13-15 percent chance of success. Unless these hunters are willing to invest serious time in the woods scouting or hire a Maine guide familiar with their assigned area, their chance of failure is great.\nHunting Options - While there are moose in the Central portion of the state, they are neither prevalent nor easy to find. Where northern Maine hunters have the opportunity to ride an extensive network of logging roads and hunt massive clear cuts that is simply not possible in central Maine. Moose hunters should expect to cover serious miles either on foot or via ATV scouting and spotting for moose. ATVs will allow you to cover a lot of territory but not some of the less accessible and isolated areas that hold larger moose populations. Sportsmen shouldn’t be afraid to stray far from the roads and trails and conduct hunts deep in the woods, a considerable distance from population centers. Towns such as Montville, Freedom, Palermo, Unity and Burnham contain such “moosy” areas and have higher than average yearly success rates, making them well worth exploring.\nThe option to hunt from a canoe is a clever way for moose hunters to travel into the backcountry with minimal effort. Canoes also make extraction of a moose a manageable chore, rather than a backbreaking endeavor. Canoes facilitate exploration of large waters like Dresden bog in zone 22 and Kingdom bog in Zone 23. These water based trips are best taken in the early morning or late evening and best accomplished slowly, hugging the shoreline, with one person paddling and the other in the bow carefully scanning the shoreline with a quality set of binoculars.\nCalling Moose - While hiking, ATVing or paddling hunters should incorporate calling sequences, followed by intent and careful listening for replies. Cow calling, bull grunting, shaking branches and the old trick of pouring water out of a large container (like a rubber boot) to simulate a moose urinating are all effective means of locating and/or drawing a moose into shooting range. Electronic calls in this situation are excellent, as many of the quality devices produce a decent level of volume. With practice, a metal coffee can and a cotton or leather shoestring are as effective as these electronic devices and cost mere pennies to construct. Many guides are able to vocalize moose calls using their mouths or with perhaps the assistance of a birch bark cone to increase the volume like an old school megaphone.\nMaking Sense of Moose Scents - Most hunters do not realize that moose, like deer, can be lured by sexual as well as curiosity scents. Moose are inquisitive creatures and will frequently investigate the smells of other moose or strange smells that are not perceived as dangerous. On many occasion, I have watched moose stick their heads in bear baits and sniff heavily, taking in the intoxicating smell of doughnuts. Hunters can use this trait to their distinct advantage, by using scents to pull them out of the deep woods and into shooting range. While I don’t recommend jelly doughnut as you scent spray, there are many other commercially available moose scents that are extremely effective.\nSeveral companies make different moose lures but my personal favorite is the type that is ignited and burns like an incense stick. A trick is to take a 5 gallon bucket and drill 8-10 ½ inch holes in the sides about 1 inch up from the bottom. Take a shovel and clear a patch of earth down to bare earth in an area slightly bigger than the bottom of a 5 gallon bucket. This “clearing” is to ensure that nothing catches fire while the incense sticks burn. Next take 2 or 3 sticks and poke them into the ground, light them and place the bucket over the top. The bucket will protect the slowly burning sticks from any potential rain or strong winds that could extinguish them, while still allowing the smoke to slowly escape. This set-up creates a huge scent cloud that will saturate the entire target area. Once allowed to burn all night, it is sometimes a simple matter of arriving early the next morning and shooting your moose as he drools over the smoldering bucket.\nTracking and Finding Moose - If you can manage to find moose sign it is preferable that you stay with it. Moose maintain a \"home territory\" of around one or two square miles. This does change a bit during the fall when bulls tend to wanderer, traveling up to 4 miles from their \"home\" area, in search of a suitable mate. Still compared to the travel patterns of other large game animals, this limited region allows the hunter to stake out prime travel areas in preparation of an encounter.\nThough a truly monstrous size animal with bulls nudging over 1200 pounds, they are still very difficult to locate in central Maine. Low numbers create the proverbial “needle in the hay sack” scenario, creating much difficulty in finding these titanic creatures. To locate a moose, you first need to find appropriate moose habitat. This can be done by studying your Gazeteer or using Google earth to virtually scout areas with limited human access, swamps and areas bordering small lily pad ponds.\nOf course, what can’t be seen using these “virtual” sources, is what land is and is not posted. Often hunting locations, identified on a map, turn into wasted scouting trips when you get there and realize they are gated and/or completely covered with posted signs. Even more disheartening is to scout a spot open to hunting, only to return a month later to find it posted as no trespassing. Trust me this happens all the time. Ultimately, your best alternative is to scout these areas early and find open areas, secure permission or know a local individual (or Maine Guide) who is very familiar with the area.\nOnce you locate one of these prime spots, you next need to thoroughly scout the area and attempt to locate sign. Moose sign is typically found by identifying fresh tracks, scat and/or noted feeding activity.\n*Moose Tracks - The main part of a moose track is about 6-1/2 inches long, with cows and young bulls have pointier tracks than adult bulls or deer. Track strides should measure 30\" to 40\" long. Because cow moose give birth they have a wider pelvic girdle than males. Therefore the rear leg spread (the distance between the legs) will be wider than that of a male. The tracks left behind by the female will show the rear foot as being set to the outside of the front foot, whereas the male footprint will be set in line with or slightly to the inside. In other words, when looking at the right hand side moose tracks the rear print will be (from a cow moose) on top of and to the right of the front track. This method is of course riddled with inaccuracies.\n*Moose Scat - Due to seasonal variety in a diet, moose scat tends to come in a number of different “flavors”, in accordance with what it has been eating. Scat varies widely between a meal of pond lilies and that of fir bow tips. The best way to see this difference is through pictures of moose scat linked below.\n*Moose Browse and Feeding Sign - Moose are notorious grazers, like cows they slowly munch along through the wilderness snacking on willow, alder and fir bows. Moose will strip bark off willow and alders trunks to get to the nutrient rich cambium layer. These types of disturbances can look like giant deer rubbing areas with dozens of trees affected. Moose will also create rubs on trees much like a deer to work the velvet off their antlers. Fir bows will be clipped cleanly off like a pair of hedge clippers cut them. Sap oozing from these cuts can help to determine if an area has seen recent activity.\nMoose can frequently be found, during early mornings and late evenings, patrolling shallow ponds and dipping their heads under the water to uproot their favorite food, the common water lily. These salt rich plants are a moose favorite. Hunters finding small ponds filled with these treats would be well served to stake out these spots in early morning and late evenings.\nConclusion - As a hunter, harvesting a moose is the pinnacle of an outdoorsman’s hunting career. To be fortunate enough to be chosen to pursue and potentially harvest the largest game animal in North America is truly a unique experience. I like nothing better than to help facilitate a sportsman successfully harvesting a moose, as their excitement in the endeavor is always infectious. Anyone is planning a central Maine moose hunt, please contact me and ask questions, I would be happy to assist.\nAdditional Moose Resource Links from Inland Fisheries and Wildlife:","As the world becomes more conscious of sustainability, the environmental impact of industries and businesses is under the microscope. Cryptocurrency is one of the newer and rapidly evolving industries that has been called out for its high energy consumption and carbon footprint. However, the cryptocurrency sector is also built on innovation, decentralization, and financial democratization. The challenge is to balance these two seemingly opposing goals – sustainability and innovation. In this article, we will explore the environmental concerns of cryptocurrency and ways to mitigate its impact while continuing to promote its benefits.\nRead more: Silvergate Collapse Dragging Down Bitcoin Volume\nCryptocurrency is a digital currency that uses encryption techniques to regulate the generation of units of currency and verify the transfer of funds. It is decentralized and operates on a peer-to-peer network that enables secure, fast, and anonymous transactions. Cryptocurrency has gained popularity in recent years, with bitcoin being the most well-known example. However, the process of generating cryptocurrency, known as mining, requires significant computational power, which consumes a massive amount of energy.\nThe amount of energy consumed by cryptocurrency mining has sparked concerns about its environmental impact. According to a 2021 report by the Cambridge Center for Alternative Finance, the annual energy consumption of bitcoin mining alone is estimated to be around 128.84 TWh, which is more than the energy consumption of Argentina. The high energy consumption of cryptocurrency mining has led to an increase in greenhouse gas emissions, as the majority of the energy used comes from non-renewable sources like coal and natural gas.\nThe Environmental Impact of Cryptocurrency\nCryptocurrency mining is a highly energy-intensive process that requires specialized computer equipment and software. The process involves solving complex mathematical problems to verify transactions and add new blocks to the blockchain. The first miner to solve the puzzle is rewarded with a certain amount of cryptocurrency.\nTo mine cryptocurrency, miners need to use powerful computers, which consume a considerable amount of energy. The energy consumption of cryptocurrency mining is a function of the computing power used and the time it takes to solve the problem. As the difficulty of the problem increases, more computing power is required, which leads to a higher energy consumption.\nThe high energy consumption of cryptocurrency mining has a significant impact on the environment. Most of the energy used to power cryptocurrency mining comes from non-renewable sources like coal and natural gas. The combustion of fossil fuels leads to the release of greenhouse gases like carbon dioxide, which contributes to global warming and climate change. The increase in greenhouse gas emissions from cryptocurrency mining is a concern as it undermines global efforts to reduce carbon emissions.\nMitigating the Environmental Impact of Cryptocurrency\nThe environmental impact of cryptocurrency mining can be mitigated by adopting sustainable practices. Some of the ways to reduce the energy consumption of cryptocurrency mining are:\nUsing renewable energy sources: One of the most effective ways to reduce the environmental impact of cryptocurrency mining is to use renewable energy sources like solar, wind, and hydroelectric power. Some cryptocurrency mining companies are already using renewable energy to power their operations. For example, the cryptocurrency mining company, Square, has committed to becoming carbon neutral by 2030.\nDeveloping energy-efficient mining equipment: Cryptocurrency mining companies can reduce their energy consumption by developing energy-efficient mining equipment. For example, some companies are developing ASICs (application-specific integrated circuits) that are designed to consume less energy than traditional computer processors.\nImplementing proof-of-stake consensus mechanism: Another way to reduce the energy consumption of cryptocurrency mining is to implement the proof-of-stake consensus mechanism. Proof-of-stake is an alternative to proof-of-work, which is the current consensus mechanism used by most cryptocurrencies. Proof-of-stake does not require miners to solve complex mathematical problems to verify transactions. Instead, it relies on a random selection process to choose a validator who is responsible for verifying transactions. Validators are required to hold a certain amount of cryptocurrency, which acts as a stake. If a validator acts maliciously, their stake is forfeited. The proof-of-stake consensus mechanism is less energy-intensive than proof-of-work, making it a more sustainable alternative.\nCarbon offsetting: Cryptocurrency mining companies can offset their carbon emissions by investing in renewable energy projects or purchasing carbon credits. Carbon offsetting is a way to neutralize the carbon emissions associated with cryptocurrency mining by investing in sustainable projects that reduce carbon emissions.\nBalancing Sustainability and Innovation\nCryptocurrency has the potential to revolutionize the financial industry by providing financial democratization and decentralization. However, this cannot come at the expense of the environment. The challenge is to balance sustainability with innovation. The cryptocurrency industry needs to take responsibility for its environmental impact and take steps to mitigate its carbon footprint. It is essential to recognize that sustainability and innovation are not mutually exclusive goals, but rather complementary.\nSustainability is a key factor in the long-term success of cryptocurrency. As the world becomes more conscious of the environment, consumers are looking for sustainable products and services. The cryptocurrency industry needs to recognize this trend and take steps to reduce its carbon footprint. By adopting sustainable practices, the cryptocurrency industry can attract a more environmentally conscious audience.\nCryptocurrency has the potential to transform the financial industry by providing financial democratization and decentralization. However, the high energy consumption of cryptocurrency mining has sparked concerns about its environmental impact. To balance sustainability and innovation, the cryptocurrency industry needs to take responsibility for its carbon footprint and adopt sustainable practices. By using renewable energy sources, developing energy-efficient mining equipment, implementing proof-of-stake consensus mechanism, and carbon offsetting, the cryptocurrency industry can mitigate its environmental impact. Sustainability is not a trade-off for innovation, but rather a complementary goal that is essential for the long-term success of the cryptocurrency industry."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:9ae1ba4f-1e70-4d1c-986e-169331b2eeed>","<urn:uuid:16d423b7-32cb-493a-822d-c068233478c4>"],"error":null}
{"question":"¿Cuál tiene más impacto en el confort interior: el sistema de enclosure o el HVAC system?","answer":"Both systems are crucial for indoor comfort but serve different functions. The enclosure (building envelope) provides the fundamental separation between interior and exterior environments, controlling heat flow, air flow, water vapor, and other environmental phenomena. Meanwhile, the HVAC system actively manages indoor comfort through temperature control, ventilation, and air quality maintenance. While the enclosure creates the basic conditions for habitability, the HVAC system allows for precise control and optimization of the indoor environment, including temperature, humidity, and air quality. Both systems work together, as an efficient HVAC system requires a well-designed enclosure to function optimally.","context":["- Project plans\n- Project activities\n- Legislation and standards\n- Industry context\n- Specialist wikis\nLast edited 21 Jan 2021\nPrinciples of enclosure\n‘Enclosure’ is the term given to any part of a building that physically separates the external from the interior environment. It is often referred to as the ‘building envelope’, although ‘enclosure’ is considered the more precise term.\nHuman physiology is capable of tolerating only a narrow range of environmental conditions. Beyond this range, health and wellbeing are compromised. Through the materialisation of volumes, architecture is able to create enclosed spaces in the form of structures. A building consists of a collection of spaces bounded by separators of the interior environment, and separators of the exterior environment (the enclosure).\n- The roof system.\n- The above-grade wall system (including windows and doors).\n- The below-grade wall system.\n- The base floor system.\n- Strength and rigidity.\n- Control of heat flow.\n- Control of air flow.\n- Control of water vapour flow.\n- Control of liquid water movement.\n- Stability and durability of materials.\n- Aesthetic considerations.\nIn addition to Hutcheon’s principles, there are also considerations relating to the natural phenomena occurring in the external world, and the functions required. Some of the environmental phenomena, or ‘loadings’, that can impact on enclosure include:\n- Gravity (i.e. structural loads).\n- Climate and weather.\n- Seismic forces.\n- Noise and vibration.\n- Soil type.\n- Organic agents (i.e. aerobic life forms such as insects and mould).\n- Inorganic agents (i.e. natural and artificial substances such as radon and methane).\n- Support: To support, resist and transfer all structural forms of loading imposed by the interior and exterior environments.\n- Control: To control, air transfer, heat, sound, access and security, privacy, the provision of views and daylight, and so on.\n- Finish: To finish the enclosure surfaces in terms of visual, aesthetic, durability, and so on.\n- Distribute: To distribute services or utilities such as electricity, communications, water, and so on.\nGenerally, enclosures are either monolithic or composite assemblies. Monolithic enclosures involve a single material acting as the structure, the cladding and interior finish, such as load-bearing masonry. In composite assemblies, separate materials or combinations are assigned critical control functions, such as control of heat transfer or air leakage.\nIn general terms, enclosure types include can be categorised as the following:\n- Compact or distributed.\n- High rise or low rise.\n- Permeable or impermeable.\n- Transparent or opaque.\n- Passive or active.\n- Massive or lightweight.\n- Temporary or permanent.\n- Single or multiple units.\n- Hybrids: Combinations of the above.\nNB Urban Design Guidelines for Victoria, published by the State of Victoria (Australia) in 2016, defines enclosure (or 'sense of enclosure') as: ‘Where the building frontage height, street width and street tree canopy creates a feeling of a contained space within the street.’\n Related articles on Designing Buildings Wiki\n- Building design.\n- Building pathology.\n- Building technology.\n- Concept architectural design.\n- Fabric structures.\n- Structure definition.\n- The building as climate modifier.\n- The development of structural membranes.\n- The history of fabric structures.\n External references\nFeatured articles and news\nECA comments on findings of BEIS Green Jobs Task Force.\nWhy government can't support public transport forever.\nGovernment introduces the Information Management Mandate.\nDesigning and building for the future.\nFabricating mystical connections between nature and architecture.\nIHBC issues responses to ECO4 and PAS 2035.\nThe narrative power of video gaming technology.\nReport examines the possibilities and limitations of localised actions.\nThis multilingual dictionary translates technical terms and concepts.\nFrom simple, two room buildings to massive, elaborate structures.","For building tenants and homeowners, creating a comfortable and healthy indoor atmosphere is of utmost importance. A well built, appropriately sized, and integrated HVAC (Heating, Ventilation, and Air Conditioning) system is necessary to achieve the highest levels of indoor comfort and air quality. We will explore the significance of a correctly sized and integrated HVAC system for assuring interior comfort, health, and general well-being in this guest post.\n- HVAC system right-sizing is one of the core components of an efficient HVAC system. This entails planning and setting up a system that is properly tailored to the room’s heating and cooling requirements. Large systems may regularly cycle on and off, causing inconsistency in temperature, increased wear and tear, and inefficient use of energy. Systems that are too small could struggle to provide the appropriate levels of comfort, which would be uncomfortable and use a lot of energy. An HVAC system that is the proper size will provide maximum comfort, energy efficiency, and lifespan.\n- Integration of Heating, Ventilation, and Air Conditioning: A smooth integration of heating, ventilation, and air conditioning is provided by an integrated HVAC system. The system can efficiently regulate temperature, humidity, and air quality by merging these components. Integration makes it easier to coordinate heating and cooling load, resulting in an area with balanced airflow and temperature distribution. An integrated system also encourages regular fresh air exchange, which lowers the danger of indoor contaminants and maintains a healthy indoor environment.\n- Energy Efficiency and Sustainability: An correctly sized and integrated HVAC system helps with both of these factors. Energy use can be optimized by removing inefficiencies brought on by excessive or undersized systems. Integration additionally enables cutting-edge control systems like programmable thermostats, intelligent zoning, and energy recovery ventilation. These characteristics lessen the environmental impact while regulating energy use, cutting waste, and lowering utility bills.\n- Indoor air quality and occupant health: It is essential to incorporate sufficient ventilation to preserve good indoor air quality and advance occupant well-being. An integrated HVAC system makes sure that old inside air is regularly replaced with new outdoor air, eliminating pollutants, allergens, and too much humidity. By preventing the accumulation of interior contaminants including mold, allergens, and volatile organic compounds (VOCs), effective ventilation lowers the risk of respiratory problems and improves the quality of indoor air in general.\n- Automation and smart technology developments have completely changed the way HVAC systems work. Smart thermostats, sensors, and automation can be used in conjunction with integrated systems to optimize energy use and improve comfort. Smart technology enables remote monitoring and control, allowing facility managers or homeowners to make changes and get real-time data on system performance, air quality, and energy usage. A more effective and practical indoor environment is facilitated by this degree of automation and control.\nA properly sized, integrated HVAC system is necessary for producing a cozy, healthy interior atmosphere. Homeowners and building occupants can benefit from increased comfort, lower energy costs, and improved general wellbeing by designing and installing HVAC systems that are appropriately matched to the space, integrating heating, ventilation, and air conditioning functions, optimizing energy efficiency, ensuring good indoor air quality, and utilizing smart technology. A good decision for immediate benefits and a long-term investment in interior comfort, health, and sustainability is to invest in a well-designed, integrated HVAC system."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:9a955a33-ed9c-4774-8936-c1a619150ff5>","<urn:uuid:b79dd2ad-5443-4a64-b8ae-7fddbc245a18>"],"error":null}
{"question":"How do cognitive biases affect decision-making in politics, and what role do achievement goals play in shaping motivation and behavior?","answer":"Cognitive biases strongly influence political decision-making, as demonstrated by an Emory University experiment where both Bush and Kerry supporters showed no increased activity in brain areas associated with reasoning when presented with contradictory information about their candidate. Instead, emotional circuits lit up, and when given reconciling information, their reward circuits activated similarly to drug addicts receiving a dose. This shows people tend to emotionally justify their preferred conclusions. Regarding achievement goals, they influence motivation and behavior in two main ways: through mastery goals, which are intrinsically motivated and focus on gaining comprehensive knowledge, and performance goals, which are extrinsically motivated and focus on attaining positive outcomes or avoiding negative ones. Mastery goals tend to produce deeper, more sustained learning, while performance goals may lead to higher grades but shallower understanding.","context":["The power of bias\nby Jonathan Rosenblum\nMarch 22, 2006\nResearchers at Emory University in Atlanta recently devised an interesting experiment to test the reasoning, or the lack thereof, of political partisans.\nPrior to the 2004 American presidential election, a group of Kerry supporters and a group of Bush supporters were each given six statements by their candidate. Next they were given pieces of information that documented a blatant contradiction between their candidate’s first statement and his subsequent words or deeds. At that point, the subjects were asked to consider the apparent discrepancy between their candidate’s initial statement and the second statement or behavior and to rate the degree of contradiction involved. Finally, they were given a third statement that might reconcile the first and second piece of information, and asked to reconsider the degree of contradiction involved.\nWhile being presented with these tasks, the subjects’ brains were being monitored by magnetic resonance to determine what areas of the brain were most active. The investigators found that the presentation of the information raising questions about the honesty or consistency of the subject’s favored candidate triggered no increased activity in the brain in areas normally associated with reasoning. Instead a network of emotional circuits lit up.\nWhen the third statement offering a possible reconciliation of the first two was presented, the brain circuits that regulate negative emotions such as sadness and disgust shut down, while those involved in behavior reward were activated, in a manner comparable to that seen in drug addicts after receiving a dose.\nDrew Westen, chairman of the clinical psychology department at Emory, described the findings: \"It appears that the partisans twirl the cognitive kaleidoscope until they get the conclusions they want, and then they get massively reinforced for it.\"\nIntelligence apparently had no impact on the subjects’ responses. Stupid Bush supporters and intelligent Kerry supporters (just kidding) reacted in an identical fashion. As Westen summarized the results, \"Everyone from executives and judges to scientists and politicians may reason to emotionally based judgments when they have a vested interest in how to interpret the ‘facts.’\"\nThese findings, while admittedly only preliminary, are highly suggestive. For one thing, they provide experimental confirmation of a point emphasized by Rabbi Eliyahu Eliezer Dessler already in the late 1920s, as he confronted the cult of science of his time. Rabbi Dessler taught his students to beware that the conclusions of supposedly objective scientists are often heavily tainted by their prior biases.\nSuppose, for instance, a scientist writes that the world is the product of purely random events reflecting no guiding purpose, and claims that modern science supports his claim. Even if we grant that this person possesses a keen intellect and is well-educated, writes Rabbi Dessler, we must recognize that his moral character is likely no more than average, and that he has never seriously tackled his own moral failings. When arguing a point upon which depends \"whether he will be obliged to struggle constantly with his baser desires . . . or whether he will live with no restraints on those desires other than those he deigns to place on them,\" says Rabbi Dessler, no one can \"seriously believe that he will arrive at a true conclusion merely by the exercise of his intellectual powers.\"\nThe Emory findings may also help explain a phenomenon that I have long noticed: the curious immunity of many intellectuals to empirical reality.\nIntellectuals love theories. They become emotionally invested in those theories, and will defend them long after the contradictions have mounted, as Thomas Kuhn long ago pointed out in his classic The Structure of Scientific Revolutions. They would rather modify the theory to explain why the emperor appears naked than admit that he has no clothes. Freudianism, for instance, held sway over Western intellectuals for nearly a century, despite the lack of empirical support for either the theory or its efficacy as a therapeutic tool.\nThe Emory study also helps me better understand the awe I often feel in the presence of those who have had the fortitude to re-examine the emotional core of their lives and ask whether it is true – e.g., geirei tzedek, ba’alei teshuva, and even those neo-conservatives whom Irving Kristol famously described as \"liberals mugged by reality.\" We should feel awe because, as the Emory study makes clear, the power of bias is very strong and receives powerful emotional reinforcement from our brains.\nWafa Sultan, is a Syrian-born Arabic-speaking psychologist, now living in the United States, who had the courage to say openly on Al Jazeera, \"The Jews have come through the tragedy [of the Holocaust], and forced the world to respect them with their knowledge, not their terror, with their work, not their crying or yelling. . . . We have not seen a single Jew blow himself up in a German restaurant. We have not seen a single Jew destroy a church.\" Apart from the pure physical bravery involved, it is truly wondrous to contemplate someone who has freed herself from the prejudices of the Moslem society in which she was born to such extent.\nFinally, the Emory experiment provided me with a renewed appreciation of the \"milchemes HaTorah\" described by the Gemara in Kiddushin. The chavrusah (study partner) system of learning forces us to subject that which is dear to us – our chiddushim (novellae) – to continual scrutiny. Every time we offer a solution to a particular project, we find sitting across from us a study partner who has no ego invested in our chiddush and will do everything he can to refute it.\nThose who are raised in this system of learning are constantly challenged to overcome the natural bias in favor of their own intellectual progeny, and to pursue truth instead. The training is far from fool-proof. As Rabbi Dessler noted, it can only work in conjunction with rigorous work on our characters as well. But work it does.\nEvery time a gadol b’Torah stops a shiur in the middle, in response to a student’s question, even though he could have easily found numerous plausible ways to save his chiddush, we are witnessing a rare feat of elevating reason over emotion. Just how rare, Dr. Westen has shown us.\nRelated Topics: Jewish Ethics\nreceive the latest by email: subscribe to the free jewish media resources mailing list","Cognitive and achievement approaches to motivation examine how factors like achievement goals and cognitive dissonance influence motivation.\n- Summarize the roles of achievement and cognition in motivation\n- According to the achievement approach to motivation, the need for achievement drives accomplishment and performance and thereby motivates our behavior. People are motivated by different goals related to achievement, such as mastery or performance goals.\n- Mastery goals are a form of intrinsic motivation that tend to be associated with the satisfaction of mastering the material at hand.\n- Performance goals are extrinsically motivated and tend to be associated with wanting to attain positive outcomes or avoid negative outcomes.\n- Cognitive approaches to motivation focus on how a person’s cognitions—and especially cognitive dissonance—influence their motivation.\n- The theory of cognitive dissonance proposes that people have a motivational drive to reduce contradictory cognitions by either changing or justifying their attitudes, beliefs, and behaviors.\n- masterTo learn to a high degree of proficiency.\n- extrinsicExternal, separable from the thing itself, inessential.\n- intrinsicInnate, inherent, inseparable from the thing itself, essential.\n- cognitive dissonanceA conflict or anxiety resulting from inconsistencies between one’s beliefs and one’s actions or other beliefs.\nMotivation describes the wants or needs that direct behavior toward a goal. When we refer to someone as being motivated, we mean that the person is trying hard to accomplish a certain task; having motivation is clearly important for someone to perform well. Both the achievement and cognitive approaches to motivation examine the various factors that influence our motivation.\nAccording to the achievement approach to motivation, the need for achievement drives accomplishment and performance and thereby motivates our behavior. People may be motivated by different goals related to achievement, and each of these goals affect one’s motivation—and thereby behavior—differently. For instance, a student might be motivated to do well in an algebra class because it’s interesting and will be useful to her in later courses (i.e., to master the material); to get good grades (i.e., to perform well); or to avoid a poor or failing mark (i.e., to avoid performing poorly). These goals are not mutually exclusive, and may all be present at the same time.\nMastery and Performance Goals\nMastery goals tend to be associated with the satisfaction of mastering something—in other words, gaining control, proficiency, comprehensive knowledge, or sufficient skill in a given area (such as mastering the art of cooking). Mastery goals are a form of intrinsic motivation (arising from internal forces) and have been found to be moreeffective than performance goals at sustaining students’ interest in a subject. In one review of research about learning goals, for example, students with primarily mastery orientations toward a course they were taking not only tended to express greater interest in the course, but also continued to express interest well beyond the official end of the course and to enroll in further courses in the same subject (Harackiewicz, et al., 2002; Wolters, 2004).\nPerformance goals, on the other hand, are extrinsically motivated (arising from external factors) and can have both positive and negative effects. Students with performance goals often tend to get higher grades than those who primarily express mastery goals, and this advantage is often seen both in the short term (with individual assignments) and in the long term (with overall grade point average when graduating). However, there is evidence that performance-oriented students do not actually learn material as deeply or permanently as students who are more mastery-oriented (Midgley, Kaplan, & Middleton, 2001).\nA possible reason is that measures of performance, such as test scores, often reward relatively shallow memorization of information; in other words, information that is “crammed” before a test is only remembered in the short-term and often forgotten immediately after the test. Because the “performance” is over, there are no negative consequences for forgetting the information relatively quickly, and this can prevent performance-oriented students from processing the information more thoughtfully or deeply. Another possible reason is that by focusing on gaining recognition as the top performer in a peer group, a performance orientation encourages competition with peers. Giving and receiving help from classmates is thus not in the self-interest of a performance-oriented student, and the resulting isolation can limit the student’s learning.\nCognitive approaches to motivation focus on how a person’s motivation is influenced by their cognitions or mental processes. Of particular interest is the role of cognitive dissonance on motivation. Cognitive dissonance occurs when a person experiences conflict, contradiction, or inconsistency in their cognitions. These contradictory cognitions may be attitudes, beliefs, or awareness of one’s behavior. Dissonance is strongest when a discrepancy has been noticed between one’s self-concept and one’s behavior. If you do something you are ashamed of or act in a way that is counter to an idea you have about yourself (for example, if you consider yourself an honest person but then lie to your parents when they ask about your future plans), you are likely to feel cognitive dissonance afterward.\nThe theory of cognitive dissonance proposes that people have a motivational drive to reduce dissonance in their cognitions by either changing or justifying their attitudes, beliefs, and behaviors. How a person chooses to respond to the dissonance depends on the strength of various motivating factors. For example, smoking cigarettes increases the risk of cancer, which is threatening to the self-concept of the individual who smokes. When the smoker hears evidence suggesting that smoking might cause cancer (cognitive component), they can either choose to stop smoking (change the behavioral component) or choose to reject the causal link. Since smoking is physically addictive, most smokers choose to minimize their acknowledgement of the risk rather than change their behavior. The addiction is more motivating than the fear of possible long-term medical consequences, so the less-motivating idea is minimized and discounted. Most of us believe ourselves to be intelligent and rational, and the idea of doing something self-destructive causes dissonance. To reduce this uncomfortable tension, smokers might make excuses for themselves, such as “I’m going to die anyway, so it doesn’t matter.”\nAnother application of cognitive dissonance occurs in the case of effort justification. Dissonance is aroused whenever individuals voluntarily engage in an unpleasant activity to achieve some desired goal; this dissonance can be reduced by exaggerating the desirability of the goal. The more time, money, or effort someone invests in an activity, the more they will convince themselves that they made a wise choice and that their efforts were worth it. A child who has to work and save for a bicycle, for example, will value it more and take better care of it than if the bicycle was given as a gift, with no effort on the part of the child."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"comparison"},{"categorization_name":"user_conversational_style","category_name":"informal_direct"}],"document_ids":["<urn:uuid:d0b33c62-18f5-4728-b4c0-5354dc886c61>","<urn:uuid:e30c1471-5932-4cf8-a5cc-8fece05ebc2a>"],"error":null}
{"question":"What similarities exist between how cultural psychiatry and the neuroscience minor program address the topic of emotion and cognition?","answer":"Both fields examine emotion and cognition, but from different perspectives. In cultural psychiatry, as presented in the McGill lectures, emotion is explored through cultural specificity, particularly in relation to trauma and the self, as discussed by Allan Young. The neuroscience minor program approaches these topics through courses like Cognitive Neuroscience and Cognition, focusing on the biological and computational aspects. Both fields recognize the importance of understanding emotional and cognitive processes, but cultural psychiatry emphasizes cultural context while neuroscience focuses on neural mechanisms.","context":["As I’ve written about on this site before, one of the best short programs for anyone interested in culture and mental health is the Summer School in Social and Cultural Psychiatry held annually at McGill. The course at the center of the curriculum is “Cultural Psychiatry: A Critical Introduction” which consists primarily of lectures by Laurence Kirmayer, Allan Young, Cécile Rousseau, Eric Jarvis and other members of McGill’s Division of Social and Transcultural Psychiatry. If you can’t make it to the Summer School itself, you can now view videos of all of the “Cultural Psychiatry” lectures from 2012 online for free. This is a fantastic introduction to cultural psychiatry delivered by scholars who have been teaching the course for many years. The videos have also been produced very professionally, with slides edited in and multiple camera angles (!)\nThe entire series of lectures can be viewed on the Summer School’s site. I am also embedding the first two videos, which serve as a kind of introduction to the course, and listing the others – with links to the individual videos on YouTube along with brief descriptions.\nAlso, check out this separate post on the videos the from Summer School’s new “Critical Neuroscience” course.\nCultural Psychiatry: A Critical Introduction\nTo understand what is built into our mental health practice in order to look at it critically, Laurence Kirmayer examines the culture of psychiatry and psychology.\nLaurence Kirmayer looks at the shifting meanings of culture, especially in an increasingly mobile global population. He also discusses how culture might figure in psychiatric nosology.\nLaurence Kirmayer looks at three forms of somatization, the problem of dualism in medicine, cultural idioms of distress and how experience is expressed in and through the body.\nLaurence Kirmayer looks at medically unexplained symptoms, the potential meaning of somatic symptoms and the cultural context of symptom reporting.\nLaurence Kirmayer examines how knowledge is generated in the field of cultural psychiatry and what the limitations in the field are. The lecture covers philosophical concerns on the nature of experience and cultural difference.\nLaurence Kirmayer looks at issues that arise when trying to look across cultures, for example working in one culture with tools that were developed elsewhere. He discusses problems in cross-cultural comparisons and its methodological implications.\nLaurence Kirmayer discusses dissociation, trance and related issues in cultural psychiatry.\nLaurence Kirmayer continues the discussion of dissociation, trance and related issues in cultural psychiatry.\nAllan Young looks at culture and psychiatry: how living within a particular culture impacts on the epidemiology of psychiatric disorders; culture in psychiatry: culture permeates psychiatric practices, there are ethnocentric concepts that take into account a Western world view; and the cultures of psychiatry: just as there are cultures based on nationality and race, there are different cultures of the practice and study of psychiatry.\nAllan Young continues his examination of the culture of psychiatry.\nAllan Young discusses the different notions of the self. Is our idea of the self a distinctly Western notion? Is there a self? Is it essential? Are there cultures without a sense of self?\nAllan Young discusses notions of self in relation to trauma and emotion. Are there culturally specific emotions?\nEric Jarvis compares how different countries treat different aspects of psychosis, particularly in immigrant and migrant groups. He also looks at the clinical and global implications of culture.\nConstantin Tranulis continues the discussion on culture and psychosis with an emphasis on research possibilities in this field.\nCecile Rousseau discusses her research and work with migrant children. She speaks about the challenges of globalization, its impact on children and the implications it has on services.\nLaurence Kirmayer discusses the impacts of colonization on health, and identity, adaptation and the problem of suicide in indigenous populations.\nLaurence Kirmayer discusses the role cultural continuity plays in mental health. He also discusses resilience and the concept of the ecocentric self.\nLaurence Kirmayer looks at universal aspects of healing, healing traditions in different cultures and the role of emotion and catharsis in healing.\nLaurence Kirmayer discusses what is psychotherapy, psychotherapy as narrative processes, mode of narration, and cultural concepts of the person.\nGiven the role culture plays in the illness experience, was are the implications for mental health services? Laurence Kirmayer looks at some approaches and models, and investigates the rationale behind them.\nA continuation of the discussion of the implications of cultural psychiatry for mental health services.\nWhat are the forthcoming tasks for cultural psychiatry? How do we complement the focus on the biological individual with an interactional view of human problems?","Minor in Neuroscience\nThe Minor in Neuroscience is administered by the Department of Psychological & Brain Sciences (PBS) and is overseen by the Neuroscience Steering Committee. The majority of Neuroscience courses are taught by PBS and Biology faculty and, thus, Neuroscience course numbers are most often designated as PSYCxxx or BIOxxx.\nThe minor in neuroscience is intended to provide formal recognition for students who have concentrated some of their academic work in the interdisciplinary area of neuroscience. The minor can accommodate majors from all other departments, including Biology.\nThe Neuroscience Minor comprises:\n1 Prerequisite course\n2 Core Courses\nand 3 Elective Courses\nSee Headings below for details\nMust complete one\n- PSYC 6 Introduction to Neuroscience\n- BIOL 34 Neurobiology\n- PSYC 1 Introductory Psychology\n- The prerequisite course CANNOT be taken as an NRO.\nMust complete two\nPSYC 45 Behavioral Neuroscience\nPSYC 46 Cellular & Molecular Neuroscience OR BIOL 12 Cell Structure & Function\nPSYC 65 Systems Neuroscience with Laboratory Note: Online permission form required.\n- Core courses CANNOT be taken as an NRO.\nMust complete three\nYou may choose from the list below as well as the Approved Neuroscience Elective Course list (see link below). Note: One course must be at the 50s level or higher.\nPSYC 21 Perception\nPSYC 27 Cognitive Neuroscience\nPSYC 45 Behavioral Neuroscience (if not taken as a core course)\nPSYC 28 Cognition\nPSYC 40 Introduction to computational neuroscience\nPSYC 46 Cellular and molecular neuroscience (if not taken as a core course)\nPSYC 50s level, 80s level, and other approved electives:\nAt the beginning of each academic year, the Neuroscience Steering Committee will announce which other courses qualify for elective credit. Below is a link to the currently approved Electives that count towards the Neuroscience minor. We recommend that you not save a copy of this pdf to your computer at the beinnging of the year, but instead use this link each time you wish to access this list. This is because we may add classes as the year progesses and those updates will appear here.\nPreviously approved courses:\n- Elective courses CANNOT be taken as an NRO.\n- While it is highly likely that subsequent offerings of a course would also count as an elective, it is not guaranteed. This is because the instructor may change and/or the amount of neuro-relevant content may change over time. Please consult each year's list of approved electives to be sure the course you plan to take counts as an elective!\n- With permission of the Neuroscience Steering Committee, other courses that are appropriate given the student's area of specialization may be taken for elective credit.\n- Multiple offerings of Psychology 50, 51, 52, and 80s-level courses of the same number may be taken as long as they cover different topics.\n- Not all 50s, 51s, 52s, and 80s-level courses count for neuroscience credit. In other words, there may be to two courses that are both numbered 'PSYC 83' however, one might be approved for neuroscience elective credit and the other may not. So you MUST consult the list of Approved Neuroscience Elective Courses for each year (via the link above).\n- Students who take Physiology 150 or PEMM 211 should register for Psychology 90 and have permission of the instructor.\n- Courses that are taken as part of another major/minor cannot be used as elective courses for Neuroscience.\n- For permission courses at 60's and 80's level, students should fill out an online permission form (link to permission/checklist form)."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"premise_categorization","category_name":"without premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_intent","category_name":"instructional"},{"categorization_name":"user_conversational_style","category_name":"formal_transactional"}],"document_ids":["<urn:uuid:b31177c1-f45a-47df-aa75-3a470bdcda7d>","<urn:uuid:7727d99b-34ce-4884-ab7a-ed85e668e74d>"],"error":null}
{"question":"Can bank filtration be implemented on any river throughout the year? I'm curious about the limitations of this water treatment method.","answer":"Bank filtration cannot be implemented on every river or in every season. During lean seasons, water may actually percolate towards the river instead of away from it. The technique is most suited to rivers that have deep alluvium aquifers, which have the right hydraulic conductivity for cleaning water. Additionally, engineers must be careful when selecting well sites because many Indian rivers change their course, which could affect the filtration system's effectiveness.","context":["River banks are efficient water filters. Haridwar shows how to make most of them\nby Bharat Lal Seth\nThe holy town of Haridwar on the banks of the Ganga has of late been receiving pilgrims of a different kind. They are students and professors from India and abroad who come to study its water supply system. Over a third (38 per cent) of the water supplied in Haridwar is naturally treated as it passes through the river banks.\nWells are sunk a little away from the banks where the river water percolating sideways collects. Sand or earth acts as a filter and the time for which water is retained in it—a month in case of Haridwar—renders 99.99 per cent of the bacteria inactive, says the supplier Uttarakhand Jal Sansthan (UJS). The system is so efficient that water needs minimal treatment; only a small dose of sodium hypochlorite is added to deal with possible contamination during supply to houses. UJS is saving Rs 75 lakh on treatment—including the operation cost—every year, estimates its executive engineer Subodh Gupta.\nThe purification technique called bank filtration is not new. Several countries are using it on a large scale. In Slovakia, for example, half the water drawn from groundwater is bank filtered. But in India most water suppliers are yet to identify wells that contain water percolated from a nearby river or lake. This information can help them increase the capacity of such wells and set up an alarm system to check their contamination, as Haridwar has done.\nIn 2004, UJS, in collaboration with IIT RRoorkee and a German team, sunk monitoring wells near one of its infiltration wells named IW-18. The monitoring wells intercept water two weeks before it reaches IW-18. Checking water quality in them can alert UJS about contamination in advance, said Indu Mehrohtra, professor at IIT Roorkee.\nGeological surveys by the European Union River Bank Filtration Network revealed that the aquifer beneath IW-18 would yield water between 6m and 19m below the ground. IW-18 was 7m deep, so it was tapping only 1m of the aquifer. Small wonder it would run dry after four hours of pumping water. UJS would pump water for four hours twice a day.\nThe water utility decided to drill IW-18 to 19 m to tap the entire depth of the aquifer. But UJS engineers encountered hard rock at 12m. “Having accessed a further 5m of the aquifer, we can now draw water for more than the stipulated eight hours,” said Gupta.\nThe engineers also changed the width of the well. All UJS wells are 30m wide but in IW-18 they inserted a 300 mm-wide pipe while deepening it. UJS then calculated that sinking six 300mm-wide wells 17m deep would cost Rs 90 lakh and yield 7,200 kilolitres of water a day. This is five times what a 10m-wide and 7m-deep well yields at a cost of Rs 100 lakh. But b y the time UJS found ways of increasing the yield of its infiltration wells, the Uttar Pradesh Jal Nigam, in charge of water infrastructure, had begun work on six such wells, 6-7m deep, to meet Haridwar’s demand till 2025.\nOther towns by rivers can still learn from the UJS experiment since the bank filtration technique has many benefits. It is easy to operate, requires little maintenance, keeps water free of chemicals used to kill germs and does not require obstructing river flow by building barrages. But it is not applicable to every river and every season—in lean season water may percolate towards the river. It is most suited to rivers having deep alluvium aquifers, said Anupam Singh, professor at Nirma University in Ahmedabad, which is involved in implementing the technique in Gujarat. These aquifers, formed by deposition of sand and silt, have a hydraulic conductivity suitable for cleaning water.\nEngineers also have to be careful in selecting the site of wells because in India many rivers change course, said Chittaranjan Ray, faculty at the University of Hawaii who has written two books on bank filtration. CSE/Down To Earth Feature Service"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"premise_categorization","category_name":"with premise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_intent","category_name":"information_seeking"},{"categorization_name":"user_conversational_style","category_name":"polite_conversational"}],"document_ids":["<urn:uuid:74f478a9-0150-46e8-9f3b-9e155a389ad6>"],"error":null}