{"question":"How to measure distances and directions on a topographical map of India? Need step-by-step process for accurately marking locations.","answer":"To measure distances and mark directions on a topographical map of India: 1) Use the scale provided on the map to measure distances between locations. 2) Use the eight cardinal points and indicated bearings to mark directions between different locations. 3) Identify prominent villages and towns using the index at the bottom of the sheet. 4) Use the index to also identify types of land use and means of communication in the area.","context":["There is one paper of two hours duration carrying 80 marks and Internal Assessment of 20 marks. The Paper consists of two parts - Part I and Part II. Part I (compulsory) consists of two questions. Question 1 is based on Topographical Map. Question 2 is based on outline Map of India. In Part II, you are expected to answer any five questions.\n1. Interpretation of Topographical Maps\na. Identification of simple landforms marked by contours, triangulated height, spot heights, surveyed trees, bench marks, relative height and colour tints or other symbols on a topographical survey of India map.\nb. Measuring distances using the scale given therein and marking directions between different locations, using eight cardinal points and indicated bearings.\nc. Marking the site of prominent villages and/or towns, types of land use and means of communication with the help of the index given at the bottom of the sheet.\nd. Identification of drainage and settlement patterns.\n2 Map of India\nA question is set to locate and label on an outline map of India. You are expected to locate and label the following items - mountains, plateaus, plains, rivers and water bodies, towns, coastal features, minerals, rainfall and wind\nMountains and Plateaus: Himalayas, Karakoram, Aravali, Vindhyas, Satpura, Western and Eastern Ghats, Nilgiris, Garo, Khasi, Jaintia. Deccan, Chota Nagpur, Malwa Plateaus.\nPlains: Indo-Gangetic Plains, Coastal plains - Konkan, Malabar, Coromandal and the Northern Circar.\nRivers: Indus, Ravi, Beas, Chenab, Jhelum, Satluj, Ganga, Yamuna, Ghaghra, Gomti, Gandak, Kosi, Chambal, Betwa, Son, Damodar, Bhrahmaputra, Narmada, Tapti, Mahanadi, Godavari, Krishna and Cauveri, Tungabhadra.\nWater Bodies: Gulf of Kutch, Gulf of Khambhat, Gulf of Mannar, Palk Strait, Andaman Sea and Chilka Lake.\nPasses: Karakoram, Nathu-La Passes.\nLatitude and Longitudes: Tropic of Cancer, Standard Meridian (82° 30’E).\nDirection of Monsoon Winds: South West (Arabian and Bay of Bengal Branches) North East Monsoon.\nDistribution of Minerals: Oil - Mumbai High (Offshore Oil Field) Digboi. Iron - Singhbhum, Coal - Jharia.\nSoil Distribution - Alluvial, Laterite, Black and Red Soil.\nTowns - Delhi, Mumbai, Kolkata, Chennai, Hyderabad, Bangalore, Kochi, Srinangar, Vishakhapatnam, Allahabad.\nPopulation - Distribution of Population (Densely and sparsely).\n3. Location, Extent and Physical features\nPosition and Extent of India. (through Map only)\nThe physical features of India - mountains, plateaus, plains and rivers (through Map only)\n4. The climate of India\nDistribution of temperature, rainfall, winds in summer and winter and the factors affecting the climate of the area. Monsoon and its mechanism.\nSeasons – March to May – hot and dry summer; June to September – South West Monsoon; October to November - retreating monsoon. December to February – cool and dry winter.\nMap showing distribution of temperature, rainfall, and monsoon winds.\n5. Soils in India\nTypes of soils (alluvial, black, red and laterite), composition and characteristics such as colour, texture, minerals, crops associated, soil erosion – causes, prevention and need for conservation.\n6. Natural vegetation of India\nTypes of vegetation (tropical evergreen, tropical deciduous, tropical desert, littoral and mountain), distribution and correlation with their environment, uses of important trees, need for conservation and various measures.\n7. Water Resources\nImportance of irrigation, means of irrigation, need for conservation, rain water harvesting. and its importance.\n8. Minerals in India\nCoal, petroleum, iron ore, manganese, bauxite, limestone – uses and their distribution.\n9. Agriculture in India\nTypes of agriculture in India: shifting, subsistence, intensive, extensive, plantation, mixed, commercial. Indian Agriculture – problems and solutions.\nAgricultural seasons (rabi, kharif, zayad), climatic conditions, soil, methods of cultivation, processing and distribution of the following crops:\n10. Industries in India: - Agro based Industry and Mineral based Industry.\nAgro based Industry - Sugar, Cotton Silk, Woollen and Jute Textiles.\nMineral based Industry - Iron, Steel, Heavy Engineering, Petro Chemical and Electronics.\nRoads – Express Highways, National highways, Golden Quadrilateral, Railway – Narrow, Metre, Broad gauge, Air ways, Water ways – Major Sea Ports Advantages and disadvantages of these transport.\n12. Waste generation and management\n(a) Sources of waste - domestic, industrial, agricultural, Municipal, Medical and nuclear plants. Domestic waste: paper, glass, plastic, rags, kitchen waste, etc.\nIndustrial: mining operations, cement factories, oil refineries, construction units.\nAgricultural: plant remains, animal waste, processing waste.\nMunicipal: sewage, degradable and non-degradable waste from offices, etc.\nBiomedical waste: needles, syringes, soiled dressings, pathological waste from hospitals, medical labs.\nNuclear waste: radioactive waste.\n(b) Impact of waste accumulation - spoilage of landscape, pollution, health hazards, effect on terrestrial, aquatic (fresh water and marine) life. Self-explanatory.\n(c) Need for management of waste. Self-explanatory.\n(d) Methods of safe disposal of waste - segregation, dumping, composting, drainage, treatment of effluents before discharge, incineration, use of scrubbers and electrostatic precipitators. Segregation of domestic waste into biodegradable and non-biodegradable by households; sweeping from gardens to be converted to compost; sewage treatment plants, incinerators in group housings.\n(e) Need for reducing, reusing and recycling waste. Methods would involve governmental, social and individual initiatives.\nGovernmental initiatives: not building large dams for generating hydro electric power which leads to less land being submerged and less displacement of people. Improving efficiency of existing technologies and introducing new ecofriendly technologies.\nSocial initiatives: creating awareness and building trends of sensitive use of resources and products, e.g. reduced use of electricity, etc.\nIndividual: developing an ethical environmental consciousness e.g. refusing use of polybags, styrofoam containers, etc; reusing: plastic and glass containers; recycling: e.g. paper – this will reduce demand on wood and save trees."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:60744699-8f30-44d8-9069-e762204f00f3>"],"error":null}
{"question":"Why is agriculture considered more harmful to the environment than transportation?","answer":"Agriculture emits more greenhouse gases than all cars, trucks, trains, and airplanes combined. This is largely due to methane released by cattle and rice farms, nitrous oxide from fertilized fields, and carbon dioxide from the cutting of rain forests to grow crops or raise livestock. Agriculture is also the biggest user of water supplies and a major polluter through fertilizer and manure runoff.","context":["When we think about threats to the environment, we tend to picture cars and smokestacks, not dinner. But the truth is, our need for food poses one of the biggest dangers to the planet.\nAgriculture is among the greatest contributors to global warming, emitting more greenhouse gases than all our cars, trucks, trains, and airplanes combined—largely from methane released by cattle and rice farms, nitrous oxide from fertilized fields, and carbon dioxide from the cutting of rain forests to grow crops or raise livestock. Farming is the thirstiest user of our precious water supplies and a major polluter, as runoff from fertilizers and manure disrupts fragile lakes, rivers, and coastal ecosystems across the globe. Agriculture also accelerates the loss of biodiversity. As we’ve cleared areas of grassland and forest for farms, we’ve lost crucial habitat, making agriculture a major driver of wildlife extinction.\nThe environmental challenges posed by agriculture are huge, and they’ll only become more pressing as we try to meet the growing need for food worldwide. We’ll likely have two billion more mouths to feed by mid-century—more than nine billion people. But sheer population growth isn’t the only reason we’ll need more food. The spread of prosperity across the world, especially in China and India, is driving an increased demand for meat, eggs, and dairy, boosting pressure to grow more corn and soybeans to feed more cattle, pigs, and chickens. If these trends continue, the double whammy of population growth and richer diets will require us to roughly double the amount of crops we grow by 2050.\nUnfortunately the debate over how to address the global food challenge has become polarized, pitting conventional agriculture and global commerce against local food systems and organic farms. The arguments can be fierce, and like our politics, we seem to be getting more divided rather than finding common ground. Those who favor conventional agriculture talk about how modern mechanization, irrigation, fertilizers, and improved genetics can increase yields to help meet demand. And they’re right. Meanwhile proponents of local and organic farms counter that the world’s small farmers could increase yields plenty—and help themselves out of poverty—by adopting techniques that improve fertility without synthetic fertilizers and pesticides. They’re right too.\nBut it needn’t be an either-or proposition. Both approaches offer badly needed solutions; neither one alone gets us there. We would be wise to explore all of the good ideas, whether from organic and local farms or high-tech and conventional farms, and blend the best of both.\nI was fortunate to lead a team of scientists who confronted this simple question: How can the world double the availability of food while simultaneously cutting the environmental harm caused by agriculture? After analyzing reams of data on agriculture and the environment, we proposed five steps that could solve the world’s food dilemma.\nStep One: Freeze Agriculture’s Footprint\nFor most of history, whenever we’ve needed to produce more food, we’ve simply cut down forests or plowed grasslands to make more farms. We’ve already cleared an area roughly the size of South America to grow crops. To raise livestock, we’ve taken over even more land, an area roughly the size of Africa. Agriculture’s footprint has caused the loss of whole ecosystems around the globe, including the prairies of North America and the Atlantic forest of Brazil, and tropical forests continue to be cleared at alarming rates. But we can no longer afford to increase food production through agricultural expansion. Trading tropical forest for farmland is one of the most destructive things we do to the environment, and it is rarely done to benefit the 850 million people in the world who are still hungry. Most of the land cleared for agriculture in the tropics does not contribute much to the world’s food security but is instead used to produce cattle, soybeans for livestock, timber, and palm oil. Avoiding further deforestation must be a top priority.\nStep Two: Grow More on Farms We’ve Got\nStarting in the 1960s, the green revolution increased yields in Asia and Latin America using better crop varieties and more fertilizer, irrigation, and machines—but with major environmental costs. The world can now turn its attention to increasing yields on less productive farmlands—especially in Africa, Latin America, and eastern Europe—where there are “yield gaps” between current production levels and those possible with improved farming practices. Using high-tech, precision farming systems, as well as approaches borrowed from organic farming, we could boost yields in these places several times over.\nNearly all new food production in the next 25 years will have to come from existing agricultural land.\nfeed and fuel\nOnly 55 percent of food-crop calories directly nourish people. Meat, dairy, and eggs from animals raised on feed supply another 4 percent.\nImproving nutrient and water supplies where yields are lowest could result in a 58 percent increase in global food production.\nStep Three: Use Resources More Efficiently\nWe already have ways to achieve high yields while also dramatically reducing the environmental impacts of conventional farming. The green revolution relied on the intensive—and unsustainable—use of water and fossil-fuel-based chemicals. But commercial farming has started to make huge strides, finding innovative ways to better target the application of fertilizers and pesticides by using computerized tractors equipped with advanced sensors and GPS. Many growers apply customized blends of fertilizer tailored to their exact soil conditions, which helps minimize the runoff of chemicals into nearby waterways.\nOrganic farming can also greatly reduce the use of water and chemicals—by incorporating cover crops, mulches, and compost to improve soil quality, conserve water, and build up nutrients. Many farmers have also gotten smarter about water, replacing inefficient irrigation systems with more precise methods, like subsurface drip irrigation. Advances in both conventional and organic farming can give us more “crop per drop” from our water and nutrients.\nStep Four: Shift Diets\nIt would be far easier to feed nine billion people by 2050 if more of the crops we grew ended up in human stomachs. Today only 55 percent of the world’s crop calories feed people directly; the rest are fed to livestock (about 36 percent) or turned into biofuels and industrial products (roughly 9 percent). Though many of us consume meat, dairy, and eggs from animals raised on feedlots, only a fraction of the calories in feed given to livestock make their way into the meat and milk that we consume. For every 100 calories of grain we feed animals, we get only about 40 new calories of milk, 22 calories of eggs, 12 of chicken, 10 of pork, or 3 of beef. Finding more efficient ways to grow meat and shifting to less meat-intensive diets—even just switching from grain-fed beef to meats like chicken, pork, or pasture-raised beef—could free up substantial amounts of food across the world. Because people in developing countries are unlikely to eat less meat in the near future, given their newfound prosperity, we can first focus on countries that already have meat-rich diets. Curtailing the use of food crops for biofuels could also go a long way toward enhancing food availability.\nA World Demanding More\nStep Five: Reduce Waste\nAn estimated 25 percent of the world’s food calories and up to 50 percent of total food weight are lost or wasted before they can be consumed. In rich countries most of that waste occurs in homes, restaurants, or supermarkets. In poor countries food is often lost between the farmer and the market, due to unreliable storage and transportation. Consumers in the developed world could reduce waste by taking such simple steps as serving smaller portions, eating leftovers, and encouraging cafeterias, restaurants, and supermarkets to develop waste-reducing measures. Of all of the options for boosting food availability, tackling waste would be one of the most effective.\nTaken together, these five steps could more than double the world’s food supplies and dramatically cut the environmental impact of agriculture worldwide. But it won’t be easy. These solutions require a big shift in thinking. For most of our history we have been blinded by the overzealous imperative of more, more, more in agriculture—clearing more land, growing more crops, using more resources. We need to find a balance between producing more food and sustaining the planet for future generations.\nThis is a pivotal moment when we face unprecedented challenges to food security and the preservation of our global environment. The good news is that we already know what we have to do; we just need to figure out how to do it. Addressing our global food challenges demands that all of us become more thoughtful about the food we put on our plates. We need to make connections between our food and the farmers who grow it, and between our food and the land, watersheds, and climate that sustain us. As we steer our grocery carts down the aisles of our supermarkets, the choices we make will help decide the future.\nJonathan Foley directs the Institute on the Environment at the University of Minnesota. Jim Richardson’s portraits of farmers are the latest in his body of work documenting agriculture. George Steinmetz’s big-picture approach reveals the landscapes of industrial food.\nThe magazine thanks The Rockefeller Foundation and members of the National Geographic Society for their generous support of this series of articles.\nAll maps and graphics: Virginia W. Mason and Jason Treat, NGM Staff. A World Demanding More, source: David Tilman, University of Minnesota. Agriculture's Footprint, source: Roger LeB. Hooke, University of Maine. Maps, source: Global Landscapes Initiative, Institute on the Environment, University of Minnesota."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:074508bb-27ee-4e22-b7af-98c88649656e>"],"error":null}
{"question":"What methods do researchers use to preserve vent samples for later study, and how do autonomous underwater vehicles assist in mapping these deep-sea features?","answer":"Researchers preserve vent samples through multiple methods: they store small pieces in ultra-cold freezers at -80°C for DNA sequencing analysis, preserve some pieces for microscopy to study cell growth, and collect parts for mineral analysis to determine biotic or abiotic formation. The mapping process is conducted by specialized autonomous underwater vehicles like MBARI's D. Allan B., which uses three different types of sonar to create detailed maps of the seafloor. This AUV can fly close to the seafloor and distinguish fine-scale details of just 15 centimeters in size, which would be impossible to detect with traditional ship-based sonar systems.","context":["There are days you find yourself on a ship, at 2am, covered in mud. Those days you wonder, “How did I get here?” Then you realize what an amazing opportunity you have.\nHydrothermal vents are one of the most exciting areas on the planet for microbiologists, as they are one of the few places where we can channel our inner naturalists. The microbes give life to all the bigger creatures inhabiting the vent field and the presence of microbial mat tells us where to look for vent fluid seeping out of the seafloor. The microbes live near the fluid because it contains their food (electron donors), and they turn the energy stored in the vent fluid chemicals into biomass, forming the basis of the food chain at vents just like plants do on the surface. Some vent creatures – like the tubeworms and clams at Pescadero – find themselves a shortcut in the food chain and rather than eating microbes, they “grow” their own, hosting symbionts living in a special organ. So from my perspective, understanding the biology of a vent field starts at the microbes.\nThe trouble is, microbes are not too easy to characterize. You can not grab one single microbe, bring it up, and dissect it to see what it looks like or understand how it works. So what we do is sample the microbes living in (and on) rocks, in the sediment, and in symbiosis with the animals. That means that when ROV Subastian comes up after 12 hours of diving, our work really starts.\nWe pluck any obvious macro-biology off the rocks and hand it over to the zoologists on board, who will image and characterize them. Then some parts are collected for mineral analysis, to see whether there is evidence of biotic or abiotic formation. Finally, we put small pieces in the ultra-cold freezer (-80 ‘C) to preserve them for community analysis by DNA sequencing. Sometimes pieces of rock are also preserved for microscopy, so we can try to find cells that grow on the rock later.\nIf we are lucky, the sediment cores come to the surface undisturbed, and we will be able to capture the gradients starting at the seafloor and going down into the mud. Gradients like these form because microbes use up the resources in the water, and because there is mixing with fluid coming up from below. Along these gradients many different species can find a place to live: we like to figure out who is where, and why.\nSo what we do is take a core, stick disk that looks a bit like a hockey in the bottom and push the sediment up. We then slice off 1cm or 3 cm pieces and prepare these for 10 different analyses. As you can imagine, this takes a while, and can get a little messy and terribly smelly (of rotten eggs and gasoline yesterday) so we have to do it outside. And that is why you can find me out on the aft deck at 2am, covered in mud, with a broad grin on my face.","|11 May 2012||Share this article|\nMBARI discovers new deep-sea hydrothermal vents using sonar-mapping robot\n“As the remotely operated vehicle (ROV) descended into the blue depths above the Alarcón Rise, the control room was abuzz with anticipation,\" wrote MBARI geologist Julie Martin in her April 22nd cruise log. \"Today we [are] planning to dive on one of the strangest environments in the deep sea: a hydrothermal vent field.” Adding to the team’s excitement was the fact that this hydrothermal vent field had never been explored before. In fact, it had only just been discovered... by a robot.\nIn February 2012, MBARI researchers embarked on a three-month-long expedition to the Gulf of California, the long, narrow body of water between Baja California and mainland Mexico. During the final leg of this expedition, marine geologists studied the Alarcón Rise, an active volcanic region near the mouth of the gulf. Volcanic \"spreading centers\" such as the Alarcón Rise are hotbeds of volcanic activity, where underwater volcanoes spread lava across the seafloor and hydrothermal vents spout water heated by magma beneath the seafloor to over 550 degrees Fahrenheit.\nGiven the geological characteristics of Alarcón Rise, chief scientist David Clague and his submarine volcanism research group had a pretty strong suspicion that they might find hydrothermal vents in the area. However, after narrowing the search area to 200 square miles of seafloor, searching for vents would be like looking for a proverbial needle in a haystack. Using an ROV to search vast swaths of the seafloor and entire seamount ridges for vents would be “virtually impossible and incredibly time consuming,” according to Martin.\nIn 2003, Clague and his colleague Robert Vrijenhoek spent two dives unsuccessfully searching for vents within just a few kilometers of the newly discovered site. At that time, Clague and Vrijenhoek chose their dive sites based on only low-resolution maps of the seafloor and the presence of abnormally warm water in the area.\nThis time, however, Clague’s team knew exactly where to look. Two weeks earlier, MBARI’s autonomous underwater vehicle (AUV), the D. Allan B. had completed a survey of Alarcón Rise using sound (sonar) to create detailed maps of the seafloor. Shortly into the first ROV dive on this unexplored area, researchers were awed by tall chimney structures nearly identical to those revealed on the AUV’s high resolution seafloor maps.\nAs expedition leader, David Clague named the first of the newly discovered vent fields the \"Meyibó\" vent field. In the language of the Kiliwa, the indigenous people of northern Baja California, Meyibó means \"time of favorable sun.\" According to Miguel Tellez, a professor of geology in the Department of Marine Sciences at the University of Baja California (UABC), Meyibó \"implies happiness, the celebration of a good harvest, and a time for new learning.\"\nThe second vent field and the individual active chimneys in both fields have yet to be named. The chimneys will be named for groups indigenous to Baja California and Sonora, Mexico. The suggestion to use indigenous languages and names of native peoples came from Rigoberto Guardado, a marine science professor at University of Baja California who was one of three Mexican collaborators on this leg of MBARI's Gulf of California expedition.\nMBARI's mapping AUV is a specialized robotic submersible that flies over the seafloor using three different types of sonar to map features as little as 15 centimeters (five inches) tall. The AUV’s proximity to the seafloor enables its onboard sonars to distinguish fine-scale details that would be invisible to traditional ship-based sonar systems.\nOver the past couple of years, the mapping AUV has generated several new discoveries, including documenting a recent underwater lava flow and assessing the risk of oil leaking from a sunken ship off California’s coast. Since 2006, the mapping AUV has discoverd unknown chimneys at three locations along the Juan de Fuca Ridge system off the Washington-Oregon coast.\nThe successful identification of hydrothermal chimneys using sonar has exciting implications. These spectacular features are of interest to geologists, chemists, and biologists alike. Hydrothermal vents are regions where cool seawater seeps down through cracks in the seafloor where it is heated by the hot magma below the earth’s crust. As the water is heated, it becomes buoyant, rising back towards the surface and spewing out of the seafloor like a geyser.\nWhen hot vent water meets the near-freezing water on the deep seafloor, the dissolved chemicals in the hot water form into mineral particles and precipitate around the vent to form a chimney. These chimneys can grow more than a meter over a period of just a few days. However, they can also become clogged with mineral precipitates, causing the vent to become inactive. Thus, vent fields like those discovered at Alarcón Rise often have numerous “dead” chimneys scattered about.\nHydrothermal vents transport heat and chemicals into the ocean, providing an energy source that supports a robust community of deep-sea organisms, many of which are not found in any other ocean ecosystem. Knowing where and with what frequency hydrothermal vents occur is valuable information for scientists seeking to deepen our understanding of the biology, chemistry, and geology of the seafloor. MBARI’s seafloor-mapping AUV is helping oceanographers spend less time looking for, and more time looking at, these fascinating deep-sea phenomena.\nArticle by Dana Lacono\nFor more information on this article, please contact Kim Fulton-Bennett:\n(831) 775-1835, firstname.lastname@example.org"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:d43e2c81-222f-42ef-8aca-bb80c42a7666>","<urn:uuid:e0486d23-b23a-46ac-8cec-c82489c73fb9>"],"error":null}
{"question":"How do fire detection systems protect heritage buildings while minimizing visual impact, and what are the space-saving advantages of vertical farming compared to traditional agriculture?","answer":"Fire detection systems protect heritage buildings through specialized approaches that balance protection with aesthetics. These include placing sounders in hollowed-out books, housing control panels in custom cabinets, positioning detectors in room corners and above picture rails, and using wireless detectors to avoid visible wiring. For maximum effectiveness while remaining discreet, detectors can be aligned with plaster detailing and sounders placed behind thick curtains. As for vertical farming, it significantly reduces space requirements compared to conventional agriculture by expanding production vertically rather than horizontally. While traditional agriculture requires vast horizontal spaces (estimated 1.5 billion hectares globally), vertical farming uses multilayer indoor systems that can be implemented in various locations, from urban centers to rural areas, helping address the shrinking availability of arable land while increasing yield per unit area.","context":["Protecting our past\n09 February 2017\nWarren Moyle and Bill Jordan explore how fire detection systems are protecting heritage sites housing our national treasures, and historic townhouses that are home to multi-million-pound apartments across the UK\nPROTECTING HERITAGE properties is a complex challenge that requires numerous factors to be considered. Life safety should clearly always be paramount in any system specification, but it’s a sad fact that any fire occurring in a historic building will most likely result in the loss of priceless and irreplaceable contents, and run the risk of structural damage and loss of architectural features.\nAs well as the usual need for early warning, designers of fire detection systems for heritage properties must consider the need to avoid false alarms, particularly as many heritage buildings are either tourist attractions, which enjoy a high number of visitors, or flat conversions that house multiple residents.\nHistoric buildings are, by definition, old and thus were constructed at a time when there were few, if any, building or safety standards. Although most of them were originally built as homes, historic buildings are now used for various purposes. Many of them now house a lot of modern equipment, such as computer systems, generators and catering facilities.\nThis affects the type of fire detection systems that need to be installed and also requires fire safety specialists to ensure that an effective fire evacuation strategy is in place.\nMaintenance is another consideration. Heritage buildings undergo regular preservation and repair works, which involve risks that need to be factored into the design of a fire detection system. Hot works, such as welding and soldering, present a major fire risk, particularly when carried out around combustible materials, such as wooden beams, thatch, and old furnishings.\nAnother less critical, but still significant, issue is that of aesthetics, especially in buildings where historical and archaeological integrity have been preserved. In such cases, it is important to balance the fire protection needs of a site and ensuring compliance with insurance companies and/or building-authority standards with the need to make its detection systems as unobtrusive as possible.\nOne of the most popular ways of achieving this is by placing the system’s most obvious elements within specially designed housings. As an example, Firetecnics recently worked on an old manor house, where the owner wanted the sounders in the library to be as inconspicuous as possible. The sounders were positioned within hollowed-out, matching, leather-covered books and so completely blended in with their surroundings.\nAnother client, who owns a property near Harrods, in London, commissioned a pair of walnut cabinets to be built – one to house his control panel and the other for his own use. The visual impact of the two together is impressive. We’re currently exploring a similar option for a historic church, where the fire brigade has ruled that the control panel must be positioned within the entrance, but doing so would spoil the look of this lovely old building.\nWhere logistics and budgets don’t allow for total concealment, there are many clever ways of making fire detection as discreet as possible. It’s an established fact that the human eye is automatically drawn to the centre of a room, so by positioning elements in the corners of a room, rather than the middle, the visual impact is less noticeable. We also use techniques such as placing detectors above top picture rails, lining them up in even numbers alongside plaster detailing, and positioning sounders behind the thick curtains that are often found in heritage properties. While this does mean more sounders are needed to do the job effectively, this cost can be justified by the aesthetic gain.\nIt is important to remain flexible and adopt a common-sense approach to historic properties, while still meeting fire regulations. One of our projects involved an old building, which had been converted into flats and had a beautiful 200-year-old wooden door. Ideally, this would have been a fire door but the owner of the building was obviously very keen to keep the existing door. So we used the very simple solution of placing a detector directly on either side of the door to maximise fire and smoke detection in its vicinity.\nManual call points (MCP) are probably the most challenging aspect as, by their nature, they are the one element of a detection system that needs to be visible. Even in a converted house where residents are aware of what they need to do in an emergency, visitors to the building may also need to know how to activate an alarm system. In historic buildings that are open to public visitors, this visibility is even more crucial. While we can’t hide an MCP, it is certainly possible to blend it into its surroundings. Examples of this include mounting them on material that matches other aesthetic detailing in the property, such as brass plates, or even making a framed surround to lessen the contrast between the wall and the MCP.\nAlthough the approach to fire detection in historic buildings needs to be tailored to fit a range of specific criteria, the most popular product type Apollo specifies for such applications is a multisensor range, such as the SOTERIA collection, which can be programmed to switch smoke sensors over to heat sensors at different times of the day.\nThe range uses new optical sensing technology, PureLight®, to detect smoke particles entering its chambers. This unique system marks a new stage in the development of advanced optical technology, which increases the reliability of fire detection while resulting in fewer false alarms, making it ideal for the challenges often presented by heritage properties.\nNumerous other technical developments have been integrated into the SOTERIA design, including an advanced-technology chip sensor to improve smoke detection, and a sleek, low-profile design, which means less dust penetrates the outer casing. We have also designed the detectors to be less sensitive to any dust that does accumulate over long periods of time.\nWe have worked quite closely with English Heritage in the past. Many of its sites are remote and in countryside settings, making them susceptible to insects. Bug screens inside detectors have been successful in keeping insects out of the optical chamber. Careful design of this chamber also ensures that any insect small enough to penetrate the mesh barrier has fewer opportunities to interrupt the operation of the smoke detector.\nWireless detectors are a great way of providing effective fire detection without a tangle of visible wires and cables in architecturally-sensitive buildings. They also avoid the need for invasive drilling. An example of this can be seen in Exeter Cathedral. One of the finest examples of Gothic architecture in the country, it features the longest uninterrupted stretch of decorated vaulting in the world. Heritage Lottery Funding was secured to transform the upper floor of the cloisters into a dedicated space providing modern learning facilities, while retaining the historic legacy of the building. As part of this programme of work, XPander was installed to provide fire detection without compromising the aesthetics of the ceiling.\nFrom a manufacturer’s point of view, we need to ensure that our detection systems do not stand out like a sore thumb. On the installation and design sides, we must strive to provide clever solutions to conceal and minimise a detection system’s visibility. It’s common to hold four or five site meetings with property owners, management companies and guardians before we even put pen to paper to provide designs and costings.\nEach historic building is unique, and while we can’t advise on a one-size-fits-all system, our many years of experience have shown that the best approach is often to opt for either the reliability and flexibility of a multisensor system, the discreet option of a wireless solution, or, indeed, a combination of the two. By working closely together, Apollo and Firetecnics can continue to ensure effective fire detection, while remaining sympathetic to a building’s aesthetic and historical integrity.\nWarren Moyle is senior product support engineer at Apollo Fire Detectors and Bill Jordan is quality manager at Firetecnics Systems","Agricultural systems around the world need to adapt to the rapidly changing environmental, demographic, and socioeconomic landscapes, and new alternative practices, such as vertical agriculture, may offer new opportunities to accelerate such adaptation.\nNext Gen Farming Without Soil and 90% Less Water | GRATEFUL\nWhat is vertical farming and why is it important\nModern agricultural systems encompass an estimated 1.5 billion hectares of the world’s surface area. With a growing population and resource needs, the availability of arable land is shrinking rapidly.\nSince the agricultural revolution, conventional agriculture has focused on practices requiring considerable quantities of space, water, fertilizer, and pesticides. The past 50 years have seen an accelerating rate of increase in these requirements as modern food production aims to increase productivity in the hopes of addressing growing food insecurity.\nLooking into the future, yield production is forecasted to decrease due to widespread environmental and socioeconomic changes that will generate unpredictable consequences on food systems.\nIn response, many strategies have been developed as alternatives to conventional agricultural practices. These strategies have focused on key principles and their combination to be effective: require less space, less water, and increase yield per unit of area. Moreover, due to the negative effects of agrichemicals, modern practices have also aimed to use significantly less to avoid potentially adverse effects for humans and animals.\nOne such alternative is the development of vertical agriculture, also referred to as vertical farming. As the name implies, vertical agriculture relies on expanding production vertically and not horizontally. Vertical agriculture is a multilayer indoor plant production system that allows for precise control of growth factors, such as light, temperature, humidity, carbon dioxide concentration, water, and nutrients.\nThis allows for the growing and production of crops year-round, completely independent of solar light and other external conditions. Indeed, vertical agriculture makes use of key concepts within ecology and physiology to optimize growing and fertilization within controlled conditions. For instance, elements of photobiology, thermomorphogenesis, hydroponics, and genetic breeding, are all used commonly across systems of vertical agriculture.\nBenefits, challenges, and disadvantages moving from horizontal to vertical farming\nAs a result of tight control over crop breeding, growing, and harvesting, vertical farming provides several benefits relative to conventional methods of ‘horizontal’ food production. This was the topic of a literature review by Kalantari et al. published in 2016 in the Journal of Landscape Ecology.\nFrom a systems perspective, the enclosed design prevents pests and diseases from entering by the adoption of a high level of hygiene, continuous monitoring, and non-chemical disinfection, providing security from crops. Moreover, recent technology has also allowed for automated control over environmental conditions by using sensors and imaging techniques in combination with crop simulation models and artificial intelligence, limiting the need for physical labor.\nVertical farming also allows for flexible organization, with designs ranging from large vertical walls covered with crops to large hangars or re-used shipping containers that can be transported. Consequently, vertical agricultural systems, can comprise many varying sizes and be located within many different areas from the middle of highly urbanized cities to more suburban or rural areas.\nMoreover, the verticality element of this system also provides nutrient and water flow, helping to reuse costly resources. The reduction in space also means there is a significant increase in yield per area, holding extensive potential for a future world of urbanization.\nFrom an economic perspective, vertical farming also provides for more jobs in localized areas and is community-focused by addressing the needs of immediate areas, which in turn can provide food at a lower price. Finally, the optionality of location for vertical systems also allows producers to reduce transport costs, as consumers may access them within urban areas, or transport can be minimized to nearby areas.\nHowever, despite the design, environmental, and economic advantages, vertical farming also incorporates several issues that remain a challenge to its broader implementation as a system.\nVertical farming has a high energy requirement and needs extensive investment costs to implement and develop successfully. Moreover, indoor issues relating to excessive UV, heat, and ozone-induced plant damage may have unpredictable repercussions for plant growth.\nAdditionally, vertical systems are difficult to adapt to a larger scale. They are costly to build and maintain and have yet to demonstrate the ability to provide food for larger areas than community-scale populations. This would make it difficult to implement in areas at higher risk of food insecurity, such as developing agricultural nations.\nThe lack of empirical research on a broader scale has meant that vertical farming has yet to develop past the concept stage on community levels, as persistent issues make it difficult to break through to a larger scale.\nImage Credit: YEINISM/Shutterstock.com\nGrowing skywards - the implications of vertical farming in a rapidly populating and changing world\nAmong the development of alternative agricultural practices, vertical agriculture provides a promising solution for many of the challenges facing current agricultural policies. However, for vertical systems to be integrated on a larger scale requires further technological progress and economic investment.\nNevertheless, gradually implementing more verticality, or combining it with other practices aiming for more sustainable practices may be promising. For instance, the combination of verticality with other practices such as intercropping may be particularly beneficial for developing more sustainable food systems.\nIncorporating technological progress into vertical systems also holds promise, with automated sensors and machinery able to operate near-independently. Progress in gene editing and plant genome modifications will also allow for faster, bigger, and healthier crops, allowing vertical agriculture to produce more over time.\nThroughout agricultural history, farming systems have typically spread over large spans of land, yet the reduction in arable land, as well as the increase in demand to house growing populations, means that such strategies need to be reconsidered, and vertical agriculture may play a role looking into the future.\nContinue Reading: Benefits of Vertical Agriculture and Hydroponics\n- Beacham, A. M., Vickers, L. H., & Monaghan, J. M. (2019). Vertical farming: a summary of approaches to growing skywards. The Journal of Horticultural Science and Biotechnology, 94(3), 277–283. doi: 10.1080/14620316.2019.1574214\n- Chaudhry A. R. and Mishra V. P.,(2019) A Comparative Analysis of Vertical Agriculture Systems in Residential Apartments, Advances in Science and Engineering Technology International Conferences (ASET), 2019, pp. 1-5, doi: 10.1109/ICASET.2019.8714358\n- Sarkar, A., & Majumder, M. (2015). Opportunities and Challenges in Sustainability of Vertical Eco-Farming: A Review. Journal of Advanced Agricultural Technologies, 2(2). doi: 10.12720/joaat.2.2.98-105\n- SharathKumar, M., Heuvelink, E., & Marcelis, L. F. (2020). Vertical Farming: Moving from Genetic to Environmental Modification. Trends in Plant Science, 25(8), 724–727. doi: 10.1016/j.tplants.2020.05.012"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:3ec3e63a-3c46-46a3-a767-f675bec074a8>","<urn:uuid:3566e03d-b8ac-4d36-b13c-9d56f4d7ba0e>"],"error":null}
{"question":"How do the prevention strategies differ between rodent and termite infestations in residential buildings, particularly regarding bait placement and environmental modifications?","answer":"The prevention strategies for rodents and termites differ significantly in several ways. For rodents, control relies on a 'three lines of defense' approach: fenceline, building perimeter, and building interior. Bait stations must be placed in areas of high rodent activity and along their runways, with special attention to doorways and elevated locations. These stations need to be secured, lockable, and tamper-resistant. For termites, prevention focuses on eliminating wood-to-soil contact, maintaining at least 6 inches between wood and soil, reducing moisture, repairing leaks, and proper ventilation. Additionally, homeowners should avoid placing cellulose materials like firewood or mulch near the house, and consider using rocks or rubber mulch instead. Both pest control strategies emphasize proper placement and protection of treatment materials, but rodent control focuses on intercepting their movement patterns while termite control emphasizes modifying environmental conditions that attract them.","context":["With rodent season underway, now is the ideal time for pest managers to pause and recap the ‘three lines of defence’ approach to rodent control.\nRegardless of which rodent bait is used, it is the placement of the bait that will determine the effectiveness of the treatment. When developing a baiting strategy, a professional pest manager should always think of the ‘three lines of defence’ against rodents: the fenceline, the external perimeter of the building, and the building interior.\nIt is imperative that the bait is placed in areas of high activity, or at the very least, as close as possible to rodent runways. Research has shown that no matter how attractive a bait, it is unusual for a rodent to move more than a few feet from its normal pathway. The most effective control option is to stop rodents at the fenceline. However, second generation anticoagulants baits are not registered for fenceline use, so when considering the possibility of fenceline baiting, products such as Rampage should be considered instead.\nHowever, for a variety of access and safety reasons, it is not always possible to secure the fenceline of a property. In such cases, the pest manager should focus on the second line of defence: the building perimeter. As one of the most common rodent entry points into a building is the doorway, it is important to place bait on both the inside and outside of doorways.\nBoth rats and mice can, and will, climb to gain access to a building. During an inspection, be sure to check for rodent markings in elevated locations. If signs of rodent activity are detected, be prepared to secure baits and/or traps in those areas.\nOnce inside a building, bait placement will depend on whether the treatment is for rats or mice. They exhibit different travel habits; mice do not move far from their harbourage whereas rats, particularly roof rats, may travel significant distances. Rats will also jump to reach an attractive food source. It has been reported that some auditors do not accept bait stations fixed off of the ground. A few inches will not affect the uptake of bait but it might keep bait clear from water in a wash-down area, for example in commercial accounts. Pest managers should be prepared to argue for what they know is right!\nWhether treating rats or mice, baits should be placed in secured, lockable, and tamper-resistant bait stations. Bait stations protect the bait from the environment – rainfall, dust, and wind can all reduce bait effectiveness – and also serve to protect non-targeted species. Good bait stations should be child-proof and dog-proof as well as prohibiting access by larger animals.\nStations with secured bait appeal to rodent behaviour, as they provide asylum for rodents to eat in peace and out of apparent danger. Secured bait offers traceability, as pest managers can measure the quantity of bait being consumed and determine what is eating it. It is impossible to track what happens to unsecured bait, whether it be dragged and/or dropped, returned to the nest, taken by a non-target creature, or simply moved. Bait stations also look professional, and can be customised to fit a client’s needs.\nWhen using rodenticides, pest managers must read and understand the implications of the label for each rodenticide selected. Bait placements will be stipulated by the manufacturer and will vary somewhat from product to product. If there is any uncertainty about the placement or use of a particular rodenticide, it’s useful to remember that ‘the label is the law’.\nMore information on rats and mice.","Fact Sheet FS338\nTermites are the most destructive insects to homes. In the U.S., termite control costs alone exceeds $1.5 billion each year. Most people are not aware of termites until significant damage to the structure has occurred. It is therefore very important to understand the basic biology, prevention, and control of termites.\nAll naturally occurring termites in the northeastern U.S. belong to a group called subterranean termites (genus Reticulitermes) (Figure 1). The common name comes from their habit of nesting in the soil. Their nests are rarely found above ground. Subterranean termites build mud tubes and bring soil to the wood where they feed (Figure 2 and 3). In the southern U.S. and western U.S., drywood and dampwood termite species are also found. They nest in wood above ground. Occasionally, drywood termites may be transported along with infested furniture or timber to other parts of the country. Drywood termites do not bring soil to the wood, and they produce pellets as they feed. These termites usually do not cause damage to structures outside their natural distribution areas.\nTermites are social insects with various castes. Most of the members are white in color. Only the king, the queen, and swarmers (also called alates) are darkly pigmented. Soldiers' heads are reddish. Most people encounter termites during spring when termite swarmers (Figure 2) leave their natal nests. Termite swarmers may be confused with ant swarmers but can be distinguished by the following:\n|Points of Difference||Termite Swarmers||Ant Swarmers|\n|Size of wings||Hind wings are approximately the same size as the forewings.||Hind wings are much smaller than the forewings.|\n|Color of wings||Dusky||Clear|\n|Veins of wings||Weak||Very distinct|\n|Shape of body||The abdomen is joined snugly to the thorax, without a node or waist.||The abdomen is distinctly separated from the thorax by a very small node or thin waist.|\nEach subterranean termite colony contains reproductives (a king, a queen, and some neotenic reproductives), larvae, nymphs, workers, and soldiers (Figure 1). The neotenic reproductives are non-alates, and they produce fewer eggs than the queen. The majority of colony members are workers. They look for food, feed the other members of the colony, and maintain the nest and foraging tunnels. Soldiers have rectangular brownish heads and long mandibles. They typically account for 1–2% of the members. Nymphs develop into alates or neotenic reproductives. A colony may have hundreds or thousands of neotenic reproductives. The neotenic reproductives play an important role in the growth and expansion of colonies. Thus, killing the queen will not cause the colony to die. A mature colony has 60,000 to 1 million individuals.\nSubterranean termites naturally occur in forests and urban environments where cellulose materials (such as stumps, mulch, and dead trees) exist. Most of the houses in the U.S. contain wood materials. When the wood materials become moist or contact the soil, termites will find their way into the wood. Workers constantly forage for food. Their only food is cellulose material such as wood and paper. Under ideal conditions, a termite colony of 60,000 would consume about 5 grams, or 1/5 ounce of wood each day. A subterranean termite colony may cover 260 ft linear distance.\nA new subterranean termite colony can be formed in two ways:\n- a pair of swarmers (a male and a female alate) mate, drop wings, and build a nest;\n- budding from an existing colony.\nThe swarming adults only appear once or twice from mature colonies every year. Hundreds or thousands of swarmers leave the nest during warm sunny days between April–June. This is when most homeowners first notice termites in their homes. After a brief flight, the swarmers soon drop their wings and form pairs. They then look for a moist, sheltered place to build a nest. It takes several years for the colony to mature and produce swarmers. As the colony grows, part of it may separate from the main colony and form a new colony. Neotenic reproductives head such budded colonies.\nHow to Conduct a Proper Termite Inspection?\nIt is not difficult for a layperson to inspect a home for termites when obvious signs are present (Figure 3 and 4). Unfortunately, mud tubes are often absent, making it very difficult to determine whether termites are present. It is even more difficult for a layperson to distinguish between old termite damage and new termite damage. For these reasons, homeowners are encouraged to consult with experienced professionals for a quality inspection. The following are the general procedures for conducting a termite inspection. First, obtain inspection tools. A flashlight and a probing tool (garden trowel, screw driver, or ice pick) are a must. Knee pads, a hard hat, a mirror, collection bottles for specimens, and a moisture meter will help with an inspection. An inspection should focus in areas where there is high moisture and wood or other cellulose materials: window sills, the bottom of door frames, the crawl space, the basement, and foundations immediately above ground, etc. Look for mud tubes and damaged wood. Tap the exposed wood with a probing tool. A hollow sound may reveal damage beneath the wood surface. Old houses may have mud tubes from previous infestations. Gently break a small section of the mud tubes to determine whether they are moist and whether termites are currently present. Active mud tubes can be distinguished from old mud tubes by a darker color, moist soil, and presence of live termites. Use a garden trowel to examine the mulch near the foundation. If termites are found in the mulch, the structure should be very thoroughly inspected, since termites can occur in the immediate vicinity. It is a good idea to inspect the house at least once a year in the summer to detect termite activity.\nIt is usually much easier and cheaper to conduct preventive work than to treat an infestation. Thus, property owners should follow common sense rules to avoid expensive treatment and repair afterwards.\nFirst, avoid wood-to-soil contact or use treated wood if wood has to be used in a moist area. Ideally, any wood in a structure should be at least 6 inches above the soil. Repaint any wood that is close to the ground every few years to prevent moisture intrusion. Do not bury any wood materials (stumps, branches, wood debris) when building a home. Do not place cellulose materials (such as firewood, mulch) immediately adjacent to a house. Using rocks or rubber mulch near the house will reduce the probability of termites remaining near the house. Cut down shrubs or large trees and remove stumps near the foundation to reduce the presence of roots and plant debris.\nSecond, reduce moisture and promptly repair leaks. The gutters and down spouts should be properly installed and maintained. Seal cracks and holes on exterior walls to prevent moisture getting inside the walls. Crawl spaces should be properly ventilated.\nThird, use treated wood or steel in porches and other areas that are susceptible to termite attack.\nFourth, install stainless steel mesh or pesticide-impregnated sheeting for new construction. These technologies are currently not widely used in the U.S.\nOnce termites are found in a home, treatment is necessary to kill them. There are two types of treatments commonly used: soil treatment and baiting. Proper treatment requires special training, equipment, and materials. Homeowners should look for licensed professionals to properly eliminate termite infestations.\nA soil treatment requires digging a trench around the exterior perimeter of the house. A liquid insecticide is then applied to the trench to form a continuous barrier between the house and the soil outside of the house (Figure 5). In conjunction with the exterior treatment, the insecticide may be applied also locally inside homes in areas where termite activities are found. In areas covered by concrete or wood, small holes are drilled every 12 inches or as the label directs (Figure 6). Insecticides are then injected with a rod, and the holes are plugged or otherwise filled. Because termites nest in the soil and constantly travel back and forth between the soil and the wood, they come in contact with the treated soil. When properly treated, a colony may be eliminated within one or two months. The soil treatment can be effective for more than 5 years.\nCommonly used liquid termiticides can be grouped into non-repellent and repellent termiticides. Non-repellent termiticides include fipronil, imidacloprid, and chlorfenapyr. These active ingredients are relatively slow-acting. Termites freely re-enter treated soil. They pick up enough dose of the material as they pass through the treated areas and transfer the chemical to their nestmates, causing the death of the whole colony. Commonly used repellent termiticides include permethrin, cypermethrin, fenvalerate, and bifenthrin. They belong to a chemical group called pyrethroids. Pyrethroids are highly repellent to termites and provide a barrier around the structure. However, pyrethroid treatments do not diminish termite populations in soil. Termites in the vicinity of the treatment would continue to forage freely. Thus, non-repellent termiticides are preferred materials used by professionals for treating structures that are already infested.\nBaiting is another commonly used termite treatment method. In this method, plastic tubes are installed underground at approximately 10 ft. intervals encircling the house (Figure 7). Each tube contains wood and/or toxic bait. The bait tubes are examined regularly, from monthly to annually, depending on the type of bait tubes being used. Termites foraging around the house will eventually find the wood or bait. The bait is more palatable than the wood and contains a slow-acting material. Termite workers eat the bait and bring the material to the rest of the members of the colony. A colony will be killed over a few months once termites find the bait. Aboveground termite bait stations can be placed inside houses where termite activity is found (Figure 8). Each termite bait needs to be inspected periodically (quarterly to annually) to ensure enough bait remains, and that termites are eliminated. Commonly used active ingredients in baits are noviflumuron, hexaflumuron, and diflubenzuron.\nIn general, liquid treatment is often cheaper than bait treatment, depending upon the size of the structure to be treated. Liquid treatment provides immediate protection to the structure. Disadvantages are:\n- The procedures are somewhat destructive (such as drilling holes into concrete surfaces, digging a trench).\n- A large volume of insecticides is applied to the environment.\n- Structures with a well, spring, or cistern nearby cannot be treated with liquid insecticides due to possible water contamination issues.\nBait treatment has little environmental impact because the bait is contained inside capped plastic tubes. After termites are eliminated, the bait can be removed and no pesticides remain on the property. The equipment needed for bait treatment is simple. The treatment causes little destruction to the property. Disadvantages of this method include:\n- It may take several months or more for termites to find the stations.\n- The lag time between monitoring and baiting extends the period to achieve termite control.\n- The baits need to be inspected periodically and maintained.\n- Homeowners, their children, or pets may dislodge the bait stations through gardening, playing, or mowing activities.\nDo I Need Annual Termite Inspection Service?\nPest control companies often recommend annual inspection services after termite treatment to detect future termite activity. This is helpful to detect termite infestation early. Homes surrounded by large trees, old stumps, or beds of deep mulch are more likely to have new infestations over time. Alternatively, homeowners may use over-the-counter monitoring stations for monitoring termite activity. Strategically place the monitors in areas that favor termite survival such as in the mulch, besides a stump or wooden deck. Inspect the monitors a few times a year between April–November. Termite infestations in the northeastern U.S. almost always start from outside. Installing monitoring stations around homes is a good proactive method for detecting new termite activity.\nMention or display of a trademark, proprietary product, or firm in text or figures does not constitute an endorsement by Rutgers Cooperative Extension and does not imply approval to the exclusion of other suitable products or firms.\nPhoto credits: University of Georgia (Figure 4 right); Changlu Wang (Figures 1–3, 4 left, 5–8).\nCopyright © 2022 Rutgers, The State University of New Jersey. All rights reserved.\nFor more information: njaes.rutgers.edu.\nCooperating Agencies: Rutgers, The State University of New Jersey, U.S. Department of Agriculture, and Boards of County Commissioners. Rutgers Cooperative Extension, a unit of the Rutgers New Jersey Agricultural Experiment Station, is an equal opportunity program provider and employer."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:228ccddc-477c-4239-ba21-b492f99f88bf>","<urn:uuid:0a77eddf-99e5-48a2-844b-70a663c9da84>"],"error":null}
{"question":"¿Cuáles son los beneficios of using a smart home hub, and what development challenges do manufacturers face when creating smart home products?","answer":"Smart home hubs offer the key benefit of controlling the widest array of devices from one central app or interface and getting them working together. They allow users to create multi-step automated actions across different manufacturers' devices and provide centralized control. As for manufacturing challenges, according to industry surveys, developers face significant issues with connectivity (especially human-machine interface and device-to-internet connectivity), lack of reference architectures for hardware and software (30% of respondents), complicated hardware specifications (23%), and insufficient technical expertise to build connected solutions (22%). Security and privacy concerns are also major challenges in smart home product development.","context":["How does the smart home of your dreams look? Maybe your front door automatically unlocks when it detects your phone is nearby, disabling the home alarm system and triggering the lights and air conditioning to turn on. The door automatically locks behind you. As the sun sets, the lighting in the room softens, the A/C shuts off, and the blinds close. The key is to get the smart home components you buy to communicate—and for that, you need a smart home hub. But what is a smart home hub? This guide will answer that question and more.\nNot everyone with a smart home setup needs a dedicated smart home hub, however. You can do a lot without one. The greatest advantage of having a hub is being able to control the widest array of devices from one central app or interface and get them working together. Here’s what else you need to know about smart home hubs before deciding to buy one.\nWhat is a smart home hub?\nA smart home hub is like a central command for a smart home. It’s hardware or software (often both) that connects to smart home devices, allowing you to control them all from one app. Let’s say you have smart light bulbs, a smart lock on your front door, and a smart thermostat. Rather than manage each one separately, you can connect them all to a smart home hub and control them from one place.\nYou can also create multi-step actions that all your devices will do automatically, even if they’re made by different manufacturers. For example, you can set up a series of actions so that when you arrive home and your smart lock detects your phone, it automatically triggers the lights to turn on, your home alarm to turn off, and the thermostat to adjust to a temperature of your choosing.\nSmart home hub examples\nThe variety of smart home hubs on the market today makes it a little confusing to see how they are all related. Some are standalone products, and some have the technology bundled into another product. Some are hardware plus an app, and some are just apps.\nOne straightforward example of a smart home hub, which was among the first ever to be created, is the SmartThings Hub by Samsung. It looks similar to a router, and in fact, it connects to your router to work. Wink Hub 2 is another straightforward example.\nSmartThings has other smart home hubs, too, such as the Samsung SmartThings WiFi. It’s a Wi-Fi router and smart hub bundled into one device. Bundling together a smart home hub and router makes sense. When other products start bundling in smart home hub technology, however, it can get a little more confusing. Smart home hubs can be bundled into home security system panels, streaming boxes such as the Apple TV 4K, and other devices.\nMost smart speakers are smart home hubs—or at least they act like them. The Amazon Echo Plus (2nd Gen) properly doubles as a smart home hub, while the original Amazon Echo isn’t technically a hub. The original Echo still provides so much of the same functionality, however, that for many people, the difference isn’t meaningful.\nWhat about software-only hubs? Eve is one such hub for Apple HomeKit devices. The Stringify app lets you connect a variety of smart home devices to it. Wink’s app works with or without the hub, although there are some advantages to having a hub, which will be further explained later. Yonomi is another that’s compatible with not only iOS and Android but also Apple Watch and Android Wear.\n- What is Google Home?\n- Google Home vs. Google Home Mini: Which is better?\n- Apple Homepod vs Google Home vs Amazon Echo\n- How to use Google Home as an intercom system\nHow do smart home hubs work?\nA smart home hub is the command center for your smart home. You connect other devices to the hub. The hub gives you remote control access to these devices from an app, or through another connected device, such as a smart speaker.\nHow all of your devices connect to the hub is part of the magic. Many smart home hubs support more than one connection type (protocol). Wi-Fi and Bluetooth are two well-known ways of wirelessly connecting devices. Other smart home gadgets use a different protocol, such as Zigbee or Z-Wave.\nThere are other types of protocols for connecting devices around your home that aren’t even wireless, such as universal powerline bus, or UPB. That method uses your home’s electrical wiring to transmit communication, and it can be peer-to-peer. UPB is still commonly used in intercom systems. Before the days of DIY wireless products, UPB and other early protocols, such as X10, were the only option for creating a smart home. These days, UPB is still around, and the devices that use them sometimes even have an option to connect to a smart home hub, so you can still rely on one app to run everything.\nWhy can’t I connect everything via Wi-Fi?\nYou might be wondering, “Couldn’t I simply buy all Wi-Fi-compatible devices and connect to them that way?” Yes, you could, but depending on your setup, it might create problems.\nIf you connect too many devices to your home Wi-Fi, then the whole network slows down. Additionally, Wi-Fi connectivity tends to drain batteries faster than other protocols do. Depending on your setup, you’d probably end up with a few different apps to control your devices rather than one app to manage them all. And not all smart home devices are Wi-Fi-enabled, so you’d be cutting yourself short on the products you could buy. You could still create automations across smart home devices from different manufacturers using IFTTT, an app that stands for “if this, then that.” But overall, relying on Wi-Fi alone to set up a smart home is not ideal.\nWhy do smart home hubs connect to smart speakers?\nIf you’re interested in buying a smart home hub, you’ll see a bunch of them advertise that they support Amazon Alexa or Google Home. This can be confusing if you thought smart speakers already were smart home hubs! What gives?\nAs mentioned, some smart speakers are true home hubs. Amazon Echo Plus is the prime example because it supports the protocol Zigbee, although it doesn’t support Z-Wave. So, you can connect devices if they use the Zigbee protocol.\n- Amazon Alexa is the home assistant you never knew you needed\n- 12 essential Amazon Echo accessories for your smart home\n- You can now use Amazon Alexa devices as an intercom system\n- The Amazon Fire Stick can help you finally cut the cord\nOther speakers don’t quite measure up because they don’t support commonly used protocols for connectivity, or because the digital assistant app is really the hub and not the speaker itself.\nIn any event, you might set up your smart home using a smart home hub and add a smart speaker so that you can use voice commands around your home. If you choose a hub that doesn’t rely on having a speaker, you don’t have to worry about a device that’s listening to everything that goes on under your roof.\nDo I need a smart home hub?\nUltimately, do you need a smart home hub at all? It depends on:\n- How many smart home products you need to control\n- How elaborately you want to connect them\n- The ease with which you want to be able to control your devices\n- Whether you already own something that can act as a hub, given your setup.\nYou don’t necessarily need a smart home hub to have elements of a smart home in place. If you plan to add to your smart home over time, a hub will certainly give you a lot of benefits, and it may be to your advantage to start with early on so that you can master it long before your home looks like Tony Stark’s.","Smart-home products encompass a variety of devices that includes smart lighting, connected appliances, security systems, and health-care monitoring. A 2020 Jabil Smart Home Technology Trends survey of IoT manufacturing stakeholders, conducted by Dimensional Research, found that the market has more than doubled in two years, according to nearly 60% of respondents. Forty percent of stakeholders experienced 2× to 5× growth.\nWhile demand and adoption of these devices continue to grow, product development teams face increasing challenges around security, privacy, and connectivity. One of the biggest design challenges is connectivity, according to the Jabil survey, with human-machine interface and device-to-internet connectivity (Wi-Fi, 5G, etc.) leading the trouble spots.\nOther product development challenges, cited by survey takers, include a lack of reference architectures that address both hardware and software (30%), increasingly complicated specs for hardware creation (23%), and a lack of technical expertise and capabilities to build connected solutions (22%).\nThis month’s issue on smart homes takes a look at some of the devices and systems that can help product developers ease some of these challenges around security, connectivity, and technical expertise, as well as time-to-market concerns.\nOne of the biggest problem solvers for helping designers reduce hardware development work and simplify connectivity are wireless microcontroller (MCU) modules. These highly integrated modules provide matching circuits, security, connectivity, pre-certifications, and, in some cases, pre-provisioning for the cloud. They also come with hardware reference designs that offer benefits in several areas, including lower design risk and faster time to market. In some cases, designing with these modules requires little to no RF design expertise.\nToday’s developers face more challenges than ever when making their design decisions, including longer development times and higher security threats, said Maurizio Di Paolo Emilio, editor-in-chief of Power Electronics News. Development kits and other prototyping platforms are vital to accelerate innovation and are one of the best ways to get started on any IoT project, he said. Maurizio discusses a few development kit and platform examples that can help developers get started on their projects.\nA big part of IoT challenges is security, agreed industry players. Nitin Dahad, editor-in-chief of Embedded.com, reports that security for smart homes is all about identity and trust. Protecting smart-home devices from security risks requires a suite of protection, he said, which includes secure identity, secure authorization/attestation, secure provisioning, and secure over-the-air updates for when the device is in the field.\nOne component area that is being significantly impacted by the “smart” trend, whether it is homes, buildings, vehicles, industrial equipment, or medical devices, is MCUs. In most IoT applications, power consumption and enhanced security have become the biggest pain points for designers. Over the past six months alone, many chipmakers have developed new product offerings that address these issues.\nBut it’s more than just low standby current and advanced security features like symmetric and asymmetric cryptography, advanced key management, and tamper detection. These chipmakers are also providing a range of development boards, tools, and kits, including IDEs, software suites, and application examples, all designed to ease product development.\nThere are still several challenges that impede IoT growth, said Jake Alamat, vice president and general manager of IoT Home & Consumer at Silicon Labs, and it will require intuitive user experiences, increased security, and greater interoperability for the transformation into intelligent IoT smart homes. He said: “There are still too many incompatible protocols; numerous smart-home devices don’t work effectively across multiple ecosystems; and there is an increased need for device security. So what needs to be done to enable the future smart home?”\nWhile more work still needs to be done, said Alamat, “new industry collaboration is opening doors for greater interoperability and developing new standards that will extend the intelligent, secure boundaries of the home beyond what we can imagine today.”\n(Cover image: Shutterstock)"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:628b7e3e-c21d-4137-b59b-bd988d5a3776>","<urn:uuid:b1306df8-d9b4-45ff-bb14-5d80c37a47a3>"],"error":null}
{"question":"How has astronomy evolved from ancient sky observations to modern scientific understanding, and what role does current imaging technology play in this field?","answer":"Astronomy evolved from early humans observing sky phenomena like eclipses and seasonal cycles to a systematic scientific discipline. Through contributions from figures like Ptolemy, Copernicus, Newton, and Hubble, our understanding of cosmic objects like nebulae, pulsars, and black holes has grown significantly. Modern imaging technology has become crucial to astronomical research, with various methods including video equipment, CCD cameras, and telescopes that compensate for Earth's rotation. These tools, combined with computer software, enable both professional astronomers and hobbyists to capture and study celestial objects, making astronomy accessible to everyone while advancing scientific research.","context":["Astronomy: 3000 Years of Stargazing\nDuration: 34 min. Language: English (UK & US), Spanish, Catalan, French, Thai, Japanese, Brazilian Portuguese, Chinese Technical data: resolution up to 3.2K, audio 5.1 available Audience: General audience\nAward winner at the 2010 International Planetarium Movie Festival at GwaCheon National Science Museum in South Korea\nThe sky, and everything that happens in it, has always piqued our curiosity. Eclipses, the regular cycle of the seasons, and the motion of the stars have fascinated us since our earliest ancestors looked up to the sky.\nIn the learned company of a cartoon Einstein, this show takes audiences on a tour of the major astronomical milestones of the last 3,000 years -- from the cosmological models of antiquity, through the Ptolemaic system of epicycles, to the contributions of Copernicus, Newton, Hubble and many others. We explore the discoveries made possible by the use of technology -- from the first telescope used by Galileo -- to modern ones in use on Earth and in space. All have revealed the beauty of the cosmos.\nCome along for an exploration of nebulae, pulsars, and black holes and 3,000 years of astronomy exploration!\nPreview the full show Astronomy: 3000 Years of Stargazing in current language versions:\n- English (US version)\n- English (UK version)\n- Brazilian Portuguese\nScience Education Content\nAstronomy: 3000 Years of Stargazing focuses on the history of astronomy with emphasis on how people have used the sky throughout history. First we used it to help understand and predict the seasonal cycles of weather. Then, with the invention of instruments to observe objects in the sky, the science of astronomy developed into a system of understanding the intrinsic nature of those objects. These concepts are woven together along with historical and cultural perceptions of the sky and help relate the information presented in the show to the lives of students, families, and the general public.\nShow content is relevant in the following subject areas:\nPhysical Science, Life Science, and Earth and Space Science Standards\n- objects in the sky: Sun, Moon, stars, planets, galaxies, quasars\n- properties of stars\n- creation of the universe (the Big Bang)\n- position and motion of objects in space\nHistory and Nature of Science\n- astronomy as a human endeavor\n- history of astronomy and historical perspectives\n- the nature of astronomy\nScience and Technology\n- Understanding of how astronomy technology has been developed and how it works\nScience as Inquiry\n- Understanding the concepts behind the motions of sky objects\n- Realizing how we know what we know about astronomy\n- What one needs in order to understand the cosmos\nScience in Personal and Social Perspectives\n- change in the sky\n- early maps of the sky\n- rise of scientific exploration of the universe\n- development of the modern science of astronomy\n- early human history, cultures, and perceptions of the sky\nThis show adheres to principles put forth in the National Academy of Sciences' Education Standards (USA) published in 1996. For more details, visit the NAS Standards Web site.","Author : Lowell Bradford\nAstrophotography is the process of taking photos of objects in space. Whether photographing a celestial object visible with the naked eye, such as the moon or a group of stars, or astronomers photographing space with the Hubble telescope, photographing space is astrophotography. The practice of astrophotography does not date as far back as some other sciences, simply because it depends on photography. Photography didn’t become a viable invention until the early 19th century. The first case of astrophotography took place in 1840 when the moon was photographed for the first time. Over a century later, astrophotography is the method used to capture the world’s most phenomenal images of space. Astrophotography is available to everyone. From professional astronomers to the backyard skygazer, all can enjoy the wonders of astrophotography.\nOverview of Astrophotography\nThere are several methods used in astrophotography including various techniques, camera and video equipment, and various telescopes. Understanding the type of equipment used in astrophotography will help ensure you select the best methods for taking photos. The first thing to understand is that astrophotography is different from standard photography. Those accustomed to taking standard photos may find photographing celestial objects is more difficult than anticipated. Lighting, shadows, atmospheric changes, and the distance of heavenly objects must be taken into account to ensure the best photos are captured.\nTelescopes should be adjusted to accommodate for the rotation of the earth. This is accomplished by setting equipment to rotate in an opposite manner from earth. Telescope mounts are an important device used in astrophotography that ensures photos captured are precisely timed and accurately track heavenly objects. Preventing tracking errors is vital to successful astrophotography and there are modern-day breakthroughs in computer science that helps make that happen.\nAstrophotography is one of the oldest forms of science-based photography. Beginning in the 19th century, early astrophotography consisted of photographing the moon, stars, eclipses, and nebulas. The procedure requires long exposure and early images were known for being quite blurry. It was not uncommon for telescopes to lose power, focus, or direction during a prolonged photo shoot.\nThe earliest astrophotography photos captured were of the moon, followed by the sun and stars. It wasn’t until the early 20th century that astrophotography began to become an important scientific research method. The invention of refracting telescopes enabled more powerful imaging to be captured. As the 20th century progressed, new telescopes such as the Hale, Samuel Oschin, and Hubble revolutionized the art of professional astrophotography. By the late 20th century, new telescopes and equipment allowed for some of the most awe-inspiring space photos to be captured for the first time in history.\nWhile astrophotography is an important tool used in scientific applications, it is also a popular hobby. Advances in video equipment, standard, and digital cameras have enabled the backyard enthusiast to capture amazing photos. Computer software makes it possible to take photos that appear as though a professional took them. From the novice to the advanced hobbyist, everyone can enjoy the art and science of astrophotography.\nA variety of media devices and equipment setups are used. Examples include video and webcams, CCD, over-the-counter cameras, single lens and digital single lens cameras. Computer software may be used to adjust cameras and telescopes to zoom in on certain objects then take pictures. Photos taken may be edited in image processing software, for a clearer picture. Please consult the following links for additional information on astrophotography."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:a854c9f8-deb9-448a-8b6e-96fea2ca143a>","<urn:uuid:98662f17-7243-44e6-8b93-f249dc488e6a>"],"error":null}
{"question":"What are the three key population groups that experience higher rates of chronic disease hospitalizations in Australia?","answer":"Three often overlapping groups had higher rates of hospitalisations for chronic diseases: people living in remote areas, people living in areas of socioeconomic disadvantage, and Aboriginal and Torres Strait Islander Australians.","context":["Mapping variation is an invaluable tool for understanding how our healthcare system is providing care, but gathering the data is only the first step. Understanding the underlying reasons for marked differences in the use of some health services across Australia, and considering how we can improve, are key for translating this work into better outcomes for patients.\nSometimes variation is expected, and even a good thing – for instance, when it reflects a response to differences in patient needs or choice of treatment options. When a difference in the use of health services does not reflect these factors, it is unwarranted variation and represents an opportunity for the health system to improve. This improvement may involve increasing access to treatment options that produce better outcomes for patients, or reducing treatment with little or uncertain benefit.\nThis Second Australian Atlas of Healthcare Variation examines variation in 18 clinical items. It paints a picture of variation in the use of a number of interventions not covered in the first Atlas, such as hospitalisations for appendicectomy and caesarean section in younger women. Some interventions are investigated in this Atlas to build on the findings from the first Atlas – for example, examining hysterectomy and endometrial ablation separately, and examining rates of cataract surgery using a more comprehensive dataset.\nThe Australian Commission on Safety and Quality in Health Care (the Commission) has consulted widely to interpret the data in the second Atlas. Clinicians, policymakers, epidemiologists, researchers and consumers have helped us identify the likely drivers of variation for each type of hospitalisation examined and, most importantly, what needs to be done to improve care. The Atlas contains a number of clear recommendations based on the best available evidence.\nWe have aimed not simply to identify an issue, but to identify specific and achievable paths for improvement and further exploration.\nWhat are the reasons for variation?\nSystem factors that favour particular treatment options may explain variation in use of some procedures. For example, higher rates of hysterectomy in some areas of Australia and a higher rate in Australia than in other comparable countries could be due in part to lower uptake of less invasive alternatives, such as the levonorgestrel intrauterine system and endometrial ablation to manage heavy menstrual bleeding. Awareness and availability of less invasive treatments could see more women deciding to receive more conservative evidence-based treatments as an alternative to hysterectomy.\nVariation can also stem from ‘indication creep’, where the use of a procedure or treatment grows beyond the original patient group in which it was trialled and shown to be valuable. A lack of evidence in this new patient group can then lead to clinicians having widely different beliefs about which patients are most likely to benefit from the procedure. For example, lumbar spinal fusion was initially used to treat spinal deformities and fractures, but the use of this operation has extended; it is now also used in some instances when people have symptoms arising from degenerative disease. The variation in use now may reflect differences in clinician opinions on the efficacy of the procedure in this newer patient group.\nFor other items in the Atlas, the major contributors to variation are clear, although the specific factors may differ between areas. Higher rates of hospitalisation for conditions such as chronic obstructive pulmonary disease (COPD) and diabetes can be explained partly by higher rates of the conditions in some areas of Australia. Differences in the implementation of integrated care, which can help prevent people with chronic diseases deteriorating, are also likely to contribute to the variation in hospitalisations for these conditions. Some hospitalisations for chronic diseases are inevitable. However, the magnitude of the difference between areas of Australia and the sheer number of patients hospitalised highlight the need to do better – in both preventing the underlying conditions and enabling people with chronic diseases to stay as well as they can be.\nThe patterns in the Atlas also tell a story about inequity. Three often overlapping groups had higher rates of hospitalisations for the chronic diseases examined: people living in remote areas, people living in areas of socioeconomic disadvantage, and Aboriginal and Torres Strait Islander Australians. A whole-of-health‑sector approach, and indeed a whole-of-government approach, is needed to make changes that ensure that all Australians have an equal chance for good health.\nWhere to from here?\nMore effective models of care\nSuboptimal health care in the community can contribute to conditions worsening to the point where hospitalisation is necessary. For example, if diabetes is not well managed, patients risk developing diabetic foot disease. In the most severe cases, this can lead to hospitalisation and amputation of the affected toes, foot or lower leg.\nA fundamental component of system changes to reduce these potentially preventable hospitalisations must be a shift to a better integrated primary care system, with a stronger role in coordinating care.1 Critically, health systems also need to better support patients with chronic disease to reduce the progression of conditions such as COPD, diabetes and heart failure, to minimise disability and improve patients’ quality of life.\nThe implementation of a Health Care Home model, currently being trialled by the Australian Government Department of Health, could greatly improve appropriateness and coordination of care for patients with multiple chronic and complex conditions.1The Health Care Home model supports integrated and coordinated team care, and targets the most intensive health services to those with the greatest needs.2 The model allows better sharing of information between patients and members of the healthcare team using My Health Record. Evidence‑based planning tools created for Health Care Homes further support high-quality care. Trials of similar models in the United States have shown reductions in hospitalisations, as well as reduced costs.3\nIn the management of specific conditions, we can learn from examples of things going right –the Atlas contains many such positive stories, where clinical teams are leading the delivery of best achievable care. For example, an Australian multidisciplinary, integrated primary and secondary care diabetes service in Brisbane has approximately halved the rate of hospitalisations due to diabetes complications.4 The success of this model is particularly encouraging given that the patients had complex type 2 diabetes and were from socioeconomically disadvantaged areas.\nMaking health care truly accessible for Aboriginal and Torres Strait Islander Australians\nMuch higher rates of the potentially preventable hospitalisations examined in the Atlas among Aboriginal and Torres Strait Islander Australians compared with other Australians point to poor access to appropriate care in the community, as well as a higher burden of the factors causing chronic diseases, such as social disadvantage, smoking and obesity. Conversely, for some procedures, such as cataract surgery, the substantially lower rates of treatment despite a higher prevalence of poor sight due to cataract indicate inadequate service delivery, which is not tailored to the population’s needs.\nFor Aboriginal and Torres Strait Islander Australians, availability of health services in urban and regional centres does not necessarily equate to accessibility.5Services need to be not only affordable and physically accessible, but also culturally safe. For Aboriginal and Torres Strait Islander Australians living in remote areas, physical distances compound the challenges to accessing culturally safe health care.\nThe National Safety and Quality Health Service (NSQHS) Standards, developed by the Commission, provide a nationally consistent statement about the level of care consumers can expect from health service organisations. Accreditation to the NSQHS Standards is mandatory for all hospitals and day procedure facilities. The NSQHS Standards (second edition) include a number of actions that focus specifically on providing care for Aboriginal and Torres Strait Islander Australians. These include strategies to improve the cultural competency and cultural awareness of the health workforce to meet the needs of Aboriginal and Torres Strait Islander patients, and health services working in partnership with Aboriginal and Torres Strait Islander Australians from local communities to meet their healthcare needs.\nAboriginal and Torres Strait Islander staff are key to engaging with Aboriginal and Torres Strait Islander patients, and a sufficient number of trained Aboriginal and Torres Strait Islander health workers is essential for service success. Several innovative models of care have reduced hospitalisations and improved health outcomes among Aboriginal and Torres Strait Islander Australians. For example, a model of out-of-hospital health care has produced encouraging reductions in hospitalisations among Aboriginal and Torres Strait Islander children in both urban and remote areas of Western Australia. The program is based on nurse‑led coordination of care, and partnerships with Aboriginal Community Controlled Health Services, general practitioners, allied health professionals, specialist doctors and other community health workers.6 Significant decreases in hospitalisations and emergency department presentations, and improved attendance at out-of-hospital appointments were seen during the four-year evaluation of the program. Outreach services are also showing promise. For example, a home-based outreach case management program that provides holistic, multidisciplinary care for Aboriginal and Torres Strait Islander Australians with diabetes has achieved significant improvements in blood pressure and diabetes control.7\nPreventing chronic conditions\nSmoking is a contributor to many of the chronic conditions examined in the Atlas. The smoking rate among Aboriginal and Torres Strait Islander Australians was 41% in 2014–15, more than twice the rate among non-Indigenous Australians.8 Addressing smoking, particularly among Aboriginal and Torres Strait Islander Australians, people at socioeconomic disadvantage and people living in remote areas, could help reduce the disparity in potentially preventable hospitalisations seen in these groups and the hospitalisation rate for smoking‑related conditions overall.9 Supporting healthy eating and physical activity through prevention programs and supportive environments also has great potential to prevent obesity and reduce rates of many conditions highlighted in the Atlas, particularly type 2 diabetes and coronary heart disease. Reducing rates of obesity would also have a substantial impact on the prevalence of osteoarthritis of the knee and the demand for knee replacement.\nGreater use of health technology\nTechnology is part of the solution for improving access to health care in remote areas. Telehealth is being used effectively in some parts of Australia.10 However, this technology has the potential for much wider use to improve access to health care in regional and remote areas. For example, a trial of telephone support for people with heart failure in rural and regional areas has shown a 30% reduction in rates of death or hospitalisation compared with usual care.11\nPatients as active partners in their care\nSo much in health is about self-care. The huge potential for lifelong good health depends on individual understanding of the importance of good food, a healthy weight and regular exercise. People also need to be able – and motivated – to eat well and exercise. When illness occurs, it is the patients themselves who need to take their diabetes medications every day, quit smoking or do the exercises to manage their back pain. Supporting patients to be active and effective partners in their health care has the potential to greatly improve health outcomes.12,13\nAddressing health literacy is vital to ensure that patients understand health information and have the confidence to act on it. Improving health literacy and the quality of health information will also help patients evaluate treatment options through understanding their risks and likely benefits. This is particularly important for procedures with uncertain benefits and risks of long-lasting consequences.\nBetter use of data\nCollecting data through clinical quality registries on symptoms before treatment and clinical outcomes, including patient-reported outcomes, will fill gaps in knowledge where the evidence on benefits is unclear. This is particularly important for new surgical techniques and devices, and use of established procedures in new patient groups that are likely to have a major impact on patient outcomes or health system use.\nOne of the issues with health data collection in Australia and elsewhere is that information about the health care that patients receive is split across multiple collections, such as hospital statistics, Medicare Benefits Schedule and Pharmaceutical Benefits Scheme datasets. It is difficult to form an accurate picture of healthcare quality without tracking experiences across these data divides, but this has proved difficult. Linking data from different sources can let us drill down more deeply into the patterns of healthcare use, and gives a more detailed picture of the investigation and treatment of health problems. For example, linked data could show whether someone who has a heart attack in a regional area of Australia has the same chance of having the recommended investigations and treatment as someone in the city. The data could also show whether, following a heart attack, people have equal access to good secondary preventive care, regardless of where they live. Better access to linked data inthe future will allow this kind of detailed analysis on a national scale.\nData are also a tool for health services to examine their practice at a local level. The data in the Atlas allow comparison of rates of particular interventions in local areas, and should prompt reflection on the underlying reasons where large variation is found.\nThe patterns shown in the maps in this Atlas and the accompanying commentaries show that there are many opportunities for making meaningful changes in Australia’s delivery of health care. Our recommendations highlight that action is needed at all levels – from addressing the social determinants of health through to better data collection, system changes and providing the best supports for individual clinician–patient interactions."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:6699c2de-1e66-4354-8ad9-961f8ebb499a>"],"error":null}
{"question":"medical requirements sport pilot helicopter training compare different same","answer":"Sport Pilot certification requires only a valid state driver's license and the ability to affirm general good health, without using substances that impair judgment. In contrast, helicopter training requires at least a current third-class medical certificate obtained through examination by a qualified aviation medical examiner, which also provides the student pilot certificate. The helicopter requirements are more stringent as medical certification is mandatory.","context":["|TRAINING & INSTRUCTION:\nTraining In General:\nMost students are complete in 6 to 7 full days of flying.\nIMPORTANT: Don't be discouraged by 'training' or 'instruction'. We built our curriculum to be fun, meaningful & real. Sure, we have to jump through some hoops, but we're not here to rack up hours on our way to becoming commercial airline pilots... We believe there's more to being an instructor than watching the hour meter & cashing checks. We train you to fly the way we know you will... You can expect the training you receive, complimented by your common sense, to be very unique & valuable no matter how many hours you have or don't have.\nNote: Basic outlines of our curriculum for both Ultralight & Sport Pilot are below.\nDisclaimer to General Aviation Pilots: Please give this sport & these aircraft the respect they have earned. You will save yourself a considerable amount of frustration, alienation & deaf ears by showing up with a humble, ready-to-learn attitude. Metaphorically speaking, no biker is interested in how many hours you have in a car... And, if you keep yakking about it, you'll probably get beat up here too...\nINSTRUCTIONAL PRICING - Click Here\nULTRALIGHT (1-Seat) (FAR/AIM Part 103)\n- Take Intro-Flight & come to your own conclusions.\n- Take Instruction from FAA Certified Instructor.\n- Just take instruction and purchase aircraft later.\n- Purchase aircraft for benefit of training in your own aircraft & discounted instructional rate.\n- Ground School. 3-5 hours.\n- Aircraft Setup & Breakdown, Preflight, & Maintenance.\n- Theory of weight-shift flight.\n- Ultralight Regulations & Exemptions.\n- Airport Patterns & General Aviation Protocol.\n- Meteorology as it pertains to Recreational Pilots.\n- Aviation Chart & Map Understanding. Airspace.\n- Written Examinations.\n- Expect 6-12 total hours of flight instruction as normal.\n- Preflight, Ground Handling, Taxi & Airport Protocol.\n- Entry level flying! Accent, Descent, Turns & Altitude Control.\n- Familiarization with flying maneuvers.\n- Stalls, slow flight, fast flight, & wing/thrust control.\n- Crosswind operations. Landing introduction.\n- Power on & off landings. Touch & Goes.\n- Cross Country and Navigational Flight.\n- Preparation for Solo. Practice.\n- Pass general tests, Solo & receive Pilot Status. Enjoy the lifestyle!\nSPORT PILOT (2-Seat) (FAR/AIM Part 61-J)\nBasically, the requirements for a Sport Pilot license are 15 hours of instruction & 5 hours of supervised solo. Take a written test and a final check ride. A detailed description of the requirements can be found in the FAR/AIM.\nWHAT THE HELL IS \"SPORT PILOT?\"\nRecently the Federal Aviation Administration created a new category of pilot called the Sport Pilot. This initiative has dramatically reduced the cost of learning to fly and opened up 100's of new options for aircraft.\nALRIGHT, HOW MUCH TIME IS NECESSARY?\nSport Pilot certificates require as little as 20 hours of instruction from an FAA certified flight instructor.\nSO, WHAT CAN I DO WITH A SPORT PILOT CERTIFICATE?\nThe holder of a sport pilot license may fly a light sport aircraft (a 2 place aircraft weighing less than 1320 lbs) unlimited distances within the United States , in good weather, during daylight hours.\nANY OTHER DETAILS?\nAlong with the new pilot rules, the FAA created a new category of aircraft called the Light Sport Aircraft (LSA). These new airplanes offer brand new aircraft with modern technology, similar capabilities to GA & low operating costs.\nIn addition to new sport pilot candidates, this new Sport Pilot License is available to current and former pilots who have chosen to NOT renew their medical certificates.\nWELL, HOW MUCH IS THIS GOING TO COST ME?\nThe FAA requires 15 hours of flight training and 5 hours of supervised solo prior to taking a Practical Exam (your final exam). We understand that each individual learns differently and at a different pace, however flying consistently (two or three times per week)(or traveling to to one of our locations for a one week concentrated course) and diligently studying your written material(available on DVD), should keep your costs to a minimum.\nThe cost of a full Sport Pilot course for a student who progresses through the course is approximately $3,300 - $4,000. This includes both the cost of aircraft rental and hiring your instructor. This is tremendous savings when compared to the typical $8,000 - $12,000 required for a private pilot certificate at most flight schools.\nWHAT ARE THE BASIC \"QUAlIFICATIONS\"?\nThe fundamental qualifications for becoming a sport pilot are quite simple. Remember, the main idea behind the sport pilot movement is to open up the exciting world of recreational aviation to many more enthusiasts. We love aviation and want to share that joy with you and many others!\nAccordingly, the qualifications are modest:\n• At least 17 years of age\n• Valid state driver's license\n• Proficient in the English language\n• Be able to affirm general good health and not using substances or medications that impede judgment, cognition, or motor skills\nWHAT DO I NEED TO START MY FLIGHT TRAINING?\nA desire to learn and a driver’s license.\nFor the most part, you can show up for your first flight lesson with nothing but a sense of adventure. The aircraft and fuel are included in your price of instruction, so all you need are yourself, your learning skills, and your desire for FUN!\nJust bring your valid state driver's license and get started!\nWHERE CAN I FLY?\nOnce you've achieved your sport pilot certificate, you'll likely find yourself exploiting every possible opportunity to get out and FLY!\nYou might plan an ambitious coast-to-coast trip, hopping your way across the country through clear-weather routes. Maybe you'll fly away with a friend for a weekend getaway or spend a few hours practicing at the local airport. Or maybe you’ll take in scenic vistas from above. Whatever the plan -- however simple or grandiose -- it will center on enjoying the world from a different perspective, and feeling the exhilaration, freedom, and satisfaction of flying an aircraft yourself.\nIS FLYING SAFE?\nRecreational flight is statistically among the safest outdoor motor sports. Although every activity - even walking outside to collect your morning newspaper - carries a degree of inherent risk, flying for fun falls well within the safety margins that most people expect for recreational activity.\nAnd flying as a sport pilot eliminates several risk factors, making it potentially even safer than other general-aviation flying. Because the sport pilot's flight activities by definition entail daylight, favorable weather, good visibility, and light aircraft capable of low-speed flight, the sport pilot can focus on FUN instead of worry.\nHOW LONG WILL THIS TAKE?\nThe minimum number of flight training hours for achieving a sport pilot certificate to fly an airplane is 20. Those training hours will include dual instruction (instructor and student), cross-country flying (departing one airport and landing at another), and solo flights. The number of days or weeks required to log those 20 or more hours of instruction is really up to you and your instructor. Will you set aside several days for immersive flight instruction or will you spread out the flight lessons over a period of weeks?\nWHAT ARE MY PRIVILEGES & LIMITATIONS?\nWhen operating as a sport pilot, you as the pilot must operate within the following guidelines of the sport pilot certificate:\n• Operate as pilot in command of a sport pilot eligible aircraft\n• Carry a passenger and share expenses (fuel, oil, airport expenses, and aircraft rental);\n• Fly during the daytime using visual flight rules (VFR). Three statute miles visibility and visual contact with the ground are required.\n• Cross-country flying anywhere in the U.S.\n• Fly up to 10,000 feet above mean sea level (MSL).\n• Fly solo or with one passenger.\n• Share operating expenses with another person.\n• Fly in Class E and G airspace (and B, C, and D airspace with appropriate training).\n• Allows sport pilots to fly production (standard airworthiness certificate) and experimental amateur-built aircraft that meet the definition of a light-sport aircraft.\n• Allows rental of special light-sport aircraft (S-LSA).\n• Prohibited from Class A airspace.\n• Prohibited from flying in Class B, C, or D airspace until you receive training and a logbook endorsement from an instructor.\n• No flights outside the U.S. without prior permission from the foreign aviation authority.\n• May not tow any object.\n• No flights while carrying a passenger or property for compensation, hire, or\n• Prohibited from flying in furtherance of a business.\nCan my Sport Pilot Training count towards more advanced flight training?\nYes. Although many Sport Pilots will remain recreational pilots only others may choose to go on to more advanced training such as Private Pilot, Instrument Rated Pilot, Commercial Pilot, Certified Flight Instructor, and ATP. Your Sport Pilot training is an excellent foundation for this advanced training and is often far cheaper than initial training at a more advanced school.\nThis content is courtesty of Sportpilot.com and SportPlanesFlorida.com. This is one of the better interpretations we have seen. More (specific) information can be found in FAR/AIM. We recommend looking at this information.\n© 2020 TrikeSchool, All rights reserved.\nUtilizing WhYDevelop Web Site Technologies","NEW! - GI-Bill TrainingFind VA-Approved Schools\nFind Aviation Schools Flight SchoolsAircraft Maintenance TrainingHelicopter SchoolsFlight Dispatcher CoursesAir Traffic Controller SchoolsAviation Management DegreesAvionics Technician TrainingCertified Flight Instructor TrainingFlight Instructor TrainingInternational Aviation SchoolsInstrument Rating CoursesMulti Engine TrainingSeaplane Rating CoursesSport Pilot SchoolsTime Building SchoolsTurbine & Jet Transition CoursesType Rating CoursesUnmanned Aircraft Systems\nHelicopter Flight TrainingFive Things to Consider When Looking at Helicopter Flight Schools\nBy Kyle Garrett\nLearning to fly helicopters is an exciting and fulfilling experience, but decisions made early in your flight training have a noticeable effect on your enjoyment of flight training. In order to have the best possible experience learning to fly helicopters, consider these five things: eligibility requirements, training cost, training aircraft and their availability, instructor availability, and flight school stability.\nAre you eligible?\nThe very first thing you need to know is whether you are eligible for at least a student pilot certificate and what it will take to get your private pilot certificate. Thankfully, the FAA has strict requirements that must be met and they don't keep them a secret. It is important that you understand any possible limitations you may experience early in the process. In general, you must be at least 16 years old, be able to read, speak and understand English, and hold at least a current third-class medical certificate. A medical examination at a qualified aviation medical examiner will provide you with both a medical certificate and student pilot certificate. While a student pilot certificate is not required for training with an instructor, you must have one for solo training flights and it is recommended you see the medical examiner as early in your training as possible in case any complications should arise.\nMore so than fixed wing flight training, the complexities of helicopters make flight training a potentially costly endeavour. It is important that you look at the costs and take stock of how you plan to finance your flight training. One important note, the FAA may requires only a minimum of forty flight hours, but you will, like most students, probably require more than 50 hours. Ask your school for a detailed cost estimate, including any additional costs such as insurance, ground school, etc. You want to make sure your training isn't interrupted by financial issues.\nTalking of training costs, perhaps the easiest way to save money is consider the training aircraft available and choose something less expensive. For example, a small two-place helicopter, like a Robinson R-22 will typically rent for less than a larger more-powerful helicopter, like a Bell Jet Ranger. Even though you may not plan to fly this type of helicopter after training, learning in a smaller, lighter helicopter will save money and be simpler. You can always get transition training later.\nAircraft and Instructor Availability\nOnce a budget is nailed down and you've got an idea of what helicopters are available, you should consider the scheduling availability of both instructors and helicopters. Whether a particular flight school can accommodate your schedule is key to how well your training goes. If you can only fly on the weekends, but instructors, helicopters, or both are only available mid-day during the week, you will probably have trouble sticking to a schedule.\nFlight School Stability\nThis may be a sign of the times, but flight schools are like any business and sometimes they have to close. While this is a pretty rare occurrence, you'll want to be aware of any potential issues before starting your training. You don't want to have your training interrupted by having to find a new school. Infinitely more likely is that your instructor may leave. Before you get too invested, see if there are several instructors on staff and how this transition might be handled. An instructor leaving is not the end of the world, but it can be a pretty big speed bump.\nHelicopter flight training is one of the most exciting and fulfilling experiences you can have, but it is important you build your training on a solid foundation. Doing your research ahead of time and ensuring that you are eligible, able to afford it and choose a stable, well-staffed, and well equipped school will provide you the best possible helicopter flight training experience.\nMore Helicopter Pilot Career And Training Info\nHelicopter Pilot Articles\n- Five Things To Consider Before Starting Helicopter Lessons\n- Helicopter Pilot Careers\n- Helicopter Pilot Training\n- Helicopter Pilot Salary\n- Helicopter Pilot Licenses and Ratings\n- Helicopter Pilot Jobs\n- Get the Most From Your Helicopter Flight School Experience\n- Helicopter Schools - Three Ways to Maximize Your Training\n- Helicopter Schools - Five Things to Consider\nGI-Bill and Veterans Benefits for Helicopter Pilot Training\nHelicopter Financial Aid & Scholarship Links\n- HelicopterScholarships.com - Helicopter training for UK pilots\n- AOPA Online - AOPA Air Safety Foundation Scholarships\n- EEA Young Eagles - Scholarships and Internships\n- The Ninety-Nines, Inc. - Chapter & Section Scholarships\n- Landings: Scholarships\n- Helicopter Scholarships - Whirly Girls\n- Vertical Flight Foundation - Scholarships\n- Northeast Helicopters - Helicopter Training Scholarships\n- Helicopter Foundation International - Scholarships\n- Griffin Helicopters (UK) - Dennis Kenyon Jr Scholarship\n- Aero Club of New England - Scholarships\n- University Aviation Association - UAA Scholarships"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:bee5ab90-382c-4eac-93d2-8808baf0f138>","<urn:uuid:11c9e226-cc44-484d-82c4-d1045db90f70>"],"error":null}
{"question":"What structural adaptations did the Tewa people at Tsankawi and the Maya at Yaxchilan make to their challenging environments?","answer":"At Tsankawi, the Tewa people carved footpaths and stairways into the soft volcanic rock (tuff) for easier mesa-top access. They built cave dwellings (cavates) into the rock face with stone buildings in front for temperature regulation, adapting to the area's minimal rainfall of 15 inches per year. At Yaxchilan, the Maya adapted to their terrain by constructing more than 120 structures across three complexes (Great Plaza, Grand Acropolis, and Small Acropolis), skillfully integrating them into the limestone hills using terraces, stairways, and platforms. They even built a bridge to handle the changing water levels of the Usumacinta River that would otherwise paralyze the settlement.","context":["After our visit to Bandelier National Monument, we drove about 12 miles (19km) to Tsankawi (sank-ah-WEE) – a Tewa word meaning “village between two canyons at the clump of sharp, round cacti”. The Tsankawi Village Trail is but a small portion of the protected lands within Bandelier.In addition to being a part of the National Monument, Tsankawi is also an archaeological site that is culturally significant to the people of San Ildefonso Pueblo, who are descendants of the Ancestral Tewa people who once inhabited Tsankawi several thousand years ago.\nWhen you enter the park, don’t forget to pick up a trail guide!Like the Frey Trail at the main park, Tsankawi Village Trail is self-guided and has various numbered markers along the way that tell you more about what you’re looking at. The loop is 1.5-miles (2.4km) in length.\nA large portion of the Tsankawi trail takes hikers through various footpaths and stairways that were cut into the tuff (soft volcanic rock) by the Tewa. These footpaths provided the Tewa with safer and easier access to the mesa-top. As you walk along the routes, you can’t help but imagine what daily life would have been like.Imagine walking on these in the rain or during the winter!Once you reach the mesa-top there is a spectacular view of the Sangre de Cristo Mountains, the Jemez Mountains and the Río Grande Valley. It’s really a sight to see! I spent a lot of time just taking in the landscape.The village portion of the trail had about 275 ground-floor rooms, many of which were only one to two storeys high. These rooms were used for everything including sleeping, cooking and storing crops and other supplies.\nIt is believed that the Tewa made Tsankawi their home sometime during the 1400s, where they built their houses and other structures using volcanic rock and adobe. Like the people living at the Frijoles Canyon, the people of Tsankawi took advantage of building cavates (cave dwellings) into the rock face. Many of the caves had stone buildings built out front, which helped to keep the dwellings warm in winter and cool in summer.Because the area receives only about 15 inches of rain per year, Tsankawi experiences periods of prolonged drought. Despite this, the Tewa found ways to thrive through foraging for native plants and cultivating beans, corn and squash.\nPlants, like the Tewa people, also adapted to the dry landscape. The types of plants that can be found along the trail are typical of piñon-juniper woodlands and include piñon, yucca, rabbitbrush, salt brush, juniper and mountain mahogany. The Tewa and other Ancient Pueblo people used these plants for food, medicine, dyes, spices and tools – many of which are still used by the Pueblo people of today.While the Tewa were able to live at Tsankawi for generations at some point around the late 16th century, the Tewa left. Archaeologists believe they relocated due to heavy drought and other factors, such as the soil becoming infertile due to years of farming and the depletion of resources. It is believed the Tewa had to venture out further and further to gather even the most basic of resources including fire wood.\nAs time went on, the buildings fell into ruin due to the elements – the roofs collapsed, the walls crumbled and washed away. As a result, artifacts such as pottery and tools were washed away and the rubble and sand covering everything. Plants eventually began to grow all over the disturbed ground, further obscuring what was once visible.\nToday you can find shards of pottery and other artifacts along the footpaths. In addition to encountering these small pieces of the past, you can also see many petroglyphs carved into the rock face.Much of Tsankawi and nearly 3,000 other archaeological sites at Bandelier remain unexcavated – only a handful have been. This is largely due to the cultural significance of the area to the San Ildefonso Pueblo, but thanks to modern technology much can be learned about the site without ever having to uncover it.For more information on Bandelier National Monument, check out my previous post!\n∆ ∆ ∆","Yaxchilan Ruins - Chiapas\nBy Car and Boat – Mayan Ruin Adventure\nOur journey to Yaxchilan Ruins had us heading down the Usumacinta River in Chiapas, Mexico. Located a fair distance from the Riviera Maya, the state of Chiapas is environmentally diverse with pasture lands, mountains, jungles, rivers, waterfalls and canyons. Culturally, Chiapas is rich with colonial cities, and has a long history of Mayan settlements.\nThe Usumacinta Province in Chiapas – Mexico\nYaxchilan is one of many Mayan archaeological sites located along the Usumacinta River, the borderline between Mexico and Guatemala. The area is referred to as the “Usumacinta Province” with settlements sharing architectural features that define the area. Cities fought for power throughout the centuries which makes the history intriguing and packed with interesting stories. Read more about this area in our review of the Bonampak Ruins that shows another influential Mayan settlement on the Usumacinta River.\nBrief History of Yaxchilan\nYaxchilan was a large urban center and the dominant power of the Usumacinta River during the Classic Era. The location of Yaxchilan is unique. It was built on the curve of the Usumacinta River which proved a natural moat for protection. Only the south side of the site is exposed to land.\nThe settlement existed from 350 A.D. to 850 A.D. with the height of its power ranging from 650 A.D. to 800 A.D. In 654 A.D., Yaxchilan found itself at war with Palenque in a fight for power and land. From 681 to 742, the city remained relatively small but grew to a regional capital that lasted into the early 9th Century. 740 AD was the building hey day when the famous lintels were carved and created. It is these architectural details that tell the story of Yaxchilan and enabled archaeologists to unravel the Mayan history in the area.\nThe first published documentation of the site seems to have been a brief mention by Juan Galindo in 1833,with exploration starting in 1882.Study and documentation has continued throughout the 20th and 21st Century by many national and international groups.\nThe ancient name for the city was Pa’ Chan meaning “cleft (or broken) sky” but the Mayan name, Yaxchilán, means “green stones.” Today, some Lacandon Maya still make pilgrimages to Yaxchilan to carry out rituals to the Maya gods.\nArchitectural Highlights of Yaxchilan\nThere are more than 120 structures in the central area of Yaxchilan that make up three complexes: the Great Plaza, located in the lower part parallel to the river; the Grand Acropolis; and the Small Acropolis. All of these areas are skillfully adapted to the contours of the low limestone hills and attach to each other with the use of terraces, stairways, and platforms. Highly decorated temples, pyramids, and luxurious palaces clustered along the grand plaza extend along the shores of the Usumacinta River.\nThe highlight of the buildings are the stelae, lintels, alters, stairs, bas-relief stucco carvings, and mural paintings. Almost every building has a doorway decorated with carved lintels that tell a story through some of the best preserved carvings in the Mayan world.\nNot mentioned in many tour books or overviews of Yaxchilan is the Bridge of Yaxchilan, which is now a large pile of stones. The changing water levels in the Usumacinta River would paralyze the settlement throughout the year. The kings of Yaxchilan built a bridge from the settlement to land so villagers could cross the river safely when the river water rose.\nWhat We Love About Yaxchila\nWe love the diverse and rich landscape of Chiapas. Yaxchilan is a great excuse for us to get out of dodge and experience a different Mexican landscape. We like to take the trip from Palenque or San Cristabol as these stops add to our adventure and knowledge of the Maya. The boat ride is a blessing and makes this Mayan ruin adventure a true adventure.\nGetting to Yaxchilan\nTravel by rental car or public bus to San Cristabol or Palenque. From there is it recommended to hire a tour company to get you to Yaxchilan or you may spend days trying to connect cars, with boats, and buses. Be ready for a 45 minute boat ride to the ruins. Many companies combine the day with a trip to Bonampak Ruins, which is not a bad idea."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:84832fdd-465c-4b0a-a8e7-fcb5a94f551b>","<urn:uuid:629d2536-d191-44f9-aec7-f2a2eb07e4e2>"],"error":null}
{"question":"What are the unique characteristics of basking sharks' feeding mechanism and how much water can they filter per hour?","answer":"Basking sharks are filter feeders that use gill rakers to catch plankton from the water. They can filter up to 1.5 million litres of water per hour. As the second largest shark species in the world, growing over 10m in length, they often feed at the surface where plankton is abundant, with their nose, dorsal fin, and tail fin visible above water.","context":["Our range of tours offer you the opportunity to see a range of wildlife including whales, porpoise, dolphins, basking sharks, otter, golden and white tailed sea eagles and much more in our European Special Area of Conservation - Firth of Lorn. However, with wildlife there can be no guarantees, please call us on 01852 300003 or email us at firstname.lastname@example.org to ask us what we've been seeing recently - we will always give you an honest answer !\nOur routes throughout the Firth of Lorn pass through some incredibly biodiverse ecosystems, on land and at sea, supporting a great range of animals from the smallest of fish to the largest land mammal and bird of prey the UK has to offer. Our year round residents are spectacular enough but we also have some incredible seasonal visitors too. This page showcases some of the wildlife we regularly get to see on our trips.\nYear Round Wildlife - Residents\nThe White Tailed Sea Eagle\nThis is the UK's largest raptor and are often thought of as the fourth largest eagles in the world, their wingspan can be up to 2.5m, they are so large they are commonly referred to as ‘flying barn doors’. Less aerially agile than the golden eagle they soar on flat wings but they are still formidable predators, taking mammals, birds and fish as prey, they are opportunistic feeders and will even feed on carrion. They develop their white tail when they reach adulthood at around 5 years of age and can live up to 25 years.\nIt is a joy to see them back in the UK after human persecution lead to their local extinction in the UK in the early 1900s. A reintroduction effort started on the Isle of Rum in 1975 and now 40 years on they seem to be flourishing and spreading down the islands and on to the mainland. The gaelic name for the white tailed sea eagle is \"iolaire sùil na grѐine\" meaning ‘eagle of the sunlit eye’, for its beautiful golden eye.\nThe Golden Eagle\nThe second largest raptor in the UK with a wing span of around 2m, although not the largest of our raptors they do have an air of majesty and have long been symbolic of power and strength throughout their range. These large birds are expert hunters and feed on small mammals, birds and sometimes carrion. Their keen eyesight helping them hone in on prey from great distance, they have awe inspiring agility in flight and during courtship displays perform undulating flight, flying upwards then swooping at great speed.\nThey will have up to 5 eyries in their extremely large territories which can be up to 150Km², nesting on craggy cliffs and in tall trees, they will add nesting material each year they reuse a nest, which can make nesting structures incredibly large! Tending to lay 2-3 eggs per clutch it is usually only one chick that fledges, although in some rare cases twins and even triplets have been reared to fledging.\nThe Buzzard is the most numerous bird of prey in the UK, it is a medium sized raptor with a large range of colour morphs, from very pale to almost black, they have a paler patch on the underside of the wings that can help identify them during flight.\nThe Common Seal\nThe common seal is smaller than the grey seal, the larger males being up to 1.9m, they have a very dog like profile with their large eyes and pronounced muzzle. Named common as they have a wide geographic range they are less numerous than the grey seal in the UK. They pup in early summer and the pups are able to swim hours after birth, but they do have a little encouragement from mum and if they get a little tired mum's there for a piggyback! As adults they are able to dive for up to 30 minutes and have been found to go to a depth of 500m, however their normal dive profile tends to be less than 4 minutes at a depth of less than 20m to get their favourite prey of small fish like sand eels.\nThe Grey Seal\nThe grey seal is the larger of the two species of seal that breed in the UK, the bulls (males) can reach over 2m in length with the females being slightly smaller at around 1.8m. They are superbly adapted to life at sea, being able to hold their breath for over 1 hour and like all true seals don’t have external ear structures and have small forelimbs. They can only shuffle about on land but have fantastic agility underwater. They spend most of their lives at sea but we often see them hauled out resting and regulating their temperature, because although their blubber helps them stay warm underwater sometimes they need to warm up on land.\nThe Harbour Porpoise\nThe harbour porpoise is the smallest cetacean we see in British waters, they are a coastal species never ranging further than 10km from land. Growing up to 1.9m, they have a small triangular dorsal fin in the middle of their back. Locally they are also known as the puffing pig as when they surface to breath they make a sneezing like sound. Like all the toothed cetaceans they use echolocation to hunt their prey and will often feed collaboratively, numerous individuals rounding up a shoal of fish into a tight ball and forcing them close to the surface, the porpoise then take it in turn to pick fish off from the shoal.\nThe Bottlenose Dolphin\nThe bottlenose dolphin is a large dolphin reaching sizes of up to 3.8m and can weigh up to 650kg, with a dark grey dorsal surface, paler underside and short beak they are an instantly recognisable species. They are a surface active species, breaching and spy hopping and performing great aerial acrobatics, they can be very inquisitive and will often come and ride the bow waves next to boats! Usually swimming at speeds of around 6mph they can reach up to 30 mph for short periods of time. Although resident in our waters all year they have an extremely large home range, from the southern tip of Islay all the way up to Skye. Please scroll to the bottom of this page to see some video footage, taken in early 2016 by crew member James Gilpin, of dolphin encounters on our Corryvreckan Wildlife Tours.\nShags and Cormorants\nThese very closely related seabirds can be quite hard to tell apart, unless side by side as the cormorant is about a third larger than the shag. Shags tend to be a jet black in colour whereas the cormorants are more brown in colour and have a larger creamy patch on their throat and chest. During the breeding season the shags have a pronounced crest which is why they are so named. For seabirds they have relatively little waterproofing oil in their feathers so they get waterlogged wings quite quickly making them less efficient fliers and swimmers so they need to haul out to dry off, they are often seen with their wings out; increasing the surface area for speeding up the drying process.\nThe red deer is the largest land mammal in the UK, a stag can stand at around 137cm at the shoulder and weigh up to 190kg! They breed during the rut in autumn where the males defend a territory and battle it out for breeding rights to a harem of hinds. They fight as a last resort, bellowing and parallel walking to infer dominance and it's only if the males are fairly evenly matched will they fight, using their antlers as weapons. They cast their antlers each year and grow new ones towards the end of spring, it is the fastest growing mammalian tissue and while growing is covered in a velvet like fur, filled with blood vessels supplying energy and nutrients for growth.\nThe fallow deer are not a native species to the UK and were introduced in the 11th century, the fallow deer on Scarba being introduced as a game species much later. Standing at just under a metre at shoulder height the males, called bucks will weigh about 100kg less than the red deer stags. They have palmate antlers rather than the branched antlers of the red deer and primarily feed on grass.\nThe European otter, growing up to 1.1m in length, its tail makes up a third of its total length helping propel them through the water. When they live in fresh water habitats European otters display what is known as crepuscular activity, being most active around dawn and dusk, however when they live in a coastal habitat their activity fluctuates with the tide so we often see them in the middle of the day. Hunting for their favourite quarry of small fish, but they will also eat crustaceans and frogs. Unlike the other marine mammals in the area otters lack a layer of blubber so have 2 layers of fur to provide insulation whilst foraging in the water.\nSeasonal Wildlife - Visitors\nThe Minke Whale\nReaching our area towards the end of June each year they spend up to 8 weeks feeding in the food rich water around the Garvellach Islands and Gulf of Corryvreckan.\nThe minke whale is a baleen whale, a group of whales that, rather than having teeth, have large baleen plates to filter fish like herring and sand eels from the water. The Baleen plates are made up of keratin bristles that trap fish as they expel the large quantities of water they have just taken in. Minke whales are the smallest rorquals, a group within the baleen whales that have large groves on the underside of their throats that allow expansion when taking in gulps of water when feeding. We only see the minke whales in the summer; usually July and August, during the winter they are further south, breeding in warmer waters. They can reach up to 10m in length and weigh in at around 8 tons.\nWe have dedicated Whale & Wildlife Watching Tours once we have been regularly seeing whales in our area. Please contact us on 01852 300003 to ask whether we have been seeing whales, we will always give you an honest answer.\nThe Basking Shark\nBasking sharks also pass through our area during the summer. They are second largest shark in the world and can grow over 10m in length. Similar to the whale shark which is the largest shark in the world the basking shark is also a filter feeder, using gill rakers that catch plankton out of the water it takes in, they can filter up to 1.5 million litres of water per hour! They often feed at the surface where plankton is abundant at certain times of the day and often their nose, tip of their dorsal fin and caudal (tail) fin can be seen above the water.\nComing to the coast to breed during the spring and summer we see lots of these seabirds down in the gulf of Corryvreckan which is a great feeding site before they head back out to sea towards the end of August where they spend the majority of the year.\nThe kittiwakes are the UKs most delicate looking gull, as adults they have black wing tips and black legs and feet, as juveniles (like the one pictured) they have a dark patch behind the eye and a black band on the leading edge of their wing.\nThey have a diet of small fish, primarily sand eels and their breeding success fluctuates in relation to sand eel abundance.\nThe common guillemot is a member of the auk family and relative to the black guillemot that is resident all year. They are slender seabirds, with a white belly and black back, and use their wings to propel them through the water to catch fish. They come to the coast during the spring to nest on ledges on cliff faces, they defend a small nesting territory and will literally be touching their nearest neighbour; to prevent them from falling off the cliff-face their eggs are cone shaped.\nThe razorbill is another auk, named because its beak has the same shape as an old fashioned cut-throat razor, which also comes to the coast to breed. Although they nest further up on the cliff-faces than the common guillemot so they don’t need the pyriform egg shape.\nThe gannet is a large seabird with a wingspan of 1.8m, as an adult they are predominantly white with black wing tips, a creamy face and neck and icy blue eyes. In flight they look pterodactyl like with a bend in the wing at their carpal (wrist) joint. They soar high above the water often at about 30m high looking for fish, on spotting a fish they plummet like arrows at speeds reaching up to 60mph into the water after their prey, they can dive down to 20m depth after a fish and stay underwater for 30 seconds. They often feed in association with cetaceans as they bring shoals of fish close to the surface of the water. The gannets we see in our area breed down on Ailsa Craig in the firth of Clyde and travel up to feed in the rich waters in the gulf of the Corryvreckan.\n*Page content - J. Finnigan."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"advanced_reasoning"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:b4597882-ae6b-4257-9a28-0bf67fef2fda>"],"error":null}
{"question":"What challenges did both Amy Rowat and Marie Skłodowska-Curie face in pursuing their scientific education?","answer":"Both scientists faced different educational challenges. Rowat initially planned to major in biology but switched to physics after an engaging physics class, eventually earning a degree in physics with multiple minors. Marie Curie faced more severe obstacles - she couldn't access higher education in Poland as a woman and had to work as a teacher to help fund her sister's education before finally studying at the Sorbonne in Paris, where she had to overcome the additional challenge of learning French to complete her studies.","context":["Some physicists spend their lives obsessed with questions about the possibility of parallel universes, or of travel at the speed of light. Amy Rowat is obsessed with the mechanical properties of the tiny cell nucleus, and how changes in its shape affect the cell’s physiological function. She also is applying her training as a soft-matter physicist to the question of how to make the perfect pie crust.\nRowat is a postdoctoral fellow in Harvard’s Department of Physics and School of Engineering and Applied Sciences (SEAS), and her principal research uses microfluidic tools, in which liquids are forced through channels much smaller than a human hair, to look at single cells, particularly the cells’ nuclei. She is using these tools to explore the largely neglected mechanical aspects of cell biology.\nBut food, in all its forms, also peaks her scientific interest. What at a molecular level, she wonders, makes a truly flaky pie crust flaky?\nFew people see the world, or cooking, the way Rowat does. “Food is essentially cells and nuclei,” she says. But over the past four years as a postdoc at Harvard, Rowat has been using the principles of physics to advance our basic understanding of how the structure of a cell affects its function – and at the same time she’s been using the physics of food and cooking to get people excited about science. (Rowat, in fact, is one of the developers of a new Gen Ed course that will teach soft-matter physics with, among other things, cooking demonstrations by world-renowned chefs. During the 2010-11 academic year she will be one of the people leading the course.)\nThere are about 2,000 postdoctoral fellows in the labs at Harvard Medical School, the School of Public Health, the Faculty of Arts and Sciences, and the School of Engineering and Applied Sciences, and there are thousands of others in the laboratories of Harvard’s affiliated hospitals. This is the first in a series of articles about a few of those young postdocs on the verge of launching their independent careers.\nRowat’s path to a Harvard physics lab began in Guelph, Ontario, where as a child she was fascinated with biology. “I never had chemistry kits or my own microscope, but I really loved to build things and I was really curious about nature and the world around me,” Rowat says.\nWhen Rowat began her undergraduate education at Mount Allison University in New Brunswick, she was drawn to questions in biophysics, the study of biological molecules and the larger structures they assemble into, even before she knew what biophysics was. She had planned to major in biology until a physics class changed her mind. “My [physics] professor was fantastic and developed this course and style of teaching where all the students were engaged doing experiments and solving problems, and I thought that was challenging and exciting,” she recalls.\nWorking in a research lab during her summers also had a huge impact on her, she says. “Unless I had the opportunity to work in a science lab as an undergraduate, I would have assumed from science classes that science was not a very creative subject,” she says. But during one impactful summer in the research lab, Rowat created and studied thin layers of polymer films. “I developed a fascination with membranes,” Rowat says. She went on to earn an undergraduate degree in physics – with minors in French, Asian studies, and mathematics.\nHer graduate work, conducted in the laboratory of Ole Mouritsen, at the Technical University of Denmark and University of Southern Denmark, took her into the world of human biology. She worked on biomembrane physics, conducting experiments in which she deformed the nucleus and nuclear envelope of live cells. This research helped reveal the mechanical properties of the nuclear envelope, and also inspired Rowat to think more about what was going on inside of the nucleus, within the context of the whole cell.\n“I decided that the nuclear envelope and nuclear membranes would be a great [doctoral] topic because there’s very little that’s understood about the physical properties of the interface between the cytoplasm of a cell and the cell nucleus, which is where the important, essential-to-life processes happen,” says Rowat. After completing her Ph.D. at the University of Southern Denmark, Rowat joined David Weitz’s lab in Harvard’s School of Engineering and Applied Sciences and Department of Physics because she was interested in working on problems regarding cell mechanics and in developing new microfluidic technologies. She currently uses microfluidic tools for single-cell studies, and recently developed a microfluidic tool to trap single yeast cells, which are widely used in biology as a model system.\nThus far, Harvard’s Office of Technology Development has filed three patent applications on behalf of Rowat and co-inventors for microfluidic-related work.\nIn 2007 Rowat proposed a new topic for the Harvard Holiday Lecture Series: pizza.\n“I said, ‘Have you ever done a topic on food? I think that would be really popular because everyone loves food,’” she remembers. The lecture title was “The Science of Pizza,” and the talk was a hit. In 2008, Rowat and her colleagues collaborated with a local chocolate company that provided taste samples for that year’s lecture. “Each person was doing their own science experiment using their senses. We tried to get across what it’s like to be a scientist and what a scientist does,” she says. “You make observations, you come up with creative ideas to solve problems or interpret your results.”\nFor Rowat, learning the answers to basic questions about the physics of food and cooking have helped her to understand more about the underlying science she’s researching, and she feels strongly about spreading the word to others that science is an exciting, creative, and dynamic field. “I think that when there’s scientific illiteracy, that can lead to many social problems,” says Rowat.\nThe postdoc has also used food for topics in the weekly Squishy Physics Lectures, a series of informal research presentations devoted to soft-matter physics. “It really is a good community-building event, where people from different labs come together and interact,” says Rowat. Lecturers and listeners of the squishy physics series come from many area universities and even other countries, some are from as close as the Massachusetts Institute of Technology and others from as far away as Germany, Israel, and Korea. “I think that’s really important in a university, to have these intellectually stimulating discussions and conversations,” says Rowat.\nOne of the most memorable lectures for her was a seminar during Oktoberfest. “I invited a brewer and a yeast biologist,” says Rowat. She developed the idea during a time when her research was working with yeast and engineering devices to study yeast lineages.\nConsidering her postdoc period, Rowat says that Harvard “has been a fantastic place. It’s been a great place to be able to discuss my research with really great colleagues, both within my group and the Harvard community,” she says. Her main philosophy, to take risks and pursue creative passions and ideas, has led Rowat not only to cutting-edge research in soft physics, but also to enjoy running, yoga, and of course cooking meals for friends in her limited spare time. “Sometimes I’ll be cooking with more of an artistic perspective,” she says. “There are other times I’m thinking much more about what’s going on at the molecular level and how I could make a flakier pie crust.” Her most recent creation was a gooseberry pie.\nSo is she ultimately more passionate about the soft-matter physics that underlie that flakier pie crust, or the joy of cooking? Rowat won’t say. But if she could have lunch with either Michael Faraday, the 19th century chemist and physicist whose photograph Albert Einstein kept in his study, or Julia Child, she replies:\n“I’ve heard Julia Child is quite a character.”","“I am among those who think that science has great beauty.”\nAccording to the analysis of senior university staff in Europe, men are three times more likely than women to reach senior levels. However, the participation of women in science and technology can and has contributed to increasing innovation, quality and competitiveness of scientific and industrial research and needs to be promoted.1 One of the brightest and most inspiring people, who pushed the borders of the existing science was Marie Skłodowska-Curie, the first woman to be awarded the Nobel prize and the only person who has ever won Nobel Prizes in both physics and chemistry. She discovered two new chemical elements – radium and polonium and carried out the first research into the treatment of tumours with radiation.2\nMarie Skłodowska-Curie was born in 1867 in Warsaw, Poland.3 Since within her childhood, Marie was recognised for her remarkable prodigious memory and she graduated in 1883 from high school with a gold medal as the top student. However, her father did not have the possibility to financially support her idea to go to university and moreover higher education was not available for girls in Poland. Therefore, she took work as a teacher and from her earnings, she helped her sister Bronia to complete her medical studies in Paris. In 1891, Marie followed her sister to Paris where she studied chemistry, mathematics and physics at the Sorbonne, Paris’s most prestigious university. One of the challenges she faced was, of course, learning French, since the courses were taught in French.2 Almost two years later, Marie finished as the top student in master’s physics degree course. Being extremely curious and willing to always study, in 1884 she also obtained a master’s degree in chemistry. Then she attempted to find a job in Poland, but again as being a woman she could not get a position at the university. Therefore, she decided to return to Paris and begin a PhD in physics.\n“Unknown in Paris, I was lost in the great city, but the feeling of living there alone, taking care of myself without any aid, did not at all depress me. If sometimes I felt lonesome, my usual state of mind was one of calm and great moral satisfaction.”\nIn 1895 (aged 28) she married Pierre Curie, who had been completed a PhD in physics and had become a professor. During her PhD, Marie began to investigate uranium as a new and exciting area to work in. Marie and Pierre discovered polonium, which was named after Marie’s homeland. In 1903, Marie completed her PhD and, in the same year, she was awarded the Nobel Prize in Physics. She shared the prize with Pierre Curie and Henri Becquerel. The births of Marie’s two daughters, Irene and Eve, in 1897 and 1904 did not interrupt her scientific work.\nAfter the death of her husband Pierre Curie in 1906, she had become the Chair of Physics at the Sorbonne and after so many years, she became the first female professor at the University of Paris.\n“I have frequently been questioned, especially by women, of how I could reconcile family life with a scientific career. Well, it has not been easy.”\nIn 1911, she was awarded the Nobel Prize for Chemistry for the discovery of the elements radium and polonium, the isolation of radium and the study its nature and compounds.\nDuring World War 1, Marie set up radiology medical units near battle lines to allow X-rays to be taken of wounded soldiers.\n“We must not forget that when radium was discovered no one knew that it would prove useful in hospitals. The work was one of pure science. And this is a proof that scientific work must not be considered from the point of view of the direct usefulness of it. It must be done for itself, for the beauty of science, and then there is always the chance that a scientific discovery may become like the radium a benefit for humanity.”\nShe travelled widely to talk about science. In addition, she had the satisfaction of seeing the development of the Curie Foundation in Paris and the inauguration in 1932 in Warsaw of the Radium Institute, where her sister Bronia became the director.4\nMarie died aged 66 of leukaemia, most likely developed from the radioactivity she had been exposed to during her career. Even her books and papers are so radioactive that they are now stored in lead boxes.\nToday, Marie Skłodowska-Curie actions (MSCA) support researchers at all stages of their careers, irrespective of nationality. The fellowship program – first named Marie Curie Actions – was created by the European Union/European Commission to support research and to reveal new pathways for many brilliant minds who are passionate about science.\n“After all, science is essentially international, and it is only through lack of the historical sense that national qualities have been attributed to it.”\nThis article appeared first on www.etn.redmud.org website."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:b9d2b9ae-7cd9-4adc-8213-1cd5db126638>","<urn:uuid:88fdd8a6-cc42-45e1-bce6-ecd36a861253>"],"error":null}
{"question":"What are the main benefits of sports variation for youth athletes, and what negative effects does early specialization have on them?","answer":"Sports variation provides several key benefits for youth athletes: it allows them to develop multiple physical qualities simultaneously, making them more robust and better equipped to handle unexpected physical demands. It also maintains high enjoyment levels and intrinsic motivation. In contrast, early specialization has significant negative effects: it leads to a higher injury risk due to repetitive movements and tissue strain, and can cause psychological issues like burnout, depression, and anxiety. Research shows that intense, year-round training in a single sport while excluding others is not essential for achieving elite performance levels, and can actually lead to increased rates of physical inactivity and dropout among children.","context":["Youth sport is often dominated by an adult lens. Founder of Changing the Game Project, John O’Sullivan shares his excellent article about his recent trip to Australia, his subsequent reflections and the importance of ensuring that sport serves the needs of young people above all else.\nIn late February I was traveling and speaking in Sydney and Perth, Australia, working for a variety of sports organizations including the Football (Soccer) Federation of Australia, Football New South Wales, the Western Australia Department of Sports and Recreation, and the WA Aussie Rules Football Committee. While on site, I got to visit the brand new $1.5 billion Optus Stadium in Perth. As I toured the stadium with Project Manager Ronnie Hurst, he kept speaking about the guiding principle they used to build this incredible venue. From the placement of every seat and cup holder to how much leg room it had, from the location of every food stand to access for people with disabilities, every single aspect of the four-year project had to check a single box:\nEverything they built or designed had to put the comfort and viewing experience of the fans above all else. After all, wasn’t it about them? Wasn’t it their money and willingness to spend it that paid the bills and allowed the stadium to exist in the first place? As a result, anything that did not benefit the fans did not become part of the project. This simple guiding principle of “Fans First” was essential to building an incredible fan experience in the stadium.\nIt got me thinking about youth sports all over the world, from grassroots clubs to schools to professional team academies across every sport. Shouldn’t they have a guiding principle as well? If our goal is to build an incredible child experience in sports, shouldn’t everyone ask themselves, whenever they make a decision:\nDoes this serve the needs of the children first?\nThe simple question should be asked by everyone in sports, over and over. It is the box that should be ticked whenever a decision is made, or a program is implemented. It is the question that should be asked by every coach as he or she designs a practice, or signs up for competition. Much like Optus Stadium asked “Does this benefit the fans?” and Google always ticks the box “Don’t be evil”, every youth sports decision should adhere to “This serves the needs of the children.”\nYet so often that does not happen. If we want children to keep coming back to sports it better fulfill their wants and needs. Yet over and over, decisions are made, programs are implemented, teams are formed, and competitions are structured to serve the needs of adult administrators, coaches and parents, but not the kids. Organizations rarely ask “Is this what best serves the kids?” This is maddening. This is destructive. This is why kids quit to the tune of 70% dropout before middle school.\nMy friend Dr. Richard Bailey, head of research at the International Council of Sport Science and Physical Education says it best: “Let’s be honest, most elite sports programmes are not designed to meet children’s needs; they are designed entirely for adult ambitions.”\nHere are 6 examples of common situations that do not serve the children playing sports, and potential solutions:\n1. Adult politics in youth sports:\nI am so tired of seeing great coaches not hired, kids not being selected, and youth sports organizations making decisions based on egos and keeping adults happy, to the detriment of the kids. I am tired of hearing small town sport leagues kicking out families who stepped up against an abusive coach or influential parent.\nSolution: We need good people to run for local sports boards. We need people who can see the big picture and beyond their child’s team. But perhaps, in the long run, I often wonder if the model of parent-run non-profit boards and sport organizations is broken and will ever be able to make proper decisions regarding the well-being of many children when so many decisions are made because of egos and the well-being of one child – their own. Perhaps it is time for a rethink (but that is for another article).\n2. Competition and match sizes that do not fit the needs of the child:\nAmerican football 3rd graders still playing 11v11 tackle football, and up until recently American 8-year-olds playing 11v11 soccer or full ice hockey, or as I just learned about Aussie Rules football children playing 18v18 at every age! The adult pushback to playing small-sided games in every sport has been immense, and completely misguided. The kids’ game is NOT supposed to look like the adult version, which is played with the most complicated rules, the most players, and the biggest spaces. We don’t have adult chairs in a 1st-grade classroom, we have child-size furniture. We don’t teach calculus to 2nd graders because “that is what math is going to be like later on, so we need to get you ready for it now.” We teach building blocks so they are prepared for it later on. If the child version looks like the adult version in team sports you are doing it wrong and NOT SERVING THE KIDS!\nSolution: After Belgium bombed out in the group stage of Euro 2000, they knew they needed a revamp. The result was a total rethink of how children’s soccer was done. In my upcoming podcast with Belgium FA Coaching Education Director Kris Van Der Haegen we speak about these changes, including what they call “Dribbling Soccer” for 5-7-year-olds. They play 2v2, one field player, and one goalkeeper, with short games and constantly changing opponents. It fits the needs of young children learning to dribble, not wanting or needing to pass, and scoring lots of goals. They play small-sided games until U14, as does Spain and many other countries we want to emulate in soccer. Belgium has gone from #66 to #1 in the world soccer rankings, Spain won a World Cup and 2x European Championships. In the US, we still have grumblings on every sideline about how 11-year-olds need a larger field to play correctly. Ahhhhh!\n3. Forcing children to choose a single sport far too young, and not teaching fundamental movement skills before layering sport-specific techniques, which leads to increased injury rates and burnout. Just this week more evidence was released about the detriments of single-sport specialization and too many hours of organized activity before age 12 by the American Academy of Orthopedic Surgeons. They don’t want our business! Among the findings:\n- 54.7% of parents encouraged their children to specialize in a single sport\n- 57.2% of parents hoped for their children to play in college or pro (only about 4% of high school athletes play in college, less than 1% go pro)\n- Children whose parents invest in private lessons and trainers have a higher injury risk than those children who do not.\n- Solution: Let young children sample multiple sports. Forcing a 9-year-old to play only soccer, basketball, baseball or any sport and making them promise to not play other sports DOES NOT SERVE THE BEST INTEREST OF THAT CHILD! Yet it happens all the time! Not providing fundamental movement training withinyour sport also does not serve the interest of the child. We must stop short-changing our children’s physical literacy development in order to win some meaningless competition or increase the bottom line of the business.\n4.Early “talent” identification despite the evidence that you are choosing the older kids, not the better ones.\nThe evidence of the Relative Age Effect is clear: when we select “talented” and “gifted” individuals at young ages, we are basically selecting the ones who won the calendar lottery and are born closer to the arbitrary calendar cutoff. The younger we make cuts and funnel kids out of the system, the more likely we are to get it wrong. Heck, the NFL wastes tens of millions of dollars a year trying to select 23-year-old talent, do you really think we can effectively select 9-year-olds? I have written about this here and here if you want to go deeper, but in a nutshell, making cuts and giving additional resources to select groups of effectively ‘older not better’ kids at young ages does not serve the needs of the children.\nSolution: Psychologist Johan Fallby says it best in this great interview with Mark O’Sullivan: “As many as possible, as long as possible, in the best environment possible.” Teach them all, devote resources to as many kids as possible, let them grow and see what happens after that. In Belgium, for example, if a doctor approves it a child can play DOWN a year with kids the same developmental age as him or her, allowing them to compete with similar ‘age” kids. Imagine that happening in Little League baseball?\n5. Joystick sideline coaching from parents and coaches that prevents learning. The amount of clubs that condone sideline coaching from parents and coaches, who are simply trying to ensure that their kids do not fail the sports test, is astonishing. So many talk a good game but when asked to confront it they say “the parents are the paying customer, they can do what they want.” That is not what a doctor says about a sick child even though the parent pays the bill. Neither should sports clubs.\nSolution: Clubs and schools should have a zero-tolerance policy for sideline coaching and abusive behavior toward referees. They need to stop being afraid some parent might take little Johnny and leave if they tell dad to be quiet. Good riddance. You will gain far more members by serving the needs of the kids and not Johnny’s dad. And along with that, slowly but surely the intelligence and problem solving will happen on the field and not on the sidelines. We need to Win the Race to the Right Finish Line.\n6. Coaches who create learning environments that are the least effective way to transfer knowledge:\nMany coaches still have kids stand in lines and mindlessly repeat the same technique over and over believe this is the most effective way to teach, or what we call blocked practice. It is the least effective way to teach if you want techniques to transfer to match situations that require assessment and decision making on the fly. Yet drive out to almost any field and I see coaches running sessions, and coaching directors ignoring those poorly run sessions, where there are no defenders, no decisions, no direction to the game, and thus no long-term learning. Lots of cones and lines look neat and tidy, but they do not serve the kids. Training ugly does.\nSolution: Randomize your practices. Use the whole-part-whole method where you play first and last, with smaller size activities in the middle of training. Play games, and add defenders as soon as possible if you are practicing to be better during the competition, instead of just at practice. (Read this book, Make it Stick by Peter Brown if you want all the direct research links.) Just stop hitting 50 x 7 irons in a row and calling it learning or saying it is the most effective way to make someone better. Mark Bennett’s, founder of PDS Coaching, has what he calls the Rule of 3, a three-step process to solving problems where (1) the player works it out (2) the player with another player works it out, and finally (3) coach and players work it out. Our job as a coach, as Mark O’Sullivan says, “is not to correct everything, it is to observe them solving the problems themselves.”\nThis topic is more a book than a blog so I will stop my rant here. But I will leave you with this.\nIf you are a coach or sports administrator, please stop making decisions without asking “Does this serve the needs of the children playing?”\nIf you are a parent signing your child up for an activity, ask the organization “How does competition format serve the needs of the child?”\nIf you are a member of an organization, when you go to the AGM and they tell you how money is being budgeted, or which coaches are assigned where, or what funds are being used for, ask “How does this serve the needs of the children playing?”\nPerth stadium put its fans first and built an incredible venue.\nYouth sports must start putting its kids first to build an incredible experience.\nAbout the Author:\nJohn O'Sullivan is the founder of Changing the Game Project after two decades as a player and coach at youth, high school and professional level. John has written a #1 best selling book titled, 'Changing the Game: The Parents Guide to Raising Happy, High Performing Athletes and Giving Youth Sports Back to our Kids'. He holds the USSF A license, NSCAA Advanced National Diploma and US Youth Soccer National Youth Coaching License. John has also been a speaker at TEDx.","The Benefits of Sports Variation\nThere is a common line of thought suggesting that to become great at something, you need to focus on that single thing and dedicate as much time to it as humanly possible. For the most part, this does hold some truth to it.\nTo become the best in the world at a given sport, you do need to practice that specific sport a hell of a lot.\nThis is quite obvious, and really, it makes a whole lot of sense.\nHowever, for us as humans, this specialization may also be to our determinant.\nIn fact, there is a reason to believe that actively participating in different sports may be much more beneficial for health. Additionally, it could benefit physical development and musculoskeletal capacity more than simply sticking to the single sport indefinitely.\nWhat is Sports Specialization?\nWe could simply define sports specialization as the intense, year-round training of a single sport, in conjunction with the exclusion of other sports (Malina, 2010).\nWhile variations on this general definition do certainly exist, they all have a very clear thing in common. They indicate that a person sacrifices the potential performance of other sports to focus all their energy and effort into one.\nThis may undoubtedly have merit in adult athletes who aim to reach the top level of their chosen sport. However, there is research to suggest that it may actually have some rather negative effects in certain populations.\nRelated Article: Developing & Maintaining Athleticism\nEarly Sports Specialization\nWhen researching sports specialization, the most common group discussed is children and youth athletes. This is often in conjunction with the term early sports specialization.\nAs previously discussed, there is a general agreement that the number of hours spent deliberately training for a particular sport will positively correlate with the level of achievement in that sport. It is a concept that stems from early research on musicians.\nThis particular research found that those musicians who had spent over 10,000 hours practicing their instrument of choice were almost always more successful. This is especially when compared to their peers who had accumulated 7000 hours or less.\nMoreover, this same research showed that those musicians who began their training at around 5 years of age were much more likely to become more successful than those who started later in life.\nEssentially, those who began after age 5 were unable to catch up.\nHowever, research in athletes has not shown the same association.\nIn fact, research has consistently demonstrated that early intense training is not essential for achieving elite levels of performance in all sports. Rather, for most sports, early diversification and sports variation is much more likely to lead to success (Jayanthi, 2013).\nAlternatively, in this scenario, early sports variation has some rather significant downfalls.\nWhat Are the Negatives Associated With Early Sports Specialization?\nYoung athletes who specialize early have been shown to be at a significantly greater risk of injury than those individuals who perform a variety of sports. This is irrespective of whether the total training volume between the two groups is the same (Jayanthi, 2015).\nWhile we cannot be one hundred percent sure as to the reason for this finding, it is likely two-fold.\nFirstly, those athletes who specialize in a single sport will undoubtedly find themselves repeating the same movements over and over again. This repetition can easily overload specific tissues of the body, leading to an increased risk of injury.\nConversely, those athletes who diversify do not get this same repetitive tissue strain.\nSecondly, those athletes who participate in a variety of different sports get the opportunity to develop several physical qualities simultaneously. As a result, they have the capacity to become a more robust individual, in which they can more effectively handle the physical demands placed upon them in unexpected sporting scenarios.\nAlternatively, while those who specialize may become extremely competent at the specific skills involved in their chosen sport, their physical capacity remains very narrow. In other words, they can only handle the physical demands placed on them within a specific sporting context.\nAs a result, they are less equipped to handle the unexpected physical demands that may arise in open competition, thus increasing their risk of acute injury.\nIn conjunction with the physical impact of early specialization, it has also been suggested to have associated psychological implications (Myer, 2015).\nYou see, there are increased pressures associated with intense, specialized, training and competitions. This has been shown to significantly increase the risk of developing burnout, depression, and anxiety.\nAdditionally, research on young athletes has shown that professionalized, adult-style practices can actually inhibit talent development pathways.\nSpecifically, it has been shown that children and adolescents need to enjoy the activities of their domain to ensure intrinsic motivation. Without enjoyment, there is no desire to maintain participation or achieve goals.\nAs a result, increased rates of physical inactivity and drop out are increasingly common in children who specialize too early (Wall, 2007).\nWhat is Sports Variation?\nOn the flip side, we have sports variation.\nAs its name suggests, sports variation (also known as sports diversification) essentially describes the process of actively performing more than one sport across the year’s duration.\nThis doesn’t necessarily mean trying to become the best in the world at several different activities. Rather you should partake in a number of sports that require different skill sets. As a result, this variation ensures the development of several physical characteristics simultaneously, whilst also keeping enjoyment levels high across the year.\nWith all this in mind, it essentially eliminates the negative effects of early specialization. If your child does look to have a career in elite sport, they shouldn’t really begin to specialize until they get closer to college.\nBut how does this affect us as adults?\nWhat Are The Benefits of Sports Variation in Adults?\nAs adults, we have a tendency to gravitate towards those activities that we enjoy – which makes sense. If we are training for the sole purpose of maintaining a fit and healthy lifestyle, then why not do things that we find fun?\nThis is likely to improve adherence, motivation, and therefore effectiveness.\nHowever, it does again have some pitfalls.\nMuch like in the scenario of early specialization outlined above, limiting yourself to strictly performing one specific task for years on end greatly increase your chance of developing an overuse injury.\nSimilarly, it can also limit the development of numerous physical qualities.\nWhile this doesn’t sound all that bad, it is important to note that to maintain a high quality of life throughout the duration of your lifetime, you need to maintain the physical capacity to navigate through life effectively (Rennemark, 2009).\nThis means you need to have adequate strength, power, and reaction time. Additionally, you have the ability to respond to unexpected situations effectively. They are all skills that come with actively choosing to diversify the sports that you play, either in a competitive or social setting.\nNot to mention that fact that performing team sports in more social environments have been shown to have a myriad of mental health benefits. Health benefits include increased self-esteem, lower risks of depression and anxiety, and even heightened life satisfaction (Eime, 2013).\nIn short, sports variety can have a huge impact on your life.\nRelated Article: The Best Workout Combination: Endurance Training and HIIT\nHow Should I Train for Sports variety?\nSo, we know that participating in different sports can have an immensely positive impact on both the physical and psychological aspects of your life. But most of us also realize that there is some risk associated with performing a sport we are not accustomed too.\nNamely, the risk of injury, and of course, the chance of being somewhat average at it (which obviously improves through participation).\nBut this risk can be mitigated by ensuring that you are adequately prepared for any sport in a completely physical sense. This is done by dedicating your own training to developing the most important physical qualities that underpin all sport.\nSpecifically, aerobic capacity, muscle strength, muscle power, and landing ability.\nBy ensuring that you train each of these qualities in a gym environment, you can be certain that you are at the very least prepared for the physical demands of any sport. It will not only improve your performance capabilities but also lowering your risk of injury.\nThe Best Sports Variety Gym Program\nTaking the above into consideration, I have put together a simple gym program that can be performed 2-3 times per week. This program targets the fours key qualities outlined above, ensuring that you are well prepared for literally any sport.\nIn conclusion, sports variety is so much more than just playing different sports. It provides the opportunity to develop the broad foundation of physical capacity required to navigate daily life effectively.\nIn children, this can lead to seriously improved outcomes in sporting scenarios and improvement in health. Similarly, in adults, it can lead to better functional capacity, psychological health, and even greater quality of life.\nIt truly is one of the most enjoyable ways to boost health and function on the planet.\nMalina, Robert M. “Early sports specialization: roots, effectiveness, risks.” Current sports medicine reports 9.6 (2010): 364-371.\nJayanthi, Neeru, et al. “Sports specialization in young athletes: evidence-based recommendations.” Sports Health 5.3 (2013): 251-257.\nJayanthi, Neeru A., et al. “Sports-specialized intensive training and the risk of injury in young athletes: a clinical case-control study.”. The American journal of sports medicine 43.4 (2015): 794-801.\nMyer, Gregory D., et al. “Sports specialization, part I: does early sports specialization increase negative outcomes and reduce the opportunity for success in young athletes?.” Sports Health 7.5 (2015): 437-442.\nWall, Michael, and Jean Côté. “Developmental activities that lead to dropout and investment in sport.”. Physical education and sport pedagogy 12.1 (2007): 77-87.\nRennemark, Mikael, et al. “Relationships between physical activity and perceived qualities of life in old age. Results of the SNAC study.” Aging and Mental Health 13.1 (2009): 1-8.\nEime, Rochelle M., et al. “A systematic review of the psychological and social benefits of participation in sport for children and adolescents. Informing development of a conceptual model of health through sport.”. International journal of behavioral nutrition and physical activity 10.1 (2013): 98.\nYou Might Like:"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:422d9b76-9796-40d5-a327-562be52b8ec0>","<urn:uuid:2ebecfaf-ceb2-4e7a-96f8-9adbc516466a>"],"error":null}
{"question":"What's the relationship between failure rate calculation methods and true cost analysis in manufacturing?","answer":"Failure rate data can be obtained through field failure reports, historical data, government/commercial sources, life testing, and cycle testing, focusing on the frequency of failures. True cost analysis, on the other hand, examines the comprehensive financial impact by tracking all affected departments, expenditures, wasted fixed and variable costs, consequential costs, and lost sales profits throughout the organization. While failure rate helps predict the likelihood of failures, true cost analysis reveals the full economic impact when failures occur, which often includes hidden costs not typically recognized by management.","context":["Business Article - Equipment Failure and the Cost of failure.\nBy Mike Sondalini\n(Above chart borrowed from Mike Sondalini's book 'Defect and Failure True Costing - 2006')\nThe Huge Instantaneous Impact on Your Business of equipment failure cost and defect cost . When an equipment failure incident occurs there is a consequential loss of profits and amassing of costs. The cost of equipment failure includes lost profit, the cost of the equipment repair, the fixed and variable operating costs wasted during the equipment downtime and a myriad of consequential costs that reverberate and surge through the business. These are all paid for by the organization and seen as poor financial performance by business operations management. The costs of equipment failure cannot be escaped and are counted in millions of dollars of lost profit per year. Total defect and failure true costs are not normally recognized by managers, yet they can send businesses bankrupt. In the instance of a failure all its costs and losses are automatically incurred on the business. These costs can only be prevented by precluding the failure in the first place. This article explains the 'instantaneous cost of failure' (ICOF) and introduces a proactive technique, 'Defect and Failure True Costing', that adds economics to RCM and FMEA to help companies recognize and prevent this tremendous waste of money.\nThe Cost of Failure to a Business\nWhen a business operates it expends fixed and variable costs to make a product which it sells for a profit. Figure 1 is graphical representations of a business in operation. The business produces a product that requires an input of costs which it sells to pay for them and make a profit.\nThe business has fixed costs that it must carry regardless of how much it produces. These include the cost of building rent, the manager's salary, the permanent staff and employees' wages, insurances, equipment leases, etc. There are variable costs as well, such as fuel, power, hire labor, raw materials to make product, etc. From doing business a profit is made that keeps it trading.\nWe can take the same analogy of normal business operations management down to normal equipment operation. When plant and equipment are running each item has a fixed cost, a variable cost and generates a contribution to the overall business profit. It is reasonable to look at every machine and various equipment in the plant operation, as contributing their share to the total profit of the business - their 'profit contribution'. When equipment in the plant cannot be used for production it cannot contribute to profit. If the reason it is not operating is because it has failed, then not only is it not contributing profit, it also imposes added equipment repair costs on the business.\nIn Figure 2 an equipment failure incident occurs at time t1 that stops the operation. A number of things immediately happen to the business. Future profits are lost because no product can be made (though inventory can still be sold until it is gone). The fixed costs continue accumulating but are now wasted because no product is being produced. Some of the variable costs will fall because they are not used, whereas some, like maintenance, will suddenly rise in response to the incident. Other variable costs are retained in the expectation that the equipment will get back into operation quickly. These are also wasted because they are no longer involved in making saleable product. Usually workers are put onto other duties they are not meant to be doing. Losses and wastes continue until the plant is back in operation at time t2. The cost for repair from a severe outage can be many times the profit made in the same time period (the dotted outline in Figure 2).\nWhen a equipment failure happens many people suddenly get involved in solving it. Meetings are held, overtime is worked, subcontractors are brought-in, engineers investigate, parts and spares are purchased to get back in operation. Instead of the variable costs being a proportion of production, as intended, they instead rise and take on a life of their own in response to the failure. The losses grow proportionally bigger the longer the equipment repair takes or the greater the consequences of the equipment failure.\nYou can see from the shaded areas in Figure 2 that when an equipment failure happens the cost to the business is lost future profits, plus immediately wasted fixed costs, plus immediately wasted variable costs, plus the added variable costs needed to get the operation back in production. There are many other consequential costs too.\nWhen equipment fails, operators stop normal duties that make money and start doing duties that cost money. The production supervisors and operators, the maintenance supervisors, planners, purchasers and repairmen start spending time and money addressing the stoppage.\nIf it escalates managers from several departments get involved - production, maintenance, sales, dispatch, finance - wanting to know what is being done to fix the stoppage. Meetings are held formally in meeting rooms and impromptu in corridors. Parts are purchased and specialists may be brought in. Customers do not get deliveries and liability clauses may be invoked. Word can spread that the company does not meet its schedules and future business is lost. A rushed work-around is developed that puts people at higher risk of injury. Items are brought, men are moved, materials and equipment are transported in an effort to get production going. Time and money better used on business-building activities is drawn into the 'black hole of equipment failure'.\nOn and upward the costs build, and on and on people throughout the company waste time because of the equipment failure and non-procedural business operations management. The company pays for all of it from its profits, which is then reflected in poor financial performance. The reactive costs and the resulting wastes start immediately upon equipment failure and continue until the last cent is paid on the final invoice. Some consequential costs may continue for years after the equipment failure.\nThese lost and wasted moneys can be considered as the 'instantaneous cost of failure' or 'instantaneous loss'. They are instantaneous because as soon as an equipment failure happens they will need to be spent. There is no alternative but to spend them to get the business back into production. The moneys spent to fix the problem, the lost income from no production, the payment of unproductive labor, the handling of the company-wide business operations management disruptions and the loss of future business is money lost forever. However, they would not have had to be spent if the equipment failure had not happened!\nThe money to fix equipment failures and to carry wasted costs is paid by the business from profits. This explains why having many equipment failures, or a few big equipment failures close in time, can cause a business to become unprofitable. Figure 3 shows the situation of a business suffering repeated equipment failures (although the failures do not need to be equipment failures). The losses and wasted costs accumulate in production, the cost of maintenance climbs, the knock-on costs and wasted time across the business rise and profits in the business fall.\nTotal and True Instantaneous Failure Cost and Defect Cost\nEach business organization is different and each defect, error and failure it suffers has different consequences. The total cost to the business organization of an incident will be shared amongst the departments and people involved. The proportion of the cost each department ends up carrying depends on the extent of its involvement.\nThe total and true costs incurred by a business from a failure event reverberate and surge throughout the business organization. The 60 consequential costs listed below reflect a good number of them, though there are others specific to each organization and you will need to identify and record them.\n· Labor : both direct and indirect\n§ overtime/penalty rates\n· Product waste\n§ replacement production\n§ lost production\n§ lost spot sales\n§ off-site storage\n§ emergency hire\n§ utility repairs\n§ temporary accommodation\n§ replacement parts\n§ fabricated parts\no welding consumables\no workshop hire\n§ design changes\n§ inventory replenishment\n§ quality control\n§ energy waste\n§ emergency hire\n§ damaged items\n· Additional capital\n§ replacement equipment\n§ new insurance spares\n§ buildings and storage\n§ penalty payments\n§ lost future sales\n§ legal fees\n§ loss of future contracts\n§ environmental clean-up\n§ death and injury\n§ safety rectification\n§ Documents and reports\n§ purchase orders\n§ meeting rooms\n§ planning, schedule changes\n§ investigations and audits\n§ invoicing and matching\nThe sum totals of the organization-wide 'instantaneous costs of failure' (ICOF) are not usually considered when the cost of a failure incident is determined. This means that most companies (business operations management) do not fully appreciate the huge consequential costs they incur from every failure incident. Few companies would cost the time spent by the accounts clerk in matching invoices to purchase orders raised because of a failure. But the truth is the clerk would not be doing the work if there had been no failure. The cost was incurred only because the failure happened.\nThe same logic applies for all the costs due to a failure - if there had been no failure there would have been no costs and no waste. Prevent failures and you will make a lot more money.\nThe full cost of all 'instantaneous losses' from a failure incident can be calculated in a spreadsheet. Simply trace all the departments and people affected by an incident, identify all the expenditures and costs incurred throughout the company, determine the fixed and variable costs wasted, discover the consequential costs, find-out the profit from sales lost and tally them all up. It will astound people when you show them how much money was destroyed by one small equipment failure.\nIt is not important to know how many times a failure incident happens to justify calculating the instantaneous cost of failure. It is only important to ask what would be the cost if it did happen. An extraordinary example was when a 150-mm diameter PVC pipe carrying softened, de-mineralised water for a major power station failed at a glued joint and began starving the water supply for three boilers supplying steam to six steam turbines. It was only by supplementing the water supply with raw mains water that the power plant remained in operation until the failure was repaired.\nHad the failure progressed to its disastrous conclusion an entire city of 1.5 million people, and its industry, would have lost power as each turbine progressively stopped from loss of steam. The repair of the pipe was done for several thousand dollars, but the consequence of the failure was in the hundreds of millions of dollars. Had the instantaneous cost of failure been calculated first, far greater precautions would have been put into place to control the hidden risks inherent in the job.\nPreventing the Consequences of Failure\nIt only needs a few large catastrophes close together in time or many smaller problems occurring regularly to totally destroy an organization's profitability.\nBy finding the Defect and Failure True Cost of defects and failures you highlight to everyone that many risks to the business previously considered minor are actually high and they require a strong proactive management plan be put into place to address those risks. The frequency of occurrence maybe low but the cost consequences are massive and so the real risk for the organization is actually high (risk = frequency x consequence).\nDAFT Costing provides a means to rate the consequences of decisions and failure incidents using real money. The cost outcomes can be clearly and truly identified and priced. Those that are unfavorable can be analyzed and modified to reduce the risk. This means the methodology can be used to analyze its consequences and associated costs before a decision is made.\nBecause DAFT Cost is a means to indicate the real costs of knock-on consequences (TDC) from a decision it can be used to make good decisions in any situation where understanding the right financial consequences are critical to its success.\nIdentifying total defect cost and failure cost using 'Instantaneous Cost of Failure' shows how vast amounts of money are wasted throughout an organization when a failure happens. The bigger the failure, or the more often one happens; the more resources and money are lost. The profits that could have been made are gone, wasted, and they can never be recouped.\nThe next time your operation has a failure do an ICOF on it - what did it really cost your company? If someone designs or selects equipment ask them for its 'instantaneous cost of failure'? Ask them what is its 'instantaneous cost of maintenance'? What are the cost consequences to the profitability of the business from its failure?\nIt is critical to your company's profitability that failures are prevented. Failures can be stopped when companies understand the size of the losses and introduce systems, training and behaviors to prevent them. The 'instantaneous cost of failure' method shows people the instantaneous losses from failure, and the great profit in doing the right things rightly.\n|'Defect and Failure True Costing', that adds economics to RCM and FMEA to help companies recognize and prevent this tremendous waste of money.|\nTo learn more about this, be sure and download Mike Sondalini's Ebook 'Defect and Failure True Costing' at http://www.feedforward.com.au/Defect_failure_waste_cost.htm\nAlso you may be interested in his Ebook, co-authored by Don Fitchett; 'True Downtime Cost Analysis - 2nd Edition' which can be downloaded at http://www.feedforward.com.au/downtime_costing_activity_based.htm\nFeel free to copy and distribute this article in it's entirety as long as you let us know, maintain all credits/ links and it is free.","Failure rate is the frequency with which an engineered system or component fails, expressed in failures per unit of time. It is usually denoted by the Greek letter λ (lambda) and is often used in reliability engineering.\nThe failure rate of a system usually depends on time, with the rate varying over the life cycle of the system. For example, an automobile's failure rate in its fifth year of service may be many times greater than its failure rate during its first year of service. One does not expect to replace an exhaust pipe, overhaul the brakes, or have major transmission problems in a new vehicle.\nIn practice, the mean time between failures (MTBF, 1/λ) is often reported instead of the failure rate. This is valid and useful if the failure rate may be assumed constant – often used for complex units / systems, electronics – and is a general agreement in some reliability standards (Military and Aerospace). It does in this case only relate to the flat region of the bathtub curve, which is also called the \"useful life period\". Because of this, it is incorrect to extrapolate MTBF to give an estimate of the service lifetime of a component, which will typically be much less than suggested by the MTBF due to the much higher failure rates in the \"end-of-life wearout\" part of the \"bathtub curve\".\nThe reason for the preferred use for MTBF numbers is that the use of large positive numbers (such as 2000 hours) is more intuitive and easier to remember than very small numbers (such as 0.0005 per hour).\nThe MTBF is an important system parameter in systems where failure rate needs to be managed, in particular for safety systems. The MTBF appears frequently in the engineering design requirements, and governs frequency of required system maintenance and inspections. In special processes called renewal processes, where the time to recover from failure can be neglected and the likelihood of failure remains constant with respect to time, the failure rate is simply the multiplicative inverse of the MTBF (1/λ).\nA similar ratio used in the transport industries, especially in railways and trucking is \"mean distance between failures\", a variation which attempts to correlate actual loaded distances to similar reliability needs and practices.\nFailure rates are important factors in the insurance, finance, commerce and regulatory industries and fundamental to the design of safe systems in a wide variety of applications.\nFailure Rate Data\nFailure rate data can be obtained in several ways. The most common means are:\n- From field failure rate reports, statistical analysis techniques can be used to estimate failure rates. For accurate failure rates the analyst must have a good understanding of equipment operation, procedures for data collection, the key environmental variables impacting failure rates, how the equipment is used at the system level, and how the failure data will be used by system designers.\n- Historical data about the device or system under consideration\n- Many organizations maintain internal databases of failure information on the devices or systems that they produce, which can be used to calculate failure rates for those devices or systems. For new devices or systems, the historical data for similar devices or systems can serve as a useful estimate.\n- Government and commercial failure rate data\n- Handbooks of failure rate data for various components are available from government and commercial sources. MIL-HDBK-217F, Reliability Prediction of Electronic Equipment, is a military standard that provides failure rate data for many military electronic components. Several failure rate data sources are available commercially that focus on commercial components, including some non-electronic components.\n- Time lag is one of the serious drawbacks of all failure rate estimations. Often by the time the failure rate data are available, the devices under study have become obsolete. Due to this drawback, failure-rate prediction methods have been developed. These methods may be used on newly-designed devices to predict the device's failure rates and failure modes. Two approaches have become well known, Cycle Testing and FMEDA.\n- Life Testing\n- The most accurate source of data is to test samples of the actual devices or systems in order to generate failure data. This is often prohibitively expensive or impractical, so that the previous data sources are often used instead.\n- Cycle Testing\n- Mechanical movement is the predominant failure mechanism causing mechanical and electromechanical devices to wear out. For many devices, the wear-out failure point is measured by the number of cycles performed before the device fails, and can be discovered by cycle testing. In cycle testing, a device is cycled as rapidly as practical until it fails. When a collection of these devices are tested, the test will run until 10% of the units fail dangerously.\n- Failure modes, effects, and diagnostic analysis (FMEDA) is a systematic analysis technique to obtain subsystem / product level failure rates, failure modes and design strength. The FMEDA technique considers:\n- All components of a design,\n- The functionality of each component,\n- The failure modes of each component,\n- The effect of each component failure mode on the product functionality,\n- The ability of any automatic diagnostics to detect the failure,\n- The design strength (de-rating, safety factors) and\n- The operational profile (environmental stress factors).\nGiven a component database calibrated with field failure data that is reasonably accurate , the method can predict product level failure rate and failure mode data for a given application. The predictions have been shown to be more accurate than field warranty return analysis or even typical field failure analysis given that these methods depend on reports that typically do not have sufficient detail information in failure records. Failure modes, effects, and diagnostic analysis\nFailure Rate in the Discrete Sense\nThe failure rate can be defined as the following:\n- The total number of failures within an item population, divided by the total time expended by that population, during a particular measurement interval under stated conditions. (MacDiarmid, et al.)\nAlthough the failure rate, , is often thought of as the probability that a failure occurs in a specified interval given no failure before time , it is not actually a probability because it can exceed 1. Erroneous expression of the failure rate in % could result in incorrect perception of the measure, especially if it would be measured from repairable systems and multiple systems with non-constant failure rates or different operation times. It can be defined with the aid of the reliability function, also called the survival function, , the probability of no failure before time .\n- , where is the time to (first) failure distribution (i.e. the failure density function).\nover a time interval = from (or ) to . Note that this is a conditional probability, where the condition is that no failure has occurred before time . Hence the in the denominator.\nHazard rate and ROCOF (rate of occurrence of failures) are often incorrectly seen as the same and equal to the failure rate.[clarification needed] To clarify; the more promptly items are repaired, the sooner they will break again, so the higher the ROCOF. The hazard rate is however independent of the time to repair and of the logistic delay time.\nFailure rate in the continuous sense\nCalculating the failure rate for ever smaller intervals of time results in the hazard function (also called hazard rate), . This becomes the instantaneous failure rate or we say instantaneous hazard rate as approaches to zero:\nA continuous failure rate depends on the existence of a failure distribution, , which is a cumulative distribution function that describes the probability of failure (at least) up to and including time t,\nwhere is the failure time. The failure distribution function is the integral of the failure density function, f(t),\nThe hazard function can be defined now as\nMany probability distributions can be used to model the failure distribution (see List of important probability distributions). A common model is the exponential failure distribution,\nwhich is based on the exponential density function. The hazard rate function for this is:\nThus, for an exponential failure distribution, the hazard rate is a constant with respect to time (that is, the distribution is \"memory-less\"). For other distributions, such as a Weibull distribution or a log-normal distribution, the hazard function may not be constant with respect to time. For some such as the deterministic distribution it is monotonic increasing (analogous to \"wearing out\"), for others such as the Pareto distribution it is monotonic decreasing (analogous to \"burning in\"), while for many it is not monotonic.\nDecreasing Failure Rate\nA decreasing failure rate (DFR) describes a phenomenon where the probability of an event in a fixed time interval in the future decreases over time. A decreasing failure rate can describe a period of \"infant mortality\" where earlier failures are eliminated or corrected and corresponds to the situation where λ(t) is a decreasing function.\nFor a renewal process with DFR renewal function, inter-renewal times are concave. Brown conjectured the converse, that DFR is also necessary for the inter-renewal times to be concave, however it has been shown that this conjecture holds neither in the discrete case nor in the continuous case.\nIncreasing failure rate is an intuitive concept caused by components wearing out. Decreasing failure rate describes a system which improves with age. Decreasing failure rates have been found in the lifetimes of spacecraft, Baker and Baker commenting that \"those spacecraft that last, last on and on.\" The reliability of aircraft air conditioning systems were individually found to have an exponential distribution, and thus in the pooled population a DFR.\nCoefficient of variation\nWhen the failure rate is decreasing the coefficient of variation is ⩾ 1, and when the failure rate is increasing the coefficient of variation is ⩽ 1. Note that this result only holds when the failure rate is defined for all t ⩾ 0 and that the converse result (coefficient of variation determining nature of failure rate) does not hold.\nFailure rates can be expressed using any measure of time, but hours is the most common unit in practice. Other units, such as miles, revolutions, etc., can also be used in place of \"time\" units.\nFailure rates are often expressed in engineering notation as failures per million, or 10−6, especially for individual components, since their failure rates are often very low.\nThe Failures In Time (FIT) rate of a device is the number of failures that can be expected in one billion (109) device-hours of operation. (E.g. 1000 devices for 1 million hours, or 1 million devices for 1000 hours each, or some other combination.) This term is used particularly by the semiconductor industry.\nThe relationship of FIT to MTBF may be expressed as: MTBF = 1,000,000,000 x 1/FIT.\nUnder certain engineering assumptions (e.g. besides the above assumptions for a constant failure rate, the assumption that the considered system has no relevant redundancies), the failure rate for a complex system is simply the sum of the individual failure rates of its components, as long as the units are consistent, e.g. failures per million hours. This permits testing of individual components or subsystems, whose failure rates are then added to obtain the total system failure rate.\nAdding \"redundant\" components to eliminate a single point of failure improves the mission failure rate, but makes the series failure rate (also called the logistics failure rate) worse—the extra components improve the mean time between critical failures (MTBCF), even though the mean time before something fails is worse.\nSuppose it is desired to estimate the failure rate of a certain component. A test can be performed to estimate its failure rate. Ten identical components are each tested until they either fail or reach 1000 hours, at which time the test is terminated for that component. (The level of statistical confidence is not considered in this example.) The results are as follows:\nEstimated failure rate is\nor 799.8 failures for every million hours of operation.\n- Electrical & Mechanical Component Reliability Handbook. exida. 2006.\n- Goble, William M.; Iwan van Beurden (2014). Combining field failure data with new instrument design margins to predict failure rates for SIS Verification. Proceedings of the 2014 International Symposium - BEYOND REGULATORY COMPLIANCE, MAKING SAFETY SECOND NATURE, Hilton College Station-Conference Center, College Station, Texas.\n- W. M. Goble, \"Field Failure Data – the Good, the Bad and the Ugly,\" exida, Sellersville, PA \n- Finkelstein, Maxim (2008). \"Introduction\". Failure Rate Modelling for Reliability and Risk. Springer Series in Reliability Engineering. pp. 1–84. doi:10.1007/978-1-84800-986-8_1. ISBN 978-1-84800-985-1.\n- Brown, M. (1980). \"Bounds, Inequalities, and Monotonicity Properties for Some Specialized Renewal Processes\". The Annals of Probability. 8 (2): 227–240. doi:10.1214/aop/1176994773. JSTOR 2243267.\n- Shanthikumar, J. G. (1988). \"DFR Property of First-Passage Times and its Preservation Under Geometric Compounding\". The Annals of Probability. 16 (1): 397–406. doi:10.1214/aop/1176991910. JSTOR 2243910.\n- Brown, M. (1981). \"Further Monotonicity Properties for Specialized Renewal Processes\". The Annals of Probability. 9 (5): 891–895. doi:10.1214/aop/1176994317. JSTOR 2243747.\n- Yu, Y. (2011). \"Concave renewal functions do not imply DFR interrenewal times\". Journal of Applied Probability. 48 (2): 583–588. arXiv:1009.2463. doi:10.1239/jap/1308662647.\n- Proschan, F. (1963). \"Theoretical Explanation of Observed Decreasing Failure Rate\". Technometrics. 5 (3): 375–383. doi:10.1080/00401706.1963.10490105. JSTOR 1266340.\n- Baker, J. C.; Baker, G. A. S. . (1980). \"Impact of the space environment on spacecraft lifetimes\". Journal of Spacecraft and Rockets. 17 (5): 479. Bibcode:1980JSpRo..17..479B. doi:10.2514/3.28040.\n- Saleh, Joseph Homer; Castet, Jean-François (2011). \"On Time, Reliability, and Spacecraft\". Spacecraft Reliability and Multi-State Failures. p. 1. doi:10.1002/9781119994077.ch1. ISBN 9781119994077.\n- Wierman, A.; Bansal, N.; Harchol-Balter, M. (2004). \"A note on comparing response times in the M/GI/1/FB and M/GI/1/PS queues\" (PDF). Operations Research Letters. 32: 73–76. doi:10.1016/S0167-6377(03)00061-0.\n- Gautam, Natarajan (2012). Analysis of Queues: Methods and Applications. CRC Press. p. 703. ISBN 978-1439806586.\n- Xin Li; Michael C. Huang; Kai Shen; Lingkun Chu. \"A Realistic Evaluation of Memory Hardware Errors and Software System Susceptibility\". 2010. p. 6.\n- \"Reliability Basics\". 2010.\n- Vita Faraci. \"Calculating Failure Rates of Series/Parallel Networks\". 2006.\n- \"Mission Reliability and Logistics Reliability: A Design Paradox\".\n- Goble, William M. (2018), Safety Instrumented System Design: Techniques and Design Verification, Research Triangle Park, NC 27709: International Society of AutomationCS1 maint: location (link)\n- Blanchard, Benjamin S. (1992). Logistics Engineering and Management (Fourth ed.). Englewood Cliffs, New Jersey: Prentice-Hall. pp. 26–32. ISBN 0135241170.\n- Ebeling, Charles E. (1997). An Introduction to Reliability and Maintainability Engineering. Boston: McGraw-Hill. pp. 23–32. ISBN 0070188521.\n- Federal Standard 1037C\n- Kapur, K. C.; Lamberson, L. R. (1977). Reliability in Engineering Design. New York: John Wiley & Sons. pp. 8–30. ISBN 0471511919.\n- Knowles, D. I. (1995). \"Should We Move Away From 'Acceptable Failure Rate'?\". Communications in Reliability Maintainability and Supportability. International RMS Committee, USA. 2 (1): 23.\n- MacDiarmid, Preston; Morris, Seymour; et al. (n.d.). Reliability Toolkit (Commercial Practices ed.). Rome, New York: Reliability Analysis Center and Rome Laboratory. pp. 35–39.\n- Modarres, M.; Kaminskiy, M.; Krivtsov, V. (2010). Reliability Engineering and Risk Analysis: A Practical Guide (2nd ed.). CRC Press. ISBN 9780849392474.\n- Mondro, Mitchell J. (June 2002). \"Approximation of Mean Time Between Failure When a System has Periodic Maintenance\" (PDF). IEEE Transactions on Reliability. 51 (2): 166–167. doi:10.1109/TR.2002.1011521.\n- Rausand, M.; Hoyland, A. (2004). System Reliability Theory; Models, Statistical methods, and Applications. New York: John Wiley & Sons. ISBN 047147133X.\n- Turner, T.; Hockley, C.; Burdaky, R. (1997). The Customer Needs A Maintenance-Free Operating Period. 1997 Avionics Conference and Exhibition, No. 97-0819, P. 2.2. Leatherhead, Surrey, UK: ERA Technology Ltd.\n- U.S. Department of Defense, (1991) Military Handbook, “Reliability Prediction of Electronic Equipment, MIL-HDBK-217F, 2"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:571e6e5b-12f7-41d1-a7f4-09588cd8a76d>","<urn:uuid:49583b26-2c35-46f6-90fd-02a202067e06>"],"error":null}
{"question":"What natural hazards threaten critical infrastructure in New Zealand and the US, and how is advanced technology being used to assess these risks?","answer":"In New Zealand, the Alpine Fault poses a major earthquake hazard with a 69% probability of rupturing in the next 50 years, potentially producing a magnitude 8 earthquake that could damage infrastructure. In the US, wildfires increasingly threaten power and communication infrastructure, especially in drought conditions. Advanced technology, particularly LiDAR, is being used in both contexts to assess risks - in NZ to map fault traces and potential ground deformation, and in the US to analyze vegetation near power lines and assess wildfire risk through detailed canopy mapping and fuel load analysis. This data supports mitigation efforts and infrastructure planning.","context":["The 850 km-long Alpine Fault (AF) is one of the world’s great laterally-slipping active faults (like California’s San Andreas Fault), which currently accommodates about 80% of the motion between the Australian and Pacific tectonic plates in the South Island of New Zealand (NZ). Well-dated sedimentary layers preserved in swamps and lakes adjacent to the AF currently provide one of the world’s most spatially and temporally complete record of large ground rupturing earthquakes (Howarth et al., 2018). Importantly these records reveal that major earthquakes occur with greater regularity on the AF than any other known fault, releasing a Magnitude (Mw) 7 to 8 earthquake on average every 249 ± 58 years and that the most recent earthquake was around Mw 8 in 1717 AD prior to European arrival. This computes to a conditional probability of 69% that the AF will rupture in the next 50 years. For a country that has recently had several notable earthquakes (e.g. 2010 Mw 7.1 Canterbury, 2016 Mw 7.8 Kaikoura) and has an economy heavily reliant on tourism, the next AF earthquake is the one NZ is trying to prepare for (note that a Mw 8 earthquake is about thirty times the energy release of a Mw 7).\nThe more data we can gather as scientists to constrain (1) the magnitude of the next AF earthquake, (2) the amount of lateral and vertical slip (offset roads, powerlines, etc.), (3) the coseismic effects (ground shaking, landslides, liquefaction), and (4) the duration it takes the landscape to recover (muddy rivers, increased sediment supply, prolonged landsliding), the more we can anticipate expected hazards and foster societal resilience.\nDespite its name, the AF is almost completely obscured beneath a dense temperate rain-forest canopy, which has hindered fine-scale geomorphic studies. Relatively low quality airborne LiDAR (2 m-resolution bare-earth model) was first collected in 2010 for a 32 km-length of the central AF. Despite being the best studied portion of the AF, 82 % of the fault traces identified in the LiDAR were previously unmapped (Barth et al., 2012). The LiDAR reveals the width and style of ground deformation. Interpretation of the bare-earth landscape in combination with on the ground sampling, allows single earthquake displacements, uplift rates, recurrence of landslides, and post-earthquake sedimentation rates to be quantified. A new 2019 airborne LiDAR dataset collected along 230 km-length of the southern AF has great potential to improve our understanding of this relatively “well-behaved” fault system, what to expect from its next earthquake, and to give us insight into considerably more complex fault systems like the San Andreas.\nThe LAStools software will be used to check the quality of the data (reclassing ground points and removing any low ground classed outliers if needed) and create a seamless digital terrain model (DTM) from the 1695 tiled LAS files provided. The DTM will be used to create derivative products including contours, slope map, aspect map, single direction B&W hillshades, multi-directional hillshades, and slope-colored hillshades to interpret the fault and landslide related landscape features hidden beneath the dense temperate rain-forest. The results will be used as seed data to seek national-level science funding to field verify interpretations and collect samples to determine ages of features (geochronology). The ultimate goal is to improve our understanding of the Alpine Fault prior to its next major earthquake and to communicate those findings effectively through publications in open access peer-reviewed journal articles and meetings with NZ regional councils.\n+ airborne LiDAR survey collected in 2019 using a Riegl LSM-Q780 sensor by AAM New Zealand\n+ provided data are as 1695 LAS files organized into 500 m x 500 m tiles and classified as ground and non-ground points (75 pts/m2 or ~0.8 ground-classed pts/m2; 320 GB total)\n1) check the quality of the ALS data [lasinfo, lasoverlap, lasgrid]\n2) [if needed] remove any low and high ground-classed outliers [lasnoise]\n3) [if needed] reclassify ground and non-ground points [lasground]\n4) create Digital Terrain Model (DTM) from ground points [blast2dem]\nHowarth, J.D., Cochran, U.A., Langridge, R.M., Clark, K.J., Fitzsimons, S.J., Berryman, K.R., Villamor, P., Strong, D.T. (2018) Past large earthquakes on the Alpine Fault: paleosismological progress and future directions. New Zealand Journal of Geology and Geophysics, v. 61, 309-328, doi: 10.1080/00288306.2018.1465658\nBarth, N.C., Toy, V.G., Langridge, R.M., Norris, R.J. (2012) Scale dependence of oblique plate-boundary partitioning: new insights from LiDAR, central Alpine Fault, New Zealand. Lithosphere 4(5), 435-448, doi: 10.1130/L201.1","A 1.211Mb PDF of this article as it appeared in the magazine complete with images is available by clicking HERE\nWildfires pose an on-going hazard to people, homes, critical infrastructure and the environment across the Nation. With climate change and trending drought conditions, the occurrence and intensity of wildfires is increasing annually. Fire seasons are getting longer, fires are becoming more intense, and the subsequent impacts are more devastating.\nWhile this change in wildfire frequency and intensity is putting people living in the Wildland Urban Interface at more risk, it is also affecting industries that service society through communication, power and other related facilities. In particular, electric and communication companies have substantial risk and liability from wildfires. This includes the risk of potential damage to critical infrastructure assets, such as utility poles, structures and transmission/distribution conductors.\nIn addition, power lines are often a cause of fire ignitions, especially when lines are blown down due to high winds and extreme weather conditions. This issue is not restricted to certain areas of the U.S., although the Santa Ana winds that occur in Southern California are frequently reported in the press. Some of the wildfires that occurred in the 2007 Firestorm in San Diego County were caused by downed power lines. This can result in significant liability for electric utility companies.\nWith recent advancements in GIS technology, LiDAR data collection, and fire science, solutions are now readily available to proactively analyze wildfire risk and potential exposure. This information can be used to support mitigation of vegetation and fuels to reduce the potential for ignition and extensive fire spread when fires do occur. Electric companies are actively engaging in risk analysis to support vegetation management activities to reduce potential damage and liability. LiDAR data provides a detailed source of vegetation data for power line rightof-ways as well as surrounding areas.\nIn addition to incorporating wildfire risk analysis as a standard element of asset management practices, companies are also interested in real-time monitoring of fire incidents, including simulating fire spread, and evaluating potential impacts for individual fires in real time. This information can be obtained in minutes of a fire notification, providing capabilities to alert key managers on potential consequences. This information is key for decision making in support of response, suppression, and infrastructure logistics and activities. By providing real time analysis, more accurate information is available to support response for company field crews, as well as coordination with fire management agency partners and the public.\nMethods for Mapping Vegetation, Fuels, Landscape Characteristics and Infrastructure\nVegetation encroachment is an on-going issue for electric utility companies. Practices and programs are in place to support on-going inspection and trimming of vegetation to minimize potential damages to lines. LiDAR data is a critical data source for providing detailed, accurate data to support ROW vegetation management. LiDAR data is collected along a corridor and processed to identify \"points-of-interest\" (POI). These POIs are then used to develop projects that are designed to raise structures and their associated conductors or to trim the surrounding vegetation, decreasing the likelihood of \"grow-in\" or \"fall-in\" occurrences.\nFigure 1 presents an example of using LiDAR data to identify areas where vegetation is encroaching on power line ROWs and could potentially damage lines during certain weather conditions.\nTraditionally vegetation and fuels data has been derived using remote sensing processing methods to determine vegetation species, type and fuels load. Satellite or aerial imagery is a common source providing data at resolutions varying from 1m to 30m on the ground. Medium resolution data, such as 10m to 30m, is ideal for wildfire risk analysis across large landscapes, such as counties or states. This data can be used for fire behavior analysis providing adequate scale outputs when combined with asset data.\nFor local areas and specific sites or ROWs, more detailed data is often required. LiDAR provides the most robust and cost-effective approach for acquiring high resolution data, for vegetation, canopy mapping, and infrastructure mapping (poles, towers and conductors). Recent methods have been developed to analyze LiDAR data to provide very detailed representations of vegetation canopy. This is important for localized canopy fire potential mapping. While not commonly applied in fire management agencies or private industry today, LiDAR holds promise for providing the most accurate mapping of site-specific fire hazards in the future. It is anticipated that LiDAR will become the norm for data acquisition for certain industries, especially when conditions of man-made features combined with natural vegetated landscapes are important.\nWhile LiDAR mapping specifically addresses vegetation encroachment concerns, it does not immediately address risk from wildfire due to infrastructure-caused ignitions. The potential damage from wildfires is an issue for power companies in two ways: 1) potential damage to infrastructure assets from wildfires that start elsewhere and burn into infrastructure, and 2) potential damage to homes, people and commercial buildings from wildfires caused by power lines. The second issue can have substantial financial liability associated with it as seen in recent legal decisions across the Nation.\nAnalyzing Wildfire Risk for Infrastructure Assets\nWildfire risk assessment and fire behavior analysis methods are well defined in the fire management arena. The fire science, while being sophisticated, is available through custom programs and vendors, to define potential fire conditions and quantify areas of greatest risk. In particular, the development of GIS datasets that define surface fuels, canopy fire potential, rate of spread and flame length (fire intensity) provide excellent information to aid companies in determining the risk surrounding infrastructure assets.\nSurface fuels are a definition of the expected fire behavior based on fuel loads for specific vegetation types, given density, and topographic conditions (elevation, slope, aspect). Of special concern is the potential for a canopy fire to occur, as compared to a surface fire. Canopy fires occur in specific situations when weather conditions and vegetation characteristics conspire to produce extreme fire situations where spread and intensity can cause extreme conditions and significant damage. Other fire behavior outputs, such as rate of spread define how quickly a fire will move across the landscape given active weather, fuels and topography; flame length is a measure of fire intensity, describing the conditions of a fire front. Higher flames generally mean worse conditions and greater potential for damage when a fire reaches an asset, infrastructure or building.\nDetermining Risk to Prioritize Mitigation Projects\nCompanies must be concerned about risk conditions not only at the location of assets, but also adjacent to assets, and surrounding proximity. Consideration of surrounding fire behavior conditions is a critical element in determining those areas of most concern, and prioritizing mitigation activities to reduce fuel loads and vegetation density. Identifying these areas helps companies proactively work with private landowners and local government agencies in planning activities to reduce and mitigate risk. This typically involves fuel treatments to minimize fire intensity and spread should a fire occur.\nFigure 2 presents examples of wildfire risk analysis outputs that are used to assess conditions around infrastructure assets, and lead to identification of priority areas for mitigation and partner collaboration. The map on the left portrays fire behavior Rate of Spread (measured in chains per hour) for an area in San Diego. Orange and red areas represent extreme spread conditions where a wildfire will move quickly across the landscape. Note the conditions not only within the Right of Ways (ROW) but also in areas adjacent to those areas. The map on the right presents a zoomed in example of the same data showing only the ROS within the ROW. These orange and red areas should be considered as priorities for vegetation management and fuels mitigation.\nEnhancing Asset Management Risk Evaluation\nThe mapping of wildfire risk can also be combined directly with infrastructure data to aid in the calculation of risk scores for assets. Traditionally risk scores are derived using asset management software by considering inspection data and asset characteristics, such as age of the asset, voltage/capacity, recovery complexity, number of identified defects (based on inspection), and other Failure Modes. However, recently some companies are expanding this risk evaluation to include wildfire risk, at or near, the particular asset. Wildfire risk data provides additional information that can useful in determining priorities for conducting asset inspections, or establishing priorities for work orders to correct asset deficiencies identified by field inspections. Figure 3 presents an example of where wildfire intensity data has been assigned to poles and integrated into the VUEWorks asset management software for consideration during failure mode analysis and subsequent work order prioritization.\nMonitoring Active Wildfires and Quantifying Potential Impacts\nElectric utility infrastructure can also be a source of wildfire ignitions, resulting in substantial liability to companies. Typical situations occur when high winds and extreme weather can cause power lines to fall and spark underlying vegetation. Many companies actually have their own firefighter crews for initial attack and suppression of wildfires to respond to these situations. In addition, companies actively collaborate with local fire management agencies to provide supplemental resources during any fire scenario.\nWhile risk analysis methods help to proactively mitigate risk, they do not address real-time incident requirements. Having current information about when ignitions occur, where they are, and their potential for damage is important information used to direct company resources for suppression efforts. Timely, accurate information about an incident is critical for reducing damages and potential liability.\nWith recent advancements in fire modeling and GIS technologies, tools are now available to provide services to monitor active fire incidents, simulate the spread of fires, and calculate potential impacts and damage in real time. DTSwildfire (Orlando, FL) offers a suite of advanced capabilities to meet these needs using a web and mobile subscription service. By integrating with local, state and federal dispatch systems, DTSwildfire is able to track verified active incidents, and then simulate fire spread on-the-fly, providing the basis for quantifying potential impacts to infrastructure, people and homes. Analysis is done automatically in less than 2 minutes providing incident impact reports via email quickly to key company decision makers. This approach monitors fire status and informs when thresholds for potential damage are met. An interactive web mapping application provides more advanced tools for qualified users to conduct more detailed analysis should the incident warrant, particularly for large fires that extend beyond a day.\nThe impact analysis uses a range of data sources to produce the summary report. Often this may include census data, local parcel and assessor data used to identify home locations and values, detailed building locations, and proprietary company data on ratepayers, customers and infrastructure assets.\nFigure 4 presents examples of the real-time incident monitoring outputs generated by the DTSwildfire subscription service. The map on the left shows the expected spread of a wildfire for 12 hours during typical Santa Ana event conditions in San Diego County. The simulation mimics an ignition caused by a downed power line. The map on the right presents a more detailed view of the ignition location and local spread conditions near adjacent homes and infrastructure.\nFigure 5 presents an example of the DTSwildfire interactive web mapping application with typical impact analysis reports that are generated in seconds for any fire simulation.\nConclusion & Next Steps\nField activities for vegetation management, asset inspections, and hardening of power lines can be costly. Knowing the most at-risk areas is critical information to help prioritize where field activities should be focused, and where investments should be made. Understanding wildfire risk and the potential impacts is also key to reducing corporate liability and justifying insurance coverage determination. This approach will provide an immediate Return-on-Investment to any utility looking to quantify their potential fire risk as well as implementing a program to mitigate these risks in their highest consequence areas.\nThe integration of wildfire risk analysis, asset management and advanced modes of data acquisition, such as LiDAR, offer many benefits to electric utilities and service providers. These include:\nProactively mitigate risk through the ability to target and prioritize vegetation management activities and homeowner prevention programs\nMore accurately estimate costs for mitigation activities by identifying risk near company assets.\n$ Where should you focus vegetation management, mitigation and prevention efforts?\n$ Where should you prioritize more frequent inspections of network infrastructure and assets?\n$ Prioritize expenditures for line maintenance by including surrounding wildfire risk conditions.\n$ Where are jurisdictional collaboration, agreements and partnerships required?\nKnow where to locate new assets to minimize potential risk and damage in the future.\nIdentify those home and business owners who are located in high risk areas around your assets to support outreach and prevention programs.\n$ Identify and potentially justify additional costs to rate payers in high risk areas to support company mitigation efforts.\nReal-time forecasting of where a wildfire is going and what is actually happening.\n$ Immediately determine potential impacts to support operations and response efforts.\n$ Prioritize service restoration and mobilization of resources.\n$ Minimize employee risk for field teams.\nThrough technology and scientific advancements, opportunities now exist to enhance the ability of electric utility and communication companies to better address and respond to infrastructure maintenance requirements by integrating consideration of wildfire risk and impacts. In the future, the integration of wildfire modeling will become the norm and ultimately help to reduce the damage and liability caused by wildfires for this industry. The need for more detailed data, from technologies like LiDAR, will be critical to providing the most accurate, and up-to-date information possible.\nJason Amadori is the CMO of DTS and VueWorks. He specializes in building custom Asset Management solutions utilizing LiDAR, GIS and custom software solutions.\nDavid Buckley is an expert in the application of remotely sensed fire data to support GIS and database applications.\nA 1.211Mb PDF of this article as it appeared in the magazine complete with images is available by clicking HERE"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"sensitive"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:c8308ded-583e-4b67-9405-5145baa706cf>","<urn:uuid:14563be1-fde2-4039-869e-99b1ee86da18>"],"error":null}
{"question":"When mixing colors, what is the key difference between how complementary colors work in printing versus color theory in graphic design?","answer":"In printing, complementary colors work through subtractive color mixing, where combining two primary subtractive colors (cyan, magenta, yellow) produces a primary additive color. For example, magenta and cyan create blue, yellow and magenta create red, and yellow and cyan produce green. In graphic design color theory, complementary colors are instead defined as colors opposite each other on the color wheel, and rather than being mixed, they are used together to create high contrast and high impact color combinations. When placed next to each other, complementary colors in graphic design appear brighter and more vibrant.","context":["Interactive Java Tutorials\nPigments and dyes are responsible for most of the color that humans see in the real world. Books, magazines, signs, and billboards are printed with colored inks that create colors through the process of color subtraction. This interactive tutorial explores how individual subtractive primary colors can be separated from a full-color photograph, and then how they can be reassembled to create the original scene.\nThe tutorial initializes with a color photograph of mixed fruit displayed in the upper left-hand corner of the tutorial window. Adjacent to and below the full color photograph are the four individual color separations that result from dissecting the image into cyan (C), magenta (M), yellow (Y), and black (K) components. In order to operate the tutorial, use the mouse cursor to superimpose the color separations over one another. As additional separations are added, the resulting image acquires the realism evident in the color photograph.\nWhen any two of the primary subtractive colors are added, they produce a primary additive color. For example, adding magenta and cyan together produces the color blue, while adding yellow and magenta together produces red. In a similar manner, adding yellow and cyan produces green. When all three primary subtractive colors are added, the three primary additive colors are removed from white light leaving black (the absence of any color). White cannot be produced by any combination of the primary subtractive colors, which is the main reason that no mixture of colored paints or inks can be used to print white.\nHuman eyes, skin, and hair contain natural protein pigments that reflect the colors we see in the people around us (in addition to any assistance by colors used in facial makeup and hair dyes). Modern color desktop printers create beautiful prints that are produced with colored inks through the process of color subtraction. In a similar manner, automobiles, airplanes, houses, and other buildings are coated with paints containing a variety of pigments. The concept of color subtraction, as discussed above, is responsible for most of the color produced by the objects just described. For many years, artists and printers have searched for substances containing dyes and pigments that are particularly good at subtracting specific colors.\nAll color photographs, and other images that are painted or printed, are produced using just four colored inks or dyes: magenta, cyan, yellow (the subtractive primaries) and black (see Figure 1). Mixing inks or dyes having these colors in varying proportions can produce the colors necessary to reproduce just about any image or color. The three subtractive primaries could (in theory) be used alone, however the limitations of most dyes and pigments makes it necessary to add black to achieve true color tones. When an image is being prepared for printing in a book or magazine, it is first separated into the component subtractive primaries, either photographically or with a computer as illustrated above in Figure 1. Each separated component is made into a film that is used to prepare a printing plate for that color. The final image is created by sequentially printing each color plate, one on top of another, using the appropriate ink to form a composite that recreates the appearance of the original. Paint is also produced in a somewhat similar manner. Base pigments containing the subtractive primaries are mixed together to form the various colors used in final paint preparations.\nMatthew J. Parry-Hill, Robert T. Sutter and Michael W. Davidson - National High Magnetic Field Laboratory, 1800 East Paul Dirac Dr., The Florida State University, Tallahassee, Florida, 32310.\nQuestions or comments? Send us an email.\n© 1998-2013 by Michael W. Davidson and The Florida State University. All Rights Reserved. No images, graphics, scripts, or applets may be reproduced or used in any manner without permission from the copyright holders. Use of this website means you agree to all of the Legal Terms and Conditions set forth by the owners.\nThis website is maintained by our","Color Theory In Graphic Design：Brief Guide for Non-designers\nThere are few things in the design that are more important than color. Color can evoke reactions, emotions or even action all without using words. So how do we know which colors look good together? The answer is color theory in graphic design! The color theory describes the use of color in graphic design. Otherwise known as graphic design color palettes. However, it’s not only for artists as people use color theory in their everyday life as well! Whether it’s choosing an outfit or putting together a party invitation for a family members birthday color theory helps you choose a colorful graphic design!\n1.The Understandings of Color Theory In Graphic Design\nThe biggest power behind color is their ability to evoke emotions and make people feel things, but the color meaning in graphic design can seem confusing at first as colors often have different interpretations. Let’s take a look at not only what emotions they evoke but also their symbolism!\nColor Theory In Graphic Design: Red\nRed is in the “Warm” color family and tends to evoke feelings of passion, both love, and hate! Proven by the fact you can see the color red used in both imagery of Cupid, an angel of love, and demons.\nAs it’s also been associated with power and imagery of fire, violence, and warfare we tend to use red as a warning of danger or even to reprimand someone, like marking things incorrect using a large red “X” mark. However, red can also be seen as a symbol of status like when used at red carpet events or it can make someone think of red rubies.\nClick the Image to Edit\nRed is best used as an accent color, as it can be overwhelming, and even harmful to the eyes if used in large amounts!\nColor Theory In Graphic Design: Yellow\nYellow, also a warm color is considered to be one of the brightest and most energizing of the warm colors. It’s commonly associated with happiness and sunshine. However, it can also be used to convey a warning or caution as it’s commonly used in construction sighs.\nClick the Image to Edit\nUse yellow when you want a bright pop of happy energy or to draw immediate attention to an area. Yello is also great to use when creating more industrial or modern designs! If you find that yellow is too stark and bright, try using a more muted yellow.\nColor Theory In Graphic Design: Blue\nBlue is often associated with sadness however blue is also used to represent calmness and tranquility. The meaning and symbolism of blue are heavily reliant on the shade of blue. Light blues can be both refreshing and friendly while dark blues are considered stronger and reliable.\nClick the Image to Edit\nWhen using blue remember the exact shade of blue you select will matter most with how your design will be perceived. Light blues are often calming, bright blues can be refreshing or even energizing while dark blues, like navy, are great for corporate designs where reliability is a featuring trait.\nColor Theory In Graphic Design: Orange\nOrange is bright and vibrant and so also gives off an energetic vibe similar to yellow, but is much more subdued in comparison. It’s commonly used in food labels or other cooking-related products (including cell phone recipe apps) as orange is said to invoke hunger in people!\nWhile orange in its purest form is vibrant and bright, more muted forms will give off the feeling of warmth, and remind people of fall leaves. Which can be ideal for designs that want to give the feeling of being “cozy” and warm.\nClick the Image to Edit\nOrange can be a better warm option to use over red as it has all of the vibrancy and energy that the color red has but without the potentially aggressive symbolism.\nColor Theory In Graphic Design: Green\nUnsurprisingly green, a cool-toned color is considered as earthy and will invoke images of nature signifying renewal and abundance. Alternatively, green can also represent envy and jealousy as seen in the phrase “green with envy”.\nGoing with the same theme as “abundance”, green can symbolize wealth especially in countries where their currency is green.\nClick the Image to Edit\nGreen has a similar calming effect as blue, but with some of the energizing tones of yellow. The brighter the green the more energizing it will be. Muted and olive greens will work best in designs of nature and the natural world while dark greens are the most stable and representative of wealth.\nColor Theory In Graphic Design: Purple\nBefore modern-day dyes were created, purple dyes were hard to find and extremely expensive to make so only royals and the wealthy could afford them. Given this history, dark purples are associated with wealth and royalty.\nHowever, lighter purples, like lavender, are typically associated with softness and a more tender romantic love, as opposed to red which ignites more passionate love.\nClick the Image to Edit\nWhen using purples in design dark purples will give a sense of wealth and luxury while light softer purples are associated with spring and romance.\n2. What’s the Color Terminology?\nWhen talking about the color theory you may notice a few key terms pop up quite frequently. That is because describing color is best done by describing it’s hue, saturation, brightness or value. Let’s cover what exactly these terms mean!\nHue is one of the main properties of color and is the property of light by which the color of an object is classified as red, blue, green, or yellow in the color spectrum.\nGreen, orange, yellow, and blue — each of these is a hue, a color or a shade. A rainbow shows the melting of one hue into another, from red to violet, and all the shades in between. The noun hue means both a color and a shade of a color. Green is a hue, and turquoise is a hue of both green and blue!\nSaturation is the intensity of a hue from a gray tone with no saturation, to pure, vivid color with high saturation. High saturation colors will come off more colorful or deep, while low saturation images will come off muted or pastel.\nValue refers to the lightness, brightness or darkness of a color. Value in the art will refer to the shadows and highlights and will give your work more dimension. It is especially important in black and white photos, design and illustration as it will separate objects from each other and their background.\nBrightness is simply both the hue of color along with the value of the color. It refers to its lightness and its ability to replicate light or reflection. This is also known as “luminance.”\n3. Color Palette For Graphic Design\nCreating a cohesive color scheme relies on one thing: knowing what colors go well together and compliment each other. Luckily there are a number of different ways to mix, match and find the perfect color scheme for you!\nThe color wheel for graphic design is a circle with different colored sections used to show the relationship between colors. The typical color wheel includes the blue, red, and yellow primary colors. The corresponding secondary colors are then green, orange, and violet or purple.\nA mentioned above, secondary colors are colors that come from mixing two primary colors. There are three secondary colors. In RGB graphic design, the secondary colors are purple made from red mixed with blue, orange made from red mixed with yellow, and green made from yellow mixed with blue.\nTwo colors that are on opposite sides of the color wheel are called complementary colors. Complementary colors of graphic designs provide high contrast and high impact color combination. When placed together or next to one another these colors will appear brighter and more vibrant.\nA split complementary color scheme involves the use of three colors. Start with one color, find its complement and then use the two colors on either side of it.\nAnalogous colors are among the easiest to find on the color wheel. Pick any color at any point on the wheel. Now, look at any three colors directly to the left or right of the chosen color. Together, those four are a group of analogous colors. Mixing colors that are adjacent to each other creates a colorful yet harmonious look. When using an analogous color scheme it’s best to choose one main color, using the other three as accent colors. This will help keep designs from becoming too chaotic or busy.\nTertiary colors are colors made by combining a secondary color with a primary color. There are six colors considered tertiary. In the RYB color wheel, these tertiary colors are red-orange, yellow-orange, yellow-green, blue-green, blue-violet, and red-violet.\n4. Color Sites for Graphic Design\nThese sites are the top free to use color guide for graphic designers giving you easy to use tools to create graphic design palettes.\nCOLOURlovers is a creative community where people from around the world create and share colors, palettes, and patterns, discuss the latest trends and explore colorful articles. With more than 4,682,736 palettes, 10,035,451 colors, 5,844,503 patterns in 532,217 templates you’re sure to find inspiration to kick start your creative projects.\nColor Hunt is an open collection of color palettes, created by Gal Shir. Color Hunt started as a personal small project built to share trendy color combinations between a group of designer friends. The collection scaled up and now being used daily as handy resources by thousands of people all over the world.\nPaletton, a design tool for creating color combinations that work together well. It was formerly known as Color Scheme Designer.\n5. How to Create a Graphic Design with Color Theory?\nFirst, you are going to fotor.com and choose your template by going to Design and choosing Poster found in the Marketing category.\nChoose your template from the side panel, I will be choosing this Black Friday sale poster!\nNext, let’s customize the design!\nFirst, the text, let’s Click on the title text and change the font face to Alibaba Sans Bold.\nI will also be changing the “When” and “Where” to Alibaba Sans Bold.\nDouble-click on the text to change what it says, I will be changing it to “bikes and accessories”\nNow onto the colors. Let’s change the yellow to a more outdoorsy green color.\nClick on the yellow header, in the upper left-hand corner choose the pastel green color.\nChange all yellow sections to this green color. You can even change the rippled texture in the bottom right-hand corner!\nOnce done, click on all the text and change them to white using the same method. In Fotor, everything is customizable by simply clicking on it, it’s that easy!\nFinally, let’s use complementary colors to make the bike stand out. In this case, we have a lot of green. The complementary color of green is red! So let’s click on the bike, and choose red for our third color box, changing the frame of the boke to red.\nFinish up by changing the rims of the bike to white, using the second color box.\nLastly, let’s save our image by Naming out file, choosing .PNG for out the file type and then choosing High for our quality! And you are all done!\nWhat are the 3 color schemes?\nComplimentary, split complementary and triadic are three of the most commonly used color schemes. It’s also best to choose one main color with other colors being supportive or accent colors to keep the color scheme from becoming too busy.\nWhat are good color combinations?\nComplementary colors will always combine to make a cohesive design. However, choosing a natural color like beige, black or white and then adding in an accent color will always work.\nWhat color matches with all colors?\nBlack and white will match with all colors and fit into any color combination. Choosing one color to be your accent color and then using black or white as your main colors will help the accent color pop more.\nWhat colors will be popular in 2020?\nSoft, rosy hues and muted natural greens and blues are the predicted trending colors for 2020. Earthy nutruelas will also be commonly featured in the upcoming year!\nWhat color catches the eye first?\nYellow is considered one of the most eye-catching and loudest of the colors even when muted due to it’s bright, sometimes neon-like, nature.\nColor is something so simple, that some people may take it for granted, but in reality, there’s a whole science to color and why we use it! Luckily for us, however, color doesn’t need to be hard and there are countless tools out there to help you match colors and find your perfect color scheme. The color wheel is one of the most simple yet most powerful! Remember there are no best color combinations for graphic design, so experiment and be creative!\nFotor is a free online picture editor and graphic designer, allowing you to use online photo editing tools, such as add filters, frames, text, stickers, and effects…and apply design tools to make creative photo designs and graphics. Online photoshop and graphic design software have never been so easy! Also, if you want to get more helpful and inspirational tips, please visit our blog home page to extend your reading."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:109b23a2-7bb7-4262-8e7e-ab0616ab4fef>","<urn:uuid:7b97ea1c-f55a-4c14-8b41-9b2efdec8a70>"],"error":null}
{"question":"How do prospect theory and behavioral incentives challenge the assumption that people make rational economic decisions?","answer":"Both prospect theory and behavioral incentives demonstrate that people don't behave as rational economic agents. Prospect theory shows that rather than calculating absolute utility, people evaluate options relative to reference points and are influenced by cognitive biases - they're risk-averse for gains but risk-seeking for losses. Similarly, real-world examples of incentives backfiring, like increased late pickups after nursery fines were introduced, show that people don't respond to economic motivators in purely rational ways. The effectiveness of incentives depends heavily on social and emotional factors rather than just economic calculations. This challenges traditional expected utility theory which assumes people always make optimal choices based on rational analysis of outcomes.","context":["What is it? Reward and punishment have long been used to direct behaviour. However, introducing extrinsic motivational consequences such as these can fundamentally change how we view a situation, in turn altering our motivation and subsequent action. As such, they must be carefully applied in order to be effective, and to avoid pitfalls which drive unintended behaviour.\nFor example, the introduction of a monetary fine for parents who are late to collect their children from nursery actually increased the rate of overdue pick-ups1. In this situation, the financial penalty transforms a typically social transaction into an economic one, and absolves parents of guilt by enabling them to “pay” for their tardiness.\nReports suggest economic fines for those travelling to holiday or second homes during lockdown have also been ineffective. So whilst it is natural to assume applying consequences always increase behaviour and aversive ones will decrease it, the above examples make clear that incentives and punishment need to carefully designed. It is essential for us to consider emotional and social factors, as well as economic and tangible ones.\nHow to use it:\n- To Apply or Not To Apply – dieters rewarded for losing weight with financial incentives often regain weight once a programme is stopped2. This is because external rewards can detract from more naturally occurring motivation, meaning when the incentive is removed, individuals are left with no reason to maintain behaviour3. Therefore, we should be mindful of applying incentives to situations in which they might damage pre-existing motivation, such as prosocial action like volunteering4.\n- Considering the Cause – we should segment different populations and identify their most compelling motivatiors dependent on the precise situation. For example, affluent second home owners are unlikely to be deterred from visiting their properties during lockdown by a £30 fine. However, identifying more salient punishments, such as social stigma, or public shaming, may be more effective. Tackling the largest group of perpetrators first will help shift social norms around undesirable action.\n- Reference Points – the value of something depends on where we see it from. Framing options in relation to a reference point (for example, in terms of the ‘cost of a cup of coffee’) makes it easier for us to compute costs and rewards5. As such, we can change the perception of punishment and reward by contextualizing them within different reference points. For example, comparing staying home during the relatively short period of lockdown to experiencing the rest of a lifetime after Covid-19.\n1.Levitt, S. D. & Dubner, S. Freakonomics. (B DE BOOKS, 2014).\n2.Cawley, J. & Price, J. A. A case study of a workplace wellness program that offers financial incentives for weight loss. J. Health Econ. 32, 794–803 (2013).\n3.Vansteenkiste, M., Niemiec, C. P. & Soenens, B. The Development Of The Five Mini-Theories Of Self-Determination Theory: A Historical Overview, Emerging Trends, and Future Directions. (2008). Doi:10.1108/S0749-7423(2010)000016a007\n4.Chakravarti, A. & Thomas, M. Why paying people to donate blood does not pay. 149–186 (2015). doi:10.1057/9781137466693_10\n5.Thornton, R. L. The demand for, and impact of, learning HIV status. Am. Econ. Rev. 98, 1829–1863 (2008).","What is Amos Tversky and Daniel Kahneman’s prospect theory? How does it explain human behavior?\nProspect theory is a theory in economics developed by Amos Tversky and Daniel Kahneman. It says that Utility depends on changes from one’s reference point rather than absolute outcomes. The theory suggests that people don’t always behave rationally.\nWe’ll cover what Kahneman’s prospect theory is, how it works, and how it challenges traditional utility theory.\nThe Prospect Theory of Kahneman\nBefore we detail prospect theory, let’s look at the theory it challenged, expected utility theory.\nTraditional Expected Utility Theory\nTraditional “expected utility theory” asserts that people are rational agents that calculate the utility of each situation and make the optimum choice each time.\nIf you preferred apples to bananas, would you rather have a 10% chance of winning an apple, or 10% chance of winning a banana? Clearly you’d prefer the former.\nThe expected utility theory explained cases like these, but failed to explain the phenomenon of risk aversion, where in some situations a lower-expected-value choice was preferred.\nConsider: Would you rather have an 80% chance of gaining $100 and a 20% chance to win $10, or a certain gain of $80?\nThe expected value of the former is greater (at $82) but most people choose the latter. This makes no sense in classic utility theory—you should be willing to take a positive expected value gamble every time.\nFurthermore, it ignores how differently we feel in the case of gains and losses. Say Anthony has $1 million and Beth has $4 million. Anthony gains $1 million and Beth loses $2 million, so they each now have $2 million. Are Anthony and Beth equally happy?\nObviously not – Beth lost, while Anthony gained. Puzzling with this concept led Kahneman to develop prospect theory.\nProspect Theory v. Expected Utility Theory\nThe key insight from the above example is that evaluations of utility are not purely dependent on the current state. Utility depends on changes from one’s reference point. Utility is attached to changes of wealth, not states of wealth. And losses hurt more than gains. This is Kahneman’s prospect theory definition.\nConsider that you probably don’t know your wealth to the nearest hundred, or even the nearest thousand. But the loss of $100—from an overcharge, or a parking ticket—is very acute. Isn’t this odd?\nProspect Theory Thought Experiments\nConsider these two prospect theory problems:\n1. You have been given $1,000. Which do you choose:\n- 50% chance to win another $1,000 and 50% chance to get $0, or\n- Get an additional $500 for sure\n2. You have been given $2,000. Which do you choose:\n- 50% chance to lose $1,000, and 50% chance to lose $0\n- Lose an additional $500 for sure\n(Shortform note: if you’re in the fortunate position of having enough wealth so that these numbers aren’t very compelling to you, try multiplying them by 10x or even 100x, to get to a place where you think hard about the outcome.)\nNote these are completely identical problems. In both cases, you have the certainty of ending with $1,500, or equal chances of having $1,000 or $2,000. Yet you probably chose different answers.\nYou were probably risk-averse in problem 1 (choosing the sure bet), and risk-seeking in problem 2 (choosing the chance). This is because your reference points were different – from one point you were gaining, and in the other you were losing. And losses hurt more than gains, so you try to protect yourself against a loss.\nProspect theory seeks to explain all of the above.\nProspect Theory in 3 Points\n1. When you evaluate a situation, you compare it to a neutral reference point.\n- Usually this refers to the status quo you currently experience. But it can also refer to an outcome you expect or feel entitled to, like an annual raise. When you don’t get something you expect, you feel crushed, even though your status quo hasn’t changed.\n2. Diminishing marginal utility applies to changes in wealth (and to sensory inputs).\n- Going from $100 to $200 feels much better than going from $900 to $1,000. The more you have, the less significant the change feels.\n3. Losses of a certain amount trigger stronger emotions than a gain of the same amount.\n- Evolutionarily, the organisms that treated threats more urgently than opportunities tended to survive and reproduce better. We have evolved to react extremely quickly to bad news.\nThe master image of prospect theory is this:\nThe graph shows how psychological value changes according to the change in dollar amount. The middle of the two axes is the reference point—with no change, there is no psychological value. Psychological value is positive with positive dollar amounts (when you gain money) and negative with negative dollar amounts.\nThis graph was established through a host of experiments investigating how people perceive gains and losses, and how they trade off decisions like Anthony and Beth above. This demonstrates prospect theory.\nNote two important properties of the prospect theory curve shown:\n- Diminishing marginal value: the curve isn’t a straight line on either end. The more money you gain, the less value each increment of money gives you. The same is true of losing money—losing $100 causes less anguish than 10x the anguish of losing $10.\n- Loss aversion: the curve on the left of the y-axis has a steeper slope than the curve to the right of the y-axis. This signifies that the psychological pain of losing $100 is greater in intensity than the joy in gaining $100. Losses hurt more than gains do.\nPeople have different curves, depending on their sensitivity to loss aversion. The ratio of slopes is called the “loss aversion ratio.” For most people, the ratio ranges between 1.5 to 2.5 – people would have to gain $200 to offset a loss of $100. In contrast, professional risk takers, like stock traders, are more tolerant of losses and have a lower loss aversion ratio, possibly because they have psychologically adapted to large fluctuations.\nRevisiting Anthony and Beth\nLet’s revisit the scenario with Anthony and Beth. Anthony has $1 million and Beth has $4 million. They both have the following choices:\n- 50% chance of ending with $1 million or 50% chance of ending with $4 million\n- 100% chance of ending with $2 million\nAnthony chooses the option 2, while Beth chooses option 1.\nProspect Theory now explains why. From the curve above, see that:\n- For gains, 100% of 100 is larger than 50% of 200, because of how the curve flattens with more money. People are risk averse to get gains. Anthony would rather lock in a certain gain than risk getting nothing.\n- For losses, 50% of -200 is less negative than 100% of -100. People are risk seeking to avoid losses. Beth would rather risk losing more if there’s a chance she keeps her money, than to have a certain loss.\nWhile Bernoulli presented utility as an absolute logarithmic scale starting from 0, prospect theory calibrates the curve to the reference point. People feel differently depending on whether they’re gaining or losing money.\nProspect Theory Implications\nThere are a few practical implications of prospect theory.\nConsider which is more meaningful to you:\n- Going from a 0% chance of winning $1 million to 5% chance\n- Going from a 5% chance of winning $1 million to 10% chance\nMost likely you felt better about the first than the second. The mere possibility of winning something (that may still be highly unlikely) is overweighted in its importance. We fantasize about small chances of big gains. We obsess about tiny chances of very bad outcomes. This is an effect of prospect theory.\nNow consider how you feel about these options on the opposite end of probability:\n- In a surgical procedure, going from a 90% success rate to 95% success rate.\n- In a surgical procedure, going from a 95% success rate to 100% success rate\nMost likely, you felt better about the second than the first. Outcomes that are almost certain are given less weight than their probability justifies. 95% success rate is actually fantastic! But it doesn’t feel this way, because it’s not 100%. This is another effect of prospect theory.\nStatus Quo Bias\nYou like what you have and don’t want to lose it, even if your past self would have been indifferent about having it. For example, if your boss announces a raise, then ten minutes later said she made a mistake and takes it back, this is experienced as a dramatic loss. However, if you heard about this happening to someone else, you likely would see the change as negligible. Again, an effect of prospect theory.\nThe context in which a decision is made makes a big difference in the emotions that are invoked and the ultimate decision. Even though a gain can be logically equivalently defined as a loss, because losses are so much more painful, different framings may feel very different.\nFor example, a medical procedure with a 90% chance of survival sounds more appealing than one with a 10% chance of mortality, even though they’re identical.\nThe Fourfold Pattern of Prospect Theory\nPutting prospect theory into another summary form, here’s a 2×2 grid showing how people feel about risk in different situations. The upper left quadrant shows how people feel about a high probability of a gain, the upper right shows how people feel about a high probability of a loss, and so on.\n|HIGH PROBABILITY |\n|95% chance to win $10,000 vs 100% chance to win $9,500|\nFear of disappointment\nAccept unfavorable settlement\nExample: Lawsuit settlement\n|95% chance to lose $10,000 vs 100% chance to lose $9,500|\nHope to avoid loss\nReject favorable settlement\nExample: Hail mary to save failing company\n|5% chance to win $10,000 vs 100% chance to win $500|\nHope of large gain\nReject favorable settlement\n|5% chance to lose $10,000 vs 100% chance to lose $500|\nFear of large loss\nAccept unfavorable settlement\nPutting it all together – two prospect theory factors are at work in evaluating gambles:\n- Diminishing sensitivity, so that more of the same causes less of a change in psychological value (in the graph, the slope decreases as you move further from the y-axis)\n- Inaccurate weighting of probabilities at the edges\nIn the first row of this prospect theory table, the two factors work in the same direction:\n- In the upper right quadrant, diminishing sensitivity causes loss aversion: a sure loss is painful. On the prospect theory graph, 100% of -900 is more negative than 90% of -1,000.\n- Making matters worse is the inaccurate weighting of probabilities. While 100% is weighted at 100, 90% is weighted only at 71. This certainty effect causes the 100% loss to feel much more painful than a very high chance of loss.\n- Similarly, in the positive situation on the upper right, the diminishing sensitivity makes a certain lower gain more attractive, and the certainty effect reduces the attractiveness of the gamble.\nIn the bottom row, the two factors work in opposite directions:\n- In the lower left corner, diminishing sensitivity still makes the sure gain more attractive than the chance of a gain. But the overweighting of low probabilities overcomes this effect, so people in this quadrant tend to choose the 5% gamble.\nKahneman notes that many human tragedies happen in the upper right quadrant. People who are between two very bad options take desperate gambles, accepting a high chance of making things worse to avoid a certain loss. The certain large loss is too painful, and the small chance of salvation too tempting, to decide to cut one’s losses.\nMiscellaneous Implications of Prospect Theory\nOpposing Incentives in Litigation\nLitigation is a nice example of where all of the above can cause tumult:\n- Plaintiffs file frivolous lawsuits. They have a low chance of winning, and they overweight the probability of winning (lower left corner in the fourfold pattern).\n- Defendants prefer settling frivolous lawsuits to lower the risk of a more expensive loss (lower right corner). Yet if the defendant does this habitually for each lawsuit, it invites more frivolous lawsuits, and can thus be costly in the long run.\nWe can also reverse the situations:\n- A plaintiff has a strong case and has an almost certain chance to win, but wants to avoid the small chance of a loss. She is prone to risk-aversively take a settlement (upper left corner).\n- The defendant knows she’s likely to lose, but has a small chance of winning. She’s willing to drag the case on, because the certain loss from a settlement with the plaintiff is painful and the small chance of winning is gratifying (upper right corner).\n- In this case, the defendant holds the stronger hand, and the plaintiff will settle for less than the case strength suggests.\nWhy did it take so long for someone to notice the problems with Bernoulli’s conception of utility? Kahneman notes that once you have accepted a theory and use it as a tool in your thinking, it is very difficult to notice its flaws. Even if you see inconsistencies, you reason them away, with the impression that the model somehow takes care of it, and that so many smart people who agree with your theory can’t all be wrong.\nIn Bernoulli’s theory, even when people noticed inconsistencies, they tried to bend utility theory to fit the problem. Kahneman and Tversky instead made the radical choice to abandon the idea that people are rational decision-makers, and instead took a psychological bent that assumed foibles in decision-making.\nBlind Spots in Prospect Theory\nProspect theory has holes in its reasoning as well. Kahneman argues that it can’t handle disappointment – that not all zeroes are the same. Consider two scenarios:\n- 1% chance to win $1 million and 99% chance to win nothing\n- 99% chance to win $1 million and 1% chance to win nothing.\nIn both these cases, prospect theory would assign the same value to “winning nothing.” But losing in case 2 clearly feels worse. The high probability of winning has set up a new reference point—possibly at say $800k.\nProspect theory also can’t handle regret, in which failing to win a line of gambles causes losses to become increasingly more painful.\nPeople have developed more complicated models that do factor in regret and disappointment, but they haven’t yielded enough novel findings to justify the extra complexity.\nConclusions of Prospect Theory\nWe’ve shown that humans are not rational in the decisions they make. Unfortunately, when society believes in human rationality, it also promotes a libertarian ideology in which it is immoral to protect people against their choices. “Rational people make the best decisions for themselves. Who are we to think we’re better?” This leads to beliefs like:\n- People know what they’re doing when they choose not to save for retirement or eat to obesity.\n- People who are addicted are rational agents who strongly prefer immediate gratification and accept future addiction as a consequence.\nThis belief in rationality also leads to a harsher conclusion: people apparently deserve little sympathy for putting themselves in worse situations. Elderly people who don’t save get little more sympathy than people who complain about a bill after ordering at a restaurant. Rational agents don’t make mistakes.\nBehavioral economists believe people do make mistakes and need help to make more accurate judgments. They believe freedom is a virtue worth having, but it has a cost borne by individuals who make bad choices (that are not completely their fault) and by a society that feels obligated to help them.\nThe middle ground might be libertarian paternalism, in which the state nudges people to making better decisions and give the freedom for people to opt out. This includes nudges for retirement savings, health insurance, organ donation, and easy-to-understand legal contracts.\n———End of Preview———\nLike what you just read? Read the rest of the world's best summary of \"Thinking, Fast and Slow\" at Shortform. Learn the book's critical concepts in 20 minutes or less.\nHere's what you'll find in our full Thinking, Fast and Slow summary:\n- Why we get easily fooled when we're stressed and preoccupied\n- Why we tend to overestimate the likelihood of good things happening (like the lottery)\n- How to protect yourself from making bad decisions and from scam artists"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"chinese_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:0b70f0a8-d8d1-4c28-8f25-34a4de5d98af>","<urn:uuid:4b723642-1f7c-4fa9-a84d-3988159e51ec>"],"error":null}
{"question":"Can you explain what happens in stress-strain curve when testing glass rod under tension? Want to understand mechanical behavior.","answer":"When a glass rod undergoes tensile testing, its stress-strain curve exhibits a sudden break. This occurs due to brittle failure of the material.","context":["Quiz: Mechanical Engineering\nExam: UPPSC AE\nTopic: Manufacturing Engineering\nEach question carries 3 mark.\nNegative marking: 1 mark\nTime: 10 Minute\nQ1. The stress strain curve for a glass rod during tensile test would exhibit\n(a) A straight line\n(b) A parabola\n(c) A sudden break\n(d) None of these\nQ2. Ability of a material to exhibit considerable elastic recovery on release of load, is known as:\nQ3. Martensite is a supersaturated solution of carbon in:\n(a) Alpha iron\n(b) Beta iron\n(c) Gamma iron\n(d) Delta iron\nQ4. The crystal structure of Chromium (Cr) at room temperature is\n(a) Body-Centered Cubic (BCC)\n(b) Face-Centered Cubic (FCC)\n(c) Hexagonal Close-Packed (HCP)\n(d) Simple Cubic (SC)\nQ5. Chills are:\n(a) Mixed in sand to enhance its collapsibility\n(b) Metallic objects to provide very high heat extraction capability of sand mould\n(c) Metallic objects to provide strength of sand mould\n(d) Metallic objects to provide very low heat extraction capability of sand mould\nQ6. In an Oxy-acetylene welding, the ratio of oxygen to acetylene in reducing flame is\n(a) 0.5: 1\n(b) 1: 3.1\n(c) 1: 1.2\n(d) 1: 1\nQ7. The merchant circle is used to\n(a) Improve the quality of a product\n(b) Find the principal stress and principal strain\n(c) Fix the price of a product in the market\n(d) Establish the shear angle relationship in machining\nQ8. Oblique cutting system is also known as:\n(a) One-dimensional cutting system\n(b) Two-dimensional cutting system\n(c) Three-dimensional cutting system\n(d) None of these\nQ9. Which of the following lathes is suitable for a small engineering workshop involved in repair work?\n(a) Center lathe\n(b) Tool-room lathe\n(c) Special purpose lathe\n(d) Capstan lathe\nQ10. It is desired to perform the operations like drilling, reaming, counter-boring etc. on a work-piece. Which of the following machines will be used?\n(a) Sensitive drilling machine\n(b) Radial drilling machine\n(c) Gang drilling machine\n(d) Multiple spindle drilling machine\nSol. The stress -strain curve for a glass rod during tensile test would exhibit a sudden break due to brittle failure.\nSol. Resilience is the ability of the material to exhibit considerable elastic recovery on release of load.\nSol. A meta stable phase of steel formed by transformation of austenite below a temperature 240 Deg C is known as martensite. It is an interstitial solid solution of carbon in α-iron.\nSol. Chromium has Body-centered Cubic structure at room temperature. BCC structure has 2 atoms in a unit cell.\nSol. There are certain areas in the casting that needs high heat extraction of heat due to variation in geometry of casting. For this purpose, we use chills that provide critical cooling rate.\n1) Neutral flame\nRatio of O2 /C2H2 is 1:1 =2.5 mole oxygen/1 mole C2H2\n2) Carburizing flame\nRatio less than 3.071:1\n3) Oxidizing flame\nRatio more than 3.071:1\nSol. According to Merchant circle relationship,\nHere, ϕ is shear angle, α is rake angle and β is friction angle.\nSol. When the cutting edge of the tool is inclined or making an angle (other than 90°) with the direction of feed, then it is called oblique cutting. This cutting is also called three-dimensional cutting because of force components in all three directions.\nSol. For small engineering workshops, which are involved in repair works; center lathe is suitable. The main common tasks that can be performed on center lathe:\nSol. When we place a number of single spindle drilling machine columns together on a common work-table, then the machine is called gang drilling machine."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:6c7298a5-7dfe-410e-8e52-44b40598e547>"],"error":null}
{"question":"How do the diagnostic approaches and treatment methods differ between sevoflurane-induced seizures and focal epilepsy with ictal abdominal pain when establishing a definitive diagnosis?","answer":"For sevoflurane-induced seizures, diagnosis is primarily based on the temporal relationship with anesthesia emergence and is treated acutely with medications like propofol, midazolam, and phenytoin, often requiring intensive care management. For focal epilepsy with ictal abdominal pain, diagnosis requires a more complex approach, including exclusion of common gastrointestinal causes, documentation of paroxysmal abdominal pain, and confirmation through EEG abnormalities, particularly during 24-hour monitoring. Treatment involves long-term antiepileptic medications, with carbamazepine showing effectiveness. In sevoflurane cases, future avoidance of the agent is recommended, while in focal epilepsy, ongoing anticonvulsant therapy is typically needed for seizure control.","context":["anesthesia, emergence, focal, seizures, sevoflurane\nR Roy. Repetitive Focal Seizures after Sevoflurane Anesthesia. The Internet Journal of Anesthesiology. 2008 Volume 20 Number 2.\nPurpose: Sevoflurane induced seizure during emergence from anesthesia that have been described were mainly generalized tonic-clonic in nature. This is the first time a case of focal seizure like activity during emergence from sevoflurane anesthesia is being reported.Clinical features: We describe a patient who developed several episodes of focal seizures like activity during emergence from sevoflurane anesthesia. The seizures subsided after treatment with intravenous propofol, midazolam and phenytoin and patient required intensive care management.Conclusion: The report emphasizes that sevoflurane is capable of producing both generalized and focal seizures during emergence from anaesthesia.\nSevoflurane has gained popularity as a inhalational agent of choice because of its rapid induction and emergence from anesthesia. We report a rare and serious adverse effect of sevoflurane i.e. sevoflurane induced seizures and that too during emergence from anesthesia. Till date all the seizure like activity reported during emergence with sevoflurane have been generalized tonic-clonic in nature. There has been no reports of focal seizure like activity with administration of sevoflurane during emergence from anesthesia with all the available literatures. This is the first time, we are reporting sevoflurane induced focal seizure during emergence from anesthesia in a previously healthy female. Sevoflurane-induced seizures are most often described during mask induction of anesthesia when high concentrations of the volatile agent are administered concomitant with alveolar hyperventilation . The occurrence of seizure like activity during emergence from sevoflurane anesthesia has been rarely reported .\nA 20-year-old female was admitted to the plastic surgery unit with a hyperpigmented\nnaevus over the bridge of the nose with extension on both side cheek. She was planned\nfor excision of the naevus and forehead rotation flap. A thorough pre-operative\nevaluation was done prior to the surgery. During the preoperative evaluation, the patient\ndenied any history of previous anesthesia, drug allergies, neurological disease, or a\nseizure disorder. Preoperative investigations were within normal limits.\nThe patient was premedicated with Inj. Glycopyrolate 0.2mg, Inj. Pentazocine 0.5\nmg/kg, Inj.Midazolam 1mg and Inj.Diclofenac 75mg i.m. The patient was preoxygenated and\nanesthesia was induced with Inj.Propofol 2mg/kg and Inj.Vecuronium 0.1mg/kg till the\nloss of eyelash reflex.. The patient was ventilated with oxygen, nitrous oxide and sevoflurane\n1% for five minutes and intubated orally with 7.0mm flexometallic cuffed endotracheal\ntube. Anesthesia was maintained with sevoflurane at an inspired concentration of\napproximately 1% with mixture of N2O/O2 50:50 (3 liters/min). The intraoperative\nmonitoring included NIBP, ECG, SPO2 and EtCO2. Ventilation was adjusted to maintain\nnormocarbia. The surgery lasted for three hours. All the vitals parameters were normal\nthroughout the surgery. Arterial oxygen saturation determined using pulse oximetry\nremained greater than 98% during the intraoperative course. No episodes of hypoxemia\nSevoflurane and nitrous oxide was discontinued upon completion of the procedure and neuromuscular blockade was reversed. Suddenly the patient started having focal seizures involving the left upper extremity and left side face lasting 30- 40seconds. Inj.Propofol 20mg IV was administered with immediate cessation of seizure activity. Patient blood pressure dropped to 80/40 mmHg. 6mg of inj. Ephedrine was given with restoration of blood pressure. The patient was shifted to intensive care unit (ICU). During transportation she had another episode of similar seizure lasting around 30seconds. The patient was awake, confused and slow in responding to oral commands in between the convulsive episodes. On arrival in the ICU she had another episode of similar seizures with hypotension. A bolus dose of midazolam 2mg i.v was given to suppress the seizures. Inj. Noradenaline was started to correct the hypotension. She was put on assisted ventilation. After half hour the blood pressure stabilized and she was given a loading dose of phenytoin 10 mg • kg –1 i.v and maintenance phenytoin therapy (1 mg • kg –1 every eight hours) initiated. An initial arterial blood gas analysis was normal with adequate oxygenation. Laboratory analysis revealed normal plasma electrolyte (Na + = 135 mEq • L –1 ; K + = 4.8 mEq • L –1 ; Cl – = 105 mEq • L –1 ), blood glucose (110 mg • dL –1 ), and creatinine (0.9 mg • dL –1 ) concentrations. Patient history was reconfirmed with her parents who gave no history of seizure activity earlier neither any family history of seizure disorder. No further seizure like activity occurred and she was haemodynamically stable. She was weaned from mechanical ventilation the following morning. After extubation, the patient displayed normal mental and neurological status and reported no recollection of perioperative events. An EEG was done the next morning, which showed normal electrical activity. Oral Phenytoin was continued in the postoperative period and continued for four weeks. She underwent flap revision under local anesthesia, which was uneventful. The patient was discharged on the 20 th postoperative day. The patient has experienced no further seizure-like activity to date. She was also given a medical alert card regarding avoidance of sevoflurane for future anesthetics.\nSevoflurane propensity to cause epileptiform EEG activity and produce clinical evidence of seizures (e.g., isolated clonus with or without tremor, frank tonic-clonic motor activity) in patients with or without a history of epilepsy has been recognized increasingly  Jaaskelainen et al concluded that sevoflurane consistently produces epileptiform discharges and is dose dependently epiletogenic at surgical level of Anesthesia . Rewari and Sethi reported a recurrence of focal seizure activity in an infant with past history of focal seizures during induction of anesthesia with Sevoflurane .\nSeizures like activity have been rarely reported during emergence from sevoflurane anesthesia. Mohanram et al reported a case of repeated generalized seizure-like activity without any haemodynamic changes during emergence from anesthesia which subsided after phenytoin therapy . Hilty and Drummond reported two consecutive episodes of tonic-clonic activity during emergence from sevoflurane anesthesia lasting 30 seconds that was abolished after phentyoin therapy . Terasako and Ishii reported a generalized clonic and tonic seizures like movement lasting 40 sec during emergence which necessitated no therapy . Singh M et al reported postoperative myoclonic seizures lasting 20-30 seconds that was abolished after sodium valproate . Many of the features associated with our patient presentation were similar to those earlier reported in that the seizure like activity was recurrent in nature and occurred on emergence. The focal nature of seizure like activity and associated haemodynamic collapse was the novel feature of our case. Our patient had no previous history of seizure disorder and there were no peri-operative events such as hypoxia, hypocarbia, hypoglycemia or electrolyte imbalances,neither any pro-convulsive drug was given intraoperatively which could have lowered the seizure threshold. The focal seizure like activity associated were observed during emergence from anesthesia and therefore were attributed to sevoflurane. The current case report and those reported earlier warrant a thorough investigation into the etiology of seizure like activity during emergence from sevoflurane anesthesia.","- Case report\n- Open Access\nFocal epilepsy with ictal abdominal pain: a case report\nItalian Journal of Pediatrics volume 39, Article number: 76 (2013)\nFocal epilepsy with ictal abdominal pain is an unusual partial epilepsy characterized by paroxysmal episodes of abdominal or visceral pain, disturbance of awareness and electroencephalographic abnormalities. We describe a new case of ictal abdominal pain in which gastrointestinal complaints were the only manifestation of seizures and review the previously described pediatric patients. In our patient clinical findings, ictal EEG abnormalities, and a good response to antiepileptic drugs allowed us to make a diagnosis of focal epilepsy with ictal abdominal pain. This is a rare epileptic phenomenon that should be suspected in patients with unexplained paroxysmal abdominal pain and migraine-like symptoms. We suggest that, after the exclusion of more common etiologies, focal epilepsy with ictal abdominal pain should be considered in patients with paroxysmal abdominal pain and ictal EEG abnormalities.\nRecurrent episodes of abdominal pain are common in children and adults. Several pathological conditions can lead to paroxysmal gastrointestinal symptoms, such as porphiria, cyclical vomiting, intestinal malrotation, peritoneal bands, and abdominal migraine . Psychological and emotional factors may also play an important role in some patients with gastrointestinal disorders. However, in a number of patients the episodic nature of abdominal pain can be suggestive for a diagnosis of epilepsy . Epileptiform EEG abnormalities, loss or alteration of consciousness, and a good response to antiepileptic drugs are other features that can lead to a diagnosis of focal epilepsy with ictal abdominal pain [2, 3]. We describe one child affected by epilepsy which had recurrent and severe abdominal pain as the only manifestation of epileptic seizures.\nAn 8-year-old boy was born at 39 weeks of gestation by selective cesarean section. The pregnancy was complicated by a sudden reduction in fetal heart rate. All developmental milestones were regularly achieved. There was no family history of epilepsy. The boy experienced recurrent episodes of abdominal pain since about 6 months of age. He described the pain as “a sword that pierces my belly”, localized mainly in the epigastric region and its duration varied from a few minutes (more frequently) to 1 hour, with a frequency of 5–8 episodes per day. The intensive abdominal pain was almost always associated with pallor and nausea, but not accompanied by scream or cry. The attacks were sudden in onset and had spontaneous resolution. There was no impairment of consciousness, also in longer episodes, and he never had convulsions; the paroxysms were followed by increased sleep. He underwent a negative abdominal investigation including complete blood count, stool examinations for ova and parasites, abdominal ultrasound and upper gastrointestinal endoscopy. Physical and neurological examinations were normal.\nInterictal EEG during wakefulness and sleep displayed bilateral spikes and diphasic sharp-waves localized over the temporal leads with a marked increase in frequency during drowsiness. A 24-hours EEG recording showed several bilateral synchronous and asynchronous temporal spikes during wakefulness and nocturnal sleep. At 9:30 in the morning a seizure characterized by severe abdominal pain in the epigastric region with nausea and pallor was recorded. The ictal EEG showed rhythmic spikes on the centro-temporal regions (Figure 1). Magnetic resonance imaging of the brain was normal. The patient started treatment with Carbamazepine (CBZ) (20 mg/kg/day) with a progressive decrease in seizure frequency. At the last follow-up, when he was 9-years old, he was seizure free.\nEpigastric sensations are frequent symptoms in patients with partial epilepsy and may include abdominal pain, nausea, vomiting and hunger, and have been reported to be the most common aura in temporal lobe epilepsy [3–5]. Painful epileptic auras were reported in 4.1% of 25 patients with focal epilepsy by Nair et al. . Abdominal pain was present in 5% of all abdominal auras in temporal lobe epilepsy and 50% in frontal lobe epilepsy . However, gastrointestinal complaints, in particular abdominal pain, may be the only manifestation of epileptic activity [1, 3, 4, 7]. Unexplained paroxysmal gastrointestinal complaints, impairment of consciousness, and focal abnormal EEG are the main criteria to establish a diagnosis of focal epilepsy with ictal abdominal pain, but not all the criteria need to be present in each case [2, 3, 6]. In addition, a variety of migraine-like disturbances such as nausea, headache, dizziness, and visual hallucinations may be associated with pain during the attacks . When the migraine-like symptoms are present it is often difficult to differentiate focal epilepsy with ictal abdominal pain from migraine or other neurological disorders, such as Panayiotopoulos syndrome. The abrupt onset, the spontaneous resolution, and the relatively short duration of episodes may be helpful for a correct and early diagnosis of focal epilepsy with ictal abdominal pain. Another helpful distinguishing feature of epilepsy with severe abdominal pain could be the localization of ictal pain, that is most commonly periumbilical or upper abdominal and rarely spreads to involve other body parts, such as in our patient [1–4].\nEEG abnormalities have been reported in most patients with focal epilepsy and ictal abdominal pain [1, 4]. Few reports described ictal EEGs: during the seizure the EEG often shows a runs of high voltage slow waves and generalized spike and wave discharges [2–4, 8]. In our patient, 24-hours EEG was suggestive of a focal onset, as in two reports that showed clear focal EEG changes over the left hemisphere during an episode of abdominal pain [9, 10]. Table 1 shows the clinical characteristics of our patient and the previous pediatric cases described in literature (Table 1) [1–5, 7–13].\nThe pathophysiology of focal epilepsy with ictal abdominal pain remains unknown. Abdominal sensations reproduced by stimulating the insula and sylvian fissure, suggest that these areas may have an important role in explaining the origin of focal epilepsy with ictal abdominal pain . Phan et al. , reported an unusual case of ictal abdominal pain occurring in the setting of parietal lobe haemorrhage and suggested a possible role of the somatosensory area in pain perception. Supplementary motor area was considered as another possible location for abdominal pain. Occasionally focal epilepsy with ictal abdominal pain has been related to brain tumors and brain disorders [2, 8]. Previous reports on ictal abdominal pain have shown right parieto-occipital encephalomalacia, biparietal atrophy and bilateral perisylvian polymicrogyria .\nIn conclusion, our patient showed recurrent attacks of severe abdominal pain as the only manifestation of epileptic seizure. Focal epilepsy with ictal abdominal pain is a rare epileptic phenomenon that should be suspected in patients with unexplained paroxysmal abdominal pain and migraine-like symptoms. The correct diagnosis at the onset may be difficult to establish; in these cases prolonged EEG recordings with 24-hours monitoring must be considered to facilitate the clinical diagnosis.\nWritten informed consent was obtained from the patient’s parents for the publication of this report.\nZdraveska N, Kostovski A: Epilepsy presenting only with severe abdominal pain. J Pediatr Neurosci. 2010, 5 (2): 169-70. 10.4103/1817-1745.76123. 10.4103/1817-1745.76123\nFranzon RC, Lopes CF, Schmutzler KM, Morais MI, Guerreiro MM: Recurrent abdominal pain: when should an epileptic seizure be suspected?. Arq Neuropsiquiatr. 2002, 60 (3-A): 628-30.\nGarcía-Herrero D, Fernández-Torre JL, Barrasa J, Calleja J, Pascual J: Abdominal epilepsy in an adolescent with bilateral perisylvian polymicrogyria. Epilepsia. 1998, 39 (12): 1370-4. 10.1111/j.1528-1157.1998.tb01340.x.\nDutta SR, Hazarika I, Chakravarty BP: Abdominal epilepsy, an uncommon cause of recurrent abdominal pain: a brief report. Gut. 2007, 56 (3): 439-41. 10.1136/gut.2006.094250.\nYoung GB, Blume WT: Painful epileptic seizures. Brain. 1983, 106 (Pt 3): 537-54.\nNair DR, Najm I, Bulacio J, Lüders H: Painful auras in focal epilepsy. Neurology. 2001, 57 (4): 700-2. 10.1212/WNL.57.4.700.\nHasan N, Razzaq A: Abdominal epilepsy. J Coll Physicians Surg Pak. 2004, 14 (6): 366-7.\nSiegel AM, Williamson PD, Roberts DW: Localized pain associated with seizures originating in the parietal lobe. Epilepsia. 1999, 40 (7): 845-855. 10.1111/j.1528-1157.1999.tb00790.x.\nMitchell WG, Greenwood RS, Messenheimer JA: Abdominal epilepsy. Cyclic vomiting as the major symptom of simple partial seizures. Arch Neurol. 1983, 40 (4): 251-2. 10.1001/archneur.1983.04050040081017.\nDouglas EF, White PT: Abdominal epilepsy-a reappraisal. J Pediatr. 1971, 78 (1): 59-67. 10.1016/S0022-3476(71)80264-0.\nYingkun F: Abdominal epilepsy. Chin Med J. 1980, 93 (3): 135-148.\nSinghi PD, Kaur S: Abdominal epilepsy misdiagnosed as psychogenic pain. Postgrad Med J. 1988, 64 (750): 281-282. 10.1136/pgmj.64.750.281.\nAgrawal P, Dhar NK, Bhatia MS, Malik SC: Abdominal epilepsy. Indian J Pediatr. 1989, 56 (4): 539-541. 10.1007/BF02722438.\nPhan TG, Cascino GD, Fulgham J: Ictal abdominal pain heralding parietal lobe haemorrhage. Seizure. 2001, 10 (1): 56-9. 10.1053/seiz.2000.0472.\nThe authors declare that they have no financial and non-financial competing interests.\nCC (Medical Doctor) drew the first draft with the assistance and contribution of NEM (Medical Doctor); DR (Medical Doctor) reviewed relevant articles on the literature under the supervision of PC (Director of the Department of Pediatric Neuroscience Unit); PC revised the final draft. All authors contributed to the intellectual contents and approved the final version.\nAuthors’ original submitted files for images\nBelow are the links to the authors’ original submitted files for images.\nAbout this article\nCite this article\nCerminara, C., El Malhany, N., Roberto, D. et al. Focal epilepsy with ictal abdominal pain: a case report. Ital J Pediatr 39, 76 (2013). https://doi.org/10.1186/1824-7288-39-76\n- Temporal Lobe Epilepsy\n- Focal Epilepsy\n- Severe Abdominal Pain\n- Gastrointestinal Complaint"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:47e2e092-b75b-46e3-8b8d-8f3bb1ef902e>","<urn:uuid:cccf4014-f5cc-4604-b8c7-6f5f2eab220d>"],"error":null}
{"question":"How does Ceres' classification compare historically between its discovery in 1801 and modern times? Please provide a detailed analysis of its changing status.","answer":"Ceres has undergone several major classification changes since its discovery. When Giuseppe Piazzi discovered it on January 1, 1801, it was initially considered a planet. At the time, it was found between Mars and Jupiter where astronomers believed there was a 'missing planet.' Later, upon discovering it was part of a larger group of similar bodies, it was reclassified as an asteroid for over 150 years. Finally, in 2006, the International Astronomical Union (IAU) officially classified Ceres as a dwarf planet. Ceres is the largest body in the asteroid belt with a diameter of about 950 km, containing approximately one-third of the belt's total mass. Its size is significant enough that its own gravity has pulled it into a spherical shape, which is one of the criteria for dwarf planet classification under the IAU's 2006 definition.","context":["The IAU General Assembly meets every three years. on August 24, 2006 the Assembly passed a resolution that redefined the\ndefinition of a planet, which classified Ceres, 2003 UB313 and Pluto as dwarf planets, and reduced the number of planets in\nthe solar system to 8.\nThe details are presented below\nKuiper Belt Objects(KBOs)/Trans Neptunian Objects Due to its small size and distant location, some astronomers argue that Pluto isn't a planet at all.\nThey think of it as giant asteroid, or comet. If so, Pluto would then become the largest known member of the Kuiper belt, a disc full of icy bodies that swarm outside the planets.\nThere is a disk-shaped region called Kuiper belt, past the orbit of Neptune roughly 450 crore km to 1500 crore kms from the\nSun. It is considered to be the source of the short-period comets. Astronomers also call the objects in this belt\nTrans-Neptunian objects. There are estimated to be perhaps 70,000 Trans Neptunian Objects (TNOs), each at least 100 km\nacross. Some astronomers for a long time kept arguing that Pluto & Charon should be considered to be large Kupier belt\nAs a growing area of the sky is searched for Kupier belt objects, astronomers are beginning to detect the larger objects.\nThey have over 1000 km diameter. The first such object discovered was Varuna, measured in 2001. Later they announced the\ndiscovery of Quaoar, Sedna, 2003 UB313 and many more. About 10 such objects have been found so far. Of these 2003 UB313 is\nsuspected to be bigger than Pluto. It is at a distance of 1770 crore km from the Sun.\nPluto’s classification as a \"dwarf planet\"\nThere has recently been considerable controversy about the classification of Pluto. It was classified as the ninth planet\nshortly after its discovery and remained so for 75 years. But on 2006 Aug 24 the IAU decided on a new definition of \"planet\"\nwhich does not include Pluto. Pluto is now classified as a \"dwarf planet\", a class distict from \"planet\".\nThe International Astronomical Union (IAU) unites national astronomical societies from around the world. It is\ninternationally recognized by astronomers as the official authority responsible for naming stars, planets, asteroids, and\nother celestial bodies and phenomena, and is the official body of astronomy.\nThe XXVIth General Assembly of the International Astronomical Union was held during August 14 to August 25, 2006 in Prague,\nCzech Republic. The IAU General Assembly meets every three years. on August 24, 2006 the Assembly passed a resolution that\nredefined the definition of a planet, which classified Ceres, 2003 UB313 and Pluto as dwarf planets, and reduced the number\nof planets in the solar system to 8.\nThe Eight Official Planets\nThe 2006 redefinition of \"planet\" by the International Astronomical Union (IAU) states that, in the solar system, a planet is\nan astronomical object if it satisfies the following conditions\n1.The object must be in orbit around a star, but not be a star itself.\n2.The object must be massive enough to be a sphere by its own gravitational force. More specifically, its own gravity should\npull it into a shape of hydrostatic equilibrium.\n3.It must have cleared the neighborhood around its orbit. This means nothing of comparable mass may orbit near the planet.\nA non-satellite body fulfilling only the first two criteria is classified as a \"dwarf planet\", whilst a non-satellite body\nfulfilling only the first criterion is termed a \"small solar system body\" (SSSB).\nAccording to the definition, there are currently eight planets and three dwarf planets known in the solar system.\nThe three dwarf planets are : Pluto, Ceres and 2003 UB 213 (xena)\nCeres is a dwarf planet in the asteroid belt. Its name is derived from the Roman goddess Ceres - the goddess of growing\nplants and of motherly love. It was discovered on January 1, 1801, by Giuseppe Piazzi. With a diameter of about 950 km, Ceres\nis by far the largest and most massive body in the asteroid belt: it contains approximately a third of the belt's total mass.\nThe classification of Ceres has changed more than once. At the time of its discovery it was considered a planet, but upon the\nrealization that it represented the first of a class of many similar bodies, it was reclassified as an asteroid for over 150\nThe New Solar System with Eight Planets and Three Dwarf Planets","Books & Music\nFood & Wine\nHealth & Fitness\nHobbies & Crafts\nHome & Garden\nNews & Politics\nReligion & Spirituality\nTravel & Culture\nTV & Movies\nAsteroid Facts for Kids\nAsteroids used to annoy astronomers by making streaks their photos and hiding more interesting things. But now they are the interesting things. Here is their story.\nThe first asteroid was discovered on New Year's Day in 1801.\nGiuseppe Piazzi discovered the object that he named Ceres. It was between Mars and Jupiter where astronomers thought there was a \"missing planet\". They assumed Piazzi had found it. The four biggest asteroids were often called planets until nearly the end of the 19th century.\nAsteroids have also been called planetoids, minor planets and small solar system bodies.\nWhen Pallas was discovered the year after Ceres, William Herschel thought it odd that two planets were in similar orbits. It seemed to him that they were a new type of object. He suggested the name \"asteroid\" (starlike), because through a telescope they looked more like stars than planets. In 2006 the International Astronomical Union (IAU) officially classified Ceres as a dwarf planet and the other asteroids as small Solar System bodies.\nAsteroids are some of the leftovers from making the Solar System.\nAstronomers think the planets formed by accretion, which means matter clumping together into bigger and bigger objects. The material in the asteroid belt started to accrete. But it couldn't hold together as a planet because Jupiter's gravity kept breaking it up. The asteroids are very interesting to astronomers because they contain material from the early Solar System.\nThere are over a quarter of a million known asteroids, over 12,000 of them named.\nCeres is the biggest asteroid and it's only 940 km (580 miles) in diameter. Most are much smaller. Since they can be as small as a dust grain, there could be billions of them. But even if you could collect all the material in the asteroid belt, it wouldn't make much of a planet. You'd need 25 asteroid belts to get something as massive as the Moon.\nThe asteroid belt isn't really as crowded as it seems.\nClick to see a diagram of the inner Solar System. They try to give you an idea of the numbers of asteroids, but they make it look crowded. The belt is more than 1 astronomical unit (AU) across and its circumference is bigger than the orbit of Mars. One AU is the distance from the Earth to the Sun, 150 million km (93 million miles). There is lots of space for all those rocks.\nExcept for Ceres, the asteroids are all sorts of irregular shapes.\nCeres has enough mass for gravity to pull it into the shape of a ball. That's why it's a dwarf planet. You can see what some of the others look like in this collection of asteroid pictures.\nAlmost all asteroids are made of rock, but about 5% are metallic - iron and nickel.\nThe meteorites on Earth have mostly come from the asteroid belt. The iron meteorites were highly prized by ancient peoples who didn't have the technology to get iron from iron ore.\nThere are asteroids outside the asteroid belt.\nWe are very interested in the asteroids that cross Earth's orbit. They're called Near Earth Objects. More than 600 of these are also classified as Potentially Hazardous Asteroids. The Minor Planet Center of the International Astronomical Union keeps a close watch on these, and there are other projects searching for new ones. Unfortunately, we don't yet know what to do if we find one. Millions of years ago an asteroid hitting Earth may have caused the dinosaurs to die out.\nNASA's Dawn mission is visiting the asteroid belt.\nNASA launched Dawn in 2007 and from July 2011 to August 2012 it studied the asteroid Vesta. Dawn has been studying Ceres since 2015.\nThe person who discovers an asteroid usually gets to name it, as long as it doesn't break the rules of the Minor Planet Center.\nThe first asteroids were named for goddesses. But by now the list includes the names of the discoverers and their families, of scientists, writers, artists, movie stars and many more. The names can't be duplicates, offensive, or of living political or military figures.\nThe youngest person to discover an asteroid was Luigi Sannino in Italy.\nIn September 1999, 18-year-old Sannino was observing with P. Pietrapiana at the Monte Viseggi Observatory when they found the asteroid which was later named Palmaria.\nMy Pinterest board \"Asteroids and meteors\" has some related images.\nContent copyright © 2015 by Mona Evans. All rights reserved.\nThis content was written by Mona Evans. If you wish to use this content in any manner, you need written permission. Contact Mona Evans for details.\nWebsite copyright © 2016 Minerva WebWorks LLC. All rights reserved."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:d7ac5cb1-0afb-4712-bdd8-56fc0cc6f366>","<urn:uuid:c0058658-7aac-4aff-a31e-de4c36165382>"],"error":null}
{"question":"How do water infrastructure issues reflect citizen-state relationships, and what solutions are being proposed to ensure equitable access?","answer":"Water infrastructure powerfully reflects citizen-state relationships, particularly in postcolonial contexts where marginalized groups have been historically excluded from state services. In India, this manifests through informal settlements being shut out of state-provided water networks, while in the US, there are stark disparities in water access along racial and economic lines. To address these inequities, several solutions are proposed: investing in the Water Infrastructure Workforce Development Program ($5 million annually for 10 years) to create job opportunities in underserved communities, providing at least $3 billion for a Low-Income Households Water Assistance Program to prevent water shutoffs, and implementing targeted investments in Small & Disadvantaged Communities Programs to improve access in historically underserved areas.","context":["Decolonizing Infrastructure in India and the US: A Conversation with Malini Ranganathan\nRecently, many scholars in the humanistic social sciences have begun to focus on the more-than-human agency of nonhuman natures, things, objects, and materials. Within this posthuman turn, objects are not simply inert backdrops for the ordering of social life but are actively involved in creating new sociopolitical orders. Infrastructure has emerged as a useful analytical tool to critique unequal power relations between people, economic systems, and the state, and to challenge conventional frameworks of urban-rural, North-South, and human-nonhuman. Moreover, thinking about infrastructure can add a much needed postcolonial and decolonial impetus to academic scholarship.\nAn assistant professor in the School of International Service at American University, Dr. Malini Ranganathan takes a critical look at questions of social and environmental justice through an intersectional and antiracist lens. Malini uses urban water infrastructure as a lens to study processes of neoliberal marketization, speculative urbanism, informality, and the (re)production of caste, class, and gendered othering in Bangalore, southern India. Her recent work has also focused on the issue of urban resilience and abolitionist climate justice in Washington, DC.\nOn March 15, 2019, I sat down with Malini on the occasion of her visit to the University of Wisconsin–Madison Geography Department to give the 2019 Treacy Lecture. We discuss decolonization as theory and practice, analytical frameworks for our current ecological crisis, her research in urban India and Washington, DC, and the potential for North-South collaborations and solidarity movements within academia and beyond.\nStream or download our conversation here. Interview highlights, edited for clarity, follow.\nPodcast: Play in new window | Download\nSubscribe: Stitcher | TuneIn | RSS\nThis transcript has been edited for length and clarity.\nSiddharth Menon: Your doctoral dissertation focused on the city of Bangalore, where you used water as a lens to study the modernization of the Indian economy since the 1990s, as well as speculative urbanism, informality, and systems of differentiation like caste and class dynamics. What makes water such a good lens to look at these kinds of metanarratives and structures? Why water, and why situate the research in Bangalore?\nMalini Ranganathan: Water is a resource that is indispensable to life. It thus becomes a resource to fight for and about which to ask very fundamental ethical questions. In a context in which poor informal groups, such as those living in slums or on the outskirts of major cities in India, had been historically shut out of state-provided water, the fact that market-oriented logics were introduced into the water sector, particularly from the early 2000s onwards, really brought forth and elicited grassroots responses, quite vehement responses, both in terms of resistance and negotiation. Because water raises such important ethical questions, it allows a window into citizen-state relations, especially in postcolonial polities where historically marginalized groups have been left out of state-provided networks.\nBangalore, in the state of Karnataka, had been a forerunner in market-oriented reforms. Karnataka’s political economy, especially because it had very reform-oriented chief ministers in the 2000s, was a preferred destination for international debtors. International lenders liked that it was reform-minded, the software powerhouse of India, and tech oriented. They liked the fact that Karnataka had shown an interest in World Bank relationships and borrowing from the World Bank. And, so, in the early 2000s, a slew of international organizations, international financial institutions, the World Bank, the US Agency for International Development, the Asian Development Bank, and the Japan Bank for International Cooperation, all descended on Bangalore to create out of it the best practice for urban governance reforms and market-oriented logics.\nWater allows a window into citizen-state relations.\nThese organizations used the water sector as exemplary of infrastructure that needed reform, because it was said to be overrun by corrupt bureaucrats and an inefficient public sector, and it had a lot of “leakage,” which is the technocratic term for water being consumed illegally or by informal groups like slum dwellers. So, you had this technical language of market rationality and efficiency infusing the water sector. That was very interesting to me, as someone interested in environmental issues, but also the politics of market-oriented reforms and the role of development organizations. Even in my prior work on rural development, I had seen the impact that international development loans had on countries throughout the Global South, but especially in India. Bangalore also has a thriving civil society—various citizen and civic movements engaging with these reforms and the government—and I thought it would be a great place to ask the empirical questions that I was interested in asking.\nSM: From that early work focusing on the Global South, and particularly India, your more recent work has shifted to looking at—and I know these frameworks are problematic—the Global North. Your recent article with Eve Bratman focuses on shifting our framework from urban resilience to abolitionist climate justice, and looking at Washington, DC, as a case study for that. What’s at stake with that shift? And why the shift to working in a Global North context like DC?\nMR: This an interesting question for all of us to try to grapple with. What does it mean when folks are trying to work across the so-called South-North divide? What are the openings provided by that? What are some of the challenges? For me, I am a professor based in Washington, DC, at the School of International Service, that trains students to do foreign policy, international development, peace and conflict resolution, and to think about these global questions. And it was of concern to me that students were very comfortable thinking about “underdevelopment” or “poverty” or “disenfranchisement” elsewhere around the world, but when it came to thinking about those questions in their own backyard—when it came to histories of racial segregation in the United States, processes of dispossessing minorities, and the US state and its implications in global empire— they were much less comfortable talking about those questions. So, it became a pedagogical challenge, and necessity, to really bring in the US not as an exceptional case of liberty and democracy and equality, but as another case to study in terms of vexing questions of inequality and development.\nDC is a paradigmatic city of racial and environmental inequality. If you look at a map of DC, some people have referred to it as an apartheid city because there’s literally a line separating the city by race and by class. You have in the northeast and southeast parts of DC populations that are over 90 percent African American, where the poverty rate is much, much higher than in the rest of the city, where basic amenities and infrastructures are deteriorating and sorely lacking, where there are multifold challenges in terms of health, environment, and social equity outcomes.\nI was interested in looking more deeply at the production of those inequalities, what led to this stark apartheid geography, and then specifically looking at what this has meant in terms of environmental outcomes. And it so happened that at the time I started thinking about this, the District of Columbia government started publishing a series of climate resilience plans, thinking about, in the aftermath of Hurricane Sandy and Hurricane Katrina, how are we going to make American cities more resilient, more able to cope with future extreme weather events wrought by climate change? These plans selectively put the lens on particular vulnerabilities while also neglecting longstanding inequalities wrought by racial capitalism, racial segregation, and environmental racism.\nI wanted to challenge and critique these frameworks of resilience. One of the ways that Eve Bratman and I found was, what does the vocabulary of climate justice bring into the fold that resilience has neglected? We asked two questions: what are current conditions of climate change vulnerability and precarity in some of these neighborhoods in DC that are supposedly prone to extreme weather events or climate vulnerability? And, second, how are residents currently combating and coping with these vulnerabilities and precarities?\nAnd one of the things we argue is that mainstream resilience thinking—this kind of top-down expert-driven resilience thinking— is focused on technical solutions and future-gazing, in the sense that it’s focusing on how we “climate proof” DC, especially these vulnerable neighborhoods, from future extreme weather. It’s not focused on some of the historical causes of harm. So, we sought to do a historical analysis, and this is key for building theory and for a deepening a sense of how to actually achieve justice.\nSM: Exactly on that point, like you rightly mentioned, there are places like the Global South in the Global North and places like the North in the South. So, in terms of doing research in both these analytical categories, what are the potential opportunities and the challenges of theoretically drawing from both of these contexts and also in terms of building solidarity movements between disenfranchised, disempowered, oppressed, marginalized people across both contexts?\nMR: Thank you for that question. I think there’s more and more interest, particularly among younger scholars, for breaking down these long-held ontological and epistemological boundaries and categories, as you call them, between North and South. And you see a trend in the academy of folks doing this. I feel like I’m also part of this trend, and in many ways have learned from and benefited from the early scholars who have done this, and I’m also helping to chart a path for others to do it as well.\nSpeaking very concretely about the DC case, my colleague Eve Bratman wrote an article in Third World Quarterly, “Development’s Paradox: is Washington DC a Third World city?” in which she outlined really startling indicators in especially northeast and southeast DC on, for instance, things like the prevalence of HIV/AIDS and how much higher it was in these areas than other parts of DC; in fact, they were approximating Haiti and other places that we normally associate with the Global South.\nI want to challenge and critique these frameworks of resilience. What does the vocabulary of climate justice bring into the fold that resilience has neglected?\nIn my own work on water, I found that there are geographies within the US that are often out of sight out of mind that have conditions of peripheral urbanization with inadequate access to water and sanitation. They still rely on privies for sanitation. You see that in the US deep South, on the US-Mexico border, in California’s Central Valley. So, empirically, the realities are there. And then the question analytically is, well, then what do we learn from social science research on the Global South that can help us better understand these geographies in the US? And then, conversely, what can we learn about the production of elite spaces in the Global North that is helpful in terms of global capital and understanding the production of elite spaces in the Global South? I think that theory building for understanding global geographies is a compelling way to do things. It’s exciting and it’s also difficult.\nFeatured image: Water infrastructure in Washington, DC. Photo by Alexandr Trubetskoy, 2011.\nPodcast music: “Gloves” by Julian Lynch. Used with permission.\nDr. Malini Ranganathan is Assistant Professor in the School of International Service at American University, where she broadly researches urban environmental justice in India and the US. A critical urban geographer by training, she examines the politics of water, flood risk, and property-making in Bangalore/Bengaluru, India, focusing on colonial and postcolonial projects around infrastructure and space and how these shape and are shaped by social difference. Dr. Ranganathan also investigates urban environmental and housing inequality in America and is researching prospects for antiracist climate justice in Washington, DC. Website. Twitter. Contact.\nSiddharth Menon is a Ph.D. student in Geography at University of Wisconsin–Madison. His research interests lie at the intersection of economic and cultural geography, political ecology, and STS. For his Ph.D. dissertation, Siddharth is working on an ethnography of concrete as building construction technology in Kerala, southern India to highlight the possibilities and challenges of building with concrete in the Anthropocene. Website. Twitter. Contact.\nYou must be logged in to post a comment.","CLEAN AND SAFE WATER\nPolluted water can cause serious and costly health issues, and environmental justice communities across the U.S. are disproportionately affected by water contamination and failing water infrastructure. During the COVID-19 pandemic too many people in lower income communities, Black communities, Tribal communities, and other people of color, have either had their water service disconnected or never had reliable clean water in the first place. At the same time climate change is disrupting our water systems: fresh water will become more scarce (droughts, lack of snowpack, and over-pumping of aquifers), overly abundant (storms and flooding) and more polluted than ever before. The unfinished business of reducing pollution into waters of the U.S. leaves this resource vulnerable and new information on everyday chemicals continues to demonstrate that they are more dangerous to our water supplies and public health than we knew. Investing in building clean and safe water infrastructure is critical to an equitable recovery and creates high-quality jobs while building strong, resilient communities.\nSpecific Policy Proposals:\nRepair Clean Water Infrastructure. Ensuring that all communities have affordable, reliable, and sustainable access to safe drinking water and appropriate wastewater and stormwater treatment and disposal must be a top priority. Our public water systems and communities of all sizes are grappling with the need for water infrastructure maintenance or improvements while rising rates are making basic drinking water and wastewater service unaffordable for low income consumers across the country. For communities and business to thrive, Congress must invest in and fix our decades old crumbling water infrastructure. By spurring development of good paying union jobs, focusing particularly on efforts to expand job opportunities in environmental justice communities, we can ensure all communities not only have clean water but a thriving economy as well. Congress should invest $10 billion a year for both the Clean Water State Revolving Fund and the Drinking Water State Revolving Fund, with at least 20% set aside for disadvantaged communities and 20% for green infrastructure, as well as a one time investment of $100 billion to begin making progress on our huge water infrastructure backlog.\nProtect our Children From Toxic Lead. Every child deserves to drink clean water and be protected from the damaging consequences of toxic lead pollution. Unfortunately, more than nine million homes are serviced by lead service lines in the U.S. Further, a recent report from the Harvard School of Public Health shows that millions of children are drinking lead in the water at school. Congress must provide $45 billion over 10 years for the Reducing Lead in Drinking Water program to provide grants and technical assistance for completely replacing lead service lines in households, daycare centers, and schools. Congress must also provide $1 billion over 5 years for the School Lead in Drinking Water Program to provide filters and retrofit or update schools to ensure all school children have access to clean, lead-free water.\nProtect Communities from Sewage and Stormwater Overflow. It is unacceptable that in 2020, too many communities, particularly low income communities and communities of color, lack adequate sewage and stormwater infrastructure. As climate change warms the atmosphere, it is altering the hydrologic cycle, changing the amount, timing, form, and intensity of precipitation. Unfortunately, our water infrastructure was not built with these changes in mind, which threaten our water quality, public health, and safety. Congress must proactively address our aging water infrastructure and prioritize programs that benefit environmental justice communities that lack and cannot afford to pay for necessary water infrastructure upgrades. Funding the Sewer Overflow and Stormwater Reuse Municipal Control grant program at $225 million per year would provide crucial cost-sharing to pay for important infrastructure to manage sewer overflow and stormwater, improving public health, creating jobs and helping communities thrive.\nEnsure Rural Communities Have Clean Water. Congress must make significant infrastructure investments that ensure all of our communities are provided with clean water, including rural areas who are in desperate need of updated systems. The USDA’s Water & Waste Disposal Loan & Grant Program is designed to help rural communities (targeting areas with populations of 10,000 or less, tribal lands and underserved communities) who tend to lack the necessary funding to invest in better sewage systems. Congress must provide $1.75 billion per year, including $750 million in grants, to ensure rural communities have access to clean water, bolster public health, and can grow and prosper.\nStop Sewage from Entering our Waterways. Aging pipes discharge 900 billion gallons of untreated sewage into U.S. waterways annually. Significant portions of many municipal systems are now approaching 40 to 50 years in age. Congress must invest in updating our wastewater infrastructure to prevent this unchecked pollution, especially in underserved communities that are the most at risk, including low-income communities, communities of color, and rural communities. To prevent untreated sewage from damaging our waterways, Congress must fund the Decentralized Wastewater Grant Program with $750 million per year.\nSecure Safe Drinking Water for Underserved Communities. All families and individuals have a basic human right to clean water, yet many low income communities and communities of color cannot trust that their water is safe. Securing safe drinking water for underserved communities must be a top priority. Congress should make targeted investments in the Small & Disadvantaged Communities Program ($60 million per year), Alaska Native Villages & Rural Communities Program ($120 million per year), and the US-Mexico Border Water Infrastructure Program ($100 million per year) to improve access to clean water in some of the communities who have been denied it for far too long.\nSupport the Low-Income Households Drinking Water and Wastewater Assistance Program. We urge Congress to enact protections to prevent residential water shutoffs and mandate safe reconnections of households previously disconnected. Congress should provide at least $3 billion in immediate funding for a Low-Income Households Drinking Water and Wastewater Assistance Program to ensure households stay connected to essential water and sanitation services.\nInvest in the Water Infrastructure Workforce Development Program. Water infrastructure jobs are high paying and located across the country. A coordinated water infrastructure workforce development program will create a pipeline to new job opportunities in communities that need them most, and bolster our vital water systems — which are in urgent need of repair, maintenance, and upgrades. Congress must fund this program with $5 million per year for at least 10 years. By specifically targeting job training opportunities in low-income communities and communities of color, we can provide high-quality, family-sustaining careers, help communities suffering from a lack of investment and job opportunities, and support a sector that desperately needs new workers.\nLINKS TO RESOURCES\nClean Water for All Coalition Water Stimulus Letter:\nThe National Association of Clean Water Agencies (NACWA) concluded that every $1 billion provided for water and wastewater infrastructure projects creates between 20,000 and 27,000 jobs and an economic ripple effect that adds between $2.87 and $3.46 billion to the economy.\nA combined $1.5 billion annual investment over 5 years in these programs: School Drinking Fountain Replacement program, Sewer Overflow Control Grants program, Alaska Native Villages and Rural Communities Water Grant program, U.S.-Mexico Border Water Infrastructure program, and Small & Disadvantaged Communities program. In each year, this would create 7,074 direct jobs and 19,415 total jobs.\nBrookings Report Renewing the Water Workforce: Improving water infrastructure and creating a pipeline to opportunity\nBlueGreen Alliance’s Clean Water, Good Jobs Policy Brief\nUSDA Rural Development’s Water & Waste Disposal Loan & Grant Program"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:5dc0a30c-d43e-4cd1-8df1-d262309bb73c>","<urn:uuid:e51c9ad0-922c-45a9-9532-8f79b56ae76a>"],"error":null}
{"question":"What are the differences between native vegetation and turfgrass in protecting soil during extreme weather conditions?","answer":"Native vegetation and turfgrass show significant differences in soil protection during extreme weather. Native plants, with their deep root systems, successfully withstand intense storms and hold soil in place, as demonstrated in Dubuque County's July 2011 storm where a three-year-old prairie planting remained largely intact while turfgrass was scoured to bare soil. Turfgrass, characterized by shallow root systems, is more vulnerable to erosion during heavy storms. During drought conditions, turfgrass becomes dormant, turning bluish-gray and brittle, and can only survive 4-5 weeks of dormancy before the rhizomes, roots, and crowns dehydrate. This comparison shows that native vegetation provides superior erosion control during storms, while turfgrass requires careful irrigation management to survive extreme conditions.","context":["How hard & soft armor control storm water\nAnyone driving through Iowa last spring was likely to see plenty of water across the landscape. As drivers and residents alike make use of Iowa roadways, it is important to keep in mind the function of the right-of-way system, especially during severe storm events. Both “hard armor” structures and the “soft armor” of native plant communities serve a purpose during these times.\nCulverts help divert roadway runoff and reduce erosion to road beds, ditches and banks. Strategically placed culverts can help maintain a stable velocity and proper flow of runoff through road ditches by controlling the rate of release. According to the “Local Roads Maintenance Worker’s Manual” (Center for Transportation Research, Iowa State University), water on or under the roadway is the single most significant cause of damage to the roadway. Therefore, it is critical to manage runoff in a way that minimizes damage to the road. When heavy rains occur, however, culverts also can be disguised under high water that appears to be still. During heavy rain events—similar to what Iowa experienced this past fall—ditch culverts can move water through the drainage system at a high rate.\nAnyone working on, playing in or accessing areas near ditch culverts with high standing water should proceed with caution. While the water may appear to be stable on its surface, the drainage system below is still performing, creating an undertow effect. Especially large storms can turn ditch bottoms into temporary water features on the landscape. While things may look calm on the surface, water is still moving below.\nIn many instances, standing water above culverts can indicate debris or sediment clogs; however, heavy, intense rainfall can overwhelm drainage systems designed to manage smaller capacities. These larger storm events also can create significant erosion and sedimentation problems in areas with poor vegetation or in plant communities with shallow root systems. Integrated roadside vegetation management programs, particularly those that incorporate native grasses\nand forbs in roadside seeding mixes, can successfully manage this erosion. Instead of losing topsoil and vegetation to fast-flowing water, the deep roots of native plants hold the soil in place and withstand the brute force of the storm.\nIn July 2011, Dubuque County experienced almost 15 in. of rainfall in just a few hours. Streams and roadsides were quickly overwhelmed. The intense storm surge sent trees toppling and scoured turfgrass to bare soil. Conversely, a three-year-old prairie planting withstood the deluge with minimal impact. Inspection of the area one week after the storm event indicated that the deeper root systems stayed intact better than turf. In many instances, debris that wrapped around trees aided the downstream erosion. In the areas planted with native vegetation, debris still wrapped around trees, but had significantly less impact on soil erosion.\nWith seasonal weather on the way, it is important to stay mindful of the systems in place for drainage, as well as ways to minimize damages from major storm events. In most instances, culverts continue to drain, even in high water. And as we experience flooding and extreme rainfall events, it is worth noting the value of native vegetation as a preventive measure against erosion and sedimentation after the fact.","Managing Turf During Drought\nManaging Turf Drought in Kansas City\nIRRIGATE, MOW, FERTILIZE\nMost turfgrasses are resilient and will recover from drought conditions with minimal injury if proper management practices are implemented prior to and after the onset of stress. Turf needs proper amounts of moisture and nutrients to aid in the recovery process. Proper cultural practices will also aid in drought tolerance and recovery.\nCool season grasses perform best in temperatures between 60 degrees and 75 degrees Fahrenheit, whereas warm season turf performs well at temperatures above 80 degrees Fahrenheit. Low soil moisture and increased temperatures between the months of June and August are very stressful to cool season turf. Due to low soil moisture, the first signs of dormancy will be a loss of color in the turf. The turf will take on a bluish-gray color and become very dry and brittle. Applying as little as one inch of water per week can help the turf to retain color. Turfgrass can usually survive 4-5 weeks of dormancy with minimal damage. Dormant grass will not recover if the rhizomes, roots, and crowns dehydrate.\nProper irrigation practices are critical during times of drought. It is important to water deeply and infrequently opposed to lightly and frequently. Watering deeply helps the water to penetrate the soil, allowing more moisture availability and retention. Watering lightly and frequently keeps the moisture on the surface, causing quicker evaporation and shallow root development. Water should be applied in the early morning hours (between 5:00 a.m. – 9:00 a.m.). Avoid irrigation in the afternoon due to rapid evaporation and heat build up. Also, avoid irrigation in the evening due to the promotion of disease pressure. In times of water bans, one half inch of water every one to two weeks may be enough to keep the crown, rhizomes, and roots hydrated.\nMOWING IN KANSAS CITY\nCertain precautions should be made when mowing turf during drought conditions. Always be sure that mower blades are sharp to avoid tearing the turf. Turf mowed with dull mower blades will take on a brownish-gray cast soon after mowing because of moisture loss through the torn tip of the grass blades. Do not remove more than one third of the new growth in any one mowing. Removing too much top growth will cause a reduction in root mass and will not enable the plant to retain enough moisture. Alternate mowing patterns to avoid soil compaction caused by tire tracking. These practices will help control stress during drought conditions, but should be practiced the entire season.\nFERTILIZATION IN SHAWNEE, KS\nProper fertilization will help the turf become more stress-resistant and allow the plant to recover more quickly from the stress. Providing the turf with adequate potassium prior to drought conditions will help make the plant more stress-, insect- and disease-resistant and enable quicker recovery in the fall. Using slow release nitrogen will help keep the plant growing during drought periods and help the turf maintain its color. Avoid quick release nitrogen before and during drought periods. This practice can flush top growth and can predispose the plant to injury much quicker due to an out of balance root to shoot ratio (root growth to top growth ratio). It is best to use a 1:1 ratio of nitrogen to potassium to improve stress tolerance within the plant.\nHydrophobic (water repellent) soils are found in many areas where turfgrass is grown. The use of long-term residual wetting agents will help alleviate problems associated with localized dry spot from hydrophobic soils for up to an entire growing season. Long-term residual wetting agents, like LESCOFlo Ultra, work best when applied preventively prior to moisture stress. Shorter residual wetting agents, such as LESCOWet Plus, also help alleviate problem areas from localized dry spot, but may need to be applied on a more frequent schedule. It is critically important that both long- and short-term residual wetting agents be incorporated into the soil with irrigation for the products to become effective in the alleviation of localized dry spot conditions. Keep in mind that wetting agents do not assist in water retention. They allow for more efficient use of the available water by reducing the surface tension and allowing water to penetrate the hydrophobic layer of soil.\nThese practices, combined with other cultural practices such as aerification for thatch reduction, will help turf during stressful conditions. Turf should be re-evaluated at the end of August and preparations made for overseeding or sodding areas that do not recover."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:cf9139fd-483a-4ba9-af40-537411c8615b>","<urn:uuid:fc99412a-902c-43b7-b29f-de9ad983286a>"],"error":null}
{"question":"What are the security risks of using public WiFi networks, and how can WiFi sniffing tools be used to demonstrate these vulnerabilities?","answer":"Public WiFi networks pose significant security risks as their traffic is usually unsecured and lacks proper encryption to protect internet data. Hackers can perform man-in-the-middle (MITM) attacks by creating fake public networks with names similar to legitimate venues, allowing them to steal sensitive information like credit card numbers and passwords. They can also use packet sniffing software to record large amounts of data. As for WiFi sniffing tools, applications like CommView for WiFi and Kismet can demonstrate these vulnerabilities by capturing and analyzing raw packets sent over airwaves. These tools can track IP connections, show network details, and reveal decrypted packets when connected to a network, making them effective for understanding potential security weaknesses.","context":["One way to bolster your understanding of Wi-Fi security is to do some hacking yourself. That doesn’t mean you should infiltrate a company’s network or snoop on a neighbor’s setup. Rather, ethical hacking and legitimate Wi-Fi penetration testing – done in cooperation with the network owner – can help you learn more about the strengths and limitations of wireless security. Understanding potential Wi-Fi vulnerabilities can help you to better protect the networks you manage and ensure safer connections when you access other wireless networks.\nStart with a Wi-Fi stumbler\nGeneral purpose Wi-Fi stumblers are the simplest and most innocent tools to add to your pen testing kit. Though typically passive tools, they serve an important purpose. They allow you to see what access points (AP) are nearby and their details, such as the signal level, security/encryption type, and media access control (MAC) address. It’s a tool even a hacker would utilize to find the next victim.\nUsing a stumbler, you might find networks set with weak security, like WEP or the original version of WPA. Or, a walk throughout a property might reveal rogue APs set up by employees or others that could be opening your network to attack. Even if there are APs set with a hidden or non-broadcasted service set identifier (SSID), some stumblers can quickly reveal them.\nVistumbler is an open source Windows application that displays the basic AP details, including the exact authentication and encryption methods, and can even speak the SSID and signal level.\nIt also displays graphs of signal levels and channel usage. It's highly customizable and offers flexible configuration options. Vistumbler supports AP names to help distinguish them, also helping to detect rogue access points. It also supports GPS logging and live tracking within the application using Google Earth.\nIf don’t want to lug around a laptop and have a mobile device, consider using the AirPort Utility on your iOS device or a download an app on your Android. If a free stumbling app doesn’t cut it, check out our review of some commercial options.\nWifi Analyzer is a free Android app you can use for finding access points on your Android-based smartphone or tablet. It lists the basic details for access points on the 2.4-GHz band and on supported devices on the 5-GHz band as well.\nYou can export the access point list (in XML format) by sending it to email or another app or take a snapshot of the screens. It also features graphs showing signals by channel, history and usage rating, and it has a signal meter feature to help find access points.\nWi-Fi sniffers go further than stumblers. Instead of just grabbing the network details, sniffers capture and show and/or analyze the raw packets sent over the air waves. Captured traffic can be imported into other tools, such as an encryption cracker. Or, some sniffers have the functionality included to do some analysis or cracking as well. In addition, some sniffers look for and report only on certain network traffic, such as those designed to reveal passwords sent in clear-text.\nCommView for WiFi is a popular commercial Wi-Fi sniffer and analyzer that offers a 30-day limited trial. It has a stumbler feature to show network details, plus channel utilization stats and graphs. It can track IP connections and records any VoIP sessions. The tool also lets you capture and see the raw packets.\nIf you’re connected to a Wi-Fi network, you can input its PSK passphrase so the decrypted packets will be shown. You can also set rules to filter the data you see and set alarms to track rogue devices. Other cool features include a traffic generator to do some spoofing, node reassociation to manually kick clients off, and TCP reconstruction to better view the captured data (text or photos).\nKismet is an open source Wi-Fi stumbler, packet sniffer, and intrusion-detection system that can run on Windows (with WSL framework), Mac OS X, Linux, and BSD. It shows the access point details, including the SSID of \"hidden\" networks. It can also capture the raw wireless packets, which you can then import into Wireshark, TCPdump, and other tools. In Windows, Kismet only works with CACE AirPcap wireless adapters due to the limitation of Windows drivers. It does, however, support a variety of wireless adapters in Mac OS X and Linux.\nTools reveal Wi-Fi details\nWirelessKeyView from NirSoft is a simple yet neat tool that lists all the WEP, WPA, and WPA2 keys or passphrases stored on the Windows computer you run it on.\nAlthough it was pretty easy to reveal saved keys in Windows 7 and prior versions via the usual Windows GUI, Microsoft made it more difficult in Windows 10. WirelessKeyView quickly gets you an exportable list of all saved networks no matter the OS versions.\nTools like WirelessKeyView can reveal how a compromised or stolen device may contain sensitive information beyond documents. It also shows the importance of using 802.1x authentication, where users would have individual login credentials for the Wi-Fi and aren’t susceptible to this type of issue.\nAircrack-ng is an open source suite of tools to perform WEP and WPA/WPA2-Personal key cracking.\nIt runs on Windows, Mac OS X, Linux, and OpenBSD. It's also downloadable as a VMware image and Live CD. You can view nearby Wi-Fi networks, including hidden or non-broadcasted SSIDs. You can also capture the raw packets, inject and replay traffic, and possibly crack the encryption keys once enough packets have been captured.\nGet a full software suite with a Linux distro\nOne of the most popular pen testing distros is Kali Linux. In addition to a typical Linux OS install on a computer, you can make a live bootable disc or download VMware or VirtualBox images. It contains a huge list of security and forensics tools, some of which you can utilize for Wi-Fi pen testing. For instance, Kismet and Aircrack-ng tools are included.\nA few of the other Wi-Fi tools included with Kali Linux are Reaver to hack a network via an insecure WPS PIN, FreeRadius-WPE to perform man-in-the-middle attacks on 802.1X authentication, and Wifi Honey to create a honey pot to lure in clients to connect to a fake AP in hopes of capturing their traffic and performing man-in-the-middle attacks.\nGo all out with a hardware tool\nIf you’re really serious about wireless security and playing around with its vulnerabilities, you have to get a taste of WiFi Pineapple. It’s a hardware-based solution specifically designed for Wi-Fi auditing and pen testing. You can scan, target, intercept, and report on many wireless threats and weaknesses.\nWiFi Pineapple has a router-like look and feel, including its web GUI.\nYou can do things like see client details of each AP, send de-authentication packets, and automatically create fake APs by mimicking nearby SSIDs for some man-in-the-middle fun. You can also capture the web browsing data of others and spoof DNS replies to confuse users or send them to spoof sites.\nWiFi Pineapple currently offers two hardware options: a pocket-sized single-band NANO starting at $99.99 and a router-like dual-band TETRA (see a full review) starting at $199.99.\nEric Geier is a freelance tech writer. Keep up with his writings on Facebook or Twitter. He’s also the founder of NoWiresSecurity, which provides a cloud-based Wi-Fi security service, and Wi-Fi Surveyors, which provides RF site surveying.","What is a VPN? How is it used? Why is it needed now more than ever before? Read on.\nDo you want to protect your online identity, stay safe on public wifi or bypass censorship on the internet? Then this article is for you.\nFirst a little background on how the internet world works: Your public IP address is discoverable by browsers, websites, service providers, and other devices. This opens the door for your privacy to be compromised. It can also mean that sensitive information falls into malicious hands. When using a VPN, instead of your public IP address being displayed, it uses the address of the VPN server that all of your internet activity is routed through. This VPN server could be located anywhere in the world, which makes it impossible for those interested to find out your true location, let alone any personal information.\nMoreover, VPNs have lists of countries, after you select one, you appear to be using the internet not from your actual location, but from the location of the virtual server. VPNs secure and protect your online identity. Most of the trusted VPN service providers use the latest encryption keys to hide your data from anyone trying to spy on your digital lifestyle. If servers are not obfuscated, however, your ISP can see if you are using a VPN, but it cannot decipher the contents of your internet traffic. It means your ISP cannot see anything you do while you are connected.\nThe Virtual Private Network (VPN) Market is projected to grow at a CAGR of 6.39% to reach US$50.153 billion by 2024, from US$34.591 billion in 2018.\nThe demand for VPNs will grow on account of the increasing cybercrime issues, as VPNs provide a secure and private network for individuals to access. In addition to this, many online services are acquiring VPN service providers to provide their own VPN services to users. However, since VPNs carry data to a different server before taking the user to the desired webpage, they witness some performance and speed issues, which restrains the demand for these services during the forecasted period.\nHere is a look at three VPN use cases you should know about.\n- By Pass Geo Restriction\nGeo-restriction or geo-blocking is a method to restrict or limit access of specified content based on the user’s geographic location. Average internet users usually encounter geo-restrictions on a daily basis while trying to access streaming platforms as they allow different content for different countries. Additionally, governments implement geo-restriction technologies to block sites or specific online services.\nHow does geo-blocking work? All of your devices on the internet have their unique series of numbers called an Internet Protocol address (known as ‘IP address’). Your laptop, phone, and each device connected to the internet have IPs, which are provided by your internet service provider (ISP). Therefore, your ISP knows your IP address. When you visit a website, the IP address of your network is sent to the server so it knows where it has to send the content.\nAlthough your IP address is not significant on its own, using specialized software, it is possible to track your online behavior effortlessly, monitor which websites you visit and when. Also, to some extent, it is possible to know the geographical location of your device. This is how a site ‘knows’ from which country you are accessing. Then, website administrators apply geo-blocking based on this information. Moreover, geo-blocking applies when traveling. Meaning, if you are an American visiting France, you will only access the content available in France.\nIs bypassing geo-blocking legal? The legality of getting around geo-restrictions is unclear and varies by country. In the European Union, some forms of geo-blocking are illegal. Companies are not allowed to discriminate against consumers based on their location for online sales of specific services.\nHowever, streaming platforms, such as Netflix, claim that bypassing geo-blocking can be considered as a violation of copyright and licensing regulations, they also justify the use of methods to detected and block various anonymizer services, like VPNs.\nThere are tools to get around geo-restrictions, VPNs are the most common and, usually, easy to use for a less tech-savvy audience. While using a VPN service, you can quickly change your location and have unrestricted and fast access to any website. You can choose your desired location, or let us offer an optimal choice for you.\nLocal VPN servers represent a private, controlled network. It creates a virtual tunnel, where your data is encrypted so that no one can track or monitor your online activities.\nVPN masks your actual IP address and allocates you with one from your chosen country. For instance, if you are in the USA, you can quickly select a remote VPN server in Japan, the website will think you are accessing it from Japan.\nVPNs also help to bypass government-induced censorship. In this case, VPNs not only help to achieve internet freedom but also – to fully secure your data from the prying eyes of snoopers.\n2. Avoid Government Censorship\nInternet censorship is a process of blocking, limiting, filtering or manipulating internet content in any way. It is a method of suppression used by the governments which control what can be accessed, published or viewed online. Although censorship might seem like something done by oppressive governments, the scope of it has been increasing alarmingly in many democratic countries. More than 60 countries engage in some form of state-sponsored censorship.\nRestrictions and manipulations vary from limiting access to digital content (such as movies, series or music), blocking certain websites or services (Skype, Telegram, WhatsApp, Youtube, Netflix, etc.) or filtering information perceived as unwanted (for instance, opposing the government in any way)\nWho is usually affected by internet censorship? Various attempts to tighten internet control and crack down online freedom have a harmful impact on journalists, human rights activists, marginalized communities, as well as ordinary internet users, who want to access information or services online. Why do governments engage in various forms of internet censorship? The intents vary. In can be done to spread the government’s views, particular agendas, and to stop government critics and various opposing views. There are a few methods to surf the internet without borders. A VPN (a virtual private network) is a robust tool to access free information online. Also, it is safe, because it hides your online activities from the censors.\n3. Stay Safe on Public Wi-fi\nPublic WiFi can be a goldmine for dangerous lurkers posing security threats. It’s convenient, yet, dangerous to use while traveling or dining out in the city.\nAll the traffic within a public WiFi network is usually unsecured, meaning it does not use proper encryption to protect your internet data. Your sensitive information sent via an unsecured WiFi network (such as credit card numbers, passwords, chat messages) becomes an easy target for hackers.\nWhen it comes to stealing your data, hackers get quite creative. One of the ways they attack is called man-in-the-middle (MITM). Cybercriminals will create their fake public network. In most cases, the name will be similar to the name of the place with access to a public network (like a restaurant or hotel) nearby. Then, hackers will snoop on your private information and target data on your devices.\nOn top of that, hackers can install packet sniffing software. It is particularly dangerous because it records massive amounts of data which later can be processed on their demand.\nBe aware that there are many other ways to undermine your privacy while you’re connected to a public WiFi. The internet is full of video tutorials and step-by-step guides on how to hack someone’s computer over a WiFi network.\nAll of the WiFi networks are vulnerable to hacking. If you are not alone using the network, chances are someone is spying on your online activities. At best it is your ISP, at worst – scammers lurking for your passwords, bank account details or other sensitive information.\nIn 2017 Belgian researchers discovered that WPA2 protocol used by the vast majority of WiFi networks is unsafe.\nAccording to the report, the WPA2 protocol can be broken using novel attacks potentially exposing personal data.\nThe vulnerability can affect a broad range of operating systems and devices – including Android, Apple, Windows, Linux, OpenBSD, MediaTek, etc. Basically, if you have a device that connects to WiFi, it can be affected. The situation is a little different in the European Union since the General Data Protection Regulation (GDPR) took effect. ISPs processing Europeans’ data must be compliant to the GDPR. They have to make sure they store personal data only with the consent and when it’s not linkable to an individual.\nWhat can you do to protect your online identity? It is the best option to shield your private information from cybercriminals. If you are connected to a VPN, your connection is secure even if you’re on a public WiFi hotspot."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:9d651953-dd79-4eb1-9de1-302ada799521>","<urn:uuid:12c8ab66-0868-4caa-a24a-6f28dd74cf97>"],"error":null}
{"question":"What are the key components of digital literacy education, and how can its impact be measured effectively in learning programs?","answer":"Digital literacy education involves multiple components including the ability to use digital technology, communicate digitally, evaluate information, and create digital content. It encompasses skills like interpreting media, reproducing data, and applying knowledge in digital environments. The impact of such programs can be measured through both outputs (like number of participants and sessions attended) and outcomes (such as changes in learners' skills and confidence levels). Projects can use frameworks like the Essential Digital Skills Framework to assess progress, while measuring broader impacts through pre- and post-intervention evaluations, tracking specific skills development, and analyzing long-term behavioral changes in digital engagement.","context":["Digital Citizenship. Digital Citizenship.\nAFTRR – Alliance for Technology Refurbishing & Reuse. National Cyber Security Alliance. How to be a google power user | Cool Infographic Images. DI. Library's. Student Led EdCamp Period Takes NCMS To New Heights – NCMS Innovates. Last summer, we began to implement a series of changes to our school and programs in the interest of better serving the needs of our students and community. We never expected the reaction these changes have generated – and how they’ve rippled throughout the school. Today, we’ve got Digital Shop, the Black Mesa Learning Management System (LMS), Idea Street, Design Thinking, and a “manifesto” that has taken a life of its own throughout the building. And that’s just the beginning. One of the most remarkable developments – something that has pushed student (and staff) engagement to previously unforseen heights – is our EdCamp period.\nEarlier in the year, I wrote several blogs about it: The students even designed and produced a video about it: Fast forward to the present – late January 2016. During this ungraded experience – that takes place every day – our students work far, far harder on their work, projects, and ideas than they seem to in other classes. A prime example: students in Mrs. Digital Literacy Resource Platform. 21 Windows Shortcuts For Better Windows Experience. Windows shortcuts are meant to make work more easier and faster, sometimes it nags users when their mice malfunctioned and don’t work, and if they are not aware of the windows shortcuts things go even worse. For rapid access and for smart work users should aware of some exceptional windows shortcuts. STG is sharing the best and easy windows shortcut which will allow users to perform their tasks more easily and specifically and will turn them a smart window user.\nHere are some pointers that'll save you Googling \"How to Google. \" This infographic certainly added something new to a topic that we knew our client's target audience really cared about. Actionable content + Modular design 23000+ social shares Referring Sites: WirtschaftsWoche Hubspot News.com.au Adweek BGR State Of Digital Le Journal du Geek Embed Code: <p>How To Be A Google Power User [Infographic] by the team at<a href=' id='the_img_link'> NeoMam</a></p> Digital literacy standards. Digital Literacy. Digital Literacy. Digital literacy standards. Teacher Resources | National Tech Goes Home. EveryoneOn.org.\nHome | digitalliteracy.gov. Digital Literacy Home. Welcome to the Microsoft Digital Literacy curriculum. Whether you are new to computing or have some experience, Digital Literacy will help you develop a fundamental understanding of computers. The courses help you learn the essential skills to begin computing with confidence, be more productive at home and at work, stay safe online, use technology to complement your lifestyle, and consider careers where you can put your skills to work. Use the menu below to see the Digital Literacy curricula and courses available in your preferred language.\nAfter you select a language, click “go”, and the offers available will appear in a new dropdown box. Select an offer, and click “go”, and you will be taken to the appropriate page. The Microsoft Digital Literacy curriculum has three levels. The Basic curriculum features a course called A First Course Toward Digital Literacy. The Standard curriculum is available in four versions. Digital Literacy Definition and Resources. What is Digital Literacy? The ability to use digital technology, communication tools or networks to locate, evaluate, use and create information. 1The ability to understand and use information in multiple formats from a wide range of sources when it is presented via computers. 2 A person’s ability to perform tasks effectively in a digital environment...\nLiteracy includes the ability to read and interpret media, to reproduce data and images through digital manipulation, and to evaluate and apply new knowledge gained from digital environments. 3 What is a Digital Learning Librarian? The Digital Learning Librarian at the University of Illinois works collaboratively with librarians and faculty to create tools that help to integrate the library into the teaching and learning process. One result is the creation of online resources that focus on infusing library and information skills with instructional technology to help individuals obtain digital literacy. @ Other Institutions...\nResources. What is digital literacy? Digital literacy is the topic that made the ETMOOC learning space so irresistible to me… I think as educators we spout off about wanting our students to be digitally literate, but not many of us (myself included) have a firm grasp about what that actually means, and quite a number of us are still attempting to become digitally literate ourselves. Whatever that means. It turns out, defining digital literacy isn’t such an easy task. The etmooc community was fortunate enough to hear Doug Belshaw speak on this topic in a recent webinar. I’ve followed Doug on Twitter for quite some time, and it turns out his dissertation investigates just what is digital literacy… and his TED talk can be viewed here. Doug explained that digital literacy is quite ambiguous, and he doesn’t have all of the answers when it comes to defining these terms. 30 definitions of digital literacy represented in one of the first texts about the topic (from Gilster, published in 1998!!)\nCornell University - Digital Literacy Resource. US Digital Literacy.\nWhat is digital literacy doug belshaw edd thesis final. Information Technology » Innovate Northeast Florida - Kelly McCarthy. Tech Goes Home. Non-profit. National Digital Inclusion Alliance. The Internet is Important to Everyone. Research | Connected Nation. Newsletter Sign-up Contact Information First Name* Address 1 City Email* Last Name* Address 2 Zip Code Phone I currently do not have broadband Why Not? Please Choose One *Denotes required field Choose Topic Coverage Area Year View Business Survey View Residential Survey.\nNova Scotia Community Access Program. Digital Nova Scotia. Social Inclusion: Societal & Organizational Implications for Information Systems. People-Aware Computing Lab - Cornell University. Universal Usability: Resources. Web Sites, Articles, and Tutorials Organizations and Conferences Researchers and Practitioners Vicki Hanson, IBM Simon Harper, University of Manchester (UK) Sarah Horton, Dartmouth College Jonathan Lazar, Towson University Alan Newell, University of Dundee Whitney Quesenbery, Whitney Interactive Design Andrew Sears, UMBC Ben Shneiderman, University of Maryland Gregg C.\nVanderheiden , University of Wisconsin-Madison Cynthia Waddell, International Center for Disability Resources on the Internet (ICDRI)","How to measure the impact of a Digital Champion project\nGeneral evaluation principles & resources\nWhen we measure the impact of a digital inclusion project it’s good to consider all stages of the project including:\n- Why you are evaluating – to demonstrate impact? To understand what works and why?\n- Who the audience for the evaluation will be: internal/external (for instance a funder), general public or beneficiaries?\n- Researching what stakeholders and beneficiaries would find most effective and have maximum impact.\n- Developing a Theory of Change or a Logic Model to facilitate structured planning and evaluation of the proposed activities. Information to help develop these can be found in the resources section.\nIdentify the project stakeholders and incorporate their views– clearly identifying what’s important to them to make the project successful. Stakeholders could include the beneficiaries of your project, as well others such as the funder of the project. Consider what questions to ask and what the areas for discussion might be. For funders this could be, why have they funded you and what impact do they expect to see? For beneficiaries this could be what the barriers they face are, what are potential digital hooks to develop their interest in support and training?\nDecide what you want to measure and how you will collect the information and data you need. Are you interested in counting just the number of participants, or do you want to evidence how gaining new skills has had an impact on the individual? It’s good to start to collect data at the start of the project so you can track progress over time.\nOnce the project is up and running it can be valuable to re-visit the assumptions that underpinned the evaluation model and adjust them to reflect operational experience.\nOutput and outcome information enables projects to assess the impact of the work they are doing. The National Lottery Community Fund provides some useful definitions for the terms involved.\nOutputs are the products, services or facilities that result from an organisation’s or project’s activities. For example in a programme to improve well-being amongst older people, outputs might include the different types of interventions being offered by projects, or the numbers of people overall participating in activities under the programme.\nOutcomes are the changes, benefits, learning or other effects that result from what the project or organisation makes, offers or provides. For example, for the same well-being programme, outcomes might be improvements in clients’ physical or emotional health, or projects’ improved ability to extend their reach to different client groups.\nImpact is the broader or longer-term effects of a project’s or organisation’s outputs, outcomes and activities. For example, in addition to an understanding of the extent to which projects funded by the well-being programme have achieved their outcomes, there might be a longer-term change in the way some projects work with their clients, new partnerships may have developed, or policy may have been influenced at a local or wider level.\nOne Digital event – Measuring the impact of digital inclusion projects\nIn November 2018 the Community of Practice held a learning event on ‘Measuring Impact’ – the keynote opening presentations are available as well as information from the different workshops. There are also some useful background resources on evaluation.\nMeasuring the impact on learners, Digital Champions & partnerships\nA useful summary of How to measure the impact of a Digital Champion project on learners can be accessed here.\nA discussion paper on How to measure the impact of working in partnership can be accessed here.\nSome useful tips on How to measure the impact on Digital Champions can be accessed here.\nRecording digital inclusion outputs\nProjects will record outputs such as numbers of learners reached and how many sessions they have attended.\nThe Digital Unite Digital Champions Network (DCN) supports One Digital projects to use a tally facility to record the numbers of people accessing digital training and support. The DCN also has a web app enabling Champions to record their learner interactions quickly and easily.\nOutcomes for the individual\nOne option for digital inclusion projects wanting to develop measures for the impact on the individual would be to capture the developing skills of learners in more detail. This could be linked to the Essential Digital Skills Framework which provides a set of skills for life and work. SCVO have developed a toolkit for projects to use with their learners to measure their confidence levels before and after a learning intervention.\nThere is also an international framework, DQ (Digital Intelligence) Framework, which is a set of global standards for digital literacy, skills and readiness.\nLearner progress in developing these skills can be used to measure support provided by digital inclusion projects. The Framework also includes Digital Foundation skills which are needed by a new learner and cover the basics of setting up and using a device(s).\nSome projects will aim to develop digital skills in all areas of the Framework, others will focus on particular outcomes which arise once digital skills are being confidently used. Some projects may capture learning after a specific period of training and intervention. Others may be interested in looking at longitudinal impact over a period of time on the individual. Here are some examples to consider.\nA training centre providing a 6-8 week structured programme of learning supported by a professional Digital Champion would be likely to use all elements of the Framework to review skills developed. The centre would aim to provide learners with a full range of digital skills.\nAn employment project might measure Foundation skills, confidence and motivation plus specific skills needed to move into work such as CV building or searching for jobs online.\nA mental health project supporting people transitioning from a residential setting to successfully living in the local community, might look at how digital skills and confidence develop over a period of two years.\nWider digital inclusion project outcomes\nPossible outcomes that you might want to consider are outlined in the table. The majority of digital skills projects are set up to achieve improved digital skills for the individual beneficiary. Sometimes organisations establishing digital skills projects have multiple outcomes they want to achieve.\nSome digital inclusion projects are set up alongside a particular organisational digital transformation (a change to a digital delivery of a service) and carry out projects alongside that change.\nA social housing provider wanting to support tenants to access services such as booking repairs online sets up a project which develops a new repairs software system, staff receive training in using the new system. Once launched, staff promote the advantages of using the new system and identify individual tenants who need support to use the new online repairs booking system.\nA young people’s project working with 16-24 year old’s who are not in work, education or training sets up a digital skills group to support their gaining IT skills that will help them find work, at the same time they run some sessions on managing your digital footprint and staying safe online.\nA partnership project which includes charities, the libraries service, a local supermarket and local authority frontline staff runs a project designed to improve public knowledge about where local people can go to access free Wi-Fi and 1-1 support.\nThe table below provides some example outcomes that could be used in measuring the impact of a project. The examples are illustrative of an approach that looks at the impact on the individual, for an organisation and for the wider community.\nLinked outcomes and other barriers to digital skills development\nDigital skills development is one element of digital inclusion support and overcoming the barriers people face could include measuring support to improve –\n- Literacy and confidence in using English\n- Financial capability\n- Access to appropriate devices to get online (such as recycled laptop schemes, or schemes with IT providers to access cheaper tablets)\n- Access to social tariffs to reduce costs of having broadband at home for people on income based welfare benefits\nDigital Inclusion Evaluation Toolkit\nThere is a useful Digital Inclusion Evaluation Toolkit published by the Department for Digital Culture Media and Sport (DCMS) and developed with a range of partner contributions – the outcome categories and suggested measures for each outcome are fully identified in the Toolkit. More about the background to the development of the Toolkit and the case for using it can be found in this blog by Douglas White, Head of Advocacy at Carnegie UK Trust.\nIf you are interested in learning more about how organisations have evaluated the impact of their digital inclusion projects here are some examples."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:1290eaa0-a71d-41f1-85ed-d548c8345ea6>","<urn:uuid:26be8dd7-1502-430a-b0a4-232677595834>"],"error":null}
{"question":"Could you explain how the concept of standardization differs between STEP implementation Level 4 (Knowledge Base) and the APWG's anti-phishing information sharing model?","answer":"The STEP Knowledge Base implementation (Level 4) and APWG's sharing model represent different approaches to standardization. STEP's Level 4 stores data as objects with attached constraint rules from the STEP standard, allowing for complex relationships to be checked and enforced without additional programming effort. In contrast, the APWG's model uses the IODEF data format with anti-phishing extensions for specific use cases, and actually allows for proprietary formats in certain scenarios. For example, browser vendors prefer proprietary formats for maintaining URL block lists as it gives them flexibility to format and deliver data using secure protocols that exactly meet their needs. This demonstrates how STEP aims for comprehensive standardization while modern sharing models like APWG's allow for a mix of standardized and proprietary formats based on specific needs.","context":["The STandard for the Exchange of Product model data (STEP) is currently being developed by the International Organization for Standardization (ISO). It is intended to contain the unambiguous representation of product data throughout the entire life cycle of a product.\nThe unique feature of STEP is that it integrates product data. In today's environment, the data for a product is managed in many different systems with little integration and with data redundancy. The data redundancy is primarily caused by the lack of integration. Data must be replicated from system to system in order to maintain a correlation between the systems. STEP proposes to alleviate this problem by providing a single product data storage standard which integrates the data.\nThe STEP standard does not intend to standardize how an enterprise gets its data into the standard representation. The intention is to provide a migration path. The migration path is described by the levels of implementation of STEP. These levels are:\nLevel 1- Passive File\nLevel 2- Active File\nLevel 3- Database\nLevel 4- Knowledge Base\nThe passive file exchange is a flat ASCII file which contains the STEP data. This level is basically translator based. The active file is a memory resident image of the STEP data which can be used interactively by applications. The database implementation level provides additional facilities whereby applications can be constructed that take views of the data which are different than the native storage format, and support relational, network, hierarchical and object oriented types of queries. The knowledge base implementation stores the data as objects complete with attached constraint rules from the STEP standard. The first three levels of implementation can not attach rules to entities and attributes without the use of application code even though these rules can be and are defined in the standard.\nSTEP can be somewhat directly implemented. The standard is written in the EXPRESS language that was developed for the purpose of allowing the standard to be human readable and computer interpretable. This means that programs can be written to use the standard as input. The standard defines the data structure and the rules for data storage. A program can read the standard and determine the data storage structure. The program can also be provided with mappings between STEP entities and attributes to an application's entities and attributes. This structure allows for complete restructure with a minimum of programming effort. This is especially true when entities and attributes are added, modified or deleted from the standard.\nEXPRESS is designed to be implementation independent and thus support the four levels of implementation defined for STEP to date. It defines a common interpretation of the standard in terms of data and rules. This common interpretation can be used to drive computer processes. In order to do so, both the data definitions and the rules must be implemented. In levels 1 through 3, the data definitions can be used to define mappings to other applications or between versions of the standard. In level 4, this can be extended to where not only data can be mapped, but processing requirements (in the form of EXPRESS local and global rules) and complex relationships can be checked and enforced without additional programming effort.\nEXPRESS has a number of dialects which are:\nThe overall structure of the standard is summarized here which is also known as STEP On A Page. In words, the basic breakdown is:\nFurther information on all these parts and their current status can be found here.\nThe piece of STEP which actually gets implemented, exchanged, or shared is the Application Protocols. Here is some various information on some of these:\nApplication protocols can be implemented via flat/ physical file translators via STEP Part 21 or via the Standard Data Access Interface (SDAI).\nAn important question to ask when considering the implementation of STEP is: Why should my company move to STEP rather than continue using existing accepted standards?\nThis is an important question. Existing standards, their implementation and use can provide some interesting bench marks with which to compare STEP. There is an important difference between STEP and most other standards. The difference is that most other standards deal only with particular application areas or particular deliverables or products. STEP is intended to store all data for a product throughout its life-cycle with out regard to discipline or application area.\nSTEP is an international standard. This designation gives STEP a distinct advantage over company, industry, and national standards for companies in today's economy. STEP data can be exchanged and shared across international boundaries. This means that products designed in one country could be produced anywhere in the world.\nSTEP is intended to deal with all types of products and all data related to those products. It also forces the integration of product data. This integration is a very distinguishing feature of STEP. No other standard currently offers this capability. This feature is very important to consider when evaluating STEP. Standards which do not provide this integration provide benefits only if data is reused. The integration provided in STEP can be an important advantage even if data is never exchanged. Integration of data and the systems that deal with that data eliminates or at least severely reduces redundancy. This eliminates the need for tasks required to support the redundancy.","In previous blogs and the recent RSA Perspective paper, I have emphasized the need to work through use cases, requirements, and sharing models before thinking about which standards best fit a use case and where they are necessary. As the co-chair of the Internet Engineering Task Force (IETF) Managed Incident Lightweight Exchange (MILE) working group, I’m often asked about the use of standards for information sharing, so let’s dive in!\nThe types of sharing between entities should be limited to what is useful and effective to assess and address threats, or provide proactive defense capabilities. The figure below depicts a few information exchange scenarios between small, medium, and large organizations (green and red circles), and analysis centers (blue circles), building from the groupings of who shares data in my previous blog.\nThe analysis centers in the figure may include consortiums, Internet service providers, threat intelligence feed service providers, and industry focused Information Sharing and Analysis Centers (ISACs). The analysis centers are usually interested in varied, large, and sometimes complex data sets. Additionally, the data may be focused on a specific use case or problem area of business value and importance to a specific user group, such as distributed denial of service (DDoS) or advanced persistent threats (APTs) that combine multiple attack vectors. In these instances, the analysis centers will often benefit from the use of standards to automate their information exchanges (depicted with the grey arrows).\nThe organizations (Org.) depicted in the figure may include large, medium, and small enterprises. Larger enterprises with sophisticated capabilities are typically interested in actionable data, whereas medium and small organizations are interested in having inherently secure systems with prioritized and sometimes automated remediation options. The larger organizations will have use cases for which they will benefit from the use of standards (depicted with green arrows). The smaller and medium sized organizations may receive protections directly from vendors or threat intelligence providers with or without the use of standards (depicted with blue and red arrows) in this evolved eco-system.\nSince the useful data to exchange varies between user groups and use case cases, it may only make sense to standardize formats for the data most commonly exchanged or at least often enough where there is a benefit from automation. Information that is less common to exchange and may vary over time, can be handled through unstructured data or formats and extensions that may not be standardized formally (private) or even proprietary. Developing a trusted ecosystem that is limited to the exchange of meaningful and actionable data between research analysis centers, members of a research analysis center, and customers of service provider intelligence threat feeds is a complex problem with promising solutions that continue to emerge and evolve.\nThe RSA Perspective outlined a few effective sharing models, let’s expand on the Anti-Phishing Working Group (APWG) example and think about where standards are important. In the figure below, think of the Cyber Crime Data Clearinghouse as an example analysis center.\nIn this sharing model, the APWG analyzed the anti-phishing use case and determined what information was important to exchange. Reports of anti-phishing incidents are aggregated into their repository, or Cyber Crime Clearinghouse, using the IODEF data format either through a web form or the receipt of formatted IODEF documents. The IODEF data model plus the anti-phishing extension defined in RFC5901 meet their requirements for the information needed in this use case and enable interoperable exchanges in and out of their repository. The recent move to full automation enabled consistent exchanges of information, such that the receiver is able to parse and interpret exactly what was intended by the sender. That is where standards are important, and there is no shortage of them!\nYou will see that APWG vendor members, such as the browser vendors, have access to the repository or receive feeds of data in an IODEF format. The RSA Anti-Fraud Command Center is an example of a member exchanging data with the APWG in IODEF formatted documents. RSA assists with the takedown of malware distribution servers or other compromised systems working through a mature process including law enforcement, who also work closely with the APWG exchanging IODEF formatted data. The browser vendors exchange information such as URLs for malicious web sites from repositories, including the APWG, to maintain URL block lists in browsers. Since each vendor that maintains this service for their browsers, they are responsible for updating the block lists within their product eco-system; therefore proprietary formats are actually preferred! Why? Well, it is pretty simple; they have the flexibility to format the data and deliver it using secure protocols that exactly meet their needs, which may evolve over time. Since the list is a focused set of data and includes a small number of elements, it only makes sense to use a proprietary format. The same is true for other threat intelligence providers or vendor solutions that exchange information within their product eco-system.\nI’m hearing the acronyms IODEF/RID, STIX/TAXII, CybOX, CVF, CVRF, OVAL, etc. what does this mean and should I care?\nIf you are designing the information exchanges in a sharing model, then yes, otherwise, no! We need to advance the conversation to center around use cases and automate effective sharing to the point that this just works. End users should not have to be aware of data formats, they should only be aware that their needs are met in data exchanges, products, and protections offered.\nHow do I decide what to use?\nThe data formats and transports are typically suited to different use cases. Evaluating what you need to share with whom for a particular use case may result in using a focused data format with some partners and a more comprehensive set of data formats with others. This answer really depends upon the objectives for the sharing model.\nDoes one size fit all?\nNo. In the RSA Perspective paper, the mail abuse operators use their own format standardized through the IETF, the Abuse Reporting Format (ARF), because it meets their needs. The APWG uses IETF’s IODEF standard with extensions as it suits their needs. The Financial Services Information Sharing and Analysis Center (FS-ISAC) has decided to use MITRE’s STIX as they are interested in exchanging the threat information covered by that specification. And the list continues…"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:70620729-4a3f-454f-9bc9-e2e1779daa2f>","<urn:uuid:3754a0a2-62c5-4f89-9542-0f2958494013>"],"error":null}
{"question":"What are the physical challenges for transport systems in Utah, and what cybersecurity risks do modern transport face worldwide?","answer":"In Utah, transport systems face extreme physical conditions with temperatures dropping well below freezing in mountain winters and becoming extremely hot in desert summers, requiring components like Enable-IT Ethernet extenders that can operate in temperature ranges from -22°F to 167°F. On the cybersecurity front, modern transport systems globally face significant risks, with ransomware attacks on the transportation industry increasing by 186% in 2020. These attacks can lead to disruptions like interruption of traffic lights, electronic signals, railway systems, ticketing machines, and blocked access to backend systems and data.","context":["UTA (Utah Transit Authority) – Energizes its Transportation Grid\nMeeting Evolving Transportation System Needs\nEstablished in 1970, the Utah Transit Authority (UTA) has become a multi-modal transportation leader that is 100-percent accessible with 69 light rail vehicles, 30 commuter rail cars and more than 600 buses. UTA’s TRAX light rail system is currently averaging more than 58,300 riders a day along its 15-mile Salt Lake-Sandy line and the 4-mile University Line.\nProviding the transportation system that gets us to work, takes children to school and shuttles us to entertainment wherever we wish to go may seem trivial, but can be a complex business. In the state of Utah, temperatures drop well below freezing in mountain winters and get extremely hot in the desert summers. Magnetic sensors in the street and traffic monitoring cameras can be mounted hundreds of feet from the traffic control system and often have no existing power cabling except at the control system box.\nPowering Remote Traffic Surveillance Cameras with Power Over Ethernet (PoE)\nProviding power to devices (cameras, ticket machines and street sensors) can get expensive. Previously, every device would require its own electrical connection, power adapter and cable. When considering cable runs of over 2000 feet, this can get expensive as well. The solution is to provide electricity via Power over Ethernet (PoE) technology that allows both power and data to be transmitted over the same Category 5 (a.k.a. Cat 5) cable that is used for data. However, the PoE standard is often too short for most traffic-related requirements.\nThe Utah Transit Authority needed to run both PoE power and Ethernet data over long distances. They leveraged Enable-IT’s solid state Ethernet extender technology to achieve their goals. The UTA can now power older remote ticketing systems, modern PoE cameras and street sensors 24x7x365 on its light rail and bus transportation station congestion control networks up to 2500 feet away.\nOperating in the Extreme Winter Cold and Summer Heat\nOutdoor temperatures in Utah greatly fluctuate throughout the year. The cameras, magnetic sensors, control systems and wires heat and cool throughout the day. Such dynamic conditions require solid state electronic components. The UTA tested several components and selected the Enable-IT 860 and 865 Ethernet extenders, which can support temperature ranges as extreme as from -30°C (-22°F) to +75°C (167°F) over long distances.\nGoing the Distance – Over Existing Infrastructure\nThe story doesn’t end at the traffic stop. Traffic networks are a connection of traffic stops throughout the city, county and state. Some stops are significant distances from each other. To leverage the power of a traffic network, these stops must all be connected. Ethernet only runs 328 feet, so connecting stops that are a mile apart, can become a major issue. Also, traffic networks are traditionally tied together with regular phone wire, not Category 5 Ethernet cable. Replacing countless miles of cable gets extremely expensive.\nFortunately the UTA tested Enable-IT’s 860 long distance Ethernet extender, capable of powering Ethernet up to 6000 feet over existing telephone cable as well as Category 5 Ethernet cable. This proved to be the solution needed to help tie everything together on the UTA’s existing infrastructure.","Keeping transport safe, secure, and sustainable\nKeeping transport safe, secure, and sustainable\nWhether by rail, road, air, or sea, transport and logistics systems play a critical role in supporting the global infrastructure network. Today’s transportation systems are more sophisticated than ever, comprising complex plans, large volumes of real-time information, and connectivity through the industrial internet of things (IIoT).\nCybersecurity for the transportation sector faces two primary challenges:\nThe digitisation of railway systems, such as the European signaling ERTMS system (European Rail Traffic Management System), has made public transport systems vulnerable to a new generation of cybersecurity threats. In 2020 alone, ransomware attacks on the global transportation industry saw a 186% increase year-over-year.\nThese challenges require continuous updating of cybersecurity practices and new methodologies to keep transport systems agile. Failure to take action can be costly and disruptive, potentially exposing organisations to liability and legal action, particularly when customer data breaches are involved.\nAs global transportation and logistics networks continue to evolve alongside the rise of new technologies, organisations like yours must recognise the importance of cyber resilience and its ability to protect cargo and passengers.\nResilience to cybersecurity attacks requires more than just creating controls and processes. Your transport firm’s resilience begins and ends with the people in charge of data and assets. From IT personnel to executives, every employee must adopt a cybersecurity-first mindset. This means recognising that people are the first line of defence against threats. Through training and inhouse workshops, safety and security can be at the heart of your practises and corporate culture.\nThe rapid digitalisation of transport networks puts you at risk of disruptions caused by data breaches. As more devices and transport control systems depend on online connectivity, vulnerabilities will inevitably arise, increasing the potential for sensitive data to wind up in unscrupulous hands.\nWith new threats constantly emerging, you must do your due diligence to identify risks and shut down attacks before they affect operations. Steps that you can take to manage cybersecurity threats include:\nAt TÜV SÜD, our cybersecurity specialists provide advisory, assessment, training, audit, and certification services specifically designed for the transportation industry. Our solutions cover all aspects of IT security in the transportation sector, from rail cybersecurity and IT penetration testing to TS 50701 and IEC 62443 (railway) certification projects.\nOvercoming external and internal cybersecurity issues and threats to critical IT infrastructure requires expertise and experience. Powered by over 150 years of safety and security experience, TÜV SÜD delivers unbiased advice and trusted in-house safety and security workshops for transportation firms.\nAs a leader in helping organisations worldwide navigate through their digital transformation journey, TÜV SÜD is acutely aware of the cybersecurity challenges that come with digitalisation. We are here to work alongside your team to conduct comprehensive security tests and stay on top of new regulatory requirements in the transportation industry.\nWork with a trusted name in cybersecurity to ensure the resilience of your transportation networks and keep your customers’ data safe. Our team of 25,000 multidisciplinary experts from more than 1,000 locations around the world is here for you.\nAligning systems with security-by-design principles\nRailway companies must follow risk management procedures and conduct continuous auditing according to industry-specific guidance. Adhering to security-by-design principles ensures that the networks and technologies supporting your infrastructure are designed and built securely.\nMeeting KRITIS requirements for IT security\nIn Germany, KRITIS providers, such as water, food, electricity, and transport, are required under the IT Security Act 2.0 to demonstrate that they are enforcing cybersecurity measures to protect their systems. The constant threats you face require you to maintain the highest safety and security standards.\nSecuring products and systems data operational risks\nYour firm has a wide range of datasets flowing between physical and digital systems, allowing cybercriminals to hide in the heavy traffic to attack and control informational and operational data. Your IT infrastructure must have the appropriate security measures and defences in place to manage these risks.\nDetecting and responding to security incidents\nApart from compromised data, cybercrime can have a debilitating impact on railway companies due to disruptions like for example: 1. Interruption of traffic lights, electronic signals, toll booths, and railway signal systems 2. Disruption of ticketing machines and fare gates 3. Blocked access to backend systems and data\nPenalties and fines for non-compliance\nKRITIS organisations that fail to comply with industry regulations face fines of up to €20 million or up to 4% of their annual turnover, whichever is higher. Failure to comply with KRITIS regulations may also lead to imprisonment and the prosecution of responsible executives, managers, and personnel.\nEnsuring the availability and resilience of operational systems\nCyberattacks on transport networks can have lingering effects that last weeks, if not months. Attacks that can paralyse transportation systems and networks include: DDoS attacks, DNS attacks, credential stuffing, brute force attacks, DNS spoofing, malware, data manipulation and content theft, and phishing.\nA compact overview of the functional safety regulation landscape\nLearn more about the safety challenges for autonomous machinery\nLearn about current trends and challenges and get an overview about opportunities offered by functional safety."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:59b6bd24-09f0-415c-9f8c-23ab24f0bdd2>","<urn:uuid:83c51e42-7251-472e-8c9a-d89ae9db46e6>"],"error":null}
{"question":"Does Multiple Sulfatase Deficiency affect speech development in a similar way to how gluten sensitivity impacts nutrient absorption?","answer":"While both conditions can impact bodily functions, they work differently. Multiple Sulfatase Deficiency (MSD) directly affects speech development as part of its symptoms, with children showing delayed development and impaired speech. In contrast, gluten sensitivity affects nutrient absorption in the intestines due to intestinal damage, but does not directly impact speech development. The nutrient absorption issues in gluten sensitivity can be reversed by following a gluten-free diet, while MSD's speech impacts are part of the genetic disorder's core symptoms.","context":["Multiple sulfatase deficiency is a very rare hereditary metabolic disorder in which all of the known sulfatase enzymes (thought to be seven in number) are deficient or inoperative. Major symptoms include mildly coarsened facial features, deafness, and an enlarged liver and spleen (hepatosplenomegaly). Abnormalities of the skeleton may occur, such as curvature of the spine (lumbar kyphosis) and the breast bone. The skin is usually dry and scaly (ichthyosis). Before symptoms are noticeable, children with this disorder usually develop more slowly than normal. They may not learn to walk or speak as quickly as other children.\nSymptoms of multiple sulfatase deficiency usually start during the first or second year of life. Children with this disorder usually have coarse facial features and they are often deaf. The liver and spleen are usually enlarged. Curvature of the lower portion of the spine and an abnormal breast bone usually also occur. In addition, the skin is dry, scaly and itchy (ichthyosis). Development is usually delayed in children with this disorder. Children with multiple sulfatase deficiency may not walk normally and their speech is usually impaired.\nLaboratory tests show abnormalities in cells of the bone marrow and in white blood cells. The bone behind the nasal bones (sella turcica) is J- shaped and the little bones of fingers and toes (phalanges) are broader than normal. Levels of urinary sulfatides are higher than normal. A deficiency of several enzymes (arylsulfatase A, B, and C, two steroid sulfatases and four other sulfatases) occurs. In normal concentration, these enzymes are needed to break down certain carbohydrates known as “mucopolysaccharides”.\nMultiple sulfatase deficiency is a hereditary disorder transmitted through autosomal recessive genes.\nRecessive genetic disorders occur when an individual inherits the same abnormal gene for the same trait from each parent. If an individual receives one normal gene and one gene for the disease, the person will be a carrier for the disease, but usually will not show symptoms. The risk for two carrier parents to both pass the defective gene and, therefore, have an affected child is 25% with each pregnancy. The risk to have a child who is a carrier like the parents is 50% with each pregnancy. The chance for a child to receive normal genes from both parents and be genetically normal for that particular trait is 25%. The risk is the same for males and females.\nAll individuals carry 4-5 abnormal genes. Parents who are close relatives (consanguineous) have a higher chance than unrelated parents to both carry the same abnormal gene, which increases the risk to have children with a recessive genetic disorder.\nSymptoms are caused by a deficiency of the enzyme arylsulfatase A, B, and C, 2 steroid sulfatases, and 4 other sulfatases that are needed for the breakdown of certain carbohydrates known as “mucopolysaccharides”.\nMultiple sulfatase deficiency is present at birth, although symptoms of this disorder don’t become noticeable until the first or second year of life. It is a very rare disorder affecting males and females in equal numbers.\nTreatment for the symptoms of skeletal abnormalities in multiple sulfatase deficiency is symptomatic and supportive. An orthopedist can provide treatment for curvature of the spine. Dermatologic symptoms (ichthyosis) are treated by applying skin softening (emollient) ointments, preferably plain petroleum jelly. This can be especially effective after bathing while the skin is still moist. Salicylic acid gel is another particularly effective ointment. The skin should be covered at night with an airtight, waterproof dressing when this ointment is used. Lactate lotion can also be an effective treatment.\nInformation on current clinical trials is posted on the Internet at www.clinicaltrials.gov. All studies receiving U.S. government funding, and some supported by private industry, are posted on this government web site.\nFor information about clinical trials being conducted at the NIH Clinical Center in Bethesda, MD, contact the NIH Patient Recruitment Office:\nTollfree: (800) 411-1222\nTTY: (866) 411-1010\nFor information about clinical trials sponsored by private sources, contact:\nMancini GMS, van Diggelen OP. Multiple Sulfatase Deficiency. In: NORD Guide to Rare Disorders. Lippincott Williams & Wilkins. Philadelphia, PA. 2003:484.\nHopwood JJ, Ballabio A. Multiple sulfatase deficiency and the nature of the sulfatase family. In: Scriver CR, Beaudet AL, Sly WS, et al. Eds. The Metabolic Molecular Basis of Inherited Disease. 7th ed. McGraw-Hill Companies. New York, NY; 2001:3725-32.\nMancini GM, van Diggelen OP, Huijmans JG, et al. Pitfalls in the diagnosis of multiple sulfatase deficiency. Neuropediatrics. 2001;32:38-40.\nMacaulay RJ, Lowry NJ, Casey RE. Pathological findings of multiple sulfatase deficiency reflect the pattern of enzyme deficiencies. Pediatr Neurol. 1998;19:372-76.\nCastano Suarez E, Segurado Rodriguez, Guerra Tapia A, et al. Ichthyosis: the skin manifestation of multiple sulfatase deficiency. Pediatr Dermatol. 1997;14:269-72.\nSchmidt B, Selmer T, Ingendoh A, et al. A novel amino acid modification in the sulfatases that is defective in multiple sulfatase deficiency. Cell. 1995;82:271-78.\nRommerskirch W, von Figura K. Multiple sulfatase deficiency: catalytically inactive sulfatases are expressed from retrovirally introduced sulfatase cDNAs. Proc Natl Acad Sci U S A. 1992;89:2561-65.\nFROM THE INTERNET\nMcKusick VA, Ed. Online Mendelian Inheritance in Man (OMIM). The Johns Hopkins University. Entry Number; 272200: Last Edit Date; 11/17/98.\nU. S. National Library of Medicine. Multi[ple Congenital Anomaly/Mental Retardation (MCA/MR) Syndromes. Last Updated: 27 October, 1999.\nvon Figura K. Inherited disorders caused by faulty protein modification in the secretory route.\nProc. German Society for Biochemistry and Molecular Biology. 2002:2pp.\nMaire I. Mucosulfatidosis. 2002:1p.\nAustralian Leukodystrophy Support Group Inc. Multiple sulfatase deficiency (MSD).\nAustralian Leukodystrophy Support Group Inc. Metachromatic Leukodystrophy (MLD).\nThe information in NORD’s Rare Disease Database is for educational purposes only and is not intended to replace the advice of a physician or other qualified medical professional.\nThe content of the website and databases of the National Organization for Rare Disorders (NORD) is copyrighted and may not be reproduced, copied, downloaded or disseminated, in any way, for any commercial or public purpose, without prior written authorization and approval from NORD. Individuals may print one hard copy of an individual disease for personal use, provided that content is unmodified and includes NORD’s copyright.\nNational Organization for Rare Disorders (NORD)\n55 Kenosia Ave., Danbury CT 06810 • (203)744-0100","Gluten is the protein found in wheat, rye, and barley, but it’s also found in foods like ice cream and ketchup. Gluten-free diets are typically followed by people suffering from a gluten sensitivity or celiac disease, a condition that causes a negative reaction to gluten and results in damage to the intestines. This damage makes it difficult for the body to absorb necessary nutrients and leads to vitamin and mineral deficiencies.\nGluten-free diets have become part of the weight loss fad. However, a gluten-free diet isn’t necessarily healthier and often leads to weight gain. Many gluten-free products are high in processed carbs and sugar. A person not dealing with gluten sensitivity or celiac disease would be better off shopping for a variety of high-fiber carbs, lean proteins, colorful fruits and veggies, and healthy fats. One hundred percent whole-wheat barley, wheat, and rye are also packed with fiber, which can help lower cholesterol and improve digestive health.\nPros of Eliminating Gluten\n- If you have a gluten intolerance or sensitivity, you may have inflammation or damage to the intestinal tract.\n- Eating gluten free can help reverse this damage and inflammation.\n- Encourages label reading and more awareness of food.\n- Leads to a healthier diet filled with less processed foods.\n- Introduces higher quality grains, like quinoa, into your diet.\nWhen Dining Out, Talk It Out :One of the biggest challenges in maintaining a gluten-free diet is decoding a restaurant menu. Don’t be shy. Talk with your server or the chef and explain your dietary needs — they’re there to satisfy you.\nCons of Eliminating Gluten\n- Reduced carbohydrate intake due to lack of education on nutrients (not all carbs have gluten)\n- Lack of fiber from traditional sources can lead to digestive issues\n- Possible weight gain from eating gluten-free products, which often contain higher levels of fat and sugar\n- Possible weight gain as the intestinal track recovers and begins to absorb nutrients properly\n- Possible weight loss and consumption of a nutrient deficient diet from eliminating too many foods for fear of a negative reaction\nBottom line: If you think you have a gluten sensitivity or celiac disease, see your naturopath or doctor. Don’t go on a gluten-free diet without checking with them first. Going gluten-free and then getting checked by your doctor can affect the results of the blood test used to diagnose celiac disease.\nSay Bye-Bye to Bread…Mostly: Perhaps the most difficult step in a gluten-free diet is bidding farewell to bread as you know it — that includes white, wheat, marble, and rye. Also off limits are bagels, muffins, croissants, hamburger buns, scones — you get the idea. Yes, even pizza. But don’t despair. There are alternatives.\nA person with celiac disease (CD) cannot eat food that contains gluten, a protein found in wheat, rye, barley and in most oats (unless specifically labeled gluten-free).\nCD is NOT an allergy. It is an autoimmune disorder, like type 1 diabetes, Crohn’s, rheumatoid arthritis, and a host of others. Automimmune disorders are the result of an overactive immune system which mistakes some part of the body as a pathogen and attacks it.\nIn CD, when gluten is ingested, the body attacks the wall of the small intestine and destroy’s the body’s ability to process food and absorb nutrients. There is also a recognized form of celiac disease that attacks the skin, called dermatitis herpetiformis (DH). Celiac may also affect the liver, thyroid and nervous system in some people, but to what degree scientists are still uncertain.\nWhen a person with CD stops ingesting gluten, the body (usually) gradually stops attacking itself and the intestine/skin is able to heal. Any re-introduction of gluten starts the autoimmune process back up again, whether or not any other symptoms appear. The amount of gluten required to kick off the autoimmune response has been measured in the ‘parts per million’, so it is very tiny.\nYou Have Gluten-Free Bread Choices : Many health foods stores and some major supermarkets now carry gluten-free products, including an assortment of breads. These are often made with rice or potato flour instead of wheat products. Just check the label to make sure it says “100% gluten-free.”\nA gluten-free diet is a diet that excludes the protein gluten. Gluten is found in grains such as wheat, barley, rye and triticale (a cross between wheat and rye).\nA gluten-free diet is used to treat celiac disease, an autoimmune disorder that can appear at any age and is caused by an intolerance to gluten. Gluten causes inflammation in the small intestines of people with celiac disease. Eating a gluten-free diet helps people with celiac disease control their signs and symptoms and prevent complications.\nInitially, following a gluten-free diet may be frustrating. But with time, patience and creativity, you’ll find there are many foods that you already eat that are gluten-free and you will find substitutes for gluten-containing foods that you can enjoy.\nGluten ‘Red Flags’ : People on a gluten-free diet need a sharp eye for labels. Some ingredient red flags are obvious, like wheat, wheat gluten, barley, or rye. But some foods have “stealth” gluten. Two terms to watch for are malt (which is made from barley) and hydrolyzed vegetable protein (it often contains wheat). And while oats do not contain gluten, they may also increase symptoms, including abdominal pain, bloating, and diarrhea.\nSwitching to a gluten-free diet is a big change and, like anything new, it takes some getting used to. You may initially feel deprived by the diet’s restrictions. However, try to stay positive and focus on all the foods you can eat. You may also be pleasantly surprised to realize how many gluten-free products , such as bread and pasta, are now available. Many specialty grocery stores sell gluten-free foods. If you can’t find them in your area, check with a celiac support group or go online.\nIf you’re just starting with a gluten-free diet, it’s a good idea to consult a dietitian who can answer your questions and offer advice about how to avoid gluten while still eating a healthy, balanced diet.\nMany healthy and delicious foods are naturally gluten-free:\n- Beans, seeds, nuts in their natural, unprocessed form\n- Fresh eggs\n- Fresh meats, fish and poultry (not breaded, batter-coated or marinated)\n- Fruits and vegetables\n- Most dairy products\nIt’s important to make sure that they are not processed or mixed with gluten-containing grains, additives or preservatives. Many grains and starches can be part of a gluten-free diet:\n- Corn and cornmeal\n- Gluten-free flours (rice, soy, corn, potato, bean)\n- Hominy (corn)\nAvoid all food and drinks containing:\n- Barley (malt, malt flavoring and malt vinegar are usually made from barley)\n- Rye Triticale (a cross between wheat and rye)\nAvoiding wheat can be challenging because wheat products go by numerous names. Consider the many types of wheat flour on supermarket shelves – bromated, enriched, phosphated, plain and self-rising. Here are other wheat products to avoid:\n- Durum flour\n- Graham flour\nAvoid unless labeled ‘gluten-free’\nIn general, avoid the following foods unless they’re labeled as gluten-free or made with corn, rice, soy or other gluten-free grain:\n- Cakes and pies\n- Cookies and crackers\n- French fries\n- Imitation meat or seafood\n- Processed luncheon meats\n- Salad dressings\n- Sauces, including soy sauce\n- Seasoned rice mixes\n- Seasoned snack foods, such as potato and tortilla chips\n- Soups and soup bases\n- Vegetables in sauce\nCertain grains, such as oats, can be contaminated with wheat during growing and processing stages of production. For this reason, doctors and dietitians generally recommend avoiding oats unless they are specifically labeled gluten-free.\nCheers! You Can Still Raise a Glass : Wine and liquors are generally gluten-free, so you can still raise a glass and offer a toast, no matter what the occasion.\nWatch for cross-contamination\nCross-contamination occurs when gluten-free foods come into contact with foods that contain gluten. It can happen during the manufacturing process, for example, if the same equipment is used to make a variety of products. Some food labels include a “may contain” statement if this is the case. But be aware that this type of statement is voluntary. You still need to check the actual ingredient list. If you’re not sure whether a food contains gluten, don’t buy it or check with the manufacturer first to ask what it contains.\nCross-contamination can also occur at home if foods are prepared on common surfaces or with utensils that weren’t thoroughly cleaned after being used to prepare gluten-containing foods. Using a common toaster for gluten-free bread and regular bread is a major source of contamination, for example. Consider what steps you need to take to prevent cross-contamination at home, school or work.\nStay Symptom-Free: For most people with celiac disease, even small amounts of gluten can cause symptoms like gas and bloating, changes in bowel movements, weight loss, fatigue, and weakness. That’s why going gluten-free can be a big help — no matter how mild or serious your symptoms. Note: Check with your health care provider before making any major dietary changes.\nPeople with celiac disease who eat a gluten-free diet experience fewer symptoms and complications of the disease. People with celiac disease must eat a strictly gluten-free diet and must remain on the diet for the remainder of their lives.\nNot getting enough vitamins\nPeople who follow a gluten-free diet may have low levels of certain vitamins and nutrients in their diets. Ask your dietitian to review your diet to see that you’re getting enough of key nutrients like iron, calcium, fiber, thiamin, riboflavin, niacin and folate.\nNot sticking to the gluten-free diet\nIf you accidentally eat a product that contains gluten, you may experience abdominal pain and diarrhea. Some people experience no signs or symptoms after eating gluten, but this doesn’t mean it’s not damaging their small intestines. Even trace amounts of gluten in your diet may be damaging, whether or not they cause signs or sympt"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:f697200b-d894-4379-b63d-8ba10a05aef0>","<urn:uuid:06939337-7fbe-4e61-bd58-534c29affe63>"],"error":null}
{"question":"Can you play both COD Mobile and Urban Terror without paying any money?","answer":"Yes, both games are free to play. Call of Duty Mobile is explicitly stated to be free to download and play on Android and iOS devices. Similarly, Urban Terror is described as a 'free online multiplayer first person Hollywood tactical shooter'.","context":["COD Mobile or Call of Duty Mobile has started to become one of the most famous and successful games of the year. It is similar to PUBG Mobile both in gameplay and expansion strategy. They both are famous games for the PC and later they expanded to mobile devices too. The one for Mobile devices is free to play. This means that the player is not required to pay any fee to download the version made for Android and iOS devices. This was the move that made it even more popular. Gameloop and Tencent have been working to keep all the versions in sync with regular updates.\nGameloop Call of Duty Mobile emulator for PC\nFor players who prefer playing games on their computer, Gameloop Emulator for Windows 10 PC has been released. This emulator will allow the players to emulate COD Mobile on their PCs. This means that with the help of a keyboard and mouse, the player would be able to able to proceed with the gameplay.\nAll you need to do is download the executable file of this Gameloop emulator for Windows 10 PC and run it.\nOn the home screen, select Call of Duty from the list of games available. And it will take you to the landing page for Call of Duty.\nIt will start downloading the Gaming engine on which the game will run when you click the Download button.\nWhen it is done downloading the gaming engine, it will start to download the main game of Call of Duty Mobile from its servers.\nAfter the main game is successfully downloaded, you can just click on Play to start the game.\nIt will then work just like it does on an Android device – but the only difference would be the fact that you will be using your mouse and a keyboard to control your character during the gameplay.\nControls for Call of Duty Mobile on Gameloop Emulator\nThe following controls are available by default when you play Call of Duty Mobile on Gameloop Emulator,\n- Move: WASD [W – A – S – D keys on the left side of Keyboard]\n- Jump/Surface: Space button\n- Crouch/Go Prone/Slide: Press C to Couch. Hold C to Go Prone.\n- Reload: R\n- Free: Alt\n- Sprint: Shift + W\n- Stun Grenade: 3\n- Cluster Grenade: 4\n- Air Strike, Nova Gas or Smoke Grenade: 5\n- Multi-bang: 6\n- Hemostatic or First Aid Kit: 7/8/9\n- Pick Up Items, Open Doors, Parachute etc: F\n- Change loadout: F\n- Ragdoll: F4\n- Game Stats/Match Stats: Tab\n- Mic On/Off: Y\n- Run: =\n- Volume On/Off: T\n- Settings: ESC\n- Map: M\n- Chat/Player Statis: F2\n- Slide while running: Shift + W + Hold C\nIf you want to toggle any of these settings, you can click on the 3 horizontal bars on the top right corner of the emulator and select Settings which is the second option from the bottom. Make changes to the language, Display Quality, Resolution, Graphics Engine Rendering and more.\nThere are some mobile-specific controls on the right portion of the game screen if someone wants to record the gameplay, take a screenshot and more.","Posts by John Wagenleitner:\nThe 30+ clan is much larger than what you see here. We are a part of a larger organization of teams playing different games. Each team has its own Captain. The Captain chooses several officers to help run the team. Treasurer, Server Admin, Webmaster, that sort of thing. All team members are bound to follow the 30+ Charter. This is a set of guidelines to adhere to while being a member of 30+. You can find the link to the Charter and the main 30+ website in the above menu.\n30+ game servers are a place where adults over 30 can game together without any swearing, racisms, sexism, or trash talk. That doesn’t mean you can’t play on our server if you’re under 30. Everyone is welcome. It just means that if you want to join the Clan, you must be at least 29 ½.\nOur members have access to the member’s only sections of our website where they can have their own bios page, Wiki’s or Blogs, private or otherwise. As members you have guaranteed access to our servers with an Rcon password. You hang around with other members and close friends of the clan on TeamSpeak3, chatting and laughing it up. You can submit your own map cycles to be put into rotation, and take part in other 30+ events like shooting contests, 3 vs 3, or scrimming against other clans. You also get your own team email account. The biggest bonus is the camaraderie and hours of fun with a good group of people. Fun and good sportsmanship are our motto.\nCheaters beware! We have a zero tolerance towards cheaters.\nURT 30+ exists for the purpose of entertainment and friendship. Regardless of the Team or the game being played, we strive to present a mature attitude toward our playing, as far as being fair minded and good sports are concerned. We play for the sheer enjoyment of playing.\nAnyone playing on our servers should be aware of our rules. By entering the server, you are agreeing to follow our rules. If you break the rules, you will be warned and then kicked or permanently banned.\nThe server is paid for by our members and as such, in the case of a full server, we will kick to make room for a member. However, we do have some reserved slots for members, and this rarely occurs.\n- No swearing, sexist, racist, or offensive comments are allowed. That includes names, tags, or logos.\n- No intentional Team Killing. Friendly Fire is on.\n- No Spawn Camping or throwing grenades into the respawn.\n- No Trash Talking or Trolling.\n- Always Autojoin, meaning, don’t join a team because your friend is on it or because it’s winning.\n- Don’t change teams to balance things out. The 30+ Admins will manage the server. We quite often will force players to the other team at the half way point of the map in order to balance the teams. This usually makes for an exciting finish.\nNo password needed. Please add your URT in-game nickname.\nFor those of you not familiar with TeamSpeak 3.\nTeamSpeak is a gaming communications software allowing you live voice communication with other players in the game. Get in the action with live communications. Hear your opponent go down, and warn your teammates of incoming fire. All you need is a microphone. Headsets are the preferred method of choice.\nYou can download the free client here http://www.instantteamspeak.com//teamspeak-download.php\nCreate a new Bookmark and add ts3urt30.ts.nfoservers.com to the Address field. Enter your in-game handle to the Nickname field.\nWe at 30+ have some rules that we ask you to follow when gaming on our servers or speaking on our TeamSpeak channel. We’ve added the channel for you to improve your gaming experience by having a place to gather with your friends while playing urban terror. We do have some basic rules we wish you to follow. These are the same rules you must follow while playing URT on our server.\n- No swearing, in any language\n- No trash talking\n- No racist or sexist remarks\n- No cheating\n- No offensive nicknames\nPlease report anyone who is breaking the rules to urt30plus[at]gmail.com\nBreaking the rules means you are subject to getting banned. Just have fun!\nPlease use Push-To-Talk to activate your mic\nGo to Settings/Options and click on Capture. Select Push-To-Talk and click on the button to select your hotkey. Simply Push-To-Talk with the key of your choice. This is so that you do not have your microphone open at all times, otherwise everyone else will be hearing the feedback from your speakers, and everything else that is going on in your house. Once you get used to using the hotkey, it becomes second nature.\nThanks for gaming with 30+\nGood Luck & Have Fun\nUrban Terror – The game!\nI can quote from the official URT site http://www.urbanterror.info that Urban Terror is a free online multiplayer first person Hollywood tactical shooter, but that wouldn’t really give you a feel for the game, so let me expand on that and introduce you to the experience.\nBefore I do that, let me tell you what it isn’t. It isn’t like any other first person shooter I’ve played. While some of the online multiplayer games like Battlefield 3, Crysis 2, Call of Duty Modern warfare 3, and Left For Dead, just to mention a few, have the most amazing graphics, they don’t come close to the gameplay you will experience in Urban Terror.\nGameplay meaning the way you interact with your player. The players ability to quickly get into action and move around the maps.\nPlayer models can jump,\nclimb up ladders,\nThey can also jump to, and grab ledges, and pull themselves up.\nThey can sprint, and jump into a powerslide, strafing at the same time.\nUrban Terror has a large selection of weapons to choose from, and you don’t have to pay for them. You have access to whatever you are able to carry. This means you can only choose so many at a time. For instance, you may prefer to wear a helmet, carry a primary weapon, a sidearm, a medpack and some HE nades. Now, that would be about the limit of what you can select, at a time. Let’s say you decided to choose a secondary weapon. Then you would have to give up the medpack, for example. Or if you choose a silencer, you may have to hand in your nades. But the nice thing is you can pick up weapons and objects that other players drop when they die. But watch out, they may be out of ammo, and sometimes…they go Boom!\nThese are just a few of the things that make this game work so well. The way you can interact and become a part of the experience so quickly. You’re in the action from the moment you connect, and after you die, you get to take a quick guzz of whatever your pleasure is while you respawn in a matter of seconds. Don’t get caught with a handful of chips when you respawn though, cuz you’ll be right back in the action when you do.\nHere is an in depth look at the weapons in Urban Terror Urban Terror Weapons\nUrban Terror – The Experience\nI’m creeping along the rooftops. The sounds of sporadic gunfire permeate the air. Someone has a bead on me. Rounds are whizzing passed my ear. I pop my head up for a half second. A flash of light and a movement. I see him across the street firing from a window.\nI continue on my way as though I didn’t see him. Then I go into a crouch and go back the way I came. I take cover behind a chimney stack, pop up, zoom the scope on my G36, and open fire on the window. He still thinks I’m moving in the other direction. Too bad for him. My Heads Up Display tells me I scored the kill with two hits. His body flops back out of sight.\nCries come over the radio, “The base is being overrun!”\nI turn back and dash across the roofs, pulling the pin from my grenade. I’m counting the seconds before making the toss, praying my timing is right and the nade doesn’t explode in my hand. I throw it at the two enemy troops advancing on our flag.\n“Twofer!” I yell over TeamsSpeak. “Got em both.”\n“Area secured,” I send over the com. “I’m going for the flag!”\nI jumped down to the street and sprint down the road, jumping, gaining momentum. I hit my crouch button and go into a power-slide, strafing to the right. I get another kill, but a round pings off my helmet, knocking my health down to almost nothing.\nOne of the enemy throws smoke in front of his own flag area.\n“I need a Medic!” I cry out over the com, taking cover behind a fence.\nOne of my guys jumps to help me out. He hits his medic bind. I stay focused on the smoke, covering us. I hear the tell tale sign of bandaging. My health meter starts to climb. Then I’m hit again from out of the smoke. He must be wearing infrared goggles. I return fire where I saw the muzzle flash and score a lucky hit. My medic finishes the job and we quickly move into the adjacent yard.\nI check my HUD and see that my team is all moving in. We have them pinned. We’re going for the flag."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:d80b0d4b-fba4-4462-aa2f-1af8b2e6b120>","<urn:uuid:6374cd41-3d1e-4b7a-9dd7-f45abae5da54>"],"error":null}
{"question":"How does one measure the level of bilingualism in a person according to research from Switzerland?","answer":"According to research, bilingualism can be measured through relative proficiency using a formula that calculates the bilingual quotient. This method provides a useful basis for comparing and grouping bilinguals. However, when this method was developed, it had only been tested in pilot studies and had not yet been tried out on Swiss bilinguals, though initial tests showed it could be applicable there.","context":["LINGUIST List 23.3396|\nTue Aug 14 2012\nReview: General Linguistics; Sociolinguistics: Weinreich (2011)\nEditor for this issue: Joseph Salmons\nFrom: Ilaria Fiorentini <ilafiorelibero.it>\nSubject: Languages in Contact\nE-mail this message to a friend\nDiscuss this message\nAnnounced at http://linguistlist.org/issues/22/22-4961.html\nAUTHOR: Weinreich, Uriel\nTITLE: Languages in Contact\nSUBTITLE. French, German and Romansh in twentieth-century Switzerland\nPUBLISHER: John Benjamins\nIlaria Fiorentini, University of Pavia/Free University of Bozen, Italy\n\"Languages in Contact: French, German, and Romansh in Twentieth-Century\nSwitzerland\" is Uriel Weinreich's (1926-1967) doctoral dissertation, defended in\n1951 with the title \"Research Problems in Bilingualism, with Special Reference\nto Switzerland\" and until now unpublished. 60 years later, William Labov and\nRonald I. Kim have edited this faithful reproduction of the original typescript\n(the text has been digitized and reproduced in full), with an introduction,\nnotes and comments. The introductory chapter presents the life and legacy of\nWeinreich, as well as the background of his fieldwork in Switzerland, and shows\nhow \"Languages in Contact\" (1953, henceforth LiC), perhaps Weinreich's most\nimportant work, grew out of this dissertation; finally, it summarizes the\ndevelopments of multilingual situation in Switzerland up to 2011.\nThe volume is divided into four parts. The first (Chapters 1-3) deals with\ngeneral research problems: large parts of it will be incorporated in LiC. In\nChapter 1, Weinreich discusses psychological and neurological theories of\nbilingualism, enumerating methods for characterizing and measuring it, such as\nrelative proficiency, which, by means of a formula, calculate the bilingual\nquotient, \"a highly useful basis for comparing and grouping bilinguals\" (p. 5).\nThis \"new tool\", however, was still at its starting point, and, as Weinreich\nnotes, it had never been tried out on Swiss bilinguals (even if a few pilot\ntests had shown its applicability there). The author analyzes reasons for\npositive attitudes toward languages and describes the circumstances surrounding\nthe learning of languages. Then he deals with the alleged effects of\nbilingualism on language learning performance, intelligence, group\nidentification, character formation and emotional adjustment (in the latter\ncase, he concludes that there is no evidence for detrimental effects of\nbilingualism on emotional life). Weinreich also analyzes educational problems\nwith regards to bilingualism, especially the organization of schools in\nbilingual areas and the best pedagogical methods for teaching two languages,\ngiving a partial bibliography on educational difficulties.\nIn Chapter 2 the author outlines patterns of bilingual communities, in which the\nfunctions of the two languages may be diagrammed for the various levels of use\n(such as literature, church, administration, etc.). Nevertheless, the division\nbetween two languages is not always clear: this is the problem of intermediate\nlanguages, \"intermediate forms existing between the mother-tongue and the\nother-tongue\" (p.23). Weinreich defines mother-tongue groups, i.e. those\nsegments of the population including \"all those who speak language A AS THEIR\nMOTHER-TONGUE, whether or not they also use language B in other functions\"(p.\n21), and points out that most bilingual communities often have two mother-tongue\ngroups within them , with a socially higher-standing group that \"claims for\nitself the levels of use which are endowed with social and cultural prestige\"\n(p. 25). The author then introduces the notion of counter-prestige, \"which\nevolves as a reaction to prestige\" (ibidem), foreshadowing the notion of covert\nprestige, expounded later most famously by Peter Trudgill (1972) and William\nLabov (2006). Finally, he stresses the difficulty in obtaining satisfactory\nstatistics and maps on bilingual populations.\nChapter 3 deals with linguistic problems of bilingualism. After sketching\ntheoretical questions on bilingualism, Weinreich discusses the distinction\nbetween borrowing in speech and borrowed elements in language. He points out\nthat borrowing includes not only words or morphemes generally, but also\n\"phonetic, morphological and syntactic patterns. Where there is no introduction\nof segmental material from one language into the other, it is sometimes more\nconvenient to speak of interference\" (p. 40). While discussing the challenge of\nobserving the act of borrowing, he introduces what would now be called the\nobserver's paradox (Labov 1972). As for interference and borrowings, he\ndescribes phonetic aspects, morphological aspects (in his opinion, the\npossibility that morphological patterns can be borrowed \"cannot be excluded on\nprinciple, unless the possibility of language change is denied\", p. 48),\nsyntactic and lexical aspects. As for the last, he lists reasons for borrowing\n(the need for names for certain objects or concepts, social prestige, and so\non), concluding that all these motivations still do not solve \"all the problems\nof lexical borrowings\" (p. 54), like the fact that some words are never borrowed\nand \"neither cultural, structural, nor social (prestige) considerations account\nfor this difference\" (ibidem). At the end of the chapter, the differences in the\ntotal amount of borrowing (individual differences, different types of\nbilingualism, monolingual or bilingual interlocutors), the structural and\ncultural resistance to borrowing, substratum and convergent development are\nThe second part (Chapters 4-5) deals with the bilingualism in Switzerland, the\ndissertation's main focus. In Chapter 4 the author describes the different\ncontact situations which can (or could) be found in quadrilingual Switzerland:\nGerman-French (only between Biel and Fribourg), German-Italian (almost no\ncontact), Italian-Romansh (generally no communication), German-Romansh\n(discussed later). Weinreich sketches a revised map of language territories,\nfrom which it seems evident that \"the contact between language territories and\nlanguage communities in Switzerland is considerably smaller than is usually\nassumed on the basis of misleading maps\" (p. 70). Then the author outlines the\nfeatures of the federal government, cantonal governments and communal\nadministrations in Switzerland.\nChapter 5 deals with intralingual relations in Switzerland. First Weinreich\noutlines the regional and social differentiation of Schwyzertütsch and the\nlanguage patterns of German Switzerland, where Schwyzertütsch was used as the\nspoken language while \"the functions of a written language are generally\nfulfilled by Standard German\" (p. 84). The author describes the prestige factors\nof Schwyzertütsch (such as its status as a mother tongue), the attempts to\nbroaden its functions, and the linguistic effects of German-Swiss bilingualism,\nsuch as the slow but steady impact of Standard German on the dialect. Then he\ndeals with the contact situation between Standard French and patois, a case of\nirreversible language shift: patois had been driven out completely from the\nschools, and had thus become associated \"with lack of education and peasant\nbackwardness\" (p. 109), even if there were efforts to preserve it, at least as a\nvehicle \"for lyrical expression and of local patriotic sentiment\" (p. 111). In\nItalian Switzerland, Standard Italian was in contact \"not only with local\ndialects corresponding to the French patois or to the several varieties of\nSchwyzertütsch, but also with a Common Lombardic dialect\" (p. 114). Finally,\nWeinreich deals with Romansh and its dialects: Upper and Lower Engadinian,\nSutsilvan, Protestant and Catholic Sursilvan, Surmiran, each one with a standard\nfor writing (the creation of a single supradialectal written standard -- the\n\"Rumantsch Grischun\" by Heinrich Schmid -- did not come about until 1982).\nPart III (Chapters 6-8) deals with the stable, long term language contact\nbetween German and French. Chapter 6 describes general features of French-German\nbilingualism. Weinreich details these bilingual segments of the Swiss\npopulation, dividing them according to geographic zones and providing a history\nof the bilingual populations inside French territory. He describes the\nlinguistic effects of bilingualism on vocabulary and grammar, and notes that the\nFrench-German grammatical cross-influence had not yet been investigated, as well\nas the phonetic influences; in his opinion, it was \"certainly to be expected\nthat German settlers who learn French will have traces of their native sounds in\ntheir French speech, and vice versa\" (p. 141).\nChapter 7 presents a case study on the static bilingual situation in the canton\nof Fribourg, \"where no observable language shift is taking place\" (p. 143). The\nlanguage patterns of the area are described in detail, and, for each of the 22\ncommunes, the total population and numbers of mother-tongue speakers are given.\nWeinreich identifies seven predominantly German communes, six highly mixed\ncommunes and nine communes with predominantly French-speaking populations, and\ngives for each commune a rate of bilingualism (none, slight, widespread, very\nwidespread). The author then describes the religious dominations of the area,\nbecause \"in the particularly mixed villages, the difference in denomination may\nbe even more prominent than that in language\" (pp. 153-154) and the organization\nof schools, which appears to have a \"conservatory, stabilizing effect on the\nbilingual situation\" (p. 160). He takes into account also the organization of\nchurches and administrative activities and concludes that, even diachronically,\nthis region can be termed \"a stable bilingual area\" (p. 175).\nChapter 8 describes the linguistic effects of bilingualism in Fribourg.\nWeinreich first analyzes the nature of bilingualism of the area and its\nsociocultural context and then speech mixture and mutual influence between the\ntwo languages involved (French and Schwyzertütsch), outlining and comparing\ntheir sound systems and investigating morphological, syntactic and lexical\nPart IV (Chapters 9-14) deals with the contact between German and Romansh in a\nsituation of ongoing shift. Chapter 9 describes the general features of\nGerman-Romansh bilingualism. First of all, Weinreich sketches a brief history of\nthe receding Romansh language territory. He states that since the 1870s native\nspeakers of Romansh had ceased to be linguistically self-sufficient due to the\neconomic development of the canton, so that knowledge of German had become\nvital. Nonetheless, it appeared that Romansh enjoyed a \"very strong prerational\n('mother-tongue') prestige\" (p. 209). Weinreich's conclusion was that the area\nwas rapidly approaching complete bilingualism. Finally, he describes the\nlinguistic effects of German-Romansh bilingualism on vocabulary, grammar and\nChapter 10 is a case study on the dynamic bilingual situation in Central\nGrisons, where \"an observable language shift is taking place\" (p. 215).\nWeinreich describes the area and the population, as well as the language\npatterns, identifying eight communes with Romansh majorities and eleven with\nGerman majorities. As already pointed out, knowledge of German was essential to\nthe population, while the local Romansh variety was considered an inferior,\n\"substandard\" language: it was not taught in school and it was almost entirely\nexcluded from local administration. Language shift was visible both\ndiachronically, by looking at the changing proportions of mother-tongue\nsegments; and synchronically, as the percentage of Romansh mother-tongue\nspeakers was declining and they were, on the whole, \"OLDER than those of German\nmother-tongue\" (p. 251-252); the Germanization in the area was therefore \"a\nself-accelerating process\" (p. 265). There were, however, rationalized motives\nfor retaining Romansh, as the alleged advantages in learning other languages and\naccessibility to the Romansh cultural heritage.\nChapter 11 deals with Raetoroman actions aimed at containing language shift and\nhalting Germanization, e.g. the attempt to standardize Sutsilvan dialects, the\nfoundation of Romansh kindergartens (the so-called \"scolettas\" which, in\nWeinreich's opinion, had demonstrated that local Romansh could \"be taught to\nchildren and be made a source of pleasure for them\", p. 289), and the proposal\nfor a language conservation law. A broad plan for revitalizing Romansh Sutselva\nwas made by Dr. Gangale, a Romance philologist and scholar of Romansh, and\nincluded even \"psychological treatments\": \"At first it is hard to decide whether\nthis scheme is ridiculously foolish or frighteningly ingenious\" (p. 295). Thanks\nto the campaign \"to contain and reverse the language shift\", according to\nWeinreich many Raetoromans were convinced \"that they had responsibilities toward\ntheir language\" (p. 298).\nChapter 12 deals with the linguistic effects of bilingualism in Central Grisons,\nwhere there was a real bilingual speech community, since \"the two languages\noverlap in certain functions\" (p. 302). Weinreich describes this bilingualism\nand the types of mixture (incidental speech mixture -- \"while German elements\nare tolerated practically without any limit, the reverse possibility … is kept\nstrictly within bounds\", p. 302 and habitualized mixture -- for German, \"there\nIS a tendency for borrowing to become habitual, and thus a part of the\nlanguage\", p. 303). The phonetic influences between the two languages, with a\ncomparison of the two sound systems, and the morphological, syntactic and\nlexical influences are described at the end of the chapter.\nChapter 13 is a short description of German-Italian bilingualism: there was \"no\nGerman-Italian border bilingualism to speak of\" (p. 325), only scattered\nbilingual minorities in both German and Italian Switzerland.\nChapter 14 summarizes what has been laid out in the four parts of the book with\ngeneral conclusions on intralingual and interlingual relations, concluding that\n\"it seems that bilingualism can be studied properly only on an interdisciplinary\nbasis\" (p. 336).\nThe appendices report excerpts from interviews with bilingual children (appendix\nA), the questionnaire for students of the Cantonal school in Chur (appendix B),\ninformation about the Romansh League's language poll (appendix C) and a guide to\nbilingual placenames (appendix D).\nThe importance of this volume lays first in the fact that, until now, few\nlinguists have been aware that the main themes of LiC were first proposed in\nembryonic form here and even fewer had access to it. Besides its undeniable\nutility, the book is pleasant reading also thank to the inclusion of photos,\nhand-drawn diagrams and other material personally prepared by the author. The\neditors specify that, although this is a faithful reproduction of Weinreich's\ndoctoral dissertation, they have made some adjustments to the original,\nreplacing old-fashioned linguistic usages and obsolete or idiosyncratic terms\nsuch as \"morphologic\", \"unilingual\", or \"bilinguality\" with \"morphological\",\n\"monolingual\", or \"bilingualism\" respectively (although in the whole Chapter 10\n\"bilinguality\" is used instead) and dividing the original \"unwieldy\" third\nchapter of Part II \"into several more easily digestible pieces (chapter 6-13)\"\nWeinreich's preliminary research on Romansh Switzerland was coordinated by André\nMartinet, who later wrote the preface to LiC (1968). The author spent two years\n(1949-50) in Switzerland for his fieldwork, travelling throughout the country,\n\"compiling statistics on linguistic knowledge and use\" (Kim 2011: 103), taking\nphotos (some of which are reproduced here), becoming familiar \"with the full\nspectrum of everyday life in the villages and towns\" (ibidem), describing\nhistorical, political and socio-economic aspects of the population which spoke\nthe languages he was studying. This exhaustive research led to a deep knowledge\nof the full context of the speakers' lives, social behavior and interaction and\nto a valuable, detailed and most of all, for that time, innovative analysis of\nthe various aspects of the contact between two languages, whose linguistic\noutcomes, in his opinion, \"could not be deduced from a comparison of their\nstructures alone\" (Kim 2011: 108).\nAmong the many concepts here introduced or foreshadowed by Weinreich are his\ninnovative position on the borrowability of morphemes, the notion of\n\"counter-prestige\", the concept of \"domain of language use\" (developed by\nFishman 1965), the problem of the observer's paradox (developed by Labov 1972),\nthe distinction between \"stable\" and \"dynamic\" bilingualism, the phonemic over-\nand under-differentiation, and so on. Many of these inspired intuitions will be\ndeveloped in LiC, as well as, for instance, what can be considered the\n\"incunabulum\" of contrastive linguistics (Cardona 1974), i.e. his comparison of\nthe phonological systems of Schwyzertütsch and French to establish the effects\non speech of this bilingualism.\nThe importance of the entire Weinreich's work has been remarked on by William\nLabov, regarded as the father of variationist sociolinguistics and who\nacknowledges that many of his own intuitions come ultimately from Weinreich,\nadmitting that \"to this day, I do not know how many of my ideas I brought to\nlinguistics, and how many I got from Weinreich. I would like to think that my\nstudents are as lucky as I was, but I know better than that\" (Labov 2001: 459).\nCardona, Giorgio Raimondo. 1974. Preface to the Italian edition of Weinreich,\nUriel. Languages in Contact: Findings and Problems. Torino: Boringhieri. VII -\nFishman, Joshua A. 1965. Who speaks what language to whom and when. La\nLinguistique, vol. I, n. 2. 67-88.\nKim, Ronald I. 2011. Uriel Weinreich and the birth of modern contact\nlinguistics. In Piotr P. Chruszczewski and Zdzisław Wąsik (eds.). Languages in\nContact 2010. 99-111.\nLabov, William. 1972. Sociolinguistic Patterns. Oxford: Blackwell.\nLabov, William. 2001. How I got into linguistics, and what I got out of it.\nHistoriographia Linguistica 28:3. 455-66.\nLabov, William. 2006 . The Social Stratification of English in New York\nCity. 2nd edn. Cambridge: Cambridge University Press.\nMartinet, André. 1968. Preface to Weinreich, Uriel. Languages in Contact:\nFindings and Problems. The Hague-Paris-New York: Mouton. VII-IX.\nTrudgill, Peter. 1972. Sex, Covert Prestige and Linguistic Change in the Urban\nBritish English of Norwich. Language in Society 1. 175-195.\nWeinreich, Uriel. 1953. Languages in Contact: Findings and Problems. New York:\nLinguistic Circle of New York.\nABOUT THE REVIEWER\nAfter earning an M.A. in Linguistics at the University of Turin with a\nthesis on the Italian suffix -ATA, Ilaria Fiorentini is now a PhD student\nat the University of Pavia and the Free University of Bozen (Italy). Her\ndoctoral research deals with the contact situations in the Ladin valleys of\nTrentino Alto Adige/Südtirol, with particular attention to code-mixing\nphenomena among Ladin, Italian and German. Her primary research interests\ninclude sociolinguistics, contact linguistics and pragmatics.\nRead more issues|LINGUIST home page|Top of issue\nPage Updated: 14-Aug-2012\nWhile the LINGUIST List makes every effort to ensure the linguistic relevance of sites listed\non its pages, it cannot vouch for their contents."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:917d65c1-9b22-4065-926f-2b03632bfda8>"],"error":null}
{"question":"How do celestial navigation techniques interface with modern flight planning requirements, and what specific accuracy standards are required for different route types?","answer":"Celestial navigation involves precise calculations and measurements, as demonstrated by the detailed process of taking star shots with a sextant and using tools like the Polhemus computer to determine position fixes with accuracy within nautical miles. This relates to modern navigation requirements where different routes demand specific accuracy standards - RNAV routes in India have distinct Required Navigation Performance (RNP) values: L, M, N, and P routes require RNP 10, while Q routes require more precise RNP 5 navigation accuracy. Aircraft must maintain their Actual Navigation Performance (ANP) within these limits and notify Air Traffic Control if accuracy degrades beyond the required standard, demonstrating how both traditional and modern navigation methods emphasize precise position awareness.","context":["Now that we have discussed the technique of flight navigation we can look at an example of how it is actually done. I will use the Polhemus computer to illustrate this process which will also show how handy this device is but it can also be done on a plotting sheet though not so conveniently. ( If your have not already read Working the celestial sight in flight you should do that before reading this page.)\nWe are ferrying a 1978 Cessna Skyhawk (Cessna 172) from Casablanca Morocco to Porto Santo Island in the Madeira Islands on October 4, 2008 (1.jpg and 2.jpg.) We will be using a Kollsman MA-2 hand held sextant which has a two minute averager but the same procedures would be followed with any bubble sextant.\nThe coordinates of the departure airport are 33º 33' north, 7º 40' west. The destination airport is at 33º 04' north, 16º 21 west. Using the 35º latitude disk we label the graticle and we then plot the locations of both airports and draw a course line between them (3.jpg ) then we place the disk on the base ( 4.jpg.)\nNow, by rotating the disk so that the destination is directly above the departure, we can read out the distance by counting the grid lines between the departure and the destination. Departure is down from the center at minus 215 (visually interpolating between 210 and 220) and destination is up from the center at plus 219 for a total distance of 434 NM (5.jpg.)\nWe can also read out the true course at the \"TRUE INDEX\" which is 266.5º, which we will round to 266º (6.jpg.)\nLooking at the aircraft flight manual climb schedule (7.jpg), we see that we will climb at an airspeed of about 70 knots and that it will take 21 minutes to climb to our planned cruising altitude of 10,000 feet (Flight Level 100, FL100) using 3.7 gallons of fuel in the climb and covering 27 NM. Since we also used 1.1 gallons taxiing we will have used up 4.8 gallons by the time we level off FL100. After we have leveled off at the top of the climb (TOC) we adjust our throttle to make 2500 rpm and lean the mixture control.\nSince the air temperature is about standard (15º C at sea level and cooling off about 2º C per thousand feet) this power setting will produce 61% power which will give us a true airspeed of 114 knots and a fuel flow rate of 6.8 gallons per hour (8.jpg.)\nSince we have covered 27 NM in the climb we will have an additional 407 NM to cover in cruise after we reach the TOC. Winds are forecast \"light and variable\" so using our true airspeed of 114 knots we compute that it will take an additional 3 hours +34 minutes for the enroute phase of the flight for a total flight time of 3+55. We will burn 24.3 gallons in cruise plus the climb and taxi fuel means we will burn a total of 29.1 gallons out of our total fuel on board of 50 gallons leaving us a comfortable 20.9 gallons fuel reserve.\nWe take off at 1745Z and climb on course on a true heading of 266º and level off 21 minutes later at FL100 at 1806Z and set the power and auto pilot. Our ETA is 2140Z, 3+55 after takeoff. We plan on taking a celestial fix at 1920Z to allow for an enroute leg longer than one hour so as to allow for the determination of an accurate wind vector. We will cover 140 NM during the 1+14 minute cruise from TOC at 1806Z to 1920Z fix time. Since we covered 27 NM in the climb we will be 167 NM from departure at the planned fix time. To plot the 1920Z DR we count up 167 NM on the grid from the departure. Since the departure was at minus 215 on the Polhemus grid we simply subtract 167 from 215 and place a mark on our course line at the minus 48 grid line, visually interpolating between 40 and 50 (9.jpg.)\nRotating the grid to north up by placing 360° at the True Index we can read out the coordinates of our 1920Z DR of 33º 27' north, 11º 00' west (15.jpg.)\nAbout a half hour before the fix time we start planning our fix. It only takes about ten minutes to do the actual pre-computations for a three star fix but, since we must start shooting the first star nine minutes prior to fix time, we don't want to cut it too close, we want to leave some time for contingencies. We will be using H.O 249 Volume 1, Selected Stars, since it is the most convenient. First we look in the Air Almanac for 1920Z, October 4, 2008 and take out the GHA of Aries without interpolation, 303º 51' (10.pdf.) We select an AP of 33º 00' north and 10º 51' west so that the LHA Aries will be 293º exactly. We use only one AP since we are using H.O. 249 vol. 1 and we are accounting for motion of the observer (MOO) and the motion of the body (MOB) mathematically, not advancing the earlier LOPs to the fix time. We then look at the 33º north page of H.O 249, vol. 1 and looking at 293° LHA Aries we see that the selected stars are Alpheratz, Enif, Altair, Antares, Arcturus, Alkaid and Kochab (11.pdf.) Because the visibility is limited in a Cessna 172 by its high wing we must choose, not the three recommended stars, but Kochab, Arctures and Antares. We take out the Hc's and Zn's for the three bodies, 36° 57' and 341° for Kochab; 19° 16' and 281º for Arcturus; 16º 29' and 222º for Antares. We will shoot Kochab first since it is nearly on the wing tip and so advancing its LOP the most to the fix time will have little effect on its accuracy. We plan our shooting schedule, Kochab at 1912Z (eight minutes before fix time), Arcturus at 1916Z (four minutes early) and Antares at 1920Z. We enter this data our the Celestial Precomputation form (12.jpg,)\nif using the Polhemus computer or the 1 minute adjustment tables from H.O 249 or (13.jpg )\nif using the 4 minute adjustment tables from H.O. 249. We only compute LHA for the the fix time shot but we use the same LHA, 293º to take out the Hc's for all three bodies and enter this data on the form in the row labeled \"HA HO 249\" and enter the Zn's in the same columns and also on the left side of the form for computation of the motions adjustments. We also take out the \"Precession and Nutation\" correction for 2008 for LHA Aries of 293º and for a latitude of 33º north, either visually interpolating or simply taking the nearest tabulated value since they are all small, we'll use .6 NM at 241º and enter it on the appropriate form (14.pdf.) We will use this to adjust the fix position. We plot the AP (an upside down \"V\") on the Polhemus grid at 33º 00' north, 10º 51' west, visually interpolating (15.jpg.)\nNow set the front of the Polhemus \"SET TRACK\" pointer to 266°, the \"SET GS\" (ground speed) to 114 knots and the \"SET LAT\" to 33° and then tighten the nut to keep the settings from changing (16.jpg.) (We could also do the same adjustments using the MOB and MOO tables from H.O 249.)\nLooking around the outside edge for the Zn's of each body we find the relative Zn's (ZN-TR) and enter them on the form, 75º for Kochab, 15º for Arcturus and 44º for Antares. Using each body's Zn look in the \"CORRECTION FOR MOTION OF THE BODY\" window and take out the MOB one minute correction and enter it in the appropriate blank (17.jpg , 18.pdf. and 19.pdf.)\nSince all three Zn's are to the west they are all found on the black scale so the signs are all plus. Next, using the relative Zn's (ZN-TR) look in the \"CORRECTION FOR MOTION OF THE OBSERVER\" window and take out the one minute corrections for MOO (20.jpg , 21.pdf and 22.pdf.)\nSince all of the relative Zns are ahead of the plane they are all found on the white scale making all of their signs negative. We sum the MOB and MOO adjustments to the \"ONE MINUTE ADJ.\" line keeping track of the signs. Since we are planning the first shot 8 minutes early, the second shot 4 minutes early and the last shot on time, we multiply the one minute adjustments by the time intervals to produce the total motions adjustments. (We get identical values if we use the one minute MOB and MOO tables. If we are using the 4 minute adjustment tables we multiply by 2 and 1 adjustment periods respectively and get the same values.) We look at the refraction table (23.pdf ) in the 10,000 foot altitude column and take out the refraction correction for each body and enter it on the form with a plus sign and carry them to the \"MISCEL\" line. Add the total motion adjustment to the MISCEL line to arrive at \"TOTAL ADJ.\" and carry to the right side of the form into the appropriate columns. Combine the \"HA\" from H.O. 249 with the total adj. to arrive at precomputed altitude (Hp.)\nThe last bit of information we take from the Polhemus is the Coriolis correction which is found in the \"CORIOLIS & WANDER CORR.\" window. Look at the latitude, 33º, and take out the coriolis correction of 1.7 NM (20.jpg ),\n( 2 NM if taken from the H.O. 249 table 23.pdf .) We will use this to move the plotted fix 1.7 NM in direction 356º , 90º to the right of the track to account for Coriolis. (Alternatively we could make the same adjustment to to the AP prior to plotting the LOP's, dealers choice. We could also use the Polhemus to derive a Coriolis correction to be applied to each Hp mathematically but that is a needless complication especially at low air speeds.)\nWe are now done with the precomputations and can relax until time to shoot Kochab. About 1908Z we get the sextant ready, illumination on, bubble formed, averager set and altitude set to about 37º. We also make sure that the directional gyro is set and that the autopilot is set to heading mode. We look out the window, locate Kochab and bring it into the center of the bubble. At 1911Z we trigger the averager and continually adjust the altitude knob to keep Kochab centered in the bubble. Two minutes later the shutter on the sextant automatically closes ending the two minute shooting period and the average time of the shot is 1912Z. (If using an A-10A and some other sextants you must keep track of the progress of the shot and stop at the two minute mark.) The sextant altitude of Kochab is 37º 35'. We enter this in the form, compare it to the already adjusted and corrected Hp and determine that the intercept is 7 NM TOWARD Kochab, Zn 341º. (No need to correct Hs for refraction as this was already taken care of by applying the refraction correction with reversed sign to adjusted Hp.)\nWe complete the same steps with Arcturus and Antares and get an Hs of 19º 51' for Arcturus and an Hs of 16º 20' for Antares producing intercepts of 9 NM AWAY and 11 NM AWAY respectively. Using the Polhemus we plot the three LOPs. First we set the Zn of Kochab, 341º, at the TRUE INDEX and then measure up 7 NM from the A.P since this is a TOWARD intercept. and draw the Kochab LOP parallel to the right-left grid lines on the Polhemus base (24.jpg and 25.jpg .)\nWe do the same for the Arcturus and Antares LOP's remembering to measure down from the AP since these are AWAY intercepts 26.JPG , 27.JPG , 28.JPG and 29.JPG.)\nThese three lines form the traditional \"cocked hat\" of a celestial fix. We then move the fix from the center of the cocked hat 1.7 (or 2) NM in direction 356º for Coriolis and then .6 NM in direction 241º for precession and nutation. We do this with visual interpolation since these are small values (30.jpg.)\nIf larger, we would set the respective Zn's under the TRUE INDEX and measure up the appropriate amount for each of these corrections. The fix is 33º 13' north, 10º 41' west.\nShowing the convenience of the Polhemus even more, we quickly find the wind encountered in flight and the new course and distance to the destination. Since our DR in this case is also our \"no wind position\" or \"air position\" where we would be if there were no wind, so any difference between the DR and the fix must be caused by the wind. We now rotate the disk to place the fix directly below the DR and read out the distance between the DR and the fix which shows how far the wind pushed the plane, in this case 21 NM (31.jpg.)\nSince the plane flew for 1+14 in cruise we divide this 21 NM by this amount of time and find the wind speed of 17 knots. We now find the direction of true wind as it is already aligned with the \"TRUE INDEX\" which shows 311º (32.jpg.)\nNow we can rotate the disk to place the destination directly above the fix and find the distance and true course to the destination of 284 NM (33.jpg), course 270º (34.jpg .)\nUsing this new course and the measured winds on our E-6B or MB-2A we calculate a new wind correction angle, new heading, new ground speed, new ETA and new fuel required. Wind correction angle will be 6º RIGHT making the new heading of 276º . The new ground speed will be 101 knots for the remaining distance of 284 NM which means it will take an additional 2+48 to arrive, making the new ETA of 2208Z. This means that we will arrive 28 minutes later than planned, using an extra 3.2 gallons reducing our fuel reserve to 17.7 gallons which is still a comfortable safety margin, more than two hours of extra fuel. We will plan on taking two more fixes at 2020Z and 2120Z to confirm that we are staying on course and maintaining our schedule to ensure having adequate fuel. We may omit the 2120Z fix if we are already receiving radio navigation guidance from the destination which should be only about 80 NM ahead at that time.\nAlthough this is a fairly short flight it is still very useful to get the celestial fixes so that we can be sure we are not running into a strong headwind or getting blown far off course.\nCelnav is done the same way in faster aircraft. Since most jets are flight planned at about .7 mach, about 450 knots, this just makes the adjustment for MOO and Coriolis larger but the same methods are used. Using the Polhemus it takes only 40 seconds to plot the three LOP's, about 13 seconds each, and just 30 seconds total to determine the wind speed and direction and the course and distance to destination. Then 25 seconds on the MB-2A gives you wind correction angle and ground speed and another 30 seconds gives you time to destination and fuel required. So by doing precomputations and by using the Polhemus you can have the fix and the new heading, ETA and fuel required, only two minutes after finishing the last shot. Try that with other other computation and plotting method!","Change in aviation is met with heavy resistance, and even a ten year old technology is considered relatively new. With the introduction of Performance Based Navigation (PBN) in the Indian Airspace, confusion still exists on RNAV (aRea NAVigation), RNP (Required Navigation Performance), and where this RNAV/RNP are implemented in the Indian ATS.\nThe basic airway system (in India and the world over) was constructed based on sensors: the VOR and the NDB stations and receivers on board the airplane, which provide the capability to fly to, or from a radio station along one of its “radials”. These radio stations are scattered, purposely, across the country, and the airway system is constructed by simply “connecting the dots”, and an aircraft’s position is always relative to one of these stations. Example: Waypoint LATID is 77NM from Bangalore International Airport’s VOR (BIA), on a radial of 012°of BIA.\nWhen an aircraft’s navigation system has a little more intelligence: the ability to scan and receive signals from multiple such radio ground stations, or from self contained navigation aids, such as the Inertial Reference System (IRS), or from the globally available GPS satellite constellation, and determine the aircraft’s position in terms of the World Geodesic System 1984 (WGS-84) coordinates, it provides the ability to determine the aircraft’s absolute position, rather than referencing it to a sparse set of radio stations. Example: Waypoint LATID is N14° 28.6’ E077° 56.9’.\nThe advantage with absolute position is freedom in the lateral: an aircraft can determine its absolute position, and fly to another waypoint whose absolute position is known, without having to stick to a “radial” or a VOR station. The ability to fly “Direct-To” another waypoint from the present position offers an easily comprehendible advantage: fuel savings through shorter, more direct routes. This freedom in the lateral, and the ability to navigate freely in an area, gives rise to RNAV, or Area Navigation.\nIndian airspace is comprised mostly of “W” routes, which are, as per AAI, exclusively available for domestic operators only. According to ICAO Annex 11, a “W” route is NOT an Area Navigation Route, which means, the airway is constructed with reference to ground radio beacons, and are mostly direct from one beacon to another.\nThe other airways in India are “A”, “B”, “G”, “L”, “M”, “N”, “P”, “Q”, “R”, “UL”, “UM”. Of these, “L”, “M”, “N”, “P” and “Q” are area navigation routes. This means that these routes are not constrained to fly between ground based radio stations, but are instead optimised, more direct routes that save fuel. The “Q” routes were recently introduced in 2012, in July.\nSince flying these routes implies a reliance on the aircraft’s complex navigation system (which authorities have no operational control of) rather than the simpler ground referenced navigation system (which authorities maintain), it is imperative that in the interest of safety, the complex area navigation system be capable of a certain navigation accuracy, also termed the navigation performance.\nCertain routes, and certain procedures may require a higher navigation accuracy and its associated certainty, while others may be less demanding. To quantify these “higher and lesser” accuracies, the term “Required Navigation Performance” (RNP) was introduced, which stipulates the minimum navigational accuracy that must be guaranteed, with a certainty of 95% availability.\nWith RNP, of the many requirements, the aircraft must be capable of displaying the Actual Navigation Performance (ANP). As long as the actual navigation performance is within the limits of the RNP, everyone’s happy. But if the ANP gets worse than the RNP, that’s when Air Traffic Control must be notified so they can keep close eye on you and other airplanes in relation to your aircraft, and direct you based on conventional navigational practices.\nThe Area Navigation Routes – “L”, “M”, “N”, “P” – are all RNP 10 in India. The newly introduced “Q” routes, are all RNP 5. This means that your aircraft’s navigation accuracy must be better than 5 NM if it is to fly along the newly introduced 7 “Q” routes: Q1 – Q7. If however the ANP of the aircraft is 5.5 NM, then the accuracy is not enough to fly the “Q” routes, but accurate enough to fly thee RNP 10 routes: “L”, “M”, “N”, “P”.\nThe benefits of the RNP routes are evident. The newly introduced “Q” routes connect Delhi to Mumbai, Ahmedabad, Udaipur, and Vadodra. Picking “Q1”, which is Mumbai to Delhi (BBB- DPN), there are 13 waypoints in between the starting (BBB) VOR and the ending (DPN) VOR. Except for one, none of the other waypoints are ground based radio aids. The total ground distance between Mumbai and Delhi along Q1 is 633NM. The domestic non-RNAV “W13N” route between Mumbai and Delhi, has 5 waypoints in between, three of which are ground based radio aids (VOR). The ground distance along W13N is 653NM. A347, another non-RNAV route between Mumbai and Delhi, has 9 waypoints in between, three of which are ground based radio aids. The ground distance along A347 is 735NM. Compared to W13N and A347, Q1 saves 20NM and 102NM of ground distance, which translates to a saving of between 2 minutes and 14 minutes of flying time. A heavy Airbus A320, flying at FL350 at 76Tonnes, can save between 124 kg and 634 kg of fuel, which translates to a saving of between INR 11,000 and INR 56,227 per Mumbai-Delhi flight. Another advantage is the smooth flight path, as opposed to the zig-zag of non-RNAV routes.\nIndigo’s 11 daily direct flights from Mumbai to the capital can save the airline about INR 1,21,000 per day, one way alone! Air India, with 12 direct flights, saves INR 1,32,000 one way, per day.\nAircraft with high navigation performance are allowed to fly the RNP routes. With higher accuracy, more airplanes can be squeezed on an airway. The “Q” routes allow aircraft to aircraft longitudinal separation of 50NM, while W13N allowed for a 10 minute separation, which translates to around 75NM. Theoretically, up to 13 airplanes may now fly on Q1, at any point of time, as compared to 9 on W13N. The capacity of the Indian Air Traffic System (ATS) has increased 44% on this route alone.\nRNP and RNAV arrivals and departures are already in use, explained in another article which shall follow soon."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:44354464-9ed4-4ff8-ad96-a84025525234>","<urn:uuid:9e905dd3-e235-451b-a382-3d90bf443244>"],"error":null}
{"question":"What role do separation techniques play in chemical analysis, and what are the primary hazard classifications that need to be considered when performing these procedures?","answer":"Separation techniques play a crucial role in chemical analysis by converting mixtures into distinct product mixtures, with at least one being enriched in one or more constituents. These techniques are essential for enhancing the purity of substances and include methods like filtration, distillation, and chromatography. When performing these procedures, workers must consider the three primary hazard categories: biological hazards (such as microorganism contamination), chemical hazards (which require proper classification under the Globally Harmonized System), and physical hazards. These hazards must be identified and controlled to ensure safe laboratory operations.","context":["Hyphenated separation techniques refers to a combination of two (or more) techniques to detect and separate chemicals from solutions most often the other technique is some. Journal of chromatography and separation techniques discusses the latest research innovations and important developments in this field. Such separation techniques include filtration or evaporation separation process, or a separation method, or simply a separation, is a methodology to attain any mass. Separation of mixtures - explained chem academy loading chrmatography, decantation and magnetic separation separation techniques - duration. Lab #2 physical separation techniques introduction when two or more substances, that do not react chemically, are blended together, the result is a mixture in which each component retains its individual identity and.\nSeparation techniques are an important part of chemistry however, their importance is not just limited to chemistry they are also used in our daily lives separation techniques are methods used to separate and/or purify mixtures. A separation process is a method to achieve any phenomenon that converts a mixture of chemical substance into two or more distinct product mixtures, which may be referred to as mixture, at least one of which is enriched in one or more of the mixture's constituents. In this worksheet, students study the techniques used to separate mixtures and solutions into their components. Separation techniques i have designed this lesson, it is a very fun and engaging lesson for year 7 or 8 carousel activity is fun for the students (differentiated) and a some pair assessment as well. 23:15:56 2 why we need separation techniques deal with the separation of mixtures to enhance purity of substances are important because.\n, euroscicon conference separation techniques 2018 will be conducted on theme: launching the innovative ideas and technologies of separation techniques. There are several types of separation techniques, including hand separation, filtration, distillation, chromatography and centrifugation other methods include absorption, crystallization, decantation, evaporation and extraction.\nSeparating funnel magnetic separation precipitation let’s discuss some of the separation techniques using a separating funnel: a separating funnel is used for the. We can separate an analyte and an interferent if there is a significant difference in at least one of their chemical or physical properties this section provides a partial list of separation. Separating mixtures: techniques and applications a mixture is a combination of two or more pure substances that are not chemically combinedmixtures come in.\nJoin taz tally for an in-depth discussion in this video separation techniques, part of photography: exploring composition. The food processing industry uses various techniques to transform food ingredients into different forms for consumers separation techniques. A key stage 3 revision and recap resource for science, covering chemical reactions, compounds and molecules it also covers mixtures and techniques for separating their ingredients, like distillation.\nChemistry - separation techniques separation techniques when we perform an experiment, we end up with a mixture of substances rather than only one we should know how to separate the mixtures a single substance that has no other substances mixing with it is known as pure substance.\nKey 1 separation techniques name - _____ 1) a red-brown solution of bromine in water (=1 01 ) is poured into a separatory funnel trichloroethane (=1 34. Separation techniques when students mixed two common substances there was a new precipitate formed in a solutionfiltration and evaporation filtration is a method for the separation of the parts of a heterogeneous mixture a heterogeneous mixture was formed. Separation methods ways to separate so processes bases on differences in physical properties are used to separate component numerous techniques have. Introduction to separation techniques identify and explain the principles behind a particular separation technique that is used in daily life and in industry identify an appropriate separation technique to separate a mixture based on the physical properties of the components of the mixture. To separate mixtures in a compound by using different techniques.","Name And Describe One Of Each Of The Three Primary Hazard Categories Pdf\n- and pdf\n- Thursday, May 27, 2021 6:26:14 AM\n- 2 comment\nFile Name: name and describe one of each of the three primary hazard categories .zip\nWe know that running a research lab is a challenge, to say the least.\n- Laboratory Hazards and Risks\n- A Guide to the Most Common Workplace Hazards\n- Division of Research Safety\nThere are a range of environmental health hazards that affect our wellbeing. Hazards can be grouped together to improve understanding and action planning.\nHazards exist in every workplace, but how do you know which ones have the most potential to harm workers? By identifying hazards at your workplace, you will be better prepared to control or eliminate them and prevent accidents, injuries, property damage and downtime. The meaning of the word hazard can be confusing.\nLaboratory Hazards and Risks\nPart one outlines the step-by-step process for classifying your hazardous chemicals. Under the new Globally Harmonized System GHS of SDS and Label authoring, chemical manufacturers, importers, and distributors are required to update the way they classify and communicate the hazards of their products. GHS classification of hazards is divided into class and category. These describe the nature and, if applicable, the degree of hazard of the chemical product. A chemical will have a hazard class, and within that class are several hazard categories, of which one or more will apply.\nA Guide to the Most Common Workplace Hazards\nSeptember is National Food Safety Education Month, and we wanted to highlight methods for implementing safer food practices. The CDC has reported that 48 million people per year get sick from a foodborne illness, many of which are preventable. It is important for food service professionals to be aware of the primary types of food safety hazards, and the best methods of prevention. There are four primary categories of food safety hazards to consider: biological, chemical, physical, and allergenic. Understanding the risks associated with each can dramatically reduce the potential of a foodborne illness. Each have their own unique characteristics, but all can be avoided through a robust food safety management system FSMS. Biological hazards are characterized by the contamination of food by microorganisms.\nA hazard is a potential source of harm. Substances, events, or circumstances can constitute hazards when their nature would allow them, even just theoretically, to cause damage to health, life, property, or any other interest of value. The probability of that harm being realized in a specific incident , combined with the magnitude of potential harm, make up its risk , a term often used synonymously in colloquial speech. Hazards can be classified in several ways. They can be classified as natural, anthropogenic , technological , or any combination therefore, such as in the case of the natural phenomenon of wildfire becoming more common due to human-made climate change or more harmful due to changes in building practices.\nDivision of Research Safety\nTo complete the first step in any workplace risk assessment, you must identify the hazards in your workplace. Not all hazards are obvious and they will be unique to your workplace. Therefore, we have created this guide to help you understand the different categories of hazards and where they might be present. The rest of this article focuses on hazards, including where they might be found in different workplaces. We also provide you with a range of further resources to make your risk assessment process as smooth as possible.\nThe evaluation is performed by classifying each chemical based on published toxicological or other data to determine its physical and health hazards. Manufacturers and distributors are required to provide safety data sheets to their clients that describe the results of the classification and all known hazards of a chemical. Some manufacturers already comply with this regulation. The main health and physical hazards have to be indicated on the product label by using pictograms, signal words, and standardized hazard statements. The pictograms are explained in the next paragraph.\nThe datasheets list in a standard format different hazards to which a worker, in the normal course of normal work, may be exposed to. It provides several measures for the prevention of occupational accidents and diseases.\nDifferent types of hazards\nВроде. - У Соши был голос провинившегося ребенка. - Помните, я сказала, что на Нагасаки сбросили плутониевую бомбу. - Да, - ответил дружный хор голосов. - Так вот… - Соши шумно вздохнула.\nЧетверо. Всего трое. Халохот стиснул револьвер в руке, не вынимая из кармана. Он будет стрелять с бедра, направляя дуло вверх, в спину Беккера. Пуля пробьет либо позвоночник, либо легкие, а затем сердце.\nДэвид, прости. Он увидел пятна света."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:4190c60c-c63c-4618-ab20-1fdf7ff0fa7d>","<urn:uuid:fef858e5-b8da-4465-acbf-bb36944c6f28>"],"error":null}
{"question":"What's the relationship between animal intelligence and ethics, and how does this apply specifically to Border Collies' cognitive abilities and moral considerations?","answer":"Animal intelligence is closely linked to ethical considerations, as demonstrated by how cognitive abilities influence our moral obligations. Border Collies exemplify this connection - they are considered one of the most intelligent dog breeds, with documented cases like Chaser who understands over 1,000 words of human vocabulary, demonstrating sophisticated cognitive capabilities. From an ethical perspective, their intelligence and ability to learn creates special moral considerations, as they can develop complex social bonds and require proper mental stimulation. Without adequate cognitive engagement, these dogs can develop destructive behaviors, highlighting how their intelligence creates specific welfare needs that must be addressed. Their capacity for learning and social bonding suggests they deserve particular ethical consideration regarding their treatment and care, requiring both physical exercise and mental stimulation to ensure their wellbeing.","context":["By Lori Gruen\nDuring this clean and complete advent to animal ethics, Lori Gruen weaves jointly poignant and provocative case reports with discussions of moral concept, urging readers to have interaction severely and empathetically examine our therapy of different animals. In transparent and obtainable language, Gruen presents a survey of the problems significant to human-animal family members and a reasoned new viewpoint on present key debates within the box. She analyses and explains a number theoretical positions and poses demanding questions that at once motivate readers to hone their moral reasoning talents and to enhance a defensible place approximately their very own practices. Her e-book could be a useful source for college students in a variety of disciplines together with ethics, environmental reports, veterinary technological know-how, women's reviews, and the rising box of animal reports and is a fascinating account of the topic for basic readers with out earlier heritage in philosophy.\nRead or Download Ethics and Animals: An Introduction (Cambridge Applied Ethics) PDF\nSimilar Animals books\nThey're brother and sister, yet Angus is larger. he's a great, courageous, and shrewdpermanent dog—and he likes that. Sadie is not as fast to learn—or to obey. Angus thinks she's terrified of every thing, yet Sadie is aware that is not real. Newbery Medalist Cynthia Voigt's tale of border collie domestic dogs turning out to be up on a farm in Maine is for animal fans of every age, and for a person who is ever had—or ever questioned what it'd be wish to have—a brother or sister similar to themselves, yet very, very diverse.\nNew from Smithsonian Books, After the Ice is an eye-opening examine the winners and losers within the high-stakes tale of Arctic transformation, from international locations to local peoples to animals and the very panorama itself. writer Alun Anderson explores the consequences of worldwide warming amid new geopolitical rivalries, combining technology, company, politics, and event to supply a desirable narrative portrait of this quickly altering land of unheard of international value.\nDavid Clement-Davies’s first novel used to be released to nice acclaim, together with a rave overview from Watership Down writer richard Adams: “it is a riveting tale and merits to be broadly learn. it really is the best anthropomorphic fantasies identified to me. ”\nEven supposing Tabby attempts to maintain him hot, Mr. Putter catches a chilly during this comfortable addition to the sequence.\nAdditional info for Ethics and Animals: An Introduction (Cambridge Applied Ethics)\nIt's to the practices that we are going to now flip. 1 Plumwood 2000. 2 www. youtube. com/watch? v=E51DyWl_q0c. three Hatkoff, et al. 2007. four Cronin 1996a: three. five Thorpe 1950 as brought up in Griffiths 2004: 614. 6 Bateson 1991: 21–2 as stated in Griffiths, et al. 2009: 605. 7 Griffiths 2002: 70–85. eight Oyama 2007. nine Nicholl & Russell 2001: one hundred sixty five. 10 Karen Strier wrote: “I were learning an identical crew of monkeys, referred to as northern muriquis, in a small woodland in southeastern Brazil for almost 28 years. whilst i started my examine they have been known as Brachyteles arachnoides. thus, and in the lifetimes of a few of the members in my unique research workforce, they have been reclassified as a brand new species, B. hypoxanthus, to differentiate them (as northern muriquis) from the southern muriqui, which has retained the unique Latin identify. ” Strier 2010. eleven There are debates approximately devices of choice (e. g. , even if genes, cells, organisms, and/or populations finally evolve) between biologists and philosophers. For a philosophically wealthy dialogue, see Godfrey-Smith 2009. There are a few, so much particularly David Sloan Wilson and Eliott Sober, who argue that choice will be understood to function on teams, and the overdue Steven Jay Gould, who argued for species choice. See, for a commence, Wilson 1993 and Lieberman & Vrba 2005. 12 My colleague Barry Chernoff instructed me in regards to the case of siblicide between armadillos who're truly genetically exact twins. So while one armadillo kills her sibling she is, in a feeling, killing herself. He additionally pointed out that male fish, who take care of the younger, once in a while can be compelled to kill the mum as she is threatening to kill and devour her younger. thirteen Gould 1997: thirteen. 14 Singer 1990: nine 15 Teichman 1985: 176. sixteen till particularly lately it made experience to assert that people enhance in a woman's physique, yet given the fluidity of gender and the truth that a few those that have been born with girl reproductive organs determine as males and a few of these males have given beginning to youngsters, it truly is extra exact to assert that people, no less than till know-how turns into extra complicated, strengthen in uterine environments, no matter if these belong to participants deciding on as “women,” “men,” or another gender class. 17 for instance, because the results of advancements in reproductive applied sciences, together with cloning, surrogate moms could be a diversified species from the constructing child. 18 As pointed out in wooden 1998. 19 Kant 1798: 7, 127. 20 Locke 1690: Bk. II, Ch. 27, sect. nine. 21 there were arguments approximately tips on how to get round this counterintuitive challenge for Kant. There are 3 attainable responses. One is to indicate that non-persons are morally massive ultimately. although Kant believed that animals have been mere issues, it sounds as if he didn't certainly think shall we put off them any method we would have liked. within the Lectures on Ethics he makes it transparent that we've got oblique tasks to animals, tasks that aren't towards them, yet in regard to them, insofar as our remedy of them can impact our tasks to people. And you'll be able to argue an identical will be precise of these people who're no longer people.","The Border Collie is a medium sized herding dog that grows to between 30 and 55 pounds as an adult.\nThis breed is alert, intelligent, and very active. They’re often black and white, but can actually come in a large variety of colors and markings,\nThis breed is generally healthy, but can display herding instincts that can be problematic if in a home with small animals and children.\nSo what else should we know about this energetic breed?\nWhat’s In This Guide\n- Border Collies At A Glance\n- In-depth Breed Review\n- Training And Care\n- Pros And Cons Of Getting A Border Collie\nThe distinctive and highly intelligent Border Collie is a breed that is popular with dog lovers everywhere.\nBorder Collie FAQs\nOur readers’ most popular and frequently asked questions about this breed.\n- Do Border Collies make good house pets?\n- Can Border Collies be aggressive?\n- What age do Border Collies live to?\nNot to mention their meteoric rise as dog agility performers.\nBreed At A Glance\n- Popularity: 35 of 193 on the American Kennel Club’s rankings\n- Purpose: Bred as a sheepdog by Scottish farmers in the 19th Century this is still the premier sheepdog in the world. This breed is also used as a service dog for psychiatric patients.\n- Weight: 30-55lbs\n- Temperament: Bright, alert and energetic. Friendly but does have herding instincts.\nBut how much do you know about the Border Collies’ other characteristics? And would a Border Collie puppy be the right new addition to your family life?\nBorder Collie Breed Review: Contents\n- History and original purpose\n- Fun facts about Border Collie\n- Border Collie appearance\n- Border Collie temperament\n- Training and exercise\n- Health and care\n- Do Border Collies make good family pets\n- Rescuing a Border Collie\n- Finding a Border Collie puppy\n- Raising a Border Collie puppy\n- Popular Border Collie breed mixes\n- Products and accessories\nHistory and original purpose\nThis is a born and bred working, herding dog.\nThey are thought to have first been bred as we would recognize them today in the 19th Century.\nTheir unique breed was developed in the Scottish borders, to help farmers maintain sheep herds. They were specifically bred for their obedience and ability to learn.\nThey are generally thought of as one of the most intelligent breeds of dog, and the most trainable.\nFun facts about Border Collies\nThis breed is something of a celebrity favorite.\nFrom famous owners like Paul McCartney and Grace Kelly to Jon Bon Jovi and Cheryl Cole, this dog keeps some illustrious company.\nActors and musicians however, are not this breed’s only claim to fame. Mary the Danish Crown Princess and even Queen Victoria made this pup their choice of companion.\nThese dogs are super bright. In fact, there is a Border Collie named Chaser who understands over 1,000 words of human vocabulary!\nTheir brightness might be why this breed is the go-to dogs for some film and television roles. You might know Shep from the television show Blue Peter. Or maybe Rex and Fly from the movie Babe.\nBorder Collie appearance\nBorder Collies are a striking breed. Despite being varied in appearance they are very distinctive and easy to identify.\nThey commonly have black and white, or blue merle, coats and a traditional wolf-like body and head shape. Their ears are semi-erect and their eyes are brown or blue. They are also known for occasionally having eyes of differing colors.\nTheir coats can be anything from smooth to rough, and long to short.\nAs they are mostly bred with work in mind rather than keeping to a specific look, they can vary fairly widely in appearance. Although they are still recognizable as a part of this breed.\nBorder Collie temperament\nThese dogs can make very loving family pets. However, as they have predominantly been bred for work they are not always naturally social. Having said that, they are eager to learn!\nIf you get a Border Collie puppy, it is very important to socialize him very well. This is especially true if he’ll be around young children and other animals.\nThis breed has inherited some natural herding instincts. This means that they might be inclined to nip. It also means that they could be prone to guarding behavior.\nGuarding behavior might mean some aggressive growling, barking or even snapping.\nWith that it mind this study ranks the the breed low on its list of dogs prone to aggressive tendencies. However, it does suggest that this breed is prone to display territorial aggression.\nPutting in some time and dedication when they are young, will mean that you will have a lifetime of happiness with a sociable and stress-free dog later on.\nTraining and exercise\nAs an intelligent, working breed Collies do require mental stimulation as well as exercise.\nThey are popularly used for agility and fly-ball training, as well as traditional sheepdog trials and obedience competitions. Their brilliant sense of smell also makes them great tracking dogs.\nHowever, due to this eagerness to learn and perform, Collies do not always make the best house pets. When bored they will have a tendency towards destructive behaviour such as chewing furniture and destroying your property.\nThey are best kept in an active household, where they can use their brains and their body on a regular basis.\nYou can read some of our training guides here:\n- Puppy Potty Training Schedule And Finishing Touches\n- The Evidence for Positive Reinforcement Training in Dogs\n- Crate Training A Puppy – The Ultimate Expert Guide\n- Dog Training Guides – Lessons And Exercises From Pippa Mattinson\nKeeping your dog occupied\nTo share a happy home with your Collie, you not only need to keep him clean but also occupy his brain.\nKeeping a collie busy, physically or mentally, is an important task too.\nYou don’t need to go for a forced march every day, but do make sure he gets a good daily walk or run. You can spend some time additional time teaching him a new skill or playing fetch in the garden.\nBorder Collie health and care\nThese dogs are a healthy bunch in general. Because they are bred to work and need to be fit to fulfill this role, they are generally in good physical shape. This said, there are some conditions and diseases they are more prone to.\n- Hip Dysplasia\n- Collie Eye Anomaly (CEA)\n- Progressive Retinal Atrophy (PRA) blindness\nThere are also health problems associated with the gene that causes the merle coat color pattern in dogs. These can be avoided by ensuring that two merle dogs are not mated to one another.\nYou can read about the complications associated with merle gene here. Also, you can find more information in the footnotes of this one.\nWhen choosing a Border Collie puppy, health testing is very important. Make sure the breeder provides you with the hip scores of their parents, and evidence of clear eye certificates as well.\nThis will seriously reduce the chance of your puppy having these particular genetic problems. For more information about the importance of health testing your pup you can consult the International Sheepdog Society here.\nGood news! Borders can live to a ripe old age! A study conducted in English found that this breed is among the dogs that live the longest.\nThe same study concluded that the average Border lifespan is 13.5 years. Some can live as long as 17 years!\nBorder Collie Grooming\nThese are not low maintenance dogs.\nAs most of them have fairly long coats, they will require regular grooming for all that shedding. Daily grooming can help to keep those tangles at bay.\nThe more regularly you groom your dog, and from an early age, the easier it will be for you both. So if you are thinking of getting a Border puppy, make brushing his fur a daily task from the word go.\nKeep it a part of your daily routine. That way you will find keeping his coat tangle free and manageable won’t take more than a couple of minutes a day.\nA dog who enjoys being brushed will also find this a very bonding experience with his owner.\nFeeding your Border Collie\nProviding your dog with the right kind of food is one of the best things you can do to keep him happy and healthy.\nWhat’s best to feed your dog depends on multiple factors. Older dogs for example, need approximately 20% less calories than younger dogs.\nBorder Pups need quite a lot more calories than other dogs. We have an entire guide to feeding Border Collie Pups here.\nThere are so many great options for feeding your adult dog. These include: kibble, wet-food, BARF and homemade diets. The amount of choice can be overwhelming.\nSo, be sure to check out our guides on the best way to food your Border.\n- Best Wet Dog Food – A Complete Guide to Finding What’s Best for Your Dog\n- Best Dog Food for Senior Dogs with Sensitive Stomachs\n- Feeding your puppy commercial dog food: the pros and cons of kibble\n- Raw Food for Puppies: How to Feed Your Puppy on Natural Raw Food\n- Best Inexpensive Dog Food\n- 9 Great ideas for natural raw dog food\nDo Border Collies make good family pets\nWhile Borders can make fantastic companions there are some things to consider before introducing one to your family. This is particularly true with young dogs or those who haven’t been properly socialized.\nIt is not uncommon to see a young Border Collie try to herd the children in his family. Whilst this might seem funny at first glance, remember that you don’t want to encourage this behaviour.\nYour children and your puppy both need to learn what is appropriate behaviour from the very beginning, so make sure to set boundaries. Let your children know that as soon as this behaviour begins, they are to leave the room or sit down and ignore him.\nHe may well also try and nip or bite children, as a part of this herding instinct. This can be upsetting to some children, who will emit high pitched noises that the dog finds stimulating, and so will encourage the negative behaviour.\nUnfortunately, for this reason if you have very young children this breed may not be an ideal choice for you at the moment.\nRescuing a Border Collie\nRescuing a dog can be a really rewarding experience for you and your new furry friend.\nThere are lots of great reasons to adopt a dog aside from the obvious benefit of giving a dog a new lease on life! For example, when you rescue a dog you can meet him and see if his personality matches with you and your family.\nAnother attractive reason to adopt is that the Border Collie price from reputable breeders is between $600-$800\nSome dog shelters will provide health testing, so be sure to ask that if have done all the proper health testing for these pups. They should be able to tell you about any pre-existing conditions a dog may have.\nYou can read our guide to adopting vs buying here.\nFinding a Border Collie puppy\nWhen beginning your search for a new companion we strongly advise that you avoid pet stores and puppy mills.\nPups that come from puppy farms will have no socialization. Not to mention the bitch you pup came from is almost certainly not receiving the care she needs. This might have a knock-on effect for your puppy.\nIf you see Border Collie puppies advertised and you think they may be from a farm, it’s best not to buy them. You can read our guide to spotting a puppy farm here.\nPet stores are somewhat in the same boat as puppy farms. Pups in these stores were likely sourced from puppy farms and do not receive the care they need.\nMany people are finding their new companion pooch in a mixed breed. Mixed breeds can be an excellent alternative to purebred dogs. Finding a mix may be a lot easier than finding a purebred too!\nThere is lots to consider and finding a puppy may seem daunting. Not to worry, here’s our complete guide to finding a puppy:\nRaising a Border Collie puppy\nCaring for a vulnerable puppy is a big responsibility. There are some great guides to help you with all aspects of puppy care and training.\nYou’ll find them listed on our Border Collie puppy page.\nPlus, you can find out all about puppy development stages here.\nPopular Border Collie breed mixes\n- German Shepherd Border Collie mix\n- Blue Heeler Border Collie mix\n- Border Collie Lab mix\n- Border Collie Australian Shepherd mix\nComparing the Border Collie with other breeds\nIn this article, we compare the Border vs the Collie.\nCollie vs Border Collie: Which of These Is the Right Companion for You?\nOther dog breeds you might want to consider\nPerhaps you had your heart set on this breed but it didn’t work out. Maybe some of these breeds would be suitable.\n- Labrador Retriever: A friendly and super popular breed\n- Golden Retriever: Beauties with hearts of gold\n- German Shepherd: For those with lots of space!\n- Good Family Dogs – A Complete Guide To Choosing Your Family Pet\nPros And Cons of Getting A Border Collie\n- Puppies that haven’t been socialized may nip or show guarding behavior.\n- These dogs need plenty of exercise, which isn’t suitable for everyone.\n- Those with the merle gene are susceptible to many health concerns\n- This is a sturdy breed that is usually in good health\n- They are incredibly intelligent\n- These dogs love to learn and will enjoy being trained\nProducts and accessories\n- Best Toys for Border Collies – Keeping Their Brains and Bodies Busy\n- Border Collie Grooming – Top Tips For Shiny Fur & Healthy Skin\nBorder Collie Breed Rescues\n- Border Collie Rescue Directory – This directory links to no fewer than 85 Collie rescues in the United States.\n- Midwest Border Collie Rescue\n- Come Bye BC Rescue\n- Western Border Collie Rescue (Facebook)\n- Sweet Border Collie Rescue\nNorth American Border Collie Rescue Network\n- Border Collie Trust GB\n- Wiccaweys Rescued Border Collies and Sheepdogs\n- The Border Collie Spot\n- Border Collie Rescue Online\n- Valgrays Border Collie & Animal Rescue\n- Creekside Farm\n- Hull’s Haven Border Collie Rescue\n- Okanagan Collie Rescue\n- Southern Ontario Border Collie Rescue\n- Border Collie Rescue Ontario\n- That’ll Do Collie Rescue (Facebook)\nDo you know of any more rescues? Let us know in the comments.\nIf you are looking for an easy-going companion, who doesn’t need much attention, then a collie might not be the right choice for you.\nBut if you want a partner who will work hard alongside you as a team, who is eager to learn and if you have plenty of time to devote to him every day, then he could be just what you are looking for.\nMake sure to tell us about your Border Collie in the comments below!\nThis article has been extensively revised and updated for 2019\nReferences And Resources\n- Gough A, Thomas A, O’Neill D. 2018 Breed Predispositions to Disease In Dogs and Cats. Wiley Blackwell\n- O’Neill et al. 2013. Longevity and Mortality of Dogs Owned In England. The Veterinary Journal\n- Schalamon et al. 2006. Analysis of Dog Bites In Children Who Are Younger Than 17 Years. Pediatrics\n- Duffy D et al. Breed differences in canine aggression. Applied Animal Behaviour Science 2008\n- Strain G. Deafness prevalence and pigmentation and gender associations in dog breeds at risk. The Veterinary Journal 2004\n- Packer et al. 2015. Impact of Facial Conformation On Canine Health. PlosOne\n- Adams VJ, et al. 2010. Results of a Survey of UK Purebred Dogs. Journal of Small Animal Practice.\n- Border collie comprehends over 1,000 object names\n- Per Arvelius et al. 2013. Measuring herding behavior in Border collie—effect of protocol structure on usefulness for selection, Journal of Veterinary Behavior.\n- Blackshaw, J. 1991. An overview of types of aggressive behaviour in dogs and methods of treatment, Applied Animal Behaviour Science\n- The National Academies (2006). “Your Dog’s Nutritional Needs: A Science-Based Guide for Pet Owners”. Nutrient Requirements of Dogs and Cats."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:1ebd69e9-1514-440a-8f8c-4c734c0da654>","<urn:uuid:78778d4e-54e8-438f-85a0-8061ad8674c3>"],"error":null}
{"question":"What is the etymology of 'deviled eggs', and what are the critical food handling practices for egg-based dishes in outdoor settings?","answer":"The term 'deviled' in reference to food appeared in 1786, and by the 19th century specifically referred to spicy or zesty preparations, including eggs prepared with mustard and pepper. In some US regions, alternative names like 'stuffed eggs', 'salad eggs', or 'angel eggs' are used. For outdoor food safety with egg-based dishes, it's crucial to keep them chilled as bacteria grow quickly at room temperature and faster in hot sun. Use coolers packed with ice or freezer packs, maintain proper hand hygiene, and keep raw and cooked foods separate. Never leave perishable foods at room temperature for more than two hours, and this time decreases further in higher temperatures.","context":["Deviled Eggs | Hard Boiled Eggs Facts & Techniques\nWhen life gives you hard boiled eggs, we say, let’s make Deviled Eggs. Holidays like Easter, often, we are left with a bunch of leftover eggs we aren’t always sure what to do with before they perish.\nWhat would be better than making a batch of deviled eggs?\nBe sure to make extras because we all know there is never enough 🙂\nLet’s answer the main question of the day.\nWhy are they called deviled eggs?\nSounds kinda odd, doesn’t it? Rest a sure it has nothing to do with the devil, yet they are called deviled eggs in part due to the ingredients used to season them.\nEtymology of the Term “Deviled” Eggs\nThe term “deviled”, in reference to food, was in use in the 18th century, with the first known print reference appearing in 1786. In the 19th century, it came to be used most often with spicy or zesty food, including eggs prepared with mustard, pepper or other ingredients stuffed in the yolk cavity.\nIn parts of the Southern and Midwestern United States, the terms “stuffed eggs”, “salad eggs”, and “dressed eggs.” The term “angel eggs” has also been used in association with deviled eggs stuffed with “healthier” (less fat and cholesterol) alternatives\nWhat makes a great Deviled Egg?\nThe honest answer is passion and cooking with love. These words could not be truer 🙂 Yet, the cooking of the egg is the “Key” to great deviled eggs.\nBelieve it or not, in culinary school, this was a contested answer of” how to perfectly boil an egg”. I’ll explain some of the things to consider when your goal is perfection.\nHere are my Finds & Facts on Hard Boiled Eggs\n- Always start with cold water – If you add the eggs to hot water, the exterior of the eggs will be overcooked and rubbery\n- Cooking time changes by one minute for every 500ft feet above sea level. The reason is this raises the boiling point of water by one degree. Meaning the temperature of boiling water increases by one degree as your elevation moves 500ft higher. At sea level -to- 500ft, water boils at 212 degrees. Yet, if you are at 1000ft above sea level the temperature of boiling water increases to 213 degrees. This means you must adjust your cooking time per 500ft. This could be why you’ve heard so many different cooking times to boil an egg.\nHow to adjust for elevation. At sea level the perfect boiled egg takes 12 minutes from the time the water reaches a boil. Each step above 500ft above sea level adds one minute of cooking time.\n- Do not overfill the pot with eggs. If you add a bunch of eggs that are cold this will increase the cook time due to the fact the water will take longer to come to a boil due to the cold eggs. If you must add a bunch of eggs at once, make sure the eggs are room temperature.\n- When in doubt, cook the eggs for less time versus longer. Overcooked eggs are gross.\n- How can you tell if the boiled egg is overcooked? Overcooked boil eggs will have a green hue around the outside of the egg. The eggs will also have a harder texture and become grainy.\nDo You Know Just How Good Eggs Are For You?\nCheck Out The Amazing Truths About Eggs\nClick Photo Below\nHow to Hard Boil the Perfect Egg\n- Add eggs to pot, do not stack eggs on top of each other, at a maximum cover the bottom of the pot with eggs.\n- Add “Only” cold water to cover the eggs. Add enough water that the tops of the eggs are well below the waterline.\n- Heat the water on high heat. The faster you can get the water to boil the better. If you heat the water slowly the eggs are going to be overcooked.\n- Once the water reaches a rapid boil turn off the heat and remove from the heat. Cover with a lid or plate and start your timer. At sea level, set the timer for 12 minutes exactly.\n- Once the timer goes off remove the eggs immediately and place them into an ice bath to stop the cooking process.\n- The ice water will make peeling the hard boil egg a little easier.\nI have a new ingredient for you to add to your deviled eggs recipe\nAdding avocado to the creamy egg yolks creates an amazing texture.\nAvocado adds such a smooth texture you could even reduce the number of eggs you use.\nWe all know Avocadoes are good for us. Did you know they are a Super Food now?\nHere’s an article I wrote on Avocadoes. A really good read.\nAdd To Your Yummly Recipes\n- 12 Boiled Eggs Halved\n- 1/3 cup Mayonnaise\n- 1 tsp Dijon Mustard\n- 1/4 cup Shallots Chopped or two medium shallot\n- 3 Gherkins Pickles (Finely Chopped) sweet pickles work really well\n- 1-to-2 tsp Fresh Chives sliced thin - save some for garnish\n- Optional Angeled Ingredient - Half of one Avocado\n- Optional Deviled Ingredient - 2 Slices Bacon (Cooked and Chopped)\n- Garnish 1/2 tsp Paprika\n- 1/2 tsp of Sea Salt\n- 1/4 tsp of Pepper Black or White\n- Place the eggs in a large pot and cover completely with cold water. Bring to a boil and then cover and remove from the heat. Let the eggs sit in the hot water for 11 minutes, then drain and cool in an ice bath.\n- Peel Eggs and cut lengthwise and remove the yolks and place into mixing bowl.\n- Add, mayonnaise, mustard, shallots, pickles, chives(part-save some for garnish, avocado, bacon, salt and pepper. Mix ingredients.\n- You could use a piping bag, I like using ziplock plastic bags. Or you could use a spoon to fill.\n- Be generous, fill over the top of the egg side.\n- Garnish with Paprika and chives.\nAdd To Your Yummly Recipes\nBe sure to check out our Egg Cookery Series.\nChef Steven Pennington\nLe Cordon Bleu Chef sharing food adventures from around the world with a style of cooking rooted in southwestern flavors using French culinary technique.","Risky Foods to Watch Out For continued...\nUndercooked eggs were the most commonly eaten risky food. That includes eggs served sunny-side up as well as raw eggs used in preparation of hollandaise sauce, meringue, Caesar salad dressing, and the like.\nFood safety authorities have long steered people away from sunny-side-up, soft-boiled, or \"over- easy\" eggs, all of which carry the risk of salmonella. If you must eat runny eggs or use them in recipes, Weis suggests you buy pasteurized eggs, which have been briefly heated to destroy bacteria. They are available -- typically at a small premium -- at many supermarkets. If you're ordering sunny-side-up eggs at a restaurant, ask if they're pasteurized, she suggests.\nRisky Take-Home Food\nFood safety experts are keeping an eye on some worrying trends in food consumption. One is the growing preference for unprocessed \"natural\" foods available at farmers' markets. While many of these foods may be very healthy, unpasteurized dairy products and juices are more likely to carry a variety of nasty bacteria, experts say. \"You can have fresh and locally-produced food that is also safe,\" Scallan says, \"and safe food means pasteurized milk and juices.\"\nAnother trend: Buying prepared food from supermarkets and then bringing it home for the family. It's dangerous to leave perishable food at room temperature for more than two hours, notes Shelly Feist of the nonprofit Partnership for Food Safety Education. That window shortens considerably when temperatures are higher -- such as when a prepared meal is placed in a hot car. So be sure to eat prepared food soon after purchasing it, Feist says.\nFood Safety in the Outdoors\nThe warm weather that makes springtime so inviting also creates an ideal breeding environment for bacteria and other pathogens found in food. Here's some advice from The Partnership for Food Safety Education on how to apply food-safety tips to the out-of-doors:\n- Clean. Wash your hands -- as well as utensils, cutting boards, and countertops -- in hot soapy water before and after preparing each food item. Also wash produce, which can carry harmful bacteria. As easy as these directives can be to ignore while in the kitchen, it's even easier when you're outside and playing with the dog, with the Frisbee, or with your baby niece, Feist notes. Don't drop your guard!\n- Separate. Sure, it's tempting to pour that sauce you used to marinate raw burgers or chicken wings onto the cooked food. It's also tempting to place the cooked grub back onto the plate that held your raw meat. Don't give in! Always keep raw meat and its juices separate from cooked food. If you want to reuse that sauce, then boil it first.\n- Cook. It's the best way to kill bad bugs. When grilling, preheat the coals on your grill for 20 to 30 minutes, or until the coals are lightly coated with ash. Use a meat thermometer to ensure that hamburgers and red meats are cooked to 160 degrees and ground poultry to 165 degrees. Poultry breast should be cooked to 170 degrees; dark meat (wings and thighs) should be cooked to 180 degrees. Poultry juices should run clear. Fish should be opaque and flaky.\n- Chill. No, this doesn't refer to what you do after breaking open a Bud at the barbecue. Bacteria grow quickly at room temperature and even faster under the hot sun. So refrigerate meat while it marinates, and keep that potato salad in a cooler that's well-packed with ice or freezer packs.\nMore tips are available at the Partnership's web site, www.fightbac.org."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"language-ambiguous"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:a9f60f6c-c1c8-44f1-8398-534ffa0ded39>","<urn:uuid:0153f205-e5b4-4085-bdc6-58c60f9f165a>"],"error":null}
{"question":"What sustainable solutions exist in Uganda for both food and energy production? Describe current examples.","answer":"In Uganda, sustainable food solutions include traditional recipes like Kabalagala pancakes, which use locally available ingredients like cassava flour and sweet bananas. For energy production, companies like Pamoja Cleantech are implementing sustainable solutions through biomass-based electricity systems that use agricultural residues and wood from sustainable sources. They combine serving telecommunication towers as anchor customers while providing electricity to rural communities. The systems incorporate agroforestry plots and hedgerows for sustainable biomass supply, which can simultaneously improve ecosystem health and agricultural productivity in the region.","context":["616. Kabalagala (Ugandan Pancakes)#BreadBakers\nKabalagala (Ugandan Pancakes)#BreadBakers\nSo many different types its unbelievable\nI didn’t have any theme in mind when I volunteered to host the month of February for the Bread Bakers group.As Christmas got over and the year was ending I knew I had to think of a theme pretty soon. I went back to the rules for participants and there the word pancake kept staring at me. The only pancakes I know are the usual Indian ones like dosas, uttappams, chilas etc and of course the typical fluffy American style ones dripping with melted butter and honey or maple syrup. Did a bit of research (what would we do without Google?) and hey presto the world opened up to me with a variety of pancakes I’d never heard of. So I challenged the members to the theme of pancakes from different parts of the world, to venture out to pancakes they’ve not made before.\nI am so grateful that the members took time to research and have come up with different pancakes. Check the list below and please visit each blog to find out the different names and recipes of the pancakes.\nSo basically the dictionary describes pancakes as a thin flat cake of batter fried on both sides on a griddle or in a frying pan. However, as I researched, the meaning of pancake widened. Some pancakes are baked, some are fried and some may appear like flat breads but are actually known as pancakes in the region of origin.Some have yeast as leavening agent others have baking agents. Pancakes can be sweet or savory, may contain different types of flours, fruits, vegetables. There’s a whole world of pancakes out there. Drop scones, waffles, crumpets, pikelets, oatcakes are classified as pancakes.(However countries of origin may refute that!)\nMy contribution towards this theme is a pancake from Uganda. Uganda is a landlocked East African country, neighbouring Kenya on the west side. When one mentions Uganda and the immediately one thinks of the dictator ruler Idi Amin. His rule ruined a country which at one time was known as the Pearl of Africa. Uganda has very fertile farmlands and amazing National Parks.Its exports coffee and other produces. The staple food in Uganda is maizemeal, plantains, peanuts, cassava along with meat. Kenyans love the small bananas or menvu as they are called in Uganda. They are sweet and irresistible.\nThese pancakes from Uganda are called Kabalagala. They are made from cassava (tapioca) flour, mashed sweet banana or plantains. Gluten free,sugar free they look more like doughnuts but every possible Ugandan blog post, article I read about food from Uganda describes Kabalagala as a pancake.The recipe is very simple and the pancakes were absolutely delicious with a hot cup of coffee. Kabalagala in the Luganda language means pancake made using sweet bananas and cassava flour.The original preparation made by Nubians was called kabalagara. An affluent area in the city of Kampala is named after the pancake. Kabalagala is a famous street food in Uganda, enjoyed with tea as breakfast or served with stew. It is believed that these pancakes became very famous as a cheap alternative to cakes and bread during the Idi Amin Regime as they were affordable and combination of banana and cassava keeps one’s tummy full for a long period of time.\nKABALAGALA (UGANDAN PANCAKES)\nRecipe source: Here\n2 big or 6 small over ripe bananas\n2-2¼ cups cassava(tapioca) flour\na generous pinch of salt\n¼ tsp soda bicarbonate (baking soda)\n¼ tsp pepper powder\noil for deep frying\nextra flour for dusting\n- Peel and mash the bananas.\n- Sift flour, salt and pepper powder together.\n- Add flour little by little into the mashed banana and mix with a spatula or a spoon.\n- Keep on adding the flour till its thick enough to knead.\n- Dust the worktop with some flour and knead the dough. The dough should not be sticky. I used about 2¼ cups of flour.\n- Roll it out into a ¼” thick circle. Using a cookie cutter or a glass, cut out round discs.\n- Gather up the remaining dough and roll again and cut. Keep on repeating the process till all the dough is used up.\n- Heat oil in a wok or deep frying pan over medium heat. The oil is ready when a small piece of dough put in the oil rises to the top immediately.\n- Fry the pancakes till they are golden brown.\n- Dust some icing sugar if you like before serving.\n- I found the sweetness from the ripe bananas was just right. If you have a sweeter tooth, add 1-2 tbsp sugar.\n- Original recipes do not add baking agent. Adding it makes it more chewable.\n- A little bit of pepper and salt balances the sweetness from the bananas.\n- The leftover pancakes next day became more chewy. I would recommend that you eat them the day they are prepared.\n- Before frying the pancakes, brush off the excess flour you’ve used for dusting.\nCheck out the Pancakes from different parts of the world that our fellow Bread Bakers have made this month:\n- Alagar Kovil Dosai from Sara’s Tasty Buds\n- Blueberry Dutch Baby from Hostess At Heart\n- Brown Rice Dosa (Indian Savory Crepes) from Spiceroots\n- Buckwheat, Blackberry and Saffron Drop Scones from A Shaggy Dough Story\n- Chinese Scallion Pancakes from Karen’s Kitchen Stories\n- Corn Pancakes from Kids and Chic\n- Crepes from A Baker’s House\n- Dutch Baby from Herbivore Cucina\n- Galettes de Sarrasin from The Bread She Bakes\n- Greek Tiganites from Gayathri’s Cook Spot\n- Hotteok (Korean Pancakes) from Cook’s Hideout\n- Hotteok (Korean Stuffed Pancakes) from Passion Kneaded\n- Kabalagala (Ugandan Pancakes) from Mayuri’s Jikoni\n- Keralan Yeast Appam from Food Lust People Love\n- Malpua (Sweet Indian Crepes) from SimplyVeggies\n- Oven Baked Tropical Pancakes from A Day in the Life on the Farm\n- Pannukkau (Finish Pancakes) from Cindy’s Recipes and Writings\n- Potato Latkes (Jewish Pancakes) from Sneha’s Recipes\n- Savory Finnish Baked Pancakes(Pannukakku) with Smoked Salmon from The Wimpy Vegetarian\n- Srilankan Hoppers from I camp in my Kitchen\n- Strawberry Nutella Crepes from Spill the Spices\n- Swedish Pancakes from Palatable Pastime\n- Sweet Potato Pancakes with Brown Sugar and Pecan Sauce from A Salad For All Seasons\n- Wholegrain Yeast Pancakes from Ambrosia\n#BreadBakers is a group of bread loving bakers who get together once a month to bake bread with a common ingredient or theme. You can see all our of lovely bread by following our Pinterest board right here. Links are also updated after each event on the #BreadBakers home page.\nWe take turns hosting each month and choosing the theme/ingredient. If you are a food blogger and would like to join us, just send Stacy an email with your blog URL to firstname.lastname@example.org.","Truly sustainable bioenergy in East Africa?\nThomas Buchholz – Gund Institute for Ecological Economics, University of Vermont, USA\nPeik Stenlund – Co-Founder Pamoja Cleantech AB, Stockholm, Sweden and Kampala, Uganda\nStephen Christensen – Chalmers University, Göteborg, Sweden\nLéonore Joërg – Ecole Nationale Supérieure des Mines de Nantes, France\n95% of the people in rural areas in East Africa have no access to electricity. At the same time, the telecom industry has been at the forefront of infrastructure development in rural areas and has proven to be a very strong business with positive social impact. Seven in ten Africans have their own cell phone today, network access is essentially universal in several African countries including Kenya. However, the telecom base stations in off-grid rural areas are currently powered by diesel with both high economic and environmental costs.\nParticularly bioenergy technologies such as power from small-scale gasification fueled by agricultural residues or wood from sustainable sources exhibit promising potential for expansion. While small-scale photovoltaic systems are able to provide household lighting and electricity for charging cell phones or a radio at an affordable price, gasifier systems produce electricity at scales starting around 10 kw, a scale where photovoltaic systems become costly compared to other alternatives such as diesel-fueled generators. While a 10 kW could provide electricity to around 10 households in the US only, in rural Africa this scale is able to power water pumps, grain mills, minigrids at trading centers, or medium-sized commercial loads powering for instance a telecommunication tower, thereby increasing the standard of living significantly in a whole community and region that previously had no access to electricity.\nA widespread application of these bioenergy systems is hampered by several factors including a lack of business models to run the units and the absence of frameworks to measure the potential ecological risks of putting an additional biomass-consuming system into place in an already constraint ecosystem: A growing population using biomass for over 90% of their primary energy needs and land use change from forests being cut for small scale farming and large monoculture has caused Uganda to lose over 1/3 of its forest cover over the last two decades. Implementing a biomass based electricity system can further contribute to these problems if the biomass fuel supply is not sustainably managed. While per capita wood consumption in sub-Saharan Africa is between 500 to 1,000 kg of dry wood per year, gathered mainly for cooking, a 10 kW gasifier placed in a community and providing basic electricity services would require an equivalent of another 7 kg of dry wood per capita and year. Therefore, for these systems to become truly sustainable, their design needs to consider ecological and social factors on the ground as well as fulfill larger policy goals within a regional context.\nA concept to deliver sustainable biomass-based electricity to rural communities in Africa.\nRecent entrepreneurial breakthroughs have provided ample examples that renewable sources can boost rural electrification expansion, even in regions where government-led initiatives have a poor track record. Pamoja Cleantech is a young social enterprise driven by an ambition to solve social and environmental problems with entrepreneurial means. Just three years into this venture, Pamoja has already installed three gasifiers in rural communities in Uganda with the first unit poised to be connected to a telecommunication tower by the end of the year. Serving electricity to these towers offers considerable business opportunities for Pamoja due to its replication potential. But more importantly, only the presence of such an ‘anchor load’ that offers reliable long-term business perspectives will mitigate the risk of a bank financing the undertaking and Pamoja installing and maintaining the gasifier in a rural village. Serving both, the single-largest anchor load customer as well as multiple small-scale users providing services for the community is key to this social enterprise model. This game-changing idea enables the telecom industry to make a transition from diesel to a renewable energy solution while expanding electricity access in a rural community at the same time.\nAn agroforestry plot (lower left) in Uganda next to erosion-affected fields. Adaptation potential abounds.\nA focus of Pamoja’s work is on understanding the biomass supply chain and identifying sustainable solutions that enhance an ecosystem’s resilience rather than contributing to further degradation or competing for fertile land with food production. Identifying a sustainable scale of biomass consumption might well drive the overall scale of a sustainable bioenergy system since each kW installed might require as much as one hectare of accumulated woodlots or hedgerows. Agricultural residues might be a sustainable fuel as long as their extraction rate does not reduce soil fertility or creates indirect land use change by forcing families to gather fuelwood for cooking who previously relied on these agricultural residues. Dedicated fuelwood plantations, hedgerows or agroforestry systems are another option for a sustainable biomass supply chain and can create regional ripple effects by adoption of sustainable agricultural practices; boosting ecosystem health and productivity in a whole region.\nNevertheless, the question remains where setting a path towards increased electricity consumption to improve basic living standards, will lead these communities in the long-term. If we only create new demands that can eventually not be satisfied anymore by the regional ecosystem – be it more biomass for more electricity, or more food for more people – the whole effort will contribute further to unsustainably managed landscapes. If we fail in providing better living standards in rural Africa, urban migration patterns will become disastrous by themselves from an ecological and human well-being perspective. The question what sustainability entails, what lifestyle these landscapes can sustain, what role models we can draw on, what the social consequences of introducing bioenergy to a village are, and if such a thing as sustainable development even exists, is on our minds every day when we work in African communities.\n- Pamoja website\n- Modern bioenergy systems – examples in East Africa: A clip on the Gund Institute for Ecological Economics youtube channel: http://www.youtube.com/watch?v=q-sG0guf26M\n- Buchholz, T.; Da Silva, I.. (2010) Potential of distributed wood-based biopower systems serving basic electricity needs in rural Uganda. Energy for Sustainable Development 14(1) 56-61"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"format_constrained"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:a9e91ade-eac0-4ba8-bb76-47dc90b51cb3>","<urn:uuid:63e04699-038f-47ce-9783-de571ebe6fd5>"],"error":null}
{"question":"Can you explain the primary purposes and different types of chemical peels versus dental bonding treatments?","answer":"Chemical peels and dental bonding serve different cosmetic purposes. Chemical peels use varying strengths of acids (such as glycolic, salicylic, or lactic acid) to lift away top portions of skin, revealing younger, smoother-looking skin and making it look brighter and fresher. In contrast, dental bonding comes in two types: direct composite bonding, which uses tooth-colored composite putty to fill cavities, fix chips, or minimize space between teeth, and adhesive bonding, which uses an etchant to attach materials like veneers, bridges, and crowns to teeth.","context":["The effects of a chemical peel range from simply making skin look brighter and fresher to literally peeling years off your skin. Collectively, chemical peels are among the most popular nonsurgical cosmetic procedures, according to the American Society for Aesthetic Plastic Surgery.\nChemicals peels use varying strengths of acids to lift away the top portions of your skin, says Mohiba Tareen, MD, a dermatologist at Advanced Dermatology, P.C./Center for Laser and Cosmetic Surgery and clinical instructor of dermatology at Columbia University, both in New York City. The goal is to reveal younger, smoother-looking skin.\nAnd there are plenty of options to choose from, starting with a mild glycolic chemical peel you can give yourself at home to a deeper trichloroacetic acid or TCA chemical peel available at a doctor’s office. Consider the pros and cons and healing time of each type to help you decide if a professional chemical peel is right for you.\nAt Home Chemical Peel\nHome chemical peels won’t give you the significant results you can get from professional chemical peels, but they can help your skin look brighter and fresher, Dr. Tareen says. You can choose from salicylic acid, lactic acid, and glycolic acid chemical peels.\n- Pros: For most people, home chemical peels are safe and can make your skin look better. They’re also very affordable. Most cost under $50, and some are even under $20.\n- Cons: The results won’t be as significant as what you would get at a spa or doctor’s office. Also, peels that are higher than 50 percent acid can cause scarring, especially if you have darker skin, so be sure to buy a peel that’s a lower percentage of acid, Tareen says. If you’re a woman of color, read the label and search for a chemical peel specifically made for your skin tone.\nChemical Peels at a Spa\nThe professional chemical peels you get at a spa tend to be mild to medium peels, which can help you look brighter and fresher and help with wrinkles and scarring, Tareen says. How much skin is lifted away depends on the strength of the peel being used, how heavily the chemicals are applied to the skin, and how long the chemicals stay on the skin’s surface.\nThe mildest peels used at a spa will probably contain glycolic acid or another alpha hydroxy acid (AHA). A TCA chemical peel will penetrate more deeply than AHAs.\nAlthough prices vary, chemical peels at a spa may cost $70 to $400, depending on how deep it is and where you’re getting it done.\n- Pros: Light and medium peels are safe and fairly painless. You may only feel slight stinging or tingling from the chemicals. You can also leave the spa and go about your normal day after getting a light or medium chemical peel, but be sure to use sunscreen.\n- Cons: Because of the risk of scarring, Tareen doesn’t recommend getting a peel that’s higher than 50 to 60 percent acid at a spa. And if you’re a woman of color, she recommends going to a dermatologist for a chemical peel to lower your risk of scarring.\nChemical Peels at the Doctor’s Office\nDermatologists and plastic surgeons offer light, medium, and deep peels. If you’re looking for a deep peel that will make your skin look significantly younger, Tareen recommends going to a dermatologist or plastic surgeon rather than a spa.\n- Pros: A trained doctor or surgeon will know what type of peels to use that are best for your skin tone, lowering your risk of complications like scarring, changes in pigmentation, or infection. A doctor is also the most qualified practitioner to give a deep peel, which involves treating the skin with a high concentration of acid for about an hour, then covering the area with petroleum jelly or adhesive coverings that stay on for one or two days. You’ll need to have your heart rate monitored during the procedure. And you’ll have to go back every day for dressing changes, Tareen says.\n- Cons: A doctor’s office will be the most costly option, depending on how deep a chemical peel you choose. A light peel may cost up to $300, while a medium peel may cost between $500 and $1,000 at a doctor’s office, Tareen says. A deep peel can be as much as $3,000, but the effects can last for up to 20 years. Keep in mind that the deeper the peel, the longer the recovery time.\nTreating your skin with an acidic solution can cause some serious complications, so it’s important to know the risks of doing it at home, at a spa, and at a doctor’s office. Following these guidelines will help you decide what’s best for you.\nLearn more in the Everyday Health Skin and Beauty Center.","How Much Does Dental Bonding Cost?\nIf you visit your dentist regularly, at some point or another you have probably received a dental boding treatment. Not sure what dental bonding is? The dental bonding procedure is when either bonding adhesive or composite material is permanently secured to your teeth. Dental bonding treatment is used for a variety of reasons, from cosmetic fixes to treating cavities. If your dentist has recently recommended you may need a dental bonding treatment, you may be wondering how much the bonding treatment will cost. Below is a more information on dental bonding treatment and some tips on how to cover the cost.\nTypes of Bonding\nThere are two types of dental bonding that are commonly used in a variety of dental treatments.\nDirect composite bonding is when a neutral tooth-colored composite putty is applied directly to the teeth to fill cavities, fix chips, or minimize space between teeth. Direct composite bonding can be performed in one dental visit and can be an inexpensive treatment approach to fix gaps, cracks, discoloration, and crooked teeth.\nAdhesive bonding is when material is attached to the teeth using an etchant. An etchant is an adhesive material. The etchant is securely attached through a curing process, using a high intensity light. Adhesive bonding is more commonly used to attach veneers, bridges, and crowns in the mouth.\nThe Bonding Process\nDental bonding treatment can be quickly completed in one dental visit. Prior to the bonding, a rubber dam is placed in the mouth to keep the treatment area dry and allow access to the area without interference. Next, a phosphoric solution is applied to prepare the surface of the teeth for the bonding. The teeth are now ready for either the direct composite bonding or the adhesive bonding treatment.\nDental Bonding Costs\nApproximating the costs of a dental bonding treatment can be difficult due to the various factors that will impact the cost. This estimation is further complicated by the fact that the dental bonding procedure is usually performed in combination with other treatments. Dental bonding is commonly part of an elective or cosmetic treatment and not based on need. If the bonding treatment is elective, cosmetic dental insurance plans may not fully cover the treatment costs. Some of the other factors that impact the costs of dental bonding treatment include:\n* Your dentist’s training and years of experience\n* The geographic location where your dentist is located\n* Type of bonding treatment used and purpose of treatment\nDental bonding that is needed with composite veneers usually costs between $300 and $700 for each veneer. However, the installation of porcelain veneers using bonding may cost on average between $600 and $1,500 for each veneer. If you are trying to estimate the costs of the bonding treatment you need, the best place to start is to have a discussion with your dentist. After you talk with your dentist you may also want to contact your dental insurance provider to determine if dental bonding is covered by your plan or if you have to pay out of pocket for the treatment."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"search_synthesis"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"concise_direct"}],"document_ids":["<urn:uuid:477801ec-4fca-4083-946a-20474f8e3b96>","<urn:uuid:65080af2-b4dd-4b23-b876-64d476220f2c>"],"error":null}
{"question":"What are the distinct approaches to lead hazard monitoring between property-based inspections and water system testing, particularly in terms of frequency and targeting?","answer":"Property-based lead inspections are conducted based on specific triggers and timing defined by local laws, often targeting certain categories of housing (by age or tenure) or geographic areas with high risk. Meanwhile, water system testing follows a more standardized schedule - Richmond's DPU conducts annual lead testing of water leaving the treatment plant, performs customer sampling at 50 locations every three years, and responds to specific customer requests as needed. The water testing is systematic and regular, while housing inspections can be more varied in timing and may be geographically targeted based on risk factors or housing characteristics.","context":["Very good article that compares, contrasts, and analyzes the effectiveness of housing-based primary prevention policies—laws that aim to identify and fix lead hazards before children become poisoned—including a look at the context surrounding their implementation:\nKorfmacher KS, Hanley ML. Are local laws the key to ending childhood lead poisoning?. J Health Polit Policy Law. 2013;38(4):757–813. doi:10.1215/03616878-2208603\nQuoting from the conclusion ;)\nLead-poisoning prevention may be one of the most complex issue areas in which municipalities—particularly smaller cities—are innovating new policies. Because lead is a “health problem with a housing solution,” it often requires new partnerships between health and housing agencies. Local lead laws operate in a complex legal environment of case, state, and federal law—understanding this setting and the roles of agencies at multiple levels of government can be daunting. Additionally, technical knowledge about cost-effective ways to inspect for, address, and monitor lead hazards is constantly evolving as the result of ongoing research and practical experience. Thus designing and implementing a cost-effective, up-to-date, technically sound lead law presents many challenges to local policy makers.\nAs one member of Rochester’s city council said, “Lead was the first policy in which we recognized that our staff did not hold the needed expertise, and we had to partner with outside agencies, community groups, and academic institutions to design a smart policy. After that, we used that approach with many new policies.”\nWhat did the authors of this paper do?\nThe authors conducted a process to first identify laws passed since the year 2000, then chose 19 to analyze, and 8 to do case studies on. Here are my favorite excerpts:\nTheir framework for comparing the structure of the laws is:\n- What is the target housing? (what is the category of housing affected by the law, typically by age or tenure)\n- What is the timing for proactive inspections?\n- What is the trigger for inspections? (what situations lead to an inspection)\n- What is the inspection type? (visual, dust wipe, full risk assessment, etc. and type of instrument used)\nSome of the key resources needed to effectively implement a local law are:\n- inspection resources (trained staff and adequate funding)\n- enforcement tools\n- evaluation systems\n7 points to analyze when crafting a new law\nQuoting the authors directly for this entire section:\nOur analysis suggests that prior to passing a new law it is critical to assess the local environment to inform policy choices, including:\nPhysical environment (geographic targeting): To what extent are lead hazards dispersed throughout the city versus clustered in neighborhoods that can be geographically targeted for implementation? What is the appropriate target housing? Example: If EBL or housing data show that most lead poisoning occurs in one section of a city, the lead law could target that area. If EBL cases are more dispersed, it might be more appropriate to target all rental housing, or all units occupied by children.\nHealth status and systems: What percentage of high-risk children receive blood lead tests? What percentage of these have EBL? Example: If EBL testing rates are low, it may not be prudent to rely on past EBL data for geographic targeting.\nPublic awareness (by residents, landlords, and community leaders) of the connection between lead poisoning and health, educational, and social outcomes. Example: In communities with high lead awareness, relying on provisions like self-referral (complaint by tenant) or voluntary certification by landlords is more likely to succeed.\nEconomy/housing market (rental revenues, vacancy rates, foreclosures, etc.): Are there any financial assistance programs available to subsidize needed repairs? Example: If the housing market is strong or financial resources are widely available, a local law might require full abatement requirements. If not, the more economical approach of interim controls plus ongoing monitoring of maintenance practices may be more realistic.\nState legal environment: Does the locality have the authority to implement a local lead law? Does state law have provisions (such as the power to enact stronger provisions in designated high-risk areas) that can serve as a framework for local action? Example: Depending on the jurisdiction, authority for local actions varies from being specifically authorized, to being preempted or limited, and in some cases is permitted but may require approval. This may limit the targeting options available to local policy makers.\nCase law: What are the relevant court rulings and settlements related to lead hazards, duty to maintain properties, inspections, and landlord liability? Example: Recent local cases in which damages are assessed against landlords may affect landlords’ perceptions of their legal liability for lead poisoning (and thus their economic incentive to maintain lead safety, independent of a local leadlaw). In such situations, voluntary certification or compliance is more likely to be effective than when liability is limited.\nImplementation resources: What is the public (city inspectors) and private (number of certified risk assessors and sampling technicians) capacity for conducting proactive inspections? Are property owners and maintenance staff trained and certified renovators? What enforcement tools currently exist that the city could access for this policy (fines, lead court, etc.)? Is there adequate funding for enforcement? Example: We identified several cases in which implementation was limited by insufficient city resources to inspect and enforce the law. If a law relies on private inspectors, sufficient time and resources must be allocated to develop technical capacity to conduct required inspections.8. Political climate: Are there supporters inside and outside government to champion the law? Who will be against the law (e.g., landlord groups) and how powerful are they? Is it possible to address the opposition’s key concerns while still enacting a viable law? Example: Strong political and community support for lead-poisoning prevention makes it more likely that a local law will be passed and effectively implemented.","Saturday, February 6, 2016\nWednesday, February 3, 2016\nThe City of Richmond Department of Public Utilities (DPU) has been monitoring and addressing the potential for lead contamination of drinking water for more than 20 years. When the Environmental Protection Agency (EPA) issued its Lead and Copper Rule in 1991, DPU began collecting data and making changes to its system to maintain compliance with all requirements. The EPA’s Lead and Copper Rule focuses on treatment techniques for lead and requires water systems to control the corrosivity of the water and conduct monitoring as needed. DPU did testing as part of the “Loop Study” to best determine the optimal chemical, chemical dose and pH to keep lead from leaching into the city's drinking water.\n|DPU's Water Treatment Cycle|\nOver the years, DPU has upgraded its systems to ensure proper chemical doses. There will also be a new calcium hydroxide system going into service in the next two months to better control the pH of the finished water. DPU monitors the pH of the water with online equipment that provides instantaneous results, and also runs tests twice a day to verify proper chemical dosage.\nIn addition to the process control performed at the Water Treatment Plant, water that leaves the plant is also monitored. It is tested for lead content by the Virginia Department of Consolidated Laboratories once a year. The results for this testing have always been well below the 15 µg/L action level set by EPA.\nEvery three years DPU is required to collect water samples from customers at 50 different locations throughout the service area. These samples are tested for lead and copper concentrations and the report is submitted to the Virginia Department of Health. Since the program started, the system has been in compliance for both lead and copper levels.\nAs needed, based on information or a request that indicates there may be a lead issue at a specific customer location (e.g., medical exam may show high levels of lead in the body, or lead pipes or lead solder is discovered during plumbing repairs), the city will conduct sampling at the site and provide the customer with the results of the testing. This not a frequent issue. Over the last four years, DPU has responded to 24 requests and all results have come back below the EPA action levels.\nWhat You Can Do\nThe water service line, after the water meter, and the pipes in all buildings are owned by the property owner and they decide how and when to renew those pipes. When pipes are replaced, it must be done so in compliance with current codes and lead-free fixture standards.\nIf customers are concerned about the possibility of lead in drinking water, they should flush the taps by letting the water run for at least 60 seconds. If your dwelling has a lead service line, you should flush water for an additional two to three minutes to ensure you're getting fresh water from the water main. To conserve water, you can collect the water being flushed and use for cleaning purposes or watering plants."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"illustrative_examples"}],"document_ids":["<urn:uuid:f9283e53-5ded-4c89-878c-2ab94aacc2ea>","<urn:uuid:8dbd53a8-32cd-42a2-98e7-a9ac861e3b62>"],"error":null}
{"question":"How can I tell if I have a glenoid labrum tear in my shoulder? What are the main symptoms to look out for?","answer":"The main symptoms of a glenoid labrum tear include a numbing and diffused pain in the posterior portion of the shoulder, popping or clicking sounds during shoulder movements, stiffness of the shoulder joints, shoulder strength weakening, and feelings of shoulder dislocation. These tears can occur from injuries like car accidents, falls, heavy lifting, poor posture while bending or standing, and weakness in shoulder muscles due to joint problems or poor bone health.","context":["The glenoid labrum is cartilage and connective tissue that is found in the shoulder and the hip joint. If you are experiencing shoulder instability, it’s possible that you are suffering from a glenoid labrum tear. Common causes of a labral tear include shoulder injuries from car accidents, falls, heavy lifting that puts excess strain on shoulders, poor posture while bending or standing, and weakness in shoulder muscles due to joint problems or poor bone health. Some of the symptoms include shoulder pain, discomfort, and inability to move the shoulder and arm properly.\nThe symptoms of a shoulder labral tear are very similar to that of a Rotator cuff tear, but may localize in a slightly different portion of the shoulder, and the effects may also be different. The visible symptoms are:\n- A numbing and diffused pain in the posterior portion of the shoulder\n- Varied shoulder movements cause some popping or clicking sounds\n- Stiffness of the shoulder joints\n- Shoulder strength weakening\n- Feelings of a shoulder dislocation\nCauses and Types\nThe types of tears that can be medically diagnosed can be classified into two categories:\nTraumatic labral tears: these tears are mainly the result of an incident of shoulder dislocation, or any injury from the heavy lifting. Athletes, weight lifters, gymnasts and even construction workers can be at risk of this type of injury.\nNon-traumatic labral tears: this type of tear is mainly due to weakening or the instability of the shoulder joint. The tear mainly occurs when increased strenuous activity is experienced by the already weak muscle or joint. The shoulder joints are comparatively more mobile than the traumatic tears.\nNot all labral tears are severe, some go unnoticed. It has been seen that the injury can even heal on its own, if suitable care is taken, and the injured area is not put under any further stress. One of the main reasons for improper healing is the lack of blood supply due to the injured portion of the shoulder. The continuous strain to an already injured shoulder can simply worsen the situation.\nIf the pain seems to be sustainable and prolonged, even after taking pain killers, it is highly recommended to consult a doctor for the situation before it gets worse. The first thing the orthopaedic surgeon will do is examine the patient’s previous medical history and then further administer the examination to test the condition of the area of the glenoid labrum. The tests that are performed involve moving the shoulder, which results in a popping or clicking of the shoulder joints. This is then used to determine whether the shoulder of the patient is stable or not.\nMagnetic Resonance Imaging or otherwise known as MRI is also done. Labral tears can even be diagnosed with certainty without any arthroscopic surgery.\nThe doctor tends to prescribe anti-inflammatory medications until the proper diagnosis of the condition can be made. The types of treatment may include non-surgical as well as surgical methods, if the condition becomes worse. The physician may reattach the ligament and thereby strengthen the shoulder socket with the simultaneous folding and pleating of the tissues in the adjoining areas.\nSome of the types of treatments are:\nOne of the most important non-surgical treatments that are suggested are physical therapy and exercises. It is very important to choose the best professionals for the physical therapy since improper treatment will not only delay the process of healing, but can also make the situation worse than before.\nWhen the physical pain is minor and not prolonged, the doctors tend to suggest treatment that includes several exercises and body movements to repair the damaged tissues.\nSome of the types of exercises are:\nStrengthening exercises: Improvements to the strength of the muscle is one of the first aims of the treatment. The physical therapist tends to suggest some strengthening exercises, which includes external rotation and internal rotation exercises that mainly helps in repairing and healing the muscles of the shoulder blades and also the shoulder joints.\nStretching Exercises: Stretching exercises help in restoring the normal balance and flexibility of the muscles that surround the injured area, to aid them and work more efficiently.\nWall walk-up: The Wall walk-up is one of the most recommended exercises for a labrum tear. The procedure includes standing near a door, and then grabbing onto the wall frame. The patients needs to walk up the frame as much as they can bear while using their fingers for aid and then count up to ten and repeat the same for three more times.\nShoulder flexion: In this exercise, the patient needs to be seated on a chair, with their backs straight, and then carefully raise their injured arm above the head. Make sure that their elbows are straight and then leave the arm for a few seconds. Repeat for a few more times, making sure that extra strain is not applied.\nWhen the pain fails to subside with the non-surgical treatments, the physician tends to suggest an arthroscopic surgery as the last resort. In this type of surgery, a tube-like instrument, known as an arthroscope, is inserted into the joint by making a small incision in order to view and further repair the injury. The success rates of these types of surgery has been quite high and have been every effective in healing any labral tear or any dislocation of the shoulder joints.\nRead more on labrum surgery."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:f2ebede7-2b48-42c8-9d1d-705a8d2cc259>"],"error":null}
{"question":"What similarities exist between John Gower's Confessio Amantis and The Canterbury Tales in terms of their narrative structure and temporal context?","answer":"Both works were created in the late 14th century and featured story collections, but with different framing devices. The Confessio Amantis was framed by the confession of a frustrated lover and was structured as a remedia amoris (remedy for love), while The Canterbury Tales was structured as a storytelling contest between pilgrims with each character intended to tell four stories. The Canterbury Tales remained unfinished at Chaucer's death, written in Middle English that showed influences of German, Norman French, and Old Norse languages.","context":["the Discourse of Love's Labor from Ovid through Chaucer and Gower\nPublication Year: 2012\nPublished by: The Catholic University of America Press\nTitle Page, Copyright\nDownload PDF (44.0 KB)\nDownload PDF (56.2 KB)\nWriting a book that conjoins two of the most fundamental areas of human experience has been a daunting challenge and a continuous pleasure. While it is not true that nothing matters beyond love and work, it would be difficult to find two more basic building blocks by which humans construct meaning and purpose for their lives...\n1. The Discourse of Love’s Labor and Its Cultural Contexts\nDownload PDF (120.0 KB)\nErotic love has been a major theme in Western literature at least since the poetry of Sappho but at no time more so than in the Middle Ages. Although partially tied in complex ways to human biology, “love” is also in large part a cultural construct, which changes and evolves as it passes from culture to culture and epoch to epoch.1 Its complexity...\n2. Labor Omnia Vincit: Roman Attitudes toward Workand Leisure and the Discourse of Love’s Labor in Ovid’s Ars amatoria\nDownload PDF (157.2 KB)\nJohn Gower claims to have constructed his Confessio Amantis from two sources: the world “in olde dayes passed” and the world “which neweth everi dai.” Most critics would agree that something similar could be said of Chaucer’s love poetry, and their sources for the past were, of course, “olde...\n3. Noble Servitium: Aspects of Labor Ideology in the Christian Middle Ages and Love’s Labor in the De amore of Andreas Capellanus\nDownload PDF (170.1 KB)\nThe last chapter demonstrated that Ovid co-opted the Roman discourse of labor and incorporated it subversively to present the works of love as activities of otium negotiosum (busy leisure), not otium otiosum (unproductive idleness). He did this on several levels, borrowing key words, motifs, figures, and even a didactic genre from Roman labor discourse. Comparing courtship to the labor of soldiers...\n4. Homo Artifex: Monastic Labor Ideologies, Urban Labor, and Love’s Labor in Alan of Lille’s De planctu naturae\nDownload PDF (134.8 KB)\nAlan of Lille’s De planctu naturae, written around the years 1160 to 1170, is a significantly different work from either Ovid’s Ars amatoria or Andreas Capellanus’s De amore. Far from a handbook of love, the De planctu is a menippean satire, a moral work, written in alternating sections of prose and verse, in the tradition of Boethius’s...\n5. Repos Travaillant: The Discourse of Love’s Laborin the Roman de la rose\nDownload PDF (229.9 KB)\nThe Roman de la rose is a comprehensive treatment of all aspects of love and includes a healthy dose of the discourse of love as passion. The discourse of love’s labor, however, is at its very heart, for this long poem of nearly 22,000 lines presents the first and, to a much greater extent, the second labors of Ovid’s Ars amatoria (finding and winning...\n6. The Vice of Acedia and the Gentil Occupacioun in Gower’s Confessio Amantis\nDownload PDF (192.8 KB)\nLike all of the works studied so far, John Gower’s Confessio Amantis treats the subject of love.1 Unlike most of the others so far, however, the Confessio is not an art of love. It is a story collection framed by the confession of a frustrated lover. Far from an ars amatoria, in fact, the Confessio is a remedia amoris.2 By the end of the work, the narrator...\n7. Love’s Bysynesse in Chaucer’s Amatory Fiction\nDownload PDF (220.9 KB)\nChaucer’s love poetry, like that of Ovid, takes its particular coloring from the discourse of love’s labor, and his labor discourse double-voices not only that of Ovid, Alan of Lille, and the authors of the Roman de la rose but also that of his own contemporary society. Of course, the discourse of love’s labor is placed into dialog with the...\nDownload PDF (72.2 KB)\nDownload PDF (103.7 KB)\nDownload PDF (96.4 KB)\nIndex to Authors Cited\nDownload PDF (48.2 KB)\nPage Count: 312\nPublication Year: 2012","When reading old texts it can be hard to wade through the different meanings, the odd spellings, and the foreign grammar that preceded our modern tongue. English is a language that grew from German and in the 14th century this was quite apparent in how people spoke. Scholars of medieval studies are well-versed in such things, but most of us have no idea how to read or speak in medieval English. Well, now there is an app that let’s you hear Chaucer’s The Canterbury Tales read aloud in the language of the day.\nThe book manuscript, written by Geoffrey Chaucer towards the end of his life, is largely thought to be unfinished. Over the centuries dozens of interpretations of his final work have been published. However, being a collection of short stories, the individual tales themselves stand on their own. It has been said that Chaucer intended to have each character tell 4 stories.\nWhat we do know is that the stories were written in Middle English, a language in use from roughly 1150 to 1500. This style of English was influenced not only by the German language, but also Norman French and by Old Norse spoken by the Vikings who conquered parts of the British Isles.\nThe app for listening to The Canterbury Tales, called General Prologue, named after the opening section of the original manuscript, was developed at the University of Saskatchewan. The project was led by one their English teachers, Peter Robinson. In a press release Robinson said, “We want the public, not just academics, to see the manuscript as Chaucer would have likely thought of it–as a performance that mixed drama and humor.”\nThis makes a lot of sense when you consider that the 24 stories in the book are being told as part of a contest to see who has the best tale of all the pilgrims traveling from London to Canterbury. Had this scenario played out in real life, the storytellers would have been boisterous, articulate, funny, and maybe even a bit shocking at times.\nWith the Middle English accent and the heartfelt pronunciation on the app, it reminds one a bit of the Swedish Chef from The Muppets, something that we can’t help but think would have pleased a famous contributor to the project, Terry Jones. A founding member of Monty Python who served as both actor and writer, Jones was also a scholar in medieval texts- although most people have no idea that he was so multi-talented! How fitting that something he worked on should be released so soon after his death. The General Prologue app was released on February 1st, 2020, on what would have been Jones’ 78th birthday and only 11 days after he passed away.\nThis translation is the first of 3 that have been planned. In addition to hearing the General Prologue, later apps from the same project will allow listeners to hear The Miller’s Tale, among other sections of the book.\nThe words were taken from the “Hengwrt Chaucer”, a manuscript believed to have been created before Chaucer’s death by a scribe in his employ, Adam Pinkhurst. The tome is held at the The National Library of Wales, Aberystwyth, Wales, UK.\nThe illustrations in this book are so complex and elegant, although you can imagine that after more than 600 years, the paper has changed color and the hues of the illustrations have lost a bit of their saturation. Using the app you can see the “General Prologue” in all it’s medieval glory, traced line-by-line as the app reads aloud for you.\nYou can download the General Prologue app on your Android or iPhone or peruse the desktop version to hear The Canterbury Tales spoken in in true Middle English and view the elegant (if tattered) original medieval manuscript."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"cross-lingual-multisentence"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:582a98d2-46a4-4ed7-aac6-8fe975210af3>","<urn:uuid:751bbbbf-028d-45a2-83c8-0772c0ae2141>"],"error":null}
{"question":"How do bearing measurement approaches differ between ACL aftermarket bearings and standardized tolerance systems?","answer":"ACL bearings require measurements in .0001 of an inch using precise and expensive tools, while standardized systems like ISO and AFBMA use structured tolerance grades. The ISO system measures in microns with grades from P0 to P4, and AFBMA uses grades from Abec1 to Abec7 measured in ten-thousandths of an inch. ACL represents a 'one size fits all' approach, unlike the standardized systems which offer multiple tolerance grades for different precision needs.","context":["Engine: How to determine Bearing size (Main & Rod)\nFirst, Why would you want to do this?\nOEM honda bearings are not all the same size, even in the exact same applications.\nHonda uses slightly different sized bearings, coded by a number and a color.\nSecond, why not just buy ACL bearings and call it a day? ACL bearings are good quality, and proven in high HP motors, but they do not come in the different tolerances that the OEM ones are available in.... more of a one size fits all. I went with OEM just for a piece of mind, and will be double checking the clearances with plastigauge of course.\nFirst I want to show this chart I have found on Honda-Tech.com (a great resource for information on all Hondas)Click for full size:\neach bearing is coded with a number, and a dab of paint. the paint is on the edge of the bearing, the number is stamped on the crank and rod.\nHere is how to read the crank bearings: (click to see ful size image)\n(I put the red type over the stampings on the crank, they didnt show up well in the pics)\nThis pic shows where you will get the numbers from for the crank bearings. Also it is important that you NOT mix up the order of your bearings when you take the bottom end of your motor apart! Write it down and label them with a marker- if you get them mixed up, you made a ton more work for yourself.\nso from the pic, you can see that bearings from left to right read 3, 2, 3, 3, 2. the \"6\" that is stamped in the cast area does not matter, only the smaller stamed #'s in the flat parts of the crank matter.\nNow after you write those down in order, look at the other half of the bearings for the color.\n(click to see ful size image)\nthe dab of paint can be hard to see (not visible in pic), it is on the thin edge in the center of the half circle. Also be sure you have your bearings in the right order here, not 180° backwards,\n(brown looked more like orange, green was easy to see)\nSo I need one \"Brown 3\" bearing two \"Brown 2\" bearings and two \"Green 3\" bearings. Go to the parts counter at the dealer and that is what you tell them.\nLooking back at the chart, you can see that these are close in size. if one of them was a \"Pink 1\" I might double check my readings since that is so far from the rest of the bearings.\nRod time: (click to see ful size image)\non the left is the OEM D16z6 rod. Right is Eagle aftermarket rods.\nAll the OEM roda have a number of 1-4 stamped on them. The number is on one of the edges where the cap meets the rod. All the stock rods had the \"2\" stamped on them. That is your number for the rod bearing. For finding the color, use the same method as before and look for the dab of paint on the edges of the bearings. Mine were all green.\nSo if I was going to use these rods again, I would go to the dealer and order a set of four \"Green 2\" rod bearings.\nSince I am using aftermarket rods instead, I need to get the correct size for those.\nI did some searching on here and found that the Eagle rods are the equivelant of #3 oem rods for D-series. (they also said B-series are equivelant to #2)\nSo I would buy four \"Green 3\" rod bearings.\nAnd that's how to figure out what size OEM bearings to use when re-building your D-series\nAnd double check ALL clearances with plastigauge!\nlet me know if I forgot anything so I can keep this how-to accurate\nUPDATE: So I went to the dealership to order the bearings.... All the Green main bearings were on severe back order, the Browns were discontinued!\nLooks like ACL bearings for this D16z6!\nI should clarify, The reason I wanted OEM was because I don't have the tools to measure the crank to determine which ACL's to order. After Honda told me there was no way to get the OEM's, I dropped the stripped bottom end off at an engine shop because I didn't have the time or tools to do it myself. ACL's are proven bearings. If I was doing it myself with plastigage, I would have liked to had OEM so I could have a piece of mind that my new bearings were as close to the correct size as possible.\nThe difference between ACL and OEM is OEM measures them in a single number + a color (easy for people like me who couldn't measure to .0001 of an inch). ACL's must be measured in .0001 of an inch with very precise (and expensive) tools.","How to measure the tolerance of ball bearings\nDo you understand what bearing tolerances really mean? We've written this simple guide to explain the intricacies of the topic. To learn more about what \"mean bore deviation\" and \"single bore deviation\" refer to, keep reading.\nTolerances control the dimensional accuracy of a ball bearing\nFor exact tolerance limits, please view our TOLERANCE TABLES\nISO bearing tolerances start at P0 and then move upwards to precision grades P6, P5 and then P4. ISO tolerances are measured in thousandths of a millimetre (or microns). AFBMA (ABEC) bearing tolerances are often measured in ten-thousandths of an inch with grades Abec1, Abec3, Abec5 and Abec7.\nDifferent Abec tolerance grades exist for instrument bearings (Abec1P, Abec3P, Abec5P, Abec7P) and thin-section bearings (Abec5T, Abec7T).\nIf the shaft and housing match the bearing tolerances and the bearing has tighter tolerances, it will enhance shaft/housing fit, reduce noise/vibration and lower starting/running torque.\nYou can learn more about ball bearing noise and vibration here.\nThis determines how much the actual measurement is allowed to deviate from the nominal dimension, including bearing bore tolerance. The nominal dimension is what the manufacturer lists; for example, 6200 has a nominal bore of 10mm and 688 has a nominal bore of 8mm.\nDeviation limits are crucial. Without international tolerance standards for bearings (ISO and AFBMA), manufacturers could vary. This might result in receiving a 688 bearing (8mm bore) with a 7mm bore that won’t fit. Deviation tolerances allow for smaller but not larger bores or outer diameter (OD).\nIf you’re not sure how to measure bore diameter, head to this page.\nMean bore/OD deviation\nSingle plane refers to bore diameter deviation, crucial for shaft-housing fit. Bearings aren’t perfectly round and when measured in microns, variations become clear.\nIf you take single measurements across different parts of a bearing's inner ring, you will get different readings so, what do you take as the bore size? This is where mean deviation helps. It involves measuring multiple points in one radial plane to find the average diameter of the ring’s bore or OD.\nThis drawing represents an inner bearing ring. The arrows represent various measurements taken across the bore in different directions to help discover the mean size.\nHowever, a set of measurements in one radial plane is not enough. We need to check measurements in multiple radial planes to ensure the bore is within tolerance along its length.\nLet’s say that a mean bore deviation tolerance for a P0 bearing is +0/-8 microns. This means that the mean bore can be between 7.992mm and 8.000mm. The same principle applies to the outer ring.\nWidth deviation, like bore and OD, needs to stay within tolerances. While width is less critical, tolerances are wider. A width deviation of +0/-120 for a 4mm wide bearing means it should be between 4mm and 3.880mm at any point.\nVariation tolerances ensure roundness. In this drawing of a badly out-of-round 688 inner ring, the largest measurement is 9.000mm and the smallest 7.000mm.\nCalculating the mean bore size (9.000 + 7.000 ÷ 2) gives 8.000mm. It’s within mean bore deviation tolerance, but the bearing is still unusable. This shows that deviation and variation must complement each other.\nSingle bore/OD variation\nIn simple terms, bore/OD diameter variation occurs in a single radial plane In the diagram above, bore measurements range from 8.000mm and 7.996mm. The difference between the largest and smallest is 0.004mm, making the bore diameter variation in this single radial plane 0.004mm or 4 microns.\nMean bore/OD diameter variation\nThanks to mean bore/OD deviation and single bore/OD variation, we’re confident that our bearings are close enough to the correct size and are round enough. But what if there’s excessive taper on the bore or OD, as shown in the amplified diagram above? This is why we also have mean bore and OD variation limits.\nTo obtain mean bore or OD variation, we record the mean bore or OD in different radial planes and then check the difference between the largest and smallest. Assume that on the left here, the top set of measurements gives a mean bore size of 7.999mm, the middle is 7.997mm and the bottom is 7.994mm. Take the smallest away from the largest (7.999 - 7.994) and the result is 0.005mm. Our mean bore variation is 5 microns.\nAgain, very straightforward. Let's assume, for a particular bearing, the permitted width variation is 15 microns. When checking the width of the inner or outer ring at various points, the largest measurement should not be more than 15 microns greater than the smallest measurement.\nRadial runout of the assembled bearing inner/outer ring is yet another important aspect of bearing tolerances. If the mean deviation of both the inner and outer rings is okay and their roundness is within the allowed range, surely that's all we need to worry about?\nLook at this diagram of a bearing inner ring. The bore deviation is okay and so is the bore variation, but look at how the ring width varies. Like everything else, ring width varies around the circumference and radial runout tolerances specify how much this can vary.\nInner ring runout\nThis is tested by measuring all points on a circle of the inner ring during one revolution, while the other ring stays still. Then subtract the smallest measurement from the largest. The radial runout figures given in the tolerance tables show the maximum variation allowed. The difference in ring thickness here is excessive to illustrate the point more clearly.\nOuter ring runout\nTo check the outer ring runout, measure all points in a circle on the outer ring during one revolution. Then subtract the smallest measurement from the largest.\nThis tolerance ensures the bearing inner ring surface is close enough to a right angle with the inner ring face. Tolerance figures for face runout/bore are only given for bearings of P5 and P4 precision grades.\nAll points near the inner ring’s face bore are measured during one revolution, while the outer ring is still. The bearing is then turned over and the other side of the bore is checked. Take the largest measurement away from the smallest to get the face runout/bore tolerance.\nThis tolerance ensures the bearing outer ring surface is close enough to a right angle with the outer ring face. Tolerance figures for face runout/OD are given for P5 and P4 precision grades.\nMeasure all points near the outer ring's face in one circle during one revolution while keeping the inner ring still. The bearing is then turned over and the other side of the outer ring is checked. Take the largest measurement away from the smallest to get the face runout/OD bore tolerance.\nFace runout and raceway runout assess the angle of the ring raceway surface compared to the ring face."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:1d5c7f30-ea71-4fe4-81ca-1302b1f32eb3>","<urn:uuid:35c790ae-6876-42fe-951e-f0f3075d2d39>"],"error":null}
{"question":"How do the responsibilities of a Cyber Defense team compare to those of a Security Operations Center (SOC) in terms of their core monitoring and response functions?","answer":"Both Cyber Defense teams and SOCs have overlapping but distinct responsibilities. Cyber Defense teams focus on protecting networks and systems from cyber attacks, with skills centered on identifying, preventing, responding to, and recovering from attacks. They implement defensive measures like firewalls and malware countermeasures. SOCs, on the other hand, provide continuous 24/7 monitoring and incident response through a centralized command structure. They take in telemetry from across an organization's IT infrastructure, manage alerts, perform threat response, conduct root cause investigations, and handle log management. While both are concerned with protection against cyber threats, SOCs typically have a more comprehensive operational role including compliance management, security refinement, and coordination across an organization's security framework.","context":["Cyber attacks can include attempts to access data or systems, destruction of data or systems, or interference with normal operations. Cyber defense strategies may include detection and response to attacks, prevention of attacks, and damage control after attacks.\nCyber attacks can have a wide range of consequences, from minor inconvenience to major business losses. Cybersecurity is a critical part of protecting computer networks and systems from cyber attacks. Cybersecurity includes preparing for and responding to cyber attacks, protecting data and systems, and enforcing security measures.\nA cyber defense system is a set of measures taken to protect computer networks and their data from attack.\nCyber Defense is the process of protecting information and systems from cyber attacks. Cyber Security is the practice of protecting information and systems from unauthorized access, use, disclosure, or destruction.\nCyber Security Defense is the practice of protecting information networks and systems from unauthorized access, use, disclosure, or destruction. Cyber Security Defense includes protecting data and systems from malicious actors, protecting against natural disasters and cyber attacks, and mitigating the impact of incidents.\nCyber Defense is the practice of protecting networks and systems from cyber attacks. Cyber Defense skills include the ability to identify and prevent cyber attacks, as well as the ability to respond to and recover from them.\nCybersecurity is one of the most important aspects of business today. The global economy is increasingly dependent on the internet and cyber crime has become a major threat to companies and the public. Cybersecurity is essential to protect our information and our systems from being compromised.\nCyber security is a critical part of business today. Many companies have been victims of cyber crime, and cyber crime costs businesses billions of dollars each year. Cybersecurity is not only important for companies that are targets of cyber attacks, but it is also important for companies that are simply vulnerable to cyber attacks. Cybersecurity helps protect data, protects against online fraud, and helps protect against other types of attacks.\nCybersecurity is an important part of any organization. It helps to protect against cyber attacks, which can damage or destroy data, disrupt operations, and expose confidential information. Cybersecurity also helps to protect employees and customers from identity theft and other online scams. By taking steps to protect your organization’s data and operations, you can ensure that your business remains safe and competitive.\nCybersecurity is an important issue for students, as cyber crime is on the rise. According to the National Cyber Security Alliance, cyber crime costs businesses and individuals more than $400 billion each year. Cybersecurity is not just about protecting yourself online; it’s also about protecting your data and networks. If you’re a student, make sure you’re up to date on your cyber security skills.\nCyber security professionals work to protect organizations’ information systems from cyber attacks. They use a variety of methods to detect and prevent cyber attacks, including monitoring networks, conducting research, and developing defensive strategies. Cyber security professionals must have a strong knowledge of computer security concepts and techniques, as well as the ability to analyze data and assess risks. They must also be able to communicate effectively with other members of an organization’s security team and with customers.\nCybersecurity is important in the digital world because it helps protect businesses and individuals from cyber attacks. Cyber attacks can include attacks on computer systems, data breaches, and unauthorized access to personal information. Cybersecurity can help protect businesses from financial losses caused by cyber attacks, and it can protect individuals from identity theft and other types of cyber crime.\nCyber Defense is the process of protecting computer networks and systems from unauthorized access, use, or disclosure. Cyber Defense may involve the use of defensive measures such as firewalls, intrusion detection/prevention systems, and countermeasures against viruses and other malware. Cyber Defense also may involve the detection, investigation, and prosecution of cyber crime.","Security Operation Center (SOC) is a centralized function within an organization employing people, processes, and technology to continuously monitor and improve an organization's security posture while preventing, detecting, analyzing, and responding to cybersecurity incidents.\nA SOC acts like the hub or central command post, taking in telemetry from across an organization's IT infrastructure, including its networks, devices, appliances, and information stores, wherever those assets reside. The proliferation of advanced threats places a premium on collecting context from diverse sources. Essentially, the SOC is the correlation point for every event logged within the organization that is being monitored. For each of these events, the SOC must decide how they will be managed and acted upon.\nThe function of a security operations team and, frequently, of a security operations center (SOC), is to monitor, detect, investigate, and respond to cyberthreats around the clock. Security operations teams are charged with monitoring and protecting many assets, such as intellectual property, personnel data, business systems, and brand integrity. As the implementation component of an organization's overall cybersecurity framework, security operations teams act as the central point of collaboration in coordinated efforts to monitor, assess, and defend against cyberattacks.\nSOCs have been typically built around a hub-and-spoke architecture, Wherein, spokes of this model can incorporate a variety of systems, such as vulnerability assessment solutions, governance, risk and compliance (GRC) systems, application and database scanners, intrusion prevention systems (IPS), user and entity behavior analytics (UEBA), endpoint detection and remediation (EDR), and threat intelligence platforms (TIP).\nThe SOC is usually led by a SOC manager, and may include incident responders, SOC Analysts (levels 1, 2 and 3), threat hunters and incident response manager(s). The SOC reports to the CISO, who in turn reports to either the CIO or directly to the CEO\n1. Take Stock of Available Resources\nThe SOC is responsible for two types of assets—the various devices, processes and applications they’re charged with safeguarding, and the defensive tools at their disposal to help ensure this protection.\n2. Preparation and Preventative Maintenance\nEven the most well-equipped and agile response processes are no match for preventing problems from occurring in the first place. To help keep attackers at bay, the SOC implements preventative measures, which can be divided into two main categories.\n3. Continuous Proactive Monitoring\nTools used by the SOC scan the network 24/7 to flag any abnormalities or suspicious activities. Monitoring the network around the clock allows the SOC to be notified immediately of emerging threats, giving them the best chance to prevent or mitigate harm. Monitoring tools can include a SIEM or an EDR, better even a SOAR or an XDR, the most advanced of which can use behavioral analysis to “teach” systems the difference between regular day-to-day operations and actual threat behavior, minimizing the amount of triage and analysis that must be done by humans.\n4. Alert Ranking and Management\nWhen monitoring tools issue alerts, it is the responsibility of the SOC to look closely at each one, discard any false positives, and determine how aggressive any actual threats are and what they could be targeting. This allows them to triage emerging threats appropriately, handling the most urgent issues first.\n5. Threat Response\nThese are the actions most people think of when they think of the SOC. As soon as an incident is confirmed, the SOC acts as first responder, performing actions like shutting down or isolating endpoints, terminating harmful processes (or preventing them from executing), deleting files, and more. The goal is to respond to the extent necessary while having as small an impact on business continuity as possible.\n6. Recovery and Remediation\nIn the aftermath of an incident, the SOC will work to restore systems and recover any lost or compromised data. This may include wiping and restarting endpoints, reconfiguring systems or, in the case of ransomware attacks, deploying viable backups in order to circumvent the ransomware. When successful, this step will return the network to the state it was in prior to the incident.\n7. Log Management\nThe SOC is responsible for collecting, maintaining, and regularly reviewing the log of all network activity and communications for the entire organization. This data helps define a baseline for “normal” network activity, can reveal the existence of threats, and can be used for remediation and forensics in the aftermath of an incident. Many SOCs use a SIEM to aggregate and correlate the data feeds from applications, firewalls, operating systems and endpoints, all of which produce their own internal logs.\n8. Root Cause Investigation\nIn the aftermath of an incident, the SOC is responsible for figuring out exactly what happened when, how and why. During this investigation, the SOC uses log data and other information to trace the problem to its source, which will help them prevent similar problems from occurring in the future.\n9. Security Refinement and Improvement\nCybercriminals are constantly refining their tools and tactics—and in order to stay ahead of them, the SOC needs to implement improvements on a continuous basis. During this step, the plans outlined in the Security Road Map come to life, but this refinement can also include hands-on practices such as red-teaming and purple-teaming.\n10. Compliance Management\nMany of the SOC’s processes are guided by established best practices, but some are governed by compliance requirements. The SOC is responsible for regularly auditing their systems to ensure compliance with such regulations, which may be issued by their organization, by their industry, or by governing bodies. Examples of these regulations include GDPR, HIPAA, and PCI DSS. Acting in accordance with these regulations not only helps safeguard the sensitive data that the company has been entrusted with—it can also shield the organization from reputational damage and legal challenges resulting from a breach.\nWhile dealing with incidents monopolizes much of the SOC's resources, the chief information security officer (CISO) is responsible for the larger picture of risk and compliance. To bridge operational and data silos across these functions, an effective strategy requires an adaptive security architecture that enables organizations to enact optimized security operations. This approach increases efficiency through integration, automation, and orchestration, and reduces the amount of labor hours required while improving your information security management posture.\nAn optimized security operations model requires the adoption of a security framework that makes it easy to integrate security solutions and threat intelligence into day-to-day processes. SOC tools like centralized and actionable dashboards help integrate threat data into security monitoring dashboards and reports to keep operations and management apprised of evolving events and activities. By linking threat management with other systems for managing risk and compliance, SOC teams can better manage overall risk posture. Such configurations support continuous visibility across systems and domains and can use actionable intelligence to drive better accuracy and consistency into security operations. Centralized functions reduce the burden of manual data sharing, auditing, and reporting throughout.\nOperationalizing threat management should start with a thoughtful assessment. In addition to defenses, an organization should evaluate processes and policies. Where is the organization strong? What are the gaps? What is the risk posture? What data is collected, and how much of that data is used?\nWhile every organization is different, certain core capabilities and security operations best practices represent due care today. A reasonable threat management process starts with a plan, and includes discovery (including baseline calculation to promote anomaly detection, normalization, and correlation), triage (based on risk and asset value), analysis (including contextualization), and scoping (including iterative investigation). Threat management processes feed prioritized and characterized cases into incident response programs. A well-defined response plan is absolutely key to containing a threat or minimizing the damage from a data breach.\nFigure 1. Threat management plans integrate and structure many processes across security and IT operations.\nEffective visibility and threat management will draw on many data sources, but it can be hard to sort out the useful and timely information. The most valuable data has proven to be event data produced by countermeasures and IT assets, indicators of compromise (IoCs) produced internally (via malware analysis) and externally (via threat intelligence feeds), and system data available from sensors (e.g., host, network, database, etc.).\nData sources like these are not just an input to threat management. They add context and make the information valuable and actionable for more precise, accurate, and speedy assessment throughout the iterative and interactive threat management effort. Access to, and effective use of, the right data to support plans and procedures is a measure of organizational maturity. A \"mature\" scenario would include a workflow that hands off the right information or permits direct action within operational consoles and across products. This flow integrates IT operations and security teams and tools into incident response when there is a critical event.\nAll these assessments will help prioritize where an increase in investment or reduction of friction is needed to make threat management implementation match goals. Consultants and penetration tests can help benchmark strategy and organizational maturity and health check security response against attacks to obtain a current measure of an organization’s ability to detect and contain malicious events. By comparing against peer enterprises, this vetted review can help justify and explain the need to redirect or invest in cybersecurity operations resources."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:065ceb4f-e0fa-4fab-995a-43dd55e1c9a7>","<urn:uuid:7631c2bf-c7d2-402b-ba66-e5742b0b7e52>"],"error":null}
{"question":"How did Robin Anderson impact East African art through Gallery Watatu, and what challenges would modern African institutions face in digitally archiving the gallery's history?","answer":"Robin Anderson co-founded Gallery Watatu, which became the leading East African contemporary art gallery, serving as a platform for serious contemporary art with regular exhibitions by leading artists. She was also known for her elegant batiks on silk, which pioneered what became a thriving batik industry in South Africa. As for digital archiving challenges, African institutions would face several obstacles including: lack of financial resources for digital preservation equipment and software, absence of preservation policies at national levels, concerns about maintaining authenticity and reliability of electronic records, and issues with privacy and access control. Additionally, there would be technical challenges related to proliferation of document formats and the need for constant updates to maintain access to archived materials.","context":["National Museums of Kenya and The Murumbi Trust present an exhibition of artworks by nine acclaimed women of East Africa, titled “Pioneer Women of the Arts.” The opening ceremony will take place on Sunday, September 9 at 2:00pm, at the Nairobi Gallery in Nairobi, Kenya.\nThe exhibition will highlight works from legendary East African artists Margaret Trowell, Joy Adamson, Magdalene Odundo, Rosemary Karuga, Geraldine Robarts, Robin Anderson, Yony Wai-Te, Nani Croze, and Theresa Musoke, and will be open through December 8, 2018. AMB. Amina C Mohamed, EGH, Cabinet Secretary for Education will lead the ceremony.\nThe exhibition’s entertainment will be a performance by PAPILLON, a young Kenyan musician and protégé of the world-famous Kenyan musician Ayub Ogada.\nPAPILLON creates his own instruments based on African instruments thousands of years old and writes his own music in an effort to preserve authentic Kenyan music not influenced by Western rap and hip hop. PAPILLON follows in the footsteps of Kenya’s African Heritage Festival, founded by Alan Donovan, which travels the world with its cast of models, musicians, acrobats, stilt walkers, hair dressers, chefs and others.\nEach artist showcased in “Pioneer Women of the Arts” was selected based on the various paths that have paved their existence in the art realm, and their earned acclaim through their unique struggle. These female artists have generated a significant impact on the art and culture of East Africa.\nTrowell, with her six books and art school at the prestigious Makerere University in Uganda, which was the best in the region – and perhaps all of Africa, is undoubtedly an influential pioneer artist and teacher to whom all artists in East Africa owe a debt. Her main goal in creating art, Trowell said, was to “make it plain that art is of the people and natural to the people.”\nAdamson, conceivably best known for her children’s books and later TV series “Born Free,” has also had a tremendous impact on the preservation of African culture. Throughout her travels, Adamson realized that she must paint the people of Kenya in their many tribal dresses before they were abandoned for Western wear and imports. She spent six and a half years living in all parts of Kenya during this pursuit after the Kenyan Government commissioned her to make a comprehensive record of all the traditional dress and ornaments of the people of Kenya.\nOdundo holds the highest position in international arts of any East African, as Chancellor of the University of Creative Arts in the UK. She has received an OBE by the Queen of England for her service to the arts, and has achieved international acclaim for her ceramic and glass works, which have been collected in museums globally. Odundo is known for being one of the world’s greatest contemporary potters.\nKaruga was the first woman to attend the prestigious Makerere University School of Fine Arts. She has exhibited her works with the leading artists of the continent and has been a mentor to world-renowned ceramicist Magdalene Odundo. Karuga pioneered a unique form of collage using local materials, and was eventually given a Lifetime Achievement Award for her contributions to the art world.\nRobarts has lifelong experience as a painter and University Lecturer in Fine Arts, including Makerere University and Kenyatta University. She is always pushing the boundaries of what paint, color, and new materials can achieve, and loves exploring the world to bring her inimitable style to her creations. Robarts is also a prominent worker with grassroots women’s groups in Kenya and has worked to bring art, health, and economic opportunities to communities who had previously struggled to survive.\nAnderson has made a lasting impact on the art world of East Africa, perhaps most famously as the co-founder of the leading East African contemporary art gallery, Gallery Watatu, which served as a platform for serious contemporary art in East Africa with recurrent exhibitions by leading artists. Inspired by the people and wildlife of Kenya, Anderson was a forerunner for what became a burgeoning industry of batiks in South Africa with her elegant batiks on silk.\nWai-Te is most famous for co-founding the first major contemporary art gallery in East Africa, Gallery Watatu, which has become the leading contemporary art gallery in the region. Her wildebeest and wildlife paintings populate hotels and public buildings throughout East Africa and have gained her an international following and reputation synonymous with the safari style of the region. Through her Wildebeests Workshops, Wai-Te has worked and trained many East African women’s groups and artists.\nCroze, as an artist, educator, and environmentalist, has brought another dimension to the art of East Africa. Founder of the Kitengela Glass Research and Training Trust, a center for recycling used glass into art, Croze has conducted numerous glass training workshops for young women from Kibera slums creating glass beads, pottery, and mosaics. Her monumental stained-glass and recycled glass works appear in numerous public spaces, including the courtyard and entry of the National Museums of Kenya.\nMusoke was one of the first women to obtain a degree from Makerere University at a time when very few African women were attending University. Her distinctive works romanticizing wildlife in a moody mixture of abstract batik and oil paintings have won Musoke great acclaim. She also taught art at Makerere and other leading art institutions in East Africa.","Digital preservation can be defined as the process and activities which stabilize and protect digital records and publications in forms which are retrievable, readable and usable over time. Digital preservation could also be defined as a set of processes and activities that ensure continued access to information and all kinds of records, scientific and cultural heritage existing in digital formats. This includes the preservation of materials resulting from digital reformatting but particularly information that is born-digital and has no analog counterpart. Digital preservation is an ongoing process of managing data for continued access and use.\nThe adoption of Information Communication Technologies (ICTs) has revolutionized the conduct of business and has greatly enhanced information accessibility. In particular, organizations are not only able to store large amounts of information but can also have quick access to it. This has improved service delivery and has ensured that policy makers react rapidly to social and economic developments. Further, the general public can also access information in remote areas. ICT has enabled archivists, records managers and librarians to carry out their mandate: that of information capture, preservation and dissemination. While use of ICT has occasioned these many benefits it has also brought challenges that have to be addressed. Principally, this new development has led to the generation of information in digital form which has to be managed. In spite of the benefits accruable, the technology has presented tremendous challenges which information professionals should be concerned with.\nThe purpose of preservation is to ensure protection of information of enduring value for access by present and future generations (Conway, 1990: 206). Libraries and archives have served as the central institutional focus for preservation, and both types of institutions include preservation as one of their core functions. In recent decades, many major libraries and archives have established formal preservation programs for traditional materials which include regular allocation of resources for preservation, preventive measures to arrest deterioration of materials, remedial measures to restore the usability of selected materials, and the incorporation of preservation needs and requirements into overall program planning.\nCHALLENGES OF DIGITAL PRESERVATION\nAn African perspective on preservation ought not to be different from other perspectives. However, digital preservation is often discussed in terms of technology, infrastructure and practices. Africa is largely composed of developing nations and thus has peculiar problems.\nIn African institutions these factors are attributed to:\nMost African countries have no policies on handling information be they in print; let alone in electronic format. In some African countries, years after independence they are still struggling with enacting a libraries act and as a result most institutions operate within a no policy framework. An enabling policy framework would allow institutions to implement various preservation strategies that are in line with their own parent institutions but operate within the overall country policy framework. These policy frameworks are essential especially if they can feed into broader continental policies such as the NEPAD initiative (The New Partnership for Africa’s Development which is a VISION and STRATEGIC Framework for Africa’s renewal). The NEPAD initiative itself is very silent on the preservation of Africa’s knowledge resources although it places prominence on the improvement of information and communication infrastructure (ICT). The improvement of ICT infrastructure will do well if there are policy frameworks at the country level that support the preservation and permanent storage of African knowledge resources wherever they might be found and in whatever format they might in.\nAfrica’s infrastructure is still lacking in handling large preservation of knowledge resources, especially resources that are in electronic form. Access to ICT facilities is a daily struggle for most institutions that are just barely managing to maintain access to print resources to be able to meet the daily requirement for academic learning in higher educational institutions.\nPreservation of knowledge resources is a continuous process not just a one off issue. To implement an effective and efficient preservation policy, there is need for commitment at both the institutional and national levels that preservation of the knowledge resources will be an incremental process that will be carried on from one generation to another. This effort entails that financial resources be committed to such a venture over long periods of time. This trend in funding has affected all areas of library operations including money that could be allocated for preservation of scholarly information materials. Financial commitments would also be needed to purchase and preserve the digital knowledge resources to permanently make them accessible to users, now and in the future.\nFinancial resources available for libraries and archives continue to decrease and will likely do so for the near future. The argument for preserving digital information has not effectively made it into public policy. There is little enthusiasm for spending resources on preservation at the best of times and without a concerted effort to bring the issues into the public eye, the preservation of digital information will remain a cloistered issue. The importance of libraries has been diminished in the popular press as the pressures from industry encourage consumers to see libraries as anachronistic while the Internet and electronic products such as Microsoft Encarta are promoted as inevitable replacements. Until this situation changes, libraries and archives will continue to be asked to do more with less both in terms of providing traditional library services, as well as new digital library services: preservation will have to encompass both kinds of collections.\nTechnical knowledge on the digital elements of electronic documents is largely lacking among staff that are in preservation departments. The presence of preservation departments in most of the libraries and information centers is really in name only as most of them concentrate on book and journal binding. This is coupled with the lack of preservation training. This lack of knowledge extends to deficient know-how on the equipment and software that is required for the preservation of digital information resources.\nDigital Technology Challenges\nDigital technology poses several challenges in the preservation of digital information resources. These are among others; technology comes in different formats, the cost of maintaining international standards of digital formats is expensive as it is often based on paying for upgrades to match the technology both the hardware and software. These come with subscriptions costs; so in essence a library/information center/archival center would have to subscribe to hardware; software and then to the electronic journal. This is unlike the paper format which has relatively changed very little since it was discovered as papyrus in Egypt 3000 BC. The electronic document is fairly new and has changed forms since then. If it is not the document changing from MS Word, PDF, html XML etc; it is the software requirement to be able to open and read the document. For example, if the document is in PDF you will need a PDF reader; JPEG would require a JPEG; just as a TIFF formatted document would require a Tiff reader. This means that institutions are always forced to change the facilities so they can meet various requirements such as software and hardware. Digital preservation presumes that there should be constant and continuous learning on the part of preservation staff both in software knowledge as well as hardware. This is because digital preservation methods are always changing depending on the nature of the hardware and software applied.\nDigitization of information requires obtaining copyright permission from various publishers to be able to duplicate anything in large quantities. However, most licensing agreements for journals or books produced by major publishers prohibit duplication of electronic documents or local storage of the document. What is allowed when one has a subscription is usually the online access to the particular journal for instance, without the subscribing institution having permanent access to content of the journal. Once subscription ends, access to the electronic content of journal is not possible. It is unlike in the print subscription model where once one has subscribed to the journal, the institution will have permanent access to the journal because the journal will be physically present the libraries own space.\nRecording media for digital materials are vulnerable to deterioration and catastrophic loss, and even under ideal conditions they are short lived relative to traditional format materials. Although librarians/archivists have been battling acid-based papers, thermo-fax, nitrate film, and other fragile media for decades, the threat posed by magnetic and optical media is qualitatively different. They are the first reusable media and they can deteriorate rapidly, making the time frame for decisions and actions to prevent loss is a matter of years, not decades. While acid paper is prone to deterioration, becoming brittle and yellowing with age, the deterioration may not become apparent for some decades and progresses slowly. It remains possible to retrieve information without loss once deterioration is noticed. Digital data recording media may deteriorate more rapidly and once the deterioration starts, in most cases there may already be data loss. This characteristic of digital forms leaves a very short time frame for preservation decisions and actions.\nMore insidious and challenging than media deterioration is the problem of obsolescence in retrieval and playback technologies. Information technologies are essentially obsolete every 18 months. Innovation in the computer hardware, storage, and software industries continues at a rapid pace, usually yielding greater storage and processing capacities at lower cost. Devices, processes, and software for recording and storing information are being replaced with new products and methods on a regular three- to five-year cycle, driven primarily by market forces. This dynamic creates an unstable and unpredictable environment for the continuance of hardware and software over a long period of time and represents a greater challenge than the deterioration of the physical medium. Many technologies and devices disappear as the companies that provide them move on to new product lines, often without backwards compatibility and ability to handle older technologies, or the companies themselves disappear. Records created in digital form in the first instance and those converted retrospectively from paper or microfilm to digital form is equally vulnerable to technological obsolescence.\nAnother challenge is the absence of established standards, protocols, and proven methods for preserving digital information. With few exceptions, digital library research has focused on architectures and systems for information organization and retrieval, presentation and visualization, and administration of intellectual property rights (Levy and Marshall). The critical role of digital libraries and archives in ensuring the future accessibility of information with enduring value has taken a back seat to enhancing access to current and actively used materials. As a consequence, digital preservation remains largely experimental and replete with the risks associated with untested methods; and digital preservation requirements have not been factored into the architecture, resource allocation, or planning for digital libraries.\nProliferation of document and media formats\nThere is a proliferation of document and media formats, each one potentially carrying their own hardware and software dependencies. Copying these formats from one storage device to another is simple. However, merely copying bits is not sufficient for preservation purposes: if the software for making sense of the bits (that is for retrieving, displaying, or printing) is not available, then the information will be, for all practical purposes, lost. Libraries will have to contend with this wide variety of digital formats. Many digital library collections will not have originated in digital form but come from materials that were digitized for particular purposes. Those digital resources which come to libraries from creators or other content providers will be wildly heterogeneous in their storage media, retrieval technologies and data formats. Libraries which seek out materials on the Internet will quickly discover the complexity of maintaining the integrity of links and dealing with dynamic documents that have multimedia contents, back-end script support, and embedded objects and programming.\nConcerns of authenticity and reliability\nThe authenticity and reliability of electronic records are often questioned because of possible changes to content or structure. Authenticity can be defined as the ability of the records to be reliable over time and act as evidence of organizational transactions. Reliability on the other hand, refers to a record’s authority and trustworthiness, and this is tied to the ability of a record to stand for a fact it is about. A number of authors among them, Hoffman and MacNeil, have argued that there are no guarantees of authenticity and reliability in the electronic environment, as records can be deleted or changed at any time. It is, therefore, important that electronic records are managed to ensure that they remain authentic and reliable as evidence. Perhaps in the paper environment, one can say that this is more straightforward, as records are physical objects, and this makes identification of their characteristics easier than it is in the virtual world. The records provide evidence of actions, but the computer systems may fail to capture the necessary information about the context of the creation and the use of records.\nAccess to electronic records and concerns of privacy\nThe use of computers has enabled organizations to create databases that now handle huge amounts of data on-line, which is made accessible anywhere and anytime. This has raised concerns that if the information is not properly managed, it may be made available too easily, resulting in lack of protection for the citizen’s individual rights. Further, the vast amount of information maintained about individuals by both government and private organizations threatens their privacy. Ojedokum has highlighted some of the privacy infringement as unauthorized acquisition of data, unauthorized penetration into computer networks. Computers allow fast and inexpensive communication of information and the collection and storage of large amounts of data. At the same time, these capabilities allow individuals and organizations to access information.\nPower cuts and backup strategies\nPower cuts and irregular electricity supplies are a major barrier. In most African countries there are limited power distribution networks which do not even reach rural areas where the majority of the population lives. African cities with higher population that have been experiencing power cuts include but are not limited to Accra, Dares Salaam, Lagos, Gaborone, Nairobi, Harare etc. These power cuts have disrupted business operations. Increased dependence on computers and their services for data processing also means increased reliance on the power supplies that keep the systems operating. Power failure means that organizations may lose valuable information and time. It is estimated that 50-70% of businesses that lose their data due to power cuts never recover it, and some go out of business. There is a need, therefore, for systems that will maintain quality power supply and protect electronic systems.\nInternet Bandwidth (Digital Divide)\nThe digital divide is still a major hindrance. In many parts of Africa there is little access to computers and the Internet. In those parts where there is Internet access, the resources, such as bandwidth, are severely limited or extremely expensive. Some digital preservation systems, such as LOCKSS, have questionable applicability. In the case of LOCKSS, a group of sites collaboratively maintain the integrity of collections. LOCKSS, however, does not cater for unstable and irregular bandwidth availability – its algorithms will not make the most efficient use of bandwidth and may exacerbate problems at sites with poor bandwidth. All online archives need to make use of bandwidth in a way that is both minimal and cognizant of the differences among sites.\nSkills and Education\nLibrarians, archivists and information professionals in African institutions are arguably not as technically skilled as their counterparts in other parts of the world. The availability of computer systems in some parts of the continent has the effect that curators of information do not receive sufficient training in electronic systems. Digital media is not the norm for many forms of communication and information storage. The level of education of the general population in many African countries also is a problem. The number of literate individuals, as well as the number of individuals with access to a computer and the Internet is lower than elsewhere in the world. This creates a challenge for digital preservation both in terms of collection building, especially for end-user submissions, and dissemination. Novel solutions are needed for both these problems to make digital archives effective.\nDigital collections facilitate access, but do not facilitate preservation. Being digital means being ephemeral. Digital places greater emphasis on the here-and-now rather than the long-term, just-in-time information rather than just-in-case. The research program for digital preservation has only recently been initiated to develop strategies, guidelines, and standards. The challenges to digital preservation are considerable and will require a concerted effort on the part of librarians and archivists to rise up to these challenges and assert in public forums the importance of protecting a fragile digital heritage.\n1. Douwe Drijfhout. 2006. Challenges in terms of Digital Preservation. LIASA Conference 2006.\nwww.nlsa.ac.za/...preservation.../Drijfhout.Challenges%20in%20terms %20of%20Digital%20P reservation.pdf\n2. Christine W. Kanyengo. 2006. Managing Digital Information Resources in Africa: Preserving the Integrity of Scholarship\n3. Hussein Suleman. An African Perspective on Digital Preservation\n4. Margret Hedstrom. Digital Preservation: A Time Bomb for Digital Libraries www.eric.ed.gov/ERICWebPortal/recordDetail?accno=EJ586788\n5. Margret Hedstrom. Digital Preservation: Problems and Prospects\n6. Terry Kuny. 1997. A Digital Dark Ages? Challenges in the Preservation of Electronic Information. 63rd IFLA Council and General Conference"],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:43a32c84-d4b0-414f-8c07-60828bdb127b>","<urn:uuid:ee408182-9bc7-4f44-994d-f50aa7dea141>"],"error":null}
{"question":"What are les différences in payload carrying capacity entre Skylark II et M300 RTK drones?","answer":"The Skylark II UAV has a maximum payload weight of 10kg, while the M300 RTK has a lower maximum payload capacity of 2.7kg. However, the M300 RTK offers the unique capability to mount up to 3 payloads simultaneously, including third-party payloads such as gas detectors, loudspeakers, and multispectral sensors.","context":["Skylark II UAV, Israel\nThe Skylark II is a close-range tactical unmanned air vehicle (UAV) system principally designed for Israeli, Canadian and Korean defence forces to carry out intelligence, surveillance, target acquisition and reconnaissance operations. The UAV was designed and manufactured by Elbit Systems. It was first unveiled in 2006 and is derived from the Skylark I.\nThe UAV is primarily used for worst weather reconnaissance, data collection and target marking for missions ranging more than 50km.\nSkylark II UAV orders and deliveries\nIn November 2009, two Skylark II UAVs were purchased by the Czech military for CZK50m (about $3m). The Korean Army selected the aircraft in a bidding process in December 2007.\nThe Canadian Army acquired five Skylarks as part of a programme to expand its UAV fleet. The vehicles will be deployed for operation in Israel, Afghanistan and Iraq. The Canadian Army is planning to purchase five more in the future as they cost less than conventional UAVs.\nISO Group - Spare Parts, Components and Logistics for Military Vehicles and Land Equipment\nISO Group, headquartered in West Melbourne, Florida, is a world leader...\nSkylark II design\nThe Skylark II is a man-packed, hand-launched mini UAV system designed to operate in the battlefield using deployable Humvee-class field vehicles. It can be operated by a two-person crew from a ground control station (GCS). The vehicle is incorporated with a built-in launcher, GPS, a night camera and a laser marker. It is designed to perform brigade-level operations.\nIt is self-reliant and can capture high-resolution images and videos of battlefields under adverse weather conditions.\nThe Skylark II is equipped with an electro-optical/infrared multi-sensor, a cross-coupled display, a thermal imager, a laser illuminator and an optical laser designator, which is used for targeting battlefields. The thermal imager is used to capture high-resolution images during the night by penetrating through clouds, rain, smoke, fog and smog.\nThe Skylark II is powered by a single electrical motor manufactured by Bental Industries. The engine can produce a maximum of 4kW. It is controlled by a battery pack contained in the payload pod underneath the main boom.\nThe propulsion system uses dual-channel permanent magnet brushless motors. A controller switches off one channel when cruising as this requires less power.\nIf one of the systems fails, the control will automatically switch to another system, allowing the vehicle to continue the mission or cancel and return to the ground safely.\nThe Skylark II features a payload of 10kg and has a flight endurance of six hours. It can operate in medium and low altitudes. It has a service ceiling of 4,572m and a maximum altitude of 16,000ft. The maximum take-off and payload weights of the aircraft are 43kg and 10kg, respectively.\nThe processing, retrieving and storing of real-time data is undertaken by the ground control station. Two crew operators can control the GCS system, which comprises a ground data terminal, a remote video terminal and a flight line tester/loader. State-of-art commercial off-the-shelf technology is used for converting the sensor data.","- 15 km Max Transmission1\n- 55-min Max Flight Time2\n- 6 Directional Sensing & Positioning\n- Primary Flight Display\n- IP45 Rating\n- -20°C to 50°C Operating Temperature\n- Hot-swappable Battery\n- UAV Health Management System\nThe all-new OcuSync Enterprise enables transmission up to 15 km away and supports triple-channel3 1080p video. Real-time auto-switching between 2.4 GHz and 5.8 GHz4 enables more reliable flight near high-interference environments, while AES-256 encryption offers secure data transmission.\nThe refined airframe and propulsion system design gives you a more efficient and stable flight, even in harsh conditions.\nMax Flight Time\nMax Descend Speed5\nConfigure your M300 RTK to fit your mission needs. Mount up to 3 payloads simultaneously, with a maximum payload capacity of 2.7 kg.\nRecord mission actions such as aircraft movement, gimbal orientation, photo shooting, and zoom level to create sample mission files for future automated inspections.\nAutomate routine inspections and capture consistent results every time. Onboard AI recognizes the subject of interest and identifies it in subsequent automated missions to ensure consistent framing.\nCreate up to 65,535 waypoints and set multiple actions for one or more payloads, including 3rd party ones, at each waypoint. Flightpath planning is also optimized to maximize flexibility and efficiency for your missions.\nMark an object in camera or map view with a quick tap, and advanced sensor fusion algorithms will immediately calculate its coordinates, which are projected to all camera views as an AR icon. The location of the subject is automatically shared with another remote controller, or to online platforms such as DJI FlightHub9.\nIdentify and follow moving subjects like people, vehicles, and boats with the Smart Track function, where auto-zoom is applied for steady tracking and viewing. The subject’s dynamic location is continuously acquired and shared to another remote controller or to DJI FlightHub9.\nThe M300 RTK adopts a new Primary Flight Display (PFD) that integrates flight, navigation, and obstacle information to empower the pilot with exceptional situational awareness.\nEither operator can now obtain control of the aircraft or payload with a single tap. This creates new possibilities for mission strategies as well as higher flexibility during operations.\nTo enhance in-flight safety and aircraft stability, dual-vision and ToF sensors appear on all six sides of the aircraft, offering a maximum detection range of up to 40 m, with options to customize the aircraft’s sensing behavior via the DJI Pilot App. Even in complex operating environments, this 6 Directional Sensing and Positioning system helps keep the aircraft and the mission safe.\nThe new integrated Health Management System displays the current status of all systems, notification logs, and a preliminary troubleshooting guide. Also in the system are the aircraft’s flight logs, duration, and mileage throughout its entire lifecycle, and tips on aircraft care and maintenance.\nThe M300 RTK’s built-in advanced redundancy systems help keep your critical missions going even in unexpected scenarios.\n-20°C to 50°C\nAirSense ADS-B Receiver\nThe Zenmuse H20N integrates starlight sensors into its zoom and wide-angle cameras\nEstimate your M300 RTK’s flight time based on the payload configuration.\nIntegrate a variety of 3rd party payloads like gas detectors, loudspeakers, multispectral sensors, and more. Payload SDK supports DJI SkyPort, DJI SkyPort V2, and DJI X-Port. These greatly reduce the payload development lifecycle and maximize the potential of your payloads in more diverse scenarios.\nWith a large network of 3rd party mobile applications, you can unlock the capabilities of your drone platform to meet specialized mission needs. Utilizing Mobile SDK, the M300 RTK supports highly customizable mobile app development.\nWith Pilot 2's built-in MQTT based protocols in DJI Cloud API, you can directly connect the Matrice 300 RTK to Third-Party cloud platforms without having to develop an App. Access the drone's hardware, video live-stream, and photo data.\nQuickly assess a situation and plan accordingly while improving officer and bystander safety.\nConduct inspections of pipelines, well sites, and more while keeping workers away from risky areas.\nDigitize surveying workflows and improve efficiency in data collection and analysis.\nMatrice 300 RTK is available for order through official DJI Dealers. Contact us below for the team to reach out.\n1.Unobstructed, free of interference, when FCC compliant. Maximum flight range specification is a proxy for radio link strength and resilience. Always fly your drone within visual line of sight unless otherwise permitted.\n2. Actual flight time may vary because of the environment and payload configurations.\n3. Each RC supports two streams. Triple-stream channeling is only supported with dual RC.\n4. Due to local policies, some countries do not support 5.8 GHz transmission.\n5. Achieved in Forward Flight using S Mode.\n6. The service ceiling of 7000 m is achievable with high altitude propellers.\n7. This feature is only supported when the aircraft is paired with the Zenmuse H20 Series payloads.\n8. This feature is only supported when the aircraft is paired with the Zenmuse H20 Series payloads.\n9. Support for location sharing via DJI FlightHub is coming soon.\n10. D-RTK 2 Mobile Station for Matrice 200 Series V2 and Phantom 4 RTK can be upgraded to support Matrice 300 RTK.\n11. The CSM Radar will be available soon."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"code-switched"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:28c0de4f-a900-4fb7-8b0e-e048f062a9b5>","<urn:uuid:469779b3-9781-4cd5-be93-41108b4c32f7>"],"error":null}
{"question":"What are the key challenges in managing protected areas across marine and terrestrial environments?","answer":"Protected areas in both marine and terrestrial environments face significant management challenges. Marine protected areas, despite being vital for ocean conservation, often remain as 'paper parks' due to weak capacity and inadequate funding for effective management. Similarly, terrestrial protected area management requires sufficient and predictable financial resources, which the GEF addresses through various mechanisms like conservation trust funds, payment for ecosystem services schemes, and tourism fees. Both types of protected areas need improved management effectiveness, including better integration with tourism, support for participatory management approaches, and sustainable financing mechanisms. Additionally, they must be designed for resilience linked to climate change and improved buffer zone management.","context":["Life on earth began in the seas which harbors a wider range of biological diversity than land. Despite the fact that a majority of life forms exist in the oceans and seas, and that they are, the biggest provider of human sustenance in many countries and the most common means of goods transport, they are increasingly becoming degraded due to wide neglect and their status as common property.\nIUCN defines marine protected areas as: \"Any area of intertidal or sub-tidal terrain, together with its overlying water and associated flora, fauna, historical and cultural features, which has been reserved by law or other effective means to protect part or all of the enclosed environment,\" MPAs are ocean areas that have been demarcated with limited human activity, or in some cases total prohibition, to conserve and protect the natural marine resources, ecosystems and genetic diversity. In addition to ecological objectives marine protected areas also serve to ensure human welfare, education, recreation and cultural preservation. They are generally governed with more stringent regulations than surrounding areas.\nDespite two thirds of the earth surface is covered by oceans, marine protected areas only makes up a fraction of terrestrial protected areas. Coastal areas remain poorly represented in the national protected area networks of the countries participating in MFF, and many vitally important or threatened coastal ecosystems do not have protected status. Greater representation is required to address these gaps in coverage, and to ensure that critical ecosystems are conserved.\nAs part of the MFF preparatory activities a gap analysis to review existing protected area coverage, identify regionally or nationally under-represented ecosystems, and recommend areas in need of additional protection was conducted. The MFF countries are at different stages in developing their protected area systems, and needs vary accordingly. Even where good intended protected areas have been established in coastal areas, some remain “paper parks”, as there is weak capacity and inadequate funding to manage them effectively. There is also a strong need to support measures to improve management effectiveness, including designing protected areas for resilience linked to climate change, improved integration with tourism, support to participatory management approaches, improving buffer zone management, and identifying sustainable financing mechanisms. MFF has approved its first large project under this PoW,” Evaluating and improving the management effectiveness of Thailand's Marine and Coastal Protected Areas.\nFor details about Actions, Outputs and Results, click [ + ]\n|Programmes of Work||Actions/Outputs||Contribution to results|\n|13. Building national systems of marine and coastal protected areas that contribute to a regional network||\nReturning mangroves to Tanjung Panjang, Indonesia © IUCN, 2018\nMangrove forests worldwide have been vanishing at astonishing rates. Tanjung Panjang, Indonesia, which has lost over 60% of its mangroves in the last 3 decades, exemplifies this trend. The creation of aquaculture in nature reserves has in part led to this decrease in mangrove cover. With the help of IUCN's Restoration Opportunities Assessment Methodology (ROAM), local experts and several NGOs are working with current land users and the local government to restore forest landscapes and strive for a more sustainable future.\nKochi, India 28 Oct 2013\nRepresentatives from more than 12 countries attended a Regional Fisheries Symposium from October 28 to 30 in Kochi, India with the goal of exploring ecosystem-based approaches to protecting fisheries and marine biodivers...","Creating parks and protected areas is one of the most effective conservation strategies to protect biodiversity. Protected areas provide habitat for many species, but they also provide essential goods and ecosystem services for human well-being. For example, many protected areas act as natural reservoirs for agriculturally important biodiversity, including wild crop relatives, pollinators and pest control. In addition, one-third (33 of 105) of the world’s largest cities including Mumbai, New York, Sofia, Bogotá, Dar es Salaam, Melbourne, Quito, Tokyo and Sydney receive a significant proportion of their drinking water supplies directly from forest protected areas. Read more+\nIn 1992, protected areas only covered 4 percent of the globe. As of 2015, based on data from the World Database on Protected Areas and the Protected Planet report, 15.4 percent of the world’s terrestrial and inland water areas, covering 20.6 million km2, 10.9 percent of coastal waters (0-12 nautical miles), and 8.4 percent of marine areas within national jurisdiction (exclusive economic zone; 0-200 nautical miles) are included under a total of about 209,000 designated protected areas.\nWhat We Do\nThe GEF helps create sustainable protected area systems by providing support to countries to:\n1) effectively establish and protect ecologically viable and climate-resilient representative samples of a country’s terrestrial and marine ecosystems and provide adequate coverage of threatened species at a sufficient scale to ensure their long-term persistence\n2) ensure that sufficient and predictable financial resources are available to support protected area management costs\n3) build individual and institutional capacity to manage protected areas such that they achieve their conservation objectives.\nThe GEF promotes the participation and capacity building of Indigenous Peoples and local communities, especially women, in the design, implementation and management of protected area projects through established frameworks such as Indigenous and community conserved areas.1 The GEF also promotes protected area co-management between government and Indigenous Peoples and local communities where such management models are appropriate.\n1Indigenous and Community Conserved Areas are natural sites, resources and species’ habitats conserved in voluntary and self-directed ways by Indigenous Peoples and local communities.\nGEF support to the establishment and management of protected area systems and associated buffer zones and biological corridors has arguably been the GEF’s greatest achievement during the last 20 years. Supporting the management of protected areas is not only a sound investment in biodiversity conservation and sustainable use, but also provides significant additional economic and environmental benefits beyond the existence value of biodiversity. Read more+\nSince its inception, the GEF has invested in improving the management of 3,300 protected areas covering an area of about 860 million ha, an area larger than the size of Brazil. In addition, the GEF has supported 60 countries to implement system-wide protected area finance strategies through a combination of conservation trust funds (40 worldwide totaling US$300 million), payment for ecosystem services schemes, revolving funds, tourism fees, ecosystem service valuation and other financial mechanisms to provide steady, reliable funding for protected area management and biodiversity conservation.\nThe GEF Amazon Region Protected Areas (ARPA) project in Brazil received the inaugural Development Impact Honors award from the U.S. Department of the Treasury in 2012 for helping Brazil achieve a four-year decline in deforestation rates. The two phases of the project created 37.5 million ha of new protected, established sustainable development centers and consolidated 32 million ha of existing areas. ARPA's two phases ensure the protection of nearly 70 million ha of rainforest, which will save more than 1.1 billion tons of CO2 emissions."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"content_constrained"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:42923eae-893c-4c12-ae95-31e23acdfd2c>","<urn:uuid:16a0e916-f943-431f-835f-a3424947c964>"],"error":null}
{"question":"Is water-based paint flammable, and how does the NASA atomic oxygen treatment affect paint pigments?","answer":"Water-based paints like acrylic, vinyl, and latex are non-flammable due to their water content. Some water-based acrylic paints even come in flame-retardant patterns. As for the NASA atomic oxygen treatment's effect on pigments, it does not react with oxides, so most paint pigments remain unaffected during the restoration process. The exposure can be carefully timed to stop before reaching organic pigments, ensuring the paint's integrity is maintained.","context":["Press Release 94-40\nLori J. Rachul\nNASA Technology Utilized for State-of-the-Art Restorations\nCleveland, OH -- NASA's Lewis Research Center, in concert with the Cleveland Museum of Art, has developed a varnish (lacquer) removal technique that will enable museums and art collectors to more safely restore paintings. This non-contact method is less harsh than traditional methods, which not only remove varnish but often remove paint pigments and cause paint to swell.\nLewis' Technology Utilization Office began investigating varnish removal techniques after discussing conservation needs with the Cleveland Museum of Art. Over the years the museum's conservation department encountered numerous varnishes that could not be safely removed using traditional solvents and was eager to investigate new alternatives. \"Most artists before the Impressionists intentionally varnished their paintings to protect them and make the colors appear richer,\" explained Cleveland Museum of Art's Chief Conservator Bruce Christman. \"As varnish ages it tends to yellow, causing the painting to lose its perception of depth. Restoration typically involves removing the varnish with organic solvents, which may cause swelling or leaching of the paint layers. We began working with Lewis to develop a new method of restoration to use on varnishes that cannot be removed with conventional methods.\"\nA Lewis team experimented with a thermal energy atomic oxygen plasma, originally developed to simulate the space environment in low Earth orbit, and discovered that it easily removed organic materials from paint and painted canvas samples.\n\"The oxygen atoms and ions in the thermal energy plasma chemically react with the surface and remove any organic material present,\" explained Sharon Rutledge, Electro-Physics Branch. \"Atomic oxygen will not react with oxides, so most paint pigments won't be affected by the reaction.\" For paintings containing organic pigments, the exposure can be carefully timed to stop the removal short of the pigment.\nAccording to Rutledge, tests of the atomic oxygen method on a painted canvas test sample and color samples from the museum show great promise. \"The lacquer was easily removed from all the samples and no noticeable change in appearance was observed after the fresh lacquer was applied,\" she said. \"Most importantly, there was no removal or disturbance of the paint pigment on the surface.\"\nWith the development of the atomic oxygen technique, Lewis is discussing collaborative activities with the conservation department at New York University's Institute of Fine Arts to restore a Monet painting damaged in a fire in the 1950s. Lewis is also pursuing partnerships with the Smithsonian Institute Analytical Laboratory and Buffalo State College's Conservation Department to restore other paintings damaged by smoke and fire. \"This is another good example of how technology developed for space applications can have great potential for applications in areas that are often seemingly unrelated to aerospace technology,\" commented Bruce Banks, chief of the Electro-Physics Branch. \"Such unique applications serve as a reminder to us that we should always keep our eyes open to diverse opportunities for utilizing technology, which may on the surface appear only relevant to space applications.\"\n# # #\n- end -\ntext-only version of this release\nNASA Glenn Research Center news releases are available automatically by sending an Internet electronic mail message to:\nLeave the subject and body blank. The system will reply with a confirmation via e-mail of each subscription. You must reply to that\nmessage to begin your subscription.\nTo unsubscribe, address an e-mail message to:\nLeave the subject and body blank.","If you want to burn a painting with a butane torch or create artwork in your oven, you may wonder, “Is paint flammable?” The result has fantastic effects. However, you must take measures while storing and using the type of paints to avoid any paint become flammable or combustible.\nSome paints are flammable, while others are combustible. Your paint’s response is determined by its ingredients. Water-based paints, such as acrylic and latex, are not often flammable. When exposed to heat, oil-based paints and spray paints may catch fire.\nTable of Contents\nFlammable vs Combustible: What’s the Difference?\nCombustible is occasionally used interchangeably with flammable. However, there are some differences between them. A substance’s flammability and combustibility are determined by its properties and flashpoints.\nMaterial vapours may ignite or catch fire on the liquid surface at temperatures as low as flashpoints. Depending on its flashpoint, many materials may be divided into many categories.\nA flammable substance is defined as having a flashpoint temperature higher than 100 degrees Fahrenheit by the OSHA regulations the United States Department of Labor (F) set out. OSHA continues by saying:\nFlammable liquids can catch fire and burn rapidly at standard operating temperatures. A low-temperature flame is all that is required to light them.\nCombustible liquids may catch fire when temperatures rise over the normal working range. They often include paints and have a high flammability rating.\nVapour may ignite even when liquids don’t often do so. A liquid’s flashpoint determines the minimum temperature required for combustion in air. Therefore, flammable liquids are a significant source of fire hazards.\nFor example, when flammable liquid vapours burn quickly, vast amounts of heat and deadly black smoke result. Because they may burst at high temperatures, combustibles emit a vapour that can ignite the atmosphere and do much more damage.\nAs volatile and combustible as they may be, paint thinner, and other paints are among the most dangerous. Understanding the dangers and the proper way to operate with these substances is essential for your safety.\nIs Paint Flammable?\nLiquid paint is seldom flammable in practice. Paints and solvents may generate toxic or combustible vapours. Paint fumes may catch fire as the temperature rises or a neighbouring fire breaks out, which is very hazardous.\nCombustible paints include aerosols and oil-based paints. Flammable ingredients are often found in oil-based paints, varnishes, and stains.\nAcrylic, vinyl, and latex are water-based paints that will not burn. Painters often utilize water-based, non-flammable paints. Certain water-based acrylic paints come in flame-retardant patterns. The combustible nature of certain water-based paints is another matter.\nThe paint depends on its primary material if it is flammable or combustible. In terms of flammable paint, the most prevalent varieties are:\nAerosol paints need propellants like butane and propane.\nCombustible chemicals such as toluene, methanol, and ketones may be present in oil-based paints.\nPaints containing alcohol are called “alcohol-based.” Alcohol is a highly flammable and easily combustible liquid.\nTo get the greatest results, study the paint’s ingredients. Include a description of the product’s potential dangers, as well as an indication of whether it’s flammable or not.\nIs Paint Combustible?\nSome paints have the potential to catch fire. Spray paints and aerosols have a high flammability index. They catch fire as soon as they’re pierced or exposed to extreme temperatures. The danger of fire and explosion is increased with products that employ spray-on oil-based paint.\nTypes Of Flammable Paints\nAn oil-, alcohol-or solvent-based paint is likely to be combustible if the paint includes these ingredients. Combustible paint is most often seen in the following forms:\nAs a result of the gas fuels, it contains, spray paint is very flammable. This includes propane and butane, which may be explosive when mixed with paint. An explosion caused by vapours that have escaped from old, broken, or poorly sealed cans can be very hazardous. In addition, spray paints may occasionally flashback because of the high pressure in the container.\nHeat is drawn back into a limited place by escaping gases that have ignited (almost like an invisible fuse string). During a grenade-like explosion, the liquid within the canister might become shrapnel.\nBecause of this, spray paint is not combustible once applied and hardened. It is safe to touch after it has dried since gas fuels are no longer present.\nIf you have spray paints around the house or at work, keep them away from heat and flames and apart from any flammable substances. You should also properly dispose of them if they are empty, broken, or obsolete.\nSolvent Based Paint\nToxic fumes from solvent-based paint make it dangerous. Consequently, solvent-based paint should be kept in a cool, dry location away from other flammable things, and it should never be used in an area where heat or flames might be present. It is no longer flammable after drying since the solvents’ combustible qualities have been removed.\nOil paints are very combustible if you look at how oil is utilized in heaters and fires. As the solvents evaporate and the paint dries, oil paint becomes non-flammable.\nOn the other hand, oil paintings will be reduced to ash in the case of a home fire due to the combustibility of dry oil paint. Keep your oil paints away from heat and fire, and always shut the tubes after use if you like an oil painting.\nOil-based Enamel Paint\nThe solvents in the oil and the oil-based makeup of certain enamel paints make them combustible. On the other hand, water-based enamel paints may be applied, stored, and produced with the same gloss level as oil-based paints.\nExterior House Paint\nPaint for the outside of a home that is oil-based and so combustible is often used.\nYou should be cautious when storing large quantities of these paints at home or on a construction site since they might catch fire because they are oil-based.\nWhenever feasible, utilize an outside paint business and ensure it is far from your property. It’s also a good idea to keep minors out of the paint shop and keep it out of direct sunlight and heat.\nOil-Based Epoxy Paint\nThe volatile solvents in oil-based epoxy paint make it a combustible product. In contrast, water-based epoxy paint is non-flammable and hence safer to use.\nTake additional measures if you have a lot of oil-based paint in your house. For example, it often paints garage floors and other hard-to-paint surfaces. Curing epoxy paint reduces its flammability, although the resin itself remains flammable.\nTypes Of Non-Flammable Paint\nFor the most part, water-based paints don’t catch fire. To put it another way, water is a non-flammable liquid utilized to put out fires.\nIt is possible to utilize paint as a flame retardant if the water content is high enough. Certain water-based paintings may dry out and become combustible, as with any rule.\nYour home will be protected from the spread of a blaze if you apply flame-resistant paint.\nExamples of non-flammable paints include:\nIt is safe to use and store water-based acrylic paint in your house since it does not burn. Acrylic paint, however, turns into plastic polymer when it cures.\nWater colours are one of the most harmless types of paint, thanks to their high water content. They are equally safe in dried form and are ideal for children to ingest. They are undoubtedly allowed on aeroplanes since they are non-flammable and non-hazardous. difference between them and acrylics can be found on this post for better under standing to get desired painting results on different surfaces.\nEmulsion paint is now made with water-based latex. In other words, most emulsion paints are non-flammable and safe to use and store in your house.\nEmulsion paints with flame retardant properties are widely available, making them an excellent choice for interior designers.\nWater-based latex paints are non-combustible. As it dries, latex paint has a rubbery feel but is not flammable.\nWater-based glass paint is non-flammable. Even after drying, it is non-flammable and safe to use. know more about how to paint glass windows easily here for stunning results from the paint job.\nThis type of paint is non-flammable since it is water-based. Fire retardant paint is widely considered to stop and delay flame spread.\nFabric paint is non-flammable since it is water-based. It’s safe to have about the house because it’s non-toxic and odourless.\nWater-based latex paints are the house’s most common type of wall paint. Because they don’t include easily combustible solvents, water-based paints aren’t flammable.\nHazards of Flammable or Combustible Paints\nPaint thinners should be handled and stored safely to prevent fires and explosions. After usage, flammable and combustible paints pose a risk. If you don’t properly dispose of the paint, you might put yourself and others at risk.\nThe health risks of working with flammable or combustible compounds go beyond the damage caused by fire or explosions. For example, if you inhale the vapours, you may experience the following:\nDisease and Illness\nBefore using or storing paint in your house, follow all the instructions on the container, including the safety warnings. Poisoning, chemical burns, and flames can all be avoided this way.\nSafety Tips for Working with Paint and Paint Thinner\nIf you use a potentially toxic or flammable paint thinner, you should always take measures. The materials must be used, stored, and disposed of correctly. Precautions must be taken while using paint and paint thinner.\nUse only in mixture with paint.\nIf paint thinner is mixed with anything other than paint, a potentially lethal reaction might result. Adding a thickener to an oil-based paint is a very typical practice. Don’t combine colours that aren’t exactly alike. If you’re not sure, ask the paint maker.\nMake Use of Safety Equipment\nWhen working with paint and thinners, use safety equipment. Wear safety equipment like gloves, goggles, and a respirator or mask. Protect your work area with a tarp or newspaper.\nWhen dealing with significant volumes of dangerous vapours, always work in a well-ventilated area and wear a respirator. Avoid crowded places. Thinners’ harmful vapours can cause headaches, vertigo, nausea, and breathing difficulties.\nIf you must work indoors, open a window or door. A carport is an excellent way to shelter your project from the weather. If required, include a fan.\nNever place it near flammable items.\nAccidents occur. Keep any combustible materials in a safe place. If your storage is insufficient, you risk starting a fire and causing damage to the structure. Thinners and spray paints are incredibly combustible and might result in a catastrophic explosion.\nDon’t use it to clean.\nEven though certain thinners are meant for cleaning walls, counters, and floors, never use paints or thinners to clean your workstation. The combustible items might readily catch fire and explode.\nAvoid Consuming Neighboring Foods\nSome paints and thinners create fumes and vapour’s that can harm humans. Avoid dining near your job since the food may absorb these toxins and cause long-term damage.\nCleanse Thoroughly After Use\nWear gloves and wash your hands often, especially before eating. Clean thoroughly with soap and water after using the paints and thinners. Organize your work environment. Inspect the containers for spills as well.\nKeep paints in a safe place. Place flammable or combustible materials safely away from flames, sparks, or a fire source. Make sure that it is out of reach of minors.\nIf you are unsure whether your paint is flammable, store it carefully just in case. Some companies also provide safety storage cabinets for storing flammable or combustible goods in bulk.\nUnderstanding how to store, use, and dispose of paints is critical for safety reasons. Flammable and combustible things provide a fire risk and can result in severe physical damage. Always practice safety by inspecting the components of your paints and keeping them safely away from children.\nCan paint cause a fire?\nMost paints lose their combustibility after drying because the combustible gases reduce and dissipate. However, many paints can become flammable after drying and thus catch fire if exposed to high heat.\nIs it safe to leave paint in a hot garage?\nPaint should never be kept in a garage. When the paint is subjected to high temperatures, its consistency changes and loses value. Long-term paint storage will turn it into hazardous waste that must be disposed of properly. know more about proper disposal of acrylics in this guide for reduced environmental impact and harm to human life.\nIs it possible for the paint can explode?\nAny paint compound poses a significant risk of fire or explosion. Paint should always be stored in a well-ventilated, dry area away from heat sources and direct sunlight.\nIs it permissible to leave paint in the car?\nWhen the paint is cold, it thickens; when heated, it thins. Before utilizing paint stored in a hot or cold (seasonally) automotive trunk, the temperature must be returned to 70°F-77°F (20-25°C). Because of its thick application, challenging paint has poor flow qualities and tends to droop.\nBeing associated with art and craft field since decades as a hobbyist and life long learner has given me an opportunity to learn many new things related to art, craft, paints and pottery which i am trying to share with your guys on this website. I have expertise of being professional painter and potter for the last 20+ years\nI have learned mind blowing cool tips and insights which makes me a person with ability to improvise and come up with creative ideas and solutions to make stunning and impeccable art pieces of all types which are adored by people across the globe on this website and other platform."],"question_categories":[{"categorization_name":"answer_type","category_name":"multi-aspect"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:554e6d89-09c8-4b15-896c-d531e13c58f8>","<urn:uuid:d6ae26e2-df88-439c-8d27-b495c5bc383a>"],"error":null}
{"question":"What changes do photographers make to skin in professional family photos?","answer":"In professional family photography, photographers make careful adjustments to skin, including balancing skin color and white balance, even within families where skin tones naturally differ. They also do skin smoothing and clean up work around areas like the nose.","context":["Professional photography is not just about capturing the image on camera: it’s also about post-production. Many people are aware of the concept of digital image manipulation especially when it comes to photographs of models and celebrities. Skin smoothing, wrinkle softening and body slimming are well-known examples in a wide range of possibilities. We sometimes get involved with this type of digital artistry; but family photography often requires a slightly different approach. This article looks a little behind the scenes at the work involved after an image has been captured on camera.\nAt Light Republic most of our work is in the studio, so the lighting is already very well controlled and modified. But even then, when the Raw images comes off the camera, we undertake a number of careful adjustments to optimise the image ready for printing. We routinely do do this regardless of whether we are producing a wall portrait or a digital high resolution image for our client. It’s all about attention to detail. (We are sometimes horrified when we hear stories about photographers offering their clients unprocessed image files straight from the camera!).\nThis recent Light Republic image of Daniel and Jessica is a well exposed professional image captured in a high key style in the studio. But there are, nonetheless, a number of adjustments and enhancements we have made for our clients:\n1. Skin colour/white balance. One would expect skin colour to differ markedly – even within families – but here the overall image white balance has given Jessica’s face a slight yellow cast. We have balanced this up to closer to her brother’s skin hue.\n2. Clean-up around the nose area!\n3. Floor shadow. Without a doubt we want to retain some element of shadow, because otherwise it would like the children are just suspended in a white void. So the key here is to lift the shadow so that it fits the the overall lighting (high-key) style without distracting the viewer from the subjects.\n4. Fluff/dust. We have removed this from Daniel’s shirt; and would generally check for this on clothes and photographic backgrounds.\n5. Distracting elements. We have removed the section of Jessica’s white shirt hanging from the bottom of her back. This takes a fair amount of digital skill – and this is an example of something we would undertake at the request of her parents.\n6. Differential sharpening. The plane of Daniel’s head was slightly different to his sister’s, so we undertook some local sharpening in this area of the image.\n7. General sharpening – for printing purposes.\n8. General contrast and saturation adjustment (taking care not to block any of the shadow areas).\nHere are some other examples of enhancements/adjustments that we might do at the studio. In this next photograph, the original image shows that the younger child had just been crying (not unusual with babies and toddlers!). But at the request of the parents we altered the image to remove the blotches and tears. This is an example of keeping the image ‘real’, but making certain elements less distracting.\nThe final image (again at the parents’ instigation) shows how a professional family photograph can be improved with some gentle head-swapping of the two girls. This particular change is quite subtle. But often in our Surrey studio we might be photographing a large generations shoot containing many children. The chances of getting the perfect single image (with all sitters smiling, looking their best and with direct camera contact) is remote. So the ability to convincingly head-swap between images is often very important. (And yes, we have heard all the jokes about swapping Dad for Tom Cruise!).\nAs a general rule we undertake about 80% of the adjustments before the image viewing (usually about a week later). When the clients have made their final selections we would return to the Raw file to full optimise the image (and retouch the image according to any personal requirements highlighted by the customer)."],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"moderate"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"non_native_learner"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:c9fab9a1-4d62-413c-98a4-eb8d0e676626>"],"error":null}
{"question":"What's the safest way to write identification notes on the back of photographs without damaging them?","answer":"Use a soft lead pencil (6B pencils are best) to make small notes on the reverse of the photograph. Do not use pens, stamps, adhesive labels, adhesive tape, or post-it notes, as these can leave impressions in the photo paper and cause chemical damage over time, impairing both the image and the integrity of the paper.","context":["As technology progresses many people are digitising their photographic collections. In undertaking this process it is important to take care of your original materials for access in the future. The following guidelines are some simple steps that you can undertake to protect your photographic collection for the long-term.\nBefore discussing storage it is important to keep in mind that most photographic material is made up of several layers. The most important being:\nSupport layer (base): In the case of prints this is either fibre based photographic paper or resin coated paper (B&W and colour prints post 1970). For negative and transparencies this will be either cellulose nitrate (1890-1950) cellulose acetate (c.1925 to present) or polyester (1960 onwards).\nImage layer (emulsion): Generally this is composed of a gelatin emulsion containing image forming materials. The most common image forming materials are silver in the case of black and white material and chromogenic dyes for colour material.\nEach of these layers has its own particular deterioration characteristics and, in some cases, have different storage requirements. When looking at storage you need to consider both environment and enclosures.\nPhotographic material is susceptible to deterioration by elevated levels of temperature, moisture, air borne pollutants (dust, peroxides, acids) ultraviolet light and manual handling.\nFor long term storage, archival institutions generally aim for temperatures between 8-12°C for black and white material and below 5°C for colour material. They also try and maintain a stable relative humidity moisture range between 30-40 per cent. The basic principle to keep in mind is to achieve a cool, dry and stable environment.\nIf storing at home a clean, cool, dry and dark environment is best, such as an internal cupboard in a bedroom. The following areas should be avoided as they are susceptible to fluctuations in temperature and relative humidity (moisture):\n- cupboards next to a water source such as kitchen, bathroom or laundry\n- cupboards next to an external wall\nIf you intend to construct shelving, use coated metal rather than wood, as wood may, over time, release harmful vapours and provide a home for insects.\nStore oversize items flat in drawers or boxes as unrolling material later can damage the material.\nAlways store deteriorating material separately from the rest of your collection.\nThere are a variety of suitable storage enclosures that can be used for long-term storage of your photographic collection. Use of a multi-layered system i.e. sleeve, box, cabinet will provide you with increased protection.\nPlastics: Plastic sleeves, envelopes and bags are suitable for storing prints and negatives in good condition. They should be made of a chemically inert material such as uncoated polyester, polypropylene or polyethylene. Avoid enclosures made from polyvinyl chloride (PVC) as this produces harmful products when it deteriorates.\nPaper: Archival quality acid free paper envelopes are suitable for storing prints and deteriorated negatives. It is generally recommended that the paper be unbuffered as some photographic processes react adversely to buffered papers. Seamless envelopes are preferred.\nAlbums: When choosing an album there are several things you need to be aware of. Avoid self-adhesive albums (also known as magnetic albums) and any products that contain PVC plastics. Use archival quality photo-corners to adhere photographs to pages in non-adhesive albums.\n- When choosing enclosures look for materials or products that have passed the 'PAT’ (Photographic Activity Test). This certifies that the material is safe for use with photographs.\n- Prints and negatives are best packaged individually but if packaging in groups is unavoidable, the photos should be interleaved with acid-free paper or board. Package loose material in small groups in files or folders, then package inside boxes.\nWhen framing a photograph insure that the backing and insert are of archival quality materials. Avoid wooden frames, which may be constructed using glues and varnishes. These may break down over time to form gases harmful to the photograph.\nTry to minimise the amount of light exposure while on display. Avoid exposure to direct sunlight and if possible use an ultraviolet light filtering glazing in the frame. Colour photographs are particularly susceptible to deterioration by over exposure to intense light.\nAvoid hanging locations near heating or cooling sources as these will produce adverse temperature and moisture conditions that can affect the photograph.\nAlways wash and dry your hands before handling photographs. As natural oils can accelerate deterioration, handle photographs by the edges ensuring that your fingerprints never touch the surface of the photo. Lint-free gloves may be used if they don’t make handling difficult.\nBe aware that the photo emulsion is easily scratched by stacking of photographs.\nNever fasten or bind photographs with metal pins, paper clips, staples or rubber bands.\nUse a blower or soft brush (available from photographic stores) to gently clean dusty photographic material.\nUse a soft lead pencil, which is inert and easy to use, for making a small note on the reverse of the photograph. (6B pencils are best.) It is important to identify photographs for future reference but do not use pens, stamps or adhesive labels as any of these may leave an impression in the photo paper and, over time, damage the photograph at a chemical level, impairing the image and integrity of the paper.\nDo not use adhesive tape, post-it notes or sticky labels when identifying photographs.\nDo not have food or drink near photographs. Aside from the possibility of spillage, consumables may attract mould and insects.\nAny advice regarding care, handling, storage and packaging of photographs is doubly applicable to digital prints (prints produced on a printer from digital images, rather than by photographic development.) Digital prints are even more susceptible to the detrimental effects of light, air and moisture, so especial care should be taken to keep them safe from exposure to these elements.\n*NFSA cannot supply acid free packaging but you may be able to obtain it from companies specialising in its manufacture, such as Zetta Florence, (Victoria) Conservation Resources (Vic) and Australian Paper (NSW)\nPlease note — The advice given here in regard to the care of photographs is based on best international experience and the best information available to the NFSA. However, given the different circumstances applying to the condition of any particular video we cannnot be responsible for the application of this advice in any particular circumstances. To be sure of the best care of your video you may wish to seek specialist advice.\nDownload: Caring for your photographic materials"],"question_categories":[{"categorization_name":"answer_type","category_name":"factoid"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"long"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:55d6efb5-d646-4751-8951-430b7136b72d>"],"error":null}
{"question":"What types of water management organizations exist in Viera versus Shelley?","answer":"In Viera, water management is overseen by multiple entities including Community Development Districts (CDDs), which are special purpose government units that handle community-wide infrastructure and services, and the Viera Stewardship District (VSD) which manages stormwater and aquatic control. In Shelley, water management is primarily handled by the Snake River Valley Irrigation District (SRVID), which maintains water rights tied to the land and operates two main canals - the Cedar Point Canal and Snake River Canal. SRVID has some of the oldest and most secure water rights in the upper Snake River Valley area, dating back to 1889, with rights to water storage in American Falls, Jackson, and Palisades Reservoirs.","context":["What is a homeowners’ association (HOA)?\nAn HOA is a not-for-profit corporation created by a real estate developer to manage and maintain a neighborhood or community. Control of the HOA eventually transfers from the developer to a volunteer Board of Directors made up of HOA members (homeowners). Membership in the HOA(s) is mandatory. With the purchase of your home, you have agreed to become a member and to comply with all recorded Declarations of Covenants, Conditions and Restrictions (CCRs) for your particular neighborhood. You may also live in an area with multiple HOA’s such as Viera which has Master Associations and Neighborhood/District Associations.\nWhat is a Community Development District (CDD)?\nA Community Development District is a local unit of special purpose government created to provide for the long-term, specific needs of its residents. Created pursuant to Chapter 190, Florida Statutes, a CDD’s main powers are to plan, finance, construct, operate, and maintain community-wide infrastructure and services specifically for the benefit of its residents. Within the entire Viera area, there are only two CDDs – Heritage Isle CDD and Viera East CDD.\nWhat is the Viera Stewardship District (VSD)?\nThe Viera Stewardship District (VSD) is a local unit of special purpose government created by Special Act of Florida Legislature (Ch. 2006-360; F.S. Chapter 189). The VSD covers an area measuring 13,442 acres and has responsibilities including, but not limited to, the maintenance and management of certain areas such as stormwater, aquatic weed control, Preferred Cover Type (PCT) tree maintenance, street lighting and environmental conservation (Viera Wilderness Park) within its boundaries.\nHow do I pay my assessments?\nEach HOA is responsible for the billing and collection of its assessments. Because you live in an area with multiple HOA’s, you will have multiple invoices/charges to pay. Check with the management company providing these services to your association for your specific billing cycles. Assessments may be collected monthly, quarterly, semi-annually or annually, depending upon which associations you belong to. The Central Viera Community Association, Inc. (CVCA), your master association, invoices annually for assessments and these assessments are due each January 1st.\nIf I notice a deed violation in my neighborhood, what do I do?\nCall the management company hired by your association to enforce the covenants and discuss the issue with a professional community association manager. Often times, there are behaviors or activities that we want the HOA to act upon; however, this does not necessarily mean that a violation of the covenants has occurred. Furthermore, your association may have adopted rules or policies that work in conjunction with the covenants that may refine or add more clarity to the enforcement process. Each association is different in its approach to enforcement so a conversation with your association manager should provide you with enough guidance on how to handle a specific issue.\nHow do I report a maintenance issue in an HOA park or common area?\nContact your association’s management company. For CVCA, this is Fairway Management of Brevard, Inc.\nAm I allowed to make changes or improve the exterior of my home including landscaping?\nAll exterior modifications to your home requires approval from your neighborhood/district HOA or CVCA. This includes, but is not limited to, painting your home or front door, adding a porch or lanai, adding a fence, building any kind of shade structure or pergola or building a pool. For landscaping, there are some instances where approval may not be required. Replacing dead plants or adding plants to an existing bed does not require approval. However, larger projects such as adding trees or significantly changing the footprint of your current landscaping beds do require approval. It is best to always check with your management company first – before you purchase project supplies or do any work outside your home. Otherwise, the expense to remove or change your modification can be a very costly mistake.\nWho do I contact if I want to make a modification request?\nAlways start with your management company – they will point you in the right direction. Depending upon which neighborhood you live in, the modifications approval may be with your neighborhood/district association or the approval may be with the master association (CVCA). There is a modification form for CVCA under How Do I? on the main menu.\nAre garage/yard sales allowed in Viera?\nGarage/yard sale rules are specific to each neighborhood/district. There are some that allow it (with approval from the association) and some that do not allow any time of garage/yard sale. There may also be only designated times during the year where you can participate in a neighborhood/district-wide yard sale. Contact your association manager for the rules in your specific neighborhood.\nHow do I find out my irrigation schedule?\nThe regulations governing irrigation vary throughout all of Viera. The schedule for your home depends on the source of irrigation water for your neighborhood. Irrigation restrictions apply to water withdrawn from the ground (wells) or from surface water. Where reclaimed water is available, individual irrigation wells are not permitted. Click here to visit the St. Johns River Water Management District webpage regarding irrigation restrictions and for more information on the rules and regulation governing irrigation. For many areas in Viera, the irrigation source is reclaimed water and Viera is served by the Brevard County Utilities Department. Please visit their website for more information regarding the use of reclaimed water for an irrigation water source.\nIs Viera a city/town?\nNo. Viera is a community in the unincorporated area of Brevard County.\nWhen will Viera get its own post office?\nOver the past few years, post offices all over the country have been closing due to the use of the internet and email instead of mailing items, which has decreased the demand for USPS services and locations. While there have been requests and discussions over the years on bringing a post office location to Viera, the decision ultimately will have to be determined by USPS based on current volumes and demand. For your personal mail handling, USPS Post Offices are organized by zip codes – not necessarily by cities or communities. Therefore, you are not required to use “Rockledge” (32955 zip code) or “Melbourne” (32940 zip code) on your mailed items – you can use “Viera”, even though you live in a zip code where the post office is located within the jurisdictional boundaries of those cities. Since the post office only looks at the postal code for delivery purposes, we can all proudly use “Viera” as our hometown.\nI was told that the Viera Roundabout was created to preserve the tree in the center. Is that true?\nAlthough this has been a long standing rumor in Viera, no, this is not true. The centerpiece tree in the roundabout was relocated to this location as an aesthetic enhancement for the circle.","SHELLEY — Homes in the city are using 1300 gallons of city water per day during the summer. A new requirement for developers could change that.\nShelley landowners are part of the Snake River Valley Irrigation District, meaning their properties have water rights tied to them. But because the city doesn’t have an ordinance requiring developers to use those water rights, many have opted out leaving small residential lot owners to use city water to irrigate their lawns. Now the SRVID is prohibiting developers of any sized lot from opting out, possibly incentivizing them to use canal water for irrigation instead of city water.\n“We now require new development remain in our district and continue to pay the tax for such water,” Snake River Valley Irrigation District manager Steve Neilson said.\nThe city of Shelley agrees with SRVID that developers within the city limits need to use the district’s water to irrigate their lawn.\n“It’s a good idea,” Shelley Mayor Stacey Pascoe said. “This will allow the city’s water to go a lot further than it does today.”\nSome lots in the city’s older section have access to ditches that run from SRVID’s canals. These properties are still within the district, and their owners pay a tax for the water right.\n“Individual homes within the city use an average of 175 gallons of water per day during the winter months. However, during the summer months, the average home uses 1300 gallons per day,” City of Shelley engineer Dave Noel of Forsgren Engineering, said. “If you were to remove this extra demand, your water rights would go a lot further.”\n“It’s my opinion that we need a secondary irrigation system in new developments to preserve these water rights in the long run,” Councilman Leaf Watson said.\nAside from the potential benefit to the city’s water supply by using more canal water, continuing to allow landowners to opt-out could have potentially negative consequences.\n“Not only are we losing our tax base, but we may lose the water that is associated with this land,” Neilson said.\nShelley City Councilman Leaf Watson said he believes that by requiring developers to remain in the irrigation district and hopefully using canal water for irrigation the water rights will be preserved.\nSRVID has water rights assigned to the land within their district. Unlike a canal company, these water rights are tied to the land. The property owner is obligated to pay a tax for such water.\nSRVID’s area runs between Gem Lake and Firth. It includes all land lying east of the river out to the Sand Creek area east of Shelley.\nThe district maintains two canals – the Cedar Point Canal and the Snake River Canal. The Snake River lateral splits and runs on both sides of the butte, east of Shelley. These canals bisect the area irrigating land all the way to Kimball Hill, which is south of Firth.\nSRVID has some of the most secure water rights in the upper Snake River Valley area because the district has some of the oldest water rights for use and storage water that is at the lowest point in the area’s reservoirs.\n“We have 1889 rights that are at the bottom of the American Falls, Jackson, and Palisades Reservoirs,” Neilson said.\nAt present, the city is looking to SRVID to force new development to retain their water right and construct a secondary system. The city currently has no ordinance requiring a secondary water system on new development."],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"search_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"knowledgeable_generalist"},{"categorization_name":"user_follow_up_likelihood","category_name":"moderate_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"practical_steps"}],"document_ids":["<urn:uuid:447a8db6-e6b6-499b-a81d-96ad444d3ce9>","<urn:uuid:0ca9816d-f34e-4074-ae51-8fcab240450d>"],"error":null}
{"question":"Why has the $100 billion climate finance goal not been met?","answer":"The $100 billion annual climate finance goal has not been met due to several challenges. According to the OECD report, developed countries failed to meet their commitment by 2020 and even by 2021. The shortfall is attributed to the stagnation of private financing over the past decade, challenges in scaling up private sector involvement (especially in adaptation initiatives), and issues with the definition of climate finance. The lack of a universally agreed-upon definition has led to problems like double-counting and questionable project classifications.","context":["OECD’s Insights for Global Action\nThe recent report by the Organization for Economic Cooperation and Development (OECD) brings forth critical insights into the climate finance commitments of economically developed countries, particularly in the context of their promise to mobilize $100 billion annually towards climate mitigation and adaptation needs of developing nations.\nGS – 3 (Environmental Pollution & Degradation)\nClimate Finance, COP 26, Kyoto Protocol, Paris Agreement, United Nations Framework Convention on Climate Change (UNFCCC), Green Climate Fund (GCF)\nEvaluate the significance of climate finance in the global climate change discourse and analyze the challenges in meeting the financial commitments as per the Paris Agreement, emphasizing the responsibilities of developed countries towards developing nations. (250 words)\nWhat is Climate Finance:\n- Climate finance pertains to the funding acquired from local, national, or transnational sources, encompassing public, private, and alternative financial channels to enhance actions addressing climate change by supporting both mitigation and adaptation endeavors.\n- The UNFCCC, Kyoto Protocol, and Paris Agreement advocate for financial support from those Developed Countries that has more financial resources to aid Developing Countries that are susceptible to the impacts of climate change.\n- This principle aligns with the concept of “Common but Differentiated Responsibility and Respective Capabilities” (CBDR).\n- COP26 : Fresh commitments were made to provide financial support for developing nations in achieving global goals for adapting to climate change effects.\nDimensions of the Article:\n- OECD Report Overview\n- Developing Countries’ Financial Needs\nOECD Report Overview:\n- The report underscores the failure of economically developed countries to meet their commitment of jointly mobilizing $100 billion annually for climate mitigation and adaptation in 2021, marking a year beyond the 2020 deadline.\n- Analyzing the consequences of the financial gap, the report sheds light on the challenges faced by developing nations in addressing climate mitigation (e.g., renewable energy adoption) and adaptation needs (e.g., resilient agriculture).\n- The shortfall in climate finance impacts trust among developing countries regarding the commitment of developed nations in addressing the climate crisis.\n- The report delves into the composition of climate finance, emphasizing the prevalence of loans in the financial support provided by developed nations. This analysis highlights potential debt stress in poorer countries, raising concerns about the conditions attached to such financial support.\n- The concept of additionality in climate finance is explored, focusing on the UNFCCC stipulation that developed countries must provide new and additional financial resources.\n- The lack of a universally agreed-upon definition of ‘climate finance’ is discussed, pointing out the intentional ambiguity maintained by developed countries. The consequences of this ambiguity, such as double-counting and questionable project classifications, are emphasized.\n- The report also highlights the stagnation of private financing for climate action over the past decade and addresses the challenges faced in scaling up private sector involvement, especially in climate adaptation initiatives.\n- The OECD report’s recommendations for course correction, including de-risking strategies and the role of multilateral development banks in mobilizing private finance, are also discussed.\nDeveloping Countries’ Financial Needs:\n- The article scrutinizes the adequacy of the $100 billion goal, questioning its origin and relevance in comparison to the actual climate investment needs of developing countries.\n- The report suggests estimations indicating a significant gap between the committed amount and the required funds for climate investments in the coming years.\n- While acknowledging the potential contribution of the private sector, the governments and multilateral development banks should also remain indispensable in enabling climate action.\n- The need for a more transparent, accountable, and universally agreed-upon definition of climate finance is important for effective global collaboration.\n- The OECD report serves as a catalyst for reevaluating current approaches, prompting nations to reassess their climate finance commitments and strategies for a more sustainable and inclusive future.\nMeanwhile , the COP 28 in Dubai shall offer a vital opportunity to reinvigorate global efforts and commitments, emphasizing the urgency of addressing the climate finance crisis to secure a sustainable and equitable climate future."],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"moderate"},{"categorization_name":"question_formulation","category_name":"natural_synthesis"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"english_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"conceptual_explanation"}],"document_ids":["<urn:uuid:b80c14db-f85f-43ac-afaf-595ca9c5f7ef>"],"error":null}
{"question":"Could the German Pak 38 50mm and the 88mm Flak 18 anti-tank guns penetrate the same thickness of armor at 1000 meters distance?","answer":"No. The Pak 38 50mm could penetrate 96mm of armor at 1000 meters, while the 88mm Flak could handle much thicker armor - at 1100 meters it could penetrate 108mm of steel armor while maintaining enough power to detonate its charge and destroy the tank and crew.","context":["Let’s Talk About The Other Things That Killed Shermans, Mines and AT guns, The True Hidden Menace.\nMines, What can we say about mines, no one likes them, but they still do a job that has to be done. We only going to cover anti-tank mines, since Anti-personnel mines do not harm tanks.\nThe Riegel mine 43/44.\nThis mine was steel cased, anti-tank mine that looked like a long rectangular box. It had 17.8 pounds of TNT explosive in it. That’s enough BOOM to really mess up a tank’s suspension. A really unlucky Sherman might have this go off right under the tank’s rear belly, where the armor was thinnest, and blow into the engine compartment and really knock the tank out. In most cases, this mine would do enough damage the tank would need battalion level repair if that tank wasn’t written off for a total rebuild.\nProduction on these things started in 43, and by the end of the war, they had produced over 3 million of them. Apparently, there is no safe way to disable this mine, and the recommended way of removing it is to just blow it up. There is no telling how many of these took out Shermans, but it was probably a large percentage of the Mine losses in 44/45.\nThe Topfmine A, B, and C.\nThese mines went into service in 1944. They were made from wood pulp and cardboard, with tar for waterproofing. They had a bigger charge but the metal cased mines probably worked better. These mines went into production for two reasons, they were harder for mine detectors to detect, and the case was cheap and easy to produce and used no steel. The 13-pound charge would do a lot of suspension damage.\nThe Tellermine 29:\nThis mine was developed in the early 30s and was mostly used in training but saw limited use in Normandy. 13-pound charge meant it would be effective, but its age made it primitive.\nThe Tellermine 35:\nThis mine was used for the entire war and could even be used underwater. Steel cased like the older model, this one went into production in 35. This mine had a slightly smaller 12-pound charge. Most of the time this mine would just blow a track off, and damage the suspension, but it could get lucky and do more damage.\nThe Tellermine 42:\nThis mine was an improvement on the 35 and used the same charge. It had improved anti-handling devices. This would be a very common mine through the end of the war. It went into production in 42 and was quickly superseded by the 43 models.\nThe Tellermine 43:\nImage courtesy of the LoneSentry\nA further improvement on the 42 models, cheaper to produce, with the same charge, this mine went into production, you guessed it, in 1943.\nH-S mine 4672:\nThis shaped charge mine went into production in late 44 and was used to the end of the war. It basically was a panzerfaust head used as a mine. The mine shot the head out of the ground hoping for a belly hit. This mine would be bad news for wet ammo rack Shermans. Only 59,000 were made, making it rare. This mine was very effective even with its small 3-pound charge. The Germans felt the heads were better-used don Panzerfausts, explaining the limited production.\nPanzer Stab 43:\nThis mine was very much like the 4672 mines but didn’t launch the projectile. This mine was even rarer and was discontinued, probably because it worked, by the Germans the same year it went into production. Around 25k got made before they killed it.\nThat’s a lot of mines and that’s just the mines the Germans made, I’m sure they used any stocks of captured mines they got their thieving paws on. So I’ll add Russian, British, and US AT mines here soon too. Mines always accounted for 10 to 30% of tank losses depending on the year, month, and theater you look at the losses in. There are a few pictures of Sherman tank catastrophically blown up, with the whole upper hull ripped away. They are labeled as mine damaged, and in a few cases, the labeling mentions two mines being put into the same hole, I think the ones labeled ‘mine damage’ probably lost the part about two mines in the hole to time. I suspect those photos of blown-up Shermans are cases of two or more mines in one hole or an even bigger explosive like a dud Arty shell, or aircraft bomb could be put in the hole too.\nTankers probably really hated mines, in many cases minefields would be covered by well-hidden AT gun positions or even tanks, and in this role, the Panther was a pretty good tank, since it didn’t need to move much. Hitting a mine in an ambush like that could be very deadly for the crew when they bailed out to look at the damage or retreat to the rear. The random leftover mine, or stumbling into a minefield not covered by AT guns would be a big inconvenience but rarely resulted in a fully destroyed tank or lost crew members.\nAT guns, cheap and easy to produce, these guns were a big threat to tanks but had little value to a mobile force.\nAT guns were just what they sound like, large, Anti-Tank guns, on towable mounts. Most were as small and low slung as possible. Unless it was a US 3 inch AT gun, then they are huge. Even guns normally not a huge threat to a Sherman like the PAK 38 50mm AT gun could punch through the Shermans side if it was hidden well enough for the Shermans to give them the shot. All the larger PAK guns had no trouble punching right through most Shermans. Guns set up in ambush would have pre-range cards, giving them an advantage in shooting and getting hits. They are much easier to hide than a tank and can even have bunkers built around them. Those are all reasons why these things made Sherman tanker’s lives harder.\nTowed AT guns have a lot of negatives. For one, they are towed, by trucks, or halftracks, they have to be limbered and unlimbered, or set up or packed up to go. This is very hard to do in a useful way if you’re attacking with a mechanized force. By the time the guns are set up, if done at safe distances, the battle has moved on. At guns only have a small lightly armored shield, the crews would have to rely on personal foxholes or larger trench works if they had time. The more time it had to get in place and camouflaged the position the better things would be for the gun and crew. But unless they had fortifications with overhead cover for the gun and crew, making it effectively a fixed gun, any kind of indirect fire weapon is going to make their lives hard. If the artillery fire wasn’t killing the crew, it would at least be keeping it from firing.\nAbout half of the US tank destroyer battalions used only towed anti-tank guns. The battalions were not very successful, even during German offensives like the Battle of the Bulge. Both tracked TD battalions and towed were quickly disbanded after WWII, and towed anti-tank guns would not be a big part of most western nation’s militaries after the war either. AT guns would prove very useful to the Germans from mid-war on after they were losing. They had a lot of these guns, and they accounted for a lot of tank kills. It was hard to determine in many cases what type of gun killed a tank, but tanks were much rarer than AT guns.\nThe Sherman 75mm tanks were actually better at dealing with AT guns than the later model tanks that had the 76mm gun since it had a smaller explosive charge. It was far from useless though. A tank’s best way of dealing with an AT gun was to shoot the hell out of it with all guns available once it was spotted, and sometimes if the crew was suppressed, they’d even get a dose of the tracks.\nPak 38 50mm AT Gun:\nThis little gun was the main German AT gun from 1941 until superseded by the Pak 40. It was still used until the end of the war. The Germans were so desperate they couldn’t afford to retire any weapons. Crewed by five men, it could be moved around pretty handily by the crew but required a light truck or some kind of tow vehicle to go any real distance. I won’t go into great detail about the gun but it needed to be very close to a Sherman to knock it out from the front, not so much from the sides. Nearly 10,000 produced.\nPak 40 75mm AT Gun:\nThis gun was larger; almost double the weight of the Pak 38. This gun could also take the Sherman out at the combat ranges they normally faced each other. The Germans made nearly 20,000 of these guns, so they are probably responsible for a lot of knocked out Shermans. In some cases, the same type of gun may have knocked the same Sherman out multiple times. This gun required a bigger truck or halftrack to haul, but overall, it was a great gun.\nPak 43 88mm AT Gun:\nThis ‘fearsome’ gun had the same PR people as the big cats, but at least, in this case, the gun performed well, though not to the mythical levels some would have you believe. No it can’t take out an M1 ‘Abrahams’, it could take out any allied tank it faced, but it was nearly as rare as the Tiger I&II. They only produced around 2000 of these guns, so they only outnumber the combined Tigers production number of 1839, by a small margin. Overkill for most of the combat it saw, it would have been more useful if the Allies had made the same mistake of wasting resources on heavy tanks, but since they didn’t, this gun was almost entirely a waste of time. The gun weighed almost 10,000 pounds, and it was an awkward, gun mount, even worse than the US 76 AT gun mount. It needed a very large tow vehicle and its size and weight limited where it could be employed.\nFlak 18/36/37 88mm dual-purpose AA/AT Guns.\nAnother ‘mythical’ German weapon, this one started life as a mediocre AA gun that was pressed into use as a direct fire weapon when needed. As a direct fire weapon, it was pretty good, these larger and much more powerful guns were better at penning armor than anything being mounted on a tank before or at the beginning of the war. Capable of destroying all the French and British tanks the Germans faced, this gun could even handle the T-34 and KV-1/2 tanks, and it was the only thing the Germans had in any real numbers that could. This led to it being mounted in the Tiger I. The Pak 43 was more powerful, but this gun was more numerous with over 20,000 being produced. If any allied troops were right when they thought an 88 was shooting at them it would be one of these.\nThere was a Flak 41 88mm, but it was a failed attempt to improve upon the 18/36/37 failings as an AA gun. The reason the basic 88 Flak gun failed as an AA gun was that it had optical range finding, and couldn’t lob a shell high enough to hit US heavy bombers, even the older models like the B-17. They also lacked radar ranging or laying, unlike the superior US M1/2/3 90mm AA gun system. Had these guns not found their nitch in the direct fire role they would have gone down in history as the mediocre AA guns they were.\nNext up, Panzerfausts, or AT-sticks as I now call them.\nSources: Armored Thunderbolt by Zaloga, Yeide’s The Tank Killers, The Infantries Armor, and Steel Victory, Sherman by Hunnicutt, Combat Lessons, The Rank and file, what they do and how they are doing it 1-7, and 9 WWII Armor, Ballistics and Gunnery by Bird and Livingston, TM4 Sherman tank at war by Green, Tanks are a Might Fine Thing by Stout, the Lone Sentry, TM9-1940 Land mines, TME9-369A German 88MM AA Gun, TME30-451 Handbook on German Armed Forces 1945, DOA Army Battle Casualties and Non-Battle Deaths in WWII, FKSM 17-3-2 Armor in Battle, Another River, another town by Irwin, Wargaming’s Operation Think Tank Videos .","The early development\nAfter the First World War it was by the Treaty of Versailles that\nit was conducted that Germany was no longer allowed to manufacture weapons in their own\ncountry. To surpass this rule, Krupp sent some technicians to Bofors in Sweden, between 1920 and 1930.\nWhen Hitler came to power in 1933, the technicians from Krupp returned to their homeland with\nthe design of a 8.8cm (or 88mm) anti-aircraft gun. The most distinctive feature on the weapon was the barrel.\nThis was constructed of several segments so damaged pieces would be easy to replace. Another plus\nwas that there was no special machinery necessary, and they could be produced in substantial numbers.\nDirectionequipment on an 8.8cm Flak 37\nAfter a few changes, the first production cannon was named the 8.8cm Flak 36. ‘Flak’\nstood for FlugzeugAbwehrKanone. More improvements led to the model 37. The best way to spot\nwhich Falk is Which of the two models is to look at the directionequipment, which are placed in boxes on\na Flak 37, and were round on the Flak 36.\nDirectionequipment on an 8.8cm Flak 37\nA well trained crew could shoot 15 shells of 9 kilo each to a height of\n12 km within one minute. Until the end of the war, the Flak 36/37 and the Flak 18\nvariant were the basis of the German anti-aircraft units.\nIn 1935, during the Spanish Civil War, the German Luftwaffe brought\nthe Kondorlegion to Spain to gain experience with the new equipment and to test\nnew tactics in the support of Franco. In Spain an 8.8cm Flak 18 with a smoother\nbarrel was used. Here was discovered that it was not only a great anti-aircraft gun,\nbut also a superb cannon for targets at ground level. It had a devastating effect on\ntargets and moral, the last on both sides. In Germany a special direction finder and\nanti-tank ammunition was developed for the 8.8cm. But this sidestep was only for extra\nsupport when needed, because the standard anti-tank cannon, the 50mm PaK 38 was manufactured in plenty of numbers.\nA 50mm PaK\n38, the standard anti-tank cannon\nEarly in the Second World War, the PaK (Panzerabwehrkanone) 38 with armoured piercing\nshells had a penetration ability of 96 mm at a 1000 meters, enough for every British\ntank at that time.\nBut the 8.8cm proved itself in the west during the blitzkrieg of Europe. During a desperate\nattempt to break out of their predicament, 74 British tanks tried to escape on May 21,\n1940 near Arras. The German 35mm was the only anti-tank gun around and was not sufficient\nenough. But with the assistance of 105mm artillery and 8.8cm’s the British were stopped.\nNot only vehicles were the targets for the 88’s, also stationary targets, like bunkers\nand casemats were shot at. From relative short distances some fortresses of the Maginot\nLine were fired upon with devastating effect.\nAn 8.8cm Flak\n18 fires at the Maginot Line\nDuring the war in the desert in 1941, the anti-tank guns were scattered\nover great distances. To overcome the empty spaces between these guns, the Lufwaffe had 24\n88’s Flak guns for the army to use. Because of their high profile, it had a two meter\nhigh protective plate, the gun was a prominent target. To protect the gun it was necessary\nto dig it in behind an earth wall.\nAn 8.8cm in\naction in North-Africa\nThe first shots in anger on ground targets in North-Africa came during the\ndefense of Halfaya. A British tank, with armour of 80 mm in 1941, was an easy target for over\n2200 meters, even under an angle 30°. At 2000 meter it could penetrate 90 mm. Over a distance of\n1100 meters, the 8.8cm could handle 108 mm of steel, detonate a small charge that brought death\nto any tank and it’s crew.\nA Flak 37 has a plane in sight\n(notice the rings on the barrel, one for every downed aircraft)\nBecause at one time there were enough 8.8cm’s 8.8cm anti-aircraft guns with the Luftwaffe,\nspare ones were lent for anti-tank duties. In the field the anti-aircraft equipment was taken of, and the\nguns adapted for ground targets.\nThe success of the anti-tank roll of the 88’s, the Africa Corps was equiped with extra guns. During the\nbattle of Alamein, the ‘Korps’ had 86 guns to use against the British troops.\nA Flak 37 is preserved at the\nOmaha Beach Museum, Vierville-sur-Mer\nWhen during the Russian campaign, the German troops came across the T-34 tank, a\nlot of the anti-tank guns proofed insufficient. Even the 50mm PAK 38 cannon was useless. Only ammo with\na core of wolfram could penetrate the Russian armour. Unfortunately for Germany, this material was hard\nto come by. So once more they went to Krupp to produce a 75mm gun, and a special 8.8cm for the anti-tank\nroll. This would lead to the PAK 43. An advantage with this cannon was it’s ability to fire when it was\non his wheels. For the 8.8cm Flak it was always necessary to placed it on it’s platform before it could\ngo into action. Also, the profile of the PAK 43 was brought down to a height of 1.50 meter.\nAn 8.8cm PAK\n43/41, Omaha Beach Museum\nThe taught behind the concept was to improve it’s action by extending the room for\na larger shell. But during production problems came to light that required to adapt the undercarriage.\nThe gun, the 8.8cm PAK 43/41 became struggle for the troops to handle in the Russian mud. But, nevertheless,\nthe PAK 43/41 was a excellent weapon with a penetration of 168 mm from a distance of 1100 meters under an angle\nof 30°. As far as 3300 meters, the gun had more effect the 8.8cm Flak op 1100 meters! Despite it’s clumsiness in\nthe field, it was a very deadly weapon from 160 to 3300 meters. It is known that at one time it killed six T-34\ntanks in Russia from as far as 3900 meters! The 23 kilo heavy shell had one nasty habit, after every shot a black\ncloud of smoke was produced that with calm weather obscured the view for the next shot, and gave the enemy the\nposition of the gun.\nDifferent sorts, only one size, 8.8cm\n(a Flak 18 outside the museum at Falaise)\nDuring the war, the 8.8cm PAK 43 cannon found it’s way in German tanks,\nlike the Jagdpanther. Was gun firstly developed as a weapon of defense in the anti-aircraft\nroll, in the tank it became a tactic fast moving attack weapon. But the Allies had the armour\non their tanks improved and also better anti-tank guns mounted like the British 17-pounder\nand the American 90mm. But the 8.8cm stayed to the end of the war a horror to the Allied crews\nin their tanks. During the battles around Caen the gun was terribly effective. Especially when ‘Operation Goodwood’ was launched on July 18, 1944.\nThe German defense destroyed at least 220 British tanks, mainly with the 88’s.\nA Flak trainingsunit\nThe 88’s stayed in the frontline of the Flak air defense. In the progress of\nthe war, the air defense became very accurate, thanks also to radar. In 1942, around 15.000 8.8cm\nguns were in the frontline of the German air defense. During 1944 3501 American planes fell victim\nto the anti aircraft fire. Another 600 were shot down by German fighters. In November, 1944, when\nMerseburg was attacked, 56 B-17’s were shot down or damaged by Flak alone.\nA posed picture with Luftwaffe Flak crew\nAfter the war a lot of the old 8.8cm’s found their way in the armies of the\nEast-European nations. Some of these guns stayed operational until the sixties before the were\nreplaced for Russian anti aircraft missiles.\nAfter the battle, another British plane is added\nThe picture below shows the decoration given to a successful German Flak unit.\nFor every downed plane there were points to be given. When a crew reached 16 points, the medal,\nwith a 8.8cm Flak gun in the middle, was officially handed over.\nThe decoration for a successful"],"question_categories":[{"categorization_name":"answer_type","category_name":"comparison"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"concise"},{"categorization_name":"question_formulation","category_name":"natural_inference"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"expert_specialist"},{"categorization_name":"user_follow_up_likelihood","category_name":"unlikely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:6dd85d2b-fb07-4216-9379-bde1b3930cd5>","<urn:uuid:aa910c53-d9c5-47fc-9b82-3063a998e58b>"],"error":null}
{"question":"How does neurotransmitter release occur at synapses? Please explain the step-by-step process.","answer":"Neurotransmitter release occurs through a precise mechanism at synapses: 1) An action potential reaches the presynaptic bouton, 2) This activates calcium channels (VDCCs) causing local calcium influx, 3) Proteins in the active zone detect the increased calcium, 4) This triggers synaptic vesicles to fuse with the membrane, 5) The fusion releases neurotransmitters into the synaptic cleft, 6) The neurotransmitters diffuse across the cleft and bind to receptors on the postsynaptic membrane, 7) This binding induces changes in the postsynaptic neuron. The process requires formation of SNARE complexes and is regulated by proteins like Munc18, Munc13, and RIM that help 'prime' the vesicles for release.","context":["A diagram of a typical central nervous system synapse. The proteins of the active zone are represented as dark brown pyramids on the upper neuron terminal\nThe active zone or synaptic active zone is a term first used by Couteaux and Pecot-Dechavassinein in 1970 to define the site of neurotransmitter release. Two neurons make contact through structures called synapses allowing them to communicate with each other. As shown in the diagram on the right, a synapse consists of the presynaptic bouton of one neuron which stores vesicles containing neurotransmitter (uppermost in the picture) and a second, postsynaptic neuron which bears receptors for the neurotransmitter (at the bottom). When an action potential reaches the presynaptic bouton, the contents of the vesicles are released into the synaptic cleft and the released neurotransmitter travels across the cleft to the postsynaptic neuron (the lower structure in the picture) and activates the receptors on the postsynaptic membrane.\nThe active zone is the region in the presynaptic bouton that mediates neurotransmitter release and is composed of the presynaptic membrane and a dense collection of proteins called the cytomatrix at the active zone (CAZ). The CAZ is seen under the electron microscope to be a dark (electron dense) area close to the membrane. Proteins within the CAZ tether synaptic vesicles to the presynaptic membrane and mediate synaptic vesicle fusion, thereby allowing neurotransmitter to be released reliably and rapidly when an action potential arrives.\nThe function of the active zone is to ensure that neurotransmitters can be reliably released in a specific location of a neuron and only released when the neuron fires an action potential. As an action potential propagates down an axon it reaches the axon terminal called the presynaptic bouton. In the presynaptic bouton, the action potential activates calcium channels (VDCCs) that cause a local influx of calcium. The increase in calcium is detected by proteins in the active zone and forces vesicles containing neurotransmitter to fuse with the membrane. This fusion of the vesicles with the membrane releases the neurotransmitters into the synaptic cleft (space between the presynaptic bouton and the postsynaptic membrane). The neurotransmitters then diffuse across the cleft and bind to ligand gated ion channels and G-protein coupled receptors on the postsynaptic membrane. The binding of neurotransmitters to the postsynaptic receptors then induces a change in the postsynaptic neuron. The process of releasing neurotransmitters and binding to the postsynaptic receptors to cause a change in the postsynaptic neuron is called neurotransmission.\nThe active zone is present in all chemical synapses examined so far and is present in all animal species. The active zones examined so far have at least two features in common, they all have protein dense material that project from the membrane and tethers synaptic vesicles close to the membrane and they have long filamentous projections originating at the membrane and terminating at vesicles slightly farther from the presynaptic membrane. The protein dense projections vary in size and shape depending on the type of synapse examined. One striking example of the dense projection is the ribbon synapse (see below) which contains a \"ribbon\" of protein dense material that is surrounded by a halo of synaptic vesicles and extends perpendicular to the presynaptic membrane and can be as long as 500 nm! The glutamate synapse contains smaller pyramid like structures that extend about 50 nm from the membrane. The neuromuscular synapse contains two rows of vesicles with a long proteinaceous band between them that is connected to regularly spaced horizontal ribs extending perpendicular to the band and parallel with the membrane. These ribs are then connected to the vesicles which are each positioned above a peg in the membrane (presumably a calcium channel). Previous research indicated that the active zone of glutamatergic neurons contained a highly regular array of pyramid shaped protein dense material and indicated that these pyramids were connected by filaments. This structure resembled a geometric lattice where vesicles were guided into holes of the lattic. This attractive model has come into question by recent experiments. Recent data shows that the glutamatergic active zone does contain the dense protein material projections but these projections were not in a regular array and contained long filaments projecting about 80 nm into the cytoplasm.\nThere are at least five major scaffold proteins that are enriched in the active zone; UNC13/Munc13, RIMs (Rab3-interacting molecule), Bassoon, Piccolo/aczonin, ELKS, and liprins-α. These scaffold proteins are thought to be the constituents of the dense pyramid like structures of the active zone and are thought to bring the synaptic vesicles into close proximity to the presynaptic membrane and the calcium channels. The protein ELKS binds to the cell adhesion protein, β-neurexin, and other proteins within the complex such as Piccolo and Bassoon. β-neurexin then binds to cell adhesion molecule, neuroligin located on the postsynaptic membrane. Neuroligin then interacts with proteins that bind to postsynaptic receptors. Protein interactions like that seen between Piccolo/ELKS/β-neurexin/neuroligin ensures that machinery that mediates vesicle fusion is in close proximity to calcium channels and that vesicle fusion is adjacent to postsynaptic receptors. This close proximity vesicle fusion and postsynaptic receptors ensures that there is little delay between the activation of the postsynaptic receptors and the release of neurotransmitters.\nNeurotransmitter release mechanism\nThe release of neurotransmitter is accomplished by the fusion of neurotransmitter vesicles to the presynaptic membrane. Although the details of this mechanism are still being studied there is a consensus on some details of the process. Synaptic vesicle fusion with the presynaptic membrane is known to require a local increase of calcium from as few as a single, closely associated calcium channels and the formation of highly stable SNARE complexes. One prevailing model of synaptic vesicle fusion is that SNARE complex formation is catalyzed by the proteins of the active zone such as Munc18, Munc13, and RIM. The formation of this complex is thought to \"prime\" the vesicle to be ready for vesicle fusion and release of neurotransmitter (see below: releasable pool). After the vesicle is primed then complexin binds to the SNARE complex this is called \"superprimed.\" The vesicles that are superprimed are within the readily releasable pool (see below) and are ready to be rapidly released. The arrival of an action potential opens voltage gated calcium channels near the SNARE/complexin complex. Calcium then binds to changes the conformation of synaptotagmin. This change in conformation of allows synaptotagmin to then dislodge complexin, bind to the SNARE complex, and bind to the target membrane. When synaptotagmin binds to both the SNARE complex and the membrane this induces a mechanical force on the membrane so that it causes the vesicle membrane and presynaptic membrane to fuse. This fusion opens a membrane pore that releases the neurotransmitter. The pore increases in size until the entire vesicle membrane is indistinguishable from the presynaptic membrane.\nSynaptic vesicle cycle\nThe presynaptic bouton has an efficiently orchestrated process to fuse vesicles to the presynaptic membrane to release neurotransmitters and regenerate neurotransmitter vesicles. This process called the synaptic vesicle cycle maintains the number of vesicles in the presynaptic bouton and allows the synaptic terminal to be an autonomous unit. The cycle begins with (1) a region of the golgi apparatus is pinched off to form the synaptic vesicle and this vesicle is transported to the synaptic terminal. At the terminal (2) the vesicle is filled with neurotransmitter. (3) The vesicle is transported to the active zone and docked in close proximity to the plasma membrane. (4) During an action potential the vesicle is fuses with the membrane, releases the neurotransmitter and allows the membrane proteins previously on the vesicle to diffuse to the peri-active zone. (5) In the peri-active zone the membrane proteins are sequestered and are endocytosed forming a clathrin coated vesicle. (6) The vesicle is then filled with neurotransmitter and is then transported back to the active zone.\nThe endocytosis mechanism is slower than the exocytosis mechanism. This means that in intense activity the vesicle in the terminal can become depleted and no longer available to be released. To help prevent the depletion of synaptic vesicles the increase in calcium during intense activity can activate calcineurin which dephosphorylate proteins involved in clathrin-mediated endocytosis.\nThe synapse contains at least two clusters of synaptic vesicles, the readily releasable pool and the reserve pool. The readily releasable pool is located within the active zone and connected directly to the presynaptic membrane while the reserve pool is clustered by cytoskeletal and is not directly connected to the active zone.\nThe releasable pool is located in the active zone and is bound directly to the presynaptic membrane. It is stabilized by proteins within the active zone and bound to the presynaptic membrane by SNARE proteins. These vesicles are ready to release by a single action potential and are replenished by vesicles from the reserve pool. The releasable pool is sometimes subdivided into the readily releasable pool and the releasable pool.\nThe reserve pool is not directly connected to the active zone. The increase in presynaptic calcium concentration activates the calcium sensitive phosphatase, calcineurin. Calcineurin dephosphorylates a protein, synapsin, that mediates the clustering of the reserve pool vesicles. Dephosphorylation of synapsin mobilize vesicles in the reserve pool and allows the vesicles to migrate to the active zone and replenish the readily releasable pool.\nThe periactive zone surrounds the active zone and is the site of endocytosis of the presynaptic terminal. In the periactive zone, scaffolding proteins such as intersectin 1 recruit proteins that mediate endocytotis such as dynamin, clathrin and endophilin. In Drosophilia the intersectin homolog, Dap160, is located in the periactive zone of the neuromuscular junction and mutant Dap160 deplete synaptic vesicles during high frequency stimulation.\nRibbon Synapse Active Zone\nThe ribbon synapse is a special type of synapse found in sensory neurons such as photoreceptor cells, retinal bipolar cells, and hair cells. Ribbon synapses contain a dense protein structure that tethers an array of vesicles perpendicular to the presynaptic membrane. In an electron micrograph it appears as a ribbon like structure perpendicular to the membrane. Unlike the 'traditional' synapse, ribbon synapses can maintain a graded release of vesicles. In other words the more depolarized a neuron the higher the rate of vesicle fusion. The Ribbon synapse active zone is separated into two regions, the archiform density and the ribbon. The archiform density is the site of vesicle fusion and the ribbon stores the releasable pool of vesicles. The ribbon structure is composed primarily of the protein RIBEYE, about 64-69% of the ribbon volume, and is tethered to the archiform density by scaffolding proteins such as Bassoon.\nProteins of the Active Zone\n|ELKS (ERCs or CAST)|\n|Docking and Priming|\n|syntaxin||Located on the synaptic membrane and binds to SNAP-25 and synaptobrevin to mediate vesicle fusion.|\n|Voltage-dependent calcium channel (VDCC)||Allows the rapid influx of calcium during an action potential.|\nMeasuring Neurotransmitter Release\nNeurotransmitter release can be measured by determining the amplitude of the postsynaptic potential after triggering an action potential in the presynaptic neuron. Measuring neurotransmitter release this way can be problematic because the effect of the postsynaptic neuron to the same amount of released neurotransmitter can change over time. Another way is to measure vesicle fusion with the presynaptic membrane directly using a patch pipette. A cell membrane can be thought of as a capacitor in that positive and negative ions are stored on both sides of the membrane. The larger the area of membrane the more ions that are necessary to hold the membrane at a certain potential. In electrophysiology this means that a current injection into the terminal will take less time to charge a membrane to a given potential before vesicle fusion than it will after vesicle fusion. The time course to charge the membrane to a potential and the resistance of the membrane is measured and with these values the capacitance of the membrane can be calculated by the equation Tau/Resistance=Capacitance. With this technique researchers can measure synaptic vesicle release directly by measuring increases in the membrane capacitance of the presynaptic terminal.\n- Chemical synapse\n- Neurotransmitter vesicle\n- Vesicle fusion\n- Paired Pulse Facilitation\n- Craig C. Garner and Kang Shen. Structure and Function of Vertebrate and Invertebrate Active Zones. Structure and Functional Organization of the Synapse. Ed: Johannes Hell and Michael Ehlers. Springer, 2008.\n- R. Grace Zhai and Hugo J. Bellen. The Architecture of the Active Zone in the Presynaptic Nerve Terminal. Physiology 19:262-270, 2004.\n- Phillips GR et al. The presynaptic particle web: ultrastructure, composition, dissolution, and reconstitution. Neuron 32: 63–77, 2001\n- Mark L. Harlow et al. The architecture of active zone material at the frog's. neuromuscular junction. NATURE, VOL 409, 25 JANUARY 2001\n- Siksou et al. Three-Dimensional Architecture of Presynaptic Terminal Cytomatrix. The Journal of Neuroscience, June 27, 2007 • 27(26):6868–6877\n- Ziv and Garner. CELLULAR AND MOLECULARMECHANISMS OF PRESYNAPTIC ASSEMBLY. VOLUME 5, MAY 2004, 385 - 399.\n- Georgiev, Danko D .; James F . Glazebrook (2007). \"Subneuronal processing of information by solitary waves and stochastic processes\". In Lyshevski, Sergey Edward. Nano and Molecular Electronics Handbook. Nano and Microengineering Series. CRC Press. pp. 17–1–17–41. ISBN 978-0-8493-8528-5.\n- Heidelberger et al.(1994) Calcium dependence of the rate of exocytosis in a synaptic terminal. Nature. Vol. 371. 513-515\n- Stanley EF Single calcium channels and acetylcholine release at a presynaptic nerve terminal. Neuron 11:1007 (1993)\n- Atasoy and Kavalali. Neurotransmitter Release Machinery: Components of the Neuronal SNARE Complex and Their Function. Structural and Functional Orgnanization of the Synapse Hell and Ehlers (eds.) 2008\n- Z. Pang and T. Sudhof. Cell biology of Ca2+-triggered exocytosisCurrent Opinion in Cell Biology. Volume 22, Issue 4, August 2010, Pages 496-505\n- C. Carr and M. Munson. Tag team action at the synapse. EMBO reports (2007) 8, 834 - 838\n- Nadja Jung and Volker Haucke. Clathrin-Mediated Endocytosis at Synapses. Volume 8, Issue 9, pages 1129–1136, September 2007\n- Cesca et al. (2010) The synapsins: Key actors of synapse function and plasticity. Progress in Neurobiology. Vol. 91. 313-348.\n- Dergai et al. Intersectin 1 forms complexes with SGIP1 and Reps1 in clathrin-coated pits. Biochemical and Biophysical Research Communications. Volume 402, Issue 2, 12 November 2010, Pages 408-413\n- Marie et al. Dap160/Intersectin Scaffolds the Periactive Zone to Achieve High-Fidelity Endocytosis and Normal Synaptic Growth. Neuron. Volume 43, Issue 2, 22 July 2004, Pages 207-219\n- George Zanazzi & Gary Matthews. The Molecular Architecture of Ribbon Presynaptic Terminals.Mol Neurobiol (2009) 39:130-148\n- Gersdorff H. and Matthews G. (1994) Dynamics of synaptic vesicle fusion and membrane retrieval in synaptic terminals. Nature. Vol 367. 735-739"],"question_categories":[{"categorization_name":"answer_type","category_name":"procedural"},{"categorization_name":"multilingual_categorization","category_name":"monolingual"},{"categorization_name":"answer_length","category_name":"detailed"},{"categorization_name":"question_formulation","category_name":"natural_simple"},{"categorization_name":"question_length","category_name":"short"}],"user_categories":[{"categorization_name":"user_language_proficiency","category_name":"spanish_native_fluent"},{"categorization_name":"user_profile","category_name":"novice_inquirer"},{"categorization_name":"user_follow_up_likelihood","category_name":"likely_follow_up"},{"categorization_name":"user_preferred_response_style","category_name":"structured_comparison"}],"document_ids":["<urn:uuid:f439abdd-32e8-4517-800a-f86235014da3>"],"error":null}